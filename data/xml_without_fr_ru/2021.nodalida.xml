<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.nodalida">
  <volume id="main" ingest-date="2021-05-31">
    <meta>
      <booktitle>Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)</booktitle>
      <editor><first>Simon</first><last>Dobnik</last></editor>
      <editor><first>Lilja</first><last>Øvrelid</last></editor>
      <publisher>Linköping University Electronic Press, Sweden</publisher>
      <address>Reykjavik, Iceland (Online)</address>
      <month>May 31--2 June</month>
      <year>2021</year>
      <url hash="d102b425">2021.nodalida-main</url>
    </meta>
    <frontmatter>
      <url hash="4da07233">2021.nodalida-main.0</url>
      <bibkey>nodalida-2021-nordic</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Operationalizing a National Digital Library : The Case for a Norwegian Transformer Model<fixed-case>N</fixed-case>orwegian Transformer Model</title>
      <author><first>Per E</first><last>Kummervold</last></author>
      <author><first>Javier</first><last>De la Rosa</last></author>
      <author><first>Freddy</first><last>Wetjen</last></author>
      <author><first>Svein Arne</first><last>Brygfjeld</last></author>
      <pages>20–29</pages>
      <abstract>In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library. The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian</a> outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian Bokml</a> and <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian Nynorsk</a>. Our model also improves the mBERT performance for other languages present in the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> such as <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a>, and <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>. For <a href="https://en.wikipedia.org/wiki/Language">languages</a> not included in the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>, the weights degrade moderately while keeping strong multilingual properties. Therefore, we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.</abstract>
      <url hash="51be5fb2">2021.nodalida-main.3</url>
      <bibkey>kummervold-etal-2021-operationalizing</bibkey>
    </paper>
    <paper id="4">
      <title>Large-Scale Contextualised Language Modelling for <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian</a><fixed-case>N</fixed-case>orwegian</title>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <pages>30–40</pages>
      <abstract>We present the ongoing NorLM initiative to support the creation and use of very large contextualised language models for <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian</a> (and in principle other Nordic languages), including a ready-to-use software environment, as well as an experience report for data preparation and training. This paper introduces the first large-scale monolingual language models for <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian</a>, based on both the ELMo and BERT frameworks. In addition to detailing the training process, we present contrastive benchmark results on a suite of NLP tasks for <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian</a>. For additional background and access to the data, <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, and software, please see : http://norlm.nlpl.eu</abstract>
      <url hash="a9eecc98">2021.nodalida-main.4</url>
      <bibkey>kutuzov-etal-2021-large</bibkey>
      <pwccode url="https://github.com/ltgoslo/NorBERT" additional="true">ltgoslo/NorBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/norec-fine">NoReC_fine</pwcdataset>
    </paper>
    <paper id="11">
      <title>A Baseline Document Planning Method for <a href="https://en.wikipedia.org/wiki/Automated_journalism">Automated Journalism</a></title>
      <author><first>Leo</first><last>Leppänen</last></author>
      <author><first>Hannu</first><last>Toivonen</last></author>
      <pages>101–111</pages>
      <abstract>In this work, we present a method for content selection and document planning for automated news and report generation from structured statistical data such as that offered by the European Union’s statistical agency, <a href="https://en.wikipedia.org/wiki/EuroStat">EuroStat</a>. The <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is driven by the data and is highly topic-independent within the <a href="https://en.wikipedia.org/wiki/Data_set">statistical dataset domain</a>. As our approach is not based on <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>, it is suitable for introducing news automation to the wide variety of domains where no <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data</a> is available. As such, it is suitable as a low-cost (in terms of implementation effort) baseline for document structuring prior to introduction of domain-specific knowledge.</abstract>
      <url hash="0f5954ac">2021.nodalida-main.11</url>
      <bibkey>leppanen-toivonen-2021-baseline</bibkey>
    </paper>
    <paper id="16">
      <title>Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification</title>
      <author><first>Samuel</first><last>Rönnqvist</last></author>
      <author><first>Valtteri</first><last>Skantsi</last></author>
      <author><first>Miika</first><last>Oinonen</last></author>
      <author><first>Veronika</first><last>Laippala</last></author>
      <pages>157–165</pages>
      <abstract>This article studies register classification of documents from the unrestricted web, such as news articles or opinion blogs, in a multilingual setting, exploring both the benefit of training on multiple languages and the capabilities for zero-shot cross-lingual transfer. While the wide range of linguistic variation found on the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a> poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved. In this study, we show that training on multiple languages 1) benefits languages with limited amounts of register-annotated data, 2) on average achieves performance on par with monolingual models, and 3) greatly improves upon previous zero-shot results in <a href="https://en.wikipedia.org/wiki/Finnish_language">Finnish</a>, <a href="https://en.wikipedia.org/wiki/French_language">French</a> and <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a>. The best results are achieved with the multilingual XLM-R model. As data, we use the CORE corpus series featuring register annotated data from the unrestricted web.</abstract>
      <url hash="271cadbf">2021.nodalida-main.16</url>
      <bibkey>ronnqvist-etal-2021-multilingual</bibkey>
    <title_pt>Multilíngue e Zero-Shot estão se aproximando da classificação de registro monolíngue da Web</title_pt>
      <title_ar>متعدد اللغات و Zero-Shot يقترب من تصنيف سجل الويب أحادي اللغة</title_ar>
      <title_es>Multilingüe y Zero-Shot se acerca a la clasificación de registros web monolingües</title_es>
      <title_hi>बहुभाषी और शून्य-शॉट मोनोलिंगुअल वेब रजिस्टर वर्गीकरण पर बंद हो रहा है</title_hi>
      <title_zh>多言与零镜头方近单语网络寄存器类</title_zh>
      <title_ja>多言語およびゼロショットは、単語ウェブレジスタ分類でクローズインしています</title_ja>
      <title_ga>Tá Ilteangach agus Urchar Nialasach ag druidim le hAicmiú Clár Gréasáin Aonteangach</title_ga>
      <title_hu>Többnyelvű és Zero-Shot bezáródik az egynyelvű webes regiszterek osztályozására</title_hu>
      <title_ka>მრავალენგური და ნულ- სურათი კლასიფიკაციაში დახურება</title_ka>
      <title_it>Multilingue e Zero-Shot si stanno avvicinando alla classificazione del registro web monolingue</title_it>
      <title_kk>Көп тілді және нөл- жол бір тілді Веб регистр классификациясында жабылады</title_kk>
      <title_lt>Daugiakalbis ir nulinis tirpalas užbaigiamas Monolingual Web Registry klasifikacijoje</title_lt>
      <title_mk>Мултијазичен и нула- пукање се затвора на класификацијата на монолингвален веб- регистар</title_mk>
      <title_ml>മണോളില്‍ വെബ് റിജിസ്റ്റര്‍ ക്ലാസിഷനില്‍ അടച്ചുകൊണ്ടിരിക്കുന്നു</title_ml>
      <title_ms>Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification</title_ms>
      <title_el>Η πολύγλωσση και μηδενική βολή πλησιάζει στο μονογλωσσικό μητρώο ιστού Ταξινόμηση</title_el>
      <title_mt>Il-Klassifikazzjoni tar-Reġistru Monolingwali tal-Internet Multilingual u Zero-Shot qed tingħalaq</title_mt>
      <title_no>Fleirspråk og null-skytt lukkar i ved å klassifisera mellomspråk nettregistrering</title_no>
      <title_ro>Multilingv și Zero-Shot se apropie de clasificarea registrului web monolingv</title_ro>
      <title_sr>Većina jezika i nula pucnjava se zatvara u klasifikaciji jednojezičkog web registra</title_sr>
      <title_si>Multilanguage and Zero-Shot are close in Monolingual Web Recorder Classified</title_si>
      <title_so>Luuqado badan iyo suurtogal-shoo waxay ku xidhan yihiin fasaxa diiwaangelinta internetka ee Monolingual</title_so>
      <title_sv>Flerspråkiga och Zero-Shot närmar sig en enspråkig webbregisterklassificering</title_sv>
      <title_mn>Олон хэл болон Zero-Shot нь Монолингийн Веб Регистр Классификацийн хувьд</title_mn>
      <title_ta>பல மொழி மற்றும் பூஜ்ஜி- ஷாட் மோனோலிங்கல் இணைய பதிவு வகைப்படுத்தலில் மூடுகிறது</title_ta>
      <title_pl>Wielojęzyczny i Zero-Shot zbliża się do jednojęzycznego rejestru internetowego</title_pl>
      <title_ur>Multilingual and Zero-Shot are closing in Monolingual Web Register Classification</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Nhiều ngôn ngữ và Zero-Shot đang đóng cửa vào đơn vị Đánh dấu mạng.</title_vi>
      <title_hr>Većina jezika i nula pucnjava se zatvara na jednojezičkom registraciji web registracije</title_hr>
      <title_bg>Многоезичният и нулевият изстрел наближава класификацията на едноезичния уеб регистър</title_bg>
      <title_nl>Meertalig en Zero-Shot nadert op Monolingual Web Register Classification</title_nl>
      <title_ko>다중 언어와 제로 렌즈가 단어 네트워크 어역 분류에 접근하고 있습니다</title_ko>
      <title_da>Flersproget og Zero-Shot er ved at lukke ind på ensproget webregisterklassificering</title_da>
      <title_id>Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification</title_id>
      <title_sw>Mpigo wa lugha nyingi na risasi zisizo na maandishi yanafungwa kwenye tovuti ya uandikishaji wa Kimonolinguli</title_sw>
      <title_tr>Çoklu diller we Zero-Shot Monoli Dilli Web Register Seçgisinde ýapylýar</title_tr>
      <title_fa>تعداد زبان‌های زیادی و صفر-شلیک در کلاس ثبت‌نامه‌ی وب یک زبان بسته می‌شود</title_fa>
      <title_af>Multilingual en Zero-Shot sluit op Monolingueel Web Register Klassifikasie</title_af>
      <title_sq>Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification</title_sq>
      <title_hy>Բազլեզու և զրո-կրակը փակվում է MonoLingue վեբ ռեժիստրի դասակարգում</title_hy>
      <title_az>Çoxlu dil və Sıfır-Shot Monoling Web Register Klasifikasyonunda Qapılır</title_az>
      <title_de>Multilingual und Zero-Shot nähern sich der Monolingual Web Register Klassifizierung</title_de>
      <title_ca>Multilingüe i zero-Shot s'encerra a la classificació del registre web monolingüe</title_ca>
      <title_cs>Vícejazyčný a Zero-Shot se blíží na jednojjazyčném webovém rejstříku Klasifikace</title_cs>
      <title_et>Mitmekeelne ja Zero-Shot läheneb ühekeelsele veebiregistri klassifikatsioonile</title_et>
      <title_am>Multilingual and Zero-Shot are closing in Monolingual Web Register Classification</title_am>
      <title_bs>Većina jezika i nula pucnjava se zatvara u klasifikaciji jednojezičkog web registra</title_bs>
      <title_bn>মোনোলোলিভাল ওয়েব রেজিস্টার ক্লাসিকেশনে অনেক ভাষা এবং শুট বন্ধ হচ্ছে</title_bn>
      <title_fi>Monikielinen ja Zero-Shot lähestyvät monikielistä verkkorekisteriä</title_fi>
      <title_jv>Multilenguang lan nulo-shot iku diputalo ning Monolngual web regiter</title_jv>
      <title_sk>Večjezični in Zero-Shot se približujeta enojezični spletni register Klasifikacija</title_sk>
      <title_he>רישום רב-שפתי ואפס סוגרים את שיעור רשום האינטרנט Monolingual</title_he>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_bo>སྐད་རིགས་དབྱེ་སྤྱོད་དང་Zero-Shot Monolingual Web Register སྒྲིག་འཛིན་གྱི་ནང་དུ་སྒོ་རྒྱག་ཡོད་པ</title_bo>
      <abstract_ar>تسجل دراسات هذه المقالة تصنيف المستندات من الويب غير المقيد ، مثل المقالات الإخبارية أو مدونات الرأي ، في بيئة متعددة اللغات ، واستكشاف كل من فوائد التدريب على لغات متعددة وإمكانيات النقل عبر اللغات بدون طلقة. في حين أن النطاق الواسع للتنوع اللغوي الموجود على الويب يشكل تحديات لتصنيف التسجيل ، فقد أظهرت الدراسات الحديثة أنه يمكن تحقيق مستويات جيدة من النقل عبر اللغات من مجموعة CORE الإنجليزية الشاملة إلى لغات أخرى. في هذه الدراسة ، نوضح أن التدريب على لغات متعددة 1) يفيد اللغات بكميات محدودة من البيانات المشروحة بالتسجيل ، 2) يحقق متوسط الأداء على قدم المساواة مع النماذج أحادية اللغة ، و 3) يتحسن بشكل كبير على النتائج الصفرية السابقة باللغة الفنلندية ، الفرنسية والسويدية. يتم تحقيق أفضل النتائج مع نموذج XLM-R متعدد اللغات. كبيانات ، نستخدم سلسلة CORE corpus التي تعرض تسجيل البيانات المشروحة من الويب غير المقيد.</abstract_ar>
      <abstract_pt>Este artigo estuda a classificação de registros de documentos da web irrestrita, como artigos de notícias ou blogs de opinião, em um ambiente multilíngue, explorando tanto o benefício do treinamento em vários idiomas quanto os recursos para transferência de vários idiomas. Embora a ampla variedade de variações linguísticas encontradas na web represente desafios para a classificação de registros, estudos recentes mostraram que bons níveis de transferência entre idiomas do extenso corpus CORE inglês para outros idiomas podem ser alcançados. Neste estudo, mostramos que o treinamento em vários idiomas 1) beneficia idiomas com quantidades limitadas de dados anotados por registro, 2) em média alcança desempenho equivalente a modelos monolíngues e 3) melhora muito os resultados anteriores de tiro zero em finlandês, francês e sueco. Os melhores resultados são alcançados com o modelo multilíngue XLM-R. Como dados, utilizamos a série CORE corpus com dados cadastrais anotados da web irrestrita.</abstract_pt>
      <abstract_es>Este artículo estudia la clasificación de registros de documentos de la web sin restricciones, como artículos de noticias o blogs de opinión, en un entorno multilingüe, explorando tanto el beneficio de la capacitación en múltiples idiomas como las capacidades de transferencia interlingüística sin posibilidad de transferencia interlingüística. Si bien la amplia gama de variaciones lingüísticas que se encuentran en la web plantea desafíos para la clasificación de registros, estudios recientes han demostrado que se pueden lograr buenos niveles de transferencia multilingüe del extenso corpus CORE en inglés a otros idiomas. En este estudio, mostramos que la capacitación en varios idiomas 1) beneficia a los idiomas con cantidades limitadas de datos anotados por el registro, 2) en promedio logra un rendimiento a la par de los modelos monolingües y 3) mejora en gran medida los resultados previos de tiro cero en finés, francés y sueco. Los mejores resultados se obtienen con el modelo XLM-R multilingüe. Como datos, utilizamos la serie de corpus CORE que incluye datos anotados de registro de la web sin restricciones.</abstract_es>
      <abstract_zh>本文究于多言之境网络(如新闻文论博客)注册分类于文档,讨多种语言培训之益,及零次跨语移之功。 虽于网络上见广言语异于注册分类,而近者研明,从广英语CORE语料库到他语之跨言移水平可致也。 于此论之,多种语言之教1)使注册注有限之言受益,2)均至与单语相似,3)大改前芬兰语,法语与瑞典语之零拍摄也。 用多言 XLM-R 模可获得最佳效。 为数者,吾用CORE语料库系列,其包自不拘者网络注注数也。</abstract_zh>
      <abstract_ja>この記事では、多言語環境でのニュース記事やオピニオンブログなどの制限のないウェブからの文書の分類を研究し、複数言語でのトレーニングの利点とゼロショットのクロスリンガル転送機能の両方を探求します。ウェブ上で発見された幅広い言語変動は、レジスタ分類に課題をもたらすが、最近の研究では、広範な英語のコアコーパスから他の言語への良好なレベルのクロスリンガル転送を達成できることが示されている。この研究では、複数の言語に関するトレーニングが、1 ）登録されたデータの量が限られている言語に利益をもたらすこと、2 ）平均的に単一言語モデルと同等のパフォーマンスを達成すること、3 ）以前のフィンランド語、フランス語、スウェーデン語のゼロショット結果よりも大幅に改善することを示しています。最良の結果は、多言語XLM - Rモデルで達成されます。データとしては、制限のないウェブからの注釈付きデータを登録することを特徴とするコアコーパスシリーズを使用しています。</abstract_ja>
      <abstract_hi>यह लेख अप्रतिबंधित वेब से दस्तावेजों के वर्गीकरण को पंजीकृत करता है, जैसे कि समाचार लेख या राय ब्लॉग, एक बहुभाषी सेटिंग में, कई भाषाओं पर प्रशिक्षण के लाभ और शून्य-शॉट क्रॉस-लिंगुअल ट्रांसफर की क्षमताओं दोनों की खोज करते हैं। जबकि वेब पर पाए जाने वाले भाषाई भिन्नता की विस्तृत श्रृंखला रजिस्टर वर्गीकरण के लिए चुनौतियां पैदा करती है, हाल के अध्ययनों से पता चला है कि व्यापक अंग्रेजी कोर कॉर्पस से अन्य भाषाओं में क्रॉस-लिंगुअल हस्तांतरण के अच्छे स्तर प्राप्त किए जा सकते हैं। इस अध्ययन में, हम दिखाते हैं कि कई भाषाओं पर प्रशिक्षण 1) सीमित मात्रा में रजिस्टर-एनोटेटेड डेटा के साथ भाषाओं को लाभ पहुंचाता है, 2) औसतन मोनोलिंगुअल मॉडल के बराबर प्रदर्शन प्राप्त करता है, और 3) फिनिश, फ्रेंच और स्वीडिश में पिछले शून्य-शॉट परिणामों पर बहुत सुधार करता है। बहुभाषी XLM-R मॉडल के साथ सर्वोत्तम परिणाम प्राप्त किए जाते हैं। डेटा के रूप में, हम अप्रतिबंधित वेब से रजिस्टर एनोटेट डेटा की विशेषता वाली कोर कॉर्पस श्रृंखला का उपयोग करते हैं।</abstract_hi>
      <abstract_ga>Déanann an t-alt seo staidéar ar aicmiú doiciméad ón ngréasán neamhshrianta, ar nós ailt nuachta nó blaganna tuairime, i suíomh ilteangach, ag fiosrú an tairbhe a bhaineann le hoiliúint ar iltheangacha agus na hacmhainní le haghaidh aistrithe trasteangacha gan urchar. Cé go gcruthaíonn an raon leathan éagsúlachta teanga atá le fáil ar an ngréasán dúshláin maidir le haicmiú cláir, léirigh staidéir a rinneadh le déanaí gur féidir leibhéil mhaithe aistrithe tras-teanga a bhaint amach ó chorpas fairsing Béarla CORE go teangacha eile. Sa staidéar seo, léirímid go dtéann oiliúint ar iltheangacha 1) chun sochair do theangacha le méideanna teoranta sonraí cláraithe, 2) go mbaintear amach feidhmíocht ar chomhchéim le samhlacha aonteangacha, agus 3) go bhfeabhsaítear go mór é ar thorthaí nialasacha san Fhionlainnis roimhe seo, Fraincis agus Sualainnis. Baintear na torthaí is fearr amach leis an tsamhail ilteangach XLM-R. Mar shonraí, úsáidimid an tsraith CORE corpus ina bhfuil sonraí anótáilte cláir ón ngréasán neamhshrianta.</abstract_ga>
      <abstract_hu>Ez a cikk a korlátozás nélküli internetről származó dokumentumok, például hírek vagy vélemény blogok osztályozását tanulmányozza többnyelvű környezetben, feltárva mind a többnyelvű képzés előnyeit, mind pedig a nulla-shot keresztnyelvű transzfer lehetőségeit. Míg az interneten található nyelvi variációk széles skálája kihívásokat jelent a nyilvántartások besorolása szempontjából, a közelmúltbeli tanulmányok azt mutatták, hogy a kiterjedt angol CORE korpusztól más nyelvekre való átvitel jó szintje érhető el. Ebben a tanulmányban azt mutatjuk, hogy a több nyelven folytatott képzés 1) korlátozott mennyiségű regisztrációs adattal rendelkező nyelveket használ, 2) átlagosan az egynyelvű modellekkel egyenlő teljesítményt ér el, és 3) jelentősen javul a korábbi nulla-shot eredményekhez képest finn, francia és svéd nyelven. A legjobb eredményeket a többnyelvű XLM-R modell érheti el. Adatként a korlátozás nélküli internetről származó jegyzetelt adatokat tartalmazó CORE corpus sorozatot használjuk.</abstract_hu>
      <abstract_el>Αυτό το άρθρο μελετά την ταξινόμηση εγγράφων από τον απεριόριστο ιστό, όπως άρθρα ειδήσεων ή ιστολόγια γνώμης, σε ένα πολύγλωσσο περιβάλλον, διερευνώντας τόσο το όφελος της κατάρτισης σε πολλές γλώσσες όσο και τις δυνατότητες για μηδενική διασυνοριακή μεταφορά. Ενώ το ευρύ φάσμα γλωσσικών παραλλαγών που υπάρχουν στο διαδίκτυο θέτει προκλήσεις για την ταξινόμηση μητρώων, πρόσφατες μελέτες έχουν δείξει ότι μπορούν να επιτευχθούν καλά επίπεδα διασυνοριακής μεταφοράς από το εκτεταμένο αγγλικό σώμα CORE σε άλλες γλώσσες. Στην παρούσα μελέτη, καταδεικνύουμε ότι η εκπαίδευση σε πολλαπλές γλώσσες 1) ωφελεί τις γλώσσες με περιορισμένες ποσότητες σχολιασμένων δεδομένων, 2) κατά μέσο όρο επιτυγχάνει απόδοση ίση με τα μονογλωσσικά μοντέλα, και 3) βελτιώνει σημαντικά σε σχέση με τα προηγούμενα αποτελέσματα μηδενικής λήψης στα φινλανδικά, γαλλικά και σουηδικά. Τα καλύτερα αποτελέσματα επιτυγχάνονται με το πολύγλωσσο μοντέλο. Ως δεδομένα, χρησιμοποιούμε τη σειρά Corpus CORE που περιλαμβάνει δεδομένα με σχόλια μητρώου από τον απεριόριστο ιστό.</abstract_el>
      <abstract_ka>ამ წესტის შესწავლობა უფრო მრავალური წესების კლასიფიკაცია, როგორც ახალგაზრულები ან მინდომის ბლოგები, მრავალური წესების კონფიკაციაში, მრავალური წესების გამოსახულების გამოსახულების და უფ თუმცა ინგლისტიკური განსხვავებები, რომელიც ინგლისტიკური კორიფიკაციის განსხვავებულია, განსხვავებული კორიფიკაციის განსხვავებებისთვის განსხვავებულია, შემდეგ განსხვავებული კორიფიკაციის შესაძლებელია, რომ ამ კვლევაში ჩვენ ჩვენ აჩვენებთ, რომ მრავალ ენაზე განაკეთება 1) გამოიყენება ენაზე, რომლებიც რეგისტრის ანოტირებული მონაცემების ზომა, 2) სინამდვილეში მონაცემებით მონაცემებით მონაცემებით მონაცემებით, და 3) წ ყველაზე საუკეთესო შედეგი მოდელი იქნება მრავალენგური XLM-R მოდელზე. როგორც მონაცემები, ჩვენ გამოყენებთ CORE corpus სერია, რომელსაც რეგისტრისტის მონაცემები არსებული ინტერფეტიდან გამოყენება.</abstract_ka>
      <abstract_it>Questo articolo studia la classificazione dei documenti provenienti dal web senza restrizioni, come articoli di notizie o blog di opinione, in un ambiente multilingue, esplorando sia i vantaggi della formazione su più lingue che le capacità di trasferimento cross-lingual zero shot. Mentre l'ampia gamma di variazioni linguistiche rilevate sul web pone sfide per la classificazione dei registri, studi recenti hanno dimostrato che è possibile raggiungere buoni livelli di trasferimento translinguale dall'ampio corpus inglese CORE ad altre lingue. In questo studio, mostriamo che la formazione su più lingue 1) favorisce le lingue con quantità limitate di dati annotati nel registro, 2) raggiunge in media prestazioni alla pari dei modelli monolingue e 3) migliora notevolmente rispetto ai precedenti risultati zero-shot in finlandese, francese e svedese. I migliori risultati si ottengono con il modello XLM-R multilingue. Come dati, utilizziamo la serie CORE corpus con dati annotati dal web senza restrizioni.</abstract_it>
      <abstract_lt>Šiame straipsnyje tiriamas neribotų interneto dokumentų, pavyzdžiui, naujienų straipsnių ar nuomonės blogų, klasifikavimas daugiakalbėje aplinkoje, išnagrinėjant mokymo įvairiomis kalbomis naudą ir galimybes neperdirbti tarpkalbinį perdavimą. Nors internete nustatytas didelis kalbų skirtumas kelia iššūkių registrų klasifikavimui, neseniai atlikti tyrimai parodė, kad galima pasiekti gerą tarpkalbinio perkėlimo lygį iš plataus anglų CORE corpus į kitas kalbas. Šiame tyrime parodome, kad mokymas įvairiomis kalbomis 1) naudingas kalboms, kuriose registruose yra ribotas duomenų kiekis, 2) vidutiniškai pasiekia rezultatus lygiaverčius vienkalbiniams modeliams ir 3) gerokai pagerėja ankstesnių nulinių rezultatų suomių, prancūzų ir švedų kalbomis. Geriausi rezultatai pasiekti naudojant daugiakalbį XLM-R model į. Kaip duomenys naudojame CORE corpus seriją, kurioje registruojami neribotos interneto anotacijos duomenys.</abstract_lt>
      <abstract_kk>Бұл мақала, жаңалық мақалалар немесе ойлау блогтар секілді, көптеген тілдерде, бірнеше тілдердің оқыту мүмкіндіктерін және нөл тілдердің көптеген аудару мүмкіндіктерін зерттеу үшін құжаттарды сақтау жүйесін зерт Интернетте табылған лингвистикалық айырмашылығының көпшілігі категориялау үшін өзгерістерді көрсетеді, соңғы зерттеулер тілдерді көпшілікті CORE корпусынан басқа тілдерге жеткізуге болады. Бұл зерттеулерде бірнеше тілдерді оқыту (1) тілдерді көмектесу үшін көмектесетін деректердің шектелген мөлшерлері бар, 2) орташа бірнеше тілдердің монолингі үлгілері бар, және 3) алдыңғы нөл шарттарының Финляндия, Француз Ең жақсы нәтижелер көп тілді XLM- R үлгісімен жетілді. Деректер үшін CORE корпус сериясын қолданамыз. Керек емес веб- тегінен жазылған деректерді көрсетеді.</abstract_kk>
      <abstract_mk>Оваа статија ја проучува класификацијата на документите од неограничениот веб, како што се новинските статии или блоговите за мислење, во мултијазични услови, истражувајќи ја и користта од обуката на повеќе јазици како и способностите за нула-снимка крстојазичен трансфер. И покрај тоа што широкиот опсег на јазични варијации пронајдени на веб-страницата претставува предизвици за класификација на регистарите, неодамнешните студии покажаа дека може да се постигнат добри нивоа на прекујазичен трансфер од екстремниот англиски корпус CORE на други јази Во оваа студија покажуваме дека обуката на повеќекратни јазици 1) има корист од јазиците со ограничени количини на регистарски анотирани податоци, 2) во просек постигнува резултати во споредба со монојазичните модели и 3) значително се подобрува со претходните резултати со нула снимка на фински, француски Најдобрите резултати се постигнати со мултијазичкиот модел XLM-R. Како податоци, ја користиме серијата CORE corpus со регистрирање на анотирани податоци од неограничен интернет.</abstract_mk>
      <abstract_ms>Artikel ini mempelajari klasifikasi dokumen dari web yang tidak terhalang, seperti artikel berita atau blog pendapat, dalam tetapan berbilang bahasa, mengeksplorasi kedua-dua keuntungan latihan dalam bahasa berbilang dan kemampuan untuk pemindahan saling bahasa tanpa tembakan sifar. Sementara julat luas variasi bahasa yang ditemui di web mengakibatkan cabaran untuk klasifikasi daftar, kajian baru-baru ini menunjukkan bahawa tahap yang baik pemindahan saling bahasa dari CORE corpus Inggeris yang luas ke bahasa lain boleh dicapai. Dalam kajian ini, kami menunjukkan bahawa latihan dalam bahasa berbilang 1) bahasa keuntungan dengan jumlah terbatas data yang dicatat-daftar, 2) rata-rata mencapai prestasi sama dengan model monobahasa, dan 3) meningkat jauh pada keputusan 0-shot terdahulu dalam bahasa Finlandia, Perancis dan Swedia. Hasil terbaik dicapai dengan model XLM-R berbilang bahasa. Sebagai data, kami menggunakan siri CORE corpus yang mengandungi daftar data yang dicatat dari web yang tidak terhalang.</abstract_ms>
      <abstract_ml>വെബില്‍ നിന്നും അസ്ഥിരപ്പെടാത്ത രേഖകളുടെ വിവരങ്ങളുടെ ക്ലാസ്ഫികേഷന്‍ രേജിസ്റ്റ് ചെയ്യുന്നു. വാര്‍ത്തകള്‍ അല്ലെങ്കില്‍ തിരിച്ചറിയുന്ന വ്യാഖ്യാപങ് വെബ് പോസിന്റെ വ്യത്യാസങ്ങളില്‍ കണ്ടെത്തിയിരിക്കുന്ന വ്യത്യാസങ്ങള്‍ക്ക് വേണ്ടി രേജിസ്റ്റര്‍ ക്ലാസ്ഫിക്ഷനുള്ള വിലാസങ്ങള്‍ക്ക് വേണ്ടി വെച്ച്  ഈ പഠനത്തില്‍ നമ്മള്‍ കാണിക്കുന്നു, പല ഭാഷകളില്‍ പരിശീലിക്കുന്നത് നാം കാണിച്ചിരിക്കുന്നു. നിര്‍ണ്ണയിക്കപ്പെട്ട വിവരങ്ങളുടെ കൂട്ടത്തില്‍ നിര്‍ണ്ണയിക്കപ്പെട്ട വ ഏറ്റവും മികച്ച ഫലങ്ങള്‍ ഏറ്റവും മികച്ച ഭാഷ എക്സ്‌എല്‍എംആര്‍ മോഡലില്‍ എത്തിയിരിക്കുന്നു. വിവരങ്ങളായി നമ്മള്‍ കോര്‍പ്പുസ് സീരിസില്‍ ഉപയോഗിക്കുന്നു. റെജിസ്റ്റര്‍ വെബില്‍ നിന്നും വിവരിച്ചുകൊടുക്കുന</abstract_ml>
      <abstract_mn>Энэ баримтын судалгаагаар хэлний хэл дээр дасгал хөдөлгөөн болон тэгш хэлний шилжүүлэх боломжуудыг олон хэлний хэл дээр суралцах боломжтой боломжуудыг судалж байна. Веб дээр олон олон хэлний өөрчлөлт бичиж буй хэлний хэлбэрээс өөрчлөлт бичиж буй хэлбэрээс илүү сайн хэлний шилжүүлэлтийг харуулж байна. Энэ судалгаанд бид олон хэл дээр суралцах нь 1) хэл дээр хязгаарлагдсан өгөгдлийн хэмжээтэй ашигтай, 2) дунджаар нэг хэл загвартай үйл ажиллагааг гаргадаг, 3) Финляндын, Француз, Шведийн өмнөх 0 шат үр дүнд их сайжруулдаг. Хамгийн сайн үр дүн нь олон хэл XLM-R загвартай гарч ирсэн. Өгөгдлийн хувьд бид CORE корпус цувралыг ашиглаж байна. Хязгааргүй веб-ээс зарцуулсан мэдээллийг харуулж байна.</abstract_mn>
      <abstract_no>Denne artikkelen studierer registrering av klassifikasjon av dokument frå ikkje-strekte nettet, slik som nye artikler eller synleg bloggar, i ein fleirspråk innstilling, og utforskar både nyttigheten på trening på fleire språk og kapasiteten for null-shot krysspråk overføring. Mens det brede området av lingviske variasjonar funne på nettet poserer utfordringar for registreringsklassifikasjon, har nyleg studiar vist at gode nivåar av krysspråk-overføring frå den ekstra engelske korpusen CORE til andre språk kan oppnå. I denne studien viser vi at opplæring på fleire språk 1) nyttar språk med begrensede mengdar av registrerte data, 2) gjennomsnittlig gjennomsnittlig gjennomsnittlig gjennomsnittlig gjennomsnittlig utvikling på par med monospråk modeller, og 3) er stort forbetra ved førre nullsatt resultat i finsk, fransk og svensk Den beste resultatene er oppnådd med den fleire språk XLM-R-modellen. Som data, bruker vi CORE-korpusserien som inneheld registrerte data frå nettet utan streking.</abstract_no>
      <abstract_pl>Niniejszy artykuł bada klasyfikację dokumentów pochodzących z nieograniczonej sieci internetowej, takich jak artykuły informacyjne czy blogi opinii, w wielojęzycznym otoczeniu, badając zarówno korzyści płynące ze szkolenia z wielu języków, jak i możliwości zerowego transferu między językami. Podczas gdy szeroki zakres zróżnicowań językowych znajdujących się w internecie stanowi wyzwanie dla klasyfikacji rejestru, ostatnie badania wykazały, że można osiągnąć dobry poziom transferu między językami z obszernego angielskiego korpusu CORE na inne języki. W niniejszym badaniu pokazujemy, że szkolenie z wieloma językami 1) korzysta z języków z ograniczoną ilością danych z adnotacjami rejestru, 2) średnio osiąga wydajność na równi z modelami jednojęzycznymi, a 3) znacznie poprawia wcześniej wyniki zero-shot w języku fińskim, francuskim i szwedzkim. Najlepsze rezultaty osiąga się z wielojęzycznym modelem XLM-R. Jako dane wykorzystujemy serię korpusów CORE zawierającą dane z adnotacji rejestru z nieograniczonej sieci internetowej.</abstract_pl>
      <abstract_ro>Acest articol studiază clasificarea documentelor de pe web fără restricții, cum ar fi articole de știri sau bloguri de opinie, într-un cadru multilingv, explorând atât beneficiile instruirii în mai multe limbi, cât și capacitățile de transfer încrucișat de zero-shot. În timp ce gama largă de variații lingvistice găsite pe internet reprezintă provocări în ceea ce privește clasificarea registrelor, studiile recente au arătat că pot fi atinse niveluri bune de transfer translingvistic de la corpul extins CORE în limba engleză la alte limbi. În acest studiu, demonstrăm că instruirea pe mai multe limbi 1) beneficiază limbile cu cantități limitate de date adnotate în registru, 2) atinge în medie performanțe egale cu modelele monolingve și 3) îmbunătățește considerabil rezultatele anterioare zero-shot în finlandeză, franceză și suedeză. Cele mai bune rezultate sunt obținute cu modelul XLM-R multilingv. Ca date, folosim seria CORE corpus care conține date adnotate de registru de pe web nelimitat.</abstract_ro>
      <abstract_sr>Ovaj članak proučava registriranje klasifikacije dokumenta iz neograničene mreže, kao što su novinski članovi ili blogovi mišljenja, u multijezičkom stanju, istražujući i korist obuke na višestrukim jezicima i mogućnosti za prebacivanje jezika nulog snimka. Iako širok raspon lingvističkih varijacija nalaženih na internetu predstavlja izazove za klasifikaciju registracija, nedavno ispitivanje pokazalo je da se mogu postići dobar nivo prevođenja preko jezika iz širokog engleskog korpusa CORE na druge jezike. U ovom ispitivanju pokazujemo da obuka na višestrukim jezicima 1) koristi jezike sa ograničenim količinama podataka o registraciji, 2) na prosjeku postiže učinkovitost na par sa monojezičkim modelima, a 3) veoma poboljšava na prethodnim rezultatima nule snimanja na finskom, francuskom i švedskom. Najbolji rezultati su postignuti sa multijezičkim XLM-R modelom. Kao podaci, koristimo seriju korpusa CORE-a koja uključuje registraciju annotiranih podataka iz neograničene mreže.</abstract_sr>
      <abstract_si>මේ ලේඛනය අධ්‍යාස කරන්නේ නැති වෙබ් වලින් ලේඛනයේ ලේඛනය ලේඛනය ලේඛනය ලේඛනය ලේඛනය හා විශ්වාස බ්ලෝග් වලින්, බොහොම භාෂාවක සැකසුම් වලින් ප වෙබ් එකේ හොයාගත්ත භාෂාවික වෙනස් විසින් විසින් විසින් විසින් විසින් අවශ්‍ය වෙනුවෙන් විසින් විසින් විසින් විසින් විසින් ප්‍ර මේ පරීක්ෂණයේදී, අපි පෙන්වන්නේ වැඩි භාෂාවල් 1) භාෂාවට ප්‍රයෝජනය කරන්න පුළුවන් භාෂාවට ප්‍රයෝජනය කරන්න, 2) පරීක්ෂණයෙන් ප්‍රයෝජනයෙන් ප්‍රයෝජන හොඳම ප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිය XLM-R ම දත්ත විදියට, අපි CORE කෝර්පුස් සීමාව පාවිච්චි කරනවා, ප්‍රතිස්ථාපනය කරන්නේ නැති වෙබ් වලින් ප්‍රති</abstract_si>
      <abstract_so>Warqaddaas wuxuu ka baranayaa qoraalka qoraalka dukumentiyada ee internetka aan rasmi lahayn, tusaale ahaan warqadaha warqadaha ama bogagga aragtida, kaas oo ku baaraandegaya faa'iidada waxbarashada luuqadaha badan iyo awoodda wareejinta luuqadaha nooca ah. Inta lagu jiro iskuulka diiwaangelinta waxaa laga helaa dhibaatooyin kala duduwan oo luuqadaha kala duduwan, waxbarashada ugu dambeysayna waxay caddaynayeen in heerarka wanaagsan ee laga soo wareejiyo qoraalka afka ingiriisiga ee CORE ilaa luuqadaha kale. Waxbarashadan waxaynu tusnaynaa in waxbarasho ku qoran luuqado kala duduwan 1) lagu faa'iido luuqado ah oo ku qoran macluumaad diiwaangelinta, 2) ugu badnaan waxyaabaha lagu sameyn karo tusaalaha afka noocyada ah, iyo 3) waxey aad u bedeshaa arimaha hore oo lagu qoray Finnish, Faraansiis iyo Iswidishka. Midhaha ugu wanaagsan waxaa lagu helaa modelka XLM-R ee luuqadaha kala duduwan. Wixii macluumaad ah, waxaynu isticmaalnaa safarka CORE oo ku qoran macluumaadka diiwaangelinta oo la caddeeyey bogagga aan la isticmaalin.</abstract_so>
      <abstract_mt>Dan l-artikolu jistudja l-klassifikazzjoni tad-dokumenti mill-internet mhux ristrett, bħal artikoli tal-a ħbarijiet jew blogs tal-opinjonijiet, f’ambjent multilingwi, li jesplora kemm il-benefiċċju tat-taħriġ dwar lingwi multipli kif ukoll il-kapaċitajiet għal trasferiment translingwi b’ritratt żero. Filwaqt li l-firxa wiesgħa ta’ varjazzjoni lingwistika misjuba fuq l-internet to ħloq sfidi għall-klassifikazzjoni tar-reġistri, studji riċenti wrew li jistgħu jinkisbu livelli tajbin ta’ trasferiment translingwistiku mill-CORE corpus estensiv Ingliż għal lingwi oħra. F’dan l-istudju, nagħmlu evidenza li t-taħriġ dwar diversi lingwi 1) jibbenefika lingwi b’ammonti limitati ta’ dejta annotata fir-reġistru, 2) bħala medja jikseb prestazzjoni daqs mudelli monolingwi, u 3) itejjeb ħafna fuq riżultati preċedenti b’ritratt żero fil-Finlandiż, Franċiż u Svediż. L-aħjar riżultati jinkisbu bil-mudell XLM-R multilingwi. Bħala dejta, aħna nużaw is-serje CORE corpus li fiha reġistru tad-dejta annotata mill-internet mhux ristrett.</abstract_mt>
      <abstract_ta>இந்த கட்டுரையில் அறியாத இணையத்திலிருந்து ஆவணங்கள் வகைப்படுத்தலை படிக்கும், செய்தி கட்டுரைகள் அல்லது கருத்துரைகள் போன்ற, பல மொழிகள் அமைப்பில், பல மொழிகளில் பயிற்ச While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved.  இந்த ஆராய்ச்சியில், நாம் பல மொழிகளில் பயிற்சியை காட்டுகிறோம் என்று காண்பிக்கிறோம் அது எல்லாம் பதிவேட்டில் குறிப்பிட்ட தகவல்களுடன் பயன்படுத்தப்படும் மொழிகளில் ஒர பல மொழி XLM-R மாதிரியால் சிறந்த முடிவுகள் அடையப்பட்டது. தகவலாக, நாம் CORE கார்ப்ஸ் தொடர்களை பயன்படுத்துகிறோம் வெளிப்படுத்தப்படாத இணையத்திலிருந்து பதிவு அறிவிக்கப்பட்ட தக</abstract_ta>
      <abstract_ur>یہ لکھا بغیر محدودیت ویب سے لکھی ہوئی دلیلیں کا کلیسٹ کریسٹر کی تعلیم کرتا ہے، جیسے اخبار لکھائی یا منظور بلاگ، ایک بہت سی زبان تنظیم میں، دونوں کی تعلیم کے فائدہ کا تحقیق کرتا ہے بہت سی زبانوں پر اور صفر-شٹ کریسٹ While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that cross-lingual transfer levels from the extensive English CORE corpus to other languages can be achieved. اس تحقیق میں ہم دکھاتے ہیں کہ تعلیم کئی زبانوں پر 1) لکھی زبانوں کا فائدہ پہنچاتا ہے جن کے ذریعہ سے محدودہ دکھائے گئے ہیں، 2) متوسط سے ایک زبان مدل کے ساتھ فائدہ پہنچاتا ہے، اور 3) فنلاندی, فرانسوی اور سوئدی کے پہلے صفر شوٹ نتیجے پر بہت اضافہ ہوتا بہترین نتیجے ملتی زبان کے XLM-R موڈل سے پہنچ جاتے ہیں. ہم نے CORE کورپوس سریریس کے طور پر استعمال کیا ہے جس میں رسیسٹر کے ذریعہ غیر محدودہ ویب سے اظہار کیا گیا ہے.</abstract_ur>
      <abstract_sv>Denna artikel studerar klassificering av dokument från den obegränsade webben, såsom nyhetsartiklar eller opinionsbloggar, i en flerspråkig miljö, och undersöker både fördelarna med utbildning i flera språk och möjligheterna för noll-skott tvärspråklig överföring. Även om det breda utbudet av språkliga variationer som finns på webben innebär utmaningar för registerklassificering, har nyligen genomförda studier visat att goda nivåer av tvärspråklig överföring från den omfattande engelska CORE-korpusen till andra språk kan uppnås. I denna studie visar vi att utbildning på flera språk 1) gynnar språk med begränsade mängder registernoterade data, 2) i genomsnitt uppnår prestanda i nivå med enspråkiga modeller och 3) förbättrar avsevärt jämfört med tidigare nollskottsresultat på finska, franska och svenska. De bästa resultaten uppnås med den flerspråkiga XLM-R modellen. Som data använder vi CORE corpus-serien med registerkommenterade data från den obegränsade webben.</abstract_sv>
      <abstract_uz>Ushbu maqola mavjud boʻlmagan veb tarkibidagi hujjatlarni tahrirlash imkoniyatini o'rganadi, masalan news maqolalari yoki maoni bog'lanuvchilari, ko'plab tillarda, ko'plab tillar uchun trening foydalanishini va ikkita tillar orqali o'zgartirish imkoniyatini o'rganadi. Veb- saytda ko'p tillar o'zgarishning kengaytmalari qo'shish uchun muammolar bo'ladi. Yaqinda o'rganishlar qo'shilgan ingliz tilidan bogʻliq tillardan CORE corpusdan boshqa tillardan uzoq darajalari imkoniyatini ko'rsatadi. Bu taʼminotda biz bir necha tillar bilan bir xil tilda taʼminlov qilishni ko'rsatdik, boshqa tillar qo'llangan maʼlumotlar bilan chegara bo'lgan tillar bilan foydalanadi, 2) o'rtasida o'rtacha monolingual modellar bilan bajarishni bajaradi va 3) oldingi nuqta ishlatilgan natijalari Finnish, Fransuzcha va Sh Ko'pchilik tili XLM-R modeli bilan eng yaxshi natijalar bajarildi. Maʼlumotlar sifatida, biz CORE corpus seriидан foydalanamiz, oʻrnatilmagan veb- sahifa haqida yangilangan maʼlumotni tanlash mumkin.</abstract_uz>
      <abstract_vi>This article studies register classification of documents from the hạn chế web, such as news articles hay ý kiến blog, in a multiple settle, exploret both the condition of đào tạo on multiple languages and the capacity for zero-shot cross-ngôn ngữ. Trong khi những nghiên cứu gần đây đầy đủ các biến đổi ngôn ngữ trên trang web gây ra nhiều thử thách cho việc phân loại đăng ký, nhưng cũng có những nghiên cứu cho thấy có khả năng truyền qua ngôn ngữ rộng rãi từ tập đoàn Cortland tiếng Anh sang các ngôn ngữ khác. Trong nghiên cứu này, chúng tôi cho thấy giáo dục về nhiều ngôn ngữ Những kết quả tốt nhất được đạt được với mô hình XLM-R đa dạng. Là dữ liệu, chúng tôi sử dụng bộ R.E. với dữ liệu ghi chú từ mạng nội bộ không hạn chế.</abstract_vi>
      <abstract_nl>Dit artikel bestudeert registerclassificatie van documenten van het onbeperkte web, zoals nieuwsberichten of opinieblogs, in een meertalige omgeving, waarbij zowel het voordeel van training in meerdere talen als de mogelijkheden voor zero-shot cross-lingual transfer wordt onderzocht. Hoewel de grote verscheidenheid aan taalkundige variaties op het web uitdagingen oplevert voor de classificatie van registers, hebben recente studies aangetoond dat goede niveaus van trans-linguale overdracht van het uitgebreide Engelse CORE-corpus naar andere talen kunnen worden bereikt. In deze studie tonen we aan dat training in meerdere talen ten goede komt aan talen met beperkte hoeveelheden gegevens met aantekeningen in het register, 2) gemiddeld prestaties behaalt die vergelijkbaar zijn met eentalige modellen, en 3) aanzienlijk verbetert ten opzichte van eerdere zero-shot resultaten in het Fins, Frans en Zweeds. De beste resultaten worden bereikt met het meertalige XLM-R model. Als data gebruiken we de CORE corpusserie met registergeannoteerde gegevens uit het onbeperkte web.</abstract_nl>
      <abstract_bg>Тази статия изучава класификацията на документите от неограничената интернет мрежа, като например статии с новини или блогове с мнения, в многоезична обстановка, изследвайки както ползите от обучението на няколко езика, така и възможностите за нулев междуезичен трансфер. Въпреки че широката гама от езикови вариации, открити в интернет, поставя предизвикателства за класификацията на регистрите, последните проучвания показват, че могат да бъдат постигнати добри нива на междуезичен трансфер от обширния английски корпус CORE към други езици. В това проучване ние показваме, че обучението на няколко езика 1) облагодетелства езици с ограничено количество анотирани данни от регистъра, 2) средно постига представяне на ниво едноезични модели и 3) значително се подобрява спрямо предишните нулеви резултати на финландски, френски и шведски език. Най-добрите резултати се постигат с многоезичния модел. Като данни използваме корпусната серия с анотирани регистри данни от неограничената интернет страница.</abstract_bg>
      <abstract_id>This article studies register classification of documents from the unrestricted web, such as news articles or opinion blogs, in a multilingual setting, exploring both the benefit of training on multiple languages and the capabilities for zero-shot cross-lingual transfer.  Sementara jangkauan luas variasi bahasa yang ditemukan di web menunjukkan tantangan untuk klasifikasi register, penelitian baru-baru ini menunjukkan bahwa tingkat yang baik dari transfer saling bahasa dari CORE corpus Inggris ekstensif ke bahasa lain dapat dicapai. Dalam penelitian ini, kami menunjukkan bahwa pelatihan dalam berbagai bahasa 1) bahasa keuntungan dengan jumlah terbatas data-annotasi register, 2) rata-rata mencapai prestasi yang sama dengan model monobahasa, dan 3) meningkat jauh pada hasil zero-shot sebelumnya dalam Finlandia, Perancis dan Swedia. Hasil terbaik dicapai dengan model XLM-R berbilang bahasa. Sebagai data, kami menggunakan seri CORE corpus yang menampilkan register data annotasi dari web yang tidak terbatas.</abstract_id>
      <abstract_hr>Ovaj članak proučava registriranje klasifikacije dokumenta iz neograničene mreže, kao što su novinski članovi ili blogovi mišljenja, u multijezičkom postavljanju, istražujući i korist obuke na višestrukim jezicima i mogućnosti prijenosa s nulom snimkom preko jezika. Iako širok raspon lingvističkih promjena nalaženih na internetu predstavlja izazove za klasifikaciju registracija, nedavna ispitivanja pokazala su da se mogu postići dobra razina preko jezika prebacivanja iz širokog engleskog korpusa CORE na druge jezike. U ovom ispitivanju pokazujemo da obuka na višestrukim jezicima 1) koristi jezike s ograničenim količinama podataka o registraciji, 2) na prosjeku postiže učinkovitost na par s monojezičkim modelima, a 3) značajno poboljšava na prethodnim rezultatima nule snimanja na finskom, francuskom i švedskom. Najbolji rezultati su postignuti s multijezičkim XLM-R modelom. Kao podaci, koristimo seriju korpusa CORE-a koja uključuje registraciju annotiranih podataka iz neograničene mreže.</abstract_hr>
      <abstract_ko>본고는 다중 언어 환경에서 비제한 네트워크에서 온 문서(예를 들어 뉴스 기사나 관점 블로그)를 등록 분류하고 다중 언어 교육의 장점과 제로 크로스 언어 이동 능력을 연구했다.비록 인터넷에서 발견된 광범위한 언어 변이가 어역 분류에 도전을 가져왔지만 최근의 연구에 의하면 대량의 영어 핵심 어료 라이브러리에서 다른 언어로의 양호한 크로스 언어 이동을 실현할 수 있다고 한다.이 연구에서 우리는 다양한 언어의 훈련 1)이 어역 주석 데이터량이 제한된 언어에 유리하고, 2) 평균적으로 단어 모델과 비슷한 성능에 이르는 것을 발견했다. 3) 핀란드어, 프랑스어, 스웨덴어에서의 제로 포 결과를 크게 개선했다.다국어 XLM-R 모델을 사용하면 최상의 효과를 얻을 수 있습니다.데이터로서 우리는 핵심 어료 라이브러리 시리즈를 사용하는데 그 중에서 무제한 네트워크에서 온 등록 주석 데이터를 포함한다.</abstract_ko>
      <abstract_fa>این مقاله تحقیقات گروه‌بندی سند‌ها را از وب غیرمحدود ثبت می‌کند، مانند مقاله‌های خبری یا بلاگ نظر، در یک تنظیم بسیاری زبان، در جستجو هر دو سود آموزش بر زبان‌های متعدد و توانایی برای انتقال زبان‌های متعدد صفر، تحقیق می‌کند. در حالی که مدت گسترده تغییرات زبان‌شناسی که در وب یافته‌اند، چالش‌هایی برای گروه‌شناسی ثبت می‌کند، تحقیقات اخیرا نشان داده‌اند که سطح خوبی از انتقال متفاوت زبان‌های زیادی از کورپوس انگلیسی CORE به زبان‌های در این مطالعه، ما نشان می دهیم که آموزش روی زبانهای متعدد 1) به زبانها سود می دهد که با تعداد محدودیت داده‌های ثبت شده، ۲) در میانگین عملکرد را با مدل‌های متعدد زبان می‌رساند، و ۳) در نتیجه‌های صفر پیشین در فنلاندی، فرانسوی و سوئدی بهتر می‌شود. بهترین نتایج با مدل XLM-R چند زبان رسیده می شوند. به عنوان اطلاعات، ما از مجموعه CORE corpus استفاده می کنیم که از وب غیرمحدودیت اطلاعات آشنا شده را مشخص می کنیم.</abstract_fa>
      <abstract_sw>Makala hii inasoma kutangaza usambazaji wa nyaraka kutoka tovuti isiyo sahihi, kama vile makala za habari au blogu za maoni, katika mazingira ya lugha mbalimbali, kwa kutambua faida ya mafunzo katika lugha mbalimbali na uwezo wa usafirishaji wa lugha zisizo na sifa. Wakati mabadiliko mengi ya lugha yanayopatikana kwenye mtandao unaposhindwa changamoto za kutangazwa kwa ajili ya uandishi wa kujiandikisha, tafiti za hivi karibuni zimeonyesha kuwa kiwango vizuri cha uhamiaji wa lugha mbalimbali kutoka makampuni ya Kiingereza ya CORE hadi lugha nyingine zinaweza kufanikiwa. Katika utafiti huu, tunaonyesha kuwa mafunzo katika lugha mbalimbali ya 1) yanafaidia lugha zenye idadi kubwa ya taarifa zinazoandikishwa, 2) kwa wastani hufanikiwa utendaji wa mifano ya lugha za kimonolinguli, na 3) kwa kiwango kikubwa kinaongezeka kwa matokeo yasiyo na sifa iliyopita katika Kifinishi, Kifaransa na Kiswadishi. Matokeo bora yamefanishwa na modeli ya XLM-R ya lugha mbalimbali. Kama taarifa, tunatumia mfululizo wa makampuni ya CORE na kuonyesha taarifa zilizotajwa kutoka kwenye tovuti isiyo sahihi.</abstract_sw>
      <abstract_da>Denne artikel undersøger klassificeringen af dokumenter fra det ubegrænsede web, såsom nyhedsartikler eller meningsblogge, i en flersproget miljø, og undersøger både fordelene ved træning i flere sprog og mulighederne for nulskud tværsproget overførsel. Mens den brede vifte af sproglige variationer, der findes på nettet, udgør udfordringer for registerklassificering, har nylige undersøgelser vist, at der kan opnås gode niveauer af tværsproget overførsel fra det omfattende engelske CORE korpus til andre sprog. I denne undersøgelse viser vi, at træning på flere sprog 1) gavner sprog med begrænsede mængder registernoterede data, 2) i gennemsnit opnår resultater på samme niveau som ensprogede modeller, og 3) forbedrer betydeligt i forhold til tidligere nulskudsresultater på finsk, fransk og svensk. De bedste resultater opnås med den flersprogede XLM-R model. Som data bruger vi CORE corpus serien med registernoterede data fra det ubegrænsede web.</abstract_da>
      <abstract_tr>Bu makala çykyş edilmedik web sahypalaryndan, täzelikler ýa-da düşünýän bloglary ýaly, bir näçe dil düzümlerinde, birnäçe diller üçin okuw gurmanyň faydasyny we zerw atly dillerden geçirmek üçin mümkinçilikleri bardyr. Web içinde bulunan lingwistiki üýtgeşmeler klasifikasy üçin kynçylyklar döredip görkezilýär, soňky araştyrmalar iňlisçe CORE korpusdan başga dillere ýetip bilýärler. Bu aramda, biz birnäçe diller üçin bilim taýýarlanmasy 1) sany diýmek isleýän dillerden, 2) ortalamada monolingw modelleri bilen taýýarlanmasy üçin gowurar, we 3) öňki 0-atak netijelerini Finlandiýa, fransuzça we Şwediýada gowurar. Iň gowy netijeler birnäçe dilli XLM-R modeli bilen berilýär. Maglumat hökmünde biz CORE korpus serisini ulanýarys, daýalanmaýan web tarapyndan berilen ýazylan maglumaty barýarlar.</abstract_tr>
      <abstract_am>ይህ ጽሑፍ በብዛት ቋንቋዎች ላይ የመጠቀም ጥቅም እና በክፍለ ቋንቋ መቀናቀል የክፍል ቋንቋ መቃወሚያ የሚችሉትን የሰነዶች ክፍልፍሎች ማነሳትን እና የክፍል ቋንቋ መቀናቀል የሚችሉትን አካባቢነትን በመጠቀም ያስተምራል፡፡ While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved.  በዚህ ትምህርት፣ በብዛት ቋንቋዎች ላይ የተጠቃሚ ቋንቋ 1) የተጠቃሚ ቁጥጥር የመረጃ ዳታዎችን የሚጠቅመውን ቋንቋዎች (2) በመተካከለኛውም ብዛት በሞሎንቋል ዓይነቶች ላይ ድምፅን ያገኛል፡፡ የበለጠ ፍሬዎች በብዙ ቋንቋ በXLM-R ሞዴል የተደረገ ነው፡፡ እንደ ዳራዎች፣ CORE ኮርፓስ ተርሚዛን እናስቀምጣለን፡፡</abstract_am>
      <abstract_de>Dieser Artikel untersucht die Registerklassifizierung von Dokumenten aus dem uneingeschränkten Web, wie Nachrichtenartikel oder Meinungsblogs, in einem mehrsprachigen Umfeld und untersucht sowohl den Nutzen von Schulungen in mehreren Sprachen als auch die Möglichkeiten für den Zero-Shot-crosslingualen Transfer. Während das breite Spektrum an sprachlichen Variationen im Web Herausforderungen für die Registerklassifizierung darstellt, haben aktuelle Studien gezeigt, dass ein guter translingualer Transfer vom umfangreichen englischen CORE-Korpus in andere Sprachen erreicht werden kann. In dieser Studie zeigen wir, dass das Training in mehreren Sprachen 1) Sprachen mit begrenzter Menge an Registerannotierten Daten zugute kommt, 2) im Durchschnitt Leistung auf Augenhöhe mit einsprachigen Modellen erzielt und 3) die bisherigen Null-Shot-Ergebnisse in Finnisch, Französisch und Schwedisch deutlich verbessert. Die besten Ergebnisse werden mit dem mehrsprachigen XLM-R Modell erzielt. Als Daten verwenden wir die CORE-Korpusserie mit Registerkommentierungen aus dem uneingeschränkten Web.</abstract_de>
      <abstract_sq>This article studies register classification of documents from the unrestricted web, such as news articles or opinion blogs, in a multilingual setting, exploring both the benefit of training on multiple languages and the capabilities for zero-shot cross-lingual transfer.  Ndërsa gama e gjerë e variacioneve gjuhësore e gjetur në internet paraqet sfida për klasifikimin e regjistrimit, studimet e fundit kanë treguar se nivele të mira të transferimit ndërgjuhësor nga korpusi i gjerë anglez CORE në gjuhë të tjera mund të arrihen. In this study, we show that training on multiple languages 1) benefits languages with limited amounts of register-annotated data, 2) on average achieves performance on par with monolingual models, and 3) greatly improves upon previous zero-shot results in Finnish, French and Swedish.  Rezultatet më të mira arrihen me modelin XLM-R shumëgjuhës. Si të dhëna, ne përdorim serinë CORE corpus që paraqet të dhëna të regjistruara nga rrjeti i pa kufizuar.</abstract_sq>
      <abstract_af>Hierdie artikel studiereer klasifikasie van dokumente registreer van die onverstrekte web, soos nuusartikels of besonderhede blogs, in 'n veelvuldige instelling, ondersoek beide die voordeel van onderwerp op veelvuldige tale en die kapasiteite vir nul-skoot kruistale oordrag. Alhoewel die wyde omvang van lingwisiese veranderinge gevind op die web poseer uitdagings vir registrasie klasifikasie, het onlangse studie vertoon dat goeie vlakke van kruistale oordrag van die uitbreidige Engelske korpus na ander tale kan bereik word. In hierdie studie, wys ons dat onderwerp op veelvuldige tale 1) voordeel tale met beperkte hoeveelheid registreerde data, 2) op gemiddelde bereik uitvoeging op par met monolinge modele, en 3) baie verbeter op vorige nul-skoot resultate in Finnish, Frans en Sweedse. Die beste resultate word bereik met die multilinglike XLM-R model. As data, gebruik ons die CORE corpus reeks wat die registrasie aangetelde data van die ongestrekte web.</abstract_af>
      <abstract_hy>Այս հոդվածը ուսումնասիրում է անսահմանափակ ցանցի փաստաթղթերի դասակարգումը, ինչպիսիք են նորությունների հոդվածները կամ կարծիքի բլոգերը, բազլեզվով միջավայրում, ուսումնասիրելով բազմալեզվով լեզուների ուսումնասիրության առավելությունը, ինչպես նաև զրոյի Մինչդեռ ցանցում գտնվող լեզվաբանական տարբերությունների լայն տարբերակը դժվարություններ է առաջացնում ռեստորանների դասակարգման համար, վերջին ուսումնասիրությունները ցույց են տալիս, որ կարելի է հասնել լեզվաբանական փոխանցման լավ մակարդակներին անգլերենի CORECorpus-ից այլ լեզուների Այս ուսումնասիրության ընթացքում մենք ցույց ենք տալիս, որ բազմալեզուների ուսումնասիրությունը 1) օգտակար է լեզուներին, որոնք ունեն սահմանափակ քանակությամբ գրված տվյալներ, 2) միջինում հասնում են մեկլեզու մոդելների և 3) մեծ բարելավում են ֆինլարենի, ֆրանսիացի և շվեդիացի դեպքում առաջին զրոյ Լավագույն արդյունքները հասնում են XLM-R բազլեզու մոդելի միջոցով: Որպես տվյալներ, մենք օգտագործում ենք CORECorpus-ի շարքը, որը ներկայացնում է անսահմանափակ ցանցի գրված տվյալներ:</abstract_hy>
      <abstract_bn>এই প্রবন্ধটি অন্যান্য ভাষায় সংবাদ প্রবন্ধ বা মতামত ব্লগের নথিগুলোর বিভাগ নিবন্ধকতা গবেষণা করছে, যেমন সংবাদ প্রবন্ধ বা ব্লগ, অনেক ভাষায় বিভিন্ন ভাষায় প্রশিক্ষণের সু While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved.  এই গবেষণায় আমরা দেখাচ্ছি যে বেশ কয়েকটি ভাষায় প্রশিক্ষণের প্রশিক্ষণ সীমিত ভাষায় সুবিধা প্রদান করা হয়েছে যার সাথে সীমিত নিবন্ধন-বিজ্ঞাত তথ্য, ২) সাধারণ ভাষায় সাধারণ ভা বহুভাষায় XLM-R মডেল দিয়ে সবচেয়ে ভাল ফলাফল অর্জন করা হয়েছে। তথ্য হিসেবে আমরা কোর্পাস সিরিজ ব্যবহার করি যেখানে রেজিস্টারের বিস্তারিত তথ্য প্রকাশ করা হয়েছে অথচ ওয়েব থেকে।</abstract_bn>
      <abstract_az>Bu m…ôktub √ßoxlu dil qurńüularńĪnda, √ßoxlu dil qurńüularńĪnda t…ôhsil etm…ô faydalarńĪnńĪ v…ô sńĪfńĪr-shot √ßoxlu dil transferisinin f…ôaliyy…ôtini t…ôhsil edir. ńįnternet i√ßind…ô bulunan dil d…ôyiŇüiklikl…ôrinin geniŇüliyi s…ôb…ôbi register klasifikasyonu √ľ√ß√ľn √ß…ôtinlikl…ôr…ô m…ôcbur ed…ôrk…ôn, son d…ôyiŇüiklikl…ôrin √ßox geniŇüliyi ńįngilis CORE korpusundan baŇüqa dill…ôr…ô istifad…ô edil…ô bil…ôc…ôyini g√∂st…ôrdil…ôr. Bu t…ôcr√ľb…ôd…ô, √ßoxlu dill…ôrd…ô t…ôcr√ľb…ô 1 il…ô m√ľ…ôyy…ôn edil…ôn dill…ôr…ô m√ľ…ôyy…ôn edilmiŇü m…ôlumatlarńĪn m…ônf…ô…ôti verir, 2 il…ô ortalama t…ôcr√ľb…ôsi monodil modell…ôri il…ô m√ľ…ôyy…ôn edilir v…ô 3) Finlandiya, FransńĪzca v…ô Ňěvediyada …ôvv…ôlki n√∂qt…ôsin sonu√ßlarńĪnńĪ √ßox yaxŇüńĪlaŇüdńĪrńĪr. ∆Źn yaxŇüńĪ sonu√ßlar √ßoxlu dil XLM-R modeli il…ô baŇüarńĪlńĪr. M…ôlumatlar kimi, CORE korpus serisini istifad…ô edirik ki, m√ľ…ôyy…ôn edilm…ômiŇü web t…ôr…ôfind…ôn m…ôlumatlarńĪ m…ôlumatlarńĪnńĪ bel…ô g√∂st…ôrir.</abstract_az>
      <abstract_ca>Aquest article estudia la classificació de documents de la web sense restriccions, com articles de notícies o blogs d'opinió, en un entorn multilingüi, explorant tant el benefici de l'entrenament en múltiples llengües com les capacitats de transfer ència translingüística de fotografies zero. Mentre que la gran varietat de variacions lingüístices trobadas a la web posen reptes per a la classificació del registre, estudis recents han demostrat que es poden aconseguir bons nivells de transfer ència translingüística del corps anglès CORE a altres llengües. En aquest estudi, demostram que la formació en múltiples llengües 1) beneficia les llengües amb quantitats limitades de dades anotats en registre, 2) aconsegueix un rendiment mitjà igual que els models monolingües, i 3) millora molt en finlandès, francès i suec els resultats anteriors de fotografia zero. Els millors resultats s'aconsegueixen amb el model XLM-R multilingüe. Com a dades, utilitzem la sèrie CORE corpus amb registres de dades anotates de la Web sense restriccions.</abstract_ca>
      <abstract_bs>Ovaj članak proučava registraciju klasifikacije dokumenta iz neograničene mreže, kao što su novinski članovi ili blogovi mišljenja, u multijezičkom stanju, istražujući i korist obuke na višestrukim jezicima i sposobnosti za prebacivanje jezika nulog snimka. Iako širok niz lingvističkih varijacija nalaženih na internetu predstavlja izazove za klasifikaciju registracija, nedavno ispitivanje pokazalo je da se mogu postići dobar nivo prekograničnog prijenosa iz širokog engleskog korpusa CORE na druge jezike. U ovom ispitivanju pokazujemo da obuka na višestrukim jezicima 1) koristi jezike sa ograničenim količinama podataka o registraciji, 2) na prosjeku postiže učinkovitost na par sa monojezičkim modelima, a 3) veoma poboljšava na prethodnim rezultatima nule snimanja na finskom, francuskom i švedskom. Najbolji rezultati su postignuti sa multijezičkim XLM-R modelom. Kao podaci, koristimo seriju CORE korpusa koja uključuje registrirane annotirane podatke iz neograničene mreže.</abstract_bs>
      <abstract_cs>Tento článek studuje klasifikaci dokumentů z neomezeného webu, jako jsou zpravodajské články nebo blogy o názorech, ve vícejazyčném prostředí, zkoumá jak přínosy školení na více jazycích, tak možnosti nulového přenosu mezi jazyky. Zatímco široká škála jazykových variací nalezených na webu představuje výzvu pro klasifikaci registrů, nedávné studie ukázaly, že lze dosáhnout dobré úrovně přenosu mezi jazyky z rozsáhlého anglického korpusu CORE do jiných jazyků. V této studii ukazujeme, že školení na více jazycích 1) přináší prospěch jazyků s omezeným množstvím dat v registru, 2) v průměru dosahuje výkonu stejného jako monojazyčné modely a 3) výrazně zlepšuje předchozí nulové výsledky ve finštině, francouzštině a švédštině. Nejlepších výsledků dosahuje vícejazyčný model XLM-R. Jako data používáme korpusovou řadu CORE obsahující registrovaná data z neomezeného webu.</abstract_cs>
      <abstract_fi>Tässä artikkelissa tutkitaan rajattomasta verkosta peräisin olevien asiakirjojen, kuten uutisartikkeleiden tai mielipitebloggien, rekisteriluokittelua monikielisessä ympäristössä, ja tutkitaan sekä monikielisen koulutuksen hyötyjä että mahdollisuuksia nollakieliseen siirtoon. Vaikka verkossa esiintyvä kielellinen vaihtelu asettaa haasteita rekisteriluokittelulle, viimeaikaiset tutkimukset ovat osoittaneet, että laaja englanninkielinen CORE-korpus voi siirtyä hyvin eri kielille. Tässä tutkimuksessa osoitetaan, että monikielinen koulutus 1) hyödyttää kieliä, joilla on rajallinen määrä rekisterimerkintöjä, 2) saavuttaa keskimäärin suorituskykyä yksikielisten mallien kanssa ja 3) parantaa huomattavasti aikaisempia nollatuloksia suomeksi, ranskaksi ja ruotsiksi. Parhaat tulokset saavutetaan monikielisellä XLM-R-mallilla. Käytämme aineistona CORE-korpussarjoja, joissa on rekisterimerkinnällä merkitty tieto rajattomasta verkosta.</abstract_fi>
      <abstract_et>Käesolevas artiklis uuritakse piiramatu veebi dokumentide, näiteks uudisteartiklite või arvamusblogide registreerimist mitmekeelses keskkonnas, uurides nii mitmekeelse koolituse eeliseid kui ka võimalusi mitmekeelseks ülekandmiseks. Kuigi veebis leiduv keeleline varieeruvus tekitab registrite klassifitseerimisele probleeme, on hiljutised uuringud näidanud, et on võimalik saavutada hea taseme keeltevahelise ülekande ulatuslikust inglise CORE korpusest teistesse keeltesse. Käesolevas uuringus näitame, et mitme keele koolitus 1) toob kasu keeltele, kus on piiratud kogus registreeritud andmeid, 2) saavutab keskmiselt võrdse tulemuse ühekeelsete mudelitega ja 3) paraneb oluliselt varasematest nullkatse tulemustest soome, prantsuse ja rootsi keeles. Parimad tulemused saavutatakse mitmekeelse XLM-R mudeliga. Andmetena kasutame CORE korpuse seeriat, mis sisaldavad piiramatult veebist registreeritud annoteeritud andmeid.</abstract_et>
      <abstract_jv>Artik iki diputara bener winih sing nggawe dokumen ning web sing or a bisa dianggawe, kaya Artik balêr nggawe winih dhéwé, lan uga sistem hukum kanggo sistem hukum sistem plural, ndheke dhéwé éntukno sistem sing gawe nguasai perusahaan langkung sampeyan sak Where's the last language Genjer-genjer diunting akeh akeh operasi nggawe ing web punika ingkang diputara winih sing nggawe winih sing, winih sing ngendalikno wong dhéwé kuwi tindakan luwih apik-langkung Nang barêng-barêng iki, kéné iso nglanggar tarjamahan kanggo langgar sapa luwih lan ingkang 1) kayané perusahaan kanggo mbangaké kantor kantor nggawe data, 2) ngono nglanggar tarjamahan kanggo nyenggap kanggo nyenggap tarjamahan karo model Monolyanse, lan 3) sing beraksi kanggo nyenggap tarjamahan liyané sing nyenggap kanggo nyenggap tarjamahan Laptop" and "Desktop Sampeyan data, kita gambar kelompus cor</abstract_jv>
      <abstract_he>המאמר הזה לומד רשום קליזציה של מסמכים מהרשת הלא מוגבלת, כמו מאמרים חדשות או בלוגים דעות, במסגרת רבות שפות, לחקור את היתרון של האימונים על שפות רבות וכל היכולות להעברת שפות דרך אפס צילומים. While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved.  במחקר הזה, אנחנו מראים שאימונים על שפות רבות 1) שפות מועילות עם כמויות מוגבלות של נתונים רשומים, 2) בממוצע משיגים ביצועים באותה מידה עם דוגמנים monolingual, 3) משתפרים באופן גדול על תוצאות אפס קודמות בפינים, צרפתיים ושוודיים. התוצאות הטובות ביותר ניתן להשיג עם מודל XLM-R רב-שפתי. בתור נתונים, אנו משתמשים בסדרת CORE corpus המכילה רשום נתונים מועטפים מהרשת ללא מוגבלות.</abstract_he>
      <abstract_sk>Ta članek proučuje registrsko klasifikacijo dokumentov iz neomejenega spleta, kot so novinarski članki ali mnenjski blogi, v večjezičnem okolju, pri čemer raziskuje koristi usposabljanja o več jezikih in zmožnosti za brezposelni medjezični prenos. Medtem ko širok obseg jezikovnih različic, ki jih najdemo na spletu, predstavlja izzive za klasifikacijo registrov, so nedavne študije pokazale, da je mogoče doseči dobro raven medjezikovnega prenosa iz obsežnega angleškega korpusa CORE v druge jezike. V tej študiji smo pokazali, da usposabljanje na več jezikih 1) koristi jezikom z omejenimi količinami registriranih podatkov, 2) v povprečju doseže enako uspešnost kot enojezični modeli in 3) močno izboljša od prejšnjih ničelnih rezultatov v finščini, francoščini in švedščini. Najboljše rezultate dosežemo z večjezičnim modelom XLM-R. Kot podatki uporabljamo serijo korpusov CORE, ki vsebuje registrske podatke z oznakami iz neomejenega spleta.</abstract_sk>
      <abstract_ha>Wannan makala na karanta fasalin takardar kwamfyutan da ba'a rubutu ba, kamar makarantar da masu basu'a da suniyoyi, a cikin muhalli na multi-lingui, sunã jarraba laban amfani da amfani da yin wa'anar wa masu yawa da abincin wa transfer na-nau'in-sifanci. A lokacin da kewayen variantun linguistic wanda aka sãmu a kan web yana da zane-zane wa tsarin lissafi, masu ƙarami sun nuna cewa, zane da zane-zane mai kyau na shige-linguin-na'ura daga makamps na Ingiriya ya faɗi zuwa wasu harshen, za'a iya sãmun. Daga wannan lõkaci, Munã nũna wa mafarinta a cikin wasu harshe 1) yana amfani da wasu zane da aka ƙayyade yawan data na rubũtar da-yanzu, 2) a kan kawaici, yana sãmun babban rabo a par da misãlai masu monoli'in, kuma 3) yana ƙaranci mai girma a gabanin jarrabo na sifanci ta farko a cikin Finnish, Faransiya da Iswidishki. An sami mafi kyaun matsala da misalin XLM-R-na'ura multi-lingui. Kama da data, za mu yi amfani da jumla'in COER na shirin wasu mutane na rubutun da aka sanar da shi daga tare webi wanda ba'a saɓa ba.</abstract_ha>
      <abstract_bo>This article studies register classification of documents from the unrestricted web, such as news articles or opinion blogs, in a multilingual setting, exploring both the benefit of training on multiple languages and the capabilities for zero-shot cross-lingual transfer. While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved. In this study, we show that training on multiple languages 1) benefits with limited amounts of register-annotated data, 2) on average achieves performance on par with monolingual models, and 3) greatly improves in previous zero-shot results in Finnish, French and Swedish. A variety of languages on this study shows that training on multiple languages 1) benefits with limited amounts of register-annotated data, 2) on average achieves performance on par with monolingual models, and 3) greatly improve དབྱིབས་འབྲས་ཤོས་ཚད་མང་ཆེ་ཤོས་ཡོད་པའི་སྐད་རིགས་XLM-R་མ་དབྱིབས་ཡོད་པ་རེད། འུ་ཅག་གིས་སྐྱོན་འབྲེལ་མཐུད་མཁན་གྱི་ཞབས་ཞུ་སྤྱོད་བཞིན་པ་ལྟར་བཀོད་ཡོད་མེད་པའི་དྲ་རྒྱའི་ནང་ནས་གསལ་བཀོད</abstract_bo>
      </paper>
    <paper id="21">
      <title>De-identification of Privacy-related Entities in Job Postings</title>
      <author><first>Kristian Nørgaard</first><last>Jensen</last></author>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>210–221</pages>
      <abstract>De-identification is the task of detecting privacy-related entities in text, such as person names, <a href="https://en.wikipedia.org/wiki/Email">emails</a> and contact data. It has been well-studied within the <a href="https://en.wikipedia.org/wiki/Medicine">medical domain</a>. The need for de-identification technology is increasing, as privacy-preserving data handling is in high demand in many domains. In this paper, we focus on <a href="https://en.wikipedia.org/wiki/Employment_website">job postings</a>. We present JobStack, a new <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> for de-identification of personal data in job vacancies on <a href="https://en.wikipedia.org/wiki/Stackoverflow">Stackoverflow</a>. We introduce <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>, comparing Long-Short Term Memory (LSTM) and Transformer models. To improve these baselines, we experiment with BERT representations, and distantly related auxiliary data via <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>. Our results show that auxiliary data helps to improve <a href="https://en.wikipedia.org/wiki/De-identification">de-identification</a> performance. While BERT representations improve performance, surprisingly vanilla BERT turned out to be more effective than BERT trained on <a href="https://en.wikipedia.org/wiki/Stackoverflow">Stackoverflow-related data</a>.</abstract>
      <url hash="a59b2641">2021.nodalida-main.21</url>
      <bibkey>jensen-etal-2021-de</bibkey>
      <pwccode url="https://github.com/kris927b/JobStack" additional="false">kris927b/JobStack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/jobstack">JobStack</pwcdataset>
    </paper>
    <paper id="28">
      <title>NLI Data Sanity Check : Assessing the Effect of <a href="https://en.wikipedia.org/wiki/Data_corruption">Data Corruption</a> on Model Performance<fixed-case>NLI</fixed-case> Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance</title>
      <author><first>Aarne</first><last>Talman</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>276–287</pages>
      <abstract>Pre-trained neural language models give high performance on natural language inference (NLI) tasks. But whether they actually understand the meaning of the processed sequences is still unclear. We propose a new diagnostics test suite which allows to assess whether a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> constitutes a good testbed for evaluating the models’ meaning understanding capabilities. We specifically apply controlled corruption transformations to widely used <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs. If model accuracy on the corrupted data remains high, then the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is likely to contain <a href="https://en.wikipedia.org/wiki/Bias_(statistics)">statistical biases</a> and artefacts that guide <a href="https://en.wikipedia.org/wiki/Prediction">prediction</a>. Inversely, a large decrease in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">model accuracy</a> indicates that the original <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> provides a proper challenge to the models’ reasoning capabilities. Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks.</abstract>
      <url hash="c358f7c4">2021.nodalida-main.28</url>
      <bibkey>talman-etal-2021-nli</bibkey>
      <pwccode url="https://github.com/Helsinki-NLP/nli-data-sanity-check" additional="false">Helsinki-NLP/nli-data-sanity-check</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="33">
      <title>Towards cross-lingual application of language-specific PoS tagging schemes<fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> tagging schemes</title>
      <author><first>Hinrik</first><last>Hafsteinsson</last></author>
      <author><first>Anton Karl</first><last>Ingason</last></author>
      <pages>321–325</pages>
      <abstract>We describe the process of conversion between the PoS tagging schemes of two languages, the Icelandic MIM-GOLD tagging scheme and the Faroese Sosialurin tagging scheme. These tagging schemes are functionally similar but use separate ways to encode fine-grained morphological information on tokenised text. As Faroese and Icelandic are lexically and grammatically similar, having a systematic method to convert between these two tagging schemes would be beneficial in the field of <a href="https://en.wikipedia.org/wiki/Language_technology">language technology</a>, specifically in research on <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> between the two languages. As a product of our work, we present a provisional version of Icelandic corpora, prepared in the Faroese PoS tagging scheme, ready for use in cross-lingual NLP applications.</abstract>
      <url hash="a590242a">2021.nodalida-main.33</url>
      <bibkey>hafsteinsson-ingason-2021-towards</bibkey>
    </paper>
    <paper id="34">
      <title>Exploring the Importance of Source Text in Automatic Post-Editing for Context-Aware Machine Translation</title>
      <author><first>Chaojun</first><last>Wang</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>326–335</pages>
      <abstract>Accurate <a href="https://en.wikipedia.org/wiki/Translation">translation</a> requires document-level information, which is ignored by sentence-level machine translation. Recent work has demonstrated that document-level consistency can be improved with automatic post-editing (APE) using only target-language (TL) information. We study an extended APE model that additionally integrates <a href="https://en.wikipedia.org/wiki/Context_(language_use)">source context</a>. A human evaluation of fluency and adequacy in EnglishRussian translation reveals that the model with access to source context significantly outperforms monolingual APE in terms of adequacy, an effect largely ignored by automatic evaluation metrics. Our results show that TL-only modelling increases <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a> without improving adequacy, demonstrating the need for conditioning on source text for automatic post-editing. They also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably.</abstract>
      <url hash="1e1ba1c4">2021.nodalida-main.34</url>
      <bibkey>wang-etal-2021-exploring</bibkey>
      <pwccode url="https://github.com/zippotju/context-aware-bilingual-repair-for-neural-machine-translation" additional="false">zippotju/context-aware-bilingual-repair-for-neural-machine-translation</pwccode>
    </paper>
    <paper id="36">
      <title>Grapheme-Based Cross-Language Forced Alignment : Results with <a href="https://en.wikipedia.org/wiki/Uralic_languages">Uralic Languages</a></title>
      <author><first>Juho</first><last>Leinonen</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <pages>345–350</pages>
      <abstract>Forced alignment is an effective <a href="https://en.wikipedia.org/wiki/Process_(engineering)">process</a> to speed up <a href="https://en.wikipedia.org/wiki/Linguistics">linguistic research</a>. However, most forced aligners are language-dependent, and under-resourced languages rarely have enough resources to train an <a href="https://en.wikipedia.org/wiki/Acoustic_model">acoustic model</a> for an aligner. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple <a href="https://en.wikipedia.org/wiki/Uralic_languages">Uralic languages</a> and <a href="https://en.wikipedia.org/wiki/English_language">English</a> as an unrelated language. We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments.</abstract>
      <url hash="091b5464">2021.nodalida-main.36</url>
      <bibkey>leinonen-etal-2021-grapheme</bibkey>
      <pwccode url="https://github.com/aalto-speech/finnish-forced-alignment" additional="false">aalto-speech/finnish-forced-alignment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="40">
      <title>Decentralized Word2Vec Using Gossip Learning<fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec Using Gossip Learning</title>
      <author><first>Abdul Aziz</first><last>Alkathiri</last></author>
      <author><first>Lodovico</first><last>Giaretta</last></author>
      <author><first>Sarunas</first><last>Girdzijauskas</last></author>
      <author><first>Magnus</first><last>Sahlgren</last></author>
      <pages>373–377</pages>
      <abstract>Advanced NLP models require huge amounts of data from various domains to produce high-quality representations. It is useful then for a few large public and private organizations to join their corpora during training. However, factors such as legislation and user emphasis on <a href="https://en.wikipedia.org/wiki/Information_privacy">data privacy</a> may prevent centralized orchestration and <a href="https://en.wikipedia.org/wiki/Data_sharing">data sharing</a> among these organizations. Therefore, for this specific scenario, we investigate how gossip learning, a massively-parallel, data-private, decentralized protocol, compares to a shared-dataset solution. We find that the application of Word2Vec in a gossip learning framework is viable. Without any tuning, the results are comparable to a traditional centralized setting, with a loss of quality as low as 4.3 %. Furthermore, the results are up to 54.8 % better than independent local training.</abstract>
      <url hash="58d0d53e">2021.nodalida-main.40</url>
      <bibkey>alkathiri-etal-2021-decentralized</bibkey>
    </paper>
    <paper id="41">
      <title>Multilingual ELMo and the Effects of Corpus Sampling<fixed-case>ELM</fixed-case>o and the Effects of Corpus Sampling</title>
      <author><first>Vinit</first><last>Ravishankar</last></author>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>378–384</pages>
      <abstract>Multilingual pretrained language models are rapidly gaining popularity in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP systems</a> for non-English languages. Most of these models feature an important corpus sampling step in the process of accumulating training data in different languages, to ensure that the signal from better resourced languages does not drown out poorly resourced ones. In this study, we train multiple multilingual recurrent language models, based on the ELMo architecture, and analyse both the effect of varying corpus size ratios on downstream performance, as well as the performance difference between monolingual models for each language, and broader multilingual language models. As part of this effort, we also make these trained <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> available for public use.</abstract>
      <url hash="c73e3d0a">2021.nodalida-main.41</url>
      <bibkey>ravishankar-etal-2021-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="46">
      <title>The Danish Gigaword Corpus<fixed-case>D</fixed-case>anish <fixed-case>G</fixed-case>igaword Corpus</title>
      <author><first>Leon</first><last>Strømberg-Derczynski</last></author>
      <author><first>Manuel</first><last>Ciosici</last></author>
      <author><first>Rebekah</first><last>Baglini</last></author>
      <author><first>Morten H.</first><last>Christiansen</last></author>
      <author><first>Jacob Aarup</first><last>Dalsgaard</last></author>
      <author><first>Riccardo</first><last>Fusaroli</last></author>
      <author><first>Peter Juel</first><last>Henrichsen</last></author>
      <author><first>Rasmus</first><last>Hvingelby</last></author>
      <author><first>Andreas</first><last>Kirkedal</last></author>
      <author><first>Alex Speed</first><last>Kjeldsen</last></author>
      <author><first>Claus</first><last>Ladefoged</last></author>
      <author><first>Finn Årup</first><last>Nielsen</last></author>
      <author><first>Jens</first><last>Madsen</last></author>
      <author><first>Malte Lau</first><last>Petersen</last></author>
      <author><first>Jonathan Hvithamar</first><last>Rystrøm</last></author>
      <author><first>Daniel</first><last>Varab</last></author>
      <pages>413–421</pages>
      <abstract>Danish language technology has been hindered by a lack of broad-coverage corpora at the scale modern <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> prefers. This paper describes the Danish Gigaword Corpus, the result of a focused effort to provide a diverse and freely-available one billion word corpus of <a href="https://en.wikipedia.org/wiki/Danish_language">Danish text</a>. The Danish Gigaword corpus covers a wide array of time periods, domains, speakers’ socio-economic status, and <a href="https://en.wikipedia.org/wiki/Danish_dialects">Danish dialects</a>.</abstract>
      <url hash="d90398be">2021.nodalida-main.46</url>
      <revision id="1" href="2021.nodalida-main.46v1" hash="ae5c62ef" />
      <revision id="2" href="2021.nodalida-main.46v2" hash="d90398be" date="2021-06-04">This revision amends an incorrect name in one of the cited works.</revision>
      <bibkey>stromberg-derczynski-etal-2021-danish</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dagw">DAGW</pwcdataset>
    </paper>
    <paper id="47">
      <title>DanFEVER : claim verification dataset for Danish<fixed-case>D</fixed-case>an<fixed-case>FEVER</fixed-case>: claim verification dataset for <fixed-case>D</fixed-case>anish</title>
      <author><first>Jeppe</first><last>Nørregaard</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <pages>422–428</pages>
      <abstract>We present a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, DanFEVER, intended for multilingual misinformation research. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is in <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a> and has the same format as the well-known English FEVER dataset. It can be used for testing methods in multilingual settings, as well as for creating models in production for the <a href="https://en.wikipedia.org/wiki/Danish_language">Danish language</a>.</abstract>
      <url hash="b787c42b">2021.nodalida-main.47</url>
      <bibkey>norregaard-derczynski-2021-danfever</bibkey>
      <pwccode url="https://github.com/StrombergNLP/danfever" additional="false">StrombergNLP/danfever</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/danfever">DanFEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
    </paper>
    <paper id="51">
      <title>NorDial : A Preliminary Corpus of Written Norwegian Dialect Use<fixed-case>N</fixed-case>or<fixed-case>D</fixed-case>ial: A Preliminary Corpus of Written <fixed-case>N</fixed-case>orwegian Dialect Use</title>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>Samia</first><last>Touileb</last></author>
      <pages>445–451</pages>
      <abstract>Norway has a large amount of dialectal variation, as well as a general tolerance to its use in the public sphere. There are, however, few available resources to study this variation and its change over time and in more informal areas, on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. In this paper, we propose a first step to creating a <a href="https://en.wikipedia.org/wiki/Corpus_linguistics">corpus of dialectal variation</a> of <a href="https://en.wikipedia.org/wiki/Norwegian_language">written Norwegian</a>. We collect a small corpus of <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a> and manually annotate them as Bokml, <a href="https://en.wikipedia.org/wiki/Nynorsk">Nynorsk</a>, any dialect, or a mix. We further perform preliminary experiments with state-of-the-art models, as well as an analysis of the data to expand this <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> in the future. Finally, we make the annotations available for future work.</abstract>
      <url hash="1f13a7c8">2021.nodalida-main.51</url>
      <bibkey>barnes-etal-2021-nordial</bibkey>
      <pwccode url="https://github.com/jerbarnes/norwegian_dialect" additional="false">jerbarnes/norwegian_dialect</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nordial">NorDial</pwcdataset>
    </paper>
    <paper id="52">
      <title>The Swedish Winogender Dataset<fixed-case>S</fixed-case>wedish <fixed-case>W</fixed-case>inogender Dataset</title>
      <author><first>Saga</first><last>Hansson</last></author>
      <author><first>Konstantinos</first><last>Mavromatakis</last></author>
      <author><first>Yvonne</first><last>Adesam</last></author>
      <author><first>Gerlof</first><last>Bouma</last></author>
      <author><first>Dana</first><last>Dannélls</last></author>
      <pages>452–459</pages>
      <abstract>We introduce the SweWinogender test set, a diagnostic dataset to measure gender bias in <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>. It is modelled after the English Winogender benchmark, and is released with reference statistics on the distribution of men and women between occupations and the association between gender and occupation in modern corpus material. The paper discusses the design and creation of the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, and presents a small investigation of the supplementary statistics.</abstract>
      <url hash="4924bf48">2021.nodalida-main.52</url>
      <bibkey>hansson-etal-2021-swedish</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    </volume>
</collection>