<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.osact">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</booktitle>
      <editor><first>Hend</first><last>Al-Khalifa</last></editor>
      <editor><first>Walid</first><last>Magdy</last></editor>
      <editor><first>Kareem</first><last>Darwish</last></editor>
      <editor><first>Tamer</first><last>Elsayed</last></editor>
      <editor><first>Hamdy</first><last>Mubarak</last></editor>
      <publisher>European Language Resource Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-51-1</isbn>
    </meta>
    <frontmatter>
      <url hash="60cb224b">2020.osact-1.0</url>
      <bibkey>osact-2020-open</bibkey>
    </frontmatter>
    <paper id="2">
      <title>AraBERT : Transformer-based Model for Arabic Language Understanding<fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case>: Transformer-based Model for <fixed-case>A</fixed-case>rabic Language Understanding</title>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Fady</first><last>Baly</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>9–15</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Arabic">Arabic language</a> is a morphologically rich language with relatively few resources and a less explored syntax compared to <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">language understanding</a>, provided they are pre-trained on a very large corpus. Such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the <a href="https://en.wikipedia.org/wiki/Arabic">Arabic language</a> in the pursuit of achieving the same success that BERT did for the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>. The performance of AraBERT is compared to multilingual BERT from <a href="https://en.wikipedia.org/wiki/Google">Google</a> and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/araBERT hoping to encourage research and applications for Arabic NLP.</abstract>
      <url hash="31a4db61">2020.osact-1.2</url>
      <language>eng</language>
      <bibkey>antoun-etal-2020-arabert</bibkey>
      <pwccode url="https://github.com/aub-mind/araBERT" additional="true">aub-mind/araBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arcd">ARCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/astd">ASTD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arsentd-lev">ArSentD-LEV</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hard">HARD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/labr">LABR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    <title_ar>أرابرت: نموذج قائم على المحولات لفهم اللغة العربية</title_ar>
      <title_es>ARaBert: modelo basado en transformadores para la comprensión del idioma árabe</title_es>
      <title_pt>AraBERT: modelo baseado em transformador para compreensão da língua árabe</title_pt>
      <title_ja>AraBERT ：アラビア語を理解するためのトランスフォーマーベースモデル</title_ja>
      <title_zh>AraBERT:变压器阿拉伯语解模</title_zh>
      <title_hi>Arabert: अरबी भाषा की समझ के लिए ट्रांसफॉर्मर-आधारित मॉडल</title_hi>
      <title_ga>AraBERT: Múnla Trasfhoirmeoir-bhunaithe le haghaidh Tuiscint na Teanga Araibise</title_ga>
      <title_ka>Arabic</title_ka>
      <title_hu>AraBERT: Transzformátor alapú modell az arab nyelv megértéséhez</title_hu>
      <title_el>Το μοντέλο που βασίζεται στον μετασχηματιστή για την κατανόηση της αραβικής γλώσσας</title_el>
      <title_it>AraBERT: Modello basato su trasformatori per la comprensione della lingua araba</title_it>
      <title_mk>АраБЕРТ: Модел за разбирање на арапскиот јазик базиран на трансформи</title_mk>
      <title_kk>AraBERT: Араб тілін түсініктіру үшін түрлендіруші негізделген модель</title_kk>
      <title_lt>AraBERT: Arabų kalbos supratimo transformatoriaus modelis</title_lt>
      <title_ms>AraBERT: Model berasaskan-Transformer untuk Pemahaman Bahasa Arab</title_ms>
      <title_mt>AraBERT: Transformer-based Model for Arabic Language Understanding</title_mt>
      <title_mn>Араберт: Араб хэл ойлголтын төлөвлөгч суурилсан загвар</title_mn>
      <title_ml>അരാബെര്‍ട്ട്: അറബി ഭാഷയ്ക്കുള്ള മോഡല്‍ പരിശോധിക്കുക</title_ml>
      <title_no>AraBERT: Transformeringsbasert modell for arabisk språk forståking</title_no>
      <title_pl>AraBERT: oparty na transformatorze model rozumienia języka arabskiego</title_pl>
      <title_ro>AraBERT: Model bazat pe transformator pentru înțelegerea limbii arabe</title_ro>
      <title_si>AraBERT: අරාබි භාෂාව තේරුම්ගන්න සඳහා ප්‍රවර්තනයක් අධාරණය කරන්න ප්‍රමාණය</title_si>
      <title_sr>AraBERT: Model na transformaciji za razumevanje arapskog jezika</title_sr>
      <title_so>AraBERT: Model ku saleysan afka Carabiga waxgarashada</title_so>
      <title_sv>AraBERT: Transformatorbaserad modell för förståelse av arabiska språk</title_sv>
      <title_ta>AraBERT: மாற்று அடிப்படையான மாதிரி அரபி மொழி புரிந்து கொள்ளும்</title_ta>
      <title_ur>عربی زبان سمجھنے کے لئے ترفنٹر بنیادی موڈل</title_ur>
      <title_vi>AraBERT: Mô hình biến hình cho hiểu biết ngôn ngữ Ả rập</title_vi>
      <title_uz>AraBERT: Transformer- based Model for Arabic language understandingName</title_uz>
      <title_da>AraBERT: Transformer-baseret model til arabisk sprogforståelse</title_da>
      <title_bg>Модел за разбиране на арабския език, базиран на трансформатори</title_bg>
      <title_nl>AraBERT: Transformer-gebaseerd model voor het begrijpen van Arabische taal</title_nl>
      <title_hr>AraBERT: Model na transformaciji za razumijevanje arapskog jezika</title_hr>
      <title_de>AraBERT: Transformatorbasiertes Modell für das Verständnis der arabischen Sprache</title_de>
      <title_fa>آرابرت: مدل تغییر دهنده برای فهمیدن زبان عربی</title_fa>
      <title_ko>아랍트: 변압기 기반의 아랍어 이해 모델</title_ko>
      <title_id>AraBERT: Model Berdasar Transformer untuk Pemahaman Bahasa Arab</title_id>
      <title_sw>AraBERT: Mradi wa Kiarabu wa Kiarabu wa Kuelewa</title_sw>
      <title_tr>AraBERT: Arapça dilini düşünmek üçin terjime edilen nusgala</title_tr>
      <title_af>AraBERT: Transformer- gebaseerde Model vir Arabiese Taal Verstaan</title_af>
      <title_sq>AraBERT: Model me bazë në transformim për kuptimin e gjuhës arabe</title_sq>
      <title_am>AraBERT: Transformer-based Model for Arabic Language Understanding</title_am>
      <title_hy>Արաբերտ. Արաբերական լեզվի հասկանալու վերափոխման հիմնված մոդել</title_hy>
      <title_az>AraBERT: Arab dili anlama üçün Transformer-based Model</title_az>
      <title_bn>AraBERT: আরবী ভাষার বুঝার জন্য পরিবর্তন ভিত্তিক মডেল</title_bn>
      <title_bs>AraBERT: Model na transformaciji za razumijevanje arapskog jezika</title_bs>
      <title_ca>AraBERT: Model de comprensió del llenguatge àrab basat en transformadors</title_ca>
      <title_et>AraBERT: Transformer-põhine mudel araabia keele mõistmiseks</title_et>
      <title_cs>AraBERT: Model založený na transformátoru pro porozumění arabskému jazyku</title_cs>
      <title_fi>AraBERT: Muuntajapohjainen malli arabian kielen ymmärtämiseen</title_fi>
      <title_jv>araBERT: Transformer-basic model for Language</title_jv>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_sk>AraBERT: transformatorski model za razumevanje arabskega jezika</title_sk>
      <title_he>AraBERT: Transformer-based Model for Arabic Language Understanding</title_he>
      <title_bo>AraBERT: Transformer-based Model for Arabic Language Understanding</title_bo>
      <abstract_ar>اللغة العربية هي لغة غنية من الناحية الشكلية مع موارد قليلة نسبيًا وبنية أقل استكشافًا مقارنة باللغة الإنجليزية. نظرًا لهذه القيود ، فإن مهام معالجة اللغة العربية الطبيعية (NLP) مثل تحليل المشاعر (SA) ، والتعرف على الكيانات المسماة (NER) ، والإجابة على الأسئلة (QA) ، أثبتت أنها صعبة للغاية في معالجتها. في الآونة الأخيرة ، مع زيادة النماذج القائمة على المحولات ، أثبتت النماذج القائمة على BERT الخاصة باللغة أنها فعالة للغاية في فهم اللغة ، بشرط أن يتم تدريبهم مسبقًا على مجموعة كبيرة جدًا. كانت هذه النماذج قادرة على وضع معايير جديدة وتحقيق أحدث النتائج لمعظم مهام البرمجة اللغوية العصبية. في هذه الورقة ، قمنا بتدريب BERT مسبقًا خصيصًا للغة العربية سعياً لتحقيق نفس النجاح الذي حققه BERT للغة الإنجليزية. تتم مقارنة أداء AraBERT بأداء BERT متعدد اللغات من Google وغيره من الأساليب الحديثة. أظهرت النتائج أن AraBERT الذي تم تطويره حديثًا حقق أداءً متطورًا في معظم مهام البرمجة اللغوية العصبية العربية التي تم اختبارها. نماذج araBERT سابقة التدريب متاحة للجمهور على https://github.com/aub-mind/araBERT على أمل تشجيع البحث والتطبيقات الخاصة بمعالجة اللغات الطبيعية العربية.</abstract_ar>
      <abstract_es>El idioma árabe es un idioma rico morfológicamente con relativamente pocos recursos y una sintaxis menos explorada en comparación con el inglés. Dadas estas limitaciones, las tareas de procesamiento del lenguaje natural (NLP) árabe, como el análisis de sentimientos (SA), el reconocimiento de entidades nombradas (NER) y la respuesta a preguntas (QA), han demostrado ser muy difíciles de abordar. Recientemente, con el aumento de los modelos basados en transformadores, los modelos basados en BERT específicos del idioma han demostrado ser muy eficientes en la comprensión del idioma, siempre que estén previamente entrenados en un corpus muy grande. Estos modelos fueron capaces de establecer nuevos estándares y lograr resultados de vanguardia para la mayoría de las tareas de PNL. En este artículo, entrenamos previamente a BERT específicamente para el idioma árabe con el fin de lograr el mismo éxito que el BERT obtuvo para el idioma inglés. El rendimiento de ARaBert se compara con el BERT multilingüe de Google y otros enfoques de última generación. Los resultados mostraron que el AraBert recientemente desarrollado logró un rendimiento de vanguardia en la mayoría de las tareas de PNL árabe probadas. Los modelos AraBert previamente entrenados están disponibles públicamente en https://github.com/aub-mind/araBERT con la esperanza de fomentar la investigación y las solicitudes de PNL árabe.</abstract_es>
      <abstract_pt>A língua árabe é uma língua morfologicamente rica com relativamente poucos recursos e uma sintaxe menos explorada em relação ao inglês. Dadas essas limitações, tarefas de Processamento de Linguagem Natural (PNL) em árabe, como Análise de Sentimento (SA), Reconhecimento de Entidade Nomeada (NER) e Resposta a Perguntas (QA), provaram ser muito desafiadoras. Recentemente, com o surgimento de modelos baseados em transformadores, os modelos baseados em BERT específicos de linguagem provaram ser muito eficientes na compreensão da linguagem, desde que sejam pré-treinados em um corpus muito grande. Esses modelos foram capazes de estabelecer novos padrões e alcançar resultados de última geração para a maioria das tarefas de PNL. Neste artigo, nós pré-treinamos o BERT especificamente para o idioma árabe na busca de alcançar o mesmo sucesso que o BERT fez para o idioma inglês. O desempenho do AraBERT é comparado ao BERT multilíngue do Google e outras abordagens de última geração. Os resultados mostraram que o recém-desenvolvido AraBERT alcançou desempenho de última geração na maioria das tarefas de PNL árabe testadas. Os modelos araBERT pré-treinados estão disponíveis publicamente em https://github.com/aub-mind/araBERT esperando encorajar pesquisas e aplicações para PNL árabe.</abstract_pt>
      <abstract_ja>アラビア語は形態論的に豊かな言語であり、英語と比較してリソースが比較的少なく、構文もあまり探求されていません。 これらの制限を考慮すると、センチメント分析（ SA ）、名前付きエンティティ認識（ NER ）、質問回答（ QA ）などのアラビア語自然言語処理（ NLP ）タスクは、非常に困難であることが証明されています。 最近、変圧器ベースのモデルの急増に伴い、言語固有のBERTベースのモデルは、非常に大きなコーパスで事前にトレーニングされている限り、言語理解において非常に効率的であることが証明されています。 このようなモデルは、ほとんどのNLPタスクで新しい基準を設定し、最先端の結果を達成することができました。 この論文では、BERTが英語で行ったのと同じ成功を目指して、アラビア語のためにBERTを事前に訓練しました。 AraBERTのパフォーマンスは、Googleの多言語BERTや他の最先端のアプローチと比較されます。 結果は、新しく開発されたAraBERTが、ほとんどのテスト済みアラビア語NLPタスクで最先端のパフォーマンスを達成したことを示しています。 事前に訓練されたaraBERTモデルは、https://github.com/aub-mind/araBERTで公開されており、アラビア語のNLPの研究と応用を奨励することを期待しています。</abstract_ja>
      <abstract_zh>比之英语,阿拉伯语为富言,资源相薄,语法索为少。 鉴于此限,情析 (SA)、名实识 (NER) 与问答 (QA) 等阿拉伯语自然语言处 (NLP) 其效甚挑战性。 近者,随转换器之激增,BERT言之效,先言之语料库。 能为之多NLP设其制度而先进之。 本文者,专阿拉伯语预练BERT,以求BERT英语同功也。 AraBERT性与Google多言BERT较之最先进者。 结果表明,新发者AraBERT多试阿拉伯语NLP最先进之性。 豫教者araBERT明于 https://github.com/aub-mind/araBERT ,愿劝阿拉伯语NLP而用之。</abstract_zh>
      <abstract_hi>अरबी भाषा अपेक्षाकृत कम संसाधनों और अंग्रेजी की तुलना में कम खोजे गए वाक्यविन्यास के साथ एक रूपात्मक रूप से समृद्ध भाषा है। इन सीमाओं को देखते हुए, अरबी प्राकृतिक भाषा प्रसंस्करण (एनएलपी) जैसे भावना विश्लेषण (एसए), नामित इकाई मान्यता (एनईआर), और प्रश्न उत्तर (क्यूए) जैसे कार्य, निपटने के लिए बहुत चुनौतीपूर्ण साबित हुए हैं। हाल ही में, ट्रांसफॉर्मर आधारित मॉडल की वृद्धि के साथ, भाषा-विशिष्ट BERT आधारित मॉडल भाषा की समझ में बहुत कुशल साबित हुए हैं, बशर्ते वे एक बहुत बड़े कॉर्पस पर पूर्व-प्रशिक्षित हों। इस तरह के मॉडल नए मानकों को स्थापित करने और अधिकांश एनएलपी कार्यों के लिए अत्याधुनिक परिणाम प्राप्त करने में सक्षम थे। इस पेपर में, हमने विशेष रूप से अरबी भाषा के लिए BERT को उसी सफलता को प्राप्त करने की खोज में पूर्व-प्रशिक्षित किया जो BERT ने अंग्रेजी भाषा के लिए किया था। AraBERT के प्रदर्शन की तुलना Google से बहुभाषी BERT और अन्य अत्याधुनिक दृष्टिकोणों से की जाती है। परिणामों से पता चला है कि नव विकसित AraBERT ने अधिकांश परीक्षण किए गए अरबी एनएलपी कार्यों पर अत्याधुनिक प्रदर्शन हासिल किया। पूर्वप्रशिक्षित araBERT मॉडल सार्वजनिक रूप से https://github.com/aub-mind/araBERT पर उपलब्ध हैं जो अरबी एनएलपी के लिए अनुसंधान और अनुप्रयोगों को प्रोत्साहित करने की उम्मीद कर रहे हैं।</abstract_hi>
      <abstract_ga>Is teanga saibhre moirfeolaíoch í an teanga Araibis le líon beag acmhainní agus comhréir nach bhfuil an oiread céanna iniúchta uirthi i gcomparáid leis an mBéarla. I bhfianaise na dteorainneacha seo, tá sé an-dúshlánach dul i ngleic le tascanna Próiseála Teanga Nádúrtha Araibis (NLP) mar Anailís Mothúchán (SA), Aitheantas Aonán Ainmnithe (NER), agus Freagra Ceist (QA). Le déanaí, le méadú ar mhúnlaí bunaithe ar chlaochladáin, tá múnlaí bunaithe ar BERT a bhaineann go sonrach le teanga an-éifeachtach maidir le tuiscint teanga, ar an gcoinníoll go bhfuil siad réamh-oilte ar chorpas an-mhór. Bhí samhlacha den sórt sin in ann caighdeáin nua a shocrú agus torthaí úrscothacha a bhaint amach i gcás fhormhór na dtascanna NLP. Sa pháipéar seo, rinneamar réamhoiliúint ar BERT go sonrach don Araibis chun an rath céanna a bhaint amach agus a rinne BERT don Bhéarla. Cuirtear feidhmíocht AraBERT i gcomparáid le BERT ilteangach ó Google agus cineálacha cur chuige úrscothacha eile. Léirigh na torthaí gur bhain an AraBERT nuafhorbartha feidhmíocht den scoth amach ar na tascanna NLP Araibis is mó a ndearnadh tástáil orthu. Tá na samhlacha araBERT réamhoilte ar fáil go poiblí ar https://github.com/aub-mind/araBERT ag súil le taighde agus iarratais ar NLP Araibis a spreagadh.</abstract_ga>
      <abstract_ka>აპაბური ენაა მორპოლოგიურად ღარილი ენაა, რომელიც პარამეტრებით რამდენიმე რესურსი და უფრო ცოტა განსხვავებული სინტაქსი ინგლისურად შედგ ამ ზომილებების განსაზღვრებით, აპაბიური თავისუფალური ენერგიის პროცესი (NLP) საქმედები, როგორც Sentiment Analysis (SA), სახელი ინტერტის განაცნობა (NER) და კითხვების განახლება (QA) იქნება ძალიან გართული. მხოლოდ, ტრანფორმენტების მოდელების გარეშე, ბერტის განსაზღვრებული მოდელები გამოიყენება მნიშვნელოვანია ენათვის განსხვავებაში, თუ ისინი მნიშვნელოვანია დიდი კორპუსზე. ასეთი მოდელები შეუძლია ახალი სტანდარტულებს დააყენოთ და გავაკეთოთ სტანდარტულების შედეგი NLP დავალებებისთვის. ამ დოკუნეში ჩვენ BERT-ს წინასწარმოადგენა განსაკუთრებით აპაბიური ენაზე იგივე წარმატების მიღება, რომელიც BERT-ს ანგლისური ენაზე გავაკეთე. არაბერტის გამოსახულება მრავალენგური BERT-თან და სხვა ხელსახულების დასახულება. წარმოდგენები გაჩვენეთ, რომ ახალი განვითარებული აპაბერტი განვითარებულია ახალგაზრულებული აპაბერტის ნამდვილეობით განვითარებულია ახალგაზრულებული აპაბური N პაბერტის მოდელები საშუალოდ აქვს https://github.com/aub-mind/araBERT იმედი, რომ აპაბიური NLP-ის განსხვავება და პროგრამების შესაძლებლობა.</abstract_ka>
      <abstract_el>Η αραβική γλώσσα είναι μια μορφολογικά πλούσια γλώσσα με σχετικά λίγους πόρους και λιγότερο διερευνημένη σύνταξη σε σύγκριση με τα αγγλικά. Δεδομένης αυτών των περιορισμών, οι εργασίες επεξεργασίας φυσικής γλώσσας αραβικής γλώσσας (όπως η ανάλυση συναισθημάτων (SA), η αναγνώριση ονομαστών οντοτήτων (NER) και η απάντηση ερωτήσεων (QA), έχουν αποδειχθεί ότι είναι πολύ δύσκολο να αντιμετωπιστούν. Πρόσφατα, με την αύξηση των μοντέλων που βασίζονται σε μετασχηματιστές, τα ειδικά γλωσσικά μοντέλα έχουν αποδειχθεί πολύ αποτελεσματικά στην κατανόηση της γλώσσας, υπό την προϋπόθεση ότι είναι προ-εκπαιδευμένα σε ένα πολύ μεγάλο σώμα. Τέτοια μοντέλα ήταν σε θέση να θέσουν νέα πρότυπα και να επιτύχουν αποτελέσματα τελευταίας τεχνολογίας για τις περισσότερες εργασίες NLP. Σε αυτή την εργασία, εκπαιδεύσαμε ειδικά για την αραβική γλώσσα με στόχο την επίτευξη της ίδιας επιτυχίας που έκανε και για την αγγλική γλώσσα. Η απόδοση του AraBERT συγκρίνεται με την πολύγλωσση BERT από την Google και άλλες σύγχρονες προσεγγίσεις. Τα αποτελέσματα έδειξαν ότι το πρόσφατα αναπτυγμένο AraBERT πέτυχε υπερσύγχρονες επιδόσεις στις περισσότερες δοκιμασμένες αραβικές εργασίες NLP. Τα προσχεδιασμένα μοντέλα είναι δημόσια διαθέσιμα στο https://github.com/aub-mind/araBERT ελπίζοντας να ενθαρρύνει την έρευνα και τις εφαρμογές για τα αραβικά NLP.</abstract_el>
      <abstract_hu>Az arab nyelv morfológiailag gazdag nyelv viszonylag kevés erőforrással és kevésbé feltárt szintaxissal az angolhoz képest. Tekintettel ezekre a korlátozásokra, az arab természetes nyelvi feldolgozás (NLP) feladataira, mint az érzelmek elemzése (SA), a nevezett entitások felismerése (NER) és a kérdések megválaszolása (QA) nagyon kihívást jelentett. Az utóbbi időben a transzformátorok alapú modelleinek megnövekedésével a nyelvspecifikus BERT-alapú modellek nagyon hatékonynak bizonyultak a nyelv megértésében, feltéve, hogy egy nagyon nagy korpuszon előkészítették őket. Az ilyen modellek képesek voltak új szabványokat meghatározni és korszerű eredményeket elérni a legtöbb NLP-feladat tekintetében. Ebben a tanulmányban a BERT-t kifejezetten az arab nyelvre képeztük előre, hogy ugyanazt a sikert érjük el, mint a BERT az angol nyelvre. Az AraBERT teljesítményét a Google többnyelvű BERT és más korszerű megközelítések hasonlítják össze. Az eredmények azt mutatták, hogy az újonnan kifejlesztett AraBERT korszerű teljesítményt ért el a legtöbb tesztelt arab NLP feladatban. Az előkészített araBERT modellek nyilvánosan hozzáférhetők a következő oldalon: https://github.com/aub-mind/araBERT az arab nemzeti politika kutatásának és alkalmazásának ösztönzésében.</abstract_hu>
      <abstract_it>La lingua araba è una lingua morfologicamente ricca con relativamente poche risorse e una sintassi meno esplorata rispetto all'inglese. Date queste limitazioni, le attività di elaborazione del linguaggio naturale arabo (NLP) come l'analisi dei sentimenti (SA), il riconoscimento delle entità denominate (NER) e la risposta alle domande (QA), si sono rivelate molto difficili da affrontare. Recentemente, con l'aumento di modelli basati su trasformatori, i modelli basati su BERT specifici per il linguaggio si sono dimostrati molto efficienti nella comprensione del linguaggio, a condizione che siano pre-formati su un corpus molto ampio. Tali modelli sono stati in grado di stabilire nuovi standard e raggiungere risultati all'avanguardia per la maggior parte delle attività PNL. In questo articolo, abbiamo pre-formato BERT specificamente per la lingua araba nella ricerca di ottenere lo stesso successo che BERT ha fatto per la lingua inglese. Le prestazioni di AraBERT sono paragonate al BERT multilingue di Google e ad altri approcci all'avanguardia. I risultati hanno mostrato che il nuovo AraBERT ha ottenuto prestazioni all'avanguardia nella maggior parte delle attività NLP arabe testate. I modelli araBERT pretrained sono disponibili al pubblico su https://github.com/aub-mind/araBERT sperando di incoraggiare la ricerca e le applicazioni per la PNL araba.</abstract_it>
      <abstract_kk>Араб тілі - ағылшын тіліне салыстырылған морфологиялық баяны тіл, салыстырмалы көп ресурстар мен бірнеше зерттеулі синтаксисі. Бұл шектеулерге қарай, араб тіл процессі (NLP) Sentiment Analysis (SA), аталған нысандарды анықтау (NER) және сұрақтар жауап беру (QA) сияқты тапсырмалар өте қиын болды. Жуырда Түрлендірушілер негіздеген үлгілерді өзгерту үшін, тілді белгілі BERT негіздеген үлгілер тілді түсініктерге өте ең ең пайдалы болып көрсетілген, егер олар өте үлкен корпус арқылы Бұл үлгілер жаңа стандартты орнатуға болды және NLP тапсырмаларының көпшілігінің күйінің нәтижесін жеткізуға болды. Бұл қағазда БЕРТ тіліне әдетте Араб тіліне берттің ағылшын тіліне жеткізген сәтті жеткізу үшін алдын- ала оқыдық. AraBERT жылдамдығы Google және басқа әртүрлі көп тілді BERT-мен салыстырылады. Нәтижелер жаңа жасалған AraBERT- тың тексерілген Араб NLP тапсырмаларының көпшілігін жеткізгенін көрсетті. АраBERT үлгілері көпшілікті қолданылады https://github.com/aub-mind/araBERT Араб NLP зерттеулерін және қолданбаларын көмектесу үшін.</abstract_kk>
      <abstract_lt>Arabų kalba yra morfologiškai turtinga kalba, turinti palyginti nedaug išteklių ir mažiau ištirtą sintaksą, palyginti su anglų kalba. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle.  Pastaruoju metu, kai daugėja transformatorių pagrįstų modelių, kalbos specifiniai BERT pagrįsti modeliai įrodė, kad yra labai veiksmingi kalbų supratimo srityje, jeigu jie yra iš anksto apmokyti labai dideliu korpusu. Tokie modeliai galėjo nustatyti naujus standartus ir pasiekti naujausius rezultatus daugeliui NLP užduočių. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language.  AraBERT veiksmingumas lyginamas su daugiakalbiu Google BERT ir kitais naujausiais metodais. Iš rezultatų matyti, kad naujai sukurtas AraBERT pasiekė pažangiausius rezultatus daugumoje išbandytų arabų NLP užduočių. Išankstinio mokymo araBERT modeliai yra viešai prieinami https://github.com/aub-mind/araBERT Tikimasi skatinti mokslinius tyrimus ir paraiškas arabų nacionalinei nacionalinei programai.</abstract_lt>
      <abstract_ms>Bahasa Arab adalah bahasa yang kaya secara morfologik dengan sumber relatif sedikit dan sintaks yang kurang dikeksplorasi dibandingkan dengan bahasa Inggeris. Mengingat keterangan-keterangan ini, tugas Pemprosesan Bahasa Alami Arab (NLP) seperti Analisi Sentiment (SA), Pengenalan Entiti bernama (NER), dan Jawapan soalan (QA), telah terbukti sangat mencabar untuk diselesaikan. Baru-baru ini, dengan tumbuhan model berasaskan pengubah, model berasaskan BERT bahasa-spesifik telah terbukti sangat efisien dalam pemahaman bahasa, dengan syarat mereka dilatih-dilatih pada korpus yang sangat besar. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks.  Dalam kertas ini, kami melatih BERT secara khusus untuk bahasa Arab dalam mengejar untuk mencapai kejayaan yang sama dengan BERT untuk bahasa Inggeris. Performasi AraBERT dibandingkan dengan BERT berbilang bahasa dari Google dan pendekatan-state-of-the-art lain. Hasilnya menunjukkan bahawa AraBERT yang baru dikembangkan mencapai prestasi terbaik pada kebanyakan tugas NLP Arab yang diuji. Model araBERT yang dilatih dahulu tersedia pada https://github.com/aub-mind/araBERT - berharap untuk mendorong penyelidikan dan aplikasi untuk NLP Arab.</abstract_ms>
      <abstract_mk>Арапскиот јазик е морфолошки богат јазик со релативно малку ресурси и помалку истражена синтакса во споредба со англискиот. Со оглед на овие ограничувања, задачите на арапскиот природен јазик процес (НЛП), како што се чувствителната анализа (СА), именуваното препознавање на ентитетите (НЕР) и одговорот на прашањата (QA), се покажаа дека се многу тешки за решавање. Неодамна, со зголемувањето на моделите базирани на трансформатори, моделите базирани на јазик БЕРТ се докажаа дека се многу ефикасни во разбирањето на јазикот, под услов дека се предобучени на многу голем корпус. Таквите модели успеаа да постават нови стандарди и да постигнат најдобри резултати за повеќето задачи на НЛП. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language.  Представноста на АраБЕРТ се споредува со мултијазичниот БЕРТ од Гугл и другите најнови пристапи. Резултатите покажаа дека новиот развиен АраБЕРТ постигна најсовремена резултат на најтестираните арапски НЛП задачи. The pretrained araBERT models are publicly available on  https://github.com/aub-mind/araBERT Се надевам дека ќе охрабри истражување и апликации за арапската НЛП.</abstract_mk>
      <abstract_mt>Il-lingwa Għarbija hija lingwa morfoloġikament rikka b’relattivament ftit riżorsi u sintaks anqas esplorat meta mqabbel mal-Ingliż. Minħabba dawn il-limitazzjonijiet, il-kompiti tal-Ipproċessar tal-Lingwi Naturali Għarab (NLP) bħall-Analiżi tas-Sentiment (SA), ir-Rikonoxximent tal-Entità bl-Isem (NER), u t-Tweġiba għall-Mistoqsijiet (QA), urew li huma sfida ħafna biex jiġu indirizzati. Dan l-a ħħar, biż-żieda ta’ mudelli bbażati fuq it-trasformaturi, mudelli bbażati fuq il-BERT speċifiċi għall-lingwa wrew li huma effiċjenti ħafna fil-fehim tal-lingwa, sakemm ikunu mħarrġa minn qabel fuq korpus kbir ħafna. Dawn il-mudelli setgħu jistabbilixxu standards ġodda u jiksbu riżultati l-aktar avvanzati għall-biċċa l-kbira tal-kompiti tal-NLP. F’dan id-dokument, aħna mħarrġin minn qabel lill-BERT speċifikament għall-lingwa Għarbija biex inkisbu l-istess suċċess li għamel il-BERT għall-lingwa Ingliża. Il-prestazzjoni ta’ AraBERT titqabbel ma’ BERT multilingwi minn Google u approċċi oħra l-aktar avvanzati. Ir-riżultati wrew li l-AraBERT li għadu kif ġie żviluppat kiseb prestazzjoni avvanzata fil-biċċa l-kbira tal-kompiti tal-NLP Għarab ittestjati. Il-mudelli araBERT mħarrġa minn qabel huma disponibbli pubblikament fuq https://github.com/aub-mind/araBERT • li jittama li jinkoraġġixxu r-riċerka u l-applikazzjonijiet għall-NLP Għarab.</abstract_mt>
      <abstract_ml>അറബി ഭാഷ ഒരു മോര്‍ഫോളജിക്കല്‍ സമ്പന്നമായ ഭാഷയാണ് ഇംഗ്ലീഷിനെക്കുറിച്ച് കുറച്ച് വിഭവങ്ങളുമുള്ള സിന്‍ടാക Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle.  അടുത്തുതന്നെ, മാറ്റങ്ങളുടെ അടിസ്ഥാനത്തിലുള്ള മോഡലുകളുടെ തൂക്കത്തില്‍, ഭാഷ-പ്രത്യേക ബെര്‍ട്ടി അടിസ്ഥാനമായ മോഡലുകള്‍ ഭാഷ ബുദ്ധിയുടെ മനസ്സില ഇത്തരം മോഡലുകള്‍ക്ക് പുതിയ സ്റ്റഡേറ്ററുകള്‍ സജ്ജീകരിക്കാന്‍ സാധിച്ചിരുന്നു. ഏറ്റവും പേരും NLP ജോലികള്‍ ഈ പത്രത്തില്‍, ഞങ്ങള്‍ ബെര്‍ട്ടിനെ പ്രത്യേകിച്ച് പരിശീലിപ്പിച്ചത് പ്രത്യേകിച്ച് അറബി ഭാഷയ്ക്ക് വേണ്ടിയാണ്. ബെര്‍ട്ടിന്  ആരാബെര്‍ട്ടിന്റെ പ്രദര്‍ശനം ഗൂഗിളില്‍ നിന്നും മറ്റു കലാകാര്‍ട്ടില്‍ നിന്നും അധിക ഭാഷ ബെര്‍ട്ടിനോടും താല്‍പര്യമാണ പരീക്ഷിച്ചതില്‍ ഏറ്റവും പരീക്ഷിക്കപ്പെട്ട അറബിര്‍ട്ടി പുതിയ പരിശോധിച്ചിരിക്കുന്ന ആരാബെര്‍ട്ടിയുടെ സ്ഥിതിയില അറബെര്‍ട്ടി മോഡലുകള്‍ പ്രത്യേകിച്ച് ലഭ്യമാണ് https://github.com/aub-mind/araBERT അറബി എംഎല്‍പിയ്ക്ക് വേണ്ടി പരിശോധനവും പ്രയോഗങ്ങളും ആശ്രയിക്കാന്‍ പ്രതീക്ഷിക്കുന്നു.</abstract_ml>
      <abstract_no>Arabisk språk er eit morfologisk rikt språk med relativt få ressursar og mindre utforska syntaks sammenlignet med engelsk. Given desse grensene, har oppgåver som Sentiment Analysis (SA), Named Entity Recognition (NER) og Question Answering (QA) vist til at det er veldig vanskeleg å løyse. Nyleg har det vist å vera veldig effektiv ved språk forståking av transformatorbaserte modeller, dersom dei er først trent på ein veldig stor korpus. Desse modeller vart i stand til å setja nye standardar og oppnå tilstanden av kunsten for dei fleste NLP-oppgåver. I denne papiret har vi forelært BERT spesielt for den arabiske språket i følgje til å oppnå det samme suksess BERT gjorde for engelske språk. Utviklinga av AraBERT er samanlikna med fleire språk BERT frå Google og andre tilstand av kunsten. Resultatet viste at den nye utviklinga AraBERT har oppnådd status-of-the-art performance på dei mest testerte arabiske NLP-oppgåva. Den pretraine araBERT-modellen er tilgjengeleg offentlig på https://github.com/aub-mind/araBERT Håp om å oppretta forskning og program for arabisk NLP.</abstract_no>
      <abstract_mn>Араб хэл нь англи хэлний харьцуулахад бага баялаг хэл, бага баялаг судалсан синтаксис юм. Эдгээр хязгаарлалууд нь Араб байгалийн хэл процесс (NLP) шиг Sentiment Analysis (SA), нэрлэгдсэн Entity Recognition (NER) болон асуулт хариулт (QA) үүсгэх нь маш хэцүү байдаг. Сүүлийн үед өөрчлөгчийн загваруудын суурилсан загваруудыг нэмэгдүүлснээр хэл дээр тодорхой BERT-ийн загварууд хэл ойлголтын тулд маш үр дүнтэй байдаг гэдгийг баталсан. Ийм загварууд шинэ стандарт гаргаж, ихэнх NLP даалгаварын төлөө урлагийн үр дүн гаргаж чадсан. Энэ цаасан дээр бид БЕРТ-г англи хэл дээр хийсэн амжилтыг хүртэхийн тулд ялангуяа Араб хэлний хувьд сургалтын тулд сургалтын төлөө хийсэн. ААБЕРТ-ын үйл ажиллагаа нь Google-ын олон хэлний БЕРТ болон бусад урлагийн тусламжтай харьцуулдаг. Үр дүнд шинэ хөгжсөн Араберт нь Араб НLP даалгаварын хамгийн шинжлэх ухааны үйл ажиллагааг гаргасан. Өнгөрсөн араберт загварууд олон нийтэд https://github.com/aub-mind/araBERT Араб NLP-ын судалгаа болон хэрэглээ дэмжих гэж найдаж байна.</abstract_mn>
      <abstract_ro>Limba arabă este o limbă bogată din punct de vedere morfologic, cu relativ puține resurse și o sintaxă mai puțin explorată în comparație cu limba engleză. Având în vedere aceste limitări, sarcini de procesare a limbii naturale arabe (PNL), cum ar fi analiza sentimentelor (SA), recunoașterea entităților denumite (NER) și răspunsul la întrebări (QA), s-au dovedit a fi foarte dificil de abordat. Recent, odată cu creșterea modelelor bazate pe transformatoare, modelele bazate pe limbă BERT specifice s-au dovedit a fi foarte eficiente în înțelegerea limbii, cu condiția ca acestea să fie pre-instruite pe un corpus foarte mare. Astfel de modele au fost capabile să stabilească noi standarde și să obțină rezultate de ultimă generație pentru majoritatea sarcinilor PNL. În această lucrare, am pregătit BERT special pentru limba arabă în căutarea de a obține același succes ca BERT pentru limba engleză. Performanța AraBERT este comparată cu BERT multilingv de la Google și cu alte abordări de ultimă generație. Rezultatele au arătat că noul AraBERT dezvoltat a obținut performanțe de ultimă oră în cele mai multe sarcini testate din PNL arabe. Modelele araBERT pretrainate sunt disponibile public pe https://github.com/aub-mind/araBERT sperând să încurajeze cercetarea și aplicațiile pentru PNL arab.</abstract_ro>
      <abstract_pl>Język arabski jest bogatym morfologicznie językiem o stosunkowo niewielkich zasobach i mniej zbadanej składni w porównaniu do angielskiego. Biorąc pod uwagę te ograniczenia, zadania przetwarzania języka naturalnego arabskiego (NLP), takie jak analiza sentymentów (SA), rozpoznawanie nazwanych podmiotów (NER) i odpowiedź na pytania (QA), okazały się bardzo trudne do rozwiązania. Ostatnio, wraz z napięciem modeli opartych na transformatorach, modele oparte na języku BERT okazały się bardzo skuteczne w rozumieniu języka, pod warunkiem że są wstępnie przeszkolone na bardzo dużym korpusie. Takie modele były w stanie wyznaczyć nowe standardy i osiągnąć najnowocześniejsze wyniki dla większości zadań NLP. W niniejszym artykule wstępnie szkoliliśmy BERT specjalnie dla języka arabskiego w dążeniu do osiągnięcia tego samego sukcesu, co BERT zrobił w języku angielskim. Wydajność AraBERT porównywana jest z wielojęzycznym BERT firmy Google i innymi najnowocześniejszymi podejściami. Wyniki pokazały, że nowo opracowany AraBERT osiągnął najnowocześniejszą wydajność w większości testowanych arabskich zadań NLP. Wstępnie trenowane modele araBERT są publicznie dostępne na stronie internetowej https://github.com/aub-mind/araBERT mając nadzieję zachęcić do badań i aplikacji dla arabskiego NLP.</abstract_pl>
      <abstract_si>අරාබි භාෂාව තමයි ප්‍රමාණ භාෂාවක් සම්බන්ධයෙන්ම සම්බන්ධයෙන් ප්‍රමාණ භාෂාවක් සහ අඩුවෙන අරාබික භාෂාව ප්‍රවේශනය (NLP) විශ්ලේෂණය (SA), නම් තියෙන ප්‍රශ්නයක් ප්‍රශ්නය (NER), සහ ප්‍රශ්නයක් ප්‍රතික්ෂණය (QA) විදිහට ප්‍රශ්නයක් විද අවසානයෙන්, වෙනස් කරණාකරණය අධාරිත මොඩේල් එක්ක, භාෂාව විශේෂ BERT අධාරිත මොඩේල් එක්ක බොහොම ප්‍රයෝජනය වෙන්න පුළුවන ඒ වගේ මොඩල් අලුත් ප්‍රමාණය සැකසුම් කරලා තියෙන්න පුළුවන් වුනා සහ NLP වැඩේ වැඩි වැඩි වැඩි වැඩි වැඩිය මේ පත්තරේ අපි BERT විශේෂයෙන් අරාබි භාෂාව සඳහා ප්‍රශ්නයක් කළා ඉංග්‍රීසි භාෂාව සඳහා BERT කරපු එකම සමහර විශේ අරාබෙර්ට්ගේ ප්‍රමාණය ගුගුල් වලින් බෙර්ට්ට් වලින් සහ අනිත් ස්ථානයක් සම්බන්ධ වෙනවා. ප්‍රතිචාරය පෙන්වන්නේ අළුත් විකාශ කරපු අරාබෙර්ට් එක්ක තත්වයේ ක්‍රියාත්මක වැඩි පරීක්ෂා කරපු අරාබි පුරුදු අරාබෙර්ට් මෝඩේල් සාමාන්‍යයෙන් ප්‍රවේශ වෙන්න පුළුවන් https://github.com/aub-mind/araBERT අරාබික් NLP සඳහා පරීක්ෂණය සහ අවශ්‍යාව සම්බන්ධ කරන්න බලාපොරොත්තු වෙනවා.</abstract_si>
      <abstract_sv>Det arabiska språket är ett morfologiskt rikt språk med relativt få resurser och en mindre utforskad syntax jämfört med engelska. Med tanke på dessa begränsningar har uppgifter som Sentiment Analysis (SA), Named Entity Recognition (NER) och Question Answering (QA) visat sig vara mycket utmanande att hantera. Nyligen har språkspecifika BERT-baserade modeller visat sig vara mycket effektiva när det gäller språkförståelse, förutsatt att de är förberedda på en mycket stor korpus. Sådana modeller kunde sätta nya standarder och uppnå toppmoderna resultat för de flesta NLP-uppgifter. I denna uppsats förberedde vi BERT specifikt för det arabiska språket i strävan efter att uppnå samma framgång som BERT gjorde för det engelska språket. AraBERT:s prestanda jämförs med flerspråkig BERT från Google och andra toppmoderna metoder. Resultaten visade att den nyutvecklade AraBERT uppnådde toppmodern prestanda på de flesta testade arabiska NLP-uppgifter. De förkränade araBERT-modellerna är tillgängliga för allmänheten på https://github.com/aub-mind/araBERT hoppas kunna uppmuntra forskning och tillämpningar för arabisk NLP.</abstract_sv>
      <abstract_so>Luqada Carabigu waa luqad hodan ah oo la xiriira rasmi yar iyo mid ka yar kaalmada la barto ingiriisiga. Xiriiriyadan waxaa lagu xaqiijiyey shaqooyin baaritaanka afka asalka ah (NLP) oo la mid ah fasaxa (SA), Aqoonsashada Entity (NER), iyo jawaabta su'aalaha (QA) inay aad u adag tahay inay tacliiyaan. Muddii ugu dhowaaday, marka qaababka beddelka lagu dhigay, tusaalooyin ku saleysan afka gaar ah BERT waxay caddeysay inay aad u faa’iido u tahay fahanka luuqada, haddii ay horay u tababartay korpus aad u weyn. Tusaaladan ayaa awoodi kara in ay sameyn karaan 标准yo cusub, waxayna heli karaan arimaha farshaxanka ee ugu badan shaqooyinka NLP. Warqadan, waxaynu horay ugu tababarinnay BERT si gaar ah ugu tababarinnay luqada Carabiga, markii aan dhamaadno liibaanka isku mid ah ee BERT u sameeyey afka Ingiriiska. Dhaqanka AraBERT waxaa la barbaranayaa BERT luuqadaha kala duduwan ee Google iyo habka kale ee farshaxanta. Abaalkii waxay muuqatay in waqtiga cusub ee soo hormariyey AraBERT gaadhay state-of-the-art performance in ugu badan la imtixaamay shaqada Carabiga NLP. Tusaalada araBERT ee la soo daabacay waxay si bayaan ah uga helaan https://github.com/aub-mind/araBERT waxaan rajaynayaa in la dhiirrigeliyo waxbarasho iyo codsiyada afka Carabiga NLP.</abstract_so>
      <abstract_sr>Arapski jezik je morfološki bogat jezik sa relativno malo resursa i manje istražena sintaksa u usporedbi sa engleskim jezikom. S obzirom na te ograničenja, radovi Arapskog prirodnog jezika (NLP) poput Sentimentne analize (SA), priznanja podataka imenovanih podataka (NER) i odgovora na pitanja (QA), dokazali su da su veoma izazovni za rješavanje. Nedavno, s porastom modela na osnovu transformatora, modeli koji su temeljeni na jeziku BERT-u pokazali su veoma efikasni na razumijevanju jezika, ako su predobučeni na veoma velikom korpusu. Takvi modeli su uspjeli postaviti nove standarde i ostvariti rezultate umetnosti za većinu zadataka NLP-a. U ovom papiru smo predobučili BERT posebno za arapski jezik u potrazi za istim uspjehom koji je BERT uradio za engleski jezik. Izvrsnost AraBERT se uspoređuje sa multijezičkim BERT-om iz Google-a i drugim pristupima umetnosti. Rezultati su pokazali da je novi razvijen AraBERT postigao državni izvor umjetnosti na najtestiranijim arapskim NLP zadatkima. Preklinjeni araBERT modeli su javno dostupni na https://github.com/aub-mind/araBERT Nadajući se da će potaknuti istraživanje i prijave za arapski NLP.</abstract_sr>
      <abstract_ta>அரபி மொழி என்பது சொற்ப சிறிய வளர்ச்சிகளுடன் மொழியில் உள்ளது மற்றும் ஆங்கிலத்திற்கு ஒப்பிடும் குறைவான தேவை இந்த எல்லைகள் கொடுத்திருந்தால், அரபி இயற்கை மொழி செயல்பாடு (NLP) சென்டிமென்ட் Analysis (SA), பெயர் பெயர் உள்ளீட்டு அறிவிப்பு (NER) மற்றும் கேள்வி பதில் (QA) போன்ற ச சமீபத்தில், மாற்றங்கள் அடிப்படையில் உள்ள மாதிரிகளின் அளவிற்கு, மொழி- குறிப்பிட்ட BERT அடிப்படையில் உள்ள மாதிரிகள் மொழி புரிந்து கொள்ள முன்ப இவ்வாறு மாதிரிகள் புதிய நிலைகளை அமைக்க முடியவில்லை மற்றும் பெரும்பாலான NLP பணிகளுக்கான நிலையில் கலை முடி இந்த காகிதத்தில், நாம் பிரெட் குறிப்பாக அரபி மொழிக்கு பயிற்சியாக முன் பயிற்சி செய்தோம் பிரெட் ஆங்கிலத்திற்கு பி அராபெர்டின் செயல்கூறு பல மொழி பிரெட்டின் ஒப்பிடுவது கூகுலில் இருந்து மற்றும் கலை நிலையில் இருந்து மற்ற நிலைம முடிவு ஏர்பெர்ட் மாதிரிகள் பொதுவாக கிடைக்கும் https://github.com/aub-mind/araBERT - அரபி NLP க்கான ஆராய்ச்சி மற்றும் பயன்பாடுகளை உறுதிப்படுத்த நம்புகிறேன்.</abstract_ta>
      <abstract_ur>عربی زبان ایک مالک زبان ہے جو نسبت تھوڑے منابع اور اندھیری سینٹکس کے مقابلہ میں کم تحقیق کیا گیا ہے۔ یہ محدودیتوں کے باعث عربی طبیعی زبان پردازش (NLP) کے کاموں کی وجہ سے سنتیمنٹ تحلیل (SA), نام رکھا ہوا اینٹیٹی شناخت (NER) اور سوال جواب دینے (QA) کو ثابت ہو چکا ہے۔ اچھے سے، تبدیل کرنے والوں کی مدلکوں کے ذریعہ، زبان-خاص BERT بنیاد مدلکوں نے زبان سمجھنے کے لئے بہت اثبات کے ساتھ ثابت کی ہے، اگر یہ ایک بہت بڑے کورپوس پر پیش آموزش کی جاتی ہیں۔ ایسے موڈلے نئی استاندارڈ بنا سکتے تھے اور ان کے اکثر NLP کاموں کے لئے موقعیت کا نتیجہ پہنچا سکتے تھے. اس کاغذ میں ہم نے BERT کو مخصوصاً عربی زبان کے لئے آموزش دی ہے جس طرح BERT انگلیسی زبان کے لئے کیا تھا۔ آرابرٹ کی عملکرد گگل اور دوسری ایٹ آرت کے قریبوں سے بہت سی زبانی BERT کے مقابلہ میں ہے. نتیجے دکھائے گئے کہ نئی آرابرٹ نے بہترین آزمائش عربی NLP کے کاموں پر استعمال کی۔ آراBERT موڈلے ظاہر طور پر موجود ہیں https://github.com/aub-mind/araBERT اس کی امید ہے کہ عربی NLP کے لئے تحقیق اور کاروباروں کی تحقیق کریں۔</abstract_ur>
      <abstract_uz>The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English.  Bu chegaralar bilan, Sentiment Analysis (SA), nomli tizimni tanlash (NER) va savol javobi (QA) kabi arab tili tabiiy jarayonlariga (NLP) vazifalar bilan ishlatiladi. Bu savol juda murakkab bo'ladi. Yaqinda o'zgarishlar asosiy modellarning ko'payishi bilan, tilning asosiy BERT asosiy modellari tilni o'rganishga juda muhim ishlaydi, agar ular juda katta katta korpusda o'rganishdan oldin. Bu modellarni yangi standardlarni oʻrnatish mumkin va ko'pchilik NLP vazifalari uchun shaxsiy narsalar natijalarini bajaradi. Bu hujjatda biz ingliz tili uchun BERT ishga tushirilgan bir muvaffaqiyatli o'rganishga tayyorlamiz. AraBERT bajarishi Google bilan bir necha tildagi BERT kabi va boshqa holatning holatiga qarang. Natijalarni ko'rsatadi, yangi taʼminlovchi AraBERT haqida ko'paytirilgan arab NLP vazifalarining eng taʼminlov sohasini bajardi. Name https://github.com/aub-mind/araBERT Arab NLP uchun tahrirlash va dasturlarni ishlab chiqarishni himoyalash mumkin.</abstract_uz>
      <abstract_vi>Ngôn ngữ Ả Rập là một ngôn ngữ ngữ có lịch sử giàu có với một số nguồn tài nguyên tương đối ít được khám phá hơn so với tiếng Anh. Dựa trên những hạn chế này, các công việc xử lý ngôn ngữ tự nhiên của Ả Rập (NLP) như là một phiên bản phân tích cảm xúc (SA), Tênd Entity recognition (NER) và thẩm vấn đáp trả (QA) đã chứng tỏ là rất khó khăn để giải quyết. Gần đây, với sự tăng vọt các mô hình dựa trên máy biến đổi, các mô hình ngôn ngữ đặc trưng của BERT đã chứng minh được rất hiệu quả trong việc hiểu biết ngôn ngữ, miễn là chúng được rèn luyện sẵn trên một tập thể rất lớn. Những mẫu này đã xác định được những tiêu chuẩn mới và đạt được kết quả tối tân cho hầu hết các công việc. Trong tờ giấy này, chúng tôi đã rèn luyện thiếu sót cho ngôn ngữ Ả Rập nhằm đạt được thành công tương tự như BERT đã làm cho ngôn ngữ Anh. Các trình diễn của AraBERT được so sánh với hỗn tạp BERT của Google và các phương pháp hiện đại khác. Kết quả cho thấy rằng AraBERT mới được phát triển đã đạt được thành quả tuyệt vời trong các công việc được thử nghiệm cao nhất của Arab ngọt ngào. The prerained araBERT xảy ra công khai trên https://github.com/aub-mind/araBERT hy vọng sẽ khuyến khích nghiên cứu và ứng dụng ngôn ngữ Chọc tức tiếng Ả Rập.</abstract_vi>
      <abstract_nl>De Arabische taal is een morfologisch rijke taal met relatief weinig bronnen en een minder verkende syntaxis in vergelijking met het Engels. Gezien deze beperkingen zijn taken zoals Sentiment Analysis (SA), Named Entity Recognition (NER) en Question Respwering (QA) zeer uitdagend gebleken. Onlangs, met de toename van transformatorgebaseerde modellen, hebben taalspecifieke BERT-gebaseerde modellen bewezen zeer efficiënt te zijn in het begrijpen van taal, mits ze vooraf zijn getraind op een zeer groot corpus. Dergelijke modellen waren in staat om nieuwe normen te stellen en state-of-the-art resultaten te behalen voor de meeste NLP-taken. In dit artikel hebben we BERT speciaal voor de Arabische taal voorgetraind om hetzelfde succes te bereiken als BERT voor de Engelse taal. De prestaties van AraBERT worden vergeleken met meertalige BERT van Google en andere state-of-the-art benaderingen. De resultaten toonden aan dat de nieuw ontwikkelde AraBERT state-of-the-art prestaties behaalde bij de meeste geteste Arabische NLP-taken. De voorgetrainde araBERT modellen zijn publiekelijk beschikbaar op https://github.com/aub-mind/araBERT In de hoop onderzoek en toepassingen voor Arabische NLP te stimuleren.</abstract_nl>
      <abstract_bg>Арабският език е морфологично богат език със сравнително малко ресурси и по-малко изследван синтаксис в сравнение с английския. Предвид тези ограничения задачите за обработка на арабски естествен език (НЛП) като анализ на чувствата (SA), разпознаване на имена на субекти (NER) и отговор на въпроси (QA), се оказаха много трудни за справяне. Напоследък, с пренапрежението на трансформаторни модели, базирани на езика модели се доказаха като много ефективни при разбирането на езика, при условие че са предварително обучени върху много голям корпус. Тези модели са в състояние да определят нови стандарти и да постигнат най-съвременни резултати за повечето задачи на НЛП. В тази статия предварително обучихме BERT специално за арабския език в стремежа да постигнем същия успех, който BERT направи за английския език. Ефективността на AraBERT се сравнява с многоезичните BERT от Google и други съвременни подходи. Резултатите показват, че новоразработеният Араберт постига най-съвременни резултати при повечето тествани задачи по арабски НЛО. Предварително обучените модели са публично достъпни на https://github.com/aub-mind/araBERT Надявайки се да насърчи изследванията и приложенията за арабското НЛП.</abstract_bg>
      <abstract_hr>Arapski jezik je morfološki bogat jezik sa relativno malo resursa i manje istražena sintaksa u usporedbi s engleskim jezikom. S obzirom na te ograničenja, radovi Arapskog prirodnog jezika (NLP) poput Sentimentne analize (SA), priznanja podataka imenovanih podataka (NER) i odgovora na pitanja (QA) pokazali su veoma izazovni za rješavanje. Nedavno, s porastom modela na temelju transformatora, modeli koji su temeljeni na jeziku BERT-u pokazali su vrlo učinkoviti u razumijevanju jezika, ako su predobučeni na veoma velikom korpusu. Takvi modeli su uspjeli postaviti nove standarde i ostvariti rezultate stanja umjetnosti za većinu zadataka NLP-a. U ovom papiru smo predobučili BERT posebno za arapski jezik u potrazi za ostvarivanjem istog uspjeha koji je BERT učinio za engleski jezik. Izvrsnost AraBERT se uspoređuje s višejezičkim BERT-om iz Google-a i drugim pristupima umjetnosti. Rezultati su pokazali da je novi razvijen AraBERT postigao stanje umjetnosti na najtestiranijim arapskim NLP zadatkima. Preklinjeni araBERT modeli su javno dostupni na https://github.com/aub-mind/araBERT Nadam se da će poticati istraživanje i prijave za arapski NLP.</abstract_hr>
      <abstract_da>Det arabiske sprog er et morfologisk rigt sprog med relativt få ressourcer og en mindre udforsket syntaks sammenlignet med engelsk. I betragtning af disse begrænsninger har Arabic Natural Language Processing (NLP) opgaver som Sentiment Analysis (SA), Named Entity Recognition (NER) og Spørgsmål Besvarelse (QA) vist sig at være meget udfordrende at tackle. I den seneste tid har sprogspecifikke BERT-baserede modeller med den stigning i transformatorbaserede modeller vist sig at være meget effektive til sprogforståelse, forudsat at de er foruddannet på et meget stort korpus. Sådanne modeller var i stand til at sætte nye standarder og opnå avancerede resultater for de fleste NLP-opgaver. I denne artikel forududdannede vi BERT specifikt til det arabiske sprog i stræben efter at opnå den samme succes, som BERT gjorde for det engelske sprog. AraBERT's ydeevne sammenlignes med flersprogede BERT fra Google og andre avancerede metoder. Resultaterne viste, at den nyudviklede AraBERT opnåede state-of-the-art præstation på de fleste testede arabiske NLP-opgaver. De prætrænede araBERT modeller er offentligt tilgængelige på https://github.com/aub-mind/araBERT i håb om at fremme forskning og anvendelse af arabisk NLP.</abstract_da>
      <abstract_de>Die arabische Sprache ist eine morphologisch reiche Sprache mit relativ wenigen Ressourcen und einer weniger erforschten Syntax im Vergleich zu Englisch. Angesichts dieser Einschränkungen haben sich Aufgaben der Verarbeitung natürlicher arabischer Sprache (NLP) wie Sentiment Analysis (SA), Named Entity Recognition (NER) und Question Answering (QA) als sehr schwierig erwiesen. In letzter Zeit haben sich sprachspezifische BERT-basierte Modelle mit der Zunahme transformatorbasierter Modelle als sehr effizient im Sprachverständnis erwiesen, vorausgesetzt, sie sind auf einem sehr großen Korpus vortrainiert. Solche Modelle konnten für die meisten NLP-Aufgaben neue Standards setzen und State-of-the-Art Ergebnisse erzielen. In diesem Beitrag haben wir BERT speziell für die arabische Sprache vortrainiert, um den gleichen Erfolg zu erzielen, wie BERT auch für die englische Sprache. Die Leistung von AraBERT wird mit mehrsprachigem BERT von Google und anderen State-of-the-Art Ansätzen verglichen. Die Ergebnisse zeigten, dass der neu entwickelte AraBERT bei den meisten getesteten arabischen NLP-Aufgaben auf dem neuesten Stand der Technik ist. Die vortrainierten araBERT Modelle sind öffentlich verfügbar auf https://github.com/aub-mind/araBERT In der Hoffnung, Forschung und Anwendungen für arabische NLP zu fördern.</abstract_de>
      <abstract_id>Bahasa Arab adalah bahasa yang murfologis kaya dengan relatif sedikit sumber daya dan sintaks yang kurang dikeksplorasi dibandingkan bahasa Inggris. Mengingat batas-batas ini, tugas Persiapan Bahasa Alami Arab (NLP) seperti Analisi Sentiment (SA), Pengenalan Entitas bernama (NER), dan Jawaban Pertanyaan (QA), telah terbukti sangat sulit untuk diselesaikan. Baru-baru ini, dengan tumbuhan model yang berdasarkan transformator, model berbasis bahasa-spesifik BERT telah terbukti sangat efisien dalam pemahaman bahasa, dengan syarat mereka dilatih-dilatih pada korpus yang sangat besar. Model tersebut mampu menetapkan standar baru dan mencapai hasil terbaik bagi kebanyakan tugas NLP. Dalam koran ini, kami melatih BERT secara khusus untuk bahasa Arab dalam pencarian untuk mencapai sukses yang sama dengan BERT untuk bahasa Inggris. Pertunjukan AraBERT dibandingkan dengan BERT berbagai bahasa dari Google dan pendekatan terbaik lainnya. Hasilnya menunjukkan bahwa AraBERT yang baru dikembangkan mencapai prestasi terbaik pada kebanyakan tugas NLP Arab yang diuji. Model araBERT yang dilatih dahulu tersedia publik di https://github.com/aub-mind/araBERT Berharap untuk mendorong penelitian dan aplikasi untuk NLP Arab.</abstract_id>
      <abstract_ko>아랍어는 형태가 풍부한 언어로 영어에 비해 자원이 상대적으로 적고 문법 탐색이 적다.이러한 한계를 감안하면 감정분석(SA), 명명실체식별(NER), 질의응답(QA) 등 아랍어자연언어처리(NLP) 임무가 매우 도전적이라는 것이 입증됐다.최근transformers 기반의 모델이 급증함에 따라 특정 언어를 바탕으로 하는 BERT 모델은 언어 이해에 매우 효과적이라는 것이 증명되었다. 전제는 그들이 매우 큰 어료 라이브러리에서 미리 훈련한 것이다.이 모델들은 대부분의 NLP 작업에 대해 새로운 기준을 설정하고 가장 선진적인 결과를 얻을 수 있다.본고에서 우리는 영어와 같은 성공을 얻기 위해 아랍어를 대상으로 BERT에 대한 예비 훈련을 실시했다.AraBERT의 성능은 구글과 다른 가장 선진적인 방법에서 나온 다국어 BERT와 비교했다.그 결과 새로 개발된 아라베트는 대부분의 테스트를 거친 아랍어 NLP 임무에서 가장 선진적인 성능을 얻었다.예비 훈련을 거친 아라비아 모형은https://github.com/aub-mind/araBERT아랍어 NLP의 연구와 응용을 장려하기를 바랍니다.</abstract_ko>
      <abstract_fa>زبان عربی یک زبان مورفولوژیکی پولدار با نسبتا کم منابع و یک سنتاکس کوچک در مقایسه با انگلیسی است. با توجه به این محدودیت، وظیفه‌های پرداخت زبان طبیعی عربی (NLP) مانند تحلیل سنتیمتر (SA), شناسایی واحد نامیده (NER) و جواب سوال (QA) ثابت کردند که برای حل کردن بسیار سخت است. اخیرا، با افزایش مدل‌های متغییر‌کننده‌ها، مدل‌های متغییر به زبان BERT ثابت شده‌اند که در درک زبانی بسیار موثر است، در صورتی که آنها پیش از این روی یک کورپوس بسیار بزرگ آموزش یافته‌اند. این مدلها توانستند استانداردهای جدید را تنظیم کنند و نتیجه‌های وضعیت هنری را برای بیشتر کارهای NLP بررسی کنند. در این کاغذ، ما BERT را پیش آموزش دادیم، مخصوصا برای زبان عربی در دنبال رسیدن موفقیت همان که BERT برای زبان انگلیسی انجام داد. عملکرد آرابرت با BERT multilingual از گوگل و دیگر نزدیک‌های موجود هنر مقایسه می‌شود. نتیجه‌ها نشان داده‌اند که آرابرت تازه توسعه داده شده‌ای در بیشترین کارهای NLP آزمایش عربی رسیده است. مدلهای آرابرت پیش‌فرض به طور عمومی دسترسی دارند https://github.com/aub-mind/araBERT امیدوارم تحقیقات و کاربردهای NLP عربی را تشویق دهم.</abstract_fa>
      <abstract_sw>Lugha ya Kiarabu ni lugha yenye utajiri wa kifolojia yenye rasilimali chache na michache yenye uchunguzi kidogo ukilinganishwa na Kiingereza. Kwa kuzingatia vizuizi hivi, kazi za Utarabu wa Utarabu (NLP) kama vile Uchambuzi wa Sentiment (SA), Utambuzi wa Ujumbe wa Jinsia (NER), na Jawabu la swali (QA), zimekwisha kuwa na changamoto kubwa ya kupambana na mkakati. Hivi karibuni, kwa kuongezeka kwa mifano ya mabadiliko yenye msingi wa lugha, mifano maalum ya BERT yenye lugha imethibitisha kuwa na ufanisi mkubwa kwa kuelewa lugha, ikiwa ni mafunzo ya awali kwa makundi makubwa. Mfano huu uliweza kuweka viwango vipya na kupata matokeo ya hali ya sanaa kwa kazi nyingi za NLP. Katika karatasi hii, tumefundisha BERT hasa kwa lugha ya Kiarabu kufuatia mafanikio yale yaliyofanya kwa lugha ya Kiingereza. Utendaji wa AraBERT unalinganishwa na BERT kwa lugha mbalimbali kutoka Google na njia nyingine za sanaa. Matokeo yalionyesha kuwa chama kipya kilichoendelea AraBERT kilifanikiwa na hali ya sanaa katika kazi za Kiarabu zilizojaribiwa zaidi. Mradi wa araBERT unapatikana kwa uwazi https://github.com/aub-mind/araBERT  hoping to encourage research and applications for Arabic NLP.</abstract_sw>
      <abstract_tr>Arap챌a dil, i흫lis챌e g철r채 birn채챌e 챌e힊me bolan morfologik bagly dildir we az g철zle첵채n syntaks. Bu 챌yky힊lara g철r채, Sentiment Analysis (SA), Adlanan Entity Recognition (NER), we Soragy jogaplama (QA) 첵aly Arap챌a Dogaty Dil i힊lemleri (NLP) t채zelikleri 챌철zmek 철r채n kyn챌ylykly bolupdyr. So흫ky wagtlarda, BERT'a tabanly terjime edenler nusgalary bilen, dilleri흫 체st체nde t채sirli nusgalary dilleri흫 d체힊체nmesinde 철r채n t채sirli bolup kan캇tlandyryl첵ar. Olar 철r채n uly korpusda 철흫-철흫체nden 철흫체nden okuw챌yl첵arlar. B채rde nusgalar t채ze standartlary guryp bil첵채rdi we NLP t채zeliklerini흫 k철p nusgasyna 첵etip bil첵채rdi. Bu kagyzda, BERT dilini i흫lis dilinde 첵etirmek 체챌in adat챌a 철흫체nde BERT'y 철흫체nde bilim berdik. AraBERT'y흫 etkinlik Googladan we ba힊ga sanat ta첵첵arlaryna gola첵la힊첵ar. Netijeler AraBERT t채ze geli힊mi힊 t채ze kanunlary arap NLP i힊inde test edilen t채ze kanunlary흫 체st체ne 첵etip bardygyny g철rkezildi. 횦agyryl첵an araBERT nusgalary publikat i챌inde bar https://github.com/aub-mind/araBERT Arap챌a NLP 체챌in barlag we uygulamalaryny t채sirlemek 체챌in umyt ed첵채rler.</abstract_tr>
      <abstract_af>Die Arabiese taal is 'n morfologiese ryk taal met relativief paar hulpbronne en 'n minder uitsoek sintaks vergelyk met Engels. Gien hierdie beperkinge, die Arabiese Natuurlike Taal-Prosessering (NLP) werke soos Sentiment Analysis (SA), Gegenoem Eenheidrekening (NER), en Fraag-Antwoord Onlangs, met die verhoeging van transformers gebaseerde modele, taal-spesifieke BERT-gebaseerde modele het bevestig om baie effektief te wees by taal verstaan, voorwaar dat hulle vooraf opgelei word op 'n baie groot korpus. Soos modele was in staat om nuwe standaarde te stel en die staat-van-kuns-resultate te bereik vir die meeste NLP-opdragte. In hierdie papier, ons het BERT voorafgeleer spesifieke vir die Arabiese taal in die volg van die selfde sukses wat BERT gedoen het vir die Engels taal. Die prestasie van AraBERT is vergelyk met multilinglike BERT van Google en ander staat-van-kuns toekoms. Die resultate het vertoon dat die nuwe ontwikkelde AraBERT staat-van-kuns-prestasie op die meeste toets van Arabiese NLP-opdragte bereik het. Die presreënde araBERT-modele is openlik beskikbaar op https://github.com/aub-mind/araBERT In die hoop om forsoeking en toepassings vir Arabiese NLP te bevestig.</abstract_af>
      <abstract_sq>Gjuha arabe është një gjuhë morfologikisht e pasur me relativisht pak burime dhe një sintaksë më pak të eksploruar krahasuar me anglishtin. Duke pasur parasysh këto kufizime, detyrat arabe të Procesimit të Gjuhave Natyrore (NLP) si Analiza e Sentimenteve (SA), njohja e emëruar e njësisë (NER) dhe përgjigja e pyetjeve (QA) kanë provuar të jenë shumë të vështira për t'u trajtuar. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient in language understanding, provided they are pre-trained on a very large corpus. Modelet e tilla ishin në gjendje të vendosin standarde të reja dhe të arrinin rezultate më të larta për shumicën e detyrave të NLP. Në këtë letër, ne paratrajnuam BERT specifikisht për gjuhën arabe në përpjekje për të arritur të njëjtin sukses që BERT bëri për gjuhën angleze. Performanca e AraBERT krahasohet me BERT shumëgjuhës nga Google dhe metoda të tjera më të larta. Rezultatet treguan se AraBERT i sapo zhvilluar arriti shfaqjen më të lartë në detyrat më të testuara të NLP arabe. Modelet e paratrajnuara të araBERT janë në dispozicion publik në https://github.com/aub-mind/araBERT  hoping to encourage research and applications for Arabic NLP.</abstract_sq>
      <abstract_am>ዐረብኛ ቋንቋ ከመንግሊዝኛ ጋር ጥቂት ሀብት እና ትንሽ የተመረጠው ሲንካስብ ነው፡፡ እነዚህን ግንኙነት፣ አረቢኛ የፍጥረት ቋንቋ ፕሮጀክት (NLP) ሥርዓቶች እንደ ሳንተርናዊ Analysis (SA), የስሜት ውጤት ማውቀት (NER) እና ጥያቄ መልስ (QA) በመቃወም በጣም አቃውሎ ነው፡፡ Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus.  እንደዚህ ዓይነቶች አዲስ standard ማዘጋጀት ይችላሉ እና ለብዙዎቹ NLP ስራቶች የ-art ውጤቶችን ለማግኘት ይችላሉ፡፡ በዚህ ገጽ BERT በተለየን ለዐረብኛ ቋንቋ ብኤርቴን ለማግኘት የኢንግሊዝኛ ቋንቋ የተደረገውን ያን ድል ለማግኘት ለመፈለግ አስቀድመን ነበር፡፡ የAraBERT ድምፅ ከጎግል እና ከሌሎች የዓርት ግንኙነት ብሪቴን ለመተካከል ነው፡፡ ፍሬዎቹ አዲስ አረቢ አርቢ አርቢ የዓረብኛ አርቢ ስርዓት የ-የ-የ-አርእስት አግኝቷል፡፡ የተከፋፈሉት አረቢብERT ሞዴላዎች በህዝብ ላይ የተገኙ ናቸው https://github.com/aub-mind/araBERT በአረቢኛ NLP ላይ ትምህርት እና ፕሮግራሞች ለማጽናናት ተስፋ አደርጋለሁ፡፡</abstract_am>
      <abstract_bn>আরবী ভাষা হচ্ছে একটি মরোফোলজিক ভাষায় সমৃদ্ধ ভাষা, যার সাথে সামান্য কিছু সম্পদ এবং ইংরেজীর তুলনায় কম অনুসন্ধান করা সিন এই সীমাবদ্ধতা দিয়ে আরবী প্রাকৃতিক ভাষার প্রক্রিয়া (এনএলপি) কাজগুলো সেন্টাইমেন্ট বিশ্লেষণ (এসএ), নামের ইনটিটি স্বীকৃতি এবং প্রশ্নের উত্তর (কিউ এ) যুক্ত সম্প্রতি পরিবর্তনের ভিত্তিক মডেলে ভিত্তিক ভিত্তিক ভিত্তিক ভিত্তিক ভিত্তিক ভিত্তিক মডেল ভাষায় ভাষা বুঝতে বেশ কার্যকর, যদি তারা অনেক বড় কো এরকম মডেলগুলো নতুন ম্যান্ডার নির্ধারণ করতে পারে এবং বেশীরভাগ NLP কাজের রাষ্ট্র-শিল্পের ফলাফল অর্জন করতে পারে। এই কাগজটিতে আমরা বিরেট প্রশিক্ষণের পূর্বে প্রশিক্ষণ প্রদান করেছি বিশেষ করে বিরেটি ইংরেজি ভাষার জন্য একই সফল পৌঁছানোর জন্য। আরাবেরেটের প্রদর্শনীর তুলনায় গুগল থেকে বহুভাষার বিবেরেট এবং অন্যান্য রাষ্ট্র-অফ শিল্পের প্রতিক্রিয়ার তুলনায়। The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks.  প্রাপ্ত আরাবেরেট মডেল প্রকাশ্যে পাওয়া যাচ্ছে https://github.com/aub-mind/araBERT আরবী এনএলপির গবেষণা এবং অ্যাপ্লিকেশন উৎসাহিত করতে আশা করি।</abstract_bn>
      <abstract_az>Arapçalıq dili çox az qüvvətli və az keşfedilmiş sintaks dilidir İngilizə qarşı. Bu limitlərə görə, ərəbcə təbiətli dil işləməsi (NLP) işləri Sentiment Analysis (SA), Adlı Entity Recognition (NER) və sual Cevapı (QA) kimi çəkilmək çox çətin olduqlarını kanıtladı. Son zamanlarda, transformatçıların modellərinin yüksəlişi ilə, dillərin BERT tabanlı modellərin dillərin anlaşılmasında çox faydalı olduğunu kanıtladı, əgər onlar çox böyük korpus üstündə öyrənmişlər. Bütün modellər yeni standartları təyin edə bilərdilər və NLP işlərinin əksəriyyəti üçün mövcuddur sonuçlarını başa düşə bilərdilər. Bu kağızda BERT dilini özlərinə ərəb dilini təhsil etdik və BERT dilini İngilizce dilinə təhsil etdiyi başarıya çatmaq üçün. AraBERT performansı Google'dan çoxlu dilli BERT və digər şəkildə sanat tərəfindən salınmışdır. Sonuçlar AraBERT'nin yeni təhsil edildiyini göstərdi ki, arab NLP işlərinin ən çox imtahana çəkilmiş səhifələrində təhsil edilmişdir. Yaxşı araBERT modelləri açıq-aşkar mövcuddur. https://github.com/aub-mind/araBERT Arapça NLP üçün araşdırma və uyğulamaları təşkil etmək istəyirlər.</abstract_az>
      <abstract_bs>Arapski jezik je morfološki bogat jezik sa relativno malo resursa i manje istražena sintaksa u usporedbi s engleskim jezikom. S obzirom na te ograničenja, radovi Arapskog prirodnog jezika (NLP) poput Sentiment Analize (SA), priznanja imenovanih entiteta (NER) i odgovora na pitanja (QA), dokazali su da su vrlo izazovni za rješavanje. Nedavno, uz porast modela baziranih na transformacijama, modeli koji su temeljeni na jeziku BERT-u pokazali su veoma efikasni na razumijevanju jezika, ako su predobučeni na veoma velikom korpusu. Takvi modeli su uspjeli postaviti nove standarde i ostvariti rezultate stanja umjetnosti za većinu zadataka NLP-a. U ovom papiru smo predobučili BERT posebno za arapski jezik u potrazi za ostvarivanjem istog uspjeha koji je BERT učinio za engleski jezik. Izvrsnost AraBERT se uspoređuje sa multijezičkim BERT-om iz Google-a i drugim pristupima umjetnosti. Rezultati su pokazali da je novi razvijen AraBERT postigao state-of-the-art performance na najtestiranijim arapskim NLP zadatkima. Preklinjeni araBERT modeli su javno dostupni na https://github.com/aub-mind/araBERT Nadam se da će poticati istraživanje i prijave za arapski NLP.</abstract_bs>
      <abstract_ca>La llengua àrab és una llengua rica morfològicament amb relativament pocs recursos i una sintaxi menys explorada en comparació amb l'anglès. Tenint en compte aquestes limitacions, tasques de processament de llenguatges naturals àrabs (NLP), com Anàlisi de Sentiments (SA), Recognició Nomada d'Entitat (NER) i Responses a Preguntes (QA), han demostrat ser molt difícils d'abordar. Recentment, amb l'augment de models basats en transformadors, els models basats en BERT per llenguatge han demostrat ser molt eficients en la comprensió del llenguatge, a condició que siguin pré-entrenats en un corpus molt gran. Aquests models van poder establir nous estàndards i aconseguir resultats més avançats per a la majoria de les tasques del NLP. En aquest article vam preparar BERT específicament per a la llengua àrab en busca d'aconseguir el mateix èxit que BERT va fer per a la llengua anglesa. El rendiment d'AraBERT es compara amb el BERT multilingüe de Google i d'altres enfocaments més avançats. Els resultats van demostrar que el AraBERT recientement desenvolupat va aconseguir un rendiment més avançat en la majoria de tasques de NLP àrabs testades. Els models araBERT pré-entrenats estan disponibles en https://github.com/aub-mind/araBERT  hoping to encourage research and applications for Arabic NLP.</abstract_ca>
      <abstract_cs>Arabský jazyk je morfologicky bohatý jazyk s relativně málo zdrojů a méně prozkoumanou syntaxi ve srovnání s angličtinou. Vzhledem k těmto omezením se ukázalo, že úkoly zpracování arabského přirozeného jazyka (NLP), jako je analýza sentimentů (SA), rozpoznávání jmenovaných entit (NER) a odpověď na otázky (QA), jsou velmi náročné na řešení. V poslední době se vzhledem k nárůstu modelů založených na transformátorech ukázaly, že jazykově specifické modely založené na BERT jsou velmi efektivní při porozumění jazyků, pokud jsou předškoleny na velmi velkém korpusu. Tyto modely byly schopny stanovit nové standardy a dosáhnout nejmodernějších výsledků pro většinu úkolů NLP. V tomto příspěvku jsme předškolili BERT speciálně pro arabský jazyk, abychom dosáhli stejného úspěchu jako BERT v angličtině. Výkon AraBERT je porovnáván s vícejazyčným BERT od Google a dalšími moderními přístupy. Výsledky ukázaly, že nově vyvinutý AraBERT dosáhl nejmodernějšího výkonu u většiny testovaných arabských NLP úloh. Předtrénované modely araBERT jsou veřejně dostupné na internetu https://github.com/aub-mind/araBERT Doufám, že podpoří výzkum a aplikace pro arabské NLP.</abstract_cs>
      <abstract_hy>The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English.  Եթե հաշվի առնենք այս սահմանափակումները, արաբական բնական լեզվի վերլուծությունը (ՆԼՊ) այնպիսի խնդիրներ, ինչպիսիք են զգացմունքների վերլուծությունը (ՍԱ), անվանված անհատականության ճանաչելը (ՆԵՌ) և հարցերի պատասխանը (QA), ապացուցել են, որ շատ դժվար Վերջերս, վերափոխողների հիմնված մոդելների աճի դեպքում լեզվի մասնավոր BER-ի հիմնված մոդելները պարզվեցին շատ արդյունավետ լեզվի հասկացության մեջ, եթե նրանք նախապատրաստված են շատ մեծ մարմնի վրա: Այսպիսի մոդելները կարողացան նոր ստանդարտներ սահմանել և լավագույն արդյունքներ հասնել ՆԼՊ խնդիրների մեծ մասի համար: Այս թղթի մեջ մենք նախապատրաստում էինք BER-ը հատկապես արաբական լեզվի համար, որպեսզի հասնենք նույն հաջողությունը, ինչ BER-ը անգլերեն լեզվի համար: Արաբերթի արտադրությունը համեմատում է Google-ի և այլ ամենահետաքրքիր մոտեցումների բազլեզու Բերթին: Արդյունքները ցույց տվեցին, որ նորից զարգացած Արաբերթը հասավ ամենաբարձր արդյունքների ամենափորձարկված արաբական ՆԼՊ-ի խնդիրների վրա: The pretrained araBERT models are publicly available on  https://github.com/aub-mind/araBERT - հույս ունելով խրախուսել ուսումնասիրություններ և ծրագրեր արաբական ՆԼՊ-ի համար:</abstract_hy>
      <abstract_fi>Arabian kieli on morfologisesti rikas kieli, jolla on suhteellisen vähän resursseja ja vähemmän tutkittu syntaksi verrattuna englantiin. Näiden rajoitusten vuoksi arabian luonnollisen kielen käsittelyn (NLP) tehtävät, kuten Sentiment Analysis (SA), Nimetty entity Recognition (NER) ja Question Answering (QA), ovat osoittautuneet erittäin haastaviksi käsitellä. Muuntajapohjaisten mallien nousun myötä kielispesifiset BERT-pohjaiset mallit ovat osoittautuneet erittäin tehokkaiksi kielen ymmärtämisessä edellyttäen, että ne on koulutettu hyvin suurelle korpuselle. Näillä malleilla pystyttiin asettamaan uusia standardeja ja saavuttamaan huipputason tulokset useimmissa NLP-tehtävissä. Tässä artikkelissa esikoulutimme BERT:n erityisesti arabian kielellä pyrkiäksemme saavuttamaan saman menestyksen kuin BERT teki englannin kielellä. AraBERTin suorituskykyä verrataan Googlen monikieliseen BERT-järjestelmään ja muihin huippuluokan lähestymistapoihin. Tulokset osoittivat, että juuri kehitetty AraBERT saavutti huipputason suorituskyvyn useimmissa testatuissa arabiankielisissä NLP-tehtävissä. Esikoulutetut araBERT-mallit ovat julkisesti saatavilla osoitteessa https://github.com/aub-mind/araBERT toivoen rohkaista tutkimusta ja sovelluksia arabian NLP.</abstract_fi>
      <abstract_et>Araabia keel on morfoloogiliselt rikas keel, millel on suhteliselt vähe ressursse ja vähem uuritud süntaks võrreldes inglise keelega. Neid piiranguid arvestades on araabia looduskeele töötlemise (NLP) ülesanded, nagu tunnete analüüs (SA), nimetatud üksuste tunnustamine (NER) ja küsimustele vastamine (QA), osutunud väga keeruliseks. Hiljuti on transformaatoritel põhinevate mudelite suurenemisega keelespetsiifilised BERT-põhised mudelid osutunud keele mõistmisel väga tõhusaks, tingimusel et nad on eelnevalt koolitatud väga suure korpusega. Sellised mudelid suutsid kehtestada uued standardid ja saavutada uusimaid tulemusi enamiku uue uue tööprogrammi ülesannete puhul. Käesolevas töös koolitasime BERT-i spetsiaalselt araabia keele jaoks, et saavutada sama edu, mida BERT tegi inglise keele puhul. AraBERTi tulemuslikkust võrreldakse Google'i mitmekeelse BERTi ja teiste kaasaegsete lähenemisviisidega. Tulemused näitasid, et äsja arendatud AraBERT saavutas tipptasemel jõudluse enamikus testitud Araabia uue õppekava ülesannetes. Eeltreenitud araBERT mudelid on avalikult kättesaadavad aadressil https://github.com/aub-mind/araBERT lootes julgustada Araabia uue õppekava teadusuuringuid ja rakendusi.</abstract_et>
      <abstract_sk>Arabski jezik je morfološko bogat jezik z relativno malo virov in manj raziskano sintakso v primerjavi z angleščino. Glede na te omejitve se je izkazalo, da so naloge obdelave arabskega naravnega jezika (NLP), kot so analiza čustev (SA), prepoznavanje imenovanih subjektov (NER) in odgovarjanje na vprašanja (QA), zelo zahtevne. V zadnjem času so se z napetostjo transformatorskih modelov jezikovno specifični modeli BERT izkazali za zelo učinkovite pri razumevanju jezika, če so predhodno usposobljeni na zelo velikem korpusu. Takšni modeli so lahko določili nove standarde in dosegli najsodobnejše rezultate za večino nalog novega delovnega programa. V tem prispevku smo BERT predhodno usposabljali posebej za arabski jezik, da bi dosegli enak uspeh kot BERT za angleški jezik. Uspešnost AraBERT se primerja z večjezičnim BERT iz Googla in drugimi najsodobnejšimi pristopi. Rezultati so pokazali, da je novo razvit AraBERT dosegel najsodobnejše zmogljivosti pri večini preskušenih arabskih nalog NLP. Predtrenirani modeli araBERT so javno dostopni na spletni strani https://github.com/aub-mind/araBERT v upanju, da bo spodbudil raziskave in aplikacije za arabsko NLP.</abstract_sk>
      <abstract_ha>Harabci na Larabci yana da wata harshe na morfologically matajiri da ma'auni kaɗan kuma da abu kaɗan da aka samu da suntakin da Ingiriya. Gida waɗannan tsaro, aka sami aikin Harabi na Fasarin Lugha Kiarabu (NLP) kamar Analyze na Saukar (SA), Ananin Entity Recognition (NER), da Jawabar QA, sun jarraba su zama mai tsananin hanyarwa zuwa takin. A yanzu, da surar misãlai masu shige masu basara a bakin ayuka, misãlai masu ƙayyade BERT masu jarraba ya zama mai amfani da fasalin harshen, ko kuma idan an yi amfani da su a kan wani umarni mai girma. Waɗannan misalin sun iya iya daidaita kima na yanzu kuma su sami matsalar-halin-art wa masu yawa na aikin NLP. Ga wannan takardan, mun yi wa tunkuɗawa da BERT na ƙayyade wa harshen Larabci, a lokacin da za'a sãmi babban rabo da BERT ya aikata sabõda harshen Ingirinsa. Sunan AraBERT kamar misalin multi-linguin BERT daga Google da wasu halin-of-the-art. Result nuna that the newly developed AraBERT achieved state-of-the-art performance on most jarrabi taskõkin NLP. Ana iya da misalin araBERT da ake ƙayyade https://github.com/aub-mind/araBERT Ina kwaɗayin su ƙara wa fitina da shiryoyin ayuka na arabu NLP.</abstract_ha>
      <abstract_bo>ཨ་རབ་ཀྱི་སྐད་ཡིག་ནི་དབྱིན་ཡིག་དང་བསྡུར་བའི་སྐད་ཡིག་ཆ་ནི་ཆ་རྐྱེན་ངལ་ཆེ་བའི་ཆ་རྐྱེན་ཡིན་པ་དང་ནང་འད Given these limits, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proved to be very challenging to tackle. འཕྲལ་ཁམས་དེ་ལྟ་བུའི་དཔེ་དབྱིབས་བཟོ་བྱེད་མཁན་གྱི་འགྱུར་བ་ལྟར་བྱས་པ་ཡིན། སྐད་ཡིག་དམིགས་བསལ་ནུས་ཀྱི་དཔེ་དབྱིབས་བྱ་རིམ་ལ་ཧ་ཅང་ཚང དཔེ་དབྱིབས་འདི་དག་གི་སྔོན་སྒྲིག་གསར་བ་སྒྲིག་ནས་གནས་སྟངས་གནས་སྟངས་དང་རྒྱས་ཁབ་ཀྱི་འབྲེལ་བ་ཡོད་པ་རྟོགས་ཐུབ་ཀྱི་ཡོད། ང་ཚོས་ཤོག་བྱང་འདིའི་ནང་དུ་ངེད་ཚོས་BERT སྔོན་སྒྲིག་འཛིན་གྱིས་ཨ་རིའི་སྐད་ཡིག AraBERT ཡི་སྒྲུབ་གྱི་བྱ་སྟངས་འདི་གུ་གལ་ཏུ་སྐད་ཡིག་ཆ་སྐྱེས་དང་འདྲ་བའི་མཐུན་སྣ་ཚོགས་དང་མཐུན་ཡོད། གྲུབ་འབྲས་གསར་བ་བཟོ་བྱས་ན། AraBERT་གིས་གསར་བ་སྔོན་སྒྲིག་གནས་སྟངས་གཤིས་ཀྱི་ལས་འགན་སྟངས་མང་ཤོས་བྱས་ཡོད། སྔོན་པ་ཡོད་པའི་ araBERT མིག་དཔེ་དབྱིབས་མང་ཆོས་སྤྱོད་ཐུབ་པ་ཡིན། https://github.com/aub-mind/araBERT ཨ་རབ་ཀྱི་NLP་ལ་འཚོལ་ཞིབ་དང་ཉེར་སྤྱོད་ཀྱི་རེ་བ་སྐོར་བྱེད་དགོས་པ་རེད།</abstract_bo>
      <abstract_he>השפה הערבית היא שפה עשירה באופן מורפולוגי עם מעט משאבים יחסית וסינטקס פחות חוקר בהשוואה לאנגלית. בהתחשב בהגבלות הללו, משימות מעבדת שפת טבעית ערבית (NLP) כמו ניתוח רגשות (SA), זיהוי איכות בשם (NER), ותשובה לשאלות (QA), הוכיחו להיות מאד מאתגרים להתמודד. לאחרונה, עם התפרצות של דוגמנים מבוססים על משתנים, דוגמנים מבוססים על BERT ספציפיים לשפה הוכיחו להיות יעילים מאוד בהבנת שפת, בהנחה שהם מאומנים מראש על קורפוס גדול מאוד. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks.  בעיתון הזה, אימנו את BERT במיוחד לשפה הערבית במטרה להשיג את אותו הצלחה שבה BERT עשה לשפה האנגלית. ההופעה של AraBERT שווה ל BERT רבולוגית מגוגל ומגישות חדשות אחרות. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks.  דוגמני araBERT המאמנים מראש זמינים לציבור על https://github.com/aub-mind/araBERT בתקווה לעודד מחקר ויישומים עבור NLP ערבית.</abstract_he>
      <abstract_jv>Ascening arap kuwi masalah punika dipunangka luwih karo ingkang dipunangka sing titimbang kelas karo ingkang. Ngomongke limiting iki, nggambar obang-obang Habang Daerapakan (NLP) sentiment Resolution (S), Ngatur EntityTaniksi (NeR), lan Sugeng-responsing (NLP), kang dipenangke Jaringan (question responsing) sing berartile nggawe ketahan ora apik. Sadurungé, nganggep nggunaké model sing paling transformer, sampek model sing saben nggawe BERT kuwi diangkat diangkat luwih apik jênêmêr, ngetoké dhéwé wis diangkat saben ngono akeh dumateng. Laptop" and "Desktop Nang peurén iki, awake dhewe luwih-luwih bantuan karo BERT ngono nganggo langgambar arap kanggo nggawe barang nggawe gerakan kanggo nggawe luwih apik BERT kanggo langgambar ingkang. Arep nggawe Rejalian sing ngomong nik araBERT mbutuhake nggawe barang kelas-karo iso nggawe barang kelas barang nggawe barang arap NLP. model araBERT padha biasane ono https://github.com/aub-mind/araBERT Awakdhéwé nglanggar aturan kanggo tukang karo aplikasi kanggo NLP arab.</abstract_jv>
      </paper>
    <paper id="5">
      <title>From Arabic Sentiment Analysis to Sarcasm Detection : The ArSarcasm Dataset<fixed-case>A</fixed-case>rabic Sentiment Analysis to Sarcasm Detection: The <fixed-case>A</fixed-case>r<fixed-case>S</fixed-case>arcasm Dataset</title>
      <author><first>Ibrahim</first><last>Abu Farha</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>32–39</pages>
      <abstract>Sarcasm is one of the main challenges for sentiment analysis systems. Its <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> comes from the <a href="https://en.wikipedia.org/wiki/Opinion">expression of opinion</a> using implicit indirect phrasing. In this paper, we present ArSarcasm, an Arabic sarcasm detection dataset, which was created through the reannotation of available Arabic sentiment analysis datasets. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> contains 10,547 tweets, 16 % of which are <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcastic</a>. In addition to <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> the <a href="https://en.wikipedia.org/wiki/Data">data</a> was annotated for sentiment and dialects. Our analysis shows the highly subjective nature of these tasks, which is demonstrated by the shift in sentiment labels based on annotators’ biases. Experiments show the degradation of state-of-the-art <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysers</a> when faced with sarcastic content. Finally, we train a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning model</a> for sarcasm detection using BiLSTM. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves an <a href="https://en.wikipedia.org/wiki/F-number">F1 score</a> of 0.46, which shows the challenging nature of the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, and should act as a basic baseline for future research on our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>.</abstract>
      <url hash="be33a23f">2020.osact-1.5</url>
      <language>eng</language>
      <bibkey>abu-farha-magdy-2020-arabic</bibkey>
    </paper>
    <paper id="9">
      <title>ALT Submission for OSACT Shared Task on Offensive Language Detection<fixed-case>ALT</fixed-case> Submission for <fixed-case>OSACT</fixed-case> Shared Task on Offensive Language Detection</title>
      <author><first>Sabit</first><last>Hassan</last></author>
      <author><first>Younes</first><last>Samih</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Ammar</first><last>Rashed</last></author>
      <author><first>Shammur Absar</first><last>Chowdhury</last></author>
      <pages>61–65</pages>
      <abstract>In this paper, we describe our efforts at OSACT Shared Task on Offensive Language Detection. The shared <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> consists of two subtasks : offensive language detection (Subtask A) and hate speech detection (Subtask B). For offensive language detection, a system combination of Support Vector Machines (SVMs) and <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Neural Networks (DNNs)</a> achieved the best results on development set, which ranked 1st in the official results for Subtask A with F1-score of 90.51 % on the test set. For hate speech detection, DNNs were less effective and a system combination of multiple SVMs with different parameters achieved the best results on development set, which ranked 4th in official results for Subtask B with F1-macro score of 80.63 % on the test set.</abstract>
      <url hash="43a38c26">2020.osact-1.9</url>
      <language>eng</language>
      <bibkey>hassan-etal-2020-alt</bibkey>
    </paper>
    <paper id="10">
      <title>ASU_OPTO at OSACT4-Offensive Language Detection for Arabic text<fixed-case>ASU</fixed-case>_<fixed-case>OPTO</fixed-case> at <fixed-case>OSACT</fixed-case>4 - Offensive Language Detection for <fixed-case>A</fixed-case>rabic text</title>
      <author><first>Amr</first><last>Keleg</last></author>
      <author><first>Samhaa R.</first><last>El-Beltagy</last></author>
      <author><first>Mahmoud</first><last>Khalil</last></author>
      <pages>66–70</pages>
      <abstract>In the past years, toxic comments and offensive speech are polluting the internet and manual inspection of these comments is becoming a tiresome task to manage. Having a <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning based model</a> that is able to filter offensive Arabic content is of high need nowadays. In this paper, we describe the model that was submitted to the Shared Task on Offensive Language Detection that is organized by (The 4th Workshop on Open-Source Arabic Corpora and Processing Tools). Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> makes use transformer based model (BERT) to detect offensive content. We came in the fourth place in subtask A (detecting Offensive Speech) and in the third place in subtask B (detecting Hate Speech).</abstract>
      <url hash="7727dc00">2020.osact-1.10</url>
      <language>eng</language>
      <bibkey>keleg-etal-2020-asu</bibkey>
    </paper>
    <paper id="16">
      <title>Multi-Task Learning using AraBert for Offensive Language Detection<fixed-case>A</fixed-case>ra<fixed-case>B</fixed-case>ert for Offensive Language Detection</title>
      <author><first>Marc</first><last>Djandji</last></author>
      <author><first>Fady</first><last>Baly</last></author>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>97–101</pages>
      <abstract>The use of <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a> has become more prevalent, which has provided tremendous opportunities for people to connect but has also opened the door for misuse with the spread of <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a> and <a href="https://en.wikipedia.org/wiki/Profanity">offensive language</a>. This phenomenon has been driving more and more people to more extreme reactions and online aggression, sometimes causing physical harm to individuals or groups of people. There is a need to control and prevent such misuse of <a href="https://en.wikipedia.org/wiki/Social_media">online social media</a> through automatic detection of profane language. The shared task on Offensive Language Detection at the OSACT4 has aimed at achieving state of art profane language detection methods for Arabic social media. Our team BERTologists tackled this problem by leveraging state of the art pretrained Arabic language model, AraBERT, that we augment with the addition of <a href="https://en.wikipedia.org/wiki/Multi-task_learning">Multi-task learning</a> to enable our model to learn efficiently from little data. Our Multitask AraBERT approach achieved the second place in both subtasks A &amp; B, which shows that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performs consistently across different tasks.</abstract>
      <url hash="72541388">2020.osact-1.16</url>
      <language>eng</language>
      <bibkey>djandji-etal-2020-multi</bibkey>
    </paper>
    </volume>
</collection>