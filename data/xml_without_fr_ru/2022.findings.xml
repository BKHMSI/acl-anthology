<?xml version='1.0' encoding='utf-8'?>
<collection id="2022.findings">
  <volume id="acl" ingest-date="2022-05-15">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: ACL 2022</booktitle>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Aline</first><last>Villavicencio</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="698a5d93">2022.findings-acl</url>
    </meta>
    <frontmatter>
      <url hash="eb6cc2a4">2022.findings-acl.0</url>
      <bibkey>findings-2022-findings</bibkey>
    </frontmatter>
    <paper id="5">
      <title><fixed-case>R</fixed-case>elation<fixed-case>P</fixed-case>rompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction</title>
      <author><first>Yew Ken</first><last>Chia</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>45-57</pages>
      <abstract>Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.</abstract>
      <url hash="8e567b7a">2022.findings-acl.5</url>
      <bibkey>chia-etal-2022-relationprompt</bibkey>
      <pwccode url="https://github.com/declare-lab/relationprompt" additional="false">declare-lab/relationprompt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wiki-zsl">Wiki-ZSL</pwcdataset>
    </paper>
    <paper id="13">
      <title>Table-based Fact Verification with Self-adaptive Mixture of Experts</title>
      <author><first>Yuxuan</first><last>Zhou</last></author>
      <author><first>Xien</first><last>Liu</last></author>
      <author><first>Kaiyin</first><last>Zhou</last></author>
      <author><first>Ji</first><last>Wu</last></author>
      <pages>139-149</pages>
      <abstract>The table-based fact verification task has recently gained widespread attention and yet remains to be a very challenging problem. It inherently requires informative reasoning over natural language together with different numerical and logical reasoning on tables (e.g., count, superlative, comparative). Considering that, we exploit mixture-of-experts and present in this paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE). Specifically, we have developed a mixture-of-experts neural network to recognize and execute different types of reasoning—the network is composed of multiple experts, each handling a specific part of the semantics for reasoning, whereas a management module is applied to decide the contribution of each expert network to the verification result. A self-adaptive method is developed to teach the management module combining results of different experts more efficiently without external knowledge. The experimental results illustrate that our framework achieves 85.1% accuracy on the benchmark dataset TabFact, comparable with the previous state-of-the-art models. We hope our framework can serve as a new baseline for table-based verification. Our code is available at https://github.com/THUMLP/SaMoE.</abstract>
      <url hash="005f294b">2022.findings-acl.13</url>
      <bibkey>zhou-etal-2022-table</bibkey>
      <pwccode url="https://github.com/thumlp/samoe" additional="false">thumlp/samoe</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
    </paper>
    <paper id="17">
      <title><fixed-case>LEVEN</fixed-case>: A Large-Scale <fixed-case>C</fixed-case>hinese Legal Event Detection Dataset</title>
      <author><first>Feng</first><last>Yao</last></author>
      <author><first>Chaojun</first><last>Xiao</last></author>
      <author><first>Xiaozhi</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Cunchao</first><last>Tu</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Yun</first><last>Liu</last></author>
      <author><first>Weixing</first><last>Shen</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>183-201</pages>
      <abstract>Recognizing facts is the most fundamental step in making judgments, hence detecting events in the legal documents is important to legal case analysis tasks. However, existing Legal Event Detection (LED) datasets only concern incomprehensive event types and have limited annotated data, which restricts the development of LED methods and their downstream applications. To alleviate these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection dataset, with 8,116 legal documents and 150,977 human-annotated event mentions in 108 event types. Not only charge-related events, LEVEN also covers general events, which are critical for legal case understanding but neglected in existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and has dozens of times the data scale of others, which shall significantly promote the training and evaluation of LED methods. The results of extensive experiments indicate that LED is challenging and needs further effort. Moreover, we simply utilize legal events as side information to promote downstream applications. The method achieves improvements of average 2.2 points precision in low-resource judgment prediction, and 1.5 points mean average precision in unsupervised case retrieval, which suggests the fundamentality of LED. The source code and dataset can be obtained from https://github.com/thunlp/LEVEN.</abstract>
      <url hash="35ebebe5">2022.findings-acl.17</url>
      <attachment type="software" hash="4ef7c368">2022.findings-acl.17.software.zip</attachment>
      <bibkey>yao-etal-2022-leven</bibkey>
      <pwccode url="https://github.com/thunlp/leven" additional="false">thunlp/leven</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/maven">MAVEN</pwcdataset>
    </paper>
    <paper id="21">
      <title>RuCCoN Clinical Concept Normalization in Russian<fixed-case>R</fixed-case>u<fixed-case>CC</fixed-case>o<fixed-case>N</fixed-case>: Clinical Concept Normalization in <fixed-case>R</fixed-case>ussian</title>
      <author><first>Alexandr</first><last>Nesterov</last></author>
      <author><first>Galina</first><last>Zubkova</last></author>
      <author><first>Zulfat</first><last>Miftahutdinov</last></author>
      <author><first>Vladimir</first><last>Kokh</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Anton</first><last>Alekseev</last></author>
      <author><first>Manvel</first><last>Avetisian</last></author>
      <author><first>Andrey</first><last>Chertok</last></author>
      <author><first>Sergey</first><last>Nikolenko</last></author>
      <pages>239-245</pages>
      <abstract>We present RuCCoN a new dataset for clinical concept normalization in <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a> manually annotated by medical professionals It contains over 16,028 entity mentions manually linked to over 2,409 unique concepts from the Russian language part of the UMLS ontology We provide train test splits for different settings stratified zero shot and CUI less and present strong baselines obtained with state of the art models such as SapBERT At present Russian medical NLP is lacking in both <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and trained <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> and we view this work as an important step towards filling this gap Our dataset and annotation guidelines are available at https://github.com/sberbank-ai-lab/RuCCoN.</abstract>
      <url hash="8f620f3a">2022.findings-acl.21</url>
      <bibkey>nesterov-etal-2022-ruccon</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/xl-bel">XL-BEL</pwcdataset>
    </paper>
    <paper id="32">
      <title>Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection</title>
      <author><first>Tulika</first><last>Bose</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <author><first>Irina</first><last>Illina</last></author>
      <author><first>Dominique</first><last>Fohr</last></author>
      <pages>372-382</pages>
      <abstract>Hate speech classifiers exhibit substantial performance degradation when evaluated on datasets different from the source. This is due to learning spurious correlations between words that are not necessarily relevant to hateful language, and hate speech labels from the training corpus. Previous work has attempted to mitigate this problem by regularizing specific terms from pre-defined static dictionaries. While this has been demonstrated to improve the generalizability of classifiers, the coverage of such methods is limited and the dictionaries require regular manual updates from human experts. In this paper, we propose to automatically identify and reduce spurious correlations using attribution methods with dynamic refinement of the list of terms that need to be regularized during training. Our approach is flexible and improves the cross-corpora performance over previous work independently and in combination with pre-defined dictionaries.</abstract>
      <url hash="c6b773c3">2022.findings-acl.32</url>
      <bibkey>bose-etal-2022-dynamically</bibkey>
      <pwccode url="https://github.com/tbose20/d-ref" additional="false">tbose20/d-ref</pwccode>
    </paper>
    <paper id="35">
      <title>Visualizing the Relationship Between Encoded Linguistic Information and Task Performance</title>
      <author><first>Jiannan</first><last>Xiang</last></author>
      <author><first>Huayang</first><last>Li</last></author>
      <author><first>Defu</first><last>Lian</last></author>
      <author><first>Guoping</first><last>Huang</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <pages>410-422</pages>
      <abstract>Probing is popular to analyze whether <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic information</a> can be captured by a well trained <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural model</a> but it is hard to answer how the change of the encoded linguistic information will affect task performance To this end we study the dynamic relationship between the encoded linguistic information and task performance from the viewpoint of <a href="https://en.wikipedia.org/wiki/Pareto_efficiency">Pareto Optimality</a> Its key idea is to obtain a set of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> which are Pareto optimal in terms of both objectives From this viewpoint we propose a method to optimize the <a href="https://en.wikipedia.org/wiki/Pareto_efficiency">Pareto optimal models</a> by formalizing it as a multi objective optimization problem We conduct experiments on two popular <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP tasks</a> i.e. <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> and <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a> and investigate the relationship between several kinds of <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic information</a> and task performances Experimental results demonstrate that the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is better than a baseline method Our empirical findings suggest that some syntactic information is helpful for NLP tasks whereas encoding more syntactic information does not necessarily lead to better performance because the model architecture is also an important factor</abstract>
      <url hash="42104ce7">2022.findings-acl.35</url>
      <bibkey>xiang-etal-2022-visualizing</bibkey>
    </paper>
    <paper id="36">
      <title>Efficient Argument Structure Extraction with <a href="https://en.wikipedia.org/wiki/Transfer_learning">Transfer Learning</a> and Active Learning</title>
      <author><first>Xinyu</first><last>Hua</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>423-437</pages>
      <abstract>The automation of extracting argument structures faces a pair of challenges on encoding long term contexts to facilitate comprehensive understanding and improving <a href="https://en.wikipedia.org/wiki/Data_efficiency">data efficiency</a> since constructing high quality argument structures is time consuming In this work we propose a novel context aware Transformer based argument structure prediction model which on five different domains significantly outperforms models that rely on <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> or only encode limited contexts To tackle the difficulty of data annotation we examine two complementary methods i transfer learning to leverage existing annotated data to boost model performance in a new target domain and ii <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)">active learning</a> to strategically identify a small amount of samples for <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> We further propose model independent sample acquisition strategies which can be generalized to diverse domains With extensive experiments we show that our simple yet effective acquisition strategies yield competitive results against three strong comparisons Combined with <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> substantial F1 score boost can be further achieved during the early iterations of <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a> across domains</abstract>
      <url hash="52042a9a">2022.findings-acl.36</url>
      <bibkey>hua-wang-2022-efficient</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cdcp">CDCP</pwcdataset>
    </paper>
    <paper id="40">
      <title><fixed-case>S</fixed-case>y<fixed-case>MC</fixed-case>o<fixed-case>M</fixed-case> - Syntactic Measure of Code Mixing A Study Of <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi Code-Mixing</title>
      <author><first>Prashant</first><last>Kodali</last></author>
      <author><first>Anmol</first><last>Goel</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <pages>472-480</pages>
      <abstract>Code mixing is the linguistic phenomenon where bilingual speakers tend to switch between two or more languages in conversations. Recent work on code-mixing in computational settings has leveraged social media code mixed texts to train NLP models. For capturing the variety of code mixing in, and across corpus, Language ID (LID) tags based measures (CMI) have been proposed. Syntactical variety/patterns of code-mixing and their relationship vis-a-vis computational model’s performance is under explored. In this work, we investigate a collection of English(en)-Hindi(hi) code-mixed datasets from a syntactic lens to propose, <tex-math>SyMCoM</tex-math>, an indicator of syntactic variety in code-mixed text, with intuitive theoretical bounds. We train SoTA en-hi PoS tagger, accuracy of 93.4%, to reliably compute PoS tags on a corpus, and demonstrate the utility of <tex-math>SyMCoM</tex-math> by applying it on various syntactical categories on a collection of datasets, and compare datasets using the measure.</abstract>
      <url hash="11f49965">2022.findings-acl.40</url>
      <bibkey>kodali-etal-2022-symcom</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/lince">LinCE</pwcdataset>
    </paper>
    <paper id="43">
      <title>Classification without (Proper) Representation: Political Heterogeneity in Social Media and Its Implications for Classification and Behavioral Analysis</title>
      <author><first>Kenan</first><last>Alkiek</last></author>
      <author><first>Bohan</first><last>Zhang</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>504-522</pages>
      <abstract>Reddit is home to a broad spectrum of political activity, and users signal their political affiliations in multiple ways—from self-declarations to community participation. Frequently, computational studies have treated political users as a single bloc, both in developing models to infer political leaning and in studying political behavior. Here, we test this assumption of political users and show that commonly-used political-inference models do not generalize, indicating heterogeneous types of political users. The models remain imprecise at best for most users, regardless of which sources of data or methods are used. Across a 14-year longitudinal analysis, we demonstrate that the choice in definition of a political user has significant implications for behavioral analysis. Controlling for multiple factors, political users are more toxic on the platform and inter-party interactions are even more toxic—but not all political users behave this way. Last, we identify a subset of political users who repeatedly flip affiliations, showing that these users are the most controversial of all, acting as provocateurs by more frequently bringing up politics, and are more likely to be banned, suspended, or deleted.</abstract>
      <url hash="4c1bc63e">2022.findings-acl.43</url>
      <bibkey>alkiek-etal-2022-classification</bibkey>
    </paper>
    <paper id="57">
      <title>Hierarchical Inductive Transfer for Continual Dialogue Learning</title>
      <author><first>Shaoxiong</first><last>Feng</last></author>
      <author><first>Xuancheng</first><last>Ren</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>693-699</pages>
      <abstract>Pre trained models have achieved excellent performance on the <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue task</a> However for the continual increase of online chit chat scenarios directly fine tuning these <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a> for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre trained models and knowledge interference among diverse dialogue tasks In this work we propose a hierarchical inductive transfer framework to learn and deploy the <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue skills</a> continually and efficiently First we introduce the adapter module into <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">pre trained models</a> for learning new dialogue tasks As the only trainable module it is beneficial for the <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue system</a> on the embedded devices to acquire new <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue skills</a> with negligible additional parameters Then for alleviating knowledge interference between tasks yet benefiting the regularization between them we further design hierarchical inductive transfer that enables new tasks to use general knowledge in the base adapter without being misled by diverse knowledge in task specific adapters Empirical evaluation and analysis indicate that our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> obtains comparable performance under deployment friendly model capacity</abstract>
      <url hash="d38f5210">2022.findings-acl.57</url>
      <bibkey>feng-etal-2022-hierarchical</bibkey>
    </paper>
    <paper id="62">
      <title>A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction</title>
      <author id="yang-liu-hk"><first>Yang</first><last>Liu</last></author>
      <author><first>Jinpeng</first><last>Hu</last></author>
      <author><first>Xiang</first><last>Wan</last></author>
      <author><first>Tsung-Hui</first><last>Chang</last></author>
      <pages>757-763</pages>
      <abstract>Few-Shot Relation Extraction aims at predicting the relation for a pair of entities in a sentence by training with a few labelled examples in each relation. Some recent works have introduced relation information (i.e., relation labels or descriptions) to assist model learning based on Prototype Network. However, most of them constrain the prototypes of each relation class implicitly with relation information, generally through designing complex network structures, like generating hybrid features, combining with contrastive learning or attention networks. We argue that relation information can be introduced more explicitly and effectively into the model. Thus, this paper proposes a direct addition approach to introduce relation information. Specifically, for each relation class, the relation representation is first generated by concatenating two views of relations (i.e., [CLS] token embedding and the mean value of embeddings of all tokens) and then directly added to the original prototype for both train and prediction. Experimental results on the benchmark dataset FewRel 1.0 show significant improvements and achieve comparable results to the state-of-the-art, which demonstrates the effectiveness of our proposed approach. Besides, further analyses verify that the direct addition is a much more effective way to integrate the relation representations and the original prototypes.</abstract>
      <url hash="ab7978e8">2022.findings-acl.62</url>
      <attachment type="software" hash="0e1fede5">2022.findings-acl.62.software.zip</attachment>
      <bibkey>liu-etal-2022-simple</bibkey>
      <pwccode url="https://github.com/lylylylylyly/simplefsre" additional="false">lylylylylyly/simplefsre</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="63">
      <title><fixed-case>MIMIC</fixed-case>ause: <fixed-case>R</fixed-case>epresentation and automatic extraction of causal relation types from clinical notes</title>
      <author><first>Vivek</first><last>Khetan</last></author>
      <author><first>Md Imbesat</first><last>Rizvi</last></author>
      <author><first>Jessica</first><last>Huber</last></author>
      <author><first>Paige</first><last>Bartusiak</last></author>
      <author><first>Bogdan</first><last>Sacaleanu</last></author>
      <author><first>Andrew</first><last>Fano</last></author>
      <pages>764-773</pages>
      <abstract>Understanding causal narratives communicated in clinical notes can help make strides towards personalized healthcare. Extracted causal information from clinical notes can be combined with structured EHR data such as patients’ demographics, diagnoses, and medications. This will enhance healthcare providers’ ability to identify aspects of a patient’s story communicated in the clinical notes and help make more informed decisions. In this work, we propose annotation guidelines, develop an annotated corpus and provide baseline scores to identify types and direction of causal relations between a pair of biomedical concepts in clinical notes; communicated implicitly or explicitly, identified either in a single sentence or across multiple sentences. We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2 shared task dataset and train four different language model based architectures. Annotation based on our guidelines achieved a high inter-annotator agreement i.e. Fleiss’ kappa (<tex-math>\kappa</tex-math>) score of 0.72, and our model for identification of causal relations achieved a macro F1 score of 0.56 on the test data. The high inter-annotator agreement for clinical text shows the quality of our annotation guidelines while the provided baseline F1 score sets the direction for future research towards understanding narratives in clinical texts. </abstract>
      <url hash="36479e9c">2022.findings-acl.63</url>
      <bibkey>khetan-etal-2022-mimicause</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="66">
      <title>Fact-Tree Reasoning for N-ary Question Answering over Knowledge Graphs</title>
      <author><first>Yao</first><last>Zhang</last></author>
      <author><first>Peiyao</first><last>Li</last></author>
      <author><first>Hongru</first><last>Liang</last></author>
      <author><first>Adam</first><last>Jatowt</last></author>
      <author><first>Zhenglu</first><last>Yang</last></author>
      <pages>788-802</pages>
      <abstract>Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts. However, it neglects the n-ary facts, which contain more than two entities. In this work, we highlight a more challenging but under-explored task: n-ary KGQA, i.e., answering n-ary facts questions upon n-ary KGs. Nevertheless, the multi-hop reasoning framework popular in binary KGQA task is not directly applicable on n-ary KGQA. We propose two feasible improvements: 1) upgrade the basic reasoning unit from entity or relation to fact, and 2) upgrade the reasoning structure from chain to tree. Therefore, we propose a novel fact-tree reasoning framework, FacTree, which integrates the above two upgrades. FacTree transforms the question into a fact tree and performs iterative fact reasoning on the fact tree to infer the correct answer. Experimental results on the n-ary KGQA dataset we constructed and two binary KGQA benchmarks demonstrate the effectiveness of FacTree compared with state-of-the-art methods.</abstract>
      <url hash="248c692c">2022.findings-acl.66</url>
      <bibkey>zhang-etal-2022-fact</bibkey>
    </paper>
    <paper id="69">
      <title>Mukayese Turkish NLP Strikes Back<fixed-case>T</fixed-case>urkish <fixed-case>NLP</fixed-case> Strikes Back</title>
      <author><first>Ali</first><last>Safaya</last></author>
      <author><first>Emirhan</first><last>Kurtuluş</last></author>
      <author><first>Arda</first><last>Goktogan</last></author>
      <author><first>Deniz</first><last>Yuret</last></author>
      <pages>846-863</pages>
      <abstract>Having sufficient resources for language X lifts it from the under resourced languages class but not necessarily from the under researched class In this paper we address the problem of the absence of organized benchmarks in the <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish language</a> We demonstrate that <a href="https://en.wikipedia.org/wiki/Language">languages</a> such as <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a> are left behind the state of the art in NLP applications As a solution we present Mukayese a set of NLP benchmarks for the <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish language</a> that contains several NLP tasks We work on one or more datasets for each <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark</a> and present two or more baselines Moreover we present four new benchmarking datasets in <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a> for language modeling sentence segmentation and <a href="https://en.wikipedia.org/wiki/Spell_checker">spell checking</a> All <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and baselines are available under https://github.com/alisafaya/mukayese</abstract>
      <url hash="e0c245a3">2022.findings-acl.69</url>
      <bibkey>safaya-etal-2022-mukayese</bibkey>
      <pwccode url="https://github.com/alisafaya/mukayese" additional="false">alisafaya/mukayese</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="73">
      <title>Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model</title>
      <author><first>Jiayi</first><last>Wang</last></author>
      <author><first>Rongzhou</first><last>Bao</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>905-915</pages>
      <abstract>Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust. However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. We question the validity of the current evaluation of robustness of PrLMs based on these non-natural adversarial samples and propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples. We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply the anomaly detector to a defense framework to enhance the robustness of PrLMs. It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks.</abstract>
      <url hash="4d370eed">2022.findings-acl.73</url>
      <bibkey>wang-etal-2022-distinguishing</bibkey>
      <pwccode url="https://github.com/lilynlp/distinguishing-non-natural" additional="false">lilynlp/distinguishing-non-natural</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="77">
      <title>GRS Combining Generation and Revision in Unsupervised Sentence Simplification<fixed-case>GRS</fixed-case>: Combining Generation and Revision in Unsupervised Sentence Simplification</title>
      <author><first>Mohammad</first><last>Dehghan</last></author>
      <author><first>Dhruv</first><last>Kumar</last></author>
      <author><first>Lukasz</first><last>Golab</last></author>
      <pages>949-960</pages>
      <abstract>We propose GRS an unsupervised approach to <a href="https://en.wikipedia.org/wiki/Sentence_simplification">sentence simplification</a> that combines text generation and text revision We start with an iterative framework in which an input sentence is revised using explicit edit operations   and add <a href="https://en.wikipedia.org/wiki/Paraphrasing">paraphrasing</a> as a new edit operation This allows us to combine the advantages of generative and revision based approaches paraphrasing captures complex edit operations   and the use of explicit edit operations in an iterative manner provides controllability and interpretability We demonstrate these advantages of GRS compared to existing methods on the Newsela and ASSET datasets</abstract>
      <url hash="d8045c78">2022.findings-acl.77</url>
      <bibkey>dehghan-etal-2022-grs</bibkey>
      <pwccode url="https://github.com/imohammad12/grs" additional="false">imohammad12/grs</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/asset">ASSET</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="79">
      <title>Distributed NLI Learning to Predict Human Opinion Distributions for Language Reasoning<fixed-case>NLI</fixed-case>: Learning to Predict Human Opinion Distributions for Language Reasoning</title>
      <author><first>Xiang</first><last>Zhou</last></author>
      <author><first>Yixin</first><last>Nie</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>972-987</pages>
      <abstract>We introduce distributed NLI a new NLU task with a goal to predict the distribution of human judgements for natural language inference We show that by applying additional distribution estimation methods namely Monte Carlo MC Dropout Deep Ensemble Re Calibration and Distribution Distillation models can capture human judgement distribution more effectively than the softmax baseline We show that MC Dropout is able to achieve decent performance without any distribution annotations while Re Calibration can give further improvements with extra distribution annotations suggesting the value of multiple annotations for one example in modeling the distribution of human judgements Despite these improvements the best results are still far below the estimated human upper bound indicating that predicting the distribution of human judgements is still an open challenging problem with a large room for improvements We showcase the common errors for MC Dropout and Re Calibration Finally we give guidelines on the usage of these methods with different levels of data availability and encourage future work on modeling the human opinion distribution for language reasoning</abstract>
      <url hash="ed25a73b">2022.findings-acl.79</url>
      <attachment type="software" hash="5a4914c7">2022.findings-acl.79.software.zip</attachment>
      <bibkey>zhou-etal-2022-distributed</bibkey>
      <pwccode url="https://github.com/easonnie/ChaosNLI" additional="false">easonnie/ChaosNLI</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chaosnli">ChaosNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="86">
      <title>What to Learn, and How: <fixed-case>T</fixed-case>oward Effective Learning from Rationales</title>
      <author><first>Samuel</first><last>Carton</last></author>
      <author><first>Surya</first><last>Kanoria</last></author>
      <author><first>Chenhao</first><last>Tan</last></author>
      <pages>1075-1088</pages>
      <abstract>Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision. While intuitive, this idea has proven elusive in practice. We make two observations about human rationales via empirical analyses:1) maximizing rationale supervision accuracy is not necessarily the optimal objective for improving model accuracy; 2) human rationales vary in whether they provide sufficient information for the model to exploit for prediction.Building on these insights, we propose several novel loss functions and learning strategies, and evaluate their effectiveness on three datasets with human rationales. Our results demonstrate consistent improvements over baselines in both label and rationale accuracy, including a 3% accuracy improvement on MultiRC. Our work highlights the importance of understanding properties of human explanations and exploiting them accordingly in model training.</abstract>
      <url hash="9ba00c79">2022.findings-acl.86</url>
      <bibkey>carton-etal-2022-learn</bibkey>
      <pwccode url="https://github.com/chicagohai/learning-from-rationales" additional="false">chicagohai/learning-from-rationales</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e-snli">e-SNLI</pwcdataset>
    <title_ar>تحليل بيانات التدريب الديناميكي العدائي في الحد</title_ar>
      <title_pt>Analisando Dados de Treinamento Dinâmico Adversarial no Limite</title_pt>
      <title_es>Análisis de datos dinámicos de entrenamiento contradictorio en el límite</title_es>
      <title_ja>限界内の動的対抗トレーニングデータの分析</title_ja>
      <title_zh>于限内分析动度</title_zh>
      <title_hi>सीमा में डायनेमिक प्रतिकूल प्रशिक्षण डेटा का विश्लेषण करना</title_hi>
      <title_ga>Anailís a dhéanamh ar Shonraí Oiliúna Sáraíochta Dinimiciúla sa Teorainn</title_ga>
      <title_ka>Name</title_ka>
      <title_hu>Dinamikus negatív edzési adatok elemzése a határon belül</title_hu>
      <title_el>Ανάλυση δυναμικών αρνητικών δεδομένων εκπαίδευσης στο όριο</title_el>
      <title_it>Analisi dei dati di allenamento avversi dinamici nel limite</title_it>
      <title_kk>Динамикалық конверсариялық оқыту деректерін шектеу</title_kk>
      <title_lt>Dinaminių nepalankiojo mokymo duomenų analizė riboje</title_lt>
      <title_mk>Анализирање на динамските податоци за непријатно обука во границата</title_mk>
      <title_ml>സീമിറ്റില്‍ ഡിനാനിക്കല്‍ പരിശീലന വിവരങ്ങള്‍ അന്വേഷിക്കുന്നു</title_ml>
      <title_ms>Menganalisis Data Latihan Melawan Dinamik dalam Had</title_ms>
      <title_mt>Analiżi ta’ Dejta Dinamika ta’ Taħriġ Adversarju fil-Limitu</title_mt>
      <title_mn>Хязгаарт шинжлэх ухааны дасгал хөдөлгөөн өгөгдлийг шинжилгээ</title_mn>
      <title_no>Analiserer dynamiske rekursarialske treningsdata i grensen</title_no>
      <title_pl>Analiza dynamicznych danych treningowych przeciwników w granicy</title_pl>
      <title_ro>Analiza datelor de antrenament adversar dinamic în limita</title_ro>
      <title_sr>Analiziranje dinamičkih naprednih podataka za obuku u granici</title_sr>
      <title_si>Name</title_si>
      <title_so>Analyzerka macluumaadka waxbarashada ee cilmiga</title_so>
      <title_sv>Analysera dynamiska negativa träningsdata i gränsen</title_sv>
      <title_ta>எல்லையில் Dynamic முன்னேற்றம் பயிற்சி தகவல்களை ஆய்வு செய்கிறது</title_ta>
      <title_ur>محدودیت میں داینامیکی اڈورسٹرین ٹرینینگ ڈاٹا تحلیل کیا جا رہا ہے</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Phân tích dữ liệu về tình trạng chờ tế bào</title_vi>
      <title_hr>Analiziranje dinamičkih poremećajnih podataka obuke u granici</title_hr>
      <title_bg>Анализиране на динамичните данни за неблагоприятно обучение в границите</title_bg>
      <title_da>Analyse af dynamiske negative træningsdata i grænsen</title_da>
      <title_nl>Dynamic Adversarial Training Data in the Limit analyseren</title_nl>
      <title_de>Analyse dynamischer Adversarial Trainingsdaten im Limit</title_de>
      <title_id>Analisasi Data Pelatihan Atas Dinamik dalam Batas</title_id>
      <title_ko>극한 상태에서 동적 대항 훈련 데이터 분석</title_ko>
      <title_fa>تحلیل داده‌های آموزش تحقیقات دینامیک در محدوده</title_fa>
      <title_af>Analiseer Dinamiese Adversariale Oefening Data in die Grens</title_af>
      <title_tr>Dinamik Ullanyş Taýramçylyk Maglumaty Çözümleme</title_tr>
      <title_sq>Analizimi i të dhënave dinamike të trajnimit kundërshtar në kufi</title_sq>
      <title_sw>Anachambua taarifa za mafunzo ya kidynamic katika Mipaka</title_sw>
      <title_hy>Դինամական հակառակ վարժման տվյալների վերլուծությունը սահմանափակում</title_hy>
      <title_am>dialogs-action</title_am>
      <title_bn>সীমান্তে ডাইনামিক প্রশিক্ষণ তথ্য বিশ্লেষণ করা হচ্ছে</title_bn>
      <title_az>Sınırda Dynamik Adversarial Training Data Analyzing</title_az>
      <title_bs>Analiziranje dinamičkih naprednih podataka za obuku u granici</title_bs>
      <title_ca>Analitzar les dades dinàmiques d'entrenament adversari al límit</title_ca>
      <title_cs>Analýza dat dynamického nepřátelského tréninku v limitu</title_cs>
      <title_et>Dünaamiliste kõrvaltoimete koolituse andmete analüüsimine piirides</title_et>
      <title_fi>Dynaamisten haittaharjoitustietojen analysointi rajassa</title_fi>
      <title_jv>Language</title_jv>
      <title_ha>Ana analyza data na Training na Dama cikin Limit</title_ha>
      <title_he>ניתוח נתוני אימון נוגדי דינמי בגבול</title_he>
      <title_sk>Analiza dinamičnih podatkov o neželenem treningu v meji</title_sk>
      <title_bo>ཚད་འཛིན་ནང་གི་སྤྱིར་གཏོང་གི་Adversarial Training Data་ཞིབ་དཔྱད་བྱེད་བཞིན་པ</title_bo>
      <abstract_ar>لإنشاء نماذج قوية عبر مجموعة واسعة من مدخلات الاختبار ، يجب أن تتضمن مجموعات بيانات التدريب أمثلة متنوعة تغطي العديد من الظواهر. جمع البيانات الديناميكية العدائية (DADC) ، حيث يصنع المعلقون أمثلة تتحدى النماذج التي تتحسن باستمرار ، يبشر بالخير كنهج لتوليد مثل هذه المجموعات التدريبية المتنوعة. أظهر العمل السابق أن تشغيل DADC على مدى 1-3 جولات يمكن أن يساعد النماذج في إصلاح بعض أنواع الأخطاء ، لكنه لا يؤدي بالضرورة إلى تعميم أفضل يتجاوز بيانات الاختبار العدائية. نجادل بأن تشغيل DADC على عدة جولات يزيد من فوائد وقت التدريب ، حيث يمكن للجولات المختلفة أن تغطي معًا العديد من الظواهر ذات الصلة بالمهام. نقدم الدراسة الأولى لـ DADC على المدى الطويل ، حيث نجمع 20 جولة من أمثلة NLI لمجموعة صغيرة من الفقرات الافتتاحية ، مع كل من المناهج العدائية وغير العدائية. النماذج المدربة على أمثلة DADC تجعل الأخطاء أقل بنسبة 26٪ في مجموعة الاختبار المنسقة من الخبراء لدينا مقارنة بالنماذج المدربة على البيانات غير العدائية. يُظهر تحليلنا أن DADC ينتج أمثلة أكثر صعوبة وأكثر تنوعًا معجميًا وتركيبيًا ، وتحتوي على عدد أقل من القطع الأثرية مقارنة بالأمثلة غير العدائية.</abstract_ar>
      <abstract_es>Para crear modelos que sean sólidos en una amplia gama de entradas de prueba, los conjuntos de datos de entrenamiento deben incluir diversos ejemplos que abarquen numerosos fenómenos. La recopilación dinámica de datos contradictorios (DADC), en la que los anotadores crean ejemplos que desafían los modelos que mejoran continuamente, es prometedora como enfoque para generar conjuntos de entrenamiento tan diversos. Trabajos anteriores han demostrado que ejecutar DADC durante 1 a 3 rondas puede ayudar a los modelos a corregir algunos tipos de error, pero no necesariamente conduce a una mejor generalización más allá de los datos de prueba contradictorios. Sostenemos que ejecutar DADC durante muchas rondas maximiza sus beneficios de tiempo de entrenamiento, ya que las diferentes rondas juntas pueden cubrir muchos de los fenómenos relevantes para la tarea. Presentamos el primer estudio de DADC a largo plazo, donde recopilamos 20 rondas de ejemplos de NLI para un pequeño conjunto de párrafos de premisas, con enfoques contradictorios y no contradictorios. Los modelos entrenados en ejemplos de DADC cometen 26\% menos errores en nuestro conjunto de pruebas seleccionadas por expertos en comparación con los modelos entrenados en datos no contradictorios. Nuestro análisis muestra que el DADC produce ejemplos que son más difíciles, más diversos desde el punto de vista léxico y sintáctico, y que contienen menos artefactos de anotación en comparación con los ejemplos no contradictorios.</abstract_es>
      <abstract_pt>Para criar modelos robustos em uma ampla variedade de entradas de teste, os conjuntos de dados de treinamento devem incluir diversos exemplos que abrangem vários fenômenos. A coleta dinâmica de dados adversários (DADC), em que os anotadores criam exemplos que desafiam modelos de melhoria contínua, é promissora como uma abordagem para gerar conjuntos de treinamento tão diversos. Trabalhos anteriores mostraram que a execução do DADC em 1-3 rodadas pode ajudar os modelos a corrigir alguns tipos de erro, mas não necessariamente leva a uma melhor generalização além dos dados de teste adversários. Argumentamos que executar o DADC em muitas rodadas maximiza seus benefícios de tempo de treinamento, pois as diferentes rodadas podem, juntas, cobrir muitos dos fenômenos relevantes para a tarefa. Apresentamos o primeiro estudo de DADC de longo prazo, onde coletamos 20 rodadas de exemplos de NLI para um pequeno conjunto de parágrafos de premissa, com abordagens adversas e não adversas. Modelos treinados em exemplos de DADC cometem 26\% menos erros em nosso conjunto de testes com curadoria de especialistas em comparação com modelos treinados em dados não adversários. Nossa análise mostra que o DADC produz exemplos que são mais difíceis, mais lexicalmente e sintaticamente diversos e contêm menos artefatos de anotação em comparação com exemplos não adversários.</abstract_pt>
      <abstract_ja>幅広いテスト入力にわたって堅牢なモデルを作成するには、トレーニングデータセットに、多数の現象にわたる多様な例を含める必要があります。 注釈者が継続的にモデルを改善することに挑戦する例を作成する動的対抗データ収集（ DADC ）は、そのような多様なトレーニングセットを生成するためのアプローチとして約束を持っています。 以前の研究では、DADCを1〜3ラウンドにわたって実行することで、モデルがいくつかのエラータイプを修正するのに役立つことが示されていますが、対抗テストデータを超えてより良い一般化につながるとは限りません。 DADCを多くのラウンドにわたって実行することは、異なるラウンドが一緒にタスクに関連する現象の多くをカバーすることができるため、トレーニング時間の利点を最大化すると主張している。 私たちは、長期的なDADCの最初の研究を提示します。ここでは、対立的アプローチと非対立的アプローチの両方を使用して、少数の前提パラグラフのための20ラウンドのNLIの例を収集します。 DADCの例でトレーニングされたモデルは、非対抗データでトレーニングされたモデルと比較して、専門家がキュレーションしたテストセットのエラーが26 \%少なくなります。 我々の分析は、DADCが、非対立的な例と比較して、より困難であり、より語彙的および構文的に多様であり、より少ない注釈アーチファクトを含む例をもたらすことを示している。</abstract_ja>
      <abstract_zh>若创于诸试输中有鲁棒性模形,练数集应包越诸示例。 动对抗性数收(DADC),其注释者作挑战不断改进模形之示例,有望于此多样化练集之法。 前之论明,行于1-3轮中DADC可以助形修非,非对抗性测试数据之外,未必善泛化也。 臣愚以为行数合DADC可最大化练时之利,异宜可共涵盖多事也。 请长DADC之第一项,为一小组提段落收20轮NLI示例,有对抗性非对抗性之法。 比于非对抗性之数,比于 DADC 示例之试,损于吾家之 26\%。 臣等之分析表明,比于非对抗性示例,DADC生示例益难,词汇语法更多样化,而包注伪影。</abstract_zh>
      <abstract_hi>मॉडल बनाने के लिए जो परीक्षण इनपुट की एक विस्तृत श्रृंखला में मजबूत हैं, प्रशिक्षण डेटासेट में विविध उदाहरण शामिल होने चाहिए जो कई घटनाओं को फैलाते हैं। गतिशील प्रतिकूल डेटा संग्रह (डीएडीसी), जहां एनोटेटर उदाहरणों को शिल्प करते हैं जो लगातार मॉडल में सुधार को चुनौती देते हैं, इस तरह के विविध प्रशिक्षण सेट उत्पन्न करने के लिए एक दृष्टिकोण के रूप में वादा करते हैं। पहले के काम से पता चला है कि 1-3 राउंड पर डीएडीसी चलाने से मॉडल को कुछ त्रुटि प्रकारों को ठीक करने में मदद मिल सकती है, लेकिन यह जरूरी नहीं कि प्रतिकूल परीक्षण डेटा से परे बेहतर सामान्यीकरण का कारण बने। हम तर्क देते हैं कि कई दौरों में डीएडीसी चलाने से इसके प्रशिक्षण-समय लाभों को अधिकतम किया जाता है, क्योंकि विभिन्न राउंड एक साथ कई कार्य-प्रासंगिक घटनाओं को कवर कर सकते हैं। हम लंबी अवधि के DADC का पहला अध्ययन प्रस्तुत करते हैं, जहां हम प्रतिकूल और गैर-प्रतिकूल दृष्टिकोण दोनों के साथ, आधार पैराग्राफ के एक छोटे से सेट के लिए एनएलआई उदाहरणों के 20 राउंड एकत्र करते हैं। डीएडीसी उदाहरणों पर प्रशिक्षित मॉडल गैर-प्रतिकूल डेटा पर प्रशिक्षित मॉडल की तुलना में हमारे विशेषज्ञ-क्यूरेटेड परीक्षण सेट पर 26\ % कम त्रुटियां करते हैं। हमारे विश्लेषण से पता चलता है कि डीएडीसी ऐसे उदाहरण देता है जो अधिक कठिन, अधिक लेक्सिकल और वाक्यात्मक रूप से विविध हैं, और गैर-प्रतिकूल उदाहरणों की तुलना में कम एनोटेशन कलाकृतियां हैं।</abstract_hi>
      <abstract_ga>Chun samhlacha a chruthú atá láidir thar raon leathan ionchuir tástála, ba cheart go n-áireodh tacair sonraí oiliúna samplaí éagsúla a chuimsíonn feiniméin iomadúla. Tá gealltanas ag bailiú sonraí sáraíochta dinimiciúla (DADC), áit a ndéanann anótálaithe samplaí a chruthú a thugann dúshlán do mhúnlaí a fheabhsú go leanúnach, mar chur chuige chun tacair oiliúna chomh héagsúil sin a ghiniúint. Léiríodh le réamhobair gur féidir le reáchtáil DADC thar 1-3 bhabhta cabhrú le samhlacha roinnt cineálacha earráide a shocrú, ach ní gá go n-eascródh ginearálú níos fearr as sonraí tástála sáraíochta. Áitímid go n-uasmhéadaíonn reáchtáil DADC thar go leor babhtaí na tairbhí a bhaineann leis maidir le ham oiliúna, toisc gur féidir leis na babhtaí éagsúla le chéile go leor de na feiniméin a bhaineann le tascanna a chlúdach. Cuirimid an chéad staidéar ar DADC níos fadtéarmaí i láthair, áit a mbailímid 20 babhta de shamplaí LNÉ do thacar beag de mhíreanna bonn, le cineálacha cur chuige sáraíochta agus neamhsháraíochta araon. Déanann samhlacha atá oilte ar shamplaí DADC 26 \% níos lú earráidí ar ár dtacar tástála coimeádta ag saineolaithe i gcomparáid le samhlacha atá oilte ar shonraí neamhsháraíochta. Léiríonn ár n-anailís go dtugann DADC samplaí atá níos deacra, níos éagsúla ó thaobh foclóireachta agus comhréire, agus a bhfuil níos lú déantúsáin nótaí iontu i gcomparáid le samplaí neamhsháraíochta.</abstract_ga>
      <abstract_ka>მოდელების შექმნა, რომლებიც ძალიან ძალიან განსხვავებული ტესტის მონაცემების გარეშე, განსწავლების მონაცემები უნდა შექმნა განსხვავებული მაგალითები, რომლებიც მრავალ დინამიკური ანტორიალური მონაცემების კოლექცია (DADC), სადაც ანტოტორიების მაგალითები, რომელიც მუშაობაც მოდელების გაუკეთებას წარმოადგენს, გვეყვებს, როგორც ასეთი განსხვავებული პირველი სამუშაო მუშაობა ჩვენება, რომ DADC 1- 3 პუნდზე გადაწყება შესაძლებელია მოდელების დახმარება, მაგრამ ეს არ უნდა უფრო უკეთესი გენერალიზაცია განსაზღვრებული ტესტი ჩვენ ვაკეთებთ, რომ DADC-ის გადაწყვეტილება მრავალ პრონეტებში მაქსიმიკურებს მისი სამყარო სამყარო სამყარო გამოსახულება, რადგან განსხვავებული პრონეტები შეუძლი ჩვენ დავიწყებთ პირველი კვლევა DADC-ის უკვე სიმართლეში, სადაც ჩვენ შევძლებთ NLI მაგალითების 20 კონდის მაგალითების მაგალითების მაგალითი პრემიზის ნაწილი, რომლებიც ორივე განსაცემებული და არ DADC მაგალითად განსწავლებული მოდელები 26\% უფრო ცოტა ჩვენი ექსპერტის კურვილი ტესტის შეცდომის შედგომა, რომლებიც არ განსწავლებელი მოდელთან განსწავლებული მოდელთან. ჩვენი ანალიზია, რომ DADC იქნება მაგალითები, რომლებიც უფრო რთული, ლექსიკურად და სინტაქტიკურად განსხვავებულია, და უფრო ცოტა ანალიზაციის არტაკტებები, რომლებიც არ განს</abstract_ka>
      <abstract_hu>Annak érdekében, hogy a vizsgálati bemenetek széles skáláján robusztus modelleket hozzon létre, a képzési adatoknak számos jelenséget átfogó példát kell tartalmazniuk. A dinamikus ellenséges adatgyűjtés (DADC), ahol a kommentátorok olyan példákat készítenek, amelyek kihívást jelentenek a folyamatosan fejlődő modellek, ígéretes megközelítést nyújtanak az ilyen sokféle képzési készletek létrehozásához. Korábbi munkák azt mutatták, hogy a DADC 1-3 fordulóban történő futtatása segíthet a modellek kijavítani bizonyos hibatípusokat, de ez nem feltétlenül vezet jobb általánosításhoz az ellenséges tesztadatokon túl. Azzal érvelünk, hogy a DADC több fordulón keresztül történő futtatása maximalizálja edzési idő előnyeit, mivel a különböző fordulók együttesen lefedhetik a feladat-releváns jelenségek számát. Bemutatjuk a hosszabb távú DADC első tanulmányát, ahol 20 forduló NLI példát gyűjtünk egy kis előzetes bekezdéshez, ellenséges és nem ellenséges megközelítésekkel. A DADC példákra képzett modellek 26\%-kal kevesebb hibát okoznak a szakértői által kiválasztott tesztkészletünkön, mint a nem ellentétes adatokra képzett modellek. Elemzésünk azt mutatja, hogy a DADC nehezebb, lexikailag és szintaktikailag sokszínűbb példákat hoz létre, és kevesebb jegyzetelési leletet tartalmaz a nem ellentétes példákhoz képest.</abstract_hu>
      <abstract_el>Για τη δημιουργία μοντέλων που είναι ανθεκτικά σε ένα ευρύ φάσμα εισροών δοκιμών, τα σύνολα δεδομένων κατάρτισης θα πρέπει να περιλαμβάνουν διάφορα παραδείγματα που καλύπτουν πολλά φαινόμενα. Η δυναμική συλλογή δεδομένων αντιδιαστάσεων (όπου σχολιαστές δημιουργούν παραδείγματα που αμφισβητούν τη συνεχή βελτίωση των μοντέλων, αποτελεί υπόσχεση ως προσέγγιση για τη δημιουργία τέτοιων διαφορετικών εκπαιδευτικών συνόλων. Προηγουμένες εργασίες έχουν δείξει ότι η εκτέλεση σε κύκλους 1-3 μπορεί να βοηθήσει τα μοντέλα να διορθώσουν ορισμένους τύπους σφαλμάτων, αλλά δεν οδηγεί απαραίτητα σε καλύτερη γενίκευση πέρα από τα αντικρουόμενα δεδομένα δοκιμής. Υποστηρίζουμε ότι η εκτέλεση σε πολλούς γύρους μεγιστοποιεί τα οφέλη του χρόνου κατάρτισης, καθώς οι διάφοροι γύροι μπορούν να καλύψουν μαζί πολλά από τα σχετικά με την εργασία φαινόμενα. Παρουσιάζουμε την πρώτη μελέτη του μακροπρόθεσμου DADC, όπου συγκεντρώνουμε είκοσι γύρους παραδειγμάτων για ένα μικρό σύνολο παραγράφων προϋποθέσεων, με αντιφατικές και μη αντιφατικές προσεγγίσεις. Τα μοντέλα που εκπαιδεύονται βάσει παραδειγμάτων κάνουν 26\% λιγότερα λάθη στο σετ δοκιμών που επιμελούνται από εμπειρογνώμονες σε σύγκριση με τα μοντέλα που εκπαιδεύονται σε μη αντίπαλα δεδομένα. Η ανάλυσή μας δείχνει ότι η DADC αποδίδει παραδείγματα που είναι πιο δύσκολα, πιο λεξικά και συντακτικά διαφορετικά, και περιέχουν λιγότερα τεχνουργήματα σχολιασμού σε σύγκριση με μη αντιπαραβαλλόμενα παραδείγματα.</abstract_el>
      <abstract_it>Per creare modelli robusti in un'ampia gamma di input di test, i set di dati di formazione dovrebbero includere esempi diversi che coprono numerosi fenomeni. La raccolta dinamica dei dati avversi (DADC), in cui gli annotatori creano esempi che sfidano modelli in continuo miglioramento, è un approccio promettente per generare set di formazione così diversi. Il lavoro precedente ha dimostrato che l'esecuzione di DADC su 1-3 turni può aiutare i modelli a correggere alcuni tipi di errore, ma non porta necessariamente a una migliore generalizzazione oltre i dati di test avversi. Sosteniamo che l'esecuzione di DADC su molti turni massimizza i suoi benefici di allenamento-tempo, in quanto i diversi turni possono coprire insieme molti dei fenomeni rilevanti per il compito. Presentiamo il primo studio del DADC a lungo termine, dove raccogliamo 20 round di esempi NLI per un piccolo set di paragrafi preminenti, con approcci sia avversari che non avversari. I modelli formati su esempi DADC producono il 26% di errori in meno sul nostro set di test curato da esperti rispetto ai modelli formati su dati non avversi. La nostra analisi mostra che DADC produce esempi che sono più difficili, più lessicamente e sintatticamente diversi e contengono meno artefatti di annotazione rispetto ad esempi non avversari.</abstract_it>
      <abstract_kk>Көптеген сынақтар келтірілген үлгілерді құру үшін, бақылау деректер жиындары көптеген мәселелерді көптеген түрлі мысалдар болуы тиіс. Динамикалық негатриялық деректер жинақтауы (DADC), бұл жерде моделдерді жақсарту үшін белгілер үлгілерді жасайтын мәселелер жасайды, бұл әртүрлі оқыту бағдарламаларын құру үшін әлемді Алдыңғы жұмыс істеген DADC 1- 3 тұлбадан орындалуда кейбір қатенің түрлерін түзетуге көмектеседі, бірақ бұл қатенің түрлерін түзетуге көмектеседі, бірақ қарсы сынақтар деректерінен ар Біз DADC жұмыс істеу көпшілігін көптеген жұмыс істеу үшін оның бақылау уақыттың мүмкіндіктерін көптеген, себебі әртүрлі жұмыс істеу үшін тапсырмалардың көптег Біз DADC ұзындық уақытты бірінші зерттеуді таңдаймыз. Бұл жерде NLI пішімдерінің 20 мәселелерін кішкентай премия параграфиялар үшін, негатриялық және негатриялық емес жағдайларды біріктіреміз. DADC мысалдарында оқылған үлгілер эксперттердің өзгертілген сынақтарымыздың 26\% деген қателерін негізгі деректер үлгілерімен салыстыру үлгілеріне сәйкес келеді. Біздің анализиямыз, DADC деген мәселелерді қатты, лексикалық және синтактикалық түрлі түрлі мәселелерді көрсетеді. Олардың негізгі мәселелерімен салыстырылған жазбалардың артефакттары аз.</abstract_kk>
      <abstract_lt>To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena.  Dinaminis priešingų duomenų rinkimas (DADC), kuriame anotatoriai rengia pavyzdžius, kurie nuolat kelia prieštaravimų modeliams tobulinti, yra pažadus, kad toks įvairių mokymo rinkinių kūrimo metodas. Ankstesnis darbas parodė, kad DADC veikimas per 1–3 raundus gali padėti modeliams ištaisyti kai kuriuos klaidų tipus, tačiau nebūtinai lemia geresnę generalizaciją nei priešingų bandymų duomenys. Mes teigiame, kad daugeliu apskritimų vykdant DADC maksimaliai padidina mokymo trukmę, nes skirtingi apskritimai kartu gali apimti daugelį su užduotimis susijusių reiškinių. We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches.  Models trained on DADC examples make 26\% fewer errors on our expert-curated test set compared to models trained on non-adversarial data.  Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.</abstract_lt>
      <abstract_ms>Untuk mencipta model yang kuat melalui julat luas input ujian, set data latihan sepatutnya mengandungi contoh-contoh berbeza yang meliputi banyak fenomena. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets.  Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data.  We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena.  We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches.  Model yang dilatih pada contoh DADC membuat 26\% kurang ralat pada set ujian yang dikurasikan oleh ahli kita dibandingkan dengan model yang dilatih pada data bukan musuh. Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.</abstract_ms>
      <abstract_ml>വിശാലമായ പരീക്ഷണ ഇന്‍പുട്ടുകളിലൂടെ കൊണ്ടുപോകുന്ന മോഡലുകള്‍ ഉണ്ടാക്കുവാന്‍ വേണ്ടി, പരിശീലനത്തിന്റെ ഡാറ്റാസറ്റു ഡൈനാമിക്ക് വിരോധമായ വിവരങ്ങള്‍ സംഘടിപ്പിക്കുന്നത് (DADC), അവിടെ വിവിധ വിവരങ്ങളുടെ പ്രവര്‍ത്തനങ്ങള്‍ എപ്പോഴും മോഡലുകള്‍ മുന്‍കൂട്ടു 1-3 റൌണ്ടില്‍ നിന്നും കൂടുതല്‍ ഡിഡിസിയെ പ്രവര്‍ത്തിപ്പിക്കുന്നത് കാണിച്ചിരിക്കുന്നുവെങ്കില്‍ ചില തെറ്റുകളുടെ തരത്തില്‍ മോഡല നമ്മള്‍ വാദിക്കുന്നത് ഡിഡിസിയെ പല റൌണ്ടുകള്‍ക്കും മേല്‍ ഓടിക്കൊണ്ടിരിക്കുന്നത് അതിന്‍റെ ട്രെയിനിങ്ങ് സമയം ഉപകാരം ഏറ്റവും  നീണ്ട കാലം ഡിഡിസിയുടെ ആദ്യത്തെ പഠനം ഞങ്ങള്‍ കൂട്ടിക്കൊണ്ടിരിക്കുന്നു. അവിടെ നമ്മള്‍ 20 റൗണ്ട് NLI ഉദാഹരണങ്ങള്‍ സംഘടിക്കുന്നു. ഒരു ചെ DADC ഉദാഹരണങ്ങളില്‍ പരിശീലിക്കപ്പെട്ട മോഡലുകള്‍ 26\% കുറച്ച് പിശകുകളാക്കുന്നു. നമ്മുടെ പരീക്ഷണത്തില്‍ പരീക്ഷണത്തിന്റെ പരിശോ നമ്മുടെ അന്വേഷണം കാണിക്കുന്നത് ഡിഡിസി കൂടുതല്‍ കഠിനമായ ഉദാഹരണങ്ങള്‍ ഉണ്ടാക്കുന്നു, കൂടുതല്‍ ലെക്സിക്സിക്കല്‍ വ്യത്യസ്ത വ്യത്യസ്തമാ</abstract_ml>
      <abstract_mt>Biex jinħolqu mudelli b’saħħithom f’firxa wiesgħa ta’ inputs tat-testijiet, settijiet ta’ dejta tat-taħriġ għandhom jinkludu eżempji varji li jkopru bosta fenomeni. Il-ġbir dinamiku tad-dejta avversarja (DADC), fejn l-annotaturi jfasslu eżempji li jikkontestaw it-titjib kontinwu tal-mudelli, għandu wegħda bħala approċċ għall-ġenerazzjoni ta’ settijiet ta’ taħriġ differenti bħal dawn. Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data.  We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena.  Aħna nippreżentaw l-ewwel studju tad-DADC fit-tul, fejn niġbru 20 ċiklu ta’ eżempji tal-NLI għal sett żgħir ta’ paragrafi premessa, b’approċċi kemm avversarji kif ukoll mhux avversarji. Il-mudelli mħarrġa fuq eżempji tad-DADC jagħmlu 26\% inqas żbalji fis-sett tat-test ikkurat mill-esperti tagħna meta mqabbel ma’ mudelli mħarrġa fuq dejta mhux avversarja. L-analiżi tagħna turi li d-DADC jagħti eżempji li huma aktar diffiċli, aktar lexikament u sintetikament differenti, u li fihom inqas artifatti ta’ annotazzjoni meta mqabbla ma’ eżempji mhux avversarji.</abstract_mt>
      <abstract_no>For å laga modeller som er sterkt i eit brett rekkje av testinndata, må opplæringsdatasett inkludere ulike eksemplar som gjer mange fenomenar. Dynamisk datasamling (DADC), der annotatorar arbeider eksemplar som utfordrer kontinuerleg forbedring av modeller, har lov som ein tilnærming for å laga slike ulike opplæringssett. Førre arbeid har vist at køyring av DADC over 1- 3 rundar kan hjelpa til modeller å retta nokre feiltypar, men det fører ikkje nødvendig til bedre generalisering enn negativ test data. Vi argumenterer at køyring av DADC over mange rundar maksimerer uttrykkingstidsfordelene sine, sidan dei ulike rundane kan dekke saman mange av oppgåvelege fenomena. Vi presenterer den første studien av langsiktige DADC, der vi samler 20 runda av NLI-eksemplar for ein liten set av premise avsnitt, med både negativ og ikkje-negativ tilnærmingar. Modellar trengte på DADC-eksemplar gjer at 26\% feil på vårt ekspertkurert test sett i sammenligning med modeller trengte på ikkje-adversariske data. Analysen vårt viser at DADC gjev eksemplar som er vanskeleg, mer leksisk og syntaksisk forskjellige, og inneheld mindre artifaktar for annotasjonar sammenlignet med ikkje-adversariske eksemplar.</abstract_no>
      <abstract_mn>Мөн олон шалгалтын өгөгдлийн хэмжээнд хүчтэй моделуудыг бий болгохын тулд сургалтын өгөгдлийн сангууд олон үзэгдлийг тооцоолж байгаа олон жишээг нэмэгдүүлэх хэрэгтэй. Динамикийн эсрэг өгөгдлийн цуглуулалт (DADC) гэдэг нь загваруудыг сайжруулахын тулд зорилгодог загваруудын жишээ бий болгож, ийм олон төрлийн сургалтын сургалтыг бүтээх арга хэмжээтэй байдаг. Өмнөх ажил нь ДАДК-г 1-3 дахин гүйцэтгэх нь загваруудын зарим алдаа төрлүүдийг засах боломжтой гэдгийг харуулсан байна. Гэхдээ энэ нь эсрэг шалгалтын мэдээллээс илүү сайн ерөнхийлөгчилгээ хүргэхгүй. Бид ДАДК-г олон хэсэгт дасгал хөдөлгөөн нь дасгал хөдөлгөөн цаг хугацааны хэрэгцээг нэмэгдүүлдэг гэдгийг хэлж байна. Яагаад гэвэл өөр хэсэг нь ажлын холбоотой олон үйл явдалыг хамтдаа Бид урт хугацааны ДАДК-ын анхны судалгааг үзүүлнэ. Бид НЛИ-ийн жижиг хэсэг хэсэг хэсэг дээр 20 давхар жишээг цуглуулдаг. DADC жишээ дээр сургалтын загварууд бидний мэргэжилтнүүд дээр сургалтын шалгалтын хэмжээнд 26\% бага алдаа гаргадаг. Бидний шинжилгээнд ДАДК илүү хэцүү, илүү лексикийн, синтактикийн төрлийн жишээг гаргаж, эсрэг биш жишээтэй харьцуулахад бага анзааралтын урлагийн жишээг бий болгож байна.</abstract_mn>
      <abstract_pl>Aby stworzyć modele solidne w szerokim zakresie wejść do testów, zestawy danych szkoleniowych powinny zawierać różne przykłady obejmujące liczne zjawiska. Dynamiczne gromadzenie danych przeciwnych (DADC), gdzie adnotatorzy tworzą przykłady, które stawiają czoła ciągłemu ulepszaniu modeli, ma obiecujące podejście do generowania tak zróżnicowanych zestawów szkoleniowych. Wcześniejsze prace wykazały, że uruchomienie DADC w rundach 1-3 może pomóc modelom naprawić niektóre typy błędów, ale niekoniecznie prowadzi do lepszego uogólnienia poza przeciwnymi danymi testowymi. Twierdzimy, że prowadzenie DADC przez wiele rund maksymalizuje korzyści z czasu treningu, ponieważ różne rundy mogą razem obejmować wiele zjawisk związanych z zadaniem. Przedstawiamy pierwsze badanie długoterminowego DADC, w którym zbieramy 20-rundy przykładów NLI dla małego zestawu akapitów założeniowych, z podejściem zarówno przeciwnym, jak i nieprzeciwnym. Modele przeszkolone na przykładach DADC powodują 26\% mniejszą liczbę błędów w naszym zestawie testów kuracjonowanych przez ekspertów w porównaniu z modelami przeszkolonymi na danych nieprzeciwnych. Nasza analiza pokazuje, że DADC daje przykłady, które są trudniejsze, bardziej zróżnicowane leksycznie i składniowo oraz zawierają mniej artefaktów adnotacyjnych w porównaniu z przykładami nieprzeciwnymi.</abstract_pl>
      <abstract_ro>Pentru a crea modele robuste într-o gamă largă de intrări de testare, seturile de date de formare ar trebui să includă exemple diverse care cuprind numeroase fenomene. Colectarea dinamică a datelor adversariale (DADC), în care adnotatorii creează exemple care provoacă îmbunătățirea continuă a modelelor, ține promisiunea ca o abordare pentru generarea unor seturi de formare atât de diverse. Lucrările anterioare au arătat că rularea DADC peste 1-3 runde poate ajuta modelele să remedieze anumite tipuri de erori, dar nu duce neapărat la o mai bună generalizare dincolo de datele adversare ale testelor. Susținem că rularea DADC pe mai multe runde maximizează beneficiile sale de antrenament-timp, deoarece diferitele runde pot acoperi împreună multe dintre fenomenele relevante pentru sarcini. Prezentăm primul studiu al DADC pe termen lung, unde colectăm 20 de runde de exemple NLI pentru un set mic de paragrafe premise, atât cu abordări adversare, cât și non-adversare. Modelele instruite pe exemple DADC fac cu 26\% mai puține erori în setul nostru de testare curatat de experți comparativ cu modelele instruite pe date non-adversare. Analiza noastră arată că DADC oferă exemple care sunt mai dificile, mai diverse din punct de vedere lexical și sintactic și conțin mai puține artefacte de adnotare comparativ cu exemplele non-adversare.</abstract_ro>
      <abstract_si>පරීක්ෂණ ප්‍රවේශයක් විශාල විශාල ප්‍රවේශයක් නිර්මාණය කරන්න, පරීක්ෂණ දත්ත සේට් වලින් විවිධ උදාහරණ උදාහ සාමාන්‍ය විරුද්ධ විරුද්ධ දත්ත සංග්රහනය (DADC), කොහේද අනතුරු ක්‍රියාත්මක විදිහට ප්‍රශ්නයක් තියෙන්නේ මොඩේල්ස් වලට ස මුලින් වැඩේ පෙන්වන්නේ DADC 1- 3 කුණු වලින් වැඩ කරන්න පුළුවන් විදිහට වැරදි වර්ගයක් නිර්මාණය කරන්න, ඒත් ඒක විරෝධ පරීක්ෂණ අපි ප්‍රශ්නයක් කරනවා DADC කිරීම ගොඩක් ප්‍රශ්නයක් වෙනුවෙන් එයාගේ ප්‍රශ්නයක් වෙලාවට ප්‍රයෝජනයක් වැඩි වෙනුවෙන් ප්‍ර අපි ලොකු වාර්තාවේ DADC ගේ පළමු අධ්‍යානය පෙන්වන්නම්, එතන අපි NLI උදාහරණ 20 ක් සම්බන්ධ කරනවා ප්‍රධාන ප්‍රධාන ප්‍රධාන ප්‍රධානයක් සම DADC උදාහරණ වලින් ප්‍රධානය කරලා තියෙන මොඩල් 26\% අඩු වැරැද්දක් අපේ විශ්වාසිත විශ්වාසිය පරීක්ෂණය සඳහා ප්‍රධානය අපේ විශ්ලේෂණය පෙන්වන්නේ DADC විශ්ලේෂණය තමයි වඩා අමාරුයි, වඩා ලෙක්සිකාරික සහ සංවිධානයෙන් වෙනස් විදියට වඩා විදියට ප</abstract_si>
      <abstract_sr>Da bi stvorili modele koji su snažni u širokom nizu testnih ulaganja, obuka podataka treba uključiti različite primjere koji šire brojne fenomene. Dinamička kolekcija negativnih podataka (DADC), gdje annotatori navode primjere koji izazivaju stalno poboljšavanje modela, obećava kao pristup stvaranju takvih različitih seta obuke. Prije posla pokazala je da vodeći DADC preko 1-3 runde može pomoći modelima da popravi neke vrste greške, ali ne mora da dovede do boljih generalizacija izvan podataka protivnih testova. Svađamo se da vodeći DADC tokom mnogih runda maksimalizira svoju korist za trening, jer različite runde mogu zajedno pokriti mnoge od fenomena vezanih za zadatak. Predstavljamo prvo proučavanje dugoročnog DADC-a, gdje skupljamo 20 rundi primjera NLI-a za mali set premijskih paragrafa, sa objektivnim i neprijateljskim pristupima. Modeli obučeni na primjerima DADC čine 26\% manje grešaka na našem testu u usporedbi s modelima obučenim na neprijateljskim podacima. Naša analiza pokazuje da DADC daje primjere koje su teže, leksičkije i sintaktički različite, i sadrže manje artifakta annotacije u usporedbi s neadversarnim primjerima.</abstract_sr>
      <abstract_so>Si aad u sameyso tusaalooyin lagu isticmaalo samooyin imtixaan badan oo kala duduwan, waxaa laga yaabaa in lagu sameeyo tusaalooyin kala duduwan oo ku dhaqdhaqaaqaya waxyaabo badan. Duumarinta macluumaadka cadaawayaasha ah (DADC), meesha ay tusaalayaasha qalabka dhibaatooyinka ay ku qasbi karaan horumarinta modellada oo joogtada ah, waxay ballanqaadaan qaab u sameynta koorasyada waxbarashada kala duduwan. Shaqo hore wuxuu tusay in dhaqdhaqaaqa DADC oo ka badan 1-3 wadooyin waxay caawin karaan tusaalayaasha inay hagaajiyaan noocyo khalad ah, laakiin laguma baahna in lagu sameeyo wax ka wanaagsan sameynta danbiyada cadaawayaasha ah. Waxaynu ka sheekaynaynaa in dhaqdhaqaaqa DADC uu ugu badiyaa manfacyada waqtiga waxbarashada, sababtoo ah wadooyin kala duduwan waxay wada dabooli karaan waxyaabo badan oo la xiriira shaqada. Waxbarashada ugu horeeyay ee DADC ee waqtiga dheer, halkaas oo aynu soo ururinaa 20 wareegg oo NLI ah, tusaalayaal ka mid ah qeybo yar oo ka mid ah kooxaha hore iyo labada qaabab ka gees ah iyo ka gees ah. Tusaalada DADC lagu baranayo waxay ka dhigaan 26\% khalad yar oo ku saabsan imtixaanka aqoonteenna la koobay, barbarta modelalka lagu tababaray macluumaadka aan cadaawayaasha ka dhigin. Analyskayagu wuxuu muujiyaa in DADC soo saaraa tusaalooyin ay ka adag yihiin, oo ay ka badan yihiin dhibaatooyin, si qalloocan ah oo ay u kala duwan yihiin, waxayna ku jiraan arrimaha wax yar oo la xiriira tusaalayaal aan ka gees ahayn.</abstract_so>
      <abstract_ta>பலவிதமான சோதனை உள்ளீடுகளுக்கு மேல் இயக்கப்பட்ட மாதிரிகளை உருவாக்குவதற்கு, பயிற்சி தகவல் அமைப்பு Dynamic எதிர்பார்வை தகவல் தொகுப்பு (DADC), அங்கு அறிவிப்பாளர்கள் செயல் உதாரணங்கள் தொடர்ந்து மாதிரிகளை மேம்படுத்தும் முறைமைகளை சவால் ச முன்னிருப்பு வேலை காண்பிக்கப்பட்டுள்ளது DADC க்கு மேல் 1- 3 சுற்றுகள் இயங்குகிறது சில பிழை வகைகளை சரிசெய்ய உதவ முடியும், ஆனால் அது எதிர We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena.  நாம் நீண்ட நீண்ட காலத்தின் முதல் படிப்பை காண்பிக்கிறோம், அங்கு நாம் NLI சுற்றிய 20 சுற்று உதாரணங்களை சேகரிக்கிறோம், ஒரு சிறிய முன் DADC உதாரணம் எங்கள் ஆராய்ச்சி DADC மிகவும் கடினமான உதாரணங்களை கொடுக்கும் என்று காண்பிக்கிறது அது மிகவும் மிகவும் கடினமாக இருக்கும், அதிகமாக மிகவும் ம</abstract_ta>
      <abstract_ur>نمڈلوں بنانے کے لئے جو آزمائش میں بہت زیادہ حصہ پر مضبوط ہیں، تدریس ڈیٹ سٹ میں مختلف مثالیں شامل کرنا چاہیے جو بہت سی مثالیں پھیلاتے ہیں۔ داینامیکی مخالف ڈیٹ اکلکسر (DADC) جہاں انڈیٹور مثالیں اڑاتے ہیں جو مدل کو ہمیشہ بہتر کرنے کے لئے چال دیتے ہیں، اس طرح طریقہ کی تدریس سٹ پیدا کرنے کے لئے وعدہ کا ذریعہ رکھتا ہے. پہلے کام نے دکھایا ہے کہ 1-3 راندوں سے DADC چلنے کی مدلکوں کی تعمیر کی مدد کر سکتی ہے، لیکن یہ ضرورت نہیں ہے کہ مخالف ٹیسٹ ڈیٹوں سے زیادہ بہتر عمومی آزمائش کی وجہ سے۔ ہم argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena. ہم پہلی مطالعہ دیر ڈاکسی کے لئے پہلی مطالعہ پیش کرتے ہیں، جہاں ہم NLI کے 20 روندے مثالیں جمع کرتے ہیں ایک چھوٹی مطالعہ کے لئے، دونوں مخالف اور غیر مخالف طریقے کے ساتھ. DADC مثالوں پر آموزش کی مدل 26\% کم خطا کرتی ہیں ہمارے متخصص کریٹ ٹیسٹ پر جو غیر مخالف ڈیٹ پر آموزش کی مدل کے مقابلے میں ہے۔ ہماری تحلیل دکھاتی ہے کہ DADC کی مثال بیان کرتا ہے جو زیادہ مشکل ہیں، زیادہ زبان سے اور سینٹیکٹی سے مختلف ہیں، اور کم مثالیں لکھاتی ہیں جو غیر مخالف مثالوں کے مقابلے میں کم مثالیں ہیں.</abstract_ur>
      <abstract_sv>F철r att skapa modeller som 채r robusta 철ver ett brett spektrum av testing책ngar b철r utbildningsdataupps채ttningar inneh책lla olika exempel som sp채nner 철ver m책nga fenomen. Dynamisk kontradiktorisk datainsamling (DADC), d채r kommentatorer skapar exempel som utmanar st채ndigt f철rb채ttrade modeller, h책ller l철fte som ett tillv채gag책ngss채tt f철r att generera s책 olika tr채ningsupps채ttningar. Tidigare arbete har visat att k철rning av DADC 철ver 1-3 omg책ngar kan hj채lpa modeller att 책tg채rda vissa feltyper, men det leder inte n철dv채ndigtvis till b채ttre generalisering bortom motstridiga testdata. Vi menar att att k철ra DADC 철ver m책nga omg책ngar maximerar dess tr채ningstidsf철rdelar, eftersom de olika omg책ngarna tillsammans kan t채cka m책nga av de uppgiftsrelevanta fenomenen. Vi presenterar den f철rsta studien av l책ngsiktig DADC, d채r vi samlar in 20 omg책ngar av NLI exempel f철r en liten upps채ttning premisstycken, med b책de motstridiga och icke-motstridiga tillv채gag책ngss채tt. Modeller utbildade p책 DADC-exempel g철r 26\% f채rre fel p책 v책r expertkuraterade testupps채ttning j채mf철rt med modeller utbildade p책 icke-kontradiktoriska data. V책r analys visar att DADC ger exempel som 채r sv책rare, mer lexiskt och syntaktiskt olika, och inneh책ller f채rre anteckningar artefakter j채mf철rt med icke-fientliga exempel.</abstract_sv>
      <abstract_mk>За да се создадат модели кои се силни низ широк опсег на тестирани внесувања, обуките на податоци треба да вклучат различни примери кои ги опфаќаат бројните феномени. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets.  Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data.  We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena.  We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches.  Models trained on DADC examples make 26\% fewer errors on our expert-curated test set compared to models trained on non-adversarial data.  Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.</abstract_mk>
      <abstract_vi>Để tạo ra những mô hình vững chắc trên một loạt các nguồn nhập thử nghiệm, các bộ dữ liệu huấn luyện phải gồm những ví dụ khác nhau bao gồm cả nhiều hiện tượng. Sự thu thập dữ liệu nhân tạo (DADC), nơi các nhà biên niên chuyên làm gương thách thức việc cải thiện các mô hình liên tục, giữ hứa hẹn như một phương pháp để tạo ra các tập đoàn đào tạo khác nhau. Việc trước đã cho thấy chạy DADC trên vòng 1-3 có thể giúp người mẫu sửa một số loại lỗi, nhưng nó không nhất thiết dẫn đến việc tổng hợp hơn dữ liệu thí nghiệm ngược nhau. Chúng tôi cho rằng chạy đua DADC trên nhiều hiệp hội tối đa hóa lợi ích thời gian huấn luyện của nó, vì các hiệp hội có thể cùng nhau bao gồm nhiều hiện tượng liên quan đến nhiệm vụ. Chúng tôi giới thiệu nghiên cứu đầu tiên về DADC lâu dài, nơi chúng tôi thu thập hàng chục hiệp về NLl ví dụ cho một số trường hợp nhỏ, với cả hai phương pháp trái ngược lẫn không đối đầu. Những mẫu được huấn luyện về ví dụ DADC khiến 26\\\\\\\\\\\\\\\\\\càngít lỗi trong bộ thử nghiệm chuyên gia so với các mẫu được huấn luyện. Phân tích của chúng tôi cho thấy rằng DADC cung cấp những ví dụ khó khăn hơn, từ vựng hơn và theo cấu trúc khác nhau hơn, và chứa ít đồ ghi chú hơn so với ví dụ không phải đối thủ.</abstract_vi>
      <abstract_uz>Name Name Birinchi vazifa 1- 3 marta DADC dasturini ishga tushirish mumkin, bir necha xato turlarini oʻrnatish mumkin, lekin u to ʻgʻri tizim maʼlumotdan bajarishi kerak emas. Biz murakkab qilamiz, ko'pchilik davlatda DADC ishga tushirishni taʼminlovchi vaqt imkoniyatini oshirish mumkin, chunki har xil guruhlar vazifaning ko'pchiligini birlashtirish mumkin. Biz uzoq DDC'ning birinchi o'qituvchisini hozir qilamiz. Bu yerda biz biz bir kichkina prezident paragraphlar uchun NLI kabi 20 rund misollarini birinchi o'qituvchimiz. Name Analytikizni ko'rsatadi, DADC juda qiyin masallarni chiqaradi, ko'proq leksikak va syntiktikk tarkibi, va bizga qisqa taʼminlovchi narsalarga ega bo'ladi.</abstract_uz>
      <abstract_nl>Om modellen te maken die robuust zijn voor een breed scala aan testinputs, moeten trainingsdatasets diverse voorbeelden bevatten die talrijke fenomenen bestrijken. Dynamic adversarian data collection (DADC), waar annotators voorbeelden maken die voortdurend verbeteren van modellen uitdagen, is veelbelovend als een benadering voor het genereren van dergelijke uiteenlopende trainingssets. Eerder onderzoek heeft aangetoond dat het uitvoeren van DADC over 1-3 rondes modellen kan helpen bepaalde fouttypen op te lossen, maar het hoeft niet noodzakelijkerwijs te leiden tot een betere generalisatie dan tegenstrijdige testgegevens. We argumenteren dat het uitvoeren van DADC over vele rondes de voordelen van de trainingstijd maximaliseert, omdat de verschillende rondes samen veel van de taak-relevante fenomenen kunnen behandelen. We presenteren de eerste studie van DADC op langere termijn, waarin we 20-ronden NLI voorbeelden verzamelen voor een kleine set premise paragrafen, met zowel tegenstrijdige als niet-tegenstrijdige benaderingen. Modellen die zijn getraind op DADC voorbeelden maken 26\% minder fouten in onze door experts geselecteerde testset in vergelijking met modellen die zijn getraind op niet-tegenstrijdige gegevens. Uit onze analyse blijkt dat DADC voorbeelden levert die moeilijker, lexicaal en syntactisch diverser zijn en minder annotatieartefacten bevatten in vergelijking met niet-tegenstrijdige voorbeelden.</abstract_nl>
      <abstract_bg>За да се създадат модели, които са здрави в широк спектър от входящи тестове, наборите от данни за обучение трябва да включват разнообразни примери, които обхващат множество явления. Динамично събиране на противоречиви данни (ДАДК), където анотаторите изработват примери, които предизвикват непрекъснато подобряващите се модели, е обещаващо като подход за генериране на такива разнообразни набори от обучение. Предишна работа е показала, че стартирането на ДАД в рамките на 1-3 кръга може да помогне на моделите да коригират някои типове грешки, но не е задължително да доведе до по-добро обобщаване извън данните от теста. Ние твърдим, че управлението на ДАДК в много кръгове увеличава ползите от времето на обучение, тъй като различните кръгове могат заедно да обхванат много от свързаните със задачата явления. Представяме първото проучване на по-дългосрочния ДАДК, където събираме 20 кръга примери от НЛИ за малък набор от параграфи от предпоставки, както с противоречиви, така и с непротиворечиви подходи. Моделите, обучени по примери правят 26\% по-малко грешки в нашия експертен набор от тестове в сравнение с моделите, обучени по неконкурентни данни. Анализът ни показва, че дава примери, които са по-трудни, по-лексично и синтактично разнообразни и съдържат по-малко анотационни артефакти в сравнение с неконкурентните примери.</abstract_bg>
      <abstract_da>For at skabe modeller, der er robuste på tværs af en lang række testinput, bør træningsdatasæt indeholde forskellige eksempler, der spænder over talrige fænomener. Dynamisk adversarial data collection (DADC), hvor kommentatorer laver eksempler, der udfordrer løbende forbedrede modeller, er lovende som en tilgang til at generere så forskelligartede træningssæt. Tidligere arbejde har vist, at kørsel af DADC over 1-3 runder kan hjælpe modeller med at rette nogle fejltyper, men det fører ikke nødvendigvis til bedre generalisering ud over modstridende testdata. Vi hævder, at kørsel af DADC over mange runder maksimerer dets træningstidsfordele, da de forskellige runder sammen kan dække mange af de opgaverelevante fænomener. Vi præsenterer den første undersøgelse af længerevarende DADC, hvor vi samler 20 runder af NLI eksempler til et lille sæt præmisser afsnit, med både modstridende og ikke-modstridende tilgange. Modeller, der er trænet i DADC eksempler, gør 26% færre fejl på vores ekspertgrupperede testsæt sammenlignet med modeller, der er trænet på ikke-modstandsdata. Vores analyse viser, at DADC giver eksempler, der er mere vanskelige, mere leksikologisk og syntaktisk forskelligartede og indeholder færre annotationsartefakter sammenlignet med ikke-modstridende eksempler.</abstract_da>
      <abstract_hr>Da bi stvorili modele koji su snažni u širokom nizu testnih ulaganja, obuka podataka trebala bi uključiti različite primjere koje šire brojne fenomene. Dinamička kolekcija negativnih podataka (DADC), gdje annotatori navode primjere koji izazivaju stalno poboljšavanje modela, obećava se kao pristup stvaranju takvih različitih obuka. Prije posla pokazala je da vodeći DADC preko 1-3 runde može pomoći modelima da popravi neke vrste greške, ali ne mora dovesti do boljih generalizacija izvan podataka o negativnim testiranjima. Tvrdimo da trčanje DADC u mnogim rundima maksimalizira svoja korist za trening, jer različite runde zajedno mogu pokriti mnoge od fenomena relevantnih zadataka. Predstavljamo prvo ispitivanje dugoročnog DADC-a, gdje skupljamo 20 krugova primjera NLI-a za mali skup premijskih paragrafa, uz neprijateljske i neprijateljske pristupe. Modeli obučeni na primjerima DADC-a čine 26\% manje grešaka na našim testovima koji su zaključeni stručnjacima u usporedbi s modelima obučenim na neprijateljskim podacima. Naša analiza pokazuje da DADC daje primjere koje su teže, leksičkije i sintaktički različite, i sadrže manje artifakta annotacije u usporedbi s neadversarnim primjerima.</abstract_hr>
      <abstract_id>Untuk menciptakan model yang kuat melalui jangkauan luas masukan tes, set data pelatihan harus mengandung contoh-contoh berbeda yang meliputi banyak fenomena. Dinamik koleksi data musuh (DADC), di mana annotator membuat contoh yang menantang terus-menerus meningkatkan model, memegang janji sebagai pendekatan untuk menghasilkan set latihan yang berbeda seperti ini. Pekerjaan sebelumnya menunjukkan bahwa menjalankan DADC selama 1-3 putaran dapat membantu model memperbaiki beberapa tipe kesalahan, tetapi tidak perlu memimpin ke generalisasi yang lebih baik diluar data ujian musuh. Kami berdebat bahwa menjalankan DADC selama banyak pusingan maksimalkan keuntungan latihan-waktu, karena pusingan yang berbeda bisa bersama-sama menutupi banyak fenomena yang relevan tugas. Kami mempersembahkan penelitian pertama dari DADC jangka panjang, di mana kami mengumpulkan 20 ronde contoh NLI untuk set kecil paragraf premise, dengan kedua pendekatan musuh dan bukan musuh. Model yang dilatih pada contoh DADC membuat 26\% kurang kesalahan pada set tes kami yang dikurasikan oleh ahli dibandingkan dengan model yang dilatih pada data bukan musuh. Analisis kami menunjukkan bahwa DADC memberikan contoh yang lebih sulit, lebih leksikal dan sintaksi berbeda, dan mengandung lebih sedikit artefak anotasi dibandingkan contoh-contoh yang tidak bertentangan.</abstract_id>
      <abstract_de>Um Modelle zu erstellen, die über ein breites Spektrum von Testeingaben hinweg robust sind, sollten Trainingsdatensätze vielfältige Beispiele enthalten, die zahlreiche Phänomene umfassen. Dynamische adversariale Datenerfassung (DADC), bei der Annotatoren Beispiele anfertigen, die kontinuierlich verbesserte Modelle herausfordern, ist ein vielversprechender Ansatz, um solch vielfältige Trainingssets zu generieren. Frühere Arbeiten haben gezeigt, dass das Ausführen von DADC über 1-3-Runden Modellen helfen kann, einige Fehlertypen zu beheben, aber es führt nicht notwendigerweise zu einer besseren Verallgemeinerung jenseits von kontroversen Testdaten. Wir argumentieren, dass das Führen von DADC über viele Runden die Vorteile der Trainingszeit maximiert, da die verschiedenen Runden zusammen viele der aufgabenrelevanten Phänomene abdecken können. Wir präsentieren die erste Studie des längerfristigen DADC, in der wir 20-Runden von NLI-Beispielen für einen kleinen Satz von Prämisse-Absätzen sammeln, mit sowohl kontradisorischen als auch nicht-kontradisorischen Ansätzen. Modelle, die auf DADC-Beispielen trainiert werden, machen 26\% weniger Fehler in unserem von Experten kuratierten Testset im Vergleich zu Modellen, die auf nicht-gegnerischen Daten trainiert wurden. Unsere Analyse zeigt, dass DADC Beispiele liefert, die schwieriger, lexikalisch und syntaktisch vielfältiger sind und weniger Annotationsartefakte enthalten als nicht-kontroverse Beispiele.</abstract_de>
      <abstract_ko>각종 테스트 입력에서 노봉성을 가진 모델을 만들기 위해 훈련 데이터 집합은 다양한 현상을 뛰어넘는 각종 예시를 포함해야 한다.동적 대항적 데이터 수집(DADC)은 이처럼 다양한 훈련 집합을 만드는 방법으로 주석자가 DADC에서 도전의 끊임없는 개선 모델을 만들 수 있는 예이다.이전의 작업에서 1-3라운드에서 DADC를 운행하면 모델이 일부 오류 유형을 복구하는 데 도움을 줄 수 있다고 밝혔지만 이것은 반드시 대항적인 테스트 데이터를 초월하는 더 좋은 범위화를 초래하는 것은 아니다.우리는 여러 바퀴가 DADC를 운행하면 훈련 시간 효율을 최대한 높일 수 있다고 생각한다. 왜냐하면 서로 다른 바퀴는 임무와 관련된 많은 현상을 함께 커버할 수 있기 때문이다.우리는 첫 번째 장기 DADC 연구를 소개했는데 그 중에서 우리는 20차례의 NLI 예시를 수집하여 대항성과 비대항성 방법을 포함한 일부 전제 단락에 사용했다.DADC 예제 훈련 기반 모델은 비대항적 데이터 트레이닝 기반 모델에 비해 전문가가 기획한 테스트 세트에서 오류가 26% 감소했다.우리의 분석에 따르면 DADC가 생성하는 예는 비대항적인 예에 비해 더욱 어렵고 어휘와 문법적으로 다양하며 주석 부품이 더 적게 포함된 것으로 나타났다.</abstract_ko>
      <abstract_sw>Ili kutengeneza mifano ambayo inavamiwa katika vifaa vingi vya majaribio, seti za mafunzo zinapaswa kuwajumuisha mifano mbalimbali ambazo zinazungumzia matukio mengi. Mkusanyiko wa takwimu za upinzani (DADC), ambapo vifaa vya watangazaji vinavyochanganya changamoto zinazoendelea kuboresha mifano, huahidi kuwa njia ya kutengeneza seti za mafunzo mbalimbali. Kazi ya awali imeonyesha kwamba kuendesha DADC zaidi ya runde 1-3 inaweza kusaidia miundo mbili kurekebisha aina za makosa, lakini haina lazima kuongezeka vizuri zaidi ya taarifa za upinzani. Tunajadili kwamba kuendelea DADC kwa zaidi ya maeneo mengi yanaongezea faida zake za muda wa mafunzo, kwa sababu maeneo mbalimbali yanaweza kuungana na mambo mengi yanayohusiana na kazi. We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches.  Modeli zilizofundishwa katika mifano ya DADC hufanya makosa 26\% wachache zaidi kwenye mtihani wetu wa wataalam uliofanyika ukilinganishwa na mifano iliyoendeshwa kwenye takwimu zisizo na upinzani. Uchambuzi wetu unaonyesha kwamba DADC hutoa mifano ambayo ni ngumu zaidi, yenye tofauti za kisaikolojia na kwa pamoja, na ina vitu vidogo vidogo vingi vinavyofanana na mifano isiyo na upinzani.</abstract_sw>
      <abstract_fa>برای ایجاد مدل‌هایی که در مجموعه‌ی وسیع وارد آزمایش‌ها قدرتمند هستند، مجموعه‌های داده‌های آموزش باید مثال‌های مختلف را شامل کنند که رویداده‌های زیادی را گسترش می‌دهند. جمع داده های دینامیک دشمنی (DADC) جایی که مثالهایی که مدل‌ها را به طور دائمی بهتر کردن چالش می‌دهند، برای تولید چنین مجموعه‌های آموزش مختلف قول می‌دهد. کارهای پیشینی نشان داده است که اجرای DADC بیش از ۱- ۳ راند می‌تواند مدل‌ها کمک کند تا برخی نوع خطا را تعمیر کند، ولی لازم نیست که آن به بهترین ترکیب عمومی بیش از داده‌های آزمایش دشمنی رخ دهد. ما بحث می‌کنیم که اجرای DADC در طول بسیاری از دوره‌ها سود آموزش زمان خود را maximize می‌کند، زیرا دوره‌های مختلف می‌توانند با هم بسیاری از اتفاقات مربوط به کار را حفظ کنند. ما اولین مطالعه‌ای از DADC مدت طولانی را پیشنهاد می‌کنیم، جایی که ما ۲۰ راند مثال NLI را برای یک مجموعه‌ی نقطه‌های کوچک، با همچنین نزدیک‌های دشمنی و غیر دشمنی جمع می‌کنیم. Models trained on DADC examples make 26\% fewer errors on our expert-curated test set compared to models trained on non-adversarial data. تحلیل ما نشان می دهد که DADC مثالهایی که سخت تر، زبان‌شناسی‌تر و متفاوت‌تر هستند، و در مقایسه با مثالهایی که غیر دشمنی هستند، آهنگ‌شناسی کمتر وجود دارد.</abstract_fa>
      <abstract_tr>Di흫e bir g철rn체힊 testi흫 giri힊inde g체첵챌li modeller bejermek 체챌in, bilim sistemalary birn채챌e g철rn체힊 철r채n eserlerde bolmaly. Dynamik te흫kili maglumat koleksi첵asy (DADC), 힊ol 첵erde n채챌e g철rn체힊ler nusgalary 체첵tgetmek 체챌in 철rnekler 챌yk첵ar, nusgalary 철r채n 체첵tgetmelidir. 횜흫ki i힊e g철ren DADC 1-3 depden 첵okarynda bir hili hata t체rlerini bejermek 체챌in k철mek edip biler, 첵철ne bu durum te흫kil testi흫 maglumatyndan has gowy d철redip bilmez. DADC'i birn채챌e g철rn체힊 첵체z체nde duran 챌yky힊 wagtyny흫 체챌in azaltylygyny azalt첵ar di첵ip pikir ed첵채ris, seb채bi farkl캇 g철rn체힊ler i힊i bilen mejbur bolan birn채챌e g철rn체힊i bilen 체첵tgedip bilerler. Biz DADC uzak durmu힊yny흫 ilkinji aralygyny g철rke첵채ris we olary흫 20 sany NLI 철rnekleri ki챌i birn채챌e paragraflar 체챌in 첵ygna첵arys. DADC mysllerinde e휓lenen nusgalar bizi흫 uzmanlarymyz 첵ok hasaplan첵an testimizde 26\% indir 첵al흫y힊lyk 첵ok hasaplan첵ar. Bizi흫 analyzamyz DADC'y흫 kyn, leksi첵aly we sintakti첵aly d체rli 철rneklerini da힊aryl첵ar we te흫kil d철w체rler bilen g철r채 첵akyn du첵gulama sungatlaryny da힊aryl첵ar.</abstract_tr>
      <abstract_af>Om modele te skep wat sterk is deur 'n wyse reek van toets inputs, moet onderwerp datastelle verskeie voorbeelde insluit wat veelvuldige fenomene uitbrei. Dinamiese texantariese data versameling (DADC), waar annotators byvoorbeelde verwerp wat voorspoedings wat voortgaan voortgaan voortgaan wat voortgaan voortgaan verbeter modele, hou belofte as 'n toegang vir die genereer van sodanige verskeie onderwerp stelle. Vorige werk het vertoon dat loop DADC oor 1- 3 ronde kan hulp modele help om sommige fout tipes te reg, maar dit doen nie noodsaaklik lei na beter generalisering buite adversariale toets data. Ons argumenteer dat die hardloop van DADC oor baie ronde maksimeer sy oefening-tyd voordeel, want die verskillende ronde kan saam baie van die taak-relevante fenomene oordeel. Ons stel die eerste studie van langer-term DADC waar ons 20 ronde van NLI voorbeelde versamel vir 'n klein stel van premise paragraaf, met beide teenstandaarlike en nie-teenstandaarlike toegang. Models onderwerp op DADC voorbeelde maak 26\% minder foute op ons ekspertiefasteerde toets stel vergelyk met modele onderwerp op nie- adversariale data. Ons analisie vertoon dat DADC voorbeelde wat moeilik is, meer leksik en sintaktisies verskeie, en bevat minder annotasie kunstenaars in vergelyking met nie-adversariale voorbeelde.</abstract_af>
      <abstract_am>በዙሪያው ፈተና ጥያቄዎች ላይ የሚነጥቁትን ምሳሌዎች ለመፍጠር፣ የዳታ ማህበረሰብ ብዙዎችን አካባቢ ምሳሌዎች እንዲያስተካክሉ ይገባዋል፡፡ የዲያኖሚክ ተቃዋሚዎች ዳታ ሰብስብ (DADC), where annotators craft ምሳሌs ሁልጊዜ በመጠቀም ሞዴሎችን የሚያቃልሉ ምሳሌዎች፣ እንደዚህ ልዩ ልዩ ተማሪ መስመር ማቀናጃ ለመፍጠር ለመስጠት ተስፋ ያደርጋል፡፡ የቀድሞው ስራ ከ1-3 ክፍል በላይ DADC ሊሮጥ የስህተት ዓይነቶችን ሊረዳ የሚችል እንደሆነ አስታውቆታል፤ ነገር ግን በተቃዋሚ ፈተና ዳታዎችን በማሻል አያስፈልግም። ብዙ አካባቢዎች ላይ DADC የሚሮጥ የግንኙነቱን የጊዜው ጥቅሞችን በማድረግ እናዋጋለን፡፡ የረጅም ቀን ዲዲ ሲ የመጀመሪያውን ትምህርት እናቀርባታለን፣ በዚያው 20 የNLI ውጤቶች ምሳሌዎችን ለመቆጣጠር እናስቀድዳለን፡፡ በDADC ምሳሌዎች ላይ የተጠቃሚ ሞዴል 26\1 በመቶ ስህተት በሚያነሳው በተቃዋሚ ዳታዎች ላይ በተማመሩ ሞዴላዎች ይደረጋሉ፡፡ ትምህርታችን DADC ከጭንቅ፣ ሌክሲካዊ እና በተለየ ልዩ ልዩ ምሳሌዎችን ያሳየዋል፡፡</abstract_am>
      <abstract_bn>To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena.  যেখানে বিভিন্ন প্রশিক্ষণ নির্মাণের জন্য বিভিন্ন প্রশিক্ষণ সেট তৈরি করার প্রতিশ্রুতি প্রদান করা হয়েছে। প্রাথমিক কাজ প্রদর্শন করা হয়েছে যে ১-৩ রাউন্ড চালানো DADC কিছু ত্রুটি ধরনের মোডেল ঠিক করতে সাহায্য করতে পারে, কিন্তু এটি বিরোধী পরীক্ষা ত আমরা যুক্তি দিচ্ছি যে অনেক রাউন্ডে ডিএডিসি চালানো হচ্ছে তার প্রশিক্ষণ-সময়ের সুবিধা বৃদ্ধি করে, কারণ বিভিন্ন রাউন্ডের বিভিন্ন আমরা দীর্ঘমেয়াদী ডিডিসির প্রথম গবেষণা উপস্থাপন করেছি, যেখানে আমরা ২০ রাউন্ড এনলির উদাহরণ সংগ্রহ করেছি একটি ছোট প্রাথমিক প্যাপারেসের জন্য, যা ডিডিসি উদাহরণে প্রশিক্ষিত মডেল ২৬\শতাংশ ভুল করে আমাদের বিশেষজ্ঞ-কার্ডের পরীক্ষার সেটের তুলনায় আমাদের বিরোধী তথ্যে প্ আমাদের বিশ্লেষণ দেখাচ্ছে যে ডিএডিসি এর উদাহরণ দেখিয়েছে যা আরো কঠিন, আরো লেক্সিক্সিক এবং সিন্টাক্সিক ভাবে বিভিন্ন ভিন্ন, আর তার মধ্য</abstract_bn>
      <abstract_hy>Որպեսզի ստեղծենք մոդելներ, որոնք ուժեղ են թեստերի տարբեր ներմուծների ընթացքում, տեղեկատվական համակարգերը պետք է ներառեն բազմաթիվ օրինակներ, որոնք ընդգրկում են բազմաթիվ երևույթներ: Դինամական հակառակորդական տվյալների հավաքածուն (DADԿ), որտեղ annoտորները օրինակներ են ստեղծում, որոնք անընդհատ բարելավում են մոդելները, խոստանում է որպես այդպիսի բազմազան ուսուցման համակարգեր ստեղծելու մոտեցում: Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data.  Մենք փաստարկում ենք, որ DADԿ-ի աշխատանքը շատ շրջանների ընթացքում մեծացնում է իր ուսուցման-ժամանակի առավելությունները, քանի որ տարբեր շրջանները միասին կարող են ծածկել խնդիրների հետ կապված երևույթները: Մենք ներկայացնում ենք երկարաժամկետ DADԿ-ի առաջին ուսումնասիրությունը, որտեղ մենք հավաքում ենք քսան շրջանակներ ՆԼԻ օրինակների մի փոքրիկ պարբերակների համար, ինչպես հակառակորդ և ոչ հակառակորդ մոտեցումների միջոցով: DADC օրինակների վրա սովորեցված մոդելները մեր փորձարկումների համակարգում ավելի քիչ սխալներ են անում, քան ոչ հակառակորդական տվյալների վրա սովորեցված մոդելները: Մեր վերլուծությունը ցույց է տալիս, որ DADԿ-ն առաջացնում է օրինակներ, որոնք ավելի դժվար են, լեքսիկական և սինտակտիկապես բազմազան, և պարունակում են ավելի քիչ annoտացիոն արտերֆեկտներ, համեմատած ոչ հակառակորդ օրինակների հետ</abstract_hy>
      <abstract_sq>Për të krijuar modele që janë të forta nëpër një gamë të gjerë të hyrjeve testuese, grupet e të dhënave të trajnimit duhet të përfshijnë shembuj të ndryshëm që përhapin fenomene të shumta. Përmbledhja dinamike e të dhënave kundërshtare (DADC), ku anotatorët krijojnë shembuj që sfidojnë përmirësimin vazhdimisht të modeleve, mban premtimin si një qasje për krijimin e grupeve të tilla të ndryshme të trainimit. Puna e mëparshme ka treguar se vrapimi i DADC mbi 1-3 raunde mund të ndihmojë modelet të rregullojnë disa tipe gabimesh, por nuk shpie domosdoshmërisht në gjeneralizim më të mirë përtej të dhënave të test it kundërshtarë. Ne argumentojmë se drejtimi i DADC në shumë raunde maksimizon përfitimet e tij të trajnimit-kohës, pasi raundet e ndryshme mund të mbulojnë së bashku shumë nga fenomenet e lidhura me detyrat. Ne paraqesim studimin e parë të DADC afat-gjatë, ku mbledhim 20 raunde shembuj NLI për një grup të vogël paragjykimesh, si kundërshtare ashtu edhe jo kundërshtare. Models trained on DADC examples make 26\% fewer errors on our expert-curated test set compared to models trained on non-adversarial data.  Analiza jonë tregon se DADC sjell shembuj që janë më të vështirë, më lexikalisht dhe sintaktikisht të ndryshëm dhe përmbajnë më pak artefakte anotacioni krahasuar me shembuj jo-kundërshtarë.</abstract_sq>
      <abstract_az>Müxtəlif məlumatlar arasında möhkəm modellər yaratmaq üçün, təhsil verilənlər təhsil etmək üçün çoxlu məlumatlar genişlənir. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continuously improving models, holds promise as a approach for generating such different training sets. Əvvəlki işin 1-3 səviyyədə DADC çalışması modellərin bəzi xəta növlərini düzəltməsinə kömək edə biləcəyini göstərdi, amma bu, əleyhinə sınamaq məlumatından daha yaxşı generalizasiya yol verməz. Biz mübahisə edirik ki, DADC çalışması çox çətinliklərdə təhsil vaxtının faydalarını artırar, çünki fərqli çətinliklərdən çoxlarını birləşdirə bilər. Daha uzun müddətli DADC təhsilinin ilk təhsilini göstərdik, ki, buna görə də düşmənçilik və düşmənçilik olmayan NLI məsəllərini 20 runda toplayırıq. DADC nümunələrində təhsil edilən modellər ekspertlərimiz təhsil edilmiş sınamamızda 26\% daha az xəta verir. Bizim analizimiz DADC'nin daha çətin, daha hekayətli və sintaktik olaraq müxtəlif məsəllərini göstərir və düşmənçilik olmayan məsəllərlə qarşılaşdırmaq üçün daha az danışmaq məsəllərini daxil edir.</abstract_az>
      <abstract_bs>Da bi stvorili modele koji su snažni u širokom nizu testnih ulaganja, podaci obuke trebali bi uključiti različite primjere koje šire brojne fenomene. Dinamička kolekcija neprijateljskih podataka (DADC), gdje annotatori navode primjere koji izazivaju stalno poboljšavanje modela, obećava se kao pristup stvaranju takvih različitih seta obuke. Prije posla pokazala je da vodeći DADC preko 1-3 runde može pomoći modelima da popravi neke vrste greške, ali ne mora da dovede do bolje generalizacije izvan podataka o negativnim testiranjima. Tvrdimo da trčanje DADC tokom mnogih runda maksimalizuje svoju korist za trening, jer različite runde zajedno mogu pokriti mnoge od fenomena vezanih za zadatak. Predstavljamo prvo ispitivanje dugoročnog DADC-a, gdje skupljamo 20 rundi primjera NLI-a za mali skup premijskih paragrafa, sa objektivnim i ne-adversarnim pristupima. Modeli obučeni na primjerima DADC-a čine 26\% manje greške na našem testu u usporedbi s modelima obučenim na neprijateljskim podacima. Naša analiza pokazuje da DADC daje primjere koje su teže, leksičkije i sintaktički različite, i sadrže manje artifakta annotacije u usporedbi s neadversarnim primjerima.</abstract_bs>
      <abstract_ca>Per crear models robustos a través d'una amplia gama d'entrades de prova, els conjunts de dades d'entrenament haurien d'incloure diversos exemples que abarcan molts fenomens. La col·lecció dinàmica de dades adversaries (DADC), on els anotators fan exemples que desafian els models de millora continua, té promesa com un enfocament per generar aquests conjunts de formació tan diversos. La feina anterior ha demostrat que executar DADC durant 1-3 rondes pot ajudar els models a arreglar alguns tipus d'errors, però no necessariament porta a una millor generalització més enllà de les dades de prova adversaria. Argumentem que executar el DADC durant moltes rondes maximitza els beneficis del temps d'entrenament, com les diferents rondes poden cobrir molts dels fenomens pertinents a la tasca. Presentam el primer estudi de DADC a llarg termini, on recollim 20 rondes d'exemples de NLI per un petit conjunt de paràgrafs premises, amb abords adversaris i no adversaris. Els models entrenats en exemples DADC fan un 26\% menys errors en el nostre conjunt d'exàmens curat per experts en comparació amb models entrenats en dades no adversaries. La nostra anàlisi mostra que el DADC produeix exemples que són més difícils, més lexicament i sinàcticament diversos, i contenen menys artefactes d'anotació comparats amb exemples no adversaris.</abstract_ca>
      <abstract_cs>Chcete-li vytvořit modely robustní napříč širokou škálou testovacích vstupů, měly by tréninkové datové sady obsahovat různé příklady, které zahrnují řadu jevů. Dynamický adversariální sběr dat (DADC), kde anotátoři vytvářejí příklady, které zpochybňují neustále zlepšující se modely, má slibný přístup k generování tak rozmanitých tréninkových sad. Předchozí práce ukázaly, že spuštění DADC přes 1-3 kola může pomoci modelům opravit některé typy chyb, ale nemusí nutně vést k lepší zobecnění nad rámec kontroverzních testovacích dat. Tvrdíme, že běh DADC v mnoha kolech maximalizuje jeho výhody v době tréninku, protože různá kola mohou společně pokrýt mnoho jevů relevantních pro úkoly. Představujeme první studii dlouhodobějšího DADC, kde shromažďujeme dvacet kol NLI příkladů pro malý soubor premisních odstavců, s nepřátelskými i nepřátelskými přístupy. Modely trénované na příkladech DADC způsobují 26\% méně chyb v naší zkušební sadě vytvořené experty ve srovnání s modely trénovanými na nepřátelských datech. Naše analýza ukazuje, že DADC přináší příklady, které jsou složitější, lexicky a syntakticky různorodější a obsahují méně anotačních artefaktů ve srovnání s nepřátelskými příklady.</abstract_cs>
      <abstract_et>Selleks et luua mudelid, mis on tugevad mitmesuguste katsesisendite puhul, peaksid koolitusandmekogumid sisaldama mitmesuguseid näiteid, mis hõlmavad paljusid nähtusi. Dünaamiline konkurentsiandmete kogumine (DADC), kus annotatorid valmistavad välja näiteid, mis väljakutsevad pidevalt täiustavaid mudeleid, on nii mitmekesiste koolituskomplektide loomisel lubaduslik lähenemisviis. Varasem töö on näidanud, et DADC käivitamine 1–3 vooru jooksul võib aidata mudelitel parandada mõningaid veatüüpe, kuid see ei pruugi tingimata kaasa tuua paremat üldistamist väljaspool vastaseid katseandmeid. Väidame, et DADC-i kasutamine paljude voorude jooksul maksimeerib selle koolitusaja eeliseid, sest erinevad voorud võivad koos katta paljusid ülesandega seotud nähtusi. Esitleme esimese pikaajalise DADC uuringu, kus kogume 20 vooru NLI näiteid väikeste eelduslõigete kogumi jaoks, nii vastandlike kui ka mittevastavate lähenemisviisidega. DADC näidete põhjal koolitatud mudelid teevad meie ekspertkureeritud testikomplektis 26% vähem vigu võrreldes mittevastavate andmete põhjal koolitatud mudelitega. Meie analüüs näitab, et DADC toob näiteid, mis on keerulisemad, leksikaalselt ja süntaktiliselt mitmekesisemad ning sisaldavad vähem annotatsiooniartefakte võrreldes mittevastavate näidetega.</abstract_et>
      <abstract_fi>Jotta voidaan luoda malleja, jotka ovat vankkoja useilla eri testisyötteillä, koulutustietojen tulisi sisältää erilaisia esimerkkejä, jotka kattavat lukuisia ilmiöitä. Dynaaminen kontrastariaalinen tiedonkeruu (DADC), jossa kommentaattorit laativat esimerkkejä, jotka haastavat jatkuvasti kehittyviä malleja, on lupaava tapa luoda näin erilaisia koulutuskokonaisuuksia. Aikaisemmat tutkimukset ovat osoittaneet, että DADC:n ajaminen 1-3 kierroksella voi auttaa malleja korjaamaan joitakin virhetyyppejä, mutta se ei välttämättä johda parempaan yleistymiseen kuin vastakkaiset testitiedot. Väitämme, että DADC:n ajaminen monilla kierroksilla maksimoi harjoitteluajan edut, sillä eri kierroksilla voidaan yhdessä kattaa monia tehtävään liittyviä ilmiöitä. Esitämme ensimmäisen pitkän aikavälin DADC:n tutkimuksen, jossa keräämme 20 kierrosta NLI-esimerkkejä pienille lähtökohtille sekä kontradikaalisilla että ei-kontradikaalisilla lähestymistavoilla. DADC-esimerkkeihin koulutetut mallit tekevät 26\% vähemmän virheitä asiantuntijakuratoidussa testisarjassamme verrattuna malleihin, jotka on koulutettu ei-vastakkaisella datalla. Analyysimme osoittaa, että DADC tuottaa esimerkkejä, jotka ovat vaikeampia, lexikaalisesti ja syntaktisesti monimuotoisempia ja sisältävät vähemmän merkintäartefakteja kuin ei-adversariaaliset esimerkit.</abstract_fi>
      <abstract_sk>Za ustvarjanje modelov, ki so robustni v širokem naboru preskusnih vhodov, bi morali nabori podatkov o usposabljanju vključevati različne primere, ki zajemajo številne pojave. Dinamično kontradikcijsko zbiranje podatkov (DADC), kjer opozorilci oblikujejo primere, ki izzivajo stalno izboljšujoče modele, je obetaven pristop k ustvarjanju tako raznolikih naborov usposabljanja. Predhodno delo je pokazalo, da lahko izvajanje DADC v 1-3 krogih modelom pomaga odpraviti nekatere vrste napak, vendar to ne vodi nujno do boljše posploševanje preko kontrastnih preskusnih podatkov. Trdimo, da vodenje DADC v številnih krogih maksimira njegove koristi v času treninga, saj lahko različni krogi skupaj pokrivajo številne pojave, pomembne za nalogo. Predstavljamo prvo študijo dolgoročnejšega DADC, kjer zbiramo 20 krogov primerov NLI za majhen sklop premičnih odstavkov, tako kontradiktorskih kot tudi ne kontradiktorskih pristopov. Modeli, usposobljeni na podlagi primerov DADC, naredijo 26% manj napak v našem strokovnem naboru preskusov v primerjavi z modeli, usposobljenimi na podlagi podatkov, ki niso konkurenčni. Naša analiza kaže, da DADC prinaša primere, ki so težji, bolj leksikološko in sintaktično raznoliki in vsebujejo manj artefaktov za označevanje v primerjavi z nekontraralnimi primeri.</abstract_sk>
      <abstract_jv>Jejaring Name politenessoffpolite"), and when there is a change ("assertivepoliteness Awak dhéwé sawian karo sistem sing dibutuhke Awak dhéwé éntuk perusahaan urip nggambar luwih-luwih DaD Name Panjenenganipun dhéwé menehi bakal sing nyimpen dadi, dadi sing luwih apik lan kelakipun sing sampeyan karo hal-hal sing gak dhéwé, akeh lanjut sing gak adhil karo hal-hal sing paling dhéwé.</abstract_jv>
      <abstract_he>כדי ליצור דוגמנים חזקים ברחבי מגוון רחב של תוצאות מבחן, קבוצות נתונים אימונים צריכות לכלול דוגמאות מגוונות שמרחבות תופעות רבות. אוסף נתונים נוגדים דינמיים (DADC), שבו המכתבים יוצרים דוגמאות שמתאתגרים בשיפור ממשיך של דוגמאות, מחזיק הבטחה כגישה לייצור קבוצות אימונים מגוונים כאלה. העבודה הקודמת הראה שהפעלת DADC במהלך 1-3 סיבובים יכולה לעזור לדוגמנים לתקן סוגי שגיאות מסויימים, אך זה לא בהכרח מוביל לגנרליזציה טובה יותר מעבר למידע מבחן יריבי. אנחנו מתווכחים שהפעלת DADC במהלך סיבובים רבים מקסימום את היתרונות של הזמן האימוני שלה, כיוון שהסיבובים השונים יכולים יחד לכסות הרבה מהתופעות רלוונטיות למשימה. אנו מציגים את המחקר הראשון של DADC לטווח ארוך יותר, שבו אנו אוספים 20 סיבובים של דוגמאות NLI עבור קבוצה קטנה של פראגמים משמעותיים, עם גישות נוגדיות וגם לא נוגדיות. דוגמאות מאומנות על דוגמאות DADC עושות 26% פחות שגיאות על קבוצת המבחנים המומחיות שלנו בהשוואה לדוגמאות מאומנות על נתונים לא יריביים. הניתוח שלנו מראה כי DADC מציג דוגמאות יותר קשות, יותר לקסית ומגוונים באופן סינטאקטי, ומכילים פחות חפצי ציונים בהשוואה לדוגמאות לא נוגדיות.</abstract_he>
      <abstract_ha>To, ka ƙiƙira misãlai waɗanda aka kife su cikin cikin wasu matsayi mai tsawo, za'a ƙunsa da tsarin mutane masu amfani da wasu misãlai da za'a spana abu mai yawa. Juyin data na motsi da za'a motsi (DADC), a inda misãlai na sanitacce masu motsi wanda ke tsõratar da misãlai masu yin amfani da daidai a ci-daidai, yana yi wa'adi kamar hanyor ya sami tsari masu yin amfani da waɗancan. Kayan aikin na gaba ya nuna cewa tafiyar da DADC kan rundunta 1-3, yana iya amfani da misãlai su daidaita wasu nau'i masu ɓarna, kuma amma ba ya lazimta ta ta ƙara tsarin mai kyau bisa data na motsi. Tuna faɗa da cẽwa, tafiyar da DADC a kan runduniya masu yawa yana faɗaɗa amfani da wa'adin mai aikin, kamar da rundunõnin dabam-dabam suke iya haɗuwa da wasu na'anar aiki da ke da muhimmi. Tuna halatar da farkon karatun na DADC, inda Muke sami misãlai 20 na NLI, wa'anar ƙarami na fassarar farko, da duk masu motsi da kuma bã-motsi. @ info: whatsthis AnalyyinMu yana nũna DADC ya fitar misãlai waɗanda suke mafi tsananin masu adadi, masu cikin littafa da tarayya, kuma yana da masu kashẽwa kaɗan da misãlai masu motsi.</abstract_ha>
      <abstract_bo>དཔེ་དབྱེ་བ་དག་གི་ནང་དུ་བརྟག་དཔེ་དབྱིབས་ཡོད་པའི་མིག Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continuously improving models, holds a promise as an approach for generating such a diverse training sets. Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data. ང་ཚོས་DADC་འཁོར་སྐྱོད་བྱས་ནས་སྐབས་ཆར་གྱི་དོན་ལས་ཕན་ཚུལ་ཆེ་ཤོས་བསྐྱེད་ཚད་མང་ཤོས་ཏེ། ང་ཚོས་DADC བརྟག་ཞིབ་འཇུག་རྒྱུ་དང་པོ་དེ་ལྟ་བུ་བཏོན་པའི་གནད་དོན་ཐེངས་ཀྱི་དཔེར་བརྗོད་ཆོག Models trained on DADC examples make 26\% fewer errors on our expert-curated test set compared to models trained on non-adversarial data. Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.</abstract_bo>
      </paper>
    <paper id="94">
      <title>Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming</title>
      <author><first>Ayush</first><last>Maheshwari</last></author>
      <author><first>Krishnateja</first><last>Killamsetty</last></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last></author>
      <author><first>Rishabh</first><last>Iyer</last></author>
      <author><first>Marina</first><last>Danilevsky</last></author>
      <author><first>Lucian</first><last>Popa</last></author>
      <pages>1188-1202</pages>
      <abstract>A critical bottleneck in supervised machine learning is the need for large amounts of labeled data which is expensive and time-consuming to obtain. Although a small amount of labeled data cannot be used to train a model, it can be used effectively for the generation of humaninterpretable labeling functions (LFs). These LFs, in turn, have been used to generate a large amount of additional noisy labeled data in a paradigm that is now commonly referred to as data programming. Previous methods of generating LFs do not attempt to use the given labeled data further to train a model, thus missing opportunities for improving performance. Additionally, since the LFs are generated automatically, they are likely to be noisy, and naively aggregating these LFs can lead to suboptimal results. In this work, we propose an LF-based bi-level optimization framework WISDOM to solve these two critical limitations. WISDOM learns a joint model on the (same) labeled dataset used for LF induction along with any unlabeled data in a semi-supervised manner, and more critically, reweighs each LF according to its goodness, influencing its contribution to the semi-supervised loss using a robust bi-level optimization algorithm. We show that WISDOM significantly outperforms prior approaches on several text classification datasets.</abstract>
      <url hash="d59c3118">2022.findings-acl.94</url>
      <attachment type="software" hash="064c9fcf">2022.findings-acl.94.software.zip</attachment>
      <bibkey>maheshwari-etal-2022-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="96">
      <title>Cross-lingual Inference with A <fixed-case>C</fixed-case>hinese Entailment Graph</title>
      <author><first>Tianyi</first><last>Li</last></author>
      <author><first>Sabine</first><last>Weber</last></author>
      <author><first>Mohammad Javad</first><last>Hosseini</last></author>
      <author><first>Liane</first><last>Guillou</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>1214-1233</pages>
      <abstract>Predicate entailment detection is a crucial task for question-answering from text, where previous work has explored unsupervised learning of entailment graphs from typed open relation triples. In this paper, we present the first pipeline for building Chinese entailment graphs, which involves a novel high-recall open relation extraction (ORE) method and the first Chinese fine-grained entity typing dataset under the FIGER type ontology. Through experiments on the Levy-Holt dataset, we verify the strength of our Chinese entailment graph, and reveal the cross-lingual complementarity: on the parallel Levy-Holt dataset, an ensemble of Chinese and English entailment graphs outperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC points.</abstract>
      <url hash="65f04b84">2022.findings-acl.96</url>
      <attachment type="software" hash="b379bced">2022.findings-acl.96.software.zip</attachment>
      <bibkey>li-etal-2022-cross</bibkey>
      <pwccode url="https://github.com/teddy-li/chineseentgraph" additional="false">teddy-li/chineseentgraph</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="108">
      <title>Graph Neural Networks for Multiparallel Word Alignment</title>
      <author><first>Ayyoob</first><last>Imani</last></author>
      <author><first>Lütfi Kerem</first><last>Senel</last></author>
      <author><first>Masoud</first><last>Jalili Sabet</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>1384-1396</pages>
      <abstract>After a period of decrease interest in word alignments is increasing again for their usefulness in domains such as typological research cross lingual annotation projection and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> Generally alignment algorithms only use <a href="https://en.wikipedia.org/wiki/Bitext">bitext</a> and do not make use of the fact that many parallel corpora are multiparallel Here we compute high quality <a href="https://en.wikipedia.org/wiki/Word_alignment">word alignments</a> between multiple language pairs by considering all language pairs together First we create a multiparallel word alignment graph joining all bilingual word alignment pairs in one graph Next we use graph neural networks GNNs to exploit the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a> Our GNN approach i utilizes information about the meaning position and language of the input words ii incorporates information from multiple parallel sentences iii adds and removes edges from the initial alignments and iv yields a prediction model that can generalize beyond the training sentences We show that community detection algorithms can provide valuable information for multiparallel word alignment Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms previous work on three word alignment datasets and on a downstream task</abstract>
      <url hash="c8f2efd4">2022.findings-acl.108</url>
      <bibkey>imani-etal-2022-graph</bibkey>
    </paper>
    <paper id="109">
      <title>Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors<fixed-case>ASR</fixed-case> Errors</title>
      <author><first>Yang</first><last>Wu</last></author>
      <author><first>Yanyan</first><last>Zhao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Song</first><last>Chen</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Xiaohuan</first><last>Cao</last></author>
      <author><first>Wenting</first><last>Zhao</last></author>
      <pages>1397-1406</pages>
      <abstract>Multimodal sentiment analysis has attracted increasing attention and lots of models have been proposed However the performance of the state of the art <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> decreases sharply when they are deployed in the real world We find that the main reason is that real world applications can only access the text outputs by the automatic speech recognition ASR models which may be with errors because of the limitation of model capacity Through further analysis of the ASR outputs we find that in some cases the sentiment words the key sentiment elements in the textual modality are recognized as other words which makes the sentiment of the text change and hurts the performance of multimodal sentiment analysis models directly To address this problem we propose the sentiment word aware multimodal refinement model SWRM which can dynamically refine the erroneous sentiment words by leveraging multimodal sentiment clues Specifically we first use the sentiment word position detection module to obtain the most possible position of the sentiment word in the text and then utilize the multimodal sentiment word refinement module to dynamically refine the sentiment word embeddings The refined embeddings are taken as the textual inputs of the multimodal feature fusion module to predict the sentiment labels We conduct extensive experiments on the real world datasets including MOSI Speechbrain MOSI IBM and MOSI iFlytek and the results demonstrate the effectiveness of our model which surpasses the current state of the art models on three datasets Furthermore our approach can be adapted for other multimodal feature fusion models easily</abstract>
      <url hash="c8c28ec6">2022.findings-acl.109</url>
      <bibkey>wu-etal-2022-sentiment</bibkey>
      <pwccode url="https://github.com/albertwy/SWRM" additional="false">albertwy/SWRM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity">Multimodal Opinionlevel Sentiment Intensity</pwcdataset>
    </paper>
    <paper id="113">
      <title>End-to-End Speech Translation for Code Switched Speech</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Matthias</first><last>Sperber</last></author>
      <author><first>Telmo</first><last>Pires</last></author>
      <author><first>Hendra</first><last>Setiawan</last></author>
      <author><first>Christian</first><last>Gollan</last></author>
      <author><first>Dominic</first><last>Telaar</last></author>
      <author><first>Matthias</first><last>Paulik</last></author>
      <pages>1435-1448</pages>
      <abstract>Code switching (CS) refers to the phenomenon of interchangeably using words and phrases from different languages. CS can pose significant accuracy challenges to NLP, due to the often monolingual nature of the underlying systems. In this work, we focus on CS in the context of English/Spanish conversations for the task of speech translation (ST), generating and evaluating both transcript and translation. To evaluate model performance on this task, we create a novel ST corpus derived from existing public data sets. We explore various ST architectures across two dimensions: cascaded (transcribe then translate) vs end-to-end (jointly transcribe and translate) and unidirectional (source -&gt; target) vs bidirectional (source &lt;-&gt; target). We show that our ST architectures, and especially our bidirectional end-to-end architecture, perform well on CS speech, even when no CS training data is used.</abstract>
      <url hash="630890be">2022.findings-acl.113</url>
      <bibkey>weller-etal-2022-end</bibkey>
      <pwccode url="https://github.com/apple/ml-code-switched-speech-translation" additional="false">apple/ml-code-switched-speech-translation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/covost">CoVoST</pwcdataset>
    </paper>
    <paper id="120">
      <title>Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Yimeng</first><last>Chen</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Chang</first><last>Su</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <pages>1524-1535</pages>
      <abstract>Natural Language Inference NLI datasets contain examples with highly ambiguous labels due to its subjectivity Several recent efforts have been made to acknowledge and embrace the existence of <a href="https://en.wikipedia.org/wiki/Ambiguity">ambiguity</a> and explore how to capture the human disagreement distribution In contrast with directly learning from gold ambiguity labels relying on special resource we argue that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> has naturally captured the human ambiguity distribution as long as its calibrated i.e. the predictive probability can reflect the true correctness likelihood Our experiments show that when model is well calibrated either by label smoothing or temperature scaling it can obtain competitive performance as prior work on both divergence scores between predictive probability and the true human opinion distribution and the accuracy This reveals the overhead of collecting gold ambiguity labels can be cut by broadly solving how to calibrate the NLI network</abstract>
      <url hash="27f686db">2022.findings-acl.120</url>
      <bibkey>wang-etal-2022-capture</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/chaosnli">ChaosNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="121">
      <title>Efficient, Uncertainty-based Moderation of Neural Networks Text Classifiers</title>
      <author><first>Jakob Smedegaard</first><last>Andersen</last></author>
      <author><first>Walid</first><last>Maalej</last></author>
      <pages>1536-1546</pages>
      <abstract>To maximize the accuracy and increase the overall acceptance of text classifiers, we propose a framework for the efficient, in-operation moderation of classifiers’ output. Our framework focuses on use cases in which F1-scores of modern Neural Networks classifiers (ca. 90%) are still inapplicable in practice. We suggest a semi-automated approach that uses prediction uncertainties to pass unconfident, probably incorrect classifications to human moderators. To minimize the workload, we limit the human moderated data to the point where the accuracy gains saturate and further human effort does not lead to substantial improvements. A series of benchmarking experiments based on three different datasets and three state-of-the-art classifiers show that our framework can improve the classification F1-scores by 5.1 to 11.2% (up to approx. 98 to 99%), while reducing the moderation load up to 73.3% compared to a random moderation.</abstract>
      <url hash="2b0f4afc">2022.findings-acl.121</url>
      <attachment type="software" hash="fdb8e0b4">2022.findings-acl.121.software.zip</attachment>
      <bibkey>andersen-maalej-2022-efficient</bibkey>
      <pwccode url="https://github.com/jsandersen/cmt" additional="false">jsandersen/cmt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="123">
      <title>Open Vocabulary Extreme Classification Using Generative Models</title>
      <author><first>Daniel</first><last>Simig</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Pouya</first><last>Yanki</last></author>
      <author><first>Kashyap</first><last>Popat</last></author>
      <author><first>Christina</first><last>Du</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Majid</first><last>Yazdani</last></author>
      <pages>1561-1583</pages>
      <abstract>The extreme multi label classification XMC task aims at tagging content with a subset of labels from an extremely large label set The label vocabulary is typically defined in advance by domain experts and assumed to capture all necessary tags However in real world scenarios this label set although large is often incomplete and experts frequently need to refine it To develop systems that simplify this process we introduce the task of open vocabulary XMC OXMC): given a piece of content predict a set of labels some of which may be outside of the known tag set Hence in addition to not having training data for some labelsas is the case in zero shot classificationmodels need to invent some labels on thefly We propose GROOV a fine tuned seq2seq model for OXMC that generates the set of labels as a flat sequence and is trained using a novel loss independent of predicted label order We show the efficacy of the approach experimenting with popular XMC datasets for which GROOV is able to predict meaningful labels outside the given vocabulary while performing on par with state of the art solutions for known labels</abstract>
      <url hash="33d9845e">2022.findings-acl.123</url>
      <bibkey>simig-etal-2022-open</bibkey>
    </paper>
    <paper id="124">
      <title>Decomposed Meta-Learning for Few-Shot Named Entity Recognition</title>
      <author><first>Tingting</first><last>Ma</last></author>
      <author><first>Huiqiang</first><last>Jiang</last></author>
      <author><first>Qianhui</first><last>Wu</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <author><first>Chin-Yew</first><last>Lin</last></author>
      <pages>1584-1596</pages>
      <abstract>Few-shot named entity recognition (NER) systems aim at recognizing novel-class named entities based on only a few labeled examples. In this paper, we present a decomposed meta-learning approach which addresses the problem of few-shot NER by sequentially tackling few-shot span detection and few-shot entity typing using meta-learning. In particular, we take the few-shot span detection as a sequence labeling problem and train the span detector by introducing the model-agnostic meta-learning (MAML) algorithm to find a good model parameter initialization that could fast adapt to new entity classes. For few-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced prototypical networks to find a good embedding space that can better distinguish text span representations from different entity classes. Extensive experiments on various benchmarks show that our approach achieves superior performance over prior methods.</abstract>
      <url hash="cc1f91ee">2022.findings-acl.124</url>
      <attachment type="software" hash="f0183f32">2022.findings-acl.124.software.zip</attachment>
      <bibkey>ma-etal-2022-decomposed</bibkey>
      <pwccode url="https://github.com/microsoft/vert-papers" additional="false">microsoft/vert-papers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2002">CoNLL 2002</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/few-nerd">Few-NERD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="127">
      <title>Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text</title>
      <author><first>Siyuan</first><last>Wang</last></author>
      <author><first>Wanjun</first><last>Zhong</last></author>
      <author><first>Duyu</first><last>Tang</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Zhihao</first><last>Fan</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>1619-1629</pages>
      <abstract>Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them. Existing methods for logical reasoning mainly focus on contextual semantics of text while struggling to explicitly model the logical inference process. In this paper, we not only put forward a logic-driven context extension framework but also propose a logic-driven data augmentation algorithm. The former follows a three-step reasoning paradigm, and each step is respectively to extract logical expressions as elementary reasoning units, symbolically infer the implicit expressions following equivalence laws and extend the context to validate the options. The latter augments literally similar but logically different instances and incorporates contrastive learning to better capture logical information, especially logical negative and conditional relationships. We conduct experiments on two benchmark datasets, ReClor and LogiQA. The results show that our method achieves state-of-the-art performance on both datasets, and even surpasses human performance on the ReClor dataset.</abstract>
      <url hash="579b2036">2022.findings-acl.127</url>
      <attachment type="software" hash="d2fab7a1">2022.findings-acl.127.software.zip</attachment>
      <bibkey>wang-etal-2022-logic</bibkey>
      <pwccode url="https://github.com/WangsyGit/LReasoner" additional="false">WangsyGit/LReasoner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
    </paper>
    <paper id="132">
      <title>Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation</title>
      <author><first>Qingyu</first><last>Tan</last></author>
      <author><first>Ruidan</first><last>He</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <pages>1672-1681</pages>
      <abstract>Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.</abstract>
      <url hash="2d20ba84">2022.findings-acl.132</url>
      <attachment type="software" hash="d9e434b3">2022.findings-acl.132.software.zip</attachment>
      <bibkey>tan-etal-2022-document</bibkey>
      <pwccode url="https://github.com/tonytan48/kd-docre" additional="false">tonytan48/kd-docre</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="136">
      <title>How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis</title>
      <author><first>Shaobo</first><last>Li</last></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <author><first>Chengjie</first><last>Sun</last></author>
      <author><first>Bingquan</first><last>Liu</last></author>
      <author><first>Zhenzhou</first><last>Ji</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>1720-1732</pages>
      <abstract>Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs). Many works show the PLMs’ ability to fill in the missing factual words in cloze-style prompts such as ”Dante was born in [MASK].” However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns? We try to answer this question by a causal-inspired analysis that quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words. We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred. Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words; (2) the dependence on the knowledge-dependent words is more effective than the positionally close and highly co-occurred words. Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.</abstract>
      <url hash="2bc28acb">2022.findings-acl.136</url>
      <bibkey>li-etal-2022-pre</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="161">
      <title>Ranking-Constrained Learning with Rationales for Text Classification</title>
      <author><first>Juanyan</first><last>Wang</last></author>
      <author><first>Manali</first><last>Sharma</last></author>
      <author><first>Mustafa</first><last>Bilgic</last></author>
      <pages>2034-2046</pages>
      <abstract>We propose a novel approach that jointly utilizes the labels and elicited rationales for text classification to speed up the training of deep learning models with limited training data. We define and optimize a ranking-constrained loss function that combines cross-entropy loss with ranking losses as rationale constraints. We evaluate our proposed rationale-augmented learning approach on three human-annotated datasets, and show that our approach provides significant improvements over classification approaches that do not utilize rationales as well as other state-of-the-art rationale-augmented baselines.</abstract>
      <url hash="ed9f5299">2022.findings-acl.161</url>
      <attachment type="software" hash="d45e7025">2022.findings-acl.161.software.zip</attachment>
      <bibkey>wang-etal-2022-ranking</bibkey>
    </paper>
    <paper id="173">
      <title>The impact of lexical and grammatical processing on generating code from <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a></title>
      <author><first>Nathanaël</first><last>Beau</last></author>
      <author><first>Benoit</first><last>Crabbé</last></author>
      <pages>2204-2214</pages>
      <abstract>Considering the seq2seq architecture of Yin and Neubig for natural language to code translation we identify four key components of importance grammatical constraints lexical preprocessing input representations and copy mechanisms To study the impact of these components we use a state of the art architecture that relies on BERT encoder and a grammar based decoder for which a formalization is provided The paper highlights the importance of the lexical substitution component in the current natural language to code systems</abstract>
      <url hash="28af5fc0">2022.findings-acl.173</url>
      <bibkey>beau-crabbe-2022-impact</bibkey>
      <pwccode url="https://gitlab.com/codegenfact/BertranX" additional="false">codegenfact/BertranX</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conala">CoNaLa</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/django">Django</pwcdataset>
    </paper>
    <paper id="176">
      <title>Your fairness may vary Pretrained language model fairness in toxic text classification</title>
      <author><first>Ioana</first><last>Baldini</last></author>
      <author><first>Dennis</first><last>Wei</last></author>
      <author><first>Karthikeyan</first><last>Natesan Ramamurthy</last></author>
      <author><first>Moninder</first><last>Singh</last></author>
      <author><first>Mikhail</first><last>Yurochkin</last></author>
      <pages>2245-2262</pages>
      <abstract>The popularity of pretrained language models in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing systems</a> calls for a careful evaluation of such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> in down stream tasks which have a higher potential for societal impact The evaluation of such <a href="https://en.wikipedia.org/wiki/System">systems</a> usually focuses on <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy measures</a> Our findings in this paper call for attention to be paid to fairness measures as well Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks English we demonstrate that focusing on accuracy measures alone can lead to <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> with wide variation in fairness characteristics Specifically we observe that <a href="https://en.wikipedia.org/wiki/Fair_division">fairness</a> can vary even more than <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> with increasing training data size and different random initializations At the same time we find that little of the fairness variation is explained by model size despite claims in the literature To improve model fairness without retraining we show that two post processing methods developed for structured tabular data can be successfully applied to a range of pretrained language models Warning This paper contains samples of offensive text</abstract>
      <url hash="08332a7e">2022.findings-acl.176</url>
      <bibkey>baldini-etal-2022-fairness</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hatexplain">HateXplain</pwcdataset>
    </paper>
    <paper id="186">
      <title>Improving Neural Political Statement Classification with Class Hierarchical Information</title>
      <author><first>Erenay</first><last>Dayanik</last></author>
      <author><first>Andre</first><last>Blessing</last></author>
      <author><first>Nico</first><last>Blokker</last></author>
      <author><first>Sebastian</first><last>Haunss</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <author><first>Sebastian</first><last>Pado</last></author>
      <pages>2367-2382</pages>
      <abstract>Many tasks in text based computational social science CSS involve 
 the classification of political statements into categories based on a domain specific codebook In order to be useful for <a href="https://en.wikipedia.org/wiki/Cascading_Style_Sheets">CSS analysis</a> these categories must be fine grained The typically skewed distribution of fine grained categories however results in 
 a challenging classification problem on the NLP side This paper proposes to make use of the hierarchical relations among categories typically present in such codebooks 
 e.g. markets and taxation are both subcategories of economy while borders is a subcategory of security We use these ontological relations as prior knowledge to establish additional constraints on the learned model thus 
 improving performance overall and in particular for infrequent categories We evaluate several lightweight variants of this intuition by extending state of the art transformer based text 
 classifiers on two datasets and multiple languages We find the most consistent improvement for an <a href="https://en.wikipedia.org/wiki/Scientific_method">approach</a> based on regularization</abstract>
      <url hash="04b06c85">2022.findings-acl.186</url>
      <attachment type="software" hash="3affc077">2022.findings-acl.186.software.zip</attachment>
      <bibkey>dayanik-etal-2022-improving</bibkey>
    </paper>
    <paper id="194">
      <title>Why don’t people use character-level machine translation?</title>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Helmut</first><last>Schmid</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>2470-2485</pages>
      <abstract>We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.</abstract>
      <url hash="cb530acb">2022.findings-acl.194</url>
      <attachment type="software" hash="d50242f4">2022.findings-acl.194.software.tgz</attachment>
      <bibkey>libovicky-etal-2022-dont</bibkey>
    </paper>
    <paper id="197">
      <title>Automatic Speech Recognition and Query By Example for Creole Languages Documentation</title>
      <author><first>Cécile</first><last>Macaire</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Emmanuel</first><last>Schang</last></author>
      <pages>2512-2520</pages>
      <abstract>We investigate the exploitation of self supervised models for two <a href="https://en.wikipedia.org/wiki/Creole_language">Creole languages</a> with few resources Gwadloupyen and Morisien Automatic language processing tools are almost non existent for these two languages We propose to use about one hour of annotated data to design an automatic speech recognition system for each language We evaluate how much data is needed to obtain a query by example system that is usable by linguists Moreover our experiments show that multilingual self supervised models are not necessarily the most efficient for <a href="https://en.wikipedia.org/wiki/Creole_language">Creole languages</a></abstract>
      <url hash="f8c16e05">2022.findings-acl.197</url>
      <bibkey>macaire-etal-2022-automatic</bibkey>
      <pwccode url="https://github.com/macairececile/asr-qbe-creole" additional="false">macairececile/asr-qbe-creole</pwccode>
    </paper>
    <paper id="207">
      <title>Long Time No See! Open-Domain Conversation with Long-Term Persona Memory</title>
      <author><first>Xinchao</first><last>Xu</last></author>
      <author><first>Zhibin</first><last>Gou</last></author>
      <author><first>Wenquan</first><last>Wu</last></author>
      <author><first>Zheng-Yu</first><last>Niu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Shihang</first><last>Wang</last></author>
      <pages>2639-2650</pages>
      <abstract>Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework with Long-Term Memory (LTM) mechanism (called PLATO-LTM). This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training. To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot. Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness.</abstract>
      <url hash="c29ec385">2022.findings-acl.207</url>
      <bibkey>xu-etal-2022-long</bibkey>
      <pwccode url="https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2022-DuLeMon" additional="false">PaddlePaddle/Research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/delemon">DuLeMon</pwcdataset>
    </paper>
    <paper id="218">
      <title>Breaking Down Multilingual Machine Translation</title>
      <author><first>Ting-Rui</first><last>Chiang</last></author>
      <author><first>Yi-Pei</first><last>Chen</last></author>
      <author><first>Yi-Ting</first><last>Yeh</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>2766-2780</pages>
      <abstract>While multilingual training is now an essential ingredient in machine translation MT systems recent work has demonstrated that it has different effects in different multilingual settings such as many to one one to many and many to many learning These training settings expose the encoder and the decoder in a machine translation model with different data distributions In this paper we examine how different varieties of multilingual training contribute to learning these two components of the <a href="https://en.wikipedia.org/wiki/Multilingualism">MT model</a> Specifically we compare bilingual models with encoders and/or decoders initialized by multilingual training We show that multilingual training is beneficial to <a href="https://en.wikipedia.org/wiki/Encoder">encoders</a> in general while it only benefits <a href="https://en.wikipedia.org/wiki/Code">decoders</a> for low resource languages LRLs We further find the important attention heads for each language pair and compare their correlations during <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference</a> Our analysis sheds light on how multilingual translation models work and also enables us to propose methods to improve performance by training with highly related languages Our many to one models for high resource languages and one to many models for LRL outperform the best results reported by Aharoni et al</abstract>
      <url hash="72ed420c">2022.findings-acl.218</url>
      <bibkey>chiang-etal-2022-breaking</bibkey>
    </paper>
    <paper id="233">
      <title>Improving Chinese Grammatical Error Detection via <a href="https://en.wikipedia.org/wiki/Data_augmentation">Data augmentation</a> by Conditional Error Generation<fixed-case>C</fixed-case>hinese Grammatical Error Detection via Data augmentation by Conditional Error Generation</title>
      <author><first>Tianchi</first><last>Yue</last></author>
      <author><first>Shulin</first><last>Liu</last></author>
      <author><first>Huihui</first><last>Cai</last></author>
      <author><first>Tao</first><last>Yang</last></author>
      <author><first>Shengkang</first><last>Song</last></author>
      <author><first>TingHao</first><last>Yu</last></author>
      <pages>2966-2975</pages>
      <abstract>Chinese Grammatical Error Detection(CGED aims at detecting grammatical errors in Chinese texts One of the main challenges for <a href="https://en.wikipedia.org/wiki/Computer-aided_design">CGED</a> is the lack of <a href="https://en.wikipedia.org/wiki/Annotation">annotated data</a> To alleviate this problem previous studies proposed various methods to automatically generate more training samples which can be roughly categorized into rule based methods and model based methods The rule based methods construct erroneous sentences by directly introducing noises into original sentences However the introduced noises are usually context independent which are quite different from those made by humans The model based methods utilize <a href="https://en.wikipedia.org/wiki/Generative_model">generative models</a> to imitate <a href="https://en.wikipedia.org/wiki/Human_error">human errors</a> The <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> may bring too many changes to the original sentences and generate semantically ambiguous sentences so it is difficult to detect grammatical errors in these generated sentences In addition generated sentences may be error free and thus become <a href="https://en.wikipedia.org/wiki/Noisy_data">noisy data</a> To handle these problems we propose CNEG a novel Conditional Non Autoregressive Error Generation model for generating Chinese grammatical errors Specifically in order to generate a context dependent error we first mask a span in a correct text then predict an erroneous span conditioned on both the masked text and the correct span Furthermore we filter out error free spans by measuring their perplexities in the original sentences Experimental results show that our proposed method achieves better performance than all compared data augmentation methods on the CGED-2018 and CGED-2020 benchmarks</abstract>
      <url hash="75d7da6e">2022.findings-acl.233</url>
      <bibkey>yue-etal-2022-improving</bibkey>
    </paper>
    <paper id="246">
      <title>Improving Robustness of Language Models from a Geometry-aware Perspective</title>
      <author><first>Bin</first><last>Zhu</last></author>
      <author><first>Zhaoquan</first><last>Gu</last></author>
      <author><first>Le</first><last>Wang</last></author>
      <author><first>Jinyin</first><last>Chen</last></author>
      <author><first>Qi</first><last>Xuan</last></author>
      <pages>3115-3125</pages>
      <abstract>Recent studies have found that removing the norm-bounded projection and increasing search steps in adversarial training can significantly improve robustness. However, we observe that a too large number of search steps can hurt accuracy. We aim to obtain strong robustness efficiently using fewer steps. Through a toy experiment, we find that perturbing the clean data to the decision boundary but not crossing it does not degrade the test accuracy. Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate friendly adversarial data. On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training on friendly adversarial data so that we can save a large number of search steps. Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via fewer steps. In addition, we provide extensive empirical results and in-depth analyses on robustness to facilitate future studies.</abstract>
      <url hash="526ea1ce">2022.findings-acl.246</url>
      <attachment type="software" hash="cb33852b">2022.findings-acl.246.software.zip</attachment>
      <bibkey>zhu-etal-2022-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="251">
      <title><fixed-case>UNIMO</fixed-case>-2: End-to-End Unified Vision-Language Grounded Learning</title>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Can</first><last>Gao</last></author>
      <author><first>Guocheng</first><last>Niu</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Hao</first><last>Liu</last></author>
      <author><first>Jiachen</first><last>Liu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>3187-3201</pages>
      <abstract>Vision-Language Pre-training (VLP) has achieved impressive performance on various cross-modal downstream tasks. However, most existing methods can only learn from aligned image-caption data and rely heavily on expensive regional features, which greatly limits their scalability and performance. In this paper, we propose an end-to-end unified-modal pre-training framework, namely UNIMO-2, for joint learning on both aligned image-caption data and unaligned image-only and text-only corpus. We build a unified Transformer model to jointly learn visual representations, textual representations and semantic alignment between images and texts. In particular, we propose to conduct grounded learning on both images and texts via a sharing grounded space, which helps bridge unaligned images and texts, and align the visual and textual semantic spaces on different types of corpora. The experiments show that our grounded learning method can improve textual and visual semantic alignment for improving performance on various cross-modal tasks. Moreover, benefiting from effective joint modeling of different types of corpora, our model also achieves impressive performance on single-modal visual and textual tasks. Our code and models are public at the UNIMO project page https://unimo-ptm.github.io/.</abstract>
      <url hash="9aa1ec50">2022.findings-acl.251</url>
      <bibkey>li-etal-2022-unimo</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli-ve">SNLI-VE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="258">
      <title>Word-level Perturbation Considering Word Length and Compositional Subwords</title>
      <author><first>Tatsuya</first><last>Hiraoka</last></author>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Kei</first><last>Uchiumi</last></author>
      <author><first>Atsushi</first><last>Keyaki</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>3268-3275</pages>
      <abstract>We present two simple modifications for word-level perturbation: Word Replacement considering Length (WR-L) and Compositional Word Replacement (CWR).In conventional word replacement, a word in an input is replaced with a word sampled from the entire vocabulary, regardless of the length and context of the target word.WR-L considers the length of a target word by sampling words from the Poisson distribution.CWR considers the compositional candidates by restricting the source of sampling to related words that appear in subword regularization.Experimental results showed that the combination of WR-L and CWR improved the performance of text classification and machine translation.</abstract>
      <url hash="eb299be3">2022.findings-acl.258</url>
      <bibkey>hiraoka-etal-2022-word</bibkey>
    </paper>
    <paper id="260">
      <title>Controlling the Focus of Pretrained Language Generation Models</title>
      <author><first>Jiabao</first><last>Ji</last></author>
      <author><first>Yoon</first><last>Kim</last></author>
      <author><first>James</first><last>Glass</last></author>
      <author><first>Tianxing</first><last>He</last></author>
      <pages>3291-3306</pages>
      <abstract>The finetuning of pretrained transformer based language generation models are typically conducted in an end to end manner where the model learns to attend to relevant parts of the input by itself However there does not exist a mechanism to directly control the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s focus This work aims to develop a control mechanism by which a user can select spans of context as highlights’’ for the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to focus on and generate relevant output To achieve this goal we augment a pretrained model with trainable focus vectors’’ that are directly applied to the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s embeddings while the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> itself is kept fixed These <a href="https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)">vectors</a> trained on automatic annotations derived from attribution methods act as indicators for context importance We test our approach on two core generation tasks dialogue response generation and abstractive summarization We also collect evaluation data where the highlight generation pairs are annotated by humans Our experiments show that the trained <a href="https://en.wikipedia.org/wiki/Focus_(computing)">focus vectors</a> are effective in steering the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> to generate outputs that are relevant to user selected highlights</abstract>
      <url hash="c5fe7303">2022.findings-acl.260</url>
      <bibkey>ji-etal-2022-controlling</bibkey>
      <pwccode url="https://github.com/question406/learningtofocus" additional="false">question406/learningtofocus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="265">
      <title>CUE Vectors Modular Training of Language Models Conditioned on Diverse Contextual Signals<fixed-case>CUE</fixed-case> Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals</title>
      <author><first>Scott</first><last>Novotney</last></author>
      <author><first>Sreeparna</first><last>Mukherjee</last></author>
      <author><first>Zeeshan</first><last>Ahmed</last></author>
      <author><first>Andreas</first><last>Stolcke</last></author>
      <pages>3368-3379</pages>
      <abstract>We propose a framework to modularize the training of neural language models that use diverse forms of context by eliminating the need to jointly   train context and within sentence encoders Our approach contextual universal embeddings CUE trains LMs on one type of contextual data and adapts to novel context types The model consists of a pretrained neural sentence LM a BERT based contextual encoder and a masked transfomer decoder that estimates LM probabilities using sentence internal and contextual evidence When contextually annotated data is unavailable our model learns to combine contextual and sentence internal information using noisy oracle unigram embeddings as a proxy Real context data can be introduced later and used to adapt a small number of parameters that map contextual data into the decoder’s embedding space We validate the CUE framework on a NYTimes text corpus with multiple metadata types for which the LM perplexity can be lowered from 36.6 to 27.4 by conditioning on context Bootstrapping a contextual LM with only a subset of the <a href="https://en.wikipedia.org/wiki/Metadata">metadata</a> during training retains of the achievable gain Training the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> initially with proxy context retains of the perplexity gain after adapting to real context Furthermore we can swap one type of pretrained sentence LM for another without retraining the context encoders by only adapting the decoder model Overall we obtain a modular framework that allows incremental scalable training of context enhanced LMs</abstract>
      <url hash="23c93066">2022.findings-acl.265</url>
      <bibkey>novotney-etal-2022-cue</bibkey>
    </paper>
    <paper id="267">
      <title>Aligned Weight Regularizers for Pruning Pretrained Neural Networks</title>
      <author><first>James</first><last>O’ Neill</last></author>
      <author><first>Sourav</first><last>Dutta</last></author>
      <author><first>Haytham</first><last>Assem</last></author>
      <pages>3391-3401</pages>
      <abstract>Pruning aims to reduce the number of parameters while maintaining performance close to the original network This work proposes a novel \emph based pruning strategy whereby the representational similarity between the pruned and unpruned versions of the same network is maximized Unlike previous approaches that treat <a href="https://en.wikipedia.org/wiki/Distillation">distillation</a> and pruning separately we use <a href="https://en.wikipedia.org/wiki/Distillation">distillation</a> to inform the pruning criteria without requiring a separate student network as in knowledge distillation We show that the proposed   implicitly encourages sparse solutions naturally complementing magnitude based pruning criteria Experiments on the GLUE and XGLUE benchmarks show that self distilled pruning increases mono- and cross lingual language model performance Self distilled pruned models also outperform smaller Transformers with an equal number of parameters and are competitive against times larger distilled networks We also observe that self distillation maximizes class separability increases the <a href="https://en.wikipedia.org/wiki/Signal-to-noise_ratio">signal to noise ratio</a> and converges faster after pruning steps providing further insights into why self distilled pruning improves generalization<i>self-distillation</i> based pruning strategy, whereby the representational similarity between the pruned and unpruned versions of the same network is maximized. Unlike previous approaches that treat distillation and pruning separately, we use distillation to inform the pruning criteria, without requiring a separate student network as in knowledge distillation. We show that the proposed <i>cross-correlation objective for self-distilled pruning</i> implicitly encourages sparse solutions, naturally complementing magnitude-based pruning criteria. Experiments on the GLUE and XGLUE benchmarks show that self-distilled pruning increases mono- and cross-lingual language model performance. Self-distilled pruned models also outperform smaller Transformers with an equal number of parameters and are competitive against (6 times) larger distilled networks. We also observe that self-distillation (1) maximizes class separability, (2) increases the signal-to-noise ratio, and (3) converges faster after pruning steps, providing further insights into why self-distilled pruning improves generalization. </abstract>
      <url hash="fdb3ff58">2022.findings-acl.267</url>
      <bibkey>o-neill-etal-2022-aligned</bibkey>
    </paper>
    <paper id="268">
      <title>Consistent Representation Learning for Continual Relation Extraction</title>
      <author><first>Kang</first><last>Zhao</last></author>
      <author><first>Hua</first><last>Xu</last></author>
      <author><first>Jiangong</first><last>Yang</last></author>
      <author><first>Kai</first><last>Gao</last></author>
      <pages>3402-3411</pages>
      <abstract>Continual relation extraction CRE aims to continuously train a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on data with new relations while avoiding forgetting old ones Some previous work has proved that storing a few typical samples of old relations and replaying them when learning new relations can effectively avoid forgetting However these memory based methods tend to overfit the memory samples and perform poorly on imbalanced datasets To solve these challenges a consistent representation learning method is proposed which maintains the stability of the relation embedding by adopting contrastive learning and knowledge distillation when replaying memory Specifically supervised contrastive learning based on a <a href="https://en.wikipedia.org/wiki/Memory_bank">memory bank</a> is first used to train each new task so that the model can effectively learn the relation representation Then contrastive replay is conducted of the samples in memory and makes the <a href="https://en.wikipedia.org/wiki/Scientific_modelling">model</a> retain the knowledge of historical relations through memory knowledge distillation to prevent the catastrophic forgetting of the old task The proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> can better learn consistent representations to alleviate <a href="https://en.wikipedia.org/wiki/Forgetting">forgetting</a> effectively Extensive experiments on FewRel and TACRED datasets show that our method significantly outperforms state of the art baselines and yield strong robustness on the imbalanced dataset</abstract>
      <url hash="99b3b70e">2022.findings-acl.268</url>
      <bibkey>zhao-etal-2022-consistent</bibkey>
      <pwccode url="https://github.com/thuiar/CRL" additional="false">thuiar/CRL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="270">
      <title>Comprehensive Multi-Modal Interactions for Referring Image Segmentation</title>
      <author><first>Kanishk</first><last>Jain</last></author>
      <author><first>Vineet</first><last>Gandhi</last></author>
      <pages>3427-3435</pages>
      <abstract>We investigate Referring Image Segmentation (RIS), which outputs a segmentation map corresponding to the natural language description. Addressing RIS efficiently requires considering the interactions happening across visual and linguistic modalities and the interactions within each modality. Existing methods are limited because they either compute different forms of interactions sequentially (leading to error propagation) or ignore intra-modal interactions. We address this limitation by performing all three interactions simultaneously through a Synchronous Multi-Modal Fusion Module (SFM). Moreover, to produce refined segmentation masks, we propose a novel Hierarchical Cross-Modal Aggregation Module (HCAM), where linguistic features facilitate the exchange of contextual information across the visual hierarchy. We present thorough ablation studies and validate our approach’s performance on four benchmark datasets, showing considerable performance gains over the existing state-of-the-art (SOTA) methods.</abstract>
      <url hash="bc704ead">2022.findings-acl.270</url>
      <attachment type="software" hash="12479973">2022.findings-acl.270.software.zip</attachment>
      <bibkey>jain-gandhi-2022-comprehensive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/google-refexp">Google Refexp</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/refcoco">RefCOCO</pwcdataset>
    </paper>
    <paper id="272">
      <title>Improving Controllable Text Generation with Position-Aware Weighted Decoding</title>
      <author><first>Yuxuan</first><last>Gu</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Sicheng</first><last>Ma</last></author>
      <author><first>Jiaming</first><last>Wu</last></author>
      <author><first>Heng</first><last>Gong</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>3449-3467</pages>
      <abstract>Weighted decoding methods composed of the pretrained language model (LM) and the controller have achieved promising results for controllable text generation. However, these models often suffer from a control strength/fluency trade-off problem as higher control strength is more likely to generate incoherent and repetitive text. In this paper, we illustrate this trade-off is arisen by the controller imposing the target attribute on the LM at improper positions. And we propose a novel framework based on existing weighted decoding methods called CAT-PAW, which introduces a lightweight regulator to adjust bias signals from the controller at different decoding positions. Experiments on positive sentiment control, topic control, and language detoxification show the effectiveness of our CAT-PAW upon 4 SOTA models.</abstract>
      <url hash="f21d317b">2022.findings-acl.272</url>
      <bibkey>gu-etal-2022-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="275">
      <title>What does it take to bake a cake The RecipeRef corpus and <a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)">anaphora resolution</a> in procedural text<fixed-case>R</fixed-case>ecipe<fixed-case>R</fixed-case>ef corpus and anaphora resolution in procedural text</title>
      <author><first>Biaoyan</first><last>Fang</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>3481-3495</pages>
      <abstract>Procedural text contains rich anaphoric phenomena yet has not received much attention in <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a> To fill this gap we investigate the textual properties of two types of procedural text recipes and chemical patents and generalize an anaphora annotation framework developed for the chemical domain for modeling anaphoric phenomena in recipes We apply this <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> to annotate the RecipeRef corpus with both bridging and coreference relations Through comparison to <a href="https://en.wikipedia.org/wiki/Chemical_patent">chemical patents</a> we show the <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> of anaphora resolution in <a href="https://en.wikipedia.org/wiki/Recipe">recipes</a> We demonstrate empirically that <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> from the chemical domain improves <a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)">resolution of anaphora</a> in recipes suggesting transferability of general procedural knowledge</abstract>
      <url hash="74bab014">2022.findings-acl.275</url>
      <bibkey>fang-etal-2022-take</bibkey>
      <pwccode url="https://github.com/biaoyanf/reciperef" additional="false">biaoyanf/reciperef</pwccode>
    </paper>
    <paper id="276">
      <title><fixed-case>MERI</fixed-case>t: <fixed-case>M</fixed-case>eta-<fixed-case>P</fixed-case>ath <fixed-case>G</fixed-case>uided <fixed-case>C</fixed-case>ontrastive <fixed-case>L</fixed-case>earning for <fixed-case>L</fixed-case>ogical <fixed-case>R</fixed-case>easoning</title>
      <author><first>Fangkai</first><last>Jiao</last></author>
      <author><first>Yangyang</first><last>Guo</last></author>
      <author><first>Xuemeng</first><last>Song</last></author>
      <author><first>Liqiang</first><last>Nie</last></author>
      <pages>3496-3509</pages>
      <abstract>Logical reasoning is of vital importance to natural language understanding. Previous studies either employ graph-based models to incorporate prior knowledge about logical relations, or introduce symbolic logic into neural models through data augmentation. These methods, however, heavily depend on annotated training data, and thus suffer from over-fitting and poor generalization problems due to the dataset sparsity. To address these two problems, in this paper, we propose MERIt, a MEta-path guided contrastive learning method for logical ReasonIng of text, to perform self-supervised pre-training on abundant unlabeled text data. Two novel strategies serve as indispensable components of our method. In particular, a strategy based on meta-path is devised to discover the logical structure in natural texts, followed by a counterfactual data augmentation strategy to eliminate the information shortcut induced by pre-training. The experimental results on two challenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate that our method outperforms the SOTA baselines with significant improvements.</abstract>
      <url hash="dbd58be1">2022.findings-acl.276</url>
      <bibkey>jiao-etal-2022-merit</bibkey>
      <pwccode url="https://github.com/sparkjiao/merit" additional="false">sparkjiao/merit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/logiqa">LogiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
    </paper>
    <paper id="285">
      <title>Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis</title>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Kun</first><last>Zhang</last></author>
      <author><first>Mengdi</first><last>Zhang</last></author>
      <author><first>Hongke</first><last>Zhao</last></author>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Enhong</first><last>Chen</last></author>
      <pages>3599-3610</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a specific aspect in the given sentence. While pre-trained language models such as BERT have achieved great success, incorporating dynamic semantic changes into ABSA remains challenging. To this end, in this paper, we propose to address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we first take the Stack-BERT layers as a primary encoder to grasp the overall semantic of the sentence and then fine-tune it by incorporating a lightweight Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention to a small region of the sentences at each step and re-weigh the vitally important words for better aspect-aware sentiment understanding. Finally, experimental results on three benchmark datasets demonstrate the effectiveness and the rationality of our proposed model and provide good interpretable insights for future semantic modeling.</abstract>
      <url hash="38af9ebf">2022.findings-acl.285</url>
      <bibkey>zhang-etal-2022-incorporating</bibkey>
    </paper>
    <paper id="291">
      <title>Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation</title>
      <author><first>Kevin</first><last>Yang</last></author>
      <author><first>Olivia</first><last>Deng</last></author>
      <author><first>Charles</first><last>Chen</last></author>
      <author><first>Richard</first><last>Shin</last></author>
      <author><first>Subhro</first><last>Roy</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>3685-3695</pages>
      <abstract>We introduce a novel setup for low resource task oriented semantic parsing which incorporates several constraints that may arise in real world scenarios lack of similar datasets models from a related domain inability to sample useful logical forms directly from a <a href="https://en.wikipedia.org/wiki/Grammar">grammar</a> and privacy requirements for unlabeled natural utterances Our goal is to improve a low resource semantic parser using utterances collected through user interactions In this highly challenging but realistic setting we investigate data augmentation approaches involving generating a set of structured canonical utterances corresponding to logical forms before simulating corresponding <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> and filtering the resulting pairs We find that such approaches are effective despite our restrictive setup in a low resource setting on the complex SMCalFlow calendaring dataset Andreas et al we observe relative improvement over a non data augmented baseline in top-1 match</abstract>
      <url hash="750b654d">2022.findings-acl.291</url>
      <bibkey>yang-etal-2022-addressing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/break">BREAK</pwcdataset>
    </paper>
    <paper id="296">
      <title>Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics</title>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>3759-3765</pages>
      <abstract>Question answering-based summarization evaluation metrics must automatically determine whether the QA model’s prediction is correct or not, a task known as answer verification. In this work, we benchmark the lexical answer verification methods which have been used by current QA-based metrics as well as two more sophisticated text comparison methods, BERTScore and LERC. We find that LERC out-performs the other methods in some settings while remaining statistically indistinguishable from lexical overlap in others. However, our experiments reveal that improved verification performance does not necessarily translate to overall QA-based metric quality: In some scenarios, using a worse verification method — or using none at all — has comparable performance to using the best verification method, a result that we attribute to properties of the datasets.</abstract>
      <url hash="8b2f89e5">2022.findings-acl.296</url>
      <bibkey>deutsch-roth-2022-benchmarking</bibkey>
    </paper>
    <paper id="306">
      <title>Chinese Synesthesia Detection New Dataset and Models<fixed-case>C</fixed-case>hinese Synesthesia Detection: New Dataset and Models</title>
      <author><first>Xiaotong</first><last>Jiang</last></author>
      <author><first>Qingqing</first><last>Zhao</last></author>
      <author><first>Yunfei</first><last>Long</last></author>
      <author><first>Zhongqing</first><last>Wang</last></author>
      <pages>3877-3887</pages>
      <abstract>In this paper we introduce a new task called synesthesia detection which aims to extract the sensory word of a sentence and to predict the original and synesthetic sensory modalities of the corresponding sensory word Synesthesia refers to the description of perceptions in one sensory modality through concepts from other modalities It involves not only a <a href="https://en.wikipedia.org/wiki/Phenomenon">linguistic phenomenon</a> but also a <a href="https://en.wikipedia.org/wiki/Cognition">cognitive phenomenon</a> structuring human thought and action which makes it become a bridge between figurative linguistic phenomenon and abstract cognition and thus be helpful to understand the deep semantics To address this we construct a large scale human annotated Chinese synesthesia dataset which contains 7,217 annotated sentences accompanied by sensory words Based on this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> we propose a family of strong and representative baseline models Upon these baselines we further propose a radical based neural network model to identify the boundary of the sensory word and to jointly detect the original and synesthetic sensory modalities for the word Through extensive experiments we observe that the importance of the proposed <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> and dataset can be verified by the statistics and progressive performances In addition our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves state of the art results on the synesthesia dataset</abstract>
      <url hash="8c19f3a7">2022.findings-acl.306</url>
      <bibkey>jiang-etal-2022-chinese</bibkey>
    </paper>
    <paper id="316">
      <title>Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations</title>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Ashwin</first><last>Srinivasan</last></author>
      <author><first>Ankita</first><last>Sharma</last></author>
      <author><first>Damien</first><last>Jose</last></author>
      <author><first>Paul</first><last>Bennett</last></author>
      <pages>4008-4020</pages>
      <abstract>Dense retrieval (DR) methods conduct text retrieval by first encoding texts in the embedding space and then matching them by nearest neighbor search. This requires strong locality properties from the representation space, e.g., close allocations of each small group of relevant texts, which are hard to generalize to domains without sufficient training data. In this paper, we aim to improve the generalization ability of DR models from source training domains with rich supervision signals to target domains without any relevance label, in the zero-shot setting. To achieve that, we propose Momentum adversarial Domain Invariant Representation learning (MoDIR), which introduces a momentum method to train a domain classifier that distinguishes source versus target domains, and then adversarially updates the DR encoder to learn domain invariant representations. Our experiments show that MoDIR robustly outperforms its baselines on 10+ ranking datasets collected in the BEIR benchmark in the zero-shot setup, with more than 10% relative gains on datasets with enough sensitivity for DR models’ evaluation. Source code is available at https://github.com/ji-xin/modir.</abstract>
      <url hash="51ee267a">2022.findings-acl.316</url>
      <bibkey>xin-etal-2022-zero</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/beir">BEIR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="320">
      <title>Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer</title>
      <author><first>Nikolai</first><last>Ilinykh</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>4062-4073</pages>
      <abstract>We explore how a multi-modal transformer trained for generation of longer image descriptions learns syntactic and semantic representations about entities and relations grounded in objects at the level of masked self-attention (text generation) and cross-modal attention (information fusion). We observe that cross-attention learns the visual grounding of noun phrases into objects and high-level semantic information about spatial relations, while text-to-text attention captures low-level syntactic knowledge between words. This concludes that language models in a multi-modal task learn different semantic information about objects and relations cross-modally and uni-modally (text-only). Our code is available here: https://github.com/GU-CLASP/attention-as-grounding.</abstract>
      <url hash="b56b0921">2022.findings-acl.320</url>
      <bibkey>ilinykh-dobnik-2022-attention</bibkey>
      <pwccode url="https://github.com/gu-clasp/attention-as-grounding" additional="false">gu-clasp/attention-as-grounding</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="322">
      <title>Structural Supervision for Word Alignment and <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a></title>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Kai</first><last>Fan</last></author>
      <author><first>Hongjia</first><last>Li</last></author>
      <author><first>Chun</first><last>Yuan</last></author>
      <pages>4084-4094</pages>
      <abstract>Syntactic structure has long been argued to be potentially useful for enforcing accurate <a href="https://en.wikipedia.org/wiki/Word_alignment">word alignment</a> and improving generalization performance of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> Unfortunately existing wisdom demonstrates its significance by considering only the syntactic structure of source tokens neglecting the rich structural information from target tokens and the structural similarity between the source and target sentences In this work we propose to incorporate the syntactic structure of both source and target tokens into the encoder decoder framework tightly correlating the internal logic of <a href="https://en.wikipedia.org/wiki/Word_alignment">word alignment</a> and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> for multi task learning Particularly we wo n’t leverage any annotated syntactic graph of the target side during training so we introduce Dynamic Graph Convolution Networks DGCN on observed target tokens to sequentially and simultaneously generate the target tokens and the corresponding syntactic graphs and further guide the word alignment On this basis Hierarchical Graph Random Walks HGRW are performed on the syntactic graphs of both source and target sides for incorporating structured constraints on machine translation outputs Experiments on four publicly available language pairs verify that our method is highly effective in capturing syntactic structure in different languages consistently outperforming baselines in alignment accuracy and demonstrating promising results in translation quality</abstract>
      <url hash="6d0f7a70">2022.findings-acl.322</url>
      <bibkey>li-etal-2022-structural</bibkey>
    </paper>
    <paper id="325">
      <title>Should We Trust This Summary Bayesian Abstractive Summarization to The Rescue<fixed-case>B</fixed-case>ayesian Abstractive Summarization to The Rescue</title>
      <author><first>Alexios</first><last>Gidiotis</last></author>
      <author><first>Grigorios</first><last>Tsoumakas</last></author>
      <pages>4119-4131</pages>
      <abstract>We explore the notion of uncertainty in the context of modern abstractive summarization models using the tools of <a href="https://en.wikipedia.org/wiki/Deep_learning">Bayesian Deep Learning</a> Our approach approximates <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a> by first extending state of the art summarization models with Monte Carlo dropout and then using them to perform multiple stochastic forward passes Based on <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a> we are able to effectively quantify uncertainty at prediction time Having a reliable uncertainty measure we can improve the experience of the end user by filtering out generated summaries of high uncertainty Furthermore uncertainty estimation could be used as a criterion for selecting samples for annotation and can be paired nicely with active learning and human in the loop approaches Finally <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a> enables us to find a Bayesian summary which performs better than a deterministic one and is more robust to <a href="https://en.wikipedia.org/wiki/Uncertainty">uncertainty</a> In practice we show that our Variational Bayesian equivalents of BART and <a href="https://en.wikipedia.org/wiki/PEGASUS">PEGASUS</a> can outperform their deterministic counterparts on multiple benchmark datasets</abstract>
      <url hash="12a71fd6">2022.findings-acl.325</url>
      <bibkey>gidiotis-tsoumakas-2022-trust</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aeslc">AESLC</pwcdataset>
    </paper>
    <paper id="326">
      <title>On the data requirements of probing</title>
      <author><first>Zining</first><last>Zhu</last></author>
      <author><first>Jixuan</first><last>Wang</last></author>
      <author><first>Bai</first><last>Li</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>4132-4147</pages>
      <abstract>As large and powerful neural language models are developed researchers have been increasingly interested in developing <a href="https://en.wikipedia.org/wiki/Diagnosis">diagnostic tools</a> to probe them There are many papers with conclusions of the form observation X$ is found in model Y$’’ using their own datasets with varying sizes Larger probing datasets bring more reliability but are also expensive to collect There is yet to be a <a href="https://en.wikipedia.org/wiki/Quantitative_research">quantitative method</a> for estimating reasonable probing dataset sizes We tackle this omission in the context of comparing two probing configurations after we have collected a small dataset from a pilot study how many additional data samples are sufficient to distinguish two different configurations We present a novel method to estimate the required number of data samples in such experiments and across several case studies we verify that our estimations have sufficient statistical power Our <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> helps to systematically construct probing datasets to diagnose neural NLP models<tex-math>X</tex-math> is found in model <tex-math>Y</tex-math>”, using their own datasets with varying sizes. Larger probing datasets bring more reliability, but are also expensive to collect. There is yet to be a quantitative method for estimating reasonable probing dataset sizes. We tackle this omission in the context of comparing two probing configurations: after we have collected a small dataset from a pilot study, how many additional data samples are sufficient to distinguish two different configurations? We present a novel method to estimate the required number of data samples in such experiments and, across several case studies, we verify that our estimations have sufficient statistical power. Our framework helps to systematically construct probing datasets to diagnose neural NLP models.</abstract>
      <url hash="3716d5cc">2022.findings-acl.326</url>
      <attachment type="software" hash="e92eb92a">2022.findings-acl.326.software.zip</attachment>
      <bibkey>zhu-etal-2022-data</bibkey>
      <pwccode url="https://github.com/spoclab-ca/probing_dataset" additional="false">spoclab-ca/probing_dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="327">
      <title>Translation Error Detection as Rationale Extraction</title>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>4148-4159</pages>
      <abstract>Recent Quality Estimation QE models based on multilingual pre trained representations have achieved very competitive results in predicting the overall quality of translated sentences However detecting specifically which translated words are incorrect is a more challenging task especially when dealing with limited amounts of training data We hypothesize that not unlike humans successful QE models rely on translation errors to predict overall sentence quality By exploring a set of feature attribution methods that assign relevance scores to the inputs to explain model predictions we study the behaviour of state of the art sentence level QE models and show that explanations i.e. rationales extracted from these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can indeed be used to detect translation errors We therefore i introduce a novel semi supervised method for word level QE and ii propose to use the QE task as a new benchmark for evaluating the plausibility of feature attribution i.e. how interpretable model explanations are to humans</abstract>
      <url hash="35feecd8">2022.findings-acl.327</url>
      <bibkey>fomicheva-etal-2022-translation</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe-pe">MLQE-PE</pwcdataset>
    </paper>
    <paper id="330">
      <title>On Length Divergence Bias in Textual Matching Models</title>
      <author><first>Lan</first><last>Jiang</last></author>
      <author><first>Tianshu</first><last>Lyu</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Meng</first><last>Chong</last></author>
      <author><first>Xiaoyong</first><last>Lyu</last></author>
      <author><first>Dawei</first><last>Yin</last></author>
      <pages>4187-4193</pages>
      <abstract>Despite the remarkable success <a href="https://en.wikipedia.org/wiki/Deep_learning">deep models</a> have achieved in Textual Matching TM tasks it still remains unclear whether they truly understand language or measure the <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a> of texts by exploiting <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">statistical bias</a> in datasets In this work we provide a new perspective to study this issue --- via the length divergence bias We find the length divergence heuristic widely exists in prevalent TM datasets providing direct cues for prediction To determine whether TM models have adopted such <a href="https://en.wikipedia.org/wiki/Heuristic">heuristic</a> we introduce an adversarial evaluation scheme which invalidates the <a href="https://en.wikipedia.org/wiki/Heuristic">heuristic</a> In this adversarial setting all TM models perform worse indicating they have indeed adopted this <a href="https://en.wikipedia.org/wiki/Heuristic">heuristic</a> Through a well designed probing experiment we empirically validate that the bias of TM models can be attributed in part to extracting the text length information during training To alleviate the length divergence bias we propose an adversarial training method The results demonstrate we successfully improve the robustness and generalization ability of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> at the same time</abstract>
      <url hash="fdf34290">2022.findings-acl.330</url>
      <bibkey>jiang-etal-2022-length</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/trecqa">TrecQA</pwcdataset>
    </paper>
    </volume>
</collection>