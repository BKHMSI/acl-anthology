<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.bea">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</booktitle>
      <editor><first>Jill</first><last>Burstein</last></editor>
      <editor><first>Ekaterina</first><last>Kochmar</last></editor>
      <editor><first>Claudia</first><last>Leacock</last></editor>
      <editor><first>Nitin</first><last>Madnani</last></editor>
      <editor><first>Ildikó</first><last>Pilán</last></editor>
      <editor><first>Helen</first><last>Yannakoudakis</last></editor>
      <editor><first>Torsten</first><last>Zesch</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, WA, USA → Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="24dd231d">2020.bea-1</url>
    </meta>
    <frontmatter>
      <url hash="d9b0a62a">2020.bea-1.0</url>
      <bibkey>bea-2020-innovative</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Complementary Systems for Off-Topic Spoken Response Detection</title>
      <author><first>Vatsal</first><last>Raina</last></author>
      <author><first>Mark</first><last>Gales</last></author>
      <author><first>Kate</first><last>Knill</last></author>
      <pages>41–51</pages>
      <abstract>Increased demand to learn English for business and education has led to growing interest in automatic spoken language assessment and teaching systems. With this shift to automated approaches it is important that systems reliably assess all aspects of a candidate’s responses. This paper examines one form of spoken language assessment ; whether the response from the candidate is relevant to the prompt provided. This will be referred to as off-topic spoken response detection. Two forms of previously proposed approaches are examined in this work : the hierarchical attention-based topic model (HATM) ; and the similarity grid model (SGM). The work focuses on the scenario when the prompt, and associated responses, have not been seen in the training data, enabling the <a href="https://en.wikipedia.org/wiki/System">system</a> to be applied to new test scripts without the need to collect data or retrain the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>. To improve the performance of the systems for unseen prompts, <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> based on easy data augmentation (EDA) and translation based approaches are applied. Additionally for the HATM, a form of prompt dropout is described. The <a href="https://en.wikipedia.org/wiki/System">systems</a> were evaluated on both seen and unseen prompts from Linguaskill Business and General English tests. For unseen data the performance of the HATM was improved using <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, in contrast to the <a href="https://en.wikipedia.org/wiki/SGM">SGM</a> where no gains were obtained. The two <a href="https://en.wikipedia.org/wiki/Computer_simulation">approaches</a> were found to be complementary to one another, yielding a combined <a href="https://en.wikipedia.org/wiki/F-number">F0.5 score</a> of 0.814 for off-topic response detection where the prompts have not been seen in training.</abstract>
      <url hash="a7612ffd">2020.bea-1.4</url>
      <doi>10.18653/v1/2020.bea-1.4</doi>
      <bibkey>raina-etal-2020-complementary</bibkey>
    </paper>
    <paper id="5">
      <title>CIMA : A Large Open Access Dialogue Dataset for Tutoring<fixed-case>CIMA</fixed-case>: A Large Open Access Dialogue Dataset for Tutoring</title>
      <author><first>Katherine</first><last>Stasaski</last></author>
      <author><first>Kimberly</first><last>Kao</last></author>
      <author><first>Marti A.</first><last>Hearst</last></author>
      <pages>52–64</pages>
      <abstract>One-to-one tutoring is often an effective means to help students learn, and recent experiments with neural conversation systems are promising. However, large open datasets of tutoring conversations are lacking. To remedy this, we propose a novel asynchronous method for collecting tutoring dialogue via crowdworkers that is both amenable to the needs of deep learning algorithms and reflective of pedagogical concerns. In this approach, extended conversations are obtained between crowdworkers role-playing as both students and tutors. The CIMA collection, which we make publicly available, is novel in that students are exposed to overlapping grounded concepts between exercises and multiple relevant tutoring responses are collected for the same input. CIMA contains several compelling properties from an educational perspective : student role-players complete exercises in fewer turns during the course of the conversation and tutor players adopt strategies that conform with some educational conversational norms, such as providing hints versus asking questions in appropriate contexts. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> enables a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> to be trained to generate the next tutoring utterance in a conversation, conditioned on a provided action strategy.</abstract>
      <url hash="01bc5caa">2020.bea-1.5</url>
      <doi>10.18653/v1/2020.bea-1.5</doi>
      <bibkey>stasaski-etal-2020-cima</bibkey>
    </paper>
    <paper id="6">
      <title>Becoming Linguistically Mature : Modeling English and German Children’s Writing Development Across School Grades<fixed-case>E</fixed-case>nglish and <fixed-case>G</fixed-case>erman Children’s Writing Development Across School Grades</title>
      <author><first>Elma</first><last>Kerz</last></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Daniel</first><last>Wiechmann</last></author>
      <author><first>Marcus</first><last>Ströbel</last></author>
      <pages>65–74</pages>
      <abstract>In this paper we employ a novel approach to advancing our understanding of the development of writing in English and German children across school grades using classification tasks. The <a href="https://en.wikipedia.org/wiki/Data">data</a> used come from two recently compiled corpora : The English data come from the the GiC corpus (983 school children in second-, sixth-, ninth- and eleventh-grade) and the German data are from the FD-LEX corpus (930 school children in fifth- and ninth-grade). The key to this paper is the combined use of what we refer to as ‘complexity contours’, i.e. series of measurements that capture the progression of linguistic complexity within a text, and Recurrent Neural Network (RNN) classifiers that adequately capture the sequential information in those contours. Our experiments demonstrate that RNN classifiers trained on complexity contours achieve higher classification accuracy than one trained on text-average complexity scores. In a second step, we determine the relative importance of the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> from four distinct categories through a Sensitivity-Based Pruning approach.</abstract>
      <url hash="b544b2f5">2020.bea-1.6</url>
      <doi>10.18653/v1/2020.bea-1.6</doi>
      <attachment type="Dataset" hash="6275ac98">2020.bea-1.6.Dataset.pdf</attachment>
      <bibkey>kerz-etal-2020-becoming</bibkey>
    </paper>
    <paper id="8">
      <title>Can <a href="https://en.wikipedia.org/wiki/Neural_network">Neural Networks</a> Automatically Score Essay Traits?</title>
      <author><first>Sandeep</first><last>Mathias</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>85–91</pages>
      <abstract>Essay traits are attributes of an essay that can help explain how well written (or badly written) the essay is. Examples of traits include <a href="https://en.wikipedia.org/wiki/Content_(media)">Content</a>, Organization, <a href="https://en.wikipedia.org/wiki/Language">Language</a>, Sentence Fluency, <a href="https://en.wikipedia.org/wiki/Word_choice">Word Choice</a>, etc. A lot of research in the last decade has dealt with automatic holistic essay scoring-where a machine rates an essay and gives a score for the essay. However, writers need feedback, especially if they want to improve their writing-which is why trait-scoring is important. In this paper, we show how a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep-learning based system</a> can outperform feature-based machine learning systems, as well as a string kernel system in scoring <a href="https://en.wikipedia.org/wiki/Essay">essay traits</a>.</abstract>
      <url hash="7665989d">2020.bea-1.8</url>
      <doi>10.18653/v1/2020.bea-1.8</doi>
      <bibkey>mathias-bhattacharyya-2020-neural</bibkey>
    </paper>
    <paper id="12">
      <title>Applications of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a> in Bilingual Language Teaching : An Indonesian-English Case Study<fixed-case>I</fixed-case>ndonesian-<fixed-case>E</fixed-case>nglish Case Study</title>
      <author><first>Zara</first><last>Maxwelll-Smith</last></author>
      <author><first>Simón</first><last>González Ochoa</last></author>
      <author><first>Ben</first><last>Foley</last></author>
      <author><first>Hanna</first><last>Suominen</last></author>
      <pages>124–134</pages>
      <abstract>Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this <a href="https://en.wikipedia.org/wiki/Data">data</a> so rich and problematic to classify. In this paper, we set out methodological considerations of using <a href="https://en.wikipedia.org/wiki/Speech_recognition">automated speech recognition</a> to build a <a href="https://en.wikipedia.org/wiki/Speech_corpus">corpus of teacher speech</a> in an Indonesian language classroom. Our preliminary results (64 % word error rate) suggest these tools have the potential to speed <a href="https://en.wikipedia.org/wiki/Data_collection">data collection</a> in this context. We provide practical examples of our data structure, details of our piloted computer-assisted processes, and fine-grained error analysis. Our study is informed and directed by genuine research questions and discussion in both the education and computational linguistics fields. We highlight some of the benefits and risks of using these emerging <a href="https://en.wikipedia.org/wiki/Technology">technologies</a> to analyze the complex work of <a href="https://en.wikipedia.org/wiki/Language_education">language teachers</a> and in <a href="https://en.wikipedia.org/wiki/Education">education</a> more generally.</abstract>
      <url hash="dd31f883">2020.bea-1.12</url>
      <doi>10.18653/v1/2020.bea-1.12</doi>
      <bibkey>maxwelll-smith-etal-2020-applications</bibkey>
    </paper>
    <paper id="13">
      <title>An empirical investigation of neural methods for content scoring of science explanations</title>
      <author><first>Brian</first><last>Riordan</last></author>
      <author><first>Sarah</first><last>Bichler</last></author>
      <author><first>Allison</first><last>Bradford</last></author>
      <author><first>Jennifer</first><last>King Chen</last></author>
      <author><first>Korah</first><last>Wiley</last></author>
      <author><first>Libby</first><last>Gerard</last></author>
      <author><first>Marcia</first><last>C. Linn</last></author>
      <pages>135–144</pages>
      <abstract>With the widespread adoption of the Next Generation Science Standards (NGSS), science teachers and online learning environments face the challenge of evaluating students’ integration of different dimensions of <a href="https://en.wikipedia.org/wiki/Science_education">science learning</a>. Recent advances in representation learning in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> have proven effective across many <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing tasks</a>, but a rigorous evaluation of the relative merits of these methods for scoring complex constructed response formative assessments has not previously been carried out. We present a detailed empirical investigation of feature-based, recurrent neural network, and pre-trained transformer models on scoring content in real-world formative assessment data. We demonstrate that recent <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural methods</a> can rival or exceed the performance of feature-based methods. We also provide evidence that different classes of neural models take advantage of different learning cues, and pre-trained transformer models may be more robust to spurious, dataset-specific learning cues, better reflecting scoring rubrics.</abstract>
      <url hash="c81cf5e9">2020.bea-1.13</url>
      <doi>10.18653/v1/2020.bea-1.13</doi>
      <bibkey>riordan-etal-2020-empirical</bibkey>
    </paper>
    <paper id="16">
      <title>GECToR   Grammatical Error Correction : Tag, Not Rewrite<fixed-case>GECT</fixed-case>o<fixed-case>R</fixed-case> – Grammatical Error Correction: Tag, Not Rewrite</title>
      <author><first>Kostiantyn</first><last>Omelianchuk</last></author>
      <author><first>Vitaliy</first><last>Atrasevych</last></author>
      <author><first>Artem</first><last>Chernodub</last></author>
      <author><first>Oleksandr</first><last>Skurzhanskyi</last></author>
      <pages>163–170</pages>
      <abstract>In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages : first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model / ensemble GEC tagger achieves an F_0.5 of 65.3/66.5 on CONLL-2014 (test) and F_0.5 of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.</abstract>
      <url hash="7ed8a32f">2020.bea-1.16</url>
      <doi>10.18653/v1/2020.bea-1.16</doi>
      <bibkey>omelianchuk-etal-2020-gector</bibkey>
      <pwccode url="https://github.com/grammarly/gector" additional="true">grammarly/gector</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
    <title_ar>GECToR - تصحيح الخطأ النحوي: الوسم وليس إعادة الكتابة</title_ar>
      <title_pt>GECToR – Correção de erros gramaticais: Tag, não reescrever</title_pt>
      <title_es>GeCtor — Corrección de errores gramaticales: etiquetar, no reescribir</title_es>
      <title_ja>GECToR –文法的誤り訂正:タグ、書き換えなし</title_ja>
      <title_hi>GECToR - व्याकरण त्रुटि सुधार: टैग, नहीं फिर से लिखना</title_hi>
      <title_zh>GECToR – 语法纠错曰:表,非重写也</title_zh>
      <title_ga>GECToR – Earráid Ghramadaí a cheartú: Clib, Ní Athscríobh</title_ga>
      <title_ka>GECToR</title_ka>
      <title_hu>GECToR - Nyelvtani hibajavítás: Címke, Nem újraírás</title_hu>
      <title_el>Διόρθωση γραμματικού σφάλματος: ετικέτα, όχι επαναγραφή</title_el>
      <title_it>GECToR - Correzione degli errori grammaticali: Tag, Non riscrivere</title_it>
      <title_kk>GECToR - Грамматикалық қатені түзету: тег, қайта жазу емес</title_kk>
      <title_ms>GECToR - Pembetulan Ralat Grammatik: Tag, Tidak Tulis Semula</title_ms>
      <title_ml>GECToR - ഗ്രാമാറ്റിക്കല്‍ പിശക് തിരിച്ചറിയുന്നത്: ടാഗ്, വീണ്ടും എഴുതുന്നതല്ല</title_ml>
      <title_mt>GECToR - Korrezzjoni ta’ Żball Grammatiku: Tag, Mhux Rikiteb</title_mt>
      <title_no>GECToR – Gramatisk feilretting: merkelapp, ikkje skriv på nytt</title_no>
      <title_mn>GECToR</title_mn>
      <title_pl>Korekta błędów gramatycznych GECToR: znacznik, nie przepisywanie</title_pl>
      <title_ro>GECToR - Corecția erorilor gramaticale: Etichetă, nu rescrie</title_ro>
      <title_sr>GECToR</title_sr>
      <title_si>GECToR - ග්‍රාමාටික් වැරදි සුදුසුම: ටැග්, නැවත ලියන්න එපා</title_si>
      <title_so>GECToR - Heshiiska galmada grammatical: Tag, Not Rewrite</title_so>
      <title_sv>GECToR - Grammatisk felkorrigering: Tagg, Inte skriva om</title_sv>
      <title_ur>GECToR - Grammatical Error Correction: Tag, Not Rewrite</title_ur>
      <title_ta>GECToR - சிறந்த பிழை திருத்தம்: ஒட்டு, மீண்டும் எழுதாது</title_ta>
      <title_mk>GECToR - Grammatical Error Correction: Tag, Not Rewrite</title_mk>
      <title_lt>GECToR - Gramatinės klaidos korekcija: ženklas, neišrašyti iš naujo</title_lt>
      <title_vi>Lỗi biểu tượng: thẻ, không phải phục hồi</title_vi>
      <title_uz>GECToR - Grammatical Error Correction: Tag, Not Rewrite</title_uz>
      <title_hr>GECToR - Gramatična korekcija greške: značka, ne ponovno pisati</title_hr>
      <title_nl>GECToR-grammaticale foutcorrectie: Tag, niet herschrijven</title_nl>
      <title_da>GECToR - Grammatisk fejlkorrektion: Tag, Ikke omskrive</title_da>
      <title_de>GECToR-Grammatical Error Correction: Tag, Not Rewrite</title_de>
      <title_id>GECToR - Koreksi Galat Grammatis: Tag, Tidak Tulis ulang</title_id>
      <title_ko>문법 오류 수정: 다시 쓰기 대신 표시</title_ko>
      <title_sw>GECToR - Uharibifu wa Tamko: Tagha, Si Rewrite</title_sw>
      <title_bg>Граматическа корекция на грешки: етикет, не пренаписване</title_bg>
      <title_sq>GECToR - Korrektimi i Gabimeve Gramatike: Tag, Not Rewrite</title_sq>
      <title_fa>GECToR - اصلاح خطای Grammatical: Tag, not Rewrite</title_fa>
      <title_af>Stencils</title_af>
      <title_hy>Comment</title_hy>
      <title_am>ምስሉን በሌላ ስም አስቀምጥ</title_am>
      <title_az>GECToR</title_az>
      <title_bs>GECToR - Gramatična korekcija greške: etiketa, ne ponovno pisati</title_bs>
      <title_bn>GECToR - গ্রামাটিক্যাল ত্রুটি সংশোধন: ট্যাগ, পুনরায় লেখা নয়</title_bn>
      <title_tr>GECToR</title_tr>
      <title_cs>Oprava gramatických chyb GECToR: značka, ne přepsat</title_cs>
      <title_et>GECToR - grammatiline vigade parandamine: silt, mitte ümber kirjutada</title_et>
      <title_fi>GECToR - Kielioppinen virheenkorjaus: Tag, Ei uudelleenkirjoittaa</title_fi>
      <title_ca>GECToR - Correcció d'Errors Gramàtics: Etiqueta, No Reescriure</title_ca>
      <title_jv>GENDOR - Gramatik Eror Ngubah: tag, Not Rewrite</title_jv>
      <title_sk>GECToR - Slovnični popravek napak: Oznaka, Ne ponovno napiši</title_sk>
      <title_he>GECToR - תיקון שגיאה גרמטית: Tag, Not Rewrite</title_he>
      <title_bo>GECToR - Grammatical Error Correction: Tag, Not Rewrite</title_bo>
      <title_ha>KCharselect unicode block name</title_ha>
      <abstract_ar>في هذا البحث ، نقدم أداة تحديد تسلسل GEC بسيطة وفعالة باستخدام مشفر محول. تم تدريب نظامنا مسبقًا على البيانات التركيبية ومن ثم صقله على مرحلتين: أولاً على مجموعة خاطئة ، وثانيًا على مجموعة من المؤسسات المتوازية الخاطئة والخالية من الأخطاء. نحن نصمم تحويلات مخصصة على مستوى الرمز المميز لتعيين رموز الإدخال المميزة للتصحيحات المستهدفة. يحقق أفضل أداة تمييز GEC أحادية الطراز / مجموعة لدينا F_0.5 من 65.3 / 66.5 في CONLL-2014 (اختبار) و F_0.5 من 72.4 / 73.6 في BEA-2019 (اختبار). تصل سرعته الاستدلالية إلى 10 أضعاف سرعة نظام seq2seq GEC القائم على المحولات.</abstract_ar>
      <abstract_pt>Neste artigo, apresentamos um tagger de sequência GEC simples e eficiente usando um codificador Transformer. Nosso sistema é pré-treinado em dados sintéticos e, em seguida, ajustado em duas etapas: primeiro em corpora com erros, e segundo em uma combinação de corpora paralelos com erros e sem erros. Projetamos transformações personalizadas em nível de token para mapear tokens de entrada para correções de destino. Nosso melhor marcador GEC de modelo único/conjunto alcança um F_0.5 de 65.3/66.5 no CONLL-2014 (teste) e F_0.5 de 72.4/73.6 no BEA-2019 (teste). Sua velocidade de inferência é até 10 vezes mais rápida que um sistema GEC seq2seq baseado em Transformer.</abstract_pt>
      <abstract_es>En este artículo, presentamos un etiquetador de secuencias GEC simple y eficiente que utiliza un codificador Transformer. Nuestro sistema se entrena previamente en datos sintéticos y luego se ajusta en dos etapas: primero en los cuerpos con errores y, en segundo lugar, en una combinación de cuerpos paralelos con errores y sin errores. Diseñamos transformaciones personalizadas a nivel de token para asignar tokens de entrada a correcciones de objetivos. Nuestro mejor etiquetador GEC de un solo modelo/conjunto logra una F_0.5 de 65.3/66.5 en CONLL-2014 (prueba) y F_0.5 de 72.4/73.6 en BEA-2019 (prueba). Su velocidad de inferencia es hasta 10 veces más rápida que la de un sistema GEC seq2seq basado en Transformer.</abstract_es>
      <abstract_zh>本文中,发一用变压器编码器简高效GEC序识器。 合数预训练之,然后分两调:一曰谬语料库,次曰过差并行语料库。 设计自定义令牌级转,以将输令牌映射到的更正。 莫若单/集成 GEC 标记器于 CONLL-2014(试)上得 65.3/66.5 F_0.5,于 BEA-2019(试)得 72.4/73.6 F_0.5。 其推理速度,盖变压器之seq2seq GEC10倍之。</abstract_zh>
      <abstract_ja>本稿では、トランスフォーマーエンコーダを用いた簡便かつ効率的なＧＥＣシーケンスタグを提示する。当社のシステムは、合成データの事前トレーニングを受け、2段階で微調整されています。1段階目は誤ったコーパス、2段階目は誤ったパラレルコーパスと誤りのないパラレルコーパスの組み合わせです。カスタムトークンレベル変換を設計して、入力トークンをターゲット修正にマッピングします。当社の最高のシングルモデル/アンサンブルGECタガーは、CONLL -2014 （試験）でF_0.5の65.3/66.5、BEA -2019 （試験）でF_0.5の72.4/73.6を達成しています。その推論速度は、トランスフォーマーベースのseq 2 seq GECシステムの最大10倍の速度です。</abstract_ja>
      <abstract_hi>इस पेपर में, हम एक ट्रांसफॉर्मर एनकोडर का उपयोग करके एक सरल और कुशल जीईसी अनुक्रम टैगर पेश करते हैं। हमारी प्रणाली को सिंथेटिक डेटा पर पूर्व-प्रशिक्षित किया जाता है और फिर दो चरणों में ठीक-ठाक किया जाता है: पहला त्रुटिपूर्ण कॉर्पोरेट पर, और दूसरा त्रुटिपूर्ण और त्रुटि-मुक्त समानांतर कॉर्पोरेट के संयोजन पर। हम लक्ष्य सुधारों के लिए इनपुट टोकन मैप करने के लिए कस्टम टोकन-स्तर परिवर्तनों को डिज़ाइन करते हैं। हमारा सबसे अच्छा एकल मॉडल / पहनावा जीईसी टैगर CONLL-2014 (परीक्षण) पर 65.3 / 66.5 का एक F_0.5 और बीईए -2019 (परीक्षण) पर 72.4 / 73.6 का F_0.5 प्राप्त करता है। इसकी अनुमान गति एक ट्रांसफॉर्मर-आधारित seq2seq GEC प्रणाली के रूप में 10 गुना तेज है।</abstract_hi>
      <abstract_ga>Sa pháipéar seo, cuirimid i láthair clibeálaí seicheamh GEC simplí agus éifeachtach ag baint úsáide as ionchódóir Trasfhoirmeoir. Déantar ár gcóras a réamhoiliúint ar shonraí sintéiseacha agus ansin mionchoigeartaithe in dhá chéim: ar dtús ar chorpora earráideach, agus sa dara háit ar mheascán de chorpora comhthreomhara earráideach agus saor ó earráidí. Dearaímid claochluithe leibhéal dearbhán chun comharthaí ionchuir a mhapáil chun ceartúcháin a spriocdhíriú. Baineann ár gclibálaí GEC aon-samhail/ensemble is fearr amach F_0.5 de 65.3/66.5 ar CONLL-2014 (tástáil) agus F_0.5 de 72.4/73.6 ar BEA-2019 (tástáil). Tá a luas tátal suas le 10 n-uaire chomh tapa le córas seq2seq GEC atá bunaithe ar Trasfhoirmeoir.</abstract_ga>
      <abstract_ka>ჩვენ ამ კაურაში გავაჩვენოთ ერთადერთი და ეფექტიური GEC წერტილების ჭდერი, რომელიც გამოყენებული ტრანფორმაციის კოდერის გამოყენებული. ჩვენი სისტემა სინტეტიკური მონაცემებზე უბრალოდ განაკეთებულია და შემდეგ ორი ფაეში უბრალოდ განაკეთებულია: პირველი შეცდომა კოპორაზე და მეორე შეცდომა და შეცდომა დაკავშირებული პარალ ჩვენ განსაკუთრებული ტექნონის დონეზე გარემოქმედებით გარემოქმედებით, რომლებიც შეიყვანის ტექნონის გარემოქმედებისთვის. ჩვენი ყველაზე საუკეთესო მოდელ/ანსემბლის GEC ტეგერი 65.3/66.5-ის F_0.5-ს CONLL-2014 (ტესტი) და F_0.5-ის 72.4/73.6-ზე BEA-2019 (ტესტი). მისი ინფრენციის სიჩქარე 10-ჯერ უფრო სიჩქარე, როგორც ტრანფორმეტრის ბაზეული seq2seq GEC სისტემა.</abstract_ka>
      <abstract_hu>Ebben a tanulmányban egy egyszerű és hatékony GEC szekvencia címkézőt mutatunk be Transformer kódolóval. Rendszerünket előkészítettük a szintetikus adatokra, majd két szakaszban finomhangoljuk: először a hibás corpora, a második pedig a hibás és hibátlan párhuzamos corpora kombinációján. Egyéni tokenszintű átalakításokat tervezünk, hogy a beviteli tokeneket a céljavításokhoz térképezzük fel. Legjobb egymodell/együttes GEC címkézőnk elérte az F_0.5-et a 65.3/66.5-ből a CONLL-2014-en (teszt) és az F_0.5-et a 72.4/73.6-ból a BEA-2019-en (teszt). Következtetési sebessége akár 10-szer olyan gyors, mint egy Transformer alapú seq2seq GEC rendszer.</abstract_hu>
      <abstract_el>Σε αυτή την εργασία, παρουσιάζουμε έναν απλό και αποτελεσματικό δείκτη ακολουθίας που χρησιμοποιεί έναν κωδικοποιητή μετασχηματιστή. Το σύστημά μας είναι προ-εκπαιδευμένο σε συνθετικά δεδομένα και στη συνέχεια συντονισμένο σε δύο στάδια: πρώτον σε εσφαλμένα σώματα, και δεύτερον σε συνδυασμό εσφαλμένων και χωρίς σφάλματα παράλληλων σωμάτων. Σχεδιάζουμε προσαρμοσμένους μετασχηματισμούς επιπέδου σήματος για να αντιστοιχίσουμε τα σήματα εισόδου στις διορθώσεις στόχων. Το καλύτερο μονο-μοντέλο/σύνολο μας επιτυγχάνει ένα F_0.5 65.3/66.5 σε CONLL-2014 (δοκιμή) και F_0.5 72.4/73.6 σε BEA-2019 (δοκιμή). Η ταχύτητα συναγωγής του είναι έως και δέκα φορές μεγαλύτερη από ένα σύστημα GEC βασισμένο στον μετασχηματιστή.</abstract_el>
      <abstract_it>In questo articolo, presentiamo un tag di sequenza GEC semplice ed efficiente utilizzando un encoder Transformer. Il nostro sistema è pre-addestrato su dati sintetici e poi perfezionato in due fasi: primo su corpora errorful e secondo su una combinazione di corpora parallela errorful e senza errori. Progettiamo trasformazioni personalizzate a livello di token per mappare i token di input alle correzioni mirate. Il nostro miglior tagger GEC monomodello/ensemble raggiunge un F_0.5 su 65.3/66.5 su CONLL-2014 (test) e un F_0.5 su 72.4/73.6 su BEA-2019 (test). La sua velocità di inferenza è fino a 10 volte più veloce di un sistema GEC seq2seq basato su Transformer.</abstract_it>
      <abstract_kk>Бұл қағазда қарапайым және эффективті GEC реттеу тегжерін түрлендіруші кодерін қолдану үшін көрсетеді. Біздің жүйеміз синтетикалық деректерге алдын- ала оқылған, кейін екі этап бойынша дұрыс түзетілген: біріншіден қате корпорада, екіншіден қате және қате бос параллелі корпорада біріктірілген. Кіріс белгілерін мақсатты түзету үшін өзгерту үшін өзгертіміз. Біздің ең жалғыз үлгі/сәйкестік GEC тегжеріміз CONLL-2014 (сынақ) және BEA-2019 (сынақтағы) 72,4/73,6 (сынақтағы) F_0,5 дегенді жеткізеді. Түрлендіру жылдамдығы, Түрлендіруші негізделген seq2seq GEC жүйесіне 10 рет жылдам.</abstract_kk>
      <abstract_mk>Во овој весник, претставуваме едноставен и ефикасен геЦ секвенциски означувач користејќи трансформер кодер. Нашиот систем е предобучен на синтетички податоци, а потоа фино прилагоден во две фази: прво на грешна корпора, и второ на комбинација на грешна и безгрешна паралелна корпора. We design custom token-level transformations to map input tokens to target corrections.  Нашиот најдобар единствен модел/ансембл ГЕЦ означувач постигнува F_0.5 од 65.3/66.5 на CONLL-2014 (тест) и F_0.5 од 72.4/73.6 на BEA-2019 (тест). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.</abstract_mk>
      <abstract_ms>In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder.  Sistem kita dilatih-dilatih pada data sintetik dan kemudian ditetapkan dalam dua tahap: pertama pada corpora yang salah, dan kedua pada kombinasi dari corpora paralel yang salah dan bebas ralat. Kami merancang perubahan aras token suai untuk peta token input ke penyesuaian sasaran. Tagger GEC tunggal-model/ensemble terbaik kami mencapai F_0.5 dari 65.3/66.5 pada CONLL-2014 (ujian) dan F_0.5 dari 72.4/73.6 pada BEA-2019 (ujian). Kelajuan kesimpulannya sehingga 10 kali lebih cepat daripada sistem GEC berasaskan Transformer seq2seq.</abstract_ms>
      <abstract_ml>ഈ പത്രത്തില്‍, നമ്മള്‍ ഒരു എളുപ്പമുള്ള ജിസി സെക്സഞ്ചര്‍ ടാഗ്ഗര്‍ കൊണ്ടുവരുന്നു. ട്രാന്‍സ്ഫോര്‍മാന്‍ കോഡെ നമ്മുടെ സിസ്റ്റത്തിന് മുന്‍പ് പരിശീലിക്കപ്പെട്ടിരിക്കുന്നു. പിന്നീട് രണ്ട് സ്റ്റേജില്‍ മുഴുവന്‍ പരിശീലിക്കപ്പെട്ടിരിക്കുന്നു. ആദ് നമ്മള്‍ സ്വന്തം ടോക്ക് നില മാറ്റങ്ങള്‍ സൃഷ്ടിക്കുന്നു. ക്രമീകരണങ്ങള്‍ക്ക് ലക്ഷ്യം വരുത്താനുള്ള അടയാ Our best single-model/ensemble GEC tagger achieves an F_0.5 of 65.3/66.5 on CONLL-2014 (test) and F_0.5 of 72.4/73.6 on BEA-2019 (test).  അതിന്റെ വേഗത 10 പ്രാവശ്യം വരെ ട്രാന്‍സ്ഫോര്‍മാര്‍ അടിസ്ഥാനമായ സെക്ക്2seq GEC സിസ്റ്റം പോലെയാണ്.</abstract_ml>
      <abstract_mn>Энэ цаасан дээр бид энгийн болон эффективны GEC дарааллын тегжер ашиглаж байна. Бидний систем синтетик өгөгдлийн талаар урьд суралцаж, дараа нь хоёр дахь дахь сайжруулагдсан. Эхлээд алдаа гарсан корпора, хоёр дахь алдаа болон алдаа гарагүй параллел корпора дээр холбогдсон. Бид хувилбарын тонны түвшин өөрчлөлтийг зориулалтын зориулалтыг газрын зураг зураг хийдэг. Бидний хамгийн шилдэг нэг загвар/загвар GEC тэмдэглэгчид CONLL-2014 оны F_0.5 болон F_0.5 болон F_0.5 нь 72.4/73.6 болон BEA-2019 оны F_0.5 (шалгалт). Үүний халдварын хурд нь Трансфер-д суурилсан seq2seq GEC системээс 10 дахин хурдан байна.</abstract_mn>
      <abstract_mt>F’dan id-dokument, qed nippreżentaw tag sempliċi u effiċjenti tas-sekwenza GEC bl-użu ta’ kodifikatur Transformer. Is-sistema tagħna hija mħarrġa minn qabel fuq dejta sintetika u mbagħad imfassla f’żewġ stadji: l-ewwel fuq corpora żbaljata, u t-tieni fuq kombinazzjoni ta’ corpora parallela żbaljata u mingħajr żbalji. Aħna niddisinjaw trasformazzjonijiet personali fil-livell tat-tokens biex nimmappjaw it-tokens tal-input għall-korrezzjonijiet fil-mira. Our best single-model/ensemble GEC tagger achieves an F_0.5 of 65.3/66.5 on CONLL-2014 (test) and F_0.5 of 72.4/73.6 on BEA-2019 (test).  Il-veloċità tal-inferenza tagħha hija sa 10 darbiet aktar mgħa ġġla minn sistema tal-GEC seg2seq ibbażata fuq it-Transformer.</abstract_mt>
      <abstract_no>I denne papiret viser vi ein enkel og effektiv GEC-sekvensmerker med ein transformeringskoder. Sistemet vårt er først trent på syntetiske data og så fint opp i to stader: først på feil korpora, og andre på ein kombinasjon av feil og feil-fri parallelle korpora. Vi design eigendefinerte transformasjonar for tokennivå til å kartera inntekens til målrettingar. Vårt beste enkelmodell/ensemble GEC-tagger oppnår ein F_0,5 av 65,3/66,5 på CONLL-2014 (test) og F_0,5 av 72,4/73,6 på BEA-2019 (test). Den inferensningsfarten er opp til 10 ganger så rask som eit transformeringsbasert seq2seq GEC-system.</abstract_no>
      <abstract_ro>În această lucrare, prezentăm un etichetor de secvență GEC simplu și eficient folosind un encoder Transformer. Sistemul nostru este pre-instruit pe date sintetice și apoi reglat fin în două etape: în primul rând pe corpore erorful, și în al doilea rând pe o combinație de corpore paralele erorful și fără erori. Proiectăm transformări personalizate la nivel de token pentru a mapa token-urile de intrare pentru corecțiile țintite. Cel mai bun etichetator GEC single-model/ansamblu obține un F_0.5 din 65.3/66.5 pe CONLL-2014 (test) și F_0.5 din 72.4/73.6 pe BEA-2019 (test). Viteza sa de deducere este de până la 10 ori mai rapidă decât un sistem GEC bazat pe Transformer.</abstract_ro>
      <abstract_pl>W artykule przedstawiamy prosty i wydajny tager sekwencji GEC z wykorzystaniem kodera Transformera. Nasz system jest wstępnie przeszkolony na podstawie danych syntetycznych, a następnie dostrojony w dwóch etapach: pierwszy na korpusach błędnych, a drugi na kombinacji korpusów równoległych bez błędów. Projektujemy niestandardowe transformacje na poziomie tokenów, aby mapować tokeny wejściowe do poprawek docelowych. Nasz najlepszy pojedynczy model/zespół tagera GEC osiąga F_0.5 od 65.3/66.5 na CONLL-2014 (test) i F_0.5 od 72.4/73.6 na BEA-2019 (test). Jego prędkość wnioskowania jest nawet 10-krotnie szybsza niż system GEC oparty na Transformerze.</abstract_pl>
      <abstract_sr>U ovom papiru predstavljamo jednostavan i efikasan oznake GEC sekvence koristeći koder transformera. Naš sistem je predobučen za sintetičke podatke, a zatim je ispravno sređen na dva faza: prvo na pogrešnoj korpori, a drugo na kombinaciji grešne i bezgrešne paralelne korpore. Mi dizajniramo transformacije na nivou znakova za mapu ulaznih znakova na ciljne korekcije. Naš najbolji jedinstveni model/ensemble GEC tagger postiže F_0,5 od 65,3/66,5 na CONLL-2014 (test) i F_0,5 od 72,4/73,6 na BEA-2019 (test). Njegova brzina infekcije je do 10 puta brže kao transformer-based seq2seq GEC sistem.</abstract_sr>
      <abstract_si>මේ පැත්තේ, අපි සාමාන්‍ය සහ හැකියාවක් GEC ක්‍රමාණය ටැගර් එකක් පෙන්වන්නේ. අපේ පද්ධතිය සංවිධාන දත්තේ ප්‍රධානය කරලා පස්සේ සංවිධානය දෙන්න ප්‍රධානය කරලා තියෙනවා: මුලින්ම වැරදි කොර්පෝරා වලින් ප්‍ අපි කැමතියි ටොකෙන් ලේවල් වෙනස් විදිහට අක්‍රියාත්මක වෙනුවෙන් ඉලක්ෂිත විදිහට ඇතුළු ටො අපේ හොඳම ප්‍රමාණය/පරීක්ෂණය GEC ටැග්ජර් එකට CONLL-2014 (පරීක්ෂණය) වල F_0.5 of 65.3/66.5 (පරීක්ෂණය) සහ F_0.5 of 72.4/73.6 on BEA-2019 (පරීක්ෂණය). එයාගේ පරීක්ෂණ වේගය 10 ක් වේගයෙන් වේගයෙන් ප්‍රවේගකයෙන් ආධාරිත seq2seq GEC පද්ධතියක් වගේ.</abstract_si>
      <abstract_so>Qoraalkan waxaynu ku soo bandhignaynaa tagger fudud oo faa’iido leh oo ku isticmaalaya qoraal turjumid ah. nidaamkayaga waxaa horay loo tababariyey macluumaadka synthetika, kadibna waxaa lagu sharraxay labada marxaladood: marka hore shirkadda qaladka ah, labaadna waxaa lagu soo bandhigaa shirkadda qaladka ah oo aan qaladka lahayn. Waxaynu u qornaa isbedelka sawirada calaamada ee caadiga ah si aan ugu sawiro hagitaanka. Jardiino ugu wanaagsan GEC tagger wuxuu gaadhaa F_0.5 oo ka mid ah 65.3/66.5 oo ku qoran CONLL-2014 (test) iyo F_0.5 oo ka mid ah 72.4/73.6 BEA-2019 (test). Fasaxa cudurku wuxuu gaadhaa ilaa 10 jeer sida nidaamka GEC ee ku saleysan turjumista 2seq.</abstract_so>
      <abstract_lt>In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder.  Mūsų sistema iš anksto apmokoma sintetiniais duomenimis, o po to patobulinta dviem etapais: pirma klaidinga korpora, antra klaidinga ir klaidinga lygiagreti korpora deriniu. Mes suprojektuojame pritaikytus žymenų lygio pokyčius, kad žymenų įvedimo žymenų žemėlapį būtų galima atlikti tikslines korekcijas. Mūsų geriausias vieno modelio ir (arba) komplekso GEC žymeklis atitinka 65.3/66.5 punkto F_0.5 CONLL-2014 (bandymas) ir 72.4/73.6 punkto F_0.5 BEA-2019 (bandymas). Jo išvados greitis yra ne greitesnis kaip 10 kartų greitesnis už Transformer pagrįstą sek2seq GEC sistemą.</abstract_lt>
      <abstract_sv>I denna uppsats presenterar vi en enkel och effektiv GEC sekvenstaggare med hjälp av en Transformer encoder. Vårt system är förinstallerat på syntetiska data och finjusterat i två steg: först på errorful corpora, och andra på en kombination av errorful och felfria parallella corpora. Vi designar anpassade transformationer på token-nivå för att kartlägga inmatningstokens till målkorrigeringar. Vår bästa engångsmodell/ensemble GEC tagger uppnår en F_0.5 av 65.3/66.5 på CONLL-2014 (test) och F_0.5 av 72.4/73.6 på BEA-2019 (test). Dess sluthastighet är upp till 10 gånger så snabb som ett Transformer-baserat sek2seq GEC-system.</abstract_sv>
      <abstract_ur>اس کاغذ میں ہم ایک ساده اور فعال GEC سطح ٹاگر کو ترنسفور اکڈر کے مطابق پیش کرتے ہیں. ہمارا سیستم سینٹٹیسی ڈیٹی پر پہلے آموزش کی گئی ہے اور پھر دو مرحلے میں ٹھیک ترکیب کی گئی ہے: پہلے غلطی شرکت پر، اور دوسرے مرتبہ غلطی اور غلطی آزاد شرکت پر۔ ہم تخصوصی ٹوکین سطح کی تغییرات طراحی کرتے ہیں تاہل اصلاح کے لئے اینپیٹ ٹوکینوں کو نقشه بنانے کے لئے۔ ہمارے سب سے بہترین single-model/ensemble GEC ٹاگر کو CONLL-2014 پر F_0.5 اور F_0.5 کو BEA-2019 میں 72.4/73.6 (امتحان) حاصل کرتا ہے۔ اس کی نازل کی गति 10 بار زیادہ تیز ہے ترنسفور-بنیاد سیک2سک GEC سیسٹم کی طرح.</abstract_ur>
      <abstract_ta>இந்த காகிதத்தில், நாம் ஒரு எளிய மற்றும் தேவையான GEC பின்னணி ஒட்டிக்கொண்டு வருகிறோம். எங்கள் அமைப்பு கூட்டிணைப்பு தகவல் முன் பயிற்சி செய்யப்பட்டுள்ளது மற்றும் இரண்டு நிலைகளில் நன்றாக முடிக்கப்பட்டது: முதலில் தவறான நிறுவனத்தில் ம நாம் தனிப்பயன் குறியீடு நிலை மாற்றங்களை வடிவமை எங்கள் சிறந்த ஒற்றை மாதிரி/ensemble GEC ஒட்டிக்காட்டி 65.3/66.5 ல் ஒரு F_ 0.5 கிடைக்கும் CONLL- 2014 (சோதனை) மற்றும் BEA-2019 (சோதனையில் 72.4/73.6 ல் F_ 0.5 பெறுக அதன் பின்னணி வேகம் 10 முறை மாற்றுதல் அடிப்படையிலுள்ள பின்னணி GEC முறைமையாக இருக்கும்.</abstract_ta>
      <abstract_vi>Trong tờ giấy này, chúng tôi giới thiệu một máy tạo chuỗi GEC đơn giản và hiệu quả sử dụng bộ mã hóa biến hình. Hệ thống của chúng tôi được huấn luyện trước về dữ liệu nhân tạo và sau đó hoàn chỉnh theo hai giai đoạn: đầu tiên là cơ thể lỗi, và thứ hai là sự kết hợp giữa cơ thể song song lỗi và sai lầm. Thiết lập biến dạng biểu tượng để vẽ bản đồ những thẻ nhập để sửa mục tiêu. Cách mô phỏng duy nhất của chúng ta có thể đạt được một cỡ cỡ lớn C5 của 85.3/6. Tốc độ nhận ra của nó lên tới mười lần nhanh như một hệ thống phát dịch độc lập.</abstract_vi>
      <abstract_uz>Bu qogʻozda, biz Transformer encoder yordamida oddiy va effektiv GEC sequence tagger bilan ishlatimiz. Bizning tizimmiz birinchi tizimdan foydalanadi va keyin ikki darajada yaxshi bog'langan: birinchi xato kompaniyada, keyin xato va xato bilan parametrlarni birlashtirishda. Tekshirish uchun tugmalar birikmasini yaratish. Bizning eng yuqori bitta model/ensemble GEC tagger 65.3/66.5 da CONLL-2014 (test) va BEA-2019da 72.4/73.6 (test) uchun F_0.5 ta'ni topadi. Ko'rinishi tezligi Transformer asosida seq2seq GEC tizimi sifatida 10 marta tez.</abstract_uz>
      <abstract_bg>В тази статия представяме прост и ефективен маркер за последователност, използващ трансформаторен кодер. Системата ни е предварително обучена за синтетични данни и след това фина настройка на два етапа: първо за грешни корпуси и второ за комбинация от грешни и безгрешни паралелни корпуси. Ние проектираме персонализирани трансформации на ниво токен, за да картографираме входните токени за целеви корекции. Най-добрият ни маркер постига F_0.5 от 65.3/66.5 на тест и F_0.5 от 72.4/73.6 на тест. Скоростта на заключението му е до 10 пъти по-бърза от базираната на трансформатор GEC система.</abstract_bg>
      <abstract_nl>In dit artikel presenteren we een eenvoudige en efficiënte GEC sequence tagger met behulp van een Transformer encoder. Ons systeem is vooraf getraind op synthetische gegevens en vervolgens verfijnd in twee fasen: ten eerste op foutieve corpora, en ten tweede op een combinatie van foutloze en foutloze parallelle corpora. We ontwerpen aangepaste transformaties op tokenniveau om invoertokens toe te wijzen aan doelcorrecties. Onze beste single-model/ensemble GEC tagger behaalt een F_0.5 van 65.3/66.5 op CONLL-2014 (test) en F_0.5 van 72.4/73.6 op BEA-2019 (test). De inferentiesnelheid is tot tien keer zo snel als een Transformer-gebaseerd seq2seq GEC-systeem.</abstract_nl>
      <abstract_da>I denne artikel præsenterer vi en enkel og effektiv GEC sekvens tagger ved hjælp af en Transformer encoder. Vores system er præ-trænet på syntetiske data og derefter finjusteret i to faser: først på errorful corpora, og anden på en kombination af fejlfri og fejlfri parallelle corpora. Vi designer brugerdefinerede token-niveau transformationer for at kortlægge input tokens til målrettede korrektioner. Vores bedste single-model/ensemble GEC tagger opnår en F_0.5 af 65.3/66.5 på CONLL-2014 (test) og F_0.5 af 72.4/73.6 på BEA-2019 (test). Dens sluthastighed er op til 10 gange så hurtig som et Transformer-baseret sek2seq GEC system.</abstract_da>
      <abstract_de>In diesem Beitrag stellen wir einen einfachen und effizienten GEC Sequence Tagger mit einem Transformer Encoder vor. Unser System ist auf synthetischen Daten vortrainiert und anschließend in zwei Stufen fein abgestimmt: erstens auf fehlerhafte Korpora und zweitens auf eine Kombination fehlerhafter und fehlerfreier paralleler Korpora. Wir entwerfen benutzerdefinierte Transformationen auf Tokenebene, um Eingabetoken Zielkorrekturen zuzuordnen. Unser bester Single-Model/Ensemble GEC Tagger erreicht eine F_0.5 von 65.3/66.5 auf CONLL-2014 (Test) und F_0.5 von 72.4/73.6 auf BEA-2019 (Test). Seine Inferenzgeschwindigkeit ist bis zu 10-mal so schnell wie ein Transformer-basiertes seq2seq GEC-System.</abstract_de>
      <abstract_hr>U ovom papiru predstavljamo jednostavan i učinkovit označavač GEC sekvencije koristeći koder transformera. Naš sustav je predobučen na sintetičkim podacima, a zatim je ispravno sređen na dva faza: prvo na pogrešnoj tijelo, a drugo na kombinaciji greške i bezgreške paralelne tijelo. Mi dizajniramo transformacije na razini znakova kako bi mapirali ulazne znakove na ciljne korekcije. Naš najbolji jedinstveni model/ensemble GEC tagger postiže F_0,5 od 65,3/66,5 na CONLL-2014 (test) i F_0,5 od 72,4/73,6 na BEA-2019 (test). Njezina brzina infekcije je do 10 puta brže kao sustav GEC na transformeri seq2seq.</abstract_hr>
      <abstract_ko>본고에서 우리는 간단하고 효과적인 GEC 시퀀스 표기기 사용 변압기 인코더를 제시했다.우리 시스템은 합성 데이터에 대해 미리 훈련한 다음에 두 단계로 나누어 미세하게 조정한다. 첫 번째 단계는 오류 자료 라이브러리이고 두 번째 단계는 오류와 무오류 평행 자료 라이브러리의 조합이다.우리는 맞춤형 영패 등급 전환을 설계하여 입력한 영패를 목표 수정에 비추었다.우리의 가장 좋은 단모형/전체 GEC 표기기는 CONLL-2014(테스트)에서 65.3/66.5의 F 0.5, BEA-2019(테스트)에서 72.4/73.6의 F 0.5에 달한다.그 추리 속도는 변압기 기반 seq2seq GEC 시스템의 10배에 달한다.</abstract_ko>
      <abstract_sw>Katika karatasi hii, tunaweka alama nyepesi na yenye ufanisi wa mfululizo wa GEC kwa kutumia kodi ya Transformer. Our system is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora.  Tunaweza kutengeneza mabadiliko ya kiwango cha alama kwa ajili ya ramani ishara za input ili kulenga sahihi. Bendera yetu ya GEC yenye mfano mzuri zaidi imefanikiwa F_0.5 ya 65.3/66.5 kwenye CONLL-2014 (jaribio) na F_0.5 ya 72.4/73.6 kwenye BEA-2019 (jaribio). Upunguzo wake umefikia mara 10 kwa haraka kama mfumo wa GEC wa zamani wa sekq2seq.</abstract_sw>
      <abstract_tr>Bu kagyzda basit we etkinlik bir GEC terjime tägleri tans edip görkezip Bizim sistemimiz sintetik verilerde öňünden öňünden eğitildi ve iki taýdan geçirildi: ilkinji hata korporasynda, ikincisi hata ve hata boş paralel korporasynda. Biz şahsy token-derejesi düzenlemek üçin girdi teknisini hedefi düzeltmek üçin bejerdik Biziň iň gowy tek-modelimiz/ensemble GEC taggerimiz CONLL-2014 (test) we BEA-2019 (test) üstünde F_0.5 we F_0.5 we F_0.5 we 72.4/73.6 we BEA-2019 (test). Onuň ýüregi ýigrenişi terjime edip seq2seq GEC sistemi ýaly 10 gezek ýigrendir.</abstract_tr>
      <abstract_af>In hierdie papier, voorsien ons 'n eenvoudige en effektief GEC sekwensiemerker met gebruik van 'n Transformer enkoder. Ons stelsel is voor-opgelei op sintetiese data en dan fin-tuned in twee stadige: eerste op foutlike korpora, en tweede op 'n kombinasie van foutlike en foutlike parallele korpora. Ons ontwerp pasmaak token-vlak transformasies na kaart invoer tokens na doel korreksies. Ons beste enkel-model/ensemble GEC-etiket bereik 'n F_0.5 van 65.3/66.5 op CONLL-2014 (toets) en F_0.5 van 72.4/73.6 op BEA-2019 (toets). Sy inferensie spoed is tot 10 maal so vinnig as 'n Transformer-gebaseerde seq2seq GEC stelsel.</abstract_af>
      <abstract_sq>Në këtë letër, ne paraqesim një shënues të thjeshtë dhe efektiv të sekuencës GEC duke përdorur një kodues Transformer. Sistemi ynë është i stërvitur në të dhënat sintetike dhe pastaj i rregulluar në dy faza: së pari në corpora të gabuara, dhe i dyti në një kombinim të corpora paralele të gabuara dhe pa gabime. Ne dizajnojmë transformime të personalizuara në nivel token për të hartuar token e hyrjes në objektiv korreksione. Tagger ynë më i mirë i një modeli/ensemble GEC arrin një F_0.5 të 65.3/66.5 në CONLL-2014 (test) dhe F_0.5 të 72.4/73.6 në BEA-2019 (test). Shpejtësia e inferencës së saj është deri në 10 herë më e shpejtë se një sistem SEq2seq GEC me bazë në Transformer.</abstract_sq>
      <abstract_id>Dalam kertas ini, kami mempersembahkan tag urutan GEC sederhana dan efisien menggunakan pengekode Transformer. Sistem kita dilatih-dilatih pada data sintetis dan kemudian disesuaikan dalam dua tahap: pertama pada corpora yang salah, dan kedua pada kombinasi dari corpora paralel yang salah dan tanpa kesalahan. We design custom token-level transformations to map input tokens to target corrections.  Tagger GEC model/ensemble terbaik kami mencapai F_0.5 dari 65.3/66.5 pada CONLL-2014 (tes) dan F_0.5 dari 72.4/73.6 pada BEA-2019 (tes). Kecepatan kesimpulannya sampai 10 kali lebih cepat dari sistem SEq2seq GEC berdasarkan Transformer.</abstract_id>
      <abstract_hy>Այս թղթի մեջ մենք ներկայացնում ենք պարզ և արդյունավետ ԳԵԿ հաջորդականության նշան՝ օգտագործելով Թանֆորմերի կոդեր: Մեր համակարգը նախապատրաստված է սինթետիկ տվյալների վրա և հետո բարձրացված է երկու փուլում. առաջինը սխալ մարմնի վրա, երկրորդը սխալ և անսխալ զուգահեռ մարմնի համադրման վրա: Մենք նախագծում ենք հնարավոր նշանների մակարդակի վերափոխություններ, որպեսզի քարտեզագրենք նշանների մուտքագրման նշանները նպատակային ուղղումների համար: Մեր ամենալավ մեկ մոդել-համակարգ ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդ Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.</abstract_hy>
      <abstract_fa>در این کاغذ، ما با استفاده از یک رمزنگار تغییر دهنده یک تاجر ردیابی GEC ساده و موثر را نشان می دهیم. سیستم ما پیش از این روی داده های سناتیک آموزش داده شده و سپس در دو مرحله درست شده است: اول روی شرکت اشتباهی و دوم روی ترکیب شرکت اشتباهی و اشتباهی آزاد است. ما تغییرات سطح معجزه‌های شخصی را طراحی می‌کنیم تا نشانه‌های ورودی را به سمت اصلاح هدف نقشه بگیریم. بهترین نمونه‌ی single-model/ensemble GEC tagger ما یک F_0.5 از 65.3/66.5 در CONLL-2014 (آزمایش) و F_0.5 از 72.4/73.6 در BEA-2019 (آزمایش) می‌رسد. سرعت آلودگی آن تا ۱۰ بار سریع تر از سیستم GEC بر اساس تغییر دهنده است.</abstract_fa>
      <abstract_bn>এই কাগজটিতে আমরা একটি সাধারণ এবং কার্যকর জিসি সেকেন্স ট্যাগার উপস্থাপন করি ট্রান্সফার্ন এনকোডার ব্যবহার করে। আমাদের সিস্টেম সিন্টেটিক ডাটা সম্পর্কে পূর্বে প্রশিক্ষণ প্রদান করা হয়েছে এবং তারপর দুই স্তরে সুন্দর করা হয়েছে: প্রথমে ভুল কর্পোরা এবং দ্বিতীয় ভুল এব We design custom token-level transformations to map input tokens to target corrections.  আমাদের সবচেয়ে ভালো মডেল/এনস্পেল জেসি ট্যাগার কনএল-২০১৪ (পরীক্ষা) অর্জন করে ৬৫. ট্রান্সফ্রান্সফার ভিত্তিক সেক্স২সেক জিসি সিস্টেম হিসেবে এর আক্রান্ত গতি ১০ বার দ্রুত।</abstract_bn>
      <abstract_ca>En aquest article presentem un etiquetador de seqüència GEC simple i eficient utilitzant un codificador Transformer. El nostre sistema està pré-entrenat en dades sintètiques i després fine-tuned en dues etapes: primer en corpora errònia, i segon en una combinació de corpora paral·lela errònia i sense errors. Desenyem transformacions personalitzades a nivell de fitxes per mapejar fitxes d'entrada per a corregir els objectius. El nostre millor etiquetador GEC amb un únic model/ensemble obté un F_0,5 de 65,3/66,5 a CONLL-2014 (test) i un F_0,5 de 72,4/73,6 a BEA-2019 (test). La velocitat d'inferència és fins a 10 vegades més ràpida que un sistema seq2seq GEC basat en Transformer.</abstract_ca>
      <abstract_am>በዚህ ፕሮግራም ቀላል እና አካባቢ የGEC ስካታዊ ምልክት እናቀርባለን፡፡ የስህተታችን ስህተት በሁለት ደረጃዎች ላይ ተማርቷል፡፡ We design custom token-level transformations to map input tokens to target corrections.  የእኛ ትልቁ ዓይነት/ምሳሌ/የGEC ጋልጋሪ 65.3/66.5 ከ65.3/66.5 ከCONLL-2014 (ፈተና) እና የ72.4/73.6 BEA-2019 (ፈተና) F_0.5 አግኝቷል፡፡ የውጤት ፍጥረት እጥፍ ወደ 10 እጥፍ ይደርሳል፡፡</abstract_am>
      <abstract_bs>U ovom papiru predstavljamo jednostavan i efikasan označavač GEC sekvence koristeći koder transformera. Naš sistem je predobučen za sintetičke podatke, a zatim je ispravno sređen na dva faza: prvo na pogrešnoj korpori, a drugo na kombinaciji grešnog i bezgrešnog paralelnog korpora. Mi dizajniramo transformacije na nivou znakova na mapu ulaznih znakova na ciljne korekcije. Naš najbolji jedinstveni model/ensemble GEC tagger postiže F_0,5 od 65.3/66.5 na CONLL-2014 (test) i F_0,5 od 72.4/73.6 na BEA-2019 (test). Njegova brzina infekcije je do 10 puta brže kao sustav GEC na transformeri seq2seq.</abstract_bs>
      <abstract_az>Bu kańüńĪzda, transformer kodlayńĪcńĪnńĪ kullanarak basit v…ô effektiv GEC sequence tagger g√∂st…ôririk. Sistemimiz sintetik m…ôlumatlardan …ôvv…ôl t…ôhsil edilmiŇüdir v…ô sonra iki d…ôf…ô d√ľz…ôldilmiŇüdir: ilk yanlńĪŇü korpora, ikincisi yanlńĪŇü v…ô hatasńĪz paralel korpora birl…ôŇüdirilmiŇüdir. Biz m√ľ…ôyy…ôn edilmiŇü token-seviyy…ôti transformasiyalarńĪnńĪ m…ôqs…ôd d√ľz…ôltm…ôy…ô g√∂nd…ôrdik. Bizim …ôn yaxŇüńĪ modell…ôrimiz/ensemble GEC etiket√ßisi CONLL-2014 (test) v…ô BEA-2019 (test) bar…ôsind…ô 72.4/73.6'un F_0.5-ini tapńĪr. Onun x…ôst…ôlik hńĪzlńĪńüńĪ Transformer tabanlńĪ seq2seq GEC sistemi kimi 10 d…ôf…ô hńĪzlńĪdńĪr.</abstract_az>
      <abstract_cs>V tomto článku představujeme jednoduchý a efektivní GEC sekvenční tagger pomocí snímače Transformer. Náš systém je předem trénován na syntetických datech a poté jemně laděn ve dvou fázích: první na chybných korpusech a druhé na kombinaci chybných a bezchybných paralelních korpusů. Navrhujeme vlastní transformace na úrovni tokenů pro mapování vstupních tokenů k cílovým opravám. Náš nejlepší single-model/ensemble GEC tagger dosahuje F_0.5 z 65.3/66.5 na CONLL-2014 (test) a F_0.5 z 72.4/73.6 na BEA-2019 (test). Rychlost inference je až desetkrát rychlejší než systém seq2seq GEC založený na Transformeru.</abstract_cs>
      <abstract_fi>Tässä työssä esittelemme yksinkertaisen ja tehokkaan GEC-sekvenssitaggerin käyttäen Transformer-kooderia. Järjestelmämme on esikoulutettu synteettisestä datasta ja sen jälkeen hienosäädetty kahdessa vaiheessa: ensin virheellisillä korpusilla ja toiseksi virheellisten ja virheettömien rinnakkaiskorpusten yhdistelmällä. Suunnittelemme mukautetut token-tason muunnokset karttaan syöttötokenit kohdistamaan korjauksia. Paras yhden mallin/kokoonpanon GEC-tagger saavuttaa F_0.5 65.3/66.5 CONLL-2014 (testi) ja F_0.5 72.4/73.6 BEA-2019 (testi). Sen päättelynopeus on jopa 10 kertaa nopeampi kuin Transformer-pohjainen seq2seq GEC järjestelmä.</abstract_fi>
      <abstract_et>Käesolevas töös tutvustame lihtsat ja tõhusat GEC jada sildistajat, kasutades Transformer kodeerijat. Meie süsteem on eelnevalt koolitatud sünteetilistele andmetele ja seejärel peenhäälestatud kahes etapis: esiteks ekslike korpuste ja teiseks ekslike ja vigadeta paralleelkorpuste kombinatsiooni. Me kujundame kohandatud tokeni tasemel muundusi, et kaardistada sisendtokenid sihtparanduste jaoks. Meie parim ühe mudeli/ansambli GEC sildistaja saavutab F_0.5 65.3/66.5 CONLL-2014 (test) ja F_0.5 72.4/73.6 BEA-2019 (test). Selle järelduskiirus on kuni 10 korda kiirem kui Transformer-põhine seq2seq GEC süsteem.</abstract_et>
      <abstract_jv>Nyong mapur iki, kita mulai un ajeng-ajeng GEC seng pangan ning koder Transformer. Sistem-siji sing ditambahak kelas-kelas nang data sinantesik lan sak gewis dipun-kelas telu wae: sampeyan tanggal kuwi corompora sing gak bener We design custom token-scale transformations to map input token to goal rections. Awake singular-model/ensamble GEC tagger success Digambungan kanggo kalih-kalih sabanjuré teka sakjane kanggo sistem seq2seq GEC sing basa gambar n' Transformer.</abstract_jv>
      <abstract_he>בעיתון הזה, אנחנו מציגים תג רצף GEC פשוט ויעיל באמצעות קודד טרנספורר. המערכת שלנו מאומנת מראש על נתונים סינטטיים ואז מתאימה בשני שלבים: ראשית על גופורה שגויה, ושנייה על שילוב של גופורה קבועה שגויה ללא שגויות. אנחנו מעצבים שינויים מתאימים ברמה של סימנים כדי למפות סימנים כניסה לתיקונים מטרה. Our best single-model/ensemble GEC tagger achieves an F_0.5 of 65.3/66.5 on CONLL-2014 (test) and F_0.5 of 72.4/73.6 on BEA-2019 (test).  מהירות ההנחה שלה עד 10 פעמים מהירה יותר ממערכת GEC המבוססת על Transformer seq2seq.</abstract_he>
      <abstract_ha>Ga wannan takardan, Munã gaurar da wata tagger mai sauƙi da mai amfani da wani kode ta Transformer. Ana tafiyar da na'urarmu a gaba ɗaya a kan data na synthetic, sa'an nan kuma aka gyãra mai kyau a cikin daraja biyu: farkon a kan koropa mai ɓarna, da na ƙarƙashin a komai da wata shirin karkacin da aka samu wata shirin wata na ɓata ba-da-ɓata. Kana ƙayyade jujjuya masu shida cikin shirin ayuka na ɗabi'a zuwa map ayukan inputi don ya yi amfani da shiryarwa. Babu mafi kyaun motsi/ensemble GEC tagger na sãmu F_0.5 daga 65.3/66.5 kan CONLL-2014 (jarrabo) da F_0.5 daga 197.4/73.6 kan BEA-2019 (jarrabãwa). Gaskiya ya kai 10 sau kasu kamar wata na GPS ta Transformer-based seq2seq.</abstract_ha>
      <abstract_bo>འོག་གི་ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོས་སྔོན་པོ་ཞིག ང་ཚོའི་མ་ལག་ནི་འདིའི་སྔོན་གྲངས་སྒྲིག་ཆ་འཕྲིན་ལས་རང་ཉིད་ཀྱི་གནས་སྟངས་གཉིས་ཀྱི་ནང་དུ་བཏོན་ཡོད། We design custom token-level transformations to map input tokens to target corrections. ང་ཚོའི་མ་དབྱིབས་གཅིག་གི་རྣམ་པ་ཞིག་དང་མཚོན་རྟགས་GEC tagger་གིས་ CONLL-2014 (བརྟག་ཞིབ་) ཐོག་ལས་ F_0.5 གནད་སྒྲིག BEA-2019 (བརྟག་ཞིབ་)ལས་F_0.5 གནད་སྒྲིག འདིའི་མཐའ་འཁོར་གྱི་མགྱོགས་ཚད་ལྡན་འགྱུར་བ་དང་གཞི་བརྟེན་ནས་seq2seq GEC་མ་ལག་གི་འགྱུར་ཚད་ལྡན་10ཡིས་མཐོ་རེད།</abstract_bo>
      <abstract_sk>V tem prispevku predstavljamo preprost in učinkovit označevalnik zaporedja GEC z uporabo transformatorskega kodirnika. Naš sistem je predhodno usposobljen za sintetične podatke in nato natančno nastavljen v dveh fazah: prvi na napačnih korpusih, drugi na kombinaciji napačnih in brez napak vzporednih korpusov. Oblikujemo transformacije na ravni žetonov po meri za kartiranje vhodnih žetonov za ciljne popravke. Naš najboljši enomodel označevalec GEC doseže F_0.5 65.3/66.5 na CONLL-2014 (test) in F_0.5 72.4/73.6 na BEA-2019 (test). Njegova sklepna hitrost je do 10-krat hitrejša kot transformatorski sistem seq2seq GEC.</abstract_sk>
      </paper>
    <paper id="17">
      <title>Interpreting Neural CWI Classifiers’ Weights as Vocabulary Size<fixed-case>CWI</fixed-case> Classifiers’ Weights as Vocabulary Size</title>
      <author><first>Yo</first><last>Ehara</last></author>
      <pages>171–176</pages>
      <abstract>Complex Word Identification (CWI) is a task for the identification of words that are challenging for <a href="https://en.wikipedia.org/wiki/Second-language_acquisition">second-language learners</a> to read. Even though the use of neural classifiers is now common in CWI, the interpretation of their parameters remains difficult. This paper analyzes neural CWI classifiers and shows that some of their parameters can be interpreted as <a href="https://en.wikipedia.org/wiki/Vocabulary_size">vocabulary size</a>. We present a novel formalization of vocabulary size measurement methods that are practiced in the applied linguistics field as a kind of neural classifier. We also contribute to building a novel <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for validating vocabulary testing and readability via <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a>.</abstract>
      <url hash="f7390621">2020.bea-1.17</url>
      <doi>10.18653/v1/2020.bea-1.17</doi>
      <attachment type="Dataset" hash="de00ecff">2020.bea-1.17.Dataset.zip</attachment>
      <bibkey>ehara-2020-interpreting</bibkey>
    </paper>
    <paper id="20">
      <title>Predicting the Difficulty and Response Time of Multiple Choice Questions Using Transfer Learning</title>
      <author><first>Kang</first><last>Xue</last></author>
      <author><first>Victoria</first><last>Yaneva</last></author>
      <author><first>Christopher</first><last>Runyon</last></author>
      <author><first>Peter</first><last>Baldwin</last></author>
      <pages>193–197</pages>
      <abstract>This paper investigates whether <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> can improve the prediction of the difficulty and response time parameters for 18,000 multiple-choice questions from a high-stakes medical exam. The type the signal that best predicts difficulty and <a href="https://en.wikipedia.org/wiki/Response_time_(technology)">response time</a> is also explored, both in terms of <a href="https://en.wikipedia.org/wiki/Abstraction_(computer_science)">representation abstraction</a> and item component used as input (e.g., whole item, answer options only, etc.). The results indicate that, for our sample, <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> can improve the prediction of item difficulty when <a href="https://en.wikipedia.org/wiki/Mental_chronometry">response time</a> is used as an auxiliary task but not the other way around. In addition, difficulty was best predicted using signal from the item stem (the description of the clinical case), while all parts of the item were important for predicting the response time.</abstract>
      <url hash="1f9e95f0">2020.bea-1.20</url>
      <doi>10.18653/v1/2020.bea-1.20</doi>
      <bibkey>xue-etal-2020-predicting</bibkey>
    </paper>
    </volume>
</collection>