<?xml version='1.0' encoding='utf-8'?>
<collection id="2022.acl">
  <volume id="long" ingest-date="2022-05-15">
    <meta>
      <booktitle>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</booktitle>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Aline</first><last>Villavicencio</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="b8317652">2022.acl-long</url>
    </meta>
    <frontmatter>
      <url hash="56ea4e43">2022.acl-long.0</url>
      <bibkey>acl-2022-association-linguistics-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>AdapLeR Speeding up Inference by Adaptive Length Reduction<fixed-case>A</fixed-case>dap<fixed-case>L</fixed-case>e<fixed-case>R</fixed-case>: Speeding up Inference by Adaptive Length Reduction</title>
      <author><first>Ali</first><last>Modarressi</last></author>
      <author><first>Hosein</first><last>Mohebbi</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>1-15</pages>
      <abstract>Pre trained language models have shown stellar performance in various <a href="https://en.wikipedia.org/wiki/Downstream_(networking)">downstream tasks</a> But this usually comes at the cost of high latency and <a href="https://en.wikipedia.org/wiki/Computation">computation</a> hindering their usage in resource limited settings In this work we propose a novel approach for reducing the <a href="https://en.wikipedia.org/wiki/Computational_cost">computational cost</a> of BERT with minimal loss in downstream performance Our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> dynamically eliminates less contributing tokens through layers resulting in shorter lengths and consequently lower <a href="https://en.wikipedia.org/wiki/Computational_cost">computational cost</a> To determine the importance of each token representation we train a Contribution Predictor for each layer using a gradient based saliency method Our experiments on several diverse classification tasks show speedups up to 22x during inference time without much sacrifice in performance We also validate the quality of the selected tokens in our method using human annotations in the ERASER benchmark In comparison to other widely used strategies for selecting important tokens such as <a href="https://en.wikipedia.org/wiki/Salience_(neuroscience)">saliency</a> and <a href="https://en.wikipedia.org/wiki/Attention">attention</a> our proposed method has a significantly lower <a href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">false positive rate</a> in generating rationales Our code is freely available at https://github.com/amodaresi/AdapLeR.</abstract>
      <url hash="a3b4bfc3">2022.acl-long.1</url>
      <attachment type="software" hash="fc212cbc">2022.acl-long.1.software.zip</attachment>
      <bibkey>modarressi-etal-2022-adapler</bibkey>
      <pwccode url="https://github.com/amodaresi/adapler" additional="false">amodaresi/adapler</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hatexplain">HateXplain</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="14">
      <title>An Unsupervised Multiple-Task and Multiple-Teacher Model for Cross-lingual Named Entity Recognition</title>
      <author><first>Zhuoran</first><last>Li</last></author>
      <author><first>Chunming</first><last>Hu</last></author>
      <author><first>Xiaohui</first><last>Guo</last></author>
      <author><first>Junfan</first><last>Chen</last></author>
      <author><first>Wenyi</first><last>Qin</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <pages>170-179</pages>
      <abstract>Cross-lingual named entity recognition task is one of the critical problems for evaluating the potential transfer learning techniques on low resource languages. Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer. However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domains. Other possible auxiliary tasks to improve the learning performance have not been fully investigated. In this study, based on the knowledge distillation framework and multi-task learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on the target domain. Specifically, an entity recognizer and a similarity evaluator are first trained in parallel as two teachers from the source domain. Then, two tasks in the student model are supervised by these teachers simultaneously. Empirical studies on the three datasets across 7 different languages confirm the effectiveness of the proposed model.</abstract>
      <url hash="07233ed3">2022.acl-long.14</url>
      <attachment type="software" hash="bc84f15b">2022.acl-long.14.software.zip</attachment>
      <bibkey>li-etal-2022-unsupervised-multiple</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="15">
      <title>Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature</title>
      <author><first>Gianluca</first><last>Moro</last></author>
      <author><first>Luca</first><last>Ragazzi</last></author>
      <author><first>Lorenzo</first><last>Valgimigli</last></author>
      <author><first>Davide</first><last>Freddi</last></author>
      <pages>180-189</pages>
      <abstract>Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks, they still struggle to address multi-input tasks such as multi-document summarization. Many solutions truncate the inputs, thus ignoring potential summary-relevant contents, which is unacceptable in the medical domain where each information can be vital. Others leverage linear model approximations to apply multi-input concatenation, worsening the results because all information is considered, even if it is conflicting or noisy with respect to a shared background. Despite the importance and social impact of medicine, there are no ad-hoc solutions for multi-document summarization. For this reason, we propose a novel discriminative marginalized probabilistic method (DAMEN) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization. Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews. Moreover, we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method.</abstract>
      <url hash="0c1f23ed">2022.acl-long.15</url>
      <bibkey>moro-etal-2022-discriminative</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>MISC</fixed-case>: A Mixed Strategy-Aware Model integrating <fixed-case>COMET</fixed-case> for Emotional Support Conversation</title>
      <author><first>Quan</first><last>Tu</last></author>
      <author><first>Yanran</first><last>Li</last></author>
      <author><first>Jianwei</first><last>Cui</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>308-319</pages>
      <abstract>Applying existing methods to emotional support conversation—which provides valuable assistance to people who are in need—has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user’s instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user’s distress. To address the problems, we propose a novel model <tex-math>\textbf{MISC}</tex-math>, which firstly infers the user’s fine-grained emotional status, and then responds skillfully using a mixture of strategy. Experimental results on the benchmark dataset demonstrate the effectiveness of our method and reveal the benefits of fine-grained emotion understanding as well as mixed-up strategy modeling.</abstract>
      <url hash="5c5ad498">2022.acl-long.25</url>
      <bibkey>tu-etal-2022-misc</bibkey>
      <pwccode url="https://github.com/morecry/misc" additional="false">morecry/misc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="30">
      <title>Cross-Utterance Conditioned <fixed-case>VAE</fixed-case> for Non-Autoregressive Text-to-Speech</title>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Cheng</first><last>Yu</last></author>
      <author><first>Guangzhi</first><last>Sun</last></author>
      <author><first>Hua</first><last>Jiang</last></author>
      <author><first>Fanglei</first><last>Sun</last></author>
      <author><first>Weiqin</first><last>Zu</last></author>
      <author><first>Ying</first><last>Wen</last></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Jun</first><last>Wang</last></author>
      <pages>391-400</pages>
      <abstract>Modelling prosody variation is critical for synthesizing natural and expressive speech in end-to-end text-to-speech (TTS) systems. In this paper, a cross-utterance conditional VAE (CUC-VAE) is proposed to estimate a posterior probability distribution of the latent prosody features for each phoneme by conditioning on acoustic features, speaker information, and text features obtained from both past and future sentences. At inference time, instead of the standard Gaussian distribution used by VAE, CUC-VAE allows sampling from an utterance-specific prior distribution conditioned on cross-utterance information, which allows the prosody features generated by the TTS system to be related to the context and is more similar to how humans naturally produce prosody. The performance of CUC-VAE is evaluated via a qualitative listening test for naturalness, intelligibility and quantitative measurements, including word error rates and the standard deviation of prosody attributes. Experimental results on LJ-Speech and LibriTTS data show that the proposed CUC-VAE TTS system improves naturalness and prosody diversity with clear margins.</abstract>
      <url hash="2c539ca6">2022.acl-long.30</url>
      <bibkey>li-etal-2022-cross-utterance</bibkey>
      <pwccode url="https://github.com/neurowave-ai/cucvae-tts" additional="false">neurowave-ai/cucvae-tts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ljspeech">LJSpeech</pwcdataset>
    </paper>
    <paper id="33">
      <title>e-<fixed-case>CARE</fixed-case>: a New Dataset for Exploring Explainable Causal Reasoning</title>
      <author><first>Li</first><last>Du</last></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Kai</first><last>Xiong</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>432-446</pages>
      <abstract>Understanding causality has vital importance for various Natural Language Processing (NLP) applications. Beyond the labeled instances, conceptual explanations of the causality can provide deep understanding of the causal fact to facilitate the causal reasoning process. However, such explanation information still remains absent in existing causal reasoning resources. In this paper, we fill this gap by presenting a human-annotated explainable CAusal REasoning dataset (e-CARE), which contains over 20K causal reasoning questions, together with natural language formed explanations of the causal questions. Experimental results show that generating valid explanations for causal facts still remains especially challenging for the state-of-the-art models, and the explanation information can be helpful for promoting the accuracy and stability of causal reasoning models.</abstract>
      <url hash="a5cf57a5">2022.acl-long.33</url>
      <bibkey>du-etal-2022-e</bibkey>
      <pwccode url="https://github.com/waste-wood/e-care" additional="false">waste-wood/e-care</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/genericskb">GenericsKB</pwcdataset>
    </paper>
    <paper id="44">
      <title>Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation</title>
      <author><first>Yingxiu</first><last>Zhao</last></author>
      <author><first>Zhiliang</first><last>Tian</last></author>
      <author><first>Huaxiu</first><last>Yao</last></author>
      <author><first>Yinhe</first><last>Zheng</last></author>
      <author><first>Dongkyu</first><last>Lee</last></author>
      <author><first>Yiping</first><last>Song</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Nevin</first><last>Zhang</last></author>
      <pages>583-595</pages>
      <abstract>Building models of natural language processing (NLP) is challenging in low-resource scenarios where limited data are available. Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks. Nonetheless, these approaches suffer from the memorization overfitting issue, where the model tends to memorize the meta-training tasks while ignoring support sets when adapting to new tasks. To address this issue, we propose a memory imitation meta-learning (MemIML) method that enhances the model’s reliance on support sets for task adaptation. Specifically, we introduce a task-specific memory module to store support set information and construct an imitation module to force query sets to imitate the behaviors of support sets stored in the memory. A theoretical analysis is provided to prove the effectiveness of our method, and empirical results also demonstrate that our method outperforms competitive baselines on both text classification and generation tasks.</abstract>
      <url hash="03e66cbb">2022.acl-long.44</url>
      <attachment type="software" hash="7f999987">2022.acl-long.44.software.zip</attachment>
      <bibkey>zhao-etal-2022-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="53">
      <title>Meta-learning via Language Model In-context Tuning</title>
      <author><first>Yanda</first><last>Chen</last></author>
      <author><first>Ruiqi</first><last>Zhong</last></author>
      <author><first>Sheng</first><last>Zha</last></author>
      <author><first>George</first><last>Karypis</last></author>
      <author><first>He</first><last>He</last></author>
      <pages>719-730</pages>
      <abstract>The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. Inspired by the recent progress in large language models, we propose <tex-math>\textit{in-context tuning}</tex-math> (ICT), which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, labeled in-context examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label given the input sequence on a collection of tasks.We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to MAML which adapts the model through gradient descent, our method leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6% average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning meta-trains the model to learn from in-context examples. On BinaryClfs, ICT improves the average AUC-ROC score by an absolute 10%, and reduces the variance due to example ordering by 6x and example choices by 2x.</abstract>
      <url hash="728d43d6">2022.acl-long.53</url>
      <bibkey>chen-etal-2022-meta</bibkey>
      <pwccode url="https://github.com/yandachen/in-context-tuning" additional="false">yandachen/in-context-tuning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="55">
      <title>Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning</title>
      <author><first>Rongzhi</first><last>Zhang</last></author>
      <author><first>Yue</first><last>Yu</last></author>
      <author><first>Pranav</first><last>Shetty</last></author>
      <author><first>Le</first><last>Song</last></author>
      <author><first>Chao</first><last>Zhang</last></author>
      <pages>745-758</pages>
      <abstract>Weakly-supervised learning (WSL) has shown promising results in addressing label scarcity on many NLP tasks, but manually designing a comprehensive, high-quality labeling rule set is tedious and difficult. We study interactive weakly-supervised learning—the problem of iteratively and automatically discovering novel labeling rules from data to improve the WSL model. Our proposed model, named PRBoost, achieves this goal via iterative prompt-based rule discovery and model boosting. It uses boosting to identify large-error instances and discovers candidate rules from them by prompting pre-trained LMs with rule templates. The candidate rules are judged by human experts, and the accepted rules are used to generate complementary weak labels and strengthen the current model. Experiments on four tasks show PRBoost outperforms state-of-the-art WSL baselines up to 7.1%, and bridges the gaps with fully supervised models.</abstract>
      <url hash="d16eff1b">2022.acl-long.55</url>
      <bibkey>zhang-etal-2022-prompt</bibkey>
      <pwccode url="https://github.com/rz-zhang/prboost" additional="false">rz-zhang/prboost</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="58">
      <title><fixed-case>HIBRIDS</fixed-case>: Attention with Hierarchical Biases for Structure-aware Long Document Summarization</title>
      <author><first>Shuyang</first><last>Cao</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>786-807</pages>
      <abstract>Document structure is critical for efficient information consumption. However, it is challenging to encode it efficiently into the modern Transformer architecture. In this work, we present HIBRIDS, which injects Hierarchical Biases foR Incorporating Document Structure into attention score calculation. We further present a new task, hierarchical question-summary generation, for summarizing salient content in the source document into a hierarchy of questions and summaries, where each follow-up question inquires about the content of its parent question-summary pair. We also annotate a new dataset with 6,153 question-summary hierarchies labeled on government reports. Experiment results show that our model produces better question-summary hierarchies than comparisons on both hierarchy quality and content coverage, a finding also echoed by human judges. Additionally, our model improves the generation of long-form summaries from long government reports and Wikipedia articles, as measured by ROUGE scores.</abstract>
      <url hash="3b983d5d">2022.acl-long.58</url>
      <bibkey>cao-wang-2022-hibrids</bibkey>
    </paper>
    <paper id="59">
      <title>De-Bias for Generative Extraction in Unified <fixed-case>NER</fixed-case> Task</title>
      <author><first>Shuai</first><last>Zhang</last></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Zeqi</first><last>Tan</last></author>
      <author><first>Yiquan</first><last>Wu</last></author>
      <author><first>Weiming</first><last>Lu</last></author>
      <pages>808-818</pages>
      <abstract>Named entity recognition (NER) is a fundamental task to recognize specific types of entities from a given sentence. Depending on how the entities appear in the sentence, it can be divided into three subtasks, namely, Flat NER, Nested NER, and Discontinuous NER. Among the existing approaches, only the generative model can be uniformly adapted to these three subtasks. However, when the generative model is applied to NER, its optimization objective is not consistent with the task, which makes the model vulnerable to the incorrect biases. In this paper, we analyze the incorrect biases in the generation process from a causality perspective and attribute them to two confounders: pre-context confounder and entity-order confounder. Furthermore, we design Intra- and Inter-entity Deconfounding Data Augmentation methods to eliminate the above confounders according to the theory of backdoor adjustment. Experiments show that our method can improve the performance of the generative NER model in various datasets.</abstract>
      <url hash="8d8c38b9">2022.acl-long.59</url>
      <bibkey>zhang-etal-2022-de</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
    </paper>
    <paper id="70">
      <title>Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension</title>
      <author><first>Linjuan</first><last>Wu</last></author>
      <author><first>Shaojuan</first><last>Wu</last></author>
      <author><first>Xiaowang</first><last>Zhang</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Shizhan</first><last>Chen</last></author>
      <author><first>Zhiqiang</first><last>Zhuang</last></author>
      <author><first>Zhiyong</first><last>Feng</last></author>
      <pages>991-1000</pages>
      <abstract>Multilingual pre-trained models are able to zero-shot transfer knowledge from rich-resource to low-resource languages in machine reading comprehension (MRC). However, inherent linguistic discrepancies in different languages could make answer spans predicted by zero-shot transfer violate syntactic constraints of the target language. In this paper, we propose a novel multilingual MRC framework equipped with a Siamese Semantic Disentanglement Model (S2DM) to disassociate semantics from syntax in representations learned by multilingual pre-trained models. To explicitly transfer only semantic knowledge to the target language, we propose two groups of losses tailored for semantic and syntactic encoding and disentanglement. Experimental results on three multilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the effectiveness of our proposed approach over models based on mBERT and XLM-100.</abstract>
      <url hash="5012be2c">2022.acl-long.70</url>
      <attachment type="software" hash="c3482b19">2022.acl-long.70.software.zip</attachment>
      <bibkey>wu-etal-2022-learning</bibkey>
      <pwccode url="https://github.com/wulinjuan/ssdm_mrc" additional="false">wulinjuan/ssdm_mrc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bipar">BiPaR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydiqa-goldp">TyDiQA-GoldP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="78">
      <title>HiTab A Hierarchical Table Dataset for <a href="https://en.wikipedia.org/wiki/Question_answering">Question Answering</a> and Natural Language Generation<fixed-case>H</fixed-case>i<fixed-case>T</fixed-case>ab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation</title>
      <author><first>Zhoujun</first><last>Cheng</last></author>
      <author><first>Haoyu</first><last>Dong</last></author>
      <author><first>Zhiruo</first><last>Wang</last></author>
      <author><first>Ran</first><last>Jia</last></author>
      <author><first>Jiaqi</first><last>Guo</last></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Shi</first><last>Han</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <pages>1094-1110</pages>
      <abstract>Tables are often created with hierarchies but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables Hierarchical tables challenge numerical reasoning by complex hierarchical indexing as well as implicit relationships of calculation and semantics We present a new dataset HiTab to study question answering QA and natural language generation NLG over hierarchical tables HiTab is a cross domain dataset constructed from a wealth of statistical reports and Wikipedia pages and has unique characteristics nearly all tables are hierarchical and QA pairs are not proposed by annotators from scratch but are revised from real and meaningful sentences authored by analysts to reveal complex numerical reasoning in statistical reports we provide fine grained annotations of quantity and entity alignment Experiments suggest that this HiTab presents a strong challenge for existing baselines and a valuable benchmark for future research Targeting hierarchical structure we devise a hierarchy aware logical form for symbolic reasoning over tables which shows high effectiveness Targeting table reasoning we leverage entity and quantity alignment to explore partially supervised training in QA and conditional generation in NLG and largely reduce spurious predictions in QA and produce better descriptions in NLG</abstract>
      <url hash="b275be98">2022.acl-long.78</url>
      <attachment type="software" hash="4f663662">2022.acl-long.78.software.zip</attachment>
      <bibkey>cheng-etal-2022-hitab</bibkey>
      <pwccode url="https://github.com/microsoft/hitab" additional="false">microsoft/hitab</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/finqa">FinQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/totto">ToTTo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="82">
      <title><fixed-case>FORTAP</fixed-case>: Using Formulas for Numerical-Reasoning-Aware Table Pretraining</title>
      <author><first>Zhoujun</first><last>Cheng</last></author>
      <author><first>Haoyu</first><last>Dong</last></author>
      <author><first>Ran</first><last>Jia</last></author>
      <author><first>Pengfei</first><last>Wu</last></author>
      <author><first>Shi</first><last>Han</last></author>
      <author><first>Fan</first><last>Cheng</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <pages>1150-1166</pages>
      <abstract>Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, a commonly used language to perform computations on numerical values in spreadsheets, is a valuable supervision for numerical reasoning in tables. Considering large amounts of spreadsheets available on the web, we propose FORTAP, the first exploration to leverage spreadsheet formulas for table pretraining. Two novel self-supervised pretraining objectives are derived from formulas, numerical reference prediction (NRP) and numerical calculation prediction (NCP). While our proposed objectives are generic for encoders, to better capture spreadsheet table layouts and structures, FORTAP is built upon TUTA, the first transformer-based method for spreadsheet table pretraining with tree attention. FORTAP outperforms state-of-the-art methods by large margins on three representative datasets of formula prediction, question answering, and cell type classification, showing the great potential of leveraging formulas for table pretraining.</abstract>
      <url hash="6cb0c6c4">2022.acl-long.82</url>
      <attachment type="software" hash="1f44e443">2022.acl-long.82.software.zip</attachment>
      <bibkey>cheng-etal-2022-fortap</bibkey>
      <pwccode url="https://github.com/microsoft/TUTA_table_understanding" additional="false">microsoft/TUTA_table_understanding</pwccode>
    </paper>
    <paper id="85">
      <title>Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning</title>
      <author><first>Swarnadeep</first><last>Saha</last></author>
      <author><first>Prateek</first><last>Yadav</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>1190-1208</pages>
      <abstract>Pre-trained sequence-to-sequence language models have led to widespread success in many natural language generation tasks. However, there has been relatively less work on analyzing their ability to generate structured outputs such as graphs. Unlike natural language, graphs have distinct structural and semantic properties in the context of a downstream NLP task, e.g., generating a graph that is connected and acyclic can be attributed to its structural constraints, while the semantics of a graph can refer to how meaningfully an edge represents the relation between two node concepts. In this work, we study pre-trained language models that generate explanation graphs in an end-to-end manner and analyze their ability to learn the structural constraints and semantics of such graphs. We first show that with limited supervision, pre-trained language models often generate graphs that either violate these constraints or are semantically incoherent. Since curating large amount of human-annotated graphs is expensive and tedious, we propose simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative graphs. Next, we leverage these graphs in different contrastive learning models with Max-Margin and InfoNCE losses. Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks. Lastly, we show that human errors are the best negatives for contrastive learning and also that automatically generating more such human-like negative graphs can lead to further improvements.</abstract>
      <url hash="beb431f4">2022.acl-long.85</url>
      <attachment type="software" hash="ce4e5973">2022.acl-long.85.software.zip</attachment>
      <bibkey>saha-etal-2022-explanation</bibkey>
      <pwccode url="https://github.com/swarnahub/explagraphgen" additional="false">swarnahub/explagraphgen</pwccode>
    </paper>
    <paper id="90">
      <title>Efficient Unsupervised Sentence Compression by Fine-tuning Transformers with Reinforcement Learning</title>
      <author><first>Demian</first><last>Ghalandari</last></author>
      <author><first>Chris</first><last>Hokamp</last></author>
      <author><first>Georgiana</first><last>Ifrim</last></author>
      <pages>1267-1280</pages>
      <abstract>Sentence compression reduces the length of text by removing non-essential content while preserving important facts and grammaticality. Unsupervised objective driven methods for sentence compression can be used to create customized models without the need for ground-truth training data, while allowing flexibility in the objective function(s) that are used for learning and inference. Recent unsupervised sentence compression approaches use custom objectives to guide discrete search; however, guided search is expensive at inference time. In this work, we explore the use of reinforcement learning to train effective sentence compression models that are also fast when generating predictions. In particular, we cast the task as binary sequence labelling and fine-tune a pre-trained transformer using a simple policy gradient approach. Our approach outperforms other unsupervised models while also being more efficient at inference time.</abstract>
      <url hash="c8e70517">2022.acl-long.90</url>
      <bibkey>ghalandari-etal-2022-efficient</bibkey>
      <pwccode url="https://github.com/complementizer/rl-sentence-compression" additional="false">complementizer/rl-sentence-compression</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sentence-compression">Sentence Compression</pwcdataset>
    </paper>
    <paper id="91">
      <title>Tracing Origins: Coreference-aware Machine Reading Comprehension</title>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>1281-1292</pages>
      <abstract>Machine reading comprehension is a heavily-studied research and test field for evaluating new pre-trained language models (PrLMs) and fine-tuning strategies, and recent studies have enriched the pre-trained language models with syntactic, semantic and other linguistic information to improve the performance of the models. In this paper, we imitate the human reading process in connecting the anaphoric expressions and explicitly leverage the coreference information of the entities to enhance the word embeddings from the pre-trained language model, in order to highlight the coreference mentions of the entities that must be identified for coreference-intensive question answering in QUOREF, a relatively new dataset that is specifically designed to evaluate the coreference-related performance of a model. We use two strategies to fine-tune a pre-trained language model, namely, placing an additional encoder layer after a pre-trained language model to focus on the coreference mentions or constructing a relational graph convolutional network to model the coreference relations. We demonstrate that the explicit incorporation of coreference information in the fine-tuning stage performs better than the incorporation of the coreference information in pre-training a language model.</abstract>
      <url hash="c93f243c">2022.acl-long.91</url>
      <bibkey>zhang-zhao-2022-tracing</bibkey>
      <pwccode url="https://github.com/bright2013/CorefAwareMRC" additional="false">bright2013/CorefAwareMRC</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quoref">Quoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
    </paper>
    <paper id="92">
      <title>WatClaimCheck A new Dataset for Claim Entailment and Inference<fixed-case>W</fixed-case>at<fixed-case>C</fixed-case>laim<fixed-case>C</fixed-case>heck: A new Dataset for Claim Entailment and Inference</title>
      <author><first>Kashif</first><last>Khan</last></author>
      <author><first>Ruizhe</first><last>Wang</last></author>
      <author><first>Pascal</first><last>Poupart</last></author>
      <pages>1293-1304</pages>
      <abstract>We contribute a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for the task of automated fact checking and an evaluation of state of the art algorithms The dataset includes claims from speeches interviews social media and news articles review articles published by professional fact checkers and premise articles used by those professional fact checkers to support their review and verify the veracity of the claims An important challenge in the use of premise articles is the identification of relevant passages that will help to infer the veracity of a claim We show that transferring a dense passage retrieval model trained with <a href="https://en.wikipedia.org/wiki/Review_article">review articles</a> improves the retrieval quality of passages in premise articles We report results for the prediction of claim veracity by inference from premise articles</abstract>
      <url hash="f4cae244">2022.acl-long.92</url>
      <bibkey>khan-etal-2022-watclaimcheck</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pubhealth">PUBHEALTH</pwcdataset>
    </paper>
    <paper id="104">
      <title>Bias Mitigation in Machine Translation Quality Estimation</title>
      <author><first>Hanna</first><last>Behnke</last></author>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>1475-1487</pages>
      <abstract>Machine Translation Quality Estimation QE aims to build <a href="https://en.wikipedia.org/wiki/Predictive_modelling">predictive models</a> to assess the quality of machine generated translations in the absence of reference translations While state of the art QE models have been shown to achieve good results they over rely on features that do not have a causal impact on the quality of a translation In particular there appears to be a partial input bias i.e. a tendency to assign high quality scores to translations that are fluent and grammatically correct even though they do not preserve the meaning of the source We analyse the partial input bias in further detail and evaluate four approaches to use auxiliary tasks for bias mitigation Two approaches use additional data to inform and support the main task while the other two are adversarial actively discouraging the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> from learning the bias We compare the methods with respect to their ability to reduce the partial input bias while maintaining the overall performance We find that training a multitask architecture with an auxiliary binary classification task that utilises additional augmented data best achieves the desired effects and generalises well to different languages and quality metrics</abstract>
      <url hash="ab945a4c">2022.acl-long.104</url>
      <attachment type="software" hash="bb443c02">2022.acl-long.104.software.zip</attachment>
      <bibkey>behnke-etal-2022-bias</bibkey>
      <pwccode url="https://github.com/agesb/transquest" additional="false">agesb/transquest</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe-pe">MLQE-PE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="114">
      <title>Principled Paraphrase Generation with Parallel Corpora</title>
      <author><first>Aitor</first><last>Ormazabal</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Aitor</first><last>Soroa</last></author>
      <author><first>Gorka</first><last>Labaka</last></author>
      <author><first>Eneko</first><last>Agirre</last></author>
      <pages>1621-1638</pages>
      <abstract>Round trip Machine Translation MT is a popular choice for <a href="https://en.wikipedia.org/wiki/Paraphrase_generation">paraphrase generation</a> which leverages readily available <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpora</a> for supervision In this paper we formalize the implicit similarity function induced by this approach and show that it is susceptible to non paraphrase pairs sharing a single ambiguous translation Based on these insights we design an alternative similarity metric that mitigates this issue by requiring the entire translation distribution to match and implement a relaxation of it through the Information Bottleneck method Our approach incorporates an adversarial term into MT training in order to learn representations that encode as much information about the reference translation as possible while keeping as little information about the input as possible Paraphrases can be generated by decoding back to the source from this representation without having to generate pivot translations In addition to being more principled and efficient than <a href="https://en.wikipedia.org/wiki/Round-trip_delay_time">round trip MT</a> our approach offers an adjustable parameter to control the fidelity diversity trade off and obtains better results in our experiments</abstract>
      <url hash="cf46ee22">2022.acl-long.114</url>
      <bibkey>ormazabal-etal-2022-principled</bibkey>
      <pwccode url="https://github.com/aitorormazabal/paraphrasing-from-parallel" additional="false">aitorormazabal/paraphrasing-from-parallel</pwccode>
    </paper>
    <paper id="125">
      <title>Composable Sparse Fine-Tuning for Cross-Lingual Transfer</title>
      <author><first>Alan</first><last>Ansell</last></author>
      <author><first>Edoardo</first><last>Ponti</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <pages>1778-1796</pages>
      <abstract>Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning. To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed. Adapters are modular, as they can be combined to adapt a model towards different facets of knowledge (e.g., dedicated language and/or task adapters). Sparse fine-tuning is expressive, as it controls the behavior of all model components. In this work, we introduce a new fine-tuning method with both these desirable properties. In particular, we learn sparse, real-valued masks based on a simple variant of the Lottery Ticket Hypothesis. Task-specific masks are obtained from annotated data in a source language, and language-specific masks from masked language modeling in a target language. Both these masks can then be composed with the pretrained model. Unlike adapter-based fine-tuning, this method neither increases the number of parameters at inference time nor alters the original model architecture. Most importantly, it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we additionally find that sparsity is crucial to prevent both 1) interference between the fine-tunings to be composed and 2) overfitting. We release the code and models at https://github.com/cambridgeltl/composable-sft.</abstract>
      <url hash="6c5ae8e7">2022.acl-long.125</url>
      <bibkey>ansell-etal-2022-composable</bibkey>
      <pwccode url="https://github.com/cambridgeltl/composable-sft" additional="false">cambridgeltl/composable-sft</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/masakhaner">MasakhaNER</pwcdataset>
    </paper>
    <paper id="126">
      <title>Toward Annotator Group Bias in Crowdsourcing</title>
      <author><first>Haochen</first><last>Liu</last></author>
      <author><first>Joseph</first><last>Thekinen</last></author>
      <author><first>Sinem</first><last>Mollaoglu</last></author>
      <author><first>Da</first><last>Tang</last></author>
      <author><first>Ji</first><last>Yang</last></author>
      <author><first>Youlong</first><last>Cheng</last></author>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Jiliang</first><last>Tang</last></author>
      <pages>1797-1806</pages>
      <abstract>Crowdsourcing has emerged as a popular approach for collecting annotated data to train <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised machine learning models</a> However annotator bias can lead to defective annotations Though there are a few works investigating individual annotator bias the group effects in annotators are largely overlooked In this work we reveal that annotators within the same demographic group tend to show consistent group bias in annotation tasks and thus we conduct an initial study on annotator group bias We first empirically verify the existence of annotator group bias in various real world crowdsourcing datasets Then we develop a novel probabilistic graphical framework GroupAnno to capture annotator group bias with an extended Expectation Maximization EM algorithm We conduct experiments on both synthetic and real world datasets Experimental results demonstrate the effectiveness of our model in modeling annotator group bias in label aggregation and model learning over competitive baselines</abstract>
      <url hash="798b0955">2022.acl-long.126</url>
      <bibkey>liu-etal-2022-toward</bibkey>
    </paper>
    <paper id="138">
      <title><fixed-case>B</fixed-case>i<fixed-case>TIIMT</fixed-case>: A Bilingual Text-infilling Method for Interactive Machine Translation</title>
      <author><first>Yanling</first><last>Xiao</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Guoping</first><last>Huang</last></author>
      <author><first>Qu</first><last>Cui</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <pages>1958-1969</pages>
      <abstract>Interactive neural machine translation (INMT) is able to guarantee high-quality translations by taking human interactions into account. Existing IMT systems relying on lexical constrained decoding (LCD) enable humans to translate in a flexible translation order beyond the left-to-right. However, they typically suffer from two significant limitations in translation efficiency and quality due to the reliance on LCD. In this work, we propose a novel BiTIIMT system, Bilingual Text-Infilling for Interactive Neural Machine Translation. The key idea to BiTIIMT is Bilingual Text-infilling (BiTI) which aims to fill missing segments in a manually revised translation for a given source sentence. We propose a simple yet effective solution by casting this task as a sequence-to-sequence task. In this way, our system performs decoding without explicit constraints and makes full use of revised words for better translation prediction. Experiment results show that BiTiIMT performs significantly better and faster than state-of-the-art LCD-based IMT on three translation tasks.</abstract>
      <url hash="7ebaf499">2022.acl-long.138</url>
      <bibkey>xiao-etal-2022-bitiimt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="141">
      <title>Divide and Denoise: Learning from Noisy Labels in Fine-Grained Entity Typing with Cluster-Wise Loss Correction</title>
      <author><first>Kunyuan</first><last>Pang</last></author>
      <author><first>Haoyu</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Ting</first><last>Wang</last></author>
      <pages>1997-2006</pages>
      <abstract>Fine-grained Entity Typing (FET) has made great progress based on distant supervision but still suffers from label noise. Existing FET noise learning methods rely on prediction distributions in an instance-independent manner, which causes the problem of confirmation bias. In this work, we propose a clustering-based loss correction framework named Feature Cluster Loss Correction (FCLC), to address these two problems. FCLC first train a coarse backbone model as a feature extractor and noise estimator. Loss correction is then applied to each feature cluster, learning directly from the noisy labels. Experimental results on three public datasets show that FCLC achieves the best performance over existing competitive systems. Auxiliary experiments further demonstrate that FCLC is stable to hyperparameters and it does help mitigate confirmation bias. We also find that in the extreme case of no clean data, the FCLC framework still achieves competitive performance.</abstract>
      <url hash="684490d1">2022.acl-long.141</url>
      <bibkey>pang-etal-2022-divide</bibkey>
    </paper>
    <paper id="142">
      <title>Towards Robustness of Text-to-<fixed-case>SQL</fixed-case> Models Against Natural and Realistic Adversarial Table Perturbation</title>
      <author><first>Xinyu</first><last>Pi</last></author>
      <author><first>Bing</first><last>Wang</last></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Jiaqi</first><last>Guo</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <pages>2007-2022</pages>
      <abstract>The robustness of Text-to-SQL parsers against adversarial perturbations plays a crucial role in delivering highly reliable applications. Previous studies along this line primarily focused on perturbations in the natural language question side, neglecting the variability of tables. Motivated by this, we propose the Adversarial Table Perturbation (ATP) as a new attacking paradigm to measure robustness of Text-to-SQL models. Following this proposition, we curate ADVETA, the first robustness evaluation benchmark featuring natural and realistic ATPs. All tested state-of-the-art models experience dramatic performance drops on ADVETA, revealing significant room of improvement. To defense against ATP, we build a systematic adversarial training example generation framework tailored for better contextualization of tabular data. Experiments show that our approach brings models best robustness improvement against ATP, while also substantially boost model robustness against NL-side perturbations. We will release ADVETA and code to facilitate future research.</abstract>
      <url hash="6c94d5d4">2022.acl-long.142</url>
      <bibkey>pi-etal-2022-towards</bibkey>
      <pwccode url="https://github.com/microsoft/ContextualSP" additional="false">microsoft/ContextualSP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cosql">CoSQL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sparc">SParC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="144">
      <title>Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages</title>
      <author><first>Ehsan</first><last>Aghazadeh</last></author>
      <author><first>Mohsen</first><last>Fayyaz</last></author>
      <author><first>Yadollah</first><last>Yaghoobzadeh</last></author>
      <pages>2037-2050</pages>
      <abstract>Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.</abstract>
      <url hash="1a232d85">2022.acl-long.144</url>
      <attachment type="software" hash="7de48646">2022.acl-long.144.software.zip</attachment>
      <bibkey>aghazadeh-etal-2022-metaphors</bibkey>
      <pwccode url="https://github.com/ehsanaghazadeh/metaphors_in_plms" additional="false">ehsanaghazadeh/metaphors_in_plms</pwccode>
    </paper>
    <paper id="151">
      <title>bert2<fixed-case>BERT</fixed-case>: Towards Reusable Pretrained Language Models</title>
      <author><first>Cheng</first><last>Chen</last></author>
      <author><first>Yichun</first><last>Yin</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Fengyu</first><last>Wang</last></author>
      <author><first>Zhi</first><last>Wang</last></author>
      <author><first>Xiao</first><last>Chen</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>2134-2148</pages>
      <abstract>In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model to a large model through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving method proposed in computer vision on the Transformer-based language model, and further improve it by proposing a novel method, advanced knowledge for large model’s initialization. In addition, a two-stage learning method is proposed to further accelerate the pre-training. We conduct extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45% and 47% computational cost of pre-training BERT<tex-math>_{\rm BASE}</tex-math> and GPT<tex-math>_{\rm BASE}</tex-math> by reusing the models of almost their half sizes.</abstract>
      <url hash="88332217">2022.acl-long.151</url>
      <bibkey>chen-etal-2022-bert2bert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="153">
      <title>"<fixed-case>Y</fixed-case>ou might think about slightly revising the title”: Identifying Hedges in Peer-tutoring Interactions</title>
      <author><first>Yann</first><last>Raphalen</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <author><first>Justine</first><last>Cassell</last></author>
      <pages>2160-2174</pages>
      <abstract>Hedges have an important role in the management of rapport. In peer-tutoring, they are notably used by tutors in dyads experiencing low rapport to tone down the impact of instructions and negative feedback.Pursuing the objective of building a tutoring agent that manages rapport with teenagers in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of a such a hybrid model approach.</abstract>
      <url hash="d4bb59f7">2022.acl-long.153</url>
      <bibkey>raphalen-etal-2022-might</bibkey>
    </paper>
    <paper id="154">
      <title>Efficient Cluster-Based <tex-math>k</tex-math>-Nearest-Neighbor Machine Translation</title>
      <author><first>Dexin</first><last>Wang</last></author>
      <author><first>Kai</first><last>Fan</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>2175-2187</pages>
      <abstract>
        <tex-math>k</tex-math>-Nearest-Neighbor Machine Translation (<tex-math>k</tex-math>NN-MT) has been recently proposed as a non-parametric solution for domain adaptation in neural machine translation (NMT). It aims to alleviate the performance degradation of advanced MT systems in translating out-of-domain sentences by coordinating with an additional token-level feature-based retrieval module constructed from in-domain data. Previous studies (Khandelwal et al., 2021; Zheng et al., 2021) have already demonstrated that non-parametric NMT is even superior to models fine-tuned on out-of-domain data. In spite of this success, <tex-math>k</tex-math>NN retrieval is at the expense of high latency, in particular for large datastores. To make it practical, in this paper, we explore a more efficient <tex-math>k</tex-math>NN-MT and propose to use clustering to improve the retrieval efficiency. Concretely, we first propose a cluster-based Compact Network for feature reduction in a contrastive learning manner to compress context features into 90+% lower dimensional vectors. We then suggest a cluster-based pruning solution to filter out 10% 40% redundant nodes in large datastores while retaining translation quality. Our proposed methods achieve better or comparable performance while reducing up to 57% inference latency against the advanced non-parametric MT model on several machine translation benchmarks. Experimental results indicate that the proposed methods maintain the most useful information of the original datastore and the Compact Network shows good generalization on unseen domains. Codes are available at https://github.com/tjunlp-lab/PCKMT.</abstract>
      <url hash="c17cde26">2022.acl-long.154</url>
      <attachment type="software" hash="5b6c9f37">2022.acl-long.154.software.zip</attachment>
      <bibkey>wang-etal-2022-efficient</bibkey>
      <pwccode url="https://github.com/tjunlp-lab/pckmt" additional="false">tjunlp-lab/pckmt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="155">
      <title>Headed-Span-Based Projective Dependency Parsing</title>
      <author><first>Songlin</first><last>Yang</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>2188-2200</pages>
      <abstract>We propose a new method for projective dependency parsing based on headed spans. In a projective dependency tree, the largest subtree rooted at each word covers a contiguous sequence (i.e., a span) in the surface order. We call such a span marked by a root word <i>headed span</i>. A projective dependency tree can be represented as a collection of headed spans. We decompose the score of a dependency tree into the scores of the headed spans and design a novel <tex-math>O(n^3)</tex-math> dynamic programming algorithm to enable global training and exact inference. Our model achieves state-of-the-art or competitive results on PTB, CTB, and UD</abstract>
      <url hash="1c51338f">2022.acl-long.155</url>
      <bibkey>yang-tu-2022-headed</bibkey>
      <pwccode url="https://github.com/sustcsonglin/span-based-dependency-parsing" additional="false">sustcsonglin/span-based-dependency-parsing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="157">
      <title>Robust Lottery Tickets for Pre-trained Language Models</title>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Bao</first><last>Rong</last></author>
      <author><first>Yuhao</first><last>Zhou</last></author>
      <author><first>Di</first><last>Liang</last></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>2211-2224</pages>
      <abstract>Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models (PLMs) contain smaller matching subnetworks(winning tickets) which are capable of reaching accuracy comparable to the original models. However, these tickets are proved to be notrobust to adversarial examples, and even worse than their PLM counterparts. To address this problem, we propose a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs. Since the loss is not differentiable for the binary mask, we assign the hard concrete distribution to the masks and encourage their sparsity using a smoothing approximation of L0 regularization.Furthermore, we design an adversarial loss objective to guide the search for robust tickets and ensure that the tickets perform well bothin accuracy and robustness. Experimental results show the significant improvement of the proposed method over previous work on adversarial robustness evaluation.</abstract>
      <url hash="e5387608">2022.acl-long.157</url>
      <bibkey>zheng-etal-2022-robust</bibkey>
      <pwccode url="https://github.com/ruizheng20/robust_ticket" additional="false">ruizheng20/robust_ticket</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="162">
      <title><fixed-case>IAM</fixed-case>: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks</title>
      <author><first>Liying</first><last>Cheng</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Ruidan</first><last>He</last></author>
      <author><first>Qian</first><last>Yu</last></author>
      <author><first>Yan</first><last>Zhang</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>2277-2287</pages>
      <abstract>Traditionally, a debate usually requires a manual preparation process, including reading plenty of articles, selecting the claims, identifying the stances of the claims, seeking the evidence for the claims, etc. As the AI debate attracts more attention these years, it is worth exploring the methods to automate the tedious process involved in the debating system. In this work, we introduce a comprehensive and large dataset named IAM, which can be applied to a series of argument mining tasks, including claim extraction, stance classification, evidence extraction, etc. Our dataset is collected from over 1k articles related to 123 topics. Near 70k sentences in the dataset are fully annotated based on their argument properties (e.g., claims, stances, evidence, etc.). We further propose two new integrated argument mining tasks associated with the debate preparation process: (1) claim extraction with stance classification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a pipeline approach and an end-to-end method for each integrated task separately. Promising experimental results are reported to show the values and challenges of our proposed tasks, and motivate future research on argument mining.</abstract>
      <url hash="ce119afb">2022.acl-long.162</url>
      <attachment type="software" hash="d549c8eb">2022.acl-long.162.software.zip</attachment>
      <bibkey>cheng-etal-2022-iam</bibkey>
      <pwccode url="https://github.com/liyingcheng95/iam" additional="false">liyingcheng95/iam</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iam-dataset">IAM Dataset</pwcdataset>
    </paper>
    <paper id="164">
      <title><fixed-case>CTRLE</fixed-case>val: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation</title>
      <author><first>Pei</first><last>Ke</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Xiaoyan</first><last>Zhu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>2306-2319</pages>
      <abstract>Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities.</abstract>
      <url hash="66e0b03c">2022.acl-long.164</url>
      <attachment type="software" hash="eca9ee80">2022.acl-long.164.software.zip</attachment>
      <bibkey>ke-etal-2022-ctrleval</bibkey>
    </paper>
    <paper id="172">
      <title>Redistributing Low-Frequency Words: Making the Most of Monolingual Data in Non-Autoregressive Translation</title>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Dacheng</first><last>Tao</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <pages>2417-2426</pages>
      <abstract>Knowledge distillation (KD) is the preliminary step for training non-autoregressive translation (NAT) models, which eases the training of NAT models at the cost of losing important information for translating low-frequency words. In this work, we provide an appealing alternative for NAT – <i>monolingual KD</i>, which trains NAT student on external monolingual data with AT teacher trained on the original bilingual data. Monolingual KD is able to transfer both the knowledge of the original bilingual data (implicitly encoded in the trained AT teacher model) and that of the new monolingual data to the NAT student model. Extensive experiments on eight WMT benchmarks over two advanced NAT models show that monolingual KD consistently outperforms the standard KD by improving low-frequency word translation, without introducing any computational cost. Monolingual KD enjoys desirable expandability, which can be further enhanced (when given more computational budget) by combining with the standard KD, a reverse monolingual KD, or enlarging the scale of monolingual data. Extensive analyses demonstrate that these techniques can be used together profitably to further recall the useful information lost in the standard KD. Encouragingly, combining with standard KD, our approach achieves 30.4 and 34.1 BLEU points on the WMT14 English-German and German-English datasets, respectively. Our code and trained models are freely available at <url>https://github.com/alphadl/RLFW-NAT.mono</url>.</abstract>
      <url hash="a7669577">2022.acl-long.172</url>
      <bibkey>ding-etal-2022-redistributing</bibkey>
    </paper>
    <paper id="179">
      <title>Alignment-Augmented Consistent Translation for Multilingual Open Information Extraction</title>
      <author><first>Keshav</first><last>Kolluru</last></author>
      <author><first>Muqeeth</first><last>Mohammed</last></author>
      <author><first>Shubham</first><last>Mittal</last></author>
      <author><first>Soumen</first><last>Chakrabarti</last></author>
      <author><first>Mausam</first><last>.</last></author>
      <pages>2502-2517</pages>
      <abstract>Progress with supervised Open Information Extraction (OpenIE) has been primarily limited to English due to the scarcity of training data in other languages. In this paper, we explore techniques to automatically convert English text for training OpenIE systems in other languages. We introduce the Alignment-Augmented Constrained Translation (AACTrans) model to translate English sentences and their corresponding extractions consistently with each other — with no changes to vocabulary or semantic meaning which may result from independent translations. Using the data generated with AACTrans, we train a novel two-stage generative OpenIE model, which we call Gen2OIE, that outputs for each sentence: 1) relations in the first stage and 2) all extractions containing the relation in the second stage. Gen2OIE increases relation coverage using a training data transformation technique that is generalizable to multiple languages, in contrast to existing models that use an English-specific training loss. Evaluations on 5 languages — Spanish, Portuguese, Chinese, Hindi and Telugu — show that the Gen2OIE with AACTrans data outperforms prior systems by a margin of 6-25% in F1.</abstract>
      <url hash="f3a29775">2022.acl-long.179</url>
      <attachment type="software" hash="9cb412fc">2022.acl-long.179.software.zip</attachment>
      <bibkey>kolluru-etal-2022-alignment</bibkey>
      <pwccode url="https://github.com/dair-iitd/moie" additional="false">dair-iitd/moie</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/x-srl">X-SRL</pwcdataset>
    </paper>
    <paper id="180">
      <title>Text-to-Table: A New Way of Information Extraction</title>
      <author><first>Xueqing</first><last>Wu</last></author>
      <author><first>Jiacheng</first><last>Zhang</last></author>
      <author><first>Hang</first><last>Li</last></author>
      <pages>2518-2533</pages>
      <abstract>We study a new problem setting of information extraction (IE), referred to as text-to-table. In text-to-table, given a text, one creates a table or several tables expressing the main content of the text, while the model is learned from text-table pair data. The problem setting differs from those of the existing methods for IE. First, the extraction can be carried out from long texts to large tables with complex structures. Second, the extraction is entirely data-driven, and there is no need to explicitly define the schemas. As far as we know, there has been no previous work that studies the problem. In this work, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem. We first employ a seq2seq model fine-tuned from a pre-trained language model to perform the task. We also develop a new method within the seq2seq approach, exploiting two additional techniques in table generation: table constraint and table relation embeddings. We consider text-to-table as an inverse problem of the well-studied table-to-text, and make use of four existing table-to-text datasets in our experiments on text-to-table. Experimental results show that the vanilla seq2seq model can outperform the baseline methods of using relation extraction and named entity extraction. The results also show that our method can further boost the performances of the vanilla seq2seq model. We further discuss the main challenges of the proposed task. The code and data are available at https://github.com/shirley-wu/text_to_table.</abstract>
      <url hash="89480a3a">2022.acl-long.180</url>
      <bibkey>wu-etal-2022-text-table</bibkey>
      <pwccode url="https://github.com/shirley-wu/text_to_table" additional="false">shirley-wu/text_to_table</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rotowire">RotoWire</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
    </paper>
    <paper id="181">
      <title>Accelerating Code Search with Deep Hashing and Code Classification</title>
      <author><first>Wenchao</first><last>Gu</last></author>
      <author><first>Yanlin</first><last>Wang</last></author>
      <author><first>Lun</first><last>Du</last></author>
      <author><first>Hongyu</first><last>Zhang</last></author>
      <author><first>Shi</first><last>Han</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <author><first>Michael</first><last>Lyu</last></author>
      <pages>2534-2544</pages>
      <abstract>Code search is to search reusable code snippets from <a href="https://en.wikipedia.org/wiki/Text_corpus">source code corpus</a> based on natural languages queries Deep learning based methods on code search have shown promising results However previous <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> focus on retrieval accuracy but lacked attention to the efficiency of the retrieval process We propose a novel method CoSHC to accelerate code search with deep hashing and code classification aiming to perform efficient code search without sacrificing too much accuracy To evaluate the effectiveness of CoSHC we apply our method 
 on five code search models Extensive experimental results indicate that compared with previous code search baselines CoSHC can save more than of retrieval time meanwhile preserving at least of retrieval accuracy</abstract>
      <url hash="3c71b13a">2022.acl-long.181</url>
      <bibkey>gu-etal-2022-accelerating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
    </paper>
    <paper id="187">
      <title>Learning Disentangled Textual Representations via Statistical Measures of Similarity</title>
      <author><first>Pierre</first><last>Colombo</last></author>
      <author><first>Guillaume</first><last>Staerman</last></author>
      <author><first>Nathan</first><last>Noiry</last></author>
      <author><first>Pablo</first><last>Piantanida</last></author>
      <pages>2614-2630</pages>
      <abstract>When working with textual data a natural application of disentangled representations is the fair classification where the goal is to make predictions without being biased or influenced by sensible attributes that may be present in the data e.g. age gender or race Dominant approaches to disentangle a sensitive attribute from textual representations rely on learning simultaneously a penalization term that involves either an adversary loss e.g. a <a href="https://en.wikipedia.org/wiki/Discriminator">discriminator</a> or an information measure e.g. <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a> However these methods require the training of a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural network</a> with several parameter updates for each update of the representation model As a matter of fact the resulting nested optimization loop is both times consuming adding complexity to the optimization dynamic and requires a fine hyperparameter selection e.g. learning rates architecture In this work we introduce a family of <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularizers</a> for learning disentangled representations that do not require training These <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularizers</a> are based on statistical measures of similarity between the <a href="https://en.wikipedia.org/wiki/Conditional_probability_distribution">conditional probability distributions</a> with respect to the sensible attributes Our novel <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularizers</a> do not require additional training are faster and do not involve additional tuning while achieving better results both when combined with pretrained and randomly initialized text encoders</abstract>
      <url hash="11e3a96f">2022.acl-long.187</url>
      <attachment type="software" hash="84afe76c">2022.acl-long.187.software.zip</attachment>
      <bibkey>colombo-etal-2022-learning</bibkey>
    </paper>
    <paper id="191">
      <title><fixed-case>GL</fixed-case>-<fixed-case>CL</fixed-case>e<fixed-case>F</fixed-case>: A Global–Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding</title>
      <author><first>Libo</first><last>Qin</last></author>
      <author><first>Qiguang</first><last>Chen</last></author>
      <author><first>Tianbao</first><last>Xie</last></author>
      <author><first>Qixin</first><last>Li</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <pages>2677-2686</pages>
      <abstract>Due to high data demands of current methods, attention to zero-shot cross-lingual spoken language understanding (SLU) has grown, as such approaches greatly reduce human annotation effort. However, existing models solely rely on shared parameters, which can only perform implicit alignment across languages. We present Global-Local Contrastive Learning Framework (GL-CLeF) to address this shortcoming. Specifically, we employ contrastive learning, leveraging bilingual dictionaries to construct multilingual views of the same utterance, then encourage their representations to be more similar than negative example pairs, which achieves to explicitly align representations of similar sentences across languages. In addition, a key step in GL-CLeF is a proposed Local and Global component, which achieves a fine-grained cross-lingual transfer (i.e., sentence-level Local intent transfer, token-level Local slot transfer, and semantic-level Global transfer across intent and slot). Experiments on MultiATIS++ show that GL-CLeF achieves the best performance and successfully pulls representations of similar sentences across languages closer.</abstract>
      <url hash="c7e4a9fe">2022.acl-long.191</url>
      <attachment type="software" hash="a10b6070">2022.acl-long.191.software.zip</attachment>
      <bibkey>qin-etal-2022-gl</bibkey>
      <pwccode url="https://github.com/lightchen233/gl-clef" additional="false">lightchen233/gl-clef</pwccode>
    </paper>
    <paper id="192">
      <title>Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource <fixed-case>NER</fixed-case></title>
      <author><first>Dong-Ho</first><last>Lee</last></author>
      <author><first>Akshen</first><last>Kadakia</last></author>
      <author><first>Kangmin</first><last>Tan</last></author>
      <author><first>Mahak</first><last>Agarwal</last></author>
      <author><first>Xinyu</first><last>Feng</last></author>
      <author><first>Takashi</first><last>Shibuya</last></author>
      <author><first>Ryosuke</first><last>Mitani</last></author>
      <author><first>Toshiyuki</first><last>Sekiya</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>2687-2700</pages>
      <abstract>Recent advances in prompt-based learning have shown strong results on few-shot text classification by using cloze-style templates.Similar attempts have been made on named entity recognition (NER) which manually design templates to predict entity types for every text span in a sentence. However, such methods may suffer from error propagation induced by entity span detection, high cost due to enumeration of all possible text spans, and omission of inter-dependencies among token labels in a sentence. Here we present a simple demonstration-based learning method for NER, which lets the input be prefaced by task demonstrations for in-context learning. We perform a systematic study on demonstration strategy regarding what to include (entity examples, with or without surrounding context), how to select the examples, and what templates to use. Results on in-domain learning and domain adaptation show that the model’s performance in low-resource settings can be largely improved with a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train instances). We also find that good demonstration can save many labeled examples and consistency in demonstration contributes to better performance.</abstract>
      <url hash="357aec09">2022.acl-long.192</url>
      <attachment type="software" hash="1f2f7528">2022.acl-long.192.software.zip</attachment>
      <bibkey>lee-etal-2022-good</bibkey>
      <pwccode url="https://github.com/ink-usc/fewner" additional="false">ink-usc/fewner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
    </paper>
    <paper id="195">
      <title>A Meta-framework for Spatiotemporal Quantity Extraction from Text</title>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Hao</first><last>Wu</last></author>
      <author><first>Haoruo</first><last>Peng</last></author>
      <author><first>Chuchu</first><last>Fan</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>2736-2749</pages>
      <abstract>News events are often associated with quantities (e.g., the number of COVID-19 patients or the number of arrests in a protest), and it is often important to extract their type, time, and location from unstructured text in order to analyze these quantity events. This paper thus formulates the NLP problem of spatiotemporal quantity extraction, and proposes the first meta-framework for solving it. This meta-framework contains a formalism that decomposes the problem into several information extraction tasks, a shareable crowdsourcing pipeline, and transformer-based baseline models. We demonstrate the meta-framework in three domains—the COVID-19 pandemic, Black Lives Matter protests, and 2020 California wildfires—to show that the formalism is general and extensible, the crowdsourcing pipeline facilitates fast and high-quality data annotation, and the baseline system can handle spatiotemporal quantity extraction well enough to be practically useful. We release all resources for future research on this topic at https://github.com/steqe.</abstract>
      <url hash="880fce61">2022.acl-long.195</url>
      <bibkey>ning-etal-2022-meta</bibkey>
    </paper>
    <paper id="201">
      <title>Sequence-to-Sequence Knowledge Graph Completion and Question Answering</title>
      <author><first>Apoorv</first><last>Saxena</last></author>
      <author><first>Adrian</first><last>Kochsiek</last></author>
      <author><first>Rainer</first><last>Gemulla</last></author>
      <pages>2814-2828</pages>
      <abstract>Knowledge graph embedding (KGE) models represent each entity and relation of a knowledge graph (KG) with low-dimensional embedding vectors. These methods have recently been applied to KG link prediction and question answering over incomplete KGs (KGQA). KGEs typically create an embedding for each entity in the graph, which results in large model sizes on real-world graphs with millions of entities. For downstream tasks these atomic entity representations often need to be integrated into a multi stage pipeline, limiting their utility. We show that an off-the-shelf encoder-decoder Transformer model can serve as a scalable and versatile KGE model obtaining state-of-the-art results for KG link prediction and incomplete KG question answering. We achieve this by posing KG link prediction as a sequence-to-sequence task and exchange the triple scoring approach taken by prior KGE methods with autoregressive decoding. Such a simple but powerful method reduces the model size up to 98% compared to conventional KGE models while keeping inference time tractable. After finetuning this model on the task of KGQA over incomplete KGs, our approach outperforms baselines on multiple large-scale datasets without extensive hyperparameter tuning.</abstract>
      <url hash="88c6b206">2022.acl-long.201</url>
      <attachment type="software" hash="7ddcb003">2022.acl-long.201.software.zip</attachment>
      <bibkey>saxena-etal-2022-sequence</bibkey>
      <pwccode url="https://github.com/apoorvumang/kgt5" additional="false">apoorvumang/kgt5</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/complexwebquestions">ComplexWebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/metaqa">MetaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ogb-lsc">OGB-LSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestionssp">WebQuestionsSP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimovies">WikiMovies</pwcdataset>
    </paper>
    <paper id="209">
      <title><fixed-case>FIBER</fixed-case>: Fill-in-the-Blanks as a Challenging Video Understanding Evaluation Framework</title>
      <author><first>Santiago</first><last>Castro</last></author>
      <author><first>Ruoyao</first><last>Wang</last></author>
      <author><first>Pingxuan</first><last>Huang</last></author>
      <author><first>Ian</first><last>Stewart</last></author>
      <author><first>Oana</first><last>Ignat</last></author>
      <author><first>Nan</first><last>Liu</last></author>
      <author><first>Jonathan</first><last>Stroud</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>2925-2940</pages>
      <abstract>We propose fill-in-the-blanks as a video understanding evaluation framework and introduce FIBER – a novel dataset consisting of 28,000 videos and descriptions in support of this evaluation framework. The fill-in-the-blanks setting tests a model’s understanding of a video by requiring it to predict a masked noun phrase in the caption of the video, given the video and the surrounding text. The FIBER benchmark does not share the weaknesses of the current state-of-the-art language-informed video understanding tasks, namely: (1) video question answering using multiple-choice questions, where models perform relatively well because they exploit linguistic biases in the task formulation, thus making our framework challenging for the current state-of-the-art systems to solve; and (2) video captioning, which relies on an open-ended evaluation framework that is often inaccurate because system answers may be perceived as incorrect if they differ in form from the ground truth. The FIBER dataset and our code are available at https://lit.eecs.umich.edu/fiber/.</abstract>
      <url hash="2db95385">2022.acl-long.209</url>
      <bibkey>castro-etal-2022-fiber</bibkey>
      <pwccode url="https://github.com/MichiganNLP/video-fill-in-the-blank" additional="false">MichiganNLP/video-fill-in-the-blank</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/activitynet-captions">ActivityNet Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vatex">VATEX</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="210">
      <title><fixed-case>K</fixed-case>en<fixed-case>M</fixed-case>e<fixed-case>SH</fixed-case>: Knowledge-enhanced End-to-end Biomedical Text Labelling</title>
      <author><first>Xindi</first><last>Wang</last></author>
      <author id="robert-e-mercer"><first>Robert</first><last>Mercer</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>2941-2951</pages>
      <abstract>Currently, Medical Subject Headings (MeSH) are manually assigned to every biomedical article published and subsequently recorded in the PubMed database to facilitate retrieving relevant information. With the rapid growth of the PubMed database, large-scale biomedical document indexing becomes increasingly important. MeSH indexing is a challenging task for machine learning, as it needs to assign multiple labels to each article from an extremely large hierachically organized collection. To address this challenge, we propose KenMeSH, an end-to-end model that combines new text features and a dynamic knowledge-enhanced mask attention that integrates document features with MeSH label hierarchy and journal correlation features to index MeSH terms. Experimental results show the proposed method achieves state-of-the-art performance on a number of measures.</abstract>
      <url hash="bfa46388">2022.acl-long.210</url>
      <bibkey>wang-etal-2022-kenmesh</bibkey>
      <pwccode url="https://github.com/xdwang0726/kenmesh" additional="false">xdwang0726/kenmesh</pwccode>
    </paper>
    <paper id="211">
      <title>A Taxonomy of Empathetic Questions in Social Dialogs</title>
      <author><first>Ekaterina</first><last>Svikhnushina</last></author>
      <author><first>Iuliana</first><last>Voinea</last></author>
      <author><first>Anuradha</first><last>Welivita</last></author>
      <author><first>Pearl</first><last>Pu</last></author>
      <pages>2952-2973</pages>
      <abstract>Effective question asking is a crucial component of a successful conversational chatbot It could help the bots manifest empathy and render the <a href="https://en.wikipedia.org/wiki/Interaction">interaction</a> more engaging by demonstrating attention to the speaker’s emotions However current dialog generation approaches do not model this subtle emotion regulation technique due to the lack of a taxonomy of questions and their purpose in social chitchat To address this gap we have developed an empathetic question taxonomy EQT with special attention paid to questions ability to capture communicative acts and their emotion regulation intents We further design a crowd sourcing task to annotate a large subset of the EmpatheticDialogues dataset with the established labels We use the crowd annotated data to develop automatic labeling tools and produce labels for the whole dataset Finally we employ information visualization techniques to summarize co occurrences of question acts and intents and their role in regulating interlocutors emotion These results reveal important question asking strategies in <a href="https://en.wikipedia.org/wiki/Social_relation">social dialogs</a> The EQT classification scheme can facilitate computational analysis of questions in datasets More importantly it can inform future efforts in empathetic question generation using neural or hybrid methods</abstract>
      <url hash="3d0cdf9a">2022.acl-long.211</url>
      <attachment type="software" hash="40897cc2">2022.acl-long.211.software.zip</attachment>
      <bibkey>svikhnushina-etal-2022-taxonomy</bibkey>
    </paper>
    <paper id="212">
      <title>Enhanced Multi-Channel Graph Convolutional Network for Aspect Sentiment Triplet Extraction</title>
      <author><first>Hao</first><last>Chen</last></author>
      <author><first>Zepeng</first><last>Zhai</last></author>
      <author><first>Fangxiang</first><last>Feng</last></author>
      <author><first>Ruifan</first><last>Li</last></author>
      <author><first>Xiaojie</first><last>Wang</last></author>
      <pages>2974-2985</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE) is an emerging sentiment analysis task. Most of the existing studies focus on devising a new tagging scheme that enables the model to extract the sentiment triplets in an end-to-end fashion. However, these methods ignore the relations between words for ASTE task. In this paper, we propose an Enhanced Multi-Channel Graph Convolutional Network model (EMC-GCN) to fully utilize the relations between words. Specifically, we first define ten types of relations for ASTE task, and then adopt a biaffine attention module to embed these relations as an adjacent tensor between words in a sentence. After that, our EMC-GCN transforms the sentence into a multi-channel graph by treating words and the relation adjacent tensor as nodes and edges, respectively. Thus, relation-aware node representations can be learnt. Furthermore, we consider diverse linguistic features to enhance our EMC-GCN model. Finally, we design an effective refining strategy on EMC-GCN for word-pair representation refinement, which considers the implicit results of aspect and opinion extraction when determining whether word pairs match or not. Extensive experimental results on the benchmark datasets demonstrate that the effectiveness and robustness of our proposed model, which outperforms state-of-the-art methods significantly.</abstract>
      <url hash="04ad68cb">2022.acl-long.212</url>
      <attachment type="software" hash="41129d32">2022.acl-long.212.software.zip</attachment>
      <bibkey>chen-etal-2022-enhanced</bibkey>
      <pwccode url="https://github.com/ccchenhao997/emcgcn-aste" additional="false">ccchenhao997/emcgcn-aste</pwccode>
    </paper>
    <paper id="220">
      <title>Learned Incremental Representations for <a href="https://en.wikipedia.org/wiki/Parsing">Parsing</a></title>
      <award>Best Paper</award>
      <author><first>Nikita</first><last>Kitaev</last></author>
      <author><first>Thomas</first><last>Lu</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>3086-3095</pages>
      <abstract>We present an incremental syntactic representation that consists of assigning a single discrete label to each word in a sentence where the label is predicted using strictly incremental processing of a prefix of the sentence and the sequence of labels for a sentence fully determines a <a href="https://en.wikipedia.org/wiki/Parse_tree">parse tree</a> Our goal is to induce a syntactic representation that commits to syntactic choices only as they are incrementally revealed by the input in contrast with standard representations that must make output choices such as attachments speculatively and later throw out conflicting analyses Our learned <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representations</a> achieve 93.72 F1 on the <a href="https://en.wikipedia.org/wiki/Penn_Treebank">Penn Treebank</a> with as few as bits per word and at bits per word they achieve 94.97 F1 which is comparable with other state of the art parsing models when using the same pre trained embeddings We also provide an analysis of the representations learned by our <a href="https://en.wikipedia.org/wiki/System">system</a> investigating properties such as the interpretable syntactic features captured by the <a href="https://en.wikipedia.org/wiki/System">system</a> and mechanisms for deferred resolution of syntactic ambiguities</abstract>
      <url hash="f73edfe2">2022.acl-long.220</url>
      <bibkey>kitaev-etal-2022-learned</bibkey>
      <pwccode url="https://github.com/thomaslu2000/incremental-parsing-representations" additional="false">thomaslu2000/incremental-parsing-representations</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="222">
      <title>Misinfo Reaction Frames: Reasoning about Readers’ Reactions to News Headlines</title>
      <author><first>Saadia</first><last>Gabriel</last></author>
      <author><first>Skyler</first><last>Hallinan</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Pemi</first><last>Nguyen</last></author>
      <author><first>Franziska</first><last>Roesner</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>3108-3127</pages>
      <abstract>Even to a simple and short news headline, readers react in a multitude of ways: cognitively (e.g. inferring the writer’s intent), emotionally (e.g. feeling distrust), and behaviorally (e.g. sharing the news with their friends). Such reactions are instantaneous and yet complex, as they rely on factors that go beyond interpreting factual content of news.We propose Misinfo Reaction Frames (MRF), a pragmatic formalism for modeling how readers might react to a news headline. In contrast to categorical schema, our free-text dimensions provide a more nuanced way of understanding intent beyond being benign or malicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced dataset of reactions to over 25k news headlines focusing on global crises: the Covid-19 pandemic, climate change, and cancer. Empirical results confirm that it is indeed possible for neural models to predict the prominent patterns of readers’ reactions to previously unseen news headlines. Additionally, our user study shows that displaying machine-generated MRF implications alongside news headlines to readers can increase their trust in real news while decreasing their trust in misinformation. Our work demonstrates the feasibility and importance of pragmatic inferences on news headlines to help enhance AI-guided misinformation detection and mitigation.</abstract>
      <url hash="9b8ab0df">2022.acl-long.222</url>
      <bibkey>gabriel-etal-2022-misinfo</bibkey>
      <pwccode url="https://github.com/skgabriel/mrf-modeling" additional="false">skgabriel/mrf-modeling</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coaid">CoAID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/realnews">RealNews</pwcdataset>
    <title_ar>توقع الحكم القانوني عبر استخراج الحدث مع القيود</title_ar>
      <title_pt>Previsão de julgamento legal por meio de extração de eventos com restrições</title_pt>
      <title_es>Predicción de sentencias legales mediante extracción de eventos con restricciones</title_es>
      <title_ja>制約付きイベント抽出による法的判断予測</title_ja>
      <title_zh>以约束事取决占候</title_zh>
      <title_hi>बाधाओं के साथ घटना निष्कर्षण के माध्यम से कानूनी निर्णय भविष्यवाणी</title_hi>
      <title_ga>Réamh-mheastachán Breithiúnais Dlí trí Eastóscadh Imeachta le Srianta</title_ga>
      <title_hu>Jogi ítélet-előrejelzés esemény kivonásával, korlátozásokkal</title_hu>
      <title_el>Πρόβλεψη νομικής κρίσης μέσω εξαγωγής γεγονότων με περιορισμούς</title_el>
      <title_ka>კონფიგურაციის გადაწყვეტი მოვლენების გამოყენებით</title_ka>
      <title_lt>Teisinio sprendimo numatymas ištraukiant įvykius su apribojimais</title_lt>
      <title_kk>Оқиға тарқату шектері арқылы оқиғаның шектері</title_kk>
      <title_it>Predizione del giudizio legale tramite estrazione di eventi con vincoli</title_it>
      <title_mk>Предвидување на правната пресуда преку екстракција на настани со ограничувања</title_mk>
      <title_ms>Prediksi Hakim Hukum melalui Ekstraksi Peristiwa Dengan Hadangan</title_ms>
      <title_ml>കോണ്‍സ്റ്റെന്‍റുകളോടൊപ്പം നിയമവിധിയുടെ മുന്‍ഗണനം</title_ml>
      <title_mn>Хязгаарлалтайгаар үйл явдал гаргахад хууль шүүмжлэх төлөвлөгөө</title_mn>
      <title_mt>Tbassir ta’ Sentenza Legali permezz ta’ Estrazzjoni ta’ Avvenimenti b’Limitazzjonijiet</title_mt>
      <title_no>Legg avtale ved hendinga med begrensningar</title_no>
      <title_pl>Prognoza orzeczeń prawnych poprzez ekstrakcję zdarzeń z ograniczeniami</title_pl>
      <title_ro>Predicția judecății judecătorești prin extragerea evenimentelor cu constrângeri</title_ro>
      <title_sr>Predviđanje pravnog suda putem izvlačenja događaja sa ograničenjima</title_sr>
      <title_si>නීතිය නිරීක්ෂණ ප්‍රධානය අවස්ථානය අවස්ථානය නිර්මාණය සඳහා ප්‍රධානය</title_si>
      <title_so>Ka hortagga xukunka sharciga ee ka soo bixinta dhaqdhaqaalaha</title_so>
      <title_sv>Rättslig dom förutsägelse via händelseutvinning med begränsningar</title_sv>
      <title_ta>நிகழ்வு வெளியேறுதல் மூலம் சட்ட விதிவாதிப்பு முன்னுரிமை</title_ta>
      <title_ur>قانونی فیصلہ پیش بینی حادثہ اٹھانے کے ذریعہ</title_ur>
      <title_uz>@ info: whatsthis</title_uz>
      <title_vi>Quyết định pháp lý qua sự tiết lộ với giới hạn</title_vi>
      <title_bg>Прогнозиране на съдебно решение чрез извличане на събитие с ограничения</title_bg>
      <title_nl>Juridische oordeelvoorspelling via gebeurtenisextractie met beperkingen</title_nl>
      <title_da>Juridisk dom forudsigelse via hændelsesudtrækning med begrænsninger</title_da>
      <title_hr>Predvidba pravnog suda putem izvlačenja događaja s ograničenjima</title_hr>
      <title_id>Prediksi Hakim Hukum melalui Ekstraksi Kejadian Dengan Kebatasan</title_id>
      <title_ko>구속 사건 추출에 기초한 법률 판결 예측</title_ko>
      <title_de>Rechtsprechung über Ereignisextraktion mit Einschränkungen</title_de>
      <title_fa>پیشنهاد قضاوت قانونی از طریق اخراج اتفاق با محدودیت</title_fa>
      <title_tr>Ýabşyrlamak bilen Hudaly Çikleme Wasp</title_tr>
      <title_af>Regtige oordeel voorvloediging deur gebeurtenis uitpak met beheinings</title_af>
      <title_sw>Utawala wa Sheria kupitia Utoa wa Matukio na Mashitaka</title_sw>
      <title_sq>Parashikimi i gjykimit ligjor nëpërmjet ekstrahimit të ngjarjeve me kufizime</title_sq>
      <title_am>የሕግ ፍርድ ምርጫዎች</title_am>
      <title_hy>Օրինակական դատաստանի կանխատեսումը իրադարձությունների հանման միջոցով սահմանափակումներով</title_hy>
      <title_az>凉饲捬즙湤楲楬즙渠噡煩祹즙琠쎇쒱燄뇅鿄넠癡獩瓉饳楬즙⁈慱焠奡牧쒱污浡⁐牥摩捴楯渊</title_az>
      <title_bn>প্রশিক্ষকদের সাথে ইভেন্ট এক্সট্র্যাকশনের মাধ্যমে আইন বিচার প্রেকশন</title_bn>
      <title_cs>Předpověď právního rozsudku prostřednictvím extrakce událostí s omezeními</title_cs>
      <title_et>Õigusliku kohtuotsuse prognoosimine sündmuste väljavõtmise kaudu piirangutega</title_et>
      <title_bs>Predviđanje pravnog suda putem izvlačenja događaja s ograničenjima</title_bs>
      <title_ca>Predicció del judici legal a través de l'extracció d'eventos amb restriccions</title_ca>
      <title_fi>Oikeudellisen tuomion ennustaminen tapahtumapurkamisen ja rajoitusten avulla</title_fi>
      <title_jv>Perusahaan hukum sing dibenalke Kemerdekaan Winter</title_jv>
      <title_sk>Napoved pravne sodbe prek odvzema dogodkov z omejitvami</title_sk>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_bo>ཚད་འཛིན་བྱས་པའི་བྱ་འགུལ་ཕྱིར་འདོན་དང་བསྟུན་ནས་ཉེན་ཁ་ཤས་ཆེན་བྱེད་པ</title_bo>
      <title_he>צפוי שיפוט משפטי באמצעות חיתוך אירועים עם מגבלות</title_he>
      <abstract_ar>في حين تم إحراز تقدم كبير في مهمة التنبؤ بالحكم القانوني (LJP) في السنوات الأخيرة ، يمكن أن تُعزى التنبؤات غير الصحيحة التي قدمتها نماذج SOTA LJP جزئيًا إلى فشلهم في (1) تحديد معلومات الحدث الرئيسي التي تحدد الحكم ، و (2) استغلال قيود تناسق المهام المتقاطعة الموجودة بين المهام الفرعية لـ LJP. لمعالجة نقاط الضعف هذه ، نقترح EPM ، نموذج تنبؤ قائم على الأحداث مع قيود ، والذي يتجاوز نماذج SOTA الحالية في الأداء على مجموعة بيانات LJP قياسية.</abstract_ar>
      <abstract_es>Si bien se han logrado avances significativos en la tarea de Predicción de Sentencias Legales (LJP) en los últimos años, las predicciones incorrectas realizadas por los modelos SOTA LJP pueden atribuirse en parte a su incapacidad para (1) localizar la información clave del evento que determina el juicio y (2) aprovechar la coherencia entre tareas. restricciones que existen entre las subtareas de LJP. Para abordar estas debilidades, proponemos EPM, un modelo de predicción basado en eventos con restricciones, que supera a los modelos SOTA existentes en rendimiento en un conjunto de datos LJP estándar.</abstract_es>
      <abstract_pt>Embora um progresso significativo tenha sido feito na tarefa de Previsão de Julgamento Judicial (LJP) nos últimos anos, as previsões incorretas feitas pelos modelos SOTA LJP podem ser atribuídas em parte à sua falha em (1) localizar as principais informações do evento que determinam o julgamento, e (2) explorar as restrições de consistência entre tarefas que existem entre as subtarefas do LJP. Para resolver esses pontos fracos, propomos o EPM, um modelo de previsão baseado em eventos com restrições, que supera os modelos SOTA existentes em desempenho em um conjunto de dados LJP padrão.</abstract_pt>
      <abstract_ja>近年、法的判断予測（ Legal Judgment Prediction, LJP ）のタスクで大きな進展が見られたが、SOTA LJPモデルが行った誤った予測は、(1)判断を決定する重要なイベント情報を特定できなかったこと、(2) LJPのサブタスク間に存在するクロスタスク整合性制約を利用できなかったことが一因である。これらの弱点に対処するために、私たちはEPMを提案します。これは制約のあるイベントベースの予測モデルで、標準的なLJPデータセットのパフォーマンスで既存のSOTAモデルを上回ります。</abstract_ja>
      <abstract_zh>虽近岁断法(LJP)取重大进展于法,而SOTA LJP形之失疑归因于其未(1)定位决机,及(2)用LJP子之跨务一致性约束。 为此等弱,吾等发EPM,此乃事之占,有约束,于准LJP数集上过于今SOTA。</abstract_zh>
      <abstract_hi>जबकि हाल के वर्षों में कानूनी निर्णय भविष्यवाणी (एलजेपी) के कार्य पर महत्वपूर्ण प्रगति हुई है, सोटा एलजेपी मॉडल द्वारा की गई गलत भविष्यवाणियों को उनकी विफलता के लिए जिम्मेदार ठहराया जा सकता है (1) निर्णय निर्धारित करने वाली प्रमुख घटना जानकारी का पता लगाने के लिए, और (2) एलजेपी के उप-कार्यों के बीच मौजूद क्रॉस-टास्क स्थिरता बाधाओं का फायदा उठाना। इन कमजोरियों को संबोधित करने के लिए, हम ईपीएम, बाधाओं के साथ एक ईवेंट-आधारित भविष्यवाणी मॉडल का प्रस्ताव करते हैं, जो एक मानक एलजेपी डेटासेट पर प्रदर्शन में मौजूदा SOTA मॉडल को पार करता है।</abstract_hi>
      <abstract_ga>Cé go bhfuil dul chun cinn suntasach déanta ar thasc na Réamhthuar Breithiúnais Dlí (LJP) le blianta beaga anuas, is féidir na réamh-mheastacháin mhíchearta a rinne samhlacha SOTA LJP a chur i leith go páirteach as a dteip (1) an fhaisnéis príomhtheagmhais a chinneann an breithiúnas a aimsiú, i.e. agus (2) leas a bhaint as na srianta comhsheasmhachta tras-tasc atá ann i measc fhothascanna an LJP. Chun aghaidh a thabhairt ar na laigí seo, molaimid EPM, Samhail Tuartha Imeachta-bhunaithe le srianta, a sháraíonn na samhlacha SOTA atá ann cheana i bhfeidhmíocht ar thacar sonraí caighdeánach LJP.</abstract_ga>
      <abstract_ka>მნიშვნელოვანი პროგრესი შეიძლება შემდეგ წინ წინ წინ წინ გადაწყვეტილების (LJP) პარამეტრებში, როცა SOTA LJP მოდელების შეცდომა წინაწყვეტილება შეიძლება ატრიბუტირება, რომელიც გადაწყვეტილებას განსაზღვრას გასაკეთებელი მოვლენის ინფორმაციას (1) დააღმოჩენ ჩვენ EPM-ს, მოვლენების დასაბამისი მოდექციის მოდელი, რომელიც უფრო უფრო მუშაობს SOTA მოდელების გამოყენებაში, სტანდარტული LJP მონაცემების შესაბამისთვის.</abstract_ka>
      <abstract_hu>Bár az elmúlt években jelentős előrelépés történt a Jogi Ítélet-előrejelzés (LJP) feladatában, a SOTA LJP modellek által tett helytelen előrejelzések részben annak tulajdoníthatók, hogy (1) nem találták meg az ítéletet meghatározó kulcsfontosságú eseményinformációkat, és (2) nem használták ki az LJP alcsoportjai között fennálló kölcsönös feladatkövetkezetességi korlátokat. Ezeknek a hiányosságoknak a kezelésére javasoljuk az EPM-et, egy eseményalapú előrejelzési modellt, amely meghaladja a meglévő SOTA modelleket egy standard LJP adatkészleten.</abstract_hu>
      <abstract_el>Ενώ έχει σημειωθεί σημαντική πρόοδος όσον αφορά το έργο της πρόβλεψης νομικών κρίσεων (LJP) τα τελευταία χρόνια, οι λανθασμένες προβλέψεις που έγιναν από τα μοντέλα SOTA LJP μπορούν να αποδοθούν εν μέρει στην αποτυχία τους (1) να εντοπίσουν τις βασικές πληροφορίες συμβάντος που καθορίζουν την κρίση και (2) να εκμεταλλευτούν τους περιορισμούς συνοχής μεταξύ των επιμέρους εργασιών του LJP. Για να αντιμετωπιστούν αυτές οι αδυναμίες, προτείνουμε ένα μοντέλο πρόβλεψης βασισμένο σε συμβάντα με περιορισμούς, το οποίο ξεπερνά τα υπάρχοντα μοντέλα SOTA σε απόδοση σε ένα τυποποιημένο σύνολο δεδομένων LJP.</abstract_el>
      <abstract_it>Mentre negli ultimi anni sono stati compiuti progressi significativi sul compito della previsione del giudizio legale (LJP), le previsioni errate fatte dai modelli SOTA LJP possono essere attribuite in parte alla loro incapacità di (1) individuare le informazioni chiave sugli eventi che determinano il giudizio e (2) sfruttare i vincoli di coerenza cross-task esistenti tra le sottoattività di LJP. Per affrontare queste debolezze, proponiamo EPM, un modello di previsione basato su eventi con vincoli, che supera i modelli SOTA esistenti in termini di prestazioni su un set di dati LJP standard.</abstract_it>
      <abstract_kk>Соңғы жылдарда SOTA LJP үлгілерінің тапсырмасының (LJP) тапсырмасында маңызды жұмыс істегенде, SOTA LJP үлгілерінің жарамсыз тапсырмаларын (1) анықтайтын кілт оқиға мәліметін табуға болады, және (2) LJP ішкі тапсырмалардың ортасындағы көп тапсырмалардың тұра Бұл зақымдарды өзгерту үшін біз EPM дегенді, оқиға негіздеген Prediction үлгісін шектеу үлгісін қолданамыз. Бұл барлық SOTA үлгілерін стандартты LJP деректер жиынында орындалатын.</abstract_kk>
      <abstract_lt>While significant progress has been made on the task of Legal Judgment Prediction (LJP) in recent years, the incorrect predictions made by SOTA LJP models can be attributed in part to their failure to (1) locate the key event information that determines the judgment, and (2) exploit the cross-task consistency constraints that exist among the subtasks of LJP.  Siekiant pašalinti šiuos trūkumus, siūlome EPM, įvykiais pagrįsto prognozavimo modelį su apribojimais, kuris viršija esamus SOTA modelius, veikiančius standartiniame LJP duomenų rinkinyje.</abstract_lt>
      <abstract_mk>Иако во последниве години е постигнат значителен напредок во врска со задачата на Предвидувањето на правната судбина (ЛЈП), погрешните предвидувања направени од моделите на СОТА ЛЈП може делумно да се припишат на нивното неуспех (1) да се лоцираат клучните информации за настаните кои ја одредуваат судбината и (2) да се искористат ограничувањата на константноста на крстозадачите кои пост За да ги решиме овие слабости, ние предлагаме ЕПМ, модел на предвидување базиран на настани со ограничувања, кој ги надминува постојните модели на СОТА во изведба на стандарден податок на ЛЈП.</abstract_mk>
      <abstract_ms>Walaupun kemajuan yang signifikan telah dilakukan pada tugas Penyataan Hakim Hukum (LJP) dalam tahun-tahun terakhir, ramalan yang salah dilakukan oleh model LJP SOTA boleh dianggap sebahagian daripada kegagalan mereka untuk (1) mencari maklumat peristiwa utama yang menentukan penghakiman, dan (2) mengeksploitasikan batasan konsistensi tugas-salib yang wujud di antara sub-tanya LJP. Untuk mengatasi kelemahan ini, kami cadangkan EPM, Model Prediksi Berasas Kejadian dengan keterangan, yang melebihi model SOTA yang wujud dalam prestasi pada set data LJP piawai.</abstract_ms>
      <abstract_ml>അടുത്ത വര്‍ഷങ്ങളില്‍ നിയമവിധിയുടെ (LJP) ജോലിയില്‍ പ്രവർത്തിച്ചിരിക്കുന്ന പ്രവർത്തികമായ പ്രവര്‍ത്തനങ്ങള്‍ നടത്തിയിരിക്കുമ്പോള്‍, SOTA LJP മോഡലുകള്‍ ഉണ്ടാക്കിയ തെറ്റായ പ്രവചനങ്ങള്‍ ഭാഗമായി പ്രാവര്‍ത് ഈ ദുര്‍ബല്യങ്ങളെ സംസാരിക്കാന്‍ ഞങ്ങള്‍ എപിഎം പ്രിസ്റ്റ് ചെയ്യുന്നു, സാധാരണ LJP ഡാറ്റാസറ്റില്‍ നിലവിലുള്ള SOTA മോഡലുകള്‍ പ്രവര്‍ത്തിപ്പ</abstract_ml>
      <abstract_mt>Filwaqt li sar progress sinifikanti fil-kompitu tat-Tbassir tas-Sentenza Legali (LJP) f’dawn l-aħħar snin, it-tbassir mhux korrett magħmul mill-mudelli tas-SOTA LJP jista’ jiġi attribwit parzjalment għan-nuqqas tagħhom li (1) jinstabu l-informazzjoni ewlenija dwar l-avveniment li tiddetermina s-sentenza, u (2) jisfruttaw ir-restrizzjonijiet ta’ konsistenza bejn ix-xogħlijiet li jeżistu fost is-sottomistoqsijiet tal-LJP. To address these weaknesses, we propose EPM, an Event-based Prediction Model with constraints, which surpasses existing SOTA models in performance on a standard LJP dataset.</abstract_mt>
      <abstract_mn>Сүүлийн жилүүдэд хуулийн Шүүмжлэх Эмчилгээ (LJP) даалгаврын тухай маш чухал хөгжлийн даалгавар гарч ирсэн ч SOTA LJP загварын буруу таамаглал (1) болон шүүмжлэг тодорхойлж чадахгүй (2) тодорхойлж чадах чухал үйл явдал мэдээллийг олж мэднэ. Эдгээр сул байдлыг зохицуулахын тулд бид EPM-г зөвшөөрөх боломжтой Хязгаарлалтын төлөвлөгөөний загвар, стандарт LJP өгөгдлийн суурь дээр суурилсан SOTA загваруудыг илүү өндөртэй.</abstract_mn>
      <abstract_pl>Chociaż w ostatnich latach poczyniono znaczące postępy w zakresie prognozowania orzeczeń prawnych (LJP), nieprawidłowe prognozy dokonane przez modele SOTA LJP można częściowo przypisać ich niezdolności do (1) zlokalizowania kluczowych informacji o zdarzeniu, które decydują o wyroku, oraz (2) wykorzystania ograniczeń spójności między zadaniami, które istnieją wśród podzadań LJP. Aby rozwiązać te słabości, proponujemy EPM, model predykcyjny oparty na zdarzeniach z ograniczeniami, który przewyższa istniejące modele SOTA pod względem wydajności standardowego zbioru danych LJP.</abstract_pl>
      <abstract_sr>Iako je u poslednjih godina ostvaren značajan napredak na zadatku predviđanja pravnog suda (LJP), nepravde predviđanja koje su napravili modeli SOTA LJP-a mogu biti pripisana na njihov neuspjeh (1) da pronađu ključnu informaciju o događajima koja određuje presudu, i (2) iskorištavaju ograničenja konsekvencije prekršaja koja postoje među podstavama LJP-a. Da bi se riješili ovim slabostima, predlažemo EPM, model predviđanja na osnovu događaja sa ograničenjima, koji prelazi postojećim SOTA modelima u izvođenju na standardnom setu podataka LJP-a.</abstract_sr>
      <abstract_ro>Deși s-au înregistrat progrese semnificative în ceea ce privește sarcina Predicției Judecății Juridice (LJP) în ultimii ani, previziunile incorecte făcute de modelele SOTA LJP pot fi atribuite în parte incapacității acestora de a (1) localiza informațiile cheie despre evenimente care determină hotărârea și (2) de a exploata constrângerile de coerență între sarcini care există printre subactivitățile LJP. Pentru a aborda aceste puncte slabe, propunem EPM, un Model de Predicție bazat pe evenimente cu constrângeri, care depășește modelele SOTA existente în performanță pe un set de date standard LJP.</abstract_ro>
      <abstract_no>Mens det er gjort viktig framgang på oppgåva av rettsprøytebruk (LJP) i løpet av siste år, kan dei feil foregåva som er gjort av SOTA LJP-modeller delvis bli tilgjengelege til å finna nøkkelhendingsinformasjonen som bestemmer sprøytebruken, og (2) bruka grensene for krusteoppgåver som finst mellom underspørsmålene av LJP. For å handtere desse tyrkene, foreslår vi EPM, eit hendingsbasert forhåndsvisningsmodell med begrensningar, som overpassar eksisterande SOTA-modeller i utføring på ein standard LJP-datasett.</abstract_no>
      <abstract_sv>Även om betydande framsteg har gjorts när det gäller uppgiften för rättslig bedömning (LJP) under de senaste åren, kan de felaktiga förutsägelser som gjorts av SOTA LJP-modeller delvis tillskrivas att de inte (1) lokaliserar den viktigaste händelseinformation som avgör domen, och (2) utnyttjar de begränsningar av konsekvens mellan olika uppgifter som finns bland LJP:s underuppgifter. För att åtgärda dessa svagheter föreslår vi EPM, en händelsebaserad förutsägelsemodell med begränsningar, som överträffar befintliga SOTA-modeller i prestanda på en standard LJP datauppsättning.</abstract_sv>
      <abstract_si>මෙහෙම විශේෂ ප්‍රධානය කරලා තියෙන්නේ නීතිය ප්‍රධානය (LJP) ක්‍රියාත්මක වෙනුවෙන්, SOTA LJP මොඩලයෙන් කරපු වැරදි ප්‍රධානය සඳහා ප්‍රධානය සඳහා ප්‍රධානය සඳහා ප්‍රධානය සඳහා ප්‍රධා අපි මේ දුර්වලට ප්‍රශ්නයක් කරන්න, EPM එක ප්‍රශ්නයක් කරනවා, සිද්ධ විශ්නයක් ප්‍රශ්නයක් තියෙනවා, ඒක ප්‍රශ්නයක් LJP දත්ත සැට වල ඉ</abstract_si>
      <abstract_ta>சமீபத்திய ஆண்டுகளில் சட்ட தீர்ப்பு முன்னேற்றம் செய்யப்பட்டுள்ளது இந்த பலஹீனங்களை விளக்க, நாம் EPM, ஒரு நிகழ்வு அடிப்படையான முன்னேற்றம் மாதிரி மாதிரியை பரிந்துரைக்கிறோம். இது ஏற்கனவே இருக்கும் SOTA மாதிர</abstract_ta>
      <abstract_so>While significant progress has been made on the task of Legal Judgment Prediction (LJP) in recent years, the incorrect predictions made by SOTA LJP models can be attributed in part to their failure to (1) locate the key event information that determines the judgment, and (2) exploit the cross-task consistency constraints that exist among the subtasks of LJP.  Si a an ugu hadlno tabaryaradaas, waxaan soo jeedaynaa EPM, taas oo ah Model qaynuunno ku saleysan dhacdooyinka, kaas oo ku qoran sameynta sameynta samooyinka SOTA oo ku qoran danbiyada caadiga ah ee LJP.</abstract_so>
      <abstract_ur>حالانکہ اگلے سالوں میں قانونی فیصلہ کی پیشنهاد (LJP) کے کام پر اہم اضافہ کی گئی ہے، سوٹا LJP موڈل کے ذریعہ سے غلطی پیشنهادیں ان کی غلطی کے بارے میں (1) کو پہنچا سکتی ہیں جو فیصلہ کا فیصلہ کرتا ہے، اور (2) کرس-ٹاکس کی تعلطی کے محدودیت کو استعمال کرتا ہے جو LJP کے نیچے محدودیت میں موجود ہیں. ان کمزوریوں کے بارے میں ہم EPM کی پیشنهاد کریں گے، ایک ایڈینٹ بنیاد کی پیشنهاد موڈل کی محدودیت کے ساتھ، جو ایک استاندارد LJP ڈیٹ سٹ پر موجود SOTA موڈل سے زیادہ گزر جاتا ہے.</abstract_ur>
      <abstract_uz>Yaqinda SOTA LJP modellari yaratilgan notoʻgʻri taʼminlovchi narsalar (LJP) vazifani aniqlash muvaffaqiyatsiz tugadi va LJP vazifalarida mavjud boʻlgan vazifalarni ishlatish mumkin. Bu kasalliklarni talab qilish uchun biz EPM, hodisa asosidagi Prediction Model bilan murakkab qilamiz. Bu holatda mavjud SOTA modellarini andoza LJP maʼlumot tizimini bajarishga moslash mumkin.</abstract_uz>
      <abstract_vi>Mặc dù trong những năm gần đây đã có tiến bộ đáng kể về nhiệm vụ định hạn phán quyết pháp (LJP) nhưng dự đoán sai của SOTA LJP có thể được cho là đôi khi chúng không xác định vị trí các thông tin quan trọng quyết định sự kiện quyết định quyết định quyết định quyết định quyết định quyết định quyết định quyết định, và (2) tận dụng những giới hạn liên quan giữa các chi tiết của LJP. Để giải quyết những yếu điểm này, chúng tôi đề nghị EPM, một mô hình dự đoán dựa vào sự kiện có hạn chế, mà vượt qua các mô hình SOE còn sót trong các trình phục vụ trên một bộ dữ liệu mẫu LJP tiêu chuẩn.</abstract_vi>
      <abstract_bg>Въпреки че през последните години е постигнат значителен напредък по задачата за прогнозиране на съдебните решения, неправилните прогнози, направени от моделите SOTA LJP, могат отчасти да бъдат приписани на неспособността им (1) да намерят ключовата информация за събитията, която определя решението, и (2) да експлоатират ограниченията за съгласуваност между задачите, които съществуват сред подзадачите на LJP. За да се справим с тези слабости, предлагаме модел за прогнозиране въз основа на събития с ограничения, който надминава съществуващите модели по производителност на стандартен набор от данни.</abstract_bg>
      <abstract_da>Selv om der i de seneste år er sket betydelige fremskridt med hensyn til opgaven med Juridisk Judgment Prediction (LJP), kan de ukorrekte forudsigelser fra SOTA LJP-modeller til dels tilskrives, at de ikke (1) har fundet de vigtigste begivenhedsoplysninger, der bestemmer dommen, og (2) udnytte de tværfaglige konsistensbegrænsninger, der findes blandt LJP's underopgaver. For at afhjælpe disse svagheder foreslår vi EPM, en begivenhedsbaseret forudsigelsesmodel med begrænsninger, som overgår eksisterende SOTA-modeller i ydeevne på et standard LJP datasæt.</abstract_da>
      <abstract_nl>Hoewel er in de afgelopen jaren aanzienlijke vooruitgang is geboekt met de taak van Legal Judgment Prediction (LJP), kunnen de onjuiste voorspellingen van SOTA LJP-modellen gedeeltelijk worden toegeschreven aan het feit dat zij (1) de belangrijkste gebeurtenisinformatie die het oordeel bepaalt, niet hebben kunnen lokaliseren en (2) de cross-task consistentie beperkingen benutten die bestaan tussen de subtaken van LJP. Om deze zwakke punten aan te pakken, stellen we EPM voor, een Event-based Prediction Model met beperkingen, dat bestaande SOTA modellen overtreft in prestaties op een standaard LJP dataset.</abstract_nl>
      <abstract_id>Sementara kemajuan yang signifikan telah dilakukan dalam tugas Pengadilan Hukum (LJP) dalam tahun-tahun terakhir, prediksi yang salah yang dilakukan oleh model LJP SOTA dapat dianggap sebagian dari gagal mereka untuk (1) menemukan informasi peristiwa kunci yang menentukan penghakiman, dan (2) mengeksploitasi batas konsistensi tugas salib yang ada di antara subtasks LJP. Untuk mengatasi kelemahan ini, kami mengusulkan EPM, Model Prediksi Berdasarkan Kejadian dengan batas, yang melebihi model SOTA yang ada dalam prestasi pada set data standar LJP.</abstract_id>
      <abstract_ko>최근 몇 년 동안 법적 판단 예측(LJP) 임무에 큰 진전을 거두었지만, SOTA LJP 모델의 잘못된 예측 부분은 그들이 판단을 결정하는 관건적인 사건 정보를 찾지 못했기 때문이며, LJP 하위 임무 사이에 존재하는 크로스 임무 일치성 제약을 이용하지 못했기 때문이다.이러한 단점을 해결하기 위해 우리는 이벤트에 기반한 제약이 있는 예측 모델인 EPM을 제시했는데 이것은 표준 LJP 데이터 집합에서 기존의 SOTA 모델을 능가한다.</abstract_ko>
      <abstract_hr>Iako je u posljednjih godina ostvaren značajan napredak na zadatku predviđanja pravnog suda (LJP), nepravedne predviđanja koje su napravili modeli SOTA LJP-a mogu biti pripisane dio njihovog neuspjeha (1) locirati ključne informacije o događajima koji određuju sudbinu, te (2) iskoristiti ograničenje konsekvencije prekršaja koja postoje među podstavama LJP-a. Da bi se riješili ovim slabostima, predlažemo EPM, model predviđanja na temelju događaja s ograničenjima, koji prelazi postojeće SOTA modele u provedbi na standardnom setu podataka LJP-a.</abstract_hr>
      <abstract_de>Während bei der Aufgabe der Legal Judgment Prediction (LJP) in den letzten Jahren erhebliche Fortschritte erzielt wurden, können die falschen Vorhersagen von SOTA LJP Modellen zum Teil darauf zurückzuführen sein, dass sie (1) die wichtigsten Ereignisinformationen, die das Urteil bestimmen, nicht lokalisieren und (2) die aufgabenübergreifenden Konsistenzbeschränkungen ausnutzen konnten, die unter den Teilaufgaben von LJP existieren. Um diese Schwachstellen zu beheben, schlagen wir EPM vor, ein ereignisbasiertes Vorhersagemodell mit Einschränkungen, das bestehende SOTA-Modelle in Bezug auf die Leistung eines Standard-LJP-Datensatzes übertrifft.</abstract_de>
      <abstract_af>Alhoewel betekende vordering is gemaak op die taak van Regtige oordeel voorskou (LJP) in onlangse jaar, kan die verkeerde voorskou wat deur SOTA LJP modele gemaak is, in deel aan hul mislukking om (1) die sleutel gebeurtenis inligting te vind wat die oordeel bepaal, en (2) die kruis-taak konsistensiehouers wat bestaan onder die subtaske van LJP. Om hierdie swakhede te adres, voorstel ons EPM, 'n Begeefde-gebaseerde voorskrifte-model met beheininge, wat oorgaan bestaande SOTA-modele in uitvoering op 'n standaard LJP-datastel.</abstract_af>
      <abstract_fa>در حالی که در سال های اخیر پیش‌بینی‌های قانونی (LJP) پیش‌بینی‌های غلط توسط مدل‌های SOTA LJP، پیش‌بینی‌های غلطی که توسط مدل‌های SOTA LJP انجام داده‌اند، بخشی از شکست‌هایشان به (1) اطلاعات کلیدی رویداد را که تصمیم دادن قضاوت را تعیین می‌کند، و (2) محدودیت‌های هماهنگی‌کننده‌ای که در می برای حل این ضعیفی، ما EPM را پیشنهاد می‌کنیم، یک مدل پیشنهاد بر اساس رویداد با محدودیت، که مدل‌های SOTA موجود در انجام روی یک مجموعه داده‌های LJP استاندارد از حد گذشته است.</abstract_fa>
      <abstract_tr>Soňky ýyllar içinde Ulusal Haýsy Prediksiýasy (LJP) täsirinde wajyp ilerlemesi bolup geçdi. SOTA LJP modelleri tarapyndan ýalňyş önlemeler (1) hökmü berýän açyk wajyp maglumatyny ýerleşdirip biler, we (2) LJP subtasklarynyň içinde bar cross-task durmuşy mümkin edýär Bu zaçyrlyklara çözmek üçin, EPM'i, taýdan tabanly bir Prediksiýa Modeli çyzgylyklar bilen üýtgedýäris. Bu SOTA nusgalaryny standart LJP datawatlarynda etmäge üstün edýär.</abstract_tr>
      <abstract_sw>Wakati maendeleo makubwa yamefanyika katika kazi ya Udhibiti wa Sheria (LJP) katika miaka ya hivi karibuni, utabiri usio sahihi uliofanywa na Mradi wa SOTA LJP unaweza kuwepo sehemu ya kushindwa kutafuta taarifa za tukio muhimu zinazohitimisha hukumu, na (2) kutumia vikwazo vikwazo vinavyofanana na kazi za kupitia mipango ambayo yanakuwepo miongoni mwa kazi za LJP. Ili kuzungumzia udhaifu huu, tunapendekeza Waziri Mkuu wa EPM, Mradi wa Udhibiti anayeishi matukio yenye vikwazo, ambacho kinapitisha mifano ya SOTA yanayopo kwenye utendaji wa takwimu za LJP za kiwango cha kawaida.</abstract_sw>
      <abstract_sq>Ndërsa përparimi i rëndësishëm është bërë në detyrën e parashikimit të gjykimit ligjor (LJP) në vitet e fundit, parashikimet e gabuara të bërë nga modelet SOTA LJP mund të atribuohen pjesërisht në dështimin e tyre në (1) gjetjen e informacionit kryesor të ngjarjes që përcakton gjykimin dhe (2) shfrytëzimin e kufizimeve të konsistencës ndër detyrat që ekzistojnë midis nëndetyrave të LJP. Për t'i trajtuar këto dobësi, ne propozojmë EPM, një model parashikimi me bazë në ngjarje me kufizime, i cili tejkalon modelet ekzistuese të SOTA në shfaqje në një sistem të dhënash standard LJP.</abstract_sq>
      <abstract_az>Son illərdə Yaxşı hökm Prediction (LJP) işində möhkəm tədbir edilmiş halda, SOTA LJP modellərinin yanlış tədbir edilmiş tədbirlərin bir hissəsi olaraq (1) hökmünü müəyyən edən anahtar olaraq məlumatlarını yerinə yetirə bilər və (2) LJP subtasklarının arasında olan çətinlikli tədbirləri istifadə edə bilər. Bu zəifliklərdən çəkinmək üçün EPM'i təbliğ edirik, olaraq-tabanlı təbliğ Modeli, standart LJP verilənlərin quruluğunda esoqları SOTA modellərindən üstün edir.</abstract_az>
      <abstract_bs>Iako je u posljednjih godina ostvaren značajan napredak na zadatku predviđanja pravnog suda (LJP), nepravde predviđanja koje su izvedene modelima SOTA LJP-a mogu biti pripisana na njihov neuspjeh (1) da pronađu ključnu informaciju o događajima koja određuje presudu, i (2) iskorištavaju ograničenja konsekvencije prekršaja koja postoje među podstavama LJP-a. Da bi se riješili ovim slabostima, predlažemo EPM, model predviđanja na osnovu događaja s ograničenjima, koji prelazi postojećim SOTA modelima u provedbi na standardnom setu podataka LJP-a.</abstract_bs>
      <abstract_am>በአሁኑ ዓመታት ውስጥ የሕግ ፍርድ አካባቢ (LJP) ስራ ላይ የተደረገ ግንኙነት ሲሆን፣ የSOTA LJP ዓይነቶች የተደረገውን የስህተት ትንቢት በክፍል በመጠቀም ይችላል (1) ፍርድን በሚያረጋግጥ የቁልፍ ጉዳይ መረጃ ማግኘት እና (2) ከLJP ስራቶች መካከል ያለውን የሥልጣን ግንኙነት ግንኙነት ግንኙነት በመጠቀም ይችላል። እነዚህን ድካሞች ለመቀበል፣ የሁኔታ ላይ የተመሠረተውን የፕሮግራም ሞዴል፣ የSOTA ሞዴላዎችን በተቃውሞ LJP ዳታተር ማድረጊያውን ለመጠቀም እናስጀጋለን፡፡</abstract_am>
      <abstract_hy>Մինչդեռ վերջին տարիների ընթացքում կարևոր առաջընթաց է կատարվել իրավաբանական դատողության կանխատեսման (LJP) խնդրի վրա, սխալ կանխատեսումները, որ կատարվել են SOta LJP մոդելների կողմից, կարող են մասամբ պատասխանվել նրանց ձախողվելուն (1) գտնելու կարևոր իրադարձությունների տեղեկատվությունը, որը որոշում է դատողությունը, և (2) օգտագործելու LJP ենթախնդիրների Այս թույլությունները լուծելու համար մենք առաջարկում ենք EpM-ը, իրադարձությունների վրա հիմնված կանխատեսման մոդելը, որն ունի սահմանափակումներ, որը գերազանցում է գոյություն ունեցող SOta-մոդելները, երբ արտադրվում է ստանդարտ LJP տվյալների համակար</abstract_hy>
      <abstract_bn>সাম্প্রতিক বছরগুলোতে আইন বিচার প্রেসিডেন্ট (এলজেপি) কাজের উপর গুরুত্বপূর্ণ অগ্রগতি তৈরি করা হয়েছে, এসওটা এলজেপি মডেলের ভুল ভবিষ্যৎবাণী তাদের ব্যর্থ হয়েছে (১) বিচারের সিদ্ধান্ত নির্ধারণের মূল এই দুর্বলতা নিয়ে কথা বলার জন্য আমরা ইপিএমিটির প্রস্তাব দিচ্ছি একটি ইভেন্ট ভিত্তিক ভিত্তিক প্রেসিডেশন মডেল, যা স্ট্যান্ডার্ড এলজেপি ডাটাসেট</abstract_bn>
      <abstract_ca>Malgrat que s'ha aconseguit progrés significatius en la tasca de la Predicció Jurídica (LJP) en els últims anys, les prediccions incorrectes fetes pels models SOTA LJP poden ser atribuïdes en part al seu fracàs de (1) localitzar la informació clau sobre l'evento que determina el judici, i (2) explotar les restriccions de consistencia entre les tasques cruzades que existeixen entre les subtaskes de LJP. To address these weaknesses, we propose EPM, an Event-based Prediction Model with constraints, which surpasses existing SOTA models in performance on a standard LJP dataset.</abstract_ca>
      <abstract_cs>Zatímco v posledních letech bylo dosaženo významného pokroku v oblasti předpovědi právního rozsudku (LJP), nesprávné předpovědi modelů SOTA LJP lze částečně přičíst jejich neschopnosti (1) nalézt klíčové informace o událostech, které určují rozsudek, a (2) využít omezení konzistence mezi úkoly, která existují mezi dílúkoly LJP. Pro řešení těchto slabých stránek navrhujeme EPM, událostně založený predikční model s omezeními, který překonává existující SOTA modely ve výkonu na standardní LJP datové sadě.</abstract_cs>
      <abstract_et>Kuigi viimastel aastatel on tehtud märkimisväärseid edusamme õiguslike kohtuotsuste prognoosimise ülesandes, võib SOTA LJP mudelite ebaõigeid prognoose osaliselt seostada sellega, et need ei suutnud (1) leida otsust määravat olulist sündmuste teavet ja (2) kasutada ära LJP alamülesannete hulgas esinevaid ülesannetevahelise järjepidevuse piiranguid. Nende puuduste kõrvaldamiseks pakume välja EPM, piirangutega sündmuspõhine prognoosimismudel, mis ületab olemasolevaid SOTA mudeleid standardse LJP andmekogumi jõudluse poolest.</abstract_et>
      <abstract_fi>Vaikka oikeudellisten tuomioiden ennakoinnin (Legal Judgment Prediction, LJP) tehtävässä on viime vuosina edistytty merkittävästi, SOTA LJP -mallien virheellisten ennusteiden voidaan katsoa osittain johtuvan siitä, ettei niissä ole (1) löydetty tuomiota määrittäviä keskeisiä tapahtumatietoja ja (2) hyödynnetty LJP:n alitehtävissä esiintyviä tehtävien välistä johdonmukaisuutta koskevia rajoituksia. Näiden heikkouksien korjaamiseksi ehdotamme EPM:tä, tapahtumapohjaista ennustemallia, jolla on rajoituksia, joka ylittää nykyiset SOTA-mallit standardissa LJP-aineistossa.</abstract_fi>
      <abstract_jv>Alpha Ditawak dhéwé nggambar weakness iki, kita ngobudhakan PPM, model prediksiyon sing basa gambar n'ar aturan sing bisa nguasakno, dadi wis mulasar model sing misil nang atawa iki bakal terus nyimpen dataset Ljp awak dhéwé.</abstract_jv>
      <abstract_ha>Alhãli kuwa an sami mafariko mai girma a kan aikin Legal Domainment (LJP) a cikin shekara na farko, watau misãlai wanda aka aikata na SOTA LJP, za'a iya ƙayyade shi rabo da rabo zuwa (1) ya gane information ga maɓallin halin da ke hukunta hukuncin, kuma (2) ya yi amfani da kandamta ta mai daidaita wa-aikin da ke cikin buƙatan aikin LJP. Domin ka yi magana ga maras wannan rauni, Muke buƙata wata Model Prediction a kan Gani-Gani, da kuma ana sami misãlai da SOTA wanda ke gaba a kan cikakken danne na LJP na daidaita.</abstract_ha>
      <abstract_sk>Čeprav je bil v zadnjih letih dosežen znaten napredek pri nalogi napovedovanja pravnih sodb (LJP), je mogoče napačne napovedi modelov SOTA LJP deloma pripisati temu, da niso (1) našli ključnih informacij o dogodkih, ki določajo sodbo, in (2) izkoristili omejitev mednalog skladnosti, ki obstajajo med podnalogami LJP. Za odpravo teh pomanjkljivosti predlagamo EPM, model napovedovanja dogodkov z omejitvami, ki presega obstoječe modele SOTA glede zmogljivosti standardnega nabora podatkov LJP.</abstract_sk>
      <abstract_he>למרות שהתקדמות משמעותית נעשתה במשימה של שיפוט משפטי (LJP) בשנים האחרונות, התחזויות הלא נכונות שעשויות על ידי דוגמנים של SOTA LJP יכולות להיות מחויבות חלקית לכישלותם לאתר (1) מידע האירוע המפתח שמקבע את השיפוט, ו (2) לנצל את חובלות התקבילות המשימה הצלבית שקיים בין התשובות של LJP. כדי להתמודד עם החולשות האלה, אנו מציעים EPM, מודל תקיפה מבוסס על אירועים עם מחסומים, שמעביר את מודלים SOTA קיימים בהופעה על קבוצת מידע LJP סטנדרטית.</abstract_he>
      <abstract_bo>While significant progress has been made on the task of Legal Judgment Prediction (LJP) in recent years, the incorrect predictions made by SOTA LJP models can be attributed in part to their failure to (1) locate the key event information that determines the judgment, and (2) exploit the cross-task consistency constraints that exist among the subtasks of LJP. To address these weaknesses, we propose EPM, an Event-based Prediction Model with constraints, which surpasses existing SOTA models in performance on a standard LJP dataset.</abstract_bo>
      </paper>
    <paper id="224">
      <title>Achieving Conversational Goals with Unsupervised Post-hoc Knowledge Injection</title>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last></author>
      <author><first>Harsh</first><last>Jhamtani</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>3140-3153</pages>
      <abstract>A limitation of current neural dialog models is that they tend to suffer from a lack of specificity and informativeness in generated responses, primarily due to dependence on training data that covers a limited variety of scenarios and conveys limited knowledge. One way to alleviate this issue is to extract relevant knowledge from external sources at decoding time and incorporate it into the dialog response. In this paper, we propose a post-hoc knowledge-injection technique where we first retrieve a diverse set of relevant knowledge snippets conditioned on both the dialog history and an initial response from an existing dialog model. We construct multiple candidate responses, individually injecting each retrieved snippet into the initial response using a gradient-based decoding method, and then select the final response with an unsupervised ranking step. Our experiments in goal-oriented and knowledge-grounded dialog settings demonstrate that human annotators judge the outputs from the proposed method to be more engaging and informative compared to responses from prior dialog systems. We further show that knowledge-augmentation promotes success in achieving conversational goals in both experimental settings.</abstract>
      <url hash="43ff93cb">2022.acl-long.224</url>
      <attachment type="software" hash="c828ce18">2022.acl-long.224.software.zip</attachment>
      <bibkey>majumder-etal-2022-achieving</bibkey>
      <pwccode url="https://github.com/majumderb/poki" additional="false">majumderb/poki</pwccode>
    </paper>
    <paper id="225">
      <title>Generated Knowledge Prompting for Commonsense Reasoning</title>
      <author><first>Jiacheng</first><last>Liu</last></author>
      <author><first>Alisa</first><last>Liu</last></author>
      <author><first>Ximing</first><last>Lu</last></author>
      <author><first>Sean</first><last>Welleck</last></author>
      <author><first>Peter</first><last>West</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>3154-3169</pages>
      <abstract>It remains an open question whether incorporating external knowledge benefits <a href="https://en.wikipedia.org/wiki/Commonsense_reasoning">commonsense reasoning</a> while maintaining the flexibility of pretrained sequence models To investigate this question we develop generated knowledge prompting which consists of generating knowledge from a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> then providing the knowledge as additional input when answering a question Our method does not require task specific supervision for knowledge integration or access to a structured knowledge base yet it improves performance of large scale state of the art models on four commonsense reasoning tasks achieving state of the art results on numerical commonsense NumerSense general commonsense CommonsenseQA 2.0 and scientific commonsense QASC benchmarks Generated knowledge prompting highlights large scale language models as flexible sources of external knowledge for improving <a href="https://en.wikipedia.org/wiki/Commonsense_reasoning">commonsense reasoning</a> Our code is available at \url<url>github.com/liujch1998/GKP</url>
      </abstract>
      <url hash="2093a82d">2022.acl-long.225</url>
      <bibkey>liu-etal-2022-generated</bibkey>
      <pwccode url="https://github.com/liujch1998/gkp" additional="false">liujch1998/gkp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/numersense">NumerSense</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
    </paper>
    <paper id="226">
      <title>Training Data is More Valuable than You Think A Simple and Effective Method by Retrieving from Training Data</title>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Yuwei</first><last>Fang</last></author>
      <author id="yang-liu-microsoft"><first>Yang</first><last>Liu</last></author>
      <author><first>Siqi</first><last>Sun</last></author>
      <author><first>Ruochen</first><last>Xu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>3170-3179</pages>
      <abstract>Retrieval based methods have been shown to be effective in NLP tasks via introducing external knowledge However the indexing and retrieving of large scale corpora bring considerable computational cost Surprisingly we found that REtrieving from the traINing datA REINA only can lead to significant gains on multiple NLG and NLU tasks We retrieve the labeled training instances most similar to the input text and then concatenate them with the input to feed into the model to generate the output Experimental results show that this simple method can achieve significantly better performance on a variety of NLU and NLG tasks including summarization machine translation language modeling and question answering tasks For instance our proposed method achieved state of the art results on XSum BigPatent and CommonsenseQA Our code is released https://github.com/microsoft/REINA</abstract>
      <url hash="8aaca7ae">2022.acl-long.226</url>
      <bibkey>wang-etal-2022-training</bibkey>
      <pwccode url="https://github.com/microsoft/reina" additional="false">microsoft/reina</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bigpatent">BigPatent</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/piqa">PIQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="229">
      <title>TruthfulQA Measuring How Models Mimic Human Falsehoods<fixed-case>T</fixed-case>ruthful<fixed-case>QA</fixed-case>: Measuring How Models Mimic Human Falsehoods</title>
      <author><first>Stephanie</first><last>Lin</last></author>
      <author><first>Jacob</first><last>Hilton</last></author>
      <author><first>Owain</first><last>Evans</last></author>
      <pages>3214-3252</pages>
      <abstract>We propose a benchmark to measure whether a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> is truthful in generating answers to questions The benchmark comprises questions that span categories including health law finance and politics We crafted questions that some humans would answer falsely due to a false belief or misconception To perform well models must avoid generating false answers learned from imitating human texts We tested GPT-3 GPT Neo J GPT-2 and a T5 based model The best <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> was truthful on of questions while human performance was Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans The largest models were generally the least truthful This contrasts with other NLP tasks where performance improves with model size However this result is expected if false answers are learned from the training distribution We suggest that scaling up models alone is less promising for improving truthfulness than fine tuning using training objectives other than imitation of text from the web</abstract>
      <url hash="899f2d01">2022.acl-long.229</url>
      <attachment type="software" hash="2ce91883">2022.acl-long.229.software.zip</attachment>
      <bibkey>lin-etal-2022-truthfulqa</bibkey>
      <pwccode url="https://github.com/sylinrl/truthfulqa" additional="false">sylinrl/truthfulqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/truthfulqa">TruthfulQA</pwcdataset>
    </paper>
    <paper id="231">
      <title>Right for the Right Reason Evidence Extraction for Trustworthy Tabular Reasoning</title>
      <author><first>Vivek</first><last>Gupta</last></author>
      <author><first>Shuo</first><last>Zhang</last></author>
      <author><first>Alakananda</first><last>Vempala</last></author>
      <author><first>Yujie</first><last>He</last></author>
      <author><first>Temma</first><last>Choji</last></author>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <pages>3268-3283</pages>
      <abstract>When pre trained contextualized embedding based models developed for <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured data</a> are adapted for structured tabular data they perform admirably However recent probing studies show that these models use spurious correlations and often predict inference labels by focusing on false evidence or ignoring it altogether To study this issue we introduce the task of Trustworthy Tabular Reasoning where a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> needs to extract evidence to be used for <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a> in addition to predicting the label As a case study we propose a two stage sequential prediction approach which includes an evidence extraction and an <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference stage</a> First we crowdsource evidence row labels and develop several unsupervised and supervised evidence extraction strategies for InfoTabS a tabular NLI benchmark Our evidence extraction strategy outperforms earlier <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> On the downstream tabular inference task using only the automatically extracted evidence as the premise our approach outperforms prior benchmarks</abstract>
      <url hash="15f54a5f">2022.acl-long.231</url>
      <bibkey>gupta-etal-2022-right</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
    </paper>
    <paper id="235">
      <title>Direct Speech-to-Speech Translation With Discrete Units</title>
      <author><first>Ann</first><last>Lee</last></author>
      <author><first>Peng-Jen</first><last>Chen</last></author>
      <author><first>Changhan</first><last>Wang</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Sravya</first><last>Popuri</last></author>
      <author><first>Xutai</first><last>Ma</last></author>
      <author><first>Adam</first><last>Polyak</last></author>
      <author><first>Yossi</first><last>Adi</last></author>
      <author><first>Qing</first><last>He</last></author>
      <author><first>Yun</first><last>Tang</last></author>
      <author><first>Juan</first><last>Pino</last></author>
      <author><first>Wei-Ning</first><last>Hsu</last></author>
      <pages>3327-3339</pages>
      <abstract>We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation. We tackle the problem by first applying a self-supervised discrete speech encoder on the target speech and then training a sequence-to-sequence speech-to-unit translation (S2UT) model to predict the discrete representations of the target speech. When target text transcripts are available, we design a joint speech and text training framework that enables the model to generate dual modality output (speech and text) simultaneously in the same inference pass. Experiments on the Fisher Spanish-English dataset show that the proposed framework yields improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features. When trained without any text transcripts, our model performance is comparable to models that predict spectrograms and are trained with text supervision, showing the potential of our system for translation between unwritten languages.</abstract>
      <url hash="7de498ca">2022.acl-long.235</url>
      <bibkey>lee-etal-2022-direct</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="239">
      <title>Dataset Geography Mapping Language Data to Language Users</title>
      <author><first>Fahim</first><last>Faisal</last></author>
      <author><first>Yinkai</first><last>Wang</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>3381-3411</pages>
      <abstract>As language technologies become more ubiquitous there are increasing efforts towards expanding the <a href="https://en.wikipedia.org/wiki/Language_diversity">language diversity</a> and coverage of natural language processing NLP systems Arguably the most important factor influencing the quality of modern NLP systems is data availability In this work we study the geographical representativeness of NLP datasets aiming to quantify if and by how much do NLP datasets match the expected needs of the language speakers In doing so we use entity recognition and linking systems also making important observations about their cross lingual consistency and giving suggestions for more robust evaluation Last we explore some geographical and economic factors that may explain the observed dataset distributions</abstract>
      <url hash="8ce456a5">2022.acl-long.239</url>
      <bibkey>faisal-etal-2022-dataset</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/masakhaner">MasakhaNER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
    </paper>
    <paper id="240">
      <title><fixed-case>ILDAE</fixed-case>: Instance-Level Difficulty Analysis of Evaluation Data</title>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>3412-3425</pages>
      <abstract>Knowledge of difficulty level of questions helps a teacher in several ways, such as estimating students’ potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions. Can we extract such benefits of instance difficulty in Natural Language Processing? To this end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE) in a large-scale setup of 23 datasets and demonstrate its five novel applications: 1) conducting efficient-yet-accurate evaluations with fewer instances saving computational cost and time, 2) improving quality of existing evaluation datasets by repairing erroneous and trivial instances, 3) selecting the best model based on application requirements, 4) analyzing dataset characteristics for guiding future data creation, 5) estimating Out-of-Domain performance reliably. Comprehensive experiments for these applications lead to several interesting results, such as evaluation using just 5% instances (selected via ILDAE) achieves as high as 0.93 Kendall correlation with evaluation using complete dataset and computing weighted accuracy using difficulty scores leads to 5.2% higher correlation with Out-of-Domain performance. We release the difficulty scores and hope our work will encourage research in this important yet understudied field of leveraging instance difficulty in evaluations.</abstract>
      <url hash="bf67bcc4">2022.acl-long.240</url>
      <attachment type="software" hash="8f283d36">2022.acl-long.240.software.zip</attachment>
      <bibkey>varshney-etal-2022-ildae</bibkey>
      <pwccode url="https://github.com/nrjvarshney/ildae" additional="false">nrjvarshney/ildae</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quartz">QuaRTz</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quarel">QuaRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="249">
      <title>How Do We Answer Complex Questions: Discourse Structure of Long-form Answers</title>
      <author><first>Fangyuan</first><last>Xu</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <pages>3556-3572</pages>
      <abstract>Long-form answers, consisting of multiple sentences, can provide nuanced and comprehensive answers to a broader set of questions. To better understand this complex and understudied task, we study the functional structure of long-form answers collected from three datasets, ELI5, WebGPT and Natural Questions. Our main goal is to understand how humans organize information to craft complex answers. We develop an ontology of six sentence-level functional roles for long-form answers, and annotate 3.9k sentences in 640 answer paragraphs. Different answer collection methods manifest in different discourse structures. We further analyze model-generated answers – finding that annotators agree less with each other when annotating model-generated answers compared to annotating human-written answers. Our annotated data enables training a strong classifier that can be used for automatic analysis. We hope our work can inspire future research on discourse-level modeling and evaluation of long-form QA systems.</abstract>
      <url hash="e5b088e7">2022.acl-long.249</url>
      <bibkey>xu-etal-2022-answer</bibkey>
      <pwccode url="https://github.com/utcsnlp/lfqa_discourse" additional="false">utcsnlp/lfqa_discourse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="253">
      <title>ConditionalQA A Complex Reading Comprehension Dataset with Conditional Answers<fixed-case>C</fixed-case>onditional<fixed-case>QA</fixed-case>: A Complex Reading Comprehension Dataset with Conditional Answers</title>
      <author><first>Haitian</first><last>Sun</last></author>
      <author><first>William</first><last>Cohen</last></author>
      <author><first>Ruslan</first><last>Salakhutdinov</last></author>
      <pages>3627-3637</pages>
      <abstract>We describe a Question Answering QA dataset that contains complex questions with conditional answers i.e. the answers are only applicable when certain conditions apply We call this dataset ConditionalQA In addition to conditional answers the dataset also features 
 long context documents with information that is related in logically complex ways 
 multi hop questions that require compositional logical reasoning 
 a combination of extractive questions yes no questions questions with multiple answers and not answerable questions 
 questions asked without knowing the answers We show that ConditionalQA is challenging for many of the existing QA models especially in selecting answer conditions We believe that this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> will motivate further research in answering complex questions over long documents</abstract>
      <url hash="e52275c5">2022.acl-long.253</url>
      <bibkey>sun-etal-2022-conditionalqa</bibkey>
      <pwccode url="https://github.com/haitian-sun/conditionalqa" additional="false">haitian-sun/conditionalqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conditionalqa">ConditionalQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/policyqa">PolicyQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasper">QASPER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sharc">ShARC</pwcdataset>
    </paper>
    <paper id="256">
      <title>An Investigation of the (In)effectiveness of Counterfactually Augmented Data</title>
      <author><first>Nitish</first><last>Joshi</last></author>
      <author><first>He</first><last>He</last></author>
      <pages>3668-3681</pages>
      <abstract>While pretrained language models achieve excellent performance on natural language understanding benchmarks, they tend to rely on spurious correlations and generalize poorly to out-of-distribution (OOD) data. Recent work has explored using counterfactually-augmented data (CAD)—data generated by minimally perturbing examples to flip the ground-truth label—to identify robust features that are invariant under distribution shift. However, empirical results using CAD during training for OOD generalization have been mixed. To explain this discrepancy, through a toy theoretical example and empirical analysis on two crowdsourced CAD datasets, we show that: (a) while features perturbed in CAD are indeed robust features, it may prevent the model from learning unperturbed robust features; and (b) CAD may exacerbate existing spurious correlations in the data. Our results thus show that the lack of perturbation diversity limits CAD’s effectiveness on OOD generalization, calling for innovative crowdsourcing procedures to elicit diverse perturbation of examples.</abstract>
      <url hash="6e7c7875">2022.acl-long.256</url>
      <attachment type="software" hash="b5626b0e">2022.acl-long.256.software.zip</attachment>
      <bibkey>joshi-he-2022-investigation</bibkey>
      <pwccode url="https://github.com/joshinh/investigation-cad" additional="false">joshinh/investigation-cad</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
    </paper>
    <paper id="257">
      <title>Inducing Positive Perspectives with Text Reframing</title>
      <award>Outstanding Paper</award>
      <author><first>Caleb</first><last>Ziems</last></author>
      <author><first>Minzhi</first><last>Li</last></author>
      <author><first>Anthony</first><last>Zhang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>3682-3700</pages>
      <abstract>Sentiment transfer is one popular example of a text style transfer task where the goal is to reverse the sentiment polarity of a text With a sentiment reversal comes also a reversal in meaning We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task To facilitate rapid progress we introduce a large scale benchmark Positive Psychology Frames with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoretically motivated reframing strategies Then we evaluate a set of state of the art text style transfer models and conclude by discussing key challenges and directions for future work</abstract>
      <url hash="8f56b787">2022.acl-long.257</url>
      <bibkey>ziems-etal-2022-inducing</bibkey>
      <pwccode url="https://github.com/gt-salt/positive-frames" additional="false">gt-salt/positive-frames</pwccode>
    </paper>
    <paper id="261">
      <title>The Moral Integrity Corpus A Benchmark for Ethical Dialogue Systems</title>
      <author><first>Caleb</first><last>Ziems</last></author>
      <author><first>Jane</first><last>Yu</last></author>
      <author><first>Yi-Chia</first><last>Wang</last></author>
      <author><first>Alon</first><last>Halevy</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>3755-3773</pages>
      <abstract>Conversational agents have come increasingly closer to human competence in open domain dialogue settings however such models can reflect insensitive hurtful or entirely incoherent viewpoints that erode a user’s trust in the moral integrity of the system Moral deviations are difficult to mitigate because moral judgments are not universal and there may be multiple competing judgments that apply to a situation simultaneously In this work we introduce a new resource not to authoritatively resolve moral ambiguities but instead to facilitate systematic understanding of the intuitions values and moral judgments reflected in the utterances of dialogue systems The Moral Integrity Corpus MIC is such a resource which captures the moral assumptions of 38k prompt reply pairs using 99k distinct Rules of Thumb RoTs Each RoT reflects a particular <a href="https://en.wikipedia.org/wiki/Morality">moral conviction</a> that can explain why a chatbot’s reply may appear acceptable or problematic We further organize RoTs with a set of moral and social attributes and benchmark performance for attribute classification Most importantly we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions but they still struggle with certain scenarios Our findings suggest that MIC will be a useful resource for understanding and language models implicit moral assumptions and flexibly benchmarking the integrity of conversational agents To download the data see https://github.com/GT-SALT/mic</abstract>
      <url hash="2def8105">2022.acl-long.261</url>
      <bibkey>ziems-etal-2022-moral</bibkey>
      <pwccode url="https://github.com/gt-salt/mic" additional="false">gt-salt/mic</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ethics-1">ETHICS</pwcdataset>
    </paper>
    <paper id="279">
      <title>Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide <fixed-case>MLP</fixed-case></title>
      <author><first>Lukas</first><last>Galke</last></author>
      <author><first>Ansgar</first><last>Scherp</last></author>
      <pages>4038-4051</pages>
      <abstract>Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today’s state of the art. We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting and is comparable with HyperGAT. Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all state-of-the-art models. These results question the importance of synthetic graphs used in modern text classifiers. In terms of efficiency, DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based models like TextGCN require setting up an <tex-math>\mathcal{O}(N^2)</tex-math> graph, where <tex-math>N</tex-math> is the vocabulary plus corpus size. Finally, since Transformers need to compute <tex-math>\mathcal{O}(L^2)</tex-math> attention weights with sequence length <tex-math>L</tex-math>, the MLP models show higher training and inference speeds on datasets with long sequences.</abstract>
      <url hash="535f61d3">2022.acl-long.279</url>
      <attachment type="software" hash="21c5c547">2022.acl-long.279.software.zip</attachment>
      <bibkey>galke-scherp-2022-bag</bibkey>
      <pwccode url="https://github.com/lgalke/text-clf-baselines" additional="false">lgalke/text-clf-baselines</pwccode>
    </paper>
    <paper id="280">
      <title>Generative Pretraining for Paraphrase Evaluation</title>
      <author><first>Jack</first><last>Weston</last></author>
      <author><first>Raphael</first><last>Lenain</last></author>
      <author><first>Udeepa</first><last>Meepegama</last></author>
      <author><first>Emil</first><last>Fristed</last></author>
      <pages>4052-4073</pages>
      <abstract>We introduce ParaBLEU a paraphrase representation learning model and evaluation metric for text generation Unlike previous approaches ParaBLEU learns to understand <a href="https://en.wikipedia.org/wiki/Paraphrasis">paraphrasis</a> using generative conditioning as a pretraining objective ParaBLEU correlates more strongly with human judgements than existing <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> obtaining new state of the art results on the WMT Metrics Shared Task We show that our model is robust to data scarcity exceeding previous state of the art performance using only 50\%$ of the available training data and surpassing BLEU ROUGE and <a href="https://en.wikipedia.org/wiki/METEOR">METEOR</a> with only labelled examples Finally we demonstrate that ParaBLEU can be used to conditionally generate novel <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> from a single demonstration which we use to confirm our hypothesis that it learns abstract generalized paraphrase representations</abstract>
      <url hash="31b4c5a8">2022.acl-long.280</url>
      <bibkey>weston-etal-2022-generative</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paranmt-50m">PARANMT-50M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="283">
      <title>Word Segmentation as Unsupervised Constituency Parsing</title>
      <author><first>Raquel G.</first><last>Alhama</last></author>
      <pages>4103-4112</pages>
      <abstract>Word identification from continuous input is typically viewed as a segmentation task Experiments with human adults suggest that familiarity with syntactic structures in their native language also influences word identification in artificial languages however the relation between syntactic processing and word identification is yet unclear This work takes one step forward by exploring a radically different approach of word identification in which segmentation of a continuous input is viewed as a process isomorphic to unsupervised constituency parsing Besides formalizing the approach this study reports simulations of human experiments with DIORA Drozdov et al a neural unsupervised constituency parser Results show that this model can reproduce human behavior in word identification experiments suggesting that this is a viable approach to study word identification and its relation to syntactic processing</abstract>
      <url hash="f2c1542c">2022.acl-long.283</url>
      <bibkey>alhama-2022-word</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="284">
      <title><fixed-case>S</fixed-case>afety<fixed-case>K</fixed-case>it: First Aid for Measuring Safety in Open-domain Conversational Systems</title>
      <author><first>Emily</first><last>Dinan</last></author>
      <author><first>Gavin</first><last>Abercrombie</last></author>
      <author><first>A.</first><last>Bergman</last></author>
      <author><first>Shannon</first><last>Spruit</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Y-Lan</first><last>Boureau</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>4113-4133</pages>
      <abstract>The social impact of natural language processing and its applications has received increasing attention. In this position paper, we focus on the problem of safety for end-to-end conversational AI. We survey the problem landscape therein, introducing a taxonomy of three observed phenomena: the Instigator, Yea-Sayer, and Impostor effects. We then empirically assess the extent to which current tools can measure these effects and current systems display them. We release these tools as part of a “first aid kit” (SafetyKit) to quickly assess apparent safety concerns. Our results show that, while current tools are able to provide an estimate of the relative safety of systems in various settings, they still have several shortcomings. We suggest several future directions and discuss ethical considerations.</abstract>
      <url hash="a63224f2">2022.acl-long.284</url>
      <bibkey>dinan-etal-2022-safetykit</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/blended-skill-talk">Blended Skill Talk</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/honest-en">HONEST</pwcdataset>
    </paper>
    <paper id="286">
      <title>The Paradox of the Compositionality of <a href="https://en.wikipedia.org/wiki/Natural_language">Natural Language</a> A Neural Machine Translation Case Study</title>
      <author><first>Verna</first><last>Dankers</last></author>
      <author><first>Elia</first><last>Bruni</last></author>
      <author><first>Dieuwke</first><last>Hupkes</last></author>
      <pages>4154-4175</pages>
      <abstract>Obtaining human like performance in <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a> is often argued to require compositional generalisation Whether <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> exhibit this ability is usually studied by training <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on highly compositional synthetic data However compositionality in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> is much more complex than the rigid arithmetic like version such data adheres to and artificial compositionality tests thus do not allow us to determine how neural models deal with more realistic forms of <a href="https://en.wikipedia.org/wiki/Compositionality">compositionality</a> In this work we re instantiate three compositionality tests from the literature and reformulate them for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> NMT Our results highlight that i unfavourably models trained on more data are more compositional ii models are sometimes less compositional than expected but sometimes more exemplifying that different levels of compositionality are required and models are not always able to modulate between them correctly iii some of the non compositional behaviours are mistakes whereas others reflect the natural variation in data Apart from an empirical study our work is a call to action we should rethink the evaluation of compositionality in <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> and develop benchmarks using real data to evaluate <a href="https://en.wikipedia.org/wiki/Compositionality">compositionality</a> on <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> where composing meaning is not as straightforward as doing the math</abstract>
      <url hash="182d694a">2022.acl-long.286</url>
      <attachment type="software" hash="c9853a4f">2022.acl-long.286.software.zip</attachment>
      <bibkey>dankers-etal-2022-paradox</bibkey>
      <pwccode url="https://github.com/i-machine-think/compositionality_paradox_mt" additional="false">i-machine-think/compositionality_paradox_mt</pwccode>
    </paper>
    <paper id="297">
      <title>LexGLUE A Benchmark Dataset for Legal Language Understanding in English<fixed-case>L</fixed-case>ex<fixed-case>GLUE</fixed-case>: A Benchmark Dataset for Legal Language Understanding in <fixed-case>E</fixed-case>nglish</title>
      <author><first>Ilias</first><last>Chalkidis</last></author>
      <author><first>Abhik</first><last>Jana</last></author>
      <author><first>Dirk</first><last>Hartung</last></author>
      <author><first>Michael</first><last>Bommarito</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <author><first>Daniel</first><last>Katz</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>4310-4330</pages>
      <abstract>Laws and their interpretations legal arguments and agreements are typically expressed in writing leading to the production of vast corpora of legal text Their analysis which is at the center of legal practice becomes increasingly elaborate as these <a href="https://en.wikipedia.org/wiki/Collection_(artwork)">collections</a> grow in size Natural language understanding NLU technologies can be a valuable tool to support legal practitioners in these endeavors Their usefulness however largely depends on whether current state of the art models can generalize across various tasks in the legal domain To answer this currently open question we introduce the Legal General Language Understanding Evaluation LexGLUE benchmark a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way We also provide an evaluation and analysis of several generic and legal oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks</abstract>
      <url hash="1e361c09">2022.acl-long.297</url>
      <attachment type="software" hash="75eebd2a">2022.acl-long.297.software.zip</attachment>
      <bibkey>chalkidis-etal-2022-lexglue</bibkey>
      <pwccode url="https://github.com/coastalcph/lex-glue" additional="false">coastalcph/lex-glue</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lexglue">LexGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/casehold">CaseHOLD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/echr">ECHR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ecthr">ECtHR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="314">
      <title><fixed-case>SRL4E</fixed-case> – <fixed-case>S</fixed-case>emantic <fixed-case>R</fixed-case>ole <fixed-case>L</fixed-case>abeling for <fixed-case>E</fixed-case>motions: <fixed-case>A</fixed-case> Unified Evaluation Framework</title>
      <author><first>Cesare</first><last>Campagnano</last></author>
      <author><first>Simone</first><last>Conia</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>4586-4601</pages>
      <abstract>In the field of sentiment analysis, several studies have highlighted that a single sentence may express multiple, sometimes contrasting, sentiments and emotions, each with its own experiencer, target and/or cause. To this end, over the past few years researchers have started to collect and annotate data manually, in order to investigate the capabilities of automatic systems not only to distinguish between emotions, but also to capture their semantic constituents. However, currently available gold datasets are heterogeneous in size, domain, format, splits, emotion categories and role labels, making comparisons across different works difficult and hampering progress in the area. In this paper, we tackle this issue and present a unified evaluation framework focused on Semantic Role Labeling for Emotions (SRL4E), in which we unify several datasets tagged with emotions and semantic roles by using a common labeling scheme. We use SRL4E as a benchmark to evaluate how modern pretrained language models perform and analyze where we currently stand in this task, hoping to provide the tools to facilitate studies in this complex area.</abstract>
      <url hash="abae2774">2022.acl-long.314</url>
      <bibkey>campagnano-etal-2022-srl4e</bibkey>
      <pwccode url="https://github.com/sapienzanlp/srl4e" additional="false">sapienzanlp/srl4e</pwccode>
    </paper>
    <paper id="315">
      <title>Context Matters: A Pragmatic Study of <fixed-case>PLM</fixed-case>s’ Negation Understanding</title>
      <author><first>Reto</first><last>Gubelmann</last></author>
      <author><first>Siegfried</first><last>Handschuh</last></author>
      <pages>4602-4621</pages>
      <abstract>In linguistics, there are two main perspectives on negation: a semantic and a pragmatic view. So far, research in NLP on negation has almost exclusively adhered to the semantic view. In this article, we adopt the pragmatic paradigm to conduct a study of negation understanding focusing on transformer-based PLMs. Our results differ from previous, semantics-based studies and therefore help to contribute a more comprehensive – and, given the results, much more optimistic – picture of the PLMs’ negation understanding.</abstract>
      <url hash="63efd404">2022.acl-long.315</url>
      <bibkey>gubelmann-handschuh-2022-context</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="318">
      <title>Identifying Moments of Change from <a href="https://en.wikipedia.org/wiki/Longitudinal_study">Longitudinal User Text</a></title>
      <author><first>Adam</first><last>Tsakalidis</last></author>
      <author><first>Federico</first><last>Nanni</last></author>
      <author><first>Anthony</first><last>Hills</last></author>
      <author><first>Jenny</first><last>Chim</last></author>
      <author><first>Jiayu</first><last>Song</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <pages>4647-4660</pages>
      <abstract>Identifying changes in individuals behaviour and mood as observed via content shared on <a href="https://en.wikipedia.org/wiki/Online_and_offline">online platforms</a> is increasingly gaining importance Most research to date on this topic focuses on either a identifying individuals at risk or with a certain <a href="https://en.wikipedia.org/wiki/Mental_disorder">mental health condition</a> given a batch of posts or b providing equivalent labels at the post level A disadvantage of such work is the lack of a strong temporal component and the inability to make longitudinal assessments following an individual’s trajectory and allowing timely interventions Here we define a new <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> that of identifying moments of change in individuals on the basis of their shared content online The changes we consider are sudden shifts in mood switches or gradual mood progression escalations We have created detailed guidelines for capturing moments of change and a corpus of manually annotated user timelines 18.7 K posts We have developed a variety of baseline models drawing inspiration from related tasks and show that the best performance is obtained through context aware sequential modelling We also introduce new <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> for capturing rare events in temporal windows</abstract>
      <url hash="f4ff6095">2022.acl-long.318</url>
      <bibkey>tsakalidis-etal-2022-identifying</bibkey>
    </paper>
    <paper id="321">
      <title>Semi-Supervised Formality Style Transfer with Consistency Training</title>
      <author><first>Ao</first><last>Liu</last></author>
      <author><first>An</first><last>Wang</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>4689-4701</pages>
      <abstract>Formality style transfer (FST) is a task that involves paraphrasing an informal sentence into a formal one without altering its meaning. To address the data-scarcity problem of existing parallel datasets, previous studies tend to adopt a cycle-reconstruction scheme to utilize additional unlabeled data, where the FST model mainly benefits from target-side unlabeled sentences. In this work, we propose a simple yet effective semi-supervised framework to better utilize source-side unlabeled sentences based on consistency training. Specifically, our approach augments pseudo-parallel data obtained from a source-side informal sentence by enforcing the model to generate similar outputs for its perturbed version. Moreover, we empirically examined the effects of various data perturbation methods and propose effective data filtering strategies to improve our framework. Experimental results on the GYAFC benchmark demonstrate that our approach can achieve state-of-the-art results, even with less than 40% of the parallel data.</abstract>
      <url hash="ba0e5893">2022.acl-long.321</url>
      <bibkey>liu-etal-2022-semi</bibkey>
      <pwccode url="https://github.com/aolius/semi-fst" additional="false">aolius/semi-fst</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="323">
      <title>Rare and Zero-shot Word Sense Disambiguation using <fixed-case>Z</fixed-case>-Reweighting</title>
      <author><first>Ying</first><last>Su</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Tong</first><last>Zhang</last></author>
      <pages>4713-4723</pages>
      <abstract>Word sense disambiguation (WSD) is a crucial problem in the natural language processing (NLP) community. Current methods achieve decent performance by utilizing supervised learning and large pre-trained language models. However, the imbalanced training dataset leads to poor performance on rare senses and zero-shot senses. There are more training instances and senses for words with top frequency ranks than those with low frequency ranks in the training dataset. We investigate the statistical relation between word frequency rank and word sense number distribution. Based on the relation, we propose a Z-reweighting method on the word level to adjust the training on the imbalanced dataset. The experiments show that the Z-reweighting strategy achieves performance gain on the standard English all words WSD benchmark. Moreover, the strategy can help models generalize better on rare and zero-shot senses.</abstract>
      <url hash="7be63a9e">2022.acl-long.323</url>
      <attachment type="software" hash="f05ff30d">2022.acl-long.323.software.zip</attachment>
      <bibkey>su-etal-2022-rare</bibkey>
      <pwccode url="https://github.com/suytingwan/wsd-z-reweighting" additional="false">suytingwan/wsd-z-reweighting</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="328">
      <title>WikiDiverse A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types<fixed-case>W</fixed-case>iki<fixed-case>D</fixed-case>iverse: A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types</title>
      <author><first>Xuwu</first><last>Wang</last></author>
      <author><first>Junfeng</first><last>Tian</last></author>
      <author><first>Min</first><last>Gui</last></author>
      <author><first>Zhixu</first><last>Li</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Lihan</first><last>Chen</last></author>
      <author><first>Yanghua</first><last>Xiao</last></author>
      <pages>4785-4797</pages>
      <abstract>Multimodal Entity Linking MEL which aims at linking mentions with multimodal contexts to the referent entities from a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a> e.g. <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> is an essential task for many multimodal applications Although much attention has been paid to MEL the shortcomings of existing MEL datasets including limited contextual topics and entity types simplified mention ambiguity and restricted availability have caused great obstacles to the research and application of MEL In this paper we present WikiDiverse a high quality human annotated MEL dataset with diversified contextual topics and entity types from <a href="https://en.wikipedia.org/wiki/Wikinews">Wikinews</a> which uses <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> as the corresponding knowledge base A well tailored annotation procedure is adopted to ensure the quality of the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> Based on WikiDiverse a sequence of well designed MEL models with intra modality and inter modality attentions are implemented which utilize the visual information of images more adequately than existing MEL models do Extensive experimental analyses are conducted to investigate the contributions of different modalities in terms of MEL facilitating the future research on this task</abstract>
      <url hash="2608cbe3">2022.acl-long.328</url>
      <attachment type="software" hash="55896e6c">2022.acl-long.328.software.zip</attachment>
      <bibkey>wang-etal-2022-wikidiverse</bibkey>
      <pwccode url="https://github.com/wangxw5/wikidiverse" additional="false">wangxw5/wikidiverse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/zeshel">ZESHEL</pwcdataset>
    </paper>
    <paper id="333">
      <title><fixed-case>D</fixed-case>ialog<fixed-case>VED</fixed-case>: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation</title>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Song</first><last>Wang</last></author>
      <author><first>Bolun</first><last>Yao</last></author>
      <author><first>Weizhen</first><last>Qi</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Xiaowu</first><last>Hu</last></author>
      <author><first>Bartuer</first><last>Zhou</last></author>
      <author><first>Yi</first><last>Mao</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <author><first>Biao</first><last>Cheng</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>4852-4864</pages>
      <abstract>Dialog response generation in open domain is an important research topic where the main challenge is to generate relevant and diverse responses. In this paper, we propose a new dialog pre-training framework called DialogVED, which introduces continuous latent variables into the enhanced encoder-decoder pre-training framework to increase the relevance and diversity of responses. With the help of a large dialog corpus (Reddit), we pre-train the model using the following 4 tasks, used in training language models (LMs) and Variational Autoencoders (VAEs) literature: 1) masked language model; 2) response generation; 3) bag-of-words prediction; and 4) KL divergence reduction. We also add additional parameters to model the turn structure in dialogs to improve the performance of the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and DSTC7-AVSD benchmarks for response generation. Experimental results show that our model achieves the new state-of-the-art results on all these datasets.</abstract>
      <url hash="1f319d8c">2022.acl-long.333</url>
      <bibkey>chen-etal-2022-dialogved</bibkey>
      <pwccode url="https://github.com/lemuria-wchen/DialogVED" additional="false">lemuria-wchen/DialogVED</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dstc7-task-2">DSTC7 Task 2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="334">
      <title>Contextual Fine-to-Coarse Distillation for Coarse-grained Response Selection in Open-Domain Conversations</title>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Can</first><last>Xu</last></author>
      <author><first>Huang</first><last>Hu</last></author>
      <author><first>Bolun</first><last>Yao</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Zhihao</first><last>Fan</last></author>
      <author><first>Xiaowu</first><last>Hu</last></author>
      <author><first>Bartuer</first><last>Zhou</last></author>
      <author><first>Biao</first><last>Cheng</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>4865-4877</pages>
      <abstract>We study the problem of coarse-grained response selection in retrieval-based dialogue systems. The problem is equally important with fine-grained response selection, but is less explored in existing literature. In this paper, we propose a Contextual Fine-to-Coarse (CFC) distilled model for coarse-grained response selection in open-domain conversations. In our CFC model, dense representations of query, candidate contexts and responses is learned based on the multi-tower architecture using contextual matching, and richer knowledge learned from the one-tower architecture (fine-grained) is distilled into the multi-tower architecture (coarse-grained) to enhance the performance of the retriever. To evaluate the performance of the proposed model, we construct two new datasets based on the Reddit comments dump and Twitter corpus. Extensive experimental results on the two datasets show that the proposed method achieves huge improvement over all evaluation metrics compared with traditional baseline methods.</abstract>
      <url hash="91b4d5d8">2022.acl-long.334</url>
      <bibkey>chen-etal-2022-contextual</bibkey>
      <pwccode url="https://github.com/lemuria-wchen/CFC" additional="false">lemuria-wchen/CFC</pwccode>
    </paper>
    <paper id="337">
      <title>Packed Levitated Marker for Entity and Relation Extraction</title>
      <author><first>Deming</first><last>Ye</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>4904-4917</pages>
      <abstract>Recent entity and relation extraction works focus on investigating how to obtain a better span representation from the pre trained encoder However a major limitation of existing works is that they ignore the interrelation between spans pairs In this work we propose a novel span representation approach named Packed Levitated Markers PL Marker to consider the interrelation between the spans pairs by strategically packing the markers in the encoder In particular we propose a neighborhood oriented packing strategy which considers the neighbor spans integrally to better model the entity boundary information Furthermore for those more complicated span pair classification tasks we design a subject oriented packing strategy which packs each subject and all its objects to model the interrelation between the same subject span pairs The experimental results show that with the enhanced marker feature our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> advances baselines on six NER benchmarks and obtains a 4.1%-4.3 strict relation F1 improvement with higher speed over previous state of the art models on ACE04 and ACE05 Our code and models are publicly available at https://github.com/thunlp/PL-Marker</abstract>
      <url hash="567e5054">2022.acl-long.337</url>
      <attachment type="software" hash="1ac162a9">2022.acl-long.337.software.zip</attachment>
      <bibkey>ye-etal-2022-packed</bibkey>
      <pwccode url="https://github.com/thunlp/pl-marker" additional="false">thunlp/pl-marker</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2004">ACE 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2005">ACE 2005</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/few-nerd">Few-NERD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="340">
      <title><fixed-case>KG</fixed-case>-<fixed-case>F</fixed-case>i<fixed-case>D</fixed-case>: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering</title>
      <author><first>Donghan</first><last>Yu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Yuwei</first><last>Fang</last></author>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Yiming</first><last>Yang</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>4961-4974</pages>
      <abstract>Current Open-Domain Question Answering (ODQA) models typically include a retrieving module and a reading module, where the retriever selects potentially relevant passages from open-source documents for a given question, and the reader produces an answer based on the retrieved passages. The recently proposed Fusion-in-Decoder (FiD) framework is a representative example, which is built on top of a dense passage retriever and a generative reader, achieving the state-of-the-art performance. In this paper we further improve the FiD approach by introducing a knowledge-enhanced version, namely KG-FiD. Our new model uses a knowledge graph to establish the structural relationship among the retrieved passages, and a graph neural network (GNN) to re-rank the passages and select only a top few for further processing. Our experiments on common ODQA benchmark datasets (Natural Questions and TriviaQA) demonstrate that KG-FiD can achieve comparable or better performance in answer prediction than FiD, with less than 40% of the computation cost.</abstract>
      <url hash="5283d5ac">2022.acl-long.340</url>
      <bibkey>yu-etal-2022-kg</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="344">
      <title>CICERO A Dataset for Contextualized Commonsense Inference in Dialogues<fixed-case>CICERO</fixed-case>: A Dataset for Contextualized Commonsense Inference in Dialogues</title>
      <author><first>Deepanway</first><last>Ghosal</last></author>
      <author><first>Siqi</first><last>Shen</last></author>
      <author><first>Navonil</first><last>Majumder</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>5010-5028</pages>
      <abstract>This paper addresses the problem of dialogue reasoning with contextualized commonsense inference We curate CICERO a dataset of dyadic conversations with five types of utterance level reasoning based inferences cause subsequent event prerequisite motivation and emotional reaction The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> contains 53,105 of such inferences from 5,672 dialogues We use this dataset to solve relevant generative and discriminative tasks generation of cause and subsequent event generation of prerequisite motivation and listener’s emotional reaction and selection of plausible alternatives Our results ascertain the value of such dialogue centric commonsense knowledge datasets It is our hope that CICERO will open new research avenues into commonsense based dialogue reasoning</abstract>
      <url hash="11abc687">2022.acl-long.344</url>
      <bibkey>ghosal-etal-2022-cicero</bibkey>
      <pwccode url="https://github.com/declare-lab/CICERO" additional="false">declare-lab/CICERO</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cicero">CICERO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mutual">MuTual</pwcdataset>
    </paper>
    <paper id="345">
      <title>A Comparative Study of Faithfulness Metrics for Model Interpretability Methods</title>
      <author><first>Chun Sik</first><last>Chan</last></author>
      <author><first>Huanqi</first><last>Kong</last></author>
      <author><first>Liang</first><last>Guanqing</last></author>
      <pages>5029-5038</pages>
      <abstract>Interpretable methods to reveal the internal reasoning processes behind machine learning models have attracted increasing attention in recent years To quantify the extent to which the identified interpretations truly reflect the intrinsic decision making mechanisms various faithfulness evaluation metrics have been proposed However we find that different faithfulness metrics show conflicting preferences when comparing different interpretations Motivated by this observation we aim to conduct a comprehensive and comparative study of the widely adopted faithfulness metrics In particular we introduce two assessment dimensions namely <a href="https://en.wikipedia.org/wiki/Diagnosis">diagnosticity</a> and complexity Diagnosticity refers to the degree to which the faithfulness metric favors relatively faithful interpretations over randomly generated ones and <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">complexity</a> is measured by the average number of model forward passes According to the experimental results we find that sufficiency and comprehensiveness metrics have higher <a href="https://en.wikipedia.org/wiki/Diagnosis">diagnosticity</a> and lower <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> than the other faithfulness metrics</abstract>
      <url hash="40a5f684">2022.acl-long.345</url>
      <bibkey>chan-etal-2022-comparative</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="347">
      <title>Pass off Fish Eyes for Pearls: Attacking Model Selection of Pre-trained Models</title>
      <author><first>Biru</first><last>Zhu</last></author>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Yangdong</first><last>Deng</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Ming</first><last>Gu</last></author>
      <pages>5060-5072</pages>
      <abstract>Selecting an appropriate pre-trained model (PTM) for a specific downstream task typically requires significant efforts of fine-tuning. To accelerate this process, researchers propose feature-based model selection (FMS) methods, which assess PTMs’ transferability to a specific task in a fast way without fine-tuning. In this work, we argue that current FMS methods are vulnerable, as the assessment mainly relies on the static features extracted from PTMs. However, such features are derived without training PTMs on downstream tasks, and are not necessarily reliable indicators for the PTM’s transferability. To validate our viewpoints, we design two methods to evaluate the robustness of FMS: (1) model disguise attack, which post-trains an inferior PTM with a contrastive objective, and (2) evaluation data selection, which selects a subset of the data points for FMS evaluation based on K-means clustering. Experimental results prove that both methods can successfully make FMS mistakenly judge the transferability of PTMs. Moreover, we find that these two methods can further be combined with the backdoor attack to misguide the FMS to select poisoned models. To the best of our knowledge, this is the first work to demonstrate the defects of current FMS algorithms and evaluate their potential security risks. By identifying previously unseen risks of FMS, our study indicates new directions for improving the robustness of FMS.</abstract>
      <url hash="5cd6c5f6">2022.acl-long.347</url>
      <attachment type="software" hash="bdfea510">2022.acl-long.347.software.zip</attachment>
      <bibkey>zhu-etal-2022-pass</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="348">
      <title>Educational Question Generation of Children Storybooks via Question Type Distribution Learning and Event-centric Summarization</title>
      <author><first>Zhenjie</first><last>Zhao</last></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Dakuo</first><last>Wang</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Chengzhong</first><last>Liu</last></author>
      <author><first>Xiaojuan</first><last>Ma</last></author>
      <pages>5073-5085</pages>
      <abstract>Generating educational questions of fairytales or storybooks is vital for improving children’s literacy ability. However, it is challenging to generate questions that capture the interesting aspects of a fairytale story with educational meaningfulness. In this paper, we propose a novel question generation method that first learns the question type distribution of an input story paragraph, and then summarizes salient events which can be used to generate high-cognitive-demand questions. To train the event-centric summarizer, we finetune a pre-trained transformer-based sequence-to-sequence model using silver samples composed by educational question-answer pairs. On a newly proposed educational question-answering dataset FairytaleQA, we show good performance of our method on both automatic and human evaluation metrics. Our work indicates the necessity of decomposing question type distribution learning and event-centric summary generation for educational question generation.</abstract>
      <url hash="23a73049">2022.acl-long.348</url>
      <attachment type="software" hash="9aeebd6e">2022.acl-long.348.software.zip</attachment>
      <bibkey>zhao-etal-2022-educational</bibkey>
      <pwccode url="https://github.com/zhaozj89/Educational-Question-Generation" additional="false">zhaozj89/Educational-Question-Generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fairytaleqa">FairytaleQA</pwcdataset>
    </paper>
    <paper id="353">
      <title>A Neural Network Architecture for <a href="https://en.wikipedia.org/wiki/Program_understanding">Program Understanding</a> Inspired by Human Behaviors</title>
      <author><first>Renyu</first><last>Zhu</last></author>
      <author><first>Lei</first><last>Yuan</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <author><first>Wenyuan</first><last>Cai</last></author>
      <pages>5142-5153</pages>
      <abstract>Program understanding is a fundamental task in program language processing Despite the success existing works fail to take <a href="https://en.wikipedia.org/wiki/Human_behavior">human behaviors</a> as reference in understanding programs In this paper we consider <a href="https://en.wikipedia.org/wiki/Human_behavior">human behaviors</a> and propose the PGNN EK model that consists of two main components On the one hand inspired by the divide and conquer reading behaviors of humans we present a partitioning based graph neural network model PGNN on the upgraded AST of codes On the other hand to characterize human behaviors of resorting to other resources to help code comprehension we transform raw codes with external knowledge and apply pre training techniques for <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a> Finally we combine the two <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> generated from the two components to output code embeddings We conduct extensive experiments to show the superior performance of PGNN EK on the <a href="https://en.wikipedia.org/wiki/Automatic_summarization">code summarization</a> and code clone detection tasks In particular to show the generalization ability of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> we release a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> that is more challenging for code clone detection and could advance the development of the community Our codes and data are publicly available at https://github.com/RecklessRonan/PGNN-EK</abstract>
      <url hash="23626831">2022.acl-long.353</url>
      <attachment type="software" hash="26a6e0e4">2022.acl-long.353.software.zip</attachment>
      <bibkey>zhu-etal-2022-neural</bibkey>
      <pwccode url="https://github.com/recklessronan/pgnn-ek" additional="false">recklessronan/pgnn-ek</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codexglue">CodeXGLUE</pwcdataset>
    </paper>
    <paper id="358">
      <title>Dynamic Prefix-Tuning for Generative Template-based Event Extraction</title>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Ge</first><last>Shi</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <pages>5216-5228</pages>
      <abstract>We consider event extraction in a generative manner with template-based conditional generation.Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have two significant challenges, including using suboptimal prompts and static event type information.In this paper, we propose a generative template-based event extraction method with dynamic prefix (GTEE-DynPref) by integrating context information with type-specific prefixes to learn a context-specific prefix for each context.Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE.Additionally, our model is proven to be portable to new types of events effectively.</abstract>
      <url hash="5d579802">2022.acl-long.358</url>
      <bibkey>liu-etal-2022-dynamic</bibkey>
    </paper>
    <paper id="365">
      <title>Noisy Channel Language Model Prompting for Few-Shot Text Classification</title>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>5316-5330</pages>
      <abstract>We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.</abstract>
      <url hash="479d1462">2022.acl-long.365</url>
      <bibkey>min-etal-2022-noisy</bibkey>
      <pwccode url="https://github.com/shmsw25/Channel-LM-Prompting" additional="false">shmsw25/Channel-LM-Prompting</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="375">
      <title><tex-math>\infty</tex-math>-former: Infinite Memory Transformer</title>
      <author><first>Pedro Henrique</first><last>Martins</last></author>
      <author><first>Zita</first><last>Marinho</last></author>
      <author><first>Andre</first><last>Martins</last></author>
      <pages>5468-5485</pages>
      <abstract>Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the <tex-math>\infty</tex-math>-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the <tex-math>\infty</tex-math>-former’s attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important, <tex-math>\infty</tex-math>-former maintains “sticky memories,” being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the <tex-math>\infty</tex-math>-former’s ability to retain information from long sequences.</abstract>
      <url hash="02e8edcb">2022.acl-long.375</url>
      <bibkey>martins-etal-2022-former</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pg-19">PG-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="380">
      <title>Non-neural Models Matter: a Re-evaluation of Neural Referring Expression Generation Systems</title>
      <author><first>Fahime</first><last>Same</last></author>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Kees</first><last>Van Deemter</last></author>
      <pages>5554-5567</pages>
      <abstract>In recent years, neural models have often outperformed rule-based and classic Machine Learning approaches in NLG. These classic approaches are now often disregarded, for example when new neural models are evaluated. We argue that they should not be overlooked, since, for some tasks, well-designed non-neural approaches achieve better performance than neural ones. In this paper, the task of generating referring expressions in linguistic context is used as an example. We examined two very different English datasets (WEBNLG and WSJ), and evaluated each algorithm using both automatic and human evaluations.Overall, the results of these evaluations suggest that rule-based systems with simple rule sets achieve on-par or better performance on both datasets compared to state-of-the-art neural REG systems. In the case of the more realistic dataset, WSJ, a machine learning-based system with well-designed linguistic features performed best. We hope that our work can encourage researchers to consider non-neural models in future.</abstract>
      <url hash="c0fe8895">2022.acl-long.380</url>
      <attachment type="software" hash="6a91ff53">2022.acl-long.380.software.zip</attachment>
      <bibkey>same-etal-2022-non</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="382">
      <title>Predicate-Argument Based Bi-Encoder for Paraphrase Identification</title>
      <author><first>Qiwei</first><last>Peng</last></author>
      <author><first>David</first><last>Weir</last></author>
      <author><first>Julie</first><last>Weeds</last></author>
      <author><first>Yekun</first><last>Chai</last></author>
      <pages>5579-5589</pages>
      <abstract>Paraphrase identification involves identifying whether a pair of sentences express the same or similar meanings. While cross-encoders have achieved high performances across several benchmarks, bi-encoders such as SBERT have been widely applied to sentence pair tasks. They exhibit substantially lower computation complexity and are better suited to symmetric tasks. In this work, we adopt a bi-encoder approach to the paraphrase identification task, and investigate the impact of explicitly incorporating predicate-argument information into SBERT through weighted aggregation. Experiments on six paraphrase identification datasets demonstrate that, with a minimal increase in parameters, the proposed model is able to outperform SBERT/SRoBERTa significantly. Further, ablation studies reveal that the predicate-argument based component plays a significant role in the performance gain.</abstract>
      <url hash="898d35ff">2022.acl-long.382</url>
      <bibkey>peng-etal-2022-predicate</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pit">PIT</pwcdataset>
    </paper>
    <paper id="390">
      <title>Neural Machine Translation with Phrase-Level Universal Visual Representations</title>
      <author><first>Qingkai</first><last>Fang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>5687-5698</pages>
      <abstract>Multimodal machine translation (MMT) aims to improve neural machine translation (NMT) with additional visual information, but most existing MMT methods require paired input of source sentence and image, which makes them suffer from shortage of sentence-image pairs. In this paper, we propose a phrase-level retrieval-based method for MMT to get visual information for the source input from existing sentence-image data sets so that MMT can break the limitation of paired sentence-image input. Our method performs retrieval at the phrase level and hence learns visual information from pairs of source phrase and grounded region, which can mitigate data sparsity. Furthermore, our method employs the conditional variational auto-encoder to learn visual representations which can filter redundant visual information and only retain visual information related to the phrase. Experiments show that the proposed method significantly outperforms strong baselines on multiple MMT datasets, especially when the textual context is limited.</abstract>
      <url hash="60d19a6c">2022.acl-long.390</url>
      <bibkey>fang-feng-2022-neural</bibkey>
      <pwccode url="https://github.com/ictnlp/pluvr" additional="false">ictnlp/pluvr</pwccode>
    </paper>
    <paper id="393">
      <title><fixed-case>S</fixed-case>peech<fixed-case>T</fixed-case>5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</title>
      <author><first>Junyi</first><last>Ao</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Long</first><last>Zhou</last></author>
      <author><first>Chengyi</first><last>Wang</last></author>
      <author><first>Shuo</first><last>Ren</last></author>
      <author><first>Yu</first><last>Wu</last></author>
      <author><first>Shujie</first><last>Liu</last></author>
      <author><first>Tom</first><last>Ko</last></author>
      <author><first>Qing</first><last>Li</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Zhihua</first><last>Wei</last></author>
      <author><first>Yao</first><last>Qian</last></author>
      <author><first>Jinyu</first><last>Li</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>5723-5738</pages>
      <abstract>Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder. Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.</abstract>
      <url hash="8128b1c4">2022.acl-long.393</url>
      <bibkey>ao-etal-2022-speecht5</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/voxceleb1">VoxCeleb1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wham">WHAM!</pwcdataset>
    </paper>
    <paper id="395">
      <title>Unified Structure Generation for Universal Information Extraction</title>
      <author><first>Yaojie</first><last>Lu</last></author>
      <author><first>Qing</first><last>Liu</last></author>
      <author><first>Dai</first><last>Dai</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <pages>5755-5772</pages>
      <abstract>Information extraction suffers from its varying targets heterogeneous structures and demand specific schemas In this paper we propose a unified text to structure generation framework namely UIE which can universally model different IE tasks adaptively generate targeted structures and collaboratively learn general IE abilities from different knowledge sources Specifically UIE uniformly encodes different extraction structures via a structured extraction language adaptively generates target extractions via a schema based prompt mechanism   structural schema instructor and captures the common IE abilities via a large scale pretrained text to structure model Experiments show that UIE achieved the state of the art performance on IE tasks datasets and on all supervised low resource and few shot settings for a wide range of entity relation event and sentiment extraction tasks and their unification These results verified the effectiveness universality and transferability of <a href="https://en.wikipedia.org/wiki/UIE">UIE</a></abstract>
      <url hash="275e634d">2022.acl-long.395</url>
      <bibkey>lu-etal-2022-unified</bibkey>
      <pwccode url="https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/uie" additional="false">PaddlePaddle/PaddleNLP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="397">
      <title>Pre-training to Match for Unified Low-shot Relation Extraction</title>
      <author><first>Fangchao</first><last>Liu</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Boxi</first><last>Cao</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>5785-5795</pages>
      <abstract>Low-shot relation extraction (RE) aims to recognize novel relations with very few or even no samples, which is critical in real scenario application. Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem to be with similar target but require totally different underlying abilities. In this paper, we propose Multi-Choice Matching Networks to unify low-shot relation extraction. To fill in the gap between zero-shot and few-shot RE, we propose the triplet-paraphrase meta-training, which leverages triplet paraphrase to pre-train zero-shot label matching ability and uses meta-learning paradigm to learn few-shot instance summarizing ability. Experimental results on three different low-shot RE tasks show that the proposed method outperforms strong baselines by a large margin, and achieve the best performance on few-shot RE leaderboard.</abstract>
      <url hash="bd47af78">2022.acl-long.397</url>
      <bibkey>liu-etal-2022-pre</bibkey>
      <pwccode url="https://github.com/fc-liu/mcmn" additional="false">fc-liu/mcmn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="403">
      <title>Feeding What You Need by Understanding What You Learned</title>
      <author><first>Xiaoqiang</first><last>Wang</last></author>
      <author><first>Bang</first><last>Liu</last></author>
      <author><first>Fangli</first><last>Xu</last></author>
      <author><first>Bo</first><last>Long</last></author>
      <author><first>Siliang</first><last>Tang</last></author>
      <author><first>Lingfei</first><last>Wu</last></author>
      <pages>5858-5874</pages>
      <abstract>Machine Reading Comprehension MRC reveals the ability to understand a given text passage and answer questions based on it Existing research works in MRC rely heavily on large size models and corpus to improve the performance evaluated by <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> such as Exact Match EM$ and F_1$. However such a <a href="https://en.wikipedia.org/wiki/Paradigm">paradigm</a> lacks sufficient interpretation to model capability and can not efficiently train a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with a large corpus In this paper we argue that a deep understanding of <a href="https://en.wikipedia.org/wiki/Statistical_model">model capabilities</a> and data properties can help us feed a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> with appropriate training data based on its learning status Specifically we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi dimensional manner Based on <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> we further uncover and disentangle the connections between various <a href="https://en.wikipedia.org/wiki/Property_(philosophy)">data properties</a> and model performance Finally to verify the effectiveness of the proposed MRC capability assessment framework we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum CBBC strategy which performs a model capability based training to maximize the data value and improve training efficiency Extensive experiments demonstrate that our <a href="https://en.wikipedia.org/wiki/Design_of_experiments">approach</a> significantly improves performance achieving up to an 11.22\% 8.71\% improvement of EM$ F_1 on <a href="https://en.wikipedia.org/wiki/Functional_magnetic_resonance_imaging">MRC tasks</a><tex-math>EM</tex-math>) and <tex-math>F_1</tex-math>. However, such a paradigm lacks sufficient interpretation to model capability and can not efficiently train a model with a large corpus. In this paper, we argue that a deep understanding of model capabilities and data properties can help us feed a model with appropriate training data based on its learning status. Specifically, we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi-dimensional manner. Based on it, we further uncover and disentangle the connections between various data properties and model performance. Finally, to verify the effectiveness of the proposed MRC capability assessment framework, we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum (CBBC) strategy, which performs a model capability-based training to maximize the data value and improve training efficiency. Extensive experiments demonstrate that our approach significantly improves performance, achieving up to an 11.22% / 8.71% improvement of <tex-math>EM</tex-math> / <tex-math>F_1</tex-math> on MRC tasks.</abstract>
      <url hash="d1062933">2022.acl-long.403</url>
      <attachment type="software" hash="fa042a8f">2022.acl-long.403.software.zip</attachment>
      <bibkey>wang-etal-2022-feeding</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="404">
      <title>Probing Simile Knowledge from Pre-trained Language Models</title>
      <author><first>Weijie</first><last>Chen</last></author>
      <author><first>Yongzhu</first><last>Chang</last></author>
      <author><first>Rongsheng</first><last>Zhang</last></author>
      <author><first>Jiashu</first><last>Pu</last></author>
      <author><first>Guandan</first><last>Chen</last></author>
      <author><first>Le</first><last>Zhang</last></author>
      <author><first>Yadong</first><last>Xi</last></author>
      <author><first>Yijiang</first><last>Chen</last></author>
      <author><first>Chang</first><last>Su</last></author>
      <pages>5875-5887</pages>
      <abstract>Simile interpretation (SI) and simile generation (SG) are challenging tasks for NLP because models require adequate world knowledge to produce predictions. Previous works have employed many hand-crafted resources to bring knowledge-related into models, which is time-consuming and labor-intensive. In recent years, pre-trained language models (PLMs) based approaches have become the de-facto standard in NLP since they learn generic knowledge from a large corpus. The knowledge embedded in PLMs may be useful for SI and SG tasks. Nevertheless, there are few works to explore it. In this paper, we probe simile knowledge from PLMs to solve the SI and SG tasks in the unified framework of simile triple completion for the first time. The backbone of our framework is to construct masked sentences with manual patterns and then predict the candidate words in the masked position. In this framework, we adopt a secondary training process (Adjective-Noun mask Training) with the masked language model (MLM) loss to enhance the prediction diversity of candidate words in the masked position. Moreover, pattern ensemble (PE) and pattern search (PS) are applied to improve the quality of predicted words. Finally, automatic and human evaluations demonstrate the effectiveness of our framework in both SI and SG tasks.</abstract>
      <url hash="326cc775">2022.acl-long.404</url>
      <attachment type="software" hash="bb1ca568">2022.acl-long.404.software.zip</attachment>
      <bibkey>chen-etal-2022-probing</bibkey>
      <pwccode url="https://github.com/nairoj/Probing-Simile-from-PLM" additional="false">nairoj/Probing-Simile-from-PLM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
    </paper>
    <paper id="406">
      <title>Entailment Graph Learning with <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">Textual Entailment</a> and Soft Transitivity</title>
      <author><first>Zhibin</first><last>Chen</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>5899-5910</pages>
      <abstract>Typed entailment graphs try to learn the entailment relations between predicates from text and model them as edges between predicate nodes The construction of entailment graphs usually suffers from severe sparsity and unreliability of distributional similarity We propose a two stage method Entailment Graph with Textual Entailment and Transitivity EGT2 EGT2 learns the local entailment relations by recognizing the textual entailment between template sentences formed by typed CCG parsed predicates Based on the generated local graph EGT2 then uses three novel soft transitivity constraints to consider the logical transitivity in <a href="https://en.wikipedia.org/wiki/Logical_consequence">entailment structures</a> Experiments on benchmark datasets show that EGT2 can well model the transitivity in entailment graph to alleviate the sparsity and leads to signifcant improvement over current state of the art methods</abstract>
      <url hash="66a207da">2022.acl-long.406</url>
      <bibkey>chen-etal-2022-entailment</bibkey>
      <pwccode url="https://github.com/zacharychenpk/egt2" additional="false">zacharychenpk/egt2</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="408">
      <title>Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network</title>
      <author><first>Zheng</first><last>Gong</last></author>
      <author><first>Kun</first><last>Zhou</last></author>
      <author><first>Xin</first><last>Zhao</last></author>
      <author><first>Jing</first><last>Sha</last></author>
      <author><first>Shijin</first><last>Wang</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>5923-5933</pages>
      <abstract>In this paper, we study how to continually pre-train language models for improving the understanding of math problems. Specifically, we focus on solving a fundamental challenge in modeling math problems, how to fuse the semantics of textual description and formulas, which are highly different in essence. To address this issue, we propose a new approach called <b>COMUS</b> to <b>co</b>ntinually pre-train language models for <b>m</b>ath problem <b>u</b>nderstanding with <b>s</b>yntax-aware memory network. In this approach, we first construct the math syntax graph to model the structural semantic information, by combining the parsing trees of the text and formulas, and then design the syntax-aware memory networks to deeply fuse the features from the graph and text. With the help of syntax relations, we can model the interaction between the token from the text and its semantic-related nodes within the formulas, which is helpful to capture fine-grained semantic correlations between texts and formulas. Besides, we devise three continual pre-training tasks to further align and fuse the representations of the text and math syntax graph. Experimental results on four tasks in the math domain demonstrate the effectiveness of our approach. Our code and data are publicly available at the link: blue<url>https://github.com/RUCAIBox/COMUS</url>.</abstract>
      <url hash="f415a9ce">2022.acl-long.408</url>
      <bibkey>gong-etal-2022-continual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/math">MATH</pwcdataset>
    </paper>
    <paper id="413">
      <title>Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge</title>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Xuemeng</first><last>Hu</last></author>
      <author><first>Boyu</first><last>Wang</last></author>
      <author><first>Deyu</first><last>Zhou</last></author>
      <author><first>Qian-Wen</first><last>Zhang</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <pages>5980-5989</pages>
      <abstract>Recent years have witnessed growing interests in incorporating external knowledge such as pre-trained word embeddings (PWEs) or pre-trained language models (PLMs) into neural topic modeling. However, we found that employing PWEs and PLMs for topic modeling only achieved limited performance improvements but with huge computational overhead. In this paper, we propose a novel strategy to incorporate external knowledge into neural topic modeling where the neural topic model is pre-trained on a large corpus and then fine-tuned on the target dataset. Experiments have been conducted on three datasets and results show that the proposed approach significantly outperforms both current state-of-the-art neural topic models and some topic modeling approaches enhanced with PWEs or PLMs. Moreover, further study shows that the proposed approach greatly reduces the need for the huge size of training data.</abstract>
      <url hash="555ce37f">2022.acl-long.413</url>
      <bibkey>zhang-etal-2022-pre</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/openwebtext">OpenWebText</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="419">
      <title>Just Rank Rethinking Evaluation with Word and Sentence Similarities</title>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>C.-c.</first><last>Kuo</last></author>
      <author><first>Haizhou</first><last>Li</last></author>
      <pages>6060-6077</pages>
      <abstract>Word and sentence embeddings are useful feature representations in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> However intrinsic evaluation for <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> lags far behind and there has been no significant update since the past decade Word and sentence similarity tasks have become the de facto evaluation method It leads models to overfit to such <a href="https://en.wikipedia.org/wiki/Evaluation">evaluations</a> negatively impacting <a href="https://en.wikipedia.org/wiki/Embedding">embedding models</a> development This paper first points out the problems using <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a> as the gold standard for word and sentence embedding evaluations Further we propose a new intrinsic evaluation method called EvalRank which shows a much stronger correlation with downstream tasks Extensive experiments are conducted based on models and popular datasets to certify our judgments Finally the practical evaluation toolkit is released for future benchmarking purposes</abstract>
      <url hash="f94f180c">2022.acl-long.419</url>
      <bibkey>wang-etal-2022-just</bibkey>
      <pwccode url="https://github.com/binwang28/evalrank-embedding-evaluation" additional="false">binwang28/evalrank-embedding-evaluation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scicite">SciCite</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="421">
      <title><fixed-case>CLIP</fixed-case> Models are Few-Shot Learners: Empirical Studies on <fixed-case>VQA</fixed-case> and Visual Entailment</title>
      <author><first>Haoyu</first><last>Song</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Weinan</first><last>Zhang</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>6088-6100</pages>
      <abstract>CLIP has shown a remarkable zero-shot capability on a wide range of vision tasks. Previously, CLIP is only regarded as a powerful visual encoder. However, after being pre-trained by language supervision from a large amount of image-caption pairs, CLIP itself should also have acquired some few-shot abilities for vision-language tasks. In this work, we empirically show that CLIP can be a strong vision-language few-shot learner by leveraging the power of language. We first evaluate CLIP’s zero-shot performance on a typical visual question answering task and demonstrate a zero-shot cross-modality transfer capability of CLIP on the visual entailment task. Then we propose a parameter-efficient fine-tuning strategy to boost the few-shot performance on the vqa task. We achieve competitive zero/few-shot results on the visual question answering and visual entailment tasks without introducing any additional pre-training procedure.</abstract>
      <url hash="4d0564d8">2022.acl-long.421</url>
      <attachment type="software" hash="3ceb5371">2022.acl-long.421.software.zip</attachment>
      <bibkey>song-etal-2022-clip</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli-ve">SNLI-VE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="425">
      <title><fixed-case>S</fixed-case>ales<fixed-case>B</fixed-case>ot: Transitioning from Chit-Chat to Task-Oriented Dialogues</title>
      <author><first>Ssu</first><last>Chiu</last></author>
      <author><first>Maolin</first><last>Li</last></author>
      <author><first>Yen-Ting</first><last>Lin</last></author>
      <author><first>Yun-Nung</first><last>Chen</last></author>
      <pages>6143-6158</pages>
      <abstract>Dialogue systems are usually categorized into two types, open-domain and task-oriented. The first one focuses on chatting with users and making them engage in the conversations, where selecting a proper topic to fit the dialogue context is essential for a successful dialogue. The other one focuses on a specific task instead of casual talks, e.g., finding a movie on Friday night, playing a song. These two directions have been studied separately due to their different purposes. However, how to smoothly transition from social chatting to task-oriented dialogues is important for triggering the business opportunities, and there is no any public data focusing on such scenarios. Hence, this paper focuses on investigating the conversations starting from open-domain social chatting and then gradually transitioning to task-oriented purposes, and releases a large-scale dataset with detailed annotations for encouraging this research direction. To achieve this goal, this paper proposes a framework to automatically generate many dialogues without human involvement, in which any powerful open-domain dialogue generation model can be easily leveraged. The human evaluation shows that our generated dialogue data has a natural flow at a reasonable quality, showing that our released data has a great potential of guiding future research directions and commercial activities. Furthermore, the released models allow researchers to automatically generate unlimited dialogues in the target scenarios, which can greatly benefit semi-supervised and unsupervised approaches.</abstract>
      <url hash="90644eb9">2022.acl-long.425</url>
      <bibkey>chiu-etal-2022-salesbot</bibkey>
      <pwccode url="https://github.com/miulab/salesbot" additional="false">miulab/salesbot</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
    </paper>
    <paper id="431">
      <title><fixed-case>R</fixed-case>e<fixed-case>ACC</fixed-case>: A Retrieval-Augmented Code Completion Framework</title>
      <author><first>Shuai</first><last>Lu</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Hojae</first><last>Han</last></author>
      <author><first>Daya</first><last>Guo</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Alexey</first><last>Svyatkovskiy</last></author>
      <pages>6227-6240</pages>
      <abstract>Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing ”external” context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark.</abstract>
      <url hash="3b31f918">2022.acl-long.431</url>
      <bibkey>lu-etal-2022-reacc</bibkey>
      <pwccode url="https://github.com/celbree/reacc" additional="false">celbree/reacc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codexglue">CodeXGLUE</pwcdataset>
    </paper>
    <paper id="432">
      <title>Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in <fixed-case>D</fixed-case>oc<fixed-case>RED</fixed-case></title>
      <author><first>Quzhe</first><last>Huang</last></author>
      <author><first>Shibo</first><last>Hao</last></author>
      <author><first>Yuan</first><last>Ye</last></author>
      <author><first>Shengqi</first><last>Zhu</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>6241-6252</pages>
      <abstract>DocRED is a widely used dataset for document-level relation extraction. In the large-scale annotation, a recommend-revise scheme is adopted to reduce the workload. Within this scheme, annotators are provided with candidate relation instances from distant supervision, and they then manually supplement and remove relational facts based on the recommendations. However, when comparing DocRED with a subset relabeled from scratch, we find that this scheme results in a considerable amount of false negative samples and an obvious bias towards popular entities and relations. Furthermore, we observe that the models trained on DocRED have low recall on our relabeled dataset and inherit the same bias in the training data. Through the analysis of annotators’ behaviors, we figure out the underlying reason for the problems above: the scheme actually discourages annotators from supplementing adequate instances in the revision phase. We appeal to future research to take into consideration the issues with the recommend-revise scheme when designing new models and annotation schemes. The relabeled dataset is released at https://github.com/AndrewZhe/Revisit-DocRED, to serve as a more reliable test set of document RE models.</abstract>
      <url hash="36c7c4d3">2022.acl-long.432</url>
      <bibkey>huang-etal-2022-recommend</bibkey>
      <pwccode url="https://github.com/andrewzhe/revisit-docred" additional="false">andrewzhe/revisit-docred</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="434">
      <title>An Empirical Study of Memorization in NLP<fixed-case>NLP</fixed-case></title>
      <author><first>Xiaosen</first><last>Zheng</last></author>
      <author><first>Jing</first><last>Jiang</last></author>
      <pages>6265-6278</pages>
      <abstract>A recent study by Feldman proposed a long tail theory to explain the memorization behavior of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning models</a> However <a href="https://en.wikipedia.org/wiki/Memorization">memorization</a> has not been empirically verified in the context of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> a gap addressed by this work In this paper we use three different NLP tasks to check if the long tail theory holds Our experiments demonstrate that top ranked memorized training instances are likely atypical and removing the top memorized training instances leads to a more serious drop in test accuracy compared with removing training instances randomly Furthermore we develop an attribution method to better understand why a training instance is memorized We empirically show that our memorization attribution method is faithful and share our interesting finding that the top memorized parts of a training instance tend to be features negatively correlated with the class label</abstract>
      <url hash="78ca27f2">2022.acl-long.434</url>
      <bibkey>zheng-jiang-2022-empirical</bibkey>
      <pwccode url="https://github.com/xszheng2020/memorization" additional="false">xszheng2020/memorization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cifar-10">CIFAR-10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="437">
      <title>Guided Attention Multimodal Multitask Financial Forecasting with Inter-Company Relationships and Global and Local News</title>
      <author><first>Gary</first><last>Ang</last></author>
      <author><first>Ee-Peng</first><last>Lim</last></author>
      <pages>6313-6326</pages>
      <abstract>Most works on financial forecasting use information directly associated with individual companies (e.g., stock prices, news on the company) to predict stock returns for trading. We refer to such company-specific information as local information. Stock returns may also be influenced by global information (e.g., news on the economy in general), and inter-company relationships. Capturing such diverse information is challenging due to the low signal-to-noise ratios, different time-scales, sparsity and distributions of global and local information from different modalities. In this paper, we propose a model that captures both global and local multimodal information for investment and risk management-related forecasting tasks. Our proposed Guided Attention Multimodal Multitask Network (GAME) model addresses these challenges by using novel attention modules to guide learning with global and local information from different modalities and dynamic inter-company relationship networks. Our extensive experiments show that GAME outperforms other state-of-the-art models in several forecasting tasks and important real-world application case studies.</abstract>
      <url hash="5ebfa958">2022.acl-long.437</url>
      <attachment type="software" hash="2bd5d7c5">2022.acl-long.437.software.zip</attachment>
      <bibkey>ang-lim-2022-guided</bibkey>
    </paper>
    <paper id="442">
      <title>Universal Conditional Masked Language Pre-training for Neural Machine Translation</title>
      <author><first>Pengfei</first><last>Li</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>6379-6391</pages>
      <abstract>Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. We also introduce two simple but effective methods to enhance the CeMAT, aligned code-switching &amp; masking and dynamic dual-masking. We conduct extensive experiments and show that our CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks. Code, data, and pre-trained models are available at https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT</abstract>
      <url hash="6558a018">2022.acl-long.442</url>
      <bibkey>li-etal-2022-universal</bibkey>
      <pwccode url="https://github.com/huawei-noah/Pretrained-Language-Model" additional="false">huawei-noah/Pretrained-Language-Model</pwccode>
    </paper>
    <paper id="445">
      <title>Achieving Reliable Human Assessment of Open-Domain Dialogue Systems</title>
      <author><first>Tianbo</first><last>Ji</last></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <author><first>Gareth</first><last>Jones</last></author>
      <author><first>Chenyang</first><last>Lyu</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>6416-6437</pages>
      <abstract>Evaluation of open-domain dialogue systems is highly challenging and development of better techniques is highlighted time and again as desperately needed. Despite substantial efforts to carry out reliable live evaluation of systems in recent competitions, annotations have been abandoned and reported as too unreliable to yield sensible results. This is a serious problem since automatic metrics are not known to provide a good indication of what may or may not be a high-quality conversation. Answering the distress call of competitions that have emphasized the urgent need for better evaluation techniques in dialogue, we present the successful development of human evaluation that is highly reliable while still remaining feasible and low cost. Self-replication experiments reveal almost perfectly repeatable results with a correlation of <tex-math>r=0.969</tex-math>. Furthermore, due to the lack of appropriate methods of statistical significance testing, the likelihood of potential improvements to systems occurring due to chance is rarely taken into account in dialogue evaluation, and the evaluation we propose facilitates application of standard tests. Since we have developed a highly reliable evaluation method, new insights into system performance can be revealed. We therefore include a comparison of state-of-the-art models (i) with and without personas, to measure the contribution of personas to conversation quality, as well as (ii) prescribed versus freely chosen topics. Interestingly with respect to personas, results indicate that personas do not positively contribute to conversation quality as expected.</abstract>
      <url hash="81313516">2022.acl-long.445</url>
      <bibkey>ji-etal-2022-achieving</bibkey>
      <pwccode url="https://github.com/tianboji/dialogue-eval" additional="false">tianboji/dialogue-eval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fed">FED</pwcdataset>
    </paper>
    <paper id="449">
      <title><fixed-case>ASPECTNEWS</fixed-case>: Aspect-Oriented Summarization of News Documents</title>
      <author><first>Ojas</first><last>Ahuja</last></author>
      <author><first>Jiacheng</first><last>Xu</last></author>
      <author><first>Akshay</first><last>Gupta</last></author>
      <author><first>Kevin</first><last>Horecka</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>6494-6506</pages>
      <abstract>Generic summaries try to cover an entire document and query-based summaries try to answer document-specific questions. But real users’ needs often fall in between these extremes and correspond to aspects, high-level topics discussed among similar types of documents. In this paper, we collect a dataset of realistic aspect-oriented summaries, AspectNews, which covers different subtopics about articles in news sub-domains. We annotate data across two domains of articles, earthquakes and fraud investigations, where each article is annotated with two distinct summaries focusing on different aspects for each domain. A system producing a single generic summary cannot concisely satisfy both aspects. Our focus in evaluation is how well existing techniques can generalize to these domains without seeing in-domain training data, so we turn to techniques to construct synthetic training data that have been used in query-focused summarization work. We compare several training schemes that differ in how strongly keywords are used and how oracle summaries are extracted. Our evaluation shows that our final approach yields (a) focused summaries, better than those from a generic summarization system or from keyword matching; (b) a system sensitive to the choice of keywords.</abstract>
      <url hash="337dfe63">2022.acl-long.449</url>
      <attachment type="software" hash="14647d02">2022.acl-long.449.software.zip</attachment>
      <bibkey>ahuja-etal-2022-aspectnews</bibkey>
      <pwccode url="https://github.com/oja/aosumm" additional="false">oja/aosumm</pwccode>
    </paper>
    <paper id="450">
      <title><fixed-case>M</fixed-case>em<fixed-case>S</fixed-case>um: Extractive Summarization of Long Documents Using Multi-Step Episodic <fixed-case>M</fixed-case>arkov Decision Processes</title>
      <author><first>Nianlong</first><last>Gu</last></author>
      <author><first>Elliott</first><last>Ash</last></author>
      <author><first>Richard</first><last>Hahnloser</last></author>
      <pages>6507-6522</pages>
      <abstract>We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When MemSum iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence, 2) the global text context of the rest of the document, and 3) the extraction history consisting of the set of sentences that have already been extracted. With a lightweight architecture, MemSum obtains state-of-the-art test-set performance (ROUGE) in summarizing long documents taken from PubMed, arXiv, and GovReport. Ablation studies demonstrate the importance of local, global, and history information. A human evaluation confirms the high quality and low redundancy of the generated summaries, stemming from MemSum’s awareness of extraction history.</abstract>
      <url hash="53cc2b2a">2022.acl-long.450</url>
      <attachment type="software" hash="ce25a8f0">2022.acl-long.450.software.zip</attachment>
      <bibkey>gu-etal-2022-memsum</bibkey>
      <pwccode url="https://github.com/nianlonggu/memsum" additional="false">nianlonggu/memsum</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/govreport">GovReport</pwcdataset>
    </paper>
    <paper id="460">
      <title>Accurate Online Posterior Alignments for Principled Lexically-Constrained Decoding</title>
      <author><first>Soumya</first><last>Chatterjee</last></author>
      <author><first>Sunita</first><last>Sarawagi</last></author>
      <author><first>Preethi</first><last>Jyothi</last></author>
      <pages>6675-6689</pages>
      <abstract>Online alignment in machine translation refers to the task of aligning a target word to a source word when the target sequence has only been partially decoded. Good online alignments facilitate important applications such as lexically constrained translation where user-defined dictionaries are used to inject lexical constraints into the translation model. We propose a novel posterior alignment technique that is truly online in its execution and superior in terms of alignment error rates compared to existing methods. Our proposed inference technique jointly considers alignment and token probabilities in a principled manner and can be seamlessly integrated within existing constrained beam-search decoding algorithms. On five language pairs, including two distant language pairs, we achieve consistent drop in alignment error rates. When deployed on seven lexically constrained translation tasks, we achieve significant improvements in BLEU specifically around the constrained positions.</abstract>
      <url hash="f1aa3d5a">2022.acl-long.460</url>
      <bibkey>chatterjee-etal-2022-accurate</bibkey>
    </paper>
    <paper id="463">
      <title>Letters From the Past Modeling Historical Sound Change Through Diachronic Character Embeddings</title>
      <author><first>Sidsel</first><last>Boldsen</last></author>
      <author><first>Patrizia</first><last>Paggio</last></author>
      <pages>6713-6722</pages>
      <abstract>While a great deal of work has been done on NLP approaches to lexical semantic change detection other aspects of <a href="https://en.wikipedia.org/wiki/Language_change">language change</a> have received less attention from the NLP community In this paper we address the detection of sound change through historical spelling We propose that a sound change can be captured by comparing the relative distance through time between the distributions of the characters involved before and after the change has taken place We model these <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">distributions</a> using PPMI character embeddings We verify this hypothesis in synthetic data and then test the methods ability to trace the well known historical change of lenition of plosives in Danish historical sources We show that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are able to identify several of the changes under consideration and to uncover meaningful contexts in which they appeared The <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> has the potential to contribute to the study of open questions such as the relative chronology of sound shifts and their geographical distribution</abstract>
      <url hash="bfc34783">2022.acl-long.463</url>
      <bibkey>boldsen-paggio-2022-letters</bibkey>
    </paper>
    <paper id="467">
      <title>Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework</title>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>6775-6788</pages>
      <abstract>Simultaneous machine translation (SiMT) starts translating while receiving the streaming source inputs, and hence the source sentence is always incomplete during translating. Different from the full-sentence MT using the conventional seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture, which forces each target word to only align with a partial source prefix to adapt to the incomplete source in streaming inputs. However, the source words in the front positions are always illusoryly considered more important since they appear in more prefixes, resulting in position bias, which makes the model pay more attention on the front source positions in testing. In this paper, we first analyze the phenomenon of position bias in SiMT, and develop a Length-Aware Framework to reduce the position bias by bridging the structural gap between SiMT and full-sentence MT. Specifically, given the streaming inputs, we first predict the full-sentence length and then fill the future source position with positional encoding, thereby turning the streaming inputs into a pseudo full-sentence. The proposed framework can be integrated into most existing SiMT methods to further improve performance. Experiments on two representative SiMT methods, including the state-of-the-art adaptive policy, show that our method successfully reduces the position bias and thereby achieves better SiMT performance.</abstract>
      <url hash="d0833a90">2022.acl-long.467</url>
      <bibkey>zhang-feng-2022-reducing</bibkey>
    </paper>
    <paper id="469">
      <title>ParaDetox Detoxification with Parallel Data<fixed-case>P</fixed-case>ara<fixed-case>D</fixed-case>etox: Detoxification with Parallel Data</title>
      <author><first>Varvara</first><last>Logacheva</last></author>
      <author><first>Daryna</first><last>Dementieva</last></author>
      <author><first>Sergey</first><last>Ustyantsev</last></author>
      <author><first>Daniil</first><last>Moskovskiy</last></author>
      <author><first>David</first><last>Dale</last></author>
      <author><first>Irina</first><last>Krotova</last></author>
      <author><first>Nikita</first><last>Semenov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>6804-6818</pages>
      <abstract>We present a novel <a href="https://en.wikipedia.org/wiki/Pipeline_(computing)">pipeline</a> for the collection of <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel data</a> for the detoxification task We collect non toxic paraphrases for over 10,000 English toxic sentences We also show that this <a href="https://en.wikipedia.org/wiki/Pipeline_transport">pipeline</a> can be used to distill a large existing corpus of paraphrases to get toxic neutral sentence pairs We release two parallel corpora which can be used for the training of <a href="https://en.wikipedia.org/wiki/Detoxification_(alternative_medicine)">detoxification models</a> To the best of our knowledge these are the first parallel datasets for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> We describe our <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a> in detail to make it fast to set up for a new language or domain thus contributing to faster and easier development of new parallel resources We train several detoxification models on the collected data and compare them with several baselines and state of the art unsupervised approaches We conduct both automatic and manual evaluations All <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel data</a> outperform the state of the art <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised models</a> by a large margin This suggests that our novel datasets can boost the performance of <a href="https://en.wikipedia.org/wiki/Detoxification_(alternative_medicine)">detoxification systems</a></abstract>
      <url hash="6fe59dc1">2022.acl-long.469</url>
      <bibkey>logacheva-etal-2022-paradetox</bibkey>
      <pwccode url="https://github.com/skoltech-nlp/paradetox" additional="false">skoltech-nlp/paradetox</pwccode>
    </paper>
    <paper id="472">
      <title>Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features</title>
      <author><first>Florian</first><last>Lux</last></author>
      <author><first>Thang</first><last>Vu</last></author>
      <pages>6858-6868</pages>
      <abstract>While neural text-to-speech systems perform remarkably well in high-resource scenarios, they cannot be applied to the majority of the over 6,000 spoken languages in the world due to a lack of appropriate training data. In this work, we use embeddings derived from articulatory vectors rather than embeddings derived from phoneme identities to learn phoneme representations that hold across languages. In conjunction with language agnostic meta learning, this enables us to fine-tune a high-quality text-to-speech model on just 30 minutes of data in a previously unseen language spoken by a previously unseen speaker.</abstract>
      <url hash="0a0399ee">2022.acl-long.472</url>
      <bibkey>lux-vu-2022-language</bibkey>
      <pwccode url="https://github.com/digitalphonetics/ims-toucan" additional="false">digitalphonetics/ims-toucan</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/css10">CSS10</pwcdataset>
    </paper>
    <paper id="479">
      <title>What Makes Reading Comprehension Questions Difficult</title>
      <author><first>Saku</first><last>Sugawara</last></author>
      <author><first>Nikita</first><last>Nangia</last></author>
      <author><first>Alex</first><last>Warstadt</last></author>
      <author><first>Samuel</first><last>Bowman</last></author>
      <pages>6951-6971</pages>
      <abstract>For a natural language understanding benchmark to be useful in research it has to consist of examples that are diverse and difficult enough to discriminate among current and near future state of the art systems However we do not yet know how best to select <a href="https://en.wikipedia.org/wiki/Text_corpus">text sources</a> to collect a variety of challenging examples In this study we crowdsource multiple choice reading comprehension questions for passages taken from seven qualitatively distinct sources analyzing what attributes of passages contribute to the difficulty and question types of the collected examples To our surprise we find that passage source length and readability measures do not significantly affect question difficulty Through our manual annotation of seven reasoning types we observe several trends between passage sources and reasoning types e.g. <a href="https://en.wikipedia.org/wiki/Logical_reasoning">logical reasoning</a> is more often required in questions written for technical passages These results suggest that when creating a new benchmark dataset selecting a diverse set of passages can help ensure a diverse range of question types but that passage difficulty need not be a priority</abstract>
      <url hash="d4fb2f1d">2022.acl-long.479</url>
      <bibkey>sugawara-etal-2022-makes</bibkey>
      <pwccode url="https://github.com/nii-cl/qa-text-source-comparison" additional="false">nii-cl/qa-text-source-comparison</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mctest">MCTest</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
    </paper>
    <paper id="482">
      <title>Challenges and Strategies in Cross-Cultural <fixed-case>NLP</fixed-case></title>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Stella</first><last>Frank</last></author>
      <author><first>Heather</first><last>Lent</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Mostafa</first><last>Abdou</last></author>
      <author><first>Stephanie</first><last>Brandl</last></author>
      <author><first>Emanuele</first><last>Bugliarello</last></author>
      <author><first>Laura</first><last>Cabello Piqueras</last></author>
      <author><first>Ilias</first><last>Chalkidis</last></author>
      <author><first>Ruixiang</first><last>Cui</last></author>
      <author><first>Constanza</first><last>Fierro</last></author>
      <author><first>Katerina</first><last>Margatina</last></author>
      <author><first>Phillip</first><last>Rust</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>6997-7013</pages>
      <abstract>Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.</abstract>
      <url hash="90c53c09">2022.acl-long.482</url>
      <bibkey>hershcovich-etal-2022-challenges</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/marvl">MaRVL</pwcdataset>
    </paper>
    <paper id="483">
      <title>Prototypical Verbalizer for Prompt-based Few-shot Tuning</title>
      <author><first>Ganqu</first><last>Cui</last></author>
      <author><first>Shengding</first><last>Hu</last></author>
      <author><first>Ning</first><last>Ding</last></author>
      <author><first>Longtao</first><last>Huang</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <pages>7014-7024</pages>
      <abstract>Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning. Typically, prompt-based tuning wraps the input text into a cloze question. To make predictions, the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built. However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging.In this work, we propose the prototypical verbalizer (ProtoVerb) which is built directly from training data. Specifically, ProtoVerb learns prototype vectors as verbalizers by contrastive learning. In this way, the prototypes summarize training instances and are able to enclose rich class-level semantics. We conduct experiments on both topic classification and entity typing tasks, and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers, especially when training data is extremely scarce. More surprisingly, ProtoVerb consistently boosts prompt-based tuning even on untuned PLMs, indicating an elegant non-tuning way to utilize PLMs. Our codes are avaliable at https://github.com/thunlp/OpenPrompt.</abstract>
      <url hash="877abd99">2022.acl-long.483</url>
      <bibkey>cui-etal-2022-prototypical</bibkey>
      <pwccode url="https://github.com/thunlp/OpenPrompt" additional="false">thunlp/OpenPrompt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/few-nerd">Few-NERD</pwcdataset>
    </paper>
    <paper id="484">
      <title>Clickbait Spoiling via Question Answering and Passage Retrieval</title>
      <author><first>Matthias</first><last>Hagen</last></author>
      <author><first>Maik</first><last>Fröbe</last></author>
      <author><first>Artur</first><last>Jurk</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <pages>7025-7036</pages>
      <abstract>We introduce and study the task of clickbait spoiling generating a short text that satisfies the curiosity induced by a clickbait post Clickbait links to a web page and advertises its contents by arousing curiosity instead of providing an informative summary Our contributions are approaches to classify the type of <a href="https://en.wikipedia.org/wiki/Spoiler_(media)">spoiler</a> needed i.e. a phrase or a passage and to generate appropriate <a href="https://en.wikipedia.org/wiki/Spoiler_(media)">spoilers</a> A large scale evaluation and error analysis on a new corpus of 5,000 manually spoiled clickbait posts --- the Webis Clickbait Spoiling Corpus~2022 --shows that our spoiler type classifier achieves an accuracy of~80\% while the question answering model DeBERTa large outperforms all others in generating spoilers for both types</abstract>
      <url hash="9a9b4dc6">2022.acl-long.484</url>
      <attachment type="software" hash="8894e116">2022.acl-long.484.software.zip</attachment>
      <bibkey>hagen-etal-2022-clickbait</bibkey>
      <pwccode url="https://github.com/webis-de/acl-22" additional="false">webis-de/acl-22</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="491">
      <title>Incorporating <a href="https://en.wikipedia.org/wiki/Hierarchy">Hierarchy</a> into Text Encoder a Contrastive Learning Approach for Hierarchical Text Classification</title>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Lianzhe</first><last>Huang</last></author>
      <author><first>Xin</first><last>Sun</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>7109-7119</pages>
      <abstract>Hierarchical text classification is a challenging subtask of multi label classification due to its complex label hierarchy Existing methods encode text and label hierarchy separately and mix their representations for <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> where the <a href="https://en.wikipedia.org/wiki/Hierarchy">hierarchy</a> remains unchanged for all input text Instead of modeling them separately in this work we propose Hierarchy guided Contrastive Learning HGCLR to directly embed the hierarchy into a text encoder During training HGCLR constructs positive samples for input text under the guidance of the label hierarchy By pulling together the input text and its positive sample the text encoder can learn to generate the hierarchy aware text representation independently Therefore after training the HGCLR enhanced text encoder can dispense with the redundant hierarchy Extensive experiments on three <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark datasets</a> verify the effectiveness of HGCLR</abstract>
      <url hash="4cc605a8">2022.acl-long.491</url>
      <bibkey>wang-etal-2022-incorporating</bibkey>
      <pwccode url="https://github.com/wzh9969/contrastive-htc" additional="false">wzh9969/contrastive-htc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rcv1">RCV1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/web-of-science-dataset">WOS</pwcdataset>
    </paper>
    <paper id="493">
      <title>Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering</title>
      <author><first>Jiawei</first><last>Zhou</last></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Lan</first><last>Luo</last></author>
      <author><first>Ke</first><last>Zhan</last></author>
      <author><first>Enrui</first><last>Hu</last></author>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Jiang</last></author>
      <author><first>Zhao</first><last>Cao</last></author>
      <author><first>Fan</first><last>Yu</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Lei</first><last>Chen</last></author>
      <pages>7135-7146</pages>
      <abstract>To alleviate the data scarcity problem in training question answering systems, recent works propose additional intermediate pre-training for dense passage retrieval (DPR). However, there still remains a large discrepancy between the provided upstream signals and the downstream question-passage relevance, which leads to less improvement. To bridge this gap, we propose the HyperLink-induced Pre-training (HLP), a method to pre-train the dense retriever with the text relevance induced by hyperlink-based topology within Web documents. We demonstrate that the hyperlink-based structures of dual-link and co-mention can provide effective relevance signals for large-scale pre-training that better facilitate downstream passage retrieval. We investigate the effectiveness of our approach across a wide range of open-domain QA datasets under zero-shot, few-shot, multi-hop, and out-of-domain scenarios. The experiments show our HLP outperforms the BM25 by up to 7 points as well as other pre-training methods by more than 10 points in terms of top-20 retrieval accuracy under the zero-shot scenario. Furthermore, HLP significantly outperforms other pre-training methods under the other scenarios.</abstract>
      <url hash="0fb8f714">2022.acl-long.493</url>
      <attachment type="software" hash="6e256db4">2022.acl-long.493.software.zip</attachment>
      <bibkey>zhou-etal-2022-hyperlink</bibkey>
      <pwccode url="https://github.com/jzhoubu/hlp" additional="false">jzhoubu/hlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="495">
      <title>CAMERO Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing<fixed-case>CAMERO</fixed-case>: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing</title>
      <author><first>Chen</first><last>Liang</last></author>
      <author><first>Pengcheng</first><last>He</last></author>
      <author><first>Yelong</first><last>Shen</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <author><first>Tuo</first><last>Zhao</last></author>
      <pages>7162-7175</pages>
      <abstract>Model ensemble is a popular <a href="https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)">approach</a> to produce a low variance and well generalized model However it   induces large memory and inference costs which is often not affordable for real world deployment Existing work has resorted to sharing weights among models However when increasing the proportion of the shared weights the resulting models tend to be similar and the benefits of using <a href="https://en.wikipedia.org/wiki/Ensemble_learning">model ensemble</a> diminish To retain ensemble benefits while maintaining a low memory cost we propose a consistency regularized ensemble learning approach based on perturbed models named CAMERO Specifically we share the weights of bottom layers across all models and apply different <a href="https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)">perturbations</a> to the hidden representations for different models which can effectively promote the model diversity Meanwhile we apply a prediction consistency regularizer across the perturbed models to control the variance due to the model diversity Our experiments using large language models demonstrate that CAMERO significantly improves the <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a> performance of the ensemble model Specifically CAMERO outperforms the standard ensemble of BERT base models on the GLUE benchmark by 0.7 with a significantly smaller model size 114.2$M vs. 880.6$M</abstract>
      <url hash="f94ac94d">2022.acl-long.495</url>
      <bibkey>liang-etal-2022-camero</bibkey>
      <pwccode url="https://github.com/cliang1453/camero" additional="false">cliang1453/camero</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="496">
      <title>Interpretability for Language Learners Using Example-Based Grammatical Error Correction</title>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Ayana</first><last>Niwa</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>7176-7187</pages>
      <abstract>Grammatical Error Correction (GEC) should not focus only on high accuracy of corrections but also on interpretability for language learning.However, existing neural-based GEC models mainly aim at improving accuracy, and their interpretability has not been explored.A promising approach for improving interpretability is an example-based method, which uses similar retrieved examples to generate corrections. In addition, examples are beneficial in language learning, helping learners understand the basis of grammatically incorrect/correct texts and improve their confidence in writing.Therefore, we hypothesize that incorporating an example-based method into GEC can improve interpretability as well as support language learners.In this study, we introduce an Example-Based GEC (EB-GEC) that presents examples to language learners as a basis for a correction result.The examples consist of pairs of correct and incorrect sentences similar to a given input and its predicted correction.Experiments demonstrate that the examples presented by EB-GEC help language learners decide to accept or refuse suggestions from the GEC output.Furthermore, the experiments also show that retrieved examples improve the accuracy of corrections.</abstract>
      <url hash="9f235faf">2022.acl-long.496</url>
      <attachment type="software" hash="7dd831dd">2022.acl-long.496.software.zip</attachment>
      <bibkey>kaneko-etal-2022-interpretability</bibkey>
      <pwccode url="https://github.com/kanekomasahiro/eb-gec" additional="false">kanekomasahiro/eb-gec</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
    </paper>
    <paper id="499">
      <title><fixed-case>U</fixed-case>ni<fixed-case>X</fixed-case>coder: Unified Cross-Modal Pre-training for Code Representation</title>
      <author><first>Daya</first><last>Guo</last></author>
      <author><first>Shuai</first><last>Lu</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Yanlin</first><last>Wang</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Jian</first><last>Yin</last></author>
      <pages>7212-7225</pages>
      <abstract>Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.</abstract>
      <url hash="8ae9108d">2022.acl-long.499</url>
      <bibkey>guo-etal-2022-unixcoder</bibkey>
      <pwccode url="https://github.com/microsoft/CodeBERT" additional="true">microsoft/CodeBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cosqa">CoSQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codexglue">CodeXGLUE</pwcdataset>
    </paper>
    <paper id="505">
      <title>m<fixed-case>LUKE</fixed-case>: <fixed-case>T</fixed-case>he Power of Entity Representations in Multilingual Pretrained Language Models</title>
      <author><first>Ryokan</first><last>Ri</last></author>
      <author><first>Ikuya</first><last>Yamada</last></author>
      <author><first>Yoshimasa</first><last>Tsuruoka</last></author>
      <pages>7316-7330</pages>
      <abstract>Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities.However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks.In this study, we explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks.We train a multilingual language model with 24 languages with entity representations and showthe model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks.We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language-agnostic features.We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset.We show that entity-based prompt elicits correct factual knowledge more likely than using only word representations.</abstract>
      <url hash="9ef4c74f">2022.acl-long.505</url>
      <bibkey>ri-etal-2022-mluke</bibkey>
      <pwccode url="https://github.com/studio-ousia/luke" additional="false">studio-ousia/luke</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/relx">RELX</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="515">
      <title><fixed-case>ABC</fixed-case>: Attention with Bounded-memory Control</title>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Nikolaos</first><last>Pappas</last></author>
      <author><first>Dani</first><last>Yogatama</last></author>
      <author><first>Zhaofeng</first><last>Wu</last></author>
      <author><first>Lingpeng</first><last>Kong</last></author>
      <author><first>Roy</first><last>Schwartz</last></author>
      <author><first>Noah</first><last>Smith</last></author>
      <pages>7469-7483</pages>
      <abstract>Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights—an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.</abstract>
      <url hash="c524d141">2022.acl-long.515</url>
      <bibkey>peng-etal-2022-abc</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/realnews">RealNews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="519">
      <title>Adapting Coreference Resolution Models through <a href="https://en.wikipedia.org/wiki/Active_learning">Active Learning</a></title>
      <author><first>Michelle</first><last>Yuan</last></author>
      <author><first>Patrick</first><last>Xia</last></author>
      <author><first>Chandler</first><last>May</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>7533-7549</pages>
      <abstract>Neural coreference resolution models trained on one dataset may not transfer to new low resource domains Active learning mitigates this problem by sampling a small subset of data for annotators to label While <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a> is well defined for <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification tasks</a> its application to <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> is neither well defined nor fully understood This paper explores how to actively label coreference examining sources of model uncertainty and document reading costs We compare uncertainty sampling strategies and their advantages through thorough error analysis In both synthetic and human experiments labeling spans within the same document is more effective than annotating spans across documents The findings contribute to a more realistic development of coreference resolution models</abstract>
      <url hash="a1eb8df0">2022.acl-long.519</url>
      <bibkey>yuan-etal-2022-adapting</bibkey>
      <pwccode url="https://github.com/forest-snow/incremental-coref" additional="false">forest-snow/incremental-coref</pwccode>
    </paper>
    <paper id="527">
      <title>Overcoming a Theoretical Limitation of Self-Attention</title>
      <author><first>David</first><last>Chiang</last></author>
      <author><first>Peter</first><last>Cholak</last></author>
      <pages>7654-7664</pages>
      <abstract>Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer’s classification decisions get closer and closer to random guessing (that is, a cross-entropy of 1) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation implied by Hahn’s lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.</abstract>
      <url hash="35f2d94d">2022.acl-long.527</url>
      <bibkey>chiang-cholak-2022-overcoming</bibkey>
      <pwccode url="https://github.com/ndnlp/parity" additional="false">ndnlp/parity</pwccode>
    </paper>
    <paper id="528">
      <title>Prediction Difference Regularization against Perturbation for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a></title>
      <author><first>Dengji</first><last>Guo</last></author>
      <author><first>Zhengrui</first><last>Ma</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>7665-7675</pages>
      <abstract>Regularization methods applying input perturbation have drawn considerable attention and have been frequently explored for NMT tasks in recent years Despite their simplicity and effectiveness we argue that these methods are limited by the under fitting of training data In this paper we utilize prediction difference for ground truth tokens to analyze the fitting of token level samples and find that under fitting is almost as common as over fitting We introduce prediction difference regularization PD R a simple and effective method that can reduce over fitting and under fitting at the same time For all token level samples PD R minimizes the prediction difference between the original pass and the input perturbed pass making the model less sensitive to small input changes thus more robust to both perturbations and under fitted training data Experiments on three widely used WMT translation tasks show that our approach can significantly improve over existing perturbation regularization methods On WMT16 En De task our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves 1.80 SacreBLEU improvement over vanilla transformer</abstract>
      <url hash="3f6fc1cb">2022.acl-long.528</url>
      <bibkey>guo-etal-2022-prediction</bibkey>
    </paper>
    <paper id="529">
      <title>Make the Best of Cross-lingual Transfer: Evidence from <fixed-case>POS</fixed-case> Tagging with over 100 Languages</title>
      <author><first>Wietse</first><last>de Vries</last></author>
      <author><first>Martijn</first><last>Wieling</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>7676-7685</pages>
      <abstract>Cross-lingual transfer learning with large multilingual pre-trained models can be an effective approach for low-resource languages with no labeled training data. Existing evaluations of zero-shot cross-lingual generalisability of large pre-trained models use datasets with English training data, and test data in a selection of target languages. We explore a more extensive transfer learning setup with 65 different source languages and 105 target languages for part-of-speech tagging. Through our analysis, we show that pre-training of both source and target language, as well as matching language families, writing systems, word order systems, and lexical-phonetic distance significantly impact cross-lingual performance. The findings described in this paper can be used as indicators of which factors are important for effective zero-shot cross-lingual transfer to zero- and low-resource languages.</abstract>
      <url hash="5b592b59">2022.acl-long.529</url>
      <bibkey>de-vries-etal-2022-make</bibkey>
      <pwccode url="https://github.com/wietsedv/xpos" additional="false">wietsedv/xpos</pwccode>
    </paper>
    <paper id="531">
      <title>How Do <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Models Perform on End-to-End Data-to-Text Generation?</title>
      <author><first>Xunjian</first><last>Yin</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>7701-7710</pages>
      <abstract>With the rapid development of deep learning, Seq2Seq paradigm has become prevalent for end-to-end data-to-text generation, and the BLEU scores have been increasing in recent years. However, it is widely recognized that there is still a gap between the quality of the texts generated by models and the texts written by human. In order to better understand the ability of Seq2Seq models, evaluate their performance and analyze the results, we choose to use Multidimensional Quality Metric(MQM) to evaluate several representative Seq2Seq models on end-to-end data-to-text generation. We annotate the outputs of five models on four datasets with eight error types and find that 1) copy mechanism is helpful for the improvement in Omission and Inaccuracy Extrinsic errors but it increases other types of errors such as Addition; 2) pre-training techniques are highly effective, and pre-training strategy and model size are very significant; 3) the structure of the dataset also influences the model’s performance greatly; 4) some specific types of errors are generally challenging for seq2seq models.</abstract>
      <url hash="a5f44984">2022.acl-long.531</url>
      <bibkey>yin-wan-2022-seq2seq</bibkey>
      <pwccode url="https://github.com/xunjianyin/seq2seqondata2text" additional="false">xunjianyin/seq2seqondata2text</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/totto">ToTTo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
    </paper>
    <paper id="534">
      <title><fixed-case>L</fixed-case>i<fixed-case>LT</fixed-case>: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding</title>
      <author><first>Jiapeng</first><last>Wang</last></author>
      <author><first>Lianwen</first><last>Jin</last></author>
      <author><first>Kai</first><last>Ding</last></author>
      <pages>7747-7757</pages>
      <abstract>Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure. Code and model are publicly available at https://github.com/jpWang/LiLT.</abstract>
      <url hash="f6854d8c">2022.acl-long.534</url>
      <bibkey>wang-etal-2022-lilt</bibkey>
      <pwccode url="https://github.com/jpwang/lilt" additional="false">jpwang/lilt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cord">CORD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ephoie">EPHOIE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/funsd">FUNSD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rvl-cdip">RVL-CDIP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xfun">XFUND</pwcdataset>
    </paper>
    <paper id="536">
      <title>Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining</title>
      <author><first>Subhabrata</first><last>Dutta</last></author>
      <author><first>Jeevesh</first><last>Juneja</last></author>
      <author><first>Dipankar</first><last>Das</last></author>
      <author><first>Tanmoy</first><last>Chakraborty</last></author>
      <pages>7774-7786</pages>
      <abstract>Identifying argument components from <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured texts</a> and predicting the relationships expressed among them are two primary steps of <a href="https://en.wikipedia.org/wiki/Argument_mining">argument mining</a> The intrinsic complexity of these <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> demands powerful learning models While pretrained Transformer based Language Models LM have been shown to provide state of the art results over different NLP tasks the scarcity of manually annotated data and the highly domain dependent nature of argumentation restrict the capabilities of such models In this work we propose a novel transfer learning strategy to overcome these challenges We utilize argumentation rich social discussions from the   subreddit as a source of unsupervised argumentative discourse aware knowledge by finetuning pretrained LMs on a selectively masked language modeling task Furthermore we introduce a novel prompt based strategy for inter component relation prediction that compliments our proposed finetuning method while leveraging on the discourse context Exhaustive experiments show the generalization capability of our method on these two tasks over within domain as well as out of domain datasets outperforming several existing and employed strong baselines<i>ChangeMyView</i> subreddit as a source of unsupervised, argumentative discourse-aware knowledge by finetuning pretrained LMs on a selectively masked language modeling task. Furthermore, we introduce a novel prompt-based strategy for inter-component relation prediction that compliments our proposed finetuning method while leveraging on the discourse context. Exhaustive experiments show the generalization capability of our method on these two tasks over within-domain as well as out-of-domain datasets, outperforming several existing and employed strong baselines.</abstract>
      <url hash="04afb0ab">2022.acl-long.536</url>
      <attachment type="software" hash="d7982853">2022.acl-long.536.software.zip</attachment>
      <bibkey>dutta-etal-2022-unsupervised</bibkey>
      <pwccode url="https://github.com/jeevesh8/arg_mining" additional="false">jeevesh8/arg_mining</pwccode>
    </paper>
    <paper id="537">
      <title>Entity-based Neural Local Coherence Modeling</title>
      <author><first>Sungho</first><last>Jeon</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <pages>7787-7805</pages>
      <abstract>In this paper, we propose an entity-based neural local coherence model which is linguistically more sound than previously proposed neural coherence models. Recent neural coherence models encode the input document using large-scale pretrained language models. Hence their basis for computing local coherence are words and even sub-words. The analysis of their output shows that these models frequently compute coherence on the basis of connections between (sub-)words which, from a linguistic perspective, should not play a role. Still, these models achieve state-of-the-art performance in several end applications. In contrast to these models, we compute coherence on the basis of entities by constraining the input to noun phrases and proper names. This provides us with an explicit representation of the most important items in sentences leading to the notion of focus. This brings our model linguistically in line with pre-neural models of computing coherence. It also gives us better insight into the behaviour of the model thus leading to better explainability. Our approach is also in accord with a recent study (O’Connor and Andreas, 2021), which shows that most usable information is captured by nouns and verbs in transformer-based language models. We evaluate our model on three downstream tasks showing that it is not only linguistically more sound than previous models but also that it outperforms them in end applications.</abstract>
      <url hash="be01df80">2022.acl-long.537</url>
      <bibkey>jeon-strube-2022-entity</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gcdc">GCDC</pwcdataset>
    </paper>
    <paper id="551">
      <title>LinkBERT Pretraining Language Models with Document Links<fixed-case>L</fixed-case>ink<fixed-case>BERT</fixed-case>: Pretraining Language Models with Document Links</title>
      <author><first>Michihiro</first><last>Yasunaga</last></author>
      <author><first>Jure</first><last>Leskovec</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <pages>8003-8016</pages>
      <abstract>Language model LM pretraining captures various knowledge from <a href="https://en.wikipedia.org/wiki/Text_corpus">text corpora</a> helping downstream tasks However existing methods such as BERT model a single document and do not capture <a href="https://en.wikipedia.org/wiki/Coupling_(computer_programming)">dependencies</a> or knowledge that span across documents In this work we propose LinkBERT an LM pretraining method that leverages links between documents e.g. <a href="https://en.wikipedia.org/wiki/Hyperlink">hyperlinks</a> Given a <a href="https://en.wikipedia.org/wiki/Text_corpus">text corpus</a> we view it as a graph of documents and create LM inputs by placing linked documents in the same context We then pretrain the LM with two joint self supervised objectives masked language modeling and our new proposal document relation prediction We show that LinkBERT outperforms BERT on various downstream tasks across two domains the general domain pretrained on <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> with <a href="https://en.wikipedia.org/wiki/Hyperlink">hyperlinks</a> and biomedical domain pretrained on <a href="https://en.wikipedia.org/wiki/PubMed">PubMed</a> with citation links LinkBERT is especially effective for multi hop reasoning and few shot QA +5 absolute improvement on HotpotQA and TriviaQA and our biomedical LinkBERT sets new states of the art on various BioNLP tasks +7 on BioASQ and USMLE We release our pretrained models LinkBERT and BioLinkBERT as well as code and data</abstract>
      <url hash="614e0859">2022.acl-long.551</url>
      <bibkey>yasunaga-etal-2022-linkbert</bibkey>
      <pwccode url="https://github.com/michiyasunaga/LinkBERT" additional="false">michiyasunaga/LinkBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc2gm">BC2GM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/biosses">BIOSSES</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/blurb">BLURB</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/chemprot">ChemProt</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ddi">DDI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gad">GAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hoc-1">HOC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jnlpba">JNLPBA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mmlu">MMLU</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medqa-usmle">MedQA-USMLE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmedqa">PubMedQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="557">
      <title>Situated Dialogue Learning through Procedural Environment Generation</title>
      <author><first>Prithviraj</first><last>Ammanabrolu</last></author>
      <author><first>Renee</first><last>Jia</last></author>
      <author><first>Mark</first><last>Riedl</last></author>
      <pages>8099-8116</pages>
      <abstract>We teach goal driven agents to interactively act and speak in situated environments by training on generated curriculums Our agents operate in LIGHT Urbanek et al 2019)---a large scale crowd sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language Goals in this <a href="https://en.wikipedia.org/wiki/Environment_(systems)">environment</a> take the form of character based quests consisting of <a href="https://en.wikipedia.org/wiki/Character_(arts)">personas</a> and motivations We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals In particular we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution --- an easier environment is one that is more likely to have been found in the unaugmented dataset An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zero shot performance on never before seen quests</abstract>
      <url hash="ac28f982">2022.acl-long.557</url>
      <bibkey>ammanabrolu-etal-2022-situated</bibkey>
    </paper>
    <paper id="559">
      <title>Program Transfer for Answering Complex Questions over Knowledge Bases</title>
      <author><first>Shulin</first><last>Cao</last></author>
      <author><first>Jiaxin</first><last>Shi</last></author>
      <author><first>Zijun</first><last>Yao</last></author>
      <author><first>Xin</first><last>Lv</last></author>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Jinghui</first><last>Xiao</last></author>
      <pages>8128-8140</pages>
      <abstract>Program induction for answering complex questions over knowledge bases KBs aims to decompose a question into a multi step program whose execution against the KB produces the final answer Learning to induce programs relies on a large number of parallel question program pairs for the given KB However for most KBs the gold program annotations are usually lacking making learning difficult In this paper we propose the approach of program transfer which aims to leverage the valuable program annotations on the rich resourced KBs as external supervision signals to aid program induction for the low resourced KBs   that lack program annotations For program transfer we design a novel two stage parsing framework with an efficient ontology guided pruning strategy First a sketch parser translates the question into a high level program sketch which is the composition of functions Second given the question and sketch an argument parser searches the detailed arguments from the KB for functions During the searching we incorporate the KB ontology to prune the search space The experiments on ComplexWebQuestions and WebQuestionSP show that our method outperforms SOTA methods significantly demonstrating the effectiveness of program transfer and our framework Our codes and datasets can be obtained from https://github.com/THU-KEG/ProgramTransfer</abstract>
      <url hash="a5af6aaa">2022.acl-long.559</url>
      <bibkey>cao-etal-2022-program</bibkey>
      <pwccode url="https://github.com/thu-keg/programtransfer" additional="false">thu-keg/programtransfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/complexwebquestions">ComplexWebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="576">
      <title><fixed-case>PPT</fixed-case>: Pre-trained Prompt Tuning for Few-shot Learning</title>
      <author><first>Yuxian</first><last>Gu</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>8410-8423</pages>
      <abstract>Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient, whereas it is much worse under few-shot learning settings, which may hinder the application of prompt tuning. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework “PPT”. To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice.</abstract>
      <url hash="baa7f72a">2022.acl-long.576</url>
      <attachment type="software" hash="3fc053ec">2022.acl-long.576.software.tgz</attachment>
      <bibkey>gu-etal-2022-ppt</bibkey>
      <pwccode url="https://github.com/thu-coai/ppt" additional="false">thu-coai/ppt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/c3">C3</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ocnli">OCNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="577">
      <title>Deduplicating Training Data Makes <a href="https://en.wikipedia.org/wiki/Language_model">Language Models</a> Better</title>
      <author><first>Katherine</first><last>Lee</last></author>
      <author><first>Daphne</first><last>Ippolito</last></author>
      <author><first>Andrew</first><last>Nystrom</last></author>
      <author><first>Chiyuan</first><last>Zhang</last></author>
      <author><first>Douglas</first><last>Eck</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Nicholas</first><last>Carlini</last></author>
      <pages>8424-8445</pages>
      <abstract>We find that existing language modeling datasets contain many near duplicate examples and long repetitive substrings As a result over of the unprompted output of language models trained on these <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> is copied verbatim from the training data We develop two tools that allow us to deduplicate training datasets --- for example removing from C4 a single word English sentence that is repeated over 60,000 times Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> We can also reduce train test overlap which affects over of the validation set of standard datasets thus allowing for more accurate evaluation Code for <a href="https://en.wikipedia.org/wiki/Data_deduplication">deduplication</a> is released at https://github.com/google-research/deduplicate-text-datasets</abstract>
      <url hash="96239663">2022.acl-long.577</url>
      <bibkey>lee-etal-2022-deduplicating</bibkey>
      <pwccode url="https://github.com/google-research/deduplicate-text-datasets" additional="false">google-research/deduplicate-text-datasets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/realnews">RealNews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wiki-40b">Wiki-40B</pwcdataset>
    </paper>
    <paper id="579">
      <title><fixed-case>I</fixed-case>nternet-Augmented Dialogue Generation</title>
      <author><first>Mojtaba</first><last>Komeili</last></author>
      <author><first>Kurt</first><last>Shuster</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>8460-8478</pages>
      <abstract>The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).</abstract>
      <url hash="f13708d7">2022.acl-long.579</url>
      <bibkey>komeili-etal-2022-internet</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/topical-chat">Topical-Chat</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="581">
      <title>Knowledge Neurons in Pretrained Transformers</title>
      <author><first>Damai</first><last>Dai</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Yaru</first><last>Hao</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>8493-8502</pages>
      <abstract>Large scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus In this paper we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons Specifically we examine the fill in the blank cloze task for BERT Given a relational fact we propose a knowledge attribution method to identify the neurons that express the fact We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts In our case studies we attempt to leverage knowledge neurons to edit such as update and erase specific factual knowledge without fine tuning Our results shed light on understanding the storage of knowledge within pretrained Transformers</abstract>
      <url hash="403af576">2022.acl-long.581</url>
      <attachment type="software" hash="f4555041">2022.acl-long.581.software.zip</attachment>
      <bibkey>dai-etal-2022-knowledge</bibkey>
      <pwccode url="https://github.com/hunter-ddm/knowledge-neurons" additional="true">hunter-ddm/knowledge-neurons</pwccode>
    </paper>
    <paper id="584">
      <title>Few-Shot Learning with <fixed-case>S</fixed-case>iamese Networks and Label Tuning</title>
      <author><first>Thomas</first><last>Müller</last></author>
      <author><first>Guillermo</first><last>Pérez-Torró</last></author>
      <author><first>Marc</first><last>Franco-Salvador</last></author>
      <pages>8532-8545</pages>
      <abstract>We study the problem of building text classifiers with little or no training data, commonly known as zero and few-shot text classification. In recent years, an approach based on neural textual entailment models has been found to give strong results on a diverse range of tasks. In this work, we show that with proper pre-training, Siamese Networks that embed texts and labels offer a competitive alternative. These models allow for a large reduction in inference cost: constant in the number of labels rather than linear. Furthermore, we introduce label tuning, a simple and computationally efficient approach that allows to adapt the models in a few-shot setup by only changing the label embeddings. While giving lower performance than model fine-tuning, this approach has the architectural advantage that a single encoder can be shared by many different tasks.</abstract>
      <url hash="f612b547">2022.acl-long.584</url>
      <bibkey>muller-etal-2022-shot</bibkey>
      <pwccode url="https://github.com/symanto-research/few-shot-learning-label-tuning" additional="false">symanto-research/few-shot-learning-label-tuning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/headqa">HeadQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/isear">ISEAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="586">
      <title>Generating Biographies on <fixed-case>W</fixed-case>ikipedia: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies</title>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>8561-8576</pages>
      <abstract>Generating factual, long-form text such as Wikipedia articles raises three key challenges: how to gather relevant evidence, how to structure information into well-formed text, and how to ensure that the generated text is factually correct. We address these by developing a model for English text that uses a retrieval mechanism to identify relevant supporting information on the web and a cache-based pre-trained encoder-decoder to generate long-form biographies section by section, including citation information. To assess the impact of available web evidence on the output text, we compare the performance of our approach when generating biographies about women (for which less information is available on the web) vs. biographies generally. To this end, we curate a dataset of 1,500 biographies about women. We analyze our generated text to understand how differences in available web evidence data affect generation. We evaluate the factuality, fluency, and quality of the generated texts using automatic metrics and human evaluation. We hope that these techniques can be used as a starting point for human writers, to aid in reducing the complexity inherent in the creation of long-form, factual text.</abstract>
      <url hash="59e971bf">2022.acl-long.586</url>
      <bibkey>fan-gardent-2022-generating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisum">WikiSum</pwcdataset>
    </paper>
    <paper id="591">
      <title>Uncertainty Determines the Adequacy of the Mode and the Tractability of Decoding in Sequence-to-Sequence Models</title>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author><first>Ilia</first><last>Kulikov</last></author>
      <author><first>Shankar</first><last>Kumar</last></author>
      <pages>8634-8645</pages>
      <abstract>In many natural language processing (NLP) tasks the same input (e.g. source sentence) can have multiple possible outputs (e.g. translations). To analyze how this ambiguity (also known as intrinsic uncertainty) shapes the distribution learned by neural sequence models we measure sentence-level uncertainty by computing the degree of overlap between references in multi-reference test sets from two different NLP tasks: machine translation (MT) and grammatical error correction (GEC). At both the sentence- and the task-level, intrinsic uncertainty has major implications for various aspects of search such as the inductive biases in beam search and the complexity of exact search. In particular, we show that well-known pathologies such as a high number of beam search errors, the inadequacy of the mode, and the drop in system performance with large beam sizes apply to tasks with high level of ambiguity such as MT but not to less uncertain tasks such as GEC. Furthermore, we propose a novel exact n-best search algorithm for neural sequence models, and show that intrinsic uncertainty affects model uncertainty as the model tends to overly spread out the probability mass for uncertain tasks and sentences.</abstract>
      <url hash="7a4428c3">2022.acl-long.591</url>
      <bibkey>stahlberg-etal-2022-uncertainty</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
    </paper>
    <paper id="592">
      <title><fixed-case>F</fixed-case>lip<fixed-case>DA</fixed-case>: Effective and Robust Data Augmentation for Few-Shot Learning</title>
      <author><first>Jing</first><last>Zhou</last></author>
      <author><first>Yanan</first><last>Zheng</last></author>
      <author><first>Jie</first><last>Tang</last></author>
      <author><first>Li</first><last>Jian</last></author>
      <author><first>Zhilin</first><last>Yang</last></author>
      <pages>8646-8665</pages>
      <abstract>Most previous methods for text data augmentation are limited to simple tasks and weak baselines. We explore data augmentation on hard tasks (i.e., few-shot natural language understanding) and strong baselines (i.e., pretrained models with over one billion parameters). Under this setting, we reproduced a large number of previous augmentation methods and found that these methods bring marginal gains at best and sometimes degrade the performance much. To address this challenge, we propose a novel data augmentation method FlipDA that jointly uses a generative model and a classifier to generate label-flipped data. Central to the idea of FlipDA is the discovery that generating label-flipped data is more crucial to the performance than generating label-preserved data. Experiments show that FlipDA achieves a good tradeoff between effectiveness and robustness—it substantially improves many tasks while not negatively affecting the others.</abstract>
      <url hash="54a74aba">2022.acl-long.592</url>
      <attachment type="software" hash="e750ca3b">2022.acl-long.592.software.zip</attachment>
      <bibkey>zhou-etal-2022-flipda</bibkey>
      <pwccode url="https://github.com/zhouj8553/flipda" additional="false">zhouj8553/flipda</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/record">ReCoRD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="595">
      <title>Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining</title>
      <author><first>Chih-chan</first><last>Tien</last></author>
      <author><first>Shane</first><last>Steinert-Threlkeld</last></author>
      <pages>8696-8706</pages>
      <abstract>This work presents methods for learning cross lingual sentence representations using paired or unpaired bilingual texts We hypothesize that the cross lingual alignment strategy is transferable and therefore a model trained to align only two languages can encode multilingually more aligned representations We thus introduce dual pivot transfer training on one language pair and evaluating on other pairs To study this theory we design <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised models</a> trained on unpaired sentences and single pair supervised models trained on bitexts both based on the unsupervised language model XLM R with its parameters frozen The experiments evaluate the models as universal sentence encoders on the task of unsupervised bitext mining on two datasets where the unsupervised model reaches the state of the art of unsupervised retrieval and the alternative single pair supervised model approaches the performance of multilingually supervised models The results suggest that bilingual training techniques as proposed can be applied to get sentence representations with multilingual alignment</abstract>
      <url hash="a5f88bf7">2022.acl-long.595</url>
      <bibkey>tien-steinert-threlkeld-2022-bilingual</bibkey>
      <pwccode url="https://github.com/cctien/bimultialign" additional="false">cctien/bimultialign</pwccode>
    </paper>
    <paper id="602">
      <title>Pyramid-<fixed-case>BERT</fixed-case>: Reducing Complexity via Successive Core-set based Token Selection</title>
      <author><first>Xin</first><last>Huang</last></author>
      <author><first>Ashish</first><last>Khetan</last></author>
      <author><first>Rene</first><last>Bidart</last></author>
      <author><first>Zohar</first><last>Karnin</last></author>
      <pages>8798-8817</pages>
      <abstract>Transformer-based language models such as BERT (CITATION) have achieved the state-of-the-art performance on various NLP tasks, but are computationally prohibitive. A recent line of works use various heuristics to successively shorten sequence length while transforming tokens through encoders, in tasks such as classification and ranking that require a single token embedding for prediction.We present a novel solution to this problem, called Pyramid-BERT where we replace previously used heuristics with a <i>core-set</i> based token selection method justified by theoretical results. The core-set based token selection technique allows us to avoid expensive pre-training, gives a space-efficient fine tuning, and thus makes it suitable to handle longer sequence lengths. We provide extensive experiments establishing advantages of pyramid BERT over several baselines and existing works on the GLUE benchmarks and Long Range Arena (CITATION) datasets.</abstract>
      <url hash="080c7b81">2022.acl-long.602</url>
      <attachment type="software" hash="f854141b">2022.acl-long.602.software.zip</attachment>
      <bibkey>huang-etal-2022-pyramid</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lra">LRA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    </volume>
  <volume id="short" ingest-date="2022-05-15">
    <meta>
      <booktitle>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</booktitle>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Aline</first><last>Villavicencio</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="45a05b97">2022.acl-short</url>
    </meta>
    <frontmatter>
      <url hash="41fb4dab">2022.acl-short.0</url>
      <bibkey>acl-2022-association-linguistics</bibkey>
    </frontmatter>
    <paper id="50">
      <title>Investigating person-specific errors in chat-oriented dialogue systems</title>
      <author><first>Koh</first><last>Mitsuda</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <author><first>Tingxuan</first><last>Li</last></author>
      <author><first>Sen</first><last>Yoshida</last></author>
      <pages>464-469</pages>
      <abstract>Creating chatbots to behave like real people is important in terms of believability. Errors in general chatbots and chatbots that follow a rough persona have been studied, but those in chatbots that behave like real people have not been thoroughly investigated. We collected a large amount of user interactions of a generation-based chatbot trained from large-scale dialogue data of a specific character, i.e., target person, and analyzed errors related to that person. We found that person-specific errors can be divided into two types: errors in attributes and those in relations, each of which can be divided into two levels: self and other. The correspondence with an existing taxonomy of errors was also investigated, and person-specific errors that should be addressed in the future were clarified.</abstract>
      <url hash="5ef2eb15">2022.acl-short.50</url>
      <bibkey>mitsuda-etal-2022-investigating</bibkey>
    </paper>
    </volume>
  <volume id="srw" ingest-date="2022-05-15">
    <meta>
      <booktitle>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</booktitle>
      <editor><first>Samuel</first><last>Louvan</last></editor>
      <editor><first>Andrea</first><last>Madotto</last></editor>
      <editor><first>Brielen</first><last>Madureira</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="a7e3d8d3">2022.acl-srw</url>
    </meta>
    <frontmatter>
      <url hash="4954b0a0">2022.acl-srw.0</url>
      <bibkey>acl-2022-association</bibkey>
    </frontmatter>
    </volume>
  <volume id="demo" ingest-date="2022-05-15">
    <meta>
      <booktitle>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</booktitle>
      <editor><first>Valerio</first><last>Basile</last></editor>
      <editor><first>Zornitsa</first><last>Kozareva</last></editor>
      <editor><first>Sanja</first><last>Stajner</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="d92e3f4d">2022.acl-demo</url>
    </meta>
    <frontmatter>
      <url hash="ad64a7d9">2022.acl-demo.0</url>
      <bibkey>acl-2022-association-linguistics-system</bibkey>
    </frontmatter>
    </volume>
  <volume id="tutorials" ingest-date="2022-05-15">
    <meta>
      <booktitle>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</booktitle>
      <editor><first>Luciana</first><last>Benotti</last></editor>
      <editor><first>Naoaki</first><last>Okazaki</last></editor>
      <editor><first>Yves</first><last>Scherrer</last></editor>
      <editor><first>Marcos</first><last>Zampieri</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="b9934aae">2022.acl-tutorials</url>
    </meta>
    <frontmatter>
      <url hash="7fd254f6">2022.acl-tutorials.0</url>
      <bibkey>acl-2022-association-linguistics-tutorial</bibkey>
    </frontmatter>
    </volume>
</collection>