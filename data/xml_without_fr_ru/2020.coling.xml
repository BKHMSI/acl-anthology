<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.coling">
  <volume id="main" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the 28th International Conference on Computational Linguistics</booktitle>
      <editor><first>Donia</first><last>Scott</last></editor>
      <editor><first>Nuria</first><last>Bel</last></editor>
      <editor><first>Chengqing</first><last>Zong</last></editor>
      <publisher>International Committee on Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="e4b60169">2020.coling-main.0</url>
      <bibkey>coling-2020-international</bibkey>
    </frontmatter>
    <paper id="4">
      <title>CharBERT : Character-aware Pre-trained Language Model<fixed-case>C</fixed-case>har<fixed-case>BERT</fixed-case>: Character-aware Pre-trained Language Model</title>
      <author><first>Wentao</first><last>Ma</last></author>
      <author><first>Yiming</first><last>Cui</last></author>
      <author><first>Chenglei</first><last>Si</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Shijin</first><last>Wang</last></author>
      <author><first>Guoping</first><last>Hu</last></author>
      <pages>39–50</pages>
      <abstract>Most pre-trained language models (PLMs) construct word representations at subword level with Byte-Pair Encoding (BPE) or its variations, by which OOV (out-of-vocab) words are almost avoidable. However, those methods split a word into subword units and make the representation incomplete and fragile. In this paper, we propose a character-aware pre-trained language model named CharBERT improving on the previous methods (such as BERT, RoBERTa) to tackle these problems. We first construct the contextual word embedding for each token from the sequential character representations, then fuse the representations of characters and the subword representations by a novel heterogeneous interaction module. We also propose a new pre-training task named NLM (Noisy LM) for unsupervised character representation learning. We evaluate our method on <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, sequence labeling, and text classification tasks, both on the original datasets and adversarial misspelling test sets. The experimental results show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> can significantly improve the performance and robustness of PLMs simultaneously.</abstract>
      <url hash="9387e4ae">2020.coling-main.4</url>
      <doi>10.18653/v1/2020.coling-main.4</doi>
      <bibkey>ma-etal-2020-charbert</bibkey>
      <pwccode url="https://github.com/wtma/CharBERT" additional="false">wtma/CharBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="5">
      <title>A Graph Representation of Semi-structured Data for Web Question Answering</title>
      <author><first>Xingyao</first><last>Zhang</last></author>
      <author><first>Linjun</first><last>Shou</last></author>
      <author><first>Jian</first><last>Pei</last></author>
      <author><first>Ming</first><last>Gong</last></author>
      <author><first>Lijie</first><last>Wen</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>51–61</pages>
      <abstract>The abundant <a href="https://en.wikipedia.org/wiki/Semi-structured_data">semi-structured data</a> on the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">Web</a>, such as HTML-based tables and lists, provide commercial search engines a rich information source for question answering (QA). Different from plain text passages in Web documents, Web tables and lists have inherent structures, which carry semantic correlations among various elements in tables and lists. Many existing studies treat tables and lists as flat documents with pieces of text and do not make good use of semantic information hidden in structures. In this paper, we propose a novel graph representation of Web tables and lists based on a systematic categorization of the components in <a href="https://en.wikipedia.org/wiki/Semi-structured_data">semi-structured data</a> as well as their relations. We also develop pre-training and reasoning techniques on the graph model for the QA task. Extensive experiments on several real datasets collected from a commercial engine verify the effectiveness of our approach. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> improves <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> by 3.90 points over the state-of-the-art <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="3b4d1f94">2020.coling-main.5</url>
      <doi>10.18653/v1/2020.coling-main.5</doi>
      <bibkey>zhang-etal-2020-graph</bibkey>
    </paper>
    <paper id="10">
      <title>Is Killed More Significant than Fled? A Contextual Model for Salient Event Detection</title>
      <author><first>Disha</first><last>Jindal</last></author>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>114–124</pages>
      <abstract>Identifying the key events in a document is critical to holistically understanding its important information. Although measuring the salience of events is highly contextual, most previous work has used a limited representation of events that omits essential information. In this work, we propose a highly contextual model of event salience that uses a rich representation of events, incorporates document-level information and allows for interactions between latent event encodings. Our experimental results on an event salience dataset demonstrate that our model improves over previous work by an absolute 2-4 % on standard metrics, establishing a new state-of-the-art performance for the task. We also propose a new evaluation metric that addresses flaws in previous evaluation methodologies. Finally, we discuss the importance of salient event detection for the downstream task of summarization.</abstract>
      <url hash="3840d62f">2020.coling-main.10</url>
      <doi>10.18653/v1/2020.coling-main.10</doi>
      <bibkey>jindal-etal-2020-killed</bibkey>
    </paper>
    <paper id="11">
      <title>Appraisal Theories for Emotion Classification in Text</title>
      <author><first>Jan</first><last>Hofmann</last></author>
      <author><first>Enrica</first><last>Troiano</last></author>
      <author><first>Kai</first><last>Sassenberg</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>125–138</pages>
      <abstract>Automatic emotion categorization has been predominantly formulated as text classification in which textual units are assigned to an emotion from a predefined inventory, for instance following the fundamental emotion classes proposed by Paul Ekman (fear, joy, anger, disgust, sadness, surprise) or Robert Plutchik (adding trust, anticipation). This approach ignores existing <a href="https://en.wikipedia.org/wiki/Psychology">psychological theories</a> to some degree, which provide explanations regarding the <a href="https://en.wikipedia.org/wiki/Perception">perception of events</a>. For instance, the description that somebody discovers a snake is associated with <a href="https://en.wikipedia.org/wiki/Fear">fear</a>, based on the appraisal as being an unpleasant and non-controllable situation. This emotion reconstruction is even possible without having access to explicit reports of a subjective feeling (for instance expressing this with the words I am afraid.). Automatic classification approaches therefore need to learn properties of events as <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variables</a> (for instance that the <a href="https://en.wikipedia.org/wiki/Uncertainty">uncertainty</a> and the mental or physical effort associated with the encounter of a snake leads to fear). With this paper, we propose to make such interpretations of events explicit, following theories of cognitive appraisal of events, and show their potential for <a href="https://en.wikipedia.org/wiki/Emotion_classification">emotion classification</a> when being encoded in classification models. Our results show that high quality appraisal dimension assignments in event descriptions lead to an improvement in the classification of discrete emotion categories. We make our corpus of appraisal-annotated emotion-associated event descriptions publicly available.</abstract>
      <url hash="29ca5695">2020.coling-main.11</url>
      <doi>10.18653/v1/2020.coling-main.11</doi>
      <bibkey>hofmann-etal-2020-appraisal</bibkey>
    </paper>
    <paper id="12">
      <title>A Symmetric Local Search Network for Emotion-Cause Pair Extraction</title>
      <author><first>Zifeng</first><last>Cheng</last></author>
      <author><first>Zhiwei</first><last>Jiang</last></author>
      <author><first>Yafeng</first><last>Yin</last></author>
      <author><first>Hua</first><last>Yu</last></author>
      <author><first>Qing</first><last>Gu</last></author>
      <pages>139–149</pages>
      <abstract>Emotion-cause pair extraction (ECPE) is a new task which aims at extracting the potential clause pairs of emotions and corresponding causes in a document. To tackle this task, a two-step method was proposed by previous study which first extracted emotion clauses and cause clauses individually, then paired the emotion and cause clauses, and filtered out the pairs without <a href="https://en.wikipedia.org/wiki/Causality">causality</a>. Different from this method that separated the <a href="https://en.wikipedia.org/wiki/Detection">detection</a> and the matching of emotion and cause into two steps, we propose a Symmetric Local Search Network (SLSN) model to perform the <a href="https://en.wikipedia.org/wiki/Detection">detection</a> and matching simultaneously by local search. SLSN consists of two symmetric subnetworks, namely the emotion subnetwork and the cause subnetwork. Each <a href="https://en.wikipedia.org/wiki/Subnetwork">subnetwork</a> is composed of a clause representation learner and a local pair searcher. The local pair searcher is a specially-designed cross-subnetwork component which can extract the local emotion-cause pairs. Experimental results on the ECPE corpus demonstrate the superiority of our SLSN over existing state-of-the-art methods.</abstract>
      <url hash="0d0d73ed">2020.coling-main.12</url>
      <doi>10.18653/v1/2020.coling-main.12</doi>
      <bibkey>cheng-etal-2020-symmetric</bibkey>
      <pwccode url="https://github.com/zifengcheng/SLSN" additional="false">zifengcheng/SLSN</pwccode>
    </paper>
    <paper id="14">
      <title>METNet : A Mutual Enhanced Transformation Network for Aspect-based Sentiment Analysis<fixed-case>METN</fixed-case>et: A Mutual Enhanced Transformation Network for Aspect-based Sentiment Analysis</title>
      <author><first>Bin</first><last>Jiang</last></author>
      <author><first>Jing</first><last>Hou</last></author>
      <author><first>Wanyue</first><last>Zhou</last></author>
      <author><first>Chao</first><last>Yang</last></author>
      <author><first>Shihan</first><last>Wang</last></author>
      <author><first>Liang</first><last>Pang</last></author>
      <pages>162–172</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) aims to determine the sentiment polarity of each specific aspect in a given sentence. Existing researches have realized the importance of the aspect for the ABSA task and have derived many interactive learning methods that model context based on specific aspect. However, current interaction mechanisms are ill-equipped to learn complex sentences with multiple aspects, and these methods underestimate the representation learning of the aspect. In order to solve the two problems, we propose a mutual enhanced transformation network (METNet) for the ABSA task. First, the <a href="https://en.wikipedia.org/wiki/Aspect_(linguistics)">aspect enhancement module</a> in METNet improves the representation learning of the <a href="https://en.wikipedia.org/wiki/Aspect_(linguistics)">aspect</a> with contextual semantic features, which gives the aspect more abundant information. Second, METNet designs and implements a <a href="https://en.wikipedia.org/wiki/Hierarchical_organization">hierarchical structure</a>, which enhances the representations of aspect and context iteratively. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of METNet, and we further prove that METNet is outstanding in multi-aspect scenarios.</abstract>
      <url hash="40a1f556">2020.coling-main.14</url>
      <doi>10.18653/v1/2020.coling-main.14</doi>
      <bibkey>jiang-etal-2020-metnet</bibkey>
    </paper>
    <paper id="20">
      <title>Affective and Contextual Embedding for Sarcasm Detection</title>
      <author><first>Nastaran</first><last>Babanejad</last></author>
      <author><first>Heidar</first><last>Davoudi</last></author>
      <author><first>Aijun</first><last>An</last></author>
      <author><first>Manos</first><last>Papagelis</last></author>
      <pages>225–243</pages>
      <abstract>Automatic sarcasm detection from text is an important classification task that can help identify the actual sentiment in user-generated data, such as <a href="https://en.wikipedia.org/wiki/Review">reviews</a> or <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a>. Despite its usefulness, sarcasm detection remains a challenging task, due to a lack of any <a href="https://en.wikipedia.org/wiki/Intonation_(linguistics)">vocal intonation</a> or facial gestures in textual data. To date, most of the approaches to addressing the problem have relied on hand-crafted affect features, or pre-trained models of non-contextual word embeddings, such as <a href="https://en.wikipedia.org/wiki/Word2vec">Word2vec</a>. However, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> inherit limitations that render them inadequate for the task of sarcasm detection. In this paper, we propose two novel <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural network models</a> for sarcasm detection, namely ACE 1 and ACE 2. Given as input a text passage, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> predict whether it is sarcastic (or not). Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> extend the architecture of BERT by incorporating both affective and contextual features. To the best of our knowledge, this is the first attempt to directly alter BERT’s architecture and train it from scratch to build a sarcasm classifier. Extensive experiments on different datasets demonstrate that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> outperform state-of-the-art <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for sarcasm detection with significant margins.</abstract>
      <url hash="78c76a6f">2020.coling-main.20</url>
      <doi>10.18653/v1/2020.coling-main.20</doi>
      <bibkey>babanejad-etal-2020-affective</bibkey>
      <pwccode url="https://github.com/nastaranba/ace-for-sarcasm-detection" additional="false">nastaranba/ace-for-sarcasm-detection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sarc">SARC</pwcdataset>
    </paper>
    <paper id="21">
      <title>Understanding Pre-trained BERT for Aspect-based Sentiment Analysis<fixed-case>BERT</fixed-case> for Aspect-based Sentiment Analysis</title>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Lei</first><last>Shu</last></author>
      <author><first>Philip</first><last>Yu</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <pages>244–250</pages>
      <abstract>This paper analyzes the pre-trained hidden representations learned from reviews on BERT for tasks in aspect-based sentiment analysis (ABSA). Our work is motivated by the recent progress in BERT-based language models for ABSA. However, it is not clear how the general proxy task of (masked) language model trained on unlabeled corpus without annotations of aspects or opinions can provide important features for downstream tasks in ABSA. By leveraging the annotated datasets in ABSA, we investigate both the attentions and the learned representations of BERT pre-trained on reviews. We found that BERT uses very few self-attention heads to encode context words (such as prepositions or pronouns that indicating an aspect) and opinion words for an aspect. Most features in the representation of an aspect are dedicated to the fine-grained semantics of the domain (or product category) and the aspect itself, instead of carrying summarized opinions from its context. We hope this investigation can help future research in improving self-supervised learning, <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a> and <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> for ABSA. The pre-trained model and code can be found at https://github.com/howardhsu/BERT-for-RRC-ABSA.</abstract>
      <url hash="3bf1600c">2020.coling-main.21</url>
      <doi>10.18653/v1/2020.coling-main.21</doi>
      <bibkey>xu-etal-2020-understanding</bibkey>
      <pwccode url="https://github.com/howardhsu/BERT-for-RRC-ABSA" additional="true">howardhsu/BERT-for-RRC-ABSA</pwccode>
    </paper>
    <paper id="27">
      <title>Integrating External Event Knowledge for Script Learning</title>
      <author><first>Shangwen</first><last>Lv</last></author>
      <author><first>Fuqing</first><last>Zhu</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>306–315</pages>
      <abstract>Script learning aims to predict the subsequent event according to the existing <a href="https://en.wikipedia.org/wiki/Event_chain">event chain</a>. Recent studies focus on event co-occurrence to solve this problem. However, few studies integrate external event knowledge to solve this problem. With our observations, external event knowledge can provide additional knowledge like temporal or causal knowledge for understanding event chain better and predicting the right subsequent event. In this work, we integrate event knowledge from ASER (Activities, States, Events and their Relations) knowledge base to help predict the next event. We propose a new <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> consisting of knowledge retrieval stage and knowledge integration stage. In the knowledge retrieval stage, we select relevant external event knowledge from <a href="https://en.wikipedia.org/wiki/Autostereoscopy">ASER</a>. In the knowledge integration stage, we propose three methods to integrate external knowledge into our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> and infer final answers. Experiments on the widely-used Multi- Choice Narrative Cloze (MCNC) task show our approach achieves state-of-the-art performance compared to other methods.</abstract>
      <url hash="58aa8586">2020.coling-main.27</url>
      <doi>10.18653/v1/2020.coling-main.27</doi>
      <bibkey>lv-etal-2020-integrating</bibkey>
    </paper>
    <paper id="29">
      <title>Heterogeneous Graph Neural Networks to Predict What Happen Next</title>
      <author><first>Jianming</first><last>Zheng</last></author>
      <author><first>Fei</first><last>Cai</last></author>
      <author><first>Yanxiang</first><last>Ling</last></author>
      <author><first>Honghui</first><last>Chen</last></author>
      <pages>328–338</pages>
      <abstract>Given an incomplete event chain, script learning aims to predict the missing event, which can support a series of NLP applications. Existing work can not well represent the heterogeneous relations and capture the discontinuous event segments that are common in the <a href="https://en.wikipedia.org/wiki/Event_chain">event chain</a>. To address these issues, we introduce a heterogeneous-event (HeterEvent) graph network. In particular, we employ each unique word and individual event as <a href="https://en.wikipedia.org/wiki/Vertex_(graph_theory)">nodes</a> in the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>, and explore three kinds of <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">edges</a> based on <a href="https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences">realistic relations</a> (e.g., the relations of word-and-word, word-and-event, event-and-event). We also design a <a href="https://en.wikipedia.org/wiki/Message_passing">message passing process</a> to realize <a href="https://en.wikipedia.org/wiki/Information_exchange">information interactions</a> among <a href="https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity">homo or heterogeneous nodes</a>. And the discontinuous event segments could be explicitly modeled by finding the specific path between corresponding nodes in the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>. The experimental results on one-step and multi-step inference tasks demonstrate that our ensemble model HeterEvent[W+E ] can outperform existing baselines.</abstract>
      <url hash="29fb34e5">2020.coling-main.29</url>
      <doi>10.18653/v1/2020.coling-main.29</doi>
      <bibkey>zheng-etal-2020-heterogeneous</bibkey>
    </paper>
    <paper id="35">
      <title>Predicting Stance Change Using Modular Architectures</title>
      <author><first>Aldo</first><last>Porco</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>396–406</pages>
      <abstract>The ability to change a person’s mind on a given issue depends both on the arguments they are presented with and on their underlying perspectives and biases on that issue. Predicting stance changes require characterizing both aspects and the interaction between them, especially in realistic settings in which <a href="https://en.wikipedia.org/wiki/Stance_(martial_arts)">stance changes</a> are very rare. In this paper, we suggest a modular learning approach, which decomposes the task into multiple modules, focusing on different aspects of the interaction between users, their beliefs, and the arguments they are exposed to. Our experiments show that our modular approach archives significantly better results compared to the end-to-end approach using BERT over the same inputs.</abstract>
      <url hash="3b0999ca">2020.coling-main.35</url>
      <doi>10.18653/v1/2020.coling-main.35</doi>
      <bibkey>porco-goldwasser-2020-predicting</bibkey>
    </paper>
    <paper id="37">
      <title>Multimodal Review Generation with Privacy and Fairness Awareness</title>
      <author><first>Xuan-Son</first><last>Vu</last></author>
      <author><first>Thanh-Son</first><last>Nguyen</last></author>
      <author><first>Duc-Trong</first><last>Le</last></author>
      <author><first>Lili</first><last>Jiang</last></author>
      <pages>414–425</pages>
      <abstract>Users express their opinions towards entities (e.g., restaurants) via <a href="https://en.wikipedia.org/wiki/Review_site">online reviews</a> which can be in diverse forms such as <a href="https://en.wikipedia.org/wiki/Written_language">text</a>, <a href="https://en.wikipedia.org/wiki/Audience_measurement">ratings</a>, and <a href="https://en.wikipedia.org/wiki/Digital_image">images</a>. Modeling reviews are advantageous for user behavior understanding which, in turn, supports various user-oriented tasks such as <a href="https://en.wikipedia.org/wiki/Recommender_system">recommendation</a>, <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, and review generation. In this paper, we propose MG-PriFair, a multimodal neural-based framework, which generates personalized reviews with privacy and fairness awareness. Motivated by the fact that reviews might contain personal information and sentiment bias, we propose a novel differentially private (dp)-embedding model for training privacy guaranteed embeddings and an evaluation approach for sentiment fairness in the food-review domain. Experiments on our novel review dataset show that MG-PriFair is capable of generating plausibly long reviews while controlling the amount of exploited user data and using the least sentiment biased word embeddings. To the best of our knowledge, we are the first to bring user privacy and sentiment fairness into the review generation task. The dataset and source codes are available at https://github.com/ReML-AI/MG-PriFair.</abstract>
      <url hash="0981498d">2020.coling-main.37</url>
      <doi>10.18653/v1/2020.coling-main.37</doi>
      <bibkey>vu-etal-2020-multimodal</bibkey>
      <pwccode url="https://github.com/reml-ai/mg-prifair" additional="false">reml-ai/mg-prifair</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="39">
      <title>Improving Abstractive Dialogue Summarization with <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">Graph Structures</a> and Topic Words</title>
      <author><first>Lulu</first><last>Zhao</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <author><first>Jun</first><last>Guo</last></author>
      <pages>437–449</pages>
      <abstract>Recently, people have been beginning paying more attention to the abstractive dialogue summarization task. Since the information flows are exchanged between at least two interlocutors and key elements about a certain event are often spanned across multiple utterances, it is necessary for researchers to explore the inherent relations and structures of dialogue contents. However, the existing approaches often process the <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a> with sequence-based models, which are hard to capture long-distance inter-sentence relations. In this paper, we propose a Topic-word Guided Dialogue Graph Attention (TGDGA) network to model the <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a> as an interaction graph according to the topic word information. A masked graph self-attention mechanism is used to integrate cross-sentence information flows and focus more on the related utterances, which makes it better to understand the <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a>. Moreover, the topic word features are introduced to assist the decoding process. We evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on the SAMSum Corpus and Automobile Master Corpus. The experimental results show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms most of the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="ac7434a5">2020.coling-main.39</url>
      <doi>10.18653/v1/2020.coling-main.39</doi>
      <bibkey>zhao-etal-2020-improving</bibkey>
    </paper>
    <paper id="42">
      <title>Recent Neural Methods on Slot Filling and Intent Classification for Task-Oriented Dialogue Systems : A Survey</title>
      <author><first>Samuel</first><last>Louvan</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <pages>480–496</pages>
      <abstract>In recent years, fostered by <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning technologies</a> and by the high demand for conversational AI, various approaches have been proposed that address the capacity to elicit and understand user’s needs in task-oriented dialogue systems. We focus on two core tasks, slot filling (SF) and intent classification (IC), and survey how neural based models have rapidly evolved to address <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a> in dialogue systems. We introduce three neural architectures : independent models, which model SF and IC separately, joint models, which exploit the mutual benefit of the two tasks simultaneously, and transfer learning models, that scale the model to new domains. We discuss the current state of the research in <a href="https://en.wikipedia.org/wiki/Silicon_carbide">SF</a> and <a href="https://en.wikipedia.org/wiki/Silicon_carbide">IC</a>, and highlight challenges that still require attention.</abstract>
      <url hash="ba25b7f7">2020.coling-main.42</url>
      <doi>10.18653/v1/2020.coling-main.42</doi>
      <bibkey>louvan-magnini-2020-recent</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="43">
      <title>Re-framing Incremental Deep Language Models for <a href="https://en.wikipedia.org/wiki/Dialogue_processing">Dialogue Processing</a> with <a href="https://en.wikipedia.org/wiki/Multi-task_learning">Multi-task Learning</a></title>
      <author><first>Morteza</first><last>Rohanian</last></author>
      <author><first>Julian</first><last>Hough</last></author>
      <pages>497–507</pages>
      <abstract>We present a multi-task learning framework to enable the training of one universal incremental dialogue processing model with four tasks of disfluency detection, <a href="https://en.wikipedia.org/wiki/Language_model">language modelling</a>, <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a> and utterance segmentation in a simple deep recurrent setting. We show that these tasks provide positive inductive biases to each other with optimal contribution of each one relying on the severity of the noise from the task. Our live multi-task model outperforms similar individual tasks, delivers competitive performance and is beneficial for future use in conversational agents in psychiatric treatment.</abstract>
      <url hash="7a1e955e">2020.coling-main.43</url>
      <doi>10.18653/v1/2020.coling-main.43</doi>
      <bibkey>rohanian-hough-2020-framing</bibkey>
      <pwccode url="https://github.com/mortezaro/mtl-disfluency-detection" additional="false">mortezaro/mtl-disfluency-detection</pwccode>
    </paper>
    <paper id="58">
      <title>TIMBERT : Toponym Identifier For The Medical Domain Based on BERT<fixed-case>TIMBERT</fixed-case>: Toponym Identifier For The Medical Domain Based on <fixed-case>BERT</fixed-case></title>
      <author><first>MohammadReza</first><last>Davari</last></author>
      <author><first>Leila</first><last>Kosseim</last></author>
      <author><first>Tien</first><last>Bui</last></author>
      <pages>662–668</pages>
      <abstract>In this paper, we propose an approach to automate the process of place name detection in the medical domain to enable epidemiologists to better study and model the spread of viruses. We created a family of Toponym Identification Models based on BERT (TIMBERT), in order to learn in an end-to-end fashion the mapping from an input sentence to the associated sentence labeled with <a href="https://en.wikipedia.org/wiki/Toponymy">toponyms</a>. When evaluated with the SemEval 2019 task 12 test set (Weissenbacher et al., 2019), our best TIMBERT model achieves an F1 score of 90.85 %, a significant improvement compared to the state-of-the-art of 89.13 % (Wang et al., 2019).</abstract>
      <url hash="f92ae49c">2020.coling-main.58</url>
      <doi>10.18653/v1/2020.coling-main.58</doi>
      <bibkey>davari-etal-2020-timbert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="61">
      <title>Identifying Depressive Symptoms from Tweets : Figurative Language Enabled Multitask Learning Framework</title>
      <author><first>Shweta</first><last>Yadav</last></author>
      <author><first>Jainish</first><last>Chauhan</last></author>
      <author><first>Joy Prakash</first><last>Sain</last></author>
      <author><first>Krishnaprasad</first><last>Thirunarayan</last></author>
      <author><first>Amit</first><last>Sheth</last></author>
      <author><first>Jeremiah</first><last>Schumm</last></author>
      <pages>696–709</pages>
      <abstract>Existing studies on using <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> for deriving <a href="https://en.wikipedia.org/wiki/Mental_health">mental health status</a> of users focus on the depression detection task. However, for <a href="https://en.wikipedia.org/wiki/Case_management_(mental_health)">case management</a> and referral to psychiatrists, health-care workers require practical and scalable depressive disorder screening and triage system. This study aims to design and evaluate a decision support system (DSS) to reliably determine the depressive triage level by capturing fine-grained depressive symptoms expressed in user tweets through the emulation of the Patient Health Questionnaire-9 (PHQ-9) that is routinely used in clinical practice. The reliable detection of depressive symptoms from <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a> is challenging because the 280-character limit on tweets incentivizes the use of creative artifacts in the utterances and figurative usage contributes to effective expression. We propose a novel BERT based robust multi-task learning framework to accurately identify the depressive symptoms using the auxiliary task of figurative usage detection. Specifically, our proposed novel task sharing mechanism, co-task aware attention, enables automatic selection of optimal information across the BERT lay-ers and tasks by soft-sharing of parameters. Our results show that modeling figurative usage can demonstrably improve the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s robustness and <a href="https://en.wikipedia.org/wiki/Reliability_(statistics)">reliability</a> for distinguishing the <a href="https://en.wikipedia.org/wiki/Major_depressive_disorder">depression symptoms</a>.</abstract>
      <url hash="72145aa8">2020.coling-main.61</url>
      <doi>10.18653/v1/2020.coling-main.61</doi>
      <bibkey>yadav-etal-2020-identifying</bibkey>
    </paper>
    <paper id="64">
      <title>Probing Multimodal Embeddings for Linguistic Properties : the Visual-Semantic Case</title>
      <author><first>Adam</first><last>Dahlgren Lindström</last></author>
      <author><first>Johanna</first><last>Björklund</last></author>
      <author><first>Suna</first><last>Bensch</last></author>
      <author><first>Frank</first><last>Drewes</last></author>
      <pages>730–744</pages>
      <abstract>Semantic embeddings have advanced the state of the art for countless natural language processing tasks, and various extensions to multimodal domains, such as visual-semantic embeddings, have been proposed. While the power of visual-semantic embeddings comes from the distillation and enrichment of information through <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>, their inner workings are poorly understood and there is a shortage of analysis tools. To address this problem, we generalize the notion ofprobing tasks to the visual-semantic case. To this end, we (i) discuss the formalization of probing tasks for <a href="https://en.wikipedia.org/wiki/Embedding">embeddings of image-caption pairs</a>, (ii) define three concrete probing tasks within our general framework, (iii) train <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> to probe for those properties, and (iv) compare various state-of-the-art <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> under the lens of the proposed probing tasks. Our experiments reveal an up to 16 % increase in accuracy on visual-semantic embeddings compared to the corresponding unimodal embeddings, which suggest that the text and image dimensions represented in the former do complement each other.</abstract>
      <url hash="36f9d8a0">2020.coling-main.64</url>
      <doi>10.18653/v1/2020.coling-main.64</doi>
      <bibkey>dahlgren-lindstrom-etal-2020-probing</bibkey>
      <pwccode url="https://github.com/dali-does/vse-probing" additional="false">dali-does/vse-probing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="72">
      <title>Aspect-Category based Sentiment Analysis with Hierarchical Graph Convolutional Network</title>
      <author><first>Hongjie</first><last>Cai</last></author>
      <author><first>Yaofeng</first><last>Tu</last></author>
      <author><first>Xiangsheng</first><last>Zhou</last></author>
      <author><first>Jianfei</first><last>Yu</last></author>
      <author><first>Rui</first><last>Xia</last></author>
      <pages>833–843</pages>
      <abstract>Most of the aspect based sentiment analysis research aims at identifying the sentiment polarities toward some explicit aspect terms while ignores implicit aspects in text. To capture both explicit and implicit aspects, we focus on aspect-category based sentiment analysis, which involves joint aspect category detection and category-oriented sentiment classification. However, currently only a few simple studies have focused on this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a>. The shortcomings in the way they defined the task make their approaches difficult to effectively learn the inner-relations between categories and the inter-relations between categories and sentiments. In this work, we re-formalize the task as a category-sentiment hierarchy prediction problem, which contains a hierarchy output structure to first identify multiple aspect categories in a piece of text, and then predict the sentiment for each of the identified categories. Specifically, we propose a Hierarchical Graph Convolutional Network (Hier-GCN), where a lower-level GCN is to model the inner-relations among multiple categories, and the higher-level GCN is to capture the inter-relations between aspect categories and sentiments. Extensive evaluations demonstrate that our hierarchy output structure is superior over existing ones, and the Hier-GCN model can consistently achieve the best results on four benchmarks.</abstract>
      <url hash="228d9e7b">2020.coling-main.72</url>
      <doi>10.18653/v1/2020.coling-main.72</doi>
      <bibkey>cai-etal-2020-aspect</bibkey>
    </paper>
    <paper id="73">
      <title>Constituency Lattice Encoding for Aspect Term Extraction</title>
      <author><first>Yunyi</first><last>Yang</last></author>
      <author><first>Kun</first><last>Li</last></author>
      <author><first>Xiaojun</first><last>Quan</last></author>
      <author><first>Weizhou</first><last>Shen</last></author>
      <author><first>Qinliang</first><last>Su</last></author>
      <pages>844–855</pages>
      <abstract>One of the remaining challenges for aspect term extraction in <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> resides in the extraction of phrase-level aspect terms, which is non-trivial to determine the boundaries of such terms. In this paper, we aim to address this issue by incorporating the span annotations of constituents of a sentence to leverage the syntactic information in neural network models. To this end, we first construct a constituency lattice structure based on the constituents of a constituency tree. Then, we present two approaches to encoding the constituency lattice using BiLSTM-CRF and BERT as the base models, respectively. We experimented on two benchmark datasets to evaluate the two models, and the results confirm their superiority with respective 3.17 and 1.35 points gained in F1-Measure over the current <a href="https://en.wikipedia.org/wiki/State_of_the_art">state of the art</a>. The improvements justify the effectiveness of the constituency lattice for aspect term extraction.</abstract>
      <url hash="c638a7d8">2020.coling-main.73</url>
      <doi>10.18653/v1/2020.coling-main.73</doi>
      <bibkey>yang-etal-2020-constituency</bibkey>
      <pwccode url="https://github.com/leekum2018/cle4ate" additional="false">leekum2018/cle4ate</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="81">
      <title>A Dataset and Evaluation Framework for Complex Geographical Description Parsing</title>
      <author><first>Egoitz</first><last>Laparra</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <pages>936–948</pages>
      <abstract>Much previous work on <a href="https://en.wikipedia.org/wiki/Geoparsing">geoparsing</a> has focused on identifying and resolving individual <a href="https://en.wikipedia.org/wiki/Toponymy">toponyms</a> in text like Adrano, S.Maria di Licodia or Catania. However, geographical locations occur not only as individual toponyms, but also as compositions of reference geolocations joined and modified by connectives, e.g.,. between the towns of Adrano and S.Maria di Licodia, 32 kilometres northwest of Catania. Ideally, a geoparser should be able to take such text, and the geographical shapes of the toponyms referenced within it, and parse these into a geographical shape, formed by a set of coordinates, that represents the location described. But creating a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for this complex geoparsing task is difficult and, if done manually, would require a huge amount of effort to annotate the geographical shapes of not only the <a href="https://en.wikipedia.org/wiki/Geolocation">geolocation</a> described but also the reference toponyms. We present an approach that automates most of the process by combining <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> and <a href="https://en.wikipedia.org/wiki/OpenStreetMap">OpenStreetMap</a>. As a result, we have gathered a <a href="https://en.wikipedia.org/wiki/Collection_(abstract_data_type)">collection</a> of 360,187 uncurated complex geolocation descriptions, from which we have manually curated 1,000 examples intended to be used as a test set. To accompany the data, we define a new geoparsing evaluation framework along with a scoring methodology and a set of <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="c1fe1587">2020.coling-main.81</url>
      <doi>10.18653/v1/2020.coling-main.81</doi>
      <bibkey>laparra-bethard-2020-dataset</bibkey>
      <pwccode url="https://github.com/egolaparra/geocode-data" additional="true">egolaparra/geocode-data</pwccode>
    </paper>
    <paper id="82">
      <title>DocBank : A Benchmark Dataset for <a href="https://en.wikipedia.org/wiki/Document_layout_analysis">Document Layout Analysis</a><fixed-case>D</fixed-case>oc<fixed-case>B</fixed-case>ank: A Benchmark Dataset for Document Layout Analysis</title>
      <author><first>Minghao</first><last>Li</last></author>
      <author><first>Yiheng</first><last>Xu</last></author>
      <author><first>Lei</first><last>Cui</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <pages>949–960</pages>
      <abstract>Document layout analysis usually relies on <a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision models</a> to understand documents while ignoring <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">textual information</a> that is vital to capture. Meanwhile, high quality labeled datasets with both visual and textual information are still insufficient. In this paper, we present DocBank, a benchmark dataset that contains 500 K document pages with fine-grained token-level annotations for <a href="https://en.wikipedia.org/wiki/Document_layout_analysis">document layout analysis</a>. DocBank is constructed using a simple yet effective way with weak supervision from the LaTeX documents available on the arXiv.com. With DocBank, models from different modalities can be compared fairly and multi-modal approaches will be further investigated and boost the performance of <a href="https://en.wikipedia.org/wiki/Document_layout_analysis">document layout analysis</a>. We build several strong baselines and manually split train / dev / test sets for evaluation. Experiment results show that <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> trained on DocBank accurately recognize the layout information for a variety of documents. The DocBank dataset is publicly available at https://github.com/doc-analysis/DocBank.</abstract>
      <url hash="44294285">2020.coling-main.82</url>
      <doi>10.18653/v1/2020.coling-main.82</doi>
      <bibkey>li-etal-2020-docbank</bibkey>
      <pwccode url="https://github.com/doc-analysis/DocBank" additional="false">doc-analysis/DocBank</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docbank">DocBank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/publaynet">PubLayNet</pwcdataset>
    </paper>
    <paper id="84">
      <title>A High Precision Pipeline for Financial Knowledge Graph Construction</title>
      <author><first>Sarah</first><last>Elhammadi</last></author>
      <author><first>Laks</first><last>V.S. Lakshmanan</last></author>
      <author><first>Raymond</first><last>Ng</last></author>
      <author><first>Michael</first><last>Simpson</last></author>
      <author><first>Baoxing</first><last>Huai</last></author>
      <author><first>Zhefeng</first><last>Wang</last></author>
      <author><first>Lanjun</first><last>Wang</last></author>
      <pages>967–977</pages>
      <abstract>Motivated by applications such as <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, <a href="https://en.wikipedia.org/wiki/Fact-checking">fact checking</a>, and <a href="https://en.wikipedia.org/wiki/Data_integration">data integration</a>, there is significant interest in constructing knowledge graphs by extracting information from unstructured information sources, particularly text documents. Knowledge graphs have emerged as a standard for structured knowledge representation, whereby entities and their inter-relations are represented and conveniently stored as (subject, predicate, object) triples in a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> that can be used to power various downstream applications. The proliferation of financial news sources reporting on companies, <a href="https://en.wikipedia.org/wiki/Market_(economics)">markets</a>, <a href="https://en.wikipedia.org/wiki/Currency">currencies</a>, and stocks presents an opportunity for extracting valuable knowledge about this crucial domain. In this paper, we focus on constructing a <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> automatically by <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a> from a large corpus of financial news articles. For that purpose, we develop a high precision knowledge extraction pipeline tailored for the <a href="https://en.wikipedia.org/wiki/Financial_services">financial domain</a>. This pipeline combines multiple information extraction techniques with a financial dictionary that we built, all working together to produce over 342,000 compact extractions from over 288,000 financial news articles, with a precision of 78 % at the top-100 extractions. The extracted triples are stored in a <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> making them readily available for use in downstream applications.</abstract>
      <url hash="eb85f42b">2020.coling-main.84</url>
      <doi>10.18653/v1/2020.coling-main.84</doi>
      <bibkey>elhammadi-etal-2020-high</bibkey>
    </paper>
    <paper id="88">
      <title>Automatic Charge Identification from Facts : A Few Sentence-Level Charge Annotations is All You Need</title>
      <author><first>Shounak</first><last>Paul</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <author><first>Saptarshi</first><last>Ghosh</last></author>
      <pages>1011–1022</pages>
      <abstract>Automatic Charge Identification (ACI) is the task of identifying the relevant charges given the facts of a situation and the statutory laws that define these <a href="https://en.wikipedia.org/wiki/Criminal_charge">charges</a>, and is a crucial aspect of the <a href="https://en.wikipedia.org/wiki/Procedural_law">judicial process</a>. Existing works focus on learning charge-side representations by modeling relationships between the charges, but not much effort has been made in improving fact-side representations. We observe that only a small fraction of sentences in the facts actually indicates the charges. We show that by using a very small subset (3 %) of fact descriptions annotated with sentence-level charges, we can achieve an improvement across a range of different ACI models, as compared to modeling just the main document-level task on a much larger dataset. Additionally, we propose a novel model that utilizes sentence-level charge labels as an auxiliary task, coupled with the main task of document-level charge identification in a multi-task learning framework. The proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> comprehensively outperforms a large number of recent <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> for ACI. The improvement in performance is particularly noticeable for the rare charges which are known to be especially challenging to identify.</abstract>
      <url hash="cadbda1c">2020.coling-main.88</url>
      <doi>10.18653/v1/2020.coling-main.88</doi>
      <bibkey>paul-etal-2020-automatic</bibkey>
    </paper>
    <paper id="89">
      <title>Context-Aware Text Normalisation for Historical Dialects</title>
      <author><first>Maria</first><last>Sukhareva</last></author>
      <pages>1023–1036</pages>
      <abstract>Context-aware historical text normalisation is a severely under-researched area. To fill the gap we propose a context-aware normalisation approach that relies on the state-of-the-art methods in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> and <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>. We propose a multidialect normaliser with a context-aware reranking of the candidates. The reranker relies on a word-level n-gram language model that is applied to the five best normalisation candidates. The results are evaluated on the historical multidialect datasets of <a href="https://en.wikipedia.org/wiki/German_language">German</a>, <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>, <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese</a> and <a href="https://en.wikipedia.org/wiki/Slovene_language">Slovene</a>. We show that incorporating dialectal information into the training leads to an accuracy improvement on all the <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>. The context-aware reranking gives further improvement over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>. For three out of six datasets, we reach a significantly higher <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> than reported in the previous studies. The other three results are comparable with the current state-of-the-art. The code for the reranker is published as open-source.</abstract>
      <url hash="b2951d1f">2020.coling-main.89</url>
      <doi>10.18653/v1/2020.coling-main.89</doi>
      <bibkey>sukhareva-2020-context</bibkey>
    </paper>
    <paper id="91">
      <title>Exploring Amharic Sentiment Analysis from Social Media Texts : Building Annotation Tools and Classification Models<fixed-case>A</fixed-case>mharic Sentiment Analysis from Social Media Texts: Building Annotation Tools and Classification Models</title>
      <author><first>Seid Muhie</first><last>Yimam</last></author>
      <author><first>Hizkiel Mitiku</first><last>Alemayehu</last></author>
      <author><first>Abinew</first><last>Ayele</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>1048–1060</pages>
      <abstract>This paper presents the study of <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> for Amharic social media texts. As the number of social media users is ever-increasing, social media platforms would like to understand the latent meaning and sentiments of a text to enhance decision-making procedures. However, low-resource languages such as <a href="https://en.wikipedia.org/wiki/Amharic">Amharic</a> have received less attention due to several reasons such as lack of well-annotated datasets, unavailability of computing resources, and fewer or no expert researchers in the area. This research addresses three main research questions. We first explore the suitability of existing <a href="https://en.wikipedia.org/wiki/Programming_tool">tools</a> for the <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis task</a>. Annotation tools are scarce to support large-scale annotation tasks in <a href="https://en.wikipedia.org/wiki/Amharic">Amharic</a>. Also, the existing <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing platforms</a> do not support Amharic text annotation. Hence, we build a social-network-friendly annotation tool called ‘ASAB’ using the <a href="https://en.wikipedia.org/wiki/Telegram_(software)">Telegram bot</a>. We collect 9.4k tweets, where each tweet is annotated by three Telegram users. Moreover, we explore the suitability of machine learning approaches for Amharic sentiment analysis. The FLAIR deep learning text classifier, based on network embeddings that are computed from a distributional thesaurus, outperforms other <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised classifiers</a>. We further investigate the challenges in building a sentiment analysis system for <a href="https://en.wikipedia.org/wiki/Amharic">Amharic</a> and we found that the widespread usage of <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> and <a href="https://en.wikipedia.org/wiki/Literal_and_figurative_language">figurative speech</a> are the main issues in dealing with the problem. To advance the <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> research in <a href="https://en.wikipedia.org/wiki/Amharic">Amharic</a> and other related low-resource languages, we release the dataset, the annotation tool, source code, and models publicly under a permissive.</abstract>
      <url hash="07578b97">2020.coling-main.91</url>
      <doi>10.18653/v1/2020.coling-main.91</doi>
      <bibkey>yimam-etal-2020-exploring</bibkey>
    </paper>
    <paper id="92">
      <title>Effective Few-Shot Classification with Transfer Learning</title>
      <author><first>Aakriti</first><last>Gupta</last></author>
      <author><first>Kapil</first><last>Thadani</last></author>
      <author><first>Neil</first><last>O’Hare</last></author>
      <pages>1061–1066</pages>
      <abstract>Few-shot learning addresses the the problem of <a href="https://en.wikipedia.org/wiki/Machine_learning">learning</a> based on a small amount of training data. Although more well-studied in the domain of <a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a>, recent work has adapted the Amazon Review Sentiment Classification (ARSC) text dataset for use in the few-shot setting. In this work, we use the ARSC dataset to study a simple application of transfer learning approaches to few-shot classification. We train a single binary classifier to learn all few-shot classes jointly by prefixing class identifiers to the input text. Given the text and class, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> then makes a binary prediction for that text / class pair. Our results show that this simple approach can outperform most published results on this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. Surprisingly, we also show that including domain information as part of the task definition only leads to a modest improvement in model accuracy, and zero-shot classification, without further fine-tuning on few-shot domains, performs equivalently to few-shot classification. These results suggest that the classes in the ARSC few-shot task, which are defined by the intersection of domain and rating, are actually very similar to each other, and that a more suitable dataset is needed for the study of few-shot text classification.</abstract>
      <url hash="49dcc430">2020.coling-main.92</url>
      <doi>10.18653/v1/2020.coling-main.92</doi>
      <bibkey>gupta-etal-2020-effective</bibkey>
    </paper>
    <paper id="97">
      <title>Meet Changes with Constancy : Learning Invariance in Multi-Source Translation</title>
      <author><first>Jianfeng</first><last>Liu</last></author>
      <author><first>Ling</first><last>Luo</last></author>
      <author><first>Xiang</first><last>Ao</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Jian</first><last>Ye</last></author>
      <pages>1122–1132</pages>
      <abstract>Multi-source neural machine translation aims to translate from parallel sources of information (e.g. languages, <a href="https://en.wikipedia.org/wiki/Image_(journal)">images</a>, etc.) to a single target language, which has shown better performance than most one-to-one systems. Despite the remarkable success of existing models, they usually neglect the fact that multiple source inputs may have inconsistencies. Such differences might bring noise to the task and limit the performance of existing multi-source NMT approaches due to their indiscriminate usage of input sources for target word predictions. In this paper, we attempt to leverage the potential complementary information among distinct sources and alleviate the occasional conflicts of them. To accomplish that, we propose a source invariance network to learn the <a href="https://en.wikipedia.org/wiki/Invariant_(mathematics)">invariant information</a> of parallel sources. Such <a href="https://en.wikipedia.org/wiki/Computer_network">network</a> can be easily integrated with multi-encoder based multi-source NMT methods (e.g. multi-encoder RNN and transformer) to enhance the <a href="https://en.wikipedia.org/wiki/Translation">translation</a> results. Extensive experiments on two multi-source translation tasks demonstrate that the proposed approach not only achieves clear gains in translation quality but also captures implicit invariance between different sources.</abstract>
      <url hash="9b0b537b">2020.coling-main.97</url>
      <doi>10.18653/v1/2020.coling-main.97</doi>
      <bibkey>liu-etal-2020-meet</bibkey>
    </paper>
    <paper id="109">
      <title>How Relevant Are Selectional Preferences for Transformer-based Language Models?</title>
      <author><first>Eleni</first><last>Metheniti</last></author>
      <author><first>Tim</first><last>Van de Cruys</last></author>
      <author><first>Nabil</first><last>Hathout</last></author>
      <pages>1266–1278</pages>
      <abstract>Selectional preference is defined as the tendency of a predicate to favor particular arguments within a certain <a href="https://en.wikipedia.org/wiki/Context_(language_use)">linguistic context</a>, and likewise, reject others that result in conflicting or implausible meanings. The stellar success of contextual word embedding models such as BERT in NLP tasks has led many to question whether these models have learned linguistic information, but up till now, most research has focused on syntactic information. We investigate whether <a href="https://en.wikipedia.org/wiki/Bert_Diaries">Bert</a> contains information on the selectional preferences of words, by examining the probability it assigns to the dependent word given the presence of a <a href="https://en.wikipedia.org/wiki/Headword">head word</a> in a sentence. We are using word pairs of head-dependent words in five different syntactic relations from the SP-10 K corpus of selectional preference (Zhang et al., 2019b), in sentences from the ukWaC corpus, and we are calculating the correlation of the plausibility score (from SP-10 K) and the model probabilities. Our results show that overall, there is no strong positive or negative correlation in any syntactic relation, but we do find that certain head words have a strong correlation and that masking all words but the head word yields the most positive correlations in most scenarios which indicates that the semantics of the predicate is indeed an integral and influential factor for the selection of the argument.</abstract>
      <url hash="2cd4b323">2020.coling-main.109</url>
      <doi>10.18653/v1/2020.coling-main.109</doi>
      <bibkey>metheniti-etal-2020-relevant</bibkey>
      <pwccode url="https://github.com/lenakmeth/bert_selectional_preferences" additional="false">lenakmeth/bert_selectional_preferences</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sp-10k">SP-10K</pwcdataset>
    </paper>
    <paper id="111">
      <title>A Retrofitting Model for Incorporating <a href="https://en.wikipedia.org/wiki/Semantic_relation">Semantic Relations</a> into Word Embeddings</title>
      <author><first>Sapan</first><last>Shah</last></author>
      <author><first>Sreedhar</first><last>Reddy</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>1292–1298</pages>
      <abstract>We present a novel retrofitting model that can leverage relational knowledge available in a knowledge resource to improve <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. The knowledge is captured in terms of relation inequality constraints that compare similarity of related and unrelated entities in the context of an anchor entity. These <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraints</a> are used as training data to learn a non-linear transformation function that maps original word vectors to a <a href="https://en.wikipedia.org/wiki/Vector_space">vector space</a> respecting these <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraints</a>. The transformation function is learned in a similarity metric learning setting using Triplet network architecture. We applied our model to synonymy, antonymy and hypernymy relations in <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a> and observed large gains in performance over original distributional models as well as other retrofitting approaches on word similarity task and significant overall improvement on lexical entailment detection task.</abstract>
      <url hash="1fe4134f">2020.coling-main.111</url>
      <doi>10.18653/v1/2020.coling-main.111</doi>
      <bibkey>shah-etal-2020-retrofitting</bibkey>
    </paper>
    <paper id="112">
      <title>Lexical Relation Mining in Neural Word Embeddings</title>
      <author><first>Aishwarya</first><last>Jadhav</last></author>
      <author><first>Yifat</first><last>Amir</last></author>
      <author><first>Zachary</first><last>Pardos</last></author>
      <pages>1299–1311</pages>
      <abstract>Work with neural word embeddings and lexical relations has largely focused on confirmatory experiments which use human-curated examples of semantic and syntactic relations to validate against. In this paper, we explore the degree to which lexical relations, such as those found in popular validation sets, can be derived and extended from a variety of neural embeddings using classical clustering methods. We show that the Word2Vec space of word-pairs (i.e., offset vectors) significantly outperforms other more contemporary methods, even in the presence of a large number of noisy offsets. Moreover, we show that via a simple <a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search">nearest neighbor approach</a> in the offset space, new examples of known relations can be discovered. Our results speak to the amenability of offset vectors from non-contextual neural embeddings to find semantically coherent clusters. This simple approach has implications for the exploration of emergent regularities and their examples, such as emerging trends on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> and their related posts.</abstract>
      <url hash="5de54110">2020.coling-main.112</url>
      <doi>10.18653/v1/2020.coling-main.112</doi>
      <bibkey>jadhav-etal-2020-lexical</bibkey>
    </paper>
    <paper id="114">
      <title>BERT-based Cohesion Analysis of <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese Texts</a><fixed-case>BERT</fixed-case>-based Cohesion Analysis of <fixed-case>J</fixed-case>apanese Texts</title>
      <author><first>Nobuhiro</first><last>Ueda</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>1323–1333</pages>
      <abstract>The meaning of natural language text is supported by <a href="https://en.wikipedia.org/wiki/Cohesion_(linguistics)">cohesion</a> among various kinds of entities, including <a href="https://en.wikipedia.org/wiki/Coreference">coreference relations</a>, predicate-argument structures, and <a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)">bridging anaphora relations</a>. However, predicate-argument structures for nominal predicates and bridging anaphora relations have not been studied well, and their analyses have been still very difficult. Recent advances in neural networks, in particular, self training-based language models including <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> (Devlin et al., 2019), have significantly improved many natural language processing tasks, making it possible to dive into the study on analysis of cohesion in the whole text. In this study, we tackle an integrated analysis of cohesion in <a href="https://en.wikipedia.org/wiki/Japanese_literature">Japanese texts</a>. Our results significantly outperformed existing studies in each task, especially about 10 to 20 point improvement both for zero anaphora and coreference resolution. Furthermore, we also showed that <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> is different in nature from the other <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> and should be treated specially.</abstract>
      <url hash="f8b6087d">2020.coling-main.114</url>
      <doi>10.18653/v1/2020.coling-main.114</doi>
      <bibkey>ueda-etal-2020-bert</bibkey>
      <pwccode url="https://github.com/nobu-g/cohesion-analysis" additional="false">nobu-g/cohesion-analysis</pwccode>
    </paper>
    <paper id="115">
      <title>Schema Aware Semantic Reasoning for Interpreting Natural Language Queries in Enterprise Settings</title>
      <author><first>Jaydeep</first><last>Sen</last></author>
      <author><first>Tanaya</first><last>Babtiwale</last></author>
      <author><first>Kanishk</first><last>Saxena</last></author>
      <author><first>Yash</first><last>Butala</last></author>
      <author><first>Sumit</first><last>Bhatia</last></author>
      <author><first>Karthik</first><last>Sankaranarayanan</last></author>
      <pages>1334–1345</pages>
      <abstract>Natural Language Query interfaces allow the end-users to access the desired information without the need to know any specialized query language, <a href="https://en.wikipedia.org/wiki/Computer_data_storage">data storage</a>, or schema details. Even with the recent advances in NLP research space, the state-of-the-art QA systems fall short of understanding implicit intents of real-world Business Intelligence (BI) queries in enterprise systems, since <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">Natural Language Understanding</a> still remains an AI-hard problem. We posit that deploying ontology reasoning over domain semantics can help in achieving better <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a> for <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA systems</a>. In this paper, we specifically focus on building a Schema Aware Semantic Reasoning Framework that translates natural language interpretation as a sequence of solvable tasks by an ontology reasoner. We apply our framework on top of an ontology based, state-of-the-art natural language question-answering system ATHENA, and experiment with 4 benchmarks focused on BI queries. Our experimental numbers empirically show that the Schema Aware Semantic Reasoning indeed helps in achieving significantly better results for handling BI queries with an average accuracy improvement of ~30 %</abstract>
      <url hash="0ff2fefc">2020.coling-main.115</url>
      <doi>10.18653/v1/2020.coling-main.115</doi>
      <bibkey>sen-etal-2020-schema</bibkey>
    </paper>
    <paper id="117">
      <title>What Can We Learn from Noun Substitutions in Revision Histories?</title>
      <author><first>Talita</first><last>Anthonio</last></author>
      <author><first>Michael</first><last>Roth</last></author>
      <pages>1359–1370</pages>
      <abstract>In community-edited resources such as <a href="https://en.wikipedia.org/wiki/WikiHow">wikiHow</a>, sentences are subject to revisions on a daily basis. Recent work has shown that resulting improvements over time can be modelled computationally, assuming that each revision contributes to the improvement. We take a closer look at a subset of such revisions, for which we attempt to improve a <a href="https://en.wikipedia.org/wiki/Computational_model">computational model</a> and validate in how far the assumption that ‘revised means better’ actually holds. The subset of revisions considered here are noun substitutions, which often involve interesting semantic relations, including <a href="https://en.wikipedia.org/wiki/Synonym">synonymy</a>, <a href="https://en.wikipedia.org/wiki/Opposite_(semantics)">antonymy</a> and <a href="https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy">hypernymy</a>. Despite the high <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic relatedness</a>, we find that a <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised classifier</a> can distinguish the revised version of a sentence from an original version with an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> close to 70 %, when taking context into account. In a human annotation study, we observe that annotators identify the revised sentence as the ‘better version’ with similar performance. Our analysis reveals a fair agreement among annotators when a revision improves fluency. In contrast, noun substitutions that involve other lexical-semantic relationships are often perceived as being equally good or tend to cause disagreements. While these findings are also reflected in classification scores, a comparison of results shows that our model fails in cases where humans can resort to factual knowledge or intuitions about the required level of <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">specificity</a>.</abstract>
      <url hash="0cf41e25">2020.coling-main.117</url>
      <doi>10.18653/v1/2020.coling-main.117</doi>
      <bibkey>anthonio-roth-2020-learn</bibkey>
    </paper>
    <paper id="118">
      <title>Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity</title>
      <author><first>Anne</first><last>Lauscher</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Edoardo Maria</first><last>Ponti</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>1371–1383</pages>
      <abstract>Unsupervised pretraining models have been shown to facilitate a wide range of downstream NLP applications. These <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>, however, retain some of the limitations of traditional static word embeddings. In particular, they encode only the distributional knowledge available in raw text corpora, incorporated through language modeling objectives. In this work, we complement such distributional knowledge with external lexical knowledge, that is, we integrate the discrete knowledge on word-level semantic similarity into pretraining. To this end, we generalize the standard BERT model to a multi-task learning setting where we couple BERT’s masked language modeling and next sentence prediction objectives with an auxiliary task of binary word relation classification. Our experiments suggest that our Lexically Informed BERT (LIBERT), specialized for the word-level semantic similarity, yields better performance than the lexically blind vanilla BERT on several language understanding tasks. Concretely, LIBERT outperforms BERT in 9 out of 10 tasks of the GLUE benchmark and is on a par with BERT in the remaining one. Moreover, we show consistent gains on 3 benchmarks for lexical simplification, a task where knowledge about word-level semantic similarity is paramount, as well as large gains on lexical reasoning probes.</abstract>
      <url hash="9cf734fa">2020.coling-main.118</url>
      <doi>10.18653/v1/2020.coling-main.118</doi>
      <bibkey>lauscher-etal-2020-specializing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="125">
      <title>A Deep Generative Distance-Based Classifier for Out-of-Domain Detection with Mahalanobis Space</title>
      <author><first>Hong</first><last>Xu</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>Sihong</first><last>Liu</last></author>
      <author><first>Zijun</first><last>Liu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>1452–1460</pages>
      <abstract>Detecting out-of-domain (OOD) input intents is critical in the task-oriented dialog system. Different from most existing methods that rely heavily on manually labeled OOD samples, we focus on the unsupervised OOD detection scenario where there are no labeled OOD samples except for labeled in-domain data. In this paper, we propose a simple but strong generative distance-based classifier to detect OOD samples. We estimate the class-conditional distribution on feature spaces of DNNs via Gaussian discriminant analysis (GDA) to avoid over-confidence problems. And we use two distance functions, Euclidean and Mahalanobis distances, to measure the <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence score</a> of whether a test sample belongs to OOD. Experiments on four benchmark datasets show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> can consistently outperform the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="82084b9d">2020.coling-main.125</url>
      <doi>10.18653/v1/2020.coling-main.125</doi>
      <bibkey>xu-etal-2020-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="126">
      <title>Contrastive Zero-Shot Learning for Cross-Domain Slot Filling with Adversarial Attack</title>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <author><first>Cheng</first><last>Niu</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>1461–1467</pages>
      <abstract>Zero-shot slot filling has widely arisen to cope with data scarcity in target domains. However, previous approaches often ignore constraints between slot value representation and related slot description representation in the latent space and lack enough <a href="https://en.wikipedia.org/wiki/Robust_statistics">model robustness</a>. In this paper, we propose a Contrastive Zero-Shot Learning with Adversarial Attack (CZSL-Adv) method for the cross-domain slot filling. The contrastive loss aims to map slot value contextual representations to the corresponding slot description representations. And we introduce an adversarial attack training strategy to improve <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">model robustness</a>. Experimental results show that our model significantly outperforms state-of-the-art baselines under both zero-shot and few-shot settings.</abstract>
      <url hash="4a5bfa01">2020.coling-main.126</url>
      <doi>10.18653/v1/2020.coling-main.126</doi>
      <bibkey>he-etal-2020-contrastive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="128">
      <title>Contextual Argument Component Classification for Class Discussions</title>
      <author><first>Luca</first><last>Lugini</last></author>
      <author><first>Diane</first><last>Litman</last></author>
      <pages>1475–1480</pages>
      <abstract>Argument mining systems often consider <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a>, i.e. information outside of an argumentative discourse unit, when trained to accomplish tasks such as argument component identification, <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>, and relation extraction. However, prior work has not carefully analyzed the utility of different <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual properties</a> in context-aware models. In this work, we show how two different types of contextual information, local discourse context and speaker context, can be incorporated into a computational model for classifying argument components in multi-party classroom discussions. We find that both context types can improve performance, although the improvements are dependent on context size and position.</abstract>
      <url hash="3c83c686">2020.coling-main.128</url>
      <doi>10.18653/v1/2020.coling-main.128</doi>
      <bibkey>lugini-litman-2020-contextual</bibkey>
    </paper>
    <paper id="130">
      <title>Pre-trained Language Model Based Active Learning for Sentence Matching</title>
      <author><first>Guirong</first><last>Bai</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Zaiqing</first><last>Nie</last></author>
      <pages>1495–1504</pages>
      <abstract>Active learning is able to significantly reduce the annotation cost for <a href="https://en.wikipedia.org/wiki/Data-driven_learning">data-driven techniques</a>. However, previous active learning approaches for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> mainly depend on the entropy-based uncertainty criterion, and ignore the characteristics of <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>. In this paper, we propose a pre-trained language model based active learning approach for sentence matching. Differing from previous active learning, it can provide linguistic criteria from the pre-trained language model to measure instances and help select more effective instances for annotation. Experiments demonstrate our approach can achieve greater <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> with fewer labeled training instances.</abstract>
      <url hash="1d6ae405">2020.coling-main.130</url>
      <doi>10.18653/v1/2020.coling-main.130</doi>
      <bibkey>bai-etal-2020-pre</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="132">
      <title>Using a Penalty-based Loss Re-estimation Method to Improve Implicit Discourse Relation Classification</title>
      <author><first>Xiao</first><last>Li</last></author>
      <author><first>Yu</first><last>Hong</last></author>
      <author><first>Huibin</first><last>Ruan</last></author>
      <author><first>Zhen</first><last>Huang</last></author>
      <pages>1513–1518</pages>
      <abstract>We tackle implicit discourse relation classification, a task of automatically determining semantic relationships between arguments. The attention-worthy words in arguments are crucial clues for classifying the discourse relations. Attention mechanisms have been proven effective in highlighting the attention-worthy words during <a href="https://en.wikipedia.org/wiki/Encoding_(memory)">encoding</a>. However, our survey shows that some inessential words are unintentionally misjudged as the attention-worthy words and, therefore, assigned heavier attention weights than should be. We propose a penalty-based loss re-estimation method to regulate the attention learning process, integrating penalty coefficients into the computation of loss by means of overstability of attention weight distributions. We conduct experiments on the Penn Discourse TreeBank (PDTB) corpus. The test results show that our loss re-estimation method leads to substantial improvements for a variety of <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanisms</a>, and it obtains highly competitive performance compared to the state-of-the-art methods.</abstract>
      <url hash="f5f91966">2020.coling-main.132</url>
      <doi>10.18653/v1/2020.coling-main.132</doi>
      <bibkey>li-etal-2020-using</bibkey>
    </paper>
    <paper id="134">
      <title>Knowledge Graph Embedding with Atrous Convolution and Residual Learning</title>
      <author><first>Feiliang</first><last>Ren</last></author>
      <author><first>Juchen</first><last>Li</last></author>
      <author><first>Huihui</first><last>Zhang</last></author>
      <author><first>Shilei</first><last>Liu</last></author>
      <author><first>Bochao</first><last>Li</last></author>
      <author><first>Ruicheng</first><last>Ming</last></author>
      <author><first>Yujia</first><last>Bai</last></author>
      <pages>1532–1543</pages>
      <abstract>Knowledge graph embedding is an important task and <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> will benefit lots of downstream applications. Currently, deep neural networks based methods achieve state-of-the-art performance. However, most of these existing <a href="https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations">methods</a> are very complex and need much time for training and inference. To address this issue, we propose a simple but effective atrous convolution based knowledge graph embedding method. Compared with existing state-of-the-art methods, our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> has following main characteristics. First, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> effectively increases <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">feature interactions</a> by using atrous convolutions. Second, to address the original information forgotten issue and vanishing / exploding gradient issue, it uses the residual learning method. Third, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> has simpler structure but much higher parameter efficiency. We evaluate our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> on six benchmark datasets with different evaluation metrics. Extensive experiments show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is very effective. On these diverse datasets, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> achieves better results than the compared state-of-the-art methods on most of evaluation metrics. The source codes of our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> could be found at https://github.com/neukg/AcrE.</abstract>
      <url hash="18a739c2">2020.coling-main.134</url>
      <doi>10.18653/v1/2020.coling-main.134</doi>
      <bibkey>ren-etal-2020-knowledge</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k">FB15k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18">WN18</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18rr">WN18RR</pwcdataset>
    </paper>
    <paper id="138">
      <title>TPLinker : Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking<fixed-case>TPL</fixed-case>inker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking</title>
      <author><first>Yucheng</first><last>Wang</last></author>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Yueyang</first><last>Zhang</last></author>
      <author><first>Tingwen</first><last>Liu</last></author>
      <author><first>Hongsong</first><last>Zhu</last></author>
      <author><first>Limin</first><last>Sun</last></author>
      <pages>1572–1582</pages>
      <abstract>Extracting entities and relations from <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured text</a> has attracted increasing attention in recent years but remains challenging, due to the intrinsic difficulty in identifying overlapping relations with shared entities. Prior works show that joint learning can result in a noticeable performance gain. However, they usually involve sequential interrelated steps and suffer from the problem of <a href="https://en.wikipedia.org/wiki/Exposure_bias">exposure bias</a>. At training time, they predict with the ground truth conditions while at inference it has to make extraction from scratch. This discrepancy leads to error accumulation. To mitigate the issue, we propose in this paper a one-stage joint extraction model, namely, TPLinker, which is capable of discovering overlapping relations sharing one or both entities while being immune from the <a href="https://en.wikipedia.org/wiki/Exposure_bias">exposure bias</a>. TPLinker formulates joint extraction as a token pair linking problem and introduces a novel handshaking tagging scheme that aligns the boundary tokens of entity pairs under each relation type. Experiment results show that TPLinker performs significantly better on overlapping and multiple relation extraction, and achieves state-of-the-art performance on two public datasets.</abstract>
      <url hash="8a46bf86">2020.coling-main.138</url>
      <doi>10.18653/v1/2020.coling-main.138</doi>
      <bibkey>wang-etal-2020-tplinker</bibkey>
      <pwccode url="https://github.com/131250208/TPlinker-joint-extraction" additional="false">131250208/TPlinker-joint-extraction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nyt11-hrl">NYT11-HRL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="141">
      <title>Unsupervised Deep Language and Dialect Identification for Short Texts</title>
      <author><first>Koustava</first><last>Goswami</last></author>
      <author><first>Rajdeep</first><last>Sarkar</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Theodorus</first><last>Fransen</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <pages>1606–1617</pages>
      <abstract>Automatic Language Identification (LI) or Dialect Identification (DI) of short texts of closely related languages or dialects, is one of the primary steps in many natural language processing pipelines. Language identification is considered a solved task in many cases ; however, in the case of very closely related languages, or in an unsupervised scenario (where the languages are not known in advance), performance is still poor. In this paper, we propose the Unsupervised Deep Language and Dialect Identification (UDLDI) method, which can simultaneously learn sentence embeddings and cluster assignments from short texts. The UDLDI model understands the sentence constructions of languages by applying attention to character relations which helps to optimize the clustering of languages. We have performed our experiments on three short-text datasets for different <a href="https://en.wikipedia.org/wiki/Language_family">language families</a>, each consisting of closely related languages or dialects, with very minimal training sets. Our experimental evaluations on these datasets have shown significant improvement over state-of-the-art <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a> and our model has outperformed state-of-the-art LI and DI systems in <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised settings</a>.</abstract>
      <url hash="5a84e94b">2020.coling-main.141</url>
      <doi>10.18653/v1/2020.coling-main.141</doi>
      <bibkey>goswami-etal-2020-unsupervised</bibkey>
    </paper>
    <paper id="145">
      <title>Improving Long-Tail Relation Extraction with Collaborating Relation-Augmented Attention</title>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Tao</first><last>Shen</last></author>
      <author><first>Guodong</first><last>Long</last></author>
      <author><first>Jing</first><last>Jiang</last></author>
      <author><first>Tianyi</first><last>Zhou</last></author>
      <author><first>Chengqi</first><last>Zhang</last></author>
      <pages>1653–1664</pages>
      <abstract>Wrong labeling problem and long-tail relations are two main challenges caused by distant supervision in <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a>. Recent works alleviate the wrong labeling by selective attention via multi-instance learning, but can not well handle long-tail relations even if hierarchies of the relations are introduced to share knowledge. In this work, we propose a novel <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a>, Collaborating Relation-augmented Attention (CoRA), to handle both the wrong labeling and long-tail relations. Particularly, we first propose relation-augmented attention network as base model. It operates on sentence bag with a sentence-to-relation attention to minimize the effect of wrong labeling. Then, facilitated by the proposed base model, we introduce collaborating relation features shared among relations in the hierarchies to promote the relation-augmenting process and balance the training data for long-tail relations. Besides the main training objective to predict the relation of a sentence bag, an auxiliary objective is utilized to guide the relation-augmenting process for a more accurate bag-level representation. In the experiments on the popular benchmark dataset NYT, the proposed CoRA improves the prior state-of-the-art performance by a large margin in terms of <a href="https://en.wikipedia.org/wiki/Precision_(statistics)">Precision@N</a>, <a href="https://en.wikipedia.org/wiki/Analysis_of_covariance">AUC</a> and Hits@K. Further analyses verify its superior capability in handling long-tail relations in contrast to the competitors.</abstract>
      <url hash="f1bbc9b0">2020.coling-main.145</url>
      <doi>10.18653/v1/2020.coling-main.145</doi>
      <bibkey>li-etal-2020-improving</bibkey>
      <pwccode url="https://github.com/YangLi1221/CoRA" additional="true">YangLi1221/CoRA</pwccode>
    </paper>
    <paper id="146">
      <title>ToHRE : A Top-Down Classification Strategy with Hierarchical Bag Representation for Distantly Supervised Relation Extraction<fixed-case>T</fixed-case>o<fixed-case>HRE</fixed-case>: A Top-Down Classification Strategy with Hierarchical Bag Representation for Distantly Supervised Relation Extraction</title>
      <author><first>Erxin</first><last>Yu</last></author>
      <author><first>Wenjuan</first><last>Han</last></author>
      <author><first>Yuan</first><last>Tian</last></author>
      <author><first>Yi</first><last>Chang</last></author>
      <pages>1665–1676</pages>
      <abstract>Distantly Supervised Relation Extraction (DSRE) has proven to be effective to find relational facts from texts, but it still suffers from two main problems : the wrong labeling problem and the long-tail problem. Most of the existing approaches address these two problems through flat classification, which lacks hierarchical information of relations. To leverage the informative relation hierarchies, we formulate DSRE as a hierarchical classification task and propose a novel hierarchical classification framework, which extracts the relation in a top-down manner. Specifically, in our proposed framework, 1) we use a hierarchically-refined representation method to achieve hierarchy-specific representation ; 2) a top-down classification strategy is introduced instead of training a set of local classifiers. The experiments on NYT dataset demonstrate that our approach significantly outperforms other state-of-the-art approaches, especially for the long-tail problem.</abstract>
      <url hash="42a2027f">2020.coling-main.146</url>
      <doi>10.18653/v1/2020.coling-main.146</doi>
      <bibkey>yu-etal-2020-tohre</bibkey>
    </paper>
    <paper id="156">
      <title>Combining Event Semantics and Degree Semantics for Natural Language Inference</title>
      <author><first>Izumi</first><last>Haruta</last></author>
      <author><first>Koji</first><last>Mineshima</last></author>
      <author><first>Daisuke</first><last>Bekki</last></author>
      <pages>1758–1764</pages>
      <abstract>In <a href="https://en.wikipedia.org/wiki/Formal_semantics_(linguistics)">formal semantics</a>, there are two well-developed semantic frameworks : event semantics, which treats verbs and adverbial modifiers using the notion of event, and degree semantics, which analyzes adjectives and comparatives using the notion of degree. However, it is not obvious whether these <a href="https://en.wikipedia.org/wiki/Conceptual_framework">frameworks</a> can be combined to handle cases in which the phenomena in question are interacting with each other. Here, we study this issue by focusing on natural language inference (NLI). We implement a logic-based NLI system that combines event semantics and degree semantics and their interaction with lexical knowledge. We evaluate the <a href="https://en.wikipedia.org/wiki/System">system</a> on various NLI datasets containing linguistically challenging problems. The results show that the system achieves high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracies</a> on these <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> in comparison with previous <a href="https://en.wikipedia.org/wiki/Logic_programming">logic-based systems</a> and <a href="https://en.wikipedia.org/wiki/Deep_learning">deep-learning-based systems</a>. This suggests that the two semantic frameworks can be combined consistently to handle various combinations of linguistic phenomena without compromising the advantage of either <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a>.</abstract>
      <url hash="c2b8ba8d">2020.coling-main.156</url>
      <doi>10.18653/v1/2020.coling-main.156</doi>
      <bibkey>haruta-etal-2020-combining</bibkey>
      <pwccode url="https://github.com/izumi-h/ccgcomp" additional="false">izumi-h/ccgcomp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/med">MED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
    </paper>
    <paper id="163">
      <title>Detecting de minimis Code-Switching in Historical German Books<fixed-case>G</fixed-case>erman Books</title>
      <author><first>Shijia</first><last>Liu</last></author>
      <author><first>David</first><last>Smith</last></author>
      <pages>1808–1814</pages>
      <abstract>Code-switching has long interested linguists, with computational work in particular focusing on speech and social media data (Sitaram et al., 2019). This paper contrasts these informal instances of <a href="https://en.wikipedia.org/wiki/Code-switching">code-switching</a> to its appearance in more formal registers, by examining the mixture of languages in the Deutsches Textarchiv (DTA), a corpus of 1406 primarily German books from the 17th to 19th centuries. We automatically annotate and manually inspect spans of six embedded languages (Latin, <a href="https://en.wikipedia.org/wiki/French_language">French</a>, <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Italian_language">Italian</a>, <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>, and Greek) in the corpus. We quantitatively analyze the differences between code-switching patterns in these books and those in more typically studied speech and social media corpora. Furthermore, we address the practical task of predicting code-switching from <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> of the matrix language alone in the DTA corpus. Such <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> can help reduce errors when <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">optical character recognition</a> or <a href="https://en.wikipedia.org/wiki/Speech_synthesis">speech transcription</a> is applied to a large corpus with rare embedded languages.</abstract>
      <url hash="a484b1d6">2020.coling-main.163</url>
      <doi>10.18653/v1/2020.coling-main.163</doi>
      <bibkey>liu-smith-2020-detecting</bibkey>
    </paper>
    <paper id="165">
      <title>Connecting the Dots Between <a href="https://en.wikipedia.org/wiki/Fact-checking">Fact Verification</a> and Fake News Detection</title>
      <author><first>Qifei</first><last>Li</last></author>
      <author><first>Wangchunshu</first><last>Zhou</last></author>
      <pages>1820–1825</pages>
      <abstract>Fact verification models have enjoyed a fast advancement in the last two years with the development of pre-trained language models like <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> and the release of large scale datasets such as <a href="https://en.wikipedia.org/wiki/FEVER">FEVER</a>. However, the challenging problem of fake news detection has not benefited from the improvement of fact verification models, which is closely related to fake news detection. In this paper, we propose a simple yet effective approach to connect the dots between <a href="https://en.wikipedia.org/wiki/Fact-checking">fact verification</a> and fake news detection. Our approach first employs a text summarization model pre-trained on news corpora to summarize the long news article into a short claim. Then we use a fact verification model pre-trained on the FEVER dataset to detect whether the input <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news article</a> is real or fake. Our approach makes use of the recent success of fact verification models and enables zero-shot fake news detection, alleviating the need of large scale training data to train fake news detection models. Experimental results on FakenewsNet, a benchmark dataset for fake news detection, demonstrate the effectiveness of our proposed approach.</abstract>
      <url hash="cbb3d9a5">2020.coling-main.165</url>
      <doi>10.18653/v1/2020.coling-main.165</doi>
      <bibkey>li-zhou-2020-connecting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fakenewsnet">FakeNewsNet</pwcdataset>
    </paper>
    <paper id="167">
      <title>Reasoning Step-by-Step : Temporal Sentence Localization in Videos via Deep Rectification-Modulation Network</title>
      <author><first>Daizong</first><last>Liu</last></author>
      <author><first>Xiaoye</first><last>Qu</last></author>
      <author><first>Jianfeng</first><last>Dong</last></author>
      <author><first>Pan</first><last>Zhou</last></author>
      <pages>1841–1851</pages>
      <abstract>Temporal sentence localization in videos aims to ground the best matched segment in an untrimmed video according to a given sentence query. Previous works in this field mainly rely on <a href="https://en.wikipedia.org/wiki/Attentional_control">attentional frameworks</a> to align the temporal boundaries by a soft selection. Although they focus on the visual content relevant to the query, these single-step attention are insufficient to model complex video contents and restrict the higher-level reasoning demand for this task. In this paper, we propose a novel deep rectification-modulation network (RMN), transforming this task into a multi-step reasoning process by repeating rectification and <a href="https://en.wikipedia.org/wiki/Modulation">modulation</a>. In each rectification-modulation layer, unlike existing methods directly conducting the cross-modal interaction, we first devise a rectification module to correct implicit attention misalignment which focuses on the wrong position during the cross-interaction process. Then, a modulation module is developed to capture the frame-to-frame relation with the help of sentence information for better correlating and composing the video contents over time. With multiple such layers cascaded in depth, our RMN progressively refines video and query interactions, thus enabling a further precise localization. Experimental evaluations on three public datasets show that the proposed <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> achieves state-of-the-art performance. Extensive ablation studies are carried out for the comprehensive analysis of the proposed method.</abstract>
      <url hash="028b7c90">2020.coling-main.167</url>
      <doi>10.18653/v1/2020.coling-main.167</doi>
      <bibkey>liu-etal-2020-reasoning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/charades">Charades</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades-sta">Charades-STA</pwcdataset>
    </paper>
    <paper id="174">
      <title>Language-Driven Region Pointer Advancement for Controllable Image Captioning</title>
      <author><first>Annika</first><last>Lindh</last></author>
      <author><first>Robert</first><last>Ross</last></author>
      <author><first>John</first><last>Kelleher</last></author>
      <pages>1922–1935</pages>
      <abstract>Controllable Image Captioning is a recent sub-field in the multi-modal task of Image Captioning wherein constraints are placed on which regions in an image should be described in the generated natural language caption. This puts a stronger focus on producing more detailed descriptions, and opens the door for more end-user control over results. A vital component of the Controllable Image Captioning architecture is the mechanism that decides the timing of attending to each region through the advancement of a region pointer. In this paper, we propose a novel method for predicting the timing of region pointer advancement by treating the advancement step as a natural part of the language structure via a NEXT-token, motivated by a strong correlation to the sentence structure in the training data. We find that our timing agrees with the ground-truth timing in the Flickr30k Entities test data with a <a href="https://en.wikipedia.org/wiki/Precision_(statistics)">precision</a> of 86.55 % and a recall of 97.92 %. Our model implementing this technique improves the state-of-the-art on standard captioning metrics while additionally demonstrating a considerably larger effective vocabulary size.</abstract>
      <url hash="0bbea776">2020.coling-main.174</url>
      <doi>10.18653/v1/2020.coling-main.174</doi>
      <bibkey>lindh-etal-2020-language</bibkey>
      <pwccode url="https://github.com/AnnikaLindh/Controllable_Region_Pointer_Advancement" additional="false">AnnikaLindh/Controllable_Region_Pointer_Advancement</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k-entities">Flickr30K Entities</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="182">
      <title>An Enhanced Knowledge Injection Model for Commonsense Generation</title>
      <author><first>Zhihao</first><last>Fan</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Siyuan</first><last>Wang</last></author>
      <author><first>Yameng</first><last>Huang</last></author>
      <author><first>Jian</first><last>Jiao</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Ruofei</first><last>Zhang</last></author>
      <pages>2014–2025</pages>
      <abstract>Commonsense generation aims at generating plausible everyday scenario description based on a set of provided concepts. Digging the relationship of concepts from scratch is non-trivial, therefore, we retrieve prototypes from external knowledge to assist the understanding of the scenario for better description generation. We integrate two additional modules into the pretrained encoder-decoder model for prototype modeling to enhance the knowledge injection procedure. We conduct experiment on CommonGen benchmark, experimental results show that our method significantly improves the performance on all the metrics.</abstract>
      <url hash="64c06459">2020.coling-main.182</url>
      <doi>10.18653/v1/2020.coling-main.182</doi>
      <bibkey>fan-etal-2020-enhanced</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/commongen">CommonGen</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
    </paper>
    <paper id="191">
      <title>How Positive Are You : Text Style Transfer using Adaptive Style Embedding</title>
      <author><first>Heejin</first><last>Kim</last></author>
      <author><first>Kyung-Ah</first><last>Sohn</last></author>
      <pages>2115–2125</pages>
      <abstract>The prevalent approach for unsupervised text style transfer is disentanglement between content and style. However, it is difficult to completely separate <a href="https://en.wikipedia.org/wiki/Style_(manner_of_address)">style information</a> from the content. Other approaches allow the latent text representation to contain style and the target style to affect the generated output more than the latent representation does. In both <a href="https://en.wikipedia.org/wiki/Composition_(visual_arts)">approaches</a>, however, it is impossible to adjust the strength of the style in the generated output. Moreover, those previous approaches typically perform both the sentence reconstruction and style control tasks in a single <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>, which complicates the overall architecture. In this paper, we address these issues by separating the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> into a sentence reconstruction module and a style module. We use the Transformer-based autoencoder model for sentence reconstruction and the adaptive style embedding is learned directly in the style module. Because of this separation, each <a href="https://en.wikipedia.org/wiki/Modular_programming">module</a> can better focus on its own <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>. Moreover, we can vary the style strength of the generated sentence by changing the style of the embedding expression. Therefore, our approach not only controls the strength of the style, but also simplifies the model architecture. Experimental results show that our approach achieves better style transfer performance and <a href="https://en.wikipedia.org/wiki/Preservation_(library_and_archival_science)">content preservation</a> than previous approaches.</abstract>
      <url hash="c03067ea">2020.coling-main.191</url>
      <doi>10.18653/v1/2020.coling-main.191</doi>
      <bibkey>kim-sohn-2020-positive</bibkey>
      <pwccode url="https://github.com/kinggodhj/how-positive-are-you-text-style-transfer-using-adaptive-style-embedding" additional="false">kinggodhj/how-positive-are-you-text-style-transfer-using-adaptive-style-embedding</pwccode>
    </paper>
    <paper id="195">
      <title>Grammatical error detection in transcriptions of spoken English<fixed-case>E</fixed-case>nglish</title>
      <author><first>Andrew</first><last>Caines</last></author>
      <author><first>Christian</first><last>Bentz</last></author>
      <author><first>Kate</first><last>Knill</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>2144–2162</pages>
      <abstract>We describe the collection of <a href="https://en.wikipedia.org/wiki/Transcription_(linguistics)">transcription corrections</a> and grammatical error annotations for the CrowdED Corpus of spoken English monologues on business topics. The corpus recordings were crowdsourced from native speakers of English and learners of <a href="https://en.wikipedia.org/wiki/English_language">English</a> with <a href="https://en.wikipedia.org/wiki/German_language">German</a> as their first language. The new transcriptions and annotations are obtained from different crowdworkers : we analyse the 1108 new crowdworker submissions and propose that they can be used for automatic transcription post-editing and grammatical error correction for speech. To further explore the data we train grammatical error detection models with various configurations including pre-trained and contextual word representations as input, additional <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> and auxiliary objectives, and extra <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data</a> from written error-annotated corpora. We find that a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> concatenating pre-trained and contextual word representations as input performs best, and that additional information does not lead to further performance gains.</abstract>
      <url hash="4cdfa216">2020.coling-main.195</url>
      <doi>10.18653/v1/2020.coling-main.195</doi>
      <bibkey>caines-etal-2020-grammatical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/english-web-treebank">English Web Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
    </paper>
    <paper id="197">
      <title>Style versus Content : A distinction without a (learnable) difference?</title>
      <author><first>Somayeh</first><last>Jafaritazehjani</last></author>
      <author><first>Gwénolé</first><last>Lecorvé</last></author>
      <author><first>Damien</first><last>Lolive</last></author>
      <author><first>John</first><last>Kelleher</last></author>
      <pages>2169–2180</pages>
      <abstract>Textual style transfer involves modifying the style of a text while preserving its content. This assumes that it is possible to separate <a href="https://en.wikipedia.org/wiki/Style_(manner_of_address)">style</a> from content. This paper investigates whether this separation is possible. We use sentiment transfer as our case study for style transfer analysis. Our experimental methodology frames style transfer as a multi-objective problem, balancing style shift with content preservation and <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a>. Due to the lack of parallel data for style transfer we employ a variety of adversarial encoder-decoder networks in our experiments. Also, we use of a probing methodology to analyse how these <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> encode style-related features in their latent spaces. The results of our experiments which are further confirmed by a human evaluation reveal the inherent trade-off between the multiple style transfer objectives which indicates that style can not be usefully separated from content within these style-transfer systems.</abstract>
      <url hash="91707b51">2020.coling-main.197</url>
      <doi>10.18653/v1/2020.coling-main.197</doi>
      <bibkey>jafaritazehjani-etal-2020-style</bibkey>
    </paper>
    <paper id="199">
      <title>Heterogeneous Recycle Generation for Chinese Grammatical Error Correction<fixed-case>C</fixed-case>hinese Grammatical Error Correction</title>
      <author><first>Charles</first><last>Hinson</last></author>
      <author><first>Hen-Hsen</first><last>Huang</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <pages>2191–2201</pages>
      <abstract>Most recent works in the field of grammatical error correction (GEC) rely on neural machine translation-based models. Although these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> boast impressive performance, they require a massive amount of data to properly train. Furthermore, NMT-based systems treat GEC purely as a translation task and overlook the editing aspect of it. In this work we propose a heterogeneous approach to Chinese GEC, composed of a NMT-based model, a sequence editing model, and a <a href="https://en.wikipedia.org/wiki/Spell_checker">spell checker</a>. Our methodology not only achieves a new state-of-the-art performance for Chinese GEC, but also does so without relying on <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> or GEC-specific architecture changes. We further experiment with all possible configurations of our <a href="https://en.wikipedia.org/wiki/System">system</a> with respect to model composition order and number of rounds of correction. A detailed analysis of each <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and their contributions to the correction process is performed by adapting the ERRANT scorer to be able to score Chinese sentences.</abstract>
      <url hash="c36b8935">2020.coling-main.199</url>
      <doi>10.18653/v1/2020.coling-main.199</doi>
      <bibkey>hinson-etal-2020-heterogeneous</bibkey>
    </paper>
    <paper id="203">
      <title>Formality Style Transfer with Shared Latent Space</title>
      <author><first>Yunli</first><last>Wang</last></author>
      <author><first>Yu</first><last>Wu</last></author>
      <author><first>Lili</first><last>Mou</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <author><first>WenHan</first><last>Chao</last></author>
      <pages>2236–2249</pages>
      <abstract>Conventional approaches for formality style transfer borrow models from <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>, which typically requires massive parallel data for training. However, the dataset for formality style transfer is considerably smaller than translation corpora. Moreover, we observe that informal and formal sentences closely resemble each other, which is different from the translation task where two languages have different vocabularies and <a href="https://en.wikipedia.org/wiki/Grammar">grammars</a>. In this paper, we present a new approach, Sequence-to-Sequence with Shared Latent Space (S2S-SLS), for formality style transfer, where we propose two auxiliary losses and adopt joint training of bi-directional transfer and auto-encoding. Experimental results show that S2S-SLS (with either RNN or Transformer architectures) consistently outperforms baselines in various settings, especially when we have limited data.</abstract>
      <url hash="be20c7c6">2020.coling-main.203</url>
      <doi>10.18653/v1/2020.coling-main.203</doi>
      <bibkey>wang-etal-2020-formality</bibkey>
      <pwccode url="https://github.com/jimth001/formality_style_transfer_with_shared_latent_space" additional="false">jimth001/formality_style_transfer_with_shared_latent_space</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="204">
      <title>Keep it Consistent : Topic-Aware Storytelling from an Image Stream via Iterative Multi-agent Communication</title>
      <author><first>Ruize</first><last>Wang</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Ying</first><last>Cheng</last></author>
      <author><first>Piji</first><last>Li</last></author>
      <author><first>Haijun</first><last>Shan</last></author>
      <author><first>Ji</first><last>Zhang</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>2250–2260</pages>
      <abstract>Visual storytelling aims to generate a narrative paragraph from a sequence of <a href="https://en.wikipedia.org/wiki/Image">images</a> automatically. Existing approaches construct text description independently for each image and roughly concatenate them as a story, which leads to the problem of generating semantically incoherent content. In this paper, we propose a new way for <a href="https://en.wikipedia.org/wiki/Visual_storytelling">visual storytelling</a> by introducing a topic description task to detect the global semantic context of an image stream. A story is then constructed with the guidance of the topic description. In order to combine the two generation tasks, we propose a multi-agent communication framework that regards the topic description generator and the <a href="https://en.wikipedia.org/wiki/Story_generator">story generator</a> as two agents and learn them simultaneously via iterative updating mechanism. We validate our approach on VIST dataset, where quantitative results, ablations, and human evaluation demonstrate our method’s good ability in generating stories with higher quality compared to state-of-the-art methods.</abstract>
      <url hash="b354b0e0">2020.coling-main.204</url>
      <doi>10.18653/v1/2020.coling-main.204</doi>
      <bibkey>wang-etal-2020-keep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/vist">VIST</pwcdataset>
    </paper>
    <paper id="205">
      <title>Referring to what you know and do not know : Making Referring Expression Generation Models Generalize To Unseen Entities</title>
      <author><first>Rossana</first><last>Cunha</last></author>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Adriana</first><last>Pagano</last></author>
      <author><first>Fabio</first><last>Alves</last></author>
      <pages>2261–2272</pages>
      <abstract>Data-to-text Natural Language Generation (NLG) is the computational process of generating <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> in the form of text or voice from non-linguistic data. A core micro-planning task within NLG is referring expression generation (REG), which aims to automatically generate <a href="https://en.wikipedia.org/wiki/Noun_phrase">noun phrases</a> to refer to entities mentioned as discourse unfolds. A limitation of novel REG models is not being able to generate referring expressions to entities not encountered during the training process. To solve this problem, we propose two extensions to NeuralREG, a state-of-the-art encoder-decoder REG model. The first is a copy mechanism, whereas the second consists of representing the gender and type of the referent as inputs to the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>. Drawing on the results of automatic and human evaluation as well as an ablation study using the WebNLG corpus, we contend that our proposal contributes to the generation of more meaningful referring expressions to unseen entities than the original system and related work. Code and all produced data are publicly available.</abstract>
      <url hash="d178fdc7">2020.coling-main.205</url>
      <doi>10.18653/v1/2020.coling-main.205</doi>
      <bibkey>cunha-etal-2020-referring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="208">
      <title>Automatic Detection of Machine Generated Text : A Critical Survey</title>
      <author><first>Ganesh</first><last>Jawahar</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>Laks</first><last>Lakshmanan, V.S.</last></author>
      <pages>2296–2309</pages>
      <abstract>Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by <a href="https://en.wikipedia.org/wiki/Time-division_multiple_access">TGM</a> from human written text play a vital role in mitigating such misuse of <a href="https://en.wikipedia.org/wiki/Time-division_multiple_access">TGMs</a>. Recently, there has been a flurry of works from both <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP)</a> and <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning (ML) communities</a> to build accurate detectors for <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.</abstract>
      <url hash="c42f6e6c">2020.coling-main.208</url>
      <doi>10.18653/v1/2020.coling-main.208</doi>
      <bibkey>jawahar-etal-2020-automatic</bibkey>
      <pwccode url="https://github.com/UBC-NLP/coling2020_machine_generated_text" additional="false">UBC-NLP/coling2020_machine_generated_text</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="213">
      <title>Learning with Contrastive Examples for Data-to-Text Generation</title>
      <author><first>Yui</first><last>Uehara</last></author>
      <author><first>Tatsuya</first><last>Ishigaki</last></author>
      <author><first>Kasumi</first><last>Aoki</last></author>
      <author><first>Hiroshi</first><last>Noji</last></author>
      <author><first>Keiichi</first><last>Goshima</last></author>
      <author><first>Ichiro</first><last>Kobayashi</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>2352–2362</pages>
      <abstract>Existing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for data-to-text tasks generate fluent but sometimes incorrect sentences e.g., Nikkei gains is generated when Nikkei drops is expected. We investigate <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a> trained on contrastive examples i.e., incorrect sentences or terms, in addition to correct ones to reduce such errors. We first create <a href="https://en.wikipedia.org/wiki/Rule_of_inference">rules</a> to produce contrastive examples from correct ones by replacing frequent crucial terms such as gain or drop. We then use <a href="https://en.wikipedia.org/wiki/Machine_learning">learning methods</a> with several <a href="https://en.wikipedia.org/wiki/Loss_function">losses</a> that exploit contrastive examples. Experiments on the market comment generation task show that 1) exploiting contrastive examples improves the capability of generating sentences with better lexical choice, without degrading the fluency, 2) the choice of the <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> is an important factor because the performances on different metrics depend on the types of loss functions, and 3) the use of the examples produced by some specific rules further improves performance. Human evaluation also supports the effectiveness of using <a href="https://en.wikipedia.org/wiki/Contrast_(linguistics)">contrastive examples</a>.</abstract>
      <url hash="fa2b8f55">2020.coling-main.213</url>
      <doi>10.18653/v1/2020.coling-main.213</doi>
      <bibkey>uehara-etal-2020-learning</bibkey>
      <pwccode url="https://github.com/aistairc/contrastive_data2text" additional="false">aistairc/contrastive_data2text</pwccode>
    </paper>
    <paper id="214">
      <title>MedWriter : Knowledge-Aware Medical Text Generation<fixed-case>M</fixed-case>ed<fixed-case>W</fixed-case>riter: Knowledge-Aware Medical Text Generation</title>
      <author><first>Youcheng</first><last>Pan</last></author>
      <author><first>Qingcai</first><last>Chen</last></author>
      <author><first>Weihua</first><last>Peng</last></author>
      <author><first>Xiaolong</first><last>Wang</last></author>
      <author><first>Baotian</first><last>Hu</last></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Wenxiu</first><last>Zhou</last></author>
      <pages>2363–2368</pages>
      <abstract>To exploit the <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> to guarantee the correctness of generated text has been a hot topic in recent years, especially for high professional domains such as <a href="https://en.wikipedia.org/wiki/Medicine">medical</a>. However, most of recent works only consider the information of unstructured text rather than structured information of the <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a>. In this paper, we focus on the medical topic-to-text generation task and adapt a knowledge-aware text generation model to the medical domain, named MedWriter, which not only introduces the specific knowledge from the external MKG but also is capable of learning graph-level representation. We conduct experiments on a medical literature dataset collected from <a href="https://en.wikipedia.org/wiki/Medical_journal">medical journals</a>, each of which has a set of topic words, an abstract of medical literature and a corresponding <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> from CMeKG. Experimental results demonstrate incorporating <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> into generation model can improve the quality of the generated text and has robust superiority over the competitor methods.</abstract>
      <url hash="4c8a118e">2020.coling-main.214</url>
      <doi>10.18653/v1/2020.coling-main.214</doi>
      <bibkey>pan-etal-2020-medwriter</bibkey>
    </paper>
    <paper id="226">
      <title>Context Dependent Semantic Parsing : A Survey</title>
      <author><first>Zhuang</first><last>Li</last></author>
      <author><first>Lizhen</first><last>Qu</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>2509–2521</pages>
      <abstract>Semantic parsing is the task of <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">translating natural language utterances</a> into <a href="https://en.wikipedia.org/wiki/Machine-readable_data">machine-readable meaning representations</a>. Currently, most semantic parsing methods are not able to utilize the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> (e.g. dialogue and comments history), which has a great potential to boost the semantic parsing systems. To address this issue, context dependent semantic parsing has recently drawn a lot of attention. In this survey, we investigate progress on the methods for the context dependent semantic parsing, together with the current datasets and tasks. We then point out open problems and challenges for future research in this area.</abstract>
      <url hash="3acaa8c9">2020.coling-main.226</url>
      <doi>10.18653/v1/2020.coling-main.226</doi>
      <bibkey>li-etal-2020-context</bibkey>
      <pwccode url="https://github.com/zhuang-li/Contextual-Semantic-Parsing-Paper-List" additional="false">zhuang-li/Contextual-Semantic-Parsing-Paper-List</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/csqa">CSQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sparc">SParC</pwcdataset>
    </paper>
    <paper id="227">
      <title>A Survey of Unsupervised Dependency Parsing</title>
      <author><first>Wenjuan</first><last>Han</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>2522–2533</pages>
      <abstract>Syntactic dependency parsing is an important task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. Unsupervised dependency parsing aims to learn a <a href="https://en.wikipedia.org/wiki/Dependency_grammar">dependency parser</a> from sentences that have no annotation of their correct parse trees. Despite its difficulty, unsupervised parsing is an interesting research direction because of its capability of utilizing almost unlimited unannotated text data. It also serves as the basis for other research in low-resource parsing. In this paper, we survey existing <a href="https://en.wikipedia.org/wiki/Parsing">approaches</a> to unsupervised dependency parsing, identify two major classes of <a href="https://en.wikipedia.org/wiki/Parsing">approaches</a>, and discuss recent trends. We hope that our survey can provide insights for researchers and facilitate future research on this topic.</abstract>
      <url hash="b3db2891">2020.coling-main.227</url>
      <doi>10.18653/v1/2020.coling-main.227</doi>
      <bibkey>han-etal-2020-survey</bibkey>
    </paper>
    <paper id="228">
      <title>Exploring Question-Specific Rewards for Generating Deep Questions</title>
      <author><first>Yuxi</first><last>Xie</last></author>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Dongzhe</first><last>Wang</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <pages>2534–2546</pages>
      <abstract>Recent question generation (QG) approaches often utilize the sequence-to-sequence framework (Seq2Seq) to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> to improve question quality. We design three different rewards that target to improve the <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a>, <a href="https://en.wikipedia.org/wiki/Relevance">relevance</a>, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward. We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the <a href="https://en.wikipedia.org/wiki/Motivational_salience">rewards</a> that correlate well with <a href="https://en.wikipedia.org/wiki/Judgement">human judgement</a> (e.g., relevance) lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, resulting in poorer question quality. The code is publicly available at https://github.com/YuxiXie/RL-for-Question-Generation.</abstract>
      <url hash="6d66e1d2">2020.coling-main.228</url>
      <doi>10.18653/v1/2020.coling-main.228</doi>
      <bibkey>xie-etal-2020-exploring</bibkey>
      <pwccode url="https://github.com/YuxiXie/RL-for-Question-Generation" additional="false">YuxiXie/RL-for-Question-Generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
    <paper id="229">
      <title>CHIME : Cross-passage Hierarchical Memory Network for Generative Review Question Answering<fixed-case>CHIME</fixed-case>: Cross-passage Hierarchical Memory Network for Generative Review Question Answering</title>
      <author><first>Junru</first><last>Lu</last></author>
      <author><first>Gabriele</first><last>Pergola</last></author>
      <author><first>Lin</first><last>Gui</last></author>
      <author><first>Binyang</first><last>Li</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>2547–2560</pages>
      <abstract>We introduce <a href="https://en.wikipedia.org/wiki/CHIME">CHIME</a>, a cross-passage hierarchical memory network for question answering (QA) via text generation. It extends XLNet introducing an auxiliary memory module consisting of two components : the context memory collecting cross-passage evidences, and the answer memory working as a buffer continually refining the generated answers. Empirically, we show the efficacy of the proposed architecture in the multi-passage generative QA, outperforming the state-of-the-art baselines with better syntactically well-formed answers and increased precision in addressing the questions of the AmazonQA review dataset. An additional qualitative analysis revealed the <a href="https://en.wikipedia.org/wiki/Interpretability">interpretability</a> introduced by the <a href="https://en.wikipedia.org/wiki/Memory_module">memory module</a>.</abstract>
      <url hash="4dad5176">2020.coling-main.229</url>
      <doi>10.18653/v1/2020.coling-main.229</doi>
      <bibkey>lu-etal-2020-chime</bibkey>
      <pwccode url="https://github.com/LuJunru/CHIME" additional="false">LuJunru/CHIME</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/amazonqa">AmazonQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="235">
      <title>Bi-directional CognitiveThinking Network for Machine Reading Comprehension<fixed-case>C</fixed-case>ognitive<fixed-case>T</fixed-case>hinking Network for Machine Reading Comprehension</title>
      <author><first>Wei</first><last>Peng</last></author>
      <author><first>Yue</first><last>Hu</last></author>
      <author><first>Luxi</first><last>Xing</last></author>
      <author><first>Yuqiang</first><last>Xie</last></author>
      <author><first>Jing</first><last>Yu</last></author>
      <author><first>Yajing</first><last>Sun</last></author>
      <author><first>Xiangpeng</first><last>Wei</last></author>
      <pages>2613–2623</pages>
      <abstract>We propose a novel Bi-directional Cognitive Knowledge Framework (BCKF) for <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> from the perspective of complementary learning systems theory. It aims to simulate two ways of thinking in the brain to answer questions, including reverse thinking and <a href="https://en.wikipedia.org/wiki/Inertial_frame_of_reference">inertial thinking</a>. To validate the effectiveness of our framework, we design a corresponding Bi-directional Cognitive Thinking Network (BCTN) to encode the passage and generate a question (answer) given an answer (question) and decouple the bi-directional knowledge. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> has the ability to reverse reasoning questions which can assist <a href="https://en.wikipedia.org/wiki/Inertial_frame_of_reference">inertial thinking</a> to generate more accurate answers. Competitive improvement is observed in DuReader dataset, confirming our hypothesis that bi-directional knowledge helps the QA task. The novel <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> shows an interesting perspective on machine reading comprehension and <a href="https://en.wikipedia.org/wiki/Cognitive_science">cognitive science</a>.</abstract>
      <url hash="5cc5df79">2020.coling-main.235</url>
      <doi>10.18653/v1/2020.coling-main.235</doi>
      <bibkey>peng-etal-2020-bi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dureader">DuReader</pwcdataset>
    </paper>
    <paper id="238">
      <title>Molweni : A Challenge Multiparty Dialogues-based Machine Reading Comprehension Dataset with Discourse Structure</title>
      <author><first>Jiaqi</first><last>Li</last></author>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>Zihao</first><last>Zheng</last></author>
      <author><first>Zekun</first><last>Wang</last></author>
      <author><first>Wenqiang</first><last>Lei</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>2642–2652</pages>
      <abstract>Research into the area of multiparty dialog has grown considerably over recent years. We present the Molweni dataset, a machine reading comprehension (MRC) dataset with discourse structure built over multiparty dialog. Molweni’s source samples from the Ubuntu Chat Corpus, including 10,000 dialogs comprising 88,303 utterances. We annotate 30,066 questions on this <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>, including both answerable and unanswerable questions. Molweni also uniquely contributes discourse dependency annotations in a modified Segmented Discourse Representation Theory (SDRT ; Asher et al., 2016) style for all of its multiparty dialogs, contributing large-scale (78,245 annotated discourse relations) data to bear on the task of multiparty dialog discourse parsing. Our experiments show that Molweni is a challenging dataset for current MRC models : BERT-wwm, a current, strong SQuAD 2.0 performer, achieves only 67.7 % F1 on Molweni’s questions, a 20+% significant drop as compared against its SQuAD 2.0 performance.</abstract>
      <url hash="3a17e3f7">2020.coling-main.238</url>
      <doi>10.18653/v1/2020.coling-main.238</doi>
      <bibkey>li-etal-2020-molweni</bibkey>
      <pwccode url="https://github.com/HIT-SCIR/Molweni" additional="false">HIT-SCIR/Molweni</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/molweni">Molweni</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="247">
      <title>Conversational Machine Comprehension : a Literature Review</title>
      <author><first>Somil</first><last>Gupta</last></author>
      <author><first>Bhanu Pratap Singh</first><last>Rawat</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>2739–2753</pages>
      <abstract>Conversational Machine Comprehension (CMC), a research track in conversational AI, expects the machine to understand an open-domain natural language text and thereafter engage in a multi-turn conversation to answer questions related to the text. While most of the research in Machine Reading Comprehension (MRC) revolves around single-turn question answering (QA), multi-turn CMC has recently gained prominence, thanks to the advancement in <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a> via neural language models such as BERT and the introduction of large-scale conversational datasets such as CoQA and QuAC. The rise in interest has, however, led to a flurry of concurrent publications, each with a different yet structurally similar modeling approach and an inconsistent view of the surrounding literature. With the volume of model submissions to conversational datasets increasing every year, there exists a need to consolidate the scattered knowledge in this domain to streamline future research. This literature review attempts at providing a holistic overview of CMC with an emphasis on the common trends across recently published models, specifically in their approach to tackling conversational history. The review synthesizes a generic framework for <a href="https://en.wikipedia.org/wiki/Computer-mediated_communication">CMC models</a> while highlighting the differences in recent approaches and intends to serve as a compendium of <a href="https://en.wikipedia.org/wiki/Computer-mediated_communication">CMC</a> for future researchers.</abstract>
      <url hash="93b02c89">2020.coling-main.247</url>
      <doi>10.18653/v1/2020.coling-main.247</doi>
      <bibkey>gupta-etal-2020-conversational</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
    </paper>
    <paper id="249">
      <title>Reinforced Multi-task Approach for Multi-hop Question Generation</title>
      <author><first>Deepak</first><last>Gupta</last></author>
      <author><first>Hardik</first><last>Chauhan</last></author>
      <author><first>Ravi Tej</first><last>Akella</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>2760–2775</pages>
      <abstract>Question generation (QG) attempts to solve the inverse of question answering (QA) problem by generating a natural language question given a document and an answer. While sequence to sequence neural models surpass rule-based systems for QG, they are limited in their capacity to focus on more than one supporting fact. For QG, we often require multiple supporting facts to generate high-quality questions. Inspired by recent works on multi-hop reasoning in <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA</a>, we take up Multi-hop question generation, which aims at generating relevant questions based on supporting facts in the context. We employ <a href="https://en.wikipedia.org/wiki/Multitask_learning">multitask learning</a> with the auxiliary task of answer-aware supporting fact prediction to guide the question generator. In addition, we also proposed a question-aware reward function in a Reinforcement Learning (RL) framework to maximize the utilization of the supporting facts. We demonstrate the effectiveness of our approach through experiments on the multi-hop question answering dataset, HotPotQA. Empirical evaluation shows our model to outperform the single-hop neural question generation models on both automatic evaluation metrics such as <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>, <a href="https://en.wikipedia.org/wiki/METEOR">METEOR</a>, and ROUGE and human evaluation metrics for quality and coverage of the generated questions.</abstract>
      <url hash="d1b4f202">2020.coling-main.249</url>
      <doi>10.18653/v1/2020.coling-main.249</doi>
      <bibkey>gupta-etal-2020-reinforced</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="254">
      <title>Does Chinese BERT Encode Word Structure?<fixed-case>C</fixed-case>hinese <fixed-case>BERT</fixed-case> Encode Word Structure?</title>
      <author><first>Yile</first><last>Wang</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>2826–2836</pages>
      <abstract>Contextualized representations give significantly improved results for a wide range of NLP tasks. Much work has been dedicated to analyzing the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> captured by representative models such as BERT. Existing work finds that syntactic, semantic and word sense knowledge are encoded in <a href="https://en.wikipedia.org/wiki/BERT">BERT</a>. However, little work has investigated <a href="https://en.wikipedia.org/wiki/Compound_(linguistics)">word features</a> for character languages such as <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. We investigate Chinese BERT using both attention weight distribution statistics and probing tasks, finding that (1) word information is captured by BERT ; (2) word-level features are mostly in the middle representation layers ; (3) downstream tasks make different use of word features in BERT, with POS tagging and chunking relying the most on word features, and natural language inference relying the least on such features.</abstract>
      <url hash="ff93dc0d">2020.coling-main.254</url>
      <doi>10.18653/v1/2020.coling-main.254</doi>
      <bibkey>wang-etal-2020-chinese-bert</bibkey>
      <pwccode url="https://github.com/ylwangy/BERT_zh_Analysis" additional="false">ylwangy/BERT_zh_Analysis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
    </paper>
    <paper id="259">
      <title>One Comment from One Perspective : An Effective Strategy for Enhancing Automatic Music Comment</title>
      <author><first>Tengfei</first><last>Huo</last></author>
      <author><first>Zhiqiang</first><last>Liu</last></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>2889–2899</pages>
      <abstract>The automatic generation of music comments is of great significance for increasing the popularity of music and the music platform’s activity. In human music comments, there exists high distinction and diverse perspectives for the same song. In other words, for a song, different comments stem from different musical perspectives. However, to date, this characteristic has not been considered well in research on automatic comment generation. The existing methods tend to generate common and meaningless comments. In this paper, we propose an effective multi-perspective strategy to enhance the <a href="https://en.wikipedia.org/wiki/Multiculturalism">diversity</a> of the generated comments. The experiment results on two music comment datasets show that our proposed model can effectively generate a series of diverse music comments based on different perspectives, which outperforms state-of-the-art baselines by a substantial margin.</abstract>
      <url hash="59fd3f41">2020.coling-main.259</url>
      <doi>10.18653/v1/2020.coling-main.259</doi>
      <bibkey>huo-etal-2020-one</bibkey>
      <pwccode url="https://github.com/htfhxx/commentperspective" additional="false">htfhxx/commentperspective</pwccode>
    </paper>
    <paper id="260">
      <title>A Tale of Two Linkings : Dynamically Gating between Schema Linking and Structural Linking for Text-to-SQL Parsing<fixed-case>SQL</fixed-case> Parsing</title>
      <author><first>Sanxing</first><last>Chen</last></author>
      <author><first>Aidan</first><last>San</last></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <pages>2900–2912</pages>
      <abstract>In Text-to-SQL semantic parsing, selecting the correct entities (tables and columns) for the generated <a href="https://en.wikipedia.org/wiki/SQL">SQL query</a> is both crucial and challenging ; the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> is required to connect the natural language (NL) question and the <a href="https://en.wikipedia.org/wiki/SQL">SQL query</a> to the structured knowledge in the database. We formulate two linking processes to address this challenge : schema linking which links explicit NL mentions to the database and structural linking which links the entities in the output SQL with their structural relationships in the <a href="https://en.wikipedia.org/wiki/Database_schema">database schema</a>. Intuitively, the effectiveness of these two linking processes changes based on the entity being generated, thus we propose to dynamically choose between them using a gating mechanism. Integrating the proposed method with two graph neural network-based semantic parsers together with BERT representations demonstrates substantial gains in parsing accuracy on the challenging Spider dataset. Analyses show that our proposed method helps to enhance the structure of the model output when generating complicated SQL queries and offers more explainable predictions.</abstract>
      <url hash="6e6e9cd6">2020.coling-main.260</url>
      <doi>10.18653/v1/2020.coling-main.260</doi>
      <bibkey>chen-etal-2020-tale</bibkey>
      <pwccode url="https://github.com/sanxing-chen/linking-tale" additional="false">sanxing-chen/linking-tale</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/spider-1">SPIDER</pwcdataset>
    </paper>
    <paper id="261">
      <title>Autoregressive Affective Language Forecasting : A Self-Supervised Task</title>
      <author><first>Matthew</first><last>Matero</last></author>
      <author><first>H. Andrew</first><last>Schwartz</last></author>
      <pages>2913–2923</pages>
      <abstract>Human natural language is mentioned at a specific point in time while <a href="https://en.wikipedia.org/wiki/Emotion">human emotions</a> change over time. While much work has established a strong link between <a href="https://en.wikipedia.org/wiki/Language">language use</a> and <a href="https://en.wikipedia.org/wiki/Emotion">emotional states</a>, few have attempted to model emotional language in time. Here, we introduce the task of affective language forecasting   predicting future change in language based on past changes of language, a task with real-world applications such as treating mental health or forecasting trends in consumer confidence. We establish some of the fundamental autoregressive characteristics of the task (necessary history size, static versus dynamic length, varying time-step resolutions) and then build on popular sequence models for words to instead model sequences of language-based emotion in time. Over a novel Twitter dataset of 1,900 users and weekly + daily scores for 6 <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> and 2 additional linguistic attributes, we find a novel dual-sequence GRU model with decayed hidden states achieves best results (r =.66) significantly out-predicting, e.g., a moving averaging based on the past time-steps (r =.49). We make our anonymized dataset as well as task setup and evaluation code available for others to build on.<i>affective language forecasting</i> – predicting future change in language based on past changes of language, a task with real-world applications such as treating mental health or forecasting trends in consumer confidence. We establish some of the fundamental autoregressive characteristics of the task (necessary history size, static versus dynamic length, varying time-step resolutions) and then build on popular sequence models for <i>words</i> to instead model sequences of <i>language-based emotion in time</i>. Over a novel Twitter dataset of 1,900 users and weekly + daily scores for 6 emotions and 2 additional linguistic attributes, we find a novel dual-sequence GRU model with decayed hidden states achieves best results (<tex-math>r = .66</tex-math>) significantly out-predicting, e.g., a moving averaging based on the past time-steps (<tex-math>r = .49</tex-math>). We make our anonymized dataset as well as task setup and evaluation code available for others to build on.</abstract>
      <url hash="12f5f064">2020.coling-main.261</url>
      <doi>10.18653/v1/2020.coling-main.261</doi>
      <bibkey>matero-schwartz-2020-autoregressive</bibkey>
      <pwccode url="https://github.com/matthewmatero/affectivelanguageforecasting" additional="false">matthewmatero/affectivelanguageforecasting</pwccode>
    </paper>
    <paper id="263">
      <title>End to End Chinese Lexical Fusion Recognition with Sememe Knowledge<fixed-case>C</fixed-case>hinese Lexical Fusion Recognition with Sememe Knowledge</title>
      <author><first>Yijiang</first><last>Liu</last></author>
      <author><first>Meishan</first><last>Zhang</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <pages>2935–2946</pages>
      <abstract>In this paper, we present Chinese lexical fusion recognition, a new task which could be regarded as one kind of coreference recognition. First, we introduce the task in detail, showing the relationship with coreference recognition and differences from the existing tasks. Second, we propose an <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end model</a> for the <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>, handling mentions as well as coreference relationship jointly. The model exploits the state-of-the-art contextualized BERT representations as an encoder, and is further enhanced with the sememe knowledge from HowNet by graph attention networks. We manually annotate a <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark dataset</a> for the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> and then conduct experiments on it. Results demonstrate that our final <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is effective and competitive for the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. Detailed analysis is offered for comprehensively understanding the new <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> and our proposed <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>.</abstract>
      <url hash="71ce220e">2020.coling-main.263</url>
      <doi>10.18653/v1/2020.coling-main.263</doi>
      <bibkey>liu-etal-2020-end</bibkey>
    </paper>
    <paper id="264">
      <title>Comparison by Conversion : Reverse-Engineering UCCA from Syntax and Lexical Semantics<fixed-case>UCCA</fixed-case> from Syntax and Lexical Semantics</title>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Dotan</first><last>Dvir</last></author>
      <author><first>Jakob</first><last>Prange</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>2947–2966</pages>
      <abstract>Building robust <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding systems</a> will require a clear characterization of whether and how various linguistic meaning representations complement each other. To perform a systematic comparative analysis, we evaluate the mapping between meaning representations from different frameworks using two complementary methods : (i) a rule-based converter, and (ii) a supervised delexicalized parser that parses to one framework using only information from the other as features. We apply these methods to convert the STREUSLE corpus (with syntactic and lexical semantic annotations) to UCCA (a graph-structured full-sentence meaning representation). Both methods yield surprisingly accurate target representations, close to fully supervised UCCA parser qualityindicating that UCCA annotations are partially redundant with STREUSLE annotations. Despite this substantial convergence between <a href="https://en.wikipedia.org/wiki/Conceptual_framework">frameworks</a>, we find several important areas of divergence.</abstract>
      <url hash="e0937815">2020.coling-main.264</url>
      <doi>10.18653/v1/2020.coling-main.264</doi>
      <bibkey>hershcovich-etal-2020-comparison</bibkey>
      <pwccode url="https://github.com/nert-nlp/streusle" additional="true">nert-nlp/streusle</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="267">
      <title>Normalizing Compositional Structures Across Graphbanks</title>
      <author><first>Lucia</first><last>Donatelli</last></author>
      <author><first>Jonas</first><last>Groschwitz</last></author>
      <author><first>Matthias</first><last>Lindemann</last></author>
      <author><first>Alexander</first><last>Koller</last></author>
      <author><first>Pia</first><last>Weißenhorn</last></author>
      <pages>2991–3006</pages>
      <abstract>The emergence of a variety of graph-based meaning representations (MRs) has sparked an important conversation about how to adequately represent semantic structure. MRs exhibit structural differences that reflect different theoretical and design considerations, presenting challenges to uniform linguistic analysis and cross-framework semantic parsing. Here, we ask the question of which design differences between MRs are meaningful and semantically-rooted, and which are superficial. We present a methodology for normalizing discrepancies between MRs at the compositional level (Lindemann et al., 2019), finding that we can normalize the majority of divergent phenomena using linguistically-grounded rules. Our work significantly increases the match in compositional structure between MRs and improves multi-task learning (MTL) in a low-resource setting, serving as a proof of concept for future broad-scale cross-MR normalization.</abstract>
      <url hash="f73f1ac4">2020.coling-main.267</url>
      <doi>10.18653/v1/2020.coling-main.267</doi>
      <bibkey>donatelli-etal-2020-normalizing</bibkey>
      <pwccode url="https://github.com/coli-saar/am-parser" additional="false">coli-saar/am-parser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="278">
      <title>Finding the Evidence : Localization-aware Answer Prediction for Text Visual Question Answering</title>
      <author><first>Wei</first><last>Han</last></author>
      <author><first>Hantao</first><last>Huang</last></author>
      <author><first>Tao</first><last>Han</last></author>
      <pages>3118–3131</pages>
      <abstract>Image text carries essential information to understand the scene and perform <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a>. Text-based visual question answering (text VQA) task focuses on visual questions that require reading text in images. Existing text VQA systems generate an answer by selecting from optical character recognition (OCR) texts or a fixed vocabulary. Positional information of text is underused and there is a lack of evidence for the generated answer. As such, this paper proposes a localization-aware answer prediction network (LaAP-Net) to address this challenge. Our LaAP-Net not only generates the answer to the question but also predicts a bounding box as evidence of the generated answer. Moreover, a context-enriched OCR representation (COR) for multimodal fusion is proposed to facilitate the localization task. Our proposed LaAP-Net outperforms existing approaches on three benchmark datasets for the text VQA task by a noticeable margin.</abstract>
      <url hash="f4e0af7c">2020.coling-main.278</url>
      <doi>10.18653/v1/2020.coling-main.278</doi>
      <bibkey>han-etal-2020-finding</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/st-vqa">ST-VQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/textvqa">TextVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="289">
      <title>Multi-level Alignment Pretraining for Multi-lingual Semantic Parsing</title>
      <author><first>Bo</first><last>Shao</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Weizhen</first><last>Qi</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Xiaola</first><last>Lin</last></author>
      <pages>3246–3256</pages>
      <abstract>In this paper, we present a multi-level alignment pretraining method in a unified architecture formulti-lingual semantic parsing. In this architecture, we use an adversarial training method toalign the space of different languages and use sentence level and word level parallel corpus assupervision information to align the semantic of different languages. Finally, we jointly train themulti-level alignment and semantic parsing tasks. We conduct experiments on a publicly avail-able multi-lingual semantic parsing dataset ATIS and a newly constructed <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. Experimentalresults show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms state-of-the-art methods on both <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>.</abstract>
      <url hash="9126e4ec">2020.coling-main.289</url>
      <doi>10.18653/v1/2020.coling-main.289</doi>
      <bibkey>shao-etal-2020-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
    </paper>
    <paper id="291">
      <title>Conception : Multilingually-Enhanced, Human-Readable Concept Vector Representations</title>
      <author><first>Simone</first><last>Conia</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>3268–3284</pages>
      <abstract>To date, the most successful <a href="https://en.wikipedia.org/wiki/Word_sense">word</a>, word sense, and concept modelling techniques have used large corpora and knowledge resources to produce dense vector representations that capture semantic similarities in a relatively low-dimensional space. Most current approaches, however, suffer from a monolingual bias, with their strength depending on the amount of data available across languages. In this paper we address this issue and propose Conception, a novel technique for building language-independent vector representations of concepts which places <a href="https://en.wikipedia.org/wiki/Multilinguality">multilinguality</a> at its core while retaining explicit relationships between concepts. Our approach results in high-coverage representations that outperform the state of the art in multilingual and cross-lingual Semantic Word Similarity and Word Sense Disambiguation, proving particularly robust on low-resource languages. Conception   its software and the complete set of representations   is available at https://github.com/SapienzaNLP/conception.</abstract>
      <url hash="a009e288">2020.coling-main.291</url>
      <doi>10.18653/v1/2020.coling-main.291</doi>
      <bibkey>conia-navigli-2020-conception</bibkey>
      <pwccode url="https://github.com/sapienzanlp/conception" additional="false">sapienzanlp/conception</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senseval-2-1">Senseval-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="293">
      <title>Sentence Matching with Syntax- and Semantics-Aware BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Tao</first><last>Liu</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Chengguo</first><last>Lv</last></author>
      <author><first>Ranran</first><last>Zhen</last></author>
      <author><first>Guohong</first><last>Fu</last></author>
      <pages>3302–3312</pages>
      <abstract>Sentence matching aims to identify the special relationship between two sentences, and plays a key role in many natural language processing tasks. However, previous studies mainly focused on exploiting either syntactic or semantic information for sentence matching, and no studies consider integrating both of them. In this study, we propose integrating <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> and semantics into BERT with sentence matching. In particular, we use an implicit syntax and semantics integration method that is less sensitive to the output structure information. Thus the implicit integration can alleviate the error propagation problem. The experimental results show that our approach has achieved state-of-the-art or competitive performance on several sentence matching datasets, demonstrating the benefits of implicitly integrating syntactic and semantic features in sentence matching.</abstract>
      <url hash="7d9c761a">2020.coling-main.293</url>
      <doi>10.18653/v1/2020.coling-main.293</doi>
      <bibkey>liu-etal-2020-sentence</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="295">
      <title>Homonym normalisation by word sense clustering : a case in <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a><fixed-case>J</fixed-case>apanese</title>
      <author><first>Yo</first><last>Sato</last></author>
      <author><first>Kevin</first><last>Heffernan</last></author>
      <pages>3324–3332</pages>
      <abstract>This work presents a method of word sense clustering that differentiates <a href="https://en.wikipedia.org/wiki/Homonym">homonyms</a> and merge <a href="https://en.wikipedia.org/wiki/Homophone">homophones</a>, taking <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a> as an example, where orthographical variation causes problem for <a href="https://en.wikipedia.org/wiki/Language_processing_in_the_brain">language processing</a>. It uses contextualised embeddings (BERT) to cluster tokens into distinct sense groups, and we use these <a href="https://en.wikipedia.org/wiki/Group_(mathematics)">groups</a> to normalise synonymous instances to a single representative form. We see the benefit of this normalisation in <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>, as well as in <a href="https://en.wikipedia.org/wiki/Transliteration">transliteration</a>.</abstract>
      <url hash="dc8497e0">2020.coling-main.295</url>
      <doi>10.18653/v1/2020.coling-main.295</doi>
      <bibkey>sato-heffernan-2020-homonym</bibkey>
    </paper>
    <paper id="297">
      <title>An Unsupervised Method for Learning Representations of Multi-word Expressions for Semantic Classification</title>
      <author><first>Robert</first><last>Vacareanu</last></author>
      <author><first>Marco A.</first><last>Valenzuela-Escárcega</last></author>
      <author><first>Rebecca</first><last>Sharp</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>3346–3356</pages>
      <abstract>This paper explores an unsupervised approach to learning a compositional representation function for multi-word expressions (MWEs), and evaluates it on the Tratz dataset, which associates two-word expressions with the semantic relation between the compound constituents (e.g. the label employer is associated with the noun compound government agency) (Tratz, 2011). The <a href="https://en.wikipedia.org/wiki/Composition_function">composition function</a> is based on <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a>, and is trained using the Skip-Gram objective to predict the words in the context of MWEs. Thus our approach can naturally leverage large unlabeled text sources. Further, our method can make use of provided MWEs when available, but can also function as a completely <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised algorithm</a>, using MWE boundaries predicted by a single, domain-agnostic part-of-speech pattern. With pre-defined MWE boundaries, our method outperforms the previous state-of-the-art performance on the coarse-grained evaluation of the Tratz dataset (Tratz, 2011), with an <a href="https://en.wikipedia.org/wiki/F-number">F1 score</a> of 50.4 %. The unsupervised version of our method approaches the performance of the supervised one, and even outperforms it in some configurations.</abstract>
      <url hash="c1896903">2020.coling-main.297</url>
      <doi>10.18653/v1/2020.coling-main.297</doi>
      <bibkey>vacareanu-etal-2020-unsupervised</bibkey>
    </paper>
    <paper id="300">
      <title>Sentence Analogies : Linguistic Regularities in Sentence Embeddings</title>
      <author><first>Xunjie</first><last>Zhu</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>3389–3400</pages>
      <abstract>While important properties of word vector representations have been studied extensively, far less is known about the properties of sentence vector representations. Word vectors are often evaluated by assessing to what degree they exhibit regularities with regard to relationships of the sort considered in word analogies. In this paper, we investigate to what extent commonly used sentence vector representation spaces as well reflect certain kinds of regularities. We propose a number of schemes to induce evaluation data, based on lexical analogy data as well as semantic relationships between sentences. Our experiments consider a wide range of sentence embedding methods, including ones based on BERT-style contextual embeddings. We find that different <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> differ substantially in their ability to reflect such regularities.</abstract>
      <url hash="b5f0f59b">2020.coling-main.300</url>
      <doi>10.18653/v1/2020.coling-main.300</doi>
      <bibkey>zhu-de-melo-2020-sentence</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="301">
      <title>Manifold Learning-based Word Representation Refinement Incorporating Global and Local Information</title>
      <author><first>Wenyu</first><last>Zhao</last></author>
      <author><first>Dong</first><last>Zhou</last></author>
      <author><first>Lin</first><last>Li</last></author>
      <author><first>Jinjun</first><last>Chen</last></author>
      <pages>3401–3412</pages>
      <abstract>Recent studies show that word embedding models often underestimate similarities between similar words and overestimate similarities between distant words. This results in word similarity results obtained from <a href="https://en.wikipedia.org/wiki/Embedding">embedding models</a> inconsistent with <a href="https://en.wikipedia.org/wiki/Judgement">human judgment</a>. Manifold learning-based methods are widely utilized to refine word representations by re-embedding word vectors from the original embedding space to a new refined semantic space. These methods mainly focus on preserving local geometry information through performing weighted locally linear combination between words and their neighbors twice. However, these reconstruction weights are easily influenced by different selections of neighboring words and the whole combination process is time-consuming. In this paper, we propose two novel word representation refinement methods leveraging isometry feature mapping and local tangent space respectively. Unlike previous methods, our first method corrects pre-trained word embeddings by preserving global geometry information of all words instead of local geometry information between words and their neighbors. Our second method refines word representations by aligning original and re-fined embedding spaces based on local tangent space instead of performing weighted locally linear combination twice. Experimental results obtained from standard semantic relatedness and semantic similarity tasks show that our methods outperform various state-of-the-art baselines for word representation refinement.</abstract>
      <url hash="c82f4000">2020.coling-main.301</url>
      <doi>10.18653/v1/2020.coling-main.301</doi>
      <bibkey>zhao-etal-2020-manifold</bibkey>
    </paper>
    <paper id="304">
      <title>Optimizing Transformer for Low-Resource Neural Machine Translation</title>
      <author><first>Ali</first><last>Araabi</last></author>
      <author><first>Christof</first><last>Monz</last></author>
      <pages>3429–3435</pages>
      <abstract>Language pairs with limited amounts of <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel data</a>, also known as low-resource languages, remain a challenge for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>. While the Transformer model has achieved significant improvements for many language pairs and has become the de facto mainstream architecture, its capability under low-resource conditions has not been fully investigated yet. Our experiments on different subsets of the IWSLT14 training data show that the effectiveness of <a href="https://en.wikipedia.org/wiki/Transformer">Transformer</a> under low-resource conditions is highly dependent on the hyper-parameter settings. Our experiments show that using an optimized Transformer for low-resource conditions improves the translation quality up to 7.3 BLEU points compared to using the Transformer default settings.</abstract>
      <url hash="0420d08e">2020.coling-main.304</url>
      <doi>10.18653/v1/2020.coling-main.304</doi>
      <bibkey>araabi-monz-2020-optimizing</bibkey>
    </paper>
    <paper id="308">
      <title>Towards the First Machine Translation System for <a href="https://en.wikipedia.org/wiki/Sumerian_language">Sumerian Transliterations</a><fixed-case>S</fixed-case>umerian Transliterations</title>
      <author><first>Ravneet</first><last>Punia</last></author>
      <author><first>Niko</first><last>Schenk</last></author>
      <author><first>Christian</first><last>Chiarcos</last></author>
      <author><first>Émilie</first><last>Pagé-Perron</last></author>
      <pages>3454–3460</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Sumerian_cuneiform">Sumerian cuneiform script</a> was invented more than 5,000 years ago and represents one of the oldest in history. We present the first attempt to translate <a href="https://en.wikipedia.org/wiki/Sumerian_language">Sumerian texts</a> into English automatically. We publicly release high-quality corpora for standardized training and evaluation and report results on experiments with supervised, phrase-based, and transfer learning techniques for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. Quantitative and qualitative evaluations indicate the usefulness of the <a href="https://en.wikipedia.org/wiki/Translation">translations</a>. Our proposed <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> provides a broader audience of researchers with novel access to the data, accelerates the costly and time-consuming manual translation process, and helps them better explore the relationships between <a href="https://en.wikipedia.org/wiki/Cuneiform">Sumerian cuneiform</a> and <a href="https://en.wikipedia.org/wiki/Mesopotamia">Mesopotamian culture</a>.</abstract>
      <url hash="3780e163">2020.coling-main.308</url>
      <doi>10.18653/v1/2020.coling-main.308</doi>
      <bibkey>punia-etal-2020-towards</bibkey>
    </paper>
    <paper id="309">
      <title>Using Bilingual Patents for Translation Training</title>
      <author><first>John</first><last>Lee</last></author>
      <author><first>Benjamin</first><last>Tsou</last></author>
      <author><first>Tianyuan</first><last>Cai</last></author>
      <pages>3461–3466</pages>
      <abstract>While bilingual corpora have been instrumental for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, their utility for training translators has been less explored. We investigate the use of bilingual corpora as pedagogical tools for translation in the technical domain. In a user study, novice translators revised Chinese translations of English patents through bilingual concordancing. Results show that <a href="https://en.wikipedia.org/wiki/Concordancing">concordancing</a> with an in-domain bilingual corpus can yield greater improvement in translation quality of technical terms than a general-domain bilingual corpus.</abstract>
      <url hash="12ccc4cc">2020.coling-main.309</url>
      <doi>10.18653/v1/2020.coling-main.309</doi>
      <bibkey>lee-etal-2020-using-bilingual</bibkey>
    </paper>
    <paper id="314">
      <title>Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation</title>
      <author><first>Hang</first><last>Le</last></author>
      <author><first>Juan</first><last>Pino</last></author>
      <author><first>Changhan</first><last>Wang</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>3520–3533</pages>
      <abstract>We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these <a href="https://en.wikipedia.org/wiki/Code">decoders</a> interact with each other : one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the <a href="https://en.wikipedia.org/wiki/Code">decoders</a>, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest <a href="https://en.wikipedia.org/wiki/Translation">translation</a> performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at https://github.com/formiel/speech-translation.</abstract>
      <url hash="66494499">2020.coling-main.314</url>
      <doi>10.18653/v1/2020.coling-main.314</doi>
      <bibkey>le-etal-2020-dual</bibkey>
      <pwccode url="https://github.com/formiel/speech-translation" additional="false">formiel/speech-translation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/covost">CoVoST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="315">
      <title>Multitask Learning-Based Neural Bridging Reference Resolution</title>
      <author><first>Juntao</first><last>Yu</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>3534–3546</pages>
      <abstract>We propose a multi task learning-based neural model for resolving bridging references tackling two key challenges. The first challenge is the lack of large corpora annotated with bridging references. To address this, we use <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> to help bridging reference resolution with <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>. We show that substantial improvements of up to 8 p.p. can be achieved on full bridging resolution with this <a href="https://en.wikipedia.org/wiki/Computer_architecture">architecture</a>. The second challenge is the different definitions of bridging used in different corpora, meaning that hand-coded systems or <a href="https://en.wikipedia.org/wiki/System">systems</a> using special features designed for one corpus do not work well with other corpora. Our neural model only uses a small number of corpus independent features, thus can be applied to different corpora. Evaluations with very different bridging corpora (ARRAU, ISNOTES, BASHI and SCICORP) suggest that our architecture works equally well on all corpora, and achieves the SoTA results on full bridging resolution for all corpora, outperforming the best reported results by up to 36.3 p.p..</abstract>
      <url hash="6f1b6a65">2020.coling-main.315</url>
      <doi>10.18653/v1/2020.coling-main.315</doi>
      <bibkey>yu-poesio-2020-multitask</bibkey>
      <pwccode url="https://github.com/juntaoy/dali-bridging" additional="false">juntaoy/dali-bridging</pwccode>
    </paper>
    <paper id="317">
      <title>Automatic Discovery of Heterogeneous Machine Learning Pipelines : An Application to <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a></title>
      <author><first>Suilan</first><last>Estevez-Velarde</last></author>
      <author><first>Yoan</first><last>Gutiérrez</last></author>
      <author><first>Andres</first><last>Montoyo</last></author>
      <author><first>Yudivián</first><last>Almeida Cruz</last></author>
      <pages>3558–3568</pages>
      <abstract>This paper presents AutoGOAL, a system for automatic machine learning (AutoML) that uses heterogeneous techniques. In contrast with existing AutoML approaches, our contribution can automatically build machine learning pipelines that combine techniques and algorithms from different frameworks, including shallow classifiers, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing tools</a>, and <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>. We define the heterogeneous AutoML optimization problem as the search for the best sequence of algorithms that transforms specific input data into the desired output. This provides a novel theoretical and practical approach to <a href="https://en.wikipedia.org/wiki/AutoML">AutoML</a>. Our proposal is experimentally evaluated in diverse machine learning problems and compared with alternative approaches, showing that it is competitive with other AutoML alternatives in standard benchmarks. Furthermore, it can be applied to novel scenarios, such as several NLP tasks, where existing alternatives can not be directly deployed. The <a href="https://en.wikipedia.org/wiki/System">system</a> is freely available and includes in-built compatibility with a large number of popular machine learning frameworks, which makes our approach useful for solving practical problems with relative ease and effort.</abstract>
      <url hash="d3b40df7">2020.coling-main.317</url>
      <doi>10.18653/v1/2020.coling-main.317</doi>
      <bibkey>estevez-velarde-etal-2020-automatic</bibkey>
    </paper>
    <paper id="319">
      <title>Incorporating Noisy Length Constraints into Transformer with Length-aware Positional Encodings</title>
      <author><first>Yui</first><last>Oka</last></author>
      <author><first>Katsuki</first><last>Chousa</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>3580–3585</pages>
      <abstract>Neural Machine Translation often suffers from an under-translation problem due to its limited modeling of output sequence lengths. In this work, we propose a novel approach to training a Transformer model using length constraints based on length-aware positional encoding (PE). Since length constraints with exact target sentence lengths degrade translation performance, we add random noise within a certain window size to the length constraints in the PE during the training. In the <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference step</a>, we predict the output lengths using input sequences and a BERT-based length prediction model. Experimental results in an ASPEC English-to-Japanese translation showed the proposed method produced translations with lengths close to the reference ones and outperformed a vanilla Transformer (especially in short sentences) by 3.22 points in <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>. The average translation results using our length prediction model were also better than another baseline method using input lengths for the length constraints. The proposed noise injection improved robustness for length prediction errors, especially within the window size.</abstract>
      <url hash="64bdf0c8">2020.coling-main.319</url>
      <doi>10.18653/v1/2020.coling-main.319</doi>
      <bibkey>oka-etal-2020-incorporating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
    </paper>
    <paper id="322">
      <title>Deep Inside-outside Recursive Autoencoder with All-span Objective</title>
      <author><first>Ruyue</first><last>Hong</last></author>
      <author><first>Jiong</first><last>Cai</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>3610–3615</pages>
      <abstract>Deep inside-outside recursive autoencoder (DIORA) is a neural-based model designed for unsupervised constituency parsing. During its forward computation, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> provides phrase and contextual representations for all spans in the input sentence. By utilizing the contextual representation of each leaf-level span, the span of length 1, to reconstruct the word inside the span, the model is trained without labeled data. In this work, we extend the training objective of DIORA by making use of all spans instead of only leaf-level spans. We test our new training objective on datasets of two languages : <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a>, and empirically show that our method achieves improvement in parsing accuracy over the original DIORA.</abstract>
      <url hash="1cb24d50">2020.coling-main.322</url>
      <doi>10.18653/v1/2020.coling-main.322</doi>
      <bibkey>hong-etal-2020-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="325">
      <title>Picking BERT’s Brain : Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis<fixed-case>BERT</fixed-case>’s Brain: Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis</title>
      <author><first>Michael</first><last>Lepori</last></author>
      <author><first>R. Thomas</first><last>McCoy</last></author>
      <pages>3637–3651</pages>
      <abstract>As the name implies, contextualized representations of language are typically motivated by their ability to encode context. Which aspects of context are captured by such <a href="https://en.wikipedia.org/wiki/Representation_(arts)">representations</a>? We introduce an approach to address this question using Representational Similarity Analysis (RSA). As case studies, we investigate the degree to which a verb embedding encodes the verb’s subject, a pronoun embedding encodes the pronoun’s antecedent, and a full-sentence representation encodes the sentence’s head word (as determined by a dependency parse). In all cases, we show that BERT’s contextualized embeddings reflect the linguistic dependency being studied, and that BERT encodes these dependencies to a greater degree than it encodes less linguistically-salient controls. These results demonstrate the ability of our approach to adjudicate between hypotheses about which aspects of context are encoded in representations of language.</abstract>
      <url hash="be244c3d">2020.coling-main.325</url>
      <doi>10.18653/v1/2020.coling-main.325</doi>
      <bibkey>lepori-mccoy-2020-picking</bibkey>
      <pwccode url="https://github.com/mlepori1/Picking_BERTs_Brain" additional="false">mlepori1/Picking_BERTs_Brain</pwccode>
    </paper>
    <paper id="326">
      <title>The Devil is in the Details : Evaluating Limitations of Transformer-based Methods for Granular Tasks</title>
      <author><first>Brihi</first><last>Joshi</last></author>
      <author><first>Neil</first><last>Shah</last></author>
      <author><first>Francesco</first><last>Barbieri</last></author>
      <author><first>Leonardo</first><last>Neves</last></author>
      <pages>3652–3659</pages>
      <abstract>Contextual embeddings derived from transformer-based neural language models have shown state-of-the-art performance for various tasks such as <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, and textual similarity in recent years. Extensive work shows how accurately such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can represent abstract, semantic information present in text. In this expository work, we explore a tangent direction and analyze such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>’ performance on tasks that require a more granular level of representation. We focus on the problem of textual similarity from two perspectives : matching documents on a granular level (requiring embeddings to capture fine-grained attributes in the text), and an abstract level (requiring embeddings to capture overall textual semantics). We empirically demonstrate, across two datasets from different domains, that despite high performance in abstract document matching as expected, contextual embeddings are consistently (and at times, vastly) outperformed by simple baselines like TF-IDF for more granular tasks. We then propose a simple but effective method to incorporate TF-IDF into <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that use contextual embeddings, achieving relative improvements of up to 36 % on granular tasks.</abstract>
      <url hash="916a8087">2020.coling-main.326</url>
      <doi>10.18653/v1/2020.coling-main.326</doi>
      <bibkey>joshi-etal-2020-devil</bibkey>
      <pwccode url="https://github.com/brihijoshi/granular-similarity-COLING-2020" additional="false">brihijoshi/granular-similarity-COLING-2020</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="327">
      <title>CoLAKE : Contextualized Language and Knowledge Embedding<fixed-case>C</fixed-case>o<fixed-case>LAKE</fixed-case>: Contextualized Language and Knowledge Embedding</title>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Yunfan</first><last>Shao</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Qipeng</first><last>Guo</last></author>
      <author><first>Yaru</first><last>Hu</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <pages>3660–3670</pages>
      <abstract>With the emerging branch of incorporating factual knowledge into pre-trained language models such as BERT, most existing <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> consider shallow, static, and separately pre-trained entity embeddings, which limits the performance gains of these <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>. Few works explore the potential of deep contextualized knowledge representation when injecting knowledge. In this paper, we propose the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly learns contextualized representation for both language and knowledge with the extended MLM objective. Instead of injecting only entity embeddings, CoLAKE extracts the <a href="https://en.wikipedia.org/wiki/Context_(computing)">knowledge context</a> of an entity from large-scale knowledge bases. To handle the heterogeneity of knowledge context and <a href="https://en.wikipedia.org/wiki/Context_(language_use)">language context</a>, we integrate them in a unified data structure, word-knowledge graph (WK graph). CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer encoder. We conduct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.</abstract>
      <url hash="8a4bcb0a">2020.coling-main.327</url>
      <doi>10.18653/v1/2020.coling-main.327</doi>
      <bibkey>sun-etal-2020-colake</bibkey>
      <pwccode url="https://github.com/txsun1997/CoLAKE" additional="false">txsun1997/CoLAKE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/open-entity-1">Open Entity</pwcdataset>
    </paper>
    <paper id="330">
      <title>Target Word Masking for Location Metonymy Resolution</title>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Maria</first><last>Vasardani</last></author>
      <author><first>Martin</first><last>Tomko</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>3696–3707</pages>
      <abstract>Existing metonymy resolution approaches rely on features extracted from external resources like <a href="https://en.wikipedia.org/wiki/Dictionary">dictionaries</a> and hand-crafted lexical resources. In this paper, we propose an end-to-end word-level classification approach based only on BERT, without dependencies on <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">taggers</a>, <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a>, curated dictionaries of place names, or other external resources. We show that our approach achieves the state-of-the-art on 5 datasets, surpassing conventional BERT models and <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> by a large margin. We also show that our approach generalises well to unseen data.</abstract>
      <url hash="e7833d8a">2020.coling-main.330</url>
      <doi>10.18653/v1/2020.coling-main.330</doi>
      <bibkey>li-etal-2020-target</bibkey>
      <pwccode url="https://github.com/haonan-li/TWM-metonymy-resolution" additional="false">haonan-li/TWM-metonymy-resolution</pwccode>
    </paper>
    <paper id="333">
      <title>What Meaning-Form Correlation Has to Compose With : A Study of MFC on Artificial and Natural Language<fixed-case>MFC</fixed-case> on Artificial and Natural Language</title>
      <author><first>Timothee</first><last>Mickus</last></author>
      <author><first>Timothée</first><last>Bernard</last></author>
      <author><first>Denis</first><last>Paperno</last></author>
      <pages>3737–3749</pages>
      <abstract>Compositionality is a widely discussed property of <a href="https://en.wikipedia.org/wiki/Natural_language">natural languages</a>, although its exact definition has been elusive. We focus on the proposal that <a href="https://en.wikipedia.org/wiki/Compositionality">compositionality</a> can be assessed by measuring meaning-form correlation. We analyze meaning-form correlation on three sets of <a href="https://en.wikipedia.org/wiki/Language">languages</a> : (i) artificial toy languages tailored to be compositional, (ii) a set of English dictionary definitions, and (iii) a set of English sentences drawn from literature. We find that linguistic phenomena such as <a href="https://en.wikipedia.org/wiki/Synonym">synonymy</a> and ungrounded stop-words weigh on MFC measurements, and that straightforward methods to mitigate their effects have widely varying results depending on the dataset they are applied to. Data and code are made publicly available.</abstract>
      <url hash="2000947e">2020.coling-main.333</url>
      <doi>10.18653/v1/2020.coling-main.333</doi>
      <bibkey>mickus-etal-2020-meaning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
    </paper>
    <paper id="334">
      <title>Evaluating Pretrained Transformer-based Models on the Task of Fine-Grained Named Entity Recognition</title>
      <author><first>Cedric</first><last>Lothritz</last></author>
      <author><first>Kevin</first><last>Allix</last></author>
      <author><first>Lisa</first><last>Veiber</last></author>
      <author><first>Tegawendé F.</first><last>Bissyandé</last></author>
      <author><first>Jacques</first><last>Klein</last></author>
      <pages>3750–3760</pages>
      <abstract>Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task and has remained an active research field. In recent years, transformer models and more specifically the BERT model developed at Google revolutionised the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. While the performance of transformer-based approaches such as BERT has been studied for NER, there has not yet been a study for the fine-grained Named Entity Recognition (FG-NER) task. In this paper, we compare three transformer-based models (BERT, RoBERTa, and XLNet) to two non-transformer-based models (CRF and BiLSTM-CNN-CRF). Furthermore, we apply each <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to a multitude of distinct domains. We find that transformer-based models incrementally outperform the studied non-transformer-based models in most domains with respect to the F1 score. Furthermore, we find that the choice of domains significantly influenced the performance regardless of the respective data size or the model chosen.</abstract>
      <url hash="2eef7da9">2020.coling-main.334</url>
      <doi>10.18653/v1/2020.coling-main.334</doi>
      <bibkey>lothritz-etal-2020-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="336">
      <title>A Unifying Theory of Transition-based and Sequence Labeling Parsing</title>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last></author>
      <author><first>Michalina</first><last>Strzyz</last></author>
      <author><first>David</first><last>Vilares</last></author>
      <pages>3776–3793</pages>
      <abstract>We define a <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mapping</a> from transition-based parsing algorithms that read sentences from left to right to sequence labeling encodings of syntactic trees. This not only establishes a theoretical relation between transition-based parsing and sequence-labeling parsing, but also provides a method to obtain new encodings for fast and simple sequence labeling parsing from the many existing transition-based parsers for different formalisms. Applying it to dependency parsing, we implement sequence labeling versions of four algorithms, showing that they are learnable and obtain comparable performance to existing encodings.</abstract>
      <url hash="e0de3919">2020.coling-main.336</url>
      <doi>10.18653/v1/2020.coling-main.336</doi>
      <bibkey>gomez-rodriguez-etal-2020-unifying</bibkey>
      <pwccode url="https://github.com/mstrise/dep2label" additional="false">mstrise/dep2label</pwccode>
    </paper>
    <paper id="338">
      <title>Semi-supervised Domain Adaptation for Dependency Parsing via Improved Contextualized Word Representations</title>
      <author><first>Ying</first><last>Li</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>3806–3817</pages>
      <abstract>In recent years, <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a> performance is dramatically improved on in-domain texts thanks to the rapid progress of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural network models</a>. The major challenge for current parsing research is to improve <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a> performance on out-of-domain texts that are very different from the in-domain training data when there is only a small-scale out-domain labeled data. To deal with this problem, we propose to improve the contextualized word representations via <a href="https://en.wikipedia.org/wiki/Adversarial_learning">adversarial learning</a> and fine-tuning BERT processes. Concretely, we apply <a href="https://en.wikipedia.org/wiki/Adversarial_learning">adversarial learning</a> to three representative semi-supervised domain adaption methods, i.e., direct concatenation (CON), feature augmentation (FA), and domain embedding (DE) with two useful strategies, i.e., fused target-domain word representations and orthogonality constraints, thus enabling to model more pure yet effective domain-specific and domain-invariant representations. Simultaneously, we utilize a large-scale target-domain unlabeled data to fine-tune BERT with only the language model loss, thus obtaining reliable contextualized word representations that benefit for the cross-domain dependency parsing. Experiments on a benchmark dataset show that our proposed adversarial approaches achieve consistent improvement, and fine-tuning BERT further boosts parsing accuracy by a large margin. Our single model achieves the same state-of-the-art performance as the top submitted system in the NLPCC-2019 shared task, which uses ensemble models and BERT.</abstract>
      <url hash="1f438093">2020.coling-main.338</url>
      <doi>10.18653/v1/2020.coling-main.338</doi>
      <bibkey>li-etal-2020-semi</bibkey>
    </paper>
    <paper id="341">
      <title>Learning to Prune Dependency Trees with Rethinking for Neural Relation Extraction</title>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Xue</first><last>Mengge</last></author>
      <author><first>Zhenyu</first><last>Zhang</last></author>
      <author><first>Tingwen</first><last>Liu</last></author>
      <author><first>Wang</first><last>Yubin</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <pages>3842–3852</pages>
      <abstract>Dependency trees have been shown to be effective in capturing long-range relations between target entities. Nevertheless, how to selectively emphasize target-relevant information and remove irrelevant content from the <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">tree</a> is still an open problem. Existing approaches employing pre-defined rules to eliminate <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a> may not always yield optimal results due to the <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> and variability of natural language. In this paper, we present a novel architecture named Dynamically Pruned Graph Convolutional Network (DP-GCN), which learns to prune the dependency tree with rethinking in an end-to-end scheme. In each layer of DP-GCN, we employ a selection module to concentrate on nodes expressing the target relation by a set of binary gates, and then augment the pruned tree with a pruned semantic graph to ensure the connectivity. After that, we introduce a rethinking mechanism to guide and refine the pruning operation by feeding back the high-level learned features repeatedly. Extensive experimental results demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves impressive results compared to strong <a href="https://en.wikipedia.org/wiki/Competition">competitors</a>.</abstract>
      <url hash="d6fbde07">2020.coling-main.341</url>
      <doi>10.18653/v1/2020.coling-main.341</doi>
      <bibkey>yu-etal-2020-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="342">
      <title>How Far Does BERT Look At : Distance-based Clustering and Analysis of BERT’s Attention<fixed-case>BERT</fixed-case> Look At: Distance-based Clustering and Analysis of <fixed-case>BERT</fixed-case>’s Attention</title>
      <author><first>Yue</first><last>Guan</last></author>
      <author><first>Jingwen</first><last>Leng</last></author>
      <author><first>Chao</first><last>Li</last></author>
      <author><first>Quan</first><last>Chen</last></author>
      <author><first>Minyi</first><last>Guo</last></author>
      <pages>3853–3860</pages>
      <abstract>Recent research on the multi-head attention mechanism, especially that in pre-trained models such as BERT, has shown us heuristics and clues in analyzing various aspects of the mechanism. As most of the research focus on probing tasks or hidden states, previous works have found some primitive patterns of attention head behavior by <a href="https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making">heuristic analytical methods</a>, but a more systematic analysis specific on the <a href="https://en.wikipedia.org/wiki/Attention">attention patterns</a> still remains primitive. In this work, we clearly cluster the attention heatmaps into significantly different patterns through <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised clustering</a> on top of a set of proposed <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>, which corroborates with previous observations. We further study their corresponding functions through <a href="https://en.wikipedia.org/wiki/Analytical_chemistry">analytical study</a>. In addition, our proposed <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> can be used to explain and calibrate different attention heads in Transformer models.</abstract>
      <url hash="d3d385b2">2020.coling-main.342</url>
      <doi>10.18653/v1/2020.coling-main.342</doi>
      <bibkey>guan-etal-2020-far</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="343">
      <title>An Analysis of Simple Data Augmentation for Named Entity Recognition</title>
      <author><first>Xiang</first><last>Dai</last></author>
      <author><first>Heike</first><last>Adel</last></author>
      <pages>3861–3867</pages>
      <abstract>Simple yet effective data augmentation techniques have been proposed for sentence-level and sentence-pair natural language processing tasks. Inspired by these efforts, we design and compare <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, which is usually modeled as a token-level sequence labeling problem. Through experiments on two data sets from the biomedical and materials science domains (i2b2-2010 and MaSciP), we show that simple augmentation can boost performance for both recurrent and transformer-based models, especially for small training sets.</abstract>
      <url hash="99d2ce5e">2020.coling-main.343</url>
      <doi>10.18653/v1/2020.coling-main.343</doi>
      <bibkey>dai-adel-2020-analysis</bibkey>
    </paper>
    <paper id="348">
      <title>Integrating Domain Terminology into Neural Machine Translation</title>
      <author><first>Elise</first><last>Michon</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <pages>3925–3937</pages>
      <abstract>This paper extends existing work on terminology integration into <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a>, a common industrial practice to dynamically adapt <a href="https://en.wikipedia.org/wiki/Translation">translation</a> to a specific domain. Our method, based on the use of placeholders complemented with morphosyntactic annotation, efficiently taps into the ability of the <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> to deal with symbolic knowledge to surpass the surface generalization shown by alternative techniques. We compare our approach to state-of-the-art systems and benchmark them through a well-defined evaluation framework, focusing on actual application of terminology and not just on the overall performance. Results indicate the suitability of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> in the use-case where <a href="https://en.wikipedia.org/wiki/Terminology">terminology</a> is used in a <a href="https://en.wikipedia.org/wiki/System">system</a> trained on generic data only.</abstract>
      <url hash="2e55076d">2020.coling-main.348</url>
      <doi>10.18653/v1/2020.coling-main.348</doi>
      <bibkey>michon-etal-2020-integrating</bibkey>
    </paper>
    <paper id="351">
      <title>Neural Machine Translation Models with Back-Translation for the Extremely Low-Resource Indigenous Language Bribri<fixed-case>B</fixed-case>ribri</title>
      <author><first>Isaac</first><last>Feldman</last></author>
      <author><first>Rolando</first><last>Coto-Solano</last></author>
      <pages>3965–3976</pages>
      <abstract>This paper presents a <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation model</a> and <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for the <a href="https://en.wikipedia.org/wiki/Bribri_language">Chibchan language Bribri</a>, with an average performance of BLEU 16.91.7. This was trained on an extremely small dataset (5923 Bribri-Spanish pairs), providing evidence for the applicability of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a> in extremely low-resource environments. We discuss the challenges entailed in managing training input from languages without standard orthographies, we provide evidence of successful learning of Bribri grammar, and also examine the translations of structures that are infrequent in major <a href="https://en.wikipedia.org/wiki/Indo-European_languages">Indo-European languages</a>, such as positional verbs, ergative markers, numerical classifiers and complex demonstrative systems. In addition to this, we perform an experiment of augmenting the dataset through iterative back-translation (Sennrich et al., 2016a ; Hoang et al., 2018) by using Spanish sentences to create synthetic Bribri sentences. This improves the score by an average of 1.0 BLEU, but only when the new Spanish sentences belong to the same domain as the other Spanish examples. This contributes to the small but growing body of research on Chibchan NLP.</abstract>
      <url hash="7304beb7">2020.coling-main.351</url>
      <doi>10.18653/v1/2020.coling-main.351</doi>
      <bibkey>feldman-coto-solano-2020-neural</bibkey>
      <pwccode url="https://github.com/rolandocoto/bribri-coling2020" additional="false">rolandocoto/bribri-coling2020</pwccode>
    </paper>
    <paper id="352">
      <title>Dynamic Curriculum Learning for Low-Resource Neural Machine Translation</title>
      <author><first>Chen</first><last>Xu</last></author>
      <author><first>Bojie</first><last>Hu</last></author>
      <author><first>Yufan</first><last>Jiang</last></author>
      <author><first>Kai</first><last>Feng</last></author>
      <author><first>Zeyang</first><last>Wang</last></author>
      <author><first>Shen</first><last>Huang</last></author>
      <author><first>Qi</first><last>Ju</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>3977–3989</pages>
      <abstract>Large amounts of data has made neural machine translation (NMT) a big success in recent years. But it is still a challenge if we train these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on small-scale corpora. In this case, the way of using data appears to be more important. Here, we investigate the effective use of <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data</a> for low-resource NMT. In particular, we propose a dynamic curriculum learning (DCL) method to reorder training samples in training. Unlike previous work, we do not use a static scoring function for <a href="https://en.wikipedia.org/wiki/Order_of_operations">reordering</a>. Instead, the order of training samples is dynamically determined in two ways-loss decline and model competence. This eases training by highlighting easy samples that the current <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> has enough competence to learn. We test our DCL method in a Transformer-based system. Experimental results show that DCL outperforms several strong baselines on three low-resource machine translation benchmarks and different sized data of WMT’16 En-De.</abstract>
      <url hash="104ce6f5">2020.coling-main.352</url>
      <doi>10.18653/v1/2020.coling-main.352</doi>
      <bibkey>xu-etal-2020-dynamic</bibkey>
    </paper>
    <paper id="356">
      <title>How LSTM Encodes Syntax : Exploring Context Vectors and Semi-Quantization on Natural Text<fixed-case>LSTM</fixed-case> Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text</title>
      <author><first>Chihiro</first><last>Shibata</last></author>
      <author><first>Kei</first><last>Uchiumi</last></author>
      <author><first>Daichi</first><last>Mochihashi</last></author>
      <pages>4033–4043</pages>
      <abstract>Long Short-Term Memory recurrent neural network (LSTM) is widely used and known to capture informative long-term syntactic dependencies. However, how such <a href="https://en.wikipedia.org/wiki/Information">information</a> are reflected in its internal vectors for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural text</a> has not yet been sufficiently investigated. We analyze them by learning a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> where <a href="https://en.wikipedia.org/wiki/Syntax">syntactic structures</a> are implicitly given. We empirically show that the <a href="https://en.wikipedia.org/wiki/Context_(computing)">context update vectors</a>, i.e. outputs of internal gates, are approximately quantized to binary or ternary values to help the language model to count the depth of nesting accurately, as Suzgun et al. (2019) recently show for synthetic Dyck languages. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as <a href="https://en.wikipedia.org/wiki/Phrase_structure">VP</a> and <a href="https://en.wikipedia.org/wiki/Phrase_structure">NP</a>. Moreover, with an L1 regularization, we also found that it can accurately predict whether a word is inside a <a href="https://en.wikipedia.org/wiki/Phrase_structure">phrase structure</a> or not from a small number of components of the context vector. Even for the case of <a href="https://en.wikipedia.org/wiki/Learning">learning</a> from raw text, context vectors are shown to still correlate well with the <a href="https://en.wikipedia.org/wiki/Phrase_structure">phrase structures</a>. Finally, we show that natural clusters of the functional words and the part of speeches that trigger phrases are represented in a small but principal subspace of the context-update vector of LSTM.</abstract>
      <url hash="d22aefe9">2020.coling-main.356</url>
      <doi>10.18653/v1/2020.coling-main.356</doi>
      <bibkey>shibata-etal-2020-lstm</bibkey>
    <title_ar>كيف يشفر LSTM النحو: استكشاف متجهات السياق والتكميم شبه الكمي على النص الطبيعي</title_ar>
      <title_es>Cómo codifica LSTM la sintaxis: exploración de vectores de contexto y semicuantificación en texto natural</title_es>
      <title_pt>Como o LSTM codifica a sintaxe: explorando vetores de contexto e semiquantização em texto natural</title_pt>
      <title_ja>LSTMが構文をエンコードする方法:コンテキストベクトルの探索と自然テキストの半定量化</title_ja>
      <title_zh>LSTM 何以编码语法:原自然之上下文向量半量化</title_zh>
      <title_hi>कैसे LSTM वाक्यविन्यास encodes: प्राकृतिक पाठ पर संदर्भ वैक्टर और अर्ध परिमाणीकरण की खोज</title_hi>
      <title_ga>Conas a Ionchódaíonn LSTM Comhréir: Veicteoirí Comhthéacs a Iniúchadh agus Leathchainníochtú ar Théacs Nádúrtha</title_ga>
      <title_el>Πώς κωδικοποιεί τη σύνταξη: Εξερεύνηση διανυσμάτων περιβάλλοντος και ημι-ποσοτικοποίηση σε φυσικό κείμενο</title_el>
      <title_ka>როგორ LSTM სინტექსტის კონტექსტის სინტექსტი: კონტექსტის გვეკტორები და პირველი კვანტიზაცია</title_ka>
      <title_hu>Hogyan kódolja az LSTM a szintaxist: kontextusvektorok és félkvantizáció feltárása a természetes szövegen</title_hu>
      <title_it>Come LSTM codifica la sintassi: esplorazione di vettori di contesto e semi-quantizzazione sul testo naturale</title_it>
      <title_kk>LSTM синтаксисін қалай кодтамасы: Контексті векторларды және түсінікті мәтіннің жарты көлемін зерттеу</title_kk>
      <title_lt>Kaip LSTM koduoja sintaksą: konteksto vektorių tyrimas ir puskiekybinis natūralaus teksto tyrimas</title_lt>
      <title_ml>എംഎസ്റ്റിം എങ്ങനെയാണ് സിന്റാക്സ് എന്‍കോഡ് ചെയ്യുന്നത്: ഉള്ളിലുള്ള വെക്റ്ററുകളും സ്വാഭാവിക വാചകത്തില്‍</title_ml>
      <title_mk>Како LSTM го кодира синтаксот: Истражување на контекстни вектори и полуквантизација на природниот текст</title_mk>
      <title_mt>Kif LSTM tikkodifika s-Sintassa: L-Esplorazzjoni tal-Vetturi tal-Kuntest u s-Semi-Kwantizzazzjoni fuq it-Test Naturali</title_mt>
      <title_ms>How LSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization on Natural Text</title_ms>
      <title_mn>LSTM шинжлэх ухаан хэрхэн шинжлэх ухаан: Контекст векторуудыг, байгалийн текст дээр хагас хэмжээний тоо шинжлэх ухаан</title_mn>
      <title_no>Korleis LSTM kodar syntaks: Utforskar kontekstvektorar og halvkvantisering på naturtekst</title_no>
      <title_pl>Jak LSTM koduje składnię: badanie wektorów kontekstowych i półkwantyzacja tekstu naturalnego</title_pl>
      <title_ro>Cum LSTM codează sintaxa: explorarea vectorilor de context și semi-cuantificarea pe text natural</title_ro>
      <title_so>Sida LSTM uu u kooban yahay kaalmada la-Syntada: Baadayida wadanka hoose iyo Semi-Quantification ku qoran qoraalka asalka ah</title_so>
      <title_si>LSTM සංකේතය කොහොමද සංකේතය: සංකේතය වෙක්ටර්ස් සහ සාමාන්‍ය පාළුවට ප්‍රවේශනය කරන්න</title_si>
      <title_sr>Kako LSTM kodira sintaks: istraživanje kontekstskih vektora i polu kvantizacije na prirodnom tekstu</title_sr>
      <title_sv>Hur LSTM kodar syntax: utforska kontextvektorer och halvkvantisering på naturlig text</title_sv>
      <title_ta>எப்படி LSTM ஒத்திசைவை குறியிடுகிறது: சாதாரண உரையில் உள்ள வெக்டார் மற்றும் பாதி குறியீடு</title_ta>
      <title_ur>LSTM کیسے سینٹکس کوڈ کرتا ہے: کانٹکس ویکتروں اور نصف- کوانتیزی طبیعی متن پر</title_ur>
      <title_uz>Comment</title_uz>
      <title_vi>Cách Mật mã LSD: Thăm dò sự lây nhiễm ngữ cảnh và phân loại văn bản tự nhiên</title_vi>
      <title_hr>Kako LSTM kodira sintaks: istraživanje kontekstskih vektora i polu kvantizacije prirodnog teksta</title_hr>
      <title_da>Hvordan LSTM koder syntaks: Udforskning af kontekstvektorer og halvkvantisering på naturlig tekst</title_da>
      <title_id>Bagaimana LSTM Mengenkod Sintaks: Menjelajah Vektor Konteks dan Semi-Quantisasi pada Teks Alami</title_id>
      <title_nl>Hoe LSTM syntaxis codeert: Verken contextvectoren en semi-kwantificatie op natuurlijke tekst</title_nl>
      <title_de>Wie LSTM Syntax kodiert: Untersuchung von Kontextvektoren und Halbquantifizierung auf natürlichem Text</title_de>
      <title_ko>LSTM의 문법 인코딩 방법: 자연 텍스트의 상하문 벡터와 반량화 탐색</title_ko>
      <title_sw>Namna LSTM inavyoorodhesha kodi: Kuchunguza Vectors of Context and Section Quantification on Natural Text</title_sw>
      <title_fa>چگونه سینتکس LSTM رمزبندی می‌کند: تحقیق ویکتورهای متصل و نصف Quantization بر متن طبیعی</title_fa>
      <title_bg>Как кодира синтаксиса: проучване на контекстни вектори и полуквантоване на естествен текст</title_bg>
      <title_sq>Si LSTM kodon sintaksën: Shqyrtimi i vektorëve të kontekstit dhe gjysmë-kuantizimit në tekstin natyror</title_sq>
      <title_af>Hoe LSTM enkodeer sintaks: Ondersoek Konteks Vektors en Semi- Quantisasie op Natuurlike Teks</title_af>
      <title_tr>LSTM Sintaks Nasıl Kodlayır: Kontekst vektörleri ve doğal Metin yarısını Tarama</title_tr>
      <title_am>ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s</title_am>
      <title_hy>Comment</title_hy>
      <title_az>LSTM Sintaksi nec톛 kodlay캼r: T톛bi톛tli Metin 칲zerind톛 Kontekst Vekt칬rl톛rini v톛 Yar캼-Quantizat캼n캼 Exploring</title_az>
      <title_bn>কিভাবে এলস্টিএম সিন্ট্যাক্স এনকোড করে: বিষয়বস্তু ভেক্টর এবং স্বাভাবিক টেক্সটের বিষয়বস্তু বিশ্লেষণ করা</title_bn>
      <title_bs>Kako LSTM kodira sintaks: istraživanje kontekstskih vektora i polu kvantizacije na prirodnom tekstu</title_bs>
      <title_ca>Com LSTM codifica la sintaxi: Explorar els vectors contextuals i la semiquantificació del text natural</title_ca>
      <title_et>Kuidas LSTM kodeerib süntaksit: konteksti vektorite uurimine ja loodusliku teksti poolkvantiseerimine</title_et>
      <title_cs>Jak LSTM kóduje syntaxi: zkoumání kontextových vektorů a polokvantizace na přírodním textu</title_cs>
      <title_fi>Kuinka LSTM koodaa syntaksia: Kontekstivektorien tutkiminen ja luonnontekstin puolikvantitointi</title_fi>
      <title_sk>Kako LSTM kodira sintakso: raziskovanje kontekstnih vektorjev in polkvantizacija naravnega besedila</title_sk>
      <title_he>Comment</title_he>
      <title_ha>@ item Text character set</title_ha>
      <title_jv>Text</title_jv>
      <title_bo>LSTM སྦྲེལ་མཐུད་སྟངས་ལག་སྟར་འདྲ་བྱེད་སྟངས། རང་བཞིན་ཡི་གེའི་ཁྱད་ཚད་ལྟ་བཤེར་བྱེད་བཞིན་པ</title_bo>
      <abstract_ar>تُستخدم الشبكة العصبية المتكررة للذاكرة طويلة المدى (LSTM) على نطاق واسع ومعروفة بالتقاط التبعيات النحوية طويلة المدى بالمعلومات. ومع ذلك ، فإن كيفية انعكاس هذه المعلومات في المتجهات الداخلية للنص الطبيعي لم يتم التحقيق فيها بشكل كافٍ حتى الآن. نقوم بتحليلها من خلال تعلم نموذج لغوي حيث يتم إعطاء التراكيب النحوية ضمنيًا. نظهر بشكل تجريبي أن متجهات تحديث السياق ، أي مخرجات البوابات الداخلية ، يتم قياسها تقريبًا إلى قيم ثنائية أو ثلاثية لمساعدة نموذج اللغة على حساب عمق التداخل بدقة ، مثل Suzgun et al. (2019) عرض مؤخرًا للغات Dyck الاصطناعية. بالنسبة لبعض الأبعاد في متجه السياق ، نظهر أن عمليات تنشيطها مرتبطة ارتباطًا وثيقًا بعمق تراكيب العبارة ، مثل VP و NP. علاوة على ذلك ، مع تنظيم L1 ، وجدنا أيضًا أنه يمكن أن يتنبأ بدقة بما إذا كانت الكلمة موجودة داخل بنية عبارة أم لا من عدد صغير من مكونات متجه السياق. حتى في حالة التعلم من النص الخام ، يظهر أن متجهات السياق لا تزال مرتبطة جيدًا بتراكيب العبارة. أخيرًا ، نوضح أن المجموعات الطبيعية للكلمات الوظيفية وجزء الخطابات التي تؤدي إلى تشغيل العبارات يتم تمثيلها في مساحة فرعية صغيرة ولكنها رئيسية لمتجه تحديث السياق لـ LSTM.</abstract_ar>
      <abstract_es>La red neuronal recurrente de memoria a corto plazo (LSTM) se usa ampliamente y se sabe que captura dependencias sintácticas informativas a largo plazo. Sin embargo, aún no se ha investigado suficientemente cómo se refleja esa información en sus vectores internos para el texto natural. Los analizamos aprendiendo un modelo de lenguaje en el que se dan implícitamente estructuras sintácticas. Demostramos empíricamente que los vectores de actualización de contexto, es decir, las salidas de las puertas internas, se cuantifican aproximadamente a valores binarios o ternarios para ayudar al modelo de lenguaje a contar la profundidad del anidamiento con precisión, como lo demuestran recientemente Suzgun et al. (2019) para los lenguajes Dyck sintéticos. Para algunas dimensiones en el vector de contexto, mostramos que sus activaciones están altamente correlacionadas con la profundidad de las estructuras de frases, como VP y NP. Además, con una regularización L1, también encontramos que puede predecir con precisión si una palabra está dentro de una estructura de frase o no a partir de un pequeño número de componentes del vector de contexto. Incluso para el caso de aprender del texto sin procesar, se muestra que los vectores de contexto se correlacionan bien con las estructuras de las frases. Finalmente, mostramos que los grupos naturales de las palabras funcionales y la parte de los discursos que activan las frases se representan en un subespacio pequeño pero principal del vector de actualización de contexto de LSTM.</abstract_es>
      <abstract_pt>Long Short-Term Memory Recurrent Network (LSTM) é amplamente utilizado e conhecido por capturar dependências sintáticas informativas de longo prazo. No entanto, como tais informações são refletidas em seus vetores internos para texto natural ainda não foi suficientemente investigado. Nós os analisamos aprendendo um modelo de linguagem onde as estruturas sintáticas são dadas implicitamente. Mostramos empiricamente que os vetores de atualização de contexto, ou seja, saídas de portas internas, são aproximadamente quantizados para valores binários ou ternários para ajudar o modelo de linguagem a contar a profundidade de aninhamento com precisão, como Suzgun et al. (2019) mostram recentemente para linguagens sintéticas Dyck. Para algumas dimensões no vetor de contexto, mostramos que suas ativações estão altamente correlacionadas com a profundidade das estruturas de frases, como VP e NP. Além disso, com uma regularização L1, também descobrimos que ela pode prever com precisão se uma palavra está dentro de uma estrutura frasal ou não a partir de um pequeno número de componentes do vetor de contexto. Mesmo para o caso de aprendizado com texto bruto, os vetores de contexto ainda se correlacionam bem com as estruturas de frases. Por fim, mostramos que os agrupamentos naturais das palavras funcionais e a parte das falas que acionam as frases são representados em um pequeno mas principal subespaço do vetor de atualização de contexto do LSTM.</abstract_pt>
      <abstract_ja>長期記憶再帰ニューラルネットワーク（ ＬＳＴＭ ）は、情報的な長期構文依存性を捕捉するために広く使用され、知られている。 しかしながら、そのような情報が自然なテキストの内部ベクトルにどのように反映されるかは、まだ十分に調査されていない。 構文構造が暗黙的に与えられている言語モデルを学習することで、それらを分析します。 文脈更新ベクトル、すなわち内部ゲートの出力は、Suzgun et al .( 2019)が最近合成Dyck言語について示したように、言語モデルが入れ子の深さを正確にカウントするのに役立つように、二進数または三進数値に近い量子化されていることを経験的に示している。 コンテキストベクトルのいくつかの次元について、我々は、それらの活性化が、ＶＰ及びＮＰなどの語句構造の深さと高度に相関していることを示す。 さらに、L 1規則化では、単語がフレーズ構造内にあるかどうかをコンテキストベクトルの少数のコンポーネントから正確に予測できることもわかりました。 生テキストから学習する場合でも、コンテキストベクトルは依然としてフレーズ構造と良好に相関することが示されている。 最後に、機能単語の自然なクラスタと、フレーズを引き起こすスピーチの一部が、LSTMのコンテキスト更新ベクトルの小さいが主なサブスペースで表されることを示します。</abstract_ja>
      <abstract_zh>长短期记递归神经网络(LSTM)为博用,且已知用于获信息性之长句法依赖性。 然此等信息,在自然文本之内载体未尽究也。 吾以一言析之,其句法结构隐式也。 臣等以经验言之,上下文更新向量(即内司所输)近量化为二进制或三元直,以助言语准量嵌套深度,如Suzgun等(2019)近为合成Dyck语所示。 其于上下文向量之维度,明其激活与短语构(如VP与NP)之深相关也。 此外因L1正则化,犹见其从上下文向量少量组件中准测单词在短语结构否。 虽学于始,上下文向量犹关短语构。 最后,我明白聚类发短语词性在 LSTM 上下文更新向量一小而要子空中。</abstract_zh>
      <abstract_hi>Long Short-Term Memory recurrent neural network (LSTM) व्यापक रूप से उपयोग किया जाता है और जानकारीपूर्ण दीर्घकालिक वाक्यात्मक निर्भरताओं को कैप्चर करने के लिए जाना जाता है। हालांकि, प्राकृतिक पाठ के लिए इस तरह की जानकारी अपने आंतरिक वैक्टर में कैसे परिलक्षित होती है, इसकी अभी तक पर्याप्त जांच नहीं की गई है। हम एक भाषा मॉडल सीखकर उनका विश्लेषण करते हैं जहां वाक्यात्मक संरचनाएं निहित रूप से दी जाती हैं। हम अनुभवजन्य रूप से दिखाते हैं कि संदर्भ अद्यतन वैक्टर, यानी आंतरिक गेट्स के आउटपुट, भाषा मॉडल को सटीक रूप से घोंसले के शिकार की गहराई की गणना करने में मदद करने के लिए लगभग बाइनरी या टर्नरी मूल्यों के लिए क्वांटाइज्ड होते हैं, जैसा कि सुज़गुन एट अल (2019) हाल ही में सिंथेटिक डाइक भाषाओं के लिए दिखाते हैं। संदर्भ वेक्टर में कुछ आयामों के लिए, हम दिखाते हैं कि उनके सक्रियण वीपी और एनपी जैसे वाक्यांश संरचनाओं की गहराई के साथ अत्यधिक सहसंबद्ध हैं। इसके अलावा, एक एल 1 नियमितीकरण के साथ, हमने यह भी पाया कि यह सटीक रूप से भविष्यवाणी कर सकता है कि कोई शब्द एक वाक्यांश संरचना के अंदर है या नहीं संदर्भ वेक्टर के घटकों की एक छोटी संख्या से। यहां तक कि कच्चे पाठ से सीखने के मामले के लिए, संदर्भ वैक्टर को अभी भी वाक्यांश संरचनाओं के साथ अच्छी तरह से सहसंबंधित दिखाया गया है। अंत में, हम दिखाते हैं कि कार्यात्मक शब्दों के प्राकृतिक समूहों और भाषणों के हिस्से को ट्रिगर करने वाले भाषणों का हिस्सा जो वाक्यांशों को ट्रिगर करता है, एलएसटीएम के संदर्भ-अपडेट वेक्टर के एक छोटे लेकिन प्रमुख सबस्पेस में दर्शाया जाता है।</abstract_hi>
      <abstract_ga>Úsáidtear go forleathan líonra néaraíoch athfhillteach Cuimhne Gearrthéarmach (LSTM) go forleathan agus is eol é chun spleáchais chomhréire fadtéarmacha faisnéiseacha a ghabháil. Mar sin féin, níl imscrúdú leordhóthanach déanta go fóill ar an gcaoi a léirítear an fhaisnéis sin ina veicteoirí inmheánacha do théacs nádúrtha. Déanaimid anailís orthu trí mhúnla teanga a fhoghlaim ina dtugtar struchtúir chomhréire go hintuigthe. Léirímid go heimpíreach go bhfuil na veicteoirí nuashonraithe comhthéacs, i.e. aschuir na ngeataí inmheánacha, cainníochtaithe a bheag nó a mhór go luachanna dénártha nó trínártha chun cabhrú leis an tsamhail teanga doimhneacht an neadaithe a chomhaireamh go cruinn, mar a dúirt Suzgun et al. (2019) seó le déanaí do theangacha sintéiseacha Dyck. I gcás roinnt toisí sa veicteoir comhthéacs, léirímid go bhfuil a gcuid gníomhachtaí comhghaolaithe go mór le doimhneacht struchtúir frása, mar shampla VP agus NP. Ina theannta sin, le gnáthú L1, fuaireamar amach freisin gur féidir leis a thuar go cruinn an bhfuil focal laistigh de struchtúr frása nó nach bhfuil ó líon beag comhpháirteanna den veicteoir comhthéacs. Fiú i gcás na foghlama ó théacs amh, taispeántar go bhfuil comhghaol maith fós idir veicteoirí comhthéacs agus struchtúir na bhfrása. Ar deireadh, léirímid go ndéantar braislí nádúrtha de na focail fheidhmiúla agus an chuid d’óráidí a spreagann frásaí a léiriú i bhfospás beag ach príomhúil de veicteoir comhthéacs-nuashonraithe LSTM.</abstract_ga>
      <abstract_ka>ძირითადი სიმხმარების რეკურენტი ნეირალური ქსელი (LSTM) widely used and known to capture informative long-term syntactic dependencies. მაგრამ, როგორ ეს ინფორმაცია განსაზღვრებულია ჩემი ინტერქსტური გვექტორებში, როგორ ჩემი ინტერქსტური ტექსტისთვის უკვე არ იყო ჩვენ ისინი ანალიზებთ ენის მოდელის შესწავლებით, სადაც სინტაქტიკური სტრუქტურები უნდა იყოს. ჩვენ ემპერიკურად ჩვენ აჩვენებთ, რომ კონტექსტური განახლებელი გვეკტენტები, მაგალითად ინტერესტური გეკტების გადასვლები, უფრო კვანტიზებულია ბიუნარი ან ტერნარიური მნიშვნელობებისთვის, როგორც სინტეტიკური დასვლების ზოგიერთი განზომილებებისთვის კონტექსტურის გვეკტორიში, ჩვენ ჩვენ ჩვენ ჩვენ ჩვენ ჩვენ ჩვენ ჩვენ ჩვენებთ, რომ მათი აქტივაციები ძალიან კონტექსტურაციების სიმაღლ დამატებით, L1 რეგილარიზაციით, ჩვენ დავიწყეთ, რომ ის შეუძლია წარმოდგინოთ თუ სიტყვა ფრაზის სტრუქტურაციაში არის ან არა კონტექსტურის პატარა კომპონენტების კონ კონტექსტური ტექსტიდან მესწავლის შემთხვევაში, კონტექსტური გვექტორები ჩვენებულია, რომ ფრაზების სტრუქტურებისთვის კონტექსტურა საბოლოოდ, ჩვენ ჩვენ აჩვენებთ, რომ ფუნქციონალური სიტყვების ნაირთი კლასტერი და სიტყვების ნაწილი, რომელიც გამოწყენებს ფრაზების ნაირთი, მაგრამ პირველური სამყაროში LSTM-ის კონ</abstract_ka>
      <abstract_el>Το επαναλαμβανόμενο νευρωνικό δίκτυο μακράς βραχυπρόθεσμης μνήμης (χρησιμοποιείται ευρέως και είναι γνωστό για να συλλάβει ενημερωτικές μακροπρόθεσμες συντακτικές εξαρτήσεις. Ωστόσο, ο τρόπος με τον οποίο οι πληροφορίες αυτές αντικατοπτρίζονται στα εσωτερικά διανύσματά τους για φυσικό κείμενο δεν έχει ακόμη διερευνηθεί επαρκώς. Τους αναλύουμε μαθαίνοντας ένα γλωσσικό μοντέλο όπου οι συντακτικές δομές δίνονται έμμεσα. Δείχνουμε εμπειρικά ότι τα διανύσματα ενημέρωσης περιβάλλοντος, δηλαδή οι εξόδοι των εσωτερικών πυλών, είναι περίπου κβαντισμένα σε δυαδικές ή τριμηνιαίες τιμές για να βοηθήσουν το γλωσσικό μοντέλο να μετρήσει με ακρίβεια το βάθος της φωλιάσματος, όπως δείχνουν πρόσφατα για τις συνθετικές γλώσσες του Ντάικ. Για ορισμένες διαστάσεις στο διάνυσμα περιβάλλοντος, δείχνουμε ότι οι ενεργοποίησές τους συσχετίζονται ιδιαίτερα με το βάθος των δομών φράσεων, όπως η VP και η NP. Επιπλέον, με μια ρύθμιση διαπιστώσαμε επίσης ότι μπορεί να προβλέψει με ακρίβεια αν μια λέξη βρίσκεται μέσα σε μια δομή φράσεων ή όχι από έναν μικρό αριθμό συστατικών του διανυσματικού πλαισίου. Ακόμη και στην περίπτωση της εκμάθησης από ακατέργαστο κείμενο, τα διανύσματα περιβάλλοντος δείχνουν ότι εξακολουθούν να συσχετίζονται καλά με τις δομές φράσεων. Τέλος, δείχνουμε ότι οι φυσικές συστάδες των λειτουργικών λέξεων και του τμήματος των ομιλιών που προκαλούν φράσεις αντιπροσωπεύονται σε ένα μικρό αλλά κύριο υποδιαστημικό διάνυσμα της ενημέρωσης περιβάλλοντος του LSTM.</abstract_el>
      <abstract_it>La rete neurale ricorrente di memoria a breve termine (LSTM) è ampiamente usata e conosciuta per catturare dipendenze sintattiche informative a lungo termine. Tuttavia, il modo in cui tali informazioni si riflettono nei suoi vettori interni per il testo naturale non è ancora stato sufficientemente studiato. Li analizziamo imparando un modello linguistico in cui sono implicitamente date strutture sintattiche. Mostriamo empiricamente che i vettori di aggiornamento del contesto, cioè le uscite di porte interne, sono approssimativamente quantizzati a valori binari o ternari per aiutare il modello linguistico a contare con precisione la profondità del nesting, come dimostrano recentemente Suzgun et al. (2019) per i linguaggi sintetici Dyck. Per alcune dimensioni nel vettore di contesto, mostriamo che le loro attivazioni sono altamente correlate con la profondità delle strutture di frase, come VP e NP. Inoltre, con una regolarizzazione L1, abbiamo anche scoperto che può prevedere con precisione se una parola è all'interno di una struttura di frase o meno da un piccolo numero di componenti del vettore di contesto. Anche nel caso di apprendimento dal testo grezzo, i vettori di contesto sono ancora ben correlati con le strutture delle frasi. Infine, mostriamo che i cluster naturali delle parole funzionali e la parte dei discorsi che innescano le frasi sono rappresentati in un piccolo ma principale sottospazio del vettore di aggiornamento del contesto di LSTM.</abstract_it>
      <abstract_kk>Қысқа уақыт жады қайталанатын невралдық желі (LSTM) көп қолданылады және мәліметтік ұзын уақыт синтактикалық тәуелдіктерді алу үшін беймәлім. Бірақ бұл мәлімет тәуелді мәтін үшін ішкі векторларында қалай көрсетіліп тұрған жоқ. Біз оларды синтактикалық құрылғылар келтірілген тіл үлгісін оқып анализ. Біз импирикалық түрде контексті жаңарту векторлары, т.е. ішкі қапшықтардың шығысы, тіл үлгісін дұрыс рет есептеу үшін, Suzgun et al. (2019) синтетикалық дик тілдер үшін көрсетеді. Контексті векторының кейбір өлшемдері үшін, олардың белсенділіктері, VP мен NP секілді сөздер құрылымының тереңілігімен байланысты деп көрсетеді. Сонымен қатар, L1 үлгіліліктемесімен де ол сөздің құрылымының ішінде немесе контексті векторының кішкентай компоненттерінен емес екенін дұрыс түсіндіре алады. Мәтіннен оқыту үшін де, контексті векторлар сөздер құрылымына әлі жақсы сәйкес келеді. Соңында, біз функциялық сөздердің табиғи кластері және сөздерді бастайтын сөздердің бөлігін LSTM контексті жаңарту векторының кішкентай, бірақ негізгі ішкі жерінде көрсетіледі.</abstract_kk>
      <abstract_hu>A hosszú rövid távú memória visszatérő neurális hálózatot (LSTM) széles körben használják és ismerik az információs hosszú távú szintaktikus függőségek rögzítésére. Azonban még nem vizsgálták megfelelően, hogy az ilyen információk hogyan tükröződnek a természetes szöveg belső vektoraiban. Elemezzük őket olyan nyelvi modell tanulásával, ahol a szintaktikus struktúrákat implicit módon megadjuk. empirikusan megmutatjuk, hogy a kontextusfrissítési vektorok, azaz a belső kapuk kimenetei, megközelítőleg bináris vagy ternáris értékekre vannak kvantizálva, hogy segítsen a nyelvmodell pontosan megszámolni a fészkelés mélységét, amint azt a Suzgun és mások (2019) mutatják a szintetikus Dyck nyelvek esetében. A kontextusvektorban lévő néhány dimenzió esetében megmutatjuk, hogy aktiválásuk nagymértékben korrelálódik a kifejezési struktúrák mélységével, mint például a VP és az NP. Ezenkívül az L1 szabályozással azt is találtuk, hogy pontosan megjósolhatja, hogy egy szó egy kifejezési struktúrában van-e vagy sem a kontextusvektor kis számú összetevőjéből. Még a nyers szövegből való tanulás esetén is kimutatható, hogy a kontextusvektorok még mindig jól korrelálnak a kifejezési struktúrákkal. Végül megmutatjuk, hogy a funkcionális szavak természetes klaszterei és a beszédek kiváltó része az LSTM kontextusfrissítő vektorának kis, de fő szubtérében jelenik meg.</abstract_hu>
      <abstract_mk>Долга краткорочна меморија рецидентната нервна мрежа (LSTM) е широко употребена и позната за заземање информативни долгорочни синтактички зависности. Сепак, како ваквите информации се рефлектираат во своите внатрешни вектори за природен текст сé уште не е доволно истражено. Ние ги анализираме со учење на јазички модел каде што инплицитно се дадени синтактички структури. Емпирички покажуваме дека векторите за контекстно ажурирање, т.е. излезите од внатрешните порти, се околу квантизирани на бинарни или тринарни вредности за да му помогнат на јазичкиот модел да ја брои длабочината на гнездото точно, како што Suzgun et al. (2019) неодамна покажуваат за синтетичките јазици Dyck. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP.  Покрај тоа, со регуларизација на L1, исто така откривме дека може точно да предвиде дали зборот е внатре во фразата структура или не од мал број компоненти на контекстниот вектор. Дури и за случајот на учење од суров текст, се покажува дека контекстните вектори сé уште се корелираат добро со фразните структури. Конечно, покажуваме дека природните групи на функционалните зборови и делот од говоровите кои предизвикуваат фрази се претставени во мал, но главен подпростор на контекстниот вектор на LSTM.</abstract_mk>
      <abstract_lt>Long Short-Term Memory recurrent neural network (LSTM) is widely used and known to capture informative long-term syntactic dependencies.  Vis dėlto, kaip tokia informacija atsispindi jos vidiniuose natūralaus teksto vektoriuose, dar nebuvo pakankamai ištirta. Analizuojame juos mokydami kalbos model į, kuriame numatomai pateikiamos sintaksinės struktūros. Empiriškai rodome, kad konteksto atnaujinimo vektoriai, t. y. vidinių vartų i šėjimai, yra maždaug kiekybiškai išmatuoti į dvišales arba trišales vertes, siekiant padėti kalbos modeliui tiksliai apskaičiuoti nesting gylį, kaip neseniai rodo Suzgun et al. (2019) sintetinių Dyck kalbų atžvilgiu. Kai kuriems konteksto vektoriaus matmenys rodo, kad jų aktyvinimas labai susijęs su frazių struktūrų gyliu, pvz., VP ir NP. Be to, nustatydami L1 reguliavimą, mes taip pat nustatėme, kad jis gali tiksliai nuspėti, ar žodis yra frazės struktūroje, ar ne iš nedidelio kontekstinio vektoriaus komponentų. Net mokymosi iš žaliavinio teksto atveju įrodoma, kad konteksto vektoriai vis dar gerai koreliuoja su frazės struktūromis. Finally, we show that natural clusters of the functional words and the part of speeches that trigger phrases are represented in a small but principal subspace of the context-update vector of LSTM.</abstract_lt>
      <abstract_mt>In-netwerk newrali rikorrenti tal-Memorja fuq medda qasira ta’ żmien twil (LSTM) jintuża b’mod wiesa’ u huwa magħruf li jaqbad dipendenzi sintattiċi informativi fuq medda twila ta’ żmien. Madankollu, kif tali informazzjoni tiġi riflessa fil-vetturi interni tagħha għat-test naturali għadha ma ġietx investigata biżżejjed. Aħna nianalizzawhom billi nitgħallmu mudell lingwistiku fejn jiġu impliċitament mogħtija strutturi sintattiċi. B’mod empiriku nuru li l-vetturi ta’ aġġornament tal-kuntest, jiġifieri l-ħruġ ta’ gates interni, huma kwantifikati bejn wieħed u i e ħor għal valuri binarji jew ternarji biex jgħinu lill-mudell tal-lingwa jgħodd il-fond tal-nidd b’mod preċi ż, kif juru Suzgun et al. (2019) reċentement għal-lingwi sintetiċi Dyck. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP.  Barra minn hekk, b’regolarizzazzjoni L1, sabna wkoll li tista’ tbassar b’mod preċiż jekk kelma tkunx ġewwa struttura ta’ frażi jew le minn numru żgħir ta’ komponenti tal-vettur tal-kuntest. Anki fil-każ tat-tagħlim mit-test mhux ipproċessat, jidher li l-vetturi tal-kuntest għadhom jikkorrelaw tajjeb mal-istrutturi tal-frażi. Fl-a ħħar nett, nuru li raggruppamenti naturali tal-kliem funzjonali u l-parti tad-diskorsi li jiskattaw il-frażijiet huma rappreżentati f’sottospazju żgħir iżda prinċipali tal-vettur ta’ aġġornament tal-kuntest tal-LSTM.</abstract_mt>
      <abstract_ml>നീണ്ട മെമ്മറി ആവര്‍ത്തിച്ചുകൊണ്ടിരിക്കുന്ന നെയൂറല്‍ ശൃംഖല (LSTM) വിശാലമായി ഉപയോഗിക്കുന്നു. വിവരങ്ങള്‍ വിവരങ്ങള്‍ നീണ്ട നീ എങ്കിലും സ്വാഭാവിക വാചകത്തിനായി ഇത്തരം വിവരങ്ങള്‍ എങ്ങനെയാണ് പ്രത്യേകിക്കുന്നതെന്ന് നോക്കിയിട്ടുള്ളത്. ഒരു ഭാഷ മോഡല്‍ പഠിപ്പിക്കുന്നത് കൊണ്ട് നമ്മള്‍ അവരെ അന്വേഷിക്കുന്നു. അവിടെ സിനിട്ടാക്റ്റിക്ക് കൂ നമ്മള്‍ ശാസ്ത്രികമായി കാണിക്കുന്നു സുസ്സുന്‍ എറ്റ് അല്‍. (2019) അടുത്തുതന്നെ സിന്തെറ്റിക്ക് ഭാഷയുടെ ആഴത്തിന്റെ ആഴത്തെ എണ്ണുന്നതിന് സഹായിക്കാന്‍ വെക്സ്റ്റര്‍ വെക്റ്റര്‍ ആഴ വെക്സ്റ്റെക്റ്റരിലെ ചില ഭാഗങ്ങള്‍ക്ക്, നമ്മള്‍ കാണിക്കുന്നു, അവരുടെ പ്രവര്‍ത്തനങ്ങള്‍ വാക്കുകളുടെ ആഴത്തിന്റെ ആഴത്തില്‍ വളരെ ബന ഒരു L1 ന്റെ നിയന്ത്രണം കൊണ്ട്, ഒരു വാക്ക് ഒരു വാക്കിന്റെ അടിസ്ഥാനത്തിലുണ്ടോ അല്ലെങ്കില്‍ വെക്സ്റ്റോറിന്റെ ചെറിയ എണ്ണം ഭാഗങ്ങളില്‍ ന ചുരുക്കുന്ന പദാവലിയില്‍ നിന്നും പഠിക്കുന്നതിനായി പോലും വെക്സ്റ്റെന്റ് വെക്റ്ററുകള്‍ വാക്കുകളുടെ അടിസ് അവസാനം, നമ്മള്‍ കാണിച്ചുകൊടുക്കുന്നത് പ്രകൃതിക വാക്കുകളുടെ സ്വാഭാവികമായ ക്ലസ്റ്ററുകളും, വാക്കുകള്‍ തുടങ്ങുന്ന വാക്കുകളുടെ ഭാഗവും എള്‍സ്റ്</abstract_ml>
      <abstract_ms>Rangkaian saraf berkurang ingatan jangka pendek panjang (LSTM) digunakan secara luas dan diketahui untuk menangkap dependensi sintaktik jangka panjang maklumat. Bagaimanapun, bagaimana maklumat tersebut diselarang dalam vektor dalamnya untuk teks semulajadi belum diselesaikan cukup. Kami menganalisis mereka dengan mempelajari model bahasa di mana struktur sintaktik secara implicit diberikan. Kami empirik menunjukkan bahawa vektor kemaskini konteks, iaitu output gerbang dalaman, sekitar dikwantifikasikan kepada nilai binari atau ternary untuk membantu model bahasa menghitung kedalaman sarang dengan tepat, seperti Suzgun et al. (2019) baru-baru ini menunjukkan untuk bahasa Dyck sintetik. Untuk beberapa dimensi dalam vektor konteks, kita menunjukkan bahawa aktivasi mereka sangat berkorelaci dengan kedalaman struktur frasa, seperti VP dan NP. Moreover, with an L1 regularization, we also found that it can accurately predict whether a word is inside a phrase structure or not from a small number of components of the context vector.  Walaupun untuk kes belajar dari teks mentah, vektor konteks dipaparkan masih berkorelasi dengan struktur frasa. Akhirnya, kita tunjukkan bahawa kumpulan alami perkataan berfungsi dan bahagian ucapan yang memicu frasa adalah mewakili dalam subspace kecil tetapi utama vektor kemaskini-konteks LSTM.</abstract_ms>
      <abstract_no>Lang kortmann minne gjentakaste neuralnettverk (LSTM) er breidd brukt og kjent for å henta informativ langsiktige syntaksiske avhengighet. Men korleis slike informasjon er reflektert i sine interne vektorar for naturtekst enno ikkje er nok undersøkt. Vi analyserer dei ved å lære eit språk-modell der syntaktiske strukturar er implisitt gitt. Vi viser empirisk at kontekstoppdateringsvectorane, dvs. utdata av interne portar, er omtrent kvantiserte til binære eller ternare verdiar for å hjelpa språk-modellen til å telja dybde på nestinga nøyaktig, som Suzgun et al. (2019) nyleg viser for syntetiske dykkspråk. For nokre dimensjonar i kontekstvektoren viser vi at aktivasjonane sine er svært korrelaterte med dybde på frasestrukturar, som VP og NP. I tillegg har vi også funne at det kan nøyaktig foregå om eit ord er inne i eit fråstruktur eller ikkje frå ein liten tal komponentar i kontekstvektoren. I tillegg til å lære frå råtekst, vert kontekstvektorane viste til å fortsatt korrelasjona godt med frasenstrukturene. I slutt viser vi at naturlige grupper av funksjonelle ord og delen av språk som utløyser frasar er representert i ein liten, men hovudplass av den kontekstoppdaterte vektoren av LSTM.</abstract_no>
      <abstract_ro>Rețeaua neurală recurentă de memorie pe termen scurt (LSTM) este utilizată pe scară largă și cunoscută pentru a capta dependențe sintactice informative pe termen lung. Cu toate acestea, modul în care aceste informații sunt reflectate în vectorii interni ai textului natural nu a fost încă investigat suficient. Le analizăm prin învățarea unui model lingvistic în care structurile sintactice sunt date implicit. Aratăm empiric că vectorii actualizării contextului, adică ieșirile porților interne, sunt aproximativ cuantificați la valori binare sau ternare pentru a ajuta modelul limbajului să numere cu precizie adâncimea cuibării, așa cum arată Suzgun et al. (2019) recent pentru limbile Dyck sintetice. Pentru unele dimensiuni din vectorul contextual, arătăm că activările lor sunt foarte corelate cu adâncimea structurilor de fraze, cum ar fi VP și NP. Mai mult decât atât, cu o regularizare L1, am descoperit, de asemenea, că poate prezice cu exactitate dacă un cuvânt se află în interiorul unei structuri de frază sau nu dintr-un număr mic de componente ale vectorului contextual. Chiar și în cazul învățării din text brut, vectorii contextului sunt încă corelați bine cu structurile frazelor. În cele din urmă, arătăm că grupurile naturale de cuvinte funcționale și partea de discursuri care declanșează fraze sunt reprezentate într-un subspațiu mic, dar principal al vectorului de actualizare a contextului LSTM.</abstract_ro>
      <abstract_si>ලොකු කොටි වාර්තාවක් මතකය ආපහු ප්‍රතික්‍රීය න්‍යූරල් ජාලය (LSTM) භාවිත වෙනවා සහ තොරතුරු ලොකු වාර්තාවක් සංවි නමුත්, මේ තොරතුරු කොහොමද ස්වභාවික පාළුවක් සඳහා ඇතුළු වෙක්ටර් වලට ප්‍රතිකෘති වෙන්නේ. අපි ඔවුන්ව විශ්ලේෂ කරනවා භාෂාවක් මොඩල් ඉගෙන ගන්න කියලා, කියලා සංවිධානය සංවිධාන අපි පෙන්වන්නේ සාමාන්‍ය වෙක්ටර් අවස්ථාව, ඉතින් ඇතුළු ගේට්ටුවන් අවස්ථාව, බායිනාරි නැත්තර් අවස්ථාවක් සඳහා භාෂාව මදුල්යට උදව් කරන්න, සුස්ගුන් ට් ල්  සම්බන්ධ වෙක්ටර් වලින් කිසිම පරීක්ෂණයක් වෙනුවෙන්, අපි පෙන්වන්නේ ඔවුන්ගේ සක්‍රියාවන් ගොඩක් සම්බන්ධ වෙනවා ක එතකොට, L1 සාමාන්‍ය විස්තරයක් සමග, අපි හොයාගත්තා ඒක හරියට පුළුවන් වචනයක් ප්‍රශ්නයක් සංවිධානයක් ඇතුලේ තියෙන්නේ නැ පිළිබඳින් ඉගෙන ඉගෙන ගන්න ප්‍රතිකාරය සමහර වෙක්ටර්ස් පෙන්වන්න පුළුවන්. අන්තිමේදි, අපි පෙන්වන්නේ වැඩ වචනයේ ස්වභාවික කොටස් සහ ප්‍රතිකාරිය වචනයේ කොටස් සහ ප්‍රතිකාරිය වචනයේ ප්‍රතිකාරිත වෙක්ටර් LSTM ග</abstract_si>
      <abstract_mn>Урт богино-Term Memory recurrent neural network (LSTM) нь мэдээллийн урт хугацааны синтактик хамааралтай хамааралтай байдлыг авах болон мэддэг. Гэхдээ энэ мэдээлэл байгалийн текст дээр хэрхэн доторх векторууд харагдаж байгааг харуулж чадахгүй. Бид тэднийг хэл загвар суралцаж шинжлэх ухаан өгдөг. Бид дотоод нутгийн газрын шинэчлэлүүд нь хэлний загварын гүн гүнзгийг тодорхойлох тулд ойролцоогоор хоёр эсвэл үеийн утгыг тооцоолж сузгуун et al. (2019) саяхан Синтетик Дик хэлний тухай харуулж байна. Төвчтөн векторын зарим хэмжээсүүдийн тулд бид тэдний үйлдвэрлэлүүдийг VP болон NP зэрэг хэлбэрийн гүн гүнзгий холбоотой гэдгийг харуулж байна. Түүнчлэн, L1-н шугам хэлбэрээр бид мөн тодорхой хэлбэрээр үг хэлбэрийн бүтэц дотор байгааг эсвэл тодорхой хэлбэрээс биш гэдгийг ойлгосон. Хөгжин текстээс суралцах тохиолдолд хүртэл контекст векторууд хэлэлцээний бүтэцтэй холбоотой байдаг. Эцэст нь бид функцийн үгийн байгалийн кластерууд болон өгүүлбэрүүдийн нэг хэсэг нь LSTM-ын контекст жагсаалттай векторын жижиг гэхдээ үндсэн суурь зайд илэрхийлэгддэг.</abstract_mn>
      <abstract_so>Shabakadda neurada ee soo socda ee xasuusta waqtiga dheer (LSTM) waxaa loo isticmaalaa oo loo yaqaan inuu qabsado macluumaad ku saabsan xirfadaha la xiriira waqtiga dheer. Si kastaba ha ahaatee sida macluumaadkaas looga fiiriyo wadooyinkooda gudaha ah ee qoraalka dabiicadda ah weli looma baahan yahay si kugu filan. Waxaannu ku baaraynaa barashada model luuqada, taasoo lagu siiyo dhismaha muusikada si aan waxtarla’aan ah. Waxaynu si fudud ugu muujinnaa in wadooyinka cusboonaysiinta irdaha gudaha ah ay qiyaastii qiimaha labaad ama beeraha dhexe lagu qiimeeyaa in lagu caawiyo modelka afka si saxda ah u tiriyo moolka guriga, sida Suzgun et al. (2019) ugu dhowaad u muujiyo luqada syntettika Dykk. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP.  Sidoo kale waxaynu aragnay in uu si sax ah u sii sheegi karo in ereygu uu ku jiro dhismaha hadalka ama in uu ka yimaado qeybo yar oo ka mid ah vectorka hoose. Xataa haddii aad wax ka baraneyso qoraalka saxda ah, waxaa sidoo kale la muujiyaa wado qalabka kooxaha ah oo ay ku xiriiraan dhismaha afka. Ugu dambaysta, waxaynu muujinnaa kooxo asalka ah ee hadallada waxqabadka iyo qayb ka mid ah hadallada ay ka soo baxaan waxay ka muuqan yihiin mid yar ee hoos-hoose ee ka mid ah vector-updates of LSTM.</abstract_so>
      <abstract_sv>Long Short-Term Memory återkommande neuralt nätverk (LSTM) används ofta och är känt för att fånga informativa långsiktiga syntaktiska beroenden. Hur sådan information återspeglas i dess interna vektorer för naturtext har dock ännu inte undersökts tillräckligt. Vi analyserar dem genom att lära oss en språkmodell där syntaktiska strukturer ges underförstått. Vi visar empiriskt att kontextuppdateringsvektorerna, dvs utgångar av interna portar, är ungefär kvantiserade till binära eller ternära värden för att hjälpa språkmodellen att räkna djupet i häckningen korrekt, vilket Suzgun et al. (2019) nyligen visar för syntetiska Dyck språk. För vissa dimensioner i kontextvektorn visar vi att deras aktiveringar är starkt korrelerade med djupet i frasstrukturer, såsom VP och NP. Dessutom, med en L1 regularisering, fann vi också att det exakt kan förutsäga om ett ord är inuti en frasstruktur eller inte från ett litet antal komponenter i sammanhangsvektorn. Även vid inlärning av obehandlad text, har kontextvektorer visat sig fortfarande korrelera bra med frasstrukturerna. Slutligen visar vi att naturliga kluster av de funktionella orden och den del av tal som utlöser fraser representeras i ett litet men huvudområde av kontextuppdateringsvektorn LSTM.</abstract_sv>
      <abstract_pl>Rekrutująca sieć neuronowa z pamięci krótkoterminowej (LSTM) jest szeroko stosowana i znana z przechwytywania informacyjnych długoterminowych zależności składni. Nie zostało jednak jeszcze wystarczająco zbadane, w jaki sposób informacje te znajdują odzwierciedlenie w wewnętrznych wektorach tekstu naturalnego. Analizujemy je poprzez uczenie się modelu językowego, w którym struktury składni są domyślnie podane. Empirycznie pokazujemy, że wektory aktualizacji kontekstu, tj. wyjścia bram wewnętrznych, są w przybliżeniu kwantyzowane do wartości binarnych lub trójstronnych, aby pomóc modelowi językowi dokładnie liczyć głębokość zagnieżdżenia, jak pokazują ostatnio Suzgun i wszyscy. (2019) dla syntetycznych języków Dycka. Dla niektórych wymiarów wektora kontekstowego pokazujemy, że ich aktywacje są wysoce skorelowane z głębokością struktur fraz, takich jak VP i NP. Ponadto, przy regularyzacji L1, stwierdziliśmy również, że może ona dokładnie przewidzieć, czy słowo znajduje się wewnątrz struktury fraz, czy nie z niewielkiej liczby składników wektora kontekstu. Nawet w przypadku uczenia się z tekstu surowego wektory kontekstowe nadal dobrze korelują ze strukturami fraz. Na koniec pokazujemy, że naturalne gromady słów funkcjonalnych i części wypowiedzi, które wywołują frazy, są reprezentowane w małej, ale głównej podprzestrzeni wektora aktualizacji kontekstu LSTM.</abstract_pl>
      <abstract_ta>நீண்ட குறுக்க- முறை நினைவகம் திரும்ப நெருக்கிய வலைப்பின்னல் (LSTM) விரிவாக பயன்படுத்தப்பட்டுள்ளது மற்றும் அறியப்பட்டுள்ளது தக ஆயினும், அதன் உள்ளார்ந்த நெறிகளில் எவ்வாறு இவ்வாறு தகவல் பிரதிபலிக்கப்படுகிறது என்றாலும் இயல்பான உரைக்கு போதுமா நாம் ஒரு மொழி மாதிரியை கற்றுக் கொண்டு அவர்களை ஆராய்ச்சி செய்கிறோம் அதில் ஒத்திசைவு அமைப்பு நாம் விருப்பமாக காட்டுகிறோம் சூசன் மற்றும் அல்லது உள் வாதில்களின் புதுப்பித்தல் வெளியீடுகள் சுக்சுன் டிக் மொழிகளின் ஆழத்தை சரியாக எண்ண உதவும் மொழி மாதிரி மதிப்புகளுக்கு சரிய சில பரிமாணங்களுக்கு, விபி மற்றும் NP போன்ற சொற்றொடர் அமைப்புகளின் ஆழத்தில் அவர்கள் செயல்பாடுகள் மிகவும் சார்ந்திருக்கிறது என மேலும், ஒரு L1 கட்டுப்பாடுடன், அது ஒரு சொற்றொடர் கட்டுப்பாட்டிற்கு உள்ளே இருக்கிறதா அல்லது சூழல் நெறியின் சிறிய எண்ணிக்கையில் இருந்து அல் குறைந்த உரையிலிருந்து கற்றுக் கூட, சூழல் நெறிகள் இன்னும் சொற்றொடர் அமைப்புகளுடன் நன்றாக இணைக்க காட்டும். இறுதியில், செயல்பாடுகளின் இயல்பான வார்த்தைகள் மற்றும் வார்த்தைகளின் பகுதி என்பதை நாம் காட்டுகிறோம் LSTM-ன் சூழல் புதுப்பித்தல் நெறியின்</abstract_ta>
      <abstract_sr>Dugo kratkoročna memorija ponovna neuralna mreža (LSTM) se široko koristi i poznaje kako bi uhvatili informativne dugoročne sintaktične zavisnosti. Međutim, kako se takve informacije odražavaju u svojim unutrašnjim vektorima za prirodni tekst još nisu dovoljno istražene. Analiziramo ih učeći jezički model gde se sintaktičke strukture implicitno daju. Mi empirički pokazujemo da su vektori za aktualizaciju konteksta, tj. izlazi unutrašnjih vrata, približno kvantizirani na binarne ili ternarne vrijednosti da pomognu jezičkom modelu da računa dubinu gnijezda tačno, kao što je Suzgun et al. (2019) nedavno pokazuju za sintetičke jezike Dycka. Za neke dimenzije u kontekstskom vektoru pokazujemo da su njihove aktivacije veoma povezani sa dubinom frazu strukture, poput VP i NP. Osim toga, sa regularizacijom L1, takođe smo otkrili da može precizno predvidjeti da li je reč unutar strukture fraze ili ne iz malih broja komponenata kontekstnog vektora. Čak i za slučaj učenja sa sirovog teksta, kontekstski vektori se pokazuju da se i dalje dobro povezuju sa strukturama fraze. Konačno, pokazujemo da su prirodni skupini funkcionalnih reči i deo govora koje okidaju rečenice predstavljeni u malom, ali glavnom podprostoru vektora LSTM-a za aktualizaciju konteksta.</abstract_sr>
      <abstract_ur>لنگ- ٹریم میموروی دوبارہ نیورل نیٹورک (LSTM) widely used and known to capture informative long-term syntactic dependencies. لیکن، یہ معلومات اس کے اندر کی ویکتروں میں کس طرح تحقیق نہیں کی گئی ہے کہ طبیعی متن کے لئے۔ ہم ان کو ایک زبان مدل سکھاتے ہیں جہاں سینٹاکٹیک ساختاریں معلوم ہوتی ہیں۔ ہم عمدہ طور پر دکھاتے ہیں کہ کنٹکس اوڈیٹ ویکتور، یعنی داخلی دروازے کے نتائج، تقریباً دوئناری یا ترنری ارزش کے مطابق دوئناری یا ترنری ارزش کی مدد کرنے کے لئے زبان موڈل کے مطابق مضبوط طریقہ کے مطابق، سوزگن et al. (2019) اچھے وقت سینٹیٹیک ڈیک زبان ہم ان کی فعالیت کو دکھاتے ہیں کہ ان کی فعالیت بالکل تعلق کی جڑ کی عمیق بنائی جاتی ہے جیسے وی پی اور ان پی۔ اور ہم نے ایک ل1 قانونی کے ساتھ بھی پایا کہ یہ ٹھیک طور پر پیش بینی کر سکتا ہے کہ ایک لفظ ساختار کے اندر ہے یا ایک چھوٹی تعداد سے نہیں ہے۔ Even for the case of learning from raw text, context vectors are shown to still be well correlated with the phrase structures. آخر میں، ہم دکھاتے ہیں کہ فعال کلمات کی طبیعی کلسٹر اور کلمات کی حصہ جو ٹریگر کلمات کو ایک چھوٹی لیکن اصلی زیر فضا میں LSTM کے کنٹکس اوڈڈیٹ ویکتور کی بنیادی جاتی ہیں.</abstract_ur>
      <abstract_uz>@ info: whatsthis Lekin, bu maʼlumotning ichki vectorlarida qanday ko'rsatilgan narsa asl matn uchun juda yetarli qidirilmaydi. Biz ularni tillar modelini o'rganish bilan analyzeriz, bu yerda syntactik tuzuvlari muvaffaqiyatli emas. Biz oddiy ko'rsatganimiz, ichki portlarning qiymatlari ikki yoki ternariy qiymatlariga qiymatlar qiymatni aniqlash mumkin, Suzgun et al (2019) yaqinda birinchi darajada bir qiymatni ko'rsatish mumkin. Ko'pchilik vektordagi bir necha shakllar uchun biz ularning amallari VP va NP kabi imkoniyatlarining yuqoriga juda bog'liq. Keyin, L1 boshqaruvi bilan biz bu so'zning bir so'z tuzuvning ichida yoki context vektorining kichkina qismlaridan emas deb hisoblash mumkin. Even for the case of learning from raw text, context vectors are shown to still correlate well with the phrase structures.  Oxirgi, biz ishlayotgan so'zlarning tabiiy soʻzlar va so'zlarni ishga tushirishga ega bo'lgan so'zlarning qismi LSTM'ning context-update vektorlarining kichkina, balki asosiy subspati.</abstract_uz>
      <abstract_vi>Mạng thần kinh tái tạo của Ký Ức Dài Hạn (LSTM) được sử dụng rộng và được biết đến để nắm giữ các quan hệ pháp thuật lâu dài thông tin. Tuy nhiên, cách mà thông tin đó được phản ánh trong cơ quan nội bộ của nó về văn bản tự nhiên vẫn chưa được nghiên cứu đủ. Chúng tôi phân tích chúng bằng cách học một mô hình ngôn ngữ nơi cấu trúc cú pháp ẩn chứa. Chúng tôi có kinh nghiệm cho thấy các véc- tơ cập nhật ngữ cảnh, ví dụ các xuất bản của cổng nội tạng, có độ lượng xấp xỉ đến các giá trị nhị phân hoặc tế bào để giúp mô hình ngôn ngữ đo độ sâu của ấp giữ chính xác, như Suzgun et al. (209) gần đây cho thấy về ngôn ngữ nhuộm tổng hợp. Đối với một số kích thước trong véc- tơ ngữ cảnh, chúng tôi cho thấy kích hoạt của chúng có mối quan hệ chặt chẽ với độ sâu của cấu trúc cụm từ, như Phó Tổng thống và NPR. Hơn nữa, với một chỉnh sửa L1, chúng tôi cũng tìm ra rằng nó có thể dự đoán chính xác nếu một từ nằm trong một cấu trúc cụm từ hay không từ một số nhỏ các thành phần của véc- tơ ngữ cảnh. Thậm chí trong trường hợp học từ văn bản nguyên bản, các véc- tơ trường hợp vẫn liên kết tốt với cấu trúc từ điển thành ngữ. Cuối cùng, chúng tôi cho thấy các cụm số tự nhiên của các từ chức năng và phần của các bài phát biểu được đại diện cho dạng chữ kích hoạt nhỏ nhưng chính của véc- tơ cập nhật ngữ cảnh của HTTM.</abstract_vi>
      <abstract_da>Langtidshukommelse tilbagevendende neurale netværk (LSTM) er meget udbredt og kendt til at fange informative langsigtede syntaktiske afhængigheder. Men hvordan sådanne oplysninger afspejles i dens interne vektorer for naturlig tekst er endnu ikke blevet tilstrækkeligt undersøgt. Vi analyserer dem ved at lære en sprogmodel, hvor syntaktiske strukturer implicit gives. Vi viser empirisk, at kontekst opdateringsvektorer, dvs. udgange af interne porte, er omtrent kvantiseret til binære eller ternære værdier for at hjælpe sprogmodellen til at tælle dybden af indlejring nøjagtigt, som Suzgun et al. (2019) for nylig viser for syntetiske Dyck sprog. For nogle dimensioner i kontekstvektoren viser vi, at deres aktiveringer er stærkt korreleret med dybden af sætningsstrukturer, såsom VP og NP. Desuden, med en L1 regulering, fandt vi også, at det nøjagtigt kan forudsige, om et ord er inde i en sætningsstruktur eller ej fra et lille antal komponenter i kontekstvektoren. Selv i tilfælde af at lære af rå tekst, vises kontekstvektorer stadig at korrelere godt med sætningsstrukturerne. Endelig viser vi, at naturlige klynger af de funktionelle ord og den del af taler, der udløser sætninger, er repræsenteret i et lille, men vigtigste underrum af kontekst-opdatering vektor LSTM.</abstract_da>
      <abstract_bg>Дългосрочната краткосрочна памет рецидивираща невронна мрежа (ЛСТМ) е широко използвана и известна с улавянето на информативни дългосрочни синтактични зависимости. Въпреки това, как тази информация се отразява в нейните вътрешни вектори за естествен текст все още не е достатъчно проучена. Анализираме ги чрез изучаване на езиков модел, в който синтактичните структури са имплицитно дадени. Емпирично показваме, че векторите за актуализация на контекста, т.е. изходите на вътрешните портали, са приблизително количествени до двоични или тристранни стойности, за да помогнат на езиковия модел да преброи точно дълбочината на гнездене, както показват наскоро за синтетичните езици на Дик. За някои измерения в контекстния вектор показваме, че техните активирания са силно корелирани с дълбочината на фразовите структури като VP и NP. Освен това, с регулировка открихме, че тя може точно да предскаже дали дадена дума е в структурата на фразата или не от малък брой компоненти на контекстния вектор. Дори в случая на учене от суров текст, контекстните вектори все още корелират добре със структурите на фразите. Накрая, показваме, че естествените клъстери от функционалните думи и частта от речите, която задейства фразите, са представени в малко, но основно подпространство на вектора за актуализация на контекста на ЛСТМ.</abstract_bg>
      <abstract_hr>Kratkoročna sjećanja ponovne neuralne mreže (LSTM) se široko koristi i poznaje kako bi uhvatili informativne dugoročne sintaktične zavisnosti. Međutim, kako se takve informacije odražavaju u svojim unutrašnjim vektorima prirodnog teksta još nisu dovoljno istražene. Analiziramo ih učeći jezički model gdje se sintaktičke strukture implicitno daju. Mi empirički pokazujemo da su vektori za aktualiziranje konteksta, tj. ishod unutrašnjih vrata, približno kvantizirani na binarne ili ternarne vrijednosti kako bi pomogli jezičkom modelu da precizno broji dubinu gnijezda, kao što je Suzgun et al. (2019) nedavno pokazuju za sintetičke jezike. Za neke dimenzije u kontekstskom vektoru pokazujemo da su njihove aktivacije visoko povezani s dubinom frazu strukture, poput VP i NP. Osim toga, s regularizacijom L1, također smo otkrili da može precizno predvidjeti da li je riječ unutar strukture fraze ili ne iz malog broja komponenta kontekstnog vektora. Čak i za slučaj učenja sa sirovog teksta, kontekstski vektori se pokazuju da se i dalje dobro povezuju sa strukturama fraza. Napokon, pokazujemo da su prirodni skupini funkcionalnih riječi i dio govora koje okidaju fraze predstavljeni u malom, ali glavnom podprostoru kontekstnog aktualnog vektora LSTM-a.</abstract_hr>
      <abstract_de>LSTM (Long-Short-Term Memory Recidiviert Neuronal Network) ist weit verbreitet und dafür bekannt, langfristige syntaktische Abhängigkeiten zu erfassen. Wie sich solche Informationen in ihren internen Vektoren für Naturtext widerspiegeln, ist jedoch noch nicht ausreichend untersucht worden. Wir analysieren sie, indem wir ein Sprachmodell lernen, in dem syntaktische Strukturen implizit gegeben sind. Wir zeigen empirisch, dass die Kontextaktualisierungsvektoren, d.h. Ausgänge interner Gates, ungefähr auf binäre oder ternäre Werte quantisiert sind, um dem Sprachmodell zu helfen, die Tiefe der Verschachtelung genau zu zählen, wie Suzgun et al. (2019) kürzlich für synthetische Dyck-Sprachen zeigen. Für einige Dimensionen im Kontextvektor zeigen wir, dass ihre Aktivierungen stark mit der Tiefe von Phrasenstrukturen korrelieren, wie VP und NP. Darüber hinaus fanden wir mit einer L1-Regularisierung auch heraus, dass es aus einer kleinen Anzahl von Komponenten des Kontextvektors genau vorhersagen kann, ob sich ein Wort innerhalb einer Phrasenstruktur befindet oder nicht. Selbst beim Lernen aus Rohtext korrelieren Kontextvektoren noch gut mit den Phrasenstrukturen. Schließlich zeigen wir, dass natürliche Cluster der funktionalen Wörter und des Teils der Reden, die Phrasen auslösen, in einem kleinen, aber prinzipiellen Subraum des Kontext-Update-Vektors von LSTM dargestellt werden.</abstract_de>
      <abstract_id>Jaringan saraf recurrent Memori Panjang Term (LSTM) sangat digunakan dan dikenal untuk menangkap dependensi sintaksi informatif jangka panjang. Namun, bagaimana informasi tersebut terrefleksi dalam vektor dalamnya untuk teks alami belum cukup diselidiki. Kami menganalisis mereka dengan mempelajari model bahasa di mana struktur sintaksi secara implicit diberikan. Kami empiris menunjukkan bahwa vektor pembaruan konteks, i.e. output dari gerbang interna, sekitar kuantisasi ke nilai binar atau ternar untuk membantu model bahasa menghitung kedalaman sarang dengan akurat, seperti Suzgun et al. (2019) baru-baru ini menunjukkan untuk bahasa Dyck sintetik. Untuk beberapa dimensi dalam vektor konteks, kita menunjukkan bahwa aktivasi mereka sangat terkait dengan kedalaman struktur frasa, seperti VP dan NP. Selain itu, dengan regularisasi L1, kami juga menemukan bahwa dapat memprediksi dengan akurat apakah kata berada dalam struktur frasa atau tidak dari sejumlah kecil komponen vektor konteks. Even for the case of learning from raw text, context vectors are shown to still correlate well with the phrase structures.  Akhirnya, kami menunjukkan bahwa kumpulan alami dari kata-kata fungsional dan bagian dari pidato yang memicu frasa adalah mewakili dalam subspace kecil tetapi utama vektor konteks-update LSTM.</abstract_id>
      <abstract_nl>Long Short-Term Memory Recidiviering Neural Network (LSTM) wordt veel gebruikt en staat erom bekend informatieve syntactische afhankelijkheden op lange termijn vast te leggen. Hoe dergelijke informatie wordt weerspiegeld in de interne vectoren voor natuurlijke tekst is echter nog niet voldoende onderzocht. We analyseren ze door een taalmodel te leren waarbij syntactische structuren impliciet gegeven zijn. We tonen empirisch aan dat de context update vectoren, d.w.z. outputs van interne gates, ongeveer gekwantiseerd zijn tot binaire of ternaire waarden om het taalmodel te helpen de diepte van nesting nauwkeurig te tellen, zoals Suzgun et al. (2019) onlangs laten zien voor synthetische Dyck talen. Voor sommige dimensies in de contextvector laten we zien dat hun activeringen sterk gecorreleerd zijn met de diepte van frasestructuren, zoals VP en NP. Bovendien hebben we met een L1 regularisatie ook ontdekt dat het nauwkeurig kan voorspellen of een woord binnen een frasestructuur zit of niet van een klein aantal componenten van de contextvector. Zelfs in het geval van leren van ruwe tekst, wordt aangetoond dat contextvectoren nog steeds goed correleren met de frasestructuren. Tot slot laten we zien dat natuurlijke clusters van de functionele woorden en het deel van toespraken die zinnen triggeren vertegenwoordigd zijn in een kleine maar belangrijkste subruimte van de context-update vector van LSTM.</abstract_nl>
      <abstract_ko>장-단기기억순환신경망(LSTM)은 장기 문법 의존 정보를 얻는 데 광범위하게 응용된다.그러나 이러한 정보가 자연 텍스트의 내부 벡터에 어떻게 반영되는지는 아직 충분한 연구를 받지 못했다.우리는 은식으로 문법 구조를 제시하는 언어 모델을 배워서 그것들을 분석한다.우리의 경험에 의하면 상하문 업데이트 벡터, 즉 내부 부서의 출력은 근사량화되어 2원 또는 3원 값으로 양화되어 언어 모델이 삽입 깊이를 정확하게 계산하는 데 도움을 준다. Suzgun 등(2019)이 최근에 합성Dyck 언어에 한 것처럼.언어 환경 벡터의 일부 차원에 대해 우리는 그것들의 활성화가 단어 구조의 깊이와 관련이 있다는 것을 발견했다. 예를 들어 VP와 NP이다.또한 L1의 정규화를 통해 우리는 상하문에서 양의 소량의 분량으로 단어가 단어 구조에 있는지 정확하게 예측할 수 있음을 발견했다.비록 원시 텍스트에서 배운 경우에도 상하문 벡터는 여전히 단어 구조와 매우 좋은 관련성을 가진다.마지막으로 우리는 기능어의 자연 집합과 촉발 단어의 음성 부분이 LSTM 상하문 업데이트 벡터의 작지만 주요한 하위 공간에서 표시된다는 것을 증명했다.</abstract_ko>
      <abstract_fa>شبکه عصبی (LSTM) دوباره حافظه کوتاه مدت طولانی استفاده می‌شود و برای گرفتن وابستگی‌های سنتاکتیک طولانی اطلاعات شناخته می‌شود. با این حال، چگونه این اطلاعات در ویکتورهای داخلی آن برای متن طبیعی به اندازه کافی تحقیق نشده است. ما آنها را با یاد گرفتن مدل زبانی تحلیل می‌کنیم که ساختارهای سنتاکتیک به طور معنی داده می‌شوند. ما به صورت عمومی نشان می دهیم که ویکتورهای آگاهی محیط، یعنی نتیجه دروازه داخلی، تقریباً به ارزش دویینی یا ترنری کمک به مدل زبان برای شماره عمیق آگاهی دقیقا، به عنوان سوزگان et al. (2019) اخیرا برای زبانهای سینتاتیک دیک نشان می دهند. برای بعضی اندازه‌ها در ویکتور محیط، نشان می‌دهیم که فعالیت‌های آنها با عمیق ساختارهای عبارت، مثل VP و NP بسیار ارتباط دارند. علاوه بر این، با یک تنظیم L1، همچنین فهمیدیم که می تواند دقیقا پیش بینی کند که آیا یک کلمه داخل یک ساختار عبارت است یا نه از تعداد اندکی از بخش‌های ویکتور محیط است. حتی برای یادگیری از متن خالی، ویکتورهای محیط هنوز با ساختارهای عبارت به خوبی ارتباط دارند. بالاخره، ما نشان می دهیم که کلاس طبیعی از کلمات عملکرد و بخشی از سخنرانی که کلمات ماجرا می کنند در یک فضای کوچک ولی اصلی از ویکتور آغاز کنترل LSTM نشان می دهند.</abstract_fa>
      <abstract_sw>Mtandao wa neura unaoendelea kukumbuka muda mrefu wa muda mrefu (LSTM) unatumiwa sana na unajulikana kukutana na kutegemea matumaini ya muda mrefu ya kukutana taarifa. Hata hivyo, namna taarifa hizi zinavyoonekana katika vectors za ndani kwa ajili ya maandishi ya asili bado haijachunguzwa vizuri. Tunawachambua kwa kujifunza muundo wa lugha ambapo muundo wa ushirikiano usio na maana. Tunaonyesha kwa makini kuwa vectors mpya za muktadha, yaani matokeo ya milango ya ndani, zinakadiriwa kwa kiasi cha thamani za binarie au za mwisho ili kusaidia muonekano wa lugha kuhesabu kina kizuizi cha makazi, kama Suzgun et al. (2019) hivi karibuni unaonyesha lugha za Dyck. Kwa baadhi ya utofauti katika vector za muktadha, tunaonyesha kwamba vitendo vyao vimeunganishwa sana na kina mfumo wa msemo, kama vile VP na NP. Moreover, with an L1 regularization, we also found that it can accurately predict whether a word is inside a phrase structure or not from a small number of components of the context vector.  Hata kwa kesi ya kujifunza kutoka kwa ujumbe mfupi, vectors za mazingira bado zinaonyesha kuwa wanaunganisha vizuri na miundombinu ya msemo. Mwisho, tunaonyesha kuwa mabadiliko ya asili ya maneno ya kazi na sehemu ya hotuba ambazo zinaibua maneno yanawakilishwa katika upande mdogo lakini wa msingi wa vector mpya wa LSTM.</abstract_sw>
      <abstract_tr>Uzun zamandyr Bu şekilde, täbiçi metin üçin bu maglumat içeri vektörlerinde nädip gözetlenmedi. Biz olary bir dil nusgasyny öwrenip sintaktik düzümleri diýip analyzýarys. Biz görkezilýän görkezilişimiz, diýmek bolsa daşary çubuklaryň netijesi, dil nusgasyny dogry ýagdaýda hasaplamak üçin bilim nusgasyna kömek etmek üçin 2-nji ýa-da üçin 3-nji mykdarlaryň derejesi, Suzgun et al. (2019) i ň soňra syntetik dyk diller üçin görkezilýär Bazı ölçüler için, VP ve NP gibi ifade yapılarının derinlikleri ile bağlantıldığını gösteriyoruz. L1 düzenlemesi ile de bu sözün bir fraz yapısının içinde olup olmadığını doğru tahmin edebileceğini fark etdik. - Hat metinden öwrenmek üçin hem, kontekst vektörleri fraz struktörleri bilen has gowy bir şekilde görkezilýär. Sonunda, funksyonal sözlerin doğal toparlarını ve sözlerin tetikleyen bir küçük fakat LSTM kontekst güncelleştirme vektörünün en önemli alt alanını gösteriyoruz.</abstract_tr>
      <abstract_af>Lang- Termine Geheue herhaalde neuralnetwerk (LSTM) is vaste gebruik en bekend om informatiewe lang- term sintaktiewe afhanklikhede te vang. Maar hoe sodanige inligting in sy interne vektore vir natuurlike teks nog nie genoeg is ondersoek nie. Ons analyseer hulle deur 'n taal model te leer waar sintaktieke strukture inplisite gegee word. Ons wys empiriese dat die konteks opdateer vektores, t.d. uitvoerdes van interne poorte, is omtrent quantiseer na binêre of ternary waardes om die taal model te help om die diepte van presies te tel, as Suzgun et al. (2019) onlangs vertoon vir sintetiese diek tale. Vir sommige dimensies in die konteksvektor, wys ons dat hulle aktiwiteite baie verbind is met die diepte van frase strukture, soos VP en NP. Ook, met 'n L1 regularisasie, ons het ook gevind dat dit kan presies voorskou of 'n woord binne 'n frase struktuur is of nie van' n klein aantal komponente van die konteksvektor nie. Selfs vir die geval van leer van raai teks, word konteksvektore vertoon om nog goed te korrelaat met die frase strukture. Eindelik, ons wys dat natuurlike klasters van die funksionele woorde en die deel van speletjies wat uittrek frases word in 'n klein maar hoofspasie van die konteks-opdateer vektor van LSTM verteenwoordig word.</abstract_af>
      <abstract_am>የረጅም የረጅም ጊዜ ማስታወስ recurrent ኔural network (LSTM) በተስፋው ተጠቃሚ እና የመረጃ መረጃ የረጅም ዘመን Syntactic ተሟጋቾች ለመያዝ የታወቀ ነው። ነገር ግን እንደዚህ ያሉ መረጃ በአውስጣዊ የጽሑፍ አካባቢ ውስጥ እንዴት እንደተመለከቱ ነው፡፡ የቋንቋን ምሳሌ በማስተማር እናስተምርላቸዋለን፡፡ በአሳማሚ እናሳየዋለን፣ የውይይት ደጆች የውጤት ውጤቶች፣ የቋንቋ ሞዴል ጥልቁን እንደ ሱዙን እና አል የሚቆጥር ጥልቅ ጥልቅ እናስረዳለን፡፡ በአካባቢው ጠቅላላ ውስጥ ያሉ አካባቢዎች፣ ሥራቸው እንደ VP እና የNP ጥልቅ በጽሑፍ ግንኙነት እንዲያያሰራሉ እናሳያቸዋለን፡፡ ከዚህም ጋር የL1 ሥርዓት ጋር ቃላት በጽሑፍ አካባቢ ውስጥ መሆኑን ወይም ከታናሹ ክፍሎች ትንሽ ጉዳይ መሆኑን በመግለጥ እናውቃለን፡፡ ለጥሩ ጽሑፍ መማር እንኳ፣ የጽሑፉ መሠረት መሳሪያዎች ደግሞ በጽሑፍ መሠረቶች ደጋፍ እንዲታሰሩ ይገልጣሉ፡፡ በመጨረሻም፣ የሥርዓት ቃላት እና የንግግሮችን መፍጠር የሚያሳየው የፍጥረት ቃላት እና የንግግር ክፍል የLSTM ማቀናጃ መሠረት በትንሽ ነገር ግን የዋነኛ ክፍል ነው፡፡</abstract_am>
      <abstract_sq>Kujtesa e gjatë afat-shkurtër rrjeti neural recurrent (LSTM) përdoret gjerësisht dhe është i njohur për të kapur varësitë sintaktike afat-gjatë informative. Megjithatë, si informacioni i tillë pasqyrohet në vektorët e tij të brendshëm për tekstin natyror nuk është hetuar ende në mënyrë të mjaftueshme. Ne i analizojmë ato duke mësuar një model gjuhësh ku strukturat sintaktike janë dhënë implicitisht. Ne empirikisht tregojmë se vektorët e përditësimit të kontekstit, i.e. daljet e portave të brendshme, janë përafërsisht kuantizuar në vlera binare apo ternare për të ndihmuar modelin gjuhësor të numërojë thellësinë e foljes me saktësi, siç tregojnë Suzgun et al. (2019) kohët e fundit për gjuhët sintetike Dyck. Për disa dimensione në vektorin e kontekstit, ne tregojmë se aktivitetet e tyre janë shumë të lidhura me thellësinë e strukturave të frazëve, të tilla si VP dhe NP. Përveç kësaj, me një rregullalizim L1, gjetëm gjithashtu se mund të parashikojë saktësisht nëse një fjalë është brenda një strukture fraze apo jo nga një numër i vogël komponentesh të vektorit të kontekstit. Edhe për rastin e mësimit nga teksti i papërpunuar, vektorët e kontekstit shfaqen të korrelohen ende mirë me strukturat e frazëve. Më në fund, ne tregojmë se grupet natyrore të fjalëve funksionale dhe pjesa e fjalimeve që shkaktojnë frazat janë përfaqësuar në një nënhapësirë të vogël por kryesore të vektorit të përditësimit të kontekstit të LSTM.</abstract_sq>
      <abstract_hy>Լանգ կարճ ժամանակահատվածի հիշողության կրկնվող նյարդային ցանցը (LSMT) լայնորեն օգտագործվում է և հայտնի է ինֆորմատիվ երկար ժամանակահատվածի սինտակտիկ կախվածությունների ձայնագրման համար: Այնուամենայնիվ, ինչպե՞ս է այդ տեղեկատվությունը արտացոլում բնական տեքստի ներքին վեկտորներում, դեռևս բավարար չափով չի ուսումնասիրել: We analyze them by learning a language model where syntactic structures are implicitly given.  Մենք էմպրիկապես ցույց ենք տալիս, որ կոնտեքստի վերականգնման վեկտորները, այսինքն ներքին դարպասների արտադրումները, մոտավորապես քվանտավորված են երկու կամ երկրորդ արժեքների վրա, որպեսզի օգնեն լեզվի մոդելը ճշգրիտ հաշվարկել խմբավարման խորությունը, ինչպես Suzգուն և այլն (2019 Կոնտեքստի վեկտորի որոշ չափումների համար մենք ցույց ենք տալիս, որ նրանց ակտիվացվածքները շատ կապված են արտահայտության կառուցվածքների խորության հետ, ինչպիսիք են VP և NP: Ավելին, L1-ի կարգավորման դեպքում մենք նաև հայտնաբերեցինք, որ այն կարող է ճշգրիտ կանխատեսել, թե բառը գտնվում է արտահայտության կառուցվածքի մեջ, թե ոչ կոնտեքստի վեկտորի մի փոքր քանակից: Նույնիսկ ոչ մշակված տեքստից սովորելու դեպքում ցույց է տալիս, որ կոնտեքստի վեկտորները դեռևս լավ կապված են արտահայտության կառուցվածքների հետ: Վերջապես, մենք ցույց ենք տալիս, որ ֆունկցիոնալ բառերի բնական խմբերը և խոսքերի մասը, որոնք արտահայտում են արտահայտություններ, ներկայացված են LSMT-ի կոնտեքստի վերականգնման վեկտորի փոքր, բայց հիմնական ենթատարածքում:</abstract_hy>
      <abstract_az>Uzun-Uzun Uzun-Term Yadın Yenidən Yenidən Nəyral Ağ (LSTM) geniş işlədilir və informativ uzun-müddət sintaktik bağlılıqları almaq üçün tanınar. Ancaq bu məlumat, təbiətli məlumat üçün iç vektorlarında necə təsirlənəcəkdir? Biz onları sintaktik quruların müəyyən edildiyi dil modelini öyrənib analizə edirik. İçki qapıların sonuçları, suzgun et al kimi, dil modelinin dəyişikliyini doğru saymaq üçün kömək etmək üçün binar və ternar qiymətlərinə kvantifikat edilmişdir. Bazı ölçülərə görə, biz onların aktivasiyaları VP və NP kimi fraz qurularının derinlikləri ilə çox bağlı olduğunu göstəririk. Daha sonra, L1 düzgünlüyü ilə, biz də gördük ki, bu sözün bir fraz quruluşunun içində olmadığını və ya müxtəlif vektorun kiçik bir növ komponentlərin içində olmadığını təmin edə bilər. Sıfır metindən öyrənmək məqsədilə də, məlumat vektörləri hələ də fraza yapılarıyla yaxşı bağlanmaq üçün göstəriləcəklər. Sonunda, fərqli sözlərin təbiətli clusterlərin və sözlərin parçasını LSTM kontekst-güncelləndirmə vektorunun kiçik, ancaq əsl alt alanında göstəririk.</abstract_az>
      <abstract_bn>দীর্ঘ সংক্ষিপ্ত-টার্ম মেমোরি পুনরাবর্তন নিউরেল নেটওয়ার্ক (LSTM) ব্যবহার করা হয় এবং তথ্য দীর্ঘমেয়াদ সিন্ট্যাকটিক নির্ভরের জন্ তবে প্রাকৃতিক টেক্সটের জন্য এই ধরনের তথ্য কিভাবে প্রতিফলিত হয়েছে তার অভ্যন্তরীণ ভেক্টরে এখনও যথেষ্ট তদন্ত করা হয় ন আমরা তাদের বিশ্লেষণ করি ভাষার মডেল শিখার মাধ্যমে যেখানে সিন্ট্যাক্টিক কাঠামোগুলো ব্যর্থতা প্রদা আমরা ক্ষমতাশীল ভাবে দেখাচ্ছি যে ভেক্টর, যেমন ইন্টারনেট গেটের আপটপুট, বাইনারি অথবা টার্নারি মানের প্রায় পরিমাণ বাইনারি বা টার্নারি মূল্য, যাতে ভাষার মডেল সঠিকভাবে প্রতিষ্ঠানে For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP.  এছাড়াও, এল১ নিয়ন্ত্রণের সাথে আমরা আবিষ্কার করেছি যে এটি সঠিকভাবে ভবিষ্যদ্বাণী করতে পারে যে একটি শব্দ কাঠামোর ভেতরে আছে কিনা কিংবা প্রেক্ষি এমনকি ক্ষুদ্র লেখা থেকে শিখার ক্ষেত্রেও প্রেক্ষিত ভেক্টর এখনো ভালো কাঠামোর সাথে সংশ্লিষ্ট। অবশেষে, আমরা দেখাচ্ছি যে কার্যকলাপ শব্দের প্রাকৃতিক ক্লাস্টার এবং ভাষণের অংশ যে বাক্ষতিগুলোর প্রতিনিধিত্ব করা হচ্ছে এলএসএমএর কন্টেক্সটেক্ট</abstract_bn>
      <abstract_bs>Dugo kratkoročna memorija se široko koristi i poznaje kako bi uhvatili informativne dugoročne sintaktične zavisnosti. Međutim, kako se takve informacije odražavaju u svojim unutrašnjim vektorima prirodnog teksta još nisu dovoljno istražene. Analiziramo ih učeći jezički model gdje se sintaktičke strukture implicitno daju. Mi empirički pokazujemo da su vektori za aktualiziranje konteksta, tj. ishod unutrašnjih vrata, približno kvantizirani na binarne ili ternarne vrijednosti kako bi pomogli jezičkom modelu da računa dubinu gnijezda tačno, kao Suzgun et al. (2019) nedavno pokazuju za sintetičke jezike. Za neke dimenzije u kontekstskom vektoru pokazujemo da su njihove aktivacije visoko povezani sa dubinom frazu strukture, poput VP i NP. Osim toga, s regularizacijom L1, također smo otkrili da može precizno predvidjeti da li je riječ unutar strukture fraze ili ne iz malog broja komponenta kontekstnog vektora. Čak i za slučaj učenja sa sirovog teksta, kontekstski vektori se pokazuju da se i dalje dobro povezuju sa strukturama fraza. Napokon, pokazujemo da su prirodni skupini funkcionalnih riječi i dio govora koje okidaju fraze predstavljeni u malom ali glavnom podprostoru vektora LSTM-a za aktualizaciju konteksta.</abstract_bs>
      <abstract_ca>La xarxa neural recurrent de memòria a curt termini (LSTM) s'utilitza ampliament i es coneix per capturar dependencies sinàctiques informatives a llarg termini. No obstant això, la manera en què aquesta informació s'reflecteix en els seus vectors interns del text natural encara no ha estat investigada suficientment. We analyze them by learning a language model where syntactic structures are implicitly given.  Mostrem empíricament que els vectors d'actualització del context e, és a dir, les sortides de portas internas, es quantifiquen aproximadament a valors binaris o ternaris per ajudar al model de llenguatge a comptar la profunditat del ninjat amb precisió, com Suzgun et al. (2019) mostren recentment per a les llengües sintètiques Dyck. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP.  A més, amb una regularització L1, també vam descobrir que pot predir exactament si una paraula està dins una estructura de frases o no d'un petit nombre de components del vector contextual. Fins i tot en el cas d'aprendre amb text brut, es demostra que els vectors de context encara correlacionen bé amb les estructures de frases. Finalment, demostram que agrupaments naturals de paraules funcionals i la part de discursos que desencadena frases són representats en un petit, però principal, subsespai del vector d'actualització del context de LSTM.</abstract_ca>
      <abstract_cs>Rekurentní neuronová síť s dlouhodobou krátkodobou pamětí (LSTM) je široce používána a známa pro zachycování informativních dlouhodobých syntaktických závislostí. Způsob, jakým se tyto informace odrážejí v jejich vnitřních vektorech pro přirozený text, však dosud nebyl dostatečně zkoumán. Analyzujeme je pomocí jazykového modelu, kde jsou implicitně dány syntaktické struktury. Empiricky ukazujeme, že vektory aktualizace kontextu, tj. výstupy interních bran, jsou přibližně kvantizovány na binární nebo ternární hodnoty, aby jazykovému modelu pomohly přesně spočítat hloubku vnoření, jak to nedávno ukazují Suzgun et al. (2019) u syntetických jazyků Dycka. U některých dimenzí kontextového vektoru ukazujeme, že jejich aktivace jsou vysoce korelovány s hloubkou frázových struktur, jako jsou VP a NP. Díky regularizaci L1 jsme také zjistili, že může přesně předpovědět, zda je slovo uvnitř frázové struktury nebo ne z malého počtu komponent kontextového vektoru. Dokonce i v případě učení se ze surového textu jsou ukázány, že kontextové vektory stále dobře korelují s frázovými strukturami. Nakonec ukazujeme, že přirozené shluky funkčních slov a části projevů, které spouštějí fráze, jsou reprezentovány v malém, ale hlavním subprostoru kontextového aktualizačního vektoru LSTM.</abstract_cs>
      <abstract_et>Pikaajalist lühiajalist mälu korduvat närvivõrku (LSTM) kasutatakse laialdaselt ja teadaolevalt jäädvustatakse informatiivseid pikaajalisi süntaktilisi sõltuvusi. Kuid seda, kuidas selline teave kajastub loodusliku teksti sisemises vektoris, ei ole veel piisavalt uuritud. Analüüsime neid keelemudeli õppides, kus süntaktilised struktuurid on kaudselt esitatud. Empiiriliselt näitame, et konteksti värskendusvektorid, st siseväravate väljundid, on ligikaudu kvantiseeritud kahe- või kolmeväärtusteks, et aidata keelemudelil pesitsemise sügavust täpselt lugeda, nagu Suzgun jt. (2019) hiljuti näitasid sünteetiliste Dyckide keelte puhul. Mõnede kontekstivektori dimensioonide puhul näitame, et nende aktiveerimine on tugevalt korrelatsioonis fraasistruktuuride, näiteks VP ja NP sügavusega. Lisaks leidsime L1 regulariseerimisega, et see suudab täpselt ennustada, kas sõna on fraasistruktuuri sees või mitte kontekstivektori väikesest arvust komponentidest. Isegi toortekstist õppimise puhul näidatakse, et konteksti vektorid on endiselt hästi korrelatsioonis fraasistruktuuridega. Lõpuks näitame, et funktsionaalsete sõnade loomulikud klastrid ja kõnede osa, mis käivitavad fraase, on esindatud LSTM konteksti-uuenduse vektori väikeses, kuid põhilises alamruumis.</abstract_et>
      <abstract_fi>Pitkän lyhyen aikavälin muistin toistuvia hermoverkkoja (LSTM) käytetään laajalti ja tiedetään keräävän informatiivisia pitkän aikavälin syntaktisia riippuvuuksia. Kuitenkin sitä, miten tällainen tieto heijastuu sen sisäisiin vektoreihin luonnolliselle tekstille, ei ole vielä riittävästi tutkittu. Analysoimme niitä oppimalla kielimallia, jossa syntaktiset rakenteet annetaan implisiittisesti. Empiirisesti osoitamme, että kontekstipäivityksen vektorit eli sisäisten porttien tuotokset kvantifioidaan likimäärin binaari- tai kolmikantaarvoiksi, jotta kielimalli pystyy laskemaan pesimisen syvyyden tarkasti, kuten Suzgun et al. (2019) äskettäin osoittivat synteettisille Dyck-kielille. Joidenkin kontekstivektorin ulottuvuuksien osalta osoitamme, että niiden aktivaatiot korreloivat voimakkaasti fraasirakenteiden syvyyteen, kuten VP ja NP. Lisäksi L1-säännöstelyn avulla huomasimme myös, että se pystyy ennustamaan tarkasti, onko sana fraasirakenteen sisällä vai ei pienestä määrästä kontekstivektorin komponentteja. Raakatekstistä opittaessa kontekstivektorit korreloivat edelleen hyvin fraasirakenteisiin. Lopuksi osoitamme, että funktionaalisten sanojen luonnolliset klusterit ja lauseita laukaisevat puheet ovat edustettuina LSTM:n kontekstipäivityksen vektorin pienessä mutta pääasiallisessa aliavaruudessa.</abstract_fi>
      <abstract_jv>AllProgressBarUpdates politenessoffpolite"), and when there is a change ("assertivepoliteness Genjer-Genjer Awak dhéwé empirecally show that the context Updates vectors, i.e. output of intermediate gateway string" in "context_BAR_stringLink politenessoffpolite"), and when there is a change ("assertivepoliteness Text FindOK</abstract_jv>
      <abstract_ha>@ action: button A lokacin da, ko da ake yi wa wannan information a cikin hanyarsa na guda wa matsayin natsuwa ba'a iya ƙidãya ba. Ana anayya su da za'a sanar da wani misalin harshe a inda an bã su da tsaro masu haɗi. Tuna nuna kwamfyutan kodi ɗin agogo, misali, masu fitarwa na ƙõfõfin guda, za'a ƙayyade kima guda ko masu ƙari ko kuma za'a yi taimako da misalin harshen da za'a lissafa muhimmin sali da gaske, kamar Suzgun et al. (2019) na nuna wa harshen Dyck na ƙarami. Ga wasu tsohon cikin masu cikin masu sakan, Munã nuna cewa aikin su yana da giraffiyar da girgije na tsarin rasmi, kamar misali, vP da NP. Za kuma, da wani L1 ya ƙayyade shi, za mu gane cewa yana iya ƙayyade magana a cikin wani salo na rubutu ko kuma ba daga ƙarami ƙarami masu cikin sakan aikin muhalli. Ko dai da za'a sanar da daga matsayin raw, za'a nuna shiryoyi cikin muhimman da za'a yi daidai da tsarin rasmi. Haƙĩƙa, Munã nũna masu natsuwa na maganar aiki da rabon magana wanda ke fara magana za'a yi tasgaro a cikin ƙarami kuma mainli subpace of the context-update of LTRM.</abstract_ha>
      <abstract_sk>Dolgoročno kratkoročno ponovno nevronsko omrežje (LSTM) se široko uporablja in zna, da zajema informativne dolgoročne sintaktične odvisnosti. Vendar pa, kako se takšne informacije odražajo v notranjih vektorjih naravnega besedila, še ni bilo dovolj raziskano. Analiziramo jih z učenjem jezikovnega modela, v katerem so implicitno navedene sintaktične strukture. Empirično kažemo, da so vektorji kontekstne posodobitve, tj. izhodi notranjih vrat, približno kvantizirani na binarne ali tristranske vrednosti, da bi jezikovni model pomagal natančno šteti globino gnezdenja, kot so pred kratkim pokazali Suzgun et al. (2019) za sintetične Dyckove jezike. Za nekatere dimenzije kontekstnega vektorja pokažemo, da so njihove aktivacije močno korelacirane z globino fraznih struktur, kot sta VP in NP. Poleg tega smo z ureditvijo L1 ugotovili tudi, da lahko natančno predvidi, ali je beseda znotraj frazne strukture ali ne iz majhnega števila komponent kontekstnega vektorja. Tudi v primeru učenja iz surovega besedila je dokazano, da kontekstni vektorji še vedno dobro korelacirajo s strukturami fraz. Na koncu pokažemo, da so naravni grozdi funkcionalnih besed in del govorov, ki sprožajo fraze, predstavljeni v majhnem, a glavnem podprostoru vektorja kontekstne posodobitve LSTM.</abstract_sk>
      <abstract_he>רשת עצבית חוזרת לזיכרון ארוך לטווח קצר (LSTM) משתמשת באופן רחב וידועה לכלוף תלויות סינטקטיות מידעיות לטווח ארוך. עם זאת, איך מידע כזה משקף בוקטורים הפנימיים שלו לטקסט טבעי עדיין לא נחקר מספיק. We analyze them by learning a language model where syntactic structures are implicitly given.  We empirically show that the context update vectors, i.e. outputs of internal gates, are approximately quantized to binary or ternary values to help the language model to count the depth of nesting accurately, as Suzgun et al. (2019) recently show for synthetic Dyck languages.  למימדים מסוימים בוקטור הקשר, אנו מראים שהפעילות שלהם קשורות מאוד לעומק של מבנים ביטויים, כמו VP ו NP. חוץ מזה, עם ניתוח L1, מצאנו גם שהוא יכול לחזות בדיוק אם מילה נמצאת בתוך מבנה ביטוי או לא ממספר קטן של רכיבים של ווקטור הקשר. אפילו במקרה של לימוד מתוך טקסט חום, הוראים ויקטורי הקשר עדיין מתחברים היטב עם מבנים המשפטים. סוף סוף, אנו מראים כי קבוצות טבעיות של המילים המפעילות והחלק של נאומים שמפעילים ביטויים מייצגים בתא-חלל קטן אבל ראשי של ווקטור הקונקסט-עדכון של LSTM.</abstract_he>
      <abstract_bo>Long-Term Memory recurrent neural network (LSTM) is widely used and known to capture informative long-term syntactic dependencies. འོན་ཀྱང་། རང་རྒྱུས་ཡིག་གི་གནས་ཚུལ་འདི་ག་དེ་རྟོགས་པའི་སྣ་ཚོགས་ནང་ལ་མཐོང་ན་ཕལ་མེད་པ་རེད། ང་ཚོས་དེ་དག་ཚོར་སྐད་ཡིག་གི་མ་དབྱིབས་ཞིབ་འཇུག་བྱེད་ཀྱི་ཡོད་པ་ལས་དབྱེ་ཞིབ་བྱེད་ཀྱི་ཡོད། We empirically show that the context update vectors, i.e. outputs of internal gates, are approximately quantized to binary or ternary values to help the language model to count the depth of nesting accurately, as Suzgun et al. (2019) recently show for synthetic Dyck languages. For some dimensions in the context vector, we show that their activations are highly correlated with the depth of phrase structures, such as VP and NP. ད་དུང་། L1 རྒྱུན་ལྡན་བཟོ་བྱས་པ་དེ་ང་ཚོས་དྲན་ཐུབ་པའི་ཐ་སྙད་ཅིག་རྟོགས་པ་ཅིན་ཡིན་ནམ། ཚིག་ཡི་གེ་ནས་སྐད་ཡིག་གི་ནང་དུ་བསླབ་པའི་ལྟ་བུའི་རྣམ་པ་དེ་ལས་སྦྲེལ་མཐུད་དང་མཉམ་དུ་མཐུན་པ་ཡིན། Finally, we show that natural clusters of the functional words and the part of speeches that trigger phrases are represented in a small but principal subspace of the context-update vector of LSTM.</abstract_bo>
      </paper>
    <paper id="358">
      <title>When and Who? Conversation Transition Based on Bot-Agent Symbiosis Learning Network</title>
      <author><first>Yipeng</first><last>Yu</last></author>
      <author><first>Ran</first><last>Guan</last></author>
      <author><first>Jie</first><last>Ma</last></author>
      <author><first>Zhuoxuan</first><last>Jiang</last></author>
      <author><first>Jingchang</first><last>Huang</last></author>
      <pages>4056–4066</pages>
      <abstract>In online customer service applications, multiple <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a> that are specialized in various topics are typically developed separately and are then merged with other <a href="https://en.wikipedia.org/wiki/Intelligent_agent">human agents</a> to a single <a href="https://en.wikipedia.org/wiki/Computing_platform">platform</a>, presenting to the users with a unified interface. Ideally the conversation can be transparently transferred between different sources of <a href="https://en.wikipedia.org/wiki/Customer_support">customer support</a> so that domain-specific questions can be answered timely and this is what we coined as a Bot-Agent symbiosis. Conversation transition is a major challenge in such online customer service and our work formalises the challenge as two core problems, namely, when to transfer and which bot or agent to transfer to and introduces a deep neural networks based approach that addresses these problems. Inspired by the net promoter score (NPS), our research reveals how the problems can be effectively solved by providing user feedback and developing deep neural networks that predict the conversation category distribution and the NPS of the dialogues. Experiments on realistic data generated from an online service support platform demonstrate that the proposed approach outperforms state-of-the-art methods and shows promising perspective for transparent conversation transition.</abstract>
      <url hash="76fabb71">2020.coling-main.358</url>
      <doi>10.18653/v1/2020.coling-main.358</doi>
      <bibkey>yu-etal-2020-conversation</bibkey>
    </paper>
    <paper id="363">
      <title>Translation vs. Dialogue : A Comparative Analysis of Sequence-to-Sequence Modeling</title>
      <author><first>Wenpeng</first><last>Hu</last></author>
      <author><first>Ran</first><last>Le</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Jinwen</first><last>Ma</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>4111–4122</pages>
      <abstract>Understanding neural models is a major topic of interest in the deep learning community. In this paper, we propose to interpret a general neural model comparatively. Specifically, we study the sequence-to-sequence (Seq2Seq) model in the contexts of two mainstream NLP tasksmachine translation and dialogue response generationas they both use the seq2seq model. We investigate how the two <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> are different and how their task difference results in major differences in the behaviors of the resulting translation and dialogue generation systems. This study allows us to make several interesting observations and gain valuable insights, which can be used to help develop better translation and dialogue generation models. To our knowledge, no such comparative study has been done so far.</abstract>
      <url hash="3fffc135">2020.coling-main.363</url>
      <doi>10.18653/v1/2020.coling-main.363</doi>
      <bibkey>hu-etal-2020-translation</bibkey>
    </paper>
    <paper id="364">
      <title>Diverse dialogue generation with context dependent dynamic loss function</title>
      <author><first>Ayaka</first><last>Ueyama</last></author>
      <author><first>Yoshinobu</first><last>Kano</last></author>
      <pages>4123–4127</pages>
      <abstract>Dialogue systems using <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> have achieved generation of fluent response sentences to user utterances. Nevertheless, they tend to produce responses that are not diverse and which are less context-dependent. To address these shortcomings, we propose a new <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a>, an Inverse N-gram loss (INF), which incorporates contextual fluency and diversity at the same time by a simple formula. Our INF loss can adjust its <a href="https://en.wikipedia.org/wiki/Loss_function">loss</a> dynamically by a weight using the inverse frequency of the tokens’ n-gram applied to Softmax Cross-Entropy loss, so that rare tokens appear more likely while retaining the fluency of the generated sentences. We trained Transformer using English and Japanese Twitter replies as single-turn dialogues using different <a href="https://en.wikipedia.org/wiki/Loss_function">loss functions</a>. Our INF loss model outperformed the baselines of SCE loss and ITF loss models in automatic evaluations such as DIST-N and ROUGE, and also achieved higher scores on our human evaluations of coherence and richness.</abstract>
      <url hash="59f5755f">2020.coling-main.364</url>
      <doi>10.18653/v1/2020.coling-main.364</doi>
      <bibkey>ueyama-kano-2020-diverse</bibkey>
    </paper>
    <paper id="365">
      <title>Towards Topic-Guided Conversational Recommender System</title>
      <author><first>Kun</first><last>Zhou</last></author>
      <author><first>Yuanhang</first><last>Zhou</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Xiaoke</first><last>Wang</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>4128–4139</pages>
      <abstract>Conversational recommender systems (CRS) aim to recommend high-quality items to users through interactive conversations. To develop an effective <a href="https://en.wikipedia.org/wiki/Computational_fluid_dynamics">CRS</a>, the support of high-quality datasets is essential. Existing CRS datasets mainly focus on immediate requests from users, while lack proactive guidance to the recommendation scenario. In this paper, we contribute a new CRS dataset named TG-ReDial (Recommendation through Topic-Guided Dialog). Our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> has two major features. First, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> incorporates topic threads to enforce natural semantic transitions towards the recommendation scenario. Second, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> is created in a semi-automatic way, hence human annotation is more reasonable and controllable. Based on TG-ReDial, we present the task of topic-guided conversational recommendation, and propose an effective approach to this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. Extensive experiments have demonstrated the effectiveness of our approach on three sub-tasks, namely topic prediction, item recommendation and response generation. TG-ReDial is available at blue.<b>TG-ReDial</b> (<b>Re</b>commendation through <b>T</b>opic-<b>G</b>uided <b>Dial</b>og). Our dataset has two major features. First, it incorporates topic threads to enforce natural semantic transitions towards the recommendation scenario. Second, it is created in a semi-automatic way, hence human annotation is more reasonable and controllable. Based on TG-ReDial, we present the task of topic-guided conversational recommendation, and propose an effective approach to this task. Extensive experiments have demonstrated the effectiveness of our approach on three sub-tasks, namely topic prediction, item recommendation and response generation. TG-ReDial is available at blue<url>https://github.com/RUCAIBox/TG-ReDial</url>.</abstract>
      <url hash="37f9bf31">2020.coling-main.365</url>
      <doi>10.18653/v1/2020.coling-main.365</doi>
      <bibkey>zhou-etal-2020-towards</bibkey>
      <pwccode url="https://github.com/RUCAIBox/TG-ReDial" additional="true">RUCAIBox/TG-ReDial</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tg-redial">TG-ReDial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/douban">Douban</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/durecdial">DuRecDial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/redial">ReDial</pwcdataset>
    </paper>
    <paper id="367">
      <title>Summarize before Aggregate : A Global-to-local Heterogeneous Graph Inference Network for Conversational Emotion Recognition</title>
      <author><first>Dongming</first><last>Sheng</last></author>
      <author><first>Dong</first><last>Wang</last></author>
      <author><first>Ying</first><last>Shen</last></author>
      <author><first>Haitao</first><last>Zheng</last></author>
      <author><first>Haozhuang</first><last>Liu</last></author>
      <pages>4153–4163</pages>
      <abstract>Conversational Emotion Recognition (CER) is a crucial task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing (NLP)</a> with wide applications. Prior works in CER generally focus on modeling emotion influences solely with utterance-level features, with little attention paid on phrase-level semantic connection between utterances. Phrases carry sentiments when they are referred to emotional events under certain topics, providing a global semantic connection between utterances throughout the entire conversation. In this work, we propose a two-stage Summarization and Aggregation Graph Inference Network (SumAggGIN), which seamlessly integrates inference for topic-related emotional phrases and local dependency reasoning over neighbouring utterances in a global-to-local fashion. Topic-related emotional phrases, which constitutes the global topic-related emotional connections, are recognized by our proposed heterogeneous Summarization Graph. Local dependencies, which captures short-term emotional effects between neighbouring utterances, are further injected via an Aggregation Graph to distinguish the subtle differences between utterances containing emotional phrases. The two steps of graph inference are tightly-coupled for a comprehensively understanding of emotional fluctuation. Experimental results on three CER benchmark datasets verify the effectiveness of our proposed model, which outperforms the state-of-the-art approaches.</abstract>
      <url hash="6166bca4">2020.coling-main.367</url>
      <doi>10.18653/v1/2020.coling-main.367</doi>
      <bibkey>sheng-etal-2020-summarize</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="368">
      <title>Deconstruct to Reconstruct a Configurable Evaluation Metric for Open-Domain Dialogue Systems</title>
      <author><first>Vitou</first><last>Phy</last></author>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>4164–4178</pages>
      <abstract>Many automatic evaluation metrics have been proposed to score the overall quality of a response in open-domain dialogue. Generally, the overall quality is comprised of various aspects, such as relevancy, <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">specificity</a>, and <a href="https://en.wikipedia.org/wiki/Empathy">empathy</a>, and the importance of each aspect differs according to the task. For instance, <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">specificity</a> is mandatory in a food-ordering dialogue task, whereas <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a> is preferred in a language-teaching dialogue system. However, existing <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> are not designed to cope with such flexibility. For example, BLEU score fundamentally relies only on word overlapping, whereas BERTScore relies on <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a> between reference and candidate response. Thus, <a href="https://en.wikipedia.org/wiki/Copula_(linguistics)">they</a> are not guaranteed to capture the required <a href="https://en.wikipedia.org/wiki/Complex_system">aspects</a>, i.e., <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">specificity</a>. To design a metric that is flexible to a task, we first propose making these qualities manageable by grouping them into three groups : understandability, sensibleness, and likability, where likability is a combination of qualities that are essential for a task. We also propose a simple method to composite metrics of each aspect to obtain a single <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> called USL-H, which stands for Understandability, Sensibleness, and Likability in Hierarchy. We demonstrated that USL-H score achieves good correlations with human judgment and maintains its configurability towards different aspects and metrics.</abstract>
      <url hash="92ee61f2">2020.coling-main.368</url>
      <doi>10.18653/v1/2020.coling-main.368</doi>
      <bibkey>phy-etal-2020-deconstruct</bibkey>
      <pwccode url="https://github.com/vitouphy/usl_dialogue_metric" additional="false">vitouphy/usl_dialogue_metric</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="370">
      <title>HiTrans : A Transformer-Based Context- and Speaker-Sensitive Model for Emotion Detection in Conversations<fixed-case>H</fixed-case>i<fixed-case>T</fixed-case>rans: A Transformer-Based Context- and Speaker-Sensitive Model for Emotion Detection in Conversations</title>
      <author><first>Jingye</first><last>Li</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <author><first>Fei</first><last>Li</last></author>
      <author><first>Meishan</first><last>Zhang</last></author>
      <author><first>Yijiang</first><last>Liu</last></author>
      <pages>4190–4200</pages>
      <abstract>Emotion detection in conversations (EDC) is to detect the emotion for each utterance in conversations that have multiple speakers. Different from the traditional non-conversational emotion detection, the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> for EDC should be context-sensitive (e.g., understanding the whole conversation rather than one utterance) and speaker-sensitive (e.g., understanding which utterance belongs to which speaker). In this paper, we propose a transformer-based context- and speaker-sensitive model for EDC, namely HiTrans, which consists of two hierarchical transformers. We utilize BERT as the low-level transformer to generate local utterance representations, and feed them into another high-level transformer so that utterance representations could be sensitive to the global context of the conversation. Moreover, we exploit an auxiliary task to make our model speaker-sensitive, called pairwise utterance speaker verification (PUSV), which aims to classify whether two utterances belong to the same speaker. We evaluate our model on three benchmark datasets, namely EmoryNLP, <a href="https://en.wikipedia.org/wiki/MELD">MELD</a> and IEMOCAP. Results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms previous <a href="https://en.wikipedia.org/wiki/State-of-the-art">state-of-the-art models</a>.</abstract>
      <url hash="b46fc085">2020.coling-main.370</url>
      <doi>10.18653/v1/2020.coling-main.370</doi>
      <bibkey>li-etal-2020-hitrans</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/emorynlp">EmoryNLP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="371">
      <title>A Co-Attentive Cross-Lingual Neural Model for Dialogue Breakdown Detection</title>
      <author><first>Qian</first><last>Lin</last></author>
      <author><first>Souvik</first><last>Kundu</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <pages>4201–4210</pages>
      <abstract>Ensuring smooth communication is essential in a chat-oriented dialogue system, so that a user can obtain meaningful responses through interactions with the <a href="https://en.wikipedia.org/wiki/System">system</a>. Most prior work on dialogue research does not focus on preventing dialogue breakdown. One of the major challenges is that a <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue system</a> may generate an undesired utterance leading to a dialogue breakdown, which degrades the overall interaction quality. Hence, it is crucial for a machine to detect dialogue breakdowns in an ongoing conversation. In this paper, we propose a novel dialogue breakdown detection model that jointly incorporates a pretrained cross-lingual language model and a co-attention network. Our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> leverages effective <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> trained on one hundred different languages to generate contextualized representations. Co-attention aims to capture the interaction between the latest utterance and the conversation history, and thereby determines whether the latest utterance causes a dialogue breakdown. Experimental results show that our proposed model outperforms all previous approaches on all evaluation metrics in both the Japanese and English tracks in Dialogue Breakdown Detection Challenge 4 (DBDC4 at IWSDS2019).</abstract>
      <url hash="0f77eda7">2020.coling-main.371</url>
      <doi>10.18653/v1/2020.coling-main.371</doi>
      <bibkey>lin-etal-2020-co</bibkey>
      <pwccode url="https://github.com/nusnlp/cxm" additional="false">nusnlp/cxm</pwccode>
    </paper>
    <paper id="376">
      <title>Improving Low-Resource NMT through Relevance Based Linguistic Features Incorporation<fixed-case>NMT</fixed-case> through Relevance Based Linguistic Features Incorporation</title>
      <author><first>Abhisek</first><last>Chakrabarty</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Chenchen</first><last>Ding</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>4263–4274</pages>
      <abstract>In this study, <a href="https://en.wikipedia.org/wiki/Linguistics">linguistic knowledge</a> at different levels are incorporated into the neural machine translation (NMT) framework to improve translation quality for language pairs with extremely limited data. Integrating manually designed or automatically extracted features into the NMT framework is known to be beneficial. However, this study emphasizes that the <a href="https://en.wikipedia.org/wiki/Relevance">relevance</a> of the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> is crucial to the performance. Specifically, we propose two methods, 1) self relevance and 2) word-based relevance, to improve the representation of <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a>. Experiments are conducted on translation tasks from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to eight Asian languages, with no more than twenty thousand sentences for training. The proposed <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a> improve translation quality for all <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> by up to 3.09 <a href="https://en.wikipedia.org/wiki/BLEU">BLEU points</a>. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation.</abstract>
      <url hash="ef68f3a1">2020.coling-main.376</url>
      <doi>10.18653/v1/2020.coling-main.376</doi>
      <bibkey>chakrabarty-etal-2020-improving</bibkey>
    </paper>
    <paper id="383">
      <title>Filtering Back-Translated Data in Unsupervised Neural Machine Translation</title>
      <author><first>Jyotsana</first><last>Khatri</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>4334–4339</pages>
      <abstract>Unsupervised neural machine translation (NMT) utilizes only <a href="https://en.wikipedia.org/wiki/Monolingualism">monolingual data</a> for training. The quality of back-translated data plays an important role in the performance of <a href="https://en.wikipedia.org/wiki/Network_topology">NMT systems</a>. In <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a>, all generated pseudo parallel sentence pairs are not of the same quality. Taking inspiration from domain adaptation where in-domain sentences are given more weight in training, in this paper we propose an approach to filter back-translated data as part of the training process of unsupervised NMT. Our approach gives more weight to good pseudo parallel sentence pairs in the back-translation phase. We calculate the weight of each pseudo parallel sentence pair using sentence-wise round-trip BLEU score which is normalized batch-wise. We compare our approach with the current state of the art approaches for unsupervised NMT.</abstract>
      <url hash="3ff08612">2020.coling-main.383</url>
      <doi>10.18653/v1/2020.coling-main.383</doi>
      <bibkey>khatri-bhattacharyya-2020-filtering</bibkey>
    </paper>
    <paper id="384">
      <title>Lost in Back-Translation : Emotion Preservation in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a></title>
      <author><first>Enrica</first><last>Troiano</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>4340–4354</pages>
      <abstract>Machine translation provides powerful methods to convert text between languages, and is therefore a technology enabling a <a href="https://en.wikipedia.org/wiki/Multilingualism">multilingual world</a>. An important part of <a href="https://en.wikipedia.org/wiki/Communication">communication</a>, however, takes place at the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">non-propositional level</a> (e.g., <a href="https://en.wikipedia.org/wiki/Politeness">politeness</a>, <a href="https://en.wikipedia.org/wiki/Formality">formality</a>, emotions), and it is far from clear whether current MT methods properly translate this information. This paper investigates the specific hypothesis that the non-propositional level of emotions is at least partially lost in MT. We carry out a number of experiments in a back-translation setup and establish that (1) <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> are indeed partially lost during translation ; (2) this tendency can be reversed almost completely with a simple re-ranking approach informed by an emotion classifier, taking advantage of diversity in the n-best list ; (3) the re-ranking approach can also be applied to change <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a>, obtaining a model for emotion style transfer. An in-depth qualitative analysis reveals that there are recurring linguistic changes through which <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> are toned down or amplified, such as change of modality.</abstract>
      <url hash="6cba3e1e">2020.coling-main.384</url>
      <doi>10.18653/v1/2020.coling-main.384</doi>
      <bibkey>troiano-etal-2020-lost</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/isear">ISEAR</pwcdataset>
    </paper>
    <paper id="389">
      <title>Context-Aware Cross-Attention for Non-Autoregressive Translation</title>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Dacheng</first><last>Tao</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <pages>4396–4402</pages>
      <abstract>Non-autoregressive translation (NAT) significantly accelerates the inference process by predicting the entire target sequence. However, due to the lack of target dependency modelling in the <a href="https://en.wikipedia.org/wiki/Codec">decoder</a>, the conditional generation process heavily depends on the cross-attention. In this paper, we reveal a localness perception problem in NAT cross-attention, for which it is difficult to adequately capture source context. To alleviate this problem, we propose to enhance signals of neighbour source tokens into conventional cross-attention. Experimental results on several representative datasets show that our approach can consistently improve translation quality over strong NAT baselines. Extensive analyses demonstrate that the enhanced cross-attention achieves better exploitation of source contexts by leveraging both local and global information.</abstract>
      <url hash="af3b7f1d">2020.coling-main.389</url>
      <doi>10.18653/v1/2020.coling-main.389</doi>
      <bibkey>ding-etal-2020-context</bibkey>
    </paper>
    <paper id="390">
      <title>Does Gender Matter? Towards Fairness in Dialogue Systems</title>
      <author><first>Haochen</first><last>Liu</last></author>
      <author><first>Jamell</first><last>Dacon</last></author>
      <author><first>Wenqi</first><last>Fan</last></author>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Zitao</first><last>Liu</last></author>
      <author><first>Jiliang</first><last>Tang</last></author>
      <pages>4403–4416</pages>
      <abstract>Recently there are increasing concerns about the <a href="https://en.wikipedia.org/wiki/Fairness">fairness</a> of <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence (AI)</a> in real-world applications such as <a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a> and <a href="https://en.wikipedia.org/wiki/Recommender_system">recommendations</a>. For example, <a href="https://en.wikipedia.org/wiki/Computer_vision">recognition algorithms</a> in <a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a> are unfair to black people such as poorly detecting their faces and inappropriately identifying them as <a href="https://en.wikipedia.org/wiki/Gorilla">gorillas</a>. As one crucial application of <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI</a>, <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue systems</a> have been extensively applied in our society. They are usually built with real human conversational data ; thus they could inherit some fairness issues which are held in the real world. However, the <a href="https://en.wikipedia.org/wiki/Equity_(economics)">fairness</a> of <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue systems</a> has not been well investigated. In this paper, we perform a pioneering study about the <a href="https://en.wikipedia.org/wiki/Fair_division">fairness issues</a> in <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue systems</a>. In particular, we construct a benchmark dataset and propose quantitative measures to understand <a href="https://en.wikipedia.org/wiki/Equity_(economics)">fairness</a> in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue systems</a>, we propose two simple but effective debiasing methods. Experiments show that our <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> can reduce the bias in <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue systems</a> significantly. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and the implementation are released to foster fairness research in <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue systems</a>.</abstract>
      <url hash="5fbda8e5">2020.coling-main.390</url>
      <doi>10.18653/v1/2020.coling-main.390</doi>
      <bibkey>liu-etal-2020-gender</bibkey>
      <pwccode url="https://github.com/zgahhblhc/DialogueFairness" additional="false">zgahhblhc/DialogueFairness</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dialoguefairness">DialogueFairness</pwcdataset>
    </paper>
    <paper id="392">
      <title>Knowledge Aware Emotion Recognition in Textual Conversations via Multi-Task Incremental Transformer</title>
      <author><first>Duzhen</first><last>Zhang</last></author>
      <author><first>Xiuyi</first><last>Chen</last></author>
      <author><first>Shuang</first><last>Xu</last></author>
      <author><first>Bo</first><last>Xu</last></author>
      <pages>4429–4440</pages>
      <abstract>Emotion recognition in textual conversations (ERTC) plays an important role in a wide range of applications, such as <a href="https://en.wikipedia.org/wiki/Opinion_mining">opinion mining</a>, <a href="https://en.wikipedia.org/wiki/Recommender_system">recommender systems</a>, and so on. ERTC, however, is a challenging task. For one thing, speakers often rely on the context and commonsense knowledge to express emotions ; for another, most utterances contain neutral emotion in conversations, as a result, the confusion between a few non-neutral utterances and much more neutral ones restrains the <a href="https://en.wikipedia.org/wiki/Emotion_recognition">emotion recognition</a> performance. In this paper, we propose a novel Knowledge Aware Incremental Transformer with Multi-task Learning (KAITML) to address these challenges. Firstly, we devise a dual-level graph attention mechanism to leverage commonsense knowledge, which augments the semantic information of the utterance. Then we apply the Incremental Transformer to encode multi-turn contextual utterances. Moreover, we are the first to introduce <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> to alleviate the aforementioned confusion and thus further improve the <a href="https://en.wikipedia.org/wiki/Emotion_recognition">emotion recognition</a> performance. Extensive experimental results show that our KAITML model outperforms the state-of-the-art models across five benchmark datasets.</abstract>
      <url hash="d02dc5d3">2020.coling-main.392</url>
      <doi>10.18653/v1/2020.coling-main.392</doi>
      <bibkey>zhang-etal-2020-knowledge</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emorynlp">EmoryNLP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="395">
      <title>Leveraging Discourse Rewards for Document-Level Neural Machine Translation</title>
      <author><first>Inigo</first><last>Jauregi Unanue</last></author>
      <author><first>Nazanin</first><last>Esmaili</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Massimo</first><last>Piccardi</last></author>
      <pages>4467–4482</pages>
      <abstract>Document-level machine translation focuses on the translation of entire documents from a source to a target language. It is widely regarded as a challenging task since the <a href="https://en.wikipedia.org/wiki/Translation">translation</a> of the individual sentences in the document needs to retain aspects of the <a href="https://en.wikipedia.org/wiki/Discourse">discourse</a> at document level. However, document-level translation models are usually not trained to explicitly ensure discourse quality. Therefore, in this paper we propose a training approach that explicitly optimizes two established discourse metrics, lexical cohesion and <a href="https://en.wikipedia.org/wiki/Coherence_(linguistics)">coherence</a>, by using a reinforcement learning objective. Experiments over four different language pairs and three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches, yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair, our method has achieved an improvement of 2.46 percentage points (pp) in LC and 1.17 pp in COH over the runner-up, while at the same time improving 0.63 pp in BLEU score and 0.47 pp in F-BERT.</abstract>
      <url hash="729d0952">2020.coling-main.395</url>
      <doi>10.18653/v1/2020.coling-main.395</doi>
      <bibkey>jauregi-unanue-etal-2020-leveraging</bibkey>
    </paper>
    <paper id="396">
      <title>Effective Use of Target-side Context for Neural Machine Translation</title>
      <author><first>Hideya</first><last>Mino</last></author>
      <author><first>Hitoshi</first><last>Ito</last></author>
      <author><first>Isao</first><last>Goto</last></author>
      <author><first>Ichiro</first><last>Yamada</last></author>
      <author><first>Takenobu</first><last>Tokunaga</last></author>
      <pages>4483–4494</pages>
      <abstract>In this paper, we deal with two problems in Japanese-English machine translation of news articles. The first problem is the quality of <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpora</a>. Neural machine translation (NMT) systems suffer degraded performance when trained with <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noisy data</a>. Because there is no clean Japanese-English parallel data for <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a>, we build a novel parallel news corpus consisting of Japanese news articles translated into English in a content-equivalent manner. This is the first content-equivalent Japanese-English news corpus translated specifically for training NMT systems. The second problem involves the domain-adaptation technique. NMT systems suffer degraded performance when trained with mixed data having different <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>, such as <a href="https://en.wikipedia.org/wiki/Noisy_data">noisy data</a> and clean data. Though the existing methods try to overcome this problem by using <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">tags</a> for distinguishing the differences between corpora, it is not sufficient. We thus extend a domain-adaptation method using multi-tags to train an NMT model effectively with the clean corpus and existing parallel news corpora with some types of noise. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> increases the translation quality, and that our domain-adaptation method is more effective for learning with the multiple types of corpora than existing domain-adaptation methods are.</abstract>
      <url hash="9b223a17">2020.coling-main.396</url>
      <doi>10.18653/v1/2020.coling-main.396</doi>
      <bibkey>mino-etal-2020-effective</bibkey>
    </paper>
    <paper id="398">
      <title>Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation<fixed-case>MAP</fixed-case> Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation</title>
      <author><first>Bryan</first><last>Eikema</last></author>
      <author><first>Wilker</first><last>Aziz</last></author>
      <pages>4506–4520</pages>
      <abstract>Recent studies have revealed a number of pathologies of neural machine translation (NMT) systems. Hypotheses explaining these mostly suggest there is something fundamentally wrong with NMT as a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> or its training algorithm, <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation (MLE)</a>. Most of this evidence was gathered using maximum a posteriori (MAP) decoding, a <a href="https://en.wikipedia.org/wiki/Decision_rule">decision rule</a> aimed at identifying the highest-scoring translation, i.e. the <a href="https://en.wikipedia.org/wiki/Mode_(user_interface)">mode</a>. We argue that the evidence corroborates the inadequacy of MAP decoding more than casts doubt on the model and its training algorithm. In this work, we show that translation distributions do reproduce various statistics of the data well, but that <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> strays from such statistics. We show that some of the known pathologies and biases of NMT are due to MAP decoding and not to NMT’s statistical assumptions nor MLE. In particular, we show that the most likely <a href="https://en.wikipedia.org/wiki/Translation_(geometry)">translations</a> under the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> accumulate so little <a href="https://en.wikipedia.org/wiki/Probability_mass_function">probability mass</a> that the mode can be considered essentially arbitrary. We therefore advocate for the use of <a href="https://en.wikipedia.org/wiki/Decision_rule">decision rules</a> that take into account the translation distribution holistically. We show that an approximation to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation.</abstract>
      <url hash="97fe9a27">2020.coling-main.398</url>
      <doi>10.18653/v1/2020.coling-main.398</doi>
      <bibkey>eikema-aziz-2020-map</bibkey>
    </paper>
    <paper id="399">
      <title>Domain Transfer based Data Augmentation for Neural Query Translation</title>
      <author><first>Liang</first><last>Yao</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Weihua</first><last>Luo</last></author>
      <pages>4521–4533</pages>
      <abstract>Query translation (QT) serves as a critical factor in successful <a href="https://en.wikipedia.org/wiki/Cross-lingual_information_retrieval">cross-lingual information retrieval (CLIR)</a>. Due to the lack of parallel query samples, neural-based QT models are usually optimized with synthetic data which are derived from large-scale monolingual queries. Nevertheless, such kind of pseudo corpus is mostly produced by a general-domain translation model, making it be insufficient to guide the learning of QT model. In this paper, we extend the <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> with a domain transfer procedure, thus to revise synthetic candidates to search-aware examples. Specifically, the domain transfer model is built upon advanced Transformer, in which layer coordination and mixed attention are exploited to speed up the refining process and leverage parameters from a pre-trained cross-lingual language model. In order to examine the effectiveness of the proposed method, we collected French-to-English and Spanish-to-English QT test sets, each of which consists of 10,000 parallel query pairs with careful manual-checking. Qualitative and quantitative analyses reveal that our model significantly outperforms strong baselines and the related domain transfer methods on both translation quality and retrieval accuracy.</abstract>
      <url hash="b4be3e4c">2020.coling-main.399</url>
      <doi>10.18653/v1/2020.coling-main.399</doi>
      <bibkey>yao-etal-2020-domain</bibkey>
      <pwccode url="https://github.com/starryskyyl/dtda" additional="false">starryskyyl/dtda</pwccode>
    </paper>
    <paper id="401">
      <title>Aspectuality Across Genre : A Distributional Semantics Approach</title>
      <author><first>Thomas</first><last>Kober</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <author><first>Matthew</first><last>Stone</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>4546–4562</pages>
      <abstract>The interpretation of the <a href="https://en.wikipedia.org/wiki/Lexical_aspect">lexical aspect</a> of verbs in <a href="https://en.wikipedia.org/wiki/English_language">English</a> plays a crucial role in tasks such as recognizing textual entailment and learning discourse-level inferences. We show that two elementary dimensions of aspectual class, states vs. events, and telic vs. atelic events, can be modelled effectively with <a href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional semantics</a>. We find that a verb’s local context is most indicative of its <a href="https://en.wikipedia.org/wiki/Grammatical_aspect">aspectual class</a>, and we demonstrate that closed class words tend to be stronger discriminating contexts than <a href="https://en.wikipedia.org/wiki/Content_word">content words</a>. Our approach outperforms previous work on three datasets. Further, we present a new dataset of human-human conversations annotated with lexical aspects and present experiments that show the correlation of <a href="https://en.wikipedia.org/wiki/Telicity">telicity</a> with genre and discourse goals.</abstract>
      <url hash="33cc1b13">2020.coling-main.401</url>
      <doi>10.18653/v1/2020.coling-main.401</doi>
      <bibkey>kober-etal-2020-aspectuality</bibkey>
    </paper>
    <paper id="406">
      <title>Joint Persian Word Segmentation Correction and Zero-Width Non-Joiner Recognition Using BERT<fixed-case>P</fixed-case>ersian Word Segmentation Correction and Zero-Width Non-Joiner Recognition Using <fixed-case>BERT</fixed-case></title>
      <author><first>Ehsan</first><last>Doostmohammadi</last></author>
      <author><first>Minoo</first><last>Nassajian</last></author>
      <author><first>Adel</first><last>Rahimi</last></author>
      <pages>4612–4618</pages>
      <abstract>Words are properly segmented in the <a href="https://en.wikipedia.org/wiki/Persian_alphabet">Persian writing system</a> ; in practice, however, these writing rules are often neglected, resulting in single words being written disjointedly and multiple words written without any white spaces between them. This paper addresses the problems of <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a> and zero-width non-joiner (ZWNJ) recognition in <a href="https://en.wikipedia.org/wiki/Persian_language">Persian</a>, which we approach jointly as a sequence labeling problem. We achieved a macro-averaged F1-score of 92.40 % on a carefully collected corpus of 500 sentences with a high level of difficulty.</abstract>
      <url hash="29114f3f">2020.coling-main.406</url>
      <doi>10.18653/v1/2020.coling-main.406</doi>
      <bibkey>doostmohammadi-etal-2020-joint</bibkey>
    </paper>
    <paper id="407">
      <title>Syllable-based Neural Thai Word Segmentation<fixed-case>T</fixed-case>hai Word Segmentation</title>
      <author><first>Pattarawat</first><last>Chormai</last></author>
      <author><first>Ponrawee</first><last>Prasertsom</last></author>
      <author><first>Jin</first><last>Cheevaprawatdomrong</last></author>
      <author><first>Attapol</first><last>Rutherford</last></author>
      <pages>4619–4637</pages>
      <abstract>Word segmentation is a challenging pre-processing step for Thai Natural Language Processing due to the lack of explicit word boundaries. The previous systems rely on powerful neural network architecture alone and ignore linguistic substructures of Thai words. We utilize the linguistic observation that Thai strings can be segmented into syllables, which should narrow down the search space for the word boundaries and provide helpful features. Here, we propose a neural Thai Word Segmenter that uses syllable embeddings to capture linguistic constraints and uses dilated CNN filters to capture the environment of each character. Within this goal, we develop the first ML-based Thai orthographical syllable segmenter, which yields syllable embeddings to be used as <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> by the word segmenter. Our word segmentation system outperforms the previous state-of-the-art system in both <a href="https://en.wikipedia.org/wiki/Speed">speed</a> and <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on both in-domain and out-domain datasets.</abstract>
      <url hash="dfa4aac3">2020.coling-main.407</url>
      <doi>10.18653/v1/2020.coling-main.407</doi>
      <bibkey>chormai-etal-2020-syllable</bibkey>
    </paper>
    <paper id="409">
      <title>Morphological disambiguation from stemming data</title>
      <author><first>Antoine</first><last>Nzeyimana</last></author>
      <pages>4649–4660</pages>
      <abstract>Morphological analysis and disambiguation is an important task and a crucial preprocessing step in natural language processing of morphologically rich languages. Kinyarwanda, a morphologically rich language, currently lacks tools for automated morphological analysis. While linguistically curated finite state tools can be easily developed for <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological analysis</a>, the morphological richness of the language allows many ambiguous analyses to be produced, requiring effective disambiguation. In this paper, we propose learning to morphologically disambiguate Kinyarwanda verbal forms from a new stemming dataset collected through <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowd-sourcing</a>. Using <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a> and a feed-forward neural network based classifier, we achieve about 89 % non-contextualized disambiguation accuracy. Our experiments reveal that inflectional properties of <a href="https://en.wikipedia.org/wiki/Word_stem">stems</a> and morpheme association rules are the most discriminative features for <a href="https://en.wikipedia.org/wiki/Word-sense_disambiguation">disambiguation</a>.</abstract>
      <url hash="da1e0d78">2020.coling-main.409</url>
      <doi>10.18653/v1/2020.coling-main.409</doi>
      <bibkey>nzeyimana-2020-morphological</bibkey>
    </paper>
    <paper id="415">
      <title>Cross-lingual Transfer Learning for Grammatical Error Correction</title>
      <author><first>Ikumi</first><last>Yamashita</last></author>
      <author><first>Satoru</first><last>Katsumata</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Aizhan</first><last>Imankulova</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>4704–4715</pages>
      <abstract>In this study, we explore cross-lingual transfer learning in grammatical error correction (GEC) tasks. Many languages lack the resources required to train <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">GEC models</a>. Cross-lingual transfer learning from high-resource languages (the source models) is effective for training models of low-resource languages (the target models) for various tasks. However, in GEC tasks, the possibility of transferring grammatical knowledge (e.g., grammatical functions) across languages is not evident. Therefore, we investigate cross-lingual transfer learning methods for GEC. Our results demonstrate that <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> from other languages can improve the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">GEC</a>. We also demonstrate that proximity to source languages has a significant impact on the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of correcting certain types of errors.</abstract>
      <url hash="bcaa2415">2020.coling-main.415</url>
      <doi>10.18653/v1/2020.coling-main.415</doi>
      <bibkey>yamashita-etal-2020-cross</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/akces-gec">AKCES-GEC</pwcdataset>
    </paper>
    <paper id="417">
      <title>ContraCAT : Contrastive Coreference Analytical Templates for <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a><fixed-case>C</fixed-case>ontra<fixed-case>CAT</fixed-case>: Contrastive Coreference Analytical Templates for Machine Translation</title>
      <author><first>Dario</first><last>Stojanovski</last></author>
      <author><first>Benno</first><last>Krojer</last></author>
      <author><first>Denis</first><last>Peskov</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>4732–4749</pages>
      <abstract>Recent high scores on pronoun translation using context-aware neural machine translation have suggested that current approaches work well. ContraPro is a notable example of a contrastive challenge set for EnglishGerman pronoun translation. The high scores achieved by transformer models may suggest that they are able to effectively model the complicated set of <a href="https://en.wikipedia.org/wiki/Statistical_inference">inferences</a> required to carry out pronoun translation. This entails the ability to determine which entities could be referred to, identify which entity a source-language pronoun refers to (if any), and access the target-language grammatical gender for that entity. We first show through a series of targeted adversarial attacks that in fact current approaches are not able to model all of this information well. Inserting small amounts of <a href="https://en.wikipedia.org/wiki/Distraction">distracting information</a> is enough to strongly reduce scores, which should not be the case. We then create a new template test set ContraCAT, designed to individually assess the ability to handle the specific steps necessary for successful pronoun translation. Our analyses show that current approaches to context-aware NMT rely on a set of <a href="https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making">surface heuristics</a>, which break down when translations require real reasoning. We also propose an approach for augmenting the training data, with some improvements.</abstract>
      <url hash="c310e6b8">2020.coling-main.417</url>
      <doi>10.18653/v1/2020.coling-main.417</doi>
      <bibkey>stojanovski-etal-2020-contracat</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="420">
      <title>A Human Evaluation of AMR-to-English Generation Systems<fixed-case>AMR</fixed-case>-to-<fixed-case>E</fixed-case>nglish Generation Systems</title>
      <author><first>Emma</first><last>Manning</last></author>
      <author><first>Shira</first><last>Wein</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <pages>4773–4786</pages>
      <abstract>Most current state-of-the art systems for generating English text from Abstract Meaning Representation (AMR) have been evaluated only using automated metrics, such as <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>, which are known to be problematic for <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation</a>. In this work, we present the results of a new human evaluation which collects fluency and adequacy scores, as well as categorization of error types, for several recent AMR generation systems. We discuss the relative quality of these <a href="https://en.wikipedia.org/wiki/System">systems</a> and how our results compare to those of automatic metrics, finding that while the <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> are mostly successful in ranking systems overall, collecting human judgments allows for more nuanced comparisons. We also analyze <a href="https://en.wikipedia.org/wiki/Errors-in-variables_models">common errors</a> made by these <a href="https://en.wikipedia.org/wiki/System">systems</a>.</abstract>
      <url hash="783ef6da">2020.coling-main.420</url>
      <doi>10.18653/v1/2020.coling-main.420</doi>
      <bibkey>manning-etal-2020-human</bibkey>
    </paper>
    <paper id="423">
      <title>Manual Clustering and Spatial Arrangement of Verbs for Multilingual Evaluation and Typology Analysis</title>
      <author><first>Olga</first><last>Majewska</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Diana</first><last>McCarthy</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>4810–4824</pages>
      <abstract>We present the first evaluation of the applicability of a spatial arrangement method (SpAM) to a typologically diverse language sample, and its potential to produce semantic evaluation resources to support multilingual NLP, with a focus on verb semantics. We demonstrate <a href="https://en.wikipedia.org/wiki/Spamming">SpAM</a>’s utility in allowing for quick bottom-up creation of large-scale evaluation datasets that balance cross-lingual alignment with language specificity. Starting from a shared sample of 825 English verbs, translated into <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a>, <a href="https://en.wikipedia.org/wiki/Finnish_language">Finnish</a>, <a href="https://en.wikipedia.org/wiki/Polish_language">Polish</a>, and <a href="https://en.wikipedia.org/wiki/Italian_language">Italian</a>, we apply a two-phase annotation process which produces (i) semantic verb classes and (ii) fine-grained similarity scores for nearly 130 thousand verb pairs. We use the two types of verb data to (a) examine cross-lingual similarities and variation, and (b) evaluate the capacity of static and contextualised representation models to accurately reflect verb semantics, contrasting the performance of large language specific pretraining models with their multilingual equivalent on semantic clustering and lexical similarity, across different domains of verb meaning. We release the data from both phases as a large-scale multilingual resource, comprising 85 verb classes and nearly 130k pairwise similarity scores, offering a wealth of possibilities for further evaluation and research on multilingual verb semantics.</abstract>
      <url hash="d04c6a2a">2020.coling-main.423</url>
      <doi>10.18653/v1/2020.coling-main.423</doi>
      <bibkey>majewska-etal-2020-manual</bibkey>
      <pwccode url="https://github.com/om304/multi-spa-verb" additional="false">om304/multi-spa-verb</pwccode>
    </paper>
    <paper id="427">
      <title>Measuring Correlation-to-Causation Exaggeration in Press Releases</title>
      <author><first>Bei</first><last>Yu</last></author>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Lu</first><last>Guo</last></author>
      <author><first>Yingya</first><last>Li</last></author>
      <pages>4860–4872</pages>
      <abstract>Press releases have an increasingly strong influence on media coverage of health research ; however, they have been found to contain seriously exaggerated claims that can misinform the public and undermine public trust in science. In this study we propose an NLP approach to identify exaggerated causal claims made in health press releases that report on <a href="https://en.wikipedia.org/wiki/Observational_study">observational studies</a>, which are designed to establish <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence">correlational findings</a>, but are often exaggerated as causal. We developed a new <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> and trained <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> that can identify causal claims in the main statements in a <a href="https://en.wikipedia.org/wiki/Press_release">press release</a>. By comparing the claims made in a press release with the corresponding claims in the original research paper, we found that 22 % of <a href="https://en.wikipedia.org/wiki/Press_release">press releases</a> made exaggerated causal claims from <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence">correlational findings</a> in <a href="https://en.wikipedia.org/wiki/Observational_study">observational studies</a>. Furthermore, universities exaggerated more often than <a href="https://en.wikipedia.org/wiki/Academic_journal">journal publishers</a> by a ratio of 1.5 to 1. Encouragingly, the <a href="https://en.wikipedia.org/wiki/Exaggeration">exaggeration rate</a> has slightly decreased over the past 10 years, despite the increase of the total number of press releases. More research is needed to understand the cause of the decreasing pattern.</abstract>
      <url hash="67caa1f7">2020.coling-main.427</url>
      <doi>10.18653/v1/2020.coling-main.427</doi>
      <bibkey>yu-etal-2020-measuring</bibkey>
      <pwccode url="https://github.com/junwang4/correlation-to-causation-exaggeration" additional="false">junwang4/correlation-to-causation-exaggeration</pwccode>
    </paper>
    <paper id="428">
      <title>Inflating Topic Relevance with Ideology : A Case Study of Political Ideology Bias in Social Topic Detection Models</title>
      <author><first>Meiqi</first><last>Guo</last></author>
      <author><first>Rebecca</first><last>Hwa</last></author>
      <author><first>Yu-Ru</first><last>Lin</last></author>
      <author><first>Wen-Ting</first><last>Chung</last></author>
      <pages>4873–4885</pages>
      <abstract>We investigate the impact of political ideology biases in training data. Through a set of comparison studies, we examine the propagation of biases in several widely-used NLP models and its effect on the overall retrieval accuracy. Our work highlights the susceptibility of large, complex models to propagating the biases from human-selected input, which may lead to a deterioration of retrieval accuracy, and the importance of controlling for these <a href="https://en.wikipedia.org/wiki/Bias">biases</a>. Finally, as a way to mitigate the bias, we propose to learn a text representation that is invariant to <a href="https://en.wikipedia.org/wiki/Ideology">political ideology</a> while still judging topic relevance.</abstract>
      <url hash="1727ea61">2020.coling-main.428</url>
      <doi>10.18653/v1/2020.coling-main.428</doi>
      <bibkey>guo-etal-2020-inflating</bibkey>
      <pwccode url="https://github.com/MeiqiGuo/COLING2020-BiasStudy" additional="false">MeiqiGuo/COLING2020-BiasStudy</pwccode>
    </paper>
    <paper id="432">
      <title>Balanced Joint Adversarial Training for Robust Intent Detection and Slot Filling</title>
      <author><first>Xu</first><last>Cao</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Chongyang</first><last>Shi</last></author>
      <author><first>Chao</first><last>Wang</last></author>
      <author><first>Yao</first><last>Meng</last></author>
      <author><first>Changjian</first><last>Hu</last></author>
      <pages>4926–4936</pages>
      <abstract>Joint intent detection and slot filling has recently achieved tremendous success in advancing the performance of utterance understanding. However, many joint models still suffer from the robustness problem, especially on <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noisy inputs</a> or <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">rare / unseen events</a>. To address this issue, we propose a Joint Adversarial Training (JAT) model to improve the robustness of joint intent detection and slot filling, which consists of two parts : (1) automatically generating joint adversarial examples to attack the joint model, and (2) training the model to defend against the joint adversarial examples so as to robustify the model on small perturbations. As the generated joint adversarial examples have different impacts on the intent detection and slot filling loss, we further propose a Balanced Joint Adversarial Training (BJAT) model that applies a balance factor as a regularization term to the final loss function, which yields a stable training procedure. Extensive experiments and analyses on the lightweight models show that our proposed methods achieve significantly higher scores and substantially improve the robustness of both intent detection and slot filling. In addition, the combination of our BJAT with BERT-large achieves state-of-the-art results on two datasets.</abstract>
      <url hash="402afdc3">2020.coling-main.432</url>
      <doi>10.18653/v1/2020.coling-main.432</doi>
      <bibkey>cao-etal-2020-balanced</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="434">
      <title>Understanding Unnatural Questions Improves Reasoning over Text</title>
      <author><first>Xiaoyu</first><last>Guo</last></author>
      <author><first>Yuan-Fang</first><last>Li</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>4949–4955</pages>
      <abstract>Complex question answering (CQA) over raw text is a challenging <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. A prominent approach to this task is based on the programmer-interpreter framework, where the programmer maps the question into a sequence of reasoning actions and the <a href="https://en.wikipedia.org/wiki/Interpreter_(computing)">interpreter</a> then executes these actions on the raw text. Learning an effective CQA model requires large amounts of human-annotated data, consisting of the ground-truth sequence of reasoning actions, which is time-consuming and expensive to collect at scale. In this paper, we address the challenge of learning a high-quality programmer (parser) by projecting natural human-generated questions into unnatural machine-generated questions which are more convenient to parse. We firstly generate synthetic (question, action sequence) pairs by a data generator, and train a <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a> that associates synthetic questions with their corresponding action sequences. To capture the diversity when applied to natural questions, we learn a <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">projection model</a> to map natural questions into their most similar unnatural questions for which the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> can work well. Without any natural training data, our projection model provides high-quality action sequences for the CQA task. Experimental results show that the QA model trained exclusively with synthetic data outperforms its state-of-the-art counterpart trained on human-labeled data.</abstract>
      <url hash="d70b1c45">2020.coling-main.434</url>
      <doi>10.18653/v1/2020.coling-main.434</doi>
      <bibkey>guo-etal-2020-understanding</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="449">
      <title>A Mixture-of-Experts Model for Learning Multi-Facet Entity Embeddings</title>
      <author><first>Rana</first><last>Alshaikh</last></author>
      <author><first>Zied</first><last>Bouraoui</last></author>
      <author><first>Shelan</first><last>Jeawak</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>5124–5135</pages>
      <abstract>Various <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a> have already been proposed for learning entity embeddings from text descriptions. Such embeddings are commonly used for inferring properties of entities, for <a href="https://en.wikipedia.org/wiki/Recommender_system">recommendation</a> and entity-oriented search, and for injecting background knowledge into neural architectures, among others. Entity embeddings essentially serve as a compact encoding of a <a href="https://en.wikipedia.org/wiki/Similarity_(geometry)">similarity relation</a>, but <a href="https://en.wikipedia.org/wiki/Similarity_(geometry)">similarity</a> is an inherently multi-faceted notion. By representing entities as single vectors, existing methods leave it to downstream applications to identify these different facets, and to select the most relevant ones. In this paper, we propose a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that instead learns several vectors for each entity, each of which intuitively captures a different aspect of the considered domain. We use a mixture-of-experts formulation to jointly learn these facet-specific embeddings. The individual entity embeddings are learned using a variant of the GloVe model, which has the advantage that we can easily identify which properties are modelled well in which of the learned embeddings. This is exploited by an associated gating network, which uses pre-trained word vectors to encourage the properties that are modelled by a given embedding to be semantically coherent, i.e. to encourage each of the individual embeddings to capture a meaningful facet.</abstract>
      <url hash="bc4616c1">2020.coling-main.449</url>
      <doi>10.18653/v1/2020.coling-main.449</doi>
      <bibkey>alshaikh-etal-2020-mixture</bibkey>
      <pwccode url="https://github.com/rana-alshaikh/moeglove" additional="false">rana-alshaikh/moeglove</pwccode>
    </paper>
    <paper id="450">
      <title>Classifier Probes May Just Learn from Linear Context Features</title>
      <author><first>Jenny</first><last>Kunz</last></author>
      <author><first>Marco</first><last>Kuhlmann</last></author>
      <pages>5136–5146</pages>
      <abstract>Classifiers trained on auxiliary probing tasks are a popular tool to analyze the representations learned by neural sentence encoders such as <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> and ELMo. While many authors are aware of the difficulty to distinguish between extracting the linguistic structure encoded in the representations and learning the probing task, the validity of probing methods calls for further research. Using a neighboring word identity prediction task, we show that the token embeddings learned by neural sentence encoders contain a significant amount of information about the exact linear context of the token, and hypothesize that, with such information, learning standard probing tasks may be feasible even without additional linguistic structure. We develop this hypothesis into a framework in which analysis efforts can be scrutinized and argue that, with current <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> and baselines, conclusions that <a href="https://en.wikipedia.org/wiki/Representation_(arts)">representations</a> contain linguistic structure are not well-founded. Current probing methodology, such as restricting the <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifier’s expressiveness</a> or using strong baselines, can help to better estimate the complexity of <a href="https://en.wikipedia.org/wiki/Learning">learning</a>, but not build a foundation for speculations about the nature of the linguistic structure encoded in the learned representations.</abstract>
      <url hash="991d7e01">2020.coling-main.450</url>
      <doi>10.18653/v1/2020.coling-main.450</doi>
      <bibkey>kunz-kuhlmann-2020-classifier</bibkey>
      <pwccode url="https://github.com/jekunz/probing" additional="false">jekunz/probing</pwccode>
    </paper>
    <paper id="451">
      <title>Priorless Recurrent Networks Learn Curiously</title>
      <author><first>Jeff</first><last>Mitchell</last></author>
      <author><first>Jeffrey</first><last>Bowers</last></author>
      <pages>5147–5158</pages>
      <abstract>Recently, domain-general recurrent neural networks, without explicit linguistic inductive biases, have been shown to successfully reproduce a range of human language behaviours, such as accurately predicting number agreement between nouns and verbs. We show that such <a href="https://en.wikipedia.org/wiki/Social_network">networks</a> will also learn <a href="https://en.wikipedia.org/wiki/Number_agreement">number agreement</a> within <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">unnatural sentence structures</a>, i.e. structures that are not found within any <a href="https://en.wikipedia.org/wiki/Natural_language">natural languages</a> and which humans struggle to process. These results suggest that the models are learning from their input in a manner that is substantially different from human language acquisition, and we undertake an analysis of how the learned knowledge is stored in the weights of the network. We find that while the model has an effective understanding of singular versus plural for individual sentences, there is a lack of a unified concept of <a href="https://en.wikipedia.org/wiki/Agreement_(linguistics)">number agreement</a> connecting these processes across the full range of inputs. Moreover, the <a href="https://en.wikipedia.org/wiki/Weighting">weights</a> handling natural and unnatural structures overlap substantially, in a way that underlines the non-human-like nature of the knowledge learned by the <a href="https://en.wikipedia.org/wiki/Computer_network">network</a>.</abstract>
      <url hash="19f59145">2020.coling-main.451</url>
      <doi>10.18653/v1/2020.coling-main.451</doi>
      <bibkey>mitchell-bowers-2020-priorless</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="460">
      <title>Identifying Motion Entities in <a href="https://en.wikipedia.org/wiki/Natural_language">Natural Language</a> and A Case Study for Named Entity Recognition</title>
      <author><first>Ngoc Phuoc An</first><last>Vo</last></author>
      <author><first>Irene</first><last>Manotas</last></author>
      <author><first>Vadim</first><last>Sheinin</last></author>
      <author><first>Octavian</first><last>Popescu</last></author>
      <pages>5250–5258</pages>
      <abstract>Motion recognition is one of the basic cognitive capabilities of many life forms, however, detecting and understanding motion in text is not a trivial task. In addition, identifying motion entities in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> is not only challenging but also beneficial for a better <a href="https://en.wikipedia.org/wiki/Natural_language">natural language understanding</a>. In this paper, we present a Motion Entity Tagging (MET) model to identify entities in motion in a text using the Literal-Motion-in-Text (LiMiT) dataset for training and evaluating the model. Then we propose a new method to split clauses and phrases from complex and long motion sentences to improve the performance of our MET model. We also present results showing that motion features, in particular, entity in motion benefits the Named-Entity Recognition (NER) task. Finally, we present an analysis for the special co-occurrence relation between the person category in NER and animate entities in motion, which significantly improves the classification performance for the person category in NER.</abstract>
      <url hash="8845c18d">2020.coling-main.460</url>
      <doi>10.18653/v1/2020.coling-main.460</doi>
      <bibkey>vo-etal-2020-identifying</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
    </paper>
    <paper id="463">
      <title>User Memory Reasoning for Conversational Recommendation</title>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Honglei</first><last>Liu</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Pararth</first><last>Shah</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Philip</first><last>Yu</last></author>
      <pages>5288–5308</pages>
      <abstract>We study an end-to-end approach for conversational recommendation that dynamically manages and reasons over users’ past (offline) preferences and current (online) requests through a structured and cumulative user memory knowledge graph. This formulation extends existing state tracking beyond the boundary of a single dialog to user state tracking (UST). For this study, we create a new Memory Graph (MG)-Conversational Recommendation parallel corpus called MGConvRex with 7K+ human-to-human role-playing dialogs, grounded on a large-scale user memory bootstrapped from real-world user scenarios. MGConvRex captures human-level reasoning over user memory and has disjoint training / testing sets of users for zero-shot (cold-start) reasoning for recommendation. We propose a simple yet expandable formulation for constructing and updating the MG, and an end-to-end graph-based reasoning model that updates MG from unstructured utterances and predicts optimal dialog policies (eg recommendation) based on updated MG. The prediction of our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> inherits the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a>, providing a natural way to explain policies. Experiments are conducted for both <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">offline metrics</a> and <a href="https://en.wikipedia.org/wiki/Online_simulation">online simulation</a>, showing competitive results.</abstract>
      <url hash="5d0bdbff">2020.coling-main.463</url>
      <doi>10.18653/v1/2020.coling-main.463</doi>
      <bibkey>xu-etal-2020-user</bibkey>
    </paper>
    <paper id="464">
      <title>Diverse and Non-redundant Answer Set Extraction on Community QA based on DPPs<fixed-case>QA</fixed-case> based on <fixed-case>DPP</fixed-case>s</title>
      <author><first>Shogo</first><last>Fujita</last></author>
      <author><first>Tomohide</first><last>Shibata</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>5309–5320</pages>
      <abstract>In community-based question answering (CQA) platforms, it takes time for a user to get useful information from among many answers. Although one solution is an answer ranking method, the user still needs to read through the top-ranked answers carefully. This paper proposes a new <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> of selecting a diverse and non-redundant answer set rather than ranking the answers. Our method is based on determinantal point processes (DPPs), and it calculates the answer importance and similarity between answers by using BERT. We built a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> focusing on a Japanese CQA site, and the experiments on this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> demonstrated that the proposed method outperformed several baseline methods.</abstract>
      <url hash="d40601f3">2020.coling-main.464</url>
      <doi>10.18653/v1/2020.coling-main.464</doi>
      <bibkey>fujita-etal-2020-diverse</bibkey>
    </paper>
    <paper id="465">
      <title>An empirical analysis of existing <a href="https://en.wikipedia.org/wiki/System">systems</a> and datasets toward general simple question answering</title>
      <author><first>Namgi</first><last>Han</last></author>
      <author><first>Goran</first><last>Topic</last></author>
      <author><first>Hiroshi</first><last>Noji</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>5321–5334</pages>
      <abstract>In this paper, we evaluate the progress of our field toward solving simple factoid questions over a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a>, a practically important problem in natural language interface to database. As in other natural language understanding tasks, a common practice for this task is to train and evaluate a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> on a single dataset, and recent studies suggest that SimpleQuestions, the most popular and largest dataset, is nearly solved under this setting. However, this common setting does not evaluate the <a href="https://en.wikipedia.org/wiki/Robust_statistics">robustness</a> of the systems outside of the distribution of the used training data. We rigorously evaluate such <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of existing <a href="https://en.wikipedia.org/wiki/System">systems</a> using different <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>. Our analysis, including shifting of training and test datasets and training on a union of the <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, suggests that our progress in solving SimpleQuestions dataset does not indicate the success of more general simple question answering. We discuss a possible future direction toward this goal.</abstract>
      <url hash="7541df51">2020.coling-main.465</url>
      <doi>10.18653/v1/2020.coling-main.465</doi>
      <bibkey>han-etal-2020-empirical</bibkey>
      <pwccode url="https://github.com/aistairc/simple-qa-analysis" additional="false">aistairc/simple-qa-analysis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/freebaseqa">FreebaseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="472">
      <title>Scientific Keyphrase Identification and Classification by Pre-Trained Language Models Intermediate Task Transfer Learning</title>
      <author><first>Seoyeon</first><last>Park</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>5409–5419</pages>
      <abstract>Scientific keyphrase identification and classification is the task of detecting and classifying keyphrases from scholarly text with their types from a set of predefined classes. This task has a wide range of benefits, but it is still challenging in performance due to the lack of large amounts of <a href="https://en.wikipedia.org/wiki/Labeled_data">labeled data</a> required for training <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural models</a>. In order to overcome this challenge, we explore pre-trained language models BERT and SciBERT with intermediate task transfer learning, using 42 data-rich related intermediate-target task combinations. We reveal that intermediate task transfer learning on SciBERT induces a better starting point for target task fine-tuning compared with BERT and achieves competitive performance in scientific keyphrase identification and classification compared to both previous works and strong baselines. Interestingly, we observe that BERT with intermediate task transfer learning fails to improve the performance of scientific keyphrase identification and classification potentially due to significant catastrophic forgetting. This result highlights that <a href="https://en.wikipedia.org/wiki/Scientific_knowledge">scientific knowledge</a> achieved during the pre-training of <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> on large scientific collections plays an important role in the target <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. We also observe that sequence tagging related intermediate tasks, especially syntactic structure learning tasks such as POS Tagging, tend to work best for scientific keyphrase identification and classification.</abstract>
      <url hash="71f7b7b4">2020.coling-main.472</url>
      <doi>10.18653/v1/2020.coling-main.472</doi>
      <bibkey>park-caragea-2020-scientific</bibkey>
    </paper>
    <paper id="473">
      <title>Exploiting Microblog Conversation Structures to Detect Rumors</title>
      <author><first>Jiawen</first><last>Li</last></author>
      <author><first>Yudianto</first><last>Sujana</last></author>
      <author><first>Hung-Yu</first><last>Kao</last></author>
      <pages>5420–5429</pages>
      <abstract>As one of the most popular <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a>, <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> has become a primary source of information for many people. Unfortunately, both valid information and <a href="https://en.wikipedia.org/wiki/Rumor">rumors</a> are propagated on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> due to the lack of an automatic information verification system. Twitter users communicate by replying to other users’ messages, forming a conversation structure. Using this <a href="https://en.wikipedia.org/wiki/Structure">structure</a>, users can decide whether the information in the source tweet is a <a href="https://en.wikipedia.org/wiki/Rumor">rumor</a> by reading the tweet’s replies, which voice other users’ stances on the tweet. The majority of rumor detection researchers process such tweets based on time, ignoring the conversation structure. To reap the benefits of the Twitter conversation structure, we developed a model to detect <a href="https://en.wikipedia.org/wiki/Rumor">rumors</a> by modeling conversation structure as a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>. Thus, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s improved representation of the conversation structure enhances its rumor detection accuracy. The experimental results on two rumor datasets show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms several baseline models, including a state-of-the-art model</abstract>
      <url hash="21f44db3">2020.coling-main.473</url>
      <doi>10.18653/v1/2020.coling-main.473</doi>
      <bibkey>li-etal-2020-exploiting</bibkey>
    </paper>
    <paper id="477">
      <title>Words are the Window to the Soul : Language-based User Representations for Fake News Detection</title>
      <author><first>Marco</first><last>Del Tredici</last></author>
      <author><first>Raquel</first><last>Fernández</last></author>
      <pages>5467–5479</pages>
      <abstract>Cognitive and social traits of individuals are reflected in <a href="https://en.wikipedia.org/wiki/Usage_(language)">language use</a>. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that creates representations of individuals on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> based only on the language they produce, and use them to detect <a href="https://en.wikipedia.org/wiki/Fake_news">fake news</a>. We show that language-based user representations are beneficial for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the <a href="https://en.wikipedia.org/wiki/Social_graph">social graph</a> to assess the presence of the Echo Chamber effect in our data.</abstract>
      <url hash="d1089209">2020.coling-main.477</url>
      <doi>10.18653/v1/2020.coling-main.477</doi>
      <bibkey>del-tredici-fernandez-2020-words</bibkey>
    </paper>
    <paper id="481">
      <title>Go Simple and Pre-Train on Domain-Specific Corpora : On the Role of Training Data for Text Classification</title>
      <author><first>Aleksandra</first><last>Edwards</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <author><first>Hélène</first><last>De Ribaupierre</last></author>
      <author><first>Alun</first><last>Preece</last></author>
      <pages>5522–5529</pages>
      <abstract>Pre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including <a href="https://en.wikipedia.org/wiki/Text_classification">text classification</a>. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>, i.e., <a href="https://en.wikipedia.org/wiki/FastText">fastText</a> (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> or <a href="https://en.wikipedia.org/wiki/Language_model">language models</a>. As for the comparison, BERT outperforms all baselines in standard <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> with large training sets. However, in settings with small training datasets a simple method like <a href="https://en.wikipedia.org/wiki/FastText">fastText</a> coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.</abstract>
      <url hash="161bb6a3">2020.coling-main.481</url>
      <doi>10.18653/v1/2020.coling-main.481</doi>
      <bibkey>edwards-etal-2020-go</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="483">
      <title>Exploiting Narrative Context and A Priori Knowledge of Categories in Textual Emotion Classification</title>
      <author><first>Hikari</first><last>Tanabe</last></author>
      <author><first>Tetsuji</first><last>Ogawa</last></author>
      <author><first>Tetsunori</first><last>Kobayashi</last></author>
      <author><first>Yoshihiko</first><last>Hayashi</last></author>
      <pages>5535–5540</pages>
      <abstract>Recognition of the mental state of a human character in text is a major challenge in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. In this study, we investigate the efficacy of the <a href="https://en.wikipedia.org/wiki/Narrative">narrative context</a> in recognizing the emotional states of human characters in text and discuss an approach to make use of a priori knowledge regarding the employed emotion category system. Specifically, we experimentally show that the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of <a href="https://en.wikipedia.org/wiki/Emotion_classification">emotion classification</a> is substantially increased by encoding the preceding context of the target sentence using a BERT-based text encoder. We also compare ways to incorporate a priori knowledge of emotion categories by altering the <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> used in training, in which our proposal of <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> that jointly learns to classify positive / negative polarity of emotions is included. The experimental results suggest that, when using Plutchik’s Wheel of Emotions, it is better to jointly classify the basic emotion categories with positive / negative polarity rather than directly exploiting its characteristic structure in which eight basic categories are arranged in a wheel.</abstract>
      <url hash="b912df67">2020.coling-main.483</url>
      <doi>10.18653/v1/2020.coling-main.483</doi>
      <bibkey>tanabe-etal-2020-exploiting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/story-commonsense">Story Commonsense</pwcdataset>
    </paper>
    <paper id="485">
      <title>Few-Shot Text Classification with Edge-Labeling Graph Neural Network-Based Prototypical Network</title>
      <author><first>Chen</first><last>Lyu</last></author>
      <author><first>Weijie</first><last>Liu</last></author>
      <author><first>Ping</first><last>Wang</last></author>
      <pages>5547–5552</pages>
      <abstract>In this paper, we propose a new few-shot text classification method. Compared with supervised learning methods which require a large corpus of labeled documents, our method aims to make it possible to classify unlabeled text with few labeled data. To achieve this goal, we take advantage of advanced pre-trained language model to extract the <a href="https://en.wikipedia.org/wiki/Semantic_feature">semantic features</a> of each document. Furthermore, we utilize an edge-labeling graph neural network to implicitly models the intra-cluster similarity and the inter-cluster dissimilarity of the documents. Finally, we take the results of the graph neural network as the input of a prototypical network to classify the unlabeled texts. We verify the effectiveness of our method on a sentiment analysis dataset and a relation classification dataset and achieve the state-of-the-art performance on both tasks.</abstract>
      <url hash="e5c0f4ba">2020.coling-main.485</url>
      <doi>10.18653/v1/2020.coling-main.485</doi>
      <bibkey>lyu-etal-2020-shot</bibkey>
    </paper>
    <paper id="488">
      <title>Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification</title>
      <author><first>Timo</first><last>Schick</last></author>
      <author><first>Helmut</first><last>Schmid</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>5569–5578</pages>
      <abstract>A recent approach for few-shot text classification is to convert textual inputs to cloze questions that contain some form of task description, process them with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>’s abilities. To mitigate this issue, we devise an approach that automatically finds such a <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mapping</a> given small amounts of training data. For a number of tasks, the <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mapping</a> found by our approach performs almost as well as hand-crafted label-to-word mappings.</abstract>
      <url hash="d5041c46">2020.coling-main.488</url>
      <doi>10.18653/v1/2020.coling-main.488</doi>
      <bibkey>schick-etal-2020-automatically</bibkey>
      <pwccode url="https://github.com/timoschick/pet" additional="false">timoschick/pet</pwccode>
    </paper>
    <paper id="490">
      <title>IntKB : A Verifiable Interactive Framework for Knowledge Base Completion<fixed-case>I</fixed-case>nt<fixed-case>KB</fixed-case>: A Verifiable Interactive Framework for Knowledge Base Completion</title>
      <author><first>Bernhard</first><last>Kratzwald</last></author>
      <author><first>Guo</first><last>Kunpeng</last></author>
      <author><first>Stefan</first><last>Feuerriegel</last></author>
      <author><first>Dennis</first><last>Diefenbach</last></author>
      <pages>5591–5603</pages>
      <abstract>Knowledge bases (KBs) are essential for many downstream NLP tasks, yet their prime shortcoming is that they are often incomplete. State-of-the-art <a href="https://en.wikipedia.org/wiki/Software_framework">frameworks</a> for KB completion often lack sufficient accuracy to work fully automated without <a href="https://en.wikipedia.org/wiki/Supervisor">human supervision</a>. As a remedy, we propose : a novel interactive framework for KB completion from text based on a question answering pipeline. Our framework is tailored to the specific needs of a human-in-the-loop paradigm : (i) We generate facts that are aligned with text snippets and are thus immediately verifiable by humans. (ii) Our system is designed such that it continuously learns during the KB completion task and, therefore, significantly improves its performance upon initial zero- and few-shot relations over time. (iii) We only trigger <a href="https://en.wikipedia.org/wiki/Human–computer_interaction">human interactions</a> when there is enough information for a correct prediction. Therefore, we train our <a href="https://en.wikipedia.org/wiki/System">system</a> with negative examples and a fold-option if there is no answer. Our framework yields a favorable performance : <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> achieves a hit@1 ratio of 29.7 % for initially unseen relations, upon which <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> gradually improves to 46.2 %.</abstract>
      <url hash="146f8762">2020.coling-main.490</url>
      <doi>10.18653/v1/2020.coling-main.490</doi>
      <bibkey>kratzwald-etal-2020-intkb</bibkey>
      <pwccode url="https://github.com/bernhard2202/intkb" additional="false">bernhard2202/intkb</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nell">NELL</pwcdataset>
    </paper>
    <paper id="496">
      <title>Multimodal Sentence Summarization via Multimodal Selective Encoding</title>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Junnan</first><last>Zhu</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>5655–5667</pages>
      <abstract>This paper studies the problem of generating a summary for a given sentence-image pair. Existing multimodal sequence-to-sequence approaches mainly focus on enhancing the <a href="https://en.wikipedia.org/wiki/Codec">decoder</a> by visual signals, while ignoring that the <a href="https://en.wikipedia.org/wiki/Image">image</a> can improve the ability of the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> to identify highlights of a news event or a document. Thus, we propose a multimodal selective gate network that considers reciprocal relationships between textual and multi-level visual features, including global image descriptor, activation grids, and object proposals, to select highlights of the event when encoding the source sentence. In addition, we introduce a modality regularization to encourage the summary to capture the highlights embedded in the image more accurately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence.</abstract>
      <url hash="c739b976">2020.coling-main.496</url>
      <doi>10.18653/v1/2020.coling-main.496</doi>
      <bibkey>li-etal-2020-multimodal</bibkey>
    </paper>
    <paper id="499">
      <title>How Domain Terminology Affects Meeting Summarization Performance</title>
      <author><first>Jia Jin</first><last>Koay</last></author>
      <author><first>Alexander</first><last>Roustai</last></author>
      <author><first>Xiaojin</first><last>Dai</last></author>
      <author><first>Dillon</first><last>Burns</last></author>
      <author><first>Alec</first><last>Kerrigan</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <pages>5689–5695</pages>
      <abstract>Meetings are essential to modern organizations. Numerous meetings are held and recorded daily, more than can ever be comprehended. A meeting summarization system that identifies salient utterances from the transcripts to automatically generate meeting minutes can help. It empowers users to rapidly search and sift through large meeting collections. To date, the impact of domain terminology on the performance of meeting summarization remains understudied, despite that meetings are rich with <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a>. In this paper, we create gold-standard annotations for domain terminology on a sizable meeting corpus ; they are known as <a href="https://en.wikipedia.org/wiki/Jargon">jargon terms</a>. We then analyze the performance of a meeting summarization system with and without <a href="https://en.wikipedia.org/wiki/Jargon">jargon terms</a>. Our findings reveal that domain terminology can have a substantial impact on <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a> performance. We publicly release all domain terminology to advance research in meeting summarization.</abstract>
      <url hash="fd1ed29e">2020.coling-main.499</url>
      <doi>10.18653/v1/2020.coling-main.499</doi>
      <bibkey>koay-etal-2020-domain</bibkey>
      <pwccode url="https://github.com/ucfnlp/meeting-domain-terminology" additional="false">ucfnlp/meeting-domain-terminology</pwccode>
    </paper>
    <paper id="502">
      <title>On the Faithfulness for E-commerce Product Summarization<fixed-case>E</fixed-case>-commerce Product Summarization</title>
      <author><first>Peng</first><last>Yuan</last></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Song</first><last>Xu</last></author>
      <author><first>Youzheng</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <author><first>Bowen</first><last>Zhou</last></author>
      <pages>5712–5717</pages>
      <abstract>In this work, we present a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to generate e-commerce product summaries. The consistency between the generated summary and the product attributes is an essential criterion for the ecommerce product summarization task. To enhance the consistency, first, we encode the product attribute table to guide the process of summary generation. Second, we identify the <a href="https://en.wikipedia.org/wiki/Attribute_(grammar)">attribute words</a> from the vocabulary, and we constrain these <a href="https://en.wikipedia.org/wiki/Attribute_(grammar)">attribute words</a> can be presented in the summaries only through copying from the source, i.e., the <a href="https://en.wikipedia.org/wiki/Attribute_(grammar)">attribute words</a> not in the source can not be generated. We construct a Chinese e-commerce product summarization dataset, and the experimental results on this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> demonstrate that our models significantly improve the <a href="https://en.wikipedia.org/wiki/Faithfulness">faithfulness</a>.</abstract>
      <url hash="58f00e0f">2020.coling-main.502</url>
      <doi>10.18653/v1/2020.coling-main.502</doi>
      <bibkey>yuan-etal-2020-faithfulness</bibkey>
      <pwccode url="https://github.com/ypnlp/coling" additional="false">ypnlp/coling</pwccode>
    </paper>
    <paper id="508">
      <title>Variation in Coreference Strategies across Genres and <a href="https://en.wikipedia.org/wiki/Mass_media">Production Media</a></title>
      <author><first>Berfin</first><last>Aktaş</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>5774–5785</pages>
      <abstract>In response to (i) inconclusive results in the literature as to the properties of coreference chains in written versus spoken language, and (ii) a general lack of work on automatic coreference resolution on both spoken language and social media, we undertake a corpus study involving the various genre sections of Ontonotes, the Switchboard corpus, and a corpus of Twitter conversations. Using a set of <a href="https://en.wikipedia.org/wiki/Measurement">measures</a> that previously have been applied individually to different <a href="https://en.wikipedia.org/wiki/Data_set">data sets</a>, we find fairly clear patterns of behavior for the different genres / media. Besides their role for psycholinguistic investigation (why do we employ different coreference strategies when we write or speak) and for the placement of <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> in the spokenwritten continuum, we see our results as a contribution to approaching genre-/media-specific coreference resolution.</abstract>
      <url hash="ae9b8700">2020.coling-main.508</url>
      <doi>10.18653/v1/2020.coling-main.508</doi>
      <bibkey>aktas-stede-2020-variation</bibkey>
      <pwccode url="https://github.com/berfingit/coreference-variation" additional="false">berfingit/coreference-variation</pwccode>
    </paper>
    <paper id="512">
      <title>Using Eye-tracking Data to Predict the Readability of Brazilian Portuguese Sentences in Single-task, Multi-task and Sequential Transfer Learning Approaches<fixed-case>B</fixed-case>razilian <fixed-case>P</fixed-case>ortuguese Sentences in Single-task, Multi-task and Sequential Transfer Learning Approaches</title>
      <author><first>Sidney</first><last>Evaldo Leal</last></author>
      <author><first>João Marcos</first><last>Munguba Vieira</last></author>
      <author><first>Erica</first><last>dos Santos Rodrigues</last></author>
      <author><first>Elisângela</first><last>Nogueira Teixeira</last></author>
      <author><first>Sandra</first><last>Aluísio</last></author>
      <pages>5821–5831</pages>
      <abstract>Sentence complexity assessment is a relatively new <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a>. One of its aims is to highlight in a text which sentences are more complex to support the simplification of contents for a target audience (e.g., children, cognitively impaired users, <a href="https://en.wikipedia.org/wiki/Foreign_language">non-native speakers</a> and <a href="https://en.wikipedia.org/wiki/Literacy">low-literacy readers</a> (Scarton and Specia, 2018)). This task is evaluated using <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> of pairs of aligned sentences including the complex and simple version of the same sentence. For <a href="https://en.wikipedia.org/wiki/Brazilian_Portuguese">Brazilian Portuguese</a>, the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> was addressed by (Leal et al., 2018), who set up the first <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> to evaluate the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> in this <a href="https://en.wikipedia.org/wiki/Language">language</a>, reaching 87.8 % of <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> with <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">linguistic features</a>. The present work advances these results, using models inspired by (Gonzalez-Garduno and Sgaard, 2018), which hold the state-of-the-art for the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>, with multi-task learning and eye-tracking measures. First-Pass Duration, Total Regression Duration and Total Fixation Duration were used in two moments ; first to select a subset of linguistic features and then as an auxiliary task in the multi-task and sequential learning models. The best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> proposed here reaches the new state-of-the-art for <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese</a> with 97.5 % accuracy 1, an increase of almost 10 points compared to the best previous results, in addition to proposing improvements in the public dataset after analysing the errors of our best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>.</abstract>
      <url hash="7976d13b">2020.coling-main.512</url>
      <doi>10.18653/v1/2020.coling-main.512</doi>
      <bibkey>evaldo-leal-etal-2020-using</bibkey>
    </paper>
    <paper id="513">
      <title>Retrieving Skills from Job Descriptions : A Language Model Based Extreme Multi-label Classification Framework</title>
      <author><first>Akshay</first><last>Bhola</last></author>
      <author><first>Kishaloy</first><last>Halder</last></author>
      <author><first>Animesh</first><last>Prasad</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <pages>5832–5842</pages>
      <abstract>We introduce a deep learning model to learn the set of enumerated job skills associated with a job description. In our analysis of a large-scale government job portal mycareersfuture.sg, we observe that as much as 65 % of <a href="https://en.wikipedia.org/wiki/Job_description">job descriptions</a> miss describing a significant number of relevant skills. Our model addresses this task from the perspective of an extreme multi-label classification (XMLC) problem, where descriptions are the evidence for the binary relevance of thousands of individual skills. Building upon the current state-of-the-art language modeling approaches such as BERT, we show our XMLC method improves on an existing baseline solution by over 9 % and 7 % absolute improvements in terms of recall and normalized discounted cumulative gain. We further show that our approach effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings by taking into account the structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process. We further show that our approach, to ensure the BERT-XMLC model accounts for structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process, effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings. To facilitate future research and replication of our work, we have made the dataset and the implementation of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> publicly available.</abstract>
      <url hash="6da419b0">2020.coling-main.513</url>
      <doi>10.18653/v1/2020.coling-main.513</doi>
      <bibkey>bhola-etal-2020-retrieving</bibkey>
      <pwccode url="https://github.com/wing-nus/jd2skills-bert-xmlc" additional="false">wing-nus/jd2skills-bert-xmlc</pwccode>
    </paper>
    <paper id="515">
      <title>An Analysis of Dataset Overlap on Winograd-Style Tasks<fixed-case>W</fixed-case>inograd-Style Tasks</title>
      <author><first>Ali</first><last>Emami</last></author>
      <author><first>Kaheer</first><last>Suleman</last></author>
      <author><first>Adam</first><last>Trischler</last></author>
      <author><first>Jackie Chi Kit</first><last>Cheung</last></author>
      <pages>5855–5865</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Winograd_Schema_Challenge">Winograd Schema Challenge (WSC)</a> and variants inspired by <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> have become important benchmarks for common-sense reasoning (CSR). Model performance on the WSC has quickly progressed from chance-level to near-human using neural language models trained on massive corpora. In this paper, we analyze the effects of varying degrees of overlaps that occur between these <a href="https://en.wikipedia.org/wiki/Corpus_linguistics">corpora</a> and the test instances in WSC-style tasks. We find that a large number of test instances overlap considerably with the pretraining corpora on which state-of-the-art <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> are trained, and that a significant drop in classification accuracy occurs when <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> are evaluated on instances with minimal overlap. Based on these results, we provide the WSC-Web dataset, consisting of over 60k pronoun disambiguation problems scraped from web data, being both the largest corpus to date, and having a significantly lower proportion of overlaps with current pretraining corpora.</abstract>
      <url hash="362f2902">2020.coling-main.515</url>
      <doi>10.18653/v1/2020.coling-main.515</doi>
      <bibkey>emami-etal-2020-analysis</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="518">
      <title>Do n’t Patronize Me ! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities</title>
      <author><first>Carla</first><last>Perez Almendros</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>5891–5902</pages>
      <abstract>In this paper, we introduce a new annotated dataset which is aimed at supporting the development of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP models</a> to identify and categorize language that is patronizing or condescending towards vulnerable communities (e.g. refugees, homeless people, poor families). While the prevalence of such <a href="https://en.wikipedia.org/wiki/Language">language</a> in the general media has long been shown to have harmful effects, it differs from other types of harmful language, in that it is generally used unconsciously and with good intentions. We furthermore believe that the often subtle nature of patronizing and condescending language (PCL) presents an interesting technical challenge for the NLP community. Our analysis of the proposed dataset shows that identifying PCL is hard for standard NLP models, with language models such as BERT achieving the best results.</abstract>
      <url hash="8db103e8">2020.coling-main.518</url>
      <doi>10.18653/v1/2020.coling-main.518</doi>
      <bibkey>perez-almendros-etal-2020-dont</bibkey>
    </paper>
    <paper id="523">
      <title>WikiUMLS : Aligning UMLS to Wikipedia via Cross-lingual Neural Ranking<fixed-case>W</fixed-case>iki<fixed-case>UMLS</fixed-case>: Aligning <fixed-case>UMLS</fixed-case> to <fixed-case>W</fixed-case>ikipedia via Cross-lingual Neural Ranking</title>
      <author><first>Afshin</first><last>Rahimi</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>5957–5962</pages>
      <abstract>We present our work on aligning the <a href="https://en.wikipedia.org/wiki/Unified_Medical_Language_System">Unified Medical Language System (UMLS)</a> to <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, to facilitate manual alignment of the two resources. We propose a cross-lingual neural reranking model to match a UMLS concept with a Wikipedia page, which achieves a recall@1of 72 %, a substantial improvement of 20 % over word- and char-level BM25, enabling manual alignment with minimal effort. We release our resources, including ranked Wikipedia pages for 700k UMLSconcepts, and WikiUMLS, a dataset for training and evaluation of alignment models between <a href="https://en.wikipedia.org/wiki/Unified_Modeling_Language">UMLS</a> and <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> collected from <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a>. This will provide easier access to <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> for <a href="https://en.wikipedia.org/wiki/Health_professional">health professionals</a>, patients, and <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP systems</a>, including in multilingual settings.</abstract>
      <url hash="4044fcdc">2020.coling-main.523</url>
      <doi>10.18653/v1/2020.coling-main.523</doi>
      <bibkey>rahimi-etal-2020-wikiumls</bibkey>
      <pwccode url="https://github.com/afshinrahimi/wikiumls" additional="false">afshinrahimi/wikiumls</pwccode>
    </paper>
    <paper id="524">
      <title>The Transference Architecture for Automatic Post-Editing</title>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Hongfei</first><last>Xu</last></author>
      <author><first>Nico</first><last>Herbig</last></author>
      <author><first>Sudip Kumar</first><last>Naskar</last></author>
      <author><first>Antonio</first><last>Krüger</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>5963–5974</pages>
      <abstract>In automatic post-editing (APE) it makes sense to condition post-editing (pe) decisions on both the source (src) and the machine translated text (mt) as input. This has led to multi-encoder based neural APE approaches. A research challenge now is the search for architectures that best support the capture, preparation and provision of src and mt information and its integration with pe decisions. In this paper we present an efficient multi-encoder based APE model, called <a href="https://en.wikipedia.org/wiki/Transference">transference</a>. Unlike previous approaches, it (i) uses a transformer encoder block for src, (ii) followed by a decoder block, but without masking for self-attention on mt, which effectively acts as second encoder combining src   mt, and (iii) feeds this representation into a final decoder block generating pe. Our model outperforms the best performing systems by 1 BLEU point on the WMT 2016, 2017, and 2018 EnglishGerman APE shared tasks (PBSMT and NMT). Furthermore, the results of our model on the WMT 2019 APE task using NMT data shows a comparable performance to the state-of-the-art system. The <a href="https://en.wikipedia.org/wiki/Time_complexity">inference time</a> of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is similar to the vanilla transformer-based NMT system although our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> deals with two separate encoders. We further investigate the importance of our newly introduced second encoder and find that a too small amount of <a href="https://en.wikipedia.org/wiki/Abstraction_layer">layers</a> does hurt the performance, while reducing the number of layers of the <a href="https://en.wikipedia.org/wiki/Code">decoder</a> does not matter much.</abstract>
      <url hash="66fed489">2020.coling-main.524</url>
      <doi>10.18653/v1/2020.coling-main.524</doi>
      <bibkey>pal-etal-2020-transference</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="526">
      <title>A Simple and Effective Approach to Robust Unsupervised Bilingual Dictionary Induction</title>
      <author><first>Yanyang</first><last>Li</last></author>
      <author><first>Yingfeng</first><last>Luo</last></author>
      <author><first>Ye</first><last>Lin</last></author>
      <author><first>Quan</first><last>Du</last></author>
      <author><first>Huizhen</first><last>Wang</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>5990–6001</pages>
      <abstract>Unsupervised Bilingual Dictionary Induction methods based on the initialization and the <a href="https://en.wikipedia.org/wiki/Autodidacticism">self-learning</a> have achieved great success in similar language pairs, e.g., <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">English-Spanish</a>. But they still fail and have an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 0 % in many distant language pairs, e.g., English-Japanese. In this work, we show that this failure results from the gap between the actual initialization performance and the minimum initialization performance for the self-learning to succeed. We propose Iterative Dimension Reduction to bridge this gap. Our experiments show that this simple method does not hamper the performance of <a href="https://en.wikipedia.org/wiki/Similarity_measure">similar language pairs</a> and achieves an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 13.64 55.53 % between <a href="https://en.wikipedia.org/wiki/English_language">English</a> and four distant languages, i.e., <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a>, <a href="https://en.wikipedia.org/wiki/Vietnamese_language">Vietnamese</a> and <a href="https://en.wikipedia.org/wiki/Thai_language">Thai</a>.</abstract>
      <url hash="7185992b">2020.coling-main.526</url>
      <doi>10.18653/v1/2020.coling-main.526</doi>
      <bibkey>li-etal-2020-simple</bibkey>
    </paper>
    <paper id="527">
      <title>Data Selection for Bilingual Lexicon Induction from Specialized Comparable Corpora</title>
      <author><first>Martin</first><last>Laville</last></author>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Phillippe</first><last>Langlais</last></author>
      <pages>6002–6012</pages>
      <abstract>Narrow specialized comparable corpora are often small in size. This particularity makes it difficult to build efficient <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to acquire translation equivalents, especially for less frequent and rare words. One way to overcome this issue is to enrich the specialized corpora with out-of-domain resources. Although some recent studies have shown improvements using <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, the enrichment method was roughly conducted by adding out-of-domain data with no particular attention given to how to enrich words and how to do it optimally. In this paper, we contrast several data selection techniques to improve bilingual lexicon induction from specialized comparable corpora. We first apply two well-established data selection techniques often used in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> that is : Tf-Idf and <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross entropy</a>. Then, we propose to exploit BERT for data selection. Overall, all the proposed techniques improve the quality of the extracted bilingual lexicons by a large margin. The best performing <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is the <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross entropy</a>, obtaining a gain of about 4 points in MAP while decreasing <a href="https://en.wikipedia.org/wiki/Time_complexity">computation time</a> by a factor of 10.</abstract>
      <url hash="8944ad0c">2020.coling-main.527</url>
      <doi>10.18653/v1/2020.coling-main.527</doi>
      <bibkey>laville-etal-2020-data</bibkey>
    </paper>
    <paper id="528">
      <title>A Locally Linear Procedure for Word Translation</title>
      <author><first>Soham</first><last>Dan</last></author>
      <author><first>Hagai</first><last>Taitelbaum</last></author>
      <author><first>Jacob</first><last>Goldberger</last></author>
      <pages>6013–6018</pages>
      <abstract>Learning a mapping between word embeddings of two languages given a dictionary is an important problem with several applications. A common mapping approach is using an <a href="https://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal matrix</a>. The Orthogonal Procrustes Analysis (PA) algorithm can be applied to find the optimal <a href="https://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal matrix</a>. This <a href="https://en.wikipedia.org/wiki/Solution">solution</a> restricts the expressiveness of the translation model which may result in sub-optimal translations. We propose a natural extension of the PA algorithm that uses multiple orthogonal translation matrices to model the mapping and derive an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> to learn these multiple <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">matrices</a>. We achieve better performance in a bilingual word translation task and a cross-lingual word similarity task compared to the single matrix baseline. We also show how multiple matrices can model multiple senses of a word.</abstract>
      <url hash="b0c6a36e">2020.coling-main.528</url>
      <doi>10.18653/v1/2020.coling-main.528</doi>
      <bibkey>dan-etal-2020-locally</bibkey>
    </paper>
    <paper id="530">
      <title>The SADID Evaluation Datasets for Low-Resource Spoken Language Machine Translation of Arabic Dialects<fixed-case>SADID</fixed-case> Evaluation Datasets for Low-Resource Spoken Language Machine Translation of <fixed-case>A</fixed-case>rabic Dialects</title>
      <author><first>Wael</first><last>Abid</last></author>
      <pages>6030–6043</pages>
      <abstract>Low-resource Machine Translation recently gained a lot of popularity, and for certain languages, it has made great strides. However, it is still difficult to track progress in other languages for which there is no publicly available evaluation data. In this paper, we introduce <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark datasets</a> for <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> and its dialects. We describe our design process and motivations and analyze the <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> to understand their resulting properties. Numerous successful attempts use large monolingual corpora to augment low-resource pairs. We try to approach augmentation differently and investigate whether it is possible to improve MT models without any external sources of data. We accomplish this by bootstrapping existing <a href="https://en.wikipedia.org/wiki/Parallelism_(grammar)">parallel sentences</a> and complement this with multilingual training to achieve strong baselines.</abstract>
      <url hash="896ff392">2020.coling-main.530</url>
      <doi>10.18653/v1/2020.coling-main.530</doi>
      <bibkey>abid-2020-sadid</bibkey>
    </paper>
    <paper id="532">
      <title>Understanding Translationese in Multi-view Embedding Spaces</title>
      <author><first>Koel</first><last>Dutta Chowdhury</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>6056–6062</pages>
      <abstract>Recent studies use a combination of lexical and syntactic features to show that footprints of the source language remain visible in translations, to the extent that it is possible to predict the original source language from the translation. In this paper, we focus on embedding-based semantic spaces, exploiting departures from isomorphism between spaces built from original target language and translations into this target language to predict relations between languages in an unsupervised way. We use different views of the data   words, <a href="https://en.wikipedia.org/wiki/Part_of_speech">parts of speech</a>, semantic tags and synsets   to track translationese. Our analysis shows that (i) semantic distances between original target language and translations into this target language can be detected using the notion of <a href="https://en.wikipedia.org/wiki/Isomorphism">isomorphism</a>, (ii) language family ties with characteristics similar to linguistically motivated phylogenetic trees can be inferred from the distances and (iii) with delexicalised embeddings exhibiting source-language interference most significantly, other levels of abstraction display the same tendency, indicating the lexicalised results to be not just due to possible topic differences between original and translated texts. To the best of our knowledge, this is the first time departures from isomorphism between embedding spaces are used to track <a href="https://en.wikipedia.org/wiki/Translation_(geometry)">translationese</a>.</abstract>
      <url hash="6c65d093">2020.coling-main.532</url>
      <doi>10.18653/v1/2020.coling-main.532</doi>
      <bibkey>dutta-chowdhury-etal-2020-understanding</bibkey>
    </paper>
    <paper id="533">
      <title>Building The First English-Brazilian Portuguese Corpus for Automatic Post-Editing<fixed-case>E</fixed-case>nglish-<fixed-case>B</fixed-case>razilian <fixed-case>P</fixed-case>ortuguese Corpus for Automatic Post-Editing</title>
      <author><first>Felipe</first><last>Almeida Costa</last></author>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Adriana</first><last>Pagano</last></author>
      <author><first>Wagner</first><last>Meira</last></author>
      <pages>6063–6069</pages>
      <abstract>This paper introduces the first <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> for Automatic Post-Editing of English and a low-resource language, Brazilian Portuguese. The source English texts were extracted from the WebNLG corpus and automatically translated into <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese</a> using a state-of-the-art industrial neural machine translator. Post-edits were then obtained in an experiment with native speakers of <a href="https://en.wikipedia.org/wiki/Brazilian_Portuguese">Brazilian Portuguese</a>. To assess the quality of the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>, we performed error analysis and computed complexity indicators measuring how difficult the APE task would be. We report preliminary results of Phrase-Based and Neural Machine Translation Models on this new <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>. Data and code publicly available in our repository.</abstract>
      <url hash="dc04c14a">2020.coling-main.533</url>
      <doi>10.18653/v1/2020.coling-main.533</doi>
      <bibkey>almeida-costa-etal-2020-building</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="534">
      <title>Analysing cross-lingual transfer in <a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatisation</a> for <a href="https://en.wikipedia.org/wiki/Languages_of_India">Indian languages</a><fixed-case>I</fixed-case>ndian languages</title>
      <author><first>Kumar</first><last>Saurav</last></author>
      <author><first>Kumar</first><last>Saunack</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>6070–6076</pages>
      <abstract>Lemmatization aims to reduce the sparse data problem by relating the <a href="https://en.wikipedia.org/wiki/Inflection">inflected forms</a> of a word to its <a href="https://en.wikipedia.org/wiki/Dictionary">dictionary form</a>. However, most of the prior work on this topic has focused on high resource languages. In this paper, we evaluate cross-lingual approaches for low resource languages, especially in the context of morphologically rich Indian languages. We test our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> on six languages from two different families and develop linguistic insights into each <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>’s performance.</abstract>
      <url hash="3b55cd40">2020.coling-main.534</url>
      <doi>10.18653/v1/2020.coling-main.534</doi>
      <bibkey>saurav-etal-2020-analysing</bibkey>
    </paper>
    <paper id="535">
      <title>Neural Automated Essay Scoring Incorporating Handcrafted Features</title>
      <author><first>Masaki</first><last>Uto</last></author>
      <author><first>Yikuan</first><last>Xie</last></author>
      <author><first>Maomi</first><last>Ueno</last></author>
      <pages>6077–6088</pages>
      <abstract>Automated essay scoring (AES) is the task of automatically assigning scores to essays as an alternative to <a href="https://en.wikipedia.org/wiki/Grading_in_education">grading</a> by human raters. Conventional <a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard">AES</a> typically relies on handcrafted features, whereas recent studies have proposed <a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard">AES models</a> based on deep neural networks (DNNs) to obviate the need for <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a>. Furthermore, hybrid methods that integrate handcrafted features in a DNN-AES model have been recently developed and have achieved state-of-the-art accuracy. One of the most popular hybrid methods is formulated as a DNN-AES model with an additional <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network (RNN)</a> that processes a sequence of handcrafted sentence-level features. However, this method has the following problems : 1) It can not incorporate effective essay-level features developed in previous <a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard">AES</a> research. 2) It greatly increases the numbers of <a href="https://en.wikipedia.org/wiki/Statistical_model">model parameters</a> and tuning parameters, increasing the difficulty of model training. 3) It has an additional <a href="https://en.wikipedia.org/wiki/Random-access_memory">RNN</a> to process sentence-level features, enabling extension to various DNN-AES models complex. To resolve these problems, we propose a new hybrid method that integrates handcrafted essay-level features into a DNN-AES model. Specifically, our method concatenates handcrafted essay-level features to a distributed essay representation vector, which is obtained from an intermediate layer of a DNN-AES model. Our method is a simple DNN-AES extension, but significantly improves scoring accuracy.</abstract>
      <url hash="3f7916ab">2020.coling-main.535</url>
      <doi>10.18653/v1/2020.coling-main.535</doi>
      <bibkey>uto-etal-2020-neural</bibkey>
    </paper>
    <paper id="536">
      <title>A Straightforward Approach to Narratologically Grounded Character Identification</title>
      <author><first>Labiba</first><last>Jahan</last></author>
      <author><first>Rahul</first><last>Mittal</last></author>
      <author><first>W. Victor</first><last>Yarlott</last></author>
      <author><first>Mark</first><last>Finlayson</last></author>
      <pages>6089–6100</pages>
      <abstract>One of the most fundamental elements of <a href="https://en.wikipedia.org/wiki/Narrative">narrative</a> is <a href="https://en.wikipedia.org/wiki/Character_(arts)">character</a> : if we are to understand a narrative, we must be able to identify the characters of that narrative. Therefore, character identification is a critical task in narrative natural language understanding. Most prior work has lacked a narratologically grounded definition of character, instead relying on simplified or implicit definitions that do not capture essential distinctions between <a href="https://en.wikipedia.org/wiki/Character_(arts)">characters</a> and other referents in narratives. In prior work we proposed a preliminary definition of <a href="https://en.wikipedia.org/wiki/Character_(arts)">character</a> that was based in clear narratological principles : a <a href="https://en.wikipedia.org/wiki/Character_(arts)">character</a> is an animate entity that is important to the plot. Here we flesh out this concept, demonstrate that it can be reliably annotated (0.78 Cohen’s), and provide annotations of 170 narrative texts, drawn from 3 different corpora, containing 1,347 <a href="https://en.wikipedia.org/wiki/Co-reference">character co-reference chains</a> and 21,999 <a href="https://en.wikipedia.org/wiki/Co-reference">non-character chains</a> that include 3,937 <a href="https://en.wikipedia.org/wiki/Co-reference">animate chains</a>. Furthermore, we have shown that a <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised classifier</a> using a simple set of easily computable <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> can effectively identify these <a href="https://en.wikipedia.org/wiki/Character_(computing)">characters</a> (overall <a href="https://en.wikipedia.org/wiki/F-number">F1</a> of 0.90). A detailed error analysis shows that character identification is first and foremost affected by co-reference quality, and further, that the shorter a chain is the harder it is to effectively identify as a character. We release our code and data for the benefit of other researchers</abstract>
      <url hash="d7c76355">2020.coling-main.536</url>
      <doi>10.18653/v1/2020.coling-main.536</doi>
      <bibkey>jahan-etal-2020-straightforward</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="537">
      <title>Fine-grained Information Status Classification Using Discourse Context-Aware BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Yufang</first><last>Hou</last></author>
      <pages>6101–6112</pages>
      <abstract>Previous work on bridging anaphora recognition (Hou et al., 2013) casts the problem as a subtask of learning fine-grained information status (IS). However, these <a href="https://en.wikipedia.org/wiki/Linguistic_system">systems</a> heavily depend on many <a href="https://en.wikipedia.org/wiki/Linguistic_prescription">hand-crafted linguistic features</a>. In this paper, we propose a simple discourse context-aware BERT model for fine-grained IS classification. On the ISNotes corpus (Markert et al., 2012), our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves new state-of-the-art performances on fine-grained IS classification, obtaining a 4.8 absolute overall accuracy improvement compared to Hou et al. More importantly, we also show an improvement of 10.5 <a href="https://en.wikipedia.org/wiki/F-number">F1 points</a> for bridging anaphora recognition without using any complex hand-crafted semantic features designed for capturing the bridging phenomenon. We further analyze the trained model and find that the most attended signals for each IS category correspond well to linguistic notions of information status.</abstract>
      <url hash="d3a4e91b">2020.coling-main.537</url>
      <doi>10.18653/v1/2020.coling-main.537</doi>
      <bibkey>hou-2020-fine</bibkey>
      <pwccode url="https://github.com/IBM/bridging-resolution" additional="false">IBM/bridging-resolution</pwccode>
    </paper>
    <paper id="542">
      <title>Text Classification by Contrastive Learning and Cross-lingual Data Augmentation for Alzheimer’s Disease Detection<fixed-case>A</fixed-case>lzheimer’s Disease Detection</title>
      <author><first>Zhiqiang</first><last>Guo</last></author>
      <author><first>Zhaoci</first><last>Liu</last></author>
      <author><first>Zhenhua</first><last>Ling</last></author>
      <author><first>Shijin</first><last>Wang</last></author>
      <author><first>Lingjing</first><last>Jin</last></author>
      <author><first>Yunxia</first><last>Li</last></author>
      <pages>6161–6171</pages>
      <abstract>Data scarcity is always a constraint on analyzing speech transcriptions for automatic Alzheimer’s disease (AD) detection, especially when the subjects are non-English speakers. To deal with this issue, this paper first proposes a contrastive learning method to obtain effective representations for text classification based on monolingual embeddings of BERT. Furthermore, a cross-lingual data augmentation method is designed by building autoencoders to learn the text representations shared by both languages. Experiments on a Mandarin AD corpus show that the contrastive learning method can achieve better detection accuracy than conventional CNN-based and BERTbased methods. Our cross-lingual data augmentation method also outperforms other compared methods when using another English AD corpus for augmentation. Finally, a best <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">detection accuracy</a> of 81.6 % is obtained by our proposed <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> on the Mandarin AD corpus.</abstract>
      <url hash="ad73a32b">2020.coling-main.542</url>
      <doi>10.18653/v1/2020.coling-main.542</doi>
      <bibkey>guo-etal-2020-text</bibkey>
    </paper>
    <paper id="549">
      <title>Hierarchical Text Segmentation for Medieval Manuscripts</title>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Beatrice</first><last>Daille</last></author>
      <author><first>Dominique</first><last>Stutzmann</last></author>
      <author><first>Christopher</first><last>Kermorvant</last></author>
      <author><first>Louis</first><last>Chevalier</last></author>
      <pages>6240–6251</pages>
      <abstract>In this paper, we address the segmentation of books of hours, Latin devotional manuscripts of the late Middle Ages, that exhibit challenging issues : a complex hierarchical entangled structure, variable content, noisy transcriptions with no sentence markers, and strong correlations between sections for which topical information is no longer sufficient to draw segmentation boundaries. We show that the main state-of-the-art segmentation methods are either inefficient or inapplicable for <a href="https://en.wikipedia.org/wiki/Books_of_hours">books of hours</a> and propose a bottom-up greedy approach that considerably enhances the segmentation results. We stress the importance of such hierarchical segmentation of books of hours for historians to explore their overarching differences underlying conception about Church.</abstract>
      <url hash="0d1313a4">2020.coling-main.549</url>
      <doi>10.18653/v1/2020.coling-main.549</doi>
      <bibkey>hazem-etal-2020-hierarchical</bibkey>
      <pwccode url="https://github.com/hazemamir/greedy_text_segmentation" additional="false">hazemamir/greedy_text_segmentation</pwccode>
    </paper>
    <paper id="550">
      <title>Are We Ready for this Disaster? Towards Location Mention Recognition from Crisis Tweets</title>
      <author><first>Reem</first><last>Suwaileh</last></author>
      <author><first>Muhammad</first><last>Imran</last></author>
      <author><first>Tamer</first><last>Elsayed</last></author>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <pages>6252–6263</pages>
      <abstract>The widespread usage of <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> during emergencies has provided a new opportunity and timely resource to crisis responders for various disaster management tasks. Geolocation information of pertinent tweets is crucial for gaining <a href="https://en.wikipedia.org/wiki/Situation_awareness">situational awareness</a> and delivering aid. However, the majority of tweets do not come with <a href="https://en.wikipedia.org/wiki/Geographic_data_and_information">geoinformation</a>. In this work, we focus on the task of location mention recognition from crisis-related tweets. Specifically, we investigate the influence of different types of <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">labeled training data</a> on the performance of a BERT-based classification model. We explore several <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training settings</a> such as combing in- and out-domain data from <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> and <a href="https://en.wikipedia.org/wiki/Twitter">general-purpose and crisis-related tweets</a>. Furthermore, we investigate the effect of geospatial proximity while training on near or far-away events from the target event. Using five different <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, our extensive experiments provide answers to several critical research questions that are useful for the research community to foster research in this important direction. For example, results show that, for training a location mention recognition model, Twitter-based data is preferred over general-purpose data ; and crisis-related data is preferred over general-purpose Twitter data. Furthermore, training on <a href="https://en.wikipedia.org/wiki/Data">data</a> from geographically-nearby disaster events to the target event boosts the performance compared to training on distant events.</abstract>
      <url hash="c9601ff6">2020.coling-main.550</url>
      <doi>10.18653/v1/2020.coling-main.550</doi>
      <bibkey>suwaileh-etal-2020-ready</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="562">
      <title>Regularized Attentive Capsule Network for Overlapped Relation Extraction</title>
      <author><first>Tianyi</first><last>Liu</last></author>
      <author><first>Xiangyu</first><last>Lin</last></author>
      <author><first>Weijia</first><last>Jia</last></author>
      <author><first>Mingliang</first><last>Zhou</last></author>
      <author><first>Wei</first><last>Zhao</last></author>
      <pages>6388–6398</pages>
      <abstract>Distantly supervised relation extraction has been widely applied in knowledge base construction due to its less requirement of human efforts. However, the automatically established training datasets in distant supervision contain low-quality instances with noisy words and overlapped relations, introducing great challenges to the accurate extraction of relations. To address this problem, we propose a novel Regularized Attentive Capsule Network (RA-CapNet) to better identify highly overlapped relations in each informal sentence. To discover multiple relation features in an instance, we embed multi-head attention into the capsule network as the low-level capsules, where the subtraction of two entities acts as a new form of relation query to select salient features regardless of their positions. To further discriminate overlapped relation features, we devise disagreement regularization to explicitly encourage the diversity among both multiple attention heads and low-level capsules. Extensive experiments conducted on widely used datasets show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves significant improvements in <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a>.</abstract>
      <url hash="ed9983f1">2020.coling-main.562</url>
      <doi>10.18653/v1/2020.coling-main.562</doi>
      <bibkey>liu-etal-2020-regularized</bibkey>
    </paper>
    <paper id="565">
      <title>Graph Convolution over Multiple Dependency Sub-graphs for <a href="https://en.wikipedia.org/wiki/Relation_extraction">Relation Extraction</a></title>
      <author><first>Angrosh</first><last>Mandya</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <author><first>Frans</first><last>Coenen</last></author>
      <pages>6424–6435</pages>
      <abstract>We propose a contextualised graph convolution network over multiple dependency-based sub-graphs for <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a>. A novel method to construct multiple sub-graphs using words in shortest dependency path and words linked to entities in the dependency parse is proposed. Graph convolution operation is performed over the resulting multiple sub-graphs to obtain more informative features useful for <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a>. Our experimental results show that the proposed method achieves superior performance over the existing GCN-based models achieving state-of-the-art performance on cross-sentence n-ary relation extraction dataset and SemEval 2010 Task 8 sentence-level relation extraction dataset. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> also achieves a comparable performance to the SoTA on the TACRED dataset.</abstract>
      <url hash="ddc3de6d">2020.coling-main.565</url>
      <doi>10.18653/v1/2020.coling-main.565</doi>
      <bibkey>mandya-etal-2020-graph</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task 8</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="572">
      <title>NYTWIT : A Dataset of Novel Words in the New York Times<fixed-case>NYTWIT</fixed-case>: A Dataset of Novel Words in the <fixed-case>N</fixed-case>ew <fixed-case>Y</fixed-case>ork <fixed-case>T</fixed-case>imes</title>
      <author><first>Yuval</first><last>Pinter</last></author>
      <author><first>Cassandra L.</first><last>Jacobs</last></author>
      <author><first>Max</first><last>Bittker</last></author>
      <pages>6509–6515</pages>
      <abstract>We present the <a href="https://en.wikipedia.org/wiki/The_New_York_Times">New York Times Word Innovation Types dataset</a>, or NYTWIT, a collection of over 2,500 novel English words published in the <a href="https://en.wikipedia.org/wiki/The_New_York_Times">New York Times</a> between November 2017 and March 2019, manually annotated for their class of novelty (such as lexical derivation, dialectal variation, blending, or compounding). We present baseline results for both uncontextual and contextual prediction of novelty class, showing that there is room for improvement even for state-of-the-art NLP systems. We hope this resource will prove useful for linguists and NLP practitioners by providing a real-world environment of novel word appearance.</abstract>
      <url hash="6656790e">2020.coling-main.572</url>
      <doi>10.18653/v1/2020.coling-main.572</doi>
      <bibkey>pinter-etal-2020-nytwit</bibkey>
      <pwccode url="https://github.com/yuvalpinter/nytwit" additional="false">yuvalpinter/nytwit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nytwit">NYTWIT</pwcdataset>
    </paper>
    <paper id="575">
      <title>XED : A Multilingual Dataset for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> and Emotion Detection<fixed-case>XED</fixed-case>: A Multilingual Dataset for Sentiment Analysis and Emotion Detection</title>
      <author><first>Emily</first><last>Öhman</last></author>
      <author><first>Marc</first><last>Pàmies</last></author>
      <author><first>Kaisla</first><last>Kajava</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>6542–6552</pages>
      <abstract>We introduce XED, a multilingual fine-grained emotion dataset. The dataset consists of human-annotated Finnish (25k) and English sentences (30k), as well as projected annotations for 30 additional languages, providing new resources for many low-resource languages. We use Plutchik’s core emotions to annotate the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> with the addition of neutral to create a multilabel multiclass dataset. The dataset is carefully evaluated using language-specific BERT models and SVMs to show that XED performs on par with other similar datasets and is therefore a useful tool for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> and <a href="https://en.wikipedia.org/wiki/Emotion_detection">emotion detection</a>.</abstract>
      <url hash="7530b805">2020.coling-main.575</url>
      <doi>10.18653/v1/2020.coling-main.575</doi>
      <bibkey>ohman-etal-2020-xed</bibkey>
      <pwccode url="https://github.com/Helsinki-NLP/XED" additional="false">Helsinki-NLP/XED</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xed">XED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/goemotions">GoEmotions</pwcdataset>
    </paper>
    <paper id="576">
      <title>Human or Neural Translation?</title>
      <author><first>Shivendra</first><last>Bhardwaj</last></author>
      <author><first>David</first><last>Alfonso Hermelo</last></author>
      <author><first>Phillippe</first><last>Langlais</last></author>
      <author><first>Gabriel</first><last>Bernier-Colborne</last></author>
      <author><first>Cyril</first><last>Goutte</last></author>
      <author><first>Michel</first><last>Simard</last></author>
      <pages>6553–6564</pages>
      <abstract>Deep neural models tremendously improved <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. In this context, we investigate whether distinguishing machine from human translations is still feasible. We trained and applied 18 classifiers under two settings : a monolingual task, in which the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> only looks at the translation ; and a bilingual task, in which the source text is also taken into consideration. We report on extensive experiments involving 4 neural MT systems (Google Translate, DeepL, as well as two systems we trained) and varying the domain of texts. We show that the bilingual task is the easiest one and that transfer-based deep-learning classifiers perform best, with mean accuracies around 85 % in-domain and 75 % out-of-domain.</abstract>
      <url hash="25e5fe35">2020.coling-main.576</url>
      <doi>10.18653/v1/2020.coling-main.576</doi>
      <bibkey>bhardwaj-etal-2020-human</bibkey>
    </paper>
    <paper id="580">
      <title>Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps<fixed-case>QA</fixed-case> Dataset for Comprehensive Evaluation of Reasoning Steps</title>
      <author><first>Xanh</first><last>Ho</last></author>
      <author><first>Anh-Khoa</first><last>Duong Nguyen</last></author>
      <author><first>Saku</first><last>Sugawara</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>6609–6625</pages>
      <abstract>A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to read multiple paragraphs to answer a given question. However, current <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> do not provide a complete explanation for the <a href="https://en.wikipedia.org/wiki/Reason">reasoning process</a> from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, we introduce the <a href="https://en.wikipedia.org/wiki/Evidence-based_practice">evidence information</a> containing a reasoning path for multi-hop questions. The evidence information has two benefits : (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. We carefully design a <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a> and a set of <a href="https://en.wikipedia.org/wiki/Template_(word_processing)">templates</a> when generating a question-answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a> and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is challenging for multi-hop models and it ensures that multi-hop reasoning is required.</abstract>
      <url hash="9d4f8121">2020.coling-main.580</url>
      <doi>10.18653/v1/2020.coling-main.580</doi>
      <bibkey>ho-etal-2020-constructing</bibkey>
      <pwccode url="https://github.com/Alab-NII/2wikimultihop" additional="false">Alab-NII/2wikimultihop</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/2wikimultihopqa">2WikiMultiHopQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/complexwebquestions">ComplexWebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
    <paper id="582">
      <title>Exploring the Language of Data</title>
      <author><first>Gábor</first><last>Bella</last></author>
      <author><first>Linda</first><last>Gremes</last></author>
      <author><first>Fausto</first><last>Giunchiglia</last></author>
      <pages>6638–6648</pages>
      <abstract>We set out to uncover the unique grammatical properties of an important yet so far under-researched type of natural language text : that of short labels typically found within structured datasets. We show that such labels obey a specific type of abbreviated grammar that we call the Language of Data, with properties significantly different from the kinds of text typically addressed in <a href="https://en.wikipedia.org/wiki/Computational_linguistics">computational linguistics</a> and <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, such as ‘standard’ written language or social media messages. We analyse <a href="https://en.wikipedia.org/wiki/Orthography">orthography</a>, <a href="https://en.wikipedia.org/wiki/Part_of_speech">parts of speech</a>, and <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> over a large, bilingual, hand-annotated corpus of data labels collected from a variety of domains. We perform experiments on <a href="https://en.wikipedia.org/wiki/Lexical_analysis">tokenisation</a>, <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a>, and <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> over real-world structured data, demonstrating that models adapted to the Language of Data outperform those trained on standard text. These observations point in a new direction to be explored as future research, in order to develop new NLP tools and <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> dedicated to the Language of Data.</abstract>
      <url hash="959343fe">2020.coling-main.582</url>
      <doi>10.18653/v1/2020.coling-main.582</doi>
      <bibkey>bella-etal-2020-exploring</bibkey>
    </paper>
    <paper id="587">
      <title>Creation of Corpus and analysis in Code-Mixed Kannada-English Twitter data for Emotion Prediction<fixed-case>K</fixed-case>annada-<fixed-case>E</fixed-case>nglish <fixed-case>T</fixed-case>witter data for Emotion Prediction</title>
      <author><first>Abhinav Reddy</first><last>Appidi</last></author>
      <author><first>Vamshi Krishna</first><last>Srirangam</last></author>
      <author><first>Darsi</first><last>Suhas</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>6703–6709</pages>
      <abstract>Emotion prediction is a critical task in the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing (NLP)</a>. There has been a significant amount of work done in emotion prediction for resource-rich languages. There has been work done on code-mixed social media corpus but not on emotion prediction of Kannada-English code-mixed Twitter data. In this paper, we analyze the problem of emotion prediction on <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> obtained from code-mixed Kannada-English extracted from <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> annotated with their respective ‘Emotion’ for each tweet. We experimented with machine learning prediction models using features like Character N-Grams, Word N-Grams, Repetitive characters, and others on SVM and LSTM on our corpus, which resulted in an accuracy of 30 % and 32 % respectively.</abstract>
      <url hash="8aeb7965">2020.coling-main.587</url>
      <doi>10.18653/v1/2020.coling-main.587</doi>
      <bibkey>appidi-etal-2020-creation</bibkey>
    </paper>
    <paper id="588">
      <title>Fair Evaluation in Concept Normalization : a Large-scale Comparative Analysis for BERT-based Models<fixed-case>BERT</fixed-case>-based Models</title>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <author><first>Artur</first><last>Kadurin</last></author>
      <author><first>Zulfat</first><last>Miftahutdinov</last></author>
      <pages>6710–6716</pages>
      <abstract>Linking of biomedical entity mentions to various terminologies of chemicals, <a href="https://en.wikipedia.org/wiki/Disease">diseases</a>, <a href="https://en.wikipedia.org/wiki/Gene">genes</a>, adverse drug reactions is a challenging task, often requiring non-syntactic interpretation. A large number of biomedical corpora and state-of-the-art models have been introduced in the past five years. However, there are no general guidelines regarding the evaluation of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on these <a href="https://en.wikipedia.org/wiki/Corpus_linguistics">corpora</a> in single- and cross-terminology settings. In this work, we perform a comparative evaluation of various benchmarks and study the efficiency of state-of-the-art neural architectures based on Bidirectional Encoder Representations from Transformers (BERT) for linking of three entity types across three domains : research abstracts, drug labels, and user-generated texts on drug therapy in English. We have made the source code and results available at https://github.com/insilicomedicine/Fair-Evaluation-BERT.</abstract>
      <url hash="44d0ebf1">2020.coling-main.588</url>
      <doi>10.18653/v1/2020.coling-main.588</doi>
      <bibkey>tutubalina-etal-2020-fair</bibkey>
      <pwccode url="https://github.com/insilicomedicine/Fair-Evaluation-BERT" additional="false">insilicomedicine/Fair-Evaluation-BERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
    </paper>
    <paper id="591">
      <title>Multilingual Neural RST Discourse Parsing<fixed-case>RST</fixed-case> Discourse Parsing</title>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Ke</first><last>Shi</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>6730–6738</pages>
      <abstract>Text discourse parsing plays an important role in understanding <a href="https://en.wikipedia.org/wiki/Information_flow">information flow</a> and argumentative structure in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>. Previous research under the Rhetorical Structure Theory (RST) has mostly focused on inducing and evaluating models from the English treebank. However, the parsing tasks for other languages such as <a href="https://en.wikipedia.org/wiki/German_language">German</a>, <a href="https://en.wikipedia.org/wiki/Dutch_language">Dutch</a>, and <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese</a> are still challenging due to the shortage of annotated data. In this work, we investigate two approaches to establish a neural, cross-lingual discourse parser via : (1) utilizing multilingual vector representations ; and (2) adopting segment-level translation of the source content. Experiment results show that both methods are effective even with limited training data, and achieve state-of-the-art performance on cross-lingual, document-level discourse parsing on all sub-tasks.</abstract>
      <url hash="a1cc5e02">2020.coling-main.591</url>
      <doi>10.18653/v1/2020.coling-main.591</doi>
      <bibkey>liu-etal-2020-multilingual-neural</bibkey>
      <pwccode url="https://github.com/seq-to-mind/DMRST_Parser" additional="false">seq-to-mind/DMRST_Parser</pwccode>
    </paper>
    <paper id="593">
      <title>Tree Representations in Transition System for RST Parsing<fixed-case>RST</fixed-case> Parsing</title>
      <author><first>Jinfen</first><last>Li</last></author>
      <author><first>Lu</first><last>Xiao</last></author>
      <pages>6746–6751</pages>
      <abstract>The transition-based systems in the past studies propose a series of actions, to build a right-heavy binarized tree for the RST parsing. However, the nodes of the binary-nuclear relations (e.g., Contrast) have the same nuclear type with those of the multi-nuclear relations (e.g., Joint) in the <a href="https://en.wikipedia.org/wiki/Binary_tree">binary tree structure</a>. In addition, the reduce action only construct <a href="https://en.wikipedia.org/wiki/Binary_tree">binary trees</a> instead of multi-branch trees, which is the original RST tree structure. In our paper, we design a new nuclear type for the multi-nuclear relations, and a new action to construct a multi-branch tree. We enrich the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">feature set</a> by extracting additional refined dependency feature of texts from the Bi-Affine model. We also compare the performance of two approaches for RST parsing in the transition-based system : a joint action of reduce-shift and nuclear type (i.e., Reduce-SN) vs a separate one that applies Reduce action first and then assigns nuclear type. We find that the new devised nuclear type and <a href="https://en.wikipedia.org/wiki/Action_(philosophy)">action</a> are more capable of capturing the multi-nuclear relation and the joint action is more suitable than the separate one. Our multi-branch tree structure obtains the state-of-the-art performance for all the 18 coarse relations.</abstract>
      <url hash="29f67d78">2020.coling-main.593</url>
      <doi>10.18653/v1/2020.coling-main.593</doi>
      <bibkey>li-xiao-2020-tree</bibkey>
    </paper>
    <paper id="597">
      <title>Resource Constrained Dialog Policy Learning Via Differentiable Inductive Logic Programming</title>
      <author><first>Zhenpeng</first><last>Zhou</last></author>
      <author><first>Ahmad</first><last>Beirami</last></author>
      <author><first>Paul</first><last>Crook</last></author>
      <author><first>Pararth</first><last>Shah</last></author>
      <author><first>Rajen</first><last>Subba</last></author>
      <author><first>Alborz</first><last>Geramifard</last></author>
      <pages>6775–6787</pages>
      <abstract>Motivated by the needs of resource constrained dialog policy learning, we introduce dialog policy via differentiable inductive logic (DILOG). We explore the tasks of one-shot learning and zero-shot domain transfer with DILOG on SimDial and MultiWoZ. Using a single representative dialog from the restaurant domain, we train DILOG on the SimDial dataset and obtain 99+% in-domain test accuracy. We also show that the trained DILOG zero-shot transfers to all other domains with 99+% accuracy, proving the suitability of DILOG to slot-filling dialogs. We further extend our study to the MultiWoZ dataset achieving 90+% inform and success metrics. We also observe that these <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> are not capturing some of the shortcomings of DILOG in terms of <a href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">false positives</a>, prompting us to measure an auxiliary Action F1 score. We show that DILOG is 100x more data efficient than state-of-the-art neural approaches on MultiWoZ while achieving similar performance metrics. We conclude with a discussion on the strengths and weaknesses of DILOG.</abstract>
      <url hash="cfa7ea08">2020.coling-main.597</url>
      <doi>10.18653/v1/2020.coling-main.597</doi>
      <bibkey>zhou-etal-2020-resource</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="598">
      <title>German’s Next Language Model<fixed-case>G</fixed-case>erman’s Next Language Model</title>
      <author><first>Branden</first><last>Chan</last></author>
      <author><first>Stefan</first><last>Schweter</last></author>
      <author><first>Timo</first><last>Möller</last></author>
      <pages>6788–6796</pages>
      <abstract>In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of <a href="https://en.wikipedia.org/wiki/Document_classification">document classification</a> and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> and our results indicate that both adding more data and utilizing WWM improve <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance. By benchmarking against existing German models, we show that these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> are the best German models to date. All trained <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> will be made publicly available to the research community.</abstract>
      <url hash="355d274d">2020.coling-main.598</url>
      <doi>10.18653/v1/2020.coling-main.598</doi>
      <bibkey>chan-etal-2020-germans</bibkey>
      <pwccode url="https://github.com/dbmdz/berts" additional="true">dbmdz/berts</pwccode>
    </paper>
    <paper id="602">
      <title>Do n’t Invite BERT to Drink a Bottle : Modeling the Interpretation of Metonymies Using BERT and Distributional Representations<fixed-case>BERT</fixed-case> to Drink a Bottle: Modeling the Interpretation of Metonymies Using <fixed-case>BERT</fixed-case> and Distributional Representations</title>
      <author><first>Paolo</first><last>Pedinotti</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <pages>6831–6837</pages>
      <abstract>In this work, we carry out two experiments in order to assess the ability of BERT to capture the meaning shift associated with <a href="https://en.wikipedia.org/wiki/Metonymy">metonymic expressions</a>. We test the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> that is representative of the most common types of <a href="https://en.wikipedia.org/wiki/Metonymy">metonymy</a>. We compare BERT with the Structured Distributional Model (SDM), a model for the representation of words in context which is based on the notion of Generalized Event Knowledge. The results reveal that, while BERT ability to deal with <a href="https://en.wikipedia.org/wiki/Metonymy">metonymy</a> is quite limited, SDM is good at predicting the meaning of metonymic expressions, providing support for an account of <a href="https://en.wikipedia.org/wiki/Metonymy">metonymy</a> based on event knowledge.</abstract>
      <url hash="51018f51">2020.coling-main.602</url>
      <doi>10.18653/v1/2020.coling-main.602</doi>
      <bibkey>pedinotti-lenci-2020-dont</bibkey>
    </paper>
    <paper id="606">
      <title>Interpretable Multi-headed Attention for Abstractive Summarization at Controllable Lengths</title>
      <author><first>Ritesh</first><last>Sarkhel</last></author>
      <author><first>Moniba</first><last>Keymanesh</last></author>
      <author><first>Arnab</first><last>Nandi</last></author>
      <author><first>Srinivasan</first><last>Parthasarathy</last></author>
      <pages>6871–6882</pages>
      <abstract>Abstractive summarization at controllable lengths is a challenging task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. It is even more challenging for domains where limited training data is available or scenarios in which the length of the summary is not known beforehand. At the same time, when it comes to trusting machine-generated summaries, explaining how a summary was constructed in human-understandable terms may be critical. We propose Multi-level Summarizer (MLS), a supervised method to construct abstractive summaries of a text document at controllable lengths. The key enabler of our method is an interpretable multi-headed attention mechanism that computes attention distribution over an input document using an array of timestep independent semantic kernels. Each kernel optimizes a human-interpretable syntactic or semantic property. Exhaustive experiments on two low-resource datasets in English show that MLS outperforms strong baselines by up to 14.70 % in the METEOR score. Human evaluation of the summaries also suggests that they capture the key concepts of the document at various length-budgets.</abstract>
      <url hash="c4338c21">2020.coling-main.606</url>
      <doi>10.18653/v1/2020.coling-main.606</doi>
      <bibkey>sarkhel-etal-2020-interpretable</bibkey>
    </paper>
    <paper id="609">
      <title>CharacterBERT : Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters<fixed-case>C</fixed-case>haracter<fixed-case>BERT</fixed-case>: Reconciling <fixed-case>ELM</fixed-case>o and <fixed-case>BERT</fixed-case> for Word-Level Open-Vocabulary Representations From Characters</title>
      <author><first>Hicham</first><last>El Boukkouri</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <author><first>Hiroshi</first><last>Noji</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <author><first>Jun’ichi</first><last>Tsujii</last></author>
      <pages>6903–6915</pages>
      <abstract>Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system despite it not being intrinsically linked to the notion of <a href="https://en.wikipedia.org/wiki/Transformers_(toy_line)">Transformers</a>. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the <a href="https://en.wikipedia.org/wiki/Domain_(software_engineering)">general domain</a> is not always suitable, especially when building <a href="https://en.wikipedia.org/wiki/Conceptual_model_(computer_science)">models</a> for <a href="https://en.wikipedia.org/wiki/Domain_(software_engineering)">specialized domains</a> (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations.</abstract>
      <url hash="9b77793f">2020.coling-main.609</url>
      <doi>10.18653/v1/2020.coling-main.609</doi>
      <bibkey>el-boukkouri-etal-2020-characterbert</bibkey>
      <pwccode url="https://github.com/helboukkouri/character-bert" additional="true">helboukkouri/character-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/2010-i2b2-va">2010 i2b2/VA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/chemprot">ChemProt</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ddi">DDI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mednli">MedNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openwebtext">OpenWebText</pwcdataset>
    </paper>
    <paper id="610">
      <title>Autoregressive Reasoning over Chains of Facts with Transformers</title>
      <author><first>Ruben</first><last>Cartuyvels</last></author>
      <author><first>Graham</first><last>Spinks</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>6916–6930</pages>
      <abstract>This paper proposes an iterative inference algorithm for multi-hop explanation regeneration, that retrieves relevant factual evidence in the form of text snippets, given a natural language question and its answer. Combining multiple sources of evidence or facts for multi-hop reasoning becomes increasingly hard when the number of sources needed to make an <a href="https://en.wikipedia.org/wiki/Inference">inference</a> grows. Our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> copes with this by decomposing the selection of facts from a corpus autoregressively, conditioning the next iteration on previously selected facts. This allows us to use a pairwise learning-to-rank loss. We validate our method on <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> of the TextGraphs 2019 and 2020 Shared Tasks for explanation regeneration. Existing work on this task either evaluates facts in isolation or artificially limits the possible chains of facts, thus limiting multi-hop inference. We demonstrate that our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a>, when used with a pre-trained transformer model, outperforms the previous <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> in terms of <a href="https://en.wikipedia.org/wiki/Precision_(computer_science)">precision</a>, <a href="https://en.wikipedia.org/wiki/Time_complexity">training time</a> and inference efficiency.</abstract>
      <url hash="c97d241b">2020.coling-main.610</url>
      <doi>10.18653/v1/2020.coling-main.610</doi>
      <bibkey>cartuyvels-etal-2020-autoregressive</bibkey>
      <pwccode url="https://github.com/rubencart/LIIR-TextGraphs-14" additional="false">rubencart/LIIR-TextGraphs-14</pwccode>
    </paper>
    <paper id="611">
      <title>Augmenting NLP models using Latent Feature Interpolations<fixed-case>NLP</fixed-case> models using Latent Feature Interpolations</title>
      <author><first>Amit</first><last>Jindal</last></author>
      <author><first>Arijit</first><last>Ghosh Chowdhury</last></author>
      <author><first>Aniket</first><last>Didolkar</last></author>
      <author><first>Di</first><last>Jin</last></author>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <pages>6931–6936</pages>
      <abstract>Models with a large number of parameters are prone to <a href="https://en.wikipedia.org/wiki/Overfitting">over-fitting</a> and often fail to capture the underlying input distribution. We introduce Emix, a data augmentation method that uses interpolations of word embeddings and hidden layer representations to construct virtual examples. We show that Emix shows significant improvements over previously used <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">interpolation based regularizers</a> and <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation techniques</a>. We also demonstrate how our proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is more robust to sparsification. We highlight the merits of our proposed <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> by performing thorough quantitative and qualitative assessments.</abstract>
      <url hash="5f0a0218">2020.coling-main.611</url>
      <doi>10.18653/v1/2020.coling-main.611</doi>
      <bibkey>jindal-etal-2020-augmenting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    </volume>
  <volume id="demos" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations</booktitle>
      <editor><first>Michal</first><last>Ptaszynski</last></editor>
      <editor><first>Bartosz</first><last>Ziolko</last></editor>
      <publisher>International Committee on Computational Linguistics (ICCL)</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="40ccfbdf">2020.coling-demos.0</url>
      <bibkey>coling-2020-international-linguistics</bibkey>
    </frontmatter>
    <paper id="5">
      <title>Fast Word Predictor for On-Device Application</title>
      <author><first>Huy Tien</first><last>Nguyen</last></author>
      <author><first>Khoi Tuan</first><last>Nguyen</last></author>
      <author><first>Anh Tuan</first><last>Nguyen</last></author>
      <author><first>Thanh Lac Thi</first><last>Tran</last></author>
      <pages>23–27</pages>
      <abstract>Learning on large text corpora, <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a> achieve promising results in the next word prediction task. However, deploying these huge <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on devices has to deal with constraints of <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">low latency</a> and a small binary size. To address these challenges, we propose a fast word predictor performing efficiently on <a href="https://en.wikipedia.org/wiki/Mobile_device">mobile devices</a>. Compared with a standard <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> which has a similar word prediction rate, the proposed model obtains 60 % reduction in memory size and 100X faster inference time on a middle-end mobile device. The <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is developed as a <a href="https://en.wikipedia.org/wiki/Software_feature">feature</a> for a <a href="https://en.wikipedia.org/wiki/Online_chat">chat application</a> which serves more than 100 million users.</abstract>
      <url hash="8808a273">2020.coling-demos.5</url>
      <doi>10.18653/v1/2020.coling-demos.5</doi>
      <bibkey>nguyen-etal-2020-fast</bibkey>
    </paper>
    <paper id="10">
      <title>Discussion Tracker : Supporting Teacher Learning about Students’ Collaborative Argumentation in High School Classrooms</title>
      <author><first>Luca</first><last>Lugini</last></author>
      <author><first>Christopher</first><last>Olshefski</last></author>
      <author><first>Ravneet</first><last>Singh</last></author>
      <author><first>Diane</first><last>Litman</last></author>
      <author><first>Amanda</first><last>Godley</last></author>
      <pages>53–58</pages>
      <abstract>Teaching collaborative argumentation is an advanced skill that many K-12 teachers struggle to develop. To address this, we have developed Discussion Tracker, a classroom discussion analytics system based on novel algorithms for classifying argument moves, specificity, and collaboration. Results from a classroom deployment indicate that teachers found the <a href="https://en.wikipedia.org/wiki/Analytics">analytics</a> useful, and that the underlying <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> perform with moderate to substantial agreement with humans.</abstract>
      <url hash="e8730ac2">2020.coling-demos.10</url>
      <doi>10.18653/v1/2020.coling-demos.10</doi>
      <bibkey>lugini-etal-2020-discussion</bibkey>
    </paper>
    <paper id="11">
      <title>An Online Readability Leveled Arabic Thesaurus<fixed-case>A</fixed-case>rabic Thesaurus</title>
      <author><first>Zhengyang</first><last>Jiang</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Muhamed</first><last>Al Khalil</last></author>
      <pages>59–63</pages>
      <abstract>This demo paper introduces the online Readability Leveled Arabic Thesaurus interface. For a given user input word, this interface provides the word’s possible lemmas, <a href="https://en.wikipedia.org/wiki/Root_(linguistics)">roots</a>, English glosses, related Arabic words and phrases, and <a href="https://en.wikipedia.org/wiki/Readability">readability</a> on a five-level readability scale. This <a href="https://en.wikipedia.org/wiki/Interface_(computing)">interface</a> builds on and connects multiple existing Arabic resources and processing tools. This one-of-a-kind system enables <a href="https://en.wikipedia.org/wiki/Arabic">Arabic speakers</a> and learners to benefit from advances in <a href="https://en.wikipedia.org/wiki/Computational_linguistics">Arabic computational linguistics technologies</a>. Feedback from users of the <a href="https://en.wikipedia.org/wiki/System">system</a> will help the developers to identify lexical coverage gaps and errors. A live link to the demo is available at : http://samer.camel-lab.com/.</abstract>
      <url hash="1ac24895">2020.coling-demos.11</url>
      <doi>10.18653/v1/2020.coling-demos.11</doi>
      <bibkey>jiang-etal-2020-online</bibkey>
    </paper>
    <paper id="12">
      <title>TrainX   Named Entity Linking with Active Sampling and Bi-Encoders<fixed-case>T</fixed-case>rain<fixed-case>X</fixed-case> – Named Entity Linking with Active Sampling and Bi-Encoders</title>
      <author><first>Tom</first><last>Oberhauser</last></author>
      <author><first>Tim</first><last>Bischoff</last></author>
      <author><first>Karl</first><last>Brendel</last></author>
      <author><first>Maluna</first><last>Menke</last></author>
      <author><first>Tobias</first><last>Klatt</last></author>
      <author><first>Amy</first><last>Siu</last></author>
      <author><first>Felix Alexander</first><last>Gers</last></author>
      <author><first>Alexander</first><last>Löser</last></author>
      <pages>64–69</pages>
      <abstract>We demonstrate TrainX, a <a href="https://en.wikipedia.org/wiki/System">system</a> for Named Entity Linking for medical experts. It combines state-of-the-art entity recognition and linking architectures, such as Flair and fine-tuned Bi-Encoders based on BERT, with an easy-to-use interface for healthcare professionals. We support medical experts in annotating training data by using active sampling strategies to forward informative samples to the annotator. We demonstrate that our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is capable of linking against large knowledge bases, such as <a href="https://en.wikipedia.org/wiki/Unified_Modeling_Language">UMLS</a> (3.6 million entities), and supporting zero-shot cases, where the linker has never seen the entity before. Those zero-shot capabilities help to mitigate the problem of rare and expensive training data that is a common issue in the <a href="https://en.wikipedia.org/wiki/Medicine">medical domain</a>.</abstract>
      <url hash="d8771307">2020.coling-demos.12</url>
      <doi>10.18653/v1/2020.coling-demos.12</doi>
      <bibkey>oberhauser-etal-2020-trainx</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
    </paper>
    <paper id="16">
      <title>Epistolary Education in 21st Century : A System to Support Composition of E-mails by Students to Superiors in Japanese<fixed-case>E</fixed-case>-mails by Students to Superiors in <fixed-case>J</fixed-case>apanese</title>
      <author><first>Kenji</first><last>Ryu</last></author>
      <author><first>Michal</first><last>Ptaszynski</last></author>
      <pages>87–92</pages>
      <abstract>E-mail is a <a href="https://en.wikipedia.org/wiki/Communication">communication tool</a> widely used by people of all ages on the Internet today, often in business and formal situations, especially in <a href="https://en.wikipedia.org/wiki/Japan">Japan</a>. Moreover, Japanese E-mail communication has a set of specific rules taught using specialized guidebooks. E-mail literacy education for many Japanese students is typically provided in a traditional, yet inefficient lecture-based way. We propose a system to support Japanese students in writing E-mails to superiors (teachers, job hunting representatives, etc.). We firstly make an investigation into the importance of formal E-mails in <a href="https://en.wikipedia.org/wiki/Japan">Japan</a>, and what is needed to successfully write a formal E-mail. Next, we develop the <a href="https://en.wikipedia.org/wiki/System">system</a> with accordance to those rules. Finally, we evaluated the <a href="https://en.wikipedia.org/wiki/System">system</a> twofold. The results, although performed on a small number of samples, were generally positive, and clearly indicated additional ways to improve the <a href="https://en.wikipedia.org/wiki/System">system</a>.</abstract>
      <url hash="f09773cb">2020.coling-demos.16</url>
      <doi>10.18653/v1/2020.coling-demos.16</doi>
      <bibkey>ryu-ptaszynski-2020-epistolary</bibkey>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts</booktitle>
      <editor><first>Lucia</first><last>Specia</last></editor>
      <editor><first>Daniel</first><last>Beck</last></editor>
      <publisher>International Committee for Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="da2c5756">2020.coling-tutorials.0</url>
      <bibkey>coling-2020-international-linguistics-tutorial</bibkey>
    </frontmatter>
    </volume>
  <volume id="industry" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the 28th International Conference on Computational Linguistics: Industry Track</booktitle>
      <editor><first>Ann</first><last>Clifton</last></editor>
      <editor><first>Courtney</first><last>Napoles</last></editor>
      <publisher>International Committee on Computational Linguistics</publisher>
      <address>Online</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="397be4de">2020.coling-industry.0</url>
      <bibkey>coling-2020-international-linguistics-industry</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Query Distillation : BERT-based Distillation for Ensemble Ranking<fixed-case>BERT</fixed-case>-based Distillation for Ensemble Ranking</title>
      <author><first>Wangshu</first><last>Zhang</last></author>
      <author><first>Junhong</first><last>Liu</last></author>
      <author><first>Zujie</first><last>Wen</last></author>
      <author><first>Yafang</first><last>Wang</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>33–43</pages>
      <abstract>Recent years have witnessed substantial progress in the development of neural ranking networks, but also an increasingly heavy computational burden due to growing numbers of parameters and the adoption of model ensembles. Knowledge Distillation (KD) is a common solution to balance the effectiveness and efficiency. However, it is not straightforward to apply KD to ranking problems. Ranking Distillation (RD) has been proposed to address this issue, but only shows effectiveness on recommendation tasks. We present a novel two-stage distillation method for ranking problems that allows a smaller student model to be trained while benefitting from the better performance of the teacher model, providing better control of the inference latency and computational burden. We design a novel BERT-based ranking model structure for list-wise ranking to serve as our student model. All ranking candidates are fed to the BERT model simultaneously, such that the self-attention mechanism can enable joint inference to rank the document list. Our experiments confirm the advantages of our method, not just with regard to the inference latency but also in terms of higher-quality rankings compared to the original teacher model.</abstract>
      <url hash="844b65d2">2020.coling-industry.4</url>
      <doi>10.18653/v1/2020.coling-industry.4</doi>
      <bibkey>zhang-etal-2020-query</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="8">
      <title>Interactive Question Clarification in Dialogue via <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a></title>
      <author><first>Xiang</first><last>Hu</last></author>
      <author><first>Zujie</first><last>Wen</last></author>
      <author><first>Yafang</first><last>Wang</last></author>
      <author><first>Xiaolong</first><last>Li</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>78–89</pages>
      <abstract>Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of <a href="https://en.wikipedia.org/wiki/Human–computer_interaction">human interaction</a>, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement model</a> to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is trained using <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> with a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep policy network</a>. We evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> based on real-world user clicks and demonstrate significant improvements across several different experiments.</abstract>
      <url hash="a9390190">2020.coling-industry.8</url>
      <doi>10.18653/v1/2020.coling-industry.8</doi>
      <bibkey>hu-etal-2020-interactive</bibkey>
    </paper>
    <paper id="9">
      <title>Towards building a Robust Industry-scale Question Answering System</title>
      <author><first>Rishav</first><last>Chakravarti</last></author>
      <author><first>Anthony</first><last>Ferritto</last></author>
      <author><first>Bhavani</first><last>Iyer</last></author>
      <author><first>Lin</first><last>Pan</last></author>
      <author><first>Radu</first><last>Florian</last></author>
      <author><first>Salim</first><last>Roukos</last></author>
      <author><first>Avi</first><last>Sil</last></author>
      <pages>90–101</pages>
      <abstract>Industry-scale NLP systems necessitate two <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>. Robustness : zero-shot transfer learning (ZSTL) performance has to be commendable and 2. Efficiency : systems have to train efficiently and respond instantaneously. In this paper, we introduce the development of a production model called GAAMA (Go Ahead Ask Me Anything) which possess the above two characteristics. For <a href="https://en.wikipedia.org/wiki/Robust_statistics">robustness</a>, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> trains on the recently introduced Natural Questions (NQ) dataset. NQ poses additional challenges over older datasets like SQuAD : (a) QA systems need to read and comprehend an entire Wikipedia article rather than a small passage, and (b) NQ does not suffer from <a href="https://en.wikipedia.org/wiki/Observation_bias">observation bias</a> during construction, resulting in less lexical overlap between the question and the article. GAAMA consists of Attention-over-Attention, diversity among attention heads, hierarchical transfer learning, and synthetic data augmentation while being computationally inexpensive. Building on top of the powerful BERTQA model, GAAMA provides a 2.0 % absolute boost in F1 over the industry-scale state-of-the-art (SOTA) system on NQ. Further, we show that GAAMA transfers zero-shot to unseen real life and important domains as it yields respectable performance on two benchmarks : the BioASQ and the newly introduced CovidQA datasets.</abstract>
      <url hash="53c12c0e">2020.coling-industry.9</url>
      <doi>10.18653/v1/2020.coling-industry.9</doi>
      <bibkey>chakravarti-etal-2020-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/covidqa">CovidQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="18">
      <title>Learning Domain Terms-Empirical Methods to Enhance Enterprise Text Analytics Performance</title>
      <author><first>Gargi</first><last>Roy</last></author>
      <author><first>Lipika</first><last>Dey</last></author>
      <author><first>Mohammad</first><last>Shakir</last></author>
      <author><first>Tirthankar</first><last>Dasgupta</last></author>
      <pages>190–201</pages>
      <abstract>Performance of standard text analytics algorithms are known to be substantially degraded on consumer generated data, which are often very noisy. These algorithms also do not work well on enterprise data which has a very different nature from <a href="https://en.wikipedia.org/wiki/News_aggregator">News repositories</a>, storybooks or Wikipedia data. Text cleaning is a mandatory step which aims at <a href="https://en.wikipedia.org/wiki/Noise_reduction">noise removal</a> and <a href="https://en.wikipedia.org/wiki/Noise_reduction">correction</a> to improve performance. However, enterprise data need special cleaning methods since it contains many domain terms which appear to be noise against a standard <a href="https://en.wikipedia.org/wiki/Dictionary">dictionary</a>, but in reality are not so. In this work we present detailed analysis of characteristics of enterprise data and suggest <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a> for cleaning these repositories after domain terms have been automatically segregated from true noise terms. Noise terms are thereafter corrected in a contextual fashion. The effectiveness of the method is established through careful manual evaluation of error corrections over several standard <a href="https://en.wikipedia.org/wiki/Data_set">data sets</a>, including those available for hate speech detection, where there is deliberate distortion to avoid detection. We also share results to show enhancement in <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification accuracy</a> after noise correction.</abstract>
      <url hash="51fb6fa3">2020.coling-industry.18</url>
      <doi>10.18653/v1/2020.coling-industry.18</doi>
      <bibkey>roy-etal-2020-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="20">
      <title>ScopeIt : Scoping Task Relevant Sentences in Documents<fixed-case>S</fixed-case>cope<fixed-case>I</fixed-case>t: Scoping Task Relevant Sentences in Documents</title>
      <author><first>Barun</first><last>Patra</last></author>
      <author><first>Vishwas</first><last>Suryanarayanan</last></author>
      <author><first>Chala</first><last>Fufa</last></author>
      <author><first>Pamela</first><last>Bhattacharya</last></author>
      <author><first>Charles</first><last>Lee</last></author>
      <pages>214–227</pages>
      <abstract>A prominent problem faced by conversational agents working with large documents (Eg : email-based assistants) is the frequent presence of information in the document that is irrelevant to the assistant. This in turn makes it harder for the <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agent</a> to accurately detect intents, extract entities relevant to those intents and perform the desired action. To address this issue we present a neural model for scoping relevant information for the <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agent</a> from a large document. We show that when used as the first step in a popularly used email-based assistant for helping users schedule meetings, our proposed model helps improve the performance of the intent detection and entity extraction tasks required by the agent for correctly scheduling meetings : across a suite of 6 downstream tasks, by using our proposed method, we observe an average gain of 35 % in precision without any drop in recall. Additionally, we demonstrate that the same approach can be used for component level analysis in large documents, such as signature block identification.</abstract>
      <url hash="d262d9a1">2020.coling-industry.20</url>
      <doi>10.18653/v1/2020.coling-industry.20</doi>
      <bibkey>patra-etal-2020-scopeit</bibkey>
    </paper>
    </volume>
</collection>