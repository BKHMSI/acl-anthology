<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.eacl">
  <volume id="main" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</booktitle>
      <editor><first>Paola</first><last>Merlo</last></editor>
      <editor><first>Jorg</first><last>Tiedemann</last></editor>
      <editor><first>Reut</first><last>Tsarfaty</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="e67d310e">2021.eacl-main.0</url>
      <bibkey>eacl-2021-european</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Unsupervised Sentence-embeddings by Manifold Approximation and <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">Projection</a></title>
      <author><first>Subhradeep</first><last>Kayal</last></author>
      <pages>1–11</pages>
      <abstract>The concept of unsupervised universal sentence encoders has gained traction recently, wherein pre-trained models generate effective task-agnostic fixed-dimensional representations for phrases, sentences and paragraphs. Such methods are of varying complexity, from simple weighted-averages of word vectors to complex language-models based on bidirectional transformers. In this work we propose a novel technique to generate sentence-embeddings in an unsupervised fashion by projecting the sentences onto a fixed-dimensional manifold with the objective of preserving local neighbourhoods in the original space. To delineate such neighbourhoods we experiment with several set-distance metrics, including the recently proposed Word Mover’s distance, while the fixed-dimensional projection is achieved by employing a scalable and efficient manifold approximation method rooted in topological data analysis. We test our approach, which we term EMAP or Embeddings by Manifold Approximation and Projection, on six publicly available text-classification datasets of varying size and complexity. Empirical results show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> consistently performs similar to or better than several alternative state-of-the-art approaches.</abstract>
      <url hash="973d0cbd">2021.eacl-main.1</url>
      <bibkey>kayal-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.eacl-main.1</doi>
      <pwccode url="https://github.com/DeepK/distance-embed" additional="false">DeepK/distance-embed</pwccode>
    </paper>
    <paper id="3">
      <title>Disambiguatory Signals are Stronger in Word-initial Positions</title>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <pages>31–41</pages>
      <abstract>Psycholinguistic studies of human word processing and <a href="https://en.wikipedia.org/wiki/Lexical_access">lexical access</a> provide ample evidence of the preferred nature of word-initial versus word-final segments, e.g., in terms of attention paid by listeners (greater) or the likelihood of reduction by speakers (lower). This has led to the conjectureas in Wedel et al. (2019b), but common elsewherethat languages have evolved to provide more information earlier in words than later. Information-theoretic methods to establish such tendencies in lexicons have suffered from several methodological shortcomings that leave open the question of whether this high word-initial informativeness is actually a property of the lexicon or simply an artefact of the incremental nature of recognition. In this paper, we point out the confounds in existing methods for comparing the informativeness of segments early in the word versus later in the word, and present several new measures that avoid these confounds. When controlling for these confounds, we still find evidence across hundreds of languages that indeed there is a cross-linguistic tendency to front-load information in words.</abstract>
      <url hash="a13ee00b">2021.eacl-main.3</url>
      <bibkey>pimentel-etal-2021-disambiguatory</bibkey>
      <doi>10.18653/v1/2021.eacl-main.3</doi>
      <pwccode url="https://github.com/tpimentelms/frontload-disambiguation" additional="false">tpimentelms/frontload-disambiguation</pwccode>
    </paper>
    <paper id="5">
      <title>If you’ve got it, flaunt it : Making the most of fine-grained sentiment annotations</title>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>49–62</pages>
      <abstract>Fine-grained sentiment analysis attempts to extract <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment holders</a>, targets and polar expressions and resolve the relationship between them, but progress has been hampered by the difficulty of <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a>. Targeted sentiment analysis, on the other hand, is a more narrow task, focusing on extracting <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment targets</a> and classifying their polarity. In this paper, we explore whether incorporating holder and expression information can improve target extraction and classification and perform experiments on eight English datasets. We conclude that jointly predicting target and polarity BIO labels improves target extraction, and that augmenting the input text with gold expressions generally improves targeted polarity classification. This highlights the potential importance of annotating expressions for fine-grained sentiment datasets. At the same time, our results show that performance of current <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for predicting polar expressions is poor, hampering the benefit of this <a href="https://en.wikipedia.org/wiki/Information">information</a> in practice.</abstract>
      <url hash="215bb1f7">2021.eacl-main.5</url>
      <bibkey>barnes-etal-2021-youve</bibkey>
      <doi>10.18653/v1/2021.eacl-main.5</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
    </paper>
    <paper id="9">
      <title>Telling BERT’s Full Story : from Local Attention to Global Aggregation<fixed-case>BERT</fixed-case>’s Full Story: from Local Attention to Global Aggregation</title>
      <author><first>Damian</first><last>Pascual</last></author>
      <author><first>Gino</first><last>Brunner</last></author>
      <author><first>Roger</first><last>Wattenhofer</last></author>
      <pages>105–124</pages>
      <abstract>We take a deep look into the behaviour of self-attention heads in the transformer architecture. In light of recent work discouraging the use of <a href="https://en.wikipedia.org/wiki/Attentional_control">attention distributions</a> for explaining a model’s behaviour, we show that <a href="https://en.wikipedia.org/wiki/Attentional_control">attention distributions</a> can nevertheless provide insights into the local behaviour of attention heads. This way, we propose a distinction between local patterns revealed by <a href="https://en.wikipedia.org/wiki/Attention">attention</a> and global patterns that refer back to the input, and analyze BERT from both angles. We use gradient attribution to analyze how the output of an attention head depends on the input tokens, effectively extending the local attention-based analysis to account for the mixing of information throughout the transformer layers. We find that there is a significant mismatch between attention and attribution distributions, caused by the mixing of context inside the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>. We quantify this discrepancy and observe that interestingly, there are some patterns that persist across all layers despite the mixing.</abstract>
      <url hash="b4a60e5f">2021.eacl-main.9</url>
      <bibkey>pascual-etal-2021-telling</bibkey>
      <doi>10.18653/v1/2021.eacl-main.9</doi>
    </paper>
    <paper id="14">
      <title>Maximal Multiverse Learning for Promoting Cross-Task Generalization of Fine-Tuned Language Models</title>
      <author><first>Itzik</first><last>Malkiel</last></author>
      <author><first>Lior</first><last>Wolf</last></author>
      <pages>187–199</pages>
      <abstract>Language modeling with BERT consists of two phases of (i) unsupervised pre-training on unlabeled text, and (ii) <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> for a specific supervised task. We present a method that leverages the second phase to its fullest, by applying an extensive number of parallel classifier heads, which are enforced to be orthogonal, while adaptively eliminating the weaker heads during training. We conduct an extensive inter- and intra-dataset evaluation, showing that our method improves the generalization ability of BERT, sometimes leading to a +9 % gain in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>. These results highlight the importance of a proper fine-tuning procedure, especially for relatively smaller-sized datasets. Our code is attached as supplementary.</abstract>
      <url hash="c05f6870">2021.eacl-main.14</url>
      <bibkey>malkiel-wolf-2021-maximal</bibkey>
      <doi>10.18653/v1/2021.eacl-main.14</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="16">
      <title>Dictionary-based Debiasing of Pre-trained Word Embeddings</title>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <pages>212–223</pages>
      <abstract>Word embeddings trained on large corpora have shown to encode high levels of unfair discriminatory gender, racial, religious and ethnic biases. In contrast, human-written dictionaries describe the meanings of words in a concise, objective and an unbiased manner. We propose a method for debiasing pre-trained word embeddings using dictionaries, without requiring access to the original training resources or any knowledge regarding the word embedding algorithms used. Unlike prior work, our proposed method does not require the types of biases to be pre-defined in the form of word lists, and learns the constraints that must be satisfied by unbiased word embeddings automatically from dictionary definitions of the words. Specifically, we learn an encoder to generate a debiased version of an input <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> such that it (a) retains the semantics of the pre-trained word embedding, (b) agrees with the unbiased definition of the word according to the dictionary, and (c) remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space. Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics.</abstract>
      <url hash="ab3761eb">2021.eacl-main.16</url>
      <bibkey>kaneko-bollegala-2021-dictionary</bibkey>
      <doi>10.18653/v1/2021.eacl-main.16</doi>
      <pwccode url="https://github.com/kanekomasahiro/dict-debias" additional="false">kanekomasahiro/dict-debias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="18">
      <title>Non-Autoregressive Text Generation with Pre-trained Language Models</title>
      <author><first>Yixuan</first><last>Su</last></author>
      <author><first>Deng</first><last>Cai</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>David</first><last>Vandyke</last></author>
      <author><first>Simon</first><last>Baker</last></author>
      <author><first>Piji</first><last>Li</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>234–243</pages>
      <abstract>Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a <a href="https://en.wikipedia.org/wiki/Non-blocking_algorithm">NAG model</a> for a greatly improved performance. Additionally, we devise two mechanisms to alleviate the two common problems of vanilla NAG models : the inflexibility of prefixed output length and the conditional independence of individual token predictions. To further strengthen the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a>, sentence compression and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component.</abstract>
      <url hash="daadb168">2021.eacl-main.18</url>
      <bibkey>su-etal-2021-non</bibkey>
      <doi>10.18653/v1/2021.eacl-main.18</doi>
      <pwccode url="https://github.com/yxuansu/NAG-BERT" additional="false">yxuansu/NAG-BERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sentence-compression">Sentence Compression</pwcdataset>
    </paper>
    <paper id="21">
      <title>CD2CR : Co-reference resolution across documents and domains<fixed-case>CD</fixed-case>ˆ2<fixed-case>CR</fixed-case>: Co-reference resolution across documents and domains</title>
      <author><first>James</first><last>Ravenscroft</last></author>
      <author><first>Amanda</first><last>Clare</last></author>
      <author><first>Arie</first><last>Cattan</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <pages>270–280</pages>
      <abstract>Cross-document co-reference resolution (CDCR) is the task of identifying and linking mentions to entities and concepts across many <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">text documents</a>. Current state-of-the-art <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> assume that all documents are of the same type (e.g. news articles) or fall under the same theme. However, it is also desirable to perform CDCR across different domains (type or theme). A particular use case we focus on in this paper is the resolution of entities mentioned across <a href="https://en.wikipedia.org/wiki/Scientific_literature">scientific work</a> and <a href="https://en.wikipedia.org/wiki/Article_(publishing)">newspaper articles</a> that discuss them. Identifying the same entities and corresponding concepts in both <a href="https://en.wikipedia.org/wiki/Scientific_literature">scientific articles</a> and <a href="https://en.wikipedia.org/wiki/News">news</a> can help scientists understand how their work is represented in mainstream media. We propose a new task and English language dataset for cross-document cross-domain co-reference resolution (CD2CR). The <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> aims to identify links between entities across heterogeneous document types. We show that in this cross-domain, cross-document setting, existing CDCR models do not perform well and we provide a baseline model that outperforms current state-of-the-art CDCR models on CD2CR. Our data set, annotation tool and guidelines as well as our model for cross-document cross-domain co-reference are all supplied as open access open source resources.</abstract>
      <url hash="043a9c51">2021.eacl-main.21</url>
      <bibkey>ravenscroft-etal-2021-cd</bibkey>
      <doi>10.18653/v1/2021.eacl-main.21</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="24">
      <title>Recipes for Building an Open-Domain Chatbot</title>
      <author><first>Stephen</first><last>Roller</last></author>
      <author><first>Emily</first><last>Dinan</last></author>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Da</first><last>Ju</last></author>
      <author><first>Mary</first><last>Williamson</last></author>
      <author><first>Yinhan</first><last>Liu</last></author>
      <author><first>Jing</first><last>Xu</last></author>
      <author><first>Myle</first><last>Ott</last></author>
      <author><first>Eric Michael</first><last>Smith</last></author>
      <author><first>Y-Lan</first><last>Boureau</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>300–325</pages>
      <abstract>Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills : providing engaging talking points, and displaying knowledge, <a href="https://en.wikipedia.org/wiki/Empathy">empathy</a> and <a href="https://en.wikipedia.org/wiki/Personality">personality</a> appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these <a href="https://en.wikipedia.org/wiki/Recipe">recipes</a> with 90 M, 2.7B and 9.4B parameter models, and make our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>.</abstract>
      <url hash="9127d1ae">2021.eacl-main.24</url>
      <bibkey>roller-etal-2021-recipes</bibkey>
      <doi>10.18653/v1/2021.eacl-main.24</doi>
      <pwccode url="https://github.com/facebookresearch/ParlAI" additional="true">facebookresearch/ParlAI</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blended-skill-talk">Blended Skill Talk</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="25">
      <title>Evaluating the Evaluation of Diversity in <a href="https://en.wikipedia.org/wiki/Natural-language_generation">Natural Language Generation</a></title>
      <author><first>Guy</first><last>Tevet</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>326–346</pages>
      <abstract>Despite growing interest in natural language generation (NLG) models that produce diverse outputs, there is currently no principled method for evaluating the diversity of an NLG system. In this work, we propose a <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> for evaluating diversity metrics. The framework measures the correlation between a proposed <a href="https://en.wikipedia.org/wiki/Multiculturalism">diversity metric</a> and a <a href="https://en.wikipedia.org/wiki/Multiculturalism">diversity parameter</a>, a single parameter that controls some aspect of <a href="https://en.wikipedia.org/wiki/Multiculturalism">diversity</a> in generated text. For example, a diversity parameter might be a <a href="https://en.wikipedia.org/wiki/Binary_variable">binary variable</a> used to instruct crowdsourcing workers to generate text with either low or high content diversity. We demonstrate the utility of our framework by : (a) establishing best practices for eliciting <a href="https://en.wikipedia.org/wiki/Multiculturalism">diversity judgments</a> from humans, (b) showing that humans substantially outperform automatic metrics in estimating content diversity, and (c) demonstrating that existing methods for controlling <a href="https://en.wikipedia.org/wiki/Multiculturalism">diversity</a> by tuning a decoding parameter mostly affect form but not meaning. Our <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> can advance the understanding of different diversity metrics, an essential step on the road towards better NLG systems.<i>metrics</i>. The framework measures the correlation between a proposed diversity metric and a <i>diversity parameter</i>, a single parameter that controls some aspect of diversity in generated text. For example, a diversity parameter might be a binary variable used to instruct crowdsourcing workers to generate text with either low or high content diversity. We demonstrate the utility of our framework by: (a) establishing best practices for eliciting diversity judgments from humans, (b) showing that humans substantially outperform automatic metrics in estimating content diversity, and (c) demonstrating that existing methods for controlling diversity by tuning a “decoding parameter” mostly affect form but not meaning. Our framework can advance the understanding of different diversity metrics, an essential step on the road towards better NLG systems.</abstract>
      <url hash="a37926a9">2021.eacl-main.25</url>
      <bibkey>tevet-berant-2021-evaluating</bibkey>
      <doi>10.18653/v1/2021.eacl-main.25</doi>
      <pwccode url="https://github.com/GuyTevet/diversity-eval" additional="false">GuyTevet/diversity-eval</pwccode>
    </paper>
    <paper id="26">
      <title>Retrieval, Re-ranking and Multi-task Learning for Knowledge-Base Question Answering</title>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <author><first>Patrick</first><last>Ng</last></author>
      <author><first>Ramesh</first><last>Nallapati</last></author>
      <author><first>Bing</first><last>Xiang</last></author>
      <pages>347–357</pages>
      <abstract>Question answering over knowledge bases (KBQA) usually involves three sub-tasks, namely topic entity detection, <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity linking</a> and relation detection. Due to the large number of entities and relations inside knowledge bases (KB), previous work usually utilized sophisticated rules to narrow down the search space and managed only a subset of KBs in memory. In this work, we leverage a retrieve-and-rerank framework to access KBs via traditional information retrieval (IR) method, and re-rank retrieved candidates with more powerful neural networks such as the pre-trained BERT model. Considering the fact that directly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The <a href="https://en.wikipedia.org/wiki/Unified_Modeling_Language">unified model</a> is then trained under a multi-task learning framework. Experiments show that : (1) Our IR-based retrieval method is able to collect high-quality candidates efficiently, thus enables our method adapt to large-scale KBs easily ; (2) the BERT model improves the accuracy across all three sub-tasks ; and (3) benefiting from <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.<i>retrieve-and-rerank</i> framework to access KBs via traditional information retrieval (IR) method, and re-rank retrieved candidates with more powerful neural networks such as the pre-trained BERT model. Considering the fact that directly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning framework. Experiments show that: (1) Our IR-based retrieval method is able to collect high-quality candidates efficiently, thus enables our method adapt to large-scale KBs easily; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) benefiting from multi-task learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.</abstract>
      <url hash="273b1e13">2021.eacl-main.26</url>
      <bibkey>wang-etal-2021-retrieval</bibkey>
      <doi>10.18653/v1/2021.eacl-main.26</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="27">
      <title>Implicitly Abusive Comparisons   A New Dataset and <a href="https://en.wikipedia.org/wiki/Linguistic_description">Linguistic Analysis</a></title>
      <author><first>Michael</first><last>Wiegand</last></author>
      <author><first>Maja</first><last>Geulig</last></author>
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <pages>358–368</pages>
      <abstract>We examine the task of detecting implicitly abusive comparisons (e.g. Your hair looks like you have been electrocuted). Implicitly abusive comparisons are abusive comparisons in which <a href="https://en.wikipedia.org/wiki/Abusive_language">abusive words</a> (e.g. dumbass or scum) are absent. We detail the process of creating a novel <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for this task via <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a> that includes several measures to obtain a sufficiently representative and unbiased set of comparisons. We also present <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> experiments that include a range of <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">linguistic features</a> that help us better understand the mechanisms underlying abusive comparisons.</abstract>
      <url hash="73403334">2021.eacl-main.27</url>
      <bibkey>wiegand-etal-2021-implicitly</bibkey>
      <doi>10.18653/v1/2021.eacl-main.27</doi>
      <pwccode url="https://github.com/miwieg/implicitly_abusive_comparisons" additional="false">miwieg/implicitly_abusive_comparisons</pwccode>
    </paper>
    <paper id="29">
      <title>A Systematic Review of Reproducibility Research in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a></title>
      <author><first>Anya</first><last>Belz</last></author>
      <author><first>Shubham</first><last>Agarwal</last></author>
      <author><first>Anastasia</first><last>Shimorina</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <pages>381–393</pages>
      <abstract>Against the background of what has been termed a reproducibility crisis in science, the NLP field is becoming increasingly interested in, and conscientious about, the reproducibility of its results. The past few years have seen an impressive range of new initiatives, events and active research in the area. However, the field is far from reaching a consensus about how <a href="https://en.wikipedia.org/wiki/Reproducibility">reproducibility</a> should be defined, measured and addressed, with diversity of views currently increasing rather than converging. With this focused contribution, we aim to provide a wide-angle, and as near as possible complete, snapshot of current work on <a href="https://en.wikipedia.org/wiki/Reproducibility">reproducibility</a> in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>,</abstract>
      <url hash="37f3ec68">2021.eacl-main.29</url>
      <bibkey>belz-etal-2021-systematic</bibkey>
      <doi>10.18653/v1/2021.eacl-main.29</doi>
    </paper>
    <paper id="31">
      <title>Semantic Oppositeness Assisted Deep Contextual Modeling for Automatic Rumor Detection in <a href="https://en.wikipedia.org/wiki/Social_network">Social Networks</a></title>
      <author><first>Nisansa</first><last>de Silva</last></author>
      <author><first>Dejing</first><last>Dou</last></author>
      <pages>405–415</pages>
      <abstract>Social networks face a major challenge in the form of rumors and fake news, due to their intrinsic nature of connecting users to millions of others, and of giving any individual the power to post anything. Given the rapid, widespread dissemination of information in <a href="https://en.wikipedia.org/wiki/List_of_social_networking_websites">social networks</a>, manually detecting suspicious news is sub-optimal. Thus, research on automatic rumor detection has become a necessity. Previous works in the domain have utilized the reply relations between posts, as well as the <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a> between the main post and its context, consisting of replies, in order to obtain state-of-the-art performance. In this work, we demonstrate that semantic oppositeness can improve the performance on the task of rumor detection. We show that semantic oppositeness captures elements of discord, which are not properly covered by previous efforts, which only utilize <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a> or reply structure. We show, with extensive experiments on recent data sets for this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a>, that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves state-of-the-art performance. Further, we show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is more resistant to the variances in performance introduced by <a href="https://en.wikipedia.org/wiki/Randomness">randomness</a>.</abstract>
      <url hash="1f0b8604">2021.eacl-main.31</url>
      <bibkey>de-silva-dou-2021-semantic</bibkey>
      <doi>10.18653/v1/2021.eacl-main.31</doi>
    </paper>
    <paper id="32">
      <title>Polarized-VAE : Proximity Based Disentangled Representation Learning for Text Generation<fixed-case>VAE</fixed-case>: Proximity Based Disentangled Representation Learning for Text Generation</title>
      <author><first>Vikash</first><last>Balasubramanian</last></author>
      <author><first>Ivan</first><last>Kobyzev</last></author>
      <author><first>Hareesh</first><last>Bahuleyan</last></author>
      <author><first>Ilya</first><last>Shapiro</last></author>
      <author><first>Olga</first><last>Vechtomova</last></author>
      <pages>416–423</pages>
      <abstract>Learning disentangled representations of realworld data is a challenging open problem. Most previous methods have focused on either supervised approaches which use attribute labels or unsupervised approaches that manipulate the factorization in the latent space of models such as the variational autoencoder (VAE) by training with task-specific losses. In this work, we propose polarized-VAE, an approach that disentangles select attributes in the latent space based on proximity measures reflecting the similarity between data points with respect to these attributes. We apply our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> to disentangle the <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> and <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">syntax of sentences</a> and carry out transfer experiments. Polarized-VAE outperforms the VAE baseline and is competitive with state-of-the-art approaches, while being more a general framework that is applicable to other attribute disentanglement tasks.</abstract>
      <url hash="87cf8c2d">2021.eacl-main.32</url>
      <bibkey>balasubramanian-etal-2021-polarized</bibkey>
      <doi>10.18653/v1/2021.eacl-main.32</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="36">
      <title>FEWS : Large-Scale, Low-Shot Word Sense Disambiguation with the Dictionary<fixed-case>FEWS</fixed-case>: Large-Scale, Low-Shot Word Sense Disambiguation with the Dictionary</title>
      <author><first>Terra</first><last>Blevins</last></author>
      <author><first>Mandar</first><last>Joshi</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>455–465</pages>
      <abstract>Current models for Word Sense Disambiguation (WSD) struggle to disambiguate rare senses, despite reaching human performance on global WSD metrics. This stems from a lack of data for both modeling and evaluating rare senses in existing WSD datasets. In this paper, we introduce FEWS (Few-shot Examples of Word Senses), a new low-shot WSD dataset automatically extracted from example sentences in <a href="https://en.wikipedia.org/wiki/Wiktionary">Wiktionary</a>. FEWS has high sense coverage across different natural language domains and provides : (1) a large training set that covers many more senses than previous datasets and (2) a comprehensive evaluation set containing few- and zero-shot examples of a wide variety of senses. We establish baselines on FEWS with knowledge-based and neural WSD approaches and present transfer learning experiments demonstrating that models additionally trained with FEWS better capture rare senses in existing WSD datasets. Finally, we find <a href="https://en.wikipedia.org/wiki/Human">humans</a> outperform the best baseline models on FEWS, indicating that FEWS will support significant future work on low-shot WSD.</abstract>
      <url hash="678232f4">2021.eacl-main.36</url>
      <bibkey>blevins-etal-2021-fews</bibkey>
      <doi>10.18653/v1/2021.eacl-main.36</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="37">
      <title>MONAH : Multi-Modal Narratives for Humans to analyze conversations<fixed-case>MONAH</fixed-case>: Multi-Modal Narratives for Humans to analyze conversations</title>
      <author><first>Joshua Y.</first><last>Kim</last></author>
      <author><first>Kalina</first><last>Yacef</last></author>
      <author><first>Greyson</first><last>Kim</last></author>
      <author><first>Chunfeng</first><last>Liu</last></author>
      <author><first>Rafael</first><last>Calvo</last></author>
      <author><first>Silas</first><last>Taylor</last></author>
      <pages>466–479</pages>
      <abstract>In conversational analyses, humans manually weave multimodal information into the transcripts, which is significantly time-consuming. We introduce a system that automatically expands the verbatim transcripts of video-recorded conversations using multimodal data streams. This system uses a set of preprocessing rules to weave multimodal annotations into the verbatim transcripts and promote <a href="https://en.wikipedia.org/wiki/Interpretability">interpretability</a>. Our feature engineering contributions are two-fold : firstly, we identify the range of multimodal features relevant to detect rapport-building ; secondly, we expand the range of multimodal annotations and show that the expansion leads to statistically significant improvements in detecting rapport-building.</abstract>
      <url hash="46ff1015">2021.eacl-main.37</url>
      <bibkey>kim-etal-2021-monah</bibkey>
      <doi>10.18653/v1/2021.eacl-main.37</doi>
      <pwccode url="https://github.com/SpectData/MONAH" additional="false">SpectData/MONAH</pwccode>
    </paper>
    <paper id="38">
      <title>Does Typological Blinding Impede Cross-Lingual Sharing?</title>
      <author><first>Johannes</first><last>Bjerva</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>480–486</pages>
      <abstract>Bridging the performance gap between high- and low-resource languages has been the focus of much previous work. Typological features from <a href="https://en.wikipedia.org/wiki/Database">databases</a> such as the <a href="https://en.wikipedia.org/wiki/World_Atlas_of_Language_Structures">World Atlas of Language Structures (WALS)</a> are a prime candidate for this, as such data exists even for very low-resource languages. However, previous work has only found minor benefits from using <a href="https://en.wikipedia.org/wiki/Typology_(linguistics)">typological information</a>. Our hypothesis is that a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained in a cross-lingual setting will pick up on typological cues from the input data, thus overshadowing the utility of explicitly using such <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>. We verify this hypothesis by blinding a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to typological information, and investigate how cross-lingual sharing and performance is impacted. Our model is based on a cross-lingual architecture in which the latent weights governing the sharing between languages is learnt during training. We show that (i) preventing this <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> from exploiting <a href="https://en.wikipedia.org/wiki/Typology_(linguistics)">typology</a> severely reduces performance, while a control experiment reaffirms that (ii) encouraging sharing according to <a href="https://en.wikipedia.org/wiki/Typology_(linguistics)">typology</a> somewhat improves performance.</abstract>
      <url hash="743fbb70">2021.eacl-main.38</url>
      <bibkey>bjerva-augenstein-2021-typological</bibkey>
      <doi>10.18653/v1/2021.eacl-main.38</doi>
    </paper>
    <paper id="44">
      <title>Improving Factual Consistency Between a Response and Persona Facts</title>
      <author><first>Mohsen</first><last>Mesgar</last></author>
      <author><first>Edwin</first><last>Simpson</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>549–562</pages>
      <abstract>Neural models for response generation produce responses that are semantically plausible but not necessarily factually consistent with facts describing the speaker’s persona. These models are trained with fully <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> where the <a href="https://en.wikipedia.org/wiki/Loss_function">objective function</a> barely captures factual consistency. We propose to fine-tune these models by <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> and an efficient <a href="https://en.wikipedia.org/wiki/Reward_system">reward function</a> that explicitly captures the consistency between a response and persona facts as well as semantic plausibility. Our automatic and human evaluations on the PersonaChat corpus confirm that our approach increases the rate of responses that are factually consistent with persona facts over its supervised counterpart while retains the language quality of responses.</abstract>
      <url hash="b0a6eab0">2021.eacl-main.44</url>
      <bibkey>mesgar-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.eacl-main.44</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="45">
      <title>PolyLM : Learning about <a href="https://en.wikipedia.org/wiki/Polysemy">Polysemy</a> through <a href="https://en.wikipedia.org/wiki/Language_model">Language Modeling</a><fixed-case>P</fixed-case>oly<fixed-case>LM</fixed-case>: Learning about Polysemy through Language Modeling</title>
      <author><first>Alan</first><last>Ansell</last></author>
      <author><first>Felipe</first><last>Bravo-Marquez</last></author>
      <author><first>Bernhard</first><last>Pfahringer</last></author>
      <pages>563–574</pages>
      <abstract>To avoid the meaning conflation deficiency of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>, a number of <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> have aimed to embed individual word senses. These methods at one time performed well on tasks such as word sense induction (WSI), but they have since been overtaken by task-specific techniques which exploit contextualized embeddings. However, sense embeddings and contextualization need not be mutually exclusive. We introduce PolyLM, a method which formulates the task of learning sense embeddings as a language modeling problem, allowing contextualization techniques to be applied. PolyLM is based on two underlying assumptions about word senses : firstly, that the probability of a word occurring in a given context is equal to the sum of the probabilities of its individual senses occurring ; and secondly, that for a given occurrence of a word, one of its senses tends to be much more plausible in the context than the others. We evaluate PolyLM on <a href="https://en.wikipedia.org/wiki/Wavelength-division_multiplexing">WSI</a>, showing that it performs considerably better than previous sense embedding techniques, and matches the current state-of-the-art specialized <a href="https://en.wikipedia.org/wiki/Wavelength-division_multiplexing">WSI method</a> despite having six times fewer parameters. Code and pre-trained models are available at https://github.com/AlanAnsell/PolyLM.</abstract>
      <url hash="48e20b62">2021.eacl-main.45</url>
      <bibkey>ansell-etal-2021-polylm</bibkey>
      <doi>10.18653/v1/2021.eacl-main.45</doi>
      <pwccode url="https://github.com/AlanAnsell/PolyLM" additional="false">AlanAnsell/PolyLM</pwccode>
    </paper>
    <paper id="53">
      <title>Cross-lingual Entity Alignment with Incidental Supervision</title>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Weijia</first><last>Shi</last></author>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>645–658</pages>
      <abstract>Much research effort has been put to multilingual knowledge graph (KG) embedding methods to address the entity alignment task, which seeks to match entities in different languagespecific KGs that refer to the same real-world object. Such methods are often hindered by the insufficiency of seed alignment provided between <a href="https://en.wikipedia.org/wiki/Glossary_of_plant_morphology">KGs</a>. Therefore, we propose a new model, JEANS, which jointly represents multilingual KGs and <a href="https://en.wikipedia.org/wiki/Text_corpus">text corpora</a> in a shared embedding scheme, and seeks to improve <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity alignment</a> with incidental supervision signals from text. JEANS first deploys an entity grounding process to combine each KG with the monolingual text corpus. Then, two learning processes are conducted : (i) an embedding learning process to encode the KG and text of each language in one embedding space, and (ii) a self-learning based alignment learning process to iteratively induce the correspondence of entities and that of lexemes between embeddings. Experiments on benchmark datasets show that JEANS leads to promising improvement on <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity alignment</a> with incidental supervision, and significantly outperforms state-of-the-art methods that solely rely on internal information of KGs.</abstract>
      <url hash="ec9154e4">2021.eacl-main.53</url>
      <attachment type="Dataset" hash="9236fbcc">2021.eacl-main.53.Dataset.txt</attachment>
      <attachment type="Software" hash="9236fbcc">2021.eacl-main.53.Software.txt</attachment>
      <bibkey>chen-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.eacl-main.53</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/dbp15k">DBP15K</pwcdataset>
    </paper>
    <paper id="54">
      <title>Query Generation for Multimodal Documents</title>
      <author><first>Kyungho</first><last>Kim</last></author>
      <author><first>Kyungjae</first><last>Lee</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Young-In</first><last>Song</last></author>
      <author><first>Seungwook</first><last>Lee</last></author>
      <pages>659–668</pages>
      <abstract>This paper studies the problem of generatinglikely queries for multimodal documents withimages. Our application scenario is enablingefficient first-stage retrieval of relevant doc-uments, by attaching generated queries to <a href="https://en.wikipedia.org/wiki/Document">doc-uments</a> before indexing. We can then indexthis expanded text to efficiently narrow downto candidate matches using inverted index, sothat expensive reranking can follow. Our eval-uation results show that our proposed multi-modal representation meaningfully improvesrelevance ranking. More importantly, ourframework can achieve the state of the art inthe first stage retrieval scenarios</abstract>
      <url hash="8d14077c">2021.eacl-main.54</url>
      <bibkey>kim-etal-2021-query</bibkey>
      <doi>10.18653/v1/2021.eacl-main.54</doi>
    </paper>
    <paper id="55">
      <title>End-to-End Argument Mining as Biaffine Dependency Parsing</title>
      <author><first>Yuxiao</first><last>Ye</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <pages>669–678</pages>
      <abstract>Non-neural approaches to argument mining (AM) are often pipelined and require heavy feature-engineering. In this paper, we propose a neural end-to-end approach to AM which is based on dependency parsing, in contrast to the current state-of-the-art which relies on relation extraction. Our biaffine AM dependency parser significantly outperforms the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>, performing at F1 = 73.5 % for component identification and F1 = 46.4 % for relation identification. One of the advantages of treating AM as biaffine dependency parsing is the simple neural architecture that results. The idea of treating AM as dependency parsing is not new, but has previously been abandoned as it was lagging far behind the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>. In a thorough analysis, we investigate the factors that contribute to the success of our model : the biaffine model itself, our representation for the dependency structure of arguments, different encoders in the biaffine model, and syntactic information additionally fed to the model. Our work demonstrates that dependency parsing for AM, an overlooked idea from the past, deserves more attention in the future.</abstract>
      <url hash="d197c295">2021.eacl-main.55</url>
      <bibkey>ye-teufel-2021-end</bibkey>
      <doi>10.18653/v1/2021.eacl-main.55</doi>
    </paper>
    <paper id="57">
      <title>CTC-based Compression for Direct Speech Translation<fixed-case>CTC</fixed-case>-based Compression for Direct Speech Translation</title>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Mauro</first><last>Cettolo</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>690–696</pages>
      <abstract>Previous studies demonstrated that a dynamic phone-informed compression of the input audio is beneficial for speech translation (ST). However, they required a dedicated <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for phone recognition and did not test this solution for direct ST, in which a single <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> translates the input audio into the target language without intermediate representations. In this work, we propose the first <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> able to perform a <a href="https://en.wikipedia.org/wiki/Dynamic_compression">dynamic compression</a> of the input in direct ST models. In particular, we exploit the Connectionist Temporal Classification (CTC) to compress the input sequence according to its phonetic characteristics. Our experiments demonstrate that our <a href="https://en.wikipedia.org/wiki/Solution">solution</a> brings a 1.3-1.5 BLEU improvement over a strong <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> on two language pairs (English-Italian and English-German), contextually reducing the <a href="https://en.wikipedia.org/wiki/Memory_footprint">memory footprint</a> by more than 10 %.</abstract>
      <url hash="82f6a7af">2021.eacl-main.57</url>
      <bibkey>gaido-etal-2021-ctc</bibkey>
      <doi>10.18653/v1/2021.eacl-main.57</doi>
      <pwccode url="https://github.com/mgaido91/FBK-fairseq-ST" additional="false">mgaido91/FBK-fairseq-ST</pwccode>
    </paper>
    <paper id="60">
      <title>Top-down Discourse Parsing via Sequence Labelling</title>
      <author><first>Fajri</first><last>Koto</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>715–726</pages>
      <abstract>We introduce a <a href="https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design">top-down approach</a> to discourse parsing that is conceptually simpler than its predecessors (Kobayashi et al., 2020 ; Zhang et al., 2020). By framing the task as a sequence labelling problem where the goal is to iteratively segment a document into individual discourse units, we are able to eliminate the decoder and reduce the search space for splitting points. We explore both traditional <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent models</a> and modern pre-trained transformer models for the task, and additionally introduce a novel dynamic oracle for <a href="https://en.wikipedia.org/wiki/Top-down_parsing">top-down parsing</a>. Based on the Full metric, our proposed LSTM model sets a new state-of-the-art for RST parsing.</abstract>
      <url hash="a719a474">2021.eacl-main.60</url>
      <bibkey>koto-etal-2021-top</bibkey>
      <doi>10.18653/v1/2021.eacl-main.60</doi>
      <pwccode url="https://github.com/fajri91/NeuralRST-TopDown" additional="false">fajri91/NeuralRST-TopDown</pwccode>
    </paper>
    <paper id="64">
      <title>Neural Data-to-Text Generation with LM-based Text Augmentation<fixed-case>LM</fixed-case>-based Text Augmentation</title>
      <author><first>Ernie</first><last>Chang</last></author>
      <author><first>Xiaoyu</first><last>Shen</last></author>
      <author><first>Dawei</first><last>Zhu</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <author><first>Hui</first><last>Su</last></author>
      <pages>758–768</pages>
      <abstract>For many new application domains for data-to-text generation, the main obstacle in training neural models consists of a lack of training data. While usually large numbers of instances are available on the data side, often only very few text samples are available. To address this problem, we here propose a novel few-shot approach for this setting. Our approach automatically augments the <a href="https://en.wikipedia.org/wiki/Data">data</a> available for training by (i) generating new text samples based on replacing specific values by alternative ones from the same category, (ii) generating new text samples based on GPT-2, and (iii) proposing an automatic method for pairing the new text samples with <a href="https://en.wikipedia.org/wiki/Data">data samples</a>. As the text augmentation can introduce noise to the training data, we use cycle consistency as an objective, in order to make sure that a given data sample can be correctly reconstructed after having been formulated as text (and that text samples can be reconstructed from data). On both the E2E and WebNLG benchmarks, we show that this weakly supervised training paradigm is able to outperform fully supervised sequence-to-sequence models with less than 10 % of the training set. By utilizing all annotated data, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can boost the performance of a standard sequence-to-sequence model by over 5 BLEU points, establishing a new state-of-the-art on both datasets.</abstract>
      <url hash="7722c7b3">2021.eacl-main.64</url>
      <bibkey>chang-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.eacl-main.64</doi>
    </paper>
    <paper id="65">
      <title>Self-Training Pre-Trained Language Models for Zero- and Few-Shot Multi-Dialectal Arabic Sequence Labeling<fixed-case>A</fixed-case>rabic Sequence Labeling</title>
      <author><first>Muhammad</first><last>Khalifa</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>Khaled</first><last>Shaalan</last></author>
      <pages>769–782</pages>
      <abstract>A sufficient amount of annotated data is usually required to fine-tune pre-trained language models for downstream tasks. Unfortunately, attaining <a href="https://en.wikipedia.org/wiki/Labeled_data">labeled data</a> can be costly, especially for <a href="https://en.wikipedia.org/wiki/Variety_(linguistics)">multiple language varieties</a> and dialects. We propose to self-train pre-trained language models in zero- and few-shot scenarios to improve performance on data-scarce varieties using only resources from data-rich ones. We demonstrate the utility of our approach in the context of Arabic sequence labeling by using a language model fine-tuned on Modern Standard Arabic (MSA) only to predict named entities (NE) and part-of-speech (POS) tags on several dialectal Arabic (DA) varieties. We show that self-training is indeed powerful, improving zero-shot MSA-to-DA transfer by as large as 10 % F_1 (NER) and 2 % accuracy (POS tagging). We acquire even better performance in few-shot scenarios with limited amounts of <a href="https://en.wikipedia.org/wiki/Label_(computer_science)">labeled data</a>. We conduct an ablation study and show that the performance boost observed directly results from training data augmentation possible with DA examples via self-training. This opens up opportunities for developing DA models exploiting only MSA resources. Our approach can also be extended to other languages and tasks.<tex-math>_1</tex-math> (NER) and 2% accuracy (POS tagging). We acquire even better performance in few-shot scenarios with limited amounts of labeled data. We conduct an ablation study and show that the performance boost observed directly results from training data augmentation possible with DA examples via self-training. This opens up opportunities for developing DA models exploiting only MSA resources. Our approach can also be extended to other languages and tasks.</abstract>
      <url hash="1591bec6">2021.eacl-main.65</url>
      <bibkey>khalifa-etal-2021-self</bibkey>
      <doi>10.18653/v1/2021.eacl-main.65</doi>
      <pwccode url="https://github.com/mohammadKhalifa/zero-shot-arabic-dialects" additional="false">mohammadKhalifa/zero-shot-arabic-dialects</pwccode>
    </paper>
    <paper id="67">
      <title>Coordinate Constructions in English Enhanced Universal Dependencies : Analysis and Computational Modeling<fixed-case>E</fixed-case>nglish Enhanced <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies: Analysis and Computational Modeling</title>
      <author><first>Stefan</first><last>Grünewald</last></author>
      <author><first>Prisca</first><last>Piccirilli</last></author>
      <author><first>Annemarie</first><last>Friedrich</last></author>
      <pages>795–809</pages>
      <abstract>In this paper, we address the representation of coordinate constructions in Enhanced Universal Dependencies (UD), where relevant dependency links are propagated from conjunction heads to other conjuncts. English treebanks for enhanced UD have been created from gold basic dependencies using a heuristic rule-based converter, which propagates only core arguments. With the aim of determining which set of links should be propagated from a semantic perspective, we create a large-scale dataset of manually edited syntax graphs. We identify several <a href="https://en.wikipedia.org/wiki/Observational_error">systematic errors</a> in the original data, and propose to also propagate <a href="https://en.wikipedia.org/wiki/Adjuncts">adjuncts</a>. We observe high <a href="https://en.wikipedia.org/wiki/Inter-annotator_agreement">inter-annotator agreement</a> for this semantic annotation task. Using our new manually verified dataset, we perform the first principled comparison of rule-based and (partially novel) machine-learning based methods for conjunction propagation for <a href="https://en.wikipedia.org/wiki/English_language">English</a>. We show that learning propagation rules is more effective than hand-designing heuristic rules. When using automatic parses, our neural graph-parser based edge predictor outperforms the currently predominant pipelines using a basic-layer tree parser plus converters.</abstract>
      <url hash="deb6a8c5">2021.eacl-main.67</url>
      <attachment type="Dataset" hash="666fe678">2021.eacl-main.67.Dataset.zip</attachment>
      <bibkey>grunewald-etal-2021-coordinate</bibkey>
      <doi>10.18653/v1/2021.eacl-main.67</doi>
    </paper>
    <paper id="70">
      <title>Continuous Learning in Neural Machine Translation using Bilingual Dictionaries</title>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>830–840</pages>
      <abstract>While recent advances in <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> led to significant improvements in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> is often still not able to continuously adapt to the environment. For humans, as well as for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, <a href="https://en.wikipedia.org/wiki/Bilingual_dictionaries">bilingual dictionaries</a> are a promising knowledge source to continuously integrate new knowledge. However, their exploitation poses several challenges : The system needs to be able to perform <a href="https://en.wikipedia.org/wiki/One-shot_learning">one-shot learning</a> as well as model the <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphology</a> of source and target language. In this work, we proposed an evaluation framework to assess the ability of <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> to continuously learn new phrases. We integrate one-shot learning methods for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> with different word representations and show that it is important to address both in order to successfully make use of <a href="https://en.wikipedia.org/wiki/Bilingual_dictionary">bilingual dictionaries</a>. By addressing both challenges we are able to improve the ability to translate new, rare words and phrases from 30 % to up to 70 %. The correct lemma is even generated by more than 90 %.</abstract>
      <url hash="ad22f49b">2021.eacl-main.70</url>
      <bibkey>niehues-2021-continuous</bibkey>
      <doi>10.18653/v1/2021.eacl-main.70</doi>
    </paper>
    <paper id="71">
      <title>Adv-OLM : Generating Textual Adversaries via OLM<fixed-case>OLM</fixed-case>: Generating Textual Adversaries via <fixed-case>OLM</fixed-case></title>
      <author><first>Vijit</first><last>Malik</last></author>
      <author><first>Ashwani</first><last>Bhat</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>841–849</pages>
      <abstract>Deep learning models are susceptible to adversarial examples that have imperceptible perturbations in the original input, resulting in adversarial attacks against these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Analysis of these attacks on the state of the art transformers in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> can help improve the robustness of these models against such adversarial inputs. In this paper, we present Adv-OLM, a black-box attack method that adapts the idea of Occlusion and Language Models (OLM) to the current state of the art attack methods. OLM is used to rank words of a sentence, which are later substituted using word replacement strategies. We experimentally show that our approach outperforms other attack methods for several text classification tasks.</abstract>
      <url hash="b735b11c">2021.eacl-main.71</url>
      <bibkey>malik-etal-2021-adv</bibkey>
      <doi>10.18653/v1/2021.eacl-main.71</doi>
      <pwccode url="https://github.com/vijit-m/Adv-OLM" additional="false">vijit-m/Adv-OLM</pwccode>
    </paper>
    <paper id="74">
      <title>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</title>
      <author><first>Gautier</first><last>Izacard</last></author>
      <author><first>Edouard</first><last>Grave</last></author>
      <pages>874–880</pages>
      <abstract>Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the <a href="https://en.wikipedia.org/wiki/Natural_Questions">Natural Questions</a> and TriviaQA open benchmarks. Interestingly, we observe that the performance of this <a href="https://en.wikipedia.org/wiki/Scientific_method">method</a> significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> to efficiently aggregate and combine evidence from multiple passages.</abstract>
      <url hash="e46810b8">2021.eacl-main.74</url>
      <bibkey>izacard-grave-2021-leveraging</bibkey>
      <doi>10.18653/v1/2021.eacl-main.74</doi>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/conditionalqa">ConditionalQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="79">
      <title>A Neural Few-Shot Text Classification Reality Check</title>
      <author><first>Thomas</first><last>Dopierre</last></author>
      <author><first>Christophe</first><last>Gravier</last></author>
      <author><first>Wilfried</first><last>Logerais</last></author>
      <pages>935–943</pages>
      <abstract>Modern classification models tend to struggle when the amount of annotated data is scarce. To overcome this issue, several neural few-shot classification models have emerged, yielding significant progress over time, both in <a href="https://en.wikipedia.org/wiki/Computer_vision">Computer Vision</a> and <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a>. In the latter, such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> used to rely on fixed word embeddings, before the advent of <a href="https://en.wikipedia.org/wiki/Transformer">transformers</a>. Additionally, some <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> used in <a href="https://en.wikipedia.org/wiki/Computer_vision">Computer Vision</a> are yet to be tested in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP applications</a>. In this paper, we compare all these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, first adapting those made in the field of <a href="https://en.wikipedia.org/wiki/Digital_image_processing">image processing</a> to <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, and second providing them access to <a href="https://en.wikipedia.org/wiki/Transformer">transformers</a>. We then test these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> equipped with the same transformer-based encoder on the intent detection task, known for having a large amount of classes. Our results reveal that while methods perform almost equally on the ARSC dataset, this is not the case for the Intent Detection task, where most recent and supposedly best competitors perform worse than older and simpler ones (while all are are given access to transformers). We also show that a simple <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline</a> is surprisingly strong. All the new developed <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> as well as the evaluation framework are made publicly available.</abstract>
      <url hash="2bcf2023">2021.eacl-main.79</url>
      <bibkey>dopierre-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.eacl-main.79</doi>
      <pwccode url="https://github.com/tdopierre/FewShotText" additional="false">tdopierre/FewShotText</pwccode>
    </paper>
    <paper id="80">
      <title>Multilingual Machine Translation : Closing the Gap between Shared and Language-specific Encoder-Decoders</title>
      <author><first>Carlos</first><last>Escolano</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>José A. R.</first><last>Fonollosa</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <pages>944–948</pages>
      <abstract>State-of-the-art multilingual machine translation relies on a universal encoder-decoder, which requires retraining the entire system to add new languages. In this paper, we propose an alternative approach that is based on language-specific encoder-decoders, and can thus be more easily extended to new languages by learning their corresponding modules. So as to encourage a common interlingua representation, we simultaneously train the N initial languages. Our experiments show that the proposed approach outperforms the universal encoder-decoder by 3.28 BLEU points on average, while allowing to add new languages without the need to retrain the rest of the modules. All in all, our work closes the gap between shared and language-specific encoderdecoders, advancing toward modular multilingual machine translation systems that can be flexibly extended in lifelong learning settings.</abstract>
      <url hash="d115cb71">2021.eacl-main.80</url>
      <bibkey>escolano-etal-2021-multilingual</bibkey>
      <doi>10.18653/v1/2021.eacl-main.80</doi>
    </paper>
    <paper id="84">
      <title>Identifying Named Entities as they are Typed</title>
      <author><first>Ravneet</first><last>Arora</last></author>
      <author><first>Chen-Tse</first><last>Tsai</last></author>
      <author><first>Daniel</first><last>Preotiuc-Pietro</last></author>
      <pages>976–988</pages>
      <abstract>Identifying <a href="https://en.wikipedia.org/wiki/Named_entity">named entities</a> in written text is an essential component of the text processing pipeline used in applications such as <a href="https://en.wikipedia.org/wiki/Text_editor">text editors</a> to gain a better understanding of the semantics of the text. However, the typical experimental setup for evaluating Named Entity Recognition (NER) systems is not directly applicable to <a href="https://en.wikipedia.org/wiki/System">systems</a> that process text in real time as the text is being typed. Evaluation is performed on a sentence level assuming the end-user is willing to wait until the entire sentence is typed for entities to be identified and further linked to identifiers or co-referenced. We introduce a novel experimental setup for NER systems for applications where decisions about named entity boundaries need to be performed in an online fashion. We study how state-of-the-art methods perform under this setup in multiple languages and propose adaptations to these models to suit this new experimental setup. Experimental results show that the best <a href="https://en.wikipedia.org/wiki/System">systems</a> that are evaluated on each token after its typed, reach performance within 15 F1 points of systems that are evaluated at the end of the sentence. These show that entity recognition can be performed in this <a href="https://en.wikipedia.org/wiki/Computer_simulation">setup</a> and open up the development of other NLP tools in a similar setup.</abstract>
      <url hash="071ddca4">2021.eacl-main.84</url>
      <bibkey>arora-etal-2021-identifying</bibkey>
      <doi>10.18653/v1/2021.eacl-main.84</doi>
    </paper>
    <paper id="85">
      <title>SANDI : Story-and-Images Alignment<fixed-case>SANDI</fixed-case>: Story-and-Images Alignment</title>
      <author><first>Sreyasi</first><last>Nag Chowdhury</last></author>
      <author><first>Simon</first><last>Razniewski</last></author>
      <author><first>Gerhard</first><last>Weikum</last></author>
      <pages>989–999</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Internet">Internet</a> contains a multitude of social media posts and other of stories where <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">text</a> is interspersed with <a href="https://en.wikipedia.org/wiki/Image">images</a>. In these contexts, <a href="https://en.wikipedia.org/wiki/Image">images</a> are not simply used for general illustration, but are judiciously placed in certain spots of a story for multimodal descriptions and narration. In this work we analyze the problem of text-image alignment, and present SANDI, a methodology for automatically selecting images from an image collection and aligning them with text paragraphs of a story. SANDI combines visual tags, user-provided tags and background knowledge, and uses an Integer Linear Program to compute alignments that are semantically meaningful. Experiments show that SANDI can select and align images with texts with high quality of semantic fit.</abstract>
      <url hash="c253539a">2021.eacl-main.85</url>
      <bibkey>nag-chowdhury-etal-2021-sandi</bibkey>
      <doi>10.18653/v1/2021.eacl-main.85</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/places205">Places205</pwcdataset>
    </paper>
    <paper id="87">
      <title>El Volumen Louder Por Favor : Code-switching in Task-oriented Semantic Parsing</title>
      <author><first>Arash</first><last>Einolghozati</last></author>
      <author><first>Abhinav</first><last>Arora</last></author>
      <author><first>Lorena</first><last>Sainz-Maza Lecanda</last></author>
      <author><first>Anuj</first><last>Kumar</last></author>
      <author><first>Sonal</first><last>Gupta</last></author>
      <pages>1009–1021</pages>
      <abstract>Being able to parse code-switched (CS) utterances, such as <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish+English</a> or <a href="https://en.wikipedia.org/wiki/Hindi">Hindi+English</a>, is essential to democratize task-oriented semantic parsing systems for certain locales. In this work, we focus on Spanglish (Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances alongside their semantic parses. We examine the CS generalizability of various Cross-lingual (XL) models and exhibit the advantage of pre-trained XL language models when data for only one language is present. As such, we focus on improving the pre-trained models for the case when only <a href="https://en.wikipedia.org/wiki/English_literature">English corpus</a> alongside either zero or a few CS training instances are available. We propose two data augmentation methods for the zero-shot and the few-shot settings : fine-tune using translate-and-align and augment using a generation model followed by match-and-filter. Combining the few-shot setting with the above improvements decreases the initial 30-point accuracy gap between the zero-shot and the full-data settings by two thirds.</abstract>
      <url hash="54bcdabf">2021.eacl-main.87</url>
      <bibkey>einolghozati-etal-2021-el</bibkey>
      <doi>10.18653/v1/2021.eacl-main.87</doi>
    </paper>
    <paper id="88">
      <title>Generating Syntactically Controlled Paraphrases without Using Annotated Parallel Pairs</title>
      <author><first>Kuan-Hao</first><last>Huang</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>1022–1033</pages>
      <abstract>Paraphrase generation plays an essential role in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language process (NLP)</a>, and it has many downstream applications. However, training supervised paraphrase models requires many annotated paraphrase pairs, which are usually costly to obtain. On the other hand, the <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> generated by existing unsupervised approaches are usually syntactically similar to the source sentences and are limited in diversity. In this paper, we demonstrate that it is possible to generate syntactically various paraphrases without the need for annotated paraphrase pairs. We propose Syntactically controlled Paraphrase Generator (SynPG), an encoder-decoder based model that learns to disentangle the <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> and the <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> of a sentence from a collection of unannotated texts. The disentanglement enables SynPG to control the <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> of output paraphrases by manipulating the embedding in the syntactic space. Extensive experiments using automatic metrics and human evaluation show that SynPG performs better syntactic control than unsupervised baselines, while the quality of the generated paraphrases is competitive. We also demonstrate that the performance of SynPG is competitive or even better than supervised models when the unannotated data is large. Finally, we show that the syntactically controlled paraphrases generated by SynPG can be utilized for <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> to improve the robustness of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP models</a>.</abstract>
      <url hash="ed6ba6d9">2021.eacl-main.88</url>
      <bibkey>huang-chang-2021-generating</bibkey>
      <doi>10.18653/v1/2021.eacl-main.88</doi>
      <pwccode url="https://github.com/uclanlp/synpg" additional="false">uclanlp/synpg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paranmt-50m">PARANMT-50M</pwcdataset>
    </paper>
    <paper id="89">
      <title>Data Augmentation for Hypernymy Detection</title>
      <author><first>Thomas</first><last>Kober</last></author>
      <author><first>Julie</first><last>Weeds</last></author>
      <author><first>Lorenzo</first><last>Bertolini</last></author>
      <author><first>David</first><last>Weir</last></author>
      <pages>1034–1048</pages>
      <abstract>The automatic detection of hypernymy relationships represents a challenging problem in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. The successful application of state-of-the-art supervised approaches using <a href="https://en.wikipedia.org/wiki/Distributed_representation">distributed representations</a> has generally been impeded by the limited availability of high quality training data. We have developed two novel data augmentation techniques which generate new training examples from existing ones. First, we combine the linguistic principles of hypernym transitivity and intersective modifier-noun composition to generate additional pairs of vectors, such as small dog-dog or small dog-animal, for which a hypernymy relationship can be assumed. Second, we use generative adversarial networks (GANs) to generate pairs of vectors for which the hypernymy relation can also be assumed. We furthermore present two complementary strategies for extending an existing <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> by leveraging linguistic resources such as <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a>. Using an evaluation across 3 different datasets for hypernymy detection and 2 different <a href="https://en.wikipedia.org/wiki/Vector_space">vector spaces</a>, we demonstrate that both of the proposed automatic data augmentation and dataset extension strategies substantially improve <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> performance.</abstract>
      <url hash="27663bea">2021.eacl-main.89</url>
      <attachment type="Software" hash="ca8afe3f">2021.eacl-main.89.Software.zip</attachment>
      <attachment type="Dataset" hash="07778fe5">2021.eacl-main.89.Dataset.txt</attachment>
      <bibkey>kober-etal-2021-data</bibkey>
      <doi>10.18653/v1/2021.eacl-main.89</doi>
      <pwccode url="https://github.com/tttthomasssss/le-augmentation" additional="false">tttthomasssss/le-augmentation</pwccode>
    </paper>
    <paper id="90">
      <title>Few-shot learning through contextual data augmentation</title>
      <author><first>Farid</first><last>Arthaud</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <pages>1049–1062</pages>
      <abstract>Machine translation (MT) models used in industries with constantly changing topics, such as <a href="https://en.wikipedia.org/wiki/Translation">translation</a> or <a href="https://en.wikipedia.org/wiki/News_agency">news agencies</a>, need to adapt to new data to maintain their performance over time. Our aim is to teach a pre-trained MT model to translate previously unseen words accurately, based on very few examples. We propose (i) an experimental setup allowing us to simulate novel vocabulary appearing in human-submitted translations, and (ii) corresponding evaluation metrics to compare our approaches. We extend a data augmentation approach using a pretrained language model to create training examples with similar contexts for novel words. We compare different fine-tuning and data augmentation approaches and show that adaptation on the scale of one to five examples is possible. Combining <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> with randomly selected training sentences leads to the highest BLEU score and accuracy improvements. Impressively, with only 1 to 5 examples, our model reports better accuracy scores than a reference system trained with on average 313 parallel examples.</abstract>
      <url hash="28c875d0">2021.eacl-main.90</url>
      <bibkey>arthaud-etal-2021-shot</bibkey>
      <doi>10.18653/v1/2021.eacl-main.90</doi>
      <pwccode url="https://gitlab.com/farid-fari/fewshot-learning" additional="false">farid-fari/fewshot-learning</pwccode>
    </paper>
    <paper id="91">
      <title>Zero-shot Generalization in Dialog State Tracking through Generative Question Answering</title>
      <author><first>Shuyang</first><last>Li</last></author>
      <author><first>Jin</first><last>Cao</last></author>
      <author><first>Mukund</first><last>Sridhar</last></author>
      <author><first>Henghui</first><last>Zhu</last></author>
      <author><first>Shang-Wen</first><last>Li</last></author>
      <author><first>Wael</first><last>Hamza</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>1063–1074</pages>
      <abstract>Dialog State Tracking (DST), an integral part of modern dialog systems, aims to track user preferences and constraints (slots) in task-oriented dialogs. In real-world settings with constantly changing services, DST systems must generalize to new domains and unseen slot types. Existing methods for DST do not generalize well to new slot names and many require known ontologies of slot types and values for inference. We introduce a novel ontology-free framework that supports <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language queries</a> for unseen constraints and slots in multi-domain task-oriented dialogs. Our approach is based on generative question-answering using a conditional language model pre-trained on substantive English sentences. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> improves joint goal accuracy in zero-shot domain adaptation settings by up to 9 % (absolute) over the previous <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> on the MultiWOZ 2.1 dataset.</abstract>
      <url hash="ebfb39d7">2021.eacl-main.91</url>
      <bibkey>li-etal-2021-zero</bibkey>
      <doi>10.18653/v1/2021.eacl-main.91</doi>
    </paper>
    <paper id="94">
      <title>MIDAS : A Dialog Act Annotation Scheme for Open Domain HumanMachine Spoken Conversations<fixed-case>MIDAS</fixed-case>: A Dialog Act Annotation Scheme for Open Domain <fixed-case>H</fixed-case>uman<fixed-case>M</fixed-case>achine Spoken Conversations</title>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>1103–1120</pages>
      <abstract>Dialog act prediction in open-domain conversations is an essential language comprehension task for both dialog system building and <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse analysis</a>. Previous dialog act schemes, such as SWBD-DAMSL, are designed mainly for <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse analysis</a> in human-human conversations. In this paper, we present a dialog act annotation scheme, MIDAS (Machine Interaction Dialog Act Scheme), targeted at open-domain human-machine conversations. MIDAS is designed to assist machines to improve their ability to understand human partners. MIDAS has a <a href="https://en.wikipedia.org/wiki/Hierarchical_organization">hierarchical structure</a> and supports multi-label annotations. We collected and annotated a large open-domain human-machine spoken conversation dataset (consisting of 24 K utterances). To validate our scheme, we leveraged <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning methods</a> to train a multi-label dialog act prediction model and reached an <a href="https://en.wikipedia.org/wiki/F-number">F1 score</a> of 0.79.</abstract>
      <url hash="099d9b0a">2021.eacl-main.94</url>
      <bibkey>yu-yu-2021-midas</bibkey>
      <doi>10.18653/v1/2021.eacl-main.94</doi>
    </paper>
    <paper id="99">
      <title>Detecting Extraneous Content in Podcasts</title>
      <author><first>Sravana</first><last>Reddy</last></author>
      <author><first>Yongze</first><last>Yu</last></author>
      <author><first>Aasish</first><last>Pappu</last></author>
      <author><first>Aswin</first><last>Sivaraman</last></author>
      <author><first>Rezvaneh</first><last>Rezapour</last></author>
      <author><first>Rosie</first><last>Jones</last></author>
      <pages>1166–1173</pages>
      <abstract>Podcast episodes often contain material extraneous to the main content, such as <a href="https://en.wikipedia.org/wiki/Advertising">advertisements</a>, interleaved within the audio and the written descriptions. We present <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> that leverage both textual and listening patterns in order to detect such content in <a href="https://en.wikipedia.org/wiki/Podcast">podcast descriptions</a> and <a href="https://en.wikipedia.org/wiki/Transcript_(law)">audio transcripts</a>. We demonstrate that our models are effective by evaluating them on the downstream task of podcast summarization and show that we can substantively improve ROUGE scores and reduce the extraneous content generated in the summaries.</abstract>
      <url hash="9fdab5ab">2021.eacl-main.99</url>
      <bibkey>reddy-etal-2021-detecting</bibkey>
      <doi>10.18653/v1/2021.eacl-main.99</doi>
    </paper>
    <paper id="102">
      <title>Joint Learning of Representations for Web-tables, Entities and Types using Graph Convolutional Network</title>
      <author><first>Aniket</first><last>Pramanick</last></author>
      <author><first>Indrajit</first><last>Bhattacharya</last></author>
      <pages>1197–1206</pages>
      <abstract>Existing approaches for table annotation with entities and types either capture the structure of table using graphical models, or learn embeddings of table entries without accounting for the complete syntactic structure. We propose TabGCN, that uses Graph Convolutional Networks to capture the complete structure of tables, <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> and the training annotations, and jointly learns embeddings for table elements as well as the entities and types. To account for knowledge incompleteness, TabGCN’s embeddings can be used to discover new entities and types. Using experiments on 5 benchmark datasets, we show that TabGCN significantly outperforms multiple state-of-the-art baselines for table annotation, while showing promising performance on downstream table-related applications.</abstract>
      <url hash="b387b940">2021.eacl-main.102</url>
      <bibkey>pramanick-bhattacharya-2021-joint</bibkey>
      <doi>10.18653/v1/2021.eacl-main.102</doi>
    </paper>
    <paper id="104">
      <title>ECOL-R : Encouraging Copying in Novel Object Captioning with <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a><fixed-case>ECOL</fixed-case>-<fixed-case>R</fixed-case>: Encouraging Copying in Novel Object Captioning with Reinforcement Learning</title>
      <author><first>Yufei</first><last>Wang</last></author>
      <author><first>Ian</first><last>Wood</last></author>
      <author><first>Stephen</first><last>Wan</last></author>
      <author><first>Mark</first><last>Johnson</last></author>
      <pages>1222–1234</pages>
      <abstract>Novel Object Captioning is a zero-shot Image Captioning task requiring describing objects not seen in the training captions, but for which information is available from external object detectors. The key challenge is to select and describe all salient detected novel objects in the input images. In this paper, we focus on this challenge and propose the ECOL-R model (Encouraging Copying of Object Labels with Reinforced Learning), a copy-augmented transformer model that is encouraged to accurately describe the novel object labels. This is achieved via a specialised reward function in the SCST reinforcement learning framework (Rennie et al., 2017) that encourages novel object mentions while maintaining the caption quality. We further restrict the SCST training to the images where detected objects are mentioned in reference captions to train the ECOL-R model. We additionally improve our copy mechanism via Abstract Labels, which transfer knowledge from known to novel object types, and a Morphological Selector, which determines the appropriate inflected forms of novel object labels. The resulting model sets new state-of-the-art on the nocaps (Agrawal et al., 2019) and held-out COCO (Hendricks et al., 2016) benchmarks.</abstract>
      <url hash="274f086e">2021.eacl-main.104</url>
      <bibkey>wang-etal-2021-ecol</bibkey>
      <doi>10.18653/v1/2021.eacl-main.104</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/open-images-v4">Open Images V4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nocaps">nocaps</pwcdataset>
    </paper>
    <paper id="107">
      <title>Debiasing Pre-trained Contextualised Embeddings</title>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <pages>1256–1266</pages>
      <abstract>In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.</abstract>
      <url hash="e0ec00e2">2021.eacl-main.107</url>
      <bibkey>kaneko-bollegala-2021-debiasing</bibkey>
      <doi>10.18653/v1/2021.eacl-main.107</doi>
      <pwccode url="https://github.com/kanekomasahiro/context-debias" additional="false">kanekomasahiro/context-debias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="112">
      <title>Cross-lingual Visual Pre-training for Multimodal Machine Translation</title>
      <author><first>Ozan</first><last>Caglayan</last></author>
      <author><first>Menekse</first><last>Kuyu</last></author>
      <author><first>Mustafa Sercan</first><last>Amac</last></author>
      <author><first>Pranava</first><last>Madhyastha</last></author>
      <author><first>Erkut</first><last>Erdem</last></author>
      <author><first>Aykut</first><last>Erdem</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>1317–1324</pages>
      <abstract>Pre-trained language models have been shown to improve performance in many <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language tasks</a> substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded cross-lingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision &amp; language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain state-of-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.</abstract>
      <url hash="25cffba7">2021.eacl-main.112</url>
      <revision id="1" href="2021.eacl-main.112v1" hash="bc103401" />
      <revision id="2" href="2021.eacl-main.112v2" hash="25cffba7" date="2021-05-03">Minor revision.</revision>
      <bibkey>caglayan-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.eacl-main.112</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="114">
      <title>An Expert Annotated Dataset for the Detection of Online Misogyny</title>
      <author><first>Ella</first><last>Guest</last></author>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <author><first>Alexandros</first><last>Mittos</last></author>
      <author><first>Nishanth</first><last>Sastry</last></author>
      <author><first>Gareth</first><last>Tyson</last></author>
      <author><first>Helen</first><last>Margetts</last></author>
      <pages>1336–1350</pages>
      <abstract>Online misogyny is a pernicious social problem that risks making online platforms toxic and unwelcoming to women. We present a new hierarchical taxonomy for <a href="https://en.wikipedia.org/wiki/Misogyny">online misogyny</a>, as well as an expert labelled dataset to enable automatic classification of misogynistic content. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> consists of 6567 labels for <a href="https://en.wikipedia.org/wiki/Reddit">Reddit posts</a> and comments. As previous research has found untrained crowdsourced annotators struggle with identifying <a href="https://en.wikipedia.org/wiki/Misogyny">misogyny</a>, we hired and trained annotators and provided them with robust annotation guidelines. We report <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline classification</a> performance on the <a href="https://en.wikipedia.org/wiki/Binary_classification">binary classification task</a>, achieving <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 0.93 and <a href="https://en.wikipedia.org/wiki/F-number">F1</a> of 0.43. The codebook and datasets are made freely available for future researchers.</abstract>
      <url hash="04bd3787">2021.eacl-main.114</url>
      <attachment type="Dataset" hash="553bb53b">2021.eacl-main.114.Dataset.zip</attachment>
      <bibkey>guest-etal-2021-expert</bibkey>
      <doi>10.18653/v1/2021.eacl-main.114</doi>
      <pwccode url="https://github.com/ellamguest/online-misogyny-eacl2021" additional="false">ellamguest/online-misogyny-eacl2021</pwccode>
    </paper>
    <paper id="115">
      <title>WikiMatrix : Mining 135 M Parallel Sentences in 1620 Language Pairs from Wikipedia<fixed-case>W</fixed-case>iki<fixed-case>M</fixed-case>atrix: Mining 135<fixed-case>M</fixed-case> Parallel Sentences in 1620 Language Pairs from <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Holger</first><last>Schwenk</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Shuo</first><last>Sun</last></author>
      <author><first>Hongyu</first><last>Gong</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <pages>1351–1361</pages>
      <abstract>We present an approach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 96 languages, including several dialects or low-resource languages. We do not limit the extraction process to alignments with English, but we systematically consider all possible language pairs. In total, we are able to extract 135 M <a href="https://en.wikipedia.org/wiki/Parallelism_(grammar)">parallel sentences</a> for 16720 different language pairs, out of which only 34 M are aligned with English. This <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> is freely available. To get an indication on the quality of the extracted bitexts, we train neural MT baseline systems on the mined data only for 1886 languages pairs, and evaluate them on the TED corpus, achieving strong BLEU scores for many language pairs. The WikiMatrix bitexts seem to be particularly interesting to train MT systems between distant languages without the need to pivot through English.</abstract>
      <url hash="476948aa">2021.eacl-main.115</url>
      <bibkey>schwenk-etal-2021-wikimatrix</bibkey>
      <doi>10.18653/v1/2021.eacl-main.115</doi>
      <pwccode url="https://github.com/facebookresearch/LASER" additional="true">facebookresearch/LASER</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="116">
      <title>ChEMU-Ref : A Corpus for Modeling <a href="https://en.wikipedia.org/wiki/Anaphora_resolution">Anaphora Resolution</a> in the Chemical Domain<fixed-case>C</fixed-case>h<fixed-case>EMU</fixed-case>-Ref: A Corpus for Modeling Anaphora Resolution in the Chemical Domain</title>
      <author><first>Biaoyan</first><last>Fang</last></author>
      <author><first>Christian</first><last>Druckenbrodt</last></author>
      <author><first>Saber A</first><last>Akhondi</last></author>
      <author><first>Jiayuan</first><last>He</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>1362–1375</pages>
      <abstract>Chemical patents contain rich coreference and bridging links, which are the target of this research. Specially, we introduce a novel annotation scheme, based on which we create the ChEMU-Ref dataset from reaction description snippets in English-language chemical patents. We propose a neural approach to <a href="https://en.wikipedia.org/wiki/Anaphora_resolution">anaphora resolution</a>, which we show to achieve strong results, especially when jointly trained over coreference and bridging links.</abstract>
      <url hash="26926da4">2021.eacl-main.116</url>
      <bibkey>fang-etal-2021-chemu</bibkey>
      <doi>10.18653/v1/2021.eacl-main.116</doi>
      <pwccode url="https://github.com/biaoyanf/chemu-ref" additional="false">biaoyanf/chemu-ref</pwccode>
    </paper>
    <paper id="118">
      <title>Searching for Search Errors in Neural Morphological Inflection</title>
      <author><first>Martina</first><last>Forster</last></author>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>1388–1394</pages>
      <abstract>Neural sequence-to-sequence models are currently the predominant choice for language generation tasks. Yet, on word-level tasks, exact inference of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> reveals the empty string is often the global optimum. Prior works have speculated this phenomenon is a result of the inadequacy of neural models for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">language generation</a>. However, in the case of morphological inflection, we find that the empty string is almost never the most probable solution under the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. Further, <a href="https://en.wikipedia.org/wiki/Greedy_search">greedy search</a> often finds the <a href="https://en.wikipedia.org/wiki/Maxima_and_minima">global optimum</a>. These observations suggest that the poor calibration of many neural models may stem from characteristics of a specific subset of tasks rather than general ill-suitedness of such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">language generation</a>.</abstract>
      <url hash="1869d8d4">2021.eacl-main.118</url>
      <bibkey>forster-etal-2021-searching</bibkey>
      <doi>10.18653/v1/2021.eacl-main.118</doi>
    </paper>
    <paper id="119">
      <title>Quantifying Appropriateness of Summarization Data for <a href="https://en.wikipedia.org/wiki/Curriculum">Curriculum Learning</a></title>
      <author><first>Ryuji</first><last>Kano</last></author>
      <author><first>Takumi</first><last>Takahashi</last></author>
      <author><first>Toru</first><last>Nishino</last></author>
      <author><first>Motoki</first><last>Taniguchi</last></author>
      <author><first>Tomoki</first><last>Taniguchi</last></author>
      <author><first>Tomoko</first><last>Ohkuma</last></author>
      <pages>1395–1405</pages>
      <abstract>Much research has reported the training data of summarization models are noisy ; summaries often do not reflect what is written in the source texts. We propose an effective method of curriculum learning to train <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization models</a> from such <a href="https://en.wikipedia.org/wiki/Noisy_data">noisy data</a>. Curriculum learning is used to train sequence-to-sequence models with <a href="https://en.wikipedia.org/wiki/Noisy_data">noisy data</a>. In translation tasks, previous research quantified <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a> of the training data using two <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained with noisy and clean corpora. Because such <a href="https://en.wikipedia.org/wiki/Text_corpus">corpora</a> do not exist in summarization fields, we propose a model that can quantify <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a> from a single noisy corpus. We conduct experiments on three summarization models ; one pretrained model and two non-pretrained models, and verify our method improves the performance. Furthermore, we analyze how different curricula affect the performance of pretrained and non-pretrained summarization models. Our result on human evaluation also shows our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> improves the performance of <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization models</a>.</abstract>
      <url hash="89ed3976">2021.eacl-main.119</url>
      <bibkey>kano-etal-2021-quantifying</bibkey>
      <doi>10.18653/v1/2021.eacl-main.119</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-tifu">Reddit TIFU</pwcdataset>
    </paper>
    <paper id="124">
      <title>Civil Rephrases Of Toxic Texts With Self-Supervised Transformers</title>
      <author><first>Léo</first><last>Laugier</last></author>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <author><first>Jeffrey</first><last>Sorensen</last></author>
      <author><first>Lucas</first><last>Dixon</last></author>
      <pages>1442–1461</pages>
      <abstract>Platforms that support online commentary, from <a href="https://en.wikipedia.org/wiki/List_of_social_networking_websites">social networks</a> to <a href="https://en.wikipedia.org/wiki/Online_newspaper">news sites</a>, are increasingly leveraging <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> to assist their moderation efforts. But this process does not typically provide feedback to the author that would help them contribute according to the community guidelines. This is prohibitively time-consuming for <a href="https://en.wikipedia.org/wiki/Moderator_(Internet)">human moderators</a> to do, and <a href="https://en.wikipedia.org/wiki/Computational_model">computational approaches</a> are still nascent. This work focuses on <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that can help suggest rephrasings of toxic comments in a more civil manner. Inspired by recent progress in unpaired sequence-to-sequence tasks, a self-supervised learning model is introduced, called CAE-T5. CAE-T5 employs a pre-trained text-to-text transformer, which is fine tuned with a denoising and cyclic auto-encoder loss. Experimenting with the largest toxicity detection dataset to date (Civil Comments) our model generates sentences that are more fluent and better at preserving the initial content compared to earlier text style transfer systems which we compare with using several scoring systems and human evaluation.</abstract>
      <url hash="408859a4">2021.eacl-main.124</url>
      <attachment type="Software" hash="c533d9fa">2021.eacl-main.124.Software.zip</attachment>
      <bibkey>laugier-etal-2021-civil</bibkey>
      <doi>10.18653/v1/2021.eacl-main.124</doi>
      <pwccode url="https://github.com/LeoLaugier/conditional-auto-encoder-text-to-text-transfer-transformer" additional="false">LeoLaugier/conditional-auto-encoder-text-to-text-transfer-transformer</pwccode>
    </paper>
    <paper id="125">
      <title>Generating Weather Comments from Meteorological Simulations</title>
      <author><first>Soichiro</first><last>Murakami</last></author>
      <author><first>Sora</first><last>Tanaka</last></author>
      <author><first>Masatsugu</first><last>Hangyo</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Kotaro</first><last>Funakoshi</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>1462–1473</pages>
      <abstract>The task of generating weather-forecast comments from meteorological simulations has the following requirements : (i) the changes in numerical values for various physical quantities need to be considered, (ii) the weather comments should be dependent on delivery time and area information, and (iii) the comments should provide useful information for users. To meet these requirements, we propose a data-to-text model that incorporates three types of encoders for numerical forecast maps, observation data, and meta-data. We also introduce weather labels representing <a href="https://en.wikipedia.org/wiki/Weather_forecasting">weather information</a>, such as sunny and rain, for our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to explicitly describe useful information. We conducted automatic and human evaluations. The results indicate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performed best against <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> in terms of informativeness. We make our code and data publicly available.</abstract>
      <url hash="f69d32eb">2021.eacl-main.125</url>
      <bibkey>murakami-etal-2021-generating</bibkey>
      <doi>10.18653/v1/2021.eacl-main.125</doi>
      <pwccode url="https://github.com/titech-nlp/pinpoint-weather" additional="false">titech-nlp/pinpoint-weather</pwccode>
    </paper>
    <paper id="127">
      <title>A phonetic model of non-native spoken word processing</title>
      <author><first>Yevgen</first><last>Matusevych</last></author>
      <author><first>Herman</first><last>Kamper</last></author>
      <author><first>Thomas</first><last>Schatz</last></author>
      <author><first>Naomi</first><last>Feldman</last></author>
      <author><first>Sharon</first><last>Goldwater</last></author>
      <pages>1480–1490</pages>
      <abstract>Non-native speakers show difficulties with spoken word processing. Many studies attribute these difficulties to imprecise phonological encoding of words in the lexical memory. We test an alternative hypothesis : that some of these difficulties can arise from the non-native speakers’ phonetic perception. We train a computational model of phonetic learning, which has no access to <a href="https://en.wikipedia.org/wiki/Phonology">phonology</a>, on either one or two languages. We first show that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> exhibits predictable behaviors on phone-level and word-level discrimination tasks. We then test the model on a spoken word processing task, showing that <a href="https://en.wikipedia.org/wiki/Phonology">phonology</a> may not be necessary to explain some of the word processing effects observed in non-native speakers. We run an additional analysis of the model’s lexical representation space, showing that the two training languages are not fully separated in that space, similarly to the languages of a bilingual human speaker.</abstract>
      <url hash="5414afe7">2021.eacl-main.127</url>
      <award>Honorable Mention for Best Long Paper</award>
      <bibkey>matusevych-etal-2021-phonetic</bibkey>
      <doi>10.18653/v1/2021.eacl-main.127</doi>
    </paper>
    <paper id="128">
      <title>Bootstrapping Relation Extractors using Syntactic Search by Examples</title>
      <author><first>Matan</first><last>Eyal</last></author>
      <author><first>Asaf</first><last>Amrami</last></author>
      <author><first>Hillel</first><last>Taub-Tabib</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>1491–1503</pages>
      <abstract>The advent of <a href="https://en.wikipedia.org/wiki/Neural_network">neural-networks</a> in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> brought with it substantial improvements in supervised relation extraction. However, obtaining a sufficient quantity of training data remains a key challenge. In this work we propose a process for bootstrapping training datasets which can be performed quickly by non-NLP-experts. We take advantage of <a href="https://en.wikipedia.org/wiki/Web_search_engine">search engines</a> over syntactic-graphs (Such as Shlain et al. (2020)) which expose a friendly by-example syntax. We use these to obtain positive examples by searching for sentences that are syntactically similar to user input examples. We apply this technique to relations from TACRED and DocRED and show that the resulting <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> are competitive with <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on manually annotated data and on data obtained from distant supervision. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> also outperform <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> trained using NLG data augmentation techniques. Extending the search-based approach with the NLG method further improves the results.</abstract>
      <url hash="5f978a07">2021.eacl-main.128</url>
      <bibkey>eyal-etal-2021-bootstrapping</bibkey>
      <doi>10.18653/v1/2021.eacl-main.128</doi>
      <pwccode url="https://github.com/mataney/BootstrappingRelationExtractors" additional="false">mataney/BootstrappingRelationExtractors</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="129">
      <title>Towards a Decomposable Metric for Explainable Evaluation of Text Generation from AMR<fixed-case>AMR</fixed-case></title>
      <author><first>Juri</first><last>Opitz</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>1504–1518</pages>
      <abstract>Systems that generate natural language text from abstract meaning representations such as AMR are typically evaluated using automatic surface matching metrics that compare the generated texts to reference texts from which the input meaning representations were constructed. We show that besides well-known issues from which such <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> suffer, an additional problem arises when applying these <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> for AMR-to-text evaluation, since an abstract meaning representation allows for numerous surface realizations. In this work we aim to alleviate these issues by proposing _, a decomposable metric that builds on two pillars. The first is the principle of meaning preservation   : it measures to what extent a given <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AMR</a> can be reconstructed from the generated sentence using SOTA AMR parsers and applying (fine-grained) AMR evaluation metrics to measure the distance between the original and the reconstructed <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AMR</a>. The second pillar builds on a principle of (grammatical) form    that measures the linguistic quality of the generated text, which we implement using SOTA language models. In two extensive pilot studies we show that fulfillment of both principles offers benefits for AMR-to-text evaluation, including explainability of scores. Since _ does not necessarily rely on gold AMRs, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> may extend to other text generation tasks.<tex-math>\mathcal{M}\mathcal{F}_\beta</tex-math>, a decomposable metric that builds on two pillars. The first is the <b>principle of meaning preservation <tex-math>\mathcal{M}</tex-math>
        </b>: it measures to what extent a given AMR can be reconstructed from the generated sentence using SOTA AMR parsers and applying (fine-grained) AMR evaluation metrics to measure the distance between the original and the reconstructed AMR. The second pillar builds on a <b>principle of (grammatical) form <tex-math>\mathcal{F}</tex-math>
        </b> that measures the linguistic quality of the generated text, which we implement using SOTA language models. In two extensive pilot studies we show that fulfillment of both principles offers benefits for AMR-to-text evaluation, including explainability of scores. Since <tex-math>\mathcal{M}\mathcal{F}_\beta</tex-math> does not necessarily rely on gold AMRs, it may extend to other text generation tasks.</abstract>
      <url hash="7c470968">2021.eacl-main.129</url>
      <bibkey>opitz-frank-2021-towards</bibkey>
      <doi>10.18653/v1/2021.eacl-main.129</doi>
      <pwccode url="https://github.com/Heidelberg-NLP/MFscore" additional="false">Heidelberg-NLP/MFscore</pwccode>
    </paper>
    <paper id="130">
      <title>The Source-Target Domain Mismatch Problem in <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a></title>
      <author><first>Jiajun</first><last>Shen</last></author>
      <author><first>Peng-Jen</first><last>Chen</last></author>
      <author><first>Matthew</first><last>Le</last></author>
      <author><first>Junxian</first><last>He</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Myle</first><last>Ott</last></author>
      <author><first>Michael</first><last>Auli</last></author>
      <author><first>Marc’Aurelio</first><last>Ranzato</last></author>
      <pages>1519–1533</pages>
      <abstract>While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> and postulate that this causes the domains of the source and target language to greatly mismatch. We first formalize the concept of source-target domain mismatch, propose a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> to quantify it, and provide empirical evidence for its existence. We conclude with an empirical study of how source-target domain mismatch affects training of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a> on low resource languages. While this may severely affect <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a>, the degradation can be alleviated by combining <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a> with self-training and by increasing the amount of target side monolingual data.</abstract>
      <url hash="253c4117">2021.eacl-main.130</url>
      <bibkey>shen-etal-2021-source</bibkey>
      <doi>10.18653/v1/2021.eacl-main.130</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="132">
      <title>Understanding Pre-Editing for Black-Box Neural Machine Translation</title>
      <author><first>Rei</first><last>Miyata</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <pages>1539–1550</pages>
      <abstract>Pre-editing is the process of modifying the source text (ST) so that it can be translated by machine translation (MT) in a better quality. Despite the unpredictability of black-box neural MT (NMT), <a href="https://en.wikipedia.org/wiki/Pre-editing">pre-editing</a> has been deployed in various practical MT use cases. Although many studies have demonstrated the effectiveness of <a href="https://en.wikipedia.org/wiki/Pre-editing">pre-editing methods</a> for particular settings, thus far, a deep understanding of what <a href="https://en.wikipedia.org/wiki/Pre-editing">pre-editing</a> is and how it works for black-box NMT is lacking. To elicit such understanding, we extensively investigated human pre-editing practices. We first implemented a <a href="https://en.wikipedia.org/wiki/Communication_protocol">protocol</a> to incrementally record the minimum edits for each <a href="https://en.wikipedia.org/wiki/Translation">ST</a> and collected 6,652 instances of <a href="https://en.wikipedia.org/wiki/Pre-editing">pre-editing</a> across three translation directions, two MT systems, and four text domains. We then analysed the instances from three perspectives : the characteristics of the pre-edited ST, the diversity of pre-editing operations, and the impact of the pre-editing operations on NMT outputs. Our findings include the following : (1) enhancing the explicitness of the meaning of an ST and its syntactic structure is more important for obtaining better translations than making the ST shorter and simpler, and (2) although the impact of pre-editing on NMT is generally unpredictable, there are some tendencies of changes in the NMT outputs depending on the editing operation types.</abstract>
      <url hash="1be8b452">2021.eacl-main.132</url>
      <bibkey>miyata-fujita-2021-understanding</bibkey>
      <doi>10.18653/v1/2021.eacl-main.132</doi>
    </paper>
    <paper id="140">
      <title>WiC-TSV : An Evaluation Benchmark for Target Sense Verification of Words in Context<fixed-case>WiC-TSV</fixed-case>: <fixed-case>A</fixed-case>n Evaluation Benchmark for Target Sense Verification of Words in Context</title>
      <author><first>Anna</first><last>Breit</last></author>
      <author><first>Artem</first><last>Revenko</last></author>
      <author><first>Kiamehr</first><last>Rezaee</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <pages>1635–1645</pages>
      <abstract>We present WiC-TSV, a new multi-domain evaluation benchmark for <a href="https://en.wikipedia.org/wiki/Word-sense_disambiguation">Word Sense Disambiguation</a>. More specifically, we introduce a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> highly flexible for the evaluation of a diverse set of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. We set baseline performance on the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> using state-of-the-art <a href="https://en.wikipedia.org/wiki/Language_model">language models</a>. Experimental results show that even though these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can perform decently on the task, there remains a gap between machine and human performance, especially in out-of-domain settings. WiC-TSV data is available at https://competitions.codalab.org/competitions/23683.</abstract>
      <url hash="f4b56680">2021.eacl-main.140</url>
      <bibkey>breit-etal-2021-wic</bibkey>
      <doi>10.18653/v1/2021.eacl-main.140</doi>
      <pwccode url="https://github.com/semantic-web-company/wic-tsv" additional="false">semantic-web-company/wic-tsv</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wic-tsv">WiC-TSV</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="141">
      <title>Self-Supervised and Controlled Multi-Document Opinion Summarization</title>
      <author><first>Hady</first><last>Elsahar</last></author>
      <author><first>Maximin</first><last>Coavoux</last></author>
      <author><first>Jos</first><last>Rozen</last></author>
      <author><first>Matthias</first><last>Gallé</last></author>
      <pages>1646–1662</pages>
      <abstract>We address the problem of unsupervised abstractive summarization of collections of user generated reviews through self-supervision and control. We propose a self-supervised setup that considers an individual document as a target summary for a set of similar documents. This setting makes training simpler than previous approaches by relying only on standard <a href="https://en.wikipedia.org/wiki/Likelihood_function">log-likelihood loss</a> and mainstream models. We address the problem of <a href="https://en.wikipedia.org/wiki/Hallucination">hallucinations</a> through the use of <a href="https://en.wikipedia.org/wiki/Control_code">control codes</a>, to steer the generation towards more coherent and relevant summaries.</abstract>
      <url hash="45a8f123">2021.eacl-main.141</url>
      <bibkey>elsahar-etal-2021-self</bibkey>
      <doi>10.18653/v1/2021.eacl-main.141</doi>
    </paper>
    <paper id="145">
      <title>Active Learning for Sequence Tagging with Deep Pre-trained Models and Bayesian Uncertainty Estimates<fixed-case>B</fixed-case>ayesian Uncertainty Estimates</title>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Dmitri</first><last>Puzyrev</last></author>
      <author><first>Lyubov</first><last>Kupriyanova</last></author>
      <author><first>Denis</first><last>Belyakov</last></author>
      <author><first>Daniil</first><last>Larionov</last></author>
      <author><first>Nikita</first><last>Khromov</last></author>
      <author><first>Olga</first><last>Kozlova</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <author><first>Dmitry V.</first><last>Dylov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>1698–1712</pages>
      <abstract>Annotating training data for sequence tagging of texts is usually very time-consuming. Recent advances in <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> in conjunction with active learning open the possibility to significantly reduce the necessary annotation budget. We are the first to thoroughly investigate this powerful combination for the sequence tagging task. We conduct an extensive empirical study of various Bayesian uncertainty estimation methods and Monte Carlo dropout options for deep pre-trained models in the active learning framework and find the best combinations for different types of <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>. Besides, we also demonstrate that to acquire instances during <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a>, a full-size Transformer can be substituted with a distilled version, which yields better computational performance and reduces obstacles for applying deep active learning in practice.</abstract>
      <url hash="f0c04769">2021.eacl-main.145</url>
      <bibkey>shelmanov-etal-2021-active</bibkey>
      <doi>10.18653/v1/2021.eacl-main.145</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="149">
      <title>BERT Prescriptions to Avoid Unwanted Headaches : A Comparison of Transformer Architectures for Adverse Drug Event Detection<fixed-case>BERT</fixed-case> Prescriptions to Avoid Unwanted Headaches: A Comparison of Transformer Architectures for Adverse Drug Event Detection</title>
      <author><first>Beatrice</first><last>Portelli</last></author>
      <author><first>Edoardo</first><last>Lenzi</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Giuseppe</first><last>Serra</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <pages>1740–1747</pages>
      <abstract>Pretrained transformer-based models, such as BERT and its variants, have become a common choice to obtain state-of-the-art performances in NLP tasks. In the identification of Adverse Drug Events (ADE) from social media texts, for example, BERT architectures rank first in the leaderboard. However, a systematic comparison between these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> has not yet been done. In this paper, we aim at shedding light on the differences between their performance analyzing the results of 12 <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, tested on two standard <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a>. SpanBERT and PubMedBERT emerged as the best models in our evaluation : this result clearly shows that span-based pretraining gives a decisive advantage in the precise recognition of ADEs, and that in-domain language pretraining is particularly useful when the transformer model is trained just on biomedical text from scratch.</abstract>
      <url hash="09c7839d">2021.eacl-main.149</url>
      <bibkey>portelli-etal-2021-bert</bibkey>
      <doi>10.18653/v1/2021.eacl-main.149</doi>
      <pwccode url="https://github.com/ailabudinegit/ade" additional="false">ailabudinegit/ade</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="150">
      <title>Semantic Parsing of Disfluent Speech</title>
      <author><first>Priyanka</first><last>Sen</last></author>
      <author><first>Isabel</first><last>Groves</last></author>
      <pages>1748–1753</pages>
      <abstract>Speech disfluencies are prevalent in spontaneous speech. The rising popularity of voice assistants presents a growing need to handle naturally occurring disfluencies. Semantic parsing is a key component for understanding user utterances in voice assistants, yet most <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a> research to date focuses on <a href="https://en.wikipedia.org/wiki/Written_language">written text</a>. In this paper, we investigate semantic parsing of disfluent speech with the ATIS dataset. We find that a state-of-the-art <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a> does not seamlessly handle <a href="https://en.wikipedia.org/wiki/Disfluency">disfluencies</a>. We experiment with adding real and synthetic disfluencies at training time and find that adding synthetic disfluencies not only improves model performance by up to 39 % but can also outperform adding real disfluencies in the ATIS dataset.</abstract>
      <url hash="23dbac40">2021.eacl-main.150</url>
      <bibkey>sen-groves-2021-semantic</bibkey>
      <doi>10.18653/v1/2021.eacl-main.150</doi>
    </paper>
    <paper id="156">
      <title>We Need To Talk About Random Splits</title>
      <author><first>Anders</first><last>Søgaard</last></author>
      <author><first>Sebastian</first><last>Ebert</last></author>
      <author><first>Jasmijn</first><last>Bastings</last></author>
      <author><first>Katja</first><last>Filippova</last></author>
      <pages>1823–1832</pages>
      <abstract>(CITATION) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> to simulate real-world drift ; this is known as the covariate shift assumption. In <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead ; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.</abstract>
      <url hash="a882f04a">2021.eacl-main.156</url>
      <revision id="1" href="2021.eacl-main.156v1" hash="016692b6" />
      <revision id="2" href="2021.eacl-main.156v2" hash="a882f04a" date="2021-05-03">url update</revision>
      <award>Honorable Mention for Best Short Paper</award>
      <bibkey>sogaard-etal-2021-need</bibkey>
      <doi>10.18653/v1/2021.eacl-main.156</doi>
      <pwccode url="https://github.com/google-research/google-research/tree/master/talk_about_random_splits" additional="false">google-research/google-research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="158">
      <title>Alignment verification to improve NMT translation towards highly inflectional languages with limited resources<fixed-case>NMT</fixed-case> translation towards highly inflectional languages with limited resources</title>
      <author><first>George</first><last>Tambouratzis</last></author>
      <pages>1841–1851</pages>
      <abstract>The present article discusses how to improve translation quality when using limited training data to translate towards <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphologically rich languages</a>. The starting point is a neural MT system, used to train translation models, using solely publicly available <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel data</a>. An initial analysis of the translation output has shown that quality is sub-optimal, due mainly to an insufficient amount of training data. To improve translation quality, a hybridized solution is proposed, using an ensemble of relatively simple NMT systems trained with different <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>, combined with an open source module, designed for a low-resource MT system. Experimental results of the proposed hybridized method with multiple independent test sets achieve improvements over (i) both the best individual NMT and (ii) the standard ensemble system provided in the Marian-NMT system. Improvements over Marian-NMT are in many cases statistically significant. Finally, a qualitative analysis of translation results indicates a greater robustness for the hybridized method.</abstract>
      <url hash="661c561e">2021.eacl-main.158</url>
      <bibkey>tambouratzis-2021-alignment</bibkey>
      <doi>10.18653/v1/2021.eacl-main.158</doi>
    </paper>
    <paper id="159">
      <title>Data Augmentation for Voice-Assistant NLU using BERT-based Interchangeable Rephrase<fixed-case>NLU</fixed-case> using <fixed-case>BERT</fixed-case>-based Interchangeable Rephrase</title>
      <author><first>Akhila</first><last>Yerukola</last></author>
      <author><first>Mason</first><last>Bretan</last></author>
      <author><first>Hongxia</first><last>Jin</last></author>
      <pages>1852–1860</pages>
      <abstract>We introduce a data augmentation technique based on byte pair encoding and a BERT-like self-attention model to boost performance on spoken language understanding tasks. We compare and evaluate this method with a range of augmentation techniques encompassing generative models such as VAEs and performance-boosting techniques such as synonym replacement and <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a>. We show our method performs strongly on domain and intent classification tasks for a voice assistant and in a user-study focused on utterance naturalness and <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a>.</abstract>
      <url hash="b34d200e">2021.eacl-main.159</url>
      <bibkey>yerukola-etal-2021-data</bibkey>
      <doi>10.18653/v1/2021.eacl-main.159</doi>
    </paper>
    <paper id="160">
      <title>How to Evaluate a Summarizer : Study Design and Statistical Analysis for Manual Linguistic Quality Evaluation</title>
      <author><first>Julius</first><last>Steen</last></author>
      <author><first>Katja</first><last>Markert</last></author>
      <pages>1861–1875</pages>
      <abstract>Manual evaluation is essential to judge progress on <a href="https://en.wikipedia.org/wiki/Automatic_text_summarization">automatic text summarization</a>. However, we conduct a survey on recent summarization system papers that reveals little agreement on how to perform such evaluation studies. We conduct two evaluation experiments on two aspects of summaries’ linguistic quality (coherence and repetitiveness) to compare Likert-type and ranking annotations and show that best choice of evaluation method can vary from one aspect to another. In our survey, we also find that study parameters such as the overall number of annotators and distribution of annotators to annotation items are often not fully reported and that subsequent statistical analysis ignores grouping factors arising from one annotator judging multiple summaries. Using our evaluation experiments, we show that the total number of annotators can have a strong impact on study power and that current <a href="https://en.wikipedia.org/wiki/Statistical_inference">statistical analysis methods</a> can inflate <a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors">type I error rates</a> up to eight-fold. In addition, we highlight that for the purpose of system comparison the current practice of eliciting multiple judgements per summary leads to less powerful and reliable annotations given a fixed study budget.</abstract>
      <url hash="2f8763e4">2021.eacl-main.160</url>
      <attachment type="Dataset" hash="ab75bdfc">2021.eacl-main.160.Dataset.zip</attachment>
      <bibkey>steen-markert-2021-evaluate</bibkey>
      <doi>10.18653/v1/2021.eacl-main.160</doi>
      <pwccode url="https://github.com/julmaxi/summary_lq_analysis" additional="false">julmaxi/summary_lq_analysis</pwccode>
    </paper>
    <paper id="162">
      <title>Error Analysis and the Role of <a href="https://en.wikipedia.org/wiki/Morphology_(biology)">Morphology</a></title>
      <author><first>Marcel</first><last>Bollmann</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>1887–1900</pages>
      <abstract>We evaluate two common conjectures in <a href="https://en.wikipedia.org/wiki/Error_analysis_(linguistics)">error analysis</a> of NLP models : (i) <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">Morphology</a> is predictive of errors ; and (ii) the importance of <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphology</a> increases with the morphological complexity of a language. We show across four different <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> and up to 57 languages that of these conjectures, somewhat surprisingly, only (i) is true. Using morphological features does improve <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">error prediction</a> across tasks ; however, this effect is less pronounced with morphologically complex languages. We speculate this is because <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphology</a> is more discriminative in morphologically simple languages. Across all four tasks, <a href="https://en.wikipedia.org/wiki/Grammatical_case">case</a> and gender are the <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological features</a> most predictive of error.</abstract>
      <url hash="059162c0">2021.eacl-main.162</url>
      <award>Best Long Paper</award>
      <bibkey>bollmann-sogaard-2021-error</bibkey>
      <doi>10.18653/v1/2021.eacl-main.162</doi>
      <pwccode url="https://github.com/coastalcph/eacl2021-morpherror" additional="false">coastalcph/eacl2021-morpherror</pwccode>
    </paper>
    <paper id="170">
      <title>Attention-based Relational Graph Convolutional Network for Target-Oriented Opinion Words Extraction</title>
      <author><first>Junfeng</first><last>Jiang</last></author>
      <author><first>An</first><last>Wang</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>1986–1997</pages>
      <abstract>Target-oriented opinion words extraction (TOWE) is a subtask of aspect-based sentiment analysis (ABSA). It aims to extract the corresponding opinion words for a given opinion target in a review sentence. Intuitively, the relation between an opinion target and an opinion word mostly relies on <a href="https://en.wikipedia.org/wiki/Syntax">syntactics</a>. In this study, we design a directed syntactic dependency graph based on a dependency tree to establish a path from the target to candidate opinions. Subsequently, we propose a novel attention-based relational graph convolutional neural network (ARGCN) to exploit syntactic information over dependency graphs. Moreover, to explicitly extract the corresponding opinion words toward the given opinion target, we effectively encode target information in our model with the target-aware representation. Empirical results demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> significantly outperforms all of the existing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on four benchmark datasets. Extensive analysis also demonstrates the effectiveness of each component of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Our code is available at https://github.com/wcwowwwww/towe-eacl.</abstract>
      <url hash="e4515972">2021.eacl-main.170</url>
      <bibkey>jiang-etal-2021-attention</bibkey>
      <doi>10.18653/v1/2021.eacl-main.170</doi>
      <pwccode url="https://github.com/wcwowwwww/towe-eacl" additional="false">wcwowwwww/towe-eacl</pwccode>
    </paper>
    <paper id="174">
      <title>Acquiring a Formality-Informed Lexical Resource for Style Analysis</title>
      <author><first>Elisabeth</first><last>Eder</last></author>
      <author><first>Ulrike</first><last>Krieg-Holz</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <pages>2028–2041</pages>
      <abstract>To track different levels of formality in written discourse, we introduce a novel type of lexicon for the <a href="https://en.wikipedia.org/wiki/German_language">German language</a>, with entries ordered by their degree of (in)formality. We start with a set of words extracted from traditional lexicographic resources, extend it by sentence-based similarity computations, and let crowdworkers assess the enlarged set of lexical items on a continuous informal-formal scale as a gold standard for evaluation. We submit this <a href="https://en.wikipedia.org/wiki/Lexicon">lexicon</a> to an intrinsic evaluation related to the best regression models and their effect on predicting formality scores and complement our investigation by an extrinsic evaluation of formality on a German-language email corpus.</abstract>
      <url hash="1deb8635">2021.eacl-main.174</url>
      <bibkey>eder-etal-2021-acquiring</bibkey>
      <doi>10.18653/v1/2021.eacl-main.174</doi>
      <pwccode url="https://github.com/ee-2/i-forger" additional="false">ee-2/i-forger</pwccode>
    </paper>
    <paper id="175">
      <title>Probing into the Root : A Dataset for Reason Extraction of Structural Events from Financial Documents</title>
      <author><first>Pei</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Taifeng</first><last>Wang</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>2042–2048</pages>
      <abstract>This paper proposes a new <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> regarding event reason extraction from document-level texts. Unlike the previous causality detection task, we do not assign target events in the text, but only provide structural event descriptions, and such settings accord more with practice scenarios. Moreover, we annotate a large dataset FinReason for evaluation, which provides Reasons annotation for Financial events in company announcements. This task is challenging because the cases of multiple-events, multiple-reasons, and implicit-reasons are included. In total, FinReason contains 8,794 documents, 12,861 <a href="https://en.wikipedia.org/wiki/Financial_crisis">financial events</a> and 11,006 reason spans. We also provide the performance of existing canonical methods in <a href="https://en.wikipedia.org/wiki/Event_(computing)">event extraction</a> and machine reading comprehension on this task. The results show a 7 percentage point F1 score gap between the best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and human performance, and existing methods are far from resolving this problem.</abstract>
      <url hash="3749c52d">2021.eacl-main.175</url>
      <attachment type="Dataset" hash="728b6e4d">2021.eacl-main.175.Dataset.zip</attachment>
      <attachment type="Software" hash="9b64e17b">2021.eacl-main.175.Software.zip</attachment>
      <bibkey>chen-etal-2021-probing</bibkey>
      <doi>10.18653/v1/2021.eacl-main.175</doi>
    </paper>
    <paper id="176">
      <title>Language Modelling as a Multi-Task Problem</title>
      <author><first>Lucas</first><last>Weber</last></author>
      <author><first>Jaap</first><last>Jumelet</last></author>
      <author><first>Elia</first><last>Bruni</last></author>
      <author><first>Dieuwke</first><last>Hupkes</last></author>
      <pages>2049–2060</pages>
      <abstract>In this paper, we propose to study <a href="https://en.wikipedia.org/wiki/Language_model">language modelling</a> as a multi-task problem, bringing together three strands of research : <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>, <a href="https://en.wikipedia.org/wiki/Linguistics">linguistics</a>, and <a href="https://en.wikipedia.org/wiki/Interpretability">interpretability</a>. Based on hypotheses derived from <a href="https://en.wikipedia.org/wiki/Linguistics">linguistic theory</a>, we investigate whether <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> adhere to learning principles of <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> during training. To showcase the idea, we analyse the generalisation behaviour of <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> as they learn the linguistic concept of Negative Polarity Items (NPIs). Our experiments demonstrate that a multi-task setting naturally emerges within the objective of the more general task of <a href="https://en.wikipedia.org/wiki/Language_model">language modelling</a>. We argue that this insight is valuable for <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>, linguistics and interpretability research and can lead to exciting new findings in all three domains.</abstract>
      <url hash="0744dd96">2021.eacl-main.176</url>
      <bibkey>weber-etal-2021-language</bibkey>
      <doi>10.18653/v1/2021.eacl-main.176</doi>
    </paper>
    <paper id="177">
      <title>ChainCQG : Flow-Aware Conversational Question Generation<fixed-case>C</fixed-case>hain<fixed-case>CQG</fixed-case>: Flow-Aware Conversational Question Generation</title>
      <author><first>Jing</first><last>Gu</last></author>
      <author><first>Mostafa</first><last>Mirshekari</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <author><first>Aaron</first><last>Sisto</last></author>
      <pages>2061–2070</pages>
      <abstract>Conversational systems enable numerous valuable <a href="https://en.wikipedia.org/wiki/Application_software">applications</a>, and <a href="https://en.wikipedia.org/wiki/Question_answering">question-answering</a> is an important component underlying many of these. However, conversational question-answering remains challenging due to the lack of realistic, domain-specific training data. Inspired by this bottleneck, we focus on conversational question generation as a means to generate synthetic conversations for training and evaluation purposes. We present a number of novel <a href="https://en.wikipedia.org/wiki/Strategy">strategies</a> to improve conversational flow and accommodate varying question types and overall fluidity. Specifically, we design ChainCQG as a two-stage architecture that learns question-answer representations across multiple dialogue turns using a flow propagation training strategy. ChainCQG significantly outperforms both answer-aware and answer-unaware SOTA baselines (e.g., up to 48 % BLEU-1 improvement). Additionally, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is able to generate different types of questions, with improved <a href="https://en.wikipedia.org/wiki/Fluid_dynamics">fluidity</a> and coreference alignment.</abstract>
      <url hash="fdb2d2a9">2021.eacl-main.177</url>
      <bibkey>gu-etal-2021-chaincqg</bibkey>
      <doi>10.18653/v1/2021.eacl-main.177</doi>
      <pwccode url="https://github.com/searchableai/ChainCQG" additional="false">searchableai/ChainCQG</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
    </paper>
    <paper id="178">
      <title>The Interplay of Task Success and Dialogue Quality : An in-depth Evaluation in Task-Oriented Visual Dialogues</title>
      <author><first>Alberto</first><last>Testoni</last></author>
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <pages>2071–2082</pages>
      <abstract>When training a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on referential dialogue guessing games, the best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is usually chosen based on its task success. We show that in the popular <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end approach</a>, this choice prevents the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> from learning to generate linguistically richer dialogues, since the acquisition of language proficiency takes longer than learning the guessing task. By comparing models playing different games (GuessWhat, GuessWhich, and Mutual Friends), we show that this discrepancy is model- and task-agnostic. We investigate whether and when better language quality could lead to higher task success. We show that in GuessWhat, models could increase their <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> if they learn to ground, encode, and decode also words that do not occur frequently in the training set.</abstract>
      <url hash="c70a6106">2021.eacl-main.178</url>
      <bibkey>testoni-bernardi-2021-interplay</bibkey>
      <doi>10.18653/v1/2021.eacl-main.178</doi>
      <pwccode url="https://github.com/stanfordnlp/cocoa" additional="false">stanfordnlp/cocoa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/guesswhat">GuessWhat?!</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mutualfriends">MutualFriends</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visdial">VisDial</pwcdataset>
    </paper>
    <paper id="179">
      <title>Are you kidding me? : Detecting Unpalatable Questions on Reddit<fixed-case>R</fixed-case>eddit</title>
      <author><first>Sunyam</first><last>Bagga</last></author>
      <author><first>Andrew</first><last>Piper</last></author>
      <author><first>Derek</first><last>Ruths</last></author>
      <pages>2083–2099</pages>
      <abstract>Abusive language in <a href="https://en.wikipedia.org/wiki/Online_discourse">online discourse</a> negatively affects a large number of <a href="https://en.wikipedia.org/wiki/Social_media">social media users</a>. Many computational methods have been proposed to address this issue of <a href="https://en.wikipedia.org/wiki/Online_abuse">online abuse</a>. The existing work, however, tends to focus on detecting the more explicit forms of abuse leaving the subtler forms of abuse largely untouched. Our work addresses this gap by making three core contributions. First, inspired by the theory of impoliteness, we propose a novel task of detecting a subtler form of abuse, namely unpalatable questions. Second, we publish a context-aware dataset for the task using data from a diverse set of Reddit communities. Third, we implement a wide array of <a href="https://en.wikipedia.org/wiki/Machine_learning">learning models</a> and also investigate the benefits of incorporating <a href="https://en.wikipedia.org/wiki/Context_(language_use)">conversational context</a> into <a href="https://en.wikipedia.org/wiki/Computational_model">computational models</a>. Our results show that modeling subtle abuse is feasible but difficult due to the language involved being highly nuanced and context-sensitive. We hope that future research in the field will address such subtle forms of abuse since their harm currently passes unnoticed through existing detection systems.</abstract>
      <url hash="69b46e9a">2021.eacl-main.179</url>
      <attachment type="Dataset" hash="b81c0ea9">2021.eacl-main.179.Dataset.zip</attachment>
      <bibkey>bagga-etal-2021-kidding</bibkey>
      <doi>10.18653/v1/2021.eacl-main.179</doi>
    </paper>
    <paper id="180">
      <title>Neural-Driven Search-Based Paraphrase Generation</title>
      <author><first>Betty</first><last>Fabre</last></author>
      <author><first>Tanguy</first><last>Urvoy</last></author>
      <author><first>Jonathan</first><last>Chevelu</last></author>
      <author><first>Damien</first><last>Lolive</last></author>
      <pages>2100–2111</pages>
      <abstract>We study a search-based paraphrase generation scheme where candidate paraphrases are generated by iterated transformations from the original sentence and evaluated in terms of syntax quality, <a href="https://en.wikipedia.org/wiki/Semantic_distance">semantic distance</a>, and lexical distance. The <a href="https://en.wikipedia.org/wiki/Semantic_distance">semantic distance</a> is derived from BERT, and the lexical quality is based on GPT2 perplexity. To solve this multi-objective search problem, we propose two algorithms : Monte-Carlo Tree Search For Paraphrase Generation (MCPG) and Pareto Tree Search (PTS). We provide an extensive set of experiments on 5 <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> with a rigorous reproduction and validation for several state-of-the-art paraphrase generation algorithms. These experiments show that, although being non explicitly supervised, our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> perform well against these <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="c2f47ad3">2021.eacl-main.180</url>
      <bibkey>fabre-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.eacl-main.180</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/opusparcus">Opusparcus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
    </paper>
    <paper id="185">
      <title>FAST : Financial News and Tweet Based Time Aware Network for Stock Trading<fixed-case>FAST</fixed-case>: Financial News and Tweet Based Time Aware Network for Stock Trading</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Arnav</first><last>Wadhwa</last></author>
      <author><first>Shivam</first><last>Agarwal</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <pages>2164–2175</pages>
      <abstract>Designing profitable trading strategies is complex as <a href="https://en.wikipedia.org/wiki/Volatility_(finance)">stock movements</a> are highly stochastic ; the market is influenced by large volumes of <a href="https://en.wikipedia.org/wiki/Noisy_data">noisy data</a> across diverse information sources like news and social media. Prior work mostly treats stock movement prediction as a regression or classification task and is not directly optimized towards <a href="https://en.wikipedia.org/wiki/Profit_(economics)">profit-making</a>. Further, they do not model the fine-grain temporal irregularities in the release of vast volumes of text that the market responds to quickly. Building on these limitations, we propose a novel hierarchical, learning to rank approach that uses textual data to make time-aware predictions for ranking stocks based on expected profit. Our approach outperforms state-of-the-art methods by over 8 % in terms of cumulative profit and risk-adjusted returns in trading simulations on two benchmarks : English tweets and Chinese financial news spanning two major stock indexes and four global markets. Through ablative and qualitative analyses, we build the case for our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> as a tool for daily stock trading.</abstract>
      <url hash="ca9a10da">2021.eacl-main.185</url>
      <bibkey>sawhney-etal-2021-fast</bibkey>
      <doi>10.18653/v1/2021.eacl-main.185</doi>
    </paper>
    <paper id="186">
      <title>Building Representative Corpora from Illiterate Communities : A Reviewof Challenges and Mitigation Strategies for Developing Countries</title>
      <author><first>Stephanie</first><last>Hirmer</last></author>
      <author><first>Alycia</first><last>Leonard</last></author>
      <author><first>Josephine</first><last>Tumwesige</last></author>
      <author><first>Costanza</first><last>Conforti</last></author>
      <pages>2176–2189</pages>
      <abstract>Most well-established data collection methods currently adopted in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> depend on the as- sumption of speaker literacy. Consequently, the collected corpora largely fail to represent swathes of the global population, which tend to be some of the most vulnerable and marginalised people in society, and often live in rural developing areas. Such underrepresented groups are thus not only ignored when making modeling and system design decisions, but also prevented from benefiting from development outcomes achieved through data-driven NLP. This paper aims to address the under-representation of illiterate communities in NLP corpora : we identify potential biases and ethical issues that might arise when collecting data from rural communities with high illiteracy rates in Low-Income Countries, and propose a set of practical mitigation strategies to help future work.</abstract>
      <url hash="ab9dfa6f">2021.eacl-main.186</url>
      <bibkey>hirmer-etal-2021-building</bibkey>
      <doi>10.18653/v1/2021.eacl-main.186</doi>
    </paper>
    <paper id="195">
      <title>Content-based Models of Quotation</title>
      <author><first>Ansel</first><last>MacLaughlin</last></author>
      <author><first>David</first><last>Smith</last></author>
      <pages>2296–2314</pages>
      <abstract>We explore the task of quotability identification, in which, given a document, we aim to identify which of its passages are the most quotable, i.e. the most likely to be directly quoted by later derived documents. We approach quotability identification as a passage ranking problem and evaluate how well both feature-based and BERT-based (Devlin et al., 2019) models rank the passages in a given document by their predicted quotability. We explore this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> through evaluations on five <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> that span multiple languages (English, Latin) and genres of literature (e.g. poetry, plays, novels) and whose corresponding derived documents are of multiple types (news, journal articles). Our experiments confirm the relatively strong performance of BERT-based models on this task, with the best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, a RoBERTA sequential sentence tagger, achieving an average rho of 0.35 and NDCG@1, 5, 50 of 0.26, 0.31 and 0.40, respectively, across all five datasets.</abstract>
      <url hash="38289e3b">2021.eacl-main.195</url>
      <bibkey>maclaughlin-smith-2021-content</bibkey>
      <doi>10.18653/v1/2021.eacl-main.195</doi>
    </paper>
    <paper id="200">
      <title>Lexical Normalization for Code-switched Data and its Effect on POS Tagging<fixed-case>POS</fixed-case> Tagging</title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Özlem</first><last>Çetinoğlu</last></author>
      <pages>2352–2365</pages>
      <abstract>Lexical normalization, the translation of non-canonical data to standard language, has shown to improve the performance of many natural language processing tasks on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. Yet, using multiple languages in one utterance, also called code-switching (CS), is frequently overlooked by these normalization systems, despite its common use in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. In this paper, we propose three <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalization models</a> specifically designed to handle code-switched data which we evaluate for two language pairs : Indonesian-English and Turkish-German. For the latter, we introduce novel <a href="https://en.wikipedia.org/wiki/Data_normalization">normalization layers</a> and their corresponding language ID and POS tags for the dataset, and evaluate the downstream effect of <a href="https://en.wikipedia.org/wiki/Data_normalization">normalization</a> on POS tagging. Results show that our CS-tailored normalization models significantly outperform monolingual ones, and lead to 5.4 % relative performance increase for POS tagging as compared to unnormalized input.</abstract>
      <url hash="a098cb32">2021.eacl-main.200</url>
      <bibkey>van-der-goot-cetinoglu-2021-lexical</bibkey>
      <doi>10.18653/v1/2021.eacl-main.200</doi>
      <pwccode url="https://github.com/ozlemcek/TrDeNormData" additional="false">ozlemcek/TrDeNormData</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="201">
      <title>Structural Encoding and Pre-training Matter : Adapting BERT for Table-Based Fact Verification<fixed-case>BERT</fixed-case> for Table-Based Fact Verification</title>
      <author><first>Rui</first><last>Dong</last></author>
      <author><first>David</first><last>Smith</last></author>
      <pages>2366–2375</pages>
      <abstract>Growing concern with <a href="https://en.wikipedia.org/wiki/Misinformation">online misinformation</a> has encouraged NLP research on fact verification. Since writers often base their assertions on structured data, we focus here on verifying textual statements given evidence in tables. Starting from the Table Parsing (TAPAS) model developed for <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> (Herzig et al., 2020), we find that modeling table structure improves a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> pre-trained on <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured text</a>. Pre-training language models on English Wikipedia table data further improves performance. Pre-training on a <a href="https://en.wikipedia.org/wiki/Question_answering">question answering task</a> with column-level cell rank information achieves the best performance. With improved pre-training and cell embeddings, this approach outperforms the state-of-the-art Numerically-aware Graph Neural Network table fact verification model (GNN-TabFact), increasing <a href="https://en.wikipedia.org/wiki/Statistical_classification">statement classification accuracy</a> from 72.2 % to 73.9 % even without modeling numerical information. Incorporating numerical information with cell rankings and pre-training on a <a href="https://en.wikipedia.org/wiki/Question_answering">question-answering task</a> increases <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> to 76 %. We further analyze accuracy on statements implicating single rows or multiple rows and columns of tables, on different numerical reasoning subtasks, and on generalizing to detecting errors in statements derived from the ToTTo table-to-text generation dataset.</abstract>
      <url hash="33b84061">2021.eacl-main.201</url>
      <bibkey>dong-smith-2021-structural</bibkey>
      <doi>10.18653/v1/2021.eacl-main.201</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/totto">ToTTo</pwcdataset>
    </paper>
    <paper id="204">
      <title>Cross-Cultural Similarity Features for Cross-Lingual Transfer Learning of Pragmatically Motivated Tasks</title>
      <author><first>Jimin</first><last>Sun</last></author>
      <author><first>Hwijeen</first><last>Ahn</last></author>
      <author><first>Chan Young</first><last>Park</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <pages>2403–2414</pages>
      <abstract>Much work in cross-lingual transfer learning explored how to select better transfer languages for multilingual tasks, primarily focusing on typological and genealogical similarities between languages. We hypothesize that these measures of linguistic proximity are not enough when working with pragmatically-motivated tasks, such as <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. As an alternative, we introduce three linguistic features that capture cross-cultural similarities that manifest in linguistic patterns and quantify distinct aspects of <a href="https://en.wikipedia.org/wiki/Pragmatics">language pragmatics</a> : <a href="https://en.wikipedia.org/wiki/Context_(language_use)">language context-level</a>, <a href="https://en.wikipedia.org/wiki/Literal_and_figurative_language">figurative language</a>, and the lexification of emotion concepts. Our analyses show that the proposed pragmatic features do capture cross-cultural similarities and align well with existing work in <a href="https://en.wikipedia.org/wiki/Sociolinguistics">sociolinguistics</a> and <a href="https://en.wikipedia.org/wiki/Linguistic_anthropology">linguistic anthropology</a>. We further corroborate the effectiveness of pragmatically-driven transfer in the downstream task of choosing transfer languages for cross-lingual sentiment analysis.</abstract>
      <url hash="307caf69">2021.eacl-main.204</url>
      <bibkey>sun-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.eacl-main.204</doi>
      <pwccode url="https://github.com/hwijeen/langrank" additional="false">hwijeen/langrank</pwccode>
    </paper>
    <paper id="205">
      <title>PHASE : Learning Emotional Phase-aware Representations for Suicide Ideation Detection on <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a><fixed-case>PHASE</fixed-case>: Learning Emotional Phase-aware Representations for Suicide Ideation Detection on Social Media</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Harshit</first><last>Joshi</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <pages>2415–2428</pages>
      <abstract>Recent psychological studies indicate that individuals exhibiting <a href="https://en.wikipedia.org/wiki/Suicidal_ideation">suicidal ideation</a> increasingly turn to <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> rather than <a href="https://en.wikipedia.org/wiki/Mental_health_professional">mental health practitioners</a>. Contextualizing the build-up of such ideation is critical for the identification of users at risk. In this work, we focus on identifying <a href="https://en.wikipedia.org/wiki/Suicide">suicidal intent</a> in <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a> by augmenting linguistic models with <a href="https://en.wikipedia.org/wiki/Emotion">emotional phases</a> modeled from users’ historical context. We propose PHASE, a time-and phase-aware framework that adaptively learns features from a user’s historical emotional spectrum on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> for preliminary screening of suicidal risk. Building on clinical studies, PHASE learns phase-like progressions in users’ historical Plutchik-wheel-based emotions to contextualize suicidal intent. While outperforming state-of-the-art methods, we show the utility of temporal and phase-based emotional contextual cues for suicide ideation detection. We further discuss practical and ethical considerations.</abstract>
      <url hash="24bdfee3">2021.eacl-main.205</url>
      <bibkey>sawhney-etal-2021-phase</bibkey>
      <doi>10.18653/v1/2021.eacl-main.205</doi>
      <pwccode url="https://github.com/midas-research/phase-eacl" additional="false">midas-research/phase-eacl</pwccode>
    </paper>
    <paper id="206">
      <title>Exploiting Definitions for Frame Identification</title>
      <author><first>Tianyu</first><last>Jiang</last></author>
      <author><first>Ellen</first><last>Riloff</last></author>
      <pages>2429–2434</pages>
      <abstract>Frame identification is one of the key challenges for frame-semantic parsing. The goal of this task is to determine which <a href="https://en.wikipedia.org/wiki/Film_frame">frame</a> best captures the meaning of a target word or phrase in a sentence. We present a new model for frame identification that uses a pre-trained transformer model to generate representations for frames and lexical units (senses) using their formal definitions in <a href="https://en.wikipedia.org/wiki/FrameNet">FrameNet</a>. Our <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">frame identification model</a> assesses the suitability of a <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">frame</a> for a target word in a sentence based on the semantic coherence of their meanings. We evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on three data sets and show that <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> consistently achieves better performance than previous <a href="https://en.wikipedia.org/wiki/System">systems</a>.</abstract>
      <url hash="ce326859">2021.eacl-main.206</url>
      <bibkey>jiang-riloff-2021-exploiting</bibkey>
      <doi>10.18653/v1/2021.eacl-main.206</doi>
      <pwccode url="https://github.com/tyjiangu/fido" additional="false">tyjiangu/fido</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="207">
      <title>ADePT : Auto-encoder based Differentially Private Text Transformation<fixed-case>AD</fixed-case>e<fixed-case>PT</fixed-case>: Auto-encoder based Differentially Private Text Transformation</title>
      <author><first>Satyapriya</first><last>Krishna</last></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Christophe</first><last>Dupuy</last></author>
      <pages>2435–2439</pages>
      <abstract>Privacy is an important concern when building <a href="https://en.wikipedia.org/wiki/Statistical_model">statistical models</a> on data containing personal information. Differential privacy offers a strong definition of <a href="https://en.wikipedia.org/wiki/Privacy">privacy</a> and can be used to solve several <a href="https://en.wikipedia.org/wiki/Privacy">privacy concerns</a>. Multiple solutions have been proposed for the differentially-private transformation of datasets containing <a href="https://en.wikipedia.org/wiki/Information_sensitivity">sensitive information</a>. However, such transformation algorithms offer poor utility in Natural Language Processing (NLP) tasks due to <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a> added in the process. This paper addresses this issue by providing a utility-preserving differentially private text transformation algorithm using auto-encoders. Our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> transforms text to offer robustness against attacks and produces transformations with high semantic quality that perform well on downstream NLP tasks. We prove our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a>’s theoretical privacy guarantee and assess its privacy leakage under Membership Inference Attacks (MIA) on models trained with transformed data. Our results show that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performs better against MIA attacks while offering lower to no degradation in the utility of the underlying transformation process compared to existing <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="55dc1cc9">2021.eacl-main.207</url>
      <bibkey>krishna-etal-2021-adept</bibkey>
      <doi>10.18653/v1/2021.eacl-main.207</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="210">
      <title>Evaluating Neural Model Robustness for Machine Comprehension</title>
      <author><first>Winston</first><last>Wu</last></author>
      <author><first>Dustin</first><last>Arendt</last></author>
      <author><first>Svitlana</first><last>Volkova</last></author>
      <pages>2470–2481</pages>
      <abstract>We evaluate neural model robustness to adversarial attacks using different types of linguistic unit perturbations   character and word, and propose a new method for strategic sentence-level perturbations. We experiment with different amounts of perturbations to examine model confidence and misclassification rate, and contrast model performance with different embeddings BERT and ELMo on two benchmark datasets SQuAD and TriviaQA. We demonstrate how to improve <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance during an <a href="https://en.wikipedia.org/wiki/Adversarial_system">adversarial attack</a> by using <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensembles</a>. Finally, we analyze factors that effect model behavior under adversarial attack, and develop a new <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to predict errors during attacks. Our novel findings reveal that (a) unlike BERT, models that use ELMo embeddings are more susceptible to adversarial attacks, (b) unlike word and paraphrase, character perturbations affect the model the most but are most easily compensated for by adversarial training, (c) word perturbations lead to more high-confidence misclassifications compared to sentence- and character-level perturbations, (d) the type of question and model answer length (the longer the answer the more likely it is to be incorrect) is the most predictive of model errors in adversarial setting, and (e) conclusions about model behavior are dataset-specific.</abstract>
      <url hash="c79da292">2021.eacl-main.210</url>
      <bibkey>wu-etal-2021-evaluating</bibkey>
      <doi>10.18653/v1/2021.eacl-main.210</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="211">
      <title>Hidden Biases in Unreliable News Detection Datasets</title>
      <author><first>Xiang</first><last>Zhou</last></author>
      <author><first>Heba</first><last>Elfardy</last></author>
      <author><first>Christos</first><last>Christodoulopoulos</last></author>
      <author><first>Thomas</first><last>Butler</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>2482–2492</pages>
      <abstract>Automatic unreliable news detection is a research problem with great potential impact. Recently, several papers have shown promising results on large-scale news datasets with models that only use the article itself without resorting to any <a href="https://en.wikipedia.org/wiki/Fact-checking">fact-checking mechanism</a> or retrieving any supporting evidence. In this work, we take a closer look at these <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>. While they all provide valuable resources for future research, we observe a number of problems that may lead to results that do not generalize in more realistic settings. Specifically, we show that <a href="https://en.wikipedia.org/wiki/Selection_bias">selection bias</a> during data collection leads to undesired artifacts in the <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>. In addition, while most systems train and predict at the level of individual articles, overlapping article sources in the training and evaluation data can provide a strong <a href="https://en.wikipedia.org/wiki/Confounding">confounding factor</a> that <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> can exploit. In the presence of this <a href="https://en.wikipedia.org/wiki/Confounding">confounding factor</a>, the models can achieve good performance by directly memorizing the site-label mapping instead of modeling the real task of unreliable news detection. We observed a significant drop (10 %) in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> for all <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> tested in a clean split with no train / test source overlap. Using the observations and experimental results, we provide practical suggestions on how to create more reliable <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> for the unreliable news detection task. We suggest future dataset creation include a simple <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> as a difficulty / bias probe and future model development use a clean non-overlapping site and date split.</abstract>
      <url hash="d92dde93">2021.eacl-main.211</url>
      <award>Honorable Mention for Best Long Paper</award>
      <bibkey>zhou-etal-2021-hidden</bibkey>
      <doi>10.18653/v1/2021.eacl-main.211</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/nela-gt-2018">NELA-GT-2018</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nela-gt-2019">NELA-GT-2019</pwcdataset>
    </paper>
    <paper id="213">
      <title>Unsupervised Extractive Summarization using Pointwise Mutual Information</title>
      <author><first>Vishakh</first><last>Padmakumar</last></author>
      <author><first>He</first><last>He</last></author>
      <pages>2505–2512</pages>
      <abstract>Unsupervised approaches to extractive summarization usually rely on a notion of sentence importance defined by the <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a> between a sentence and the document. We propose new metrics of relevance and redundancy using pointwise mutual information (PMI) between sentences, which can be easily computed by a pre-trained language model. Intuitively, a relevant sentence allows readers to infer the document content (high PMI with the document), and a redundant sentence can be inferred from the summary (high PMI with the summary). We then develop a greedy sentence selection algorithm to maximize relevance and minimize redundancy of extracted sentences. We show that our method outperforms similarity-based methods on <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> in a range of domains including <a href="https://en.wikipedia.org/wiki/News">news</a>, <a href="https://en.wikipedia.org/wiki/Medical_journal">medical journal articles</a>, and <a href="https://en.wikipedia.org/wiki/Anecdote">personal anecdotes</a>.</abstract>
      <url hash="73788212">2021.eacl-main.213</url>
      <bibkey>padmakumar-he-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.eacl-main.213</doi>
      <pwccode url="https://github.com/vishakhpk/mi-unsup-summ" additional="true">vishakhpk/mi-unsup-summ</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-tifu">Reddit TIFU</pwcdataset>
    </paper>
    <paper id="215">
      <title>Deep Subjecthood : Higher-Order Grammatical Features in Multilingual BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Isabel</first><last>Papadimitriou</last></author>
      <author><first>Ethan A.</first><last>Chi</last></author>
      <author><first>Richard</first><last>Futrell</last></author>
      <author><first>Kyle</first><last>Mahowald</last></author>
      <pages>2522–2532</pages>
      <abstract>We investigate how Multilingual BERT (mBERT) encodes <a href="https://en.wikipedia.org/wiki/Grammar">grammar</a> by examining how the high-order grammatical feature of morphosyntactic alignment (how different languages define what counts as a subject) is manifested across the embedding spaces of different languages. To understand if and how morphosyntactic alignment affects contextual embedding spaces, we train classifiers to recover the subjecthood of mBERT embeddings in transitive sentences (which do not contain overt information about morphosyntactic alignment) and then evaluate them zero-shot on intransitive sentences (where subjecthood classification depends on alignment), within and across languages. We find that the resulting classifier distributions reflect the <a href="https://en.wikipedia.org/wiki/Morphosyntactic_alignment">morphosyntactic alignment</a> of their training languages. Our results demonstrate that mBERT representations are influenced by high-level grammatical features that are not manifested in any one input sentence, and that this is robust across languages. Further examining the characteristics that our classifiers rely on, we find that features such as <a href="https://en.wikipedia.org/wiki/Passive_voice">passive voice</a>, <a href="https://en.wikipedia.org/wiki/Animacy">animacy</a> and <a href="https://en.wikipedia.org/wiki/Grammatical_case">case</a> strongly correlate with classification decisions, suggesting that mBERT does not encode <a href="https://en.wikipedia.org/wiki/Subject_(grammar)">subjecthood</a> purely syntactically, but that <a href="https://en.wikipedia.org/wiki/Subject_(grammar)">subjecthood embedding</a> is continuous and dependent on semantic and discourse factors, as is proposed in much of the functional linguistics literature. Together, these results provide insight into how grammatical features manifest in contextual embedding spaces, at a level of abstraction not covered by previous work.</abstract>
      <url hash="d5f48e1e">2021.eacl-main.215</url>
      <bibkey>papadimitriou-etal-2021-deep</bibkey>
      <doi>10.18653/v1/2021.eacl-main.215</doi>
      <pwccode url="https://github.com/toizzy/deep-subjecthood" additional="false">toizzy/deep-subjecthood</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="217">
      <title>DOCENT : Learning Self-Supervised Entity Representations from Large Document Collections<fixed-case>DOCENT</fixed-case>: Learning Self-Supervised Entity Representations from Large Document Collections</title>
      <author><first>Yury</first><last>Zemlyanskiy</last></author>
      <author><first>Sudeep</first><last>Gandhe</last></author>
      <author><first>Ruining</first><last>He</last></author>
      <author><first>Bhargav</first><last>Kanagal</last></author>
      <author><first>Anirudh</first><last>Ravula</last></author>
      <author><first>Juraj</first><last>Gottweis</last></author>
      <author><first>Fei</first><last>Sha</last></author>
      <author><first>Ilya</first><last>Eckstein</last></author>
      <pages>2540–2549</pages>
      <abstract>This paper explores learning rich self-supervised entity representations from large amounts of associated text. Once pre-trained, these models become applicable to multiple entity-centric tasks such as ranked retrieval, knowledge base completion, <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, and more. Unlike other methods that harvest self-supervision signals based merely on a local context within a sentence, we radically expand the notion of <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a> to include any available text related to an entity. This enables a new class of powerful, high-capacity representations that can ultimately distill much of the useful information about an entity from multiple text sources, without any human supervision. We present several training strategies that, unlike prior approaches, learn to jointly predict words and entities   strategies we compare experimentally on downstream tasks in the TV-Movies domain, such as MovieLens tag prediction from user reviews and natural language movie search. As evidenced by results, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> match or outperform competitive baselines, sometimes with little or no fine-tuning, and are also able to scale to very large corpora. Finally, we make our <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">pre-trained models</a> publicly available. This includes Reviews2Movielens, mapping the ~1B word corpus of Amazon movie reviews (He and McAuley, 2016) to MovieLens tags (Harper and Konstan, 2016), as well as Reddit Movie Suggestions with natural language queries and corresponding community recommendations.</abstract>
      <url hash="34e58aaf">2021.eacl-main.217</url>
      <bibkey>zemlyanskiy-etal-2021-docent</bibkey>
      <doi>10.18653/v1/2021.eacl-main.217</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/movielens">MovieLens</pwcdataset>
    </paper>
    <paper id="218">
      <title>Scientific Discourse Tagging for Evidence Extraction</title>
      <author><first>Xiangci</first><last>Li</last></author>
      <author><first>Gully</first><last>Burns</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>2550–2562</pages>
      <abstract>Evidence plays a crucial role in any biomedical research narrative, providing justification for some claims and refutation for others. We seek to build models of scientific argument using <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction methods</a> from <a href="https://en.wikipedia.org/wiki/Academic_publishing">full-text papers</a>. We present the capability of automatically extracting text fragments from primary research papers that describe the evidence presented in that paper’s figures, which arguably provides the raw material of any scientific argument made within the paper. We apply richly contextualized deep representation learning pre-trained on biomedical domain corpus to the analysis of scientific discourse structures and the extraction of evidence fragments (i.e., the text in the results section describing data presented in a specified subfigure) from a set of biomedical experimental research articles. We first demonstrate our state-of-the-art scientific discourse tagger on two scientific discourse tagging datasets and its transferability to new <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>. We then show the benefit of leveraging scientific discourse tags for downstream tasks such as claim-extraction and evidence fragment detection. Our work demonstrates the potential of using evidence fragments derived from figure spans for improving the quality of scientific claims by cataloging, indexing and reusing evidence fragments as independent documents.</abstract>
      <url hash="a1617401">2021.eacl-main.218</url>
      <bibkey>li-etal-2021-scientific</bibkey>
      <doi>10.18653/v1/2021.eacl-main.218</doi>
      <pwccode url="https://github.com/jacklxc/ScientificDiscourseTagging" additional="false">jacklxc/ScientificDiscourseTagging</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed-rct">PubMed RCT</pwcdataset>
    </paper>
    <paper id="220">
      <title>StructSum : <a href="https://en.wikipedia.org/wiki/Summarization">Summarization</a> via Structured Representations<fixed-case>S</fixed-case>truct<fixed-case>S</fixed-case>um: Summarization via Structured Representations</title>
      <author><first>Vidhisha</first><last>Balachandran</last></author>
      <author><first>Artidoro</first><last>Pagnoni</last></author>
      <author><first>Jay Yoon</first><last>Lee</last></author>
      <author><first>Dheeraj</first><last>Rajagopal</last></author>
      <author><first>Jaime</first><last>Carbonell</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>2575–2585</pages>
      <abstract>Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges : (i) layout bias : they overfit to the style of training corpora ; (ii) limited abstractiveness : they are optimized to copying <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a> from the source rather than generating novel abstractive summaries ; (iii) lack of transparency : they are not interpretable. In this work, we propose a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoder-decoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN / DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a>, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.</abstract>
      <url hash="ee7f1349">2021.eacl-main.220</url>
      <bibkey>balachandran-etal-2021-structsum</bibkey>
      <doi>10.18653/v1/2021.eacl-main.220</doi>
      <pwccode url="https://github.com/vidhishanair/structured_summarizer" additional="false">vidhishanair/structured_summarizer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="222">
      <title>LSOIE : A Large-Scale Dataset for Supervised Open Information Extraction<fixed-case>LSOIE</fixed-case>: A Large-Scale Dataset for Supervised Open Information Extraction</title>
      <author><first>Jacob</first><last>Solawetz</last></author>
      <author><first>Stefan</first><last>Larson</last></author>
      <pages>2595–2600</pages>
      <abstract>Open Information Extraction (OIE) systems seek to compress the factual propositions of a sentence into a series of n-ary tuples. These tuples are useful for downstream tasks in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> like knowledge base creation, <a href="https://en.wikipedia.org/wiki/Textual_entailment">textual entailment</a>, and <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>. However, current OIE datasets are limited in both size and diversity. We introduce a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> by converting the QA-SRL 2.0 dataset to a large-scale OIE dataset LSOIE. Our LSOIE dataset is 20 times larger than the next largest human-annotated OIE dataset. We construct and evaluate several benchmark OIE models on LSOIE, providing baselines for future improvements on the task. Our LSOIE data, <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>, and code are made publicly available.</abstract>
      <url hash="7ccad692">2021.eacl-main.222</url>
      <attachment type="Dataset" hash="81248974">2021.eacl-main.222.Dataset.zip</attachment>
      <attachment type="Software" hash="cfadfd52">2021.eacl-main.222.Software.zip</attachment>
      <bibkey>solawetz-larson-2021-lsoie</bibkey>
      <doi>10.18653/v1/2021.eacl-main.222</doi>
      <pwccode url="https://github.com/Jacobsolawetz/large-scale-oie" additional="false">Jacobsolawetz/large-scale-oie</pwccode>
    </paper>
    <paper id="224">
      <title>Unsupervised Abstractive Summarization of Bengali Text Documents<fixed-case>B</fixed-case>engali Text Documents</title>
      <author><first>Radia Rayan</first><last>Chowdhury</last></author>
      <author><first>Mir Tafseer</first><last>Nayeem</last></author>
      <author><first>Tahsin Tasnim</first><last>Mim</last></author>
      <author><first>Md. Saifur Rahman</first><last>Chowdhury</last></author>
      <author><first>Taufiqul</first><last>Jannat</last></author>
      <pages>2612–2619</pages>
      <abstract>Abstractive summarization systems generally rely on large collections of document-summary pairs. However, the performance of abstractive systems remains a challenge due to the unavailability of the parallel data for low-resource languages like <a href="https://en.wikipedia.org/wiki/Bengali_language">Bengali</a>. To overcome this problem, we propose a graph-based unsupervised abstractive summarization system in the single-document setting for Bengali text documents, which requires only a Part-Of-Speech (POS) tagger and a pre-trained language model trained on Bengali texts. We also provide a human-annotated dataset with document-summary pairs to evaluate our abstractive model and to support the comparison of future abstractive summarization systems of the <a href="https://en.wikipedia.org/wiki/Bengali_language">Bengali Language</a>. We conduct experiments on this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and compare our <a href="https://en.wikipedia.org/wiki/System">system</a> with several well-established unsupervised extractive summarization systems. Our unsupervised abstractive summarization model outperforms the baselines without being exposed to any human-annotated reference summaries.</abstract>
      <url hash="b6e6dd19">2021.eacl-main.224</url>
      <bibkey>chowdhury-etal-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.eacl-main.224</doi>
      <pwccode url="https://github.com/tafseer-nayeem/BengaliSummarization" additional="false">tafseer-nayeem/BengaliSummarization</pwccode>
    </paper>
    <paper id="226">
      <title>On the Computational Modelling of Michif Verbal Morphology<fixed-case>M</fixed-case>ichif Verbal Morphology</title>
      <author><first>Fineen</first><last>Davis</last></author>
      <author><first>Eddie Antonio</first><last>Santos</last></author>
      <author><first>Heather</first><last>Souter</last></author>
      <pages>2631–2636</pages>
      <abstract>This paper presents a <a href="https://en.wikipedia.org/wiki/Finite-state_machine">finite-state computational model</a> of the <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">verbal morphology</a> of <a href="https://en.wikipedia.org/wiki/Michif">Michif</a>. Michif, the official language of the Mtis peoples, is a uniquely mixed language with Algonquian and French origins. It is spoken across the Mtis homelands in what is now called <a href="https://en.wikipedia.org/wiki/Canada">Canada</a> and the United States, but <a href="https://en.wikipedia.org/wiki/Italian_language">it</a> is highly endangered with less than 100 speakers. The verbal morphology is remarkably complex, as the already polysynthetic Algonquian patterns are combined with French elements and unique morpho-phonological interactions. The model presented in this paper, LI VERB KAA-OOSHITAHK DI MICHIF handles this complexity by using a series of composed finite-state transducers to model the concatenative morphology and phonological rule alternations that are unique to Michif. Such a rule-based approach is necessary as there is insufficient language data for an approach that uses <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>. A language model such as LI VERB KAA-OOSHITAHK DI MICHIF furthers the goals of Indigenous computational linguistics in Canada while also supporting the creation of tools for documentation, education, and revitalization that are desired by the Mtis community.</abstract>
      <url hash="c289266f">2021.eacl-main.226</url>
      <bibkey>davis-etal-2021-computational</bibkey>
      <doi>10.18653/v1/2021.eacl-main.226</doi>
    </paper>
    <paper id="227">
      <title>A Few Topical Tweets are Enough for Effective User Stance Detection</title>
      <author><first>Younes</first><last>Samih</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <pages>2637–2646</pages>
      <abstract>User stance detection entails ascertaining the position of a user towards a target, such as an entity, topic, or claim. Recent work that employs <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised classification</a> has shown that performing stance detection on vocal Twitter users, who have many tweets on a target, can be highly accurate (+98 %). However, such methods perform poorly or fail completely for less vocal users, who may have authored only a few tweets about a target. In this paper, we tackle stance detection for such <a href="https://en.wikipedia.org/wiki/User_(computing)">users</a> using two approaches. In the first approach, we improve user-level stance detection by representing tweets using contextualized embeddings, which capture latent meanings of words in context. We show that this approach outperforms two strong baselines and achieves 89.6 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and 91.3 % macro F-measure on eight controversial topics. In the second approach, we expand the tweets of a given user using their Twitter timeline tweets, which may not be topically relevant, and then we perform <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised classification</a> of the user, which entails clustering a user with other users in the training set. This approach achieves 95.6 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and 93.1 % <a href="https://en.wikipedia.org/wiki/F-measure">macro F-measure</a>.</abstract>
      <url hash="047fc377">2021.eacl-main.227</url>
      <attachment type="Dataset" hash="eb220bad">2021.eacl-main.227.Dataset.zip</attachment>
      <bibkey>samih-darwish-2021-topical</bibkey>
      <doi>10.18653/v1/2021.eacl-main.227</doi>
    </paper>
    <paper id="228">
      <title>Do Syntax Trees Help Pre-trained Transformers Extract Information?</title>
      <author><first>Devendra</first><last>Sachan</last></author>
      <author><first>Yuhao</first><last>Zhang</last></author>
      <author><first>Peng</first><last>Qi</last></author>
      <author><first>William L.</first><last>Hamilton</last></author>
      <pages>2647–2661</pages>
      <abstract>Much recent work suggests that incorporating <a href="https://en.wikipedia.org/wiki/Syntax_(programming_languages)">syntax information</a> from dependency trees can improve task-specific transformer models. However, the effect of incorporating dependency tree information into pre-trained transformer models (e.g., BERT) remains unclear, especially given recent studies highlighting how these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> implicitly encode <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a>. In this work, we systematically study the utility of incorporating dependency trees into pre-trained transformers on three representative information extraction tasks : semantic role labeling (SRL), <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, and relation extraction. We propose and investigate two distinct strategies for incorporating dependency structure : a late fusion approach, which applies a graph neural network on the output of a transformer, and a joint fusion approach, which infuses syntax structure into the transformer attention layers. These strategies are representative of prior work, but we introduce additional model design elements that are necessary for obtaining improved performance. Our empirical analysis demonstrates that these syntax-infused transformers obtain state-of-the-art results on SRL and relation extraction tasks. However, our analysis also reveals a critical shortcoming of these models : we find that their performance gains are highly contingent on the availability of human-annotated dependency parses, which raises important questions regarding the viability of syntax-augmented transformers in real-world applications.</abstract>
      <url hash="09916cab">2021.eacl-main.228</url>
      <attachment type="Software" hash="8b6fffd6">2021.eacl-main.228.Software.tgz</attachment>
      <bibkey>sachan-etal-2021-syntax</bibkey>
      <doi>10.18653/v1/2021.eacl-main.228</doi>
      <pwccode url="https://github.com/DevSinghSachan/syntax-augmented-bert" additional="false">DevSinghSachan/syntax-augmented-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="235">
      <title>Entity-level Factual Consistency of Abstractive Text Summarization</title>
      <author><first>Feng</first><last>Nan</last></author>
      <author><first>Ramesh</first><last>Nallapati</last></author>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <author><first>Cicero</first><last>Nogueira dos Santos</last></author>
      <author><first>Henghui</first><last>Zhu</last></author>
      <author><first>Dejiao</first><last>Zhang</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Bing</first><last>Xiang</last></author>
      <pages>2727–2733</pages>
      <abstract>A key challenge for abstractive summarization is ensuring factual consistency of the generated summary with respect to the original document. For example, state-of-the-art <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on existing datasets exhibit entity hallucination, generating names of entities that are not present in the source document. We propose a set of new metrics to quantify the entity-level factual consistency of generated summaries and we show that the entity hallucination problem can be alleviated by simply filtering the training data. In addition, we propose a summary-worthy entity classification task to the training process as well as a joint entity and summary generation approach, which yield further improvements in entity level metrics.</abstract>
      <url hash="3317e4bf">2021.eacl-main.235</url>
      <bibkey>nan-etal-2021-entity</bibkey>
      <doi>10.18653/v1/2021.eacl-main.235</doi>
      <pwccode url="https://github.com/amazon-research/fact-check-summarization" additional="false">amazon-research/fact-check-summarization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
    </paper>
    <paper id="239">
      <title>Diverse Adversaries for Mitigating Bias in Training</title>
      <author><first>Xudong</first><last>Han</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <pages>2760–2765</pages>
      <abstract>Adversarial learning can learn fairer and less biased models of language processing than standard training. However, current adversarial techniques only partially mitigate the problem of model bias, added to which their training procedures are often unstable. In this paper, we propose a novel approach to <a href="https://en.wikipedia.org/wiki/Adversarial_learning">adversarial learning</a> based on the use of multiple diverse discriminators, whereby <a href="https://en.wikipedia.org/wiki/Discriminator">discriminators</a> are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and stability of training.</abstract>
      <url hash="7651bd2a">2021.eacl-main.239</url>
      <bibkey>han-etal-2021-diverse</bibkey>
      <doi>10.18653/v1/2021.eacl-main.239</doi>
      <pwccode url="https://github.com/HanXudong/Diverse_Adversaries_for_Mitigating_Bias_in_Training" additional="false">HanXudong/Diverse_Adversaries_for_Mitigating_Bias_in_Training</pwccode>
    </paper>
    <paper id="240">
      <title>‘Just because you are right, does n’t mean I am wrong’ : Overcoming a bottleneck in development and evaluation of Open-Ended VQA tasks<fixed-case>I</fixed-case> am wrong’: Overcoming a bottleneck in development and evaluation of Open-Ended <fixed-case>VQA</fixed-case> tasks</title>
      <author><first>Man</first><last>Luo</last></author>
      <author><first>Shailaja Keyur</first><last>Sampat</last></author>
      <author><first>Riley</first><last>Tallman</last></author>
      <author><first>Yankai</first><last>Zeng</last></author>
      <author><first>Manuha</first><last>Vancha</last></author>
      <author><first>Akarshan</first><last>Sajja</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>2766–2771</pages>
      <abstract>GQA (CITATION) is a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for real-world visual reasoning and compositional question answering. We found that many answers predicted by the best vision-language models on the GQA dataset do not match the ground-truth answer but still are semantically meaningful and correct in the given context. In fact, this is the case with most existing visual question answering (VQA) datasets where they assume only one ground-truth answer for each question. We propose Alternative Answer Sets (AAS) of ground-truth answers to address this limitation, which is created automatically using off-the-shelf NLP tools. We introduce a semantic metric based on AAS and modify top VQA solvers to support multiple plausible answers for a question. We implement this approach on the GQA dataset and show the performance improvements.</abstract>
      <url hash="1e4dbf2c">2021.eacl-main.240</url>
      <bibkey>luo-etal-2021-just</bibkey>
      <doi>10.18653/v1/2021.eacl-main.240</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="241">
      <title>Better <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a> by Extracting <a href="https://en.wikipedia.org/wiki/Linguistic_description">Linguistic Information</a> from BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Hassan S.</first><last>Shavarani</last></author>
      <author><first>Anoop</first><last>Sarkar</last></author>
      <pages>2772–2783</pages>
      <abstract>Adding linguistic information (syntax or semantics) to neural machine translation (NMT) have mostly focused on using point estimates from pre-trained models. Directly using the capacity of massive pre-trained contextual word embedding models such as BERT(Devlin et al., 2019) has been marginally useful in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a> because effective fine-tuning is difficult to obtain for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a> without making training brittle and unreliable. We augment <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a> by extracting dense fine-tuned vector-based linguistic information from BERT instead of using point estimates. Experimental results show that our method of incorporating linguistic information helps <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a> to generalize better in a variety of training contexts and is no more difficult to train than conventional Transformer-based NMT.</abstract>
      <url hash="e67ff260">2021.eacl-main.241</url>
      <bibkey>shavarani-sarkar-2021-better</bibkey>
      <doi>10.18653/v1/2021.eacl-main.241</doi>
      <pwccode url="https://github.com/sfu-natlang/SFUTranslate" additional="false">sfu-natlang/SFUTranslate</pwccode>
    </paper>
    <paper id="242">
      <title>CLiMP : A Benchmark for Chinese Language Model Evaluation<fixed-case>CL</fixed-case>i<fixed-case>MP</fixed-case>: A Benchmark for <fixed-case>C</fixed-case>hinese Language Model Evaluation</title>
      <author><first>Beilei</first><last>Xiang</last></author>
      <author><first>Changbing</first><last>Yang</last></author>
      <author><first>Yu</first><last>Li</last></author>
      <author><first>Alex</first><last>Warstadt</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>2784–2790</pages>
      <abstract>Linguistically informed analyses of language models (LMs) contribute to the understanding and improvement of such <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>. Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP) to investigate what knowledge Chinese LMs acquire. CLiMP consists of sets of 1000 minimal pairs (MPs) for 16 syntactic contrasts in <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, covering 9 major <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese linguistic phenomena</a>. The MPs are semi-automatically generated, and human agreement with the labels in CLiMP is 95.8 %. We evaluate 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifiernoun agreement and verb complement selection are the phenomena that models generally perform best at. However, <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> struggle the most with the ba construction, binding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8 % average accuracy, while the performances of LSTMs and 5-grams are only moderately above chance level.</abstract>
      <url hash="885d7190">2021.eacl-main.242</url>
      <attachment type="Dataset" hash="49e1aab8">2021.eacl-main.242.Dataset.zip</attachment>
      <bibkey>xiang-etal-2021-climp</bibkey>
      <doi>10.18653/v1/2021.eacl-main.242</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/blimp">BLiMP</pwcdataset>
    </paper>
    <paper id="244">
      <title>Progressively Pretrained Dense Corpus Index for <a href="https://en.wikipedia.org/wiki/Open-domain_question_answering">Open-Domain Question Answering</a></title>
      <author><first>Wenhan</first><last>Xiong</last></author>
      <author><first>Hong</first><last>Wang</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>2803–2815</pages>
      <abstract>Commonly used information retrieval methods such as TF-IDF in open-domain question answering (QA) systems are insufficient to capture deep semantic matching that goes beyond lexical overlaps. Some recent studies consider the retrieval process as maximum inner product search (MIPS) using dense question and paragraph representations, achieving promising results on several information-seeking QA datasets. However, the pretraining of the <a href="https://en.wikipedia.org/wiki/Sparse_matrix">dense vector representations</a> is highly resource-demanding, e.g., requires a very large <a href="https://en.wikipedia.org/wiki/Batch_processing">batch size</a> and lots of <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training steps</a>. In this work, we propose a <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">sample-efficient method</a> to pretrain the paragraph encoder. First, instead of using heuristically created pseudo question-paragraph pairs for pretraining, we use an existing pretrained sequence-to-sequence model to build a strong question generator that creates high-quality pretraining data. Second, we propose a simple progressive pretraining algorithm to ensure the existence of effective negative samples in each batch. Across three open-domain QA datasets, our method consistently outperforms a strong dense retrieval baseline that uses 6 times more computation for training. On two of the <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> achieves more than 4-point absolute improvement in terms of answer exact match.<i>e.g.</i>, requires a very large batch size and lots of training steps. In this work, we propose a sample-efficient method to pretrain the paragraph encoder. First, instead of using heuristically created pseudo question-paragraph pairs for pretraining, we use an existing pretrained sequence-to-sequence model to build a strong question generator that creates high-quality pretraining data. Second, we propose a simple progressive pretraining algorithm to ensure the existence of effective negative samples in each batch. Across three open-domain QA datasets, our method consistently outperforms a strong dense retrieval baseline that uses 6 times more computation for training. On two of the datasets, our method achieves more than 4-point absolute improvement in terms of answer exact match.</abstract>
      <url hash="c31b18d5">2021.eacl-main.244</url>
      <bibkey>xiong-etal-2021-progressively</bibkey>
      <doi>10.18653/v1/2021.eacl-main.244</doi>
      <pwccode url="https://github.com/xwhan/ProQA" additional="false">xwhan/ProQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="245">
      <title>Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs</title>
      <author><first>Dora</first><last>Jambor</last></author>
      <author><first>Komal</first><last>Teru</last></author>
      <author><first>Joelle</first><last>Pineau</last></author>
      <author><first>William L.</first><last>Hamilton</last></author>
      <pages>2816–2822</pages>
      <abstract>Real-world knowledge graphs are often characterized by low-frequency relationsa challenge that has prompted an increasing interest in few-shot link prediction methods. These methods perform link prediction for a set of new relations, unseen during training, given only a few example facts of each relation at test time. In this work, we perform a systematic study on a spectrum of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> derived by generalizing the current state of the art for few-shot link prediction, with the goal of probing the limits of <a href="https://en.wikipedia.org/wiki/Machine_learning">learning</a> in this few-shot setting. We find that a simple, zero-shot baseline   which ignores any relation-specific information   achieves surprisingly strong performance. Moreover, experiments on carefully crafted synthetic datasets show that having only a few examples of a relation fundamentally limits models from using fine-grained structural information and only allows for exploiting the coarse-grained positional information of entities. Together, our findings challenge the implicit assumptions and inductive biases of prior work and highlight new directions for research in this area.</abstract>
      <url hash="f3f7100e">2021.eacl-main.245</url>
      <bibkey>jambor-etal-2021-exploring</bibkey>
      <doi>10.18653/v1/2021.eacl-main.245</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/wiki-one">Wiki-One</pwcdataset>
    </paper>
    <paper id="246">
      <title>ProFormer : Towards On-Device LSH Projection Based Transformers<fixed-case>P</fixed-case>ro<fixed-case>F</fixed-case>ormer: Towards On-Device <fixed-case>LSH</fixed-case> Projection Based Transformers</title>
      <author><first>Chinnadhurai</first><last>Sankar</last></author>
      <author><first>Sujith</first><last>Ravi</last></author>
      <author><first>Zornitsa</first><last>Kozareva</last></author>
      <pages>2823–2828</pages>
      <abstract>At the heart of text based neural models lay word representations, which are powerful but occupy a lot of memory making it challenging to deploy to devices with memory constraints such as <a href="https://en.wikipedia.org/wiki/Mobile_phone">mobile phones</a>, <a href="https://en.wikipedia.org/wiki/Watch">watches</a> and IoT. To surmount these challenges, we introduce ProFormer   a projection based transformer architecture that is faster and lighter making it suitable to deploy to memory constraint devices and preserve user privacy. We use LSH projection layer to dynamically generate word representations on-the-fly without embedding lookup tables leading to significant memory footprint reduction from O(V.d) to O(T), where V is the vocabulary size, d is the embedding dimension size and T is the dimension of the LSH projection representation. We also propose a local projection attention (LPA) layer, which uses self-attention to transform the input sequence of N LSH word projections into a sequence of N / K representations reducing the computations quadratically by O(K2). We evaluate ProFormer on multiple text classification tasks and observed improvements over prior state-of-the-art on-device approaches for short text classification and comparable performance for long text classification tasks. ProFormer is also competitive with other popular but highly resource-intensive approaches like BERT and even outperforms small-sized BERT variants with significant resource savings   reduces the <a href="https://en.wikipedia.org/wiki/Memory_footprint">embedding memory footprint</a> from 92.16 MB to 1.7 KB and requires 16x less computation overhead, which is very impressive making it the fastest and smallest on-device model.</abstract>
      <url hash="9a69f790">2021.eacl-main.246</url>
      <award>Honorable Mention for Best Short Paper</award>
      <bibkey>sankar-etal-2021-proformer</bibkey>
      <doi>10.18653/v1/2021.eacl-main.246</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="249">
      <title>Crisscrossed Captions : Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO<fixed-case>MS</fixed-case>-<fixed-case>COCO</fixed-case></title>
      <author><first>Zarana</first><last>Parekh</last></author>
      <author><first>Jason</first><last>Baldridge</last></author>
      <author><first>Daniel</first><last>Cer</last></author>
      <author><first>Austin</first><last>Waters</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <pages>2855–2870</pages>
      <abstract>By supporting multi-modal retrieval training and evaluation, image captioning datasets have spurred remarkable progress on <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning</a>. Unfortunately, datasets have limited cross-modal associations : images are not paired with other images, captions are only paired with other captions of the same image, there are no negative associations and there are missing positive cross-modal associations. This undermines research into how inter-modality learning impacts intra-modality tasks. We address this gap with Crisscrossed Captions (CxC), an extension of the MS-COCO dataset with human semantic similarity judgments for 267,095 intra- and inter-modality pairs. We report baseline results on <a href="https://en.wikipedia.org/wiki/CxC">CxC</a> for strong existing unimodal and multimodal models. We also evaluate a multitask dual encoder trained on both image-caption and caption-caption pairs that crucially demonstrates CxC’s value for measuring the influence of intra- and inter-modality learning.</abstract>
      <url hash="35cbca1a">2021.eacl-main.249</url>
      <bibkey>parekh-etal-2021-crisscrossed</bibkey>
      <doi>10.18653/v1/2021.eacl-main.249</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cxc">CxC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    <title_ar>التعليقات المتقاطعة: أحكام التشابه الدلالي متعددة الوسائط والداخلية الموسعة لـ MS-COCO</title_ar>
      <title_es>Subtítulos entrecruzados: Juicios de similitud semántica intramodal e intermodal ampliados para MS-COCO</title_es>
      <title_pt>Legendas cruzadas: julgamentos estendidos de semelhança semântica intramodal e intermodal para MS-COCO</title_pt>
      <title_ja>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS - COCO</title_ja>
      <title_zh>纵横之题:MS-COCO之广内联与多式联运语义相似性决</title_zh>
      <title_hi>Crisscrossed कैप्शन: एमएस-कोको के लिए विस्तारित इंट्रामोडल और इंटरमोडल सिमेंटिक समानता निर्णय</title_hi>
      <title_ga>Fotheidil Crisscrossed: Breithiúnais Chomhchosúlachta Shéimeantach Intramodal agus Idirmhódúil Leathnaithe do MS-COCO</title_ga>
      <title_hu>Válogatott feliratok: kiterjesztett intramodális és intermodális szemantikus hasonlósági ítéletek MS-COCO esetében</title_hu>
      <title_el>Επικεφαλής τίτλος: Εκτεταμένες αποφάσεις ενδοmodaler και διατροπικής σημασιολογικής ομοιότητας για MS-COCO</title_el>
      <title_ka>კრისკროსური შესახებ: გაფართებული ინტრამოდელური და ინტერმოდელური Semantic Similarity Judgments for MS- COCO</title_ka>
      <title_it>Didascalie a croce: sentenze estese di somiglianza semantica intramodale e intermodale per MS-COCO</title_it>
      <title_kk>Крискросс айдарлары: MS- COCO үшін интрамодалық және интермодалық Semantic Similarity Judgments</title_kk>
      <title_lt>Krizių kryžminiai žodžiai: išplėsti valstybių narių – COCO – vidaus ir tarpmodalinio semestinio panašumo sprendimai</title_lt>
      <title_ms>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</title_ms>
      <title_mk>Крискросирани наслови: Проширени интрамодални и интермодални семантични судии за сличност за MS-COCO</title_mk>
      <title_no>Krysskrosserte tittel: Utvida intermodal og intermodal semantisk forskjellighetsjustering for MS- COCO</title_no>
      <title_pl>Podpisy kryzysowe: Rozszerzone wyroki dotyczące podobieństwa semantycznego w odniesieniu do MS-COCO</title_pl>
      <title_mn>Крискросс дурангууд: MS-COCO-ын интрамодал болон интермодал төстэй ижил төстэй шүүмжүүд</title_mn>
      <title_ml>ക്രിസ്ക്രോസ് ചെയ്ത പ്രമേയങ്ങള്‍: എസ്- കോക്കോയ്ക്ക് വേണ്ടി വിശാലമായ ഇന്റ്രാമോഡാലും ഇന്റര്‍മോഡാല്‍ സെമാന്റ</title_ml>
      <title_sr>Raskrivena kapcija: Prošireni rasuđivanje intrramodalne i intermodalne semantičke sličnosti za MS-COCO</title_sr>
      <title_ro>Titluri clasice: Hotărâri extinse privind similitudinea semantică intramodală și intermodală pentru MS-COCO</title_ro>
      <title_mt>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</title_mt>
      <title_so>Crisscross Captions: Extended Intramodal and Intermodal Semantic Similarity Judges for MS-CO</title_so>
      <title_sv>Kryssformade bildtexter: Utvidgade domar om intramodal och intermodal semantisk likhet för MS-COCO</title_sv>
      <title_si>Crisscrossed Captions: විස්තාරිත සිමාන්තික සිමාන්තික විශ්වාසය MS-COCO සඳහා</title_si>
      <title_ta>சிகிரிஸ் தலைப்புகள்: MS- CO க்கான விரிவாக்கப்பட்ட Intramodal மற்றும் Intermodal Semantic Similarity தீர்ப்புகள்</title_ta>
      <title_ur>کریسسکروس کیپٹینز: MS-COCO کے لئے پھیلائی Intramodal and Intermodal Semantic Similarity Judgments</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Các phim khủng khiếp: Các quyết định tương đồng truyền thống truyền thống và truyền thống.</title_vi>
      <title_hr>Raskrsnuti načini: Prošireni rasuđivanje intrramodalne i intermodalne semantičke sličnosti za MS-COCO</title_hr>
      <title_bg>Крискрозирани надписи: Разширени решения за интрамодална и интермодална семантична сходство за МС-КОКО</title_bg>
      <title_nl>Crissgescrossed ondertitels: Uitgebreide intramodale en intermodale semantische gelijkenisoordelen voor MS-COCO</title_nl>
      <title_de>Crissscrossed Untertitel: Erweiterte intramodale und intermodale semantische Ähnlichkeitsurteile für MS-COCO</title_de>
      <title_id>Capsi Crisscrossed: Penghakiman Semantik Intramodal dan Intermodal Terluas untuk MS-COCO</title_id>
      <title_da>Krydsrullede billedtekster: Udvidede domme om intramodal og intermodal semantisk lighed for MS-COCO</title_da>
      <title_ko>교차 자막: MS-COCO의 확장 모드 내와 모드 간 의미 유사성 판단</title_ko>
      <title_fa>کاپیتان‌های کریسکروس: قضاوت‌های شبیه‌انگیزی داخلی و منطقه‌ای برای MS-COCO گسترده شده</title_fa>
      <title_sw>Makala yaliyokosolewa: Mahakama ya Kitengo cha Intramodal na Kimataifa kwa ajili ya MS-CO</title_sw>
      <title_tr>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</title_tr>
      <title_hy>Քրիսխրոսխային գլխարկներ. ՄՍ-ԿոԿՕ-ի համար ընդլայնված ինտրամոդալ և ինտերմոդալ սեմատիկ նման դատողություններ</title_hy>
      <title_af>Crisscrossed Titels: uitgebreide Intramodaal en Intermodaal Semantiese Ligtigheid Verordeninge vir MS- COCO</title_af>
      <title_sq>Kapsionet e kryqëzuara: Gjykimet e zgjeruara të ngjashmërisë Semantike Intramodale dhe Intermodale për MS-COCO</title_sq>
      <title_az>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</title_az>
      <title_am>የስሕተት አርእስቶች: Extended Intramodal and Intermodal Semantic Similarity Judges for MS-CO</title_am>
      <title_bs>Raskrsnuti kapiji: Prošireni sudovi o intramodalnoj i intermodalnoj semantičkoj sličnosti za MS-COCO</title_bs>
      <title_bn>সমালোচনার শিরোনাম: MS-CO এর জন্য বিস্তৃত ইন্ট্রামোডাল এবং ইন্টারমডাল সেম্যান্ডিক সেমান্টিক বিচার</title_bn>
      <title_cs>Krizové titulky: Rozšířené rozsudky o intramodální a intermodální sémantické podobnosti pro MS-COCO</title_cs>
      <title_fi>Crisscrossed captions: Extended intramodal and intermodal semantic similarity tuomiot MS-COCO</title_fi>
      <title_ca>Capcions transversals: Sentences extenses de Semància Semàtica Intramodal i Intermodal per MS-COCO</title_ca>
      <title_et>Kriisilised pealkirjad: MS-COCO laiendatud intramodaalse ja intermodaalse semantilise sarnasuse kohtuotsused</title_et>
      <title_he>תורגם ע"י Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</title_he>
      <title_sk>Krizni napisi: Razširjene intramodalne in intermodalne semantične podobnosti sodbe za MS-COCO</title_sk>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_bo>Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO</title_bo>
      <title_jv>kriScroosed Captions: Expanded Intramadal and intermodal semanti Similrity judgments for CS-COMO</title_jv>
      <abstract_ar>من خلال دعم التدريب على الاسترجاع متعدد الوسائط والتقييم ، حفزت مجموعات بيانات الصور التوضيحية تقدمًا ملحوظًا في تعلم التمثيل. لسوء الحظ ، تحتوي مجموعات البيانات على ارتباطات محدودة عبر الوسائط: لا يتم إقران الصور مع الصور الأخرى ، ويتم إقران التسميات التوضيحية فقط مع التسميات التوضيحية الأخرى للصورة نفسها ، ولا توجد ارتباطات سلبية ولا توجد ارتباطات موجبة عبر الوسائط. هذا يقوض البحث في كيفية تأثير التعلم متعدد الوسائط على المهام داخل الوسائط. نعالج هذه الفجوة باستخدام Crisscrossed Captions (CxC) ، وهو امتداد لمجموعة بيانات MS-COCO مع أحكام تشابه دلالي بشرية لـ 267095 زوجًا داخليًا ومتعدد الأساليب. نقوم بالإبلاغ عن نتائج خط الأساس على CxC للنماذج القوية الحالية أحادية الوسائط ومتعددة الوسائط. نقوم أيضًا بتقييم برنامج تشفير مزدوج متعدد المهام تم تدريبه على كل من أزواج التعليقات التوضيحية والصورة والتعليقات التوضيحية التي توضح بشكل حاسم قيمة CxC لقياس تأثير التعلم داخل الوسائط ومتعددها.</abstract_ar>
      <abstract_es>Al apoyar la capacitación y la evaluación de recuperación multimodal, los conjuntos de datos de subtítulos de imágenes han impulsado un progreso notable en el aprendizaje de la representación. Desafortunadamente, los conjuntos de datos tienen asociaciones intermodales limitadas: las imágenes no se emparejan con otras imágenes, los subtítulos solo se emparejan con otros subtítulos de la misma imagen, no hay asociaciones negativas y faltan asociaciones intermodales positivas. Esto socava la investigación sobre cómo el aprendizaje intermodal impacta en las tareas intramodales. Abordamos esta brecha con Crisscrossed Captions (CxC), una extensión del conjunto de datos MS-COCO con juicios de similitud semántica humana para 267.095 pares intra e intermodalidad. Presentamos los resultados de referencia en CxC para modelos unimodales y multimodales sólidos existentes. También evaluamos un codificador dual multitarea entrenado en pares de subtítulos de imagen y subtítulos que demuestra de manera crucial el valor de CxC para medir la influencia del aprendizaje intra e intermodal.</abstract_es>
      <abstract_pt>Ao oferecer suporte ao treinamento e avaliação de recuperação multimodal, os conjuntos de dados de legendagem de imagens estimularam um progresso notável no aprendizado de representação. Infelizmente, os conjuntos de dados têm associações multimodais limitadas: as imagens não são emparelhadas com outras imagens, as legendas são emparelhadas apenas com outras legendas da mesma imagem, não há associações negativas e estão ausentes associações modais cruzadas positivas. Isso prejudica a pesquisa sobre como a aprendizagem intermodal afeta as tarefas intramodal. Abordamos essa lacuna com legendas cruzadas (CxC), uma extensão do conjunto de dados MS-COCO com julgamentos de semelhança semântica humana para 267.095 pares intra e intermodalidade. Relatamos resultados de linha de base em CxC para modelos unimodais e multimodais fortes existentes. Também avaliamos um codificador duplo multitarefa treinado em pares de legenda e legenda de imagem que demonstra crucialmente o valor do CxC para medir a influência do aprendizado intra e intermodalidade.</abstract_pt>
      <abstract_ja>マルチモーダル検索トレーニングと評価をサポートすることで、画像キャプションデータセットは表現学習の顕著な進歩を促進しました。残念ながら、データセットのクロスモーダル関連付けは限られています。画像は他の画像とペアになっておらず、キャプションは同じ画像の他のキャプションとペアになっているだけで、負の関連付けはなく、正のクロスモーダル関連付けはありません。これは、モダリティ間学習がモダリティ内タスクにどのように影響するかに関する研究を損なう。我々は、267,095のモーダリティ内およびモーダリティ間のペアについてのヒトのセマンティック類似性判定を備えたMS - COCOデータセットの拡張であるCrisscrossed Captions (CxC)でこのギャップに対処する。強力な既存の単式およびマルチモーダルモデルのCxCのベースライン結果を報告します。また、画像キャプションとキャプションの両方のペアでトレーニングされたマルチタスクデュアルエンコーダーを評価します。これは、モーダリティ内およびモーダリティ間の学習の影響を測定するためのCxCの価値を重要に示します。</abstract_ja>
      <abstract_zh>多模态检练评估,图像字幕数集于学。 不幸者,数集有限跨模态关联:图不与他配对,标题仅与同题配对,不负关联,且缺正跨模态联。 此坏模态间学模态内事也。 CxC)以纵横之标(CxC)决此差,CxCMS-COCO数集之广,有267,095模态内模态间人语义相似性决之。 以告 CxC 之强大单式与多式联运之基线也。 又评图像-标题-标对上练之多任务双编码器,当编码器至要而证之CxC量模内模态之间学之价。</abstract_zh>
      <abstract_hi>मल्टी-मोडल पुनर्प्राप्ति प्रशिक्षण और मूल्यांकन का समर्थन करके, छवि कैप्शनिंग डेटासेट ने प्रतिनिधित्व सीखने पर उल्लेखनीय प्रगति को प्रेरित किया है। दुर्भाग्य से, डेटासेट में सीमित क्रॉस-मोडल एसोसिएशन हैं: छवियों को अन्य छवियों के साथ जोड़ा नहीं जाता है, कैप्शन केवल एक ही छवि के अन्य कैप्शन के साथ जोड़े जाते हैं, कोई नकारात्मक संघ नहीं हैं और सकारात्मक क्रॉस-मोडल संघों की कमी है। यह अनुसंधान को कमजोर करता है कि अंतर-साधन सीखने इंट्रा-मोडलिटी कार्यों को कैसे प्रभावित करता है। हम इस अंतर को क्रिसक्रॉस्ड कैप्शन (सीएक्ससी) के साथ संबोधित करते हैं, जो 267,095 इंट्रा- और इंटर-मोडलिटी जोड़े के लिए मानव शब्दार्थ समानता निर्णय के साथ एमएस-कोको डेटासेट का एक विस्तार है। हम मजबूत मौजूदा यूनिमोडल और मल्टीमॉडल मॉडल के लिए सीएक्ससी पर बेसलाइन परिणामों की रिपोर्ट करते हैं। हम छवि-कैप्शन और कैप्शन-कैप्शन जोड़े दोनों पर प्रशिक्षित एक मल्टीटास्क दोहरे एनकोडर का भी मूल्यांकन करते हैं जो महत्वपूर्ण रूप से इंट्रा- और इंटर-मोडलिटी लर्निंग के प्रभाव को मापने के लिए सीएक्ससी के मूल्य को प्रदर्शित करता है।</abstract_hi>
      <abstract_ga>Trí thacú le hoiliúint agus le meastóireacht ilmhódúil aisghabhála, spreag tacair shonraí um theidealú íomhánna dul chun cinn suntasach ar fhoghlaim ionadaíochta. Ar an drochuair, tá comhcheangail thrasmhódúla teoranta ag tacair sonraí: ní dhéantar íomhánna a phéireáil le híomhánna eile, ní dhéantar fortheidil a phéireáil ach le fotheidil eile den íomhá chéanna, níl aon chomhcheangail dhiúltacha ann agus tá comhlachais thrasmhódúla dearfacha in easnamh. Baineann sé seo an bonn ó thaighde ar an tionchar a bhíonn ag foghlaim idirmhódúlachta ar thascanna inmhódúlachta. Tugaimid aghaidh ar an mbearna seo le Fotheidil Crisscrossed (CxC), síneadh ar an tacar sonraí MS-COCO le breithiúnais chosúlachta shéimeantaigh dhaonna do 267,095 péire inmhódúlachta agus idirmhódúlachta. Tuairiscímid torthaí bonnlíne ar CxC do mhúnlaí láidre aonmhódacha agus ilmhódacha atá ann cheana féin. Déanaimid meastóireacht freisin ar ionchódóir déach il-thasc atá oilte ar phéirí íomhá-theideal agus ceannteideal a léiríonn go ríthábhachtach an luach atá ag CxC maidir le tionchar na foghlama laistigh agus idirmhódúlachta a thomhas.</abstract_ga>
      <abstract_el>Υποστηρίζοντας την εκπαίδευση και την αξιολόγηση πολλαπλών τρόπων ανάκτησης, τα σύνολα δεδομένων λεζάντας εικόνων έχουν προκαλέσει αξιοσημείωτη πρόοδο στη μάθηση αναπαράστασης. Δυστυχώς, τα σύνολα δεδομένων έχουν περιορισμένες διασυνδέσεις: οι εικόνες δεν συνδυάζονται με άλλες εικόνες, οι λεζάντες συνδυάζονται μόνο με άλλες λεζάντες της ίδιας εικόνας, δεν υπάρχουν αρνητικές συσχετίσεις και λείπουν θετικές διασυνδέσεις. Αυτό υπονομεύει την έρευνα σχετικά με τον τρόπο με τον οποίο η διαπροπική μάθηση επηρεάζει τα καθήκοντα ενδοmodality. Αντιμετωπίζουμε αυτό το χάσμα με Crissscrooss Titions (CxC), μια επέκταση του συνόλου δεδομένων MS-COCO με ανθρώπινες σημασιολογικές ομοιότητες για 267,095 ζεύγη ενδο- και ενδοmodality. Αναφέρουμε βασικά αποτελέσματα για το CxC για ισχυρά υπάρχοντα μονοmodale και πολυmodale μοντέλα. Αξιολογούμε επίσης έναν πολυλειτουργικό διπλό κωδικοποιητή εκπαιδευμένο τόσο σε ζεύγη εικόνας-λεζάντας όσο και λεζάντας-λεζάντας που καταδεικνύει καθοριστικά την αξία του για τη μέτρηση της επιρροής της ενδο- και διαμεσολαβητικής μάθησης.</abstract_el>
      <abstract_ka>მრავალმედი მოდიალური გავიღოთ განსწავლების და განსაზღვრების მიხედვით, გამოსახულების შესაძლებელი მონაცემების შესაძლებელი პროგრესი განსწავლების შესაძლებელად გამოიყენებ მართლად, მონაცემების კონფიგურაციები აქვს გადარჩენებული კრისმოდიალური აზოციაციები: გამოსახულები სხვა გამოსახულებით არიან დაკავშირებული, შესახებ მხოლოდ სხვა გამოსახულებით დაკავშირებული, არიან დარ ეს მოდილიტების შესახებ, როგორ მოდილიტების შესახებ მოდილიტების შესახებ. ჩვენ ამ განსხვავებას კრისკროსური კატაციებით (CxC), MS-COCO მონაცემების მონაცემების განსხვავება, რომელიც ადამიანის სენმანტიკური განსხვავებებით 267 095 ინტერული და ინტერმოდილიტური კოსტაციების განსხვავ ჩვენ CxC-ის მუშაობელი უნიმოდეალური და მულტიმოდეალური მოდელებისთვის შეტყობინებთ. ჩვენ ასევე მრავალ დავამუშავებთ ეუალური კოდერი, რომელიც გამოსახულებული სახელსაწყოთან და სახელსაწყოთან სახელსაწყოთან მრავალური კოდერის მნიშვნელობა CxC-ის მნიშვნელობა, რომელიც ინტერე- და ინტე</abstract_ka>
      <abstract_hu>A multimodális visszakeresési képzés és értékelés támogatásával a képalkotó adatkészletek figyelemreméltó előrelépést eredményeztek a reprezentációs tanulás terén. Sajnos az adatkészleteknek korlátozott mértékben vannak transzmodális asszociációi: a képeket nem párosítják más képekkel, a feliratokat csak ugyanannak a képnek a többi feliratával párosítják, nincsenek negatív asszociációk és hiányoznak pozitív transzmodális asszociációk. Ez aláássa azt a kutatást, hogy az intermodális tanulás milyen hatással van az intermodális feladatokra. Ezt a hiányt a Crisscrossed Captions (CxC) segítségével kezeljük, amely az MS-COCO adatkészlet kiterjesztése 267 095 intra- és intermodalitási párra vonatkozó humán szemantikai hasonlósági ítéletekkel. Az erős unimodális és multimodális modellek esetében jelentjük a CxC alapvető eredményeit. Emellett értékelünk egy multifeladatos kettős kódolót, amely mind a kép-felirat, mind a felirat-felirat párokra képzett, és amely alapvetően bizonyítja a CxC értékét az intra- és intermodalitás tanulás hatásának mérésében.</abstract_hu>
      <abstract_it>Sostenendo la formazione e la valutazione multimodali di recupero, i set di dati per la didascalia delle immagini hanno stimolato notevoli progressi nell'apprendimento della rappresentazione. Purtroppo, i dataset hanno associazioni cross-modali limitate: le immagini non sono accoppiate ad altre immagini, le didascalie sono accoppiate solo ad altre didascalie della stessa immagine, non ci sono associazioni negative e mancano associazioni cross-modali positive. Ciò compromette la ricerca sull'impatto dell'apprendimento intermodale sui compiti intramodali. Affrontiamo questo gap con Crisscrossed Captions (CxC), un'estensione del dataset MS-COCO con giudizi di somiglianza semantica umana per 267.095 coppie intra- e intermodalità. Riportiamo i risultati di base su CxC per modelli unimodali e multimodali forti esistenti. Valutiamo anche un encoder dual multitask addestrato sia sulle coppie immagine-didascalia che didascalia-didascalia che dimostra in modo cruciale il valore di CxC per misurare l'influenza dell'apprendimento intra-e intermodale.</abstract_it>
      <abstract_kk>Көптеген модельді алу және оқытуды қолдау арқылы кескін айдарындағы деректер жиындары көрсетілетін оқыту үшін белгілі жұмыс істейді. Кешіріңіз, деректер жиындарының көп модельді байланыстары шектелген: кескіндер басқа кескіндермен біріктірілмейді, айдары тек бір кескіндің басқа айдарының біріктірілмейді, негативті байланыстары жоқ және оң көп модель Бұл модельдік үйренудің ішкі ішкі тапсырмаларды қалай әсер ететін зерттеулерді бақылайды. Біз бұл кеңістікті Крискросс айдарлары (CxC) мен, көмегімен 267 095 ішкі және ішкі моделиттік қорлары үшін MS- COCO деректер қорларының кеңістігін қолданамыз. Біз CxC негізгі негізгі нәтижелерді бір модель және көп модель үлгілері үшін хабарлаймыз. Біз сондай-ақ кескін айдары мен айдарының екеуінде оқылған көптеген екі тапсырма кодерін бағалаймыз. Бұл CxC интернет- және медициналық оқыту үшін көпшілікті оқыту үшін мәнін көрсетеді.</abstract_kk>
      <abstract_ms>Dengan menyokong latihan dan penilaian pemulihan berbilang-modal, set data captioning imej telah mendorong kemajuan yang luar biasa dalam pembelajaran perwakilan. Malangnya, set data mempunyai kumpulan melintasi-modal terbatas: imej tidak dipasang dengan imej lain, tajuk hanya dipasang dengan tajuk lain imej yang sama, tiada kumpulan negatif dan hilang kumpulan melintasi-modal positif. Ini merusak kajian bagaimana pembelajaran antarmodalitas mempengaruhi tugas intra-modaliti. Kami mengatasi ruang ini dengan Capsi Crisscrossed (CxC), sambungan set data MS-COCO dengan penilaian semantik persamaan manusia untuk 267,095 pasangan intra- dan intermodaliti. We report baseline results on CxC for strong existing unimodal and multimodal models.  Kami juga menilai pengekod dua tugas berbilang yang dilatih pada pasangan imej-caption dan caption-caption yang paling penting menunjukkan nilai CxC untuk mengukur pengaruh pembelajaran intra- dan intermodaliti.</abstract_ms>
      <abstract_mk>Со поддршката на мултимодалната обука и евалуација за преземање на слики, наборите на податоци за наслови на сликите поттикнаа извонреден напредок во учењето на претставништвото. За жал, датотеките имаат ограничени крстомодални асоцијации: сликите не се парирани со други слики, насловите се парирани само со други наслови на истата слика, нема негативни асоцијации и недостасуваат позитивни крстомодални асоцијации. Ова го поткопува истражувањето за тоа како интермодијалното учење влијае на интермодијалните задачи. Ние ја решаваме оваа празнина со Crisscrossed Captions (CxC), продолжување на податоците на MS-COCO со човечки семантични пресуди за сличност за 267.095 интермодитални и меѓумодитални парови. Ние известуваме за основните резултати на CxC за силни постоечки унимодални и мултимодални модели. We also evaluate a multitask dual encoder trained on both image-caption and caption-caption pairs that crucially demonstrates CxC's value for measuring the influence of intra- and inter-modality learning.</abstract_mk>
      <abstract_mt>By supporting multi-modal retrieval training and evaluation, image captioning datasets have spurred remarkable progress on representation learning.  Sfortunatament, is-settijiet tad-dejta għandhom assoċjazzjonijiet transmodali limitati: l-immaġni mhumiex imqabbla ma’ immaġni oħra, il-intestaturi huma mqabbla biss ma’ intestaturi oħra tal-istess immaġni, ma hemm l-ebda assoċjazzjonijiet negattivi u hemm nuqqas ta’ assoċjazzjonijiet transmodali pożittivi. Dan jimmina r-riċerka dwar kif it-tagħlim inter-modalità għandu impatt fuq il-kompiti intramodali. Aħna nindirizzaw din id-distakk mal-Kapitoli Crisscrossed (CxC), estensjoni tad-dataset MS-COCO b’sentenzi semantiċi ta’ similarità umana għal 267,095 par intra- u inter-modality. Aħna nirrappurtaw ir-riżultati tal-linja bażi dwar CxC għal mudelli unimodali u multimodali eżistenti b’saħħithom. Aħna jevalwaw ukoll kodifikatur doppju multikompiti mħarreġ kemm fuq par ta' immaġni-titolu kif ukoll titolu-titolu li juri b'mod kruċjali l-valur ta' CxC għall-kejl tal-influwenza tat-tagħlim intra-modali u intermodali.</abstract_mt>
      <abstract_ml>പല-മോഡാല്‍ പിന്തുണയ്ക്കുന്നതിനെയും പിന്തുണയ്ക്കുന്നതിനെയും പിന്തുണയ്ക്കുന്നതിനാല്‍, ഇമേജ് ക്യാപ്റ്റേഷന്‍ ഡ നിര്‍ഭാഗ്യവശാല്‍, ഡാറ്റാസറ്റുകള്‍ ക്രോസ്മോഡല്‍ സംഘടികള്‍ പരിധിയിലാണ്: ചിത്രങ്ങള്‍ മറ്റു ചിത്രങ്ങളോടൊപ്പം ഇണക്കുന്നില്ല, ശേഖരങ്ങള്‍ ഒരേ ചിത്രത്തിന്റ ഇത് വിദ്യാഭ്യാസ പഠിക്കുന്നത് എങ്ങനെയാണെന്ന് ശ്രദ്ധിക്കുന്നു ക്രിസ്ക്രോസ്സ് ക്രോസ്സ് ക്രോസ് ക്രോസ് ക്രോപ്സ് തപ്പുകളുമായി ഈ വ്യത്യാസം വിശദീകരിക്കുന്നു. MS-CO ഡാറ്റാസെറ്റിന്റെ വിശേഷവും, മനുഷ്യന്‍  We report baseline results on CxC for strong existing unimodal and multimodal models.  ചിത്രത്തിന്റെ തലക്കെട്ടിന്റെയും തലക്കെട്ടിന്റെയും ജോട്ടില്‍ പഠിപ്പിക്കപ്പെട്ട ഒരു മുള്‍ട്ടിട്ടിക്കൂട്ടിക്കൊണ്ടിരിക്കുന്ന ഒരു രണ്ട് കോഡ</abstract_ml>
      <abstract_lt>Remiant daugiarūšio naudojimo mokymus ir vertinimą, vaizdo įrašų duomenų rinkiniai paskatino pastebimą pažangą atstovavimo mokymosi srityje. Deja, duomenų rinkiniai turi ribotas tarpmodalines asociacijas: vaizdai nesuderinami su kitais vaizdais, antraštės susiejamos tik su kitais to paties vaizdo antraštėmis, nėra neigiamų asociacijų ir trūksta teigiamų tarpmodalinių asociacijų. This undermines research into how inter-modality learning impacts intra-modality tasks.  We address this gap with Crisscrossed Captions (CxC), an extension of the MS-COCO dataset with human semantic similarity judgments for 267,095 intra- and inter-modality pairs.  We report baseline results on CxC for strong existing unimodal and multimodal models.  Taip pat vertiname daugelį užduočių atliekantį dvigubą kodatorių, apmokytą tiek vaizdo, tiek antraštinės dalies ir antraštinės dalies poromis, kuris iš esmės parodo CxC vertę vertinant mokymosi būdų viduje ir tarpusavio mokymosi įtaką.</abstract_lt>
      <abstract_mn>Ихэнх моделийн аврах сургалтыг дэмжиж, үнэлгээ дэмжиж, зурагт хэвлэх өгөгдлийн сангууд нь үзүүлэх сургалтын тухай гайхалтай хөгжлийг дэмжиж байна. Харамсалтай нь, өгөгдлийн сангууд өөр зурагтай холбогдож байдаг: зураг нь өөр зурагтай холбогдож байхгүй, зураг нь зөвхөн ижил зурагтай холбогдож байдаг, сөрөг холбогдол байхгүй, эерэг хэлбэртэй холбогдол байхгүй. Энэ нь зан чанарын суралцах үйл ажиллагаанд хэрхэн нөлөөлдөг талаар судалгааг бууруулдаг. Бид үүнийг Crisscrossed Captions (CxC), MS-COCO өгөгдлийн санг хүн төрөлхтний зэрэгцээ ижил хэмжээний 267,095 интро-болон интромодал хоёрын хувьд нэмэгдүүлэхэд тусалдаг. Бид CxC-ын үндсэн үр дүнг суурилсан нэг болон олон моделийн загваруудын тулд мэдээлэл өгдөг. Мөн бид олон ажлын хоёр давхар коддогчийг дүгнэж байгаа зураг, дуудлага хэвлэлийн хоёр давхар, CxC-ын үнэ цэнийг интро болон интровертийн суралцааны нөлөөлөлийг хэмжихэд үзүүлдэг.</abstract_mn>
      <abstract_sr>Podržavajući multimodalnu obuku povratka i procjenu, kompleta podataka za snimanje slika izazvala je izuzetan napredak na učenje predstavljanja. Nažalost, podaci imaju ograničene krsnomodalne asocijacije: slike nisu povezani sa drugim slikama, naslovi su povezani samo sa drugim naslovima iste slike, nema negativnih asocijacija i nedostaju pozitivne krsnomodalne asocijacije. To potakne istraživanje o tome kako međumodalno učenje utiče na zadatak unutar modaliteta. Rešavamo taj praznik sa kapcima Crisscrossed (CxC), produženjem kompleta podataka MS-COCO sa osuđivanjima ljudske semantičke sličnosti za 267.095 unutrašnjih i međumodalnih parova. Prijavljamo početne rezultate CxC za jake postojeće unimodalne i multimodalne modele. Takođe procjenjujemo dvostruki koder koji je obučen i na parovima slike i kapcije, koji su ključno pokazivali vrijednost CxC-a za mjerenje utjecaja učenja unutar i međumodaliteta.</abstract_sr>
      <abstract_si>ගොඩක් මොඩාල් ප්‍රධානය සහ විශ්ලේෂණය සඳහා පින්තූර ප්‍රධානය සඳහා පින්තූර ප්‍රධානය සඳහා පින්තූර ප්‍රධ අවාසනාවෙන්න, දත්ත සැට් සම්බන්ධ විශේෂයක් තියෙනවා: පින්තූරය අනිත් පින්තූරය සමග සම්බන්ධ වෙන්නේ නැහැ, පින්තූරය සම්බන්ධ වෙන්න මේක පරීක්ෂණය අඩුවෙනවා කොහොමද අන්තර්භාවිතාවක් ඉගෙන ගන්නේ කියලා, අන්තර්භාවිතාවක් වැඩේ අ අපි මේක ක්‍රිස්ක්‍රොස් කැප්ටන්ස් සමග මෙහෙයුම් කරනවා, MS-COCO දත්ත සැකසුම් සමග මිනිස්සුන්ගේ සැමැන්තික සමාන්‍ය විශ්වාස කරන්න 267,325 intra- සහ අ අපි CxC වලට ප්‍රතිචාර ප්‍රතිචාරයක් තියෙන්නේ බලපොරොත්තු නිමෝඩාල් සහ ගොඩක් මෝඩේල් වලට. අපි වගේම ගොඩක් වැඩි කාර්යයක් අවශ්‍ය කරනවා දුවන් කේප්ටර් එක්ක සහ පින්තූරණය සහ පින්තූර-කේප්ටර් එක්ක දෙන්නම් ප්‍රශ්නය කරලා තියෙන්න</abstract_si>
      <abstract_pl>Dzięki wspieraniu szkoleń i oceny multimodalnego odzyskiwania zbiorów danych dotyczących napisów zdjęć przyczyniły się do niezwykłego postępu w nauce reprezentacji. Niestety zbiory danych mają ograniczone skojarzenia crossmodalne: obrazy nie są sparowane z innymi obrazami, podpisy są sparowane tylko z innymi podpisami tego samego obrazu, nie ma negatywnych skojarzeń i brakuje pozytywnych skojarzeń crossmodalnych. Podważa to badania nad tym, w jaki sposób uczenie się intermodalne wpływa na zadania intermodalne. Rozwiązujemy tę lukę za pomocą Crisscross Captions (CxC), rozszerzenia zbioru danych MS-COCO o ludzkie oceny podobieństwa semantycznego dla par wewnątrz- i intermodalnych 267,095. Raportujemy wyniki bazowe dotyczące CxC dla silnych istniejących modeli unimodalnych i multimodalnych. Oceniamy również wielozadaniowy podwójny koder przeszkolony zarówno na parach obrazu-podpisu, jak i napisu-podpisu, który istotnie demonstruje wartość CxC w mierzeniu wpływu uczenia się wewnątrz- i intermodalnego.</abstract_pl>
      <abstract_no>Med å støtte fleire modal opplæring og evaluering, har biletetatassett ført til merkelige framgang ved å lære representasjonar. Dessverre har datasett begrenset krysmodale tilknytingar: bilete er ikkje saman med andre bilete, tittel er berre saman med andre tittel av det same biletet, det finst ingen negativ tilknytingar og det manglar positiv krysmodale tilknytingar. Dette understrekar forskning på korleis læring av intermodalitet påvirkar inni modalitetsoppgåver. Vi adresserer dette mellomrommet med Crisscrossed Captions (CxC), eit utviding av MS-COCO-dataset med menneskelsemantiske sprøytebruk for 267,095 intra- og mellommodalitetspar. Vi rapporterer baseline resultat på CxC for sterke unimodal og multimodal modeller som finst. Vi evaluerer også ein fleire oppgåver dual koder som treng på både biletet- tittel og tittel- par som viser CxC- verdien for å måle effekten av læring av intra- og mellommodalitet.</abstract_no>
      <abstract_ro>Prin sprijinirea formării și evaluării multimodale de recuperare, seturile de date privind subtitrarea imaginilor au stimulat progrese remarcabile în învățarea reprezentării. Din păcate, seturile de date au asocieri cross-modale limitate: imaginile nu sunt asociate cu alte imagini, subtitrările sunt asociate doar cu alte subtitrări ale aceleiași imagini, nu există asocieri negative și lipsesc asocieri cross-modale pozitive. Acest lucru subminează cercetarea privind impactul învățării intermodale asupra sarcinilor intramodale. Abordăm acest decalaj cu Crisscrossed Captions (CxC), o extensie a setului de date MS-COCO cu judecăți de similitudine semantică umană pentru 267.095 perechi intra- și intermodalitate. Raportăm rezultatele referitoare la CxC pentru modele unimodale și multimodale puternice existente. Evaluăm, de asemenea, un encoder dual multitask instruit atât pe perechile imagine-captare, cât și pe perechile captare-captare, care demonstrează esențial valoarea CxC pentru măsurarea influenței învățării intra-și intermodale.</abstract_ro>
      <abstract_so>Kaalmeynta waxbarashada dib u qaadashada iyo qiimeynta, sawirka la qabsashada macluumaadka waxaa soo baxay horumar la yaab leh oo ku saabsan waxbarashada la barto. Unfortunately, datasets have limited cross-modal associations: images are not paired with other images, captions are only paired with other captions of the same image, there are no negative associations and there are missing positive cross-modal associations.  Waxbarashadan waxaa hoos looga baaraandegayaa sida waxbarashada hab-dhexe ay u saameyso shaqada qaabka ah. Waxaannu ka sheekeysanaynaa goobahan Crisscross Captions (CxC), extension of the MS-CO dataset with humane semantic similarity for 267,095 intra- and inter-modality pairs. Waxaan wargelinaynaa resultiyada aasaasiga ah ee CxC ku saabsan tusaalooyin aad u xoog badan oo ay ku jiraan noocyo badan. Sidoo kale waxaynu qiimeynaynaa qodobka labada qodob ee loo baray sawirka iyo labada nooc oo ay si muhiim ah u muujiyaan qiimaha CxC si loo qiyaaso saameynta barashada xilliga iyo barashada xilliga dhexe.</abstract_so>
      <abstract_ta>பல- மாற்று மீட்டெடுப்பு பயிற்சி மற்றும் மதிப்பீட்டை ஆதரிக்கையால், பிம்பம் பிடிப்பு தகவல் பெறுதல் தகவல் அமைப்புகள் பிரத துரதிர்ஷ்டவசமாக, தகவல் அமைப்புகள் கிடைக்கப்பட்டுள்ளது: பிம்பங்கள் மற்ற படங்களுடன் இணைக்கப்படவில்லை, தலைப்புகள் மட்டும் ஒரே பிம்பத்தின் மற்ற தலைப்புகளுடன் ஜோ இந்த ஆராய்ச்சியை குறைவாக்குகிறது இடைமுறையில் கற்றுக்கொள்ளும் வகையில் உள்ள முறைமையான பணிகளை எவ்வா @ info நாம் அடிப்படைக்கோடு முடிவு பிம்பத்தின் தலைப்புகள் மற்றும் தலைப்புகள் ஜோடியிலும் பயிற்சி செய்யப்பட்ட பல்திருக்கும் இரட்டை குறியீட்டை மதிப்பிடுகிறோம். இது CxC விளைவு</abstract_ta>
      <abstract_sv>Genom att stﾃｶdja multimodal hﾃ､mtning utbildning och utvﾃ､rdering har bildtextningsdatauppsﾃ､ttningar sporrat anmﾃ､rkningsvﾃ､rda framsteg nﾃ､r det gﾃ､ller representationsinlﾃ､rning. Tyvﾃ､rr har datauppsﾃ､ttningar begrﾃ､nsade tvﾃ､rmodala associationer: bilder paras inte ihop med andra bilder, bildtexter paras bara ihop med andra bildtexter av samma bild, det finns inga negativa associationer och det saknas positiva tvﾃ､rmodala associationer. Detta undergrﾃ､ver forskningen om hur intermodalt lﾃ､rande pﾃ･verkar intramodalitetsuppgifter. Vi tar itu med detta gap med Crisscrossed Captions (CxC), en fﾃｶrlﾃ､ngning av MS-COCO datauppsﾃ､ttningen med mﾃ､nskliga semantiska likheter bedﾃｶmningar fﾃｶr 267 095 intra- och intermodalitetspar. Vi rapporterar basresultat pﾃ･ CxC fﾃｶr starka befintliga unimodala och multimodala modeller. Vi utvﾃ､rderar ocksﾃ･ en multitask dual encoder utbildad pﾃ･ bﾃ･de bild-bildtext och bildtext-bildtext par som avgﾃｶrande visar CxC:s vﾃ､rde fﾃｶr att mﾃ､ta pﾃ･verkan av intra- och intermodalitetslﾃ､rande.</abstract_sv>
      <abstract_ur>بہت سی موڈال پھیرنے کی تعلیم اور ارزیابی کی مدد کے ذریعہ، تصویر کاپٹینگ ڈیٹ سٹ کے ذریعہ نمایش کی تعلیم کے بارے میں بہت اچھا پیشرفت ہے. بدبختی ہے، ڈاٹ سٹ کے لئے مقدار کرس موڈال اتصال ہیں: تصاویر دوسرے تصاویروں کے ساتھ جوڑ نہیں ہوتے، کپٹ صرف ایک تصویر کے دوسرے کپٹ کے ساتھ جوڑ کئے جاتے ہیں، کوئی منفی اتصال نہیں ہے اور کوئی مثبت کرس موڈال اتصال نہیں ہوتے۔ یہ تحقیقات کے ذریعہ تحقیقات کو کمزور کرتا ہے کہ درمیان موڈلی تعلیم کس طرح موڈلی کے کاموں پر اثر دیتا ہے۔ ہم اس فاصلہ کو کریسسکروس کی کاپیشن (CxC) کے ساتھ سمجھتے ہیں، ایک مئس-COCO ڈیٹ سٹ کے مطابق 267,095 داخل اور درمیان موڈلیٹی جوڑوں کے ساتھ انسان کی سیمانٹی برابری کا فیصلہ کرنا ہے. ہم CxC پر بنیاس لین نتائج راپورٹ کر رہے ہیں مضبوط موجود غیر موڈال اور متعدم موڈل موڈل کے لئے۔ ہم نے بھی ایک multitask دوئل کوڈر کا ارزش کیا ہے جو تصویر-کپشن اور کپشن-کپشن جوڑوں پر آموزش کی جاتی ہے جو CxC کے ارزش کو دکھاتا ہے کہ اس کے اندر اور درمیان موڈلیت کی تعلیم کے مطابق اندازے کے لئے۔</abstract_ur>
      <abstract_vi>Bằng cách hỗ trợ đào tạo và đánh giá nhiều phương tiện khác nhau, chương trình đánh giá ảnh đã thúc đẩy một tiến bộ đáng chú ý về học quyền đại diện. Không may, các tập tin có giới hạn các cách mạng: hình ảnh không kết hợp với các ảnh khác, hình ảnh chỉ kết hợp với các chỉ dẫn khác của cùng một hình ảnh, không có liên quan tiêu cực nào và thiếu các liên quan tiêu biểu tích cực. Việc này ảnh hưởng đến cách học nội động. Chúng ta giải quyết vấn đề này bằng các Bắt tại giải mã, một phần mở rộng dữ liệu của xơ rải rác CO với tỉ lệ giống nhau cơ bản chung với giá 27,095 phân loại và liên cách. Chúng tôi báo cáo kết quả sơ bộ về CxC cho những mô- đun và đa phương. Chúng tôi cũng đánh giá một bộ mã hóa kép đa nhiệm được huấn luyện về cả hai cặp chụp ảnh và chụp ảnh mà cho thấy giá trị của C5C với việc đo lường ảnh hưởng của học tập nội giao và nội giao.</abstract_vi>
      <abstract_uz>Koʻproq modulni olib tashlash va qiymatni qoʻllashda, rasm olish va maʼlumot etish tugmasini taʼminlashda ajoyib muvaffaqiyatsiz qoʻlladi. Uzunasiz, maʼlumotlar tarkibi boshqa modal aloqalarida chegara boʻlgan. Rasmlar boshqa rasmlar bilan bir xil boʻlmaydi, sarlavhaslar faqat bitta rasmning boshqa sarlavhasi bilan bir nechta boʻlmaydi, negativ bogʻliqlar mavjud emas va joylashtirish usuli yoʻq. Bu o'rganishni o'rganishning har xil usuli vazifalarini qanday qilishini yaratadi. @ info Biz muvaffaqiyatli unimodal va multimodal modellari uchun CxC asosiy natijalari haqida xabar beramiz. Biz sur'ning sarlavhasi va sarlavhasining ikki marta o'rganilgan multitask kodlash qoidasini qiymatimiz. Bu yerda CxC qiymatini ko'rsatadi.</abstract_uz>
      <abstract_nl>Door multimodale retrieval training en evaluatie te ondersteunen, hebben beeldbijschriftdatasets opmerkelijke vooruitgang op het gebied van representatie learning gestimuleerd. Helaas hebben datasets beperkte crossmodale associaties: afbeeldingen worden niet gekoppeld aan andere afbeeldingen, bijschriften worden alleen gekoppeld aan andere bijschriften van dezelfde afbeelding, er zijn geen negatieve associaties en er ontbreken positieve crossmodale associaties. Dit ondermijnt het onderzoek naar de invloed van intermodaal leren op intramodale taken. We verhelpen deze kloof met Crisscrossed Captions (CxC), een uitbreiding van de MS-COCO dataset met humane semantische vergelijkingsoordelen voor 267,095 intra- en intermodaliteitsparen. We rapporteren baseline resultaten op CxC voor sterke bestaande unimodale en multimodale modellen. We evalueren ook een multitask dual encoder die getraind is op zowel beeld-bijschrift als bijschrift-bijschrift paren die de waarde van CxC voor het meten van de invloed van intra- en intermodaliteit leren cruciaal aantoont.</abstract_nl>
      <abstract_bg>Подкрепяйки обучението и оценката за мултимодално извличане, наборите от данни за надписи на изображения стимулират забележителен напредък в обучението за представяне. За съжаление, наборите от данни имат ограничени междумодални асоциации: изображенията не са сдвоени с други изображения, надписите се сдвояват само с други надписи на едно и също изображение, няма отрицателни асоциации и липсват положителни междумодални асоциации. Това подкопава изследванията за това как интермодалното обучение оказва влияние върху задачите в рамките на модалността. Ние разглеждаме тази празнота с Крискросed Captions (CxC), разширение на набор от данни MS-COCO с преценки за човешка семантична прилика за 267 095 интрамодални и интермодални двойки. Докладваме базови резултати за СхС за силни съществуващи унимодални и мултимодални модели. Също така оценяваме многозадачен двоен кодер, обучен както върху двойки изображение-надпис, така и върху надпис-надпис, който демонстрира съществено стойността на CxC за измерване на влиянието на интрамодалното и интермодалното обучение.</abstract_bg>
      <abstract_id>Dengan mendukung pelatihan dan evaluasi retrieval multi modal, set data captioning gambar telah mendorong kemajuan yang luar biasa dalam pembelajaran representation. Sayangnya, dataset memiliki asosiasi transmodal terbatas: gambar tidak berpasangan dengan gambar lain, captions hanya berpasangan dengan captions lain dari gambar yang sama, tidak ada asosiasi negatif dan tidak ada asosiasi transmodal positif. Ini merusak penelitian bagaimana intermodalitas belajar mempengaruhi tugas intramodalitas. Kami mengatasi ruang ini dengan Crisscrossed Captions (CxC), sebuah ekstensi dari dataset MS-COCO dengan penghakiman semantis persamaan manusia untuk 267.095 pasangan intra- dan intermodalitas. Kami melaporkan hasil dasar pada CxC untuk model unimodal dan multimodal yang kuat. Kami juga mengevaluasi koder dual multitask yang dilatih pada pasangan image-caption dan caption-caption yang paling penting menunjukkan nilai CxC untuk mengukur pengaruh belajar intra- dan intermodalitas.</abstract_id>
      <abstract_de>Durch die Unterst체tzung multimodaler Retrieval-Schulungen und -Evaluierungen haben Bildunterschriftsdatens채tze bemerkenswerte Fortschritte beim Lernen von Repr채sentationen angeregt. Leider haben Datens채tze begrenzte crossmodale Assoziationen: Bilder werden nicht mit anderen Bildern gepaart, Beschriftungen werden nur mit anderen Beschriftungen des gleichen Bildes gepaart, es gibt keine negativen Assoziationen und es fehlen positive crossmodale Assoziationen. Dies untergr채bt die Erforschung, wie sich intermodales Lernen auf intramodale Aufgaben auswirkt. Wir schlie횩en diese L체cke mit Crisscrossed Captions (CxC), einer Erweiterung des MS-COCO Datensatzes mit humanen semantischen 횆hnlichkeitsurteilen f체r 267,095 intra- und intermodale Paare. Wir berichten Baseline-Ergebnisse zu CxC f체r starke bestehende unimodale und multimodale Modelle. Wir evaluieren auch einen Multitask-Dual-Encoder, der sowohl f체r Bild-Untertitel- als auch f체r Untertitel-Untertitel-Paare trainiert ist, der den Wert von CxC f체r die Messung des Einflusses von intra- und intermodalem Lernen entscheidend demonstriert.</abstract_de>
      <abstract_da>Ved at understøtte multimodal træning og evaluering har datasæt med billedtekster fremmet bemærkelsesværdige fremskridt med repræsentationslæring. Desværre har datasæt begrænsede tværmodale associationer: Billeder parres ikke med andre billeder, billedtekster parres kun med andre billedtekster af det samme billede, der er ingen negative associationer, og der mangler positive tværmodale associationer. Dette underminerer forskningen i, hvordan intermodal læring påvirker intramodalitetsopgaver. Vi løser dette hul med Crisscrossed Captions (CxC), en udvidelse af MS-COCO datasættet med menneskelige semantiske lighedsvurderinger for 267.095 intra- og intermodalitetspar. Vi rapporterer baseline resultater på CxC for stærke eksisterende unimodale og multimodale modeller. Vi evaluerer også en multi-task dual encoder trænet på både billede-billedtekst og billedtekst-billedtekst par, der afgørende demonstrerer CxC's værdi for at måle indflydelsen af intra- og intermodalitet læring.</abstract_da>
      <abstract_hr>Podržavajući višemodalnu obuku i procjenu povratka, podaci podataka za snimanje slika izazvali su izuzetan napredak na učenju predstavljanja. Nažalost, podaci imaju ograničene krsnomodalne asocijacije: slike se ne povezuju s drugim slikama, naslovi se povezuju samo sa drugim naslovima iste slike, nema negativnih asocijacija i nedostaju pozitivne krsnomodalne asocijacije. To potiče istraživanje o tome kako međumodalno učenje utječe na zadatke unutar modaliteta. Ovaj prasak se obrađujemo s kapcima Crisscrossed (CxC), produženjem kompleta podataka MS-COCO-a s osuđivanjima ljudske semantičke sličnosti za 267.095 unutar i međumodalnih parova. Prijavljujemo početne rezultate CxC za jake postojeće unimodalne i multimodalne modele. Također procjenjujemo dvostruki koder koji je obučen na parovima slike i kapcije, koji su ključno pokazivali vrijednost CxC-a za mjerenje utjecaja učenja unutar i međumodaliteta.</abstract_hr>
      <abstract_fa>با پشتیبانی آموزش و ارزیابی بسیاری از مدل‌ها، مجموعه‌های داده‌های عنوان تصویر، پیشرفت فوق‌العاده در مورد یادگیری نمایش‌دهندگان را پیشنهاد داده است. متاسفانه، مجموعه‌های داده‌ها ارتباط‌های متفاوتی محدود دارند: تصویر با تصویر دیگر جفت نمی‌شوند، عنوان فقط با عنوان دیگر از یک تصویر جفت می‌شوند، ارتباط منفی وجود ندارد و ارتباط‌های متفاوتی مثبت وجود ندارد. این تحقیقات درباره اینکه یادگیری بین‌modalities چگونه بر وظیفه‌های بین‌modalities اثر می‌دهد. ما این فاصله را با کاپیتان‌های کریسسکروس (CxC) دریافت می‌کنیم، وسیله‌ای از مجموعه داده‌های MS-COCO با قضاوت‌های شبیه‌انگیز انسان برای جفت‌های ۲۶۷،095 درون و بین‌اندازی. ما نتیجه‌های پایه‌خط بر CxC گزارش می‌کنیم برای مدل‌های بی‌modal و multimodal قوی وجود دارد. ما همچنین یک قالب دوگانه‌ی چندین کار را ارزیابی می‌کنیم که روی جفت‌های عنوان تصویر و عنوان عنوان عنوان عنوان تصویر آموزش یافته شده است که به طور کلی ارزش CxC را برای اندازه‌گیری تاثیر یادگیری داخل و متوسط موادی نشان می‌دهد.</abstract_fa>
      <abstract_af>Deur te ondersteun multimodaal ontvang onderwerking en evaluering, het beeldtitelsette datastelle betekende vordering gevra op voorstelling leer. Ongelukkig, datastelle het beperk kruismodale assosiasies: beelde word nie paired met ander beelde nie, titels word slegs paired met ander titels van dieselfde beeld, daar is geen negatiewe assosiasies en daar is ontbreek positiewe kruismodale assosiasies. Hierdie onderwerp onderwerp onder die onderwerp na hoe intermodaliteit leer invloek intra-modaliteit-opdragte. Ons adres hierdie gap met Crisscrossed Captions (CxC), ân uitbreiding van die MS- COCO datastel met menslike semantiese gelykenis oordelings vir 267,095 intra- en intermodaliteit paar. Ons rapporteer basisline resultate op CxC vir sterk bestaande unimodale en multimodale modele. Ons evalueer ook 'n veelvuldige taak tweede enkoder wat op beeld-titel en caption-caption-paire opgelei is wat kruistelik CxC se waarde wys om die influens van intra- en intermodaliteit-leer te maak.</abstract_af>
      <abstract_tr>Çoklu modal gaýd etmek we deňlenmek üçin resim küpşenleri taýýarlamak üçin aýratyn öwrenmek üçin aýratyn ösümligi bellendirdi. Gynansakda, veri düzümleri çykaryp modal baglaýyşlar bardyr: suratlar başga suratlar bilen baglanmaýar, käpşenler diňe ayn suratyň başga käpşenleri bilen baglanmaýar, hiç hili täsir baglaýyşlar ýok we ymyk modal baglaýyşlar ýok. Bu modalitet öwrenmeniň modalitet zadynyň içine nähili täsiri täsirini azaltýar. Biz bu gaplary Crisscrossed Captions (CxC) ile çözeriz, MS-COCO veri setiniň 267,095 intra- we modalitet çiftleri üçin bir döwletlere golaýlaşýarys. Biz bu ýerde bolan unimodal we multimodal nusgalar üçin CxC barada esasy netijeleri baglaýarys. Biz hem surat-başlığı hem de kelime-başlığı üzerinde eğitilen bir çoklu işaretli çizgini değerlendiriyoruz. Bu şekilde CxC'nin intra ve modalitet öğrenmesinin etkisini ölçüde değerlendirmek üçin önemli bir şekilde gösteriyor.</abstract_tr>
      <abstract_sq>Duke mbështetur trajnimin dhe vlerësimin e tërheqjes multimodale, grupet e të dhënave të titullimit të imazheve kanë nxitur përparim të shquar në mësimin e përfaqësimit. Fatkeqësisht, grupet e të dhënave kanë shoqata të kufizuara ndërmodali: imazhet nuk janë të barabarta me imazhe të tjera, titujt janë të barabarta vetëm me titujt e tjerë të të njëjtit imazh, nuk ka shoqata negative dhe mungon shoqata pozitive ndërmodali. Kjo dëmton kërkimin se si mësimi ndërmodalitet ndikon në detyrat brenda modalitetit. Ne e trajtojmë këtë boshllëk me Captions Crisscrossed (CxC), një zgjerim të dataset MS-COCO me gjykime njerëzore semantike të ngjashmërisë për 267,095 çifte brenda dhe ndërmodaliteti. Ne raportojmë rezultatet bazë në CxC për modele të forta ekzistuese unimodale dhe multimodale. We also evaluate a multitask dual encoder trained on both image-caption and caption-caption pairs that crucially demonstrates CxC's value for measuring the influence of intra- and inter-modality learning.</abstract_sq>
      <abstract_am>በብዙ-modal ማስተማር እና ማስታወቂያውን በመደጋገም ምስል ማቀናጃ የዳታ ማቀናጃ ማስታወቂያውን በመግለጥ የሚያስደንቅ ግንኙነት አግኝቷል፡፡ በመከፋት፣ የዳታ ሰርቨሮች የክፍለ ሥርዓት ማኅበረሰብ ነው፤ ምስሎች ከሌሎች ምስሎች ጋር አይጋጠሙም፣ ምርጫዎች ግን በአንድ ምስል ላይ በተለያዩ አርእስቶች ብቻ ናቸው፣ ምንም የnegative ማኅበረሰብ የለውም፣ እናም የክፍለ ሥልጣን-ሞዴል ማኅበረሰቦች አይጎድሉም፡፡ ይሄ ትምህርት በሥልጣናዊ ትምህርት እንዴት በሥርዓት ላይ የሚደርስ ነው በማድረግ ያሳድጋል፡፡ ይህንን ክፍል በክሮስcross አርእስት (CxC)፣ የMS-CO ዳታዎችን ለመዘርጋት በሰው የsemantic ብጤነት ፍርድ ለ267,095 በ.አ እና በ. የCxC ውጤቶች በጽኑ unimodal እና multimodal ሞዴል ላይ እናስባለን፡፡ የምስል አርእስት እና የCxC ዋጋውን በመለኪያ እና በሥርዓት ትምህርት ላይ በሚያስፈልገው የሁለት ብልቲካባቢ የፊደል ኮድዶችን እናሳውቃለን፡፡</abstract_am>
      <abstract_hy>By supporting multi-modal retrieval training and evaluation, image captioning datasets have spurred remarkable progress on representation learning.  Դժբախտաբար, տվյալների համակարգերը սահմանափակ խաչմոդալ կապեր ունեն. պատկերները չեն զուգավորվում այլ պատկերների հետ, վերնագրերը միայն զուգավորվում են նույն պատկերի այլ վերնագրերի հետ, չկան բացասական կապեր և բացակայում են դրական խաչմոդալ կապեր: Սա նվազեցնում է հետազոտությունը, թե ինչպես է միջ-մոդեալ ուսումնասիրությունը ազդում միջ-մոդեալ խնդիրների վրա: Մենք լուծում ենք այս տարբերությունը Քրիսխրոսսփեսի (CxC) կառուցվածքների միջոցով, ՄՍ-ԿՕԿՕ տվյալների համակարգի ընդլայնումը մարդկային սեմանտիկ նմանության դատողություններով 267,085 ինտեր- և միջ-մոդեալ զույգերի համար: Մենք տեղեկացնում ենք CxC-ի հիմնական արդյունքները գոյություն ունեցող ուժեղ միամոդալ և բազմամոդալ մոդելների համար: Մենք նաև գնահատում ենք բազմախնդիրների երկու կոդեր, որը կրթություն է ստացվում և պատկերի-վերլուծության, և վերլուծության-վերլուծության զույգերի վրա, որոնք հիմնականում ցույց են տալիս CxC-ի արժեքը ինտեր- և միջ-մոդելային ուսումնասիրության ազդեցության չափման համար:</abstract_hy>
      <abstract_ko>다중모드 검색 교육과 평가를 지원함으로써 이미지 자막 데이터 집합은 표징 학습에 현저한 진전을 거두었다.불행하게도 데이터 집합의 크로스모드 관련은 유한하다. 이미지는 다른 이미지와 어울리지 않고 자막은 같은 이미지의 다른 자막과 어울리지 않으며 부정적인 관련도 없고 긍정적인 크로스모드 관련도 부족하다.이것은 크로스모드 학습이 크로스모드 임무에 어떻게 영향을 미치는지에 대한 연구를 파괴했다.우리는 교차자막(CxC)으로 이 차이를 해결했다. 이것은 MS-COCO 데이터 집합의 확장이고 267095개의 모드 내와 모드 간에 대한 인류의 의미 유사성 판단을 가지고 있다.우리는 기존의 단봉과 다봉 모델의 CxC 기선 결과를 보고했다.우리는 또한 이미지 제목과 제목 제목이 훈련에 대한 다중 임무 듀얼 인코더를 평가했는데, 이것은 CxC가 모드와 모드 사이의 학습에 미치는 영향을 측정하는 데 있어서의 가치를 매우 중요하게 증명했다.</abstract_ko>
      <abstract_sw>Kwa kuunga mkono mafunzo na kutathmini mafunzo ya upatikanaji wa mitandao mbalimbali, seti za picha zinazoonyesha maendeleo mazuri kuhusu kujifunza kwa uwakilishi. Kwa bahati mbaya, seti za taarifa zimekuwa vizuizi vikubwa vya vyama vya upande: picha hazijaunganishwa na picha nyingine, vichwa vyao vinajungwa na vichwa vingine vya picha hiyo, hakuna vyama vya hasi na hakuna vyama vyenye vyama vya upande chanya. Hii inapunguza utafiti wa namna ya kujifunza kwa namna ya aina tofauti inavyoathiri kazi za utamaduni. Tunaongelea gaidi hili na vichwa vya habari vya Crisscross (CxC), kuongezeka kwa taarifa za MS-CO zenye maamuzi yanayofanana na wanadamu 267,095 kwa wanaume wa ndani na tofauti. Tunatoa taarifa za matokeo ya msingi kwenye CxC kwa mifano yenye nguvu ya kipekee na mifano mingi. Kadhalika tunatathmini kodi ya mara mbili inayofundishwa kwa picha pamoja na viwili vya vichwa vya habari ambavyo vinaonyesha thamani ya CxC kwa kupima ushawishi wa kujifunza kwa njia za ndani na kwa njia nyingine.</abstract_sw>
      <abstract_bn>মাল্টিমোডাল পুনরুদ্ধার প্রশিক্ষণ এবং মূল্যায়নের সমর্থন দিয়ে ছবির ক্যাপ্টেশন ডাটাসেটের প্রতিনিধিত্ব শিক্ষা নিয়ে চম দুর্ভাগ্যবশত, ডাটাসেট সীমিত ক্রস-মোডাল সংস্থা: ছবিগুলো অন্যান্য ছবির সাথে জোড়া করা হয়নি, শিরোনাম শুধুমাত্র একই ছবির অন্যান্য শিরোনাম, কোন ন নেতিব কিভাবে মোডিয়াল শিক্ষা বিভিন্ন মোডিয়াল কাজের উপর প্রভাব ফেলে দেয়া হয়েছে। ক্রিস্ক্রোস ক্রোস শিরোনামের (সিক্সিসি) দ্বারা আমরা এই বিভ্রান্ত বিষয়টিকে আলোচনা করি। এমএস- কমো ডাটাসেট বিস্তারিত বিস্তারিত মানুষের সামান আমরা বেসেলাইনের ফলাফল সিক্সিতে রিপোর্ট করি বিদ্যমান ইউনিমোডাল এবং বহুটিমোডাল মডেলের জন্য। আমরা একই সাথে চিত্রের শিরোনাম এবং শিরোনামের শিরোনামের দুইবার কোডার প্রশিক্ষিত একটি মাল্টিটিক্যাডারের মূল্য মূল্য দেখাচ্ছি যা প্রকাশিত সিক্সির</abstract_bn>
      <abstract_bs>Podržavajući multimodalnu obuku i procjenu povratka, podaci podataka za snimanje slika izazvali su izuzetan napredak na učenju predstavljanja. Nažalost, podaci imaju ograničene krsnomodalne asocijacije: slike se ne povezuju sa drugim slikama, kapcije se povezuju samo sa drugim kapcima iste slike, nema negativnih asocijacija i nedostaju pozitivne krsnomodalne asocijacije. To podnosi istraživanje o tome kako međumodalno učenje utječe na zadatke unutar modaliteta. Mi obraćamo taj praznik sa kapcima Crisscrossed (CxC), produženjem kompleta podataka MS-COCO sa osuđivanjima ljudske semantičke sličnosti za 267.095 unutar i međumodalitetskih parova. Prijavljamo početne rezultate CxC za jake postojeće unimodalne i multimodalne modele. Također procjenjujemo dvostruki koder koji je obučen na parovima slike i kapcije, koji su ključno pokazivali vrijednost CxC-a za mjerenje utjecaja učenja unutar i međumodaliteta.</abstract_bs>
      <abstract_ca>Sostenint formació i evaluació multimodals de recuperació, els conjunts de dades de capturació d'imatges han impulsat un progrés notable en l'aprenentatge de representació. Malauradament, els conjunts de dades tenen associacions transmodals limitades: les imatges no estan parellades amb altres imatges, els títulos només estan parellats amb altres títulos de la mateixa imatge, no hi ha associacions negatives i falten associacions transmodals positives. Això socava la recerca sobre com l'aprenentatge intermodalitat afecta les tasques intramodalitats. Afectem aquesta diferència amb Crisscrossed Captions (CxC), una extensió del conjunt de dades MS-COCO amb judicis semàntics de similitud humana per 267.095 parells intra- i inter-modalitat. Informem els resultats basals de la CxC per a models unimodals i multimodals importants. També evaluem un codificador doble multitasca entrenat en parelles imatge-títol i títol-títol que demostren crucialment el valor de CxC per mesurar l'influència de l'aprenentatge intra- i intermodalitat.</abstract_ca>
      <abstract_az>Çoxlu modal alma təcrübəsini və değerlendirməyi dəstəkləndirək, görüntü başlıqları verilən qurğuları göstəricisi öyrənməsi barəsində möhtərəm təcrübə göstərməsini tələb etdi. Necə olaraq, veri qurğuları çox modal bağlantıları var: görüntülər başqa şəkillərlə bağlanılmaz, başlıqlar yalnız aynı suretin başqa başlıqları ilə bağlanılır, negatif bağlantılar yoxdur və pozitif çox modal bağlantılar yoxdur. Bu, modaliyyət öyrənməsinin modaliyyət işlərinin necə etkisi olduğunu araştırmağı zəifləyir. Biz bu boşluğu Crisscrossed Captions (CxC) ilə çəkirik, MS-COCO veri qutusu 267,095 intra-modalitet çiftləri üçün insan semantik bənzər hökmləri ilə uzaqlaşdırılır. Biz CxC ilə əsas səhifələrin sonuçlarını çox modal və çox modal modellər üçün xəbər veririk. Biz həmçinin CxC'nin qiymətini intra-modalitat öyrənməsinin təsirini ölçüyə görə bilinmiş çoxlu işin dual kodlayıcıs ını də təhsil edirik.</abstract_az>
      <abstract_fi>Tukemalla multimodaalista hakukoulutusta ja -arviointia kuvatekstitysten aineistot ovat edistäneet huomattavaa edistystä edustuksen oppimisessa. Valitettavasti aineistoilla on rajallisia multimodaalisia yhteyksiä: kuvia ei pariteta muiden kuvien kanssa, tekstityksiä paritetaan vain saman kuvan muiden tekstitysten kanssa, negatiivisia yhteyksiä ei ole ja positiivisia multimodaalisia yhteyksiä puuttuu. Tämä heikentää tutkimusta siitä, miten intermodaalisuus-oppiminen vaikuttaa intermodaalisuuteen liittyviin tehtäviin. Korjaamme tätä aukkoa Crisscrossed Captions (CxC), MS-COCO-aineiston laajennus ihmisen semanttisen samankaltaisuuden arvioinnilla 267 095 intra- ja intermodaalisuusparille. Raportoimme CxC:n lähtötason tulokset vahvojen olemassa olevien unimodaalisten ja multimodaalisten mallien osalta. Arvioimme myös monikäyttöistä kaksoiskoodausta, joka on koulutettu sekä kuva-kuvateksti- että kuvateksti-kuvapariin, joka osoittaa ratkaisevasti CxC:n arvon intra- ja intermodaalisuuden oppimisen vaikutuksen mittaamisessa.</abstract_fi>
      <abstract_cs>Podporou multimodálního vyhledávání školení a vyhodnocování obrazových titulků podněcovaly pozoruhodný pokrok ve vzdělávání reprezentace. Bohužel datové sady mají omezené crossmodální asociace: obrázky nejsou spárovány s jinými obrázky, titulky jsou spárovány pouze s jinými titulky stejného obrázku, neexistují žádné negativní asociace a chybí zde kladné crossmodální asociace. To podkopává výzkum toho, jak intermodální učení ovlivňuje intramodální úkoly. Tuto mezeru řešíme pomocí Crisscrossed Titions (CxC), rozšíření MS-COCO datové sady o lidské sémantické podobnosti pro 267,095 intra- a intermodalitní páry. Podáváme základní výsledky CxC pro silné existující unimodální a multimodální modely. Vyhodnocujeme také multiúlohový duální snímač trénovaný jak na párech obrázků, tak na párech titulků, který zásadně demonstruje hodnotu CxC pro měření vlivu intra- a intermodálního učení.</abstract_cs>
      <abstract_et>Mitmeliigilise päringu koolituse ja hindamise toetamisega on piltide pealdiste andmekogumid andnud märkimisväärseid edusamme esindusõppes. Kahjuks on andmekogumitel piiratud modaalseid seoseid: pilte ei paarita teiste piltidega, pealdisi paaritatakse ainult teiste sama pildi pealdistega, negatiivseid seoseid puudub ja positiivseid modaalseid seoseid puudub. See õõnestab teadusuuringuid selle kohta, kuidas intermodaalne õpe mõjutab modaalsusesiseseid ülesandeid. Selle lünga lahendame Crisscrossed Captions (CxC), mis on MS-COCO andmekogumi laiendus inimese semantilise sarnasuse otsustega 267 095 modaalsusesisese ja intermodaalsuse paari kohta. Teatame CxC baastulemustest tugevate olemasolevate ühe- ja mitmeliigiliste mudelite puhul. Samuti hindame mitmeülesandelist kahekordset kodeerijat, mis on koolitatud nii pildi-pealdise kui ka pealdise-pealdise paari jaoks, mis näitab oluliselt CxC väärtust intramodaalsuse ja intermodaalsuse õppe mõju mõõtmisel.</abstract_et>
      <abstract_sk>S podporo večmodalnemu usposabljanju in vrednotenju pridobivanja podatkov o napisovanju slik so nabori podatkov o napisovanju slik spodbudili izjemen napredek pri učenju reprezentacije. Na žalost imajo nabori podatkov omejene medmodalne povezave: slike niso združene z drugimi slikami, napisi so združeni samo z drugimi napisi iste slike, negativnih povezav ni in manjkajo pozitivne medmodalne povezave. To spodkopava raziskave o tem, kako intermodalno učenje vpliva na naloge znotraj modalnosti. To vrzel obravnavamo s Crisscrossed Captions (CxC), razširitvijo podatkovnega nabora MS-COCO s sodbami človeške semantične podobnosti za 267.095 intra- in intermodalnih parov. Poročamo osnovne rezultate CxC za močne obstoječe unimodalne in multimodalne modele. Ocenjujemo tudi večopravilni dvojni kodirnik, usposobljen za pare slike-napis in napis-napis, ki ključno dokazuje vrednost CxC za merjenje vpliva znotraj- in intermodalnega učenja.</abstract_sk>
      <abstract_ha>Yi ƙarfafa da tsarin mai motsi na multi-modal, don ka sami tsarin zane da zane-zane masu motsi, sun nuna taƙaitacce mai girma a kan lõkaci na halartawa. Babu rabo, tsarin database sun ƙayyade associations masu tsohon-tsohon mutane: ba za'a haɗa zane da wasu zane ba, tsohon sunayen su sau da wasu sunayen sunayen zane da shi kawai, ba za'a sãmu da associations masu motsi kuma ba za'a sava associations masu son sura-modal. Wannan yana ƙaranci research a kan yadda learning a cikin-moda ke shagala taskõkin-moda. Tuna address wannan gap da aka yi Kriskrysset Titanin (CxC), an yalwato wa MA-CO dataset da mutane na semantic misãli wa 267,095 guda da-moda. Muna buga wani matsalar da ke ƙaranci a kan CxC wa misãlai masu ƙaranci da ke gaba da kwamfyuta masu ƙarfi. Tuna ƙaddara koden dubu wanda aka yi wa wa kodi na zane da sunan-zane da wasu mutane, da za'a nuna kimar CxC da muhimu a ƙayyade muhimmin sha'anin tsarin da za'a karanta cikin-da-kiyaye.</abstract_ha>
      <abstract_jv>politenessoffpolite"), and when there is a change ("assertivepoliteness item-set This undermines R&amp;R&amp;R&amp;I;S.A.: Awak dhéwé jagat iki karo krisskros Captions Awak dhéwé éntuk sistem sing dadi CxC nggo gambar sistem Unimodal lan model multimodal. Awak dhéwé éntukno sistem multitask duplikasi nggawe lan sesilèh-caption lan caption-caption-caption kuwi nggawe nyimpen CxC nggawe barang nggawe gerakan kanggo mehurakno kanggo ngilanggar nggambar luwih dumateng.</abstract_jv>
      <abstract_bo>སྣ་མང་ཅིག་གི་ལྟ་རྒྱབ་སྐྱོར་དང་གནད་དོན་རིམ་ལ་རྒྱབ་སྐྱོར་བྱེད་ཀྱི་ཡོད་པ། གཟུགས ཡིན་ནའང་། གནད་སྡུད་གཞུང་སྒྲིག་ཚུ་ལ་ཆུང་བ་བཟོས་མེད་པ་དང་བརྙན་རིས་གཞན་དང་བསྡོམས་མེད་པ། པར་རིས་འདི་གཅིག་མཚུངས་ཀྱི་པར འདིས་གནས་ཚུལ་བྱ་རིམ་གྱི་དབྱེ་རིམ་གྱི་ཐབས་ལམ་ནང་གི་ལས་འཚོལ་ཞིབ་བྱེད་ཀྱི་ཡོད། We address this gap with Crisscrossed Captions (CxC), an extension of the MS-COCO dataset with human semantic similarity judgments for 267,095 intra- and inter-modality pairs. ང་ཚོས་གནས་ཡུལ་མྱུར་བའི་དབྱིབས་མཐུན་ཅན་དང་སྣ་མང་མཐུན་པའི་མིག་དཔེ་བྱེད་པར་གཞི་རྟེན་འབྲེལ་བ་ཡིན། ང་ཚོས་བརྙན་རིས་དང་བགོ་བྲིས་ཁང་གཉིས་ཀྱི་ལས་འགུལ་གྱི་རྣམ་པ་གཉིས་ཀྱིས་དབྱེ་བ་ཞིབ་འཇུག་བྱེད་ཀྱི་ཡོད་ཚད་ལ་ངེས་འཛིན་བྱེད་པའི་ནང་དུ་འཇུག་ཟམ་འཛ</abstract_bo>
      <abstract_he>על ידי תמיכה באימונים והעריכה של השיגה המולטימודלית, קבוצות נתונים של התמונות מעוררו התקדמות מדהימה על לימוד מייצג. למרבה הצער, קבוצות נתונים יש איגודות דרך מודליות מוגבלות: תמונות לא זוויות עם תמונות אחרות, כותרות זוויות רק עם כותרות אחרות של אותה תמונה, אין איגודות שליליות ואין איגודות דרך מודליות חיוביות חסרות. This undermines research into how inter-modality learning impacts intra-modality tasks.  אנו מתמודדים עם הפער הזה עם "Crisscrossed Captions" (CxC), התאריך של קבוצת מידע MS-COCO עם שיפוטים סמנטיים של בני אדם של 267,095 זוגות בתוך ומודליות בין. אנחנו מדווחים על תוצאות בסיסית על CxC עבור דוגמנים חד-מודליים ויותרים מודליים קיימים חזקים. אנחנו גם מעריכים קודד כפול במשימות רבות מאומן על זוגות תמונה-כותר וגם כותר-כותר שמוכיחים באופן חשוב את ערכו של CxC למדוד השפעה של הלימוד בתוך ומודילי.</abstract_he>
      </paper>
    <paper id="251">
      <title>ENPAR : Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction<fixed-case>ENPAR</fixed-case>:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction</title>
      <author><first>Yijun</first><last>Wang</last></author>
      <author><first>Changzhi</first><last>Sun</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Junchi</first><last>Yan</last></author>
      <pages>2877–2887</pages>
      <abstract>Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019 ; Wad-den et al., 2019) usually adopt the multi-task learning framework. However, <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> for these additional tasks such as <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> and <a href="https://en.wikipedia.org/wiki/Event_extraction">event extraction</a> are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. ENPAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity information</a> into the sentence encoder, we further utilize the <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity pair information</a>. Specifically, we devise four novel <a href="https://en.wikipedia.org/wiki/Goal">objectives</a>, i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pre-train an <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity encoder</a> and an <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity pair encoder</a>. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05.</abstract>
      <url hash="890eff5e">2021.eacl-main.251</url>
      <bibkey>wang-etal-2021-enpar</bibkey>
      <doi>10.18653/v1/2021.eacl-main.251</doi>
      <pwccode url="https://github.com/receiling/enpar" additional="false">receiling/enpar</pwccode>
    </paper>
    <paper id="255">
      <title>Modelling Context Emotions using Multi-task Learning for Emotion Controlled Dialog Generation</title>
      <author><first>Deeksha</first><last>Varshney</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>2919–2931</pages>
      <abstract>A recent topic of research in <a href="https://en.wikipedia.org/wiki/Natural_language_generation">natural language generation</a> has been the development of automatic response generation modules that can automatically respond to a user’s utterance in an empathetic manner. Previous research has tackled this task using neural generative methods by augmenting <a href="https://en.wikipedia.org/wiki/Emotion_classification">emotion classes</a> with the input sequences. However, the outputs by these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> may be inconsistent. We employ <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> to predict the emotion label and to generate a viable response for a given utterance using a common encoder with multiple decoders. Our proposed encoder-decoder model consists of a self-attention based encoder and a decoder with dot product attention mechanism to generate response with a specified emotion. We use the focal loss to handle imbalanced data distribution, and utilize the consistency loss to allow coherent decoding by the <a href="https://en.wikipedia.org/wiki/Code">decoders</a>. Human evaluation reveals that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> produces more emotionally pertinent responses. In addition, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms multiple strong baselines on automatic evaluation measures such as F1 and BLEU scores, thus resulting in more fluent and adequate responses.</abstract>
      <url hash="e5edd288">2021.eacl-main.255</url>
      <bibkey>varshney-etal-2021-modelling</bibkey>
      <doi>10.18653/v1/2021.eacl-main.255</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/topical-chat">Topical-Chat</pwcdataset>
    </paper>
    <paper id="261">
      <title>Modeling Context in Answer Sentence Selection Systems on a Latency Budget</title>
      <author><first>Rujun</first><last>Han</last></author>
      <author><first>Luca</first><last>Soldaini</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>3005–3010</pages>
      <abstract>Answer Sentence Selection (AS2) is an efficient approach for the design of open-domain Question Answering (QA) systems. In order to achieve low <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a>, traditional AS2 models score question-answer pairs individually, ignoring any information from the document each potential answer was extracted from. In contrast, more computationally expensive <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> designed for machine reading comprehension tasks typically receive one or more passages as input, which often results in better <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>. In this work, we present an approach to efficiently incorporate <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> in AS2 models. For each answer candidate, we first use unsupervised similarity techniques to extract relevant sentences from its source document, which we then feed into an efficient transformer architecture fine-tuned for AS2. Our best approach, which leverages a multi-way attention architecture to efficiently encode <a href="https://en.wikipedia.org/wiki/Context_(computing)">context</a>, improves 6 % to 11 % over non-contextual state of the art in <a href="https://en.wikipedia.org/wiki/IBM_System_i">AS2</a> with minimal impact on <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">system latency</a>. All experiments in this work were conducted in English.</abstract>
      <url hash="76b9258d">2021.eacl-main.261</url>
      <bibkey>han-etal-2021-modeling</bibkey>
      <doi>10.18653/v1/2021.eacl-main.261</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/asnq">ASNQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="263">
      <title>DISK-CSV : Distilling Interpretable Semantic Knowledge with a Class Semantic Vector<fixed-case>DISK</fixed-case>-<fixed-case>CSV</fixed-case>: Distilling Interpretable Semantic Knowledge with a Class Semantic Vector</title>
      <author><first>Housam Khalifa</first><last>Bashier</last></author>
      <author><first>Mi-Young</first><last>Kim</last></author>
      <author><first>Randy</first><last>Goebel</last></author>
      <pages>3021–3030</pages>
      <abstract>Neural networks (NN) applied to <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP)</a> are becoming deeper and more complex, making them increasingly difficult to understand and interpret. Even in applications of limited scope on fixed data, the creation of these complex black-boxes creates substantial challenges for <a href="https://en.wikipedia.org/wiki/Debugging">debugging</a>, understanding, and <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a>. But rapid development in this <a href="https://en.wikipedia.org/wiki/Field_(physics)">field</a> has now lead to building more straightforward and interpretable models. We propose a new technique (DISK-CSV) to distill knowledge concurrently from any neural network architecture for text classification, captured as a lightweight interpretable / explainable classifier. Across multiple datasets, our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> achieves better performance than the target black-box. In addition, our <a href="https://en.wikipedia.org/wiki/Scientific_method">approach</a> provides better explanations than existing techniques.</abstract>
      <url hash="c880aa69">2021.eacl-main.263</url>
      <bibkey>bashier-etal-2021-disk</bibkey>
      <doi>10.18653/v1/2021.eacl-main.263</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="264">
      <title>Attention Can Reflect <a href="https://en.wikipedia.org/wiki/Syntactic_structure">Syntactic Structure</a> (If You Let It)</title>
      <author><first>Vinit</first><last>Ravishankar</last></author>
      <author><first>Artur</first><last>Kulmizev</last></author>
      <author><first>Mostafa</first><last>Abdou</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <pages>3031–3045</pages>
      <abstract>Since the popularization of the Transformer as a general-purpose feature encoder for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, many studies have attempted to decode linguistic structure from its novel multi-head attention mechanism. However, much of such work focused almost exclusively on English   a language with <a href="https://en.wikipedia.org/wiki/Linguistic_prescription">rigid word order</a> and a lack of <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">inflectional morphology</a>. In this study, we present decoding experiments for multilingual BERT across 18 languages in order to test the generalizability of the claim that dependency syntax is reflected in attention patterns. We show that full trees can be decoded above baseline accuracy from single attention heads, and that individual relations are often tracked by the same heads across languages. Furthermore, in an attempt to address recent debates about the status of <a href="https://en.wikipedia.org/wiki/Attention">attention</a> as an explanatory mechanism, we experiment with fine-tuning mBERT on a supervised parsing objective while freezing different series of parameters. Interestingly, in steering the objective to learn explicit linguistic structure, we find much of the same structure represented in the resulting attention patterns, with interesting differences with respect to which parameters are frozen.</abstract>
      <url hash="faaaff5b">2021.eacl-main.264</url>
      <bibkey>ravishankar-etal-2021-attention</bibkey>
      <doi>10.18653/v1/2021.eacl-main.264</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="266">
      <title>CDA : a Cost Efficient Content-based Multilingual Web Document Aligner<fixed-case>CDA</fixed-case>: a Cost Efficient Content-based Multilingual Web Document Aligner</title>
      <author><first>Thuy</first><last>Vu</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>3053–3061</pages>
      <abstract>We introduce a Content-based Document Alignment approach (CDA), an efficient method to align multilingual web documents based on content in creating parallel training data for machine translation (MT) systems operating at the industrial level. CDA works in two steps : (i) projecting documents of a <a href="https://en.wikipedia.org/wiki/Web_domain">web domain</a> to a shared multilingual space ; then (ii) aligning them based on the similarity of their representations in such space. We leverage lexical translation models to build vector representations using TFIDF. CDA achieves performance comparable with state-of-the-art systems in the WMT-16 Bilingual Document Alignment Shared Task benchmark while operating in multilingual space. Besides, we created two web-scale datasets to examine the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of <a href="https://en.wikipedia.org/wiki/Computer-aided_design">CDA</a> in an industrial setting involving up to 28 languages and millions of documents. The experiments show that CDA is robust, cost-effective, and is significantly superior in (i) processing large and noisy web data and (ii) scaling to new and low-resourced languages.</abstract>
      <url hash="fc7e8449">2021.eacl-main.266</url>
      <bibkey>vu-moschitti-2021-cda</bibkey>
      <doi>10.18653/v1/2021.eacl-main.266</doi>
    </paper>
    <paper id="270">
      <title>Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation</title>
      <author><first>Goran</first><last>Glavaš</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <pages>3090–3104</pages>
      <abstract>Traditional <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experiments (with intermediate target-language parsing) show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers’ representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question : how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models?</abstract>
      <url hash="125ca9bd">2021.eacl-main.270</url>
      <attachment type="Software" hash="e51236d0">2021.eacl-main.270.Software.zip</attachment>
      <revision id="1" href="2021.eacl-main.270v1" hash="9191a894" />
      <revision id="2" href="2021.eacl-main.270v2" hash="125ca9bd" date="2021-05-03">citation change</revision>
      <award>Best Long Paper</award>
      <bibkey>glavas-vulic-2021-supervised</bibkey>
      <doi>10.18653/v1/2021.eacl-main.270</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xcopa">XCOPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="271">
      <title>Facilitating Terminology Translation with Target Lemma Annotations</title>
      <author><first>Toms</first><last>Bergmanis</last></author>
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <pages>3105–3111</pages>
      <abstract>Most of the recent work on terminology integration in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> has assumed that terminology translations are given already inflected in forms that are suitable for the target language sentence. In day-to-day work of professional translators, however, it is seldom the case as translators work with bilingual glossaries where terms are given in their dictionary forms ; finding the right target language form is part of the translation process. We argue that the requirement for apriori specified target language forms is unrealistic and impedes the practical applicability of previous work. In this work, we propose to train <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a> using a source-side data augmentation method that annotates randomly selected source language words with their target language lemmas. We show that systems trained on such augmented data are readily usable for terminology integration in real-life translation scenarios. Our experiments on terminology translation into the morphologically complex Baltic and Uralic languages show an improvement of up to 7 BLEU points over baseline systems with no means for terminology integration and an average improvement of 4 BLEU points over the previous work. Results of the <a href="https://en.wikipedia.org/wiki/Human_factors_and_ergonomics">human evaluation</a> indicate a 47.7 % absolute improvement over the previous work in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">term translation accuracy</a> when translating into <a href="https://en.wikipedia.org/wiki/Latvian_language">Latvian</a>.</abstract>
      <url hash="b9ed255d">2021.eacl-main.271</url>
      <bibkey>bergmanis-pinnis-2021-facilitating</bibkey>
      <doi>10.18653/v1/2021.eacl-main.271</doi>
      <pwccode url="https://github.com/tilde-nlp/terminology_translation" additional="false">tilde-nlp/terminology_translation</pwccode>
    </paper>
    <paper id="273">
      <title>Summarising Historical Text in Modern Languages</title>
      <author><first>Xutan</first><last>Peng</last></author>
      <author><first>Yi</first><last>Zheng</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Advaith</first><last>Siddharthan</last></author>
      <pages>3123–3142</pages>
      <abstract>We introduce the task of historical text summarisation, where documents in historical forms of a language are summarised in the corresponding <a href="https://en.wikipedia.org/wiki/Modern_language">modern language</a>. This is a fundamentally important routine to historians and digital humanities researchers but has never been automated. We compile a high-quality gold-standard text summarisation dataset, which consists of historical German and Chinese news from hundreds of years ago summarised in modern German or <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. Based on cross-lingual transfer learning techniques, we propose a summarisation model that can be trained even with no cross-lingual (historical to modern) parallel data, and further benchmark it against state-of-the-art algorithms. We report automatic and human evaluations that distinguish the historic to modern language summarisation task from standard cross-lingual summarisation (i.e., modern to modern language), highlight the distinctness and value of our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, and demonstrate that our transfer learning approach outperforms standard cross-lingual benchmarks on this task.</abstract>
      <url hash="fa33b1b1">2021.eacl-main.273</url>
      <bibkey>peng-etal-2021-summarising</bibkey>
      <doi>10.18653/v1/2021.eacl-main.273</doi>
      <pwccode url="https://github.com/Pzoom522/HistSumm" additional="false">Pzoom522/HistSumm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lcsts">LCSTS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlsum">MLSUM</pwcdataset>
    </paper>
    <paper id="274">
      <title>Challenges in <a href="https://en.wikipedia.org/wiki/Debiasing">Automated Debiasing</a> for Toxic Language Detection</title>
      <author><first>Xuhui</first><last>Zhou</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Swabha</first><last>Swayamdipta</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Noah</first><last>Smith</last></author>
      <pages>3143–3155</pages>
      <abstract>Biased associations have been a challenge in the development of <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifiers</a> for detecting toxic language, hindering both fairness and <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>. As potential solutions, we investigate recently introduced <a href="https://en.wikipedia.org/wiki/Debiasing">debiasing methods</a> for text classification datasets and <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>, as applied to toxic language detection. Our focus is on lexical (e.g., <a href="https://en.wikipedia.org/wiki/Profanity">swear words</a>, <a href="https://en.wikipedia.org/wiki/List_of_ethnic_slurs">slurs</a>, identity mentions) and dialectal markers (specifically <a href="https://en.wikipedia.org/wiki/African-American_Vernacular_English">African American English</a>). Our comprehensive experiments establish that existing <a href="https://en.wikipedia.org/wiki/Scientific_method">methods</a> are limited in their ability to prevent <a href="https://en.wikipedia.org/wiki/Bias">biased behavior</a> in current <a href="https://en.wikipedia.org/wiki/Particle_detector">toxicity detectors</a>. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with <a href="https://en.wikipedia.org/wiki/Toxicity">toxicity</a>. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.</abstract>
      <url hash="be2add1f">2021.eacl-main.274</url>
      <bibkey>zhou-etal-2021-challenges</bibkey>
      <doi>10.18653/v1/2021.eacl-main.274</doi>
      <pwccode url="https://github.com/XuhuiZhou/Toxic_Debias" additional="true">XuhuiZhou/Toxic_Debias</pwccode>
    </paper>
    <paper id="276">
      <title>Detecting Scenes in Fiction : A new Segmentation Task</title>
      <author><first>Albin</first><last>Zehe</last></author>
      <author><first>Leonard</first><last>Konle</last></author>
      <author><first>Lea Katharina</first><last>Dümpelmann</last></author>
      <author><first>Evelyn</first><last>Gius</last></author>
      <author><first>Andreas</first><last>Hotho</last></author>
      <author><first>Fotis</first><last>Jannidis</last></author>
      <author><first>Lucas</first><last>Kaufmann</last></author>
      <author><first>Markus</first><last>Krug</last></author>
      <author><first>Frank</first><last>Puppe</last></author>
      <author><first>Nils</first><last>Reiter</last></author>
      <author><first>Annekea</first><last>Schreiber</last></author>
      <author><first>Nathalie</first><last>Wiedmer</last></author>
      <pages>3167–3177</pages>
      <abstract>This paper introduces the novel task of scene segmentation on narrative texts and provides an annotated corpus, a discussion of the linguistic and narrative properties of the task and baseline experiments towards automatic solutions. A scene here is a segment of the text where time and discourse time are more or less equal, the narration focuses on one action and location and character constellations stay the same. The <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> we describe consists of German-language dime novels (550k tokens) that have been annotated in parallel, achieving an <a href="https://en.wikipedia.org/wiki/Inter-annotator_agreement">inter-annotator agreement</a> of <a href="https://en.wikipedia.org/wiki/Gamma">gamma</a> = 0.7. Baseline experiments using BERT achieve an F1 score of 24 %, showing that the task is very challenging. An automatic scene segmentation paves the way towards processing longer narrative texts like tales or novels by breaking them down into smaller, coherent and meaningful parts, which is an important stepping stone towards the reconstruction of plot in Computational Literary Studies but also can serve to improve tasks like <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>.</abstract>
      <url hash="d653f3d4">2021.eacl-main.276</url>
      <bibkey>zehe-etal-2021-detecting</bibkey>
      <doi>10.18653/v1/2021.eacl-main.276</doi>
    </paper>
    <paper id="279">
      <title>Expanding, Retrieving and Infilling : Diversifying Cross-Domain Question Generation with Flexible Templates</title>
      <author><first>Xiaojing</first><last>Yu</last></author>
      <author><first>Anxiao</first><last>Jiang</last></author>
      <pages>3202–3212</pages>
      <abstract>Sequence-to-sequence based models have recently shown promising results in generating high-quality questions. However, these models are also known to have main drawbacks such as <a href="https://en.wikipedia.org/wiki/Diversity_(politics)">lack of diversity</a> and <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">bad sentence structures</a>. In this paper, we focus on <a href="https://en.wikipedia.org/wiki/Question_answering">question generation</a> over <a href="https://en.wikipedia.org/wiki/SQL">SQL database</a> and propose a novel framework by expanding, retrieving, and infilling that first incorporates flexible templates with a neural-based model to generate diverse expressions of questions with sentence structure guidance. Furthermore, a new activation / deactivation mechanism is proposed for template-based sequence-to-sequence generation, which learns to discriminate template patterns and content patterns, thus further improves generation quality. We conduct experiments on two large-scale cross-domain datasets. The experiments show that the superiority of our question generation method in producing more diverse questions while maintaining high quality and consistency under both automatic evaluation and <a href="https://en.wikipedia.org/wiki/Evaluation">human evaluation</a>.</abstract>
      <url hash="49a4def6">2021.eacl-main.279</url>
      <bibkey>yu-jiang-2021-expanding</bibkey>
      <doi>10.18653/v1/2021.eacl-main.279</doi>
      <pwccode url="https://github.com/xiaojingyu92/eriqg" additional="false">xiaojingyu92/eriqg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/spider-1">SPIDER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="280">
      <title>Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings</title>
      <author><first>Ohjoon</first><last>Kwon</last></author>
      <author><first>Dohyun</first><last>Kim</last></author>
      <author><first>Soo-Ryeon</first><last>Lee</last></author>
      <author><first>Junyoung</first><last>Choi</last></author>
      <author><first>SangKeun</first><last>Lee</last></author>
      <pages>3213–3221</pages>
      <abstract>Word embedding is considered an essential factor in improving the performance of various <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing (NLP) models</a>. However, it is hardly applicable in real-world datasets as <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> is generally studied with a well-refined corpus. Notably, in Hangeul (Korean writing system), which has a unique writing system, various kinds of Out-Of-Vocabulary (OOV) appear from typos. In this paper, we propose a robust Hangeul word embedding model against <a href="https://en.wikipedia.org/wiki/Typographical_error">typos</a>, while maintaining high performance. The proposed model utilizes a Convolutional Neural Network (CNN) architecture with a channel attention mechanism that learns to infer the original word embeddings. The <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> train with a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> that consists of a mix of <a href="https://en.wikipedia.org/wiki/Typographical_error">typos</a> and correct words. To demonstrate the effectiveness of the proposed <a href="https://en.wikipedia.org/wiki/Scientific_modelling">model</a>, we conduct three kinds of intrinsic and extrinsic tasks. While the existing embedding models fail to maintain stable performance as the <a href="https://en.wikipedia.org/wiki/Noise_(electronics)">noise level</a> increases, the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> shows stable performance.</abstract>
      <url hash="c05d281e">2021.eacl-main.280</url>
      <bibkey>kwon-etal-2021-handling</bibkey>
      <doi>10.18653/v1/2021.eacl-main.280</doi>
    </paper>
    <paper id="281">
      <title>Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation</title>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Andy Mingren</first><last>Li</last></author>
      <author><first>Yishu</first><last>Miao</last></author>
      <author><first>Ozan</first><last>Caglayan</last></author>
      <author><first>Pranava</first><last>Madhyastha</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>3222–3233</pages>
      <abstract>This paper addresses the problem of simultaneous machine translation (SiMT) by exploring two main concepts : (a) adaptive policies to learn a good trade-off between high translation quality and low latency ; and (b) visual information to support this process by providing additional (visual) contextual information which may be available before the textual input is produced. For that, we propose a multimodal approach to simultaneous machine translation using <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a> of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a> low.</abstract>
      <url hash="f23a895b">2021.eacl-main.281</url>
      <bibkey>ive-etal-2021-exploiting</bibkey>
      <doi>10.18653/v1/2021.eacl-main.281</doi>
      <pwccode url="https://github.com/ImperialNLP/pysimt" additional="false">ImperialNLP/pysimt</pwccode>
    </paper>
    <paper id="283">
      <title>Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?</title>
      <author><first>Yixuan</first><last>Tang</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <author><first>Anthony</first><last>Tung</last></author>
      <pages>3244–3249</pages>
      <abstract>Multi-hop question answering (QA) requires a model to retrieve and integrate information from multiple passages to answer a question. Rapid progress has been made on multi-hop QA systems with regard to standard evaluation metrics, including EM and F1. However, by simply evaluating the correctness of the answers, it is unclear to what extent these <a href="https://en.wikipedia.org/wiki/System">systems</a> have learned the ability to perform multi-hop reasoning. In this paper, we propose an additional sub-question evaluation for the multi-hop QA dataset HotpotQA, in order to shed some light on explaining the reasoning process of QA systems in answering complex questions. We adopt a neural decomposition model to generate <a href="https://en.wikipedia.org/wiki/Questionnaire">sub-questions</a> for a multi-hop question, followed by extracting the corresponding <a href="https://en.wikipedia.org/wiki/Questionnaire">sub-answers</a>. Contrary to our expectation, multiple state-of-the-art multi-hop QA models fail to answer a large portion of sub-questions, although the corresponding multi-hop questions are correctly answered. Our work takes a step forward towards building a more explainable multi-hop QA system.</abstract>
      <url hash="2b7d5141">2021.eacl-main.283</url>
      <bibkey>tang-etal-2021-multi</bibkey>
      <doi>10.18653/v1/2021.eacl-main.283</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="285">
      <title>Variational Weakly Supervised Sentiment Analysis with Posterior Regularization</title>
      <author><first>Ziqian</first><last>Zeng</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <pages>3259–3268</pages>
      <abstract>Sentiment analysis is an important task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP)</a>. Most of existing state-of-the-art methods are under the <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning paradigm</a>. However, <a href="https://en.wikipedia.org/wiki/Annotation">human annotations</a> can be scarce. Thus, we should leverage more weak supervision for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. In this paper, we propose a posterior regularization framework for the variational approach to the weakly supervised sentiment analysis to better control the <a href="https://en.wikipedia.org/wiki/Posterior_probability">posterior distribution</a> of the label assignment. The intuition behind the posterior regularization is that if extracted opinion words from two documents are semantically similar, the <a href="https://en.wikipedia.org/wiki/Posterior_probability">posterior distributions</a> of two documents should be similar. Our experimental results show that the posterior regularization can improve the original variational approach to the weakly supervised sentiment analysis and the performance is more stable with smaller prediction variance.</abstract>
      <url hash="b957ea44">2021.eacl-main.285</url>
      <bibkey>zeng-song-2021-variational</bibkey>
      <doi>10.18653/v1/2021.eacl-main.285</doi>
      <pwccode url="https://github.com/HKUST-KnowComp/VWS-PR" additional="false">HKUST-KnowComp/VWS-PR</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="288">
      <title>Cognition-aware Cognate Detection</title>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Prashant</first><last>Sharma</last></author>
      <author><first>Sayali</first><last>Ghodekar</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Malhar</first><last>Kulkarni</last></author>
      <pages>3281–3292</pages>
      <abstract>Automatic detection of cognates helps downstream NLP tasks of <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a>, <a href="https://en.wikipedia.org/wiki/Cross-lingual_information_retrieval">Cross-lingual Information Retrieval</a>, <a href="https://en.wikipedia.org/wiki/Computational_phylogenetics">Computational Phylogenetics</a> and Cross-lingual Named Entity Recognition. Previous approaches for the task of cognate detection use orthographic, phonetic and semantic similarity based features sets. In this paper, we propose a novel method for enriching the feature sets, with <a href="https://en.wikipedia.org/wiki/Cognition">cognitive features</a> extracted from human readers’ gaze behaviour. We collect gaze behaviour data for a small sample of <a href="https://en.wikipedia.org/wiki/Cognate">cognates</a> and show that extracted cognitive features help the task of cognate detection. However, gaze data collection and annotation is a costly task. We use the collected gaze behaviour data to predict cognitive features for a larger sample and show that predicted cognitive features, also, significantly improve the task performance. We report improvements of 10 % with the collected gaze features, and 12 % using the predicted gaze features, over the previously proposed approaches. Furthermore, we release the collected gaze behaviour data along with our code and cross-lingual models.</abstract>
      <url hash="2225a875">2021.eacl-main.288</url>
      <award>Honorable Mention for Best Long Paper</award>
      <bibkey>kanojia-etal-2021-cognition</bibkey>
      <doi>10.18653/v1/2021.eacl-main.288</doi>
      <pwccode url="https://github.com/prashantksharma/CaCD" additional="false">prashantksharma/CaCD</pwccode>
    </paper>
    <paper id="295">
      <title>Probing the Probing Paradigm : Does Probing Accuracy Entail Task Relevance?</title>
      <author><first>Abhilasha</first><last>Ravichander</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>3363–3377</pages>
      <abstract>Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence representations</a> learned by <a href="https://en.wikipedia.org/wiki/Neural_coding">neural encoders</a>, through the lens of ‘probing’ tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level, even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks.</abstract>
      <url hash="59cd5672">2021.eacl-main.295</url>
      <bibkey>ravichander-etal-2021-probing</bibkey>
      <doi>10.18653/v1/2021.eacl-main.295</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="296">
      <title>One-class Text Classification with Multi-modal Deep Support Vector Data Description</title>
      <author><first>Chenlong</first><last>Hu</last></author>
      <author><first>Yukun</first><last>Feng</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>3378–3390</pages>
      <abstract>This work presents multi-modal deep SVDD (mSVDD) for one-class text classification. By extending the uni-modal SVDD to a multiple modal one, we build mSVDD with multiple hyperspheres, that enable us to build a much better description for target one-class data. Additionally, the end-to-end architecture of mSVDD can jointly handle neural feature learning and one-class text learning. We also introduce a mechanism for incorporating negative supervision in the absence of real negative data, which can be beneficial to the mSVDD model. We conduct experiments on Reuters and 20 Newsgroup datasets, and the experimental results demonstrate that mSVDD outperforms uni-modal SVDD and mSVDD can get further improvements when negative supervision is incorporated.</abstract>
      <url hash="4f8eb485">2021.eacl-main.296</url>
      <bibkey>hu-etal-2021-one</bibkey>
      <doi>10.18653/v1/2021.eacl-main.296</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="297">
      <title>Unsupervised Word Polysemy Quantification with Multiresolution Grids of Contextual Embeddings</title>
      <author><first>Christos</first><last>Xypolopoulos</last></author>
      <author><first>Antoine</first><last>Tixier</last></author>
      <author><first>Michalis</first><last>Vazirgiannis</last></author>
      <pages>3391–3401</pages>
      <abstract>The number of senses of a given word, or <a href="https://en.wikipedia.org/wiki/Polysemy">polysemy</a>, is a very subjective notion, which varies widely across annotators and resources. We propose a novel method to estimate <a href="https://en.wikipedia.org/wiki/Polysemy">polysemy</a> based on simple geometry in the contextual embedding space. Our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> is fully unsupervised and purely data-driven. Through rigorous experiments, we show that our rankings are well correlated, with strong statistical significance, with 6 different rankings derived from famous human-constructed resources such as <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a>, OntoNotes, <a href="https://en.wikipedia.org/wiki/Oxford">Oxford</a>, <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, etc., for 6 different standard metrics. We also visualize and analyze the correlation between the human rankings and make interesting observations. A valuable by-product of our method is the ability to sample, at no extra cost, sentences containing different senses of a given word. Finally, the fully unsupervised nature of our approach makes <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> applicable to any language. Code and data are publicly available https://github.com/ksipos/polysemy-assessment.</abstract>
      <url hash="f069d77f">2021.eacl-main.297</url>
      <bibkey>xypolopoulos-etal-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.eacl-main.297</doi>
      <pwccode url="https://github.com/ksipos/polysemy-assessment" additional="false">ksipos/polysemy-assessment</pwccode>
    </paper>
    <paper id="299">
      <title>Disfluency Correction using <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">Unsupervised and Semi-supervised Learning</a></title>
      <author><first>Nikhil</first><last>Saini</last></author>
      <author><first>Drumil</first><last>Trivedi</last></author>
      <author><first>Shreya</first><last>Khare</last></author>
      <author><first>Tejas</first><last>Dhamecha</last></author>
      <author><first>Preethi</first><last>Jyothi</last></author>
      <author><first>Samarth</first><last>Bharadwaj</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>3421–3427</pages>
      <abstract>Spoken language is different from the <a href="https://en.wikipedia.org/wiki/Written_language">written language</a> in its style and structure. Disfluencies that appear in transcriptions from <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition systems</a> generally hamper the performance of downstream NLP tasks. Thus, a disfluency correction system that converts disfluent to fluent text is of great value. This paper introduces a disfluency correction model that translates disfluent to fluent text by drawing inspiration from recent encoder-decoder unsupervised style-transfer models for text. We also show considerable benefits in performance when utilizing a small sample of 500 parallel disfluent-fluent sentences in a semi-supervised way. Our <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised approach</a> achieves a BLEU score of 79.39 on the Switchboard corpus test set, with further improvement to a BLEU score of 85.28 with semi-supervision. Both are comparable to two competitive fully-supervised models.</abstract>
      <url hash="a51cbb65">2021.eacl-main.299</url>
      <bibkey>saini-etal-2021-disfluency</bibkey>
      <doi>10.18653/v1/2021.eacl-main.299</doi>
    </paper>
    <paper id="300">
      <title>Complex Question Answering on <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a> using <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> and <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a></title>
      <author><first>Saurabh</first><last>Srivastava</last></author>
      <author><first>Mayur</first><last>Patidar</last></author>
      <author><first>Sudip</first><last>Chowdhury</last></author>
      <author><first>Puneet</first><last>Agarwal</last></author>
      <author><first>Indrajit</first><last>Bhattacharya</last></author>
      <author><first>Gautam</first><last>Shroff</last></author>
      <pages>3428–3439</pages>
      <abstract>Question answering (QA) over a knowledge graph (KG) is a task of answering a natural language (NL) query using the information stored in KG. In a real-world industrial setting, this involves addressing multiple challenges including <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity linking</a>, multi-hop reasoning over KG, etc. Traditional approaches handle these challenges in a modularized sequential manner where errors in one module lead to the accumulation of errors in downstream modules. Often these challenges are inter-related and the solutions to them can reinforce each other when handled simultaneously in an end-to-end learning setup. To this end, we propose a multi-task BERT based Neural Machine Translation (NMT) model to address these challenges. Through experimental analysis, we demonstrate the efficacy of our proposed approach on one publicly available and one proprietary dataset.</abstract>
      <url hash="fecaa223">2021.eacl-main.300</url>
      <bibkey>srivastava-etal-2021-complex</bibkey>
      <doi>10.18653/v1/2021.eacl-main.300</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/metaqa">MetaQA</pwcdataset>
    </paper>
    <paper id="304">
      <title>Communicative-Function-Based Sentence Classification for Construction of an Academic Formulaic Expression Database</title>
      <author><first>Kenichi</first><last>Iwatsuki</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>3476–3497</pages>
      <abstract>Formulaic expressions (FEs), such as ‘in this paper, we propose’ are frequently used in <a href="https://en.wikipedia.org/wiki/Scientific_literature">scientific papers</a>. FEs convey a communicative function (CF), i.e. ‘showing the aim of the paper’ in the above-mentioned example. Although CF-labelled FEs are helpful in assisting <a href="https://en.wikipedia.org/wiki/Academic_writing">academic writing</a>, the construction of FE databases requires manual labour for assigning CF labels. In this study, we considered a fully automated construction of a CF-labelled FE database using the topdown approach, in which the CF labels are first assigned to sentences, and then the FEs are extracted. For the CF-label assignment, we created a CF-labelled sentence dataset, on which we trained a SciBERT classifier. We show that the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> and <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> can be used to construct FE databases of disciplines that are different from the training data. The <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of <a href="https://en.wikipedia.org/wiki/Medical_classification">in-disciplinary classification</a> was more than 80 %, while <a href="https://en.wikipedia.org/wiki/Medical_classification">cross-disciplinary classification</a> also worked well. We also propose an FE extraction method, which was applied to the CF-labelled sentences. Finally, we constructed and published a new, large CF-labelled FE database. The evaluation of the final CF-labelled FE database showed that approximately 65 % of the FEs are correct and useful, which is sufficiently high considering practical use.</abstract>
      <url hash="84e448c8">2021.eacl-main.304</url>
      <bibkey>iwatsuki-aizawa-2021-communicative</bibkey>
      <doi>10.18653/v1/2021.eacl-main.304</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="309">
      <title>From the Stage to the Audience : Propaganda on Reddit<fixed-case>R</fixed-case>eddit</title>
      <author><first>Oana</first><last>Balalau</last></author>
      <author><first>Roxana</first><last>Horincar</last></author>
      <pages>3540–3550</pages>
      <abstract>Political discussions revolve around ideological conflicts that often split the audience into two opposing parties. Both parties try to win the argument by bringing forward information. However, often this information is misleading, and its dissemination employs <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda techniques</a>. In this work, we analyze the impact of <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a> on six major political forums on <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a> that target a diverse audience in two countries, the US and the UK. We focus on three research questions : who is posting propaganda? how does <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a> differ across the <a href="https://en.wikipedia.org/wiki/Political_spectrum">political spectrum</a>? and how is <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a> received on political forums?</abstract>
      <url hash="b5b65826">2021.eacl-main.309</url>
      <bibkey>balalau-horincar-2021-stage</bibkey>
      <doi>10.18653/v1/2021.eacl-main.309</doi>
      <pwccode url="https://github.com/nyxpho/propaganda_eacl2021" additional="false">nyxpho/propaganda_eacl2021</pwccode>
    </paper>
    <paper id="310">
      <title>Probing for <a href="https://en.wikipedia.org/wiki/Idiom_(language_structure)">idiomaticity</a> in vector space models</title>
      <author><first>Marcos</first><last>Garcia</last></author>
      <author><first>Tiago</first><last>Kramer Vieira</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Marco</first><last>Idiart</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <pages>3551–3564</pages>
      <abstract>Contextualised word representation models have been successfully used for capturing different word usages and they may be an attractive alternative for representing <a href="https://en.wikipedia.org/wiki/Idiom_(language_structure)">idiomaticity</a> in <a href="https://en.wikipedia.org/wiki/Language">language</a>. In this paper, we propose probing measures to assess if some of the expected linguistic properties of <a href="https://en.wikipedia.org/wiki/Compound_(linguistics)">noun compounds</a>, especially those related to idiomatic meanings, and their dependence on <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a> and sensitivity to <a href="https://en.wikipedia.org/wiki/Lexical_choice">lexical choice</a>, are readily available in some standard and widely used representations. For that, we constructed the Noun Compound Senses Dataset, which contains <a href="https://en.wikipedia.org/wiki/Compound_(linguistics)">noun compounds</a> and their paraphrases, in context neutral and context informative naturalistic sentences, in two languages : <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese</a>. Results obtained using four types of probing measures with models like ELMo, BERT and some of its variants, indicate that <a href="https://en.wikipedia.org/wiki/Idiom_(language_structure)">idiomaticity</a> is not yet accurately represented by contextualised models</abstract>
      <url hash="74cce4ec">2021.eacl-main.310</url>
      <attachment type="Software" hash="0b7f396a">2021.eacl-main.310.Software.zip</attachment>
      <attachment type="Dataset" hash="0294929f">2021.eacl-main.310.Dataset.zip</attachment>
      <bibkey>garcia-etal-2021-probing</bibkey>
      <doi>10.18653/v1/2021.eacl-main.310</doi>
      <pwccode url="https://github.com/marcospln/noun_compound_senses" additional="false">marcospln/noun_compound_senses</pwccode>
    </paper>
    <paper id="311">
      <title>Is the Understanding of Explicit Discourse Relations Required in Machine Reading Comprehension?</title>
      <author><first>Yulong</first><last>Wu</last></author>
      <author><first>Viktor</first><last>Schlegel</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last></author>
      <pages>3565–3579</pages>
      <abstract>An in-depth analysis of the level of language understanding required by existing Machine Reading Comprehension (MRC) benchmarks can provide insight into the reading capabilities of machines. In this paper, we propose an ablation-based methodology to assess the extent to which MRC datasets evaluate the understanding of explicit discourse relations. We define seven MRC skills which require the understanding of different <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse relations</a>. We then introduce <a href="https://en.wikipedia.org/wiki/Ablation">ablation methods</a> that verify whether these skills are required to succeed on a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. By observing the drop in performance of neural MRC models evaluated on the original and the modified <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, we can measure to what degree the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> requires these skills, in order to be understood correctly. Experiments on three large-scale datasets with the BERT-base and ALBERT-xxlarge model show that the relative changes for all skills are small (less than 6 %). These results imply that most of the answered questions in the examined <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> do not require understanding the discourse structure of the text. To specifically probe for <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>, there is a need to design more challenging <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmarks</a> that can correctly evaluate the intended skills.</abstract>
      <url hash="e73837e6">2021.eacl-main.311</url>
      <bibkey>wu-etal-2021-understanding</bibkey>
      <doi>10.18653/v1/2021.eacl-main.311</doi>
      <pwccode url="https://github.com/yulong-w/mrcdr" additional="false">yulong-w/mrcdr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
    </paper>
    <paper id="314">
      <title>Meta-Learning for Effective Multi-task and Multilingual Modelling</title>
      <author><first>Ishan</first><last>Tarunesh</last></author>
      <author><first>Sushil</first><last>Khyalia</last></author>
      <author><first>Vishwajeet</first><last>Kumar</last></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last></author>
      <author><first>Preethi</first><last>Jyothi</last></author>
      <pages>3600–3612</pages>
      <abstract>Natural language processing (NLP) tasks (e.g. question-answering in English) benefit from knowledge of other tasks (e.g., named entity recognition in English) and knowledge of other languages (e.g., question-answering in Spanish). Such shared representations are typically learned in isolation, either across tasks or across languages. In this work, we propose a meta-learning approach to learn the interactions between both <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> and languages. We also investigate the role of different <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">sampling strategies</a> used during <a href="https://en.wikipedia.org/wiki/Meta-learning">meta-learning</a>. We present experiments on five different <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> and six different languages from the XTREME multilingual benchmark dataset. Our meta-learned model clearly improves in performance compared to competitive baseline models that also include multi-task baselines. We also present zero-shot evaluations on unseen target languages to demonstrate the utility of our proposed <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>.</abstract>
      <url hash="3aedc636">2021.eacl-main.314</url>
      <bibkey>tarunesh-etal-2021-meta</bibkey>
      <doi>10.18653/v1/2021.eacl-main.314</doi>
      <pwccode url="https://github.com/ishan00/meta-learning-for-multi-task-multilingual" additional="false">ishan00/meta-learning-for-multi-task-multilingual</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="321">
      <title>Two Training Strategies for Improving <a href="https://en.wikipedia.org/wiki/Relation_extraction">Relation Extraction</a> over Universal Graph</title>
      <author><first>Qin</first><last>Dai</last></author>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Ryo</first><last>Takahashi</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>3673–3684</pages>
      <abstract>This paper explores how the Distantly Supervised Relation Extraction (DS-RE) can benefit from the use of a Universal Graph (UG), the combination of a Knowledge Graph (KG) and a large-scale text collection. A straightforward extension of a current state-of-the-art neural model for DS-RE with a UG may lead to degradation in performance. We first report that this degradation is associated with the difficulty in learning a UG and then propose two training strategies : (1) Path Type Adaptive Pretraining, which sequentially trains the model with different types of UG paths so as to prevent the reliance on a single type of UG path ; and (2) Complexity Ranking Guided Attention mechanism, which restricts the attention span according to the complexity of a UG path so as to force the model to extract features not only from simple UG paths but also from complex ones. Experimental results on both biomedical and NYT10 datasets prove the robustness of our methods and achieve a new state-of-the-art result on the NYT10 dataset. The code and datasets used in this paper are available at https://github.com/baodaiqin/UGDSRE.</abstract>
      <url hash="cbf6798b">2021.eacl-main.321</url>
      <bibkey>dai-etal-2021-two</bibkey>
      <doi>10.18653/v1/2021.eacl-main.321</doi>
      <pwccode url="https://github.com/baodaiqin/UGDSRE" additional="false">baodaiqin/UGDSRE</pwccode>
    </paper>
    </volume>
  <volume id="demos" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</booktitle>
      <editor><first>Dimitra</first><last>Gkatzia</last></editor>
      <editor><first>Djamé</first><last>Seddah</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="e71008e4">2021.eacl-demos.0</url>
      <bibkey>eacl-2021-european-chapter</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Using and comparing Rhetorical Structure Theory parsers with rst-workbench<fixed-case>R</fixed-case>hetorical <fixed-case>S</fixed-case>tructure <fixed-case>T</fixed-case>heory parsers with rst-workbench</title>
      <author><first>Arne</first><last>Neumann</last></author>
      <pages>1–6</pages>
      <abstract>I present rst-workbench, a <a href="https://en.wikipedia.org/wiki/Package_manager">software package</a> that simplifies the installation and usage of numerous end-to-end Rhetorical Structure Theory (RST) parsers. The tool offers a <a href="https://en.wikipedia.org/wiki/Web_application">web-based interface</a> that allows users to enter text and let multiple RST parsers generate analyses concurrently. The resulting RST trees can be compared visually, manually post-edited (in the browser) and stored for later usage.</abstract>
      <url hash="e4ed167f">2021.eacl-demos.1</url>
      <bibkey>neumann-2021-using</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.1</doi>
      <pwccode url="https://github.com/rst-workbench/rst-workbench" additional="false">rst-workbench/rst-workbench</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rst-dt">RST-DT</pwcdataset>
    </paper>
    <paper id="5">
      <title>MATILDA-Multi-AnnoTator multi-language InteractiveLight-weight Dialogue Annotator<fixed-case>MATILDA</fixed-case> - Multi-<fixed-case>A</fixed-case>nno<fixed-case>T</fixed-case>ator multi-language <fixed-case>I</fixed-case>nteractive<fixed-case>L</fixed-case>ight-weight Dialogue Annotator</title>
      <author><first>Davide</first><last>Cucurnia</last></author>
      <author><first>Nikolai</first><last>Rozanov</last></author>
      <author><first>Irene</first><last>Sucameli</last></author>
      <author><first>Augusto</first><last>Ciuffoletti</last></author>
      <author><first>Maria</first><last>Simi</last></author>
      <pages>32–39</pages>
      <abstract>Dialogue Systems are becoming ubiquitous in various forms and shapes-virtual assistants(Siri, <a href="https://en.wikipedia.org/wiki/Amazon_Alexa">Alexa</a>, etc.), chat-bots, customer sup-port, chit-chat systems just to name a few. The advances in <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> and their publication have democratised advanced NLP.However, data remains a crucial bottleneck. Our contribution to this essential pillar isMATILDA, to the best of our knowledge the first multi-annotator, multi-language dialogue annotation tool. MATILDA allows the creation of corpora, the management of users, the annotation of dialogues, the quick adaptation of the <a href="https://en.wikipedia.org/wiki/User_interface">user interface</a> to any language and the resolution of inter-annotator disagreement. We evaluate the <a href="https://en.wikipedia.org/wiki/Tool">tool</a> on ease of use, annotation speed and interannotation resolution for both experts and novices and conclude that this <a href="https://en.wikipedia.org/wiki/Tool">tool</a> not only supports the full pipeline for dialogue annotation, but also allows non-technical people to easily use it. We are completely open-sourcing the tool at https://github.com/wluper/matilda and provide a tutorial video1.</abstract>
      <url hash="0e3253e9">2021.eacl-demos.5</url>
      <bibkey>cucurnia-etal-2021-matilda</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.5</doi>
      <pwccode url="https://github.com/wluper/matilda" additional="false">wluper/matilda</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="8">
      <title>Forum 4.0 : An Open-Source User Comment Analysis Framework</title>
      <author><first>Marlo</first><last>Haering</last></author>
      <author><first>Jakob Smedegaard</first><last>Andersen</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Wiebke</first><last>Loosen</last></author>
      <author><first>Benjamin</first><last>Milde</last></author>
      <author><first>Tim</first><last>Pietz</last></author>
      <author><first>Christian</first><last>Stöcker</last></author>
      <author><first>Gregor</first><last>Wiedemann</last></author>
      <author><first>Olaf</first><last>Zukunft</last></author>
      <author><first>Walid</first><last>Maalej</last></author>
      <pages>63–70</pages>
      <abstract>With the increasing number of user comments in diverse domains, including comments on <a href="https://en.wikipedia.org/wiki/Digital_journalism">online journalism</a> and <a href="https://en.wikipedia.org/wiki/E-commerce">e-commerce websites</a>, the manual content analysis of these comments becomes time-consuming and challenging. However, research showed that user comments contain useful information for different domain experts, which is thus worth finding and utilizing. This paper introduces Forum 4.0, an open-source framework to semi-automatically analyze, aggregate, and visualize user comments based on labels defined by domain experts. We demonstrate the applicability of Forum 4.0 with comments analytics scenarios within the domains of <a href="https://en.wikipedia.org/wiki/Digital_journalism">online journalism</a> and <a href="https://en.wikipedia.org/wiki/App_store">app stores</a>. We outline the underlying container architecture, including the <a href="https://en.wikipedia.org/wiki/Web_interface">web-based user interface</a>, the <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning component</a>, and the <a href="https://en.wikipedia.org/wiki/Task_manager">task manager</a> for time-consuming tasks. We finally conduct <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> experiments with simulated annotations and different <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">sampling strategies</a> on existing <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> from both domains to evaluate Forum 4.0’s performance. Forum 4.0 achieves promising classification results (ROC-AUC   0.9 with 100 annotated samples), utilizing transformer-based embeddings with a lightweight <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression model</a>. We explain how Forum 4.0’s architecture is applicable for millions of user comments in real-time, yet at feasible training and classification costs.</abstract>
      <url hash="b97ba608">2021.eacl-demos.8</url>
      <bibkey>haering-etal-2021-forum</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.8</doi>
    </paper>
    <paper id="9">
      <title>SLTEV : Comprehensive Evaluation of Spoken Language Translation<fixed-case>SLTEV</fixed-case>: Comprehensive Evaluation of Spoken Language Translation</title>
      <author><first>Ebrahim</first><last>Ansari</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Mohammad</first><last>Mahmoudi</last></author>
      <pages>71–79</pages>
      <abstract>Automatic evaluation of Machine Translation (MT) quality has been investigated over several decades. Spoken Language Translation (SLT), esp. when simultaneous, needs to consider additional criteria and does not have a standard evaluation procedure and a widely used toolkit. To fill the gap, we develop SLTev, an open-source tool for assessing SLT in a comprehensive way. SLTev reports the <a href="https://en.wikipedia.org/wiki/Quality_(business)">quality</a>, <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a>, and <a href="https://en.wikipedia.org/wiki/Software_stability">stability</a> of an SLT candidate output based on the time-stamped transcript and reference translation into a target language. For quality, we rely on sacreBLEU which provides MT evaluation measures such as chrF or <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>. For <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a>, we propose two new scoring techniques. For <a href="https://en.wikipedia.org/wiki/Stability_theory">stability</a>, we extend the previously defined <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">measures</a> with a normalized Flicker in our work. We also propose a new averaging of older measures. A preliminary version of SLTev was used in the IWSLT 2020 shared task. Moreover, a growing collection of test datasets directly accessible by SLTev are provided for system evaluation comparable across papers.</abstract>
      <url hash="04b4f0b6">2021.eacl-demos.9</url>
      <bibkey>ansari-etal-2021-sltev</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.9</doi>
      <pwccode url="https://github.com/elitr/sltev" additional="false">elitr/sltev</pwccode>
    </paper>
    <paper id="12">
      <title>A Dashboard for Mitigating the COVID-19 Misinfodemic<fixed-case>COVID</fixed-case>-19 Misinfodemic</title>
      <author><first>Zhengyuan</first><last>Zhu</last></author>
      <author><first>Kevin</first><last>Meng</last></author>
      <author><first>Josue</first><last>Caraballo</last></author>
      <author><first>Israa</first><last>Jaradat</last></author>
      <author><first>Xiao</first><last>Shi</last></author>
      <author><first>Zeyu</first><last>Zhang</last></author>
      <author><first>Farahnaz</first><last>Akrami</last></author>
      <author><first>Haojin</first><last>Liao</last></author>
      <author><first>Fatma</first><last>Arslan</last></author>
      <author><first>Damian</first><last>Jimenez</last></author>
      <author><first>Mohanmmed Samiul</first><last>Saeef</last></author>
      <author><first>Paras</first><last>Pathak</last></author>
      <author><first>Chengkai</first><last>Li</last></author>
      <pages>99–105</pages>
      <abstract>This paper describes the current milestones achieved in our ongoing project that aims to understand the surveillance of, impact of and intervention on COVID-19 misinfodemic on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>. Specifically, it introduces a public dashboard which, in addition to displaying case counts in an interactive map and a navigational panel, also provides some unique features not found in other places. Particularly, the <a href="https://en.wikipedia.org/wiki/Dashboard_(business)">dashboard</a> uses a curated catalog of COVID-19 related facts and debunks of misinformation, and it displays the most prevalent information from the catalog among Twitter users in user-selected U.S. geographic regions. The paper explains how to use BERT models to match tweets with the facts and misinformation and to detect their stance towards such information. The paper also discusses the results of preliminary experiments on analyzing the spatio-temporal spread of misinformation.</abstract>
      <url hash="1014bfe6">2021.eacl-demos.12</url>
      <bibkey>zhu-etal-2021-dashboard</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.12</doi>
    </paper>
    <paper id="16">
      <title>A description and demonstration of SAFAR framework<fixed-case>SAFAR</fixed-case> framework</title>
      <author><first>Karim</first><last>Bouzoubaa</last></author>
      <author><first>Younes</first><last>Jaafar</last></author>
      <author><first>Driss</first><last>Namly</last></author>
      <author><first>Ridouane</first><last>Tachicart</last></author>
      <author><first>Rachida</first><last>Tajmout</last></author>
      <author><first>Hakima</first><last>Khamar</last></author>
      <author><first>Hamid</first><last>Jaafar</last></author>
      <author><first>Lhoussain</first><last>Aouragh</last></author>
      <author><first>Abdellah</first><last>Yousfi</last></author>
      <pages>127–134</pages>
      <abstract>Several tools and resources have been developed to deal with Arabic NLP. However, a homogenous and flexible Arabic environment that gathers these <a href="https://en.wikipedia.org/wiki/Component-based_software_engineering">components</a> is rarely available. In this perspective, we introduce SAFAR which is a monolingual framework developed in accordance with <a href="https://en.wikipedia.org/wiki/Software_requirements">software engineering requirements</a> and dedicated to <a href="https://en.wikipedia.org/wiki/Arabic">Arabic language</a>, especially, the modern standard Arabic and Moroccan dialect. After one decade of integration and development, SAFAR possesses today more than 50 tools and resources that can be exploited either using its <a href="https://en.wikipedia.org/wiki/Application_programming_interface">API</a> or using its <a href="https://en.wikipedia.org/wiki/User_interface">web interface</a>.</abstract>
      <url hash="e08e0b12">2021.eacl-demos.16</url>
      <bibkey>bouzoubaa-etal-2021-description</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.16</doi>
    </paper>
    <paper id="19">
      <title>LOME : Large Ontology Multilingual Extraction<fixed-case>LOME</fixed-case>: Large Ontology Multilingual Extraction</title>
      <author><first>Patrick</first><last>Xia</last></author>
      <author><first>Guanghui</first><last>Qin</last></author>
      <author><first>Siddharth</first><last>Vashishtha</last></author>
      <author><first>Yunmo</first><last>Chen</last></author>
      <author><first>Tongfei</first><last>Chen</last></author>
      <author><first>Chandler</first><last>May</last></author>
      <author><first>Craig</first><last>Harman</last></author>
      <author><first>Kyle</first><last>Rawlins</last></author>
      <author><first>Aaron Steven</first><last>White</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>149–159</pages>
      <abstract>We present LOME, a <a href="https://en.wikipedia.org/wiki/System">system</a> for performing multilingual information extraction. Given a text document as input, our core system identifies spans of textual entity and event mentions with a FrameNet (Baker et al., 1998) parser. It subsequently performs <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>, fine-grained entity typing, and temporal relation prediction between events. By doing so, the <a href="https://en.wikipedia.org/wiki/System">system</a> constructs an event and entity focused knowledge graph. We can further apply <a href="https://en.wikipedia.org/wiki/Modular_programming">third-party modules</a> for other types of <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a>, like <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a>. Our (multilingual) first-party modules either outperform or are competitive with the (monolingual) state-of-the-art. We achieve this through the use of multilingual encoders like XLM-R (Conneau et al., 2020) and leveraging multilingual training data. LOME is available as a Docker container on Docker Hub. In addition, a lightweight version of the <a href="https://en.wikipedia.org/wiki/System">system</a> is accessible as a web demo.</abstract>
      <url hash="13cf724f">2021.eacl-demos.19</url>
      <bibkey>xia-etal-2021-lome</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.19</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="21">
      <title>Graph Matching and Graph Rewriting : GREW tools for corpus exploration, maintenance and conversion<fixed-case>GREW</fixed-case> tools for corpus exploration, maintenance and conversion</title>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <pages>168–175</pages>
      <abstract>This article presents a set of tools built around the Graph Rewriting computational framework which can be used to compute complex rule-based transformations on linguistic structures. Application of the graph matching mechanism for <a href="https://en.wikipedia.org/wiki/Corpus_linguistics">corpus exploration</a>, error mining or quantitative typology are also given.</abstract>
      <url hash="d1a57181">2021.eacl-demos.21</url>
      <bibkey>guillaume-2021-graph</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.21</doi>
    </paper>
    <paper id="22">
      <title>Massive Choice, Ample Tasks (MaChAmp): A Toolkit for Multi-task Learning in NLP<fixed-case>M</fixed-case>a<fixed-case>C</fixed-case>h<fixed-case>A</fixed-case>mp): A Toolkit for Multi-task Learning in <fixed-case>NLP</fixed-case></title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Ahmet</first><last>Üstün</last></author>
      <author><first>Alan</first><last>Ramponi</last></author>
      <author><first>Ibrahim</first><last>Sharaf</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>176–197</pages>
      <abstract>Transfer learning, particularly approaches that combine <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> with pre-trained contextualized embeddings and <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>, have advanced the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a> tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of natural language processing tasks in a uniform toolkit, from text classification and sequence labeling to dependency parsing, masked language modeling, and text generation.</abstract>
      <url hash="90821c0d">2021.eacl-demos.22</url>
      <bibkey>van-der-goot-etal-2021-massive</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.22</doi>
      <pwccode url="https://github.com/machamp-nlp/machamp" additional="true">machamp-nlp/machamp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="26">
      <title>European Language Grid : A Joint Platform for the European Language Technology Community<fixed-case>E</fixed-case>uropean Language Grid: A Joint Platform for the <fixed-case>E</fixed-case>uropean Language Technology Community</title>
      <author><first>Georg</first><last>Rehm</last></author>
      <author><first>Stelios</first><last>Piperidis</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <author><first>Jan</first><last>Hajic</last></author>
      <author><first>Victoria</first><last>Arranz</last></author>
      <author><first>Andrejs</first><last>Vasiļjevs</last></author>
      <author><first>Gerhard</first><last>Backfried</last></author>
      <author><first>Jose Manuel</first><last>Gomez-Perez</last></author>
      <author><first>Ulrich</first><last>Germann</last></author>
      <author><first>Rémi</first><last>Calizzano</last></author>
      <author><first>Nils</first><last>Feldhus</last></author>
      <author><first>Stefanie</first><last>Hegele</last></author>
      <author><first>Florian</first><last>Kintzel</last></author>
      <author><first>Katrin</first><last>Marheinecke</last></author>
      <author><first>Julian</first><last>Moreno-Schneider</last></author>
      <author><first>Dimitris</first><last>Galanis</last></author>
      <author><first>Penny</first><last>Labropoulou</last></author>
      <author><first>Miltos</first><last>Deligiannis</last></author>
      <author><first>Katerina</first><last>Gkirtzou</last></author>
      <author><first>Athanasia</first><last>Kolovou</last></author>
      <author><first>Dimitris</first><last>Gkoumas</last></author>
      <author><first>Leon</first><last>Voukoutis</last></author>
      <author><first>Ian</first><last>Roberts</last></author>
      <author><first>Jana</first><last>Hamrlova</last></author>
      <author><first>Dusan</first><last>Varis</last></author>
      <author><first>Lukas</first><last>Kacena</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Valérie</first><last>Mapelli</last></author>
      <author><first>Mickaël</first><last>Rigault</last></author>
      <author><first>Julija</first><last>Melnika</last></author>
      <author><first>Miro</first><last>Janosik</last></author>
      <author><first>Katja</first><last>Prinz</last></author>
      <author><first>Andres</first><last>Garcia-Silva</last></author>
      <author><first>Cristian</first><last>Berrio</last></author>
      <author><first>Ondrej</first><last>Klejch</last></author>
      <author><first>Steve</first><last>Renals</last></author>
      <pages>221–230</pages>
      <abstract>Europe is a <a href="https://en.wikipedia.org/wiki/Multilingualism">multilingual society</a>, in which dozens of languages are spoken. The only option to enable and to benefit from <a href="https://en.wikipedia.org/wiki/Multilingualism">multilingualism</a> is through <a href="https://en.wikipedia.org/wiki/Language_technology">Language Technologies (LT)</a>, i.e., <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a> and <a href="https://en.wikipedia.org/wiki/Speech_technology">Speech Technologies</a>. We describe the European Language Grid (ELG), which is targeted to evolve into the primary platform and marketplace for LT in Europe by providing one umbrella platform for the European LT landscape, including research and industry, enabling all stakeholders to upload, share and distribute their services, products and resources. At the end of our EU project, which will establish a legal entity in 2022, the ELG will provide access to approx. 1300 services for all European languages as well as thousands of data sets.</abstract>
      <url hash="813f2e1a">2021.eacl-demos.26</url>
      <bibkey>rehm-etal-2021-european</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.26</doi>
    </paper>
    <paper id="27">
      <title>A New Surprise Measure for Extracting Interesting Relationships between Persons</title>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Jingun</first><last>Kwon</last></author>
      <author><first>Young-In</first><last>Song</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>231–237</pages>
      <abstract>One way to enhance user engagement in <a href="https://en.wikipedia.org/wiki/Web_search_engine">search engines</a> is to suggest interesting facts to the user. Although relationships between persons are important as a target for <a href="https://en.wikipedia.org/wiki/Text_mining">text mining</a>, there are few effective approaches for extracting the interesting relationships between persons. We therefore propose a method for extracting interesting relationships between persons from natural language texts by focusing on their <a href="https://en.wikipedia.org/wiki/Surprise_(emotion)">surprisingness</a>. Our method first extracts all personal relationships from dependency trees for the texts and then calculates surprise scores for distributed representations of the extracted relationships in an unsupervised manner. The unique point of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is that it does not require any labeled dataset with <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> for the surprising personal relationships. The results of the human evaluation show that the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> could extract more interesting relationships between persons from <a href="https://en.wikipedia.org/wiki/Japanese_Wikipedia">Japanese Wikipedia articles</a> than a popularity-based baseline method. We demonstrate our proposed <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> as a chrome plugin on <a href="https://en.wikipedia.org/wiki/Google_Search">google search</a>.</abstract>
      <url hash="04bfcaf1">2021.eacl-demos.27</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e391256b">2021.eacl-demos.27.OptionalSupplementaryMaterial.pdf</attachment>
      <revision id="1" href="2021.eacl-demos.27v1" hash="2f0652cf" />
      <revision id="2" href="2021.eacl-demos.27v2" hash="04bfcaf1" date="2021-04-20">Added missing reference page</revision>
      <bibkey>kamigaito-etal-2021-new</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.27</doi>
    </paper>
    <paper id="28">
      <title>Paladin : an annotation tool based on active and proactive learning</title>
      <author><first>Minh-Quoc</first><last>Nghiem</last></author>
      <author><first>Paul</first><last>Baylis</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <pages>238–243</pages>
      <abstract>In this paper, we present Paladin, an open-source web-based annotation tool for creating high-quality multi-label document-level datasets. By integrating <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a> and <a href="https://en.wikipedia.org/wiki/Proactive_learning">proactive learning</a> to the annotation task, <a href="https://en.wikipedia.org/wiki/Paladin">Paladin</a> makes the task less time-consuming and requiring less human effort. Although <a href="https://en.wikipedia.org/wiki/Paladin">Paladin</a> is designed for multi-label settings, the <a href="https://en.wikipedia.org/wiki/System">system</a> is flexible and can be adapted to other tasks in single-label settings.</abstract>
      <url hash="8320172b">2021.eacl-demos.28</url>
      <bibkey>nghiem-etal-2021-paladin</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.28</doi>
    </paper>
    <paper id="29">
      <title>Story Centaur : Large Language Model Few Shot Learning as a Creative Writing Tool</title>
      <author><first>Ben</first><last>Swanson</last></author>
      <author><first>Kory</first><last>Mathewson</last></author>
      <author><first>Ben</first><last>Pietrzak</last></author>
      <author><first>Sherol</first><last>Chen</last></author>
      <author><first>Monica</first><last>Dinalescu</last></author>
      <pages>244–256</pages>
      <abstract>Few shot learning with large language models has the potential to give individuals without formal <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> training the access to a wide range of text to text models. We consider how this applies to creative writers and present Story Centaur, a user interface for prototyping few shot models and a set of recombinable web components that deploy them. Story Centaur’s goal is to expose creative writers to few shot learning with a simple but powerful interface that lets them compose their own co-creation tools that further their own unique artistic directions. We build out several examples of such tools, and in the process probe the boundaries and issues surrounding generation with large language models.</abstract>
      <url hash="661edeef">2021.eacl-demos.29</url>
      <bibkey>swanson-etal-2021-story</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.29</doi>
    </paper>
    <paper id="31">
      <title>OCTIS : Comparing and Optimizing Topic models is Simple !<fixed-case>OCTIS</fixed-case>: Comparing and Optimizing Topic models is Simple!</title>
      <author><first>Silvia</first><last>Terragni</last></author>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <author><first>Bruno Giovanni</first><last>Galuzzi</last></author>
      <author><first>Pietro</first><last>Tropeano</last></author>
      <author><first>Antonio</first><last>Candelieri</last></author>
      <pages>263–270</pages>
      <abstract>In this paper, we present OCTIS, a framework for training, analyzing, and comparing Topic Models, whose optimal hyper-parameters are estimated using a Bayesian Optimization approach. The proposed <a href="https://en.wikipedia.org/wiki/Solution">solution</a> integrates several state-of-the-art <a href="https://en.wikipedia.org/wiki/Topic_model">topic models</a> and evaluation metrics. These <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> can be targeted as objective by the underlying <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">optimization procedure</a> to determine the best hyper-parameter configuration. OCTIS allows researchers and practitioners to have a fair comparison between topic models of interest, using several benchmark datasets and well-known evaluation metrics, to integrate novel algorithms, and to have an interactive visualization of the results for understanding the behavior of each model. The code is available at the following link : https://github.com/MIND-Lab/OCTIS.</abstract>
      <url hash="bdff935d">2021.eacl-demos.31</url>
      <bibkey>terragni-etal-2021-octis</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.31</doi>
      <pwccode url="https://github.com/mind-Lab/octis" additional="false">mind-Lab/octis</pwccode>
    </paper>
    <paper id="33">
      <title>Breaking Writer’s Block : Low-cost Fine-tuning of Natural Language Generation Models</title>
      <author><first>Alexandre</first><last>Duval</last></author>
      <author><first>Thomas</first><last>Lamson</last></author>
      <author><first>Gaël</first><last>de Léséleuc de Kérouara</last></author>
      <author><first>Matthias</first><last>Gallé</last></author>
      <pages>278–287</pages>
      <abstract>It is standard procedure these days to solve Information Extraction task by fine-tuning large pre-trained language models. This is not the case for generation task, which relies on a variety of techniques for controlled language generation. In this paper, we describe a <a href="https://en.wikipedia.org/wiki/System">system</a> that fine-tunes a <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation model</a> for the problem of solving writer’s block. The fine-tuning changes the conditioning to also include the right context in addition to the left context, as well as an optional list of entities, the size, the genre and a summary of the paragraph that the human author wishes to generate. Our proposed <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> obtains excellent results, even with a small number of epochs and a total cost of USD 150. The <a href="https://en.wikipedia.org/wiki/System">system</a> can be accessed as a web-service and all the code is released. A video showcasing the interface and the <a href="https://en.wikipedia.org/wiki/Physical_model">model</a> is also available.</abstract>
      <url hash="6822d1ba">2021.eacl-demos.33</url>
      <bibkey>duval-etal-2021-breaking</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.33</doi>
    </paper>
    <paper id="35">
      <title>Domain Expert Platform for Goal-Oriented Dialog Collection</title>
      <author><first>Didzis</first><last>Goško</last></author>
      <author><first>Arturs</first><last>Znotins</last></author>
      <author><first>Inguna</first><last>Skadina</last></author>
      <author><first>Normunds</first><last>Gruzitis</last></author>
      <author><first>Gunta</first><last>Nešpore-Bērzkalne</last></author>
      <pages>295–301</pages>
      <abstract>Today, most <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue systems</a> are fully or partly built using neural network architectures. A crucial prerequisite for the creation of a goal-oriented neural network dialogue system is a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> that represents typical dialogue scenarios and includes various <a href="https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)">semantic annotations</a>, e.g. intents, slots and dialogue actions, that are necessary for training a particular neural network architecture. In this demonstration paper, we present an easy to use interface and its back-end which is oriented to domain experts for the collection of goal-oriented dialogue samples. The <a href="https://en.wikipedia.org/wiki/Computing_platform">platform</a> not only allows to collect or write sample dialogues in a structured way, but also provides a means for simple annotation and interpretation of the dialogues. The <a href="https://en.wikipedia.org/wiki/Computing_platform">platform</a> itself is language-independent ; it depends only on the availability of particular <a href="https://en.wikipedia.org/wiki/Component-based_software_engineering">language processing components</a> for a specific language. It is currently being used to collect dialogue samples in <a href="https://en.wikipedia.org/wiki/Latvian_language">Latvian</a> (a highly inflected language) which represent typical communication between students and the student service.</abstract>
      <url hash="193b8deb">2021.eacl-demos.35</url>
      <bibkey>gosko-etal-2021-domain</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.35</doi>
    </paper>
    <paper id="36">
      <title>Which is Better for <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> : <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a> or <a href="https://en.wikipedia.org/wiki/MATLAB">MATLAB</a>? Answering Comparative Questions in Natural Language<fixed-case>MATLAB</fixed-case>? Answering Comparative Questions in Natural Language</title>
      <author><first>Viktoriia</first><last>Chekalina</last></author>
      <author><first>Alexander</first><last>Bondarenko</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Meriem</first><last>Beloucif</last></author>
      <author><first>Varvara</first><last>Logacheva</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>302–311</pages>
      <abstract>We present a <a href="https://en.wikipedia.org/wiki/System">system</a> for answering comparative questions (Is X better than Y with respect to Z?) in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>. Answering such questions is important for assisting humans in making informed decisions. The key component of our system is a <a href="https://en.wikipedia.org/wiki/Natural-language_user_interface">natural language interface</a> for comparative QA that can be used in <a href="https://en.wikipedia.org/wiki/Personal_assistant">personal assistants</a>, <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a>, and similar <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP devices</a>. Comparative QA is a challenging NLP task, since it requires collecting support evidence from many different sources, and direct comparisons of rare objects may be not available even on the entire Web. We take the first step towards a solution for such a task offering a testbed for comparative QA in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a> by probing several methods, making the three best ones available as an online demo.</abstract>
      <url hash="57585e54">2021.eacl-demos.36</url>
      <bibkey>chekalina-etal-2021-better</bibkey>
      <doi>10.18653/v1/2021.eacl-demos.36</doi>
    </paper>
    </volume>
  <volume id="srw" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop</booktitle>
      <editor><first>Ionut-Teodor</first><last>Sorodoc</last></editor>
      <editor><first>Madhumita</first><last>Sushil</last></editor>
      <editor><first>Ece</first><last>Takmaz</last></editor>
      <editor><first>Eneko</first><last>Agirre</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="fa4be831">2021.eacl-srw.0</url>
      <bibkey>eacl-2021-european-chapter-association</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Computationally Efficient Wasserstein Loss for Structured Labels<fixed-case>W</fixed-case>asserstein Loss for Structured Labels</title>
      <author><first>Ayato</first><last>Toyokuni</last></author>
      <author><first>Sho</first><last>Yokoi</last></author>
      <author><first>Hisashi</first><last>Kashima</last></author>
      <author><first>Makoto</first><last>Yamada</last></author>
      <pages>1–7</pages>
      <abstract>The problem of estimating the probability distribution of labels has been widely studied as a label distribution learning (LDL) problem, whose applications include age estimation, emotion analysis, and semantic segmentation. We propose a tree-Wasserstein distance regularized LDL algorithm, focusing on hierarchical text classification tasks. We propose predicting the entire label hierarchy using <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>, where the similarity between predicted and true labels is measured using the tree-Wasserstein distance. Through experiments using synthetic and real-world datasets, we demonstrate that the proposed method successfully considers the structure of labels during training, and it compares favorably with the Sinkhorn algorithm in terms of <a href="https://en.wikipedia.org/wiki/Time_complexity">computation time</a> and <a href="https://en.wikipedia.org/wiki/Memory_complexity">memory usage</a>.</abstract>
      <url hash="c769505a">2021.eacl-srw.1</url>
      <bibkey>toyokuni-etal-2021-computationally</bibkey>
      <doi>10.18653/v1/2021.eacl-srw.1</doi>
    </paper>
    <paper id="2">
      <title>Have Attention Heads in BERT Learned Constituency Grammar?<fixed-case>BERT</fixed-case> Learned Constituency Grammar?</title>
      <author><first>Ziyang</first><last>Luo</last></author>
      <pages>8–15</pages>
      <abstract>With the success of pre-trained language models in recent years, more and more researchers focus on opening the black box of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Following this interest, we carry out a qualitative and quantitative analysis of <a href="https://en.wikipedia.org/wiki/Constituency_grammar">constituency grammar</a> in attention heads of BERT and RoBERTa. We employ the syntactic distance method to extract implicit constituency grammar from the attention weights of each head. Our results show that there exist <a href="https://en.wikipedia.org/wiki/Head_(linguistics)">heads</a> that can induce some grammar types much better than baselines, suggesting that some <a href="https://en.wikipedia.org/wiki/Head_(linguistics)">heads</a> act as a proxy for <a href="https://en.wikipedia.org/wiki/Constituency_grammar">constituency grammar</a>. We also analyze how attention heads’ constituency grammar inducing (CGI) ability changes after fine-tuning with two kinds of tasks, including sentence meaning similarity (SMS) tasks and natural language inference (NLI) tasks. Our results suggest that SMS tasks decrease the average CGI ability of upper layers, while NLI tasks increase it. Lastly, we investigate the connections between CGI ability and natural language understanding ability on QQP and MNLI tasks.</abstract>
      <url hash="8db16ee9">2021.eacl-srw.2</url>
      <bibkey>luo-2021-attention</bibkey>
      <doi>10.18653/v1/2021.eacl-srw.2</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="3">
      <title>Do we read what we hear? Modeling orthographic influences on <a href="https://en.wikipedia.org/wiki/Spoken_word_recognition">spoken word recognition</a></title>
      <author><first>Nicole</first><last>Macher</last></author>
      <author><first>Badr M.</first><last>Abdullah</last></author>
      <author><first>Harm</first><last>Brouwer</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>16–22</pages>
      <abstract>Theories and models of <a href="https://en.wikipedia.org/wiki/Spoken_word_recognition">spoken word recognition</a> aim to explain the process of accessing lexical knowledge given an acoustic realization of a word form. There is consensus that <a href="https://en.wikipedia.org/wiki/Phonology">phonological and semantic information</a> is crucial for this <a href="https://en.wikipedia.org/wiki/Process_(philosophy)">process</a>. However, there is accumulating evidence that orthographic information could also have an impact on auditory word recognition. This paper presents two <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> of <a href="https://en.wikipedia.org/wiki/Spoken_word_recognition">spoken word recognition</a> that instantiate different hypotheses regarding the influence of <a href="https://en.wikipedia.org/wiki/Orthography">orthography</a> on this process. We show that these models reproduce human-like behavior in different ways and provide testable hypotheses for future research on the source of orthographic effects in spoken word recognition.</abstract>
      <url hash="9d9abec8">2021.eacl-srw.3</url>
      <bibkey>macher-etal-2021-read</bibkey>
      <doi>10.18653/v1/2021.eacl-srw.3</doi>
    </paper>
    <paper id="7">
      <title>Automatically Cataloging Scholarly Articles using Library of Congress Subject Headings</title>
      <author><first>Nazmul</first><last>Kazi</last></author>
      <author><first>Nathaniel</first><last>Lane</last></author>
      <author><first>Indika</first><last>Kahanda</last></author>
      <pages>43–49</pages>
      <abstract>Institutes are required to catalog their articles with proper subject headings so that the users can easily retrieve relevant articles from the <a href="https://en.wikipedia.org/wiki/Institutional_repository">institutional repositories</a>. However, due to the rate of proliferation of the number of articles in these repositories, it is becoming a challenge to manually catalog the newly added articles at the same pace. To address this challenge, we explore the feasibility of automatically annotating articles with Library of Congress Subject Headings (LCSH). We first use <a href="https://en.wikipedia.org/wiki/Web_scraping">web scraping</a> to extract <a href="https://en.wikipedia.org/wiki/Index_term">keywords</a> for a collection of articles from the Repository Analytics and Metrics Portal (RAMP). Then, we map these <a href="https://en.wikipedia.org/wiki/Index_term">keywords</a> to LCSH names for developing a gold-standard dataset. As a case study, using the subset of Biology-related LCSH concepts, we develop <a href="https://en.wikipedia.org/wiki/Predictive_modelling">predictive models</a> by formulating this task as a multi-label classification problem. Our experimental results demonstrate the viability of this <a href="https://en.wikipedia.org/wiki/Scientific_method">approach</a> for predicting LCSH for <a href="https://en.wikipedia.org/wiki/Academic_publishing">scholarly articles</a>.</abstract>
      <url hash="2b438779">2021.eacl-srw.7</url>
      <bibkey>kazi-etal-2021-automatically</bibkey>
      <doi>10.18653/v1/2021.eacl-srw.7</doi>
    </paper>
    <paper id="11">
      <title>Contrasting distinct structured views to learn sentence embeddings</title>
      <author><first>Antoine</first><last>Simoulin</last></author>
      <author><first>Benoit</first><last>Crabbé</last></author>
      <pages>71–79</pages>
      <abstract>We propose a self-supervised method that builds sentence embeddings from the combination of diverse explicit syntactic structures of a sentence. We assume <a href="https://en.wikipedia.org/wiki/Structure">structure</a> is crucial to building consistent representations as we expect <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence meaning</a> to be a function of both syntax and semantic aspects. In this perspective, we hypothesize that some linguistic representations might be better adapted given the considered task or sentence. We, therefore, propose to learn individual representation functions for different syntactic frameworks jointly. Again, by hypothesis, all such <a href="https://en.wikipedia.org/wiki/Function_(mathematics)">functions</a> should encode similar semantic information differently and consequently, be complementary for building better sentential semantic embeddings. To assess such hypothesis, we propose an original contrastive multi-view framework that induces an explicit interaction between models during the training phase. We make experiments combining various structures such as dependency, constituency, or sequential schemes. Our results outperform comparable <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> on several <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> from standard sentence embedding benchmarks.</abstract>
      <url hash="1c9fb6db">2021.eacl-srw.11</url>
      <bibkey>simoulin-crabbe-2021-contrasting</bibkey>
      <doi>10.18653/v1/2021.eacl-srw.11</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="12">
      <title>Discrete Reasoning Templates for Natural Language Understanding</title>
      <author><first>Hadeel</first><last>Al-Negheimish</last></author>
      <author><first>Pranava</first><last>Madhyastha</last></author>
      <author><first>Alessandra</first><last>Russo</last></author>
      <pages>80–87</pages>
      <abstract>Reasoning about information from multiple parts of a passage to derive an answer is an open challenge for reading-comprehension models. In this paper, we present an approach that reasons about complex questions by decomposing them to simpler subquestions that can take advantage of single-span extraction reading-comprehension models, and derives the final answer according to instructions in a predefined reasoning template. We focus on subtraction based arithmetic questions and evaluate our approach on a subset of the DROP dataset. We show that our approach is competitive with the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state of the art</a> while being interpretable and requires little <a href="https://en.wikipedia.org/wiki/Supervisor">supervision</a>.</abstract>
      <url hash="f2a8a80a">2021.eacl-srw.12</url>
      <bibkey>al-negheimish-etal-2021-discrete</bibkey>
      <doi>10.18653/v1/2021.eacl-srw.12</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="17">
      <title>Development of Conversational AI for Sleep Coaching Programme<fixed-case>AI</fixed-case> for Sleep Coaching Programme</title>
      <author><first>Heereen</first><last>Shim</last></author>
      <pages>121–128</pages>
      <abstract>Almost 30 % of the adult population in the world is experiencing or has experience insomnia. Cognitive Behaviour Therapy for insomnia (CBT-I) is one of the most effective treatment, but it has limitations on accessibility and availability. Utilising <a href="https://en.wikipedia.org/wiki/Technology">technology</a> is one of the possible solutions, but existing methods neglect conversational aspects, which plays a critical role in <a href="https://en.wikipedia.org/wiki/Sleep_therapy">sleep therapy</a>. To address this issue, we propose a PhD project exploring potentials of developing conversational artificial intelligence (AI) for a sleep coaching programme, which is motivated by CBT-I treatment. This PhD project aims to develop natural language processing (NLP) algorithms to allow the <a href="https://en.wikipedia.org/wiki/System">system</a> to interact naturally with a user and provide automated analytic system to support human experts. In this paper, we introduce research questions lying under three phases of the sleep coaching programme : triaging, monitoring the progress, and providing <a href="https://en.wikipedia.org/wiki/Coaching">coaching</a>. We expect this research project’s outcomes could contribute to the research domains of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> and <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI</a> but also the healthcare field by providing a more accessible and affordable sleep treatment solution and an automated analytic system to lessen the burden of human experts.</abstract>
      <url hash="09c3a2d8">2021.eacl-srw.17</url>
      <bibkey>shim-2021-development</bibkey>
      <doi>10.18653/v1/2021.eacl-srw.17</doi>
    </paper>
    <paper id="18">
      <title>Relating Relations : Meta-Relation Extraction from Online Health Forum Posts</title>
      <author><first>Daniel</first><last>Stickley</last></author>
      <pages>129–136</pages>
      <abstract>Relation extraction is a key task in <a href="https://en.wikipedia.org/wiki/Knowledge_extraction">knowledge extraction</a>, and is commonly defined as the task of identifying relations that hold between entities in text. This thesis proposal addresses the specific task of identifying meta-relations, a higher order family of relations naturally construed as holding between other relations which includes temporal, comparative, and causal relations. More specifically, we aim to develop theoretical underpinnings and practical solutions for the challenges of (1) incorporating meta-relations into conceptualisations and annotation schemes for (lower-order) relations and named entities, (2) obtaining annotations for them with tolerable cognitive load on annotators, (3) creating models capable of reliably extracting meta-relations, and related to that (4) addressing the limited-data problem exacerbated by the introduction of meta-relations into the learning task. We explore recent works in relation extraction and discuss our plans to formally conceptualise meta-relations for the domain of user-generated health texts, and create a new dataset, annotation scheme and models for meta-relation extraction.</abstract>
      <url hash="f56d1917">2021.eacl-srw.18</url>
      <bibkey>stickley-2021-relating</bibkey>
      <doi>10.18653/v1/2021.eacl-srw.18</doi>
    </paper>
    <paper id="24">
      <title>Beyond the English Web : Zero-Shot Cross-Lingual and Lightweight Monolingual Classification of Registers<fixed-case>E</fixed-case>nglish Web: Zero-Shot Cross-Lingual and Lightweight Monolingual Classification of Registers</title>
      <author><first>Liina</first><last>Repo</last></author>
      <author><first>Valtteri</first><last>Skantsi</last></author>
      <author><first>Samuel</first><last>Rönnqvist</last></author>
      <author><first>Saara</first><last>Hellström</last></author>
      <author><first>Miika</first><last>Oinonen</last></author>
      <author><first>Anna</first><last>Salmela</last></author>
      <author><first>Douglas</first><last>Biber</last></author>
      <author><first>Jesse</first><last>Egbert</last></author>
      <author><first>Sampo</first><last>Pyysalo</last></author>
      <author><first>Veronika</first><last>Laippala</last></author>
      <pages>183–191</pages>
      <abstract>We explore cross-lingual transfer of register classification for <a href="https://en.wikipedia.org/wiki/Web_page">web documents</a>. Registers, that is, text varieties such as <a href="https://en.wikipedia.org/wiki/Blog">blogs</a> or <a href="https://en.wikipedia.org/wiki/News">news</a> are one of the primary predictors of <a href="https://en.wikipedia.org/wiki/Variation_(linguistics)">linguistic variation</a> and thus affect the automatic processing of language. We introduce two new register-annotated corpora, FreCORE and SweCORE, for <a href="https://en.wikipedia.org/wiki/French_language">French</a> and <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a>. We demonstrate that deep pre-trained language models perform strongly in these languages and outperform previous <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> in <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Finnish_language">Finnish</a>. Specifically, we show 1) that zero-shot cross-lingual transfer from the large English CORE corpus can match or surpass previously published monolingual models, and 2) that lightweight monolingual classification requiring very little training data can reach or surpass our zero-shot performance. We further analyse classification results finding that certain registers continue to pose challenges in particular for cross-lingual transfer.</abstract>
      <url hash="ca6cc7ee">2021.eacl-srw.24</url>
      <bibkey>repo-etal-2021-beyond</bibkey>
      <doi>10.18653/v1/2021.eacl-srw.24</doi>
      <pwccode url="https://github.com/TurkuNLP/Multilingual-register-corpora" additional="false">TurkuNLP/Multilingual-register-corpora</pwccode>
    </paper>
    <paper id="26">
      <title>Why Find the Right One?</title>
      <author><first>Payal</first><last>Khullar</last></author>
      <pages>203–208</pages>
      <abstract>The present paper investigates the impact of the anaphoric one words in English on the Neural Machine Translation (NMT) process using English-Hindi as source and target language pair. As expected, the experimental results show that the state-of-the-art Google English-Hindi NMT system achieves significantly poorly on sentences containing anaphoric ones as compared to the sentences containing regular, non-anaphoric ones. But, more importantly, we note that amongst the <a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)">anaphoric words</a>, the <a href="https://en.wikipedia.org/wiki/Noun_class">noun class</a> is clearly much harder for NMT than the <a href="https://en.wikipedia.org/wiki/Determinative">determinatives</a>. This reaffirms the linguistic disparity of the two phenomenon in recent theoretical syntactic literature, despite the obvious surface similarities.</abstract>
      <url hash="1602c04a">2021.eacl-srw.26</url>
      <bibkey>khullar-2021-find</bibkey>
      <doi>10.18653/v1/2021.eacl-srw.26</doi>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts</booktitle>
      <editor><first>Isabelle</first><last>Augenstein</last></editor>
      <editor><first>Ivan</first><last>Habernal</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>online</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="772b85bd">2021.eacl-tutorials.0</url>
      <bibkey>eacl-2021-european-chapter-association-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Unsupervised Natural Language Parsing (Introductory Tutorial)</title>
      <author><first>Kewei</first><last>Tu</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Wenjuan</first><last>Han</last></author>
      <author><first>Yanpeng</first><last>Zhao</last></author>
      <pages>1–5</pages>
      <abstract>Unsupervised parsing learns a syntactic parser from training sentences without parse tree annotations. Recently, there has been a resurgence of interest in unsupervised parsing, which can be attributed to the combination of two trends in the NLP community : a general trend towards <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised training</a> or pre-training, and an emerging trend towards finding or modeling linguistic structures in neural models. In this tutorial, we will introduce to the general audience what unsupervised parsing does and how it can be useful for and beyond <a href="https://en.wikipedia.org/wiki/Syntactic_parsing">syntactic parsing</a>. We will then provide a systematic overview of major classes of approaches to unsupervised parsing, namely generative and discriminative approaches, and analyze their relative strengths and weaknesses. We will cover both decade-old statistical approaches and more recent neural approaches to give the audience a sense of the historical and recent development of the field. We will also discuss emerging research topics such as BERT-based approaches and visually grounded learning.</abstract>
      <url hash="3438d23c">2021.eacl-tutorials.1</url>
      <bibkey>tu-etal-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.eacl-tutorials.1</doi>
    </paper>
    <paper id="2">
      <title>Aggregating and Learning from Multiple Annotators</title>
      <author><first>Silviu</first><last>Paun</last></author>
      <author><first>Edwin</first><last>Simpson</last></author>
      <pages>6–9</pages>
      <abstract>The success of NLP research is founded on high-quality annotated datasets, which are usually obtained from multiple expert annotators or crowd workers. The standard practice to training machine learning models is to first adjudicate the disagreements and then perform the <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training</a>. To this end, there has been a lot of work on aggregating annotations, particularly for classification tasks. However, many other tasks, particularly in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, have unique characteristics not considered by standard models of <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a>, e.g., label interdependencies in sequence labelling tasks, unrestricted labels for <a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)">anaphoric annotation</a>, or preference labels for ranking texts. In recent years, researchers have picked up on this and are covering the gap. A first objective of this tutorial is to connect NLP researchers with state-of-the-art aggregation models for a diverse set of canonical language annotation tasks. There is also a growing body of recent work arguing that following the convention and training with adjudicated labels ignores any uncertainty the labellers had in their classifications, which results in models with poorer generalisation capabilities. Therefore, a second objective of this tutorial is to teach NLP workers how they can augment their (deep) neural models to learn from data with multiple interpretations.</abstract>
      <url hash="c5106835">2021.eacl-tutorials.2</url>
      <bibkey>paun-simpson-2021-aggregating</bibkey>
      <doi>10.18653/v1/2021.eacl-tutorials.2</doi>
    </paper>
    <paper id="3">
      <title>Tutorial Proposal : End-to-End Speech Translation</title>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <pages>10–13</pages>
      <abstract>Speech translation is the translation of speech in one language typically to text in another, traditionally accomplished through a combination of <a href="https://en.wikipedia.org/wiki/Speech_recognition">automatic speech recognition</a> and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. Speech translation has attracted interest for many years, but the recent successful applications of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> to both individual tasks have enabled new opportunities through joint modeling, in what we today call ‘end-to-end speech translation.’ In this tutorial we will introduce the techniques used in cutting-edge research on <a href="https://en.wikipedia.org/wiki/Speech_translation">speech translation</a>. Starting from the traditional cascaded approach, we will given an overview on data sources and model architectures to achieve state-of-the art performance with end-to-end speech translation for both high- and low-resource languages. In addition, we will discuss methods to evaluate analyze the proposed solutions, as well as the challenges faced when applying speech translation models for real-world applications.</abstract>
      <url hash="29e48e66">2021.eacl-tutorials.3</url>
      <bibkey>niehues-etal-2021-tutorial</bibkey>
      <doi>10.18653/v1/2021.eacl-tutorials.3</doi>
    </paper>
    </volume>
</collection>