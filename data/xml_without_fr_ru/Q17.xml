<?xml version='1.0' encoding='utf-8'?>
<collection id="Q17">
  <volume id="1">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 5</booktitle>
      <editor><last>Lee</last><first>Lillian</first></editor>
      <editor><last>Johnson</last><first>Mark</first></editor>
      <editor><last>Toutanova</last><first>Kristina</first></editor>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2017</year>
    </meta>
    <frontmatter>
      <bibkey>tacl-2017-transactions</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Evaluating Visual Representations for Topic Understanding and Their Effects on Manually Generated Topic Labels</title>
      <author><first>Alison</first><last>Smith</last></author>
      <author><first>Tak Yeon</first><last>Lee</last></author>
      <author><first>Forough</first><last>Poursabzi-Sangdeh</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <author><first>Niklas</first><last>Elmqvist</last></author>
      <author><first>Leah</first><last>Findlater</last></author>
      <doi>10.1162/tacl_a_00042</doi>
      <abstract>Probabilistic topic models are important tools for <a href="https://en.wikipedia.org/wiki/Index_(publishing)">indexing</a>, summarizing, and analyzing large document collections by their themes. However, promoting end-user understanding of topics remains an open research problem. We compare labels generated by users given four topic visualization techniquesword lists, word lists with bars, word clouds, and network graphsagainst each other and against automatically generated labels. Our basis of comparison is participant ratings of how well labels describe documents from the topic. Our study has two phases : a labeling phase where participants label visualized topics and a validation phase where different participants select which labels best describe the topics’ documents. Although all <a href="https://en.wikipedia.org/wiki/Visualization_(graphics)">visualizations</a> produce similar quality labels, simple visualizations such as word lists allow participants to quickly understand topics, while complex visualizations take longer but expose multi-word expressions that simpler visualizations obscure. Automatic labels lag behind user-created labels, but our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of manually labeled topics highlights <a href="https://en.wikipedia.org/wiki/Pattern">linguistic patterns</a> (e.g., <a href="https://en.wikipedia.org/wiki/Hypernymy">hypernyms</a>, phrases) that can be used to improve automatic topic labeling algorithms.</abstract>
      <pages>1–16</pages>
      <url hash="44e19482">Q17-1001</url>
      <video href="https://vimeo.com/234957075" />
      <bibkey>smith-etal-2017-evaluating</bibkey>
    </paper>
    <paper id="2">
      <title>Visually Grounded and Textual Semantic Models Differentially Decode Brain Activity Associated with Concrete and Abstract Nouns</title>
      <author><first>Andrew J.</first><last>Anderson</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Stephen</first><last>Clark</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <doi>10.1162/tacl_a_00043</doi>
      <abstract>Important advances have recently been made using computational semantic models to decode brain activity patterns associated with <a href="https://en.wikipedia.org/wiki/Concept">concepts</a> ; however, this work has almost exclusively focused on concrete nouns. How well these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> extend to decoding <a href="https://en.wikipedia.org/wiki/Noun">abstract nouns</a> is largely unknown. We address this question by applying state-of-the-art computational models to decode functional Magnetic Resonance Imaging (fMRI) activity patterns, elicited by participants reading and imagining a diverse set of both concrete and abstract nouns. One of the <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> we use is linguistic, exploiting the recent word2vec skipgram approach trained on <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>. The <a href="https://en.wikipedia.org/wiki/Second">second</a> is visually grounded, using <a href="https://en.wikipedia.org/wiki/Deep_learning">deep convolutional neural networks</a> trained on <a href="https://en.wikipedia.org/wiki/Google_Images">Google Images</a>. Dual coding theory considers concrete concepts to be encoded in the brain both linguistically and visually, and abstract concepts only linguistically. Splitting the fMRI data according to human concreteness ratings, we indeed observe that both models significantly decode the most concrete nouns ; however, accuracy is significantly greater using the text-based models for the most abstract nouns. More generally this confirms that current <a href="https://en.wikipedia.org/wiki/Computational_model">computational models</a> are sufficiently advanced to assist in investigating the representational structure of <a href="https://en.wikipedia.org/wiki/Concept">abstract concepts</a> in the brain.</abstract>
      <pages>17–30</pages>
      <url hash="8fc5c1fb">Q17-1002</url>
      <video href="https://vimeo.com/234954554" />
      <bibkey>anderson-etal-2017-visually</bibkey>
    </paper>
    <paper id="3">
      <title>Modeling Semantic Expectation : Using Script Knowledge for Referent Prediction</title>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <author><first>Asad</first><last>Sayeed</last></author>
      <author><first>Manfred</first><last>Pinkal</last></author>
      <doi>10.1162/tacl_a_00044</doi>
      <abstract>Recent research in <a href="https://en.wikipedia.org/wiki/Psycholinguistics">psycholinguistics</a> has provided increasing evidence that humans predict upcoming content. Prediction also affects <a href="https://en.wikipedia.org/wiki/Perception">perception</a> and might be a key to robustness in <a href="https://en.wikipedia.org/wiki/Language_processing_in_the_brain">human language processing</a>. In this paper, we investigate the factors that affect human prediction by building a <a href="https://en.wikipedia.org/wiki/Computational_model">computational model</a> that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts. We find that script knowledge significantly improves model estimates of human predictions. In a second study, we test the highly controversial hypothesis that <a href="https://en.wikipedia.org/wiki/Predictability">predictability</a> influences referring expression type but do not find evidence for such an effect.</abstract>
      <pages>31–44</pages>
      <url hash="44b213d8">Q17-1003</url>
      <video href="https://vimeo.com/234958123" />
      <bibkey>modi-etal-2017-modeling</bibkey>
    </paper>
    <paper id="4">
      <title>Shift-Reduce Constituent Parsing with Neural Lookahead Features</title>
      <author><first>Jiangming</first><last>Liu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <doi>10.1162/tacl_a_00045</doi>
      <abstract>Transition-based models can be fast and accurate for <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">constituent parsing</a>. Compared with chart-based models, they leverage richer features by extracting history information from a parser stack, which consists of a sequence of non-local constituents. On the other hand, during incremental parsing, <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">constituent information</a> on the right hand side of the current word is not utilized, which is a relative weakness of <a href="https://en.wikipedia.org/wiki/Shift-reduce_parsing">shift-reduce parsing</a>. To address this limitation, we leverage a fast neural model to extract <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">lookahead features</a>. In particular, we build a bidirectional LSTM model, which leverages full sentence information to predict the hierarchy of constituents that each word starts and ends. The results are then passed to a strong transition-based constituent parser as <a href="https://en.wikipedia.org/wiki/Compiler-compiler">lookahead features</a>. The resulting <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> gives 1.3 % absolute improvement in WSJ and 2.3 % in CTB compared to the baseline, giving the highest reported accuracies for fully-supervised parsing.</abstract>
      <pages>45–58</pages>
      <url hash="61ee556c">Q17-1004</url>
      <bibkey>liu-zhang-2017-shift</bibkey>
      <pwccode url="https://github.com/SUTDNLP/LookAheadConparser" additional="false">SUTDNLP/LookAheadConparser</pwccode>
    </paper>
    <paper id="5">
      <title>A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding with a Fixed Distortion Limit</title>
      <author><first>Yin-Wen</first><last>Chang</last></author>
      <author><first>Michael</first><last>Collins</last></author>
      <doi>10.1162/tacl_a_00046</doi>
      <abstract>Decoding of phrase-based translation models in the general case is known to be NP-complete, by a reduction from the traveling salesman problem (Knight, 1999). In practice, phrase-based systems often impose a hard distortion limit that limits the movement of phrases during <a href="https://en.wikipedia.org/wiki/Translation">translation</a>. However, the impact on <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> after imposing such a <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraint</a> is not well studied. In this paper, we describe a <a href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming algorithm</a> for phrase-based decoding with a fixed distortion limit. The runtime of the <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> is O(nd!lhd+1) where n is the sentence length, d is the distortion limit, l is a bound on the number of phrases starting at any position in the sentence, and h is related to the maximum number of target language translations for any source word. The <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> makes use of a novel <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a> that gives a new perspective on decoding of phrase-based models.</abstract>
      <pages>59–71</pages>
      <url hash="c3f21bbc">Q17-1005</url>
      <video href="https://vimeo.com/234951458" />
      <attachment type="presentation" hash="613fb216">Q17-1005.Presentation.pdf</attachment>
      <bibkey>chang-collins-2017-polynomial</bibkey>
    </paper>
    <paper id="6">
      <title>A Generative Model of Phonotactics</title>
      <author><first>Richard</first><last>Futrell</last></author>
      <author><first>Adam</first><last>Albright</last></author>
      <author><first>Peter</first><last>Graff</last></author>
      <author><first>Timothy J.</first><last>O’Donnell</last></author>
      <doi>10.1162/tacl_a_00047</doi>
      <abstract>We present a probabilistic model of phonotactics, the set of well-formed phoneme sequences in a language. Unlike most computational models of phonotactics (Hayes and Wilson, 2008 ; Goldsmith and Riggle, 2012), we take a fully generative approach, modeling a process where forms are built up out of subparts by phonologically-informed structure building operations. We learn an inventory of subparts by applying stochastic memoization (Johnson et al., 2007 ; Goodman et al., 2008) to a generative process for phonemes structured as an and-or graph, based on concepts of feature hierarchy from <a href="https://en.wikipedia.org/wiki/Generative_phonology">generative phonology</a> (Clements, 1985 ; Dresher, 2009). Subparts are combined in a way that allows tier-based feature interactions. We evaluate our models’ ability to capture phonotactic distributions in the lexicons of 14 languages drawn from the WOLEX corpus (Graff, 2012). Our full <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> robustly assigns higher probabilities to held-out forms than a sophisticated N-gram model for all languages. We also present novel analyses that probe model behavior in more detail.</abstract>
      <pages>73–86</pages>
      <url hash="3d16dc63">Q17-1006</url>
      <video href="https://vimeo.com/234952732" />
      <bibkey>futrell-etal-2017-generative</bibkey>
    </paper>
    <paper id="7">
      <title>Context Gates for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a></title>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <author><first>Zhengdong</first><last>Lu</last></author>
      <author><first>Xiaohua</first><last>Liu</last></author>
      <author><first>Hang</first><last>Li</last></author>
      <doi>10.1162/tacl_a_00048</doi>
      <abstract>In <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation (NMT)</a>, generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts affect the <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a>. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to the lack of effective control over the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose context gates which dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance both the <a href="https://en.wikipedia.org/wiki/Adequality">adequacy</a> and <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a> of NMT with more careful control of the information flow from contexts. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.</abstract>
      <pages>87–99</pages>
      <url hash="4ba8862c">Q17-1007</url>
      <video href="https://vimeo.com/234951517" />
      <bibkey>tu-etal-2017-context</bibkey>
      <pwccode url="https://github.com/tuzhaopeng/nmt" additional="true">tuzhaopeng/nmt</pwccode>
    </paper>
    <paper id="9">
      <title>Automatically Tagging Constructions of Causation and Their Slot-Fillers</title>
      <author><first>Jesse</first><last>Dunietz</last></author>
      <author><first>Lori</first><last>Levin</last></author>
      <author><first>Jaime</first><last>Carbonell</last></author>
      <doi>10.1162/tacl_a_00050</doi>
      <abstract>This paper explores extending shallow semantic parsing beyond lexical-unit triggers, using causal relations as a test case. Semantic parsing becomes difficult in the face of the wide variety of linguistic realizations that <a href="https://en.wikipedia.org/wiki/Causality">causation</a> can take on. We therefore base our approach on the concept of <a href="https://en.wikipedia.org/wiki/Grammatical_construction">constructions</a> from the <a href="https://en.wikipedia.org/wiki/Paradigm">linguistic paradigm</a> known as Construction Grammar (CxG). In <a href="https://en.wikipedia.org/wiki/CxG">CxG</a>, a construction is a form / function pairing that can rely on arbitrary linguistic and semantic features. Rather than codifying all aspects of each construction’s form, as some attempts to employ <a href="https://en.wikipedia.org/wiki/CxG">CxG</a> in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> have done, we propose methods that offload that problem to <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>. We describe two supervised approaches for tagging causal constructions and their arguments. Both approaches combine automatically induced pattern-matching rules with <a href="https://en.wikipedia.org/wiki/Statistical_classification">statistical classifiers</a> that learn the subtler parameters of the constructions. Our results show that these approaches are promising : they significantly outperform nave baselines for both construction recognition and cause and effect head matches.</abstract>
      <pages>117–133</pages>
      <url hash="b1778570">Q17-1009</url>
      <bibkey>dunietz-etal-2017-automatically</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="12">
      <title>Head-Lexicalized Bidirectional Tree LSTMs<fixed-case>LSTM</fixed-case>s</title>
      <author><first>Zhiyang</first><last>Teng</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <doi>10.1162/tacl_a_00053</doi>
      <abstract>Sequential LSTMs have been extended to model <a href="https://en.wikipedia.org/wiki/Tree_(graph_theory)">tree structures</a>, giving competitive results for a number of tasks. Existing methods model constituent trees by bottom-up combinations of constituent nodes, making direct use of input word information only for leaf nodes. This is different from sequential LSTMs, which contain references to input words for each <a href="https://en.wikipedia.org/wiki/Vertex_(graph_theory)">node</a>. In this paper, we propose a method for automatic head-lexicalization for tree-structure LSTMs, propagating head words from leaf nodes to every constituent node. In addition, enabled by head lexicalization, we build a tree LSTM in the top-down direction, which corresponds to bidirectional sequential LSTMs in structure. Experiments show that both <a href="https://en.wikipedia.org/wiki/Extension_(semantics)">extensions</a> give better representations of <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">tree structures</a>. Our final model gives the best results on the Stanford Sentiment Treebank and highly competitive results on the TREC question type classification task.</abstract>
      <pages>163–177</pages>
      <url hash="b21c6efb">Q17-1012</url>
      <video href="https://vimeo.com/234954994" />
      <bibkey>teng-zhang-2017-head</bibkey>
    </paper>
    <paper id="13">
      <title>Nonparametric Bayesian Semi-supervised Word Segmentation<fixed-case>B</fixed-case>ayesian Semi-supervised Word Segmentation</title>
      <author><first>Ryo</first><last>Fujii</last></author>
      <author><first>Ryo</first><last>Domoto</last></author>
      <author><first>Daichi</first><last>Mochihashi</last></author>
      <doi>10.1162/tacl_a_00054</doi>
      <abstract>This paper presents a novel hybrid generative / discriminative model of word segmentation based on nonparametric Bayesian methods. Unlike ordinary discriminative word segmentation which relies only on labeled data, our <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised model</a> also leverages a huge amounts of unlabeled text to automatically learn new words, and further constrains them by using a labeled data to segment non-standard texts such as those found in <a href="https://en.wikipedia.org/wiki/Social_networking_service">social networking services</a>. Specifically, our hybrid model combines a discriminative classifier (CRF ; Lafferty et al. (2001) and unsupervised word segmentation (NPYLM ; Mochihashi et al. (2009)), with a transparent exchange of information between these two model structures within the semi-supervised framework (JESS-CM ; Suzuki and Isozaki (2008)). We confirmed that it can appropriately segment non-standard texts like those in <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> and <a href="https://en.wikipedia.org/wiki/Sina_Weibo">Weibo</a> and has nearly state-of-the-art accuracy on standard datasets in <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a>, <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, and <a href="https://en.wikipedia.org/wiki/Thai_language">Thai</a>.</abstract>
      <pages>179–189</pages>
      <url hash="5419dcf5">Q17-1013</url>
      <video href="https://vimeo.com/238235035" />
      <bibkey>fujii-etal-2017-nonparametric</bibkey>
    <title_pt>Segmentação de palavras semissupervisionada bayesiana não paramétrica</title_pt>
      <title_ar>تجزئة كلمة بايزي غير معلمية شبه خاضعة للإشراف</title_ar>
      <title_es>Segmentación de palabras semisupervisada bayesiana no paramétrica</title_es>
      <title_ja>ノンパラメトリックベイジアンセミスペシャルワードセグメンテーション</title_ja>
      <title_zh>非参数贝叶斯半督分词</title_zh>
      <title_hi>Nonparametric Bayesian अर्ध पर्यवेक्षित वर्ड विभाजन</title_hi>
      <title_ga>Deighleog Focal Leath-mhaoirsithe Bayesian Neamhparaiméadrach</title_ga>
      <title_hu>Nem parametrikus bayesiai félig felügyelt szószegmentáció</title_hu>
      <title_ka>Name</title_ka>
      <title_it>Segmentazione vocale semisupervisionata bayesiana non parametrica</title_it>
      <title_kk>Байезия жарты бақылаған сөз сегментациясы жоқ</title_kk>
      <title_mk>Nonparametric Bayesian Semi-supervised Word Segmentation</title_mk>
      <title_lt>Neparametrinė Bayezijos pusiau prižiūrima žodžių segmentacija</title_lt>
      <title_ms>Segmentasi Kata Semi-Dijaga Bayesia Tidak Parametrik</title_ms>
      <title_mt>Segmentazzjoni tal-Kliem Semi-Sorveljata Bajesjana Mhux Parametrika</title_mt>
      <title_ml>Nonparametric Bayesian Semi-supervised Word Segmentation</title_ml>
      <title_el>Μη παραμετρική Bayesian Semi-εποπτευόμενη τμηματοποίηση λέξεων</title_el>
      <title_mn>Гэвч параметр биезийн хагас дамжуулагдсан үг хэвлэлт</title_mn>
      <title_no>Ikkje parametrisk Bayesiansk semioversikt ordsegmentasjon</title_no>
      <title_pl>Nieparametryczna Bayesońska pół nadzorowana segmentacja słowa</title_pl>
      <title_ro>Segmentarea vocală semisupravegheată bayeziană nonparametrică</title_ro>
      <title_sr>Непараметрична бејезијска половинадзорна речна сегментација</title_sr>
      <title_si>ප්‍රමාණික බේසියාන් සම්බන්ධ වචනය</title_si>
      <title_so>Nonparametric Bayesian Semi-supervised Word Segmentation</title_so>
      <title_sv>Icke parametrisk bayesian semiövervakad ordsegmentering</title_sv>
      <title_ta>அளபுருவில்லை பெய்சியன் பெமி- கண்காணிக்கப்பட்ட வார்த்தை பிரிப்பு</title_ta>
      <title_ur>غیر پارامیٹریک بیسین نصف-supervised Word Segmentation</title_ur>
      <title_vi>KCharselect unicode block name</title_vi>
      <title_uz>QFontDatabase</title_uz>
      <title_nl>Niet-parametrische Bayesian Semi-supervised Word Segmentatie</title_nl>
      <title_bg>Непараметрична бейзийска полунадзорна сегментация на думи</title_bg>
      <title_da>Ikke- parametrisk bayesisk semiovervåget ordsegmentering</title_da>
      <title_hr>Neparametrična Bayesijska polu nadzorna segmentacija riječi</title_hr>
      <title_fa>بخش کلمه‌های نیمه مراقبت بی‌اسیایی غیر پارامتریک</title_fa>
      <title_de>Nicht parametrische Bayesische Halbüberwachte Wortsegmentierung</title_de>
      <title_ko>비변수 베일스 반감독분사</title_ko>
      <title_tr>Sözlük bir Bayezi Semi-gözlemiş Kelime Segmentasyonu</title_tr>
      <title_af>Geparametriese Bayesian semi- superviseer Woord Segmentasie</title_af>
      <title_id>Nonparametric Bayesian Semi-supervised Word Segmentation</title_id>
      <title_sw>Kitendo cha Bayesia cha Semi kinachofuatiliwa kwa neno la Kutenga</title_sw>
      <title_sq>Segmentacioni i Fjalëve jo parametrik i mbikqyrur nga Bayesia</title_sq>
      <title_am>parameteric Bayesian Semi-supervised Word Segmentation</title_am>
      <title_bs>Neparametrična Bayesijska polu nadzorna segmentacija riječi</title_bs>
      <title_bn>কোনো প্যারামিট্রিক বেয়েসিয়ান সেমি- পর্যবেক্ষণ করা শব্দ বিভাগ</title_bn>
      <title_hy>Nonparametric Bayesian Semi-supervised Word Segmentation</title_hy>
      <title_et>Mitteparameetriline bayesia pooljärelevalvega sõnade segmenteerimine</title_et>
      <title_az>Parametrik olmayan Bayesiya yarısını gözləyir Kelimi Segmentasyonu</title_az>
      <title_cs>Neparametrická Bayesovská polovičně dohledová segmentace slov</title_cs>
      <title_ca>Segmentació de paraules semi-supervisada de Bayesia</title_ca>
      <title_fi>Ei-parametrinen Bayesian Semi-supervised Word Segmentation</title_fi>
      <title_sk>Neparametrična bajzijska polnadzorovana segmentacija besed</title_sk>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_jv>Languages</title_jv>
      <title_he>ניתוח מילים לא פארמטרי</title_he>
      <title_bo>Nonparametric Bayesian Semi-supervised Word Segmentation</title_bo>
      <abstract_ar>تقدم هذه الورقة نموذجًا توليديًا / تمييزيًا جديدًا هجينًا لتجزئة الكلمات بناءً على طرق بايز غير معلمية. على عكس تجزئة الكلمات التمييزية العادية التي تعتمد فقط على البيانات المصنفة ، فإن نموذجنا شبه الخاضع للإشراف يستفيد أيضًا من كميات هائلة من النص غير المصنف لتعلم "كلمات" جديدة تلقائيًا ، ويزيد من تقييدها باستخدام البيانات المصنفة لتقسيم النصوص غير القياسية مثل تلك الموجودة في خدمات الشبكات الاجتماعية. على وجه التحديد ، يجمع نموذجنا الهجين بين المصنف التمييزي (CRF ؛ Lafferty et al. (2001) وتجزئة الكلمات غير الخاضعة للإشراف (NPYLM ؛ Mochihashi et al. (2009)) ، مع تبادل شفاف للمعلومات بين هذين النموذجين داخل الهياكل شبه. إطار عمل خاضع للإشراف (JESS-CM؛ Suzuki and Isozaki (2008)). أكدنا أنه يمكن تقسيم النصوص غير القياسية بشكل مناسب مثل تلك الموجودة في Twitter و Weibo ولديه دقة متطورة تقريبًا في مجموعات البيانات القياسية باللغات اليابانية والصينية ، والتايلاندية.</abstract_ar>
      <abstract_pt>Este artigo apresenta um novo modelo híbrido generativo/discriminativo de segmentação de palavras baseado em métodos bayesianos não paramétricos. Ao contrário da segmentação discriminativa de palavras comum, que se baseia apenas em dados rotulados, nosso modelo semi-supervisionado também aproveita uma enorme quantidade de texto não rotulado para aprender automaticamente novas “palavras” e as restringe ainda mais usando dados rotulados para segmentar textos não padronizados, como aqueles encontrados em serviços de redes sociais. Especificamente, nosso modelo híbrido combina um classificador discriminativo (CRF; Lafferty et al. (2001) e segmentação de palavras não supervisionada (NPYLM; Mochihashi et al. (2009)), com uma troca transparente de informações entre essas duas estruturas de modelo dentro do semi- estrutura supervisionada (JESS-CM; Suzuki e Isozaki (2008)). Confirmamos que ela pode segmentar adequadamente textos não padronizados como os do Twitter e Weibo e tem precisão quase de última geração em conjuntos de dados padrão em japonês, chinês , e tailandês.</abstract_pt>
      <abstract_es>Este artículo presenta un novedoso modelo híbrido generativo/discriminativo de segmentación de palabras basado en métodos bayesianos no paramétricos. A diferencia de la segmentación de palabras discriminativa ordinaria, que se basa únicamente en datos etiquetados, nuestro modelo semi-supervisado también aprovecha una enorme cantidad de texto sin etiqueta para aprender automáticamente nuevas «palabras» y las restringe aún más mediante el uso de datos etiquetados para segmentar textos no estándar, como los que se encuentran en las redes sociales servicios de redes. Específicamente, nuestro modelo híbrido combina un clasificador discriminativo (CRF; Lafferty et al. (2001) y segmentación de palabras no supervisada (NPYLM; Mochihashi et al. (2009)), con un intercambio transparente de información entre estas dos estructuras modelo dentro del marco semisupervisado (JESS-CM; Suzuki e Isozaki ( 2008)). Confirmamos que puede segmentar adecuadamente textos no estándar como los de Twitter y Weibo y que tiene una precisión casi de vanguardia en conjuntos de datos estándar en japonés, chino y tailandés.</abstract_es>
      <abstract_ja>ノンパラメトリックベイズ法に基づく単語セグメンテーションの新規ハイブリッド生成/識別モデルを提示した。ラベル付けされたデータのみに依存する通常の差別的な単語セグメンテーションとは異なり、私たちの半監督モデルはまた、膨大な量のラベル付けされていないテキストを活用して自動的に新しい「単語」を学習し、ソーシャルネットワーキングサービスに見られるような非標準的なテキストをセグメント化するためにラベル付けされたデータを使用することによって、それらをさらに制約します。具体的には、私たちのハイブリッドモデルは、判別型分類子（ ＣＲＦ ； Ｌａｆｆｅｒｔｙ ｅ ｔ ａ ｌ ． （ ２ ０ ０ １ ） ）と非監視型単語セグメンテーション（ ＮＰＹＬＭ ； Ｍｏｃｈｉｈａｓｈｉ ｅ ｔ ａ ｌ ． （ ２ ０ ０ ９ ） ）を組み合わせ、これら２つのモデル構造間で半監視型フレームワーク内で透明な情報交換を行う（ ＪＥＳＳ － ＣＭ ； Ｓｕｚｕｋｉ ａｎｄ Ｉｓｏｚａｋｉ （ ２ ０ ０ ８ ） ）。TwitterやWeiboなどの非標準テキストを適切にセグメント化でき、日本語、中国語、タイ語の標準データセットにほぼ最新の精度を持つことを確認しました。</abstract_ja>
      <abstract_zh>立一本于非参数贝叶斯法者分词混而成/判别模形。 与仅依表数之普通区分性分词不同,吾半监模犹以大未标之文本自习新单词,因用标数对非标准文本(如社交网络服务中文本)细分以约束之。 具体来说,合刑合器(CRF。 Lafferty等(2001)与无监分词(NPYLM。 Mochihashi等(2009)),于半监框架内两样透明易信(JESS-CM。 铃木和矶崎新(2008))。 臣等证之,可以适割非标准本,如Twitter微博中之本,并在日语,中文泰语之数集上有近先进之准确性。</abstract_zh>
      <abstract_hi>यह पेपर गैर-पैरामीट्रिक बायेसियन विधियों के आधार पर शब्द विभाजन का एक उपन्यास हाइब्रिड जेनरेटर / भेदभावपूर्ण मॉडल प्रस्तुत करता है। सामान्य भेदभावपूर्ण शब्द विभाजन के विपरीत जो केवल लेबल किए गए डेटा पर निर्भर करता है, हमारा अर्ध-पर्यवेक्षित मॉडल स्वचालित रूप से नए "शब्दों" को सीखने के लिए बड़ी मात्रा में बिना लेबल वाले पाठ का लाभ उठाता है, और सोशल नेटवर्किंग सेवाओं में पाए जाने वाले गैर-मानक ग्रंथों को विभाजित करने के लिए लेबल किए गए डेटा का उपयोग करके उन्हें और बाधित करता है। विशेष रूप से, हमारा हाइब्रिड मॉडल एक भेदभावपूर्ण क्लासिफायर (सीआरएफ) को जोड़ता है; Lafferty et al. (2001) और असुरक्षित शब्द विभाजन (NPYLM; Mochihashi et al. (2009)), अर्ध-पर्यवेक्षित ढांचे (JESS-CM) के भीतर इन दो मॉडल संरचनाओं के बीच जानकारी के पारदर्शी आदान-प्रदान के साथ; सुजुकी और इसोज़ाकी (2008)। हमने पुष्टि की है कि यह उचित रूप से ट्विटर और वीबो में उन लोगों की तरह गैर-मानक ग्रंथों को विभाजित कर सकता है और जापानी, चीनी और थाई में मानक डेटासेट पर लगभग अत्याधुनिक सटीकता है।</abstract_hi>
      <abstract_ga>Cuireann an páipéar seo i láthair samhail giniúna/idirdhealaitheach hibrideach nua de dheighilt focal bunaithe ar mhodhanna neamhparaiméadracha Bayesian. Murab ionann agus gnáth-dheighilt focal idirdhealaitheach a bhraitheann ar shonraí lipéadaithe amháin, déanann ár múnla leath-mhaoirseachta giaráil freisin ar mhéideanna ollmhóra téacs gan lipéad chun “focail” nua a fhoghlaim go huathoibríoch, agus cuireann sé srian breise orthu trí úsáid a bhaint as sonraí lipéadaithe chun téacsanna neamhchaighdeánacha a dheighilt. iad siúd atá le fáil i seirbhísí líonraithe sóisialta. Go sonrach, comhcheanglaíonn ár múnla hibrideach aicmitheoir idirdhealaitheach (CRF; Lafferty et al. (2001) agus deighilt focal gan mhaoirseacht (NPYLM; Mochihashi et al. (2009))), le malartú trédhearcach faisnéise idir an dá struchtúr mhúnla seo laistigh den leath-struchtúr. creat maoirsithe (JESS-CM; Suzuki agus Isozaki (2008)) Dhearbhaíomar gur féidir léi téacsanna neamhchaighdeánacha mar iad siúd in Twitter agus Weibo a dheighilt go cuí agus go bhfuil cruinneas den scoth aige ar thacair sonraí caighdeánacha sa tSeapáinis agus sa tSínis. , agus Téalainnis.</abstract_ga>
      <abstract_hu>A tanulmány bemutatja a szószegmentáció új hibrid generációs/diszkriminatív modelljét, amely nem parametrikus bayesiai módszereken alapul. A hagyományos diszkriminatív szószegmentációval ellentétben, amely csak a címkézett adatokon alapul, a félig felügyelt modellünk hatalmas mennyiségű, címke nélküli szöveget is felhasznál arra, hogy automatikusan megtanulják az új "szavakat", és tovább korlátozza őket azzal, hogy címkézett adatokat használnak a nem szabványos szövegek szegmentálására, mint például a közösségi hálózati szolgáltatásokban. Különösen, hibrid modellünk egy diszkriminatív osztályozót (CRF; Lafferty et al. (2001) és felügyelet nélküli szegmentációt (NPYLM; Mochihashi et al. (2009)), amely átlátható információcserét biztosít e két modellstruktúra között a félig felügyelt kereten belül (JESS-CM; Suzuki és Isozaki (2008)). Megerősítettük, hogy megfelelően szegmensezheti a nem szabványos szövegeket, mint például a Twitteren és a Weibón, és szinte a legkorszerűbb pontossággal rendelkezik a szabványos adatkészleteken japán, kínai és thai nyelven.</abstract_hu>
      <abstract_el>Η παρούσα εργασία παρουσιάζει ένα νέο υβριδικό παραγωγικό/διακριτικό μοντέλο τμηματοποίησης λέξεων βασισμένο σε μη παραμετρικές Bayesian μεθόδους. Σε αντίθεση με τη συνηθισμένη διαφοροποιημένη τμηματοποίηση λέξεων που βασίζεται μόνο σε δεδομένα με ετικέτα, το ημι-εποπτευμένο μοντέλο μας χρησιμοποιεί επίσης τεράστιες ποσότητες χωρίς ετικέτα κειμένου για να μάθει αυτόματα νέες "λέξεις", και τις περιορίζει περαιτέρω χρησιμοποιώντας δεδομένα με ετικέτα για να ταξινομήσει μη τυποποιημένα κείμενα, όπως αυτά που βρίσκονται στις υπηρεσίες κοινωνικής δικτύωσης. Ειδικότερα, το υβριδικό μας μοντέλο συνδυάζει έναν διαχωριστικό ταξινομητή (CRF, Lafferty et al. (2001) και μια χωρίς επίβλεψη τμηματοποίηση λέξεων (NPYLM, Mochihashi et al. (2009)), με μια διαφανή ανταλλαγή πληροφοριών μεταξύ αυτών των δύο δομών μοντέλου στο πλαίσιο ημιεπιτήρησης (Suzuki και Isozaki (2008)). Επιβεβαιώσαμε ότι μπορεί να ταξινομήσει κατάλληλα μη τυποποιημένα κείμενα όπως αυτά στο Twitter και το Weibo και έχει σχεδόν υπερσύγχρονη ακρίβεια σε τυποποιημένα σύνολα δεδομένων στα ιαπωνικά, κινέζικα και ταϊλανδέζικα.</abstract_el>
      <abstract_ka>ეს დომენტი ახლა პრომენტიური ჰიბრიდის გენერაციური/დისკრიმინატიური სიტყვების სეგმენტის მოდელი, რომელიც არ პარამეტრიური ბეიზიანური მეცედიების ჩვენი პროგრამიური დისკრიმინატიური სიტყვების სეგმენტის განმავლობაში, რომელიც მხოლოდ მართლად მართლაც მართლაც მართლაც მართლაც მართლად მართლად მონაცემებული მოდელზე, ჩვენი პროგრამიური მართლაც მართლად ახალი 'სიტყვები' მოსწავლად განსაკუთრებულია, ჩვენი ჰიბრიდი მოდელი დისკრიმინატიური კლასიფიკაციატორია (CRF; Lafferty et al. (2001) და არ განსაკუთრებული სიტყვების სეგმენტია (NPYLM; Mochihashi et al. (2009)), ამ ორი მოდელური სტრუქტურების განსაკუთრებული ინფორმაციის გადაცვლა (JESS-CM; Suzuki და Isozaki (2008 ჩვენ დარწმუნეთ, რომ ეს შეუძლია საპირო, ჩინტერვირში და საიბოში სტანდარტური ტექსტების სწორედ სექმენტური სექმენტური სექმენტის მართლაც იქნება და საიბოში სექმენტური</abstract_ka>
      <abstract_it>Questo articolo presenta un nuovo modello ibrido generativo/discriminatorio di segmentazione delle parole basato su metodi bayesiani non parametrici. A differenza della normale segmentazione discriminatoria delle parole che si basa solo su dati etichettati, il nostro modello semi-supervisionato sfrutta anche un'enorme quantità di testo non etichettato per imparare automaticamente nuove 'parole', e li limita ulteriormente utilizzando dati etichettati per segmentare testi non standard come quelli trovati nei servizi di social networking. Nello specifico, il nostro modello ibrido combina un classificatore discriminante (CRF; Lafferty et al. (2001) e una segmentazione non supervisionata delle parole (NPYLM; Mochihashi et al. (2009)), con uno scambio trasparente di informazioni tra queste due strutture modello all'interno del quadro semi-supervisionato (JESS-CM; Suzuki e Isozaki (2008)). Abbiamo confermato che può segmentare in modo appropriato testi non standard come quelli su Twitter e Weibo e ha una precisione quasi all'avanguardia sui set di dati standard in giapponese, cinese e tailandese.</abstract_it>
      <abstract_kk>Бұл қағаз бейезия әдістеріне негізделген романдық гибридтік/дискриминациялық сөздерді сегментациялау үлгісін көрсетеді. Кәдімгі дискриминациялық сөздердің сегментациясы, тек жарлық деректеріне тәуелді, біздің жартық бақылау үлгіміз де жаңа 'сөздер' дегенді автоматты түрде үйрену үшін, жаңа 'сөздер' дегенді үйрену үшін үлкен мәтіндерді шектеп, жарлық деректерді қолдану ү Ескерту үшін, біздің гибрид моделіміз дискриминациялық классификациясы (CRF; Lafferty et al. (2001) және сөздер сегментациясы (NPYLM; Mochihashi et al. (2009)), бұл екі үлгі құрылғылар арасындағы мәліметті біріктіріп, жарты бақылау фреймінде (JESS- CM; Suzuki және Isozaki (2008)). Біз оның Твиттер мен Вайбо секілдерінің стандартты мәтіндері дұрыс емес екенін баптап, жапон, қытайлық және Тайландық стандартты деректер жиындарының стандартты дұрыстығы бар деп ойладық.</abstract_kk>
      <abstract_lt>Šiame dokumente pateikiamas naujas hibridinis ir (arba) diskriminacinis žodžių segmentavimo modelis, grindžiamas ne parametriniais Bayezijos metodais. Priešingai nei įprastas diskriminacinis žodžių segmentavimas, kuris grindžiamas tik pažymėtais duomenimis, mūsų pusiau prižiūrimas modelis taip pat sutelkia didžiulį kiekį nežymėto teksto, kad būtų galima automatiškai išmokti naujų žodžių, ir toliau apriboja juos naudojant pažymėtus duomenis, kad būtų galima segmentuoti nestandartinius tekstus, pvz., socialinių tinklų paslaugų tekstus. Konkrečiai, mūsų hibridinis modelis derina diskriminacinį klasifikatorių (CRF; Lafferty et al. (2001) ir nepastebimą žodžių segmentavimą (NPYLM; Mochihashi et al. (2009)), skaidrų šių dviejų modelių struktūrų keitimąsi informacija pagal pusiau prižiūrimą sistemą (JESS-CM; Suzuki ir Isozaki (2008)). Patvirtinome, kad ji gali tinkamai suskirstyti nestandartinius tekstus, kaip antai tekstus Twitter ir Weibo, ir turi beveik naujausią tikslumą standartinių duomenų rinkinių japonų, kinų ir tailandų kalbomis.</abstract_lt>
      <abstract_mk>Овој весник претставува нов хибриден генерационален/дискриминативен модел на зборна сегментација базиран на непараметричките бајезиски методи. За разлика од обичната дискриминативна сегментација на зборови која се потпира само на означени податоци, нашиот полунадгледуван модел, исто така, користи огромна количина неозначен текст за автоматски да се научи нови „зборови“, и понатаму ги ограничува со користење означени податоци за сегментирање нестандардни тексти како што се оние Специфично, нашиот хибриден модел комбинира дискриминативен класификатор (ЦРФ; Лаферти и г. (2001) и ненадгледувана словена сегментација (НПЈЛМ; Мочихаши и г. (2009)), со транспарентна размена на информации помеѓу овие две моделни структури во полунадгледуваната рамка (ЈЕСС-ЦМ; Сузуки и Исозаки (2008 Потврдивме дека може соодветно да ги дели нестандардните тексти како оние на Твитер и Вајбо и има скоро најсовремена точност на стандардните податоци на јапонски, кинески и тајландски.</abstract_mk>
      <abstract_mt>This paper presents a novel hybrid generative/discriminative model of word segmentation based on nonparametric Bayesian methods.  Għall-kuntrarju tas-segmentazzjoni tal-kliem diskriminatorja ordinarja li tiddependi biss fuq dejta ttikkettata, il-mudell nofs superviż tagħna jgħaqqad ukoll ammonti kbar ta’ test mhux ittikkettat biex jitgħallmu awtomatikament ‘kliem’ ġodda, u jkompli jirrestrinġihom billi tuża dejta ttikkettata biex taqsam testi mhux standard bħal dawk misjuba fis-servizzi tan-netwerking soċjali. B’mod speċifiku, il-mudell ibridu tagħna jikkombina klassifikatur diskriminatorju (CRF; Lafferty et al. (2001) u segmentazzjoni tal-kliem mhux sorveljata (NPYLM; Mochihashi et al. (2009)), ma’ skambju trasparenti ta’ informazzjoni bejn dawn iż-żewġ strutturi mudell fi ħdan il-qafas semisorveljat (JESS-CM; Suzuki u Isozaki (2008)). Aħna kkonfermajna li tista' taqsam b'mod xieraq testi mhux standard bħal dawk fuq Twitter u Weibo u għandha kważi l-aktar preċiżjoni avvanzata fuq settijiet ta' dejta standard fil-Ġappuniż, Ċiniż u t-Tajlandiż.</abstract_mt>
      <abstract_ms>Kertas ini memperkenalkan model hibrid baru generatif/diskriminatif segmen perkataan berdasarkan kaedah Bayesian bukan parametrik. Tidak seperti segmen perkataan diskriminatif biasa yang hanya bergantung pada data yang ditabel, model setengah-mengawasi kami juga menggunakan jumlah besar teks tidak ditabel untuk secara automatik belajar 'perkataan' baru, dan halang lebih lanjut mereka dengan menggunakan data ditabel untuk segmen teks bukan piawai seperti yang ditemui dalam perkhidmatan rangkaian sosial. Secara khusus, model hibrid kami menggabungkan pengklasifikasi diskriminatif (CRF; Lafferty et al. (2001) dan segmen perkataan tidak diawasi (NPYLM; Mochihashi et al. (2009)), dengan pertukaran maklumat yang jelas antara kedua-dua struktur model ini dalam kerangka setengah diawasi (JESS-CM; Suzuki dan Isozaki (2008)). Kami mengesahkan bahawa ia boleh segmen teks yang tidak piawai seperti dalam Twitter dan Weibo dan mempunyai ketepatan hampir terbaik pada set data piawai dalam bahasa Jepun, Cina dan Thai.</abstract_ms>
      <abstract_ml>ഈ പത്രത്തില്‍ ഒരു നോവല്‍ ഹൈബ്രിഡിന്റെ ജനററിവ്/വിവേചനയുടെ മോഡല്‍ കാണിക്കുന്നു. പാരാമെറ്റിക് ബെയിസിയന്‍ രീതികളില്‍ അടിസ് സാധാരണ വാക്കുകളുടെ വിഭാഗത്തില്‍ വ്യത്യസ്തമായ വാക്കുകള്‍ വേര്‍തിരിച്ചറിയുന്നതില്‍ മാത്രം വിശ്വസിക്കുന്നു. നമ്മുടെ സെമി-നോട്ട് നിരീക്ഷിക്കപ്പെട്ട മോഡല്‍ സ്വയം പുതിയ വാക്കുകള്‍ പഠ പ്രത്യേകിച്ച്, നമ്മുടെ ഹൈബ്രിഡ് മോഡല്‍ ഒരു വ്യത്യസ്ത വ്യവസ്ഥയെ കൂട്ടിക്കൊണ്ടിരിക്കുന്നു (CRF; Lafferty et al. (2001) പിന്നെ സൂക്ഷിക്കാത്ത വാക്ക് സംഘടിപ്പിക്കുന്നതും (NPYLM; മൊചിഹാഷി et al. (2009)), ഈ രണ്ട് മോഡല്‍ ഘ ഞങ്ങള്‍ ഉറപ്പ് വരുത്തിയിരിക്കുന്നു ഇത് ടൂട്ടരും വെയിബോയിലും സ്ഥാനമില്ലാത്ത പദാവലികള്‍ക്ക് വേര്‍പെടുത്താന്‍ സാധിക്കുന്നു. ജാപ്പാനീസ്, ചൈനീ</abstract_ml>
      <abstract_no>Denne papiret viser ein novel hybrid-generativ/diskrimineriv modell for ordsegmentasjon basert på ikkje-parametriske Bayesianske metodar. I motsetjing til vanleg diskriminasjon av ordsegmentasjon som berre dependerer på merkelige data, vår halvoversikt modellen leverer også ein stor mengd av ikkje merkelige tekst til å automatisk læra nye ord, og framleis begrenser dei ved å bruka eit merkelige data til å segmentera ikkje-standard tekstar, slik som dei funne i sosiale nettverktjenester. Spesielt kombinerer hibridmodellen vårt ein diskriminativ klassifisering (CRF; Lafferty et al. (2001) og uverkjende ordsegmentasjon (NPYLM; Mochihashi et al. (2009)), med ein gjennomsiktig utveksling av informasjon mellom disse to modelle strukturene i semioversikte rammeverket (JESS-CM; Suzuki og Isozaki (2008)). Vi stadfestig at det kan dele ikkje-standardtekstar som dei i Twitter og Weibo og har nesten kunstige nøyaktighet på standard datasett i japansk, kinesisk og Thai.</abstract_no>
      <abstract_mn>Энэ цаас биезийн арга баримтгүй үг загварын шинэ гибрид үүсгэгч/ялгаагүй загварыг харуулдаг. Шинэ үгийг автоматжуулахын тулд бидний хагас удирдлагагүй загвар мөн шинэ үгийг автоматжуулахын тулд маш их хэмжээний хэмжээг ашигладаг. Ялангуяа бидний гибрид загвар нь тархалтын хуваагдагч (CRF; Lafferty et al. (2001) болон бусад үг загвар (NPYLM; Mochihashi et al. (2009)) хоёр загварын байгууллагуудын хоорондын тодорхой мэдээллийг хагас удирдаггүй хэлбэрээр (JESS-CM; Suzuki, Isozaki (2008)-ын хоорондын тодорхой хувьцааны хувьцааны Бид үүнийг Твиттер, Вэйбо хоёр шиг стандарт биш текстүүдийг зөвхөн загварчлах боломжтой гэдгийг баталсан. Япон, Хятад, Тайланд стандарт өгөгдлийн сангийн зөв байдал бараг л байдаг.</abstract_mn>
      <abstract_pl>W artykule przedstawiono nowy hybrydowy model segmentacji słów generatywnych/dyskryminacyjnych oparty na nieparametrycznych metodach Bayesowskich. W przeciwieństwie do zwykłej segmentacji słów dyskryminacyjnych, która opiera się wyłącznie na danych etykietowanych, nasz model pół-nadzorowany wykorzystuje również ogromne ilości nieetykietowanego tekstu do automatycznego uczenia się nowych "słów" i dodatkowo ogranicza je poprzez wykorzystanie etykietowanych danych do segmentowania niestandardowych tekstów, takich jak te znajdujące się w serwisach społecznościowych. W szczególności nasz model hybrydowy łączy w sobie klasyfikator dyskryminacyjny (CRF; Lafferty i al. (2001) oraz segmentację słów bez nadzoru (NPYLM; Mochihashi i al. (2009)), z przejrzystą wymianą informacji między tymi dwoma strukturami modelowymi w ramach pół-nadzorowanych (JESS-CM; Suzuki i Isozaki (2008)). Potwierdziliśmy, że może odpowiednio segmentować niestandardowe teksty, takie jak te na Twitterze i Weibo oraz posiada niemal najnowocześniejszą dokładność na standardowych zbiorach danych w języku japońskim, chińskim i tajskim.</abstract_pl>
      <abstract_ro>Această lucrare prezintă un nou model generativ/discriminatoriu hibrid de segmentare a cuvintelor bazat pe metode bayesiane non-parametrice. Spre deosebire de segmentarea discriminativă obișnuită a cuvintelor, care se bazează numai pe date etichetate, modelul nostru semi-supravegheat utilizează, de asemenea, o cantitate uriașă de text nelimitat pentru a învăța automat noi "cuvinte", și le constrânge în continuare prin utilizarea unor date etichetate pentru a segmenta texte nestandard, cum ar fi cele găsite în serviciile de rețele sociale. Mai exact, modelul nostru hibrid combină un clasificator discriminatoriu (CRF; Lafferty et al. (2001) și segmentarea cuvintelor nesupravegheată (NPYLM; Mochihashi et al. (2009)), cu un schimb transparent de informații între aceste două structuri de model în cadrul semi-supravegheat (JESS-CM; Suzuki și Isozaki (2008)). Am confirmat că poate segmenta în mod corespunzător texte non-standard precum cele din Twitter și Weibo și are o acuratețe aproape de ultimă oră pe seturile de date standard în japoneză, chineză și thailandeză.</abstract_ro>
      <abstract_sr>Ovaj papir predstavlja nov hibridni generativni/diskriminacijski model segmentacije reèi na osnovu neparametričkih Bayesijskih metoda. Za razliku od obične diskriminacijske segmentacije riječi koja se oslanja samo na etiketirane podatke, naš polu nadzorni model takođe utiče na ogromne količine nenabeliranog teksta da automatski nauči nove 'reči', i dalje ih ograničava koristeći etiketirane podatke na segmentirane ne standardne tekste kao što su pronađene u socijalnim mrežnim uslugama. Posebno, naš hibridni model kombinira diskriminativnu klasifikaciju (CRF; Lafferty et al. (2001) i neodređenu segmentaciju riječi (NPYLM; Mochihashi et al. (2009)), sa transparentnom razmjenom informacija između tih dva modelnih struktura u polu nadzornom okviru (JESS-CM; Suzuki i Isozaki (2008)). Potvrdili smo da može odgovarajući segmentirati ne-standardne tekstove poput one na Twitter i Weibo i da ima skoro stanje umjetnosti tačnosti na standardnim setima podataka na japanskom, kineskom i Tajlandu.</abstract_sr>
      <abstract_si>මේ පැත්ත පෙන්වන්නේ නොපාරාම්ටරික බේසියාන් විදියට ආධාරිත විදියට නිර්මාණික/විශ්වාසික විදියට ප්‍ර නැත්තම් සාමාන්‍ය විශ්වාසික වචන සැකසුම් වචන සැකසුම් වලින් විතරයි ලේබල් වලින් විතරයි, අපේ සාමාන්‍ය විශ්වාසිත විදියට ප්‍රමාණයක් තියෙන්නේ ලේබල් වලින් ලේබල් වලි විශේෂයෙන්, අපේ හිබ්‍රිඩ් මොඩල් සම්බන්ධ විශ්වාසිත විශේෂකය (CRF; Lafferty et al. (2001) සහ නොසුපෙර්වසිත වචනය (NPYLM; Mochihashi et al. (2009)), සහ පාරදානම් සම්බන්ධ විශේෂකයේ තොරතුරු අතර මෙම් මොඩල් ස අපි සැකසුම් කළා ඒක ට්විටර් සහ වේබෝ වලින් ප්‍රමාණයෙන් නොප්‍රමාණයෙන් පිළිබඳ වෙන්න පුළුවන් කියලා, ජාපාන්, චීනි සහ තායින</abstract_si>
      <abstract_so>Qoraalkan wuxuu soo saaraa qaab dhaqan/takooris ah oo ku saleysan qaababka ay Bayesian ku qoran yihiin. Kala duwan qayb-takoorista ah oo ku xiran macluumaadka calaamadda oo kaliya, noocyadana halka ka ilaaliyey wuxuu sidoo kale ku fidiyaa warqad badan oo aan la qorin si ay u barato hadal cusub, waxaana sidoo kale ku qasba isticmaalka macluumaad labo ah si ay u kala qeybiso qoraal aan caadi ahayn, sida kuwa laga helay adeegyada shabakadda bulshada. Sida gaar ah, modelheenna hybrid wuxuu isku biiriyaa takooris (CRF; Lafferty et al. (2001) iyo unsupervised word segmentation (NPYLM; Mochihashi et al. (2009)), waxaana la bedelay macluumaad muuqaal ah oo u dhexeeya labadaas dhismaha model hoose-hoose-ilaaliyey (JESS-CM; Suzuki iyo Isozaki (2008). Waxaannu xaqiijinnay inay si saxda ah u qeyb-dhigi karto qoraal aan caadi ahayn sida Twitterka iyo Weibo, waxayna ku leedahay saxda xaaladda farshaxanka ah oo ku saabsan sawirada caadiga ah ee japaniya, Shiino iyo Thai.</abstract_so>
      <abstract_sv>Denna uppsats presenterar en ny hybrid generativ/diskriminerande modell av ordsegmentering baserad på icke-parametriska bayesiska metoder. Till skillnad från vanlig diskriminerande ordsegmentering som bara bygger på märkta data, utnyttjar vår halvövervakade modell också enorma mängder omärkt text för att automatiskt lära sig nya "ord", och begränsar dem ytterligare genom att använda en märkt data för att segmentera icke-standardiserade texter som de som finns i sociala nätverkstjänster. Specifikt kombinerar vår hybridmodell en diskriminerande klassificerare (CRF; Lafferty m.fl. (2001) och obevakad ordsegmentering (NPYLM; Mochihashi m.fl. (2009)), med ett transparent informationsutbyte mellan dessa två modellstrukturer inom det halvövervakade ramverket (JESS-CM; Suzuki och Isozaki (2008)). Vi bekräftade att det på lämpligt sätt kan segmentera icke-standardtexter som de i Twitter och Weibo och har nästan toppmodern noggrannhet på standarddataset på japanska, kinesiska och thailändska.</abstract_sv>
      <abstract_ta>இந்த காகிதத்தின் புதிய ஹைப்ரிட் பொதுவான/வித்தியாசமான வார்த்தை துண்டுதல் மாதிரியை கொடுக்கிறது பயீசியன் முறைமை வழக்கமான வார்த்தை பிரிவு துண்டு மட்டும் குறிப்பிட்ட தகவல் மீது நம்புகிறது, எங்கள் பாமி கண்காணிக்கப்பட்ட மாதிரி ஒரு மிகப் பெரிய எண்ணிக்கையை வழங்குகிறது புதிய 'வார்த்தைகளை தானாகவே கற்று குறிப்பிட்டு, எங்கள் ஹைப்ரிட் மாதிரி ஒரு வித்தியாசமான வகுப்பாட்டாளரை (CRF; Lafferty et al. (2001) மற்றும் பாதுகாப்பாக்கப்படாத வார்த்தை திருத்தல் (NPYLM; Mochihashi et al. (2009)), பெமி கண்காணிக்கப்பட்ட சட்டத்தில் இந்த இரண்டு மாதிர நாங்கள் உறுதிப்படுத்தினோம் அது சரியான நிலையான அல்லாத எழுத்துக்களை துண்டிக்க முடியும் என்பதை நிர்ணயித்துள்ளோம் டூட்டர் மற்றும் வெய்போவி</abstract_ta>
      <abstract_ur>This paper presents a new hybrid generative/discriminative model of word segmentation based on nonparametric Bayesian methods. جو صرف لابلیٹ ڈاٹ پر اعتماد ہے، ہماری نصف نظارت والی مدل نے اپنا بہت بڑا مقدار غیر لابلیٹ لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہوا لکھا ہے خاص طور پر، ہمارا ہیبراڈ موڈل ایک تقسیم کلاسیر (CRF; Lafferty et al. (2001) اور غیر قابل تغییر دینے والی کلاسیر (NPYLM; Mochihashi et al. (2009)) کے ساتھ ان دو موڈل ساختاروں کے درمیان نیم-supervised فرمود (JESS-CM; Suzuki اور Isozaki (2008)) کے درمیان مطلوبہ کی تبدیل کی جاتی ہے۔ ہم نے مطمئن کیا کہ یہ ٹویٹر اور ویبو میں ایسے بغیر استاندارڈ ٹیکسٹ کے مطابق قطع کر سکتا ہے اور قریب تھا کہ اسٹارڈ سٹ کے استاندارڈ سٹ پر جاپانی, چینی اور تائیل میں استاندارڈ سٹ کے مطابق صحیح ہے.</abstract_ur>
      <abstract_uz>Bu qogʻoz parametrik Bayesian usulda asoslangan so'z segment modelini yaratadi. Oddiy ajratilgan so'zlar tarkibini faqat notoʻgʻri maʼlumotga ishlatadi, bizning semi-taʼminlovchi modelimiz yangi so'zlarni avtomatik o'rganish uchun juda katta qismi yordam beradi, va ular soʻzlarni avtomatik o'rganish uchun qo'llangan maʼlumot yordamida qo'llangan soʻzlarni boshqa tarmoq xizlarida o'zgartirish mumkin. Ko'rsatilgan, bizning hybrid modelimiz ajratish darajasini (CRF; Lafferty et al. (2001) va xavfsizlanmagan so'zni ajratish (NPYLM; Mochihashi et al. (2009)) bilan bir necha saqlangan jadvaldagi ikki modellar tarkibida maʼlumotni ajratish (JESS-CM; Suzuki va Isozaki (2008). Biz ishonch qildikki, bu Twitter va Weibo kabi oddiy matnlarni o'zgartirish mumkin, va yaponiya, Xitoycha va Tay tilidagi стандарт maʼlumotlar tizimini aniqlash mumkin.</abstract_uz>
      <abstract_vi>Tờ giấy này có một mô hình nhân tạo và phân biệt các từ khác nhau dựa trên các phương pháp không phân đo định người Bayisian. Không giống phân đoạn các từ riêng biệt bình thường chỉ dựa trên các dữ liệu được dán nhãn, mô hình bán giám sát của chúng ta cũng dùng một lượng lớn các chữ chưa được dán vào để tự động học các chữ mới, và giới hạn chúng bằng cách dùng các dữ liệu dán nhãn để phân đoạn các văn bản không tiêu chuẩn như những chữ được tìm thấy trong dịch vụ mạng xã hội. Cụ thể, mô hình lai của chúng ta kết hợp một phân loại riêng biệt (CRF, Lafferty et al. (2001) và phân biệt các từ không giám sát (không riêng (không thể nói: không thể: không thể, Mochihashi et al (2009), với một trao đổi thông tin trong suốt... giữa hai cấu trúc mô hình này trong quá trình giám sát (Jesus-CM; Suzuki và Isozaki (kế thừa) Chúng tôi xác nhận rằng nó có thể phân đoạn các văn bản không tiêu chuẩn như trên Twitter và Weibo và có độ chính xác cao nhất trên các tập tin tiêu chuẩn của Nhật, Trung Quốc và Thái.</abstract_vi>
      <abstract_da>Denne artikel præsenterer en ny hybrid generativ / diskriminerende model af ordsegmentering baseret på nonparametriske bayesiske metoder. I modsætning til almindelig diskriminerende ordsegmentering, som kun er afhængig af mærkede data, udnytter vores halvovervågede model også en enorm mængde ikke-mærket tekst til automatisk at lære nye 'ord', og begrænser dem yderligere ved at bruge en mærket data til at segmentere ikke-standardtekster som dem, der findes i sociale netværkstjenester. Specielt kombinerer vores hybridmodel en diskriminerende klassificering (CRF; Lafferty m.fl. (2001) og en ukontrolleret ordsegmentering (NPYLM; Mochihashi m.fl. (2009)), med en gennemsigtig udveksling af oplysninger mellem disse to modelstrukturer inden for rammerne af den halvovervågede ramme (JESS-CM; Suzuki og Isozaki (2008)). Vi bekræftede, at det på passende vis kan segmentere ikke-standardtekster som dem i Twitter og Weibo og har næsten state-of-the-art nøjagtighed på standarddatasæt på japansk, kinesisk og thailandsk.</abstract_da>
      <abstract_de>Diese Arbeit stellt ein neuartiges hybrides generatives/diskriminierendes Modell der Wortsegmentierung vor, das auf nichtparametrischen Bayesischen Methoden basiert. Im Gegensatz zu gewöhnlicher diskriminierender Wortsegmentierung, die nur auf beschrifteten Daten beruht, nutzt unser halbüberwachtes Modell auch eine große Menge an unbekennzeichneten Texten, um automatisch neue "Wörter" zu lernen. Außerdem werden diese durch die Verwendung von beschrifteten Daten zur Segmentierung nicht standardisierter Texte, wie sie in sozialen Netzwerken gefunden werden, weiter eingeschränkt. Konkret kombiniert unser Hybridmodell einen diskriminierenden Klassifikator (CRF; Lafferty et al. (2001) und eine unüberwachte Wortsegmentierung (NPYLM; Mochihashi et al. (2009)), mit einem transparenten Informationsaustausch zwischen diesen beiden Modellstrukturen im semi-supervised framework (JESS-CM; Suzuki und Isozaki (2008)). Wir haben bestätigt, dass es nicht-standardisierte Texte wie Twitter und Weibo angemessen segmentieren kann und nahezu State-of-the-Art-Genauigkeit auf Standard-Datensätzen in Japanisch, Chinesisch und Thai aufweist.</abstract_de>
      <abstract_id>Kertas ini mempersembahkan model hibrid baru generatif/diskriminatif segmen kata berdasarkan metode Bayesia yang tidak parametrik. Unlike ordinary discriminative word segmentation which relies only on labeled data, our semi-supervised model also leverages a huge amounts of unlabeled text to automatically learn new 'words', and further constrains them by using a labeled data to segment non-standard texts such as those found in social networking services.  Secara spesifik, model hibrid kita menggabungkan klasifikasi diskriminatif (CRF; Lafferty et al. (2001) dan segmen kata yang tidak diawasi (NPYLM; Mochihashi et al. (2009)), dengan pertukaran informasi transparan antara dua struktur model ini dalam rangka semi-mengawasi (JESS-CM; Suzuki dan Isozaki (2008)). Kami mengkonfirmasi bahwa dapat segmen teks yang tidak standar seperti di Twitter dan Weibo dan memiliki akurasi hampir state-of-the-art pada set data standar dalam bahasa Jepang, Cina dan Thailand.</abstract_id>
      <abstract_bg>Настоящата статия представя нов хибриден генеративен/дискриминативен модел на сегментация на думи, базиран на непараметрични баезийски методи. За разлика от обикновената дискриминационна сегментация на думи, която разчита само на етикетирани данни, нашият полу-надзорен модел също използва огромни количества незабелязан текст, за да научи автоматично нови "думи", и допълнително ги ограничава, като използва етикетирани данни, за да сегментира нестандартни текстове като тези, намерени в социалните мрежи услуги. По-конкретно, нашият хибриден модел съчетава дискриминационен класификатор (CRF; Lafferty et al. (2001) и неконтролирана словна сегментация (NPYLM; Mochihashi et al. (2009)), с прозрачен обмен на информация между тези две структури на модела в рамките на полунадзорната рамка (JESS-CM; Suzuki и Isozaki (2008)). Потвърдихме, че може правилно да сегментира нестандартни текстове като тези в Туитър и Уайбо и има почти най-съвременна точност на стандартните набори от данни на японски, китайски и тайландски език.</abstract_bg>
      <abstract_hr>Ovaj papir predstavlja nov hibridni generativni/diskriminacijski model segmentacije riječi na temelju nenaparatnih Bayesijskih metoda. Za razliku od obične diskriminacijske segmentacije riječi koje se oslanjaju samo na označenim podacima, naš polu nadzorni model također utječe na ogromne količine nenabeliranog teksta kako bi automatski naučio nove 'riječi', a dalje ih ograničava koristeći označene podatke za segmentiranje neustandardnih tekstova kao što su pronađeni u socijalnim mrežnim uslugama. Posebno, naš hibridni model kombinira diskriminirajuću klasifikaciju (CRF; Lafferty et al. (2001) i neodređenu segmentaciju riječi (NPYLM; Mochihashi et al. (2009)), s transparentnom razmjenom informacija između tih dva modelnih struktura u polu nadziranom okviru (JESS-CM; Suzuki i Isozaki (2008)). Potvrdili smo da može odgovarajući dijelovati neustandardne tekste poput one na Twitter i Weibo i da ima skoro stanje umjetnosti preciznosti na standardnim podacima na japanskom, kineskom i tajlandskom.</abstract_hr>
      <abstract_nl>Deze paper presenteert een nieuw hybride generatief/discriminatief model van woordsegmentatie gebaseerd op niet-parametrische Bayesiaanse methoden. In tegenstelling tot gewone discriminerende woordsegmentatie die alleen gebaseerd is op gelabelde gegevens, maakt ons semi-supervised model ook gebruik van een enorme hoeveelheid niet-gelabelde tekst om automatisch nieuwe 'woorden' te leren, en beperkt deze verder door een gelabelde gegevens te gebruiken om niet-standaard teksten zoals die in sociale netwerkdiensten te segmenteren. Specifiek combineert ons hybride model een discriminatieve classificator (CRF; Lafferty et al. (2001) en onbeheerde woordsegmentatie (NPYLM; Mochihashi et al. (2009)), met een transparante uitwisseling van informatie tussen deze twee modelstructuren binnen het semi-supervised framework (JESS-CM; Suzuki en Isozaki (2008)). We hebben bevestigd dat het niet-standaard teksten zoals die in Twitter en Weibo geschikt kan segmenteren en bijna state-of-the-art nauwkeurigheid heeft op standaard datasets in het Japans, Chinees en Thais.</abstract_nl>
      <abstract_ko>비변수 베일스 방법을 바탕으로 한 혼합 생성/판별 분사 모델을 제시했다.표기 데이터에만 의존하는 일반적인 구분적 단어와 달리 우리의 반감독모델은 대량의 표기되지 않은 텍스트를 이용하여 새로운'단어'를 자동으로 학습하고 표기 데이터를 사용하여 비표준적인 텍스트(예를 들어 소셜네트워크서비스의 텍스트)를 분할함으로써 그것들을 더욱 제한한다.구체적으로 말하자면 우리의 혼합모델은 판별분류기(CRF, Lafferty 등(2001)과 무감독분사(NPYLM, 모치하시 등(2009)와 반감독틀 안의 이 두 모델 구조 간의 투명한 정보 교환(JESS-CM, Suzuki와 Isozaki(2008)을 결합시켰다.트위터와 웨이보 등 비표준 텍스트를 적절하게 분할할 수 있고 일본어, 중국어, 태국어의 표준 데이터 집합에서 가장 선진적인 정확성을 지니고 있음을 확인했다.</abstract_ko>
      <abstract_sw>Gazeti hili linaonyesha muundo wa kutengenezwa/ubaguzi wa maneno kwa kutumia mbinu zisizo za kibiashara za Bayesia. Tofauti na utofauti wa neno la kawaida ambalo linategemea tu kwa taarifa zilizowekwa, modeli yetu ya sekondari pia inatumia kiasi kikubwa cha maandishi yasiyopangwa kwa kujifunza kwa kujifunza 'maneno mpya' na pia inawazuia kwa kutumia taarifa zilizoonyesha ili kugawanya maandishi yasiyo ya kawaida kama vile zile zilizopatikana katika huduma za mitandao ya kijamii. Kwa ujumla, modeli yetu ya hybrid inaunganisha mchanganyiko wa tofauti (CRF; Differty et al. (2001) na mchanganyiko wa neno lisilo na uhakika (NPYLM; Mochihashi et al. (2009)), na kubadilishana kwa uwazi kati ya miundo mbili ya miundo mbili ndani ya mfumo ulioangaliwa na semi (JESS-CM; Suzuki na Isozaki (2008). Tulithibitisha kwamba inaweza kutengeneza maandishi yasiyo ya kawaida kama wale kwenye mtandao wa Twita na Weibo na ina karibu sahihi ya hali ya sanaa kuhusu seti za taarifa za kawaida nchini Japani, China na Thai.</abstract_sw>
      <abstract_fa>این کاغذ یک مدل ژنترافی/جدایی جدایی از جمع کردن کلمات بر اساس روش های غیر پارامتریک بیزیان را نشان می دهد. برخلاف جدا کردن کلمه‌های معمولی که تنها بر داده‌های برچسب بستگی دارد، مدل نیمه‌برچسب ما همچنین یک مقدار بزرگی از متن نامزده‌ای برای یادگرفتن کلمه‌های جدید را به خودکار محدودیت می‌کند، و بیشتر آنها را با استفاده از داده‌های برچسب برای جدا کردن متن‌های غیراستاندارد مانند آن‌ها که در خدمات شبکه‌های اج مخصوصاً مدل هیبریدی ما یک کلاسیر جدایی (CRF; Lafferty et al. (۲۰۰۱) و جدایی کلمات غیرقابل تحویل (NPYLM; Mochihashi et al. (۲۰۰۹)، با یک تبادل مشاهده اطلاعات بین این دو ساختار مدل در چهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچ ما تایید کردیم که می‌تواند متن‌های غیر استاندارد را به طور مناسب جدا کند مانند آن‌ها که توئیتر و ویبو هستند و تقریباً دقیقات هنری در مجموعه‌های استاندارد در ژاپن، چینی و تایید دارد.</abstract_fa>
      <abstract_tr>Bu kagyz Beýeziýanyň döwletlerinde daýanýan hybrid jenerativ/diskriminçy söz segmentasyň nusgasyny görkezýär. Adatça diskriminçy söz segmentasiýasynda diňe etiket edilen maglumatlara güýçlän däldir, hem biziň semi-kontrol modelimiz täze sözleri otomatik bilen öwrenmek üçin ullanmaýan täze bir şekilde täsirleýär we olaryň etiket edilen maglumatlaryny sosyal netek hızmetinde tapylan metinleri bölmek üçin ýüzünde süýtgeder. Adatça, biziň hybrid modelimiz diskriminät klassifikatçisini (CRF; Lafferty et al. (2001) we suýruklanmaýan söz segmentasyny (NPYLM; Mochihashi et al. (2009)), bu iki nusga arasynda semi-kontrol edilen çerçewçiwde bir terjime edip bilen informasiýa çykýar (JESS-CM; Suzuki we Isozaki (2008)). Biz Twitter, Weibo ýaly standart metinleriniň dogry ýagdaýynda taýýarlap biler diýip kabul etdik we ol Japonça, Çin çe we Taýlандa standart veri setirleriniň dogrylygyny hasapladyk.</abstract_tr>
      <abstract_af>Hierdie papier stel 'n roman hybrid genereerbare/diskriminasiewe model van woord segmentasie gebaseer op nie-parametriese Bayesian metodes. Ongelyks van gewone diskriminasiewe woord segmentasie wat slegs op etiketeerde data aflys, ons semi-ondersoekte model het ook 'n groot hoeveelheid ongeabelde teks aan automaties leer nuwe 'woorde', en verdere beperk hulle deur 'n etiketeerde data te gebruik na segmenteer nie-standaard teks soos wat in sosiale netwerking dienste gevind is. Spesifieke, ons hibrid model kombinieer 'n diskriminasiewe klassifiseerder (CRF; Lafferty et al. (2001) en ononderwerpende woord segmentasie (NPYLM; Mochihashi et al. (2009)), met 'n deursigtige vervang van inligting tussen hierdie twee model strukture binne die semi-onderwerp raamwerk (JESS-CM; Suzuki en Isozaki (2008)). Ons het bevestig dat dit behoorlik kan segment nie-standaard teks soos die in Twitter en Weibo en het byna staat-van-kuns-presies op standaard datastelle in Japanse, Sjinese en Thaise.</abstract_af>
      <abstract_sq>Ky dokument paraqet një model të ri hibridik gjenerativ/diskriminues të segmentimit të fjalëve bazuar në metodat jo parametrike Bayesian. Unlike ordinary discriminative word segmentation which relies only on labeled data, our semi-supervised model also leverages a huge amounts of unlabeled text to automatically learn new 'words', and further constrains them by using a labeled data to segment non-standard texts such as those found in social networking services.  Specifically, our hybrid model combines a discriminative classifier (CRF; Lafferty et al. (2001) and unsupervised word segmentation (NPYLM; Mochihashi et al. (2009)), with a transparent exchange of information between these two model structures within the semi-supervised framework (JESS-CM; Suzuki and Isozaki (2008)).  We confirmed that it can appropriately segment non-standard texts like those in Twitter and Weibo and has nearly state-of-the-art accuracy on standard datasets in Japanese, Chinese, and Thai.</abstract_sq>
      <abstract_az>Bu kağıt, parametrik Bayesiya metodlarına dayanan yeni hibrid generikatlı/diskriminatlı söz segmentasiyasının modelini göstərir. Yalnız etiketli məlumatlar üzərində təvəkkül edən sıradan diskriminativ söz segmentasiyası kimi, yarı-gözləyirli modellərimiz də yeni sözləri öyrənmək üçün çox böyük dəyişiklik məlumatları yaratdı və daha sonra etiketli məlumatları sosyal netverk servislərdə bulunan məlumatları segment etmək üçün dəyişiklik edir. Özellikle, hibrid modellərimiz yarı-gözləyirli çerçevesinin içində bu iki modellərin arasındakı məlumatları orta-gözləyir (JESS-CM; Suzuki və Isozaki (2008)) ilə birləşdirir. Biz təsdiqlədik ki, o, Twitter və Weibo kimi standart olmayan məktubları yaxşı bölüşdürə bilər və Yaponca, Çinlə və Tayla standart veri setlərinin dəqiqliyinə bənzəyir.</abstract_az>
      <abstract_am>ይህም ፕሮግራም የባይስያ ሥርዓት ባይሄስቲ ባሕያዊ ሥርዓት ላይ የቃላት ግንኙነቶችን አቀረበ፡፡ በተለየ ጥያቄ ቃላት በጽሑፎች ላይ ብቻ የሚታመን እና በጽሑፍ ማቀናቀል፣ የsemi-ተጠባባቂው ሞዴሌዎቻችን ደግሞ አዲስ ቃልን ለራሱ ማምረጥ ትልቅ የጽሑፍ ክፍል ያሰጣቸዋል፡፡ በተለየ ጊዜ የኬብሪድ ሞዴል (CRF; Lafferty et al. (2001) እና ያልጠበቀ ቃላት segmentation (NPYLM; ሞኪሐሺ et al. (2009)) በተለየ በሁለት ሞዴል አካውንቶች መካከል የተለየ የእውይይይት መረጃዎችን (JESS-CM; ሱዙኪ እና ይስozaki (2008). በትዊተር እና Weibo ያሉትን የድምፅ ጽሑፎችን እንደሚያስፈልግ አረጋገጥን፤ በጃፓን፣ ቻይና እና ታይኛ የዳርቻ ጽሑፎች የደረጃ ግንኙነት አቅራቢያ አለበት፡፡</abstract_am>
      <abstract_hy>Այս հոդվածը ներկայացնում է բառերի սեգմետրացիայի նոր հիբրիդ սերնդի և խտրականության մոդել, որը հիմնված է ոչ պարամետրիկ բեյզիացի մեթոդների վրա: Unlike ordinary discriminative word segmentation which relies only on labeled data, our semi-supervised model also leverages a huge amounts of unlabeled text to automatically learn new 'words', and further constrains them by using a labeled data to segment non-standard texts such as those found in social networking services.  Հիմաստաբար, մեր հիբրիդ մոդելը միավորում է խտրականության դասակարգում (ԿՌՖ, Լաֆերթի և այլն. (2001) և անվերահսկված բառերի սեգմետրացիա (ՆՊՅԼՄ, Մոչիհասի և այլն. (2009) այս երկու մոդելների կառուցվածքների միջև թափանցիկ տեղեկատվության փոխանակում կիսավերահսկված շրջանակում (Յեսս-ԿՄ, Սյուզուկի Մենք հաստատեցինք, որ այն կարող է պատասխանաբար բաժանել ոչ ստանդարտ տեքստերը, ինչպիսիք են Թվիթերի և Վայբոյի տեքստերը, և ունի գրեթե ամենաբարձր ճշգրտություն ստանդարտ տվյալների համակարգերի վրա ճապոներեն, չինարեն և թայլանդերեն:</abstract_hy>
      <abstract_ca>Aquest paper presenta un nou model hibridgenerador/discriminatiu de segmentació de paraules basat en mètodes baièsius no paramètrics. A diferència de la segmentació de paraules discriminatòries habitual que només es basa en dades etiquetades, el nostre model semisupervisat també aprofita una gran quantitat de text no etiquetat per aprendre automàticament noves "paraules", i altres les limita fent servir una data etiquetada per segmentar textos no estàndard com els que es troben en serveis de xarxa social. Concretament, el nostre model híbrid combina un classificador discriminatiu (CRF; Lafferty et al. (2001) i una segmentació de paraules no supervisada (NPYLM; Mochihashi et al. (2009)), amb un intercanvi transparent d'informació entre aquestes dues estructures models dins l'estructura semisupervisada (JESS-CM; Suzuki i Isozaki (2008)). Vam confirmar que pot segmentar adequadament textos no estàndard com els de Twitter i Weibo i que té gairebé la precisió més moderna en conjunts de dades estàndard en japonès, xinès i tailandes.</abstract_ca>
      <abstract_bn>এই পত্রিকাটি প্যারামেট্রিক বেয়েসিয়ান পদ্ধতির উপর ভিত্তিক শব্দের বিভিন্ন ভিন্ন ভিন্ন ভিত্তিক ভিত্তিক শব্দের বৈষম্ Unlike ordinary discriminative word segmentation which relies only on labeled data, our semi-supervised model also leverages a huge amounts of unlabeled text to automatically learn new 'words', and further constrains them by using a labeled data to segment non-standard texts such as those found in social networking services.  বিশেষ করে, আমাদের হাইব্রিড মডেল একটি বৈষম্যিক শ্রেণীবিভাগের (সিএসএফ; ল্যাফার্টি আর আল. (২০০১) এবং অরক্ষণশীল শব্দ বিভাগ (NPYLM; মোচিহি et al. (২০০৯)) স্বচ্ছতাভাবে এই দুই মডেল কাঠামোর মধ্যে স্বচ্ছতা বিনিময়ে ত আমরা নিশ্চিত করেছি যে এটি টুইটার এবং উইবোতে যেমন স্ট্যান্ডার্ড না লেখাগুলো বিভক্ত করতে পারে এবং জাপানি, চীন এবং থাই-এর স্ট্যান্ডার্ডার ডাটাসেটগ</abstract_bn>
      <abstract_fi>Tässä työssä esitellään uusi hybridi generatiivinen/diskriminatiivinen sanasegmentoinnin malli, joka perustuu ei-parametrisiin bayesilaisiin menetelmiin. Toisin kuin tavallinen syrjivä sanasegmentointi, joka perustuu vain merkittyyn dataan, puolivalvottu mallimme hyödyntää myös valtavia määriä merkitsemätöntä tekstiä oppiakseen automaattisesti uusia sanoja ja rajoittaa niitä entisestään käyttämällä merkittyä dataa segmentoidakseen epästandardeja tekstejä, kuten sosiaalisen median palveluissa. Hybridimallissamme yhdistyvät erityisesti syrjivä luokittelija (CRF; Lafferty et al. (2001) ja valvomaton sanasegmentointi (NPYLM; Mochihashi et al. (2009)), ja näiden kahden mallirakenteen välinen läpinäkyvä tietojenvaihto puolivalvotussa kehyksessä (JESS-CM; Suzuki ja Isozaki (2008)). Vahvistimme, että se pystyy asianmukaisesti segmentoimaan epätavanomaisia tekstejä, kuten Twitterissä ja Weibossa, ja sillä on lähes uusinta tarkkuutta japanin, kiinan ja thain kielissä.</abstract_fi>
      <abstract_cs>Tento článek představuje nový hybridní generativní/diskriminační model segmentace slov založený na neparametrických Bayesovských metodách. Na rozdíl od běžné diskriminační segmentace slov, která se opírá pouze o označená data, náš model s polovým dohledem také využívá obrovské množství neoznačeného textu k automatickému učení se nových "slov", a dále je omezuje použitím označených dat k segmentování nestandardních textů, jako jsou ty, které se nacházejí ve službách sociálních sítí. Konkrétně, náš hybridní model kombinuje diskriminační klasifikátor (CRF; Lafferty et al. (2001) a bez dozoru segmentaci slov (NPYLM; Mochihashi et al. (2009)), s transparentní výměnou informací mezi těmito dvěma modelovými strukturami v rámci semi-supervised frameworku (JESS-CM; Suzuki a Isozaki (2008)). Potvrdili jsme, že může vhodně segmentovat nestandardní texty, jako jsou například na Twitteru a Weibo a má téměř nejmodernější přesnost na standardních datových sadách v japonštině, čínštině a thajsku.</abstract_cs>
      <abstract_et>Käesolev töö tutvustab uudset hübriidgeneratiivset/diskrimineerivat sõna segmenteerimise mudelit, mis põhineb mitteparameetrilistel bayesia meetoditel. Erinevalt tavalisest diskrimineerivast sõnasegmenteerimisest, mis põhineb ainult märgistatud andmetel, kasutab meie pooljärelevalve all olev mudel ka tohutut hulka märgistamata teksti, et automaatselt õppida uusi sõnu, ning piirab neid veelgi, kasutades märgistatud andmeid mittestandardsete tekstide segmenteerimiseks, nagu sotsiaalvõrgustike teenustes leitavad tekstid. Täpsemalt ühendab meie hübriidmudel diskrimineeriva klassifitseerija (CRF; Lafferty jt. (2001) ja järelevalveta sõnasegmenteerimise (NPYLM; Mochihashi jt. (2009)), läbipaistva teabevahetusega nende kahe mudeli struktuuri vahel pooljärelevalve raames (JESS-CM; Suzuki ja Isozaki (2008)). Me kinnitasime, et see suudab asjakohaselt segmenteerida mittestandardseid tekste, nagu Twitteris ja Weibos, ning on peaaegu tipptasemel täpsusel jaapani, hiina ja tai keeles.</abstract_et>
      <abstract_bs>Ovaj papir predstavlja nov hibridni generativni/diskriminacijski model segmentacije riječi na temelju neparametričkih Bayesijskih metoda. Za razliku od obične diskriminacijske segmentacije riječi koja se oslanja samo na etiketirane podatke, naš polu nadzorni model također utječe na ogromne količine nenabeliranog teksta kako bi automatski naučio nove 'riječi', a dalje ih ograničava koristeći etiketirane podatke na segmentiranje neustandardnih tekstova kao što su pronađeni u službama socijalnih mreža. Posebno, naš hibridni model kombinira diskriminativnu klasifikaciju (CRF; Lafferty et al. (2001) i neodređenu segmentaciju riječi (NPYLM; Mochihashi et al. (2009)), sa transparentnom razmjenom informacija između tih dva modelnih struktura u polu nadzornom okviru (JESS-CM; Suzuki i Isozaki (2008)). Potvrdili smo da može odgovarajući segmentirati ne standardne tekste poput one na Twitter i Weibo i da ima skoro stanje umjetnosti preciznosti na standardnim podacima na japanskom, kineskom i tajlandskom.</abstract_bs>
      <abstract_he>העבודה הזו מציגה מודל חדש גידול היברידי/דיסקרטיבי של סגמנציה מילים מבוסס על שיטות בייזיות לא פרמטריות. בניגוד למחלקת מילים מיוחדת רגילה שמבוססת רק על נתונים מסומנים, המודל שלנו חצי-שולט גם משתמש בכמות עצומות של טקסט לא מסומן כדי ללמוד אוטומטית 'מילים' חדשות, ומגבלת אותם יותר על ידי השימוש של נתונים מסומנים כדי לחלק טקסטים לא סטנדרטיים כמו אלה שנמצאים בשירותים רשת חבר Specifically, our hybrid model combines a discriminative classifier (CRF; Lafferty et al. (2001) and unsupervised word segmentation (NPYLM; Mochihashi et al. (2009)), with a transparent exchange of information between these two model structures within the semi-supervised framework (JESS-CM; Suzuki and Isozaki (2008)).  אישרנו שהוא יכול לחלק בהתאם טקסטים לא סטנדרטיים כמו אלה בטוויטר וייבו ויש לו כמעט מדויק חדש במידע בסטנדרטי נתונים ביפני, סיני וטאיילנדי.</abstract_he>
      <abstract_sk>V prispevku je predstavljen nov hibridni generativni/diskriminativni model segmentacije besed, ki temelji na neparametričnih bajezijskih metodah. Za razliko od običajne diskriminacijske segmentacije besed, ki temelji samo na označenih podatkih, naš pol nadzorovan model uporablja tudi ogromne količine neoznačenega besedila za samodejno učenje novih besed in jih še dodatno omejuje z uporabo označenih podatkov za segmentiranje nestandardnih besedil, kot so besedila, ki jih najdemo v storitvah socialnih omrežij. Natančneje, naš hibridni model združuje diskriminativni klasifikator (CRF; Lafferty et al. (2001) in nenadzorovano segmentacijo besed (NPYLM; Mochihashi et al. (2009)), s pregledno izmenjavo informacij med tema dvema strukturama modela znotraj polnadzorovanega okvira (JESS-CM; Suzuki in Isozaki (2008)). Potrdili smo, da lahko ustrezno segmentira nestandardna besedila, kot sta tista v Twitterju in Weibu, ter ima skoraj najsodobnejšo točnost standardnih naborov podatkov v japonščini, kitajščini in tajščini.</abstract_sk>
      <abstract_ha>Wannan takardan na ƙunsa da wani hoton Hybri mai gabatar da/mai yin ɓarna na maganar segmentation a kan non-parametric Bayesian hanyoyi. Di daidaita da rabon maganar da aka inganci na ɗabi'a, yana dõgara kawai kan data na tsari, misalinmu wanda aka yi tsaron da shi na ƙara yana da yawan abu mai girma wa matsayin da ba'anar shi ba dõmin ya sanar da yanzu na yanzu-yanzu, kuma yana ƙudura su da amfani da data na rubutu zuwa raba matsayin na'ura, kamar waɗanda aka samu a cikin tsarin mitandanin jamii. Aka ƙayyade, misalinmu ya koma koma da wani mai rarrabo (CRF; Laffty et al. (2001) da kuma an tsare maganar segmentation (NPYLM; Mokishi et al. (2009)), da an buɗe bayani da bayani da cire-daban laban misalin biyu cikin firam wanda aka yi shekara (JESS-CM; Suzuki da Iszoaki (2006). Mun gaskata cẽwa, za ta raba matsayin waɗanda ba'a daidaita ba kamar waɗanda ke cikin Twitter da Weibo kuma yana da nesten taƙalumi na-sanar a kan daidaita matsayin taƙaita cikin japanen, China da Tai.</abstract_ha>
      <abstract_jv>This paper represents a new HyBridge Generative/Diskimiative model of word segmentation supported on nonparametris bayesi method. Genjer-genjer diunting langgar sampeyan kuwi wis dipun ciptaaken sing wis etiket data, kita model sing wis nguasai nyimpen a langgar sampeyan nganggo dolanan sing gak bener Awak dhéwé, pilihan model nyebutaké kelompok kelompok dislikasi (CF; Laffty et al.(2011) lan segmentation awak dhéwé Awak dhéwé wis ngênggunaké karo akeh basa sing gak bener tentang karo Google lan weibo lan saiki karo hal-hal layang-layang sampek awak dhéwé kuwi awak dhéwé kuwi basa sing japané, Cino lan Yulan.</abstract_jv>
      <abstract_bo>This paper presents a novel hybrid generative/discriminative model of word segmentation based on nonparametric Bayesian methods. Unlike ordinary discriminative word segmentation which relies only on labeled data, our semi-supervised model also leverages a huge amounts of unlabeled text to automatically learn new 'words', and further constrains them by using a labeled data to segment non-standard texts such as those found in social networking services. Specifically, our hybrid model combines a discriminative classifier (CRF; Lafferty et al. (2001) and unsupervised word segmentation (NPYLM; Mochihashi et al. (2009)), with a transparent exchange of information between these two model structures within the semi-supervised framework (JESS-CM; Suzuki and Isozaki (2008)). ངེད་གཉིས་ཀྱིས་ཌིས་ཌིར་དང་ཝེ་པོ་ནང་གི་ཡིག</abstract_bo>
      </paper>
    <paper id="14">
      <title>Joint Modeling of Topics, <a href="https://en.wikipedia.org/wiki/Citation">Citations</a>, and Topical Authority in Academic Corpora</title>
      <author><first>Jooyeon</first><last>Kim</last></author>
      <author><first>Dongwoo</first><last>Kim</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <doi>10.1162/tacl_a_00055</doi>
      <abstract>Much of scientific progress stems from previously published findings, but searching through the vast sea of scientific publications is difficult. We often rely on metrics of scholarly authority to find the prominent authors but these <a href="https://en.wikipedia.org/wiki/Authority">authority indices</a> do not differentiate <a href="https://en.wikipedia.org/wiki/Authority">authority</a> based on research topics. We present Latent Topical-Authority Indexing (LTAI) for jointly modeling the topics, <a href="https://en.wikipedia.org/wiki/Citation">citations</a>, and topical authority in a corpus of academic papers. Compared to previous <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, <a href="https://en.wikipedia.org/wiki/LTAI">LTAI</a> differs in two main aspects. First, it explicitly models the generative process of the <a href="https://en.wikipedia.org/wiki/Citation">citations</a>, rather than treating the <a href="https://en.wikipedia.org/wiki/Citation">citations</a> as given. Second, it models each author’s influence on citations of a paper based on the topics of the cited papers, as well as the citing papers. We fit LTAI into four academic corpora : CORA, <a href="https://en.wikipedia.org/wiki/Arxiv">Arxiv Physics</a>, <a href="https://en.wikipedia.org/wiki/Proceedings_of_the_National_Academy_of_Sciences_of_the_United_States_of_America">PNAS</a>, and <a href="https://en.wikipedia.org/wiki/Citeseer">Citeseer</a>. We compare the performance of LTAI against various baselines, starting with the <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">latent Dirichlet allocation</a>, to the more advanced models including author-link topic model and dynamic author citation topic model. The results show that LTAI achieves improved accuracy over other similar <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> when predicting words, <a href="https://en.wikipedia.org/wiki/Citation">citations</a> and authors of publications.</abstract>
      <pages>191–204</pages>
      <url hash="dfdf195e">Q17-1014</url>
      <video href="https://vimeo.com/238233899" />
      <bibkey>kim-etal-2017-joint</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cora">Cora</pwcdataset>
    </paper>
    <paper id="15">
      <title>Pushing the Limits of Translation Quality Estimation</title>
      <author><first>André F. T.</first><last>Martins</last></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <author><first>Fabio N.</first><last>Kepler</last></author>
      <author><first>Ramón</first><last>Astudillo</last></author>
      <author><first>Chris</first><last>Hokamp</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <doi>10.1162/tacl_a_00056</doi>
      <abstract>Translation quality estimation is a task of growing importance in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, due to its potential to reduce post-editing human effort in disruptive ways. However, this potential is currently limited by the relatively low <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of existing <a href="https://en.wikipedia.org/wiki/System">systems</a>. In this paper, we achieve remarkable improvements by exploiting synergies between the related tasks of word-level quality estimation and automatic post-editing. First, we stack a new, carefully engineered, neural model into a rich feature-based word-level quality estimation system. Then, we use the output of an automatic post-editing system as an extra feature, obtaining striking results on WMT16 : a word-level FMULT1 score of 57.47 % (an absolute gain of +7.95 % over the current state of the art), and a <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation score</a> of 65.56 % for sentence-level HTER prediction (an absolute gain of +13.36 %).</abstract>
      <pages>205–218</pages>
      <url hash="9b4e3d2c">Q17-1015</url>
      <video href="https://vimeo.com/234955039" />
      <bibkey>martins-etal-2017-pushing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="16">
      <title>Winning on the Merits : The Joint Effects of Content and Style on Debate Outcomes</title>
      <author><first>Lu</first><last>Wang</last></author>
      <author><first>Nick</first><last>Beauchamp</last></author>
      <author><first>Sarah</first><last>Shugars</last></author>
      <author><first>Kechen</first><last>Qin</last></author>
      <doi>10.1162/tacl_a_00057</doi>
      <abstract>Debate and <a href="https://en.wikipedia.org/wiki/Deliberation">deliberation</a> play essential roles in <a href="https://en.wikipedia.org/wiki/Politics">politics</a> and <a href="https://en.wikipedia.org/wiki/Government">government</a>, but most models presume that debates are won mainly via superior style or agenda control. Ideally, however, debates would be won on the merits, as a function of which side has the stronger arguments. We propose a predictive model of <a href="https://en.wikipedia.org/wiki/Debate">debate</a> that estimates the effects of linguistic features and the latent persuasive strengths of different topics, as well as the interactions between the two. Using a dataset of 118 Oxford-style debates, our model’s combination of content (as latent topics) and style (as linguistic features) allows us to predict audience-adjudicated winners with 74 % accuracy, significantly outperforming linguistic features alone (66 %). Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> finds that winning sides employ stronger arguments, and allows us to identify the <a href="https://en.wikipedia.org/wiki/Linguistic_feature">linguistic features</a> associated with strong or weak arguments.</abstract>
      <pages>219–232</pages>
      <url hash="1df9c5da">Q17-1016</url>
      <video href="https://vimeo.com/234953410" />
      <bibkey>wang-etal-2017-winning</bibkey>
    </paper>
    <paper id="17">
      <title>Domain-Targeted, High Precision Knowledge Extraction</title>
      <author><first>Bhavana</first><last>Dalvi Mishra</last></author>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <doi>10.1162/tacl_a_00058</doi>
      <abstract>Our goal is to construct a domain-targeted, high precision knowledge base (KB), containing general (subject, predicate, object) statements about the world, in support of a downstream question-answering (QA) application. Despite recent advances in information extraction (IE) techniques, no suitable resource for our task already exists ; existing resources are either too noisy, too named-entity centric, or too incomplete, and typically have not been constructed with a clear scope or purpose. To address these, we have created a domain-targeted, high precision knowledge extraction pipeline, leveraging <a href="https://en.wikipedia.org/wiki/Open_IE">Open IE</a>, <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a>, and a novel canonical schema learning algorithm (called CASI), that produces high precision knowledge targeted to a particular domain-in our case, elementary science. To measure the KB’s coverage of the target domain’s knowledge (its comprehensiveness with respect to science) we measure <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> with respect to an independent corpus of domain text, and show that our pipeline produces output with over 80 % precision and 23 % <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> with respect to that target, a substantially higher coverage of tuple-expressible science knowledge than other comparable resources. We have made the KB publicly available.</abstract>
      <pages>233–246</pages>
      <url hash="41bc9b67">Q17-1017</url>
      <bibkey>dalvi-mishra-etal-2017-domain</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/nell">NELL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/yago">YAGO</pwcdataset>
    </paper>
    <paper id="18">
      <title>Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling</title>
      <author><first>Gábor</first><last>Berend</last></author>
      <doi>10.1162/tacl_a_00059</doi>
      <abstract>In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations. The proposed model obtains (near) state-of-the art performance for both <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a> and <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> for a variety of languages. Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks. The proposed <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> has favorable generalization properties as <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> retains over 89.8 % of its average POS tagging accuracy when trained at 1.2 % of the total available training data, i.e. 150 sentences per language.</abstract>
      <pages>247–261</pages>
      <url hash="d2ee01f2">Q17-1018</url>
      <video href="https://vimeo.com/234952125" />
      <attachment type="presentation" hash="39e6ecde">Q17-1018.Presentation.pdf</attachment>
      <bibkey>berend-2017-sparse</bibkey>
    </paper>
    <paper id="20">
      <title>Cross-Lingual Syntactic Transfer with Limited Resources</title>
      <author><first>Mohammad Sadegh</first><last>Rasooli</last></author>
      <author><first>Michael</first><last>Collins</last></author>
      <doi>10.1162/tacl_a_00061</doi>
      <abstract>We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available. This method makes use of three steps : 1) a method for deriving cross-lingual word clusters, which can then be used in a multilingual parser ; 2) a method for transferring lexical information from a target language to source language treebanks ; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins (2015). Experiments show improvements over the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> in several languages used in previous work, in a setting where the only source of translation data is the <a href="https://en.wikipedia.org/wiki/Bible">Bible</a>, a considerably smaller corpus than the <a href="https://en.wikipedia.org/wiki/Europarl_Corpus">Europarl corpus</a> used in previous work. Results using the <a href="https://en.wikipedia.org/wiki/Europarl_corpus">Europarl corpus</a> as a source of translation data show additional improvements over the results of Rasooli and Collins (2015). We conclude with results on 38 datasets from the Universal Dependencies corpora.</abstract>
      <pages>279–293</pages>
      <url hash="c515a202">Q17-1020</url>
      <video href="https://vimeo.com/276419865" />
      <bibkey>rasooli-collins-2017-cross</bibkey>
      <pwccode url="https://github.com/rasoolims/YaraParser" additional="false">rasoolims/YaraParser</pwccode>
    </paper>
    <paper id="21">
      <title>Overcoming <a href="https://en.wikipedia.org/wiki/Variation_(linguistics)">Language Variation</a> in <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> with Social Attention</title>
      <author><first>Yi</first><last>Yang</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <doi>10.1162/tacl_a_00062</doi>
      <abstract>Variation in <a href="https://en.wikipedia.org/wiki/Language">language</a> is ubiquitous, particularly in newer forms of <a href="https://en.wikipedia.org/wiki/Writing">writing</a> such as <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. Fortunately, variation is not random ; it is often linked to social properties of the author. In this paper, we show how to exploit <a href="https://en.wikipedia.org/wiki/List_of_social_networking_websites">social networks</a> to make <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> more robust to social language variation. The key idea is linguistic homophily : the tendency of socially linked individuals to use <a href="https://en.wikipedia.org/wiki/Language">language</a> in similar ways. We formalize this idea in a novel attention-based neural network architecture, in which attention is divided among several basis models, depending on the author’s position in the <a href="https://en.wikipedia.org/wiki/Social_network">social network</a>. This has the effect of smoothing the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification function</a> across the <a href="https://en.wikipedia.org/wiki/Social_network">social network</a>, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata. This <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> significantly improves the accuracies of <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> and on review data.</abstract>
      <pages>295–307</pages>
      <url hash="27d892d3">Q17-1021</url>
      <video href="https://vimeo.com/234952612" />
      <bibkey>yang-eisenstein-2017-overcoming</bibkey>
      <pwccode url="https://github.com/yiyang-gt/social-attention" additional="false">yiyang-gt/social-attention</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ciao">Ciao</pwcdataset>
    </paper>
    <paper id="22">
      <title>Semantic Specialization of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints</title>
      <author><first>Nikola</first><last>Mrkšić</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Diarmuid</first><last>Ó Séaghdha</last></author>
      <author><first>Ira</first><last>Leviant</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <author><first>Milica</first><last>Gašić</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <author><first>Steve</first><last>Young</last></author>
      <doi>10.1162/tacl_a_00063</doi>
      <abstract>We present Attract-Repel, an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. Attract-Repel facilitates the use of constraints from mono- and cross-lingual resources, yielding semantically specialized cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct high-quality vector spaces for a plethora of different languages, facilitating semantic transfer from high- to lower-resource ones. The effectiveness of our approach is demonstrated with state-of-the-art results on semantic similarity datasets in six languages. We next show that Attract-Repel-specialized vectors boost performance in the downstream task of dialogue state tracking (DST) across multiple languages. Finally, we show that cross-lingual vector spaces produced by our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> facilitate the training of multilingual DST models, which brings further performance improvements.</abstract>
      <pages>309–324</pages>
      <url hash="c10d8a26">Q17-1022</url>
      <bibkey>mrksic-etal-2017-semantic</bibkey>
    </paper>
    <paper id="23">
      <title>Colors in Context : A Pragmatic Neural Model for Grounded Language Understanding</title>
      <author><first>Will</first><last>Monroe</last></author>
      <author><first>Robert X.D.</first><last>Hawkins</last></author>
      <author><first>Noah D.</first><last>Goodman</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <doi>10.1162/tacl_a_00064</doi>
      <abstract>We present a model of pragmatic referring expression interpretation in a grounded communication task (identifying colors from descriptions) that draws upon predictions from two recurrent neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning framework. Experiments show that this combined pragmatic model interprets color descriptions more accurately than the <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifiers</a> from which it is built, and that much of this improvement results from combining the speaker and listener perspectives. We observe that pragmatic reasoning helps primarily in the hardest cases : when the model must distinguish very similar colors, or when few utterances adequately express the target color. Our findings make use of a newly-collected corpus of human utterances in color reference games, which exhibit a variety of pragmatic behaviors. We also show that the embedded speaker model reproduces many of these pragmatic behaviors.</abstract>
      <pages>325–338</pages>
      <url hash="4b9bc699">Q17-1023</url>
      <video href="https://vimeo.com/238230459" />
      <bibkey>monroe-etal-2017-colors</bibkey>
    </paper>
    <paper id="24">
      <title>Google’s Multilingual Neural Machine Translation System : Enabling Zero-Shot Translation<fixed-case>G</fixed-case>oogle’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</title>
      <author><first>Melvin</first><last>Johnson</last></author>
      <author><first>Mike</first><last>Schuster</last></author>
      <author><first>Quoc V.</first><last>Le</last></author>
      <author><first>Maxim</first><last>Krikun</last></author>
      <author><first>Yonghui</first><last>Wu</last></author>
      <author><first>Zhifeng</first><last>Chen</last></author>
      <author><first>Nikhil</first><last>Thorat</last></author>
      <author><first>Fernanda</first><last>Viégas</last></author>
      <author><first>Martin</first><last>Wattenberg</last></author>
      <author><first>Greg</first><last>Corrado</last></author>
      <author><first>Macduff</first><last>Hughes</last></author>
      <author><first>Jeffrey</first><last>Dean</last></author>
      <doi>10.1162/tacl_a_00065</doi>
      <abstract>We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>. On the WMT’14 benchmarks, a single multilingual model achieves comparable performance for <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">EnglishFrench</a> and surpasses state-of-theart results for <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">EnglishGerman</a>. Similarly, a single multilingual model surpasses state-of-the-art results for <a href="https://en.wikipedia.org/wiki/French_language">FrenchEnglish</a> and <a href="https://en.wikipedia.org/wiki/German_language">GermanEnglish</a> on WMT’14 and WMT’15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better <a href="https://en.wikipedia.org/wiki/Translation">translation</a> of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.</abstract>
      <pages>339–351</pages>
      <url hash="f12ac0a0">Q17-1024</url>
      <video href="https://vimeo.com/238233299" />
      <bibkey>johnson-etal-2017-googles</bibkey>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="25">
      <title>Unsupervised Learning of Morphological Forests</title>
      <author><first>Jiaming</first><last>Luo</last></author>
      <author><first>Karthik</first><last>Narasimhan</last></author>
      <author><first>Regina</first><last>Barzilay</last></author>
      <doi>10.1162/tacl_a_00066</doi>
      <abstract>This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edge-wise properties reflecting single-step morphological derivations, along with global distributional properties of the entire <a href="https://en.wikipedia.org/wiki/Forest">forest</a>. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting <a href="https://en.wikipedia.org/wiki/Loss_function">objective</a> is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks : <a href="https://en.wikipedia.org/wiki/Root-finding_algorithm">root detection</a>, clustering of morphological families, and <a href="https://en.wikipedia.org/wiki/Segmentation_(biology)">segmentation</a>. Our experiments demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> yields consistent gains in all three <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> compared with the best published results.</abstract>
      <pages>353–364</pages>
      <url hash="14936135">Q17-1025</url>
      <video href="https://vimeo.com/234952859" />
      <bibkey>luo-etal-2017-unsupervised</bibkey>
    </paper>
    <paper id="26">
      <title>Fully Character-Level Neural Machine Translation without Explicit Segmentation</title>
      <author><first>Jason</first><last>Lee</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Thomas</first><last>Hofmann</last></author>
      <doi>10.1162/tacl_a_00067</doi>
      <abstract>Most existing <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a> operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT’15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of the BLEU score and human judgment.</abstract>
      <pages>365–378</pages>
      <url hash="7e29e711">Q17-1026</url>
      <video href="https://vimeo.com/234955246" />
      <bibkey>lee-etal-2017-fully</bibkey>
      <pwccode url="https://github.com/nyu-dl/dl4mt-c2c" additional="true">nyu-dl/dl4mt-c2c</pwccode>
    </paper>
    <paper id="27">
      <title>Ordinal Common-sense Inference</title>
      <author><first>Sheng</first><last>Zhang</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <doi>10.1162/tacl_a_00068</doi>
      <abstract>Humans have the capacity to draw common-sense inferences from <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> : various things that are likely but not certain to hold based on established <a href="https://en.wikipedia.org/wiki/Discourse">discourse</a>, and are rarely stated explicitly. We propose an evaluation of automated common-sense inference based on an extension of recognizing textual entailment : predicting ordinal human responses on the subjective likelihood of an inference holding in a given context. We describe a framework for extracting common-sense knowledge from corpora, which is then used to construct a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for this ordinal entailment task. We train a neural sequence-to-sequence model on this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, which we use to score and generate possible inferences. Further, we annotate subsets of previously established datasets via our ordinal annotation protocol in order to then analyze the distinctions between these and what we have constructed.</abstract>
      <pages>379–395</pages>
      <url hash="2d56fac0">Q17-1027</url>
      <bibkey>zhang-etal-2017-ordinal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="28">
      <title>Learning Distributed Representations of Texts and Entities from Knowledge Base</title>
      <author><first>Ikuya</first><last>Yamada</last></author>
      <author><first>Hiroyuki</first><last>Shindo</last></author>
      <author><first>Hideaki</first><last>Takeda</last></author>
      <author><first>Yoshiyasu</first><last>Takefuji</last></author>
      <doi>10.1162/tacl_a_00069</doi>
      <abstract>We describe a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network model</a> that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to predict entities that are relevant to the text. Our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is designed to be generic with the ability to address various NLP tasks with ease. We train the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> using a large corpus of texts and their entity annotations extracted from <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>. We evaluated the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> on three important NLP tasks (i.e., sentence textual similarity, <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity linking</a>, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. Our code and trained <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are publicly available for further academic research.</abstract>
      <pages>397–411</pages>
      <url hash="704b5b39">Q17-1028</url>
      <bibkey>yamada-etal-2017-learning</bibkey>
      <pwccode url="https://github.com/studio-ousia/ntee" additional="false">studio-ousia/ntee</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-2014">STS 2014</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tac-2010">TAC 2010</pwcdataset>
    </paper>
    <paper id="30">
      <title>Evaluating Low-Level Speech Features Against Human Perceptual Data</title>
      <author><first>Caitlin</first><last>Richter</last></author>
      <author><first>Naomi H.</first><last>Feldman</last></author>
      <author><first>Harini</first><last>Salgado</last></author>
      <author><first>Aren</first><last>Jansen</last></author>
      <doi>10.1162/tacl_a_00071</doi>
      <abstract>We introduce a method for measuring the correspondence between low-level speech features and <a href="https://en.wikipedia.org/wiki/Perception">human perception</a>, using a cognitive model of speech perception implemented directly on speech recordings. We evaluate two speaker normalization techniques using this method and find that in both cases, <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">speech features</a> that are normalized across speakers predict human data better than unnormalized speech features, consistent with previous research. Results further reveal differences across <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalization methods</a> in how well each predicts human data. This work provides a new framework for evaluating low-level representations of speech on their match to <a href="https://en.wikipedia.org/wiki/Perception">human perception</a>, and lays the groundwork for creating more ecologically valid models of <a href="https://en.wikipedia.org/wiki/Speech_perception">speech perception</a>.</abstract>
      <pages>425–440</pages>
      <url hash="b26e2d06">Q17-1030</url>
      <bibkey>richter-etal-2017-evaluating</bibkey>
    </paper>
    <paper id="32">
      <title>Unsupervised Acquisition of Comprehensive Multiword Lexicons using Competition in an n-gram Lattice</title>
      <author><first>Julian</first><last>Brooke</last></author>
      <author><first>Jan</first><last>Šnajder</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <doi>10.1162/tacl_a_00073</doi>
      <abstract>We present a new model for acquiring comprehensive multiword lexicons from large corpora based on competition among n-gram candidates. In contrast to the standard approach of simple ranking by association measure, in our model n-grams are arranged in a lattice structure based on subsumption and overlap relationships, with nodes inhibiting other nodes in their vicinity when they are selected as a lexical item. We show how the configuration of such a <a href="https://en.wikipedia.org/wiki/Lattice_(group)">lattice</a> can be optimized tractably, and demonstrate using annotations of sampled n-grams that our method consistently outperforms alternatives by at least 0.05 <a href="https://en.wikipedia.org/wiki/F-score">F-score</a> across several corpora and languages.</abstract>
      <pages>455–470</pages>
      <url hash="f672e046">Q17-1032</url>
      <video href="https://vimeo.com/277673914" />
      <bibkey>brooke-etal-2017-unsupervised</bibkey>
    </paper>
    <paper id="33">
      <title>Replicability Analysis for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a> : Testing Significance with Multiple Datasets</title>
      <author><first>Rotem</first><last>Dror</last></author>
      <author><first>Gili</first><last>Baumer</last></author>
      <author><first>Marina</first><last>Bogomolov</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <doi>10.1162/tacl_a_00074</doi>
      <abstract>With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> and can lead to erroneous conclusions. In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications : multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.</abstract>
      <pages>471–486</pages>
      <url hash="2f7781f7">Q17-1033</url>
      <video href="https://vimeo.com/285803652" />
      <bibkey>dror-etal-2017-replicability</bibkey>
      <pwccode url="https://github.com/rtmdrr/replicability-analysis-NLP" additional="false">rtmdrr/replicability-analysis-NLP</pwccode>
    </paper>
    <paper id="35">
      <title>Joint Prediction of Word Alignment with Alignment Types</title>
      <author><first>Anahita</first><last>Mansouri Bigvand</last></author>
      <author><first>Te</first><last>Bu</last></author>
      <author><first>Anoop</first><last>Sarkar</last></author>
      <doi>10.1162/tacl_a_00076</doi>
      <abstract>Current word alignment models do not distinguish between different types of alignment links. In this paper, we provide a new probabilistic model for <a href="https://en.wikipedia.org/wiki/Word_alignment">word alignment</a> where word alignments are associated with linguistically motivated alignment types. We propose a novel task of joint prediction of word alignment and alignment types and propose novel <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised learning algorithms</a> for this task. We also solve a sub-task of predicting the alignment type given an aligned word pair. In our experimental results, the generative models we introduce to model alignment types significantly outperform the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> without alignment types.</abstract>
      <pages>501–514</pages>
      <url hash="0e6d1ac1">Q17-1035</url>
      <bibkey>mansouri-bigvand-etal-2017-joint</bibkey>
    </paper>
    <paper id="36">
      <title>Aspect-augmented Adversarial Networks for Domain Adaptation</title>
      <author><first>Yuan</first><last>Zhang</last></author>
      <author><first>Regina</first><last>Barzilay</last></author>
      <author><first>Tommi</first><last>Jaakkola</last></author>
      <doi>10.1162/tacl_a_00077</doi>
      <abstract>We introduce a <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural method</a> for <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> between two (source and target) <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification tasks</a> or aspects over the same domain. Rather than training on target labels, we use a few <a href="https://en.wikipedia.org/wiki/Index_term">keywords</a> pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, yielding an improvement of 27 % on a pathology dataset and 5 % on a review dataset.</abstract>
      <pages>515–528</pages>
      <url hash="ee8eae5e">Q17-1036</url>
      <video href="https://vimeo.com/276406923" />
      <bibkey>zhang-etal-2017-aspect</bibkey>
      <pwccode url="https://github.com/yuanzh/aspect_adversarial" additional="false">yuanzh/aspect_adversarial</pwccode>
    </paper>
    <paper id="37">
      <title>Anchored Correlation Explanation : Topic Modeling with Minimal Domain Knowledge</title>
      <author><first>Ryan J.</first><last>Gallagher</last></author>
      <author><first>Kyle</first><last>Reing</last></author>
      <author><first>David</first><last>Kale</last></author>
      <author><first>Greg</first><last>Ver Steeg</last></author>
      <doi>10.1162/tacl_a_00078</doi>
      <abstract>While generative models such as Latent Dirichlet Allocation (LDA) have proven fruitful in <a href="https://en.wikipedia.org/wiki/Topic_modeling">topic modeling</a>, they often require detailed assumptions and careful specification of hyperparameters. Such model complexity issues only compound when trying to generalize <a href="https://en.wikipedia.org/wiki/Generative_model">generative models</a> to incorporate <a href="https://en.wikipedia.org/wiki/Input_(computer_science)">human input</a>. We introduce Correlation Explanation (CorEx), an alternative approach to <a href="https://en.wikipedia.org/wiki/Topic_modeling">topic modeling</a> that does not assume an underlying <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a>, and instead learns maximally informative topics through an information-theoretic framework. This <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> naturally generalizes to hierarchical and semi-supervised extensions with no additional modeling assumptions. In particular, word-level domain knowledge can be flexibly incorporated within CorEx through anchor words, allowing topic separability and <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a> to be promoted with minimal human intervention. Across a variety of datasets, metrics, and experiments, we demonstrate that CorEx produces topics that are comparable in quality to those produced by unsupervised and semi-supervised variants of LDA.</abstract>
      <pages>529–542</pages>
      <url hash="74f2e00e">Q17-1037</url>
      <video href="https://vimeo.com/276403824" />
      <bibkey>gallagher-etal-2017-anchored</bibkey>
      <pwccode url="https://github.com/gregversteeg/corex_topic" additional="false">gregversteeg/corex_topic</pwccode>
    </paper>
  </volume>
</collection>