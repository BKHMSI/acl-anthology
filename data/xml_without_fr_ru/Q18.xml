<?xml version='1.0' encoding='utf-8'?>
<collection id="Q18">
  <volume id="1">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 6</booktitle>
      <editor><last>Lee</last><first>Lillian</first></editor>
      <editor><last>Johnson</last><first>Mark</first></editor>
      <editor><last>Toutanova</last><first>Kristina</first></editor>
      <editor><last>Roark</last><first>Brian</first></editor>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2018</year>
    </meta>
    <frontmatter>
      <bibkey>tacl-2018-transactions</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Whodunnit? Crime Drama as a Case for Natural Language Understanding</title>
      <author><first>Lea</first><last>Frermann</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <doi>10.1162/tacl_a_00001</doi>
      <abstract>In this paper we argue that <a href="https://en.wikipedia.org/wiki/Crime_film">crime drama</a> exemplified in television programs such as CSI : Crime Scene Investigation is an ideal testbed for approximating real-world natural language understanding and the complex inferences associated with it. We propose to treat <a href="https://en.wikipedia.org/wiki/Crime_film">crime drama</a> as a new inference task, capitalizing on the fact that each episode poses the same basic question (i.e., who committed the crime) and naturally provides the answer when the perpetrator is revealed. We develop a new dataset based on CSI episodes, formalize perpetrator identification as a sequence labeling problem, and develop an LSTM-based model which learns from multi-modal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input.</abstract>
      <pages>1–15</pages>
      <url hash="297b5f86">Q18-1001</url>
      <video href="https://vimeo.com/285805531" />
      <bibkey>frermann-etal-2018-whodunnit</bibkey>
      <pwccode url="https://github.com/EdinburghNLP/csi-corpus" additional="false">EdinburghNLP/csi-corpus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/movieqa">MovieQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="4">
      <title>Representation Learning for Grounded Spatial Reasoning</title>
      <author><first>Michael</first><last>Janner</last></author>
      <author><first>Karthik</first><last>Narasimhan</last></author>
      <author><first>Regina</first><last>Barzilay</last></author>
      <doi>10.1162/tacl_a_00004</doi>
      <abstract>The interpretation of spatial references is highly contextual, requiring <a href="https://en.wikipedia.org/wiki/Bayesian_inference">joint inference</a> over both language and the environment. We consider the task of <a href="https://en.wikipedia.org/wiki/Spatial–temporal_reasoning">spatial reasoning</a> in a <a href="https://en.wikipedia.org/wiki/Simulation">simulated environment</a>, where an agent can act and receive rewards. The proposed <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> learns a representation of the world steered by instruction text. This design allows for precise alignment of local neighborhoods with corresponding verbalizations, while also handling global references in the instructions. We train our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> using a variant of generalized value iteration. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms state-of-the-art approaches on several metrics, yielding a 45 % reduction in goal localization error.</abstract>
      <pages>49–61</pages>
      <url hash="f62e10dd">Q18-1004</url>
      <video href="https://vimeo.com/285802158" />
      <bibkey>janner-etal-2018-representation</bibkey>
      <pwccode url="https://github.com/JannerM/spatial-reasoning" additional="false">JannerM/spatial-reasoning</pwccode>
    </paper>
    <paper id="5">
      <title>Learning Structured Text Representations</title>
      <author id="yang-liu-edinburgh"><first>Yang</first><last>Liu</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <doi>10.1162/tacl_a_00005</doi>
      <abstract>In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a>. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016 ; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanisms</a> to incorporate the structural biases. Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.</abstract>
      <pages>63–75</pages>
      <url hash="b4205089">Q18-1005</url>
      <video href="https://vimeo.com/276396538" />
      <bibkey>liu-lapata-2018-learning</bibkey>
      <pwccode url="https://github.com/nlpyang/structured" additional="true">nlpyang/structured</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="7">
      <title>Towards Evaluating Narrative Quality In Student Writing</title>
      <author><first>Swapna</first><last>Somasundaran</last></author>
      <author><first>Michael</first><last>Flor</last></author>
      <author><first>Martin</first><last>Chodorow</last></author>
      <author><first>Hillary</first><last>Molloy</last></author>
      <author><first>Binod</first><last>Gyawali</last></author>
      <author><first>Laura</first><last>McCulla</last></author>
      <doi>10.1162/tacl_a_00007</doi>
      <abstract>This work lays the foundation for automated assessments of narrative quality in student writing. We first manually score <a href="https://en.wikipedia.org/wiki/Essay">essays</a> for narrative-relevant traits and sub-traits, and measure inter-annotator agreement. We then explore linguistic features that are indicative of good narrative writing and use them to build an automated scoring system. Experiments show that our <a href="https://en.wikipedia.org/wiki/Software_feature">features</a> are more effective in scoring specific aspects of <a href="https://en.wikipedia.org/wiki/Narrative">narrative quality</a> than a state-of-the-art <a href="https://en.wikipedia.org/wiki/Software_feature">feature set</a>.</abstract>
      <pages>91–106</pages>
      <url hash="99a14735">Q18-1007</url>
      <video href="https://vimeo.com/276372446" />
      <bibkey>somasundaran-etal-2018-towards</bibkey>
    </paper>
    <paper id="8">
      <title>Evaluating the Stability of Embedding-based Word Similarities</title>
      <author><first>Maria</first><last>Antoniak</last></author>
      <author><first>David</first><last>Mimno</last></author>
      <doi>10.1162/tacl_a_00008</doi>
      <abstract>Word embeddings are increasingly being used as a tool to study <a href="https://en.wikipedia.org/wiki/Word_association">word associations</a> in specific corpora. However, it is unclear whether such embeddings reflect enduring properties of language or if they are sensitive to inconsequential variations in the source documents. We find that nearest-neighbor distances are highly sensitive to small changes in the training corpus for a variety of <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a>. For all <a href="https://en.wikipedia.org/wiki/Methodology">methods</a>, including specific documents in the training set can result in substantial variations. We show that these effects are more prominent for smaller training corpora. We recommend that users never rely on single embedding models for distance calculations, but rather average over multiple bootstrap samples, especially for small corpora.</abstract>
      <pages>107–119</pages>
      <url hash="109dc4fd">Q18-1008</url>
      <video href="https://vimeo.com/277670053" />
      <bibkey>antoniak-mimno-2018-evaluating</bibkey>
    </paper>
    <paper id="10">
      <title>Learning Representations Specialized in Spatial Knowledge : Leveraging Language and Vision</title>
      <author><first>Guillem</first><last>Collell</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <doi>10.1162/tacl_a_00010</doi>
      <abstract>Spatial understanding is crucial in many real-world problems, yet little progress has been made towards building <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> that capture <a href="https://en.wikipedia.org/wiki/Spatial_memory">spatial knowledge</a>. Here, we move one step forward in this direction and learn such <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> by leveraging a task consisting in predicting continuous 2D spatial arrangements of objects given object-relationship-object instances (e.g., cat under chair) and a simple <a href="https://en.wikipedia.org/wiki/Neural_network">neural network model</a> that learns the task from <a href="https://en.wikipedia.org/wiki/Annotation">annotated images</a>. We show that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> succeeds in this task and, furthermore, that it is capable of predicting correct spatial arrangements for unseen objects if either CNN features or <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> of the objects are provided. The differences between visual and linguistic features are discussed. Next, to evaluate the spatial representations learned in the previous <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, we introduce a <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> and a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> consisting in a set of crowdsourced human ratings of spatial similarity for object pairs. We find that both CNN (convolutional neural network) features and word embeddings predict human judgments of similarity well and that these vectors can be further specialized in spatial knowledge if we update them when training the model that predicts spatial arrangements of objects. Overall, this paper paves the way towards building distributed spatial representations, contributing to the understanding of spatial expressions in language.</abstract>
      <pages>133–144</pages>
      <url hash="3c9992cb">Q18-1010</url>
      <bibkey>collell-moens-2018-learning</bibkey>
      <pwccode url="https://github.com/gcollell/spatial-representations" additional="false">gcollell/spatial-representations</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="11">
      <title>Modeling Past and Future for Neural Machine Translation</title>
      <author><first>Zaixiang</first><last>Zheng</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Lili</first><last>Mou</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <doi>10.1162/tacl_a_00011</doi>
      <abstract>Existing neural machine translation systems do not explicitly model what has been translated and what has not during the decoding phase. To address this problem, we propose a novel mechanism that separates the source information into two parts : translated Past contents and untranslated Future contents, which are modeled by two additional recurrent layers. The Past and Future contents are fed to both the attention model and the decoder states, which provides Neural Machine Translation (NMT) systems with the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks. Specifically, the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.</abstract>
      <pages>145–157</pages>
      <url hash="5c301710">Q18-1011</url>
      <bibkey>zheng-etal-2018-modeling</bibkey>
      <pwccode url="https://github.com/zhengzx-nlp/past-and-future-nmt" additional="false">zhengzx-nlp/past-and-future-nmt</pwccode>
    </paper>
    <paper id="12">
      <title>Mapping to Declarative Knowledge for Word Problem Solving</title>
      <author><first>Subhro</first><last>Roy</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <doi>10.1162/tacl_a_00012</doi>
      <abstract>Math word problems form a natural abstraction to a range of quantitative reasoning problems, such as understanding <a href="https://en.wikipedia.org/wiki/Finance">financial news</a>, <a href="https://en.wikipedia.org/wiki/Sports_journalism">sports results</a>, and <a href="https://en.wikipedia.org/wiki/Casualty_(person)">casualties of war</a>. Solving such problems requires the understanding of several mathematical concepts such as <a href="https://en.wikipedia.org/wiki/Dimensional_analysis">dimensional analysis</a>, subset relationships, etc. In this paper, we develop declarative rules which govern the translation of natural language description of these <a href="https://en.wikipedia.org/wiki/Concept">concepts</a> to <a href="https://en.wikipedia.org/wiki/Expression_(mathematics)">math expressions</a>. We then present a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> for incorporating such <a href="https://en.wikipedia.org/wiki/Descriptive_knowledge">declarative knowledge</a> into word problem solving. Our method learns to map arithmetic word problem text to math expressions, by learning to select the relevant declarative knowledge for each operation of the solution expression. This provides a way to handle multiple <a href="https://en.wikipedia.org/wiki/Concept">concepts</a> in the same problem while, at the same time, supporting interpretability of the answer expression. Our method models the mapping to declarative knowledge as a <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a>, thus removing the need for expensive annotations. Experimental evaluation suggests that our domain knowledge based solver outperforms all other systems, and that it generalizes better in the realistic case where the training data it is exposed to is biased in a different way than the test data.</abstract>
      <pages>159–172</pages>
      <url hash="021b28ce">Q18-1012</url>
      <video href="https://vimeo.com/282338901" />
      <bibkey>roy-roth-2018-mapping</bibkey>
      <pwccode url="https://github.com/CogComp/arithmetic" additional="false">CogComp/arithmetic</pwccode>
    </paper>
    <paper id="13">
      <title>Video Captioning with Multi-Faceted Attention</title>
      <author><first>Xiang</first><last>Long</last></author>
      <author><first>Chuang</first><last>Gan</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <doi>10.1162/tacl_a_00013</doi>
      <abstract>Video captioning has attracted an increasing amount of interest, due in part to its potential for improved <a href="https://en.wikipedia.org/wiki/Accessibility">accessibility</a> and <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a>. While existing methods rely on different kinds of <a href="https://en.wikipedia.org/wiki/Visual_system">visual features</a> and model architectures, they do not make full use of pertinent semantic cues. We present a unified and extensible framework to jointly leverage multiple sorts of <a href="https://en.wikipedia.org/wiki/Feature_(computer_vision)">visual features</a> and <a href="https://en.wikipedia.org/wiki/Semantic_Web">semantic attributes</a>. Our novel <a href="https://en.wikipedia.org/wiki/Architecture">architecture</a> builds on LSTMs with two multi-faceted attention layers. These first learn to automatically select the most salient visual features or semantic attributes, and then yield overall representations for the input and output of the sentence generation component via custom feature scaling operations. Experimental results on the challenging MSVD and MSR-VTT datasets show that our framework outperforms previous work and performs robustly even in the presence of added noise to the features and attributes.</abstract>
      <pages>173–184</pages>
      <url hash="e365dc4a">Q18-1013</url>
      <bibkey>long-etal-2018-video</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/msvd">MSVD</pwcdataset>
    </paper>
    <paper id="15">
      <title>Knowledge Completion for Generics using Guided Tensor Factorization</title>
      <author><first>Hanie</first><last>Sedghi</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <doi>10.1162/tacl_a_00015</doi>
      <abstract>Given a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a> or <a href="https://en.wikipedia.org/wiki/Knowledge_base">KB</a> containing (noisy) facts about common nouns or generics, such as all trees produce oxygen or some animals live in forests, we consider the problem of inferring additional such facts at a precision similar to that of the starting KB. Such KBs capture general knowledge about the world, and are crucial for various <a href="https://en.wikipedia.org/wiki/Application_software">applications</a> such as <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>. Different from commonly studied named entity KBs such as <a href="https://en.wikipedia.org/wiki/Freebase">Freebase</a>, generics KBs involve quantification, have more complex underlying regularities, tend to be more incomplete, and violate the commonly used locally closed world assumption (LCWA). We show that existing KB completion methods struggle with this new <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, and present the first approach that is successful. Our results demonstrate that external information, such as relation schemas and <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity taxonomies</a>, if used appropriately, can be a surprisingly powerful tool in this setting. First, our simple yet effective knowledge guided tensor factorization approach achieves state-of-the-art results on two generics KBs (80 % precise) for science, doubling their size at 74%86 % precision. Second, our novel taxonomy guided, submodular, active learning method for collecting annotations about rare entities (e.g., oriole, a bird) is 6x more effective at inferring further new facts about them than multiple active learning baselines.</abstract>
      <pages>197–210</pages>
      <url hash="855b68bb">Q18-1015</url>
      <video href="https://vimeo.com/276898240" />
      <bibkey>sedghi-sabharwal-2018-knowledge</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k">FB15k</pwcdataset>
    </paper>
    <paper id="16">
      <title>Unsupervised Grammar Induction with Depth-bounded PCFG<fixed-case>PCFG</fixed-case></title>
      <author><first>Lifeng</first><last>Jin</last></author>
      <author><first>Finale</first><last>Doshi-Velez</last></author>
      <author><first>Timothy</first><last>Miller</last></author>
      <author><first>William</first><last>Schuler</last></author>
      <author><first>Lane</first><last>Schwartz</last></author>
      <doi>10.1162/tacl_a_00016</doi>
      <abstract>There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011 ; Noji and Johnson, 2016 ; Shain et al., 2016). This work extends this depth-bounding approach to probabilistic context-free grammar induction (DB-PCFG), which has a smaller parameter space than hierarchical sequence models, and therefore more fully exploits the space reductions of depth-bounding. Results for this model on <a href="https://en.wikipedia.org/wiki/Grammar">grammar acquisition</a> from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy. Moreover, <a href="https://en.wikipedia.org/wiki/Grammar">grammars</a> acquired from this <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition models.</abstract>
      <pages>211–224</pages>
      <url hash="9fe0a145">Q18-1016</url>
      <video href="https://vimeo.com/277673890" />
      <bibkey>jin-etal-2018-unsupervised</bibkey>
      <pwccode url="https://github.com/lifengjin/db-pcfg" additional="false">lifengjin/db-pcfg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="17">
      <title>Scheduled Multi-Task Learning : From Syntax to Translation</title>
      <author><first>Eliyahu</first><last>Kiperwasser</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <doi>10.1162/tacl_a_00017</doi>
      <abstract>Neural encoder-decoder models of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> have achieved impressive results, while learning linguistic knowledge of both the source and target languages in an implicit end-to-end manner. We propose a framework in which our model begins learning <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> and translation interleaved, gradually putting more focus on <a href="https://en.wikipedia.org/wiki/Translation">translation</a>. Using this approach, we achieve considerable improvements in terms of BLEU score on relatively large parallel corpus (WMT14 English to German) and a low-resource (WIT German to English) setup.</abstract>
      <pages>225–240</pages>
      <url hash="b10c863b">Q18-1017</url>
      <bibkey>kiperwasser-ballesteros-2018-scheduled</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="19">
      <title>Do latent tree learning models identify meaningful structure in sentences?</title>
      <author><first>Adina</first><last>Williams</last></author>
      <author><first>Andrew</first><last>Drozdov</last></author>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <doi>10.1162/tacl_a_00019</doi>
      <abstract>Recent work on the problem of latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting <a href="https://en.wikipedia.org/wiki/Parsing">parse</a> to interpret the sentence, all without exposure to ground-truth parse trees at training time. Surprisingly, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> often perform better at sentence understanding tasks than <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that use parse trees from conventional <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a>. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification, (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.</abstract>
      <pages>253–267</pages>
      <url hash="06cb8e8f">Q18-1019</url>
      <video href="https://vimeo.com/277673973" />
      <bibkey>williams-etal-2018-latent</bibkey>
      <pwccode url="https://github.com/NYU-MLL/spinn" additional="false">NYU-MLL/spinn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="20">
      <title>Bootstrap Domain-Specific Sentiment Classifiers from Unlabeled Corpora</title>
      <author><first>Andrius</first><last>Mudinas</last></author>
      <author><first>Dell</first><last>Zhang</last></author>
      <author><first>Mark</first><last>Levene</last></author>
      <doi>10.1162/tacl_a_00020</doi>
      <abstract>There is often the need to perform sentiment classification in a particular domain where no labeled document is available. Although we could make use of a general-purpose off-the-shelf sentiment classifier or a pre-built one for a different domain, the effectiveness would be inferior. In this paper, we explore the possibility of building domain-specific sentiment classifiers with unlabeled documents only. Our investigation indicates that in the word embeddings learned from the unlabeled corpus of a given domain, the distributed word representations (vectors) for opposite sentiments form distinct clusters, though those clusters are not transferable across domains. Exploiting such a clustering structure, we are able to utilize <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning algorithms</a> to induce a quality domain-specific sentiment lexicon from just a few typical sentiment words (seeds). An important finding is that simple linear model based supervised learning algorithms (such as linear SVM) can actually work better than more sophisticated semi-supervised / transductive learning algorithms which represent the state-of-the-art technique for sentiment lexicon induction. The induced lexicon could be applied directly in a lexicon-based method for sentiment classification, but a higher performance could be achieved through a two-phase bootstrapping method which uses the induced lexicon to assign positive / negative sentiment scores to unlabeled documents first, a nd t hen u ses those documents found to have clear sentiment signals as pseudo-labeled examples to train a document sentiment classifier v ia supervised learning algorithms (such as LSTM).</abstract>
      <pages>269–285</pages>
      <url hash="ef8bb85c">Q18-1020</url>
      <bibkey>mudinas-etal-2018-bootstrap</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="22">
      <title>Leveraging Orthographic Similarity for Multilingual Neural Transliteration</title>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Mitesh</first><last>Khapra</last></author>
      <author><first>Gurneet</first><last>Singh</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <doi>10.1162/tacl_a_00022</doi>
      <abstract>We address the task of joint training of transliteration models for multiple language pairs (multilingual transliteration). This is an instance of <a href="https://en.wikipedia.org/wiki/Multitask_learning">multitask learning</a>, where individual tasks (language pairs) benefit from sharing knowledge with related tasks. We focus on <a href="https://en.wikipedia.org/wiki/Transliteration">transliteration</a> involving related <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> i.e., <a href="https://en.wikipedia.org/wiki/Language_family">languages sharing writing systems</a> and <a href="https://en.wikipedia.org/wiki/Phoneme">phonetic properties</a> (orthographically similar languages). We propose a modified neural encoder-decoder model that maximizes parameter sharing across language pairs in order to effectively leverage orthographic similarity. We show that multilingual transliteration significantly outperforms bilingual transliteration in different scenarios (average increase of 58 % across a variety of languages we experimented with). We also show that multilingual transliteration models can generalize well to languages / language pairs not encountered during training and hence perform well on the zeroshot transliteration task. We show that further improvements can be achieved by using phonetic feature input.</abstract>
      <pages>303–316</pages>
      <url hash="09cbdbd7">Q18-1022</url>
      <bibkey>kunchukuttan-etal-2018-leveraging</bibkey>
    </paper>
    <paper id="23">
      <title>The NarrativeQA Reading Comprehension Challenge<fixed-case>N</fixed-case>arrative<fixed-case>QA</fixed-case> Reading Comprehension Challenge</title>
      <author><first>Tomáš</first><last>Kočiský</last></author>
      <author><first>Jonathan</first><last>Schwarz</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <author><first>Karl Moritz</first><last>Hermann</last></author>
      <author><first>Gábor</first><last>Melis</last></author>
      <author><first>Edward</first><last>Grefenstette</last></author>
      <doi>10.1162/tacl_a_00023</doi>
      <abstract>Reading comprehension (RC)in contrast to information retrievalrequires integrating information and reasoning about events, <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entities</a>, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial agents</a> and children learning to read. However, existing RC datasets and <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency) ; they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and set of <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> in which the reader must answer questions about stories by reading entire books or <a href="https://en.wikipedia.org/wiki/Screenplay">movie scripts</a>. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or <a href="https://en.wikipedia.org/wiki/Salience_(neuroscience)">salience</a>. We show that although humans solve the <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> easily, standard RC models struggle on the <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> presented here. We provide an analysis of the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and the challenges it presents.</abstract>
      <pages>317–328</pages>
      <url hash="62868a98">Q18-1023</url>
      <video href="https://vimeo.com/285804931" />
      <bibkey>kocisky-etal-2018-narrativeqa</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/booktest">BookTest</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cbt">CBT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mctest">MCTest</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
    <title_ar>تحدي القراءة والفهم السردي</title_ar>
      <title_pt>O desafio de compreensão de leitura NarrativeQA</title_pt>
      <title_es>El desafío de comprensión lectora de NarrativeQA</title_es>
      <title_ja>ナラティブQA読解力の課題</title_ja>
      <title_hi>NarrativeQA पठन समझ चुनौती</title_hi>
      <title_zh>叙事问答读解</title_zh>
      <title_ga>Dúshlán na Léitheoireachta NarrativeQA</title_ga>
      <title_ka>ნაპრატიური QA კითხვის კომპრეცენციის გარეშე</title_ka>
      <title_hu>A NarrativeQA olvasási megértési kihívás</title_hu>
      <title_el>Η πρόκληση κατανόησης της αφήγησηQA</title_el>
      <title_it>La sfida narrativaQA Reading Comprehension Challenge</title_it>
      <title_kk>QA Reading Comprehension Challenge</title_kk>
      <title_lt>QA Reading Comprehension Challenge</title_lt>
      <title_mk>QA Reading Comprehension Challenge</title_mk>
      <title_ms>QA Reading Comprehension Challenge</title_ms>
      <title_ml>QA വായിക്കുന്നത് Comprehension Challenge</title_ml>
      <title_mt>L-Isfida ta’ Komprensjoni Narrattiva tal-QA fil-Qara</title_mt>
      <title_mn>QA унших шаардлагатай шаардлага</title_mn>
      <title_no>NarrativQA Reading Comprehension Challenge</title_no>
      <title_pl>NarrativeQA Reading Comprehension Challenge</title_pl>
      <title_sr>Narrative QA Comprehension Challenge</title_sr>
      <title_ro>The NarrativeQA Reading Comprehension Challenge</title_ro>
      <title_si>QA Reading Compren challenge</title_si>
      <title_so>QA Reading Completion Challenge</title_so>
      <title_sv>BerättelseQA Reading Comprehension Challenge</title_sv>
      <title_ta>QA படித்தல் முடிவு சவால்</title_ta>
      <title_ur>The NarrativeQA Reading Comprehension Challenge</title_ur>
      <title_uz>QA oĘ»quvchi kompyuterName</title_uz>
      <title_vi>Câu hỏi hoàn toàn của giới hạn.</title_vi>
      <title_nl>De NarrativeQA Reading Comprehension Challenge</title_nl>
      <title_bg>Предизвикателството за разбиране на четенето на QA</title_bg>
      <title_da>The NarrativeQA Reading Comprehension Challenge</title_da>
      <title_ko>서사적 독해 도전</title_ko>
      <title_de>Die Herausforderung zum Verständnis von NarrativeQA beim Lesen</title_de>
      <title_hr>Izazov za pročitanje NarrativeQA</title_hr>
      <title_id>The NarrativeQA Reading Comprehension Challenge</title_id>
      <title_fa>تأثیر تأثیر خواندن QA</title_fa>
      <title_sw>Mgogoro wa Msomaji wa Kufungua</title_sw>
      <title_sq>QA Reading Comprehension Challenge</title_sq>
      <title_am>QA Reading Comprehension Challenge</title_am>
      <title_hy>The NarrativeQA Reading Comprehension Challenge</title_hy>
      <title_af>QA Reading Comprehension Challenge</title_af>
      <title_bs>Narrative QA Comprehension Challenge</title_bs>
      <title_bn>QA পাঠক সম্পূর্ণ চ্যালেঞ্জ</title_bn>
      <title_ca>El desafiament de la comprensió narrativaQA Reading</title_ca>
      <title_et>JutustusQA Reading Comprehension Challenge</title_et>
      <title_cs>VyprávěníQA Reading Comprehension Challenge</title_cs>
      <title_fi>KertomuksetQA Reading Comprehension Challenge</title_fi>
      <title_az>NarrativeQA Reading Comprehension Challenge</title_az>
      <title_tr>QA Reading Comprehension Challenge</title_tr>
      <title_jv>Validity</title_jv>
      <title_he>אתגר ההבנה בקריאה QA</title_he>
      <title_ha>QA reading Comprehension Challenge</title_ha>
      <title_sk>Izziv razumevanja branja zgodbeQA</title_sk>
      <title_bo>NarrativeQA Reading Comprehension Challenge</title_bo>
      <abstract_ar>يتطلب فهم القراءة (RC) - على عكس استرجاع المعلومات - تكامل المعلومات والتفكير حول الأحداث والكيانات وعلاقاتهم عبر مستند كامل. تُستخدم الإجابة على الأسئلة بشكل تقليدي لتقييم قدرة RC ، في كل من العوامل الاصطناعية وتعلم الأطفال القراءة. ومع ذلك ، فإن مجموعات بيانات ومهام المنسقين المقيمين تهيمن عليها الأسئلة التي يمكن حلها عن طريق اختيار الإجابات باستخدام المعلومات السطحية (على سبيل المثال ، تشابه السياق المحلي أو تكرار المصطلح العالمي) ؛ وبالتالي فشلوا في اختبار الجانب التكاملي الأساسي لـ RC. لتشجيع التقدم في فهم أعمق للغة ، نقدم مجموعة بيانات جديدة ومجموعة من المهام التي يجب على القارئ من خلالها الإجابة عن أسئلة حول القصص من خلال قراءة كتب كاملة أو نصوص أفلام. تم تصميم هذه المهام بحيث تتطلب الإجابة الناجحة على أسئلتهم فهم السرد الأساسي بدلاً من الاعتماد على مطابقة النمط الضحل أو البروز. نوضح أنه على الرغم من أن البشر يحلون المهام بسهولة ، إلا أن نماذج RC القياسية تكافح في المهام المعروضة هنا. نقدم تحليلاً لمجموعة البيانات والتحديات التي تطرحها.</abstract_ar>
      <abstract_es>La comprensión lectora (RC), a diferencia de la recuperación de información, requiere integrar la información y el razonamiento sobre eventos, entidades y sus relaciones en un documento completo. La respuesta a preguntas se usa convencionalmente para evaluar la capacidad de RC, tanto en agentes artificiales como en niños que aprenden a leer. Sin embargo, los conjuntos de datos y tareas de RC existentes están dominados por cuestiones que pueden resolverse mediante la selección de respuestas utilizando información superficial (por ejemplo, similitud de contexto local o frecuencia de término global); por lo tanto, no prueban el aspecto integrador esencial de RC. Para fomentar el progreso en una comprensión más profunda del lenguaje, presentamos un nuevo conjunto de datos y un conjunto de tareas en las que el lector debe responder preguntas sobre historias leyendo libros completos o guiones de películas. Estas tareas están diseñadas para que responder con éxito a sus preguntas requiera comprender la narrativa subyacente en lugar de confiar en la coincidencia de patrones superficiales o la prominencia. Demostramos que, aunque los humanos resuelven las tareas fácilmente, los modelos RC estándar tienen dificultades en las tareas que se presentan aquí. Proporcionamos un análisis del conjunto de datos y los desafíos que presenta.</abstract_es>
      <abstract_pt>A compreensão de leitura (RC) – em contraste com a recuperação de informações – requer a integração de informações e raciocínio sobre eventos, entidades e suas relações em um documento completo. A resposta a perguntas é convencionalmente usada para avaliar a habilidade de CR, tanto em agentes artificiais quanto em crianças que estão aprendendo a ler. No entanto, conjuntos de dados e tarefas de RC existentes são dominados por questões que podem ser resolvidas selecionando respostas usando informações superficiais (por exemplo, semelhança de contexto local ou frequência global de termos); eles, portanto, falham em testar o aspecto integrador essencial da CR. Para incentivar o progresso na compreensão mais profunda da linguagem, apresentamos um novo conjunto de dados e um conjunto de tarefas em que o leitor deve responder a perguntas sobre histórias lendo livros inteiros ou roteiros de filmes. Essas tarefas são projetadas para que responder com sucesso às suas perguntas exija a compreensão da narrativa subjacente, em vez de confiar na correspondência de padrões superficiais ou na saliência. Mostramos que, embora os humanos resolvam as tarefas com facilidade, os modelos RC padrão lutam nas tarefas apresentadas aqui. Fornecemos uma análise do conjunto de dados e os desafios que ele apresenta.</abstract_pt>
      <abstract_ja>理解（ RC ）の読み取り-完全な文書全体にわたってイベント、エンティティ、およびそれらの関係に関する情報と推論を統合する情報検索要件とは対照的です。 質問への回答は、人工的な薬剤と子供の読書学習の両方で、RC能力を評価するために慣習的に使用されています。 しかし、既存のRCデータセットとタスクは、表面的な情報（例えば、ローカルコンテキストの類似性やグローバルな用語の頻度）を使用して回答を選択することによって解決できる質問に支配されているため、RCの本質的な統合的側面をテストすることはできません。 言語のより深い理解を促進するために、読者は本や映画の脚本全体を読んでストーリーに関する質問に答えなければならない新しいデータセットとタスクのセットを提示します。 これらのタスクは、質問にうまく答えるために、浅いパターンマッチングや顕著さに頼るのではなく、基礎となる物語を理解する必要があるように設計されています。 人間は簡単にタスクを解決できるが、標準的なRCモデルはここで提示されたタスクに苦労することを示している。 データセットとそれが提示する課題の分析を提供します。</abstract_ja>
      <abstract_zh>与信息检索相反,读解(RC)须将事件,实体与信息推理集成一文档中。 问答常以估人工摄童子所习RC能。 然见RC数集,要在用外(如局上下文相似性全局术语频率)择对案以决之。 故其未试RC大略综之。 劝深解言语,发一新集,读者必读整本书电影剧本答故事。 设使成功答问,须解潜述,非依浅层式配显著性。 吾言人可以轻事,而RC形于此者甚矣。 我供数据集及挑战之说。</abstract_zh>
      <abstract_hi>रीडिंग कॉम्प्रिहेंशन (आरसी) - सूचना पुनर्प्राप्ति के विपरीत - एक पूर्ण दस्तावेज़ में घटनाओं, संस्थाओं और उनके संबंधों के बारे में जानकारी और तर्क को एकीकृत करने की आवश्यकता होती है। प्रश्न उत्तर का उपयोग पारंपरिक रूप से आरसी क्षमता का आकलन करने के लिए किया जाता है, दोनों कृत्रिम एजेंटों और बच्चों को पढ़ने के लिए सीखने में। हालांकि, मौजूदा आरसी डेटासेट और कार्यों में उन प्रश्नों का प्रभुत्व है जिन्हें सतही जानकारी (जैसे, स्थानीय संदर्भ समानता या वैश्विक शब्द आवृत्ति) का उपयोग करके उत्तरों का चयन करके हल किया जा सकता है; वे इस प्रकार आर सी के आवश्यक एकीकृत पहलू के लिए परीक्षण करने में विफल रहते हैं। भाषा की गहरी समझ पर प्रगति को प्रोत्साहित करने के लिए, हम एक नया डेटासेट और कार्यों का सेट प्रस्तुत करते हैं जिसमें पाठक को पूरी किताबें या फिल्म स्क्रिप्ट पढ़कर कहानियों के बारे में सवालों के जवाब देने चाहिए। इन कार्यों को डिज़ाइन किया गया है ताकि उनके सवालों का सफलतापूर्वक जवाब देने के लिए उथले पैटर्न मिलान या लचीलेपन पर भरोसा करने के बजाय अंतर्निहित कथा को समझने की आवश्यकता हो। हम दिखाते हैं कि यद्यपि मनुष्य कार्यों को आसानी से हल करते हैं, मानक आरसी मॉडल यहां प्रस्तुत कार्यों पर संघर्ष करते हैं। हम डेटासेट और इसके द्वारा प्रस्तुत चुनौतियों का विश्लेषण प्रदान करते हैं।</abstract_hi>
      <abstract_ga>Éilíonn Léamhthuiscint (RC) - i gcodarsnacht le haisghabháil faisnéise - faisnéis agus réasúnaíocht faoi imeachtaí, aonáin, agus a gcaidreamh a chomhtháthú trasna doiciméad iomlán. Go hiondúil úsáidtear freagra ceisteanna chun cumas RC a mheas, i ngníomhairí saorga agus i leanaí atá ag foghlaim na léitheoireachta. Mar sin féin, tá na tacair sonraí agus na dtascanna RC atá ann cheana féin faoi cheannas na gceisteanna is féidir a réiteach trí fhreagraí a roghnú ag úsáid faisnéis fhorleathan (m.sh. cosúlacht sa chomhthéacs áitiúil nó minicíocht téarma domhanda); dá bhrí sin ní dhéanann siad tástáil le haghaidh gné riachtanach lánpháirtithe RC. Chun dul chun cinn ar thuiscint níos doimhne ar theanga a spreagadh, cuirimid tacar sonraí nua agus sraith tascanna i láthair ina gcaithfidh an léitheoir ceisteanna faoi scéalta a fhreagairt trí leabhair iomlána nó scripteanna scannáin a léamh. Tá na tascanna seo deartha sa chaoi is go n-éilíonn freagairt rathúil a gcuid ceisteanna tuiscint a fháil ar an scéal bunúsach seachas a bheith ag brath ar mheaitseáil nó ar shuntas patrún éadomhain. Léiríonn muid, cé go réitíonn daoine na tascanna go héasca, go mbíonn samhlacha caighdeánacha RC ag streachailt ar na tascanna a chuirtear i láthair anseo. Cuirimid anailís ar fáil ar an tacar sonraí agus ar na dúshláin a chuireann sé i láthair.</abstract_ga>
      <abstract_el>Η κατανόηση ανάγνωσης (σε αντίθεση με την ανάκτηση πληροφοριών) απαιτεί την ενσωμάτωση πληροφοριών και συλλογισμού σχετικά με γεγονότα, οντότητες και τις σχέσεις τους σε ένα πλήρες έγγραφο. Η απάντηση στις ερωτήσεις χρησιμοποιείται συμβατικά για να αξιολογήσει την ικανότητα RC, τόσο σε τεχνητούς παράγοντες όσο και σε παιδιά που μαθαίνουν να διαβάζουν. Ωστόσο, τα υπάρχοντα σύνολα δεδομένων και τα καθήκοντα RC κυριαρχούν από ερωτήματα που μπορούν να επιλυθούν επιλέγοντας απαντήσεις χρησιμοποιώντας επιφανειακές πληροφορίες (π.χ. ομοιότητα τοπικού πλαισίου ή συχνότητα παγκόσμιων όρων). Συνεπώς, δεν μπορούν να ελέγξουν την ουσιαστική ενοποιητική πτυχή της RC. Για να ενθαρρύνουμε την πρόοδο στην βαθύτερη κατανόηση της γλώσσας, παρουσιάζουμε ένα νέο σύνολο δεδομένων και ένα σύνολο εργασιών στα οποία ο αναγνώστης πρέπει να απαντήσει σε ερωτήσεις σχετικά με ιστορίες διαβάζοντας ολόκληρα βιβλία ή σενάρια ταινιών. Αυτές οι εργασίες είναι σχεδιασμένες έτσι ώστε η επιτυχής απάντηση στις ερωτήσεις τους απαιτεί κατανόηση της υποκείμενης αφήγησης αντί να βασίζεται σε ρηχή αντιστοίχιση μοτίβων ή ευδιάκριτη εμφάνιση. Δείχνουμε ότι αν και οι άνθρωποι λύνουν εύκολα τα καθήκοντα, τα τυποποιημένα μοντέλα δυσκολεύονται στις εργασίες που παρουσιάζονται εδώ. Παρέχουμε μια ανάλυση του συνόλου δεδομένων και των προκλήσεων που παρουσιάζει.</abstract_el>
      <abstract_hu>Az olvasási megértés (RC) – ellentétben az információvisszakereséssel – szükségessé teszi az eseményekkel, entitásokkal és azok kapcsolataival kapcsolatos információk és érvelések integrálását egy teljes dokumentumban. A kérdések megválaszolását hagyományosan használják az RC képességek felmérésére, mind a mesterséges anyagok, mind az olvasást tanuló gyermekek esetében. A meglévő RC-adatkészleteket és feladatokat azonban olyan kérdések uralják, amelyeket felületes információk (pl. helyi kontextushasonlóság vagy globális kifejezések gyakorisága) segítségével lehet megoldani; így nem tesztelik az RC alapvető integrációs aspektusát. A nyelv mélyebb megértésének előmozdítása érdekében új adatkészletet és feladatokat mutatunk be, amelyekben az olvasónak teljes könyvek vagy filmforgatókönyvek olvasásával kell megválaszolnia a történetekkel kapcsolatos kérdéseket. Ezeket a feladatokat úgy tervezték meg, hogy a kérdéseik sikeres megválaszolása inkább a mögöttes elbeszélés megértéséhez szükséges, mintsem a sekély mintaegyezésre vagy kiemelésre támaszkodni. Megmutatjuk, hogy bár az emberek könnyen megoldják a feladatokat, a standard RC modellek küzdenek az itt bemutatott feladatokkal. Elemzést nyújtunk az adatkészletről és az általa jelentett kihívásokról.</abstract_hu>
      <abstract_ka>ინფორმაციის მიღებაზე კონტრასტურად ინფორმაციის ინტერგურაცია და პარამენტის შესახებ მოვლენები, ინტერციები და მათი შესახებ სამყარო დოკუმენტის შესახებ უნდა ინტერგ პროგრამის შესაძლებლობად კითხვის პასუხი კითხვის შესაძლებლობაში გამოყენება RC შესაძლებლობაში, ორივე მსგავსი ადვნენტებში და ბავშვებში, რომლებიც კითხვის მაგრამ არსებობს RC მონაცემები და დავალებები დომინტრებულია კითხვებით, რომლებიც შეუძლიათ გადაწყენოთ პასუხების გამოყენებით გამოიყენებით საფუძველი ინფორმაციის გამოყენებით (მა ისინი არ შეუძლებელია RC-ის მნიშვნელოვანი ინტერგრაციური ადექტის ტესტის შესახებ. რომ უფრო დიდი სიტყვის გაგრძნობის პროგრესის შესაძლებლობად, ჩვენ ახალი მონაცემების კონფიგურაციას და რაოდენობის რაოდენობების შესაძლებლობა, რომელსაც კითხველი უნდა გაუკეთოთ ეს დავალებები განაზღვრებულია, რადგან წარმატებით მისი კითხვების შესაძლებლობა უნდა გავიგოთ ქვეყნებული ისტორია, მაგრამ უფრო დარწმუნდეთ საშუალოდ სტრუქტ ჩვენ ჩვენ აჩვენებთ, რომ თუმცა ადამიანები ადამიანის დავალების ადამიანი გაუკეთება, სტანდარტური RC მოდელები აქ ჩვენებული დავალების შემდეგ ბრძნობა. ჩვენ მონაცემების ანალიზაციას და გამოცემების გამოყენება.</abstract_ka>
      <abstract_it>La comprensione della lettura (RC) – a differenza del recupero delle informazioni – richiede l'integrazione di informazioni e ragionamenti su eventi, entità e loro relazioni attraverso un documento completo. La risposta alle domande è convenzionalmente utilizzata per valutare la capacità RC, sia negli agenti artificiali che nei bambini che imparano a leggere. Tuttavia, i set di dati e le attività RC esistenti sono dominati da domande che possono essere risolte selezionando le risposte utilizzando informazioni superficiali (ad esempio, somiglianza del contesto locale o frequenza globale dei termini); non riescono quindi a testare l'aspetto integrativo essenziale della RC. Per incoraggiare i progressi nella comprensione più profonda del linguaggio, presentiamo un nuovo set di dati e una serie di compiti in cui il lettore deve rispondere alle domande sulle storie leggendo interi libri o sceneggiature cinematografiche. Questi compiti sono progettati in modo che rispondere con successo alle loro domande richiede la comprensione della narrativa sottostante piuttosto che affidarsi ad un pattern matching superficiale o salience. Mostriamo che sebbene gli esseri umani risolvano facilmente i compiti, i modelli RC standard lottano sui compiti presentati qui. Forniamo un'analisi del set di dati e delle sfide che esso presenta.</abstract_it>
      <abstract_kk>Мәліметті алу арқылы (RC) түсініктерді оқу үшін - мәліметті алу арқылы - оқиғалар, нысандар және олардың қатынастарын толық құжатта біріктіру және түсініктерін бірікт Сұрақ жауап беру әдетте RC мүмкіндігін оқу үшін, әртүрлі агенттер мен балалар оқу үшін қолданылады. Бірақ, бар RC деректер қорлары мен тапсырмалар жауаптарды таңдап, жауаптарды таңдай алатын сұрақтар (мысалы, жергілікті контекстің ұқсастығы не жалпы терминнің жиілігі); Олар РК-нің негізгі интеграциялық аспектін тексеруге болмайды. Тілдердің түсініктерін жаңа деректер жинағын және оқушы оқиғаларды оқуға не фильм скрипттерді оқу үшін оқиғалар туралы сұрақтарды жауап беру керек. Бұл тапсырмалар құрылған, сондықтан олардың сұрақтарына сәтті жауап беру керек, олардың түсініктерін түсінуге қажет болады. Бұл үлгі сәйкестіктеріне сәйкес немесе Біз адамдар тапсырмаларды оңай шешуге қарағанда, мұнда көрсетілген тапсырмалардың стандартты RC үлгілері күреседі. Біз деректер қорларының анализациясын және оның көрсетілетін мәселелерін береміз.</abstract_kk>
      <abstract_lt>Skaitymo supratimas (RC), priešingai nei informacijos gavimas, reikalauja, kad informacija ir argumentai apie įvykius, subjektus ir jų santykius būtų įtraukti į visą dokumentą. Klausimų atsakymas paprastai naudojamas vertinant RC gebėjimą tiek dirbtiniams veiksniams, tiek skaityti mokantiems vaikams. Tačiau esamiems RK duomenų rinkiniams ir užduotims dominuoja klausimai, kuriuos galima išspręsti pasirinkus atsakymus naudojant paviršinę informaciją (pvz., vietos konteksto panašumą arba pasaulinį terminų dažnį); taigi jie neišbando esminio integracinio RC aspekto. Siekiant skatinti pažangą giliau suprantant kalbą, pristatome naują duomenų rinkinį ir užduočių rinkinį, kuriame skaitytojas turi atsakyti į klausimus apie istorijas skaitydamas visas knygas ar filmų scenarijus. Šios užduotys suprojektuotos taip, kad sėkmingai atsakant į jų klausimus būtų būtina suprasti pagrindinę narraciją, o ne pasikliauti plokščiu modelio sutampamumu ar protingumu. Mes rodome, kad nors žmonės lengvai išspręsia užduotis, standartiniai RC modeliai kovoja su šioje srityje pateiktomis užduotimis. Pateikiame duomenų rinkinio ir jos keliamų iššūkių analizę.</abstract_lt>
      <abstract_mk>Читањето на разбирањето (РЦ) – во спротивност на добивањето информации – бара интеграција на информациите и размислувањето за настаните, ентитетите и нивните односи низ целиот документ. Одговорите на прашањата се конвенционално користат за проценка на способноста на РЦ, и во вештачките агенти, и во децата кои учат да читаат. Сепак, постоечките групи податоци и задачи на РЦ се доминирани од прашања кои може да се решат со избор на одговори користејќи површни информации (на пример, локална сличност на контекст или глобална фреквенција на терминот); тие не успеваат да го тестираат основниот интегративен аспект на РЦ. За да го охрабриме напредокот во подлабоко разбирање на јазикот, претставуваме нов набор на податоци и набор на задачи во кои читателот мора да одговори на прашања за приказните со читање на цели книги или филмски скрипти. Овие задачи се дизајнирани за успешно одговорување на нивните прашања да бара разбирање на основната приказна наместо да се потпираат на ниско споредување на шеми или ослободување. Ние покажуваме дека иако луѓето лесно ги решаваат задачите, стандардните RC модели се борат со задачите претставени овде. Ние обезбедуваме анализа на податоците и предизвиците кои ги претставува.</abstract_mk>
      <abstract_ms>Pembacaan pemahaman (RC)—berbeza dengan pemulihan maklumat—memerlukan penyelesaian maklumat dan alasan mengenai peristiwa, entiti, dan hubungan mereka di seluruh dokumen penuh. Jawapan soalan digunakan secara konvensional untuk menilai kemampuan RC, dalam agen buatan dan kanak-kanak belajar membaca. Namun, set data dan tugas RC yang wujud didominasi oleh soalan yang boleh diselesaikan dengan memilih jawapan menggunakan maklumat permukaan (cth., persamaan konteks setempat atau frekuensi terma global); mereka gagal menguji aspek integratif penting RC. Untuk mendorong kemajuan dalam pemahaman bahasa yang lebih dalam, kami memperkenalkan set data baru dan set tugas di mana pembaca mesti menjawab soalan tentang cerita dengan membaca seluruh buku atau skrip filem. Tugas-tugas ini dirancang supaya menjawab soalan mereka dengan berjaya memerlukan pemahaman cerita yang didasarkan daripada bergantung pada persamaan corak rendah atau kelebihan. Kami menunjukkan bahawa walaupun manusia menyelesaikan tugas dengan mudah, model RC piawai berjuang pada tugas yang dihadapkan di sini. Kami memberikan analisis set data dan cabaran yang ia perkenalkan.</abstract_ms>
      <abstract_ml>Reading comprehension (RC)—in contrast to information retrieval—requires integrating information and reasoning about events, entities, and their relations across a full document.  ചോദ്യത്തിന്റെ ഉത്തരമെടുക്കുന്നത് RC ശക്തിയെയും വായിക്കാന്‍ പഠിക്കുന്ന ഏജന്റുകളെയും കുട്ടികളെയും പരിഗണ എന്നാലും നിലവിലുള്ള RC ഡാറ്റാസറ്റുകളും ജോലികളും പരിഹരിക്കുന്ന ചോദ്യങ്ങളാല്‍ ഉത്തരം തെരഞ്ഞെടുക്കാന്‍ സാധിക്കുന്നു അതുകൊണ്ട് ആര്‍സിയുടെ പ്രധാനപ്പെട്ട ഒരുമിച്ച ഭാഗങ്ങള്‍ക്ക് പരീക്ഷിക്കാന്‍ അവര്‍ക്ക് പരാജയപ്പെ ഭാഷയുടെ ആഴത്തിലുള്ള പ്രവർത്തനങ്ങളെക്കുറിച്ച് പ്രേരിപ്പിക്കാനാണ് ഞങ്ങൾ ഒരു പുതിയ ഡാറ്റാസേറ്റ് കൊണ്ടുവരുന്നത്. ഒരു ജോലികളെ ഈ ജോലികള്‍ നിര്‍മ്മിക്കപ്പെട്ടിരിക്കുന്നു. അതുകൊണ്ട് അവരുടെ ചോദ്യങ്ങള്‍ക്ക് വിജയകരമായി ഉത്തരം നല്‍കുന്നതിനാല്‍ അടിസ്ഥാ മനുഷ്യര്‍ ജോലികള്‍ എളുപ്പമായി പരിഹരിക്കുന്നുവെങ്കിലും, ഇവിടെ കൊണ്ടുവരുന്ന ജോലികളില്‍ സാധാരണ RC മോഡലുകള്‍ പോ നമ്മള്‍ ഡാറ്റാസറ്റിന്റെ അന്വേഷണം കൊടുക്കുന്നു. അതിന്റെ സങ്കേതികളും.</abstract_ml>
      <abstract_mt>Il-fehim tal-qari (RC)—b’kuntrast mal-ġbir tal-informazzjoni—jeħtieġ l-integrazzjoni tal-informazzjoni u r-raġunament dwar avvenimenti, entitajiet, u r-relazzjonijiet tagħhom f’dokument sħiħ. It-tweġiba għall-mistoqsijiet tintuża konvenzjonalment biex tiġi vvalutata l-ħila tal-RC, kemm fl-aġenti artifiċjali kif ukoll fit-tfal li jitgħallmu jaqraw. Madankollu, settijiet ta’ dejta u kompiti e żistenti tal-RC huma ddominati minn mistoqsijiet li jistgħu jiġu solvuti billi jintgħażlu tweġibiet bl-użu ta’ informazzjoni superfiċjali (eż., similarità tal-kuntest lokali jew frekwenza globali tat-terminu); b’hekk jonqsu milli jittestjaw għall-aspett integrattiv essenzjali tal-RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts.  Dawn il-kompiti huma mfassla sabiex iwieġbu b’suċċess il-mistoqsijiet tagħhom jirrikjedu fehim tan-narrattiva sottostanti minflok jiddependu fuq tqabbil ta’ mudelli baxxi jew salienza. Aħna nuru li għalkemm il-bnedmin isolvu l-kompiti faċilment, il-mudelli standard tar-RC iħabbtu wiċċhom mal-kompiti ppreżentati hawnhekk. Aħna nipprovdu analiżi tas-sett tad-dejta u l-isfidi li jippreżenta.</abstract_mt>
      <abstract_mn>Бүгдээрээ ойлголт унших нь мэдээлэл авах эсрэг мэдээлэл, үйл явдал, байгууллагууд, харилцаа нь бүрэн баримт бүрдүүлэхэд хэрэгтэй. Хариулт асуулт хариулт нь RC чадварыг үнэлэхэд хэрэглэгддэг. Гэхдээ орших RC өгөгдлийн сангууд болон үйл ажиллагаанууд гадаргуу мэдээллийг ашиглан шийдвэрлэж болох асуултуудад давамгайлагддаг. РК-ын үндсэн бүрдүүлэх талаар шалгаж чадахгүй. Холны гүн гүнзгий ойлголтын тухай хөгжлийг дэмжихийн тулд, бид уншигч ном, кино скриптүүдийг уншиж, түүхийн тухай асуудлыг хариулна. Эдгээр үйл ажиллагаа бүтээгдэхүүний асуултуудад амжилттай хариулт өгөх нь гүехэн хэлбэрээр холбогдох эсвэл сайхан байдлыг ойлгохын оронд суурь ойлголт хэрэгтэй. Бид хүмүүс үүнийг амархан шийдвэрлэхэд хэдий ч энд тайлбарласан үйлдлийн талаар стандарт РК загварууд тулалдаг. Бид өгөгдлийн сангийн шинжлэх ухаан болон үүнийг тайлбарладаг сорилтуудыг гаргадаг.</abstract_mn>
      <abstract_pl>Rozumienie czytania (RC) – w przeciwieństwie do pozyskiwania informacji – wymaga integracji informacji i rozumowania o zdarzeniach, podmiotach i ich relacjach w całym dokumencie. Odpowiedzi na pytania są konwencjonalnie używane do oceny zdolności RC, zarówno u sztucznych środków, jak i u dzieci uczących się czytać. Jednak istniejące zbiory danych i zadania RC są zdominowane przez pytania, które można rozwiązać poprzez wybór odpowiedzi za pomocą informacji powierzchownych (np. podobieństwo kontekstu lokalnego lub częstotliwość terminów globalnych); Nie potrafią zatem sprawdzić podstawowego aspektu integracyjnego RC. Aby zachęcić do głębszego zrozumienia języka, przedstawiamy nowy zestaw danych i zestaw zadań, w których czytelnik musi odpowiadać na pytania dotyczące opowieści czytając całe książki lub scenariusze filmowe. Zadania te są zaprojektowane tak, aby skuteczne odpowiedzi na ich pytania wymagały zrozumienia podstawowej narracji zamiast polegać na płytkim dopasowaniu wzorców lub wyraźności. Pokazujemy, że chociaż ludzie łatwo rozwiązują zadania, standardowe modele RC zmagają się z przedstawionymi tutaj zadaniami. Zapewniamy analizę zbioru danych i wyzwań, jakie przedstawia.</abstract_pl>
      <abstract_no>Lesing av forståelse (RC) – i contrast til informasjonshenting – krev å integrera informasjon og rasjon om hendingar, einingar og forholdene sine over eit fullstendig dokument. Svar på spørsmål er konvensjonelt brukt for å vurdere RC-kapasitet, både kunstiske agentar og barn som lærer å lesa. Men eksisterande RC-datasett og oppgåver er dominarte av spørsmål som kan løyst ved å velja svar ved hjelp av superinformasjon (f.eks. lokale kontekstsimilaritet eller global term frekvense); dei kan derfor ikkje testa for den viktigste integrerte aspekten av RC. For å oppretta framgang på dypere forståelse av språk, presenterer vi ei ny dataset og set oppgåver som lesaren må svara på spørsmål om historiar ved å lesa heile bokar eller filmskript. Desse oppgåvene er designerte slik at så fullførleg svar på spørsmålene sine krev å forstå den underliggende narrativen i staden for å forstå sålte mønsterelementsamsvar eller saliensjon. Vi viser at selv om mennesker løyser oppgåva enkelt, støtter standard RC-modeller på oppgåva som er presentert her. Vi gjev ein analyse av datasettet og utfordringane det gjev.</abstract_no>
      <abstract_ro>Înțelegerea citirii (RC) – spre deosebire de recuperarea informațiilor – necesită integrarea informațiilor și raționamentului despre evenimente, entități și relațiile acestora într-un document complet. Răspunsul la întrebări este utilizat convențional pentru a evalua capacitatea RC, atât în agenții artificiali, cât și în copiii care învață să citească. Cu toate acestea, seturile de date și sarcinile RC existente sunt dominate de întrebări care pot fi rezolvate prin selectarea răspunsurilor utilizând informații superficiale (de exemplu, similitudinea contextului local sau frecvența termenilor globali); Astfel, nu reușesc să testeze aspectul integrativ esențial al RC. Pentru a încuraja progresul în înțelegerea mai profundă a limbii, vă prezentăm un nou set de date și un set de sarcini în care cititorul trebuie să răspundă la întrebări despre povești prin citirea unor cărți întregi sau scenarii de film. Aceste sarcini sunt concepute astfel încât răspunsul cu succes la întrebările lor necesită înțelegerea narațiunii subiacente, mai degrabă decât bazarea pe potrivirea superficială a modelelor sau saliența. Noi arătăm că, deși oamenii rezolvă sarcinile cu ușurință, modelele RC standard se luptă cu sarcinile prezentate aici. Oferim o analiză a setului de date și a provocărilor pe care le prezintă.</abstract_ro>
      <abstract_sr>Čitanje razumijevanja (RC)—u suprotnosti s prikupljanjem informacija—zahteva integraciju informacija i razgovora o događajima, entitetima i njihovim odnosima u potpunom dokumentu. Odgovor na pitanja se konvencionalno koristi za procjenu sposobnosti RC-a, u umetničkim agentima i djeci koji uče čitati. Međutim, postojeće RC podaci i zadatke dominiraju pitanjima koje mogu rešiti odabereći odgovore koristeći površne informacije (npr. lokalnu kontekstsku sličnost ili globalnu terminsku frekvenciju); Zato nisu testirali ključni integrativni aspekt RC-a. Da bi ohrabrili napredak na dublje razumijevanje jezika, predstavljamo novi set podataka i set zadataka u kojima čitač mora odgovoriti na pitanja o prièama čitajući cijele knjige ili filmske scenarije. Ovi zadaci su dizajnirani tako da uspešno odgovaraju na njihova pitanja zahteva razumeti temeljnu priču umjesto da se oslanjaju na plitke uzorke odgovarajuće ili salijencije. Pokazujemo da, iako ljudi lako rešavaju zadatke, standardni RC modeli bore se za predstavljene zadatke. Mi pružamo analizu seta podataka i izazova koje predstavlja.</abstract_sr>
      <abstract_so>Waxbarashada aasaasiga ah (RC)- si ka duwan helitaanka macluumaadka waxaa looga baahan yahay mid la qabsashada macluumaad iyo ka fikir ku saabsan dhacdooyinka, entities iyo xiriirkooda oo dhan warqad buuxa. Jawaalka su'aalaha waxaa inta badan lagu isticmaalaa qiimeynta awoodda RC, marka labada dhallinyarada la barto karo. Si kastaba ha ahaatee sawirada RC iyo shaqaalaha waxaa lagu maamulaa su'aalo ay ku xalli karaan jawaabaha lagu doorto isticmaalo macluumaad dheeraad ah (tusaale, isku mid ah xiliga deegaanka ama frequency waqtiga caalamiga ah). sidaa darteed waxay ku baaqan waayeen in ay tijaabiyaan dhinaca hoose ee RC Si aan u dhiirrigelino horumarinta hoos u dhigista luuqada, waxaynu keennaa sawir cusub iyo shuqullo badan oo uu akhriyuhu ugu jawaabo su'aalo su'aalo ku saabsan warqadaha la akhriyo buugaagta ama buugaagta filimada oo dhan. Shaqooyinkaasi waxaa loo qoray si ay u jawaabaan su'aalahooda liibaansan ugu baahan yihiin inay fahamaan sheekada hoose taasoo aan ku kalsoonaanin qaab xunxun oo ku habboon ama isbedelka. Waxaynu tusnaynaa in kastoo dadku uu si fudud u xalliyo shuqullada, asalka caadiga ah ee RC waxay ku dagaalamayaan shaqada halkan lagu soo saaray. Analys baaritaanka macluumaadka iyo dhibaatooyinka ay soo saaraan.</abstract_so>
      <abstract_sv>Läsförståelse (RC) – i motsats till informationshämtning – kräver att information och resonemang om händelser, entiteter och deras relationer integreras i ett fullständigt dokument. Frågeställning används konventionellt för att bedöma RC förmåga, både i konstgjorda agens och barn som lär sig läsa. Befintliga datauppsättningar och uppgifter domineras dock av frågor som kan lösas genom att välja svar med hjälp av ytlig information (t.ex. likhet i lokal kontext eller global termfrekvens). De misslyckas därmed med att testa den väsentliga integrativa aspekten av RC. För att uppmuntra till framsteg med djupare språkförståelse presenterar vi en ny datauppsättning och en uppsättning uppgifter där läsaren måste svara på frågor om berättelser genom att läsa hela böcker eller filmmanus. Dessa uppgifter är utformade så att framgångsrikt besvara deras frågor kräver att förstå den underliggande berättelsen snarare än att förlita sig på ytlig mönstermatchning eller saliens. Vi visar att även om människor löser uppgifterna enkelt, kämpar vanliga RC-modeller med de uppgifter som presenteras här. Vi ger en analys av datauppsättningen och de utmaningar den innebär.</abstract_sv>
      <abstract_ta>தகவல் மீட்டுதலுக்கு எதிராக தொகுப்பு (RC)- படிக்க - நிகழ்வு, உண்மைகள் மற்றும் அவங்கள் முழு ஆவணத்திற்கும் முழு தொடர்புகளை ஒன்றிணைக் கேள்வியின் பதில் வழக்கமாக RC இயல்பை மதிப்பதற்கு பயன்படுத்தப்படுகிறது, கலைஞர் மற்றும் குழந்தைகளும் படிக்க கற்று ஆயினும், இருக்கும் RC தரவுத்தளங்கள் மற்றும் பணிகள் தேர்ந்தெடுக்கப்பட்டுள்ள கேள்விகளால் தீர்மானிக்கப்படும் அது மேல்மையான தகவலை தேர அதனால் ஆர்சியின் முக்கியமான ஒருங்கிணைப்பு பகுதிக்கு அவர்கள் சோதிக்க முடியவில்லை. மொழியின் ஆழமான முன்னேற்றத்தை ஆராய்ப்பதற்கு, நாம் ஒரு புதிய தகவல் அமைப்பை கொடுக்கிறோம் மற்றும் ஒரு செயல்களை கொண்டுள்ளோம். அதில் பட இந்த செயல்கள் வடிவமைக்கப்பட்டுள்ளது அதனால் வெற்றிகரமாக அவர்களின் கேள்விகளுக்கு பதில் விற்பனை சார்ந்து அல்லது விற்பனையின் மீ மனிதர்கள் பணிகளை எளிதாக தீர்க்கும் போதும், நிலையான RC மாதிரிகள் இங்கு கொண்ட பணிகளில் போராடுகிறார்கள். நாம் தரவுத்தளத்தின் ஒரு ஆய்வு மற்றும் அது தற்போதிக்கும் சவால்களை வழங்குகிறோம்.</abstract_ta>
      <abstract_si>කියවන්න පුළුවන් (RC)—තොරතුරු ගන්න පුළුවන් වෙනුවෙන් ප්‍රතිචාරයෙන්—සංවිධානයක්, සංවිධානයක්, සහ ඔවුන් ප්‍රශ්න උත්තර දෙන්න ප්‍රශ්න පුළුවන් කියලා කියවන්න පුළුවන් RC ක්‍රියාත්මක විශ්වාස කරන්න ප්‍රයෝජ නමුත්, තියෙන්නේ RC දත්ත සැටුම් සහ වැඩක් ප්‍රශ්න වලින් ප්‍රශ්න වලින් විශ්වාස කරන්න පුළුවන් උත්තර තොරතුරු තෝරගන්න (උදා ඒ වගේම ඔවුන් RC ගේ අවශ්‍යය සම්බන්ධතාවක් වෙනුවෙන් පරීක්ෂා කරන්න බැරි වුනා. භාෂාව ගොඩක් ගොඩක් තේරුම් ගන්න, අපි අළුත් දත්ත සූදානයක් හා වැඩක් සූදානයක් පෙන්වන්න, කියවන්නේ කියවන්නේ කතාවට වග මේ වැඩේ සිද්ධිය කරලා තියෙන්නේ, ඉතින් ඔවුන්ගේ ප්‍රශ්නේ සමහරවිට උත්තර දෙන්න අවශ්‍ය වෙන්න පුළුවන් විදිහ අපි පෙන්වන්නේ මිනිස්සුන් මේ වැඩක් ලේසියෙන් විස්තර කරනවා නමුත්, මෙතන පෙන්වන්න ප්‍රමාණය RC මොඩේල් වලට අපි දත්ත සූදානයේ විශ්ලේෂණයක් සහ අභ්‍යාසයක් දෙන්නේ.</abstract_si>
      <abstract_ur>(RC) سمجھ پڑھنے کی ضرورت ہے - معلومات حاصل کرنے کے بغیر - سؤال جواب سنگتا ہے کہ RC قابلیت کی آزمائش کے لئے استعمال کیا جاتا ہے، دونوں کارساز اگنٹوں اور بچوں میں پڑھنے کی تعلیم لیتے ہیں. However, existing RC data sets and tasks are dominated by questions that can be solved by selecting answers by superficial information (e.g., local context similarity or global term frequency); اس طرح وہ RC کی ضروری تفریق کے لئے امتحان نہیں کرسکتے۔ زبان کی عمیق سمجھنے کے لئے پیشرفت کی سفارش دینے کے لئے، ہم نے ایک نوی ڈیٹ سٹ اور دنیا کے مجموعے پیش کیے ہیں جن میں پڑھنے والے کو تمام کتابوں یا فیلم اسکریٹوں کی پڑھنے کے ذریعے کہانیاں کے بارے میں سوال جواب دینے کے لئے ضر یہ کام طراحی کئے گئے ہیں تاکہ ان کے سوال کو موفق طور پر جواب دینے کی ضرورت ہے کہ دھوپ پٹرنے کے مطابق یا سائل پر بھروسہ کریں ہم دکھاتے ہیں کہ اگرچہ انسان کے کاموں کو آسانی طرح حل کرتا ہے، استاندارڈ رک موڈل یہاں پیش کیے ہوئے کاموں پر جہاد کرتا ہے. ہم نے ڈیٹسٹ کی تحلیل اور چالیوں کی تحلیل دی ہے۔</abstract_ur>
      <abstract_uz>Name Soʻrov odatda o'qishni o'rganish uchun RC qobiliyatini qidirish uchun ishlatiladi. Lekin mavjud RC maʼlumotlar va vazifalar juda muloqat maʼlumot yordamida javoblarni tanlash mumkin (m. g. lokal context- moslamasi yoki global tugma frequency) bilan boshqarish mumkin. they thus fail to test for the essential integrative aspect of RC.  Tilni eng yaxshi o'zgartirish uchun yangi maʼlumot set va shu vazifalarni qo'shish uchun o'qituvchi hamma kitoblar yoki filamu skriptlarini o'qish uchun maslahatlarni javob berishimiz kerak. Bu vazifalar yaratiladi. Ularning savollariga muvaffaqiyatli javob berish kerak, balki kichkina shaklni o'xshash modelga ishlashni yoki saliqlik bilan ishlash kerak. Biz shunday ko'ramiz, oddiy oddiy narsalarni ko'rsatuvchi bo'lsa, bu yerda koʻrilgan vazifalar uchun standard RC modellari harakat qiladi. Biz maʼlumotlar sahifasini taʼlumot qilamiz va hosil qilayotgan challenglarni bajaramiz.</abstract_uz>
      <abstract_vi>Đọc to àn bộ các thông tin (RC) 812; trái với việc thu thập thông tin 812; yêu cầu ghép thông tin và lý trí về các sự kiện, thực thể, và quan hệ của chúng qua một tài liệu đầy đủ. Câu trả lời câu hỏi được dùng thường xuyên để đánh giá khả năng của RC, trong cả nhân viên nhân tạo và trẻ em học đọc. Tuy nhiên, các tập tin và các công việc trên cộng đồng đều bị ảnh hưởng bởi những câu hỏi có thể giải quyết bằng cách chọn các câu trả lời bằng thông tin hời (ví dụ, khả năng tương đồng địa phương hay tần số toàn cầu). họ không kiểm tra được yếu tố bổ sung quan trọng của RC. Để khuyến khích tiến bộ hiểu biết sâu hơn ngôn ngữ, chúng tôi giới thiệu một tập tin mới và một tập hợp các công việc mà người đọc phải trả lời câu hỏi về câu chuyện bằng cách đọc toàn bộ sách hay tập kịch điện ảnh. Những nhiệm vụ này được thiết kế để trả lời các câu hỏi thành công đòi hỏi phải hiểu được cốt truyện cơ bản chứ không phải dựa vào sự khớp mô hình nông nổi. Chúng tôi cho thấy mặc dù con người giải quyết các nhiệm vụ dễ dàng, nhưng các mẫu nâng cao chống đối các nhiệm vụ được đưa ra. Chúng tôi cung cấp một bản phân tích dữ liệu và những thử thách nó đưa ra.</abstract_vi>
      <abstract_bg>Разбирането за четене (РК) - за разлика от извличането на информация - изисква интегриране на информация и разсъждения за събития, образувания и техните взаимоотношения в пълен документ. Отговарянето на въпроси обикновено се използва за оценка на способността както при изкуствени агенти, така и при децата, които се учат да четат. Съществуващите набори от данни и задачи обаче са доминирани от въпроси, които могат да бъдат решени чрез избор на отговори с помощта на повърхностна информация (напр. прилика в локалния контекст или глобална честота на термините); По този начин те не успяват да тестват съществения интегративен аспект на РК. За да насърчим напредъка в по-дълбокото разбиране на езика, представяме нов набор от данни и набор от задачи, в които читателят трябва да отговаря на въпроси за истории чрез четене на цели книги или филмови сценарии. Тези задачи са проектирани така, че успешното отговаряне на въпросите им изисква разбиране на основния разказ, а не разчитане на плитко съвпадение на модели или подчертаване. Показваме, че въпреки че хората решават задачите лесно, стандартните модели се борят със задачите, представени тук. Ние предоставяме анализ на набора от данни и предизвикателствата, които той представя.</abstract_bg>
      <abstract_hr>Pročitanje razumijevanja (RC)-u suprotno s prikupljanjem informacija zahtijeva integraciju informacija i razgovora o događajima, subjektima i njihovim odnosima u cijelom dokumentu. Odgovor na pitanja se konvencionalno koristi za procjenu sposobnosti RC-a, u umjetničkim agentima i djeci koji uče čitati. Međutim, postojeće RC podaci i zadatke dominiraju pitanjima koje se mogu riješiti izrazom odgovora koristeći površne informacije (npr. lokalnu sličnost konteksta ili globalnu frekvenciju termina); Zato nisu testirali temeljni integrativni aspekt RC-a. Da bi potaknuli napredak na dublje razumijevanje jezika, predstavljamo novi set podataka i set zadataka u kojima čitač mora odgovoriti na pitanja o pričama čitajući cijele knjige ili filmske skriptove. Ovi zadaci su dizajnirani tako da uspješno odgovaraju na njihova pitanja zahtijeva razumjeti temeljnu priču umjesto da se oslanjaju na plitke uzorke odgovarajuće ili salijencije. Mi pokazujemo da iako ljudi lako riješe zadatke, standardni RC modeli bore se protiv zadataka predstavljenih ovdje. Mi pružamo analizu seta podataka i izazova koje predstavlja.</abstract_hr>
      <abstract_nl>Reading understanding (RC) – in tegenstelling tot information retrieval – vereist het integreren van informatie en redeneren over gebeurtenissen, entiteiten en hun relaties in een volledig document. Vragen beantwoorden wordt conventioneel gebruikt om RC vermogen te beoordelen, zowel bij kunstmatige agenten als bij kinderen die leren lezen. Bestaande RC datasets en taken worden echter gedomineerd door vragen die kunnen worden opgelost door antwoorden te selecteren met behulp van oppervlakkige informatie (bijvoorbeeld lokale context gelijkenis of globale termfrequentie); Ze slagen er dus niet in om te testen op het essentiële integratieve aspect van RC. Om vooruitgang te bevorderen op het dieper begrijpen van taal, presenteren we een nieuwe dataset en een reeks taken waarin de lezer vragen over verhalen moet beantwoorden door volledige boeken of filmscripts te lezen. Deze taken zijn zo ontworpen dat het succesvol beantwoorden van hun vragen vereist het begrijpen van het onderliggende verhaal in plaats van te vertrouwen op oppervlakkige patroonmatching of salience. We laten zien dat hoewel mensen de taken gemakkelijk oplossen, standaard RC-modellen worstelen met de hier voorgestelde taken. We bieden een analyse van de dataset en de uitdagingen die deze met zich meebrengt.</abstract_nl>
      <abstract_de>Leseverstehen (RC) – im Gegensatz zum Informationsabruf – erfordert die Integration von Informationen und Argumentationen über Ereignisse, Entitäten und ihre Beziehungen in einem vollständigen Dokument. Die Beantwortung von Fragen wird üblicherweise verwendet, um RC-Fähigkeiten zu beurteilen, sowohl bei künstlichen Agenten als auch bei Kindern, die lesen lernen. Bestehende RC-Datensätze und Aufgaben werden jedoch von Fragen dominiert, die durch Auswahl von Antworten anhand oberflächlicher Informationen gelöst werden können (z.B. lokale Kontextähnlichkeit oder globale Termfrequenz); Damit scheitern sie daran, den wesentlichen integrativen Aspekt von RC zu testen. Um Fortschritte beim tieferen Sprachverständnis zu fördern, stellen wir einen neuen Datensatz und eine Reihe von Aufgaben vor, bei denen der Leser Fragen zu Geschichten beantworten muss, indem er ganze Bücher oder Filmskripte liest. Diese Aufgaben sind so konzipiert, dass die erfolgreiche Beantwortung ihrer Fragen das Verständnis der zugrunde liegenden Erzählung erfordert, anstatt sich auf flache Musterabgleichung oder Salienz zu verlassen. Wir zeigen, dass der Mensch zwar die Aufgaben leicht löst, aber Standard-RC-Modelle mit den hier vorgestellten Aufgaben kämpfen. Wir analysieren den Datensatz und die damit verbundenen Herausforderungen.</abstract_de>
      <abstract_da>Læseforståelse (RC) – i modsætning til informationshentning – kræver integrering af information og ræsonnement om begivenheder, enheder og deres relationer på tværs af et fuldt dokument. Spørgsmål besvarelse bruges konventionelt til at vurdere RC evne, både i kunstige agenser og børn, der lærer at læse. Eksisterende datasæt og opgaver domineres dog af spørgsmål, der kan løses ved at vælge svar ved hjælp af overfladiske oplysninger (f.eks. lokal sammenhæng lighed eller global termhyppighed). de undlader således at teste for det væsentlige integrative aspekt af RC. For at fremme fremskridt med dybere sprogforståelse præsenterer vi et nyt datasæt og et sæt opgaver, hvor læseren skal besvare spørgsmål om historier ved at læse hele bøger eller filmmanuskripter. Disse opgaver er designet således, at en vellykket besvarelse af deres spørgsmål kræver forståelse af den underliggende fortælling snarere end at stole på overfladisk mønstermatchning eller fremhævelse. Vi viser, at selvom mennesker nemt løser opgaverne, kæmper standard RC modeller med de opgaver, der præsenteres her. Vi leverer en analyse af datasættet og de udfordringer, det indebærer.</abstract_da>
      <abstract_sw>Kusoma msingi (RC) tofauti na kupata taarifa zinahitaji kuunganisha taarifa na kujadili kuhusu matukio, taasisi na mahusiano yao katika nyaraka kamili. Swali la kujibu linatumiwa kwa kawaida kutathmini uwezo wa RC, katika mashirika ya viwanda na watoto wanaojifunza kusoma. Hata hivyo, seti za data na kazi zinazopo RC zinatawala na maswali yanayoweza kutatua kwa kuchagua majibu kwa kutumia taarifa za juu (kwa mfano, mazingira yanayofanana na kiwango cha muhula wa kimataifa); Kwa hiyo hushindwa kujaribu upande wa uhalisia wa RC. Ili kuhamasisha maendeleo ya kina zaidi ya lugha, tunaweka seti mpya ya taarifa na shughuli ambazo wasomaji lazima ajibu maswali kuhusu maswali yanayohusu vitabu vyote vya filamu. Kazi hizi zimetengenezwa ili kwa mafanikio kujibu maswali yao yanahitaji kuelewa hadithi za msingi badala ya kutegemea kwa mtindo mbaya unaoingana na mauzo. Tunaonyesha kwamba ingawa binadamu wanaweza kutatua kazi hizi kwa urahisi, mitindo ya msingi wa RC hupambana na kazi zilizotolewa hapa. Tunatoa uchambuzi wa kituo cha taarifa na changamoto ambazo huwasilisha.</abstract_sw>
      <abstract_tr>Laýlamak (RC) bilen maglumat almak üçin faýly okamak üçin maglumat we taryşlamak gerek. Gerçekleri, işleri we olaryň ilişkileri doly sened bilen birleştirilýär. Sorag jogapy RC ukyplary barlamak üçin, hem sanal ajamlar hem çagalar okamak üçin ullanýar. Ýöne, bar RC maglumat setirleri we zadalary üst edip çözebilecek soraglar tarapyndan domine edildi (meselâ, ýerli kontekst meňzeşliki we dünýäb terjime frekwensi) saýlarak çözülebilir; Şonuň üçin RK'yň esasy integraty bölegi üçin barlamady. Diliň derinliklerini düşünmegi üçin ilerlemegi täzeden, okuwçynyň hemme kitaplar ýa-da film skriptlerini okap hekaýa barada soraglary jogap bermeli zadlary tapdyk. Bu zadlar bolup özleriniň soraglarynyň üstüne jogap bermek üçin peýdaly düşürmek ýa süýdalyga ynanmak yerine, düşürmeli hekaýaty düşünmek gerek bolar. Biz insanlar bu görevleri kolayca çözmesine rağmen standart RC modelleri burada sunulan görevlerden mücadele ediyoruz. Biz veri setiriniň analyzasyny we munyň sowgatlarynyň kynçylygyny berýäris.</abstract_tr>
      <abstract_af>Lees verstanding (RC)-in kontras tot inligting ontvang-benodig integreer inligting en redening oor gebeurtenis, einhede en hul verwantings oor 'n vollede dokument. Fraag antwoord is konvensionale gebruik om RC-moontlikheid te asseer, in beide kunstenaarlike agente en kinders wat leer om te lees. Alhoewel, bestaande RC datastelle en opdragte word domineer deur vrae wat kan opgelos word deur te kies antwoordes deur te gebruik superficiele inligting (bv. plaaslike konteks gelykenis of globaal term frekwensie); sodat hulle misluk om te toets vir die behoorlike integratiewe aspekte van RC. Om vordering te bevestig oor dieper verstanding van taal, laat ons 'n nuwe datastel en stel van taak voorsien waarin die leser vrae oor stories moet antwoord deur die hele boeke of filmskripte te lees. Hierdie opdragte is ontwerp sodat suksesvol hulle vrae antwoord, benodig verstaan die onderstelde narratiewe eerder as vertrou op slagte patroon ooreenstemmende of salienskap. Ons wys dat alhoewel mense die opdragte maklik oplos, standaard RC-modelle struikel op die opdragte wat hier voorgestel is. Ons verskaf 'n analiseer van die datastel en die uitdagings wat dit voorsien.</abstract_af>
      <abstract_id>Membaca pemahaman (RC)-dalam perbedaan dengan penemuan informasi-membutuhkan integrasi informasi dan alasan tentang peristiwa, entitas, dan hubungan mereka di seluruh dokumen penuh. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read.  Namun, dataset dan tugas RC yang ada didominasi oleh pertanyaan yang dapat diselesaikan dengan memilih jawaban dengan menggunakan informasi superficial (conteks similaritas konteks lokal atau frekuensi terma global); sehingga mereka gagal menguji aspek integratif penting dari RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts.  Tugas-tugas ini dirancang sehingga menjawab dengan sukses pertanyaan mereka membutuhkan pemahaman narratif yang didasarkan daripada bergantung pada persamaan pola rendah atau salinsi. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here.  Kami menyediakan analisis dataset dan tantangan-tantangannya.</abstract_id>
      <abstract_sq>Reading comprehension (RC)-in contrast to information retrieval-requires integrating information and reasoning about events, entities, and their relations across a full document.  Përgjigja e pyetjeve përdoret tradicionalisht për të vlerësuar aftësinë e RC-së, si në agjentët artificial ë ashtu edhe fëmijët që mësojnë të lexojnë. Megjithatë, të dhënat dhe detyrat ekzistuese të RC dominohen nga pyetje që mund të zgjidhen duke zgjedhur përgjigjet duke përdorur informacion sipërfaqësor (për shembull ngjashmëria e kontekstit lokal apo frekuenca globale e termit); ata kështu dështojnë të testojnë për aspektin thelbësor të integrimit të RC. Për të inkurajuar përparimin në kuptimin më të thellë të gjuhës, ne paraqesim një sërë të dhënash të reja dhe një sërë detyrash në të cilat lexuesi duhet të përgjigjet pyetjeve rreth historive duke lexuar libra të tëra apo skripte filmi. Këto detyra janë projektuar në mënyrë që përgjigja me sukses e pyetjeve të tyre të kërkojë kuptimin e rrëfimit themelor në vend që të mbështetet në përputhjen e modelit të thellë apo rëndësinë. Ne tregojmë se megjithëse njerëzit zgjidhin detyrat lehtë, modelet standarde RC luftojnë mbi detyrat e paraqitura këtu. Ne japim një analizë të të dhënave dhe sfidave që paraqet.</abstract_sq>
      <abstract_am>Reading comprehension (RC)-in contrast to information retrieval-requires integrating information and reasoning about events, entities, and their relations across a full document.  ጥያቄ መልስ በመጠቀም RC ኃይልን በማስተካከል፣ በሥልጣናት እና ልጆችም ለማንበብ የሚያስተምሩ ነው፡፡ ምንም እንኳን፣ የአሁኑ RC ዳታተሮች እና ስራዎችን በመምረጥ ጥያቄዎች በመምረጥ ላይ መረጃ በመምረጥ ይጠቅማል፡፡ ስለዚህም RC በሚያስፈልገው ጠቅላላ ጉዳይ ይሞክራሉ፡፡ ቋንቋን ለማጠናቀቅ አዲስ ዳታተር እና ስራዎችን እና አቆማለን፤ አንባቢው የዝርዝሮች ወይም የፊልም ጽሑፎችን በማንበብ ጉዳይ ላይ ጥያቄዎችን ለመመልስ ያስፈልጋል፡፡ እነዚህም ስራዎች የተዘጋጁ ጥያቄዎቻቸውን በመቀበል የደረጃ ታሪክ እንዲያስተውሉ፣ በመጠቀም ወይም በጭንቅ ምሳሌ ላይ ከመታሰል ይልቅ በመጠቀም የሚችሉትን ታሪክ እንዲያስተውሉ ይፈልጋሉ፡፡ ሰውነቱ ስራዎቹን በቀላል ቢፈጸም እናሳያቸዋለን፡፡ የዳታ ሳጥን እና የሚያሳየው ጥያቄዎችን እናሳውቃለን፡፡</abstract_am>
      <abstract_hy>Ինֆորմացիայի վերաբերյալ ընկալումների կարդալը պահանջում է ինտեգրել տեղեկատվությունը և մտածել իրադարձությունների, կազմակերպությունների և նրանց հարաբերությունների մասին ամբողջ փաստաթղթի մեջ: Հարցերի պատասխանը սովորաբար օգտագործվում է ՀԿ-ի կարողության գնահատման համար, և արհեստական գործակալների, և երեխաների համար, ովքեր սովորում են կարդալ: Այնուամենայնիվ, գոյություն ունեցող ՌԿ տվյալների համակարգերը և առաջադրանքները գերիշխում են հարցերով, որոնք կարող են լուծվել ընտրելով պատասխաններ՝ օգտագործելով մակերեսային տեղեկատվություն (օրինակ, տեղական կոնտեքստի նմանությունը կամ գլոբալ նրանք չեն ստուգում ՀԿ-ի հիմնական ինտեգրացիվ ասպեկտը: Որպեսզի խրախուսենք լեզվի ավելի խորը հասկանալու առաջընթացը, մենք ներկայացնում ենք նոր տվյալների համակարգ և առաջադրանքներ, որտեղ կարդացողը պետք է պատասխանի պատմությունների մասին հարցերին՝ կարդալով ամբողջ գրքերը կամ ֆիլմի գրքերը These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience.  Մենք ցույց ենք տալիս, որ չնայած որ մարդիկ հեշտությամբ լուծում են խնդիրները, ստանդարտ ՌԿ մոդելները պայքարում են այստեղ ներկայացված խնդիրների վրա: Մենք տրամադրում ենք տվյալների համակարգի վերլուծություն և այն առաջացնող մարտահրավերներ:</abstract_hy>
      <abstract_az>Qəbər alınması ilə (RC) anlaşılığı oxuyarkən məlumat alması haqqında məlumatları, olaraq, məlumatları və əlaqələrini bütün bir dökümə ilə birləşdirmək lazımdır. Söylə cavab vermək RC qabiliyyətini təsdiqləmək üçün, həmçin in sanatlı ajanlar və uşaqlar oxumaq öyrənmək üçün işlədilər. Ancaq, mövcud RC veri qurğuları və işləri üstünlük məlumatları istifadə edərək çəkilən cavabları seçərək dəstəklənir. buna görə də RC'nin əsas integratlı hissəsini imtahana çəkməyə başarısız oldular. Dillərin daha derin anlaşılmasını təşkil etmək üçün, oxuyucunun bütün kitablar və film skriptlərini oxuyaraq hekayələri haqqında soruşmalarına cavab verməsi lazım olan yeni verilən qurğuları və işləri təşkil edirik. Bu işlər müəyyən edilmişdir ki, suallarına müvəffəqiyyətlə cavab vermək üçün çətinliklə uyğunlaşdırma və çətinliklərə güvən etmək yerinə düzgün hekayəti anlaması lazımdır. Biz göstəririk ki, insanlar bu işləri asanlıqla çəksələr də, burada göstərilən işlərdə standart RC modelləri mübahisə edirlər. Biz verilən qurğuların analizi və göstərilən çətinləri təmin edirik.</abstract_az>
      <abstract_fa>خواندن درک (RC) در مقابل گرفتن اطلاعات نیاز دارد که اطلاعات و دلیل‌گیری در مورد اتفاقات، واحد‌ها و رابطه‌هایشان در یک سند کامل جمع شود. پاسخ سوال به طور معمولی برای ارزیابی توانایی RC استفاده می‌شود، در هر دو ماموران هنری و بچه‌ها یاد می‌گیرند که بخوانند. با این حال، مجموعه‌های داده‌های RC موجود و وظیفه‌ها توسط سوال‌هایی که می‌توانند با انتخاب جواب‌ها با استفاده از اطلاعات superficial (مثال شبیه‌سازی محلی محلی یا فرکانس اصلی جهانی) تسلیم شوند. به همین دلیل شکست نمی‌گیرند که برای نقطه‌های تفریحی اصلی RC آزمایش کنند. برای تشویق پیشرفت در درک عمیق زبان، ما یک مجموعه داده‌های جدید و مجموعه کارها را پیشنهاد می‌کنیم که در آن خواننده باید سوالات درباره داستان‌ها را با خواندن کل کتاب یا نوشته‌های فیلم جواب دهد. این وظیفه‌ها طراحی می‌شوند تا به موفقیت جواب سوال‌هایشان نیاز به فهمیدن داستان‌های بنیادی به جای استفاده کردن روی نمونه‌های گسترده یا ساده‌ای است. ما نشان می دهیم که هر چند انسان کار را به آسانی حل می کنند، مدل های استاندارد RC در مورد کارهای پیشنهاد اینجا مبارزه می کنند. ما تحلیل مجموعه داده‌ها و چالش‌هایی را که پیشنهاد می‌دهند را پیشنهاد می‌کنیم.</abstract_fa>
      <abstract_ko>읽기 이해(RC) - 정보 검색과 달리 읽기 이해는 이벤트, 실체와 그 관계에 대한 정보와 추리를 완전한 문서에 통합시켜야 한다.인공지능과 어린이의 읽기 학습에서 문답은 보통 읽기 능력을 평가하는 데 쓰인다.그러나 기존의 RC 데이터 집합과 임무는 주로 문제로 구성되어 있는데 이런 문제들은 표면 정보(예를 들어 국부 상하문 유사성이나 전역 용어 주파수)를 사용하여 답을 선택하여 해결할 수 있다.따라서 RC의 기본적인 종합적인 측면을 테스트할 수 없다.언어에 대한 깊은 이해를 촉진하기 위해 우리는 새로운 데이터 집합과 하나의 임무를 제공했다. 이런 임무에서 독자는 반드시 책 전체나 영화 각본을 읽으며 이야기와 관련된 문제를 대답해야 한다.이러한 임무의 디자인은 그들의 질문에 성공적으로 대답하기 위해 잠재적인 서사를 이해해야 하며, 얕은 패턴의 일치나 현저성에 의존하는 것이 아니라 잠재적인 서사를 이해해야 한다.인간은 이런 임무들을 쉽게 해결하지만, 표준적인 RC모델은 여기에 소개된 임무에 있어 어렵다는 점을 발견했다.우리는 데이터 집합과 그에 따른 도전에 대해 분석을 진행했다.</abstract_ko>
      <abstract_bs>Pročitanje razumijevanja (RC)-u suprotnost s prikupljanjem informacija zahtijeva integraciju informacija i razgovora o događajima, entitetima i njihovim odnosima u potpunom dokumentu. Odgovor na pitanja se konvencionalno koristi za procjenu sposobnosti RC-a, u umjetničkim agentima i djeci koji uče čitati. Međutim, postojeće RC podaci i zadatke dominiraju pitanjima koje mogu riješiti odabereći odgovore koristeći površne informacije (npr. lokalnu kontekstsku sličnost ili globalnu terminsku frekvenciju); Zato nisu testirali ključni integrativni aspekt RC-a. Da bi potaknuli napredak na dublje razumijevanje jezika, predstavljamo novi set podataka i set zadataka u kojima čitač mora odgovoriti na pitanja o pričama čitajući cijele knjige ili filmske skriptove. Ovi zadaci su dizajnirani tako da uspješno odgovaraju na njihova pitanja zahtijeva razumjeti temeljnu priču umjesto da se oslanjaju na plitke uzorke odgovarajuće ili salijencije. Mi pokazujemo da, iako ljudi lako riješe zadatke, standardni RC modeli bore se za predstavljene zadatke. Mi pružamo analizu seta podataka i izazova koje predstavlja.</abstract_bs>
      <abstract_bn>তথ্য পুনরুদ্ধারের বিরুদ্ধে সম্পূর্ণ তথ্য সম্পূর্ণ তথ্য, বস্তু এবং তাদের সম্পর্ক একত্রিত করার প্রয়োজন। প্রশ্নের উত্তর সাধারণত RC-এর ক্ষমতা মূল্যের জন্য ব্যবহার করা হয়, যা কৌতূহলিক এজেন্ট এবং শিশুদের পড়তে শিখতে পারে। তবে বিদ্যমান RC ডাটাসেট এবং কাজের মাধ্যমে প্রশ্নের মাধ্যমে নিয়ন্ত্রণ করা হচ্ছে যা অতিরিক্ত তথ্য ব্যবহার করে উত্তর নির্বাচন করে সমাধান করা যাবে (উদ তাই তারা RC এর প্রধান একত্রিত প্রান্তের জন্য পরীক্ষা করতে ব্যর্থ। ভাষার গভীর ভাষায় অগ্রগতিকে উৎসাহিত করার জন্য আমরা একটি নতুন ডাটাসেট এবং একটি কাজ উপস্থাপন করি যেখানে পাঠকের পুরো বই অথবা চলচ্চিত্রের স্ক্ এই কাজগুলো ডিজাইন করা হয়েছে যাতে সফলভাবে তাদের প্রশ্নের উত্তর প্রদান করা প্রয়োজন যে তাদের কাহিনী বুঝতে পারে তাদের কাহিনীর আমরা দেখাচ্ছি যে যদিও মানুষ সহজে কাজ সমাধান করে, তবে এখানে প্রকাশিত কাজের উপর স্থান্ডার্ডার রিসি মডেল সংগ্রাম করছ আমরা ডাটাসেটের বিশ্লেষণ এবং এটা উপস্থাপন করা চ্যালেঞ্জের ব্যাপারে বিশ্লেষণ করি।</abstract_bn>
      <abstract_ca>Reading comprehension (RC)-in contrast to information retrieval-requires integrating information and reasoning about events, entities, and their relations across a full document.  Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read.  No obstant això, els conjunts de dades i tasques de RC existents són dominats per preguntes que es poden resoldre seleccionant respostes utilitzant informació superficial (per exemple, similitud del context local o freqüència global de termes); així que no poden provar l'aspecte integrador essencial de la RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts.  These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience.  We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here.  Fem una an àlisi del conjunt de dades i dels reptes que presenta.</abstract_ca>
      <abstract_et>Lugemise arusaamine (RC) – vastupidiselt teabe hankimisele – nõuab sündmuste, üksuste ja nende suhete teabe ja arutluse integreerimist tervikdokumendis. Küsimustele vastamist kasutatakse tavaliselt RC võime hindamiseks nii kunstlike ainete kui ka lugemist õppivate laste puhul. Olemasolevates RC andmekogumites ja ülesannetes domineerivad aga küsimused, mida saab lahendada vastuste valimisega pealiskaudse teabe abil (nt kohaliku konteksti sarnasus või ülemaailmne terminite sagedus); Seega ei suuda nad riskikapitali olulist integratiivset aspekti testida. Keele sügavama mõistmise soodustamiseks esitame uue andmekogumi ja ülesannete komplekti, milles lugeja peab vastama lugude küsimustele tervete raamatute või filmide skriptide abil. Need ülesanded on kavandatud nii, et nende küsimustele edukalt vastamine nõuab pigem alusnarratiivi mõistmist, kui tugineda madalale mustrite sobitamisele või silmapaistvusele. Näitame, et kuigi inimesed lahendavad ülesandeid kergesti, on standardsed RC mudelid siin esitatud ülesannete täitmisel raske. Analüüsime andmekogumit ja selle väljakutseid.</abstract_et>
      <abstract_fi>Lukuymmärrys (RC) – toisin kuin tiedonhaku – vaatii tiedon ja päättelyn yhdistämistä tapahtumista, entiteeteistä ja niiden suhteista koko asiakirjaan. Kysymyksiin vastaamista käytetään perinteisesti RC-kyvyn arviointiin sekä keinotekoisissa tekijöissä että lukemista oppivissa lapsissa. Nykyisiä RC-aineistoja ja tehtäviä hallitsevat kuitenkin kysymykset, jotka voidaan ratkaista valitsemalla vastaukset pinnallisilla tiedoilla (esim. paikallisen kontekstin samankaltaisuus tai termien yleisyys). ne eivät siis testaa RC:n olennaista integratiivista näkökohtaa. Edistääksemme kielen syvempää ymmärtämistä esittelemme uuden aineiston ja joukon tehtäviä, joissa lukijan on vastattava tarinoihin liittyviin kysymyksiin lukemalla kokonaisia kirjoja tai elokuvakäsikirjoituksia. Nämä tehtävät on suunniteltu siten, että heidän kysymyksiinsä vastaaminen edellyttää taustalla olevan kerronnan ymmärtämistä sen sijaan, että luottaisit matalaan kuvion vastaamiseen tai korostumiseen. Osoitamme, että vaikka ihmiset ratkovat tehtävät helposti, standardit RC-mallit kamppailevat tässä esitetyissä tehtävissä. Analysoimme aineistoa ja sen tuomia haasteita.</abstract_fi>
      <abstract_cs>Porozumění čtení (RC) – na rozdíl od vyhledávání informací – vyžaduje integraci informací a uvažování o událostech, entitách a jejich vztazích napříč celým dokumentem. Odpověď na otázky se běžně používá k posouzení RC schopnosti, jak u umělých agentů, tak u dětí učících se číst. Stávajícím datovým souborům a úkolům RC ovšem dominují otázky, které lze řešit výběrem odpovědí pomocí povrchových informací (např. podobnost lokálního kontextu nebo frekvence globálních termínů); Proto nedokážou testovat základní integrační aspekt RC. Abychom podpořili pokrok v hlubším porozumění jazyku, představujeme nový datový soubor a soubor úkolů, ve kterých čtenář musí zodpovědět otázky týkající se příběhů čtením celých knih nebo filmových scénářů. Tyto úkoly jsou navrženy tak, aby úspěšné odpovědi na jejich otázky vyžadovaly porozumění základnímu příběhu spíše než spoléhat na mělké shody vzorů nebo významnost. Ukazujeme, že i když lidé řeší úkoly snadno, standardní RC modely bojují s úkoly zde prezentovanými. Poskytujeme analýzu datového souboru a výzev, které představuje.</abstract_cs>
      <abstract_jv>politenessoffpolite"), and when there is a change ("assertivepoliteness Sugeng responseng diputungan ijol-ijolan diputêrangke nggawe kapan PN, ning kelas artikse lan kelas sing oleh basa. politenessoffpolite"), and when there is a change ("assertivepoliteness yo nganggo cara-cara sing paling maneh kanggo akeh dikenalke aspek awak dhéwé Ombudhakan langkung ngerasakno akeh luwih akeh luwih apik, awak dhéwé iso nggawe dataset anyar lan nganggo perusahaan sing kudu nggawe barang langkung dhéwé ngerasakno sing nyimpen basa sing paling dhéwé karo akeh stir sing paling dhéwé uga nyong, or a basa Awak dhéwé iki dibenakno dadi, winih kanggo ngomong nik nggabungi layang-layang sing nyatakakno Awak dhéwé éntuk wong hal-hal wong liyane luwih nggawe barang-alam iki banget, nik sampek model sing bakal nggawe barang nggawe barang iki dadi. Awak dhéwé ngewehke beraksi kanggo nggawe dataset lan kanggo dianggawe kuwi nggawe</abstract_jv>
      <abstract_sk>Razumevanje branja (RC) – v nasprotju s pridobivanjem informacij – zahteva vključitev informacij in razmišljanja o dogodkih, entitetah in njihovih odnosih v celoten dokument. Odgovarjanje na vprašanja se običajno uporablja za oceno sposobnosti RC, tako pri umetnih agentih kot pri otrocih, ki se učijo brati. Vendar v obstoječih naborih podatkov in nalogah RC prevladujejo vprašanja, ki jih je mogoče rešiti z izbiro odgovorov s površinskimi informacijami (npr. podobnost lokalnega konteksta ali globalna frekvenca izrazov); Zato ne preskusijo bistvenega integrativnega vidika RC. Da bi spodbudili napredek pri globljem razumevanju jezika, predstavljamo nov nabor podatkov in nabor nalog, v katerih mora bralec odgovarjati na vprašanja o zgodbah z branjem celih knjig ali filmskih scenarijev. Te naloge so zasnovane tako, da uspešno odgovarjanje na njihova vprašanja zahteva razumevanje osnovne pripovedi, namesto da se zanašajo na plitko ujemanje vzorcev ali izpostavljenost. Pokazali smo, da čeprav ljudje enostavno rešujejo naloge, se standardni RC modeli borijo pri nalogah, predstavljenih tukaj. Zagotavljamo analizo nabora podatkov in izzivov, ki jih predstavlja.</abstract_sk>
      <abstract_he>קריאת הבנה (RC) בניגוד לשיגור מידע דורשת אינטגרציה מידע והסיבה על אירועים, יחסים, ומערכת יחסיהם בכל מסמך מלא. תשובות לשאלות משתמשות באופן קונציונלי כדי להעריך את היכולת של RC, גם בסוכנים מלאכותיים וגם ילדים ללמודים לקרוא. בכל אופן, קבוצות נתונים ומשימות RC קיימות שולטות על ידי שאלות שאפשר לפתור על ידי בחירת תשובות באמצעות מידע שטחי (למשל דמיון קונטקסט מקומי או תדירות מונח גלובלי); כך הם לא מסוגלים לבדוק את היבט האינטגרטיבי החיובי של RC. כדי לעודד התקדמות בהבנה עמוקה יותר של השפה, אנו מציגים קבוצת נתונים חדשה ומערכת משימות שבהן הקורא חייב לענות על שאלות על סיפורים על ידי קריאת ספרים שלמים או סרטים שלמים. המשימות האלה מתוכננות כדי לענות בהצלחה לשאלותיהם דורשת להבין את הסיפור הנוכחי במקום לסמוך על התאמת דפוס שטחי או מראה. אנחנו מראים שלמרות שאנשים פותרים את המשימות בקלות, דוגמנים סטנדרטיים RC נאבקים על המשימות המוציאות כאן. We provide an analysis of the dataset and the challenges it presents.</abstract_he>
      <abstract_ha>QUnicodeControlCharacterMenu Sunan tambayar ta ana amfani da shi a ɗabi'a don a iya ƙaddara abincin RC, kuma a cikin duk mataimaki da yãyen su karanta karatun. A lokacin da, ana domin tsari da masu jira na RC da aikin su na tambayi wanda za'a iya solve su da kuma za'a zãɓe su da amfani da masu tsari masu rufe (misali, misali, mazaɓa da mazaɓa mai daidaita ko kuma misali da duran muhimman duniya). Kayya, ba su yi jarraba ne masu kami na RC. Dõmin Mu ƙara gaba ga rufi da harshen, muna halatar da wani matsayi na danne da shiryoyin aikin wanda mai karantawa ya kamata ya karɓa wa maswali masu tambayar lãbãran da za'a karatun takardun littattafai ko manuscriptun filamu. Wannan aikin aka designa don a sami marubuci wajen karɓa wa maswalinsu, ana kamata su fahimta lãbãrin masu ƙaranci, kuma bã ya dõgara a kan missalin da ya yi ƙunci ko sali. Tuna nũna cewa, kuma kõ da mutum ke raba aikin su da sauƙi, misãlai na RC na tsakani a kan aikin da aka halatar da su a nan. Munã samar da anarrai ga tsari da muramman da ke gabatar da shi.</abstract_ha>
      <abstract_bo>Reading comprehension (RC)-in contrast to information retrieval-requires integrating information and reasoning about events, entities, and their relations across a full document. ཡིག་གཟུགས་ལན་གསལ་འདོན་ནི་རྩོམ་པ་ལ་ཆོས་ཉིད་དུ་གཏོང་གི་ཐབས་ཤེས་ཡོད་པ་ལས། ཡིན་ནའང་། existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g. local context similarity or global term frequency); ཆོག་ཡིན་ནའང་། དེར་བརྟེན། ཁོང་ཚོས་RC འི་ཆེད་དུ་གླེང་སྒྲིག་གི་ཆེད་དུ་བརྟག་ཞིབ་བྱེད་མི་ཐུབ། སྐད་ཡིག་གི་འདྲ་རྒྱའི་ལྟ་སྟངས་དཀའ་བར་གྱི་འཕེལ་རིམ་གྱི་འཕེལ་རིམ་ལ་གཏོང་དགོས་པའི་ནང་དུ་ཡིག་ཆ་སྒྲིག་ཡིག་ཆ་གསར་པ་ཞིག་དང་ལས་འགུལ་ བྱ་འགུལ་འདི་དག་གི་དོན་ལ་རྒྱས་ཐལ་ཐོག་ཏུ་མཐུན་པ་ལས་ཁོང་ཚོའི་དྲི་ཚིག་ལ་དང་མཉམ་དུ་མཐུན་པའི་གཟུགས་རིས་མཐུན་སྒྲིག་ ང་ཚོས་མི་རིགས་ཀྱིས་ལས་ཀ་འདི་ལས་སླ་མོལ་ལས་ཀར་ཆེན་བྱེད་པ་ཡིན་ནའང་མི་མང་གིས་ ང་ཚོས་གནས་ཚུལ་སྒྲིག་ཆ་འཕྲིན་དང་གདོང་ལེན་གྱི་དབྱེ་ཞིབ་བྱེད་ཀྱི་ཡོད།</abstract_bo>
      </paper>
    <paper id="24">
      <title>Native Language Cognate Effects on Second Language Lexical Choice</title>
      <author><first>Ella</first><last>Rabinovich</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <author><first>Shuly</first><last>Wintner</last></author>
      <doi>10.1162/tacl_a_00024</doi>
      <abstract>We present a computational analysis of cognate effects on the spontaneous linguistic productions of advanced non-native speakers. Introducing a large corpus of highly competent non-native English speakers, and using a set of carefully selected lexical items, we show that the lexical choices of non-natives are affected by cognates in their native language. This effect is so powerful that we are able to reconstruct the <a href="https://en.wikipedia.org/wiki/Phylogenetic_tree">phylogenetic language tree</a> of the <a href="https://en.wikipedia.org/wiki/Indo-European_languages">Indo-European language family</a> solely from the frequencies of specific lexical items in the English of authors with various native languages. We quantitatively analyze non-native lexical choice, highlighting cognate facilitation as one of the important phenomena shaping the language of non-native speakers.</abstract>
      <pages>329–342</pages>
      <url hash="230e2b0c">Q18-1024</url>
      <video href="https://vimeo.com/285802410" />
      <bibkey>rabinovich-etal-2018-native</bibkey>
      <pwccode url="https://github.com/ellarabi/reddit-l2" additional="false">ellarabi/reddit-l2</pwccode>
    </paper>
    <paper id="27">
      <title>Polite Dialogue Generation Without Parallel Data</title>
      <author><first>Tong</first><last>Niu</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <doi>10.1162/tacl_a_00027</doi>
      <abstract>Stylistic dialogue response generation, with valuable applications in personality-based conversational agents, is a challenging task because the response needs to be fluent, contextually-relevant, as well as paralinguistically accurate. Moreover, <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel datasets</a> for regular-to-stylistic pairs are usually unavailable. We present three weakly-supervised models that can generate diverse, polite (or rude) dialogue responses without parallel data. Our late fusion model (Fusion) merges the decoder of an encoder-attention-decoder dialogue model with a language model trained on stand-alone polite utterances. Our label-finetuning (LFT) model prepends to each source sequence a politeness-score scaled label (predicted by our state-of-the-art politeness classifier) during training, and at test time is able to generate polite, neutral, and rude responses by simply scaling the label embedding by the corresponding score. Our reinforcement learning model (Polite-RL) encourages politeness generation by assigning rewards proportional to the politeness classifier score of the sampled response. We also present two retrievalbased, polite dialogue model baselines. Human evaluation validates that while the Fusion and the retrieval-based models achieve <a href="https://en.wikipedia.org/wiki/Politeness">politeness</a> with poorer context-relevance, the LFT and Polite-RL models can produce significantly more polite responses without sacrificing dialogue quality.</abstract>
      <pages>373–389</pages>
      <url hash="0ed1c30e">Q18-1027</url>
      <bibkey>niu-bansal-2018-polite</bibkey>
    </paper>
    <paper id="29">
      <title>Learning to Remember Translation History with a Continuous Cache</title>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Tong</first><last>Zhang</last></author>
      <doi>10.1162/tacl_a_00029</doi>
      <abstract>Existing neural machine translation (NMT) models generally translate sentences in isolation, missing the opportunity to take advantage of document-level information. In this work, we propose to augment NMT models with a very light-weight cache-like memory network, which stores recent hidden representations as translation history. The <a href="https://en.wikipedia.org/wiki/Probability_distribution">probability distribution</a> over generated words is updated online depending on the translation history retrieved from the <a href="https://en.wikipedia.org/wiki/Computer_memory">memory</a>, endowing NMT models with the capability to dynamically adapt over time. Experiments on multiple domains with different topics and styles show the effectiveness of the proposed approach with negligible impact on the <a href="https://en.wikipedia.org/wiki/Computational_cost">computational cost</a>.</abstract>
      <pages>407–420</pages>
      <url hash="ed1d96f4">Q18-1029</url>
      <bibkey>tu-etal-2018-learning</bibkey>
      <pwccode url="https://github.com/longyuewangdcu/tvsub" additional="false">longyuewangdcu/tvsub</pwccode>
    </paper>
    <paper id="31">
      <title>Generating Sentences by Editing Prototypes</title>
      <author><first>Kelvin</first><last>Guu</last></author>
      <author><first>Tatsunori B.</first><last>Hashimoto</last></author>
      <author><first>Yonatan</first><last>Oren</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <doi>10.1162/tacl_a_00030</doi>
      <abstract>We propose a new generative language model for sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional language models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.</abstract>
      <pages>437–450</pages>
      <url hash="0a891ec9">Q18-1031</url>
      <video href="https://vimeo.com/285801187" />
      <bibkey>guu-etal-2018-generating</bibkey>
      <pwccode url="https://github.com/kelvinguu/neural-editor" additional="true">kelvinguu/neural-editor</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
    </paper>
    <paper id="32">
      <title>Language Modeling for Morphologically Rich Languages : Character-Aware Modeling for Word-Level Prediction</title>
      <author><first>Daniela</first><last>Gerz</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Edoardo</first><last>Ponti</last></author>
      <author><first>Jason</first><last>Naradowsky</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <doi>10.1162/tacl_a_00032</doi>
      <abstract>Neural architectures are prominent in the construction of language models (LMs). However, word-level prediction is typically agnostic of subword-level information (characters and character sequences) and operates over a closed vocabulary, consisting of a limited word set. Indeed, while subword-aware models boost performance across a variety of NLP tasks, previous work did not evaluate the ability of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to assist next-word prediction in language modeling tasks. Such subword-level informed models should be particularly effective for morphologically-rich languages (MRLs) that exhibit high type-to-token ratios. In this work, we present a large-scale LM study on 50 typologically diverse languages covering a wide variety of <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological systems</a>, and offer new LM benchmarks to the community, while considering subword-level information. The main technical contribution of our work is a novel method for injecting subword-level information into semantic word vectors, integrated into the neural language modeling training, to facilitate word-level prediction. We conduct experiments in the LM setting where the number of infrequent words is large, and demonstrate strong perplexity gains across our 50 languages, especially for morphologically-rich languages. Our code and data sets are publicly available.</abstract>
      <pages>451–465</pages>
      <url hash="8c554ad3">Q18-1032</url>
      <bibkey>gerz-etal-2018-language</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="33">
      <title>Detecting Institutional Dialog Acts in Police Traffic Stops</title>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <author><first>Camilla</first><last>Griffiths</last></author>
      <author><first>Hang</first><last>Su</last></author>
      <author><first>Prateek</first><last>Verma</last></author>
      <author><first>Nelson</first><last>Morgan</last></author>
      <author><first>Jennifer L.</first><last>Eberhardt</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <doi>10.1162/tacl_a_00031</doi>
      <abstract>We apply computational dialog methods to police body-worn camera footage to model conversations between police officers and community members in <a href="https://en.wikipedia.org/wiki/Traffic_stop">traffic stops</a>. Relying on the theory of institutional talk, we develop a labeling scheme for police speech during traffic stops, and a tagger to detect institutional dialog acts (Reasons, Searches, Offering Help) from transcribed text at the turn (78 % F-score) and stop (89 % F-score) level. We then develop speech recognition and segmentation algorithms to detect these acts at the stop level from raw camera audio (81 % F-score, with even higher accuracy for crucial acts like conveying the reason for the stop). We demonstrate that the dialog structures produced by our tagger could reveal whether officers follow law enforcement norms like introducing themselves, explaining the reason for the stop, and asking permission for searches. This work may therefore inform and aid efforts to ensure the procedural justice of police-community interactions.</abstract>
      <pages>467–481</pages>
      <url hash="3eb694fb">Q18-1033</url>
      <video href="https://vimeo.com/285803587" />
      <bibkey>prabhakaran-etal-2018-detecting</bibkey>
    </paper>
    <paper id="36">
      <title>Neural Lattice Language Models</title>
      <author><first>Jacob</first><last>Buckman</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <doi>10.1162/tacl_a_00036</doi>
      <abstract>In this work, we propose a new language modeling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities : neural lattice language models. These models construct a lattice of possible paths through a sentence and marginalize across this <a href="https://en.wikipedia.org/wiki/Lattice_(group)">lattice</a> to calculate sequence probabilities or optimize parameters. This approach allows us to seamlessly incorporate linguistic intuitions   including <a href="https://en.wikipedia.org/wiki/Polysemy">polysemy</a> and the existence of multiword lexical items   into our <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>. Experiments on multiple language modeling tasks show that English neural lattice language models that utilize polysemous embeddings are able to improve <a href="https://en.wikipedia.org/wiki/Perplexity">perplexity</a> by 9.95 % relative to a word-level baseline, and that a Chinese model that handles multi-character tokens is able to improve <a href="https://en.wikipedia.org/wiki/Perplexity">perplexity</a> by 20.94 % relative to a character-level baseline.</abstract>
      <pages>529–541</pages>
      <url hash="0d2c3822">Q18-1036</url>
      <bibkey>buckman-neubig-2018-neural</bibkey>
      <pwccode url="https://github.com/jbuckman/neural-lattice-language-models" additional="false">jbuckman/neural-lattice-language-models</pwccode>
    </paper>
    <paper id="37">
      <title>Planning, Inference and Pragmatics in Sequential Language Games</title>
      <author><first>Fereshte</first><last>Khani</last></author>
      <author><first>Noah D.</first><last>Goodman</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <doi>10.1162/tacl_a_00037</doi>
      <abstract>We study sequential language games in which two players, each with private information, communicate to achieve a common goal. In such games, a successful player must (i) infer the partner’s private information from the partner’s messages, (ii) generate messages that are most likely to help with the goal, and (iii) reason pragmatically about the partner’s strategy. We propose a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that captures all three characteristics and demonstrate their importance in capturing <a href="https://en.wikipedia.org/wiki/Human_behavior">human behavior</a> on a new goal-oriented dataset we collected using <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a>.</abstract>
      <pages>543–555</pages>
      <url hash="aa924e10">Q18-1037</url>
      <bibkey>khani-etal-2018-planning</bibkey>
      <pwccode url="https://worksheets.codalab.org/worksheets/0x052129c7afa9498481185b553d23f0f9" additional="false">worksheets/0x052129c7</pwccode>
    </paper>
    <paper id="38">
      <title>Probabilistic Verb Selection for Data-to-Text Generation</title>
      <author><first>Dell</first><last>Zhang</last></author>
      <author><first>Jiahao</first><last>Yuan</last></author>
      <author><first>Xiaoling</first><last>Wang</last></author>
      <author><first>Adam</first><last>Foster</last></author>
      <doi>10.1162/tacl_a_00038</doi>
      <abstract>In data-to-text Natural Language Generation (NLG) systems, computers need to find the right words to describe phenomena seen in the data. This paper focuses on the problem of choosing appropriate verbs to express the direction and magnitude of a percentage change (e.g., in stock prices). Rather than simply using the same verbs again and again, we present a principled data-driven approach to this problem based on Shannon’s noisy-channel model so as to bring variation and naturalness into the generated text. Our experiments on three large-scale real-world news corpora demonstrate that the proposed probabilistic model can be learned to accurately imitate human authors’ pattern of usage around verbs, outperforming the state-of-the-art method significantly.</abstract>
      <pages>511–527</pages>
      <video href="https://vimeo.com/385504366" permission="false" />
      <url hash="884435ee">Q18-1038</url>
      <bibkey>zhang-etal-2018-probabilistic</bibkey>
    </paper>
    <paper id="39">
      <title>Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification</title>
      <author><first>Xilun</first><last>Chen</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Ben</first><last>Athiwaratkun</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <author><first>Kilian</first><last>Weinberger</last></author>
      <doi>10.1162/tacl_a_00039</doi>
      <abstract>In recent years great success has been achieved in sentiment classification for <a href="https://en.wikipedia.org/wiki/English_language">English</a>, thanks in part to the availability of copious annotated resources. Unfortunately, most languages do not enjoy such an abundance of <a href="https://en.wikipedia.org/wiki/Data_(computing)">labeled data</a>. To tackle the sentiment classification problem in low-resource languages without adequate annotated data, we propose an Adversarial Deep Averaging Network (ADAN1) to transfer the knowledge learned from labeled data on a resource-rich source language to low-resource languages where only unlabeled data exist. ADAN has two discriminative branches : a sentiment classifier and an adversarial language discriminator. Both branches take input from a shared feature extractor to learn hidden representations that are simultaneously indicative for the classification task and invariant across languages. Experiments on Chinese and Arabic sentiment classification demonstrate that ADAN significantly outperforms state-of-the-art systems.</abstract>
      <pages>557–570</pages>
      <url hash="ad37da57">Q18-1039</url>
      <video href="https://vimeo.com/306129914" />
      <bibkey>chen-etal-2018-adversarial</bibkey>
      <pwccode url="https://github.com/ccsasuke/adan" additional="true">ccsasuke/adan</pwccode>
    </paper>
    <paper id="41">
      <title>Data Statements for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a> : Toward Mitigating System Bias and Enabling Better Science</title>
      <author><first>Emily M.</first><last>Bender</last></author>
      <author><first>Batya</first><last>Friedman</last></author>
      <doi>10.1162/tacl_a_00041</doi>
      <abstract>In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of <a href="https://en.wikipedia.org/wiki/Technology">technology</a> for other populations. We present a form that <a href="https://en.wikipedia.org/wiki/Statement_(computer_science)">data statements</a> can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.</abstract>
      <pages>587–604</pages>
      <url hash="92d8ab4c">Q18-1041</url>
      <video href="https://vimeo.com/359686057" />
      <bibkey>bender-friedman-2018-data</bibkey>
    </paper>
    <paper id="44">
      <title>Integrating Weakly Supervised Word Sense Disambiguation into Neural Machine Translation</title>
      <author><first>Xiao</first><last>Pu</last></author>
      <author><first>Nikolaos</first><last>Pappas</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <author><first>Andrei</first><last>Popescu-Belis</last></author>
      <doi>10.1162/tacl_a_00242</doi>
      <abstract>This paper demonstrates that word sense disambiguation (WSD) can improve neural machine translation (NMT) by widening the source context considered when modeling the senses of potentially ambiguous words. We first introduce three adaptive clustering algorithms for WSD, based on <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means</a>, Chinese restaurant processes, and <a href="https://en.wikipedia.org/wiki/Random_walk">random walks</a>, which are then applied to large word contexts represented in a low-rank space and evaluated on SemEval shared-task data. We then learn word vectors jointly with sense vectors defined by our best WSD method, within a state-of-the-art NMT system. We show that the concatenation of these <a href="https://en.wikipedia.org/wiki/Vector_space">vectors</a>, and the use of a sense selection mechanism based on the weighted average of sense vectors, outperforms several baselines including sense-aware ones. This is demonstrated by <a href="https://en.wikipedia.org/wiki/Translation">translation</a> on five language pairs. The improvements are more than 1 BLEU point over strong NMT baselines, +4 % accuracy over all ambiguous nouns and verbs, or +20 % when scored manually over several challenging words.</abstract>
      <pages>635–649</pages>
      <video href="https://vimeo.com/385255818" />
      <url hash="30cfdcb8">Q18-1044</url>
      <bibkey>pu-etal-2018-integrating</bibkey>
      <pwccode url="https://github.com/idiap/sense_aware_NMT" additional="false">idiap/sense_aware_NMT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="46">
      <title>Surface Statistics of an Unknown Language Indicate How to Parse It</title>
      <author><first>Dingquan</first><last>Wang</last></author>
      <author><first>Jason</first><last>Eisner</last></author>
      <doi>10.1162/tacl_a_00248</doi>
      <abstract>We introduce a novel <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> for delexicalized dependency parsing in a <a href="https://en.wikipedia.org/wiki/Programming_language">new language</a>. We show that useful features of the target language can be extracted automatically from an unparsed corpus, which consists only of gold part-of-speech (POS) sequences. Providing these <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> to our neural parser enables <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> to parse sequences like those in the corpus. Strikingly, our <a href="https://en.wikipedia.org/wiki/System">system</a> has no supervision in the target language. Rather, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> is a multilingual system that is trained end-to-end on a variety of other languages, so <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> learns a <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extractor</a> that works well. We show experimentally across multiple languages : (1) Features computed from the unparsed corpus improve parsing accuracy. (2) Including thousands of <a href="https://en.wikipedia.org/wiki/Synthetic_language">synthetic languages</a> in the training yields further improvement. (3) Despite being computed from unparsed corpora, our learned task-specific features beat previous work’s interpretable typological features that require parsed corpora or expert categorization of the language. Our best method improved attachment scores on held-out test languages by an average of 5.6 percentage points over past work that does not inspect the unparsed data (McDonald et al., 2011), and by 20.7 points over past grammar induction work that does not use training languages (Naseem et al., 2010).</abstract>
      <pages>667–685</pages>
      <url hash="3143cc42">Q18-1046</url>
      <bibkey>wang-eisner-2018-surface</bibkey>
    </paper>
    <paper id="47">
      <title>Attentive Convolution : Equipping CNNs with RNN-style Attention Mechanisms<fixed-case>CNN</fixed-case>s with <fixed-case>RNN</fixed-case>-style Attention Mechanisms</title>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <doi>10.1162/tacl_a_00249</doi>
      <abstract>In <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, convolutional neural networks (CNNs) have benefited less than recurrent neural networks (RNNs) from attention mechanisms. We hypothesize that this is because the <a href="https://en.wikipedia.org/wiki/Attention">attention</a> in CNNs has been mainly implemented as attentive pooling (i.e., it is applied to pooling) rather than as attentive convolution (i.e., it is integrated into <a href="https://en.wikipedia.org/wiki/Convolution">convolution</a>). Convolution is the differentiator of CNNs in that it can powerfully model the higher-level representation of a word by taking into account its local fixed-size context in the input text tx. In this work, we propose an attentive convolution network, ATTCONV. It extends the context scope of the convolution operation, deriving higher-level features for a word not only from local context, but also from information extracted from nonlocal context by the attention mechanism commonly used in RNNs. This nonlocal context can come (i) from parts of the input text tx that are distant or (ii) from extra (i.e., external) contexts ty. Experiments on sentence modeling with zero-context (sentiment analysis), single-context (textual entailment) and multiple-context (claim verification) demonstrate the effectiveness of ATTCONV in sentence representation learning with the incorporation of context. In particular, attentive convolution outperforms attentive pooling and is a strong competitor to popular attentive RNNs.1</abstract>
      <pages>687–702</pages>
      <url hash="d75d8b64">Q18-1047</url>
      <bibkey>yin-schutze-2018-attentive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    </volume>
</collection>