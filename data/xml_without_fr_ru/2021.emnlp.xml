<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.emnlp">
  <volume id="main" ingest-date="2021-11-03">
    <meta>
      <booktitle>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</booktitle>
      <editor><first>Marie-Francine</first><last>Moens</last></editor>
      <editor><first>Xuanjing</first><last>Huang</last></editor>
      <editor><first>Lucia</first><last>Specia</last></editor>
      <editor><first>Scott Wen-tau</first><last>Yih</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online and Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="45ef6613">2021.emnlp-main.0</url>
      <bibkey>emnlp-2021</bibkey>
    </frontmatter>
    <paper id="11">
      <title>Multiplex Graph Neural Network for Extractive Text Summarization</title>
      <author><first>Baoyu</first><last>Jing</last></author>
      <author><first>Zeyu</first><last>You</last></author>
      <author><first>Tao</first><last>Yang</last></author>
      <author><first>Wei</first><last>Fan</last></author>
      <author><first>Hanghang</first><last>Tong</last></author>
      <pages>133–139</pages>
      <abstract>Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, <a href="https://en.wikipedia.org/wiki/Sentence_embedding">sentence embedding</a> plays an important role. Recent studies have leveraged <a href="https://en.wikipedia.org/wiki/Graph_theory">graph neural networks</a> to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a> and natural connection relationships), nor model intra-sentential relationships (e.g, <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a> and <a href="https://en.wikipedia.org/wiki/Syntax">syntactic relationship</a> among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on the CNN / DailyMail benchmark dataset to demonstrate effectiveness of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a>.</abstract>
      <url hash="3c8bcdb2">2021.emnlp-main.11</url>
      <bibkey>jing-etal-2021-multiplex</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.11</doi>
    </paper>
    <paper id="14">
      <title>Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context</title>
      <author><first>Xinnian</first><last>Liang</last></author>
      <author><first>Shuangzhi</first><last>Wu</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>155–164</pages>
      <abstract>Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we propose a novel method for UKE, where local and global contexts are jointly modeled. From a global view, we calculate the similarity between a certain phrase and the whole document in the <a href="https://en.wikipedia.org/wiki/Vector_space">vector space</a> as transitional embedding based models do. In terms of the local view, we first build a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a> based on the document where phrases are regarded as vertices and the <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">edges</a> are similarities between vertices. Then, we proposed a new centrality computation method to capture local salient information based on the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a>. Finally, we further combine the modeling of global and local context for <a href="https://en.wikipedia.org/wiki/Ranking">ranking</a>. We evaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010) and compare with existing state-of-the-art models. The results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms most <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> while generalizing better on input documents with different domains and length. Additional ablation study shows that both the local and global information is crucial for unsupervised keyphrase extraction tasks.</abstract>
      <url hash="ebe6c57b">2021.emnlp-main.14</url>
      <bibkey>liang-etal-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.14</doi>
      <pwccode url="https://github.com/xnliang98/uke_ccrank" additional="false">xnliang98/uke_ccrank</pwccode>
    </paper>
    <paper id="17">
      <title>A Partition Filter Network for Joint Entity and Relation Extraction</title>
      <author><first>Zhiheng</first><last>Yan</last></author>
      <author><first>Chong</first><last>Zhang</last></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <pages>185–197</pages>
      <abstract>In joint entity and relation extraction, existing work either sequentially encode task-specific features, leading to an imbalance in inter-task feature interaction where <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> extracted later have no direct contact with those that come first. Or they encode entity features and relation features in a parallel manner, meaning that feature representation learning for each task is largely independent of each other except for input sharing. We propose a partition filter network to model two-way interaction between tasks properly, where <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">feature encoding</a> is decomposed into two steps : partition and <a href="https://en.wikipedia.org/wiki/Filter_(signal_processing)">filter</a>. In our <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a>, we leverage two gates : entity and relation gate, to segment <a href="https://en.wikipedia.org/wiki/Neuron">neurons</a> into two task partitions and one shared partition. The shared partition represents inter-task information valuable to both tasks and is evenly shared across two tasks to ensure proper two-way interaction. The task partitions represent intra-task information and are formed through concerted efforts of both gates, making sure that encoding of task-specific features is dependent upon each other. Experiment results on six public datasets show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performs significantly better than previous approaches. In addition, contrary to what previous work has claimed, our auxiliary experiments suggest that relation prediction is contributory to named entity prediction in a non-negligible way. The source code can be found at https://github.com/Coopercoppers/PFN.</abstract>
      <url hash="5183a8fd">2021.emnlp-main.17</url>
      <bibkey>yan-etal-2021-partition</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.17</doi>
      <pwccode url="https://github.com/Coopercoppers/PFN" additional="false">Coopercoppers/PFN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2004">ACE 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2005">ACE 2005</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="19">
      <title>Beta Distribution Guided Aspect-aware Graph for Aspect Category Sentiment Analysis with Affective Knowledge</title>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Hang</first><last>Su</last></author>
      <author><first>Rongdi</first><last>Yin</last></author>
      <author><first>Lin</first><last>Gui</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Qin</first><last>Zhao</last></author>
      <author><first>Xiaoqi</first><last>Yu</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>208–218</pages>
      <abstract>In this paper, we investigate the Aspect Category Sentiment Analysis (ACSA) task from a novel perspective by exploring a Beta Distribution guided aspect-aware graph construction based on external knowledge. That is, we are no longer entangled about how to laboriously search the sentiment clues for coarse-grained aspects from the context, but how to preferably find the words highly related to the <a href="https://en.wikipedia.org/wiki/Grammatical_aspect">aspects</a> in the context and determine their importance based on the public knowledge base. In this way, the contextual sentiment clues can be explicitly tracked in ACSA for the <a href="https://en.wikipedia.org/wiki/Grammatical_aspect">aspects</a> in the light of these aspect-related words. To be specific, we first regard each aspect as a pivot to derive aspect-aware words that are highly related to the <a href="https://en.wikipedia.org/wiki/Grammatical_aspect">aspect</a> from external affective commonsense knowledge. Then, we employ <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta Distribution</a> to educe the aspect-aware weight, which reflects the importance to the aspect, for each aspect-aware word. Afterward, the aspect-aware words are served as the substitutes of the coarse-grained aspect to construct <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a> for leveraging the aspect-related contextual sentiment dependencies in ACSA. Experiments on 6 benchmark datasets show that our approach significantly outperforms the state-of-the-art baseline methods.</abstract>
      <url hash="ae82f426">2021.emnlp-main.19</url>
      <attachment type="Software" hash="48239545">2021.emnlp-main.19.Software.zip</attachment>
      <bibkey>liang-etal-2021-beta</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.19</doi>
      <pwccode url="https://github.com/binliang-nlp/aagcn-acsa" additional="false">binliang-nlp/aagcn-acsa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="21">
      <title>Improving Multimodal fusion via Mutual Dependency Maximisation</title>
      <author><first>Pierre</first><last>Colombo</last></author>
      <author><first>Emile</first><last>Chapuis</last></author>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>231–245</pages>
      <abstract>Multimodal sentiment analysis is a trending area of research, and multimodal fusion is one of its most active topic. Acknowledging humans communicate through a variety of <a href="https://en.wikipedia.org/wiki/Communication_channel">channels</a> (i.e visual, acoustic, linguistic), multimodal systems aim at integrating different <a href="https://en.wikipedia.org/wiki/Unimodality">unimodal representations</a> into a synthetic one. So far, a consequent effort has been made on developing <a href="https://en.wikipedia.org/wiki/Computer_architecture">complex architectures</a> allowing the fusion of these <a href="https://en.wikipedia.org/wiki/Modularity">modalities</a>. However, such systems are mainly trained by minimising simple <a href="https://en.wikipedia.org/wiki/Loss_function">losses</a> such as L_1 or <a href="https://en.wikipedia.org/wiki/Cross-entropy">cross-entropy</a>. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to 4.3 on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets : CMU-MOSI and CMU-MOSEI. Our method not only achieves a new SOTA on both <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> but also produces <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>.<tex-math>L_1</tex-math> or cross-entropy. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to 4.3 on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets: CMU-MOSI and CMU-MOSEI. Our method not only achieves a new SOTA on both datasets but also produces representations that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the model.</abstract>
      <url hash="939c7b0b">2021.emnlp-main.21</url>
      <bibkey>colombo-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.21</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
    </paper>
    <paper id="23">
      <title>Progressive Self-Training with <a href="https://en.wikipedia.org/wiki/Discriminator">Discriminator</a> for Aspect Term Extraction</title>
      <author><first>Qianlong</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Wen</last></author>
      <author><first>Qin</first><last>Zhao</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>257–268</pages>
      <abstract>Aspect term extraction aims to extract <a href="https://en.wikipedia.org/wiki/Aspect_(grammar)">aspect terms</a> from a review sentence that users have expressed opinions on. One of the remaining challenges for aspect term extraction resides in the lack of sufficient <a href="https://en.wikipedia.org/wiki/Annotation">annotated data</a>. While self-training is potentially an effective method to address this issue, the pseudo-labels it yields on unlabeled data could induce <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a>. In this paper, we use two means to alleviate the noise in the pseudo-labels. One is that inspired by the curriculum learning, we refine the conventional self-training to progressive self-training. Specifically, the base model infers pseudo-labels on a progressive subset at each iteration, where samples in the <a href="https://en.wikipedia.org/wiki/Subset">subset</a> become harder and more numerous as the iteration proceeds. The other is that we use a <a href="https://en.wikipedia.org/wiki/Discriminator">discriminator</a> to filter the noisy pseudo-labels. Experimental results on four SemEval datasets show that our model significantly outperforms the previous baselines and achieves state-of-the-art performance.</abstract>
      <url hash="9ceb2642">2021.emnlp-main.23</url>
      <bibkey>wang-etal-2021-progressive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.23</doi>
    </paper>
    <paper id="24">
      <title>Reinforced Counterfactual Data Augmentation for Dual Sentiment Classification</title>
      <author><first>Hao</first><last>Chen</last></author>
      <author><first>Rui</first><last>Xia</last></author>
      <author><first>Jianfei</first><last>Yu</last></author>
      <pages>269–278</pages>
      <abstract>Data augmentation and adversarial perturbation approaches have recently achieved promising results in solving the over-fitting problem in many natural language processing (NLP) tasks including sentiment classification. However, existing studies aimed to improve the generalization ability by augmenting the training data with synonymous examples or adding random noises to <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>, which can not address the spurious association problem. In this work, we propose an end-to-end reinforcement learning framework, which jointly performs counterfactual data generation and dual sentiment classification. Our approach has three characteristics:1) the generator automatically generates massive and diverse antonymous sentences ; 2) the <a href="https://en.wikipedia.org/wiki/Discriminator">discriminator</a> contains a original-side sentiment predictor and an antonymous-side sentiment predictor, which jointly evaluate the quality of the generated sample and help the generator iteratively generate higher-quality antonymous samples ; 3) the <a href="https://en.wikipedia.org/wiki/Discriminator">discriminator</a> is directly used as the final sentiment classifier without the need to build an extra one. Extensive experiments show that our approach outperforms strong data augmentation baselines on several benchmark sentiment classification datasets. Further analysis confirms our approach’s advantages in generating more diverse training samples and solving the spurious association problem in sentiment classification.</abstract>
      <url hash="83269a66">2021.emnlp-main.24</url>
      <bibkey>chen-etal-2021-reinforced</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.24</doi>
      <pwccode url="https://github.com/nustm/rcda" additional="false">nustm/rcda</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="27">
      <title>(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys</title>
      <author><first>Kenneth</first><last>Joseph</last></author>
      <author><first>Sarah</first><last>Shugars</last></author>
      <author><first>Ryan</first><last>Gallagher</last></author>
      <author><first>Jon</first><last>Green</last></author>
      <author><first>Alexi</first><last>Quintana Mathé</last></author>
      <author><first>Zijian</first><last>An</last></author>
      <author><first>David</first><last>Lazer</last></author>
      <pages>312–324</pages>
      <abstract>Stance detection, which aims to determine whether an individual is for or against a target concept, promises to uncover <a href="https://en.wikipedia.org/wiki/Public_opinion">public opinion</a> from large streams of <a href="https://en.wikipedia.org/wiki/Social_media">social media data</a>. Yet even human annotation of social media content does not always capture stance as measured by <a href="https://en.wikipedia.org/wiki/Opinion_poll">public opinion polls</a>. We demonstrate this by directly comparing an individual’s self-reported stance to the stance inferred from their social media data. Leveraging a longitudinal public opinion survey with respondent Twitter handles, we conducted this comparison for 1,129 individuals across four salient targets. We find that <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> is high for both Pro’’ and Anti’’ stance classifications but precision is variable in a number of cases. We identify three factors leading to the disconnect between text and author stance : temporal inconsistencies, differences in constructs, and <a href="https://en.wikipedia.org/wiki/Observational_error">measurement errors</a> from both survey respondents and annotators. By presenting a <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> for assessing the limitations of stance detection models, this work provides important insight into what stance detection truly measures.</abstract>
      <url hash="6f707f4e">2021.emnlp-main.27</url>
      <attachment type="Software" hash="048b3fc4">2021.emnlp-main.27.Software.zip</attachment>
      <bibkey>joseph-etal-2021-mis</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.27</doi>
    </paper>
    <paper id="30">
      <title>Distilling <a href="https://en.wikipedia.org/wiki/Context_(language_use)">Linguistic Context</a> for Language Model Compression</title>
      <author><first>Geondo</first><last>Park</last></author>
      <author><first>Gyeongman</first><last>Kim</last></author>
      <author><first>Eunho</first><last>Yang</last></author>
      <pages>364–378</pages>
      <abstract>A computationally expensive and memory intensive <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> lies behind the recent success of language representation learning. Knowledge distillation, a major technique for deploying such a vast <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> in resource-scarce environments, transfers the knowledge on individual word representations learned without restrictions. In this paper, inspired by the recent observations that language representations are relatively positioned and have more semantic knowledge as a whole, we present a new knowledge distillation objective for language representation learning that transfers the contextual knowledge via two types of relationships across representations : Word Relation and Layer Transforming Relation. Unlike other recent <a href="https://en.wikipedia.org/wiki/Distillation">distillation techniques</a> for the <a href="https://en.wikipedia.org/wiki/Language_model">language models</a>, our contextual distillation does not have any restrictions on architectural changes between teacher and student. We validate the effectiveness of our method on challenging benchmarks of language understanding tasks, not only in architectures of various sizes but also in combination with DynaBERT, the recently proposed adaptive size pruning method.</abstract>
      <url hash="af511c21">2021.emnlp-main.30</url>
      <bibkey>park-etal-2021-distilling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.30</doi>
      <pwccode url="https://github.com/geondopark/ckd" additional="false">geondopark/ckd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="31">
      <title>Dynamic Knowledge Distillation for Pre-trained Language Models</title>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Shuhuai</first><last>Ren</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>379–389</pages>
      <abstract>Knowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency. We explore the dynamical adjustments on three aspects : teacher model adoption, data selection, and KD objective adaptation. Experimental results show that (1) proper selection of teacher model can boost the performance of student model ; (2) conducting KD with 10 % informative instances achieves comparable performance while greatly accelerates the training ; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective. We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods.</abstract>
      <url hash="d3f2ade9">2021.emnlp-main.31</url>
      <bibkey>li-etal-2021-dynamic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.31</doi>
      <pwccode url="https://github.com/lancopku/dynamickd" additional="false">lancopku/dynamickd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="36">
      <title>Graph Based Network with Contextualized Representations of Turns in Dialogue</title>
      <author><first>Bongseok</first><last>Lee</last></author>
      <author><first>Yong Suk</first><last>Choi</last></author>
      <pages>443–455</pages>
      <abstract>Dialogue-based relation extraction (RE) aims to extract relation(s) between two arguments that appear in a dialogue. Because <a href="https://en.wikipedia.org/wiki/Dialogue">dialogues</a> have the characteristics of high personal pronoun occurrences and low <a href="https://en.wikipedia.org/wiki/Information_density">information density</a>, and since most relational facts in <a href="https://en.wikipedia.org/wiki/Dialogue">dialogues</a> are not supported by any single sentence, dialogue-based relation extraction requires a comprehensive understanding of dialogue. In this paper, we propose the TUrn COntext awaRE Graph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way people understand dialogues. In addition, we propose a novel approach which treats the task of emotion recognition in conversations (ERC) as a dialogue-based RE. Experiments on a dialogue-based RE dataset and three ERC datasets demonstrate that our model is very effective in various dialogue-based natural language understanding tasks. In these experiments, TUCORE-GCN outperforms the <a href="https://en.wikipedia.org/wiki/State-of-the-art">state-of-the-art models</a> on most of the <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark datasets</a>. Our code is available at https://github.com/BlackNoodle/TUCORE-GCN.</abstract>
      <url hash="c037cdbb">2021.emnlp-main.36</url>
      <bibkey>lee-choi-2021-graph</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.36</doi>
      <pwccode url="https://github.com/blacknoodle/tucore-gcn" additional="false">blacknoodle/tucore-gcn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dialogre">DialogRE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emorynlp">EmoryNLP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="37">
      <title>Automatically Exposing Problems with Neural Dialog Models</title>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Kenji</first><last>Sagae</last></author>
      <pages>456–470</pages>
      <abstract>Neural dialog models are known to suffer from problems such as generating unsafe and inconsistent responses. Even though these <a href="https://en.wikipedia.org/wiki/Problem_solving">problems</a> are crucial and prevalent, they are mostly manually identified by model designers through interactions. Recently, some research instructs crowdworkers to goad the bots into triggering such problems. However, humans leverage superficial clues such as <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a>, while leaving systematic problems undercover. In this paper, we propose two methods including <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> to automatically trigger a dialog model into generating problematic responses. We show the effect of our methods in exposing safety and contradiction issues with state-of-the-art dialog models.</abstract>
      <url hash="fb2b6f5c">2021.emnlp-main.37</url>
      <bibkey>yu-sagae-2021-automatically</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.37</doi>
      <pwccode url="https://github.com/diandyu/trigger" additional="false">diandyu/trigger</pwccode>
    </paper>
    <paper id="38">
      <title>Event Coreference Data (Almost) for Free : Mining Hyperlinks from Online News<fixed-case>E</fixed-case>vent Coreference Data (Almost) for Free: <fixed-case>M</fixed-case>ining Hyperlinks from Online News</title>
      <author><first>Michael</first><last>Bugert</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>471–491</pages>
      <abstract>Cross-document event coreference resolution (CDCR) is the task of identifying which event mentions refer to the same events throughout a collection of documents. Annotating CDCR data is an arduous and expensive process, explaining why existing corpora are small and lack domain coverage. To overcome this bottleneck, we automatically extract event coreference data from <a href="https://en.wikipedia.org/wiki/Hyperlink">hyperlinks</a> in online news : When referring to a significant real-world event, writers often add a <a href="https://en.wikipedia.org/wiki/Hyperlink">hyperlink</a> to another article covering this event. We demonstrate that collecting <a href="https://en.wikipedia.org/wiki/Hyperlink">hyperlinks</a> which point to the same article(s) produces extensive and high-quality CDCR data and create a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> of 2 M documents and 2.7 M silver-standard event mentions called HyperCoref. We evaluate a state-of-the-art system on three CDCR corpora and find that <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on small subsets of HyperCoref are highly competitive, with performance similar to <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on gold-standard data. With our work, we free CDCR research from depending on costly human-annotated training data and open up possibilities for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages.</abstract>
      <url hash="347b18cc">2021.emnlp-main.38</url>
      <bibkey>bugert-gurevych-2021-event</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.38</doi>
      <pwccode url="https://github.com/ukplab/emnlp2021-hypercoref-cdcr" additional="false">ukplab/emnlp2021-hypercoref-cdcr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="39">
      <title>Inducing Stereotypical Character Roles from Plot Structure</title>
      <author><first>Labiba</first><last>Jahan</last></author>
      <author><first>Rahul</first><last>Mittal</last></author>
      <author><first>Mark</first><last>Finlayson</last></author>
      <pages>492–497</pages>
      <abstract>Stereotypical character roles-also known as archetypes or dramatis personae-play an important function in <a href="https://en.wikipedia.org/wiki/Narrative">narratives</a> : they facilitate efficient communication with bundles of default characteristics and associations and ease understanding of those characters’ roles in the overall narrative. We present a fully unsupervised k-means clustering approach for learning stereotypical roles given only structural plot information. We demonstrate the technique on Vladimir Propp’s structural theory of Russian folktales (captured in the extended ProppLearner corpus, with 46 tales), showing that our approach can induce six out of seven of Propp’s dramatis personae with F1 measures of up to 0.70 (0.58 average), with an additional category for minor characters. We have explored various feature sets and variations of a cluster evaluation method. The best-performing feature set comprises <a href="https://en.wikipedia.org/wiki/Plot_(graphics)">plot functions</a>, <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">unigrams</a>, tf-idf weights, and embeddings over coreference chain heads. Roles that are mentioned more often (Hero, Villain), or have clearly distinct plot patterns (Princess) are more strongly differentiated than less frequent or distinct roles (Dispatcher, Helper, Donor). Detailed error analysis suggests that the quality of the coreference chain and plot functions annotations are critical for this task. We provide all our data and code for reproducibility.</abstract>
      <url hash="6d207d87">2021.emnlp-main.39</url>
      <bibkey>jahan-etal-2021-inducing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.39</doi>
    </paper>
    <paper id="43">
      <title>Adversarial Scrubbing of Demographic Information for Text Classification</title>
      <author><first>Somnath</first><last>Basu Roy Chowdhury</last></author>
      <author><first>Sayan</first><last>Ghosh</last></author>
      <author><first>Yiyuan</first><last>Li</last></author>
      <author><first>Junier</first><last>Oliva</last></author>
      <author><first>Shashank</first><last>Srivastava</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <pages>550–562</pages>
      <abstract>Contextual representations learned by language models can often encode undesirable attributes, like demographic associations of the users, while being trained for an unrelated target task. We aim to scrub such undesirable attributes and learn fair representations while maintaining performance on the target <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>. In this paper, we present an adversarial learning framework Adversarial Scrubber (AdS), to debias contextual representations. We perform theoretical analysis to show that our <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> converges without leaking <a href="https://en.wikipedia.org/wiki/Demography">demographic information</a> under certain conditions. We extend previous evaluation techniques by evaluating <a href="https://en.wikipedia.org/wiki/Debiasing">debiasing</a> performance using Minimum Description Length (MDL) probing. Experimental evaluations on 8 datasets show that AdS generates <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> with minimal information about <a href="https://en.wikipedia.org/wiki/Demography">demographic attributes</a> while being maximally informative about the target task.</abstract>
      <url hash="86f63645">2021.emnlp-main.43</url>
      <attachment type="Software" hash="c3c357e6">2021.emnlp-main.43.Software.zip</attachment>
      <bibkey>basu-roy-chowdhury-etal-2021-adversarial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.43</doi>
      <pwccode url="https://github.com/brcsomnath/adversarial-scrubber" additional="false">brcsomnath/adversarial-scrubber</pwccode>
    </paper>
    <paper id="44">
      <title>Open-domain clarification question generation without question examples</title>
      <author><first>Julia</first><last>White</last></author>
      <author><first>Gabriel</first><last>Poesia</last></author>
      <author><first>Robert</first><last>Hawkins</last></author>
      <author><first>Dorsa</first><last>Sadigh</last></author>
      <author><first>Noah</first><last>Goodman</last></author>
      <pages>563–570</pages>
      <abstract>An overarching goal of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> is to enable machines to communicate seamlessly with humans. However, <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> can be ambiguous or unclear. In cases of <a href="https://en.wikipedia.org/wiki/Uncertainty">uncertainty</a>, humans engage in an interactive process known as repair : asking questions and seeking clarification until their uncertainty is resolved. We propose a framework for building a visually grounded question-asking model capable of producing polar (yes-no) clarification questions to resolve misunderstandings in <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a>. Our model uses an expected information gain objective to derive informative questions from an off-the-shelf image captioner without requiring any supervised question-answer data. We demonstrate our model’s ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human answerers.</abstract>
      <url hash="3eac608c">2021.emnlp-main.44</url>
      <bibkey>white-etal-2021-open</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.44</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/shapeworld">ShapeWorld</pwcdataset>
    </paper>
    <paper id="45">
      <title>Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting</title>
      <author><first>Wangchunshu</first><last>Zhou</last></author>
      <author><first>Tao</first><last>Ge</last></author>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Ke</first><last>Xu</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>571–582</pages>
      <abstract>In this paper, we propose Sequence Span Rewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the original text-infilling objective. Compared to the prevalent text infilling objectives for Seq2Seq pre-training, SSR is naturally more consistent with many downstream generation tasks that require <a href="https://en.wikipedia.org/wiki/Rewriting">sentence rewriting</a> (e.g., <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a>, <a href="https://en.wikipedia.org/wiki/Question_answering">question generation</a>, <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">grammatical error correction</a>, and paraphrase generation). We conduct extensive experiments by using SSR to improve the typical Seq2Seq pre-trained model T5 in a continual pre-training setting and show substantial improvements over T5 on various natural language generation tasks.<b>S</b>equence <b>S</b>pan <b>R</b>ewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the original text-infilling objective. Compared to the prevalent text infilling objectives for Seq2Seq pre-training, SSR is naturally more consistent with many downstream generation tasks that require sentence rewriting (e.g., text summarization, question generation, grammatical error correction, and paraphrase generation). We conduct extensive experiments by using SSR to improve the typical Seq2Seq pre-trained model T5 in a continual pre-training setting and show substantial improvements over T5 on various natural language generation tasks.</abstract>
      <url hash="9e568e70">2021.emnlp-main.45</url>
      <bibkey>zhou-etal-2021-improving-sequence</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.45</doi>
      <pwccode url="https://github.com/michaelzhouwang/sequence_span_rewriting" additional="false">michaelzhouwang/sequence_span_rewriting</pwccode>
    </paper>
    <paper id="50">
      <title>Artificial Text Detection via Examining the Topology of Attention Maps</title>
      <author><first>Laida</first><last>Kushnareva</last></author>
      <author><first>Daniil</first><last>Cherniavskii</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <author><first>Serguei</first><last>Barannikov</last></author>
      <author><first>Alexander</first><last>Bernstein</last></author>
      <author><first>Irina</first><last>Piontkovskaya</last></author>
      <author><first>Dmitri</first><last>Piontkovski</last></author>
      <author><first>Evgeny</first><last>Burnaev</last></author>
      <pages>635–649</pages>
      <abstract>The impressive capabilities of recent <a href="https://en.wikipedia.org/wiki/Generative_model">generative models</a> to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text detection, they still lack interpretability and <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> towards unseen <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data Analysis (TDA) which is currently understudied in the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. We empirically show that the features derived from the BERT model outperform count- and neural-based baselines up to 10 % on three common datasets, and tend to be the most robust towards unseen GPT-style generation models as opposed to existing methods. The probing analysis of the <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> reveals their sensitivity to the surface and syntactic properties. The results demonstrate that TDA is a promising line with respect to <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP tasks</a>, specifically the ones that incorporate surface and structural information.</abstract>
      <url hash="60727d92">2021.emnlp-main.50</url>
      <bibkey>kushnareva-etal-2021-artificial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.50</doi>
      <pwccode url="https://github.com/danchern97/tda4atd" additional="false">danchern97/tda4atd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/realnews">RealNews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="52">
      <title>Conditional Poisson Stochastic Beams<fixed-case>P</fixed-case>oisson Stochastic Beams</title>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Afra</first><last>Amini</last></author>
      <author><first>Tim</first><last>Vieira</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>664–681</pages>
      <abstract>Beam search is the default decoding strategy for many sequence generation tasks in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. The set of approximate K-best items returned by the <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> is a useful summary of the distribution for many applications ; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> into a <a href="https://en.wikipedia.org/wiki/Stochastic_process">stochastic process</a> : Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et al. (2019)’s stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.</abstract>
      <url hash="8ed3e952">2021.emnlp-main.52</url>
      <bibkey>meister-etal-2021-conditional</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.52</doi>
    </paper>
    <paper id="54">
      <title>Moral Stories : Situated Reasoning about Norms, Intents, Actions, and their Consequences</title>
      <author><first>Denis</first><last>Emelin</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Jena D.</first><last>Hwang</last></author>
      <author><first>Maxwell</first><last>Forbes</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>698–718</pages>
      <abstract>In <a href="https://en.wikipedia.org/wiki/Social_environment">social settings</a>, much of human behavior is governed by <a href="https://en.wikipedia.org/wiki/Social_norm">unspoken rules of conduct</a> rooted in <a href="https://en.wikipedia.org/wiki/Social_norm">societal norms</a>. For artificial systems to be fully integrated into <a href="https://en.wikipedia.org/wiki/Social_environment">social environments</a>, adherence to such <a href="https://en.wikipedia.org/wiki/Social_norm">norms</a> is a central prerequisite. To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. Moreover, we examine if models can anticipate likely consequences of actions that either observe or violate known norms, or explain why certain actions are preferable by generating relevant norm hypotheses. For this purpose, we introduce Moral Stories, a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines.</abstract>
      <url hash="87967e0e">2021.emnlp-main.54</url>
      <attachment type="Software" hash="75043371">2021.emnlp-main.54.Software.zip</attachment>
      <bibkey>emelin-etal-2021-moral</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.54</doi>
      <pwccode url="https://github.com/demelin/moral_stories" additional="false">demelin/moral_stories</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/moral-stories">Moral Stories</pwcdataset>
    </paper>
    <paper id="56">
      <title>Injecting Entity Types into Entity-Guided Text Generation</title>
      <author><first>Xiangyu</first><last>Dong</last></author>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>734–741</pages>
      <abstract>Recent successes in deep generative modeling have led to significant advances in natural language generation (NLG). Incorporating entities into neural generation models has demonstrated great improvements by assisting to infer the summary topic and to generate coherent content. To enhance the role of <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity</a> in NLG, in this paper, we aim to model the <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity type</a> in the decoding phase to generate contextual words accurately. We develop a novel NLG model to produce a target sequence based on a given list of entities. Our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> has a multi-step decoder that injects the entity types into the process of entity mention generation. Experiments on two public news datasets demonstrate type injection performs better than existing type embedding concatenation baselines.</abstract>
      <url hash="1c5ea289">2021.emnlp-main.56</url>
      <bibkey>dong-etal-2021-injecting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.56</doi>
      <pwccode url="https://github.com/wyu97/InjType" additional="true">wyu97/InjType</pwccode>
    </paper>
    <paper id="59">
      <title>The Impact of <a href="https://en.wikipedia.org/wiki/Positional_notation">Positional Encodings</a> on Multilingual Compression</title>
      <author><first>Vinit</first><last>Ravishankar</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>763–777</pages>
      <abstract>In order to preserve word-order information in a non-autoregressive setting, transformer architectures tend to include positional knowledge, by (for instance) adding positional encodings to token embeddings. Several modifications have been proposed over the sinusoidal positional encodings used in the original transformer architecture ; these include, for instance, separating position encodings and token embeddings, or directly modifying attention weights based on the distance between word pairs. We first show that surprisingly, while these modifications tend to improve monolingual language models, none of them result in better multilingual language models. We then answer why that is : sinusoidal encodings were explicitly designed to facilitate compositionality by allowing <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">linear projections</a> over arbitrary time steps. Higher variances in multilingual training distributions requires higher compression, in which case, <a href="https://en.wikipedia.org/wiki/Compositionality">compositionality</a> becomes indispensable. Learned absolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal embeddings in multilingual settings, but more complex positional encoding architectures lack the <a href="https://en.wikipedia.org/wiki/Inductive_bias">inductive bias</a> to effectively learn cross-lingual alignment. In other words, while sinusoidal positional encodings were designed for monolingual applications, they are particularly useful in multilingual language models.</abstract>
      <url hash="fb003bd3">2021.emnlp-main.59</url>
      <bibkey>ravishankar-sogaard-2021-impact</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.59</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="60">
      <title>Disentangling Representations of Text by Masking Transformers</title>
      <author><first>Xiongyi</first><last>Zhang</last></author>
      <author><first>Jan-Willem</first><last>van de Meent</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <pages>778–791</pages>
      <abstract>Representations from large pretrained models such as BERT encode a range of <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspects. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation ; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, toxicity from dialect in Tweets, and <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> from <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a>. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as   and often better than previously proposed methods based on variational autoencoders and adversarial training.</abstract>
      <url hash="87689b47">2021.emnlp-main.60</url>
      <bibkey>zhang-etal-2021-disentangling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.60</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="62">
      <title>Do Long-Range Language Models Actually Use Long-Range Context?</title>
      <author><first>Simeng</first><last>Sun</last></author>
      <author><first>Kalpesh</first><last>Krishna</last></author>
      <author><first>Andrew</first><last>Mattarella-Micke</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>807–822</pages>
      <abstract>Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> of the past. However, the ways in which such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8 K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2 K tokens) to these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to <a href="https://en.wikipedia.org/wiki/Textbook">textbooks</a> or magazines).</abstract>
      <url hash="dde33f63">2021.emnlp-main.62</url>
      <bibkey>sun-etal-2021-long</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.62</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/pg-19">PG-19</pwcdataset>
    </paper>
    <paper id="63">
      <title>The World of an Octopus : How Reporting Bias Influences a <a href="https://en.wikipedia.org/wiki/Language_model">Language Model</a>’s Perception of Color<fixed-case>T</fixed-case>he <fixed-case>W</fixed-case>orld of an <fixed-case>O</fixed-case>ctopus: <fixed-case>H</fixed-case>ow <fixed-case>R</fixed-case>eporting <fixed-case>B</fixed-case>ias <fixed-case>I</fixed-case>nfluences a <fixed-case>L</fixed-case>anguage <fixed-case>M</fixed-case>odel’s <fixed-case>P</fixed-case>erception of <fixed-case>C</fixed-case>olor</title>
      <author><first>Cory</first><last>Paik</last></author>
      <author><first>Stéphane</first><last>Aroca-Ouellette</last></author>
      <author><first>Alessandro</first><last>Roncone</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>823–835</pages>
      <abstract>Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that <a href="https://en.wikipedia.org/wiki/Reporting_bias">reporting bias</a>, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects ; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human’s perception of color ; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that <a href="https://en.wikipedia.org/wiki/Reporting_bias">reporting bias</a> negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.</abstract>
      <url hash="d5503eb4">2021.emnlp-main.63</url>
      <bibkey>paik-etal-2021-world</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.63</doi>
      <pwccode url="https://github.com/nala-cub/coda" additional="false">nala-cub/coda</pwccode>
    </paper>
    <paper id="66">
      <title>Semantic Novelty Detection in Natural Language Descriptions</title>
      <author><first>Nianzu</first><last>Ma</last></author>
      <author><first>Alexander</first><last>Politowicz</last></author>
      <author><first>Sahisnu</first><last>Mazumder</last></author>
      <author><first>Jiahua</first><last>Chen</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Eric</first><last>Robertson</last></author>
      <author><first>Scott</first><last>Grigsby</last></author>
      <pages>866–882</pages>
      <abstract>This paper proposes to study a fine-grained semantic novelty detection task, which can be illustrated with the following example. It is normal that a person walks a dog in the park, but if someone says A man is walking a chicken in the park, it is novel. Given a set of natural language descriptions of normal scenes, we want to identify descriptions of novel scenes. We are not aware of any existing work that solves the <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a>. Although existing novelty or anomaly detection algorithms are applicable, since they are usually topic-based, they perform poorly on our fine-grained semantic novelty detection task. This paper proposes an effective <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> (called GAT-MA) to solve the <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> and also contributes a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. Experimental evaluation shows that GAT-MA outperforms 11 <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> by large margins.</abstract>
      <url hash="62ac87aa">2021.emnlp-main.66</url>
      <bibkey>ma-etal-2021-semantic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.66</doi>
      <pwccode url="https://github.com/nianzuma/semantic-novelty-detection-in-natural-language-descriptions" additional="false">nianzuma/semantic-novelty-detection-in-natural-language-descriptions</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="67">
      <title>Jump-Starting Item Parameters for Adaptive Language Tests</title>
      <author><first>Arya D.</first><last>McCarthy</last></author>
      <author><first>Kevin P.</first><last>Yancey</last></author>
      <author><first>Geoffrey T.</first><last>LaFlair</last></author>
      <author><first>Jesse</first><last>Egbert</last></author>
      <author><first>Manqian</first><last>Liao</last></author>
      <author><first>Burr</first><last>Settles</last></author>
      <pages>883–899</pages>
      <abstract>A challenge in designing high-stakes language assessments is calibrating the test item difficulties, either a priori or from limited pilot test data. While prior work has addressed ‘cold start’ estimation of item difficulties without piloting, we devise a multi-task generalized linear model with BERT features to jump-start these estimates, rapidly improving their quality with as few as 500 test-takers and a small sample of item exposures (6 each) from a large item bank (4,000 items). Our joint model provides a principled way to compare test-taker proficiency, item difficulty, and language proficiency frameworks like the Common European Framework of Reference (CEFR). This also enables new item difficulty estimates without piloting them first, which in turn limits item exposure and thus enhances test item security. Finally, using operational data from the Duolingo English Test, a high-stakes English proficiency test, we find that the difficulty estimates derived using this method correlate strongly with lexico-grammatical features that correlate with reading complexity.</abstract>
      <url hash="9f922ac6">2021.emnlp-main.67</url>
      <bibkey>mccarthy-etal-2021-jump</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.67</doi>
    </paper>
    <paper id="68">
      <title>Voice Query Auto Completion</title>
      <author><first>Raphael</first><last>Tang</last></author>
      <author><first>Karun</first><last>Kumar</last></author>
      <author><first>Kendra</first><last>Chalkley</last></author>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Liming</first><last>Zhang</last></author>
      <author><first>Wenyan</first><last>Li</last></author>
      <author><first>Gefei</first><last>Yang</last></author>
      <author><first>Yajie</first><last>Mao</last></author>
      <author><first>Junho</first><last>Shin</last></author>
      <author><first>Geoffrey Craig</first><last>Murray</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>900–906</pages>
      <abstract>Query auto completion (QAC) is the task of predicting a search engine user’s final query from their intermediate, incomplete query. In this paper, we extend <a href="https://en.wikipedia.org/wiki/Speech_recognition">QAC</a> to the streaming voice search setting, where automatic speech recognition systems produce intermediate transcriptions as users speak. Naively applying existing methods fails because the intermediate transcriptions often do n’t form prefixes or even substrings of the final transcription. To address this issue, we propose to condition QAC approaches on intermediate transcriptions to complete voice queries. We evaluate our models on a speech-enabled smart television with real-life voice search traffic, finding that this ASR-aware conditioning improves the completion quality. Our best <a href="https://en.wikipedia.org/wiki/Methodology">method</a> obtains an 18 % relative improvement in mean reciprocal rank over previous <a href="https://en.wikipedia.org/wiki/Methodology">methods</a>.</abstract>
      <url hash="65dc89f2">2021.emnlp-main.68</url>
      <bibkey>tang-etal-2021-voice</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.68</doi>
    </paper>
    <paper id="69">
      <title>CoPHE : A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification<fixed-case>C</fixed-case>o<fixed-case>PHE</fixed-case>: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification</title>
      <author><first>Matúš</first><last>Falis</last></author>
      <author><first>Hang</first><last>Dong</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <author><first>Beatrice</first><last>Alex</last></author>
      <pages>907–912</pages>
      <abstract>Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in <a href="https://en.wikipedia.org/wiki/Prior_art">prior art</a> is evaluated with standard <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a>, <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a>, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation of the structured label space in <a href="https://en.wikipedia.org/wiki/Prior_art">prior art</a>, and propose an alternative <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a> based on the depth of the ontology. We propose a set of <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> with previously used <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontological representation</a>.</abstract>
      <url hash="0fedef02">2021.emnlp-main.69</url>
      <bibkey>falis-etal-2021-cophe</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.69</doi>
      <pwccode url="https://github.com/modr00cka/cophe" additional="false">modr00cka/cophe</pwccode>
    </paper>
    <paper id="70">
      <title>Learning Universal Authorship Representations</title>
      <author><first>Rafael A.</first><last>Rivera-Soto</last></author>
      <author><first>Olivia Elizabeth</first><last>Miano</last></author>
      <author><first>Juanita</first><last>Ordonez</last></author>
      <author><first>Barry Y.</first><last>Chen</last></author>
      <author><first>Aleem</first><last>Khan</last></author>
      <author><first>Marcus</first><last>Bishop</last></author>
      <author><first>Nicholas</first><last>Andrews</last></author>
      <pages>913–919</pages>
      <abstract>Determining whether two documents were composed by the same author, also known as authorship verification, has traditionally been tackled using <a href="https://en.wikipedia.org/wiki/Statistics">statistical methods</a>. Recently, authorship representations learned using <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> have been found to outperform alternatives, particularly in large-scale settings involving hundreds of thousands of authors. But do such <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representations</a> learned in a particular domain transfer to other domains? Or are these <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> inherently entangled with domain-specific features? To study these questions, we conduct the first large-scale study of cross-domain transfer for authorship verification considering zero-shot transfers involving three disparate domains : <a href="https://en.wikipedia.org/wiki/Amazon_(company)">Amazon reviews</a>, <a href="https://en.wikipedia.org/wiki/Fan_fiction">fanfiction short stories</a>, and <a href="https://en.wikipedia.org/wiki/Reddit">Reddit comments</a>. We find that although a surprising degree of transfer is possible between certain domains, it is not so successful between others. We examine properties of these domains that influence <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a> and propose simple but effective methods to improve transfer.</abstract>
      <url hash="cba82a32">2021.emnlp-main.70</url>
      <bibkey>rivera-soto-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.70</doi>
    </paper>
    <paper id="71">
      <title>Predicting emergent linguistic compositions through time : Syntactic frame extension via multimodal chaining</title>
      <author><first>Lei</first><last>Yu</last></author>
      <author><first>Yang</first><last>Xu</last></author>
      <pages>920–931</pages>
      <abstract>Natural language relies on a finite lexicon to express an unbounded set of emerging ideas. One result of this tension is the formation of new compositions, such that existing linguistic units can be combined with emerging items into novel expressions. We develop a framework that exploits the cognitive mechanisms of chaining and multimodal knowledge to predict emergent compositional expressions through time. We present the syntactic frame extension model (SFEM) that draws on the theory of chaining and knowledge from percept, concept, and language to infer how verbs extend their frames to form new compositions with existing and novel nouns. We evaluate SFEM rigorously on the 1) modalities of knowledge and 2) categorization models of chaining, in a syntactically parsed English corpus over the past 150 years. We show that multimodal SFEM predicts newly emerged verb syntax and arguments substantially better than competing models using purely linguistic or unimodal knowledge. We find support for an exemplar view of chaining as opposed to a prototype view and reveal how the joint approach of multimodal chaining may be fundamental to the creation of literal and figurative language uses including <a href="https://en.wikipedia.org/wiki/Metaphor">metaphor</a> and <a href="https://en.wikipedia.org/wiki/Metonymy">metonymy</a>.</abstract>
      <url hash="4d22e181">2021.emnlp-main.71</url>
      <bibkey>yu-xu-2021-predicting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.71</doi>
      <pwccode url="https://github.com/jadeleiyu/frame_extension" additional="false">jadeleiyu/frame_extension</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="74">
      <title>Revisiting the Uniform Information Density Hypothesis<fixed-case>U</fixed-case>niform <fixed-case>I</fixed-case>nformation <fixed-case>D</fixed-case>ensity Hypothesis</title>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Patrick</first><last>Haller</last></author>
      <author><first>Lena</first><last>Jäger</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>963–980</pages>
      <abstract>The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on <a href="https://en.wikipedia.org/wiki/Language_production">language production</a> have been well explored, the <a href="https://en.wikipedia.org/wiki/Hypothesis">hypothesis</a> potentially makes predictions about <a href="https://en.wikipedia.org/wiki/Sentence_processing">language comprehension</a> and <a href="https://en.wikipedia.org/wiki/Linguistic_prescription">linguistic acceptability</a> as well. Further, it is unclear how uniformity in a linguistic signalor lack thereofshould be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID’s predictions. For <a href="https://en.wikipedia.org/wiki/Acceptability">acceptability judgments</a>, we find clearer evidence that non-uniformity in <a href="https://en.wikipedia.org/wiki/Information_density">information density</a> is predictive of lower <a href="https://en.wikipedia.org/wiki/Acceptability">acceptability</a>. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or documenta finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.</abstract>
      <url hash="75220e7a">2021.emnlp-main.74</url>
      <bibkey>meister-etal-2021-revisiting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.74</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-stories">Natural Stories</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="76">
      <title>Monitoring geometrical properties of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> for detecting the emergence of new topics.</title>
      <author><first>Clément</first><last>Christophe</last></author>
      <author><first>Julien</first><last>Velcin</last></author>
      <author><first>Jairo</first><last>Cugliari</last></author>
      <author><first>Manel</first><last>Boumghar</last></author>
      <author><first>Philippe</first><last>Suignard</last></author>
      <pages>994–1003</pages>
      <abstract>Slow emerging topic detection is a task between event detection, where we aggregate behaviors of different words on short period of time, and <a href="https://en.wikipedia.org/wiki/Evolutionary_linguistics">language evolution</a>, where we monitor their long term evolution. In this work, we tackle the problem of early detection of slowly emerging new topics. To this end, we gather evidence of <a href="https://en.wikipedia.org/wiki/Weak_signals">weak signals</a> at the word level. We propose to monitor the behavior of words representation in an <a href="https://en.wikipedia.org/wiki/Embedding">embedding space</a> and use one of its <a href="https://en.wikipedia.org/wiki/Geometry">geometrical properties</a> to characterize the <a href="https://en.wikipedia.org/wiki/Emergence">emergence of topics</a>. As <a href="https://en.wikipedia.org/wiki/Evaluation">evaluation</a> is typically hard for this kind of task, we present a framework for quantitative evaluation and show positive results that outperform state-of-the-art methods. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is evaluated on two public datasets of press and scientific articles.</abstract>
      <url hash="fc15f8ce">2021.emnlp-main.76</url>
      <bibkey>christophe-etal-2021-monitoring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.76</doi>
    </paper>
    <paper id="80">
      <title>Neural Attention-Aware Hierarchical Topic Model</title>
      <author><first>Yuan</first><last>Jin</last></author>
      <author><first>He</first><last>Zhao</last></author>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Lan</first><last>Du</last></author>
      <author><first>Wray</first><last>Buntine</last></author>
      <pages>1042–1052</pages>
      <abstract>Neural topic models (NTMs) apply <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a> to topic modelling. Despite their success, NTMs generally ignore two important aspects : (1) only document-level word count information is utilized for the training, while more fine-grained sentence-level information is ignored, and (2) external semantic knowledge regarding documents, sentences and words are not exploited for the training. To address these issues, we propose a variational autoencoder (VAE) NTM model that jointly reconstructs the sentence and document word counts using combinations of bag-of-words (BoW) topical embeddings and pre-trained semantic embeddings. The pre-trained embeddings are first transformed into a common latent topical space to align their semantics with the BoW embeddings. Our model also features hierarchical KL divergence to leverage embeddings of each document to regularize those of their sentences, paying more attention to semantically relevant sentences. Both quantitative and qualitative experiments have shown the efficacy of our model in 1) lowering the reconstruction errors at both the sentence and document levels, and 2) discovering more coherent topics from real-world datasets.</abstract>
      <url hash="07933aa6">2021.emnlp-main.80</url>
      <bibkey>jin-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.80</doi>
    </paper>
    <paper id="81">
      <title>Relational World Knowledge Representation in Contextual Language Models : A Review<fixed-case>R</fixed-case>elational <fixed-case>W</fixed-case>orld <fixed-case>K</fixed-case>nowledge <fixed-case>R</fixed-case>epresentation in <fixed-case>C</fixed-case>ontextual <fixed-case>L</fixed-case>anguage <fixed-case>M</fixed-case>odels: <fixed-case>A</fixed-case> <fixed-case>R</fixed-case>eview</title>
      <author><first>Tara</first><last>Safavi</last></author>
      <author><first>Danai</first><last>Koutra</last></author>
      <pages>1053–1067</pages>
      <abstract>Relational knowledge bases (KBs) are commonly used to represent <a href="https://en.wikipedia.org/wiki/World_knowledge">world knowledge</a> in machines. However, while advantageous for their high degree of <a href="https://en.wikipedia.org/wiki/Precision_(computer_science)">precision</a> and <a href="https://en.wikipedia.org/wiki/Interpretability">interpretability</a>, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant human efforts to engineer and maintain. In this review, we take a natural language processing perspective to these limitations, examining how they may be addressed in part by training deep contextual language models (LMs) to internalize and express relational knowledge in more flexible forms. We propose to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision. Our contributions are threefold : (1) We provide a high-level, extensible taxonomy for <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">knowledge representation</a> in LMs ; (2) Within our <a href="https://en.wikipedia.org/wiki/Taxonomy_(general)">taxonomy</a>, we highlight notable models, evaluation tasks, and findings, in order to provide an up-to-date review of current knowledge representation capabilities in LMs ; and (3) We suggest future research directions that build upon the complementary aspects of LMs and KBs as knowledge representations.</abstract>
      <url hash="e8384181">2021.emnlp-main.81</url>
      <bibkey>safavi-koutra-2021-relational</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.81</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="84">
      <title>Contrastive Out-of-Distribution Detection for Pretrained Transformers</title>
      <author><first>Wenxuan</first><last>Zhou</last></author>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <pages>1100–1111</pages>
      <abstract>Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in real-world scenarios, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> often faces out-of-distribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> should identify such instances, and then either reject them during <a href="https://en.wikipedia.org/wiki/Inference">inference</a> or pass them over to <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that handle another <a href="https://en.wikipedia.org/wiki/Probability_distribution">distribution</a>. In this paper, we develop an unsupervised OOD detection method, in which only the in-distribution (ID) data are used in training. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations, such that OOD instances can be better differentiated from ID ones. These OOD instances can then be accurately detected using the <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis distance</a> in the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model’s penultimate layer</a>. We experiment with comprehensive settings and achieve near-perfect OOD detection performance, outperforming <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> drastically. We further investigate the rationales behind the improvement, finding that more compact representations through margin-based contrastive learning bring the improvement. We release our code to the community for future research.</abstract>
      <url hash="5ebfdcb0">2021.emnlp-main.84</url>
      <bibkey>zhou-etal-2021-contrastive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.84</doi>
      <pwccode url="https://github.com/wzhouad/Contra-OOD" additional="false">wzhouad/Contra-OOD</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="85">
      <title>MindCraft : Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks<fixed-case>M</fixed-case>ind<fixed-case>C</fixed-case>raft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks</title>
      <author><first>Cristian-Paul</first><last>Bara</last></author>
      <author><first>Sky</first><last>CH-Wang</last></author>
      <author><first>Joyce</first><last>Chai</last></author>
      <pages>1112–1125</pages>
      <abstract>An ideal integration of <a href="https://en.wikipedia.org/wiki/Autonomous_agent">autonomous agents</a> in a human world implies that they are able to collaborate on human terms. In particular, <a href="https://en.wikipedia.org/wiki/Theory_of_mind">theory of mind</a> plays an important role in maintaining common ground during <a href="https://en.wikipedia.org/wiki/Collaboration">human collaboration</a> and <a href="https://en.wikipedia.org/wiki/Communication">communication</a>. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners’ beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks.</abstract>
      <url hash="f79e0dbd">2021.emnlp-main.85</url>
      <bibkey>bara-etal-2021-mindcraft</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.85</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mindcraft">MindCraft</pwcdataset>
    </paper>
    <paper id="87">
      <title>Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking</title>
      <author><first>Nikita</first><last>Moghe</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <pages>1137–1150</pages>
      <abstract>Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pretrained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual representations through parallel ontologies. In this work, we enhance the transfer learning process by intermediate fine-tuning of pretrained multilingual models, where the multilingual models are fine-tuned with different but related data and/or tasks. Specifically, we use parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks suitable for downstream dialogue tasks. We use only 200 K lines of parallel data for intermediate fine-tuning which is already available for 1782 language pairs. We test our approach on the cross-lingual dialogue state tracking task for the parallel MultiWoZ (English-Chinese, Chinese-English) and Multilingual WoZ (English-German, English-Italian) datasets. We achieve impressive improvements (20 % on joint goal accuracy) on the parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla baseline with only 10 % of the target language task data and zero-shot setup respectively.</abstract>
      <url hash="481ac43c">2021.emnlp-main.87</url>
      <bibkey>moghe-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.87</doi>
      <pwccode url="https://github.com/nikitacs16/xlift_dst" additional="false">nikitacs16/xlift_dst</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="88">
      <title>ConvFiT : Conversational Fine-Tuning of Pretrained Language Models<fixed-case>ConvFiT:</fixed-case> <fixed-case>C</fixed-case>onversational Fine-Tuning of Pretrained Language Models</title>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Pei-Hao</first><last>Su</last></author>
      <author><first>Samuel</first><last>Coope</last></author>
      <author><first>Daniela</first><last>Gerz</last></author>
      <author><first>Paweł</first><last>Budzianowski</last></author>
      <author><first>Iñigo</first><last>Casanueva</last></author>
      <author><first>Nikola</first><last>Mrkšić</last></author>
      <author><first>Tsung-Hsien</first><last>Wen</last></author>
      <pages>1151–1168</pages>
      <abstract>Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose ConvFiT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and task-specialised sentence encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not required, and that LMs can be quickly transformed into effective conversational encoders with much smaller amounts of unannotated data ; 2) pretrained LMs can be fine-tuned into task-specialised sentence encoders, optimised for the fine-grained semantics of a particular task. Consequently, such specialised sentence encoders allow for treating ID as a simple semantic similarity task based on interpretable nearest neighbours retrieval. We validate the robustness and versatility of the ConvFiT framework with such similarity-based inference on the standard ID evaluation sets : ConvFiT-ed LMs achieve state-of-the-art ID performance across the board, with particular gains in the most challenging, few-shot setups.</abstract>
      <url hash="7d2efc00">2021.emnlp-main.88</url>
      <bibkey>vulic-etal-2021-convfit</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.88</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-corpus">Reddit Corpus</pwcdataset>
    </paper>
    <paper id="91">
      <title>Feedback Attribution for Counterfactual Bandit Learning in Multi-Domain Spoken Language Understanding</title>
      <author><first>Tobias</first><last>Falke</last></author>
      <author><first>Patrick</first><last>Lehnen</last></author>
      <pages>1190–1198</pages>
      <abstract>With counterfactual bandit learning, models can be trained based on positive and negative feedback received for historical predictions, with no labeled data needed. Such <a href="https://en.wikipedia.org/wiki/Feedback">feedback</a> is often available in real-world dialog systems, however, the <a href="https://en.wikipedia.org/wiki/Modular_programming">modularized architecture</a> commonly used in large-scale systems prevents the direct application of such <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a>. In this paper, we study the feedback attribution problem that arises when using counterfactual bandit learning for multi-domain spoken language understanding. We introduce an experimental setup to simulate the problem on small-scale public datasets, propose attribution methods inspired by multi-agent reinforcement learning and evaluate them against multiple baselines. We find that while directly using overall feedback leads to disastrous performance, our proposed attribution methods can allow training competitive models from user feedback.</abstract>
      <url hash="db9b6c9c">2021.emnlp-main.91</url>
      <bibkey>falke-lehnen-2021-feedback</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.91</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="93">
      <title>Extend, do n’t rebuild : Phrasing conditional graph modification as autoregressive sequence labelling</title>
      <author><first>Leon</first><last>Weber</last></author>
      <author><first>Jannes</first><last>Münchmeyer</last></author>
      <author><first>Samuele</first><last>Garda</last></author>
      <author><first>Ulf</first><last>Leser</last></author>
      <pages>1213–1224</pages>
      <abstract>Deriving and modifying graphs from natural language text has become a versatile basis technology for <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a> with applications in many subfields, such as <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a> or knowledge graph construction. A recent work used this technique for modifying scene graphs (He et al. 2020), by first encoding the original <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> and then generating the modified one based on this <a href="https://en.wikipedia.org/wiki/Code">encoding</a>. In this work, we show that we can considerably increase performance on this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> by phrasing it as graph extension instead of graph generation. We propose the first <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for the resulting graph extension problem based on autoregressive sequence labelling. On three scene graph modification data sets, this formulation leads to improvements in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> over the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> between 13 and 24 percentage points. Furthermore, we introduce a novel <a href="https://en.wikipedia.org/wiki/Data_set">data set</a> from the <a href="https://en.wikipedia.org/wiki/Biomedicine">biomedical domain</a> which has much larger linguistic variability and more complex <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a> than the scene graph modification data sets. For this <a href="https://en.wikipedia.org/wiki/Data_set">data set</a>, the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the art</a> fails to generalize, while our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can produce meaningful predictions.</abstract>
      <url hash="3065b40e">2021.emnlp-main.93</url>
      <bibkey>weber-etal-2021-extend</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.93</doi>
      <pwccode url="https://github.com/leonweber/extend" additional="false">leonweber/extend</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="95">
      <title>Learning Logic Rules for Document-Level Relation Extraction</title>
      <author><first>Dongyu</first><last>Ru</last></author>
      <author><first>Changzhi</first><last>Sun</last></author>
      <author><first>Jiangtao</first><last>Feng</last></author>
      <author><first>Lin</first><last>Qiu</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Weinan</first><last>Zhang</last></author>
      <author><first>Yong</first><last>Yu</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>1239–1250</pages>
      <abstract>Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> learned through (graph) neural networks, which makes the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> less transparent. To tackle this challenge, in this paper, we propose LogiRE, a novel probabilistic model for document-level relation extraction by learning logic rules. LogiRE treats <a href="https://en.wikipedia.org/wiki/Rule_of_inference">logic rules</a> as <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variables</a> and consists of two <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a> : a rule generator and a relation extractor. The rule generator is to generate <a href="https://en.wikipedia.org/wiki/Rule_of_inference">logic rules</a> potentially contributing to final predictions, and the relation extractor outputs final predictions based on the generated <a href="https://en.wikipedia.org/wiki/Rule_of_inference">logic rules</a>. Those two <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a> can be efficiently optimized with the expectation-maximization (EM) algorithm. By introducing <a href="https://en.wikipedia.org/wiki/Rule_of_inference">logic rules</a> into <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>, LogiRE can explicitly capture long-range dependencies as well as enjoy better interpretation. Empirical results show that significantly outperforms several strong baselines in terms of relation performance and <a href="https://en.wikipedia.org/wiki/Consistency">logical consistency</a>. Our code is available at https://github.com/rudongyu/LogiRE.</abstract>
      <url hash="e34e0708">2021.emnlp-main.95</url>
      <bibkey>ru-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.95</doi>
      <pwccode url="https://github.com/rudongyu/logire" additional="false">rudongyu/logire</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dwie">DWIE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="96">
      <title>A Large-Scale Dataset for Empathetic Response Generation</title>
      <author><first>Anuradha</first><last>Welivita</last></author>
      <author><first>Yubo</first><last>Xie</last></author>
      <author><first>Pearl</first><last>Pu</last></author>
      <pages>1251–1264</pages>
      <abstract>Recent development in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> shows a strong trend towards refining pre-trained models with a domain-specific dataset. This is especially the case for response generation where <a href="https://en.wikipedia.org/wiki/Emotion">emotion</a> plays an important role. However, existing empathetic datasets remain small, delaying research efforts in this area, for example, the development of emotion-aware chatbots. One main technical challenge has been the cost of manually annotating dialogues with the right emotion labels. In this paper, we describe a large-scale silver dataset consisting of 1 M dialogues annotated with 32 fine-grained emotions, eight empathetic response intents, and the Neutral category. To achieve this goal, we have developed a novel data curation pipeline starting with a small seed of manually annotated data and eventually scaling it to a satisfactory size. We compare its quality against a state-of-the-art gold dataset using both offline experiments and visual validation methods. The resultant <a href="https://en.wikipedia.org/wiki/Subroutine">procedure</a> can be used to create similar <a href="https://en.wikipedia.org/wiki/Data_set_(IBM_mainframe)">datasets</a> in the same domain as well as in other domains.</abstract>
      <url hash="478e2132">2021.emnlp-main.96</url>
      <attachment type="Software" hash="aa44c511">2021.emnlp-main.96.Software.zip</attachment>
      <bibkey>welivita-etal-2021-large</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.96</doi>
      <pwccode url="https://github.com/anuradha1992/edos" additional="false">anuradha1992/edos</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/emotionlines">EmotionLines</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="97">
      <title>The Perils of Using <a href="https://en.wikipedia.org/wiki/Mechanical_Turk">Mechanical Turk</a> to Evaluate Open-Ended Text Generation<fixed-case>M</fixed-case>echanical <fixed-case>T</fixed-case>urk to Evaluate Open-Ended Text Generation</title>
      <author><first>Marzena</first><last>Karpinska</last></author>
      <author><first>Nader</first><last>Akoury</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>1265–1285</pages>
      <abstract>Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">English teachers</a> provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text.</abstract>
      <url hash="2e2934c8">2021.emnlp-main.97</url>
      <bibkey>karpinska-etal-2021-perils</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.97</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="98">
      <title>Documenting Large Webtext Corpora : A Case Study on the Colossal Clean Crawled Corpus</title>
      <author><first>Jesse</first><last>Dodge</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Ana</first><last>Marasović</last></author>
      <author><first>William</first><last>Agnew</last></author>
      <author><first>Gabriel</first><last>Ilharco</last></author>
      <author><first>Dirk</first><last>Groeneveld</last></author>
      <author><first>Margaret</first><last>Mitchell</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>1286–1305</pages>
      <abstract>Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger <a href="https://en.wikipedia.org/wiki/Text_corpus">text corpora</a> to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4 ; Raffel et al., 2020), a dataset created by applying a set of <a href="https://en.wikipedia.org/wiki/Filter_(software)">filters</a> to a single snapshot of <a href="https://en.wikipedia.org/wiki/Common_Crawl">Common Crawl</a>. We begin by investigating where the <a href="https://en.wikipedia.org/wiki/Data">data</a> came from, and find a significant amount of text from unexpected sources like <a href="https://en.wikipedia.org/wiki/Patent">patents</a> and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a>) and evaluation examples from other benchmark NLP datasets. To understand the impact of the <a href="https://en.wikipedia.org/wiki/Content-control_software">filters</a> applied to create this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about <a href="https://en.wikipedia.org/wiki/Minority_group">minority individuals</a>. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.</abstract>
      <url hash="5812bef4">2021.emnlp-main.98</url>
      <bibkey>dodge-etal-2021-documenting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.98</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
    </paper>
    <paper id="103">
      <title>Conundrums in Event Coreference Resolution : Making Sense of the State of the Art</title>
      <author><first>Jing</first><last>Lu</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>1368–1380</pages>
      <abstract>Despite recent promising results on the application of span-based models for event reference interpretation, there is a lack of understanding of what has been improved. We present an empirical analysis of a state-of-the-art span-based event reference systems with the goal of providing the general NLP audience with a better understanding of the state of the art and reference researchers with directions for future research.</abstract>
      <url hash="9ed1e538">2021.emnlp-main.103</url>
      <bibkey>lu-ng-2021-conundrums</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.103</doi>
    </paper>
    <paper id="109">
      <title>Fast, Effective, and Self-Supervised : Transforming Masked Language Models into Universal Lexical and Sentence Encoders</title>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>1442–1459</pages>
      <abstract>Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on <a href="https://en.wikipedia.org/wiki/Natural_language_understanding">NLI</a>, sentence similarity, or <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrasing tasks</a> using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts <a href="https://en.wikipedia.org/wiki/Machine_learning">MLMs</a> (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentence-level tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders.</abstract>
      <url hash="d1602175">2021.emnlp-main.109</url>
      <bibkey>liu-etal-2021-fast</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.109</doi>
      <pwccode url="https://github.com/cambridgeltl/mirror-bert" additional="false">cambridgeltl/mirror-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cometa">COMETA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-benchmark">STS Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016">Semantic Textual Similarity (2012 - 2016)</pwcdataset>
    </paper>
    <paper id="110">
      <title>RuleBERT : Teaching Soft Rules to Pre-Trained Language Models<fixed-case>R</fixed-case>ule<fixed-case>BERT</fixed-case>: Teaching Soft Rules to Pre-Trained Language Models</title>
      <author><first>Mohammed</first><last>Saeed</last></author>
      <author><first>Naser</first><last>Ahmadi</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Paolo</first><last>Papotti</last></author>
      <pages>1460–1476</pages>
      <abstract>While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, and we propose a revised <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> that enables the PLM to learn how to predict precise probabilities for the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.</abstract>
      <url hash="5569ca54">2021.emnlp-main.110</url>
      <bibkey>saeed-etal-2021-rulebert</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.110</doi>
      <pwccode url="https://github.com/mhmdsaiid/rulebert" additional="false">mhmdsaiid/rulebert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/babi-1">bAbI</pwcdataset>
    </paper>
    <paper id="111">
      <title>Stepmothers are mean and academics are pretentious : What do pretrained language models learn about you?</title>
      <author><first>Rochelle</first><last>Choenni</last></author>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <author><first>Robert</first><last>van Rooij</last></author>
      <pages>1477–1491</pages>
      <abstract>In this paper, we investigate what types of <a href="https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States">stereotypical information</a> are captured by pretrained language models. We present the first dataset comprising stereotypical attributes of a range of social groups and propose a method to elicit <a href="https://en.wikipedia.org/wiki/Stereotype">stereotypes</a> encoded by pretrained language models in an unsupervised fashion. Moreover, we link the emergent stereotypes to their manifestation as basic emotions as a means to study their emotional effects in a more generalized manner. To demonstrate how our methods can be used to analyze emotion and stereotype shifts due to linguistic experience, we use <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> on news sources as a case study. Our experiments expose how attitudes towards different social groups vary across models and how quickly emotions and stereotypes can shift at the fine-tuning stage.</abstract>
      <url hash="7d796f71">2021.emnlp-main.111</url>
      <bibkey>choenni-etal-2021-stepmothers</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.111</doi>
    </paper>
    <paper id="114">
      <title>When <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential privacy</a> meets NLP : The devil is in the detail<fixed-case>NLP</fixed-case>: The devil is in the detail</title>
      <author><first>Ivan</first><last>Habernal</last></author>
      <pages>1522–1528</pages>
      <abstract>Differential privacy provides a formal approach to privacy of individuals. Applications of <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential privacy</a> in various scenarios, such as protecting users’ original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private auto-encoder for <a href="https://en.wikipedia.org/wiki/Rewriting">text rewriting</a> (Krishna et al, 2021). ADePT achieves promising results on downstream tasks while providing tight <a href="https://en.wikipedia.org/wiki/Privacy">privacy guarantees</a>. Our proof reveals that ADePT is not differentially private, thus rendering the experimental results unsubstantiated. We also quantify the impact of the error in its private mechanism, showing that the true sensitivity is higher by at least factor 6 in an optimistic case of a very small encoder’s dimension and that the amount of utterances that are not privatized could easily reach 100 % of the entire dataset. Our intention is neither to criticize the authors, nor the peer-reviewing process, but rather point out that if differential privacy applications in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> rely on formal guarantees, these should be outlined in full and put under detailed scrutiny.</abstract>
      <url hash="cdc5bc96">2021.emnlp-main.114</url>
      <bibkey>habernal-2021-differential</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.114</doi>
      <pwccode url="https://github.com/habernal/emnlp2021-differential-privacy-nlp" additional="false">habernal/emnlp2021-differential-privacy-nlp</pwccode>
    </paper>
    <paper id="115">
      <title>Achieving Model Robustness through Discrete Adversarial Training</title>
      <author><first>Maor</first><last>Ivgi</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>1529–1544</pages>
      <abstract>Discrete adversarial attacks are symbolic perturbations to a language input that preserve the output label but lead to a prediction error. While such attacks have been extensively explored for the purpose of evaluating model robustness, their utility for improving <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> has been limited to offline augmentation only. Concretely, given a trained <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, attacks are used to generate perturbed (adversarial) examples, and the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is re-trained exactly once. In this work, we address this gap and leverage discrete attacks for online augmentation, where adversarial examples are generated at every training step, adapting to the changing nature of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. We propose (i) a new discrete attack, based on <a href="https://en.wikipedia.org/wiki/Best-first_search">best-first search</a>, and (ii) random sampling attacks that unlike prior work are not based on expensive search-based procedures. Surprisingly, we find that <a href="https://en.wikipedia.org/wiki/Simple_random_sample">random sampling</a> leads to impressive gains in <a href="https://en.wikipedia.org/wiki/Robust_statistics">robustness</a>, outperforming the commonly-used offline augmentation, while leading to a speedup at training time of ~10x. Furthermore, online augmentation with search-based attacks justifies the higher <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training cost</a>, significantly improving <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> on three datasets. Last, we show that our new <a href="https://en.wikipedia.org/wiki/Attack_(computing)">attack</a> substantially improves <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> compared to <a href="https://en.wikipedia.org/wiki/Attack_(computing)">prior methods</a>.</abstract>
      <url hash="48e24533">2021.emnlp-main.115</url>
      <bibkey>ivgi-berant-2021-achieving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.115</doi>
      <pwccode url="https://github.com/Mivg/robust_transformers" additional="false">Mivg/robust_transformers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="118">
      <title>How much pretraining data do <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> need to learn syntax?</title>
      <author><first>Laura</first><last>Pérez-Mayos</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <pages>1571–1582</pages>
      <abstract>Transformers-based pretrained language models achieve outstanding results in many well-known NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of the <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>. We explore this impact on the syntactic capabilities of RoBERTa, using <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on incremental sizes of raw text data. First, we use syntactic structural probes to determine whether models pretrained on more data encode a higher amount of syntactic information. Second, we perform a targeted syntactic evaluation to analyze the impact of pretraining data size on the syntactic generalization performance of the models. Third, we compare the performance of the different models on three downstream applications : <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a>, dependency parsing and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost.</abstract>
      <url hash="d7c4b99f">2021.emnlp-main.118</url>
      <bibkey>perez-mayos-etal-2021-much</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.118</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="120">
      <title>Contrastive Explanations for Model Interpretability</title>
      <author><first>Alon</first><last>Jacovi</last></author>
      <author><first>Swabha</first><last>Swayamdipta</last></author>
      <author><first>Shauli</first><last>Ravfogel</last></author>
      <author><first>Yanai</first><last>Elazar</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>1597–1611</pages>
      <abstract>Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> that differentiate two potential decisions are captured. Our modification allows model behavior to consider only contrastive reasoning, and uncover which aspects of the input are useful for and against particular decisions. Our contrastive explanations can additionally answer for which label, and against which alternative label, is a given input feature useful. We produce contrastive explanations via both high-level abstract concept attribution and low-level input token / span attribution for two NLP classification benchmarks. Our findings demonstrate the ability of label-contrastive explanations to provide fine-grained interpretability of model decisions.</abstract>
      <url hash="6c1403c5">2021.emnlp-main.120</url>
      <bibkey>jacovi-etal-2021-contrastive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.120</doi>
      <pwccode url="https://github.com/allenai/contrastive-explanations" additional="false">allenai/contrastive-explanations</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="121">
      <title>On the Transferability of Adversarial Attacks against Neural Text Classifier</title>
      <author><first>Liping</first><last>Yuan</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last></author>
      <author><first>Yi</first><last>Zhou</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>1612–1625</pages>
      <abstract>Deep neural networks are vulnerable to adversarial attacks, where a small perturbation to an input alters the model prediction. In many cases, malicious inputs intentionally crafted for one <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> can fool another <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>. In this paper, we present the first study to systematically investigate the transferability of adversarial examples for text classification models and explore how various factors, including <a href="https://en.wikipedia.org/wiki/Network_architecture">network architecture</a>, <a href="https://en.wikipedia.org/wiki/Lexical_analysis">tokenization scheme</a>, <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a>, and model capacity, affect the transferability of adversarial examples. Based on these studies, we propose a <a href="https://en.wikipedia.org/wiki/Genetic_algorithm">genetic algorithm</a> to find an ensemble of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that can be used to induce adversarial examples to fool almost all existing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Such adversarial examples reflect the defects of the learning process and the data bias in the training set. Finally, we derive word replacement rules that can be used for model diagnostics from these adversarial examples.</abstract>
      <url hash="c0faf0f7">2021.emnlp-main.121</url>
      <bibkey>yuan-etal-2021-transferability</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.121</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="125">
      <title>mT6 : Multilingual Pretrained Text-to-Text Transformer with Translation Pairs<fixed-case>T</fixed-case>6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs</title>
      <author><first>Zewen</first><last>Chi</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Saksham</first><last>Singhal</last></author>
      <author><first>Xian-Ling</first><last>Mao</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Xia</first><last>Song</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>1671–1683</pages>
      <abstract>Multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts, which has shown promising results on many cross-lingual tasks. In this paper, we improve multilingual text-to-text transfer Transformer with translation pairs (mT6). Specifically, we explore three cross-lingual text-to-text pre-training tasks, namely, <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, translation pair span corruption, and translation span corruption. In addition, we propose a partially non-autoregressive objective for text-to-text pre-training. We evaluate the methods on seven multilingual benchmark datasets, including sentence classification, <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, and abstractive summarization. Experimental results show that the proposed <a href="https://en.wikipedia.org/wiki/MT6">mT6</a> improves cross-lingual transferability over mT5.</abstract>
      <url hash="080a730a">2021.emnlp-main.125</url>
      <bibkey>chi-etal-2021-mt6</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.125</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilingua">WikiLingua</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="127">
      <title>Speechformer : Reducing <a href="https://en.wikipedia.org/wiki/Information_loss">Information Loss</a> in Direct Speech Translation</title>
      <author><first>Sara</first><last>Papi</last></author>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>1698–1706</pages>
      <abstract>Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including <a href="https://en.wikipedia.org/wiki/Speech_translation">speech translation</a>. However, <a href="https://en.wikipedia.org/wiki/Transformer">Transformer’s quadratic complexity</a> with respect to the input sequence length prevents its adoption as is with audio signals, which are typically represented by long sequences. Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features. Therefore, potentially useful <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic information</a> is not accessible to higher-level layers in the <a href="https://en.wikipedia.org/wiki/Architecture">architecture</a>. To solve this issue, we propose Speechformer, an architecture that, thanks to reduced memory usage in the attention layers, avoids the initial <a href="https://en.wikipedia.org/wiki/Lossy_compression">lossy compression</a> and aggregates information only at a higher level according to more informed linguistic criteria. Experiments on three language pairs (ende / es / nl) show the efficacy of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and of up to 4.0 BLEU in a low resource scenario.</abstract>
      <url hash="a52d3ec5">2021.emnlp-main.127</url>
      <bibkey>papi-etal-2021-speechformer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.127</doi>
      <pwccode url="https://github.com/sarapapi/fbk-fairseq" additional="false">sarapapi/fbk-fairseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="133">
      <title>Effects of Parameter Norm Growth During Transformer Training : Inductive Bias from Gradient Descent</title>
      <author><first>William</first><last>Merrill</last></author>
      <author><first>Vivek</first><last>Ramanujan</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Roy</first><last>Schwartz</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>1766–1781</pages>
      <abstract>The capacity of <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> like the widely adopted <a href="https://en.wikipedia.org/wiki/Transformer">transformer</a> is known to be very high. Evidence is emerging that they learn successfully due to <a href="https://en.wikipedia.org/wiki/Inductive_bias">inductive bias</a> in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude (_ 2 norm) during <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training</a>, and its implications for the <a href="https://en.wikipedia.org/wiki/Emergence">emergent representations</a> within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">network</a> approximates a discretized network with saturated activation functions. Such saturated networks are known to have a reduced capacity compared to the full network family that can be described in terms of <a href="https://en.wikipedia.org/wiki/Formal_language">formal languages</a> and <a href="https://en.wikipedia.org/wiki/Automata_theory">automata</a>. Our results suggest saturation is a new characterization of an <a href="https://en.wikipedia.org/wiki/Inductive_bias">inductive bias</a> implicit in GD of particular interest for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.<tex-math>\ell_2</tex-math> norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such “saturated” networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.</abstract>
      <url hash="bdb4b222">2021.emnlp-main.133</url>
      <bibkey>merrill-etal-2021-effects</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.133</doi>
      <pwccode url="https://github.com/viking-sudo-rm/norm-growth" additional="false">viking-sudo-rm/norm-growth</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="136">
      <title>Knowledge-Aware Meta-learning for Low-Resource Text Classification</title>
      <author><first>Huaxiu</first><last>Yao</last></author>
      <author><first>Ying-xin</first><last>Wu</last></author>
      <author><first>Maruan</first><last>Al-Shedivat</last></author>
      <author><first>Eric</first><last>Xing</last></author>
      <pages>1814–1821</pages>
      <abstract>Meta-learning has achieved great success in leveraging the historical learned knowledge to facilitate the learning process of the new <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. However, merely learning the knowledge from the historical tasks, adopted by current meta-learning algorithms, may not generalize well to testing tasks when they are not well-supported by training tasks. This paper studies a low-resource text classification problem and bridges the gap between meta-training and meta-testing tasks by leveraging the external knowledge bases. Specifically, we propose KGML to introduce additional <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a> for each sentence learned from the extracted sentence-specific knowledge graph. The extensive experiments on three <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> demonstrate the effectiveness of KGML under both supervised adaptation and unsupervised adaptation settings.</abstract>
      <url hash="5e07393f">2021.emnlp-main.136</url>
      <bibkey>yao-etal-2021-knowledge</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.136</doi>
      <pwccode url="https://github.com/huaxiuyao/KGML" additional="false">huaxiuyao/KGML</pwccode>
    </paper>
    <paper id="138">
      <title>Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning</title>
      <author><first>Seonghyeon</first><last>Ye</last></author>
      <author><first>Jiseon</first><last>Kim</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>1832–1838</pages>
      <abstract>We introduce EfficientCL, a memory-efficient continual pretraining method that applies contrastive learning with novel <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> and curriculum learning. For <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, we stack two types of operation sequentially : cutoff and PCA jittering. While pretraining steps proceed, we apply curriculum learning by incrementing the augmentation degree for each difficulty step. After <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> is finished, contrastive learning is applied on projected embeddings of original and augmented examples. When finetuned on GLUE benchmark, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms baseline models, especially for sentence-level tasks. Additionally, this improvement is capable with only 70 % of <a href="https://en.wikipedia.org/wiki/Computer_data_storage">computational memory</a> compared to the baseline model.</abstract>
      <url hash="fbd8e08f">2021.emnlp-main.138</url>
      <attachment type="Software" hash="cb2d62a0">2021.emnlp-main.138.Software.zip</attachment>
      <bibkey>ye-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.138</doi>
      <pwccode url="https://github.com/vano1205/efficientcl" additional="false">vano1205/efficientcl</pwccode>
    </paper>
    <paper id="140">
      <title>DIALKI : Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization<fixed-case>DIALKI</fixed-case>: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization</title>
      <author><first>Zeqiu</first><last>Wu</last></author>
      <author><first>Bo-Ru</first><last>Lu</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Mari</first><last>Ostendorf</last></author>
      <pages>1852–1863</pages>
      <abstract>Identifying relevant knowledge to be used in conversational systems that are grounded in long documents is critical to effective response generation. We introduce a knowledge identification model that leverages the document structure to provide dialogue-contextualized passage encodings and better locate knowledge relevant to the conversation. An auxiliary loss captures the history of dialogue-document connections. We demonstrate the effectiveness of our model on two document-grounded conversational datasets and provide analyses showing generalization to unseen documents and long dialogue contexts.</abstract>
      <url hash="e3d5606e">2021.emnlp-main.140</url>
      <bibkey>wu-etal-2021-dialki</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.140</doi>
      <pwccode url="https://github.com/ellenmellon/dialki" additional="false">ellenmellon/dialki</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial-1">Doc2Dial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/holl-e">Holl-E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial">doc2dial</pwcdataset>
    </paper>
    <paper id="141">
      <title>Iconary : A Pictionary-Based Game for Testing Multimodal Communication with Drawings and Text</title>
      <author><first>Christopher</first><last>Clark</last></author>
      <author><first>Jordi</first><last>Salvador</last></author>
      <author><first>Dustin</first><last>Schwenk</last></author>
      <author><first>Derrick</first><last>Bonafilia</last></author>
      <author><first>Mark</first><last>Yatskar</last></author>
      <author><first>Eric</first><last>Kolve</last></author>
      <author><first>Alvaro</first><last>Herrasti</last></author>
      <author><first>Jonghyun</first><last>Choi</last></author>
      <author><first>Sachin</first><last>Mehta</last></author>
      <author><first>Sam</first><last>Skjonsberg</last></author>
      <author><first>Carissa</first><last>Schoenick</last></author>
      <author><first>Aaron</first><last>Sarnat</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Aniruddha</first><last>Kembhavi</last></author>
      <author><first>Oren</first><last>Etzioni</last></author>
      <author><first>Ali</first><last>Farhadi</last></author>
      <pages>1864–1886</pages>
      <abstract>Communicating with humans is challenging for <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AIs</a> because it requires a shared understanding of the world, complex <a href="https://en.wikipedia.org/wiki/Semantics_(computer_science)">semantics</a> (e.g., <a href="https://en.wikipedia.org/wiki/Metaphor">metaphors</a> or analogies), and at times <a href="https://en.wikipedia.org/wiki/Gesture_recognition">multi-modal gestures</a> (e.g., pointing with a finger, or an arrow in a diagram). We investigate these challenges in the context of Iconary, a collaborative game of drawing and guessing based on <a href="https://en.wikipedia.org/wiki/Pictionary">Pictionary</a>, that poses a novel challenge for the research community. In Iconary, a Guesser tries to identify a phrase that a Drawer is drawing by composing <a href="https://en.wikipedia.org/wiki/Icon_(computing)">icons</a>, and the Drawer iteratively revises the drawing to help the Guesser in response. This back-and-forth often uses <a href="https://en.wikipedia.org/wiki/Canonical_criticism">canonical scenes</a>, <a href="https://en.wikipedia.org/wiki/Visual_metaphor">visual metaphor</a>, or <a href="https://en.wikipedia.org/wiki/Iconography">icon compositions</a> to express challenging words, making it an ideal test for mixing language and visual / symbolic communication in <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI</a>. We propose <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a> to play Iconary and train them on over 55,000 games between human players. Our <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> are skillful players and are able to employ world knowledge in language models to play with words unseen during training.</abstract>
      <url hash="97743190">2021.emnlp-main.141</url>
      <bibkey>clark-etal-2021-iconary</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.141</doi>
      <pwccode url="https://github.com/allenai/iconary" additional="false">allenai/iconary</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iconary">Iconary</pwcdataset>
    </paper>
    <paper id="142">
      <title>Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems</title>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Wanhao</first><last>Zhou</last></author>
      <author><first>Lingjing</first><last>Kong</last></author>
      <author><first>Fengyu</first><last>Cai</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Boi</first><last>Faltings</last></author>
      <pages>1887–1898</pages>
      <abstract>As the labeling cost for different <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a> in task-oriented dialog (ToD) systems is expensive, a major challenge is to train different <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a> with the least amount of labeled data. Recently, large-scale pre-trained language models, have shown promising results for few-shot learning in ToD. In this paper, we devise a self-training approach to utilize the abundant unlabeled dialog data to further improve state-of-the-art pre-trained models in few-shot learning scenarios for ToD systems. Specifically, we propose a self-training approach that iteratively labels the most confident unlabeled data to train a stronger Student model. Moreover, a new text augmentation technique (GradAug) is proposed to better train the Student by replacing non-crucial tokens using a masked language model. We conduct extensive experiments and present analyses on four downstream tasks in ToD, including intent classification, dialog state tracking, dialog act prediction, and response selection. Empirical results demonstrate that the proposed self-training approach consistently improves state-of-the-art pre-trained models (BERT, ToD-BERT) when only a small number of labeled data are available.</abstract>
      <url hash="13353330">2021.emnlp-main.142</url>
      <bibkey>mi-etal-2021-self</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.142</doi>
      <pwccode url="https://github.com/mifei/st-tod" additional="false">mifei/st-tod</pwccode>
    </paper>
    <paper id="143">
      <title>Contextual Rephrase Detection for Reducing Friction in Dialogue Systems</title>
      <author><first>Zhuoyi</first><last>Wang</last></author>
      <author><first>Saurabh</first><last>Gupta</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <author><first>Xing</first><last>Fan</last></author>
      <author><first>Dingcheng</first><last>Li</last></author>
      <author><first>Alexander Hanbo</first><last>Li</last></author>
      <author><first>Chenlei</first><last>Guo</last></author>
      <pages>1899–1905</pages>
      <abstract>For voice assistants like <a href="https://en.wikipedia.org/wiki/Amazon_Alexa">Alexa</a>, <a href="https://en.wikipedia.org/wiki/Google_Assistant">Google Assistant</a>, and <a href="https://en.wikipedia.org/wiki/Siri">Siri</a>, correctly interpreting users’ intentions is of utmost importance. However, users sometimes experience friction with these assistants, caused by errors from different system components or <a href="https://en.wikipedia.org/wiki/User_error">user errors</a> such as slips of the tongue. Users tend to rephrase their queries until they get a satisfactory response. Rephrase detection is used to identify the rephrases and has long been treated as a task with pairwise input, which does not fully utilize the contextual information (e.g. users’ implicit feedback). To this end, we propose a contextual rephrase detection model ContReph to automatically identify rephrases from multi-turn dialogues. We showcase how to leverage the dialogue context and user-agent interaction signals, including the user’s implicit feedback and the time gap between different turns, which can help significantly outperform the pairwise rephrase detection models.</abstract>
      <url hash="e658da22">2021.emnlp-main.143</url>
      <bibkey>wang-etal-2021-contextual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.143</doi>
    </paper>
    <paper id="146">
      <title>AttentionRank : Unsupervised Keyphrase Extraction using Self and Cross Attentions<fixed-case>A</fixed-case>ttention<fixed-case>R</fixed-case>ank: Unsupervised Keyphrase Extraction using Self and Cross Attentions</title>
      <author><first>Haoran</first><last>Ding</last></author>
      <author><first>Xiao</first><last>Luo</last></author>
      <pages>1919–1928</pages>
      <abstract>Keyword or keyphrase extraction is to identify words or phrases presenting the main topics of a document. This paper proposes the AttentionRank, a hybrid attention model, to identify keyphrases from a document in an unsupervised manner. AttentionRank calculates self-attention and cross-attention using a pre-trained language model. The self-attention is designed to determine the importance of a candidate within the context of a sentence. The cross-attention is calculated to identify the semantic relevance between a candidate and sentences within a document. We evaluate the AttentionRank on three publicly available datasets against seven <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>. The results show that the AttentionRank is an effective and robust unsupervised keyphrase extraction model on both long and short documents. Source code is available on Github.</abstract>
      <url hash="b8c7b71d">2021.emnlp-main.146</url>
      <bibkey>ding-luo-2021-attentionrank</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.146</doi>
      <pwccode url="https://github.com/hd10-iupui/attentionrank" additional="false">hd10-iupui/attentionrank</pwccode>
    </paper>
    <paper id="149">
      <title>Everything Is All It Takes : A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction</title>
      <author><first>Mahsa</first><last>Yarmohammadi</last></author>
      <author><first>Shijie</first><last>Wu</last></author>
      <author><first>Marc</first><last>Marone</last></author>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Seth</first><last>Ebner</last></author>
      <author><first>Guanghui</first><last>Qin</last></author>
      <author><first>Yunmo</first><last>Chen</last></author>
      <author><first>Jialiang</first><last>Guo</last></author>
      <author><first>Craig</first><last>Harman</last></author>
      <author><first>Kenton</first><last>Murray</last></author>
      <author><first>Aaron Steven</first><last>White</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>1950–1967</pages>
      <abstract>Zero-shot cross-lingual information extraction (IE) describes the construction of an IE model for some target language, given existing annotations exclusively in some other language, typically <a href="https://en.wikipedia.org/wiki/English_language">English</a>. While the advance of pretrained multilingual encoders suggests an easy optimism of train on <a href="https://en.wikipedia.org/wiki/English_language">English</a>, run on any language, we find through a thorough exploration and extension of techniques that a combination of approaches, both new and old, leads to better performance than any one cross-lingual strategy in particular. We explore techniques including <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">data projection</a> and self-training, and how different pretrained encoders impact them. We use English-to-Arabic IE as our initial example, demonstrating strong performance in this setting for event extraction, <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a>, and dependency parsing. We then apply <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">data projection</a> and self-training to three tasks across eight target languages. Because no single set of techniques performs the best across all tasks, we encourage practitioners to explore various configurations of the techniques described in this work when seeking to improve on zero-shot training.</abstract>
      <url hash="3d756100">2021.emnlp-main.149</url>
      <bibkey>yarmohammadi-etal-2021-everything</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.149</doi>
      <pwccode url="https://github.com/shijie-wu/crosslingual-nlp" additional="true">shijie-wu/crosslingual-nlp</pwccode>
    </paper>
    <paper id="151">
      <title>Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search</title>
      <author><first>Jialu</first><last>Wang</last></author>
      <author id="yang-liu-umich"><first>Yang</first><last>Liu</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <pages>1995–2008</pages>
      <abstract>Internet search affects people’s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> in <a href="https://en.wikipedia.org/wiki/Image_retrieval">image search</a> in this work : the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both <a href="https://en.wikipedia.org/wiki/Model_(person)">models</a> suffer from severe <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a>. Therefore, we introduce two novel debiasing approaches : an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a> to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30 K benchmarks show that our methods significantly reduce the gender bias in image search models.</abstract>
      <url hash="6880d28d">2021.emnlp-main.151</url>
      <attachment type="Software" hash="ceabaa30">2021.emnlp-main.151.Software.zip</attachment>
      <bibkey>wang-etal-2021-gender</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.151</doi>
      <pwccode url="https://github.com/kuanghuei/SCAN" additional="false">kuanghuei/SCAN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco-captions">COCO Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="152">
      <title>Style Pooling : Automatic Text Style Obfuscation for Improved Classification Fairness</title>
      <author><first>Fatemehsadat</first><last>Mireshghallah</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>2009–2022</pages>
      <abstract>Text style can reveal sensitive attributes of the author (e.g. age and race) to the reader, which can, in turn, lead to privacy violations and bias in both human and algorithmic decisions based on text. For example, the style of writing in job applications might reveal protected attributes of the candidate which could lead to bias in <a href="https://en.wikipedia.org/wiki/Recruitment">hiring decisions</a>, regardless of whether <a href="https://en.wikipedia.org/wiki/Recruitment">hiring decisions</a> are made algorithmically or by humans. We propose a VAE-based framework that obfuscates stylistic features of human-generated text through style transfer, by automatically re-writing the text itself. Critically, our framework operationalizes the notion of obfuscated style in a flexible way that enables two distinct notions of obfuscated style : (1) a minimal notion that effectively intersects the various styles seen in training, and (2) a maximal notion that seeks to obfuscate by adding stylistic features of all sensitive attributes to text, in effect, computing a union of styles. Our style-obfuscation framework can be used for multiple purposes, however, we demonstrate its effectiveness in improving the fairness of downstream classifiers. We also conduct a comprehensive study on style-pooling’s effect on <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a>, semantic consistency, and attribute removal from text, in two and three domain style transfer.</abstract>
      <url hash="e3860074">2021.emnlp-main.152</url>
      <bibkey>mireshghallah-berg-kirkpatrick-2021-style</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.152</doi>
      <pwccode url="https://github.com/mireshghallah/style-pooling" additional="false">mireshghallah/style-pooling</pwccode>
    </paper>
    <paper id="153">
      <title>Modeling Disclosive Transparency in NLP Application Descriptions<fixed-case>NLP</fixed-case> Application Descriptions</title>
      <author><first>Michael</first><last>Saxon</last></author>
      <author><first>Sharon</first><last>Levy</last></author>
      <author><first>Xinyi</first><last>Wang</last></author>
      <author><first>Alon</first><last>Albalak</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>2023–2037</pages>
      <abstract>Broader disclosive transparencytruth and clarity in communication regarding the function of AI systemsis widely considered desirable. Unfortunately, it is a nebulous concept, difficult to both define and quantify. This is problematic, as previous work has demonstrated possible trade-offs and negative consequences to disclosive transparency, such as a confusion effect, where too much information clouds a reader’s understanding of what a system description means. Disclosive transparency’s subjective nature has rendered deep study into these problems and their remedies difficult. To improve this state of affairs, We introduce neural language model-based probabilistic metrics to directly model disclosive transparency, and demonstrate that they correlate with user and expert opinions of system transparency, making them a valid objective proxy. Finally, we demonstrate the use of these metrics in a pilot study quantifying the relationships between <a href="https://en.wikipedia.org/wiki/Transparency_(behavior)">transparency</a>, <a href="https://en.wikipedia.org/wiki/Confusion">confusion</a>, and user perceptions in a corpus of real NLP system descriptions.</abstract>
      <url hash="d4cf9b97">2021.emnlp-main.153</url>
      <bibkey>saxon-etal-2021-modeling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.153</doi>
      <pwccode url="https://github.com/michaelsaxon/disclosive-transparency" additional="false">michaelsaxon/disclosive-transparency</pwccode>
    </paper>
    <paper id="155">
      <title>Fairness-aware Class Imbalanced Learning</title>
      <author><first>Shivashankar</first><last>Subramanian</last></author>
      <author><first>Afshin</first><last>Rahimi</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <pages>2045–2051</pages>
      <abstract>Class imbalance is a common challenge in many NLP tasks, and has clear connections to <a href="https://en.wikipedia.org/wiki/Bias">bias</a>, in that bias in training data often leads to higher <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.</abstract>
      <url hash="6acc4529">2021.emnlp-main.155</url>
      <bibkey>subramanian-etal-2021-fairness</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.155</doi>
    </paper>
    <paper id="157">
      <title>Local Word Discovery for Interactive Transcription</title>
      <author><first>William</first><last>Lane</last></author>
      <author><first>Steven</first><last>Bird</last></author>
      <pages>2058–2067</pages>
      <abstract>Human expertise and the participation of speech communities are essential factors in the success of technologies for low-resource languages. Accordingly, we propose a new computational task which is tuned to the available knowledge and interests in an Indigenous community, and which supports the construction of high quality texts and lexicons. The task is illustrated for <a href="https://en.wikipedia.org/wiki/Kunwinjku_language">Kunwinjku</a>, a morphologically-complex Australian language. We combine a finite state implementation of a published <a href="https://en.wikipedia.org/wiki/Formal_grammar">grammar</a> with a partial lexicon, and apply this to a noisy phone representation of the signal. We locate known lexemes in the signal and use the morphological transducer to build these out into hypothetical, morphologically-complex words for human validation. We show that applying a single iteration of this method results in a relative transcription density gain of 17 %. Further, we find that 75 % of breath groups in the test set receive at least one correct partial or full-word suggestion.</abstract>
      <url hash="3c358880">2021.emnlp-main.157</url>
      <bibkey>lane-bird-2021-local</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.157</doi>
    </paper>
    <paper id="158">
      <title>Segment, Mask, and Predict : Augmenting Chinese Word Segmentation with Self-Supervision<fixed-case>C</fixed-case>hinese Word Segmentation with Self-Supervision</title>
      <author><first>Mieradilijiang</first><last>Maimaiti</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <author><first>Yuanhang</first><last>Zheng</last></author>
      <author><first>Gang</first><last>Chen</last></author>
      <author><first>Kaiyu</first><last>Huang</last></author>
      <author><first>Ji</first><last>Zhang</last></author>
      <author><first>Huanbo</first><last>Luan</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>2068–2077</pages>
      <abstract>Recent state-of-the-art (SOTA) effective neural network methods and fine-tuning methods based on pre-trained models (PTM) have been used in Chinese word segmentation (CWS), and they achieve great results. However, previous works focus on training the <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> with the fixed corpus at every iteration. The intermediate generated information is also valuable. Besides, the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of the previous <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural methods</a> is limited by the large-scale annotated data. There are a few noises in the annotated corpus. Limited efforts have been made by previous studies to deal with such problems. In this work, we propose a self-supervised CWS approach with a straightforward and effective <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a>. First, we train a word segmentation model and use it to generate the segmentation results. Then, we use a revised masked language model (MLM) to evaluate the quality of the segmentation results based on the predictions of the MLM. Finally, we leverage the evaluations to aid the training of the segmenter by improved minimum risk training. Experimental results show that our approach outperforms previous methods on 9 different CWS datasets with single criterion training and multiple criteria training and achieves better robustness.</abstract>
      <url hash="bde8d9c8">2021.emnlp-main.158</url>
      <bibkey>maimaiti-etal-2021-segment</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.158</doi>
    </paper>
    <paper id="160">
      <title>Fast WordPiece Tokenization<fixed-case>W</fixed-case>ord<fixed-case>P</fixed-case>iece Tokenization</title>
      <author><first>Xinying</first><last>Song</last></author>
      <author><first>Alex</first><last>Salcianu</last></author>
      <author><first>Yang</first><last>Song</last></author>
      <author><first>Dave</first><last>Dopson</last></author>
      <author><first>Denny</first><last>Zhou</last></author>
      <pages>2089–2103</pages>
      <abstract>Tokenization is a fundamental preprocessing step for almost all NLP tasks. In this paper, we propose efficient <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> for the WordPiece tokenization used in BERT, from <a href="https://en.wikipedia.org/wiki/Lexical_analysis">single-word tokenization</a> to <a href="https://en.wikipedia.org/wiki/Lexical_analysis">general text (e.g., sentence) tokenization</a>. When tokenizing a single word, WordPiece uses a longest-match-first strategy, known as <a href="https://en.wikipedia.org/wiki/Maximum_matching">maximum matching</a>. The best known algorithms so far are O(n2) (where n is the input length) or O(nm) (where m is the maximum vocabulary token length). We propose a novel <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> whose tokenization complexity is strictly O(n). Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is inspired by the <a href="https://en.wikipedia.org/wiki/Aho–Corasick_algorithm">Aho-Corasick algorithm</a>. We introduce additional linkages on top of the <a href="https://en.wikipedia.org/wiki/Trie">trie</a> built from the <a href="https://en.wikipedia.org/wiki/Vocabulary">vocabulary</a>, allowing smart transitions when the <a href="https://en.wikipedia.org/wiki/Trie">trie matching</a> can not continue. For general text, we further propose an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> that combines pre-tokenization (splitting the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> is 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text on average for general text tokenization.</abstract>
      <url hash="549b9ddf">2021.emnlp-main.160</url>
      <bibkey>song-etal-2021-fast</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.160</doi>
    </paper>
    <paper id="161">
      <title>You should evaluate your <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> on marginal likelihood over tokenisations</title>
      <author><first>Kris</first><last>Cao</last></author>
      <author><first>Laura</first><last>Rimell</last></author>
      <pages>2104–2114</pages>
      <abstract>Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, <a href="https://en.wikipedia.org/wiki/Formal_language">language models</a> should be evaluated on their <a href="https://en.wikipedia.org/wiki/Marginal_likelihood">marginal likelihood</a> over <a href="https://en.wikipedia.org/wiki/Lexical_analysis">tokenisations</a>. We compare different estimators for the <a href="https://en.wikipedia.org/wiki/Marginal_likelihood">marginal likelihood</a> based on <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">sampling</a>, and show that it is feasible to estimate the <a href="https://en.wikipedia.org/wiki/Marginal_likelihood">marginal likelihood</a> with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in <a href="https://en.wikipedia.org/wiki/Uncertainty">perplexity</a> to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness.</abstract>
      <url hash="2fbc818c">2021.emnlp-main.161</url>
      <bibkey>cao-rimell-2021-evaluate</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.161</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
    </paper>
    <paper id="162">
      <title>Broaden the Vision : Geo-Diverse Visual Commonsense Reasoning</title>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Liunian Harold</first><last>Li</last></author>
      <author><first>Ziniu</first><last>Hu</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>2115–2129</pages>
      <abstract>Commonsense is defined as the knowledge on which everyone agrees. However, certain types of <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a> are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models’ ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on <a href="https://en.wikipedia.org/wiki/Videocassette_recorder">VCR</a>, a standard benchmark with images primarily from Western regions. We then evaluate how well the trained <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including <a href="https://en.wikipedia.org/wiki/East_Asia">East Asia</a>, <a href="https://en.wikipedia.org/wiki/South_Asia">South Asia</a>, and <a href="https://en.wikipedia.org/wiki/Africa">Africa</a> is significantly lower than that for <a href="https://en.wikipedia.org/wiki/Western_world">Western region</a>. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that : 1) are concerned with <a href="https://en.wikipedia.org/wiki/Culture">culture-related scenarios</a>, e.g., <a href="https://en.wikipedia.org/wiki/Wedding">weddings</a>, <a href="https://en.wikipedia.org/wiki/Religion">religious activities</a>, and <a href="https://en.wikipedia.org/wiki/Festival">festivals</a> ; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR.</abstract>
      <url hash="af1525c9">2021.emnlp-main.162</url>
      <bibkey>yin-etal-2021-broaden</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.162</doi>
      <pwccode url="https://github.com/wadeyin9712/gd-vcr" additional="false">wadeyin9712/gd-vcr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gd-vcr">GD-VCR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
    </paper>
    <paper id="163">
      <title>Reference-Centric Models for Grounded Collaborative Dialogue</title>
      <author><first>Daniel</first><last>Fried</last></author>
      <author><first>Justin</first><last>Chiu</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>2130–2147</pages>
      <abstract>We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. Our dialogue agent accurately grounds referents from the partner’s utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades. Our agent substantially outperforms the previous <a href="https://en.wikipedia.org/wiki/State_of_the_art">state of the art</a> for the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, obtaining a 20 % relative improvement in successful task completion in self-play evaluations and a 50 % relative improvement in success in human evaluations.</abstract>
      <url hash="6d204d3c">2021.emnlp-main.163</url>
      <bibkey>fried-etal-2021-reference</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.163</doi>
      <pwccode url="https://github.com/dpfried/onecommon" additional="false">dpfried/onecommon</pwccode>
    </paper>
    <paper id="164">
      <title>CrossVQA : Scalably Generating Benchmarks for Systematically Testing VQA Generalization<fixed-case>C</fixed-case>ross<fixed-case>VQA</fixed-case>: Scalably Generating Benchmarks for Systematically Testing <fixed-case>VQA</fixed-case> Generalization</title>
      <author><first>Arjun</first><last>Akula</last></author>
      <author><first>Soravit</first><last>Changpinyo</last></author>
      <author><first>Boqing</first><last>Gong</last></author>
      <author><first>Piyush</first><last>Sharma</last></author>
      <author><first>Song-Chun</first><last>Zhu</last></author>
      <author><first>Radu</first><last>Soricut</last></author>
      <pages>2148–2166</pages>
      <abstract>One challenge in evaluating visual question answering (VQA) models in the cross-dataset adaptation setting is that the distribution shifts are multi-modal, making it difficult to identify if it is the shifts in visual or language features that play a key role. In this paper, we propose a semi-automatic framework for generating disentangled shifts by introducing a controllable visual question-answer generation (VQAG) module that is capable of generating highly-relevant and diverse question-answer pairs with the desired dataset style. We use it to create CrossVQA, a collection of test splits for assessing VQA generalization based on the VQA2, VizWiz, and Open Images datasets. We provide an analysis of our generated datasets and demonstrate its utility by using them to evaluate several state-of-the-art VQA systems. One important finding is that the visual shifts in cross-dataset VQA matter more than the <a href="https://en.wikipedia.org/wiki/Language_shift">language shifts</a>. More broadly, we present a scalable framework for systematically evaluating the machine with little human intervention.</abstract>
      <url hash="cd78b493">2021.emnlp-main.164</url>
      <bibkey>akula-etal-2021-crossvqa</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.164</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vqg">VQG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vizwiz">VizWiz</pwcdataset>
    </paper>
    <paper id="168">
      <title>Neural Path Hunter : Reducing <a href="https://en.wikipedia.org/wiki/Hallucination">Hallucination</a> in Dialogue Systems via Path Grounding</title>
      <author><first>Nouha</first><last>Dziri</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Osmar</first><last>Zaïane</last></author>
      <author><first>Avishek Joey</first><last>Bose</last></author>
      <pages>2197–2214</pages>
      <abstract>Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing <a href="https://en.wikipedia.org/wiki/Hallucination">hallucination</a> of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of <a href="https://en.wikipedia.org/wiki/Hallucination">hallucination</a> followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35 % based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.</abstract>
      <url hash="68faeceb">2021.emnlp-main.168</url>
      <bibkey>dziri-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.168</doi>
      <pwccode url="https://github.com/nouhadziri/neural-path-hunter" additional="false">nouhadziri/neural-path-hunter</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opendialkg">OpenDialKG</pwcdataset>
    </paper>
    <paper id="169">
      <title>Thinking Clearly, Talking Fast : Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems</title>
      <author><first>Yicheng</first><last>Zou</last></author>
      <author><first>Zhihua</first><last>Liu</last></author>
      <author><first>Xingwu</first><last>Hu</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>2215–2226</pages>
      <abstract>Human dialogue contains evolving concepts, and speakers naturally associate multiple concepts to compose a response. However, current dialogue models with the seq2seq framework lack the ability to effectively manage concept transitions and can hardly introduce multiple concepts to responses in a sequential decoding manner. To facilitate a controllable and coherent dialogue, in this work, we devise a concept-guided non-autoregressive model (CG-nAR) for open-domain dialogue generation. The proposed model comprises a multi-concept planning module that learns to identify multiple associated <a href="https://en.wikipedia.org/wiki/Concept">concepts</a> from a concept graph and a customized Insertion Transformer that performs concept-guided non-autoregressive generation to complete a response. The experimental results on two public datasets show that CG-nAR can produce diverse and coherent responses, outperforming state-of-the-art baselines in both automatic and human evaluations with substantially faster inference speed.</abstract>
      <url hash="24cc0603">2021.emnlp-main.169</url>
      <bibkey>zou-etal-2021-thinking</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.169</doi>
      <pwccode url="https://github.com/rowitzou/cg-nar" additional="false">rowitzou/cg-nar</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="170">
      <title>Perspective-taking and <a href="https://en.wikipedia.org/wiki/Pragmatics">Pragmatics</a> for Generating Empathetic Responses Focused on Emotion Causes</title>
      <author><first>Hyunwoo</first><last>Kim</last></author>
      <author><first>Byeongchang</first><last>Kim</last></author>
      <author><first>Gunhee</first><last>Kim</last></author>
      <pages>2227–2240</pages>
      <abstract>Empathy is a complex cognitive ability based on the reasoning of others’ affective states. In order to better understand others and express stronger <a href="https://en.wikipedia.org/wiki/Empathy">empathy</a> in dialogues, we argue that two issues must be tackled at the same time : (i) identifying which word is the cause for the other’s emotion from his or her utterance and (ii) reflecting those specific words in the response generation. However, previous approaches for recognizing emotion cause words in <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">text</a> require sub-utterance level annotations, which can be demanding. Taking inspiration from <a href="https://en.wikipedia.org/wiki/Social_cognition">social cognition</a>, we leverage a generative estimator to infer emotion cause words from utterances with no word-level label. Also, we introduce a novel method based on <a href="https://en.wikipedia.org/wiki/Pragmatics">pragmatics</a> to make dialogue models focus on targeted words in the input during generation. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is applicable to any dialogue models with no additional training on the fly. We show our approach improves multiple best-performing dialogue agents on generating more focused empathetic responses in terms of both automatic and human evaluation.</abstract>
      <url hash="ea891378">2021.emnlp-main.170</url>
      <bibkey>kim-etal-2021-perspective</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.170</doi>
      <pwccode url="https://github.com/skywalker023/focused-empathy" additional="false">skywalker023/focused-empathy</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/emocause">EmoCause</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/empatheticdialogues">EmpatheticDialogues</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reccon">RECCON</pwcdataset>
    </paper>
    <paper id="172">
      <title>CoLV : A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation<fixed-case>C</fixed-case>o<fixed-case>LV</fixed-case>: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation</title>
      <author><first>Haolan</first><last>Zhan</last></author>
      <author><first>Lei</first><last>Shen</last></author>
      <author><first>Hongshen</first><last>Chen</last></author>
      <author><first>Hainan</first><last>Zhang</last></author>
      <pages>2250–2261</pages>
      <abstract>Knowledge-grounded dialogue generation has achieved promising performance with the engagement of external knowledge sources. Typical approaches towards this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> usually perform relatively independent two sub-tasks, i.e., knowledge selection and knowledge-aware response generation. In this paper, in order to improve the diversity of both knowledge selection and knowledge-aware response generation, we propose a collaborative latent variable (CoLV) model to integrate these two aspects simultaneously in separate yet collaborative latent spaces, so as to capture the inherent correlation between knowledge selection and response generation. During generation, our proposed model firstly draws knowledge candidate from the latent space conditioned on the dialogue context, and then samples a response from another collaborative latent space conditioned on both the context and the selected knowledge. Experimental results on two widely-used knowledge-grounded dialogue datasets show that our model outperforms previous methods on both knowledge selection and response generation.</abstract>
      <url hash="a5ef91da">2021.emnlp-main.172</url>
      <bibkey>zhan-etal-2021-colv</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.172</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/holl-e">Holl-E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="176">
      <title>Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks</title>
      <author><first>Qingbin</first><last>Liu</last></author>
      <author><first>Pengfei</first><last>Cao</last></author>
      <author><first>Cao</first><last>Liu</last></author>
      <author><first>Jiansong</first><last>Chen</last></author>
      <author><first>Xunliang</first><last>Cai</last></author>
      <author><first>Fan</first><last>Yang</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>2301–2311</pages>
      <abstract>Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This <a href="https://en.wikipedia.org/wiki/Paradigm">paradigm</a> is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that <a href="https://en.wikipedia.org/wiki/KPN">KPN</a> effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25 % and 8.27 % of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively.</abstract>
      <url hash="68d73367">2021.emnlp-main.176</url>
      <bibkey>liu-etal-2021-domain</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.176</doi>
      <pwccode url="https://github.com/liuqingbin/knowledge-preservation-networks" additional="false">liuqingbin/knowledge-preservation-networks</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="178">
      <title>Different Strokes for Different Folks : Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks</title>
      <author><first>Yao</first><last>Qiu</last></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>2318–2327</pages>
      <abstract>Loading models pre-trained on the large-scale corpus in the general domain and fine-tuning them on specific downstream tasks is gradually becoming a paradigm in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a>. Previous investigations prove that introducing a further pre-training phase between pre-training and fine-tuning phases to adapt the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on the domain-specific unlabeled data can bring positive effects. However, most of these further <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">pre-training</a> works just keep running the conventional <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">pre-training task</a>, e.g., masked language model, which can be regarded as the <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> to bridge the data distribution gap. After observing diverse downstream tasks, we suggest that different <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> may also need a further pre-training phase with appropriate training tasks to bridge the task formulation gap. To investigate this, we carry out a study for improving multiple task-oriented dialogue downstream tasks through designing various <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> at the further pre-training phase. The experiment shows that different downstream tasks prefer different further pre-training tasks, which have intrinsic correlation and most further pre-training tasks significantly improve certain target tasks rather than all. Our investigation indicates that it is of great importance and effectiveness to design appropriate further pre-training tasks modeling specific information that benefit downstream tasks. Besides, we present multiple constructive empirical conclusions for enhancing task-oriented dialogues.</abstract>
      <url hash="b72dc2d4">2021.emnlp-main.178</url>
      <bibkey>qiu-etal-2021-different</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.178</doi>
    </paper>
    <paper id="179">
      <title>Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation</title>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Yu</first><last>Wu</last></author>
      <author><first>Shujie</first><last>Liu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>2328–2337</pages>
      <abstract>Although pre-training models have achieved great success in dialogue generation, their performance drops dramatically when the input contains an entity that does not appear in pre-training and fine-tuning datasets (unseen entity). To address this issue, existing <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a> leverage an <a href="https://en.wikipedia.org/wiki/Knowledge_base">external knowledge base</a> to generate appropriate responses. In real-world practical, the entity may not be included by the <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a> or suffer from the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a> of <a href="https://en.wikipedia.org/wiki/Knowledge_retrieval">knowledge retrieval</a>. To deal with this problem, instead of introducing <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a> as the input, we force the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to learn a better semantic representation by predicting the information in the <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a>, only based on the input context. Specifically, with the help of a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a>, we introduce two auxiliary training objectives : 1) Interpret Masked Word, which conjectures the meaning of the masked entity given the context ; 2) Hypernym Generation, which predicts the hypernym of the entity based on the context. Experiment results on two dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings.</abstract>
      <url hash="d5894ffa">2021.emnlp-main.179</url>
      <bibkey>cui-etal-2021-knowledge</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.179</doi>
      <pwccode url="https://github.com/nealcly/ke-blender" additional="false">nealcly/ke-blender</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="181">
      <title>Unsupervised Conversation Disentanglement through <a href="https://en.wikipedia.org/wiki/Co-training">Co-Training</a></title>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Zhan</first><last>Shi</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <pages>2345–2356</pages>
      <abstract>Conversation disentanglement aims to separate intermingled messages into detached sessions, which is a fundamental task in understanding multi-party conversations. Existing work on conversation disentanglement relies heavily upon human-annotated datasets, which is expensive to obtain in practice. In this work, we explore training a conversation disentanglement model without referencing any human annotations. Our method is built upon the deep co-training algorithm, which consists of two <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> : a message-pair classifier and a session classifier. The former is responsible of retrieving local relations between two messages while the latter categorizes a message to a session by capturing context-aware information. Both the two <a href="https://en.wikipedia.org/wiki/Computer_network">networks</a> are initialized respectively with pseudo data built from the <a href="https://en.wikipedia.org/wiki/Text_corpus">unannotated corpus</a>. During the deep co-training process, we use the session classifier as a reinforcement learning component to learn a session assigning policy by maximizing the local rewards given by the message-pair classifier. For the message-pair classifier, we enrich its training data by retrieving message pairs with high confidence from the disentangled sessions predicted by the session classifier. Experimental results on the large Movie Dialogue Dataset demonstrate that our proposed approach achieves competitive performance compared to previous supervised methods. Further experiments show that the predicted disentangled conversations can promote the performance on the downstream task of multi-party response selection.</abstract>
      <url hash="4afe3bd1">2021.emnlp-main.181</url>
      <bibkey>liu-etal-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.181</doi>
      <pwccode url="https://github.com/layneins/unsupervised_dialo_disentanglement" additional="false">layneins/unsupervised_dialo_disentanglement</pwccode>
    </paper>
    <paper id="184">
      <title>EARL : Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning<fixed-case>EARL</fixed-case>: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning</title>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Yong</first><last>Liu</last></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Xiaoyan</first><last>Zhu</last></author>
      <pages>2383–2395</pages>
      <abstract>Generating informative and appropriate responses is challenging but important for building human-like dialogue systems. Although various knowledge-grounded conversation models have been proposed, these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> have limitations in utilizing knowledge that infrequently occurs in the training data, not to mention integrating unseen knowledge into conversation generation. In this paper, we propose an Entity-Agnostic Representation Learning (EARL) method to introduce <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a> to informative conversation generation. Unlike traditional approaches that parameterize the specific representation for each entity, EARL utilizes the context of conversations and the relational structure of knowledge graphs to learn the category representation for entities, which is generalized to incorporating unseen entities in <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a> into conversation generation. Automatic and manual evaluations demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can generate more informative, coherent, and natural responses than baseline models.</abstract>
      <url hash="257d476e">2021.emnlp-main.184</url>
      <bibkey>zhou-etal-2021-earl</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.184</doi>
      <pwccode url="https://github.com/thu-coai/earl" additional="false">thu-coai/earl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opendialkg">OpenDialKG</pwcdataset>
    </paper>
    <paper id="185">
      <title>DialogueCSE : Dialogue-based Contrastive Learning of Sentence Embeddings<fixed-case>D</fixed-case>ialogue<fixed-case>CSE</fixed-case>: Dialogue-based Contrastive Learning of Sentence Embeddings</title>
      <author><first>Che</first><last>Liu</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Jinghua</first><last>Liu</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>2396–2406</pages>
      <abstract>Learning sentence embeddings from <a href="https://en.wikipedia.org/wiki/Dialogue">dialogues</a> has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a context-aware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets : the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> in terms of MAP and Spearman’s correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data</a> is provided.</abstract>
      <url hash="eeb39f37">2021.emnlp-main.185</url>
      <bibkey>liu-etal-2021-dialoguecse</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.185</doi>
      <pwccode url="https://github.com/wangruicn/dialoguecse" additional="false">wangruicn/dialoguecse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="187">
      <title>Not Just <a href="https://en.wikipedia.org/wiki/Classification">Classification</a> : Recognizing Implicit Discourse Relation on Joint Modeling of Classification and Generation</title>
      <author><first>Feng</first><last>Jiang</last></author>
      <author><first>Yaxin</first><last>Fan</last></author>
      <author><first>Xiaomin</first><last>Chu</last></author>
      <author><first>Peifeng</first><last>Li</last></author>
      <author><first>Qiaoming</first><last>Zhu</last></author>
      <pages>2418–2431</pages>
      <abstract>Implicit discourse relation recognition (IDRR) is a critical task in <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse analysis</a>. Previous studies only regard it as a classification task and lack an in-depth understanding of the semantics of different relations. Therefore, we first view IDRR as a <a href="https://en.wikipedia.org/wiki/Machine_learning">generation task</a> and further propose a method joint modeling of the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> and <a href="https://en.wikipedia.org/wiki/Machine_learning">generation</a>. Specifically, we propose a joint model, CG-T5, to recognize the relation label and generate the target sentence containing the meaning of relations simultaneously. Furthermore, we design three target sentence forms, including the question form, for the generation model to incorporate prior knowledge. To address the issue that large discourse units are hardly embedded into the target sentence, we also propose a target sentence construction mechanism that automatically extracts core sentences from those large discourse units. Experimental results both on Chinese MCDTB and English PDTB datasets show that our model CG-T5 achieves the best performance against several state-of-the-art systems.</abstract>
      <url hash="40e9958a">2021.emnlp-main.187</url>
      <bibkey>jiang-etal-2021-just</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.187</doi>
    </paper>
    <paper id="189">
      <title>Multimodal Phased Transformer for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a></title>
      <author><first>Junyan</first><last>Cheng</last></author>
      <author><first>Iordanis</first><last>Fostiropoulos</last></author>
      <author><first>Barry</first><last>Boehm</last></author>
      <author><first>Mohammad</first><last>Soleymani</last></author>
      <pages>2447–2458</pages>
      <abstract>Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">quadratic complexity</a> of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their <a href="https://en.wikipedia.org/wiki/Inference">inference</a> and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and <a href="https://en.wikipedia.org/wiki/Memory_footprint">memory footprint</a>. SPT uses a <a href="https://en.wikipedia.org/wiki/Sampling_(signal_processing)">sampling function</a> to generate a sparse attention matrix and compress a long sequence to a shorter sequence of hidden states. SPT concurrently captures interactions between the hidden states of different modalities at every layer. To further improve the efficiency of our method, we use Layer-wise parameter sharing and Factorized Co-Attention that share parameters between Cross Attention Blocks, with minimal impact on task performance. We evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with three sentiment analysis datasets and achieve comparable or superior performance compared with the existing methods, with a 90 % reduction in the number of parameters. We conclude that (SPT) along with parameter sharing can capture multimodal interactions with reduced model size and improved sample efficiency.</abstract>
      <url hash="c7a17995">2021.emnlp-main.189</url>
      <attachment type="Software" hash="4757fc0f">2021.emnlp-main.189.Software.zip</attachment>
      <bibkey>cheng-etal-2021-multimodal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.189</doi>
      <pwccode url="https://github.com/chengjunyan1/sp-transformer" additional="false">chengjunyan1/sp-transformer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity">Multimodal Opinionlevel Sentiment Intensity</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ur-funny">UR-FUNNY</pwcdataset>
    </paper>
    <paper id="190">
      <title>Hierarchical Multi-label Text Classification with Horizontal and Vertical Category Correlations</title>
      <author><first>Linli</first><last>Xu</last></author>
      <author><first>Sijie</first><last>Teng</last></author>
      <author><first>Ruoyu</first><last>Zhao</last></author>
      <author><first>Junliang</first><last>Guo</last></author>
      <author><first>Chi</first><last>Xiao</last></author>
      <author><first>Deqiang</first><last>Jiang</last></author>
      <author><first>Bo</first><last>Ren</last></author>
      <pages>2459–2468</pages>
      <abstract>Hierarchical multi-label text classification (HMTC) deals with the challenging task where an instance can be assigned to multiple hierarchically structured categories at the same time. The majority of prior studies either focus on reducing the HMTC task into a flat multi-label problem ignoring the vertical category correlations or exploiting the dependencies across different hierarchical levels without considering the horizontal correlations among categories at the same level, which inevitably leads to fundamental information loss. In this paper, we propose a novel HMTC framework that considers both vertical and horizontal category correlations. Specifically, we first design a loosely coupled graph convolutional neural network as the representation extractor to obtain <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> for words, documents, and, more importantly, level-wise representations for categories, which are not considered in previous works. Then, the learned category representations are adopted to capture the vertical dependencies among levels of category hierarchy and model the horizontal correlations. Finally, based on the document embeddings and category embeddings, we design a <a href="https://en.wikipedia.org/wiki/Hybrid_algorithm">hybrid algorithm</a> to predict the categories of the entire hierarchical structure. Extensive experiments conducted on real-world HMTC datasets validate the effectiveness of the proposed <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> with significant improvements over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="51924be9">2021.emnlp-main.190</url>
      <attachment type="Software" hash="5291a014">2021.emnlp-main.190.Software.zip</attachment>
      <bibkey>xu-etal-2021-hierarchical</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.190</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/web-of-science-dataset">WOS</pwcdataset>
    </paper>
    <paper id="192">
      <title>FLiText : A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks<fixed-case>FL</fixed-case>i<fixed-case>T</fixed-case>ext: A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks</title>
      <author><first>Chen</first><last>Liu</last></author>
      <author><first>Zhang</first><last>Mengchao</last></author>
      <author><first>Fu</first><last>Zhibing</last></author>
      <author><first>Panpan</first><last>Hou</last></author>
      <author><first>Yu</first><last>Li</last></author>
      <pages>2481–2491</pages>
      <abstract>In <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP)</a>, state-of-the-art (SOTA) semi-supervised learning (SSL) frameworks have shown great performance on deep pre-trained language models such as BERT, and are expected to significantly reduce the demand for manual labeling. However, our empirical studies indicate that these frameworks are not suitable for lightweight models such as TextCNN, LSTM and etc. In this work, we develop a new SSL framework called FLiText, which stands for Faster and Lighter semi-supervised Text classification. FLiText introduces an inspirer network together with the consistency regularization framework, which leverages a generalized regular constraint on the lightweight models for efficient <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security">SSL</a>. As a result, FLiText obtains new SOTA performance for lightweight models across multiple SSL benchmarks on text classification. Compared with existing SOTA SSL methods on TextCNN, FLiText improves the accuracy of lightweight model TextCNN from 51.00 % to 90.49 % on <a href="https://en.wikipedia.org/wiki/IMDb">IMDb</a>, 39.8 % to 58.06 % on <a href="https://en.wikipedia.org/wiki/Yelp">Yelp-5</a>, and from 55.3 % to 65.08 % on Yahoo ! Answer. In addition, compared with the <a href="https://en.wikipedia.org/wiki/Supervised_learning">fully supervised method</a> on the full dataset, FLiText just uses less than 1 % of labeled data to improve the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> by 6.59 %, 3.94 %, and 3.22 % on the datasets of <a href="https://en.wikipedia.org/wiki/IMDb">IMDb</a>, <a href="https://en.wikipedia.org/wiki/Yelp">Yelp-5</a>, and Yahoo ! Answer respectively.</abstract>
      <url hash="965d036a">2021.emnlp-main.192</url>
      <bibkey>liu-etal-2021-flitext</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.192</doi>
      <pwccode url="https://github.com/valuesimplex/flitext" additional="false">valuesimplex/flitext</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="195">
      <title>Transductive Learning for Unsupervised Text Style Transfer</title>
      <author><first>Fei</first><last>Xiao</last></author>
      <author><first>Liang</first><last>Pang</last></author>
      <author><first>Yanyan</first><last>Lan</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>Huawei</first><last>Shen</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>2510–2521</pages>
      <abstract>Unsupervised style transfer models are mainly based on an inductive learning approach, which represents the style as embeddings, decoder parameters, or discriminator parameters and directly applies these general rules to the test cases. However, the lacking of <a href="https://en.wikipedia.org/wiki/Parallel_corpus">parallel corpus</a> hinders the ability of these inductive learning methods on this task. As a result, it is likely to cause severe inconsistent style expressions, like ‘the salad is rude’. To tackle this problem, we propose a novel transductive learning approach in this paper, based on a retrieval-based context-aware style representation. Specifically, an attentional encoder-decoder with a retriever framework is utilized. It involves top-K relevant sentences in the target style in the transfer process. In this way, we can learn a context-aware style embedding to alleviate the above inconsistency problem. In this paper, both sparse (BM25) and dense retrieval functions (MIPS) are used, and two objective functions are designed to facilitate joint learning. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms several strong <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>. The proposed transductive learning approach is general and effective to the task of unsupervised style transfer, and we will apply it to the other two typical methods in the future.</abstract>
      <url hash="409db797">2021.emnlp-main.195</url>
      <attachment type="Software" hash="f7d33825">2021.emnlp-main.195.Software.zip</attachment>
      <bibkey>xiao-etal-2021-transductive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.195</doi>
      <pwccode url="https://github.com/xiaofei05/tsst" additional="false">xiaofei05/tsst</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="199">
      <title>ConRPG : <a href="https://en.wikipedia.org/wiki/Paraphrase_generation">Paraphrase Generation</a> using Contexts as Regularizer<fixed-case>C</fixed-case>on<fixed-case>RPG</fixed-case>: Paraphrase Generation using Contexts as Regularizer</title>
      <author><first>Yuxian</first><last>Meng</last></author>
      <author><first>Xiang</first><last>Ao</last></author>
      <author><first>Qing</first><last>He</last></author>
      <author><first>Xiaofei</first><last>Sun</last></author>
      <author><first>Qinghong</first><last>Han</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Chun</first><last>Fan</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <pages>2551–2562</pages>
      <abstract>A long-standing issue with <a href="https://en.wikipedia.org/wiki/Paraphrase_generation">paraphrase generation</a> is the lack of reliable supervision signals. In this paper, we propose a new <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised paradigm</a> for <a href="https://en.wikipedia.org/wiki/Paraphrase_generation">paraphrase generation</a> based on the assumption that the probabilities of generating two sentences with the same meaning given the same context should be the same. Inspired by this fundamental idea, we propose a pipelined system which consists of paraphrase candidate generation based on contextual language models, candidate filtering using scoring functions, and paraphrase model training based on the selected candidates. The proposed paradigm offers merits over existing paraphrase generation methods : (1) using the context regularizer on meanings, the model is able to generate massive amounts of high-quality paraphrase pairs ; (2) the combination of the huge amount of paraphrase candidates and further diversity-promoting filtering yields paraphrases with more lexical and syntactic diversity ; and (3) using human-interpretable scoring functions to select paraphrase pairs from candidates, the proposed framework provides a channel for developers to intervene with the data generation process, leading to a more controllable model. Experimental results across different tasks and datasets demonstrate that the proposed <a href="https://en.wikipedia.org/wiki/Paradigm">paradigm</a> significantly outperforms existing paraphrase approaches in both supervised and unsupervised setups.</abstract>
      <url hash="f4b77498">2021.emnlp-main.199</url>
      <bibkey>meng-etal-2021-conrpg</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.199</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="202">
      <title>Asking Questions Like Educational Experts : Automatically Generating Question-Answer Pairs on Real-World Examination Data<fixed-case>A</fixed-case>utomatically Generating Question-Answer Pairs on Real-World Examination Data</title>
      <author><first>Fanyi</first><last>Qu</last></author>
      <author><first>Xin</first><last>Jia</last></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>2583–2593</pages>
      <abstract>Generating high quality question-answer pairs is a hard but meaningful task. Although previous works have achieved great results on answer-aware question generation, it is difficult to apply them into practical application in the education field. This paper for the first time addresses the question-answer pair generation task on the real-world examination data, and proposes a new unified framework on RACE. To capture the important information of the input passage we first automatically generate (rather than extracting) keyphrases, thus this task is reduced to keyphrase-question-answer triplet joint generation. Accordingly, we propose a multi-agent communication model to generate and optimize the question and keyphrases iteratively, and then apply the generated question and keyphrases to guide the generation of answers. To establish a solid benchmark, we build our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on the strong generative pre-training model. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> makes great breakthroughs in the question-answer pair generation task. Moreover, we make a comprehensive analysis on our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, suggesting new directions for this challenging task.</abstract>
      <url hash="edcc18b7">2021.emnlp-main.202</url>
      <bibkey>qu-etal-2021-asking</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.202</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="203">
      <title>Syntactically-Informed Unsupervised Paraphrasing with Non-Parallel Data</title>
      <author><first>Erguang</first><last>Yang</last></author>
      <author><first>Mingtong</first><last>Liu</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Yujie</first><last>Zhang</last></author>
      <author><first>Yao</first><last>Meng</last></author>
      <author><first>Changjian</first><last>Hu</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <pages>2594–2604</pages>
      <abstract>Previous works on syntactically controlled paraphrase generation heavily rely on large-scale parallel paraphrase data that is not easily available for many languages and domains. In this paper, we take this research direction to the extreme and investigate whether it is possible to learn syntactically controlled paraphrase generation with nonparallel data. We propose a syntactically-informed unsupervised paraphrasing model based on conditional variational auto-encoder (VAE) which can generate texts in a specified syntactic structure. Particularly, we design a two-stage learning method to effectively train the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> using non-parallel data. The conditional VAE is trained to reconstruct the input sentence according to the given input and its <a href="https://en.wikipedia.org/wiki/Syntax">syntactic structure</a>. Furthermore, to improve the syntactic controllability and semantic consistency of the pre-trained conditional VAE, we fine-tune it using syntax controlling and cycle reconstruction learning objectives, and employ Gumbel-Softmax to combine these new learning objectives. Experiment results demonstrate that the proposed <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained only on non-parallel data is capable of generating diverse paraphrases with specified <a href="https://en.wikipedia.org/wiki/Syntax">syntactic structure</a>. Additionally, we validate the effectiveness of our method for generating syntactically adversarial examples on the sentiment analysis task.</abstract>
      <url hash="3b80fcdd">2021.emnlp-main.203</url>
      <bibkey>yang-etal-2021-syntactically</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.203</doi>
      <pwccode url="https://github.com/lanse-sir/sup" additional="false">lanse-sir/sup</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="204">
      <title>Exploring Task Difficulty for Few-Shot Relation Extraction</title>
      <author><first>Jiale</first><last>Han</last></author>
      <author><first>Bo</first><last>Cheng</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>2605–2616</pages>
      <abstract>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive results achieved, existing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> still perform suboptimally when handling hard FSRE tasks, where the relations are fine-grained and similar to each other. We argue this is largely because existing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> do not distinguish <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">hard tasks</a> from easy ones in the <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">learning process</a>. In this paper, we introduce a novel approach based on contrastive learning that learns better <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> by exploiting relation label information. We further design a <a href="https://en.wikipedia.org/wiki/Methodology">method</a> that allows the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to adaptively learn how to focus on hard tasks. Experiments on two standard datasets demonstrate the effectiveness of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a>.</abstract>
      <url hash="a582ceff">2021.emnlp-main.204</url>
      <bibkey>han-etal-2021-exploring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.204</doi>
      <pwccode url="https://github.com/hanjiale/hcrp" additional="false">hanjiale/hcrp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel-2-0">FewRel 2.0</pwcdataset>
    </paper>
    <paper id="208">
      <title>A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling</title>
      <author><first>Feiliang</first><last>Ren</last></author>
      <author><first>Longhui</first><last>Zhang</last></author>
      <author><first>Shujuan</first><last>Yin</last></author>
      <author><first>Xiaofeng</first><last>Zhao</last></author>
      <author><first>Shilei</first><last>Liu</last></author>
      <author><first>Bochao</first><last>Li</last></author>
      <author><first>Yaduo</first><last>Liu</last></author>
      <pages>2646–2656</pages>
      <abstract>Table filling based relational triple extraction methods are attracting growing research interests due to their promising performance and their abilities on extracting triples from complex sentences. However, this kind of methods are far from their full potential because most of them only focus on using local features but ignore the global associations of relations and of token pairs, which increases the possibility of overlooking some important information during triple extraction. To overcome this deficiency, we propose a global feature-oriented triple extraction model that makes full use of the mentioned two kinds of global associations. Specifically, we first generate a <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">table feature</a> for each relation. Then two kinds of global associations are mined from the generated <a href="https://en.wikipedia.org/wiki/Feature_(computer_vision)">table features</a>. Next, the mined global associations are integrated into the table feature of each relation. This generate-mine-integrate process is performed multiple times so that the table feature of each relation is refined step by step. Finally, each relation’s table is filled based on its refined table feature, and all triples linked to this relation are extracted based on its filled table. We evaluate the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on three <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark datasets</a>. Experimental results show our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is effective and it achieves state-of-the-art results on all of these <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>. The source code of our work is available at : https://github.com/neukg/GRTE.</abstract>
      <url hash="48df7407">2021.emnlp-main.208</url>
      <bibkey>ren-etal-2021-novel</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.208</doi>
      <pwccode url="https://github.com/neukg/GRTE" additional="false">neukg/GRTE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="212">
      <title>MapRE : An Effective Semantic Mapping Approach for Low-resource Relation Extraction<fixed-case>M</fixed-case>ap<fixed-case>RE</fixed-case>: An Effective Semantic Mapping Approach for Low-resource Relation Extraction</title>
      <author><first>Manqing</first><last>Dong</last></author>
      <author><first>Chunguang</first><last>Pan</last></author>
      <author><first>Zhipeng</first><last>Luo</last></author>
      <pages>2694–2704</pages>
      <abstract>Neural relation extraction models have shown promising results in recent years ; however, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance drops dramatically given only a few training samples. Recent works try leveraging the advance in few-shot learning to solve the low resource problem, where they train label-agnostic models to directly compare the semantic similarities among context sentences in the embedding space. However, the label-aware information, i.e., the relation label that contains the semantic knowledge of the relation itself, is often neglected for prediction. In this work, we propose a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> considering both label-agnostic and label-aware semantic mapping information for low resource relation extraction. We show that incorporating the above two types of mapping information in both pretraining and <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> can significantly improve the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance on low-resource relation extraction tasks.</abstract>
      <url hash="13bb9bc0">2021.emnlp-main.212</url>
      <bibkey>dong-etal-2021-mapre</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.212</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="222">
      <title>Weakly-supervised Text Classification Based on Keyword Graph</title>
      <author><first>Lu</first><last>Zhang</last></author>
      <author><first>Jiandong</first><last>Ding</last></author>
      <author><first>Yi</first><last>Xu</last></author>
      <author><first>Yingyao</first><last>Liu</last></author>
      <author><first>Shuigeng</first><last>Zhou</last></author>
      <pages>2803–2813</pages>
      <abstract>Weakly-supervised text classification has received much attention in recent years for it can alleviate the heavy burden of annotating massive data. Among them, keyword-driven methods are the mainstream where user-provided keywords are exploited to generate pseudo-labels for unlabeled texts. However, existing methods treat <a href="https://en.wikipedia.org/wiki/Index_term">keywords</a> independently, thus ignore the correlation among them, which should be useful if properly exploited. In this paper, we propose a novel framework called ClassKG to explore keyword-keyword correlation on keyword graph by GNN. Our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> is an <a href="https://en.wikipedia.org/wiki/Iterative_and_incremental_development">iterative process</a>. In each iteration, we first construct a keyword graph, so the task of assigning pseudo labels is transformed to annotating keyword subgraphs. To improve the annotation quality, we introduce a self-supervised task to pretrain a subgraph annotator, and then finetune it. With the pseudo labels generated by the subgraph annotator, we then train a <a href="https://en.wikipedia.org/wiki/Text_classification">text classifier</a> to classify the unlabeled texts. Finally, we re-extract <a href="https://en.wikipedia.org/wiki/Index_term">keywords</a> from the classified texts. Extensive experiments on both long-text and short-text datasets show that our method substantially outperforms the existing ones.</abstract>
      <url hash="fbf2cffe">2021.emnlp-main.222</url>
      <bibkey>zhang-etal-2021-weakly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.222</doi>
      <pwccode url="https://github.com/zhanglu-cst/classkg" additional="false">zhanglu-cst/classkg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="223">
      <title>Efficient-FedRec : Efficient Federated Learning Framework for Privacy-Preserving News Recommendation<fixed-case>F</fixed-case>ed<fixed-case>R</fixed-case>ec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation</title>
      <author><first>Jingwei</first><last>Yi</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Ruixuan</first><last>Liu</last></author>
      <author><first>Guangzhong</first><last>Sun</last></author>
      <author><first>Xing</first><last>Xie</last></author>
      <pages>2814–2824</pages>
      <abstract>News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users’ historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple clients to collaboratively train models without sharing their private data. However, the computation and communication cost of directly learning many existing news recommendation models in a federated way are unacceptable for user clients. In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation. Instead of training and communicating the whole <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>, we decompose the news recommendation model into a large news model maintained in the server and a light-weight user model shared on both server and clients, where news representations and user model are communicated between server and clients. More specifically, the clients request the <a href="https://en.wikipedia.org/wiki/User_model">user model</a> and <a href="https://en.wikipedia.org/wiki/News">news representations</a> from the server, and send their locally computed gradients to the server for <a href="https://en.wikipedia.org/wiki/News_aggregator">aggregation</a>. The server updates its global user model with the aggregated gradients, and further updates its news model to infer updated news representations. Since the local gradients may contain private information, we propose a secure aggregation method to aggregate gradients in a privacy-preserving way. Experiments on two real-world datasets show that our method can reduce the computation and communication cost on clients while keep promising model performance.</abstract>
      <url hash="5c3d6b96">2021.emnlp-main.223</url>
      <bibkey>yi-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.223</doi>
      <pwccode url="https://github.com/yjw1029/efficient-fedrec" additional="false">yjw1029/efficient-fedrec</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
    </paper>
    <paper id="224">
      <title>RocketQAv2 : A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking<fixed-case>R</fixed-case>ocket<fixed-case>QA</fixed-case>v2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking</title>
      <author><first>Ruiyang</first><last>Ren</last></author>
      <author><first>Yingqi</first><last>Qu</last></author>
      <author><first>Jing</first><last>Liu</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>QiaoQiao</first><last>She</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>2825–2835</pages>
      <abstract>In various <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing tasks</a>, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two <a href="https://en.wikipedia.org/wiki/Procedure_(term)">procedures</a> contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other’s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.com/PaddlePaddle/RocketQA.</abstract>
      <url hash="f214ef52">2021.emnlp-main.224</url>
      <bibkey>ren-etal-2021-rocketqav2</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.224</doi>
      <pwccode url="https://github.com/paddlepaddle/rocketqa" additional="false">paddlepaddle/rocketqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="227">
      <title>Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrieval</title>
      <author><first>Xueguang</first><last>Ma</last></author>
      <author><first>Minghan</first><last>Li</last></author>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>2854–2859</pages>
      <abstract>Recent work has shown that dense passage retrieval techniques achieve better ranking accuracy in open-domain question answering compared to sparse retrieval techniques such as <a href="https://en.wikipedia.org/wiki/BM25">BM25</a>, but at the cost of large space and memory requirements. In this paper, we analyze the <a href="https://en.wikipedia.org/wiki/Redundancy_(information_theory)">redundancy</a> present in encoded dense vectors and show that the default dimension of 768 is unnecessarily large. To improve space efficiency, we propose a simple unsupervised compression pipeline that consists of <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis (PCA)</a>, product quantization, and hybrid search. We further investigate other supervised baselines and find surprisingly that unsupervised PCA outperforms them in some settings. We perform extensive experiments on five question answering datasets and demonstrate that our best pipeline achieves good accuracyspace trade-offs, for example, 48 compression with less than 3 % drop in top-100 retrieval accuracy on average or 96 compression with less than 4 % drop. Code and data are available at.<tex-math>48\times</tex-math> compression with less than 3% drop in top-100 retrieval accuracy on average or <tex-math>96\times</tex-math> compression with less than 4% drop. Code and data are available at <url>http://pyserini.io/</url>.</abstract>
      <url hash="44e2c522">2021.emnlp-main.227</url>
      <bibkey>ma-etal-2021-simple</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.227</doi>
    </paper>
    <paper id="228">
      <title>Relation Extraction with Word Graphs from N-grams</title>
      <author><first>Han</first><last>Qin</last></author>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>2860–2868</pages>
      <abstract>Most recent studies for relation extraction (RE) leverage the dependency tree of the input sentence to incorporate syntax-driven contextual information to improve model performance, with little attention paid to the limitation where high-quality dependency parsers in most cases unavailable, especially for in-domain scenarios. To address this limitation, in this paper, we propose attentive graph convolutional networks (A-GCN) to improve neural RE methods with an unsupervised manner to build the context graph, without relying on the existence of a dependency parser. Specifically, we construct the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> from n-grams extracted from a lexicon built from pointwise mutual information (PMI) and apply <a href="https://en.wikipedia.org/wiki/Attention">attention</a> over the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>. Therefore, different word pairs from the contexts within and across <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a> are weighted in the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>.</abstract>
      <url hash="91258c4e">2021.emnlp-main.228</url>
      <bibkey>qin-etal-2021-relation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.228</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task 8</pwcdataset>
    </paper>
    <paper id="229">
      <title>A Bayesian Framework for Information-Theoretic Probing<fixed-case>B</fixed-case>ayesian Framework for Information-Theoretic Probing</title>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>2869–2887</pages>
      <abstract>Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that <a href="https://en.wikipedia.org/wiki/Probing">probing</a> should be seen as approximating a <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a>. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a>, however, assumes the true <a href="https://en.wikipedia.org/wiki/Probability_distribution">probability distribution</a> of a pair of <a href="https://en.wikipedia.org/wiki/Random_variable">random variables</a> is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agentsallowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and <a href="https://en.wikipedia.org/wiki/Information">information</a> can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.</abstract>
      <url hash="5c30af52">2021.emnlp-main.229</url>
      <bibkey>pimentel-cotterell-2021-bayesian</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.229</doi>
      <pwccode url="https://github.com/rycolab/bayesian-mi" additional="false">rycolab/bayesian-mi</pwccode>
    </paper>
    <paper id="230">
      <title>Masked Language Modeling and the Distributional Hypothesis : Order Word Matters Pre-training for Little</title>
      <author><first>Koustuv</first><last>Sinha</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Dieuwke</first><last>Hupkes</last></author>
      <author><first>Joelle</first><last>Pineau</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <pages>2888–2913</pages>
      <abstract>A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation : MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> after fine-tuning on many downstream tasksincluding tasks specifically designed to be challenging for models that ignore <a href="https://en.wikipedia.org/wiki/Word_order">word order</a>. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pre-training, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.</abstract>
      <url hash="3799ae01">2021.emnlp-main.230</url>
      <bibkey>sinha-etal-2021-masked</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.230</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="234">
      <title>Linguistic Dependencies and Statistical Dependence</title>
      <author><first>Jacob Louis</first><last>Hoover</last></author>
      <author><first>Wenyu</first><last>Du</last></author>
      <author><first>Alessandro</first><last>Sordoni</last></author>
      <author><first>Timothy J.</first><last>O’Donnell</last></author>
      <pages>2941–2963</pages>
      <abstract>Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in <a href="https://en.wikipedia.org/wiki/Cognitive_science">cognitive science</a>, <a href="https://en.wikipedia.org/wiki/Psycholinguistics">psycholinguistics</a>, and <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a>. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and <a href="https://en.wikipedia.org/wiki/Independence_(probability_theory)">statistical dependence</a> between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most   0.5. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline</a> formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.<tex-math>\approx 0.5</tex-math>. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.</abstract>
      <url hash="48121ef9">2021.emnlp-main.234</url>
      <bibkey>hoover-etal-2021-linguistic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.234</doi>
      <pwccode url="https://github.com/mcqll/cpmi-dependencies" additional="false">mcqll/cpmi-dependencies</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="236">
      <title>A Simple and Effective Positional Encoding for Transformers</title>
      <author><first>Pu-Chin</first><last>Chen</last></author>
      <author><first>Henry</first><last>Tsai</last></author>
      <author><first>Srinadh</first><last>Bhojanapalli</last></author>
      <author><first>Hyung Won</first><last>Chung</last></author>
      <author><first>Yin-Wen</first><last>Chang</last></author>
      <author><first>Chun-Sung</first><last>Ferng</last></author>
      <pages>2974–2988</pages>
      <abstract>Transformer models are permutation equivariant. To supply the order and type information of the input tokens, position and segment embeddings are usually added to the input. Recent works proposed variations of positional encodings with relative position encodings achieving better performance. Our analysis shows that the gain actually comes from moving positional information to attention layer from the input. Motivated by this, we introduce Decoupled Positional Attention for Transformers (DIET), a simple yet effective mechanism to encode position and segment information into the Transformer models. The proposed method has faster training and inference time, while achieving competitive performance on GLUE, XTREME and WMT benchmarks. We further generalize our method to long-range transformers and show performance gain.</abstract>
      <url hash="3216324f">2021.emnlp-main.236</url>
      <bibkey>chen-etal-2021-simple</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.236</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xtreme">XTREME</pwcdataset>
    </paper>
    <paper id="237">
      <title>Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models</title>
      <author><first>Anlin</first><last>Qu</last></author>
      <author><first>Jianwei</first><last>Niu</last></author>
      <author><first>Shasha</first><last>Mo</last></author>
      <pages>2989–2997</pages>
      <abstract>Relative position embedding (RPE) is a successful method to explicitly and efficaciously encode position information into Transformer models. In this paper, we investigate the potential problems in Shaw-RPE and XL-RPE, which are the most representative and prevalent RPEs, and propose two novel RPEs called Low-level Fine-grained High-level Coarse-grained (LFHC) RPE and Gaussian Cumulative Distribution Function (GCDF) RPE. LFHC-RPE is an improvement of Shaw-RPE, which enhances the perception ability at medium and long relative positions. GCDF-RPE utilizes the excellent properties of the <a href="https://en.wikipedia.org/wiki/Gaussian_function">Gaussian function</a> to amend the prior encoding mechanism in XL-RPE. Experimental results on nine authoritative datasets demonstrate the effectiveness of our methods empirically. Furthermore, GCDF-RPE achieves the best overall performance among five different <a href="https://en.wikipedia.org/wiki/Randomized_algorithm">RPEs</a>.</abstract>
      <url hash="9bc84909">2021.emnlp-main.237</url>
      <bibkey>qu-etal-2021-explore</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.237</doi>
      <pwccode url="https://github.com/menghuanlater/lfhc-gcdf-rpe" additional="false">menghuanlater/lfhc-gcdf-rpe</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cmrc">CMRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="238">
      <title>Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup</title>
      <author><first>Guang</first><last>Liu</last></author>
      <author><first>Yuzhao</first><last>Mao</last></author>
      <author><first>Huang</first><last>Hailong</last></author>
      <author><first>Gao</first><last>Weiguo</last></author>
      <author><first>Li</first><last>Xuan</last></author>
      <pages>2998–3008</pages>
      <abstract>Mixup is a recent <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularizer</a> for current deep classification networks. Through training a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> on <a href="https://en.wikipedia.org/wiki/Convex_combination">convex combinations</a> of pairs of examples and their labels, it imposes locally linear constraints on the model’s input space. However, such strict linear constraints often lead to <a href="https://en.wikipedia.org/wiki/Underfitting">under-fitting</a> which degrades the effects of <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization</a>. Noticeably, this issue is getting more serious when the resource is extremely limited. To address these issues, we propose the Adversarial Mixing Policy (AMP), organized in a min-max-rand formulation, to relax the Locally Linear Constraints in Mixup. Specifically, AMP adds a small <a href="https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)">adversarial perturbation</a> to the mixing coefficients rather than the examples. Thus, slight <a href="https://en.wikipedia.org/wiki/Nonlinear_system">non-linearity</a> is injected in-between the synthetic examples and synthetic labels. By training on these <a href="https://en.wikipedia.org/wiki/Data">data</a>, the <a href="https://en.wikipedia.org/wiki/Deep_learning">deep networks</a> are further regularized, and thus achieve a lower predictive error rate. Experiments on five text classification benchmarks and five backbone models have empirically shown that our methods reduce the error rate over Mixup variants in a significant margin (up to 31.3 %), especially in low-resource conditions (up to 17.5 %).</abstract>
      <url hash="d239d82a">2021.emnlp-main.238</url>
      <bibkey>liu-etal-2021-adversarial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.238</doi>
      <pwccode url="https://github.com/pai-smallisallyourneed/mixup-amp" additional="false">pai-smallisallyourneed/mixup-amp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="246">
      <title>Layer-wise Model Pruning based on <a href="https://en.wikipedia.org/wiki/Mutual_information">Mutual Information</a></title>
      <author><first>Chun</first><last>Fan</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <author><first>Tianwei</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Ao</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Yuxian</first><last>Meng</last></author>
      <author><first>Xiaofei</first><last>Sun</last></author>
      <pages>3079–3090</pages>
      <abstract>Inspired by mutual information (MI) based feature selection in SVMs and <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>, in this paper, we propose MI-based layer-wise pruning : for each layer of a multi-layer neural network, neurons with higher values of MI with respect to preserved neurons in the upper layer are preserved. Starting from the top softmax layer, layer-wise pruning proceeds in a top-down fashion until reaching the bottom word embedding layer. The proposed pruning strategy offers merits over weight-based pruning techniques : (1) it avoids irregular memory access since representations and matrices can be squeezed into their smaller but dense counterparts, leading to greater speedup ; (2) in a manner of top-down pruning, the proposed method operates from a more global perspective based on training signals in the top layer, and prunes each layer by propagating the effect of global signals through layers, leading to better performances at the same sparsity level. Extensive experiments show that at the same sparsity level, the proposed strategy offers both greater speedup and higher performances than weight-based pruning methods (e.g., magnitude pruning, movement pruning).</abstract>
      <url hash="8aea531e">2021.emnlp-main.246</url>
      <bibkey>fan-etal-2021-layer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.246</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="249">
      <title>Frustratingly Simple Pretraining Alternatives to Masked Language Modeling</title>
      <author><first>Atsuki</first><last>Yamaguchi</last></author>
      <author><first>George</first><last>Chrysostomou</last></author>
      <author><first>Katerina</first><last>Margatina</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>3116–3125</pages>
      <abstract>Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [ MASK ] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside <a href="https://en.wikipedia.org/wiki/Multilevel_marketing">MLM</a> other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of <a href="https://en.wikipedia.org/wiki/Machine_learning">MLM</a>. Empirical results on GLUE and SQUAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41 % of the BERT-BASE’s parameters, BERT-MEDIUM results in only a 1 % drop in GLUE scores with our best objective.</abstract>
      <url hash="113f5d0c">2021.emnlp-main.249</url>
      <bibkey>yamaguchi-etal-2021-frustratingly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.249</doi>
      <pwccode url="https://github.com/gucci-j/light-transformer-emnlp2021" additional="false">gucci-j/light-transformer-emnlp2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="250">
      <title>HRKD : Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression<fixed-case>HRKD</fixed-case>: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression</title>
      <author><first>Chenhe</first><last>Dong</last></author>
      <author><first>Yaliang</first><last>Li</last></author>
      <author><first>Ying</first><last>Shen</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <pages>3126–3136</pages>
      <abstract>On many <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing tasks</a>, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural network methods</a>. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at.<url>https://github.com/cheneydon/hrkd</url>.</abstract>
      <url hash="4c387f4d">2021.emnlp-main.250</url>
      <bibkey>dong-etal-2021-hrkd</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.250</doi>
      <pwccode url="https://github.com/cheneydon/hrkd" additional="false">cheneydon/hrkd</pwccode>
    </paper>
    <paper id="252">
      <title>Re-embedding Difficult Samples via Mutual Information Constrained Semantically Oversampling for Imbalanced Text Classification</title>
      <author><first>Jiachen</first><last>Tian</last></author>
      <author><first>Shizhan</first><last>Chen</last></author>
      <author><first>Xiaowang</first><last>Zhang</last></author>
      <author><first>Zhiyong</first><last>Feng</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Shaojuan</first><last>Wu</last></author>
      <author><first>Chunliu</first><last>Dou</last></author>
      <pages>3148–3161</pages>
      <abstract>Difficult samples of the minority class in imbalanced text classification are usually hard to be classified as they are embedded into an overlapping semantic region with the majority class. In this paper, we propose a Mutual Information constrained Semantically Oversampling framework (MISO) that can generate anchor instances to help the backbone network determine the re-embedding position of a non-overlapping representation for each difficult sample. MISO consists of (1) a semantic fusion module that learns entangled semantics among difficult and majority samples with an adaptive multi-head attention mechanism, (2) a mutual information loss that forces our model to learn new representations of entangled semantics in the non-overlapping region of the minority class, and (3) a coupled adversarial encoder-decoder that fine-tunes disentangled semantic representations to remain their correlations with the minority class, and then using these disentangled semantic representations to generate anchor instances for each difficult sample. Experiments on a variety of imbalanced text classification tasks demonstrate that anchor instances help <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> achieve significant improvements over strong baselines.</abstract>
      <url hash="6ee8a699">2021.emnlp-main.252</url>
      <bibkey>tian-etal-2021-embedding</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.252</doi>
    </paper>
    <paper id="255">
      <title>MetaTS : Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision<fixed-case>M</fixed-case>eta<fixed-case>TS</fixed-case>: Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision</title>
      <author><first>Zheng</first><last>Li</last></author>
      <author><first>Danqing</first><last>Zhang</last></author>
      <author><first>Tianyu</first><last>Cao</last></author>
      <author><first>Ying</first><last>Wei</last></author>
      <author><first>Yiwei</first><last>Song</last></author>
      <author><first>Bing</first><last>Yin</last></author>
      <pages>3183–3196</pages>
      <abstract>Sequence labeling aims to predict a fine-grained sequence of labels for the text. However, such <a href="https://en.wikipedia.org/wiki/Formulation">formulation</a> hinders the effectiveness of <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised methods</a> due to the lack of token-level annotated data. This is exacerbated when we meet a diverse range of languages. In this work, we explore multilingual sequence labeling with minimal supervision using a single unified model for <a href="https://en.wikipedia.org/wiki/Multilingualism">multiple languages</a>. Specifically, we propose a Meta Teacher-Student (MetaTS) Network, a novel meta learning method to alleviate data scarcity by leveraging large multilingual unlabeled data. Prior teacher-student frameworks of self-training rely on rigid teaching strategies, which may hardly produce high-quality pseudo-labels for consecutive and interdependent tokens. On the contrary, MetaTS allows the teacher to dynamically adapt its pseudo-annotation strategies by the student’s feedback on the generated pseudo-labeled data of each language and thus mitigate error propagation from noisy pseudo-labels. Extensive experiments on both public and real-world multilingual sequence labeling datasets empirically demonstrate the effectiveness of MetaTS.</abstract>
      <url hash="212fd56e">2021.emnlp-main.255</url>
      <bibkey>li-etal-2021-metats</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.255</doi>
    </paper>
    <paper id="256">
      <title>Neural Machine Translation with Heterogeneous Topic Knowledge Embeddings</title>
      <author><first>Weixuan</first><last>Wang</last></author>
      <author><first>Wei</first><last>Peng</last></author>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>3197–3202</pages>
      <abstract>Neural Machine Translation (NMT) has shown a strong ability to utilize <a href="https://en.wikipedia.org/wiki/Context_(language_use)">local context</a> to disambiguate the meaning of words. However, it remains a challenge for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a> to leverage <a href="https://en.wikipedia.org/wiki/Context_(language_use)">broader context information</a> like topics. In this paper, we propose heterogeneous ways of embedding topic information at the sentence level into an NMT model to improve <a href="https://en.wikipedia.org/wiki/Translation_(biology)">translation</a> performance. Specifically, the topic information can be incorporated as pre-encoder topic embedding, post-encoder topic embedding, and decoder topic embedding to increase the likelihood of selecting target words from the same topic of the source sentence. Experimental results show that NMT models with the proposed topic knowledge embedding outperform the baselines on the English-German and English-French translation tasks.</abstract>
      <url hash="5b6c5354">2021.emnlp-main.256</url>
      <bibkey>wang-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.256</doi>
      <pwccode url="https://github.com/Vicky-Wil/topic-NMT" additional="false">Vicky-Wil/topic-NMT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="259">
      <title>Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding</title>
      <author><first>Yingmei</first><last>Guo</last></author>
      <author><first>Linjun</first><last>Shou</last></author>
      <author><first>Jian</first><last>Pei</last></author>
      <author><first>Ming</first><last>Gong</last></author>
      <author><first>Mingxing</first><last>Xu</last></author>
      <author><first>Zhiyong</first><last>Wu</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>3226–3237</pages>
      <abstract>Lack of training data presents a grand challenge to scaling out spoken language understanding (SLU) to low-resource languages. Although various data augmentation approaches have been proposed to synthesize training data in low-resource target languages, the augmented data sets are often noisy, and thus impede the performance of SLU models. In this paper we focus on mitigating <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a> in augmented data. We develop a denoising training approach. Multiple models are trained with data produced by various <a href="https://en.wikipedia.org/wiki/Augmented_reality">augmented methods</a>. Those <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> provide supervision signals to each other. The experimental results show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms the existing <a href="https://en.wikipedia.org/wiki/State_(computer_science)">state</a> of the art by 3.05 and 4.24 percentage points on two <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark datasets</a>, respectively. The code will be made open sourced on github.</abstract>
      <url hash="0e80477e">2021.emnlp-main.259</url>
      <bibkey>guo-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.259</doi>
    </paper>
    <paper id="262">
      <title>Encouraging Lexical Translation Consistency for Document-Level Neural Machine Translation</title>
      <author><first>Xinglin</first><last>Lyu</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Zhengxian</first><last>Gong</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>3265–3277</pages>
      <abstract>Recently a number of approaches have been proposed to improve <a href="https://en.wikipedia.org/wiki/Translation_(biology)">translation</a> performance for document-level neural machine translation (NMT). However, few are focusing on the subject of lexical translation consistency. In this paper we apply one translation per discourse in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a>, and aim to encourage lexical translation consistency for document-level NMT. This is done by first obtaining a word link for each source word in a document, which tells the positions where the source word appears. Then we encourage the <a href="https://en.wikipedia.org/wiki/Translation">translation</a> of those words within a <a href="https://en.wikipedia.org/wiki/Hyperlink">link</a> to be consistent in two ways. On the one hand, when encoding sentences within a document we properly share <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context information</a> of those words. On the other hand, we propose an auxiliary loss function to better constrain that their translation should be consistent. Experimental results on ChineseEnglish and EnglishFrench translation tasks show that our approach not only achieves state-of-the-art performance in BLEU scores, but also greatly improves lexical consistency in <a href="https://en.wikipedia.org/wiki/Translation">translation</a>.</abstract>
      <url hash="bac39b58">2021.emnlp-main.262</url>
      <bibkey>lyu-etal-2021-encouraging</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.262</doi>
    </paper>
    <paper id="267">
      <title>Self-Supervised Quality Estimation for <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a></title>
      <author><first>Yuanhang</first><last>Zheng</last></author>
      <author><first>Zhixing</first><last>Tan</last></author>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Mieradilijiang</first><last>Maimaiti</last></author>
      <author><first>Huanbo</first><last>Luan</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <pages>3322–3334</pages>
      <abstract>Quality estimation (QE) of machine translation (MT) aims to evaluate the quality of machine-translated sentences without references and is important in practical applications of MT. Training QE models require massive parallel data with hand-crafted quality annotations, which are time-consuming and labor-intensive to obtain. To address the issue of the absence of annotated training data, previous studies attempt to develop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a> on several QE tasks in different language pairs and domains.</abstract>
      <url hash="b203d7cb">2021.emnlp-main.267</url>
      <bibkey>zheng-etal-2021-self</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.267</doi>
    </paper>
    <paper id="269">
      <title>STANKER : Stacking Network based on Level-grained Attention-masked BERT for Rumor Detection on Social Media<fixed-case>STANKER</fixed-case>: Stacking Network based on Level-grained Attention-masked <fixed-case>BERT</fixed-case> for Rumor Detection on Social Media</title>
      <author><first>Dongning</first><last>Rao</last></author>
      <author><first>Xin</first><last>Miao</last></author>
      <author><first>Zhihua</first><last>Jiang</last></author>
      <author><first>Ran</first><last>Li</last></author>
      <pages>3347–3363</pages>
      <abstract>Rumor detection on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> puts pre-trained language models (LMs), such as BERT, and auxiliary features, such as comments, into use. However, on the one hand, rumor detection datasets in Chinese companies with comments are rare ; on the other hand, intensive interaction of attention on Transformer-based models like BERT may hinder performance improvement. To alleviate these problems, we build a new Chinese microblog dataset named Weibo20 by collecting posts and associated comments from <a href="https://en.wikipedia.org/wiki/Sina_Weibo">Sina Weibo</a> and propose a new ensemble named STANKER (Stacking neTwork bAsed-on atteNtion-masKed BERT). STANKER adopts two level-grained attention-masked BERT (LGAM-BERT) models as base encoders. Unlike the original BERT, our new LGAM-BERT model takes comments as important auxiliary features and masks co-attention between posts and comments on lower-layers. Experiments on Weibo20 and three existing social media datasets showed that STANKER outperformed all compared models, especially beating the old state-of-the-art on Weibo dataset.</abstract>
      <url hash="aec4e876">2021.emnlp-main.269</url>
      <attachment type="Software" hash="91e89325">2021.emnlp-main.269.Software.zip</attachment>
      <bibkey>rao-etal-2021-stanker</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.269</doi>
      <pwccode url="https://github.com/fip-lab/stanker" additional="false">fip-lab/stanker</pwccode>
    </paper>
    <paper id="276">
      <title>GMH : A General Multi-hop Reasoning Model for KG Completion<fixed-case>GMH</fixed-case>: A General Multi-hop Reasoning Model for <fixed-case>KG</fixed-case> Completion</title>
      <author><first>Yao</first><last>Zhang</last></author>
      <author><first>Hongru</first><last>Liang</last></author>
      <author><first>Adam</first><last>Jatowt</last></author>
      <author><first>Wenqiang</first><last>Lei</last></author>
      <author><first>Xin</first><last>Wei</last></author>
      <author><first>Ning</first><last>Jiang</last></author>
      <author><first>Zhenglu</first><last>Yang</last></author>
      <pages>3437–3446</pages>
      <abstract>Knowledge graphs are essential for numerous downstream natural language processing applications, but are typically incomplete with many facts missing. This results in research efforts on multi-hop reasoning task, which can be formulated as a search process and current <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> typically perform short distance reasoning. However, the long-distance reasoning is also vital with the ability to connect the superficially unrelated entities. To the best of our knowledge, there lacks a general framework that approaches multi-hop reasoning in mixed long-short distance reasoning scenarios. We argue that there are two key issues for a general multi-hop reasoning model : i) where to go, and ii) when to stop. Therefore, we propose a general model which resolves the issues with three modules : 1) the local-global knowledge module to estimate the possible paths, 2) the differentiated action dropout module to explore a diverse set of paths, and 3) the adaptive stopping search module to avoid over searching. The comprehensive results on three datasets demonstrate the superiority of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with significant improvements against baselines in both short and long distance reasoning scenarios.</abstract>
      <url hash="6efbf08b">2021.emnlp-main.276</url>
      <bibkey>zhang-etal-2021-gmh</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.276</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/umls">UMLS</pwcdataset>
    </paper>
    <paper id="281">
      <title>Self-Supervised Curriculum Learning for Spelling Error Correction</title>
      <author><first>Zifa</first><last>Gan</last></author>
      <author><first>Hongfei</first><last>Xu</last></author>
      <author><first>Hongying</first><last>Zan</last></author>
      <pages>3487–3494</pages>
      <abstract>Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s performance has been shown sensitive to the difficulty of training examples, and CL has been shown effective to address this. In SEC, the data from different language learners are naturally distributed at different difficulty levels (some errors made by beginners are obvious to correct while some made by fluent speakers are hard), and we expect that designing a curriculum correspondingly for model learning may also help its training and bring about better performance. In this paper, we study how to further improve the performance of the state-of-the-art SEC method with CL, and propose a Self-Supervised Curriculum Learning (SSCL) approach. Specifically, we directly use the cross-entropy loss as criteria for : 1) scoring the difficulty of training data, and 2) evaluating the competence of the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>. In our approach, CL improves the model training, which in return improves the CL measurement. In our experiments on the SIGHAN 2015 Chinese spelling check task, we show that SSCL is superior to previous norm-based and uncertainty-aware approaches, and establish a new state of the art (74.38 % F1).</abstract>
      <url hash="81d36531">2021.emnlp-main.281</url>
      <bibkey>gan-etal-2021-self</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.281</doi>
    </paper>
    <paper id="282">
      <title>Fix-Filter-Fix : Intuitively Connect Any Models for Effective Bug Fixing</title>
      <author><first>Haiwen</first><last>Hong</last></author>
      <author><first>Jingfeng</first><last>Zhang</last></author>
      <author><first>Yin</first><last>Zhang</last></author>
      <author><first>Yao</first><last>Wan</last></author>
      <author><first>Yulei</first><last>Sui</last></author>
      <pages>3495–3504</pages>
      <abstract>Locating and fixing bugs is a time-consuming task. Most neural machine translation (NMT) based approaches for automatically bug fixing lack generality and do not make full use of the rich information in the source code. In NMT-based bug fixing, we find some predicted code identical to the input <a href="https://en.wikipedia.org/wiki/Software_bug">buggy code</a> (called unchanged fix) in NMT-based approaches due to high similarity between buggy and fixed code (e.g., the difference may only appear in one particular line). Obviously, unchanged fix is not the correct fix because it is the same as the <a href="https://en.wikipedia.org/wiki/Software_bug">buggy code</a> that needs to be fixed. Based on these, we propose an intuitive yet effective general framework (called Fix-Filter-Fix or F3) for <a href="https://en.wikipedia.org/wiki/Patch_(computing)">bug fixing</a>. F3 connects models with our filter mechanism to filter out the last <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s unchanged fix to the next. We propose an F3 theory that can quantitatively and accurately calculate the F3 lifting effect. To evaluate, we implement the Seq2Seq Transformer (ST) and the AST2Seq Transformer (AT) to form some basic F3 instances, called F3_ST+AT and F3_AT+ST. Comparing them with single model approaches and many model connection baselines across four datasets validates the effectiveness and generality of F3 and corroborates our findings and methodology.</abstract>
      <url hash="b364cb3a">2021.emnlp-main.282</url>
      <bibkey>hong-etal-2021-fix</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.282</doi>
    </paper>
    <paper id="283">
      <title>Neuro-Symbolic Reinforcement Learning with First-Order Logic</title>
      <author><first>Daiki</first><last>Kimura</last></author>
      <author><first>Masaki</first><last>Ono</last></author>
      <author><first>Subhajit</first><last>Chaudhury</last></author>
      <author><first>Ryosuke</first><last>Kohita</last></author>
      <author><first>Akifumi</first><last>Wachi</last></author>
      <author><first>Don Joven</first><last>Agravante</last></author>
      <author><first>Michiaki</first><last>Tatsubori</last></author>
      <author><first>Asim</first><last>Munawar</last></author>
      <author><first>Alexander</first><last>Gray</last></author>
      <pages>3505–3511</pages>
      <abstract>Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained <a href="https://en.wikipedia.org/wiki/Policy">policies</a> is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neuro-symbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text observation and external word meaning network (ConceptNet), then train a <a href="https://en.wikipedia.org/wiki/Policy">policy</a> in the <a href="https://en.wikipedia.org/wiki/Social_network">network</a> with directly interpretable logical operators. Our experimental results show RL training with the proposed method converges significantly faster than other state-of-the-art neuro-symbolic methods in a TextWorld benchmark.</abstract>
      <url hash="7e53a383">2021.emnlp-main.283</url>
      <bibkey>kimura-etal-2021-neuro</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.283</doi>
    </paper>
    <paper id="284">
      <title>Biomedical Concept Normalization by Leveraging Hypernyms</title>
      <author><first>Cheng</first><last>Yan</last></author>
      <author><first>Yuanzhe</first><last>Zhang</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Yafei</first><last>Shi</last></author>
      <author><first>Shengping</first><last>Liu</last></author>
      <pages>3512–3517</pages>
      <abstract>Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation of hypernym-hyponym entity pairs. The experimental results show that BCNH outperforms the previous state-of-the-art <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on the NCBI dataset.</abstract>
      <url hash="8389c8ba">2021.emnlp-main.284</url>
      <bibkey>yan-etal-2021-biomedical</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.284</doi>
      <pwccode url="https://github.com/yan-cheng/bcnh" additional="false">yan-cheng/bcnh</pwccode>
    </paper>
    <paper id="285">
      <title>Leveraging Capsule Routing to Associate Knowledge with Medical Literature Hierarchically</title>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Qingcai</first><last>Chen</last></author>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Wenxiu</first><last>Zhou</last></author>
      <author><first>Tingyu</first><last>Liu</last></author>
      <author><first>Xinlan</first><last>Yang</last></author>
      <author><first>Weihua</first><last>Peng</last></author>
      <pages>3518–3532</pages>
      <abstract>Integrating knowledge into text is a promising way to enrich text representation, especially in the <a href="https://en.wikipedia.org/wiki/Medicine">medical field</a>. However, undifferentiated knowledge not only confuses the text representation but also imports unexpected noises. In this paper, to alleviate this problem, we propose leveraging capsule routing to associate knowledge with <a href="https://en.wikipedia.org/wiki/Medical_literature">medical literature</a> hierarchically (called HiCapsRKL). Firstly, HiCapsRKL extracts two empirically designed text fragments from <a href="https://en.wikipedia.org/wiki/Medical_literature">medical literature</a> and encodes them into fragment representations respectively. Secondly, the capsule routing algorithm is applied to two fragment representations. Through the capsule computing and <a href="https://en.wikipedia.org/wiki/Dynamic_routing">dynamic routing</a>, each representation is processed into a new representation (denoted as caps-representation), and we integrate the caps-representations as information gain to associate knowledge with <a href="https://en.wikipedia.org/wiki/Medical_literature">medical literature</a> hierarchically. Finally, HiCapsRKL are validated on relevance prediction and medical literature retrieval test sets. The experimental results and analyses show that HiCapsRKLcan more accurately associate knowledge with <a href="https://en.wikipedia.org/wiki/Medical_literature">medical literature</a> than mainstream methods. In summary, HiCapsRKL can efficiently help selecting the most relevant knowledge to the <a href="https://en.wikipedia.org/wiki/Medical_literature">medical literature</a>, which may be an alternative attempt to improve knowledge-based text representation. Source code is released on GitHub.</abstract>
      <url hash="25fdec70">2021.emnlp-main.285</url>
      <bibkey>liu-etal-2021-leveraging</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.285</doi>
      <pwccode url="https://github.com/gdls/hicapsrkl" additional="false">gdls/hicapsrkl</pwccode>
    </paper>
    <paper id="287">
      <title>SpellBERT : A Lightweight Pretrained Model for Chinese Spelling Check<fixed-case>S</fixed-case>pell<fixed-case>BERT</fixed-case>: A Lightweight Pretrained Model for <fixed-case>C</fixed-case>hinese Spelling Check</title>
      <author><first>Tuo</first><last>Ji</last></author>
      <author><first>Hang</first><last>Yan</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>3544–3551</pages>
      <abstract>Chinese Spelling Check (CSC) is to detect and correct Chinese spelling errors. Many models utilize a predefined confusion set to learn a <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mapping</a> between correct characters and its visually similar or phonetically similar misuses but the <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mapping</a> may be out-of-domain. To that end, we propose SpellBERT, a pretrained model with graph-based extra features and independent on confusion set. To explicitly capture the two erroneous patterns, we employ a graph neural network to introduce radical and pinyin information as visual and phonetic features. For better fusing these <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> with <a href="https://en.wikipedia.org/wiki/Character_encoding">character representations</a>, we devise masked language model alike <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">pre-training tasks</a>. With this feature-rich pre-training, SpellBERT with only half size of BERT can show competitive performance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set.</abstract>
      <url hash="ebce69c2">2021.emnlp-main.287</url>
      <bibkey>ji-etal-2021-spellbert</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.287</doi>
      <revision id="1" href="2021.emnlp-main.287v1" hash="28bee265" />
      <revision id="2" href="2021.emnlp-main.287v2" hash="ebce69c2" date="2022-03-20">Removed two citations.</revision>
      <pwccode url="https://github.com/benbijituo/spellbert" additional="false">benbijituo/spellbert</pwccode>
    </paper>
    <paper id="288">
      <title>Automated Generation of Accurate &amp; Fluent Medical X-ray Reports<fixed-case>X</fixed-case>-ray Reports</title>
      <author><first>Hoang</first><last>Nguyen</last></author>
      <author><first>Dong</first><last>Nie</last></author>
      <author><first>Taivanbat</first><last>Badamdorj</last></author>
      <author><first>Yujie</first><last>Liu</last></author>
      <author><first>Yingying</first><last>Zhu</last></author>
      <author><first>Jason</first><last>Truong</last></author>
      <author><first>Li</first><last>Cheng</last></author>
      <pages>3552–3569</pages>
      <abstract>Our paper aims to automate the generation of medical reports from chest X-ray image inputs, a critical yet time-consuming task for radiologists. Existing medical report generation efforts emphasize producing human-readable reports, yet the generated text may not be well aligned to the clinical facts. Our generated medical reports, on the other hand, are fluent and, more importantly, clinically accurate. This is achieved by our fully differentiable and end-to-end paradigm that contains three complementary modules : taking the chest X-ray images and clinical history document of patients as inputs, our classification module produces an internal checklist of disease-related topics, referred to as enriched disease embedding ; the embedding representation is then passed to our transformer-based generator, to produce the medical report ; meanwhile, our generator also creates a weighted embedding representation, which is fed to our interpreter to ensure consistency with respect to disease-related topics. Empirical evaluations demonstrate very promising results achieved by our approach on commonly-used metrics concerning <a href="https://en.wikipedia.org/wiki/Fluency">language fluency</a> and clinical accuracy. Moreover, noticeable performance gains are consistently observed when additional input information is available, such as the clinical document and extra scans from different views.</abstract>
      <url hash="ff258315">2021.emnlp-main.288</url>
      <bibkey>nguyen-etal-2021-automated</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.288</doi>
      <pwccode url="https://github.com/ginobilinie/xray_report_generation" additional="false">ginobilinie/xray_report_generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chexpert">CheXpert</pwcdataset>
    </paper>
    <paper id="295">
      <title>Enhancing Multiple-choice Machine Reading Comprehension by Punishing Illogical Interpretations</title>
      <author><first>Yiming</first><last>Ju</last></author>
      <author><first>Yuanzhe</first><last>Zhang</last></author>
      <author><first>Zhixing</first><last>Tian</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Xiaohuan</first><last>Cao</last></author>
      <author><first>Wenting</first><last>Zhao</last></author>
      <author><first>Jinlong</first><last>Li</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>3641–3652</pages>
      <abstract>Machine Reading Comprehension (MRC), which requires a machine to answer questions given the relevant documents, is an important way to test machines’ ability to understand <a href="https://en.wikipedia.org/wiki/Human_language">human language</a>. Multiple-choice MRC is one of the most studied tasks in <a href="https://en.wikipedia.org/wiki/Medical_Subject_Headings">MRC</a> due to the convenience of evaluation and the flexibility of answer format. Post-hoc interpretation aims to explain a trained <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and reveal how the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> arrives at the prediction. One of the most important interpretation forms is to attribute model decisions to input <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>. Based on post-hoc interpretation methods, we assess attributions of paragraphs in multiple-choice MRC and improve the model by punishing the illogical attributions. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> can improve model performance without any external information and model structure change. Furthermore, we also analyze how and why such a self-training method works.</abstract>
      <url hash="e6b9cc09">2021.emnlp-main.295</url>
      <bibkey>ju-etal-2021-enhancing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.295</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="296">
      <title>Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models</title>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>Rumei</first><last>Li</last></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Hongzhi</first><last>Zhang</last></author>
      <author><first>Zan</first><last>Daoguang</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>3653–3660</pages>
      <abstract>The key challenge of <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the textual information carried by the <a href="https://en.wikipedia.org/wiki/Vertex_(graph_theory)">nodes</a> and <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">edges</a>. Meanwhile, pre-trained language models learn massive open-world knowledge from the large corpus, but it is in the <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language form</a> and not structured. To bridge the gap between the natural language and the structured KB, we propose three relation learning tasks for BERT-based KBQA, including relation extraction, relation matching, and relation reasoning. By relation-augmented training, the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> learns to align the natural language expressions to the relations in the <a href="https://en.wikipedia.org/wiki/Knowledge_base">KB</a> as well as reason over the missing connections in the <a href="https://en.wikipedia.org/wiki/Knowledge_base">KB</a>. Experiments on WebQSP show that our method consistently outperforms other baselines, especially when the KB is incomplete.</abstract>
      <url hash="6bc5ec1b">2021.emnlp-main.296</url>
      <bibkey>yan-etal-2021-large</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.296</doi>
      <pwccode url="https://github.com/yym6472/kbqarelationlearning" additional="false">yym6472/kbqarelationlearning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="300">
      <title>FinQA : A Dataset of <a href="https://en.wikipedia.org/wiki/Numerical_analysis">Numerical Reasoning</a> over Financial Data<fixed-case>F</fixed-case>in<fixed-case>QA</fixed-case>: A Dataset of Numerical Reasoning over Financial Data</title>
      <author><first>Zhiyu</first><last>Chen</last></author>
      <author><first>Wenhu</first><last>Chen</last></author>
      <author><first>Charese</first><last>Smiley</last></author>
      <author><first>Sameena</first><last>Shah</last></author>
      <author><first>Iana</first><last>Borova</last></author>
      <author><first>Dylan</first><last>Langdon</last></author>
      <author><first>Reema</first><last>Moussa</last></author>
      <author><first>Matt</first><last>Beane</last></author>
      <author><first>Ting-Hao</first><last>Huang</last></author>
      <author><first>Bryan</first><last>Routledge</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>3697–3711</pages>
      <abstract>The sheer volume of <a href="https://en.wikipedia.org/wiki/Financial_statement">financial statements</a> makes it difficult for humans to access and analyze a business’s financials. Robust <a href="https://en.wikipedia.org/wiki/Numerical_analysis">numerical reasoning</a> likewise faces unique challenges in this <a href="https://en.wikipedia.org/wiki/Domain_of_a_function">domain</a>. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the <a href="https://en.wikipedia.org/wiki/Mathematical_finance">finance domain</a> includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, FinQA, with Question-Answering pairs over Financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> and conduct comprehensive experiments in our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset   the first of its kind   should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available at https://github.com/czyssrs/FinQA.</abstract>
      <url hash="c58cbf87">2021.emnlp-main.300</url>
      <bibkey>chen-etal-2021-finqa</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.300</doi>
      <pwccode url="https://github.com/czyssrs/finqa" additional="false">czyssrs/finqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/finqa">FinQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="301">
      <title>FiD-Ex : Improving Sequence-to-Sequence Models for Extractive Rationale Generation<fixed-case>F</fixed-case>i<fixed-case>D</fixed-case>-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation</title>
      <author><first>Kushal</first><last>Lakhotia</last></author>
      <author><first>Bhargavi</first><last>Paranjape</last></author>
      <author><first>Asish</first><last>Ghoshal</last></author>
      <author><first>Scott</first><last>Yih</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>Srini</first><last>Iyer</last></author>
      <pages>3712–3727</pages>
      <abstract>Natural language (NL) explanations of model predictions are gaining popularity as a means to understand and verify decisions made by large black-box pre-trained models, for tasks such as Question Answering (QA) and Fact Verification. Recently, pre-trained sequence to sequence (seq2seq) models have proven to be very effective in jointly making predictions, as well as generating NL explanations. However, these models have many shortcomings ; they can fabricate explanations even for incorrect predictions, they are difficult to adapt to long input documents, and their training requires a large amount of labeled data. In this paper, we develop FiD-Ex, which addresses these shortcomings for seq2seq models by : 1) introducing sentence markers to eliminate explanation fabrication by encouraging extractive generation, 2) using the fusion-in-decoder architecture to handle long input contexts, and 3) intermediate fine-tuning on re-structured open domain QA datasets to improve few-shot performance. FiD-Ex significantly improves over prior work in terms of explanation metrics and task accuracy on five tasks from the ERASER explainability benchmark in both fully supervised and few-shot settings.</abstract>
      <url hash="93756ecd">2021.emnlp-main.301</url>
      <bibkey>lakhotia-etal-2021-fid</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.301</doi>
    </paper>
    <paper id="302">
      <title>RockNER : A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models<fixed-case>R</fixed-case>ock<fixed-case>NER</fixed-case>: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models</title>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Wenyang</first><last>Gao</last></author>
      <author><first>Jun</first><last>Yan</last></author>
      <author><first>Ryan</first><last>Moreno</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>3728–3737</pages>
      <abstract>To audit the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of named entity recognition (NER) models, we propose RockNER, a simple yet effective method to create natural adversarial examples. Specifically, at the entity level, we replace target entities with other entities of the same semantic class in <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a> ; at the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context level</a>, we use pre-trained <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> (e.g., BERT) to generate word substitutions. Together, the two levels of at- tack produce natural adversarial examples that result in a shifted distribution from the training data on which our target models have been trained. We apply the proposed method to the OntoNotes dataset and create a new benchmark named OntoRock for evaluating the robustness of existing NER models via a systematic evaluation protocol. Our experiments and analysis reveal that even the best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> has a significant performance drop, and these models seem to memorize in-domain entity patterns instead of reasoning from the context. Our work also studies the effects of a few simple data augmentation methods to improve the <a href="https://en.wikipedia.org/wiki/Robust_statistics">robustness</a> of NER models.</abstract>
      <url hash="74dd8e81">2021.emnlp-main.302</url>
      <bibkey>lin-etal-2021-rockner</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.302</doi>
      <pwccode url="https://github.com/INK-USC/RockNER" additional="false">INK-USC/RockNER</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ontorock">OntoRock</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="305">
      <title>COUGH : A Challenge Dataset and Models for COVID-19 FAQ Retrieval<fixed-case>COUGH</fixed-case>: A Challenge Dataset and Models for <fixed-case>COVID</fixed-case>-19 <fixed-case>FAQ</fixed-case> Retrieval</title>
      <author><first>Xinliang Frederick</first><last>Zhang</last></author>
      <author><first>Heming</first><last>Sun</last></author>
      <author><first>Xiang</first><last>Yue</last></author>
      <author><first>Simon</first><last>Lin</last></author>
      <author><first>Huan</first><last>Sun</last></author>
      <pages>3759–3769</pages>
      <abstract>We present a large, challenging <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, COUGH, for COVID-19 FAQ retrieval. Similar to a standard <a href="https://en.wikipedia.org/wiki/FAQ">FAQ dataset</a>, COUGH consists of three parts : FAQ Bank, Query Bank and Relevance Set. The FAQ Bank contains ~16 K <a href="https://en.wikipedia.org/wiki/FAQ">FAQ items</a> scraped from 55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query Bank and Relevance Set, where the former contains 1,236 human-paraphrased queries while the latter contains ~32 human-annotated FAQ items for each query. We analyze COUGH by testing different FAQ retrieval models built on top of <a href="https://en.wikipedia.org/wiki/BM25">BM25</a> and BERT, among which the best <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> achieves 48.8 under P@5, indicating a great challenge presented by COUGH and encouraging future research for further improvement. Our COUGH dataset is available at https://github.com/sunlab-osu/covid-faq.</abstract>
      <url hash="d2a403c3">2021.emnlp-main.305</url>
      <bibkey>zhang-etal-2021-cough</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.305</doi>
      <pwccode url="https://github.com/sunlab-osu/covid-faq" additional="false">sunlab-osu/covid-faq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cough">COUGH</pwcdataset>
    </paper>
    <paper id="307">
      <title>WinoLogic : A Zero-Shot Logic-based Diagnostic Dataset for Winograd Schema Challenge<fixed-case>W</fixed-case>ino<fixed-case>L</fixed-case>ogic: <fixed-case>A</fixed-case> Zero-Shot Logic-based Diagnostic Dataset for <fixed-case>W</fixed-case>inograd <fixed-case>S</fixed-case>chema <fixed-case>C</fixed-case>hallenge</title>
      <author><first>Weinan</first><last>He</last></author>
      <author><first>Canming</first><last>Huang</last></author>
      <author><first>Yongmei</first><last>Liu</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <pages>3779–3789</pages>
      <abstract>The recent success of neural language models (NLMs) on the <a href="https://en.wikipedia.org/wiki/Winograd_Schema_Challenge">Winograd Schema Challenge</a> has called for further investigation of the commonsense reasoning ability of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Previous diagnostic datasets rely on <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowd-sourcing</a> which fails to provide coherent commonsense crucial for solving WSC problems. To better evaluate NLMs, we propose a logic-based framework that focuses on high-quality <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a>. Specifically, we identify and collect formal knowledge formulas verified by <a href="https://en.wikipedia.org/wiki/Automated_theorem_proving">theorem provers</a> and translate such <a href="https://en.wikipedia.org/wiki/Well-formed_formula">formulas</a> into <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language sentences</a>. Based on these true knowledge sentences, adversarial false ones are generated. We propose a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> named WinoLogic with these sentences. Given a problem in WinoLogic, NLMs need to decide whether the plausible knowledge sentences could correctly solve the corresponding WSC problems in a zero-shot setting. We also ask human annotators to validate WinoLogic to ensure it is human-agreeable. Experiments show that NLMs still struggle to comprehend commonsense knowledge as humans do, indicating that their reasoning ability could have been overestimated.</abstract>
      <url hash="6e11a1ce">2021.emnlp-main.307</url>
      <attachment type="Software" hash="af7b7a6e">2021.emnlp-main.307.Software.zip</attachment>
      <bibkey>he-etal-2021-winologic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.307</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="309">
      <title>Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast</title>
      <author><first>Liang</first><last>Wang</last></author>
      <author><first>Wei</first><last>Zhao</last></author>
      <author><first>Jingming</first><last>Liu</last></author>
      <pages>3807–3815</pages>
      <abstract>In this paper, we propose to align sentence representations from different languages into a unified embedding space, where semantic similarities (both cross-lingual and monolingual) can be computed with a simple <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a>. Pre-trained <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> are fine-tuned with the translation ranking task. Existing work (Feng et al., 2020) uses sentences within the same batch as negatives, which can suffer from the issue of easy negatives. We adapt <a href="https://en.wikipedia.org/wiki/Molybdenum_disulfide">MoCo</a> (He et al., 2020) to further improve the quality of <a href="https://en.wikipedia.org/wiki/Sequence_alignment">alignment</a>. As the experimental results show, the sentence representations produced by our model achieve the new state-of-the-art on several tasks, including Tatoeba en-zh similarity search (Artetxe andSchwenk, 2019b), BUCC en-zh bitext mining, and semantic textual similarity on 7 datasets.</abstract>
      <url hash="b5197fa8">2021.emnlp-main.309</url>
      <bibkey>wang-etal-2021-aligning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.309</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="315">
      <title>Virtual Data Augmentation : A Robust and General Framework for Fine-tuning Pre-trained Models</title>
      <author><first>Kun</first><last>Zhou</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>3875–3887</pages>
      <abstract>Recent works have shown that powerful pre-trained language models (PLM) can be fooled by small perturbations or intentional attacks. To solve this issue, various data augmentation techniques are proposed to improve the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of PLMs. However, it is still challenging to augment semantically relevant examples with sufficient diversity. In this work, we present Virtual Data Augmentation (VDA), a general framework for robustly fine-tuning PLMs. Based on the original token embeddings, we construct a multinomial mixture for augmenting virtual data embeddings, where a masked language model guarantees the semantic relevance and the <a href="https://en.wikipedia.org/wiki/Gaussian_noise">Gaussian noise</a> provides the augmentation diversity. Furthermore, a regularized training strategy is proposed to balance the two aspects. Extensive experiments on six datasets show that our approach is able to improve the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of PLMs and alleviate the performance degradation under adversarial attacks. Our codes and data are publicly available at blue.<url>https://github.com/RUCAIBox/VDA</url>.</abstract>
      <url hash="3eab498f">2021.emnlp-main.315</url>
      <bibkey>zhou-etal-2021-virtual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.315</doi>
      <pwccode url="https://github.com/rucaibox/vda" additional="false">rucaibox/vda</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="317">
      <title>To be Closer : Learning to Link up Aspects with Opinions</title>
      <author><first>Yuxiang</first><last>Zhou</last></author>
      <author><first>Lejian</first><last>Liao</last></author>
      <author><first>Yang</first><last>Gao</last></author>
      <author><first>Zhanming</first><last>Jie</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>3899–3909</pages>
      <abstract>Dependency parse trees are helpful for discovering the opinion words in aspect-based sentiment analysis (ABSA) (CITATION). However, the <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">trees</a> obtained from off-the-shelf dependency parsers are static, and could be sub-optimal in ABSA. This is because the syntactic trees are not designed for capturing the interactions between opinion words and aspect words. In this work, we aim to shorten the distance between <a href="https://en.wikipedia.org/wiki/Grammatical_aspect">aspects</a> and corresponding opinion words by learning an aspect-centric tree structure. The aspect and opinion words are expected to be closer along such <a href="https://en.wikipedia.org/wiki/Tree_structure">tree structure</a> compared to the standard dependency parse tree. The learning process allows the <a href="https://en.wikipedia.org/wiki/Tree_structure">tree structure</a> to adaptively correlate the aspect and opinion words, enabling us to better identify the polarity in the ABSA task. We conduct experiments on five aspect-based sentiment datasets, and the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> significantly outperforms recent strong baselines. Furthermore, our thorough analysis demonstrates the average distance between aspect and opinion words are shortened by at least 19 % on the standard SemEval Restaurant14 (CITATION) dataset.</abstract>
      <url hash="d4c66633">2021.emnlp-main.317</url>
      <attachment type="Software" hash="7213fab1">2021.emnlp-main.317.Software.zip</attachment>
      <bibkey>zhou-etal-2021-closer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.317</doi>
      <pwccode url="https://github.com/zyxnlp/aclt" additional="false">zyxnlp/aclt</pwccode>
    </paper>
    <paper id="319">
      <title>Argument Pair Extraction with Mutual Guidance and Inter-sentence Relation Graph</title>
      <author><first>Jianzhu</first><last>Bao</last></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Jingyi</first><last>Sun</last></author>
      <author><first>Yice</first><last>Zhang</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>3923–3934</pages>
      <abstract>Argument pair extraction (APE) aims to extract interactive argument pairs from two passages of a discussion. Previous work studied this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> in the context of peer review and rebuttal, and decomposed it into a sequence labeling task and a sentence relation classification task. However, despite the promising performance, such an approach obtains the argument pairs implicitly by the two decomposed tasks, lacking explicitly modeling of the argument-level interactions between argument pairs. In this paper, we tackle the APE task by a mutual guidance framework, which could utilize the information of an argument in one passage to guide the identification of arguments that can form pairs with it in another passage. In this manner, two passages can mutually guide each other in the process of APE. Furthermore, we propose an inter-sentence relation graph to effectively model the inter-relations between two sentences and thus facilitates the extraction of argument pairs. Our proposed method can better represent the holistic argument-level semantics and thus explicitly capture the complex correlations between argument pairs. Experimental results show that our approach significantly outperforms the current <a href="https://en.wikipedia.org/wiki/State-of-the-art">state-of-the-art model</a>.</abstract>
      <url hash="50f124ad">2021.emnlp-main.319</url>
      <bibkey>bao-etal-2021-argument</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.319</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/rr">RR</pwcdataset>
    </paper>
    <paper id="321">
      <title>Improving Federated Learning for Aspect-based Sentiment Analysis via Topic Memories</title>
      <author><first>Han</first><last>Qin</last></author>
      <author><first>Guimin</first><last>Chen</last></author>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>3942–3954</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) predicts the sentiment polarity towards a particular aspect term in a sentence, which is an important task in real-world applications. To perform ABSA, the trained <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> is required to have a good understanding of the contextual information, especially the particular patterns that suggest the sentiment polarity. However, these patterns typically vary in different sentences, especially when the sentences come from different sources (domains), which makes ABSA still very challenging. Although combining labeled data across different sources (domains) is a promising solution to address the challenge, in practical applications, these labeled data are usually stored at different locations and might be inaccessible to each other due to privacy or legal concerns (e.g., the data are owned by different companies). To address this issue and make the best use of all labeled data, we propose a novel ABSA model with federated learning (FL) adopted to overcome the data isolation limitations and incorporate topic memory (TM) proposed to take the cases of data from diverse sources (domains) into consideration. Particularly, TM aims to identify different isolated data sources due to data inaccessibility by providing useful <a href="https://en.wikipedia.org/wiki/Categorical_variable">categorical information</a> for localized predictions. Experimental results on a simulated environment for <a href="https://en.wikipedia.org/wiki/FL_(programming_language)">FL</a> with three nodes demonstrate the effectiveness of our approach, where TM-FL outperforms different baselines including some well-designed <a href="https://en.wikipedia.org/wiki/FL_(programming_language)">FL frameworks</a>.</abstract>
      <url hash="7975515f">2021.emnlp-main.321</url>
      <bibkey>qin-etal-2021-improving-federated</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.321</doi>
    </paper>
    <paper id="323">
      <title>CTAL : Pre-training Cross-modal Transformer for Audio-and-Language Representations<fixed-case>CTAL</fixed-case>: Pre-training Cross-modal Transformer for Audio-and-Language Representations</title>
      <author><first>Hang</first><last>Li</last></author>
      <author><first>Wenbiao</first><last>Ding</last></author>
      <author><first>Yu</first><last>Kang</last></author>
      <author><first>Tianqiao</first><last>Liu</last></author>
      <author><first>Zhongqin</first><last>Wu</last></author>
      <author><first>Zitao</first><last>Liu</last></author>
      <pages>3966–3977</pages>
      <abstract>Existing audio-language task-specific predictive approaches focus on building complicated late-fusion mechanisms. However, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are facing challenges of <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> with limited labels and low model generalization abilities. In this paper, we present a Cross-modal Transformer for Audio-and-Language, i.e., CTAL, which aims to learn the intra-modality and inter-modality connections between audio and language through two proxy tasks on a large amount of audio-and-language pairs : masked language modeling and masked cross-modal acoustic modeling. After fine-tuning our pre-trained model on multiple downstream audio-and-language tasks, we observe significant improvements across various tasks, such as, <a href="https://en.wikipedia.org/wiki/Emotion_classification">emotion classification</a>, <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, and <a href="https://en.wikipedia.org/wiki/Speaker_verification">speaker verification</a>. On this basis, we further propose a specially-designed fusion mechanism that can be used in fine-tuning phase, which allows our pre-trained model to achieve better performance. Lastly, we demonstrate detailed ablation studies to prove that both our novel cross-modality fusion component and audio-language pre-training methods significantly contribute to the promising results. The code and pre-trained models are available at https://github.com/tal-ai/CTAL_EMNLP2021.</abstract>
      <url hash="c4cd7d64">2021.emnlp-main.323</url>
      <bibkey>li-etal-2021-ctal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.323</doi>
      <pwccode url="https://github.com/ydkwim/ctal" additional="false">ydkwim/ctal</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="325">
      <title>Mutual-Learning Improves End-to-End Speech Translation</title>
      <author><first>Jiawei</first><last>Zhao</last></author>
      <author><first>Wei</first><last>Luo</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Andrew</first><last>Gilman</last></author>
      <pages>3989–3994</pages>
      <abstract>A currently popular research area in end-to-end speech translation is the use of knowledge distillation from a <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation (MT) task</a> to improve the <a href="https://en.wikipedia.org/wiki/Speech_translation">speech translation (ST) task</a>. However, such scenario obviously only allows one way transfer, which is limited by the performance of the teacher model. Therefore, We hypothesis that the knowledge distillation-based approaches are sub-optimal. In this paper, we propose an alternativea trainable mutual-learning scenario, where the MT and the ST models are collaboratively trained and are considered as peers, rather than teacher / student. This allows us to improve the performance of end-to-end ST more effectively than with a teacher-student paradigm. As a side benefit, performance of the MT model also improves. Experimental results show that in our mutual-learning scenario, <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can effectively utilise the auxiliary information from peer models and achieve compelling results on Must-C dataset.</abstract>
      <url hash="29196ee5">2021.emnlp-main.325</url>
      <bibkey>zhao-etal-2021-mutual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.325</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="328">
      <title>Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments<fixed-case>LAW</fixed-case>) Supervision for Vision-and-Language Navigation in Continuous Environments</title>
      <author><first>Sonia</first><last>Raychaudhuri</last></author>
      <author><first>Saim</first><last>Wani</last></author>
      <author><first>Shivansh</first><last>Patel</last></author>
      <author><first>Unnat</first><last>Jain</last></author>
      <author><first>Angel</first><last>Chang</last></author>
      <pages>4018–4028</pages>
      <abstract>In the Vision-and-Language Navigation (VLN) task an embodied agent navigates a 3D environment, following natural language instructions. A challenge in this task is how to handle ‘off the path’ scenarios where an agent veers from a reference path. Prior work supervises the agent with actions based on the shortest path from the agent’s location to the goal, but such goal-oriented supervision is often not in alignment with the instruction. Furthermore, the evaluation metrics employed by prior work do not measure how much of a language instruction the agent is able to follow. In this work, we propose a simple and effective language-aligned supervision scheme, and a new <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> that measures the number of sub-instructions the agent has completed during <a href="https://en.wikipedia.org/wiki/Navigation">navigation</a>.</abstract>
      <url hash="342014f2">2021.emnlp-main.328</url>
      <bibkey>raychaudhuri-etal-2021-language</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.328</doi>
    </paper>
    <paper id="329">
      <title>How to leverage the multimodal EHR data for better medical prediction?<fixed-case>EHR</fixed-case> data for better medical prediction?</title>
      <author><first>Bo</first><last>Yang</last></author>
      <author><first>Lijun</first><last>Wu</last></author>
      <pages>4029–4038</pages>
      <abstract>Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> to improve the quality of service and reduce costs. However, the complexity of electronic health records (EHR) data is a challenge for the application of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>. Specifically, the data produced in the hospital admissions are monitored by the <a href="https://en.wikipedia.org/wiki/Electronic_health_record">EHR system</a>, which includes structured data like <a href="https://en.wikipedia.org/wiki/Thermoregulation">daily body temperature</a> and <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured data</a> like free text and laboratory measurements. Although there are some preprocessing frameworks proposed for specific <a href="https://en.wikipedia.org/wiki/Electronic_health_record">EHR data</a>, the clinical notes that contain significant clinical value are beyond the realm of their consideration. Besides, whether these different <a href="https://en.wikipedia.org/wiki/Data">data</a> from various views are all beneficial to the medical tasks and how to best utilize these <a href="https://en.wikipedia.org/wiki/Data">data</a> remain unclear. Therefore, in this paper, we first extract the accompanying clinical notes from <a href="https://en.wikipedia.org/wiki/Electronic_health_record">EHR</a> and propose a method to integrate these <a href="https://en.wikipedia.org/wiki/Data">data</a>, we also comprehensively study the different models and the <a href="https://en.wikipedia.org/wiki/Data">data leverage methods</a> for better medical task prediction performance. The results on two prediction tasks show that our fused model with different data outperforms the state-of-the-art method without clinical notes, which illustrates the importance of our fusion method and the clinical note features.</abstract>
      <url hash="e3eea29c">2021.emnlp-main.329</url>
      <bibkey>yang-wu-2021-leverage</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.329</doi>
    </paper>
    <paper id="331">
      <title>Frame Semantic-Enhanced Sentence Modeling for Sentence-level Extractive Text Summarization</title>
      <author><first>Yong</first><last>Guan</last></author>
      <author><first>Shaoru</first><last>Guo</last></author>
      <author><first>Ru</first><last>Li</last></author>
      <author><first>Xiaoli</first><last>Li</last></author>
      <author><first>Hongye</first><last>Tan</last></author>
      <pages>4045–4052</pages>
      <abstract>Sentence-level extractive text summarization aims to select important sentences from a given document. However, it is very challenging to model the importance of sentences. In this paper, we propose a novel Frame Semantic-Enhanced Sentence Modeling for Extractive Summarization, which leverages <a href="https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)">Frame semantics</a> to model sentences from both intra-sentence level and inter-sentence level, facilitating the text summarization task. In particular, intra-sentence level semantics leverage Frames and Frame Elements to model internal semantic structure within a sentence, while inter-sentence level semantics leverage Frame-to-Frame relations to model relationships among sentences. Extensive experiments on two benchmark corpus CNN / DM and NYT demonstrate that our model outperforms six state-of-the-art methods significantly.</abstract>
      <url hash="a1101768">2021.emnlp-main.331</url>
      <bibkey>guan-etal-2021-frame</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.331</doi>
    </paper>
    <paper id="332">
      <title>CAST : Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees<fixed-case>CAST</fixed-case>: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees</title>
      <author><first>Ensheng</first><last>Shi</last></author>
      <author><first>Yanlin</first><last>Wang</last></author>
      <author><first>Lun</first><last>Du</last></author>
      <author><first>Hongyu</first><last>Zhang</last></author>
      <author><first>Shi</first><last>Han</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <author><first>Hongbin</first><last>Sun</last></author>
      <pages>4053–4062</pages>
      <abstract>Code summarization aims to generate concise natural language descriptions of source code, which can help improve program comprehension and maintenance. Recent studies show that syntactic and structural information extracted from abstract syntax trees (ASTs) is conducive to summary generation. However, existing approaches fail to fully capture the rich information in ASTs because of the large size / depth of ASTs. In this paper, we propose a novel model CAST that hierarchically splits and reconstructs ASTs. First, we hierarchically split a large <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a> into a set of <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">subtrees</a> and utilize a <a href="https://en.wikipedia.org/wiki/Recursive_neural_network">recursive neural network</a> to encode the <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">subtrees</a>. Then, we aggregate the embeddings of subtrees by reconstructing the split ASTs to get the representation of the complete AST. Finally, AST representation, together with source code embedding obtained by a vanilla code token encoder, is used for <a href="https://en.wikipedia.org/wiki/Automatic_programming">code summarization</a>. Extensive experiments, including the ablation study and the human evaluation, on benchmarks have demonstrated the power of CAST. To facilitate reproducibility, our code and data are available at https://github.com/DeepSoftwareAnalytics/CAST.</abstract>
      <url hash="99a6e237">2021.emnlp-main.332</url>
      <bibkey>shi-etal-2021-cast</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.332</doi>
      <pwccode url="https://github.com/DeepSoftwareAnalytics/CAST" additional="false">DeepSoftwareAnalytics/CAST</pwccode>
    </paper>
    <paper id="335">
      <title>Transformer-based Lexically Constrained Headline Generation</title>
      <author><first>Kosuke</first><last>Yamada</last></author>
      <author><first>Yuta</first><last>Hitomi</last></author>
      <author><first>Hideaki</first><last>Tamori</last></author>
      <author><first>Ryohei</first><last>Sasano</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <author><first>Koichi</first><last>Takeda</last></author>
      <pages>4085–4090</pages>
      <abstract>This paper explores a variant of automatic headline generation methods, where a generated headline is required to include a given phrase such as a company or a product name. Previous methods using Transformer-based models generate a <a href="https://en.wikipedia.org/wiki/Headline">headline</a> including a given phrase by providing the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> with additional information corresponding to the given phrase. However, these methods can not always include the phrase in the generated <a href="https://en.wikipedia.org/wiki/Headline">headline</a>. Inspired by previous RNN-based methods generating token sequences in backward and forward directions from the given phrase, we propose a simple Transformer-based method that guarantees to include the given phrase in the high-quality generated headline. We also consider a new headline generation strategy that takes advantage of the controllable generation order of Transformer. Our experiments with the Japanese News Corpus demonstrate that our methods, which are guaranteed to include the phrase in the generated <a href="https://en.wikipedia.org/wiki/Headline">headline</a>, achieve ROUGE scores comparable to previous Transformer-based methods. We also show that our generation strategy performs better than previous <a href="https://en.wikipedia.org/wiki/Strategy">strategies</a>.</abstract>
      <url hash="07446807">2021.emnlp-main.335</url>
      <bibkey>yamada-etal-2021-transformer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.335</doi>
      <pwccode url="https://github.com/asahi-research/script-for-transformer-based-seq2bf" additional="false">asahi-research/script-for-transformer-based-seq2bf</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/jnc">JNC</pwcdataset>
    </paper>
    <paper id="337">
      <title>Gradient-Based Adversarial Factual Consistency Evaluation for Abstractive Summarization</title>
      <author><first>Zhiyuan</first><last>Zeng</last></author>
      <author><first>Jiaze</first><last>Chen</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>4102–4108</pages>
      <abstract>Neural abstractive summarization systems have gained significant progress in recent years. However, abstractive summarization often produce inconsisitent statements or false facts. How to automatically generate highly abstract yet factually correct summaries? In this paper, we proposed an efficient weak-supervised adversarial data augmentation approach to form the factual consistency dataset. Based on the artificial dataset, we train an evaluation model that can not only make accurate and robust factual consistency discrimination but is also capable of making interpretable factual errors tracing by backpropagated gradient distribution on token embeddings. Experiments and analysis conduct on public annotated summarization and factual consistency datasets demonstrate our approach effective and reasonable.</abstract>
      <url hash="fef4caa7">2021.emnlp-main.337</url>
      <bibkey>zeng-etal-2021-gradient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.337</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="339">
      <title>A Unified Encoding of Structures in Transition Systems</title>
      <author><first>Tao</first><last>Ji</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Tao</first><last>Wang</last></author>
      <author><first>Zhongqiang</first><last>Huang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Xiaoling</first><last>Wang</last></author>
      <pages>4121–4133</pages>
      <abstract>Transition systems usually contain various <a href="https://en.wikipedia.org/wiki/Dynamical_system">dynamic structures</a> (e.g., <a href="https://en.wikipedia.org/wiki/Stack_(abstract_data_type)">stacks</a>, buffers). An ideal transition-based model should encode these <a href="https://en.wikipedia.org/wiki/Mathematical_structure">structures</a> completely and efficiently. Previous works relying on <a href="https://en.wikipedia.org/wiki/Template_processor">templates</a> or neural network structures either only encode partial structure information or suffer from computation efficiency. In this paper, we propose a novel attention-based encoder unifying representation of all structures in a <a href="https://en.wikipedia.org/wiki/Transition_system">transition system</a>. Specifically, we separate two views of items on structures, namely structure-invariant view and structure-dependent view. With the help of parallel-friendly attention network, we are able to encoding <a href="https://en.wikipedia.org/wiki/Transition_state">transition states</a> with O(1) additional complexity (with respect to basic feature extractors). Experiments on the PTB and UD show that our proposed method significantly improves the test speed and achieves the best transition-based model, and is comparable to state-of-the-art methods.</abstract>
      <url hash="0198ae65">2021.emnlp-main.339</url>
      <attachment type="Software" hash="2b236b7f">2021.emnlp-main.339.Software.zip</attachment>
      <bibkey>ji-etal-2021-unified</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.339</doi>
    </paper>
    <paper id="342">
      <title>Topic Transferable Table Question Answering</title>
      <author><first>Saneem</first><last>Chemmengath</last></author>
      <author><first>Vishwajeet</first><last>Kumar</last></author>
      <author><first>Samarth</first><last>Bharadwaj</last></author>
      <author><first>Jaydeep</first><last>Sen</last></author>
      <author><first>Mustafa</first><last>Canim</last></author>
      <author><first>Soumen</first><last>Chakrabarti</last></author>
      <author><first>Alfio</first><last>Gliozzo</last></author>
      <author><first>Karthik</first><last>Sankaranarayanan</last></author>
      <pages>4159–4172</pages>
      <abstract>Weakly-supervised table question-answering (TableQA) models have achieved state-of-art performance by using pre-trained BERT transformer to jointly encoding a question and a table to produce structured query for the question. However, in practical settings TableQA systems are deployed over table corpora having topic and word distributions quite distinct from BERT’s pretraining corpus. In this work we simulate the practical topic shift scenario by designing novel challenge benchmarks WikiSQL-TS and WikiTable-TS, consisting of train-dev-test splits in five distinct topic groups, based on the popular WikiSQL and WikiTable-Questions datasets. We empirically show that, despite pre-training on large open-domain text, performance of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> degrades significantly when they are evaluated on unseen topics. In response, we propose T3QA (Topic Transferable Table Question Answering) a pragmatic adaptation framework for TableQA comprising of : (1) topic-specific vocabulary injection into BERT, (2) a novel text-to-text transformer generator (such as T5, GPT2) based natural language question generation pipeline focused on generating topic-specific training data, and (3) a logical form re-ranker. We show that T3QA provides a reasonably good baseline for our topic shift benchmarks. We believe our topic split benchmarks will lead to robust TableQA solutions that are better suited for practical deployment</abstract>
      <url hash="2c1cf0d2">2021.emnlp-main.342</url>
      <bibkey>chemmengath-etal-2021-topic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.342</doi>
      <pwccode url="https://github.com/ibm/t3qa" additional="false">ibm/t3qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitablequestions">WikiTableQuestions</pwcdataset>
    </paper>
    <paper id="343">
      <title>WebSRC : A Dataset for Web-Based Structural Reading Comprehension<fixed-case>W</fixed-case>eb<fixed-case>SRC</fixed-case>: A Dataset for Web-Based Structural Reading Comprehension</title>
      <author><first>Xingyu</first><last>Chen</last></author>
      <author><first>Zihan</first><last>Zhao</last></author>
      <author><first>Lu</first><last>Chen</last></author>
      <author><first>JiaBao</first><last>Ji</last></author>
      <author><first>Danyang</first><last>Zhang</last></author>
      <author><first>Ao</first><last>Luo</last></author>
      <author><first>Yuxuan</first><last>Xiong</last></author>
      <author><first>Kai</first><last>Yu</last></author>
      <pages>4173–4185</pages>
      <abstract>Web search is an essential way for humans to obtain information, but it’s still a great challenge for machines to understand the contents of web pages. In this paper, we introduce the task of web-based structural reading comprehension. Given a <a href="https://en.wikipedia.org/wiki/Web_page">web page</a> and a question about it, the task is to find an answer from the web page. This <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> requires a system not only to understand the <a href="https://en.wikipedia.org/wiki/Semantics">semantics of texts</a> but also the <a href="https://en.wikipedia.org/wiki/Web_design">structure of the web page</a>. Moreover, we proposed WebSRC, a novel Web-based Structural Reading Comprehension dataset. WebSRC consists of 400 K question-answer pairs, which are collected from 6.4 K web pages with corresponding HTML source code, <a href="https://en.wikipedia.org/wiki/Screenshot">screenshots</a>, and <a href="https://en.wikipedia.org/wiki/Metadata">metadata</a>. Each question in WebSRC requires a certain structural understanding of a web page to answer, and the answer is either a text span on the web page or yes / no. We evaluate various strong <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> on our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> to show the difficulty of our <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. We also investigate the usefulness of structural information and <a href="https://en.wikipedia.org/wiki/Visual_system">visual features</a>. Our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and baselines have been publicly available.</abstract>
      <url hash="12e79825">2021.emnlp-main.343</url>
      <bibkey>chen-etal-2021-websrc</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.343</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="344">
      <title>Cryptonite : A Cryptic Crossword Benchmark for Extreme Ambiguity in Language</title>
      <author><first>Avia</first><last>Efrat</last></author>
      <author><first>Uri</first><last>Shaham</last></author>
      <author><first>Dan</first><last>Kilman</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>4186–4192</pages>
      <abstract>Current NLP datasets targeting <a href="https://en.wikipedia.org/wiki/Ambiguity">ambiguity</a> can be solved by a native speaker with relative ease. We present <a href="https://en.wikipedia.org/wiki/Cryptonite">Cryptonite</a>, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in <a href="https://en.wikipedia.org/wiki/Cryptonite">Cryptonite</a> is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a challenge even for experienced solvers, though top-tier experts can solve them with almost 100 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>. Cryptonite is a challenging task for current <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> ; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, on par with the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of a rule-based clue solver (8.6 %).</abstract>
      <url hash="79aef513">2021.emnlp-main.344</url>
      <bibkey>efrat-etal-2021-cryptonite</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.344</doi>
      <pwccode url="https://github.com/aviaefrat/cryptonite" additional="false">aviaefrat/cryptonite</pwccode>
    </paper>
    <paper id="346">
      <title>Improving Query Graph Generation for Complex Question Answering over Knowledge Base</title>
      <author><first>Kechen</first><last>Qin</last></author>
      <author><first>Cheng</first><last>Li</last></author>
      <author><first>Virgil</first><last>Pavlu</last></author>
      <author><first>Javed</first><last>Aslam</last></author>
      <pages>4201–4207</pages>
      <abstract>Most of the existing Knowledge-based Question Answering (KBQA) methods first learn to map the given question to a query graph, and then convert the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> to an executable query to find the answer. The query graph is typically expanded progressively from the topic entity based on a sequence prediction model. In this paper, we propose a new solution to query graph generation that works in the opposite manner : we start with the entire <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a> and gradually shrink it to the desired query graph. This approach improves both the <a href="https://en.wikipedia.org/wiki/Efficiency">efficiency</a> and the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of query graph generation, especially for complex multi-hop questions. Experimental results show that our method achieves state-of-the-art performance on ComplexWebQuestion (CWQ) dataset.</abstract>
      <url hash="54ebb3cd">2021.emnlp-main.346</url>
      <bibkey>qin-etal-2021-improving-query</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.346</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="349">
      <title>Generic resources are what you need : Style transfer tasks without task-specific parallel training data</title>
      <author><first>Huiyuan</first><last>Lai</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>4241–4254</pages>
      <abstract>Style transfer aims to rewrite a source text in a different target style while preserving its content. We propose a novel approach to this task that leverages generic resources, and without using any task-specific parallel (sourcetarget) data outperforms existing unsupervised approaches on the two most popular style transfer tasks : formality transfer and polarity swap. In practice, we adopt a multi-step procedure which builds on a generic pre-trained sequence-to-sequence model (BART). First, we strengthen the model’s ability to rewrite by further pre-training BART on both an existing collection of generic paraphrases, as well as on synthetic pairs created using a general-purpose lexical resource. Second, through an iterative back-translation approach, we train two models, each in a transfer direction, so that they can provide each other with synthetically generated pairs, dynamically in the training process. Lastly, we let our best resulting <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> generate static synthetic pairs to be used in a supervised training regime. Besides methodology and state-of-the-art results, a core contribution of this work is a reflection on the nature of the two <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> we address, and how their differences are highlighted by their response to our approach.</abstract>
      <url hash="4b8c3c7b">2021.emnlp-main.349</url>
      <bibkey>lai-etal-2021-generic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.349</doi>
      <pwccode url="https://github.com/laihuiyuan/generic-resources-for-tst" additional="false">laihuiyuan/generic-resources-for-tst</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="350">
      <title>Revisiting Pivot-Based Paraphrase Generation : Language Is Not the Only Optional Pivot</title>
      <author><first>Yitao</first><last>Cai</last></author>
      <author><first>Yue</first><last>Cao</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>4255–4268</pages>
      <abstract>Paraphrases refer to texts that convey the same meaning with different expression forms. Pivot-based methods, also known as the <a href="https://en.wikipedia.org/wiki/Round-trip_translation">round-trip translation</a>, have shown promising results in generating high-quality paraphrases. However, existing pivot-based methods all rely on language as the pivot, where large-scale, high-quality parallel bilingual texts are required. In this paper, we explore the feasibility of using semantic and syntactic representations as the pivot for <a href="https://en.wikipedia.org/wiki/Paraphrase_generation">paraphrase generation</a>. Concretely, we transform a sentence into a variety of different semantic or syntactic representations (including AMR, UD, and latent semantic representation), and then decode the sentence back from the semantic representations. We further explore a pretraining-based approach to compress the pipeline process into an end-to-end framework. We conduct experiments comparing different approaches with different kinds of pivots. Experimental results show that taking AMR as pivot can obtain <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> with better quality than taking <a href="https://en.wikipedia.org/wiki/Language">language</a> as the pivot. The end-to-end framework can reduce <a href="https://en.wikipedia.org/wiki/Semantic_shift">semantic shift</a> when language is used as the pivot. Besides, several unsupervised pivot-based methods can generate <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> with similar quality as the supervised sequence-to-sequence model, which indicates that parallel data of paraphrases may not be necessary for <a href="https://en.wikipedia.org/wiki/Paraphrase_generation">paraphrase generation</a>.</abstract>
      <url hash="4f481e59">2021.emnlp-main.350</url>
      <bibkey>cai-etal-2021-revisiting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.350</doi>
    </paper>
    <paper id="356">
      <title>DuRecDial 2.0 : A Bilingual Parallel Corpus for Conversational Recommendation<fixed-case>D</fixed-case>u<fixed-case>R</fixed-case>ec<fixed-case>D</fixed-case>ial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation</title>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Zheng-Yu</first><last>Niu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <pages>4335–4347</pages>
      <abstract>In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between DuRecDial 2.0 and existing conversational recommendation datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in DuRecDial 2.0 is annotated in two languages, both <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, while other datasets are built with the setting of a single language. We collect 8.2k dialogs aligned across English and Chinese languages (16.5k dialogs and 255k utterances in total) that are annotated by crowdsourced workers with strict quality control procedure. We then build monolingual, multilingual, and cross-lingual conversational recommendation baselines on DuRecDial 2.0. Experiment results show that the use of additional English data can bring performance improvement for Chinese conversational recommendation, indicating the benefits of DuRecDial 2.0. Finally, this dataset provides a challenging testbed for future studies of monolingual, multilingual, and cross-lingual conversational recommendation.</abstract>
      <url hash="d8bb4476">2021.emnlp-main.356</url>
      <bibkey>liu-etal-2021-durecdial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.356</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/durecdial">DuRecDial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/redial">ReDial</pwcdataset>
    </paper>
    <paper id="357">
      <title>End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs</title>
      <author><first>Dinesh</first><last>Raghu</last></author>
      <author><first>Shantanu</first><last>Agarwal</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <author><first /><last>Mausam</last></author>
      <pages>4348–4366</pages>
      <abstract>We propose a novel <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> within end-to-end learning of task oriented dialogs (TOD), in which the dialog system mimics a troubleshooting agent who helps a user by diagnosing their problem (e.g., car not starting). Such dialogs are grounded in domain-specific flowcharts, which the agent is supposed to follow during the conversation. Our task exposes novel technical challenges for neural TOD, such as grounding an utterance to the <a href="https://en.wikipedia.org/wiki/Flowchart">flowchart</a> without explicit annotation, referring to additional manual pages when user asks a clarification question, and ability to follow unseen flowcharts at test time. We release a dataset (FLODIAL) consisting of 2,738 dialogs grounded on 12 different troubleshooting flowcharts. We also design a neural model, FLONET, which uses a retrieval-augmented generation architecture to train the dialog agent. Our experiments find that FLONET can do zero-shot transfer to unseen flowcharts, and sets a strong baseline for future research.</abstract>
      <url hash="9857fbd2">2021.emnlp-main.357</url>
      <bibkey>raghu-etal-2021-end</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.357</doi>
      <pwccode url="https://github.com/dair-iitd/flonet" additional="false">dair-iitd/flonet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/flodial">FloDial</pwcdataset>
    </paper>
    <paper id="358">
      <title>Dimensional Emotion Detection from Categorical Emotion</title>
      <author><first>Sungjoon</first><last>Park</last></author>
      <author><first>Jiseon</first><last>Kim</last></author>
      <author><first>Seonghyeon</first><last>Ye</last></author>
      <author><first>Jaeyeol</first><last>Jeon</last></author>
      <author><first>Hee Young</first><last>Park</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>4367–4380</pages>
      <abstract>We present a model to predict fine-grained emotions along the continuous dimensions of <a href="https://en.wikipedia.org/wiki/Valence_(psychology)">valence</a>, <a href="https://en.wikipedia.org/wiki/Arousal">arousal</a>, and dominance (VAD) with a corpus with categorical emotion annotations. Our model is trained by minimizing the EMD (Earth Mover’s Distance) loss between the predicted <a href="https://en.wikipedia.org/wiki/Emotion_classification">VAD score distribution</a> and the categorical emotion distributions sorted along <a href="https://en.wikipedia.org/wiki/Emotion_classification">VAD</a>, and it can simultaneously classify the emotion categories and predict the <a href="https://en.wikipedia.org/wiki/Emotion_classification">VAD scores</a> for a given sentence. We use pre-trained RoBERTa-Large and fine-tune on three different corpora with categorical labels and evaluate on EmoBank corpus with VAD scores. We show that our approach reaches comparable performance to that of the state-of-the-art classifiers in categorical emotion classification and shows significant positive correlations with the ground truth VAD scores. Also, further training with supervision of VAD labels leads to improved performance especially when dataset is small. We also present examples of predictions of appropriate emotion words that are not part of the original annotations.</abstract>
      <url hash="b0fec8db">2021.emnlp-main.358</url>
      <bibkey>park-etal-2021-dimensional</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.358</doi>
      <pwccode url="https://github.com/sungjoonpark/emotiondetection" additional="false">sungjoonpark/emotiondetection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/emobank">EmoBank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/isear">ISEAR</pwcdataset>
    </paper>
    <paper id="360">
      <title>Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection</title>
      <author><first>Xincheng</first><last>Ju</last></author>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Rong</first><last>Xiao</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Shoushan</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>4395–4405</pages>
      <abstract>Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both <a href="https://en.wikipedia.org/wiki/Aspect_(linguistics)">aspect terms</a> and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multi-modal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores the better applications. Therefore, in this paper, we are the first to jointly perform multi-modal ATE (MATE) and multi-modal ASC (MASC), and we propose a multi-modal joint learning approach with auxiliary cross-modal relation detection for multi-modal aspect-level sentiment analysis (MALSA). Specifically, we first build an auxiliary text-image relation detection module to control the proper exploitation of visual information. Second, we adopt the hierarchical framework to bridge the multi-modal connection between <a href="https://en.wikipedia.org/wiki/MATE_(software)">MATE</a> and MASC, as well as separately visual guiding for each sub module. Finally, we can obtain all aspect-level sentiment polarities dependent on the jointly extracted specific aspects. Extensive experiments show the effectiveness of our approach against the joint textual approaches, pipeline and collapsed multi-modal approaches.</abstract>
      <url hash="84931f70">2021.emnlp-main.360</url>
      <bibkey>ju-etal-2021-joint</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.360</doi>
      <pwccode url="https://github.com/manlp-suda/jml" additional="false">manlp-suda/jml</pwccode>
    </paper>
    <paper id="361">
      <title>Solving Aspect Category Sentiment Analysis as a Text Generation Task</title>
      <author><first>Jian</first><last>Liu</last></author>
      <author><first>Zhiyang</first><last>Teng</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Hanmeng</first><last>Liu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>4406–4416</pages>
      <abstract>Aspect category sentiment analysis has attracted increasing research attention. The dominant methods make use of pre-trained language models by learning effective aspect category-specific representations, and adding specific output layers to its pre-trained representation. We consider a more direct way of making use of pre-trained language models, by casting the ACSA tasks into natural language generation tasks, using natural language sentences to represent the output. Our method allows more direct use of pre-trained knowledge in seq2seq language models by directly following the task setting during <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">pre-training</a>. Experiments on several <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> show that our method gives the best reported results, having large advantages in few-shot and zero-shot settings.</abstract>
      <url hash="8f2fe667">2021.emnlp-main.361</url>
      <bibkey>liu-etal-2021-solving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.361</doi>
      <pwccode url="https://github.com/lgw863/acsa-generation" additional="false">lgw863/acsa-generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
    </paper>
    <paper id="364">
      <title>CHoRaL : Collecting Humor Reaction Labels from Millions of Social Media Users<fixed-case>CH</fixed-case>o<fixed-case>R</fixed-case>a<fixed-case>L</fixed-case>: Collecting Humor Reaction Labels from Millions of Social Media Users</title>
      <author><first>Zixiaofan</first><last>Yang</last></author>
      <author><first>Shayan</first><last>Hooshmand</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <pages>4429–4435</pages>
      <abstract>Humor detection has gained attention in recent years due to the desire to understand <a href="https://en.wikipedia.org/wiki/User-generated_content">user-generated content</a> with <a href="https://en.wikipedia.org/wiki/Literal_and_figurative_language">figurative language</a>. However, substantial individual and cultural differences in humor perception make it very difficult to collect a large-scale humor dataset with reliable humor labels. We propose CHoRaL, a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> to generate perceived humor labels on Facebook posts, using the naturally available user reactions to these posts with no manual annotation needed. CHoRaL provides both binary labels and continuous scores of <a href="https://en.wikipedia.org/wiki/Humour">humor</a> and non-humor. We present the largest <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> to date with labeled humor on 785 K posts related to COVID-19. Additionally, we analyze the expression of COVID-related humor in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> by extracting lexico-semantic and affective features from the posts, and build humor detection models with performance similar to humans. CHoRaL enables the development of large-scale humor detection models on any topic and opens a new path to the study of <a href="https://en.wikipedia.org/wiki/Humour">humor</a> on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>.</abstract>
      <url hash="05b67585">2021.emnlp-main.364</url>
      <bibkey>yang-etal-2021-choral</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.364</doi>
    </paper>
    <paper id="366">
      <title>CodRED : A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild<fixed-case>C</fixed-case>od<fixed-case>RED</fixed-case>: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild</title>
      <author><first>Yuan</first><last>Yao</last></author>
      <author><first>Jiaju</first><last>Du</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>4452–4472</pages>
      <abstract>Existing relation extraction (RE) methods typically focus on extracting relational facts between entity pairs within single sentences or documents. However, a large quantity of relational facts in <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a> can only be inferred across documents in practice. In this work, we present the problem of cross-document RE, making an initial step towards <a href="https://en.wikipedia.org/wiki/Knowledge_acquisition">knowledge acquisition</a> in the wild. To facilitate the research, we construct the first human-annotated cross-document RE dataset CodRED. Compared to existing RE datasets, CodRED presents two key challenges : Given two entities, (1) it requires finding the relevant documents that can provide clues for identifying their relations ; (2) it requires reasoning over multiple documents to extract the relational facts. We conduct comprehensive experiments to show that CodRED is challenging to existing RE methods including strong BERT-based models.</abstract>
      <url hash="e524d1d5">2021.emnlp-main.366</url>
      <bibkey>yao-etal-2021-codred</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.366</doi>
      <pwccode url="https://github.com/thunlp/codred" additional="false">thunlp/codred</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/knowledgenet">KnowledgeNet</pwcdataset>
    </paper>
    <paper id="368">
      <title>We Need to Talk About train-dev-test Splits</title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <pages>4485–4494</pages>
      <abstract>Standard train-dev-test splits used to benchmark multiple <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> against each other are ubiquitously used in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing (NLP)</a>. In this setup, the train data is used for training the model, the development set for evaluating different versions of the proposed model(s) during development, and the test set to confirm the answers to the main research question(s). However, the introduction of <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> has led to a different use of these standard splits ; the development set is now often used for <a href="https://en.wikipedia.org/wiki/Model_selection">model selection</a> during the training procedure. Because of this, comparing multiple versions of the same model during development leads to overestimation on the development data. As an effect, people have started to compare an increasing amount of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on the test data, leading to faster overfitting and expiration of our test sets. We propose to use a tune-set when developing neural network methods, which can be used for model picking so that comparing the different versions of a new <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can safely be done on the development data.</abstract>
      <url hash="8f826e85">2021.emnlp-main.368</url>
      <attachment type="Software" hash="8ea0c706">2021.emnlp-main.368.Software.tgz</attachment>
      <bibkey>van-der-goot-2021-need</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.368</doi>
      <pwccode url="https://bitbucket.org/robvanderg/tuneset" additional="false">robvanderg/tuneset</pwccode>
    </paper>
    <paper id="369">
      <title>PhoMT : A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation<fixed-case>P</fixed-case>ho<fixed-case>MT</fixed-case>: A High-Quality and Large-Scale Benchmark Dataset for <fixed-case>V</fixed-case>ietnamese-<fixed-case>E</fixed-case>nglish Machine Translation</title>
      <author><first>Long</first><last>Doan</last></author>
      <author><first>Linh The</first><last>Nguyen</last></author>
      <author><first>Nguyen Luong</first><last>Tran</last></author>
      <author><first>Thai</first><last>Hoang</last></author>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <pages>4495–4503</pages>
      <abstract>We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02 M sentence pairs, which is 2.9 M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong neural baselines and well-known automatic translation engines on our dataset and find that in both automatic and human evaluations : the best performance is obtained by fine-tuning the pre-trained sequence-to-sequence denoising auto-encoder mBART. To our best knowledge, this is the first large-scale Vietnamese-English machine translation study. We hope our publicly available dataset and study can serve as a starting point for future research and applications on Vietnamese-English machine translation. We release our dataset at : https://github.com/VinAIResearch/PhoMT</abstract>
      <url hash="587dc8f9">2021.emnlp-main.369</url>
      <bibkey>doan-etal-2021-phomt</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.369</doi>
      <pwccode url="https://github.com/vinairesearch/phomt" additional="false">vinairesearch/phomt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/phomt">PhoMT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="374">
      <title>Mind the Style of Text ! Adversarial and Backdoor Attacks Based on Text Style Transfer</title>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Yangyi</first><last>Chen</last></author>
      <author><first>Xurui</first><last>Zhang</last></author>
      <author><first>Mukai</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>4569–4580</pages>
      <abstract>Adversarial attacks and <a href="https://en.wikipedia.org/wiki/Backdoor_(computing)">backdoor attacks</a> are two common security threats that hang over <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its meaning. We design an adversarial attack method and a <a href="https://en.wikipedia.org/wiki/Backdoor_(computing)">backdoor attack method</a>, and conduct extensive experiments to evaluate them. Experimental results show that popular NLP models are vulnerable to both adversarial and backdoor attacks based on text style transferthe attack success rates can exceed 90 % without much effort. It reflects the limited ability of <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP models</a> to handle the feature of text style that has not been widely realized. In addition, the style transfer-based adversarial and backdoor attack methods show superiority to <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> in many aspects. All the code and data of this paper can be obtained at https://github.com/thunlp/StyleAttack.</abstract>
      <url hash="dd7a40e6">2021.emnlp-main.374</url>
      <bibkey>qi-etal-2021-mind</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.374</doi>
      <pwccode url="https://github.com/thunlp/styleattack" additional="false">thunlp/styleattack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="376">
      <title>Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes</title>
      <author><first>Tomasz</first><last>Limisiewicz</last></author>
      <author><first>David</first><last>Mareček</last></author>
      <pages>4589–4598</pages>
      <abstract>State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages. The novel Orthogonal Structural Probe (Limisiewicz and Mareek, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual annotated datasets. We evaluate syntactic (UD) and lexical (WordNet) structural information encoded inmBERT’s contextual representations for nine diverse languages. We observe that for languages closely related to <a href="https://en.wikipedia.org/wiki/English_language">English</a>, no transformation is needed. The evaluated information is encoded in a shared cross-lingual embedding space. For other languages, it is beneficial to apply <a href="https://en.wikipedia.org/wiki/Orthogonal_transformation">orthogonal transformation</a> learned separately for each language. We successfully apply our findings to zero-shot and few-shot cross-lingual parsing.</abstract>
      <url hash="029bce0d">2021.emnlp-main.376</url>
      <bibkey>limisiewicz-marecek-2021-examining</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.376</doi>
    </paper>
    <paper id="381">
      <title>Extracting Fine-Grained Knowledge Graphs of Scientific Claims : Dataset and Transformer-Based Results</title>
      <author><first>Ian</first><last>Magnusson</last></author>
      <author><first>Scott</first><last>Friedman</last></author>
      <pages>4651–4658</pages>
      <abstract>Recent transformer-based approaches demonstrate promising results on relational scientific information extraction. Existing datasets focus on high-level description of how research is carried out. Instead we focus on the subtleties of how experimental associations are presented by building SciClaim, a dataset of scientific claims drawn from Social and Behavior Science (SBS), <a href="https://en.wikipedia.org/wiki/PubMed">PubMed</a>, and CORD-19 papers. Our novel graph annotation schema incorporates not only coarse-grained entity spans as <a href="https://en.wikipedia.org/wiki/Vertex_(graph_theory)">nodes</a> and relations as edges between them, but also fine-grained attributes that modify entities and their relations, for a total of 12,738 labels in the corpus. By including more label types and more than twice the label density of previous datasets, SciClaim captures causal, comparative, predictive, statistical, and proportional associations over experimental variables along with their qualifications, subtypes, and evidence. We extend work in transformer-based joint entity and relation extraction to effectively infer our schema, showing the promise of fine-grained knowledge graphs in scientific claims and beyond.</abstract>
      <url hash="34b1bdff">2021.emnlp-main.381</url>
      <bibkey>magnusson-friedman-2021-extracting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.381</doi>
      <pwccode url="https://github.com/siftech/sciclaim" additional="false">siftech/sciclaim</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scirex">SciREX</pwcdataset>
    </paper>
    <paper id="385">
      <title>AVocaDo : Strategy for Adapting Vocabulary to Downstream Domain<fixed-case>AV</fixed-case>oca<fixed-case>D</fixed-case>o: Strategy for Adapting Vocabulary to Downstream Domain</title>
      <author><first>Jimin</first><last>Hong</last></author>
      <author><first>TaeHee</first><last>Kim</last></author>
      <author><first>Hyesu</first><last>Lim</last></author>
      <author><first>Jaegul</first><last>Choo</last></author>
      <pages>4692–4700</pages>
      <abstract>During the fine-tuning phase of <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>, the pretrained vocabulary remains unchanged, while <a href="https://en.wikipedia.org/wiki/Parameter">model parameters</a> are updated. The <a href="https://en.wikipedia.org/wiki/Vocabulary">vocabulary</a> generated based on the pretrained data is suboptimal for <a href="https://en.wikipedia.org/wiki/Downstream_(networking)">downstream data</a> when domain discrepancy exists. We propose to consider the <a href="https://en.wikipedia.org/wiki/Vocabulary">vocabulary</a> as an optimizable parameter, allowing us to update the <a href="https://en.wikipedia.org/wiki/Vocabulary">vocabulary</a> by expanding it with domain specific vocabulary based on a tokenization statistic. Furthermore, we preserve the embeddings of the added words from <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> to downstream data by utilizing knowledge learned from a pretrained language model with a regularization term. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> achieved consistent performance improvements on diverse domains (i.e., <a href="https://en.wikipedia.org/wiki/Biomedicine">biomedical</a>, <a href="https://en.wikipedia.org/wiki/Computer_science">computer science</a>, <a href="https://en.wikipedia.org/wiki/News">news</a>, and reviews).</abstract>
      <url hash="d7984f5f">2021.emnlp-main.385</url>
      <bibkey>hong-etal-2021-avocado</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.385</doi>
      <pwccode url="https://github.com/Jimin9401/avocado" additional="false">Jimin9401/avocado</pwccode>
    </paper>
    <paper id="388">
      <title>Can <a href="https://en.wikipedia.org/wiki/Language_model">Language Models</a> be Biomedical Knowledge Bases?</title>
      <author><first>Mujeen</first><last>Sung</last></author>
      <author><first>Jinhyuk</first><last>Lee</last></author>
      <author><first>Sean</first><last>Yi</last></author>
      <author><first>Minji</first><last>Jeon</last></author>
      <author><first>Sungdong</first><last>Kim</last></author>
      <author><first>Jaewoo</first><last>Kang</last></author>
      <pages>4723–4734</pages>
      <abstract>Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">LMs</a> contain and how we can extract that knowledge, treating <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">LMs</a> as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49 K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51 % Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.</abstract>
      <url hash="497056c4">2021.emnlp-main.388</url>
      <bibkey>sung-etal-2021-language</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.388</doi>
      <pwccode url="https://github.com/dmis-lab/biolama" additional="false">dmis-lab/biolama</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/biolama">BioLAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="391">
      <title>Learning grounded word meaning representations on similarity graphs</title>
      <author><first>Mariella</first><last>Dimiccoli</last></author>
      <author><first>Herwig</first><last>Wendt</last></author>
      <author><first>Pau</first><last>Batlle Franch</last></author>
      <pages>4760–4769</pages>
      <abstract>This paper introduces a novel approach to learn visually grounded meaning representations of words as low-dimensional node embeddings on an underlying graph hierarchy. The lower level of the <a href="https://en.wikipedia.org/wiki/Hierarchy">hierarchy</a> models <a href="https://en.wikipedia.org/wiki/Modality_(semiotics)">modality-specific word representations</a>, conditioned to another <a href="https://en.wikipedia.org/wiki/Modality_(semiotics)">modality</a>, through dedicated but communicating graphs, while the higher level puts these representations together on a single <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> to learn a representation jointly from both modalities. The topology of each <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> models similarity relations among words, and is estimated jointly with the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph embedding</a>. The assumption underlying this <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is that words sharing similar meaning correspond to <a href="https://en.wikipedia.org/wiki/Community">communities</a> in an underlying <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> in a <a href="https://en.wikipedia.org/wiki/Dimension_(vector_space)">low-dimensional space</a>. We named this model Hierarchical Multi-Modal Similarity Graph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE to simulate human similarity judgments and concept categorization, outperforming the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state of the art</a>.</abstract>
      <url hash="e5d05ba7">2021.emnlp-main.391</url>
      <bibkey>dimiccoli-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.391</doi>
      <pwccode url="https://github.com/mdimiccoli/hm-sge" additional="false">mdimiccoli/hm-sge</pwccode>
    </paper>
    <paper id="394">
      <title>On the Relation between Syntactic Divergence and Zero-Shot Performance</title>
      <author><first>Ofir</first><last>Arviv</last></author>
      <author><first>Dmitry</first><last>Nikolaev</last></author>
      <author><first>Taelin</first><last>Karidi</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>4803–4817</pages>
      <abstract>We explore the link between the extent to which syntactic relations are preserved in <a href="https://en.wikipedia.org/wiki/Translation">translation</a> and the ease of correctly constructing a <a href="https://en.wikipedia.org/wiki/Parse_tree">parse tree</a> in a zero-shot setting. While previous work suggests such a relation, it tends to focus on the macro level and not on the level of individual edgesa gap we aim to address. As a test case, we take the transfer of Universal Dependencies (UD) parsing from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to a diverse set of languages and conduct two sets of experiments. In one, we analyze zero-shot performance based on the extent to which English source edges are preserved in <a href="https://en.wikipedia.org/wiki/Translation">translation</a>. In another, we apply three linguistically motivated transformations to UD, creating more cross-lingually stable versions of it, and assess their zero-shot parsability. In order to compare <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a> performance across different schemes, we perform extrinsic evaluation on the downstream task of cross-lingual relation extraction (RE) using a subset of a standard English RE benchmark translated to <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a> and <a href="https://en.wikipedia.org/wiki/Korean_language">Korean</a>. In both sets of experiments, our results suggest a strong relation between cross-lingual stability and zero-shot parsing performance.</abstract>
      <url hash="29f721bf">2021.emnlp-main.394</url>
      <bibkey>arviv-etal-2021-relation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.394</doi>
      <pwccode url="https://github.com/ofirarviv/improving-ud" additional="false">ofirarviv/improving-ud</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/translated-tacred">Translated TACRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="395">
      <title>Improved Latent Tree Induction with Distant Supervision via Span Constraints</title>
      <author><first>Zhiyang</first><last>Xu</last></author>
      <author><first>Andrew</first><last>Drozdov</last></author>
      <author><first>Jay Yoon</first><last>Lee</last></author>
      <author><first>Tim</first><last>O’Gorman</last></author>
      <author><first>Subendhu</first><last>Rongali</last></author>
      <author><first>Dylan</first><last>Finkbeiner</last></author>
      <author><first>Shilpa</first><last>Suresh</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>4818–4831</pages>
      <abstract>For over thirty years, researchers have developed and analyzed <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for latent tree induction as an approach for unsupervised syntactic parsing. Nonetheless, modern <a href="https://en.wikipedia.org/wiki/System">systems</a> still do not perform well enough compared to their supervised counterparts to have any practical use as structural annotation of text. In this work, we present a technique that uses distant supervision in the form of <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">span constraints</a> (i.e. phrase bracketing) to improve performance in unsupervised constituency parsing. Using a relatively small number of span constraints we can substantially improve the output from DIORA, an already competitive unsupervised parsing system. Compared with full parse tree annotation, span constraints can be acquired with minimal effort, such as with a lexicon derived from <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, to find exact text matches. Our experiments show span constraints based on entities improves constituency parsing on English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to any domain where span constraints are easily attainable, and as a case study we demonstrate its effectiveness by parsing biomedical text from the CRAFT dataset.</abstract>
      <url hash="02716d62">2021.emnlp-main.395</url>
      <bibkey>xu-etal-2021-improved</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.395</doi>
      <pwccode url="https://github.com/iesl/distantly-supervised-diora" additional="false">iesl/distantly-supervised-diora</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="397">
      <title>Just Say No : Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts</title>
      <author><first>Ashutosh</first><last>Baheti</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Alan</first><last>Ritter</last></author>
      <author><first>Mark</first><last>Riedl</last></author>
      <pages>4846–4862</pages>
      <abstract>Dialogue models trained on <a href="https://en.wikipedia.org/wiki/Conversation">human conversations</a> inadvertently learn to generate <a href="https://en.wikipedia.org/wiki/Toxicity">toxic responses</a>. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with <a href="https://en.wikipedia.org/wiki/Profanity">offensive language</a> and <a href="https://en.wikipedia.org/wiki/List_of_human_positions">stance</a>. Our analysis reveals that 42 % of human responses agree with toxic comments, whereas only 13 % agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 <a href="https://en.wikipedia.org/wiki/F-number">F1</a> for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19 % reduction in agreement with offensive comments and produces 29 % fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.</abstract>
      <url hash="f240bf88">2021.emnlp-main.397</url>
      <bibkey>baheti-etal-2021-just</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.397</doi>
      <pwccode url="https://github.com/abaheti95/toxichat" additional="false">abaheti95/toxichat</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sbic">SBIC</pwcdataset>
    </paper>
    <paper id="398">
      <title>Multi-Modal Open-Domain Dialogue</title>
      <author><first>Kurt</first><last>Shuster</last></author>
      <author><first>Eric Michael</first><last>Smith</last></author>
      <author><first>Da</first><last>Ju</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>4863–4883</pages>
      <abstract>Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020 ; Roller et al., 2020). However, if we want to build <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agents</a> with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of getting humans to engage in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based conversation. We additionally investigate and incorporate safety components in our final <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, and show that such efforts do not diminish model performance with respect to human preference.</abstract>
      <url hash="9bfa83f0">2021.emnlp-main.398</url>
      <bibkey>shuster-etal-2021-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.398</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/blended-skill-talk">Blended Skill Talk</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco-captions">COCO Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/empatheticdialogues">EmpatheticDialogues</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="402">
      <title>RAST : Domain-Robust Dialogue Rewriting as Sequence Tagging<fixed-case>RAST</fixed-case>: Domain-Robust Dialogue Rewriting as Sequence Tagging</title>
      <author><first>Jie</first><last>Hao</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Liwei</first><last>Wang</last></author>
      <author><first>Kun</first><last>Xu</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>4913–4924</pages>
      <abstract>The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the <a href="https://en.wikipedia.org/wiki/Feasible_region">search space</a> is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>’s outputs may lack fluency. To alleviate this issue, we inject the loss signal from <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> over the current state-of-the-art <a href="https://en.wikipedia.org/wiki/System">systems</a> when transferring to another <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>.</abstract>
      <url hash="8ff8e055">2021.emnlp-main.402</url>
      <bibkey>hao-etal-2021-rast</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.402</doi>
    </paper>
    <paper id="404">
      <title>Dialogue State Tracking with a <a href="https://en.wikipedia.org/wiki/Language_model">Language Model</a> using Schema-Driven Prompting</title>
      <author><first>Chia-Hsuan</first><last>Lee</last></author>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Mari</first><last>Ostendorf</last></author>
      <pages>4937–4949</pages>
      <abstract>Task-oriented conversational systems often use dialogue state tracking to represent the user’s intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have been obtained using more general architectures based on pretrained language models. Here, we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide task-aware history encoding that is used for both categorical and non-categorical slots. We further improve performance by augmenting the prompting with schema descriptions, a naturally occurring source of in-domain knowledge. Our purely <a href="https://en.wikipedia.org/wiki/Generative_model">generative system</a> achieves state-of-the-art performance on MultiWOZ 2.2 and achieves competitive performance on two other <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> : MultiWOZ 2.1 and M2M. The data and code will be available at https://github.com/chiahsuan156/DST-as-Prompting.</abstract>
      <url hash="8c222adc">2021.emnlp-main.404</url>
      <bibkey>lee-etal-2021-dialogue</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.404</doi>
      <pwccode url="https://github.com/chiahsuan156/dst-as-prompting" additional="false">chiahsuan156/dst-as-prompting</pwccode>
    </paper>
    <paper id="409">
      <title>Pre-train or Annotate? Domain Adaptation with a Constrained Budget</title>
      <author><first>Fan</first><last>Bai</last></author>
      <author><first>Alan</first><last>Ritter</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <pages>5002–5015</pages>
      <abstract>Recent work has demonstrated that pre-training in-domain language models can boost performance when adapting to a new domain. However, the costs associated with pre-training raise an important question : given a fixed budget, what steps should an NLP practitioner take to maximize performance? In this paper, we study domain adaptation under budget constraints, and approach it as a customer choice problem between data annotation and pre-training. Specifically, we measure the annotation cost of three procedural text datasets and the pre-training cost of three in-domain language models. Then we evaluate the utility of different combinations of pre-training and <a href="https://en.wikipedia.org/wiki/Annotation">data annotation</a> under varying <a href="https://en.wikipedia.org/wiki/Budget_constraint">budget constraints</a> to assess which combination strategy works best. We find that, for small budgets, spending all funds on <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> leads to the best performance ; once the budget becomes large enough, a combination of data annotation and in-domain pre-training works more optimally. We therefore suggest that task-specific data annotation should be part of an economical strategy when adapting an NLP model to a new domain.</abstract>
      <url hash="7276f144">2021.emnlp-main.409</url>
      <bibkey>bai-etal-2021-pre</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.409</doi>
      <pwccode url="https://github.com/bflashcp3f/procbert" additional="false">bflashcp3f/procbert</pwccode>
    </paper>
    <paper id="416">
      <title>Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning</title>
      <author><first>Li</first><last>Zhou</last></author>
      <author><first>Kevin</first><last>Small</last></author>
      <author><first>Yong</first><last>Zhang</last></author>
      <author><first>Sandeep</first><last>Atluri</last></author>
      <pages>5103–5135</pages>
      <abstract>Motivated by suggested question generation in conversational news recommendation systems, we propose a model for generating question-answer pairs (QA pairs) with self-contained, summary-centric questions and length-constrained, article-summarizing answers. We begin by collecting a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> with questions as titles and pairing them with summaries of varying length. This dataset is used to learn a QA pair generation model producing summaries as answers that balance brevity with sufficiency jointly with their corresponding questions. We then reinforce the QA pair generation process with a differentiable reward function to mitigate <a href="https://en.wikipedia.org/wiki/Exposure_bias">exposure bias</a>, a common problem in <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation</a>. Both automatic metrics and human evaluation demonstrate these QA pairs successfully capture the central gists of the articles and achieve high answer accuracy.</abstract>
      <url hash="69962c6b">2021.emnlp-main.416</url>
      <bibkey>zhou-etal-2021-generating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.416</doi>
      <pwccode url="https://github.com/amazon-research/sc2qa-dril" additional="false">amazon-research/sc2qa-dril</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="417">
      <title>Unsupervised Paraphrasing with Pretrained Language Models</title>
      <author><first>Tong</first><last>Niu</last></author>
      <author><first>Semih</first><last>Yavuz</last></author>
      <author><first>Yingbo</first><last>Zhou</last></author>
      <author><first>Nitish Shirish</first><last>Keskar</last></author>
      <author><first>Huan</first><last>Wang</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>5136–5150</pages>
      <abstract>Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised methods</a>, which require a large amount of labeled data that is costly to collect. To address this drawback, we adopt a transfer learning approach and propose a <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training pipeline</a> that enables pre-trained language models to generate high-quality paraphrases in an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised setting</a>. Our recipe consists of task-adaptation, self-supervision, and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a <a href="https://en.wikipedia.org/wiki/Surface_form">surface form</a> dissimilar from the input, whenever the <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> emits a token contained in the source sequence, DB prevents the model from outputting the subsequent source token for the next generation step. We show with automatic and human evaluations that our approach achieves state-of-the-art performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and is robust to domain shift between the two datasets of distinct distributions. We also demonstrate that our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> transfers to <a href="https://en.wikipedia.org/wiki/Paraphrasing">paraphrasing</a> in other languages without any additional <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a>.</abstract>
      <url hash="41742221">2021.emnlp-main.417</url>
      <bibkey>niu-etal-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.417</doi>
    </paper>
    <paper id="418">
      <title>Profanity-Avoiding Training Framework for Seq2seq Models with Certified Robustness</title>
      <author><first>Hengtong</first><last>Zhang</last></author>
      <author><first>Tianhang</first><last>Zheng</last></author>
      <author><first>Yaliang</first><last>Li</last></author>
      <author><first>Jing</first><last>Gao</last></author>
      <author><first>Lu</first><last>Su</last></author>
      <author id="bo-li-vanderbilt"><first>Bo</first><last>Li</last></author>
      <pages>5151–5161</pages>
      <abstract>Seq2seq models have demonstrated their incredible effectiveness in a large variety of applications. However, recent research has shown that inappropriate language in training samples and well-designed testing cases can induce seq2seq models to output <a href="https://en.wikipedia.org/wiki/Profanity">profanity</a>. These outputs may potentially hurt the usability of seq2seq models and make the end-users feel offended. To address this problem, we propose a training framework with certified robustness to eliminate the causes that trigger the generation of profanity. The proposed training framework leverages merely a short list of <a href="https://en.wikipedia.org/wiki/Profanity">profanity examples</a> to prevent seq2seq models from generating a broader spectrum of <a href="https://en.wikipedia.org/wiki/Profanity">profanity</a>. The framework is composed of a pattern-eliminating training component to suppress the impact of language patterns with <a href="https://en.wikipedia.org/wiki/Profanity">profanity</a> in the training set, and a trigger-resisting training component to provide certified robustness for seq2seq models against intentionally injected profanity-triggering expressions in test samples. In the experiments, we consider two representative NLP tasks that seq2seq can be applied to, i.e., style transfer and dialogue generation. Extensive experimental results show that the proposed training framework can successfully prevent the NLP models from generating <a href="https://en.wikipedia.org/wiki/Profanity">profanity</a>.</abstract>
      <url hash="1063d181">2021.emnlp-main.418</url>
      <bibkey>zhang-etal-2021-profanity</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.418</doi>
    </paper>
    <paper id="419">
      <title>Journalistic Guidelines Aware News Image Captioning</title>
      <author><first>Xuewen</first><last>Yang</last></author>
      <author><first>Svebor</first><last>Karaman</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <author><first>Alejandro</first><last>Jaimes</last></author>
      <pages>5162–5175</pages>
      <abstract>The task of news article image captioning aims to generate descriptive and informative captions for news article images. Unlike conventional image captions that simply describe the content of the image in general terms, news image captions follow journalistic guidelines and rely heavily on named entities to describe the image content, often drawing context from the whole article they are associated with. In this work, we propose a new approach to this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, motivated by caption guidelines that journalists follow. Our approach, Journalistic Guidelines Aware News Image Captioning (JoGANIC), leverages the structure of captions to improve the generation quality and guide our representation design. Experimental results, including detailed ablation studies, on two large-scale publicly available datasets show that JoGANIC substantially outperforms state-of-the-art methods both on caption generation and named entity related metrics.</abstract>
      <url hash="75b8ffee">2021.emnlp-main.419</url>
      <bibkey>yang-etal-2021-journalistic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.419</doi>
      <pwccode url="https://github.com/dataminr-ai/joganic" additional="false">dataminr-ai/joganic</pwccode>
    </paper>
    <paper id="420">
      <title>AESOP : Paraphrase Generation with Adaptive Syntactic Control<fixed-case>AESOP</fixed-case>: Paraphrase Generation with Adaptive Syntactic Control</title>
      <author><first>Jiao</first><last>Sun</last></author>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>5176–5189</pages>
      <abstract>We propose to control <a href="https://en.wikipedia.org/wiki/Paraphrase_generation">paraphrase generation</a> through carefully chosen target syntactic structures to generate more proper and higher quality <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a>. Our model, AESOP, leverages a pretrained language model and adds deliberately chosen syntactical control via a retrieval-based selection module to generate fluent paraphrases. Experiments show that AESOP achieves state-of-the-art performances on semantic preservation and syntactic conformation on two benchmark datasets with ground-truth syntactic control from human-annotated exemplars. Moreover, with the retrieval-based target syntax selection module, AESOP generates paraphrases with even better qualities than the current best model using human-annotated target syntactic parses according to human evaluation. We further demonstrate the effectiveness of AESOP to improve classification models’ <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> to syntactic perturbation by <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> on two GLUE tasks.</abstract>
      <url hash="9026394d">2021.emnlp-main.420</url>
      <bibkey>sun-etal-2021-aesop</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.420</doi>
      <pwccode url="https://github.com/pluslabnlp/aesop" additional="false">pluslabnlp/aesop</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="421">
      <title>Refocusing on Relevance : Personalization in NLG<fixed-case>NLG</fixed-case></title>
      <author><first>Shiran</first><last>Dudy</last></author>
      <author><first>Steven</first><last>Bedrick</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <pages>5190–5202</pages>
      <abstract>Many NLG tasks such as <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a>, dialogue response, or <a href="https://en.wikipedia.org/wiki/Question_answering">open domain question answering</a>, focus primarily on a source text in order to generate a target response. This standard approach falls short, however, when a user’s intent or context of work is not easily recoverable based solely on that source text a scenario that we argue is more of the rule than the exception. In this work, we argue that NLG systems in general should place a much higher level of emphasis on making use of additional context, and suggest that <a href="https://en.wikipedia.org/wiki/Relevance">relevance</a> (as used in Information Retrieval) be thought of as a crucial tool for designing user-oriented text-generating tasks. We further discuss possible harms and hazards around such <a href="https://en.wikipedia.org/wiki/Personalization">personalization</a>, and argue that value-sensitive design represents a crucial path forward through these challenges.</abstract>
      <url hash="6ba7f724">2021.emnlp-main.421</url>
      <bibkey>dudy-etal-2021-refocusing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.421</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="427">
      <title>Learning Prototype Representations Across Few-Shot Tasks for Event Detection</title>
      <author><first>Viet</first><last>Lai</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>5270–5277</pages>
      <abstract>We address the <a href="https://en.wikipedia.org/wiki/Sampling_bias">sampling bias</a> and outlier issues in few-shot learning for event detection, a subtask of <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a>. We propose to model the relations between training tasks in episodic few-shot learning by introducing cross-task prototypes. We further propose to enforce prediction consistency among <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> across tasks to make the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> more robust to <a href="https://en.wikipedia.org/wiki/Outlier">outliers</a>. Our extensive experiment shows a consistent improvement on three few-shot learning datasets. The findings suggest that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is more robust when labeled data of novel event types is limited. The source code is available at http://github.com/laiviet/fsl-proact.</abstract>
      <url hash="f5bce283">2021.emnlp-main.427</url>
      <bibkey>lai-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.427</doi>
      <pwccode url="https://github.com/laiviet/fsl-proact" additional="false">laiviet/fsl-proact</pwccode>
    </paper>
    <paper id="428">
      <title>Lifelong Event Detection with Knowledge Transfer</title>
      <author><first>Pengfei</first><last>Yu</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Prem</first><last>Natarajan</last></author>
      <pages>5278–5290</pages>
      <abstract>Traditional supervised Information Extraction (IE) methods can extract structured knowledge elements from <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured data</a>, but it is limited to a pre-defined target ontology. In reality, the ontology of interest may change over time, adding emergent new types or more fine-grained subtypes. We propose a new lifelong learning framework to address this challenge. We focus on lifelong event detection as an exemplar case and propose a new problem formulation that is also generalizable to other IE tasks. In event detection and more general IE tasks, rich correlations or <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic relatedness</a> exist among hierarchical knowledge element types. In our proposed <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a>, knowledge is being transferred between learned <a href="https://en.wikipedia.org/wiki/Event_(computing)">old event types</a> and <a href="https://en.wikipedia.org/wiki/Event_(computing)">new event types</a>. Specifically, we update old knowledge with new event types’ mentions using a self-training loss. In addition, we aggregate old event types’ representations based on their similarities with new event types to initialize the new event types’ representations. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> outperforms competitive <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> with a 5.1 % absolute gain in the F1 score. Moreover, our proposed framework can boost the F1 score for over 30 % absolute gain on some new long-tail rare event types with few training instances. Our knowledge transfer module improves performance on both learned event types and new event types under the lifelong learning setting, showing that it helps consolidate old knowledge and improve novel <a href="https://en.wikipedia.org/wiki/Knowledge_acquisition">knowledge acquisition</a>.</abstract>
      <url hash="a6ec3397">2021.emnlp-main.428</url>
      <bibkey>yu-etal-2021-lifelong</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.428</doi>
      <pwccode url="https://github.com/perfec-yu/lifelong-ed" additional="false">perfec-yu/lifelong-ed</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/maven">MAVEN</pwcdataset>
    </paper>
    <paper id="432">
      <title>Adversarial Attack against Cross-lingual Knowledge Graph Alignment</title>
      <author><first>Zeru</first><last>Zhang</last></author>
      <author><first>Zijie</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Zhou</last></author>
      <author><first>Lingfei</first><last>Wu</last></author>
      <author><first>Sixing</first><last>Wu</last></author>
      <author><first>Xiaoying</first><last>Han</last></author>
      <author><first>Dejing</first><last>Dou</last></author>
      <author><first>Tianshi</first><last>Che</last></author>
      <author><first>Da</first><last>Yan</last></author>
      <pages>5320–5337</pages>
      <abstract>Recent literatures have shown that knowledge graph (KG) learning models are highly vulnerable to adversarial attacks. However, there is still a paucity of vulnerability analyses of cross-lingual entity alignment under adversarial attacks. This paper proposes an adversarial attack model with two novel attack techniques to perturb the KG structure and degrade the quality of deep cross-lingual entity alignment. First, an entity density maximization method is employed to hide the attacked entities in dense regions in two KGs, such that the derived perturbations are unnoticeable. Second, an attack signal amplification method is developed to reduce the gradient vanishing issues in the process of adversarial attacks for further improving the attack effectiveness.</abstract>
      <url hash="9385a31d">2021.emnlp-main.432</url>
      <bibkey>zhang-etal-2021-adversarial-attack</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.432</doi>
    </paper>
    <paper id="435">
      <title>Incorporating medical knowledge in BERT for clinical relation extraction<fixed-case>BERT</fixed-case> for clinical relation extraction</title>
      <author><first>Arpita</first><last>Roy</last></author>
      <author><first>Shimei</first><last>Pan</last></author>
      <pages>5357–5366</pages>
      <abstract>In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as <a href="https://en.wikipedia.org/wiki/Information_extraction">Information Extraction</a>, <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> and <a href="https://en.wikipedia.org/wiki/Question_answering">Question Answering</a>. Trained with massive general-domain text, these pre-trained language models capture rich syntactic, semantic and discourse information in the text. However, due to the differences between general and specific domain text (e.g., <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> versus clinic notes), these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> may not be ideal for <a href="https://en.wikipedia.org/wiki/Domain-specific_language">domain-specific tasks</a> (e.g., extracting clinical relations). Furthermore, it may require additional <a href="https://en.wikipedia.org/wiki/Medicine">medical knowledge</a> to understand <a href="https://en.wikipedia.org/wiki/Medical_literature">clinical text</a> properly. To solve these issues, in this research, we conduct a comprehensive examination of different techniques to add <a href="https://en.wikipedia.org/wiki/Medicine">medical knowledge</a> into a pre-trained BERT model for clinical relation extraction. Our best model outperforms the state-of-the-art systems on the benchmark i2b2 / VA 2010 clinical relation extraction dataset.</abstract>
      <url hash="f7c80bb4">2021.emnlp-main.435</url>
      <bibkey>roy-pan-2021-incorporating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.435</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/umls">UMLS</pwcdataset>
    </paper>
    <paper id="436">
      <title>ECONET : Effective Continual Pretraining of <a href="https://en.wikipedia.org/wiki/Language_model">Language Models</a> for Event Temporal Reasoning<fixed-case>ECONET</fixed-case>: Effective Continual Pretraining of Language Models for Event Temporal Reasoning</title>
      <author><first>Rujun</first><last>Han</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>5367–5380</pages>
      <abstract>While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This * * E**ffective * * CON**tinual pre-training framework for * * E**vent * * T**emporal reasoning (ECONET) improves the PTLMs’ fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks.</abstract>
      <url hash="a82f1c78">2021.emnlp-main.436</url>
      <bibkey>han-etal-2021-econet</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.436</doi>
      <pwccode url="https://github.com/pluslabnlp/econet" additional="true">pluslabnlp/econet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mc-taco">MC-TACO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/torque">Torque</pwcdataset>
    </paper>
    <paper id="441">
      <title>Corpus-based Open-Domain Event Type Induction</title>
      <author><first>Jiaming</first><last>Shen</last></author>
      <author><first>Yunyi</first><last>Zhang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>5427–5440</pages>
      <abstract>Traditional event extraction methods require predefined <a href="https://en.wikipedia.org/wiki/Event_(computing)">event types</a> and their corresponding annotations to learn event extractors. These prerequisites are often hard to be satisfied in real-world applications. This work presents a corpus-based open-domain event type induction method that automatically discovers a set of event types from a given corpus. As events of the same type could be expressed in multiple ways, we propose to represent each event type as a cluster of predicate sense, object head pairs. Specifically, our method (1) selects salient predicates and object heads, (2) disambiguates predicate senses using only a verb sense dictionary, and (3) obtains event types by jointly embedding and clustering predicate sense, object head pairs in a latent spherical space. Our experiments, on three datasets from different domains, show our method can discover salient and high-quality event types, according to both automatic and human evaluations.</abstract>
      <url hash="08ef28ef">2021.emnlp-main.441</url>
      <bibkey>shen-etal-2021-corpus</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.441</doi>
      <pwccode url="https://github.com/mickeystroller/etypeclus" additional="false">mickeystroller/etypeclus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="442">
      <title>PDALN : Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition<fixed-case>PDALN</fixed-case>: Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition</title>
      <author><first>Tao</first><last>Zhang</last></author>
      <author><first>Congying</first><last>Xia</last></author>
      <author><first>Philip S.</first><last>Yu</last></author>
      <author><first>Zhiwei</first><last>Liu</last></author>
      <author><first>Shu</first><last>Zhao</last></author>
      <pages>5441–5451</pages>
      <abstract>Cross-domain Named Entity Recognition (NER) transfers the NER knowledge from high-resource domains to the low-resource target domain. Due to limited labeled resources and domain shift, cross-domain NER is a challenging task. To address these challenges, we propose a progressive domain adaptation Knowledge Distillation (KD) approach   PDALN. It achieves superior domain adaptability by employing three components : (1) Adaptive data augmentation techniques, which alleviate cross-domain gap and label sparsity simultaneously ; (2) Multi-level Domain invariant features, derived from a multi-grained MMD (Maximum Mean Discrepancy) approach, to enable knowledge transfer across domains ; (3) Advanced KD schema, which progressively enables powerful pre-trained language models to perform domain adaptation. Extensive experiments on four benchmarks show that PDALN can effectively adapt high-resource domains to low-resource target domains, even if they are diverse in terms and writing styles. Comparison with other <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> indicates the state-of-the-art performance of PDALN.</abstract>
      <url hash="47507954">2021.emnlp-main.442</url>
      <bibkey>zhang-etal-2021-pdaln</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.442</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2016-ner">WNUT 2016 NER</pwcdataset>
    </paper>
    <paper id="453">
      <title>Average Approximates First Principal Component? An Empirical Analysis on Representations from Neural Language Models</title>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Chengyu</first><last>Dong</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>5594–5603</pages>
      <abstract>Contextualized representations based on neural language models have furthered the state of the art in various NLP tasks. Despite its great success, the nature of such <a href="https://en.wikipedia.org/wiki/Representation_(arts)">representations</a> remains a mystery. In this paper, we present an empirical property of these representationsaverage approximates <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">first principal component</a>. Specifically, experiments show that the average of these representations shares almost the same direction as the first principal component of the matrix whose columns are these representations. We believe this explains why the average representation is always a simple yet strong <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>. Our further examinations show that this property also holds in more challenging scenarios, for example, when the representations are from a model right after its random initialization. Therefore, we conjecture that this property is intrinsic to the distribution of representations and not necessarily related to the input structure. We realize that these representations empirically follow a <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> for each dimension, and by assuming this is true, we demonstrate that the empirical property can be in fact derived mathematically.</abstract>
      <url hash="ffa0cd89">2021.emnlp-main.453</url>
      <bibkey>wang-etal-2021-average</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.453</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="460">
      <title>Continual Few-Shot Learning for Text Classification</title>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Veselin</first><last>Stoyanov</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>5688–5702</pages>
      <abstract>Natural Language Processing (NLP) is increasingly relying on general end-to-end systems that need to handle many different linguistic phenomena and nuances. For example, a Natural Language Inference (NLI) system has to recognize <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment</a>, handle numbers, perform <a href="https://en.wikipedia.org/wiki/Coreference">coreference</a>, etc. Our solutions to complex problems are still far from perfect, so it is important to create systems that can learn to correct mistakes quickly, incrementally, and with little training data. In this work, we propose a continual few-shot learning (CFL) task, in which a system is challenged with a difficult phenomenon and asked to learn to correct mistakes with only a few (10 to 15) training examples. To this end, we first create benchmarks based on previously annotated data : two NLI (ANLI and SNLI) and one sentiment analysis (IMDB) datasets. Next, we present various <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> from diverse <a href="https://en.wikipedia.org/wiki/Paradigm_(disambiguation)">paradigms</a> (e.g., memory-aware synapses and Prototypical networks) and compare them on few-shot learning and continual few-shot learning setups. Our contributions are in creating a benchmark suite and evaluation protocol for continual few-shot learning on the text classification tasks, and making several interesting observations on the behavior of similarity-based methods. We hope that our work serves as a useful starting point for future work on this important topic.</abstract>
      <url hash="504d4f84">2021.emnlp-main.460</url>
      <bibkey>pasunuru-etal-2021-continual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.460</doi>
      <pwccode url="https://github.com/ramakanth-pasunuru/cfl-benchmark" additional="false">ramakanth-pasunuru/cfl-benchmark</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="461">
      <title>Efficient Nearest Neighbor Language Models</title>
      <author><first>Junxian</first><last>He</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>5703–5714</pages>
      <abstract>Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the <a href="https://en.wikipedia.org/wiki/Overhead_(computing)">inference overhead</a> and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.</abstract>
      <url hash="bbcbf48d">2021.emnlp-main.461</url>
      <bibkey>he-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.461</doi>
      <pwccode url="https://github.com/jxhe/efficient-knnlm" additional="false">jxhe/efficient-knnlm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="464">
      <title>Gradient-based Adversarial Attacks against Text Transformers</title>
      <author><first>Chuan</first><last>Guo</last></author>
      <author><first>Alexandre</first><last>Sablayrolles</last></author>
      <author><first>Hervé</first><last>Jégou</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <pages>5747–5757</pages>
      <abstract>We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with matching imperceptibility as per automated and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.</abstract>
      <url hash="41d427f7">2021.emnlp-main.464</url>
      <bibkey>guo-etal-2021-gradient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.464</doi>
      <pwccode url="https://github.com/facebookresearch/text-adversarial-attack" additional="false">facebookresearch/text-adversarial-attack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="465">
      <title>Do Transformer Modifications Transfer Across Implementations and Applications?</title>
      <author><first>Sharan</first><last>Narang</last></author>
      <author><first>Hyung Won</first><last>Chung</last></author>
      <author><first>Yi</first><last>Tay</last></author>
      <author><first>Liam</first><last>Fedus</last></author>
      <author><first>Thibault</first><last>Fevry</last></author>
      <author><first>Michael</first><last>Matena</last></author>
      <author><first>Karishma</first><last>Malkan</last></author>
      <author><first>Noah</first><last>Fiedel</last></author>
      <author><first>Noam</first><last>Shazeer</last></author>
      <author><first>Zhenzhong</first><last>Lan</last></author>
      <author><first>Yanqi</first><last>Zhou</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Nan</first><last>Ding</last></author>
      <author><first>Jake</first><last>Marcus</last></author>
      <author><first>Adam</first><last>Roberts</last></author>
      <author><first>Colin</first><last>Raffel</last></author>
      <pages>5758–5773</pages>
      <abstract>The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. Surprisingly, we find that most <a href="https://en.wikipedia.org/wiki/Mod_(video_gaming)">modifications</a> do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same <a href="https://en.wikipedia.org/wiki/Codebase">codebase</a> that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.</abstract>
      <url hash="f395c653">2021.emnlp-main.465</url>
      <bibkey>narang-etal-2021-transformer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.465</doi>
      <pwccode url="https://github.com/google-research/google-research/tree/master/transformer_modifications/" additional="false">google-research/google-research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="466">
      <title>Paired Examples as Indirect Supervision in Latent Decision Models</title>
      <author><first>Nitish</first><last>Gupta</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>5774–5785</pages>
      <abstract>Compositional, structured models are appealing because they explicitly decompose problems and provide interpretable intermediate outputs that give confidence that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is not simply latching onto data artifacts. Learning these models is challenging, however, because end-task supervision only provides a weak indirect signal on what values the latent decisions should take. This often results in the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> failing to learn to perform the intermediate tasks correctly. In this work, we introduce a way to leverage paired examples that provide stronger cues for learning latent decisions. When two related training examples share internal substructure, we add an additional training objective to encourage consistency between their latent decisions. Such an objective does not require external supervision for the values of the latent output, or even the end task, yet provides an additional training signal to that provided by individual training examples themselves. We apply our method to improve compositional question answering using neural module networks on the DROP dataset. We explore three ways to acquire paired questions in DROP : (a) discovering naturally occurring paired examples within the dataset, (b) constructing paired examples using templates, and (c) generating paired examples using a question generation model. We empirically demonstrate that our proposed approach improves both in- and out-of-distribution generalization and leads to correct latent decision predictions.</abstract>
      <url hash="31f73695">2021.emnlp-main.466</url>
      <bibkey>gupta-etal-2021-paired</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.466</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="467">
      <title>Pairwise Supervised Contrastive Learning of Sentence Representations</title>
      <author><first>Dejiao</first><last>Zhang</last></author>
      <author><first>Shang-Wen</first><last>Li</last></author>
      <author><first>Wei</first><last>Xiao</last></author>
      <author><first>Henghui</first><last>Zhu</last></author>
      <author><first>Ramesh</first><last>Nallapati</last></author>
      <author><first>Andrew O.</first><last>Arnold</last></author>
      <author><first>Bing</first><last>Xiang</last></author>
      <pages>5786–5798</pages>
      <abstract>Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with <a href="https://en.wikipedia.org/wiki/Triplet_loss">triplet loss</a> or siamese loss. Nevertheless, they share a common weakness : sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by the fact that the vanilla siamese or triplet losses only learn from individual sentence pairs or triplets, which often suffer from bad local optima. In this paper, we propose PairSupCon, an instance discrimination based approach aiming to bridge semantic entailment and contradiction understanding with high-level categorical concept encoding. We evaluate PairSupCon on various <a href="https://en.wikipedia.org/wiki/Downstream_(networking)">downstream tasks</a> that involve understanding sentence semantics at different <a href="https://en.wikipedia.org/wiki/Granularity">granularities</a>. We outperform the previous state-of-the-art method with 10%13 % averaged improvement on eight clustering tasks, and 5%6 % averaged improvement on seven semantic textual similarity (STS) tasks.</abstract>
      <url hash="0eb1a54c">2021.emnlp-main.467</url>
      <bibkey>zhang-etal-2021-pairwise</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.467</doi>
      <pwccode url="https://github.com/amazon-research/sentence-representations" additional="false">amazon-research/sentence-representations</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="474">
      <title>Classification-based Quality Estimation : Small and Efficient Models for Real-world Applications</title>
      <author><first>Shuo</first><last>Sun</last></author>
      <author><first>Ahmed</first><last>El-Kishky</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>James</first><last>Cross</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <pages>5865–5875</pages>
      <abstract>Sentence-level Quality estimation (QE) of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> is traditionally formulated as a <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression task</a>, and the performance of QE models is typically measured by <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation</a> with human labels. Recent QE models have achieved previously-unseen levels of correlation with human judgments, but they rely on large multilingual contextualized language models that are computationally expensive and make them infeasible for real-world applications. In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression task</a>. However, we argue that the level of expressiveness of a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> in a continuous range is unnecessary given the downstream applications of <a href="https://en.wikipedia.org/wiki/Quantum_electrodynamics">QE</a>, and show that reframing <a href="https://en.wikipedia.org/wiki/Quantum_electrodynamics">QE</a> as a classification problem and evaluating <a href="https://en.wikipedia.org/wiki/Quantum_electrodynamics">QE models</a> using classification metrics would better reflect their actual performance in real-world applications.</abstract>
      <url hash="db4e751d">2021.emnlp-main.474</url>
      <bibkey>sun-etal-2021-classification</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.474</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe-pe">MLQE-PE</pwcdataset>
    <title_ar>تقدير الجودة القائم على التصنيف: نماذج صغيرة وفعالة لتطبيقات العالم الحقيقي</title_ar>
      <title_pt>Estimativa de qualidade baseada em classificação: modelos pequenos e eficientes para aplicações do mundo real</title_pt>
      <title_zh>质量:宜用小形高效</title_zh>
      <title_es>Estimación de calidad basada en la clasificación: modelos pequeños y eficientes para aplicaciones del mundo real</title_es>
      <title_ja>分類ベースの品質推定：現実のアプリケーションのための小さく効率的なモデル</title_ja>
      <title_hi>वर्गीकरण-आधारित गुणवत्ता अनुमान: वास्तविक दुनिया अनुप्रयोगों के लिए छोटे और कुशल मॉडल</title_hi>
      <title_ga>Meastachán Cáilíochta Bunaithe ar Aicmiú: Múnlaí Beaga agus Éifeachtúla d'Fheidhmchláir an Domhain Réadaigh</title_ga>
      <title_el>Αξιολόγηση ποιότητας βασισμένη στην ταξινόμηση: Μικρά και αποδοτικά μοντέλα για εφαρμογές πραγματικού κόσμου</title_el>
      <title_ka>კლასიფიკაციის დაბათი კვალეტის განსაზღვრება: პატარა და ეფექტიური მოდელები რეალური მსოფლიო პროგრამებისთვის</title_ka>
      <title_hu>Osztályozás alapú minőségbecslés: Kis és hatékony modellek a valós alkalmazásokhoz</title_hu>
      <title_it>Valutazione della qualità basata sulla classificazione: modelli piccoli ed efficienti per applicazioni reali</title_it>
      <title_kk>Классификациялау негіздеген сапа оқиғасы: Шындық әлемдегі қолданбалар үшін кішкентай және эффективні үлгілер</title_kk>
      <title_lt>Klasifikacija grindžiamas kokybės vertinimas: maži ir veiksmingi realiojo pasaulio programų pavyzdžiai</title_lt>
      <title_ms>Penghargaan Kualiti Berasas Klasifikasi: Model Kecil dan Efisien untuk Aplikasi Dunia-Real</title_ms>
      <title_mk>Проценка на квалитетот базирана на класификација: Мали и ефикасни модели за реални апликации</title_mk>
      <title_ml>ക്ലാസിസ്സിക്ഷന്‍ അടിസ്ഥാനമാക്കിയ ഗ്വാലിറ്റി എസ്റ്റിമേഷന്‍: റിയല്‍ ലോക പ്രയോഗങ്ങള്‍</title_ml>
      <title_mt>Stima tal-Kwalità bbażata fuq il-Klassifikazzjoni: Mudelli Żgħar u Effiċjenti għall-Applikazzjonijiet fid-Dinja Reali</title_mt>
      <title_mn>Классификацийн үндсэн чадварын төсөөлөл: Реаль дэлхийн програмын жижиг болон эффективны загварууд</title_mn>
      <title_no>Klassifikasjonsbasert kvalitetevaluering: Lite og effektive modeller for realske verdensprogram</title_no>
      <title_pl>Ocena jakości oparta na klasyfikacji: małe i wydajne modele do zastosowań rzeczywistych</title_pl>
      <title_ro>Estimarea calității bazată pe clasificare: Modele mici și eficiente pentru aplicații din lumea reală</title_ro>
      <title_sr>Procjena kvalitete na osnovu klasifikacije: mali i efikasni modeli za aplikacije stvarnog svijeta</title_sr>
      <title_sv>Klassificeringsbaserad kvalitetsuppskattning: Små och effektiva modeller för verkliga tillämpningar</title_sv>
      <title_ta>வகைப்படுத்தல் அடிப்படையிலான தரம் கணக்கீடு: உண்மையான- உலக பயன்பாடுகளுக்கு சிறிய மற்றும் பயன்படுத்தல் மாதிரி</title_ta>
      <title_ur>کلاسیفٹ بنیاد کیلوٹی ارزش: حقیقی دنیا کے لئے چھوٹے اور عمدہ موڈل</title_ur>
      <title_so>Xisaameynta takhasuska aasaasiga ah: Yar iyo Efficient Modeles for Applications Real-World</title_so>
      <title_si>ක්ලාසිෆිකේෂන් අධිරූපය කුළුවත් අනුමාණය: පුංචි හා සක්‍රීය විද්‍යාපිත විද්‍යාපය විද්‍යා</title_si>
      <title_uz>Comment</title_uz>
      <title_vi>Hạng ước lượng: Cơ chế độ nhỏ và hiệu quả cho ứng dụng thế giới thực</title_vi>
      <title_bg>Оценка на качеството въз основа на класификация: малки и ефективни модели за приложения в реалния свят</title_bg>
      <title_de>Klassifikationsbasierte Qualitätsschätzung: Kleine und effiziente Modelle für reale Anwendungen</title_de>
      <title_id>Perkiraan Kualitas Berdasarkan Klasifikasi: Model Kecil dan Efisien untuk Aplikasi Dunia Real</title_id>
      <title_hr>Očekivanje kvalitete na temelju klasifikacije: mali i učinkoviti modeli za aplikacije stvarnog svijeta</title_hr>
      <title_nl>Kwaliteitsschatting op basis van classificatie: Kleine en efficiënte modellen voor toepassingen in de echte wereld</title_nl>
      <title_da>Klassificeringsbaseret kvalitetsvurdering: Små og effektive modeller for virkelige applikationer</title_da>
      <title_fa>ارزیابی کیفیت بر پایه کلاسیک: مدل کوچک و فعالیت برای کاربردهای دنیای واقعی</title_fa>
      <title_tr>S캇n캇flandyrma G철rn철힊i Ta첵첵arlama: Ger챌ek d체n첵채 uygulamalar 체챌in ki챌i we 첵eterlik Modeller</title_tr>
      <title_sw>Hisabu yenye sifa za darasa: Modeli ndogo na yenye ufanisi kwa ajili ya matumizi ya dunia halisi</title_sw>
      <title_af>Klassifikasie-gebaseerde Kwaliteit Estimatie: Klein en Effektiewe Modelle vir Werklike Wêreld Toepassinge</title_af>
      <title_sq>Vlerësimi i cilësisë bazuar në klasifikim: Modele të vogla dhe efikase për aplikimet e botës reale</title_sq>
      <title_am>ሁኔታ፦</title_am>
      <title_hy>Կալիֆորմացիայի հիմնված որակի գնահատականը՝ փոքրիկ և արդյունավետ մոդելներ իրական աշխարհի ծրագրերի համար</title_hy>
      <title_ko>분류 기반의 품질 평가: 실제 응용에 사용되는 소형 고효율 모델</title_ko>
      <title_bn>বাস্তব বিশ্বপূর্ণ অ্যাপ্লিকেশনের জন্য ছোট ও কার্যকর মোডেল</title_bn>
      <title_bs>Procjena kvalitete na temelju klasifikacije: mali i efikasni modeli za aplikacije realnog svijeta</title_bs>
      <title_ca>Estimat de qualitat basat en la classificació: Models petits i eficients per aplicacions del món real</title_ca>
      <title_az>Klasifikat-tabanl캼 Q캼ymet Tahm톛si: Real-world Uygulamalar캼 칲칞칲n Ki칞ik v톛 Efficient Modell톛r</title_az>
      <title_cs>Klasifikační odhad kvality: malé a efektivní modely pro reálné aplikace</title_cs>
      <title_fi>Luokitukseen perustuva laatuarvio: pienet ja tehokkaat mallit reaalimaailman sovelluksiin</title_fi>
      <title_et>Klassifikatsioonipõhine kvaliteedihinnang: väikesed ja tõhusad mudelid reaalmaailma rakenduste jaoks</title_et>
      <title_jv>Ukuran</title_jv>
      <title_he>הערכת איכות מבוססת על כיסוי: דוגמנים קטנים ויעילים לתוכניות בעולם האמיתי</title_he>
      <title_sk>Ocena kakovosti na podlagi klasifikacije: majhni in učinkoviti modeli za aplikacije v realnem svetu</title_sk>
      <title_ha>@ action: inmenu Go</title_ha>
      <title_bo>Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications</title_bo>
      <abstract_ar>تتم صياغة تقدير الجودة على مستوى الجملة (QE) للترجمة الآلية بشكل تقليدي كمهمة انحدار ، ويتم قياس أداء نماذج التيسير الكمي عادةً من خلال ارتباط بيرسون بالتسميات البشرية. حققت نماذج التيسير الكمي الحديثة مستويات غير مسبوقة من الارتباط مع الأحكام البشرية ، لكنها تعتمد على نماذج لغة سياقية متعددة اللغات باهظة الثمن من الناحية الحسابية وتجعلها غير قابلة للتطبيق في تطبيقات العالم الحقيقي. في هذا العمل ، قمنا بتقييم العديد من تقنيات الضغط النموذجية للتسهيل الكمي ووجدنا أنه على الرغم من شعبيتها في مهام البرمجة اللغوية العصبية الأخرى ، فإنها تؤدي إلى أداء ضعيف في إعداد الانحدار هذا. نلاحظ أن معلمات النموذج الكامل مطلوبة لتحقيق نتائج SoTA في مهمة الانحدار. ومع ذلك ، فإننا نجادل بأن مستوى التعبير عن نموذج في نطاق مستمر غير ضروري نظرًا للتطبيقات النهائية للتسهيل الكمي ، ونبين أن إعادة صياغة التيسير الكمي كمشكلة تصنيف وتقييم نماذج التيسير الكمي باستخدام مقاييس التصنيف سيعكس أداءها الفعلي بشكل أفضل في الواقع. تطبيقات العالم.</abstract_ar>
      <abstract_es>La estimación de calidad (QE) a nivel de frase de la traducción automática se formula tradicionalmente como una tarea de regresión, y el rendimiento de los modelos de QE se mide normalmente mediante la correlación de Pearson con etiquetas humanas. Los modelos de QE recientes han logrado niveles de correlación nunca antes vistos con los juicios humanos, pero se basan en grandes modelos lingüísticos contextualizados multilingües que son computacionalmente caros y los hacen inviables para aplicaciones del mundo real. En este trabajo, evaluamos varias técnicas de compresión de modelos para QE y descubrimos que, a pesar de su popularidad en otras tareas de PNL, conducen a un rendimiento deficiente en esta configuración de regresión. Observamos que se requiere una parametrización completa del modelo para lograr resultados de SoTA en una tarea de regresión. Sin embargo, argumentamos que el nivel de expresividad de un modelo en un rango continuo es innecesario dadas las aplicaciones posteriores de la QE, y demostramos que replantear la QE como un problema de clasificación y evaluar los modelos de QE utilizando métricas de clasificación reflejaría mejor su rendimiento real en el mundo real. aplicaciones.</abstract_es>
      <abstract_zh>古者,机器翻译之句(QE)度为归任,QE形之性常以Pearson与人相关性。 近者QE已成前所未见者,与人相关性平,然其依于多言上下文语,成本于算,而不可于用。 凡此诸事,估QE数形压缩术,虽在他NLP甚受欢迎,归置中致不佳。 吾观其归 SoTA 而须全形参数化。 然吾以为鉴于QE之下流,连范围模型之表现力水平不足,明将QE更为分类而用分类指标评估QE形将益见其实用之实也。</abstract_zh>
      <abstract_pt>A estimativa de qualidade em nível de sentença (QE) da tradução automática é tradicionalmente formulada como uma tarefa de regressão, e o desempenho dos modelos de QE é normalmente medido pela correlação de Pearson com rótulos humanos. Modelos de QE recentes alcançaram níveis nunca vistos de correlação com julgamentos humanos, mas eles contam com grandes modelos de linguagem contextualizada multilíngue que são computacionalmente caros e os tornam inviáveis para aplicações do mundo real. Neste trabalho, avaliamos várias técnicas de compressão de modelo para QE e descobrimos que, apesar de sua popularidade em outras tarefas de PNL, elas levam a um desempenho ruim nessa configuração de regressão. Observamos que uma parametrização completa do modelo é necessária para obter resultados SoTA em uma tarefa de regressão. No entanto, argumentamos que o nível de expressividade de um modelo em um intervalo contínuo é desnecessário, dadas as aplicações downstream do QE, e mostramos que reformular o QE como um problema de classificação e avaliar os modelos de QE usando métricas de classificação refletiria melhor seu desempenho real em condições reais. aplicações mundiais.</abstract_pt>
      <abstract_ja>機械翻訳の文章レベルの品質推定（ QE ）は、伝統的に回帰タスクとして策定され、QEモデルのパフォーマンスは、典型的には、人間のラベルとのピアソン相関によって測定されます。 最近のQEモデルは、人間の判断との以前には見られなかったレベルの相関を達成していますが、計算コストが高く、現実のアプリケーションでは実現不可能な大規模な多言語コンテキスト化された言語モデルに依存しています。 この研究では、QEのためのいくつかのモデル圧縮技術を評価し、他のNLPタスクで人気があるにもかかわらず、この回帰設定ではパフォーマンスが低下することを発見しました。 回帰タスクでSoTA結果を達成するには、完全なモデルパラメータ化が必要であることがわかります。 しかし、我々は、QEの下流アプリケーションを考慮すると、モデルの連続した範囲での表現性のレベルは不要であると主張し、QEを分類問題として再フレーム化し、分類指標を使用してQEモデルを評価することは、実際のアプリケーションでの実際のパフォーマンスをよりよく反映することが示されている。</abstract_ja>
      <abstract_ga>Go traidisiúnta foirmítear Meastachán Cáilíochta ar leibhéal na habairte (QE) ar aistriúchán meaisín mar thasc aischéimnithe, agus de ghnáth déantar feidhmíocht na múnlaí QE a thomhas trí chomhghaol Pearson le lipéid dhaonna. Tá leibhéil chomhghaolmhaireachta le breithiúnais dhaonna bainte amach ag samhlacha QE le déanaí nach bhfacthas riamh cheana, ach braitheann siad ar mhúnlaí móra ilteangacha teanga comhthéacsúla atá costasach ó thaobh ríomhaireacht de agus a fhágann nach féidir iad a úsáid le haghaidh feidhmeanna sa saol fíor. San obair seo, déanaimid meastóireacht ar roinnt teicnící comhbhrú samhlacha le haghaidh QE agus aimsímid, in ainneoin a n-éilimh i dtascanna NLP eile, go n-eascraíonn siad drochfheidhmíocht sa suíomh aischéimnithí seo. Breathnaímid go bhfuil gá le samhailpharaiméadarú iomlán chun torthaí SoTA a bhaint amach i dtasc cúlchéimnithe. Áitímid, áfach, nach bhfuil gá le leibhéal sainráite múnla i raon leanúnach i bhfianaise iarratais iartheachtacha QE, agus léirímid go léireodh níos fearr a bhfeidhmíocht iarbhír i bhfíor-fheidhmíocht dá ndéanfaí athfhrámáil ar QE mar fhadhb aicmithe agus trí mhúnlaí QE a mheas ag baint úsáide as méadracht aicmithe. iarratais domhan.</abstract_ga>
      <abstract_hi>मशीन अनुवाद के वाक्य-स्तर गुणवत्ता अनुमान (क्यूई) को पारंपरिक रूप से एक प्रतिगमन कार्य के रूप में तैयार किया जाता है, और क्यूई मॉडल के प्रदर्शन को आमतौर पर मानव लेबल के साथ पियर्सन सहसंबंध द्वारा मापा जाता है। हाल के क्यूई मॉडल ने मानव निर्णयों के साथ सहसंबंध के पहले-अनदेखे स्तरों को प्राप्त किया है, लेकिन वे बड़े बहुभाषी प्रासंगिक भाषा मॉडल पर भरोसा करते हैं जो कम्प्यूटेशनल रूप से महंगे हैं और उन्हें वास्तविक दुनिया के अनुप्रयोगों के लिए अव्यवहार्य बनाते हैं। इस काम में, हम क्यूई के लिए कई मॉडल संपीड़न तकनीकों का मूल्यांकन करते हैं और पाते हैं कि, अन्य एनएलपी कार्यों में उनकी लोकप्रियता के बावजूद, वे इस प्रतिगमन सेटिंग में खराब प्रदर्शन का कारण बनते हैं। हम देखते हैं कि प्रतिगमन कार्य में SoTA परिणामों को प्राप्त करने के लिए एक पूर्ण मॉडल पैरामीटराइजेशन की आवश्यकता होती है। हालांकि, हम तर्क देते हैं कि एक निरंतर सीमा में एक मॉडल की अभिव्यंजकता का स्तर क्यूई के डाउनस्ट्रीम अनुप्रयोगों को देखते हुए अनावश्यक है, और यह दर्शाता है कि क्यूई को वर्गीकरण समस्या के रूप में फिर से तैयार करना और वर्गीकरण मीट्रिक का उपयोग करके क्यूई मॉडल का मूल्यांकन करना वास्तविक दुनिया के अनुप्रयोगों में उनके वास्तविक प्रदर्शन को बेहतर ढंग से प्रतिबिंबित करेगा।</abstract_hi>
      <abstract_hu>A gépi fordítás mondatszintű minőségbecslését (QE) hagyományosan regressziós feladatként fogalmazzák meg, a QE modellek teljesítményét pedig jellemzően Pearson korrelációval mérik az emberi címkékkel. A legújabb QE modellek korábban nem látott korrelációt értek el az emberi ítéletekkel, de nagy többnyelvű kontextuális nyelvi modellekre támaszkodnak, amelyek számítástechnikailag drágák és a valós alkalmazások számára nem megfelelőek. Ebben a munkában több modell tömörítési technikát értékelünk a QE számára, és megállapítjuk, hogy annak ellenére, hogy népszerűségük más NLP feladatokban, ezek rossz teljesítményhez vezetnek ebben a regressziós beállításban. Megfigyeljük, hogy teljes modellparaméterezésre van szükség ahhoz, hogy a SotA regressziós feladatot eredményezzen. Azonban azzal érvelünk, hogy egy modell folyamatos tartományban való kifejezőképességének szintje szükségtelen, tekintettel a minőségi minőségbiztosítás downstream alkalmazásaira, és megmutatjuk, hogy a minőségbiztosítási problémának átalakítása és a minőségbiztosítási modellek osztályozási mutatók alkalmazásával való értékelése jobban tükrözné a valós alkalmazásokban való tényleges teljesítményüket.</abstract_hu>
      <abstract_ka>მანქანის გარგულისხმების კვალეტური განსაზღვრება (QE) სამუშაო სიტყვების განსაზღვრება როგორც რეგრესიის რაოდენობა, და QE მოდელების გამოსაზღვრება ტიპონალურად მოზღვრება Pearson კორელაცია QE მოდელები უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე უკვე სიტყვის მოდელების შესახებ,  ამ სამუშაოში, ჩვენ QE-სთვის რამდენიმე მოდელური კომპრესიის ტექნექციები გავამუშავებთ და აღმოჩნეთ, რომ, სხვა NLP სამუშაოში, ისინი გავამუშავებენ ცოტა კომპრე ჩვენ დავხედავთ, რომ სრულ მოდელური პარამეტრიზაცია საჭიროა SoTA წარმოდგენისთვის რეგრესიის დავალებაში. თუმცა, ჩვენ აღმოჩნეთ, რომ მოდელის გამოსახულებელობის დონე მუშაობაში QE-ის ჩვენებაში უნდა უნდა იყოს, რომ QE-ს კლასიფიკაციის პრობლემა და QE მოდელის გამოსახულებაში კლასიფიკაციის მეტრიკის გამოყენებაში უფრო უფრო უნდა გა</abstract_ka>
      <abstract_el>Η εκτίμηση της ποιότητας (QE) της μηχανικής μετάφρασης διατυπώνεται παραδοσιακά ως εργασία παλινδρόμησης και η απόδοση των μοντέλων QE συνήθως μετράται με τη συσχέτιση Pearson με ανθρώπινες ετικέτες. Τα πρόσφατα μοντέλα έχουν επιτύχει άγνωστα επίπεδα συσχέτισης με ανθρώπινες κρίσεις, αλλά βασίζονται σε μεγάλα πολύγλωσσα γλωσσικά μοντέλα που είναι υπολογιστικά ακριβά και τα καθιστούν αδύνατα για εφαρμογές πραγματικού κόσμου. Σε αυτή την εργασία, αξιολογούμε διάφορες τεχνικές συμπίεσης μοντέλων για την QE και διαπιστώνουμε ότι, παρά τη δημοτικότητά τους σε άλλες εργασίες οδηγούν σε κακή απόδοση σε αυτή τη ρύθμιση παλινδρόμησης. Παρατηρούμε ότι απαιτείται πλήρης παραμετροποίηση μοντέλου για την επίτευξη αποτελεσμάτων σε μια εργασία παλινδρόμησης. Ωστόσο, υποστηρίζουμε ότι το επίπεδο εκφραστικότητας ενός μοντέλου σε συνεχή κλίμακα είναι περιττό δεδομένου των μεταγενέστερων εφαρμογών της QE, και δείχνουν ότι ο επαναπροσδιορισμός της QE ως πρόβλημα ταξινόμησης και η αξιολόγηση των μοντέλων QE χρησιμοποιώντας μετρήσεις ταξινόμησης θα αντικατοπτρίζουν καλύτερα την πραγματική τους απόδοση σε πραγματικές εφαρμογές.</abstract_el>
      <abstract_lt>Mašininio vertimo kokybės vertinimas (QE) nuosprendžių lygiu tradiciškai formuluojamas kaip regresijos užduotis, o QE modelių veiksmingumas paprastai matuojamas pagal Pearson koreliaciją su žmogaus etiketėmis. Pastaraisiais QE modeliais buvo pasiektas anksčiau nematomas koreliacijos su žmogaus sprendimais lygis, tačiau jie grindžiami dideliais daugiakalbiais kontekstiniu kalbų modeliais, kurie yra skaičiavimo požiūriu brangūs ir sudaro juos neįmanomas naudoti realiajame pasaulyje. Šiame darbe vertiname keletą modelių kompresijos metodų QE atžvilgiu ir nustatome, kad nepaisant jų populiarumo kitose NLP užduotyse, šios regresijos sąlygomis jos daro prastus rezultatus. Pastebėjome, kad norint pasiekti SoTA rezultatus būtina atlikti visišką modelio parametrizavimą. Tačiau mes teigiame, kad modelio išraiškumo lygis nuolatiniame intervale yra nereikalingas atsižvelgiant į tolesnes QE taikymo sritis, ir rodome, kad QE pertvarkymas kaip klasifikacijos problem a ir QE modelių vertinimas naudojant klasifikacijos metrijas geriau atspindėtų jų faktinį veiksmingumą realaus pasaulio taikymo srityse.</abstract_lt>
      <abstract_mk>Проценката на квалитетот на нивото на реченици (QE) на машинскиот превод традиционално се формулира како регресна задача, а изведбата на моделите на QE е обично мерена со корелација на Пирсон со човечките етикети. Неодамнешните модели на QE постигнаа претходно невидени нивоа на корелација со човечките пресуди, но тие се потпираат на големи мултијазични контекстуализирани јазички модели кои се пресметувачки скапи и ги прават неприфатливи за реални апликации. In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting.  Забележуваме дека е потребна целосна параметризација на моделот за да се постигне СоTA резултат на регресна задача. Сепак, тврдиме дека нивото на експресивност на моделот во континуиран период е непотребно со оглед на понатамошните апликации на QE, и покажуваме дека рефирмирањето на QE како класификациски проблем и евалуирањето на QE моделите со користење на класификациските метрики подобро би ги одразувало нивните реални успешности во ре</abstract_mk>
      <abstract_it>La stima della qualità (QE) della traduzione automatica è tradizionalmente formulata come un compito di regressione e le prestazioni dei modelli QE sono tipicamente misurate dalla correlazione Pearson con le etichette umane. I recenti modelli QE hanno raggiunto livelli di correlazione inediti con i giudizi umani, ma si basano su grandi modelli linguistici contestualizzati multilingue che sono computazionalmente costosi e li rendono impossibili per le applicazioni del mondo reale. In questo lavoro, valutiamo diverse tecniche di compressione del modello per QE e scopriamo che, nonostante la loro popolarità in altre attività NLP, portano a scarse prestazioni in questa impostazione di regressione. Osserviamo che è necessaria una parametrizzazione completa del modello per ottenere risultati SoTA in un compito di regressione. Tuttavia, sosteniamo che il livello di espressività di un modello in un intervallo continuo non sia necessario, date le applicazioni a valle del QE, e mostriamo che riformulare il QE come problema di classificazione e valutare i modelli QE utilizzando metriche di classificazione rifletterebbe meglio le loro prestazioni effettive nelle applicazioni del mondo reale.</abstract_it>
      <abstract_kk>Компьютердің сөз деңгейінің сапасы бағалауы (QE) әдетте регрессия тапсырмасы ретінде формулады, және QE үлгілерін әдетте Pearson коррелациясы адамдар жарлықтарымен өлшейтін болады. Жуырдағы QE үлгілері адамдардың түсініктемелерімен алдын- ала көрсетілмеген қатынас деңгейіне жеткізді, бірақ олар компьютерлік үлкен көп тілдер үлгілікті тілдер үлгілеріне тұрады, оларды шын әлемді қолдан Бұл жұмыс ішінде QE үшін бірнеше түрлі компрессия техникаларын бағалап, басқа NLP тапсырмаларындағы мәліметтеріне қарай, олар регрессия параметрлерінде өзгертілмейді. Біз SoTA нәтижесін регрессия тапсырмасына жеткізу үшін толық үлгі параметрлерін қажет етеді деп белгіледік. Бірақ, біз үлгісінің дұрыс аумағындағы үлгіліктер деңгейі QE бағдарламаларының төменгі қолданбаларына келтірілмейді деп айтып, QE классификациялық мәселесі ретінде және QE моделдерін классификациялық метрикалық қолданбаларының шынды</abstract_kk>
      <abstract_ml>മെഷിന്‍ പരിഭാഷക്കുറിച്ചുള്ള ശിക്ഷ- നില വ്യവസ്ഥ (ക്യൂയി) പരിശോധനത്തിന്റെ പാഠമായി രീക്രഷന്‍ ജോലിയായി രൂപപ്പെടുത്തിയിരിക്കുന്നു. ക്യൂ അടുത്തിടെ ക്യൂ ഇ മോഡലുകള്‍ മുമ്പ് മനുഷ്യരുടെ വിധികളുമായി ബന്ധപ്പെടാത്ത നിലങ്ങളില്‍ എത്തിയിരിക്കുന്നു. പക്ഷെ അവരതില്‍ ആശ്രയിക്കുന്നു വലിയ മണ്ണില്‍ കണ ഈ പ്രവര്‍ത്തനത്തില്‍, ക്യൂയിക്കുവേണ്ടി കുറച്ച് മോഡല്‍ സമ്പൂര്‍ണ്ണമായ സാങ്കേതികവിദ്യകള്‍ വിലാസപ്പെടുത്തുന്നു. മറ്റു NLP ജോലികളില്‍  സോട്ടാവിന്റെ ഫലങ്ങള്‍ പൂര്‍ണ്ണമായ മോഡല്‍ പരാമീറ്ററേഷന്‍ ആവശ്യമാണെന്ന് ഞങ്ങള്‍ കാണുന്നു. എന്നാലും, നിലനില്‍ക്കുന്ന ഒരു മോഡലിന്‍റെ പ്രകടനത്തിന്‍റെ നിലപാട് ക്യൂയിയുടെ താഴ്വരയുടെ പ്രയോഗങ്ങള്‍ക്ക് ആവശ്യമില്ല എന്നും ക്യൂയിയെ ക്ലാസ്ഫിക്ഷന്‍ പ്രശ്നമാക്കുന്നതും ക്യൂ</abstract_ml>
      <abstract_no>Estimasjon av setningsnivå kvalitet (QE) av maskinsomsetjinga er tradisjonell formert som regresjonsverkt, og utviklinga av QE-modeller er vanlegvis målt av Pearson-korrelasjon med menneske etikettar. Nyleg brukte QE-modeller har oppnådd tidlegare ukjende korrelasjonsnivåar med menneske sprøytebrukar, men dei er på stor fleirspråk kontekstualiserte språk-modeller som er datamaskinen dyrt og gjer dei infeksibel for verkelege programmer. I denne arbeida evaluerer vi fleire modeller komprimeringsteknikk for QE og finn at, selv om populariteten i andre NLP-oppgåver, fører dei til dårlig utvikling i denne regresjonsinnstillingane. Vi observerer at ein fullstendig modellparameterisering er nødvendig for å oppnå SoTA-resultat i ei regresjonsbehandling. Men vi argumenterer at nivået av uttrykket av eit modell i eit kontinuerleg område er unnecessar gitt nedstrekkprogrammet av QE, og viser at reframering av QE som eit klassifikasjonsproblem og evaluering av QE-modeller med klassifikasjonsmeterikk vil betre refleksera sine faktiske uttrykk i verdens program.</abstract_no>
      <abstract_mn>Машины хөрөнгө оруулалтын хэмжээний хэмжээний качест тооцоолол (QE) нь уламжлалтаар регрессийн даалгавар гэж тооцоолж байна. QE загварын үйл ажиллагаа нь ихэвчлэн Персон хүн төрөлхтний etiketтэй холбоотой хэмж Сүүлийн үеийн QE загварууд хүн төрөлхтний шүүмжлэлтэй холбоотой хэмжээний түвшинд өмнө нь харагдаагүй түвшинд хүргэсэн байна. Гэхдээ тэд маш олон хэлний орчин үеийн хэл загваруудыг тооцоолж маш үнэтэй, бодит ертөнцийн хэр Энэ ажил дээр бид QE-ийн хэдэн загварын даралтын технологийг үнэлгээд бусад NLP даалгаврууд дээр нэр хүндрэлтэй байсан ч, тэд бусад сэтгэл хөдлөлт дээр ядуу үйл ажиллагааг хүргэж чадна. Бид SoTA-ын үр дүнг сэтгэл хөдлөлийн үйлдлийн үр дүнд бүрэн загварын параметрийг олох хэрэгтэй гэдгийг анзаарлаа. Гэхдээ бид үргэлжлүүлэх хэмжээнд загварын илэрхийллийн түвшин нь QE-ийн доорх хэрэглээнд хэрэггүй гэдгийг хэлж байна. QE-г хуваалцах асуудал болгон, QE-ийн загварыг хуваалцах нь хуваалцах хэмжээний метрикийг ашиглан хуваалцах нь бодит ертөнцийн хэрэглээнд илүү үр</abstract_mn>
      <abstract_pl>Ocena jakości (QE) tłumaczenia maszynowego jest tradycyjnie formułowana jako zadanie regresyjne, a wydajność modeli QE jest zazwyczaj mierzona korelacją Pearsona z etykietami ludzkimi. Najnowsze modele QE osiągnęły niewidoczny wcześniej poziom korelacji z osądami ludzkimi, ale opierają się na dużych wielojęzycznych kontekstualizowanych modelach językowych, które są kosztowne obliczeniowo i sprawiają, że są niewykonalne dla aplikacji w świecie rzeczywistym. W niniejszej pracy oceniamy kilka technik kompresji modeli dla QE i stwierdzimy, że pomimo ich popularności w innych zadaniach NLP, prowadzą one do słabej wydajności w tym ustawieniu regresji. Obserwujemy, że do osiągnięcia wyników SoTA w zadaniu regresji wymagana jest pełna parametryzacja modelu. Uważamy jednak, że poziom ekspresywności modelu w ciągłym zakresie jest niepotrzebny biorąc pod uwagę dalsze zastosowania QE i pokazujemy, że przekształcenie QE jako problemu klasyfikacyjnego i ocena modeli QE za pomocą wskaźników klasyfikacyjnych lepiej odzwierciedlałoby ich rzeczywistą wydajność w aplikacjach rzeczywistych.</abstract_pl>
      <abstract_ro>Estimarea calității (QE) a traducerii automate este formulată în mod tradițional ca o sarcină de regresie, iar performanța modelelor QE este de obicei măsurată prin corelația Pearson cu etichetele umane. Modelele QE recente au atins niveluri nevăzute anterior de corelație cu judecățile umane, dar se bazează pe modele lingvistice contextualizate multilingve mari, care sunt costisitoare din punct de vedere computațional și le fac imposibile pentru aplicațiile din lumea reală. În această lucrare, evaluăm mai multe tehnici de compresie a modelului pentru QE și constatăm că, în ciuda popularității lor în alte sarcini PNL, ele duc la performanțe slabe în această setare de regresie. Observăm că este necesară o parametrizare completă a modelului pentru a obține rezultatele SoTA într-o sarcină de regresie. Cu toate acestea, susținem că nivelul de expresivitate al unui model într-o gamă continuă nu este necesar având în vedere aplicațiile din aval ale QE și arătăm că reformarea QE ca problemă de clasificare și evaluarea modelelor QE folosind metrici de clasificare ar reflecta mai bine performanța lor reală în aplicațiile din lumea reală.</abstract_ro>
      <abstract_ms>Perkiraan Kualiti Aras-perkataan (QE) terjemahan mesin secara tradisional dibentuk sebagai tugas regresi, dan prestasi model QE biasanya diukur oleh korelasi Pearson dengan label manusia. Model QE baru-baru ini telah mencapai tahap korelasi yang belum pernah terlihat dengan penilaian manusia, tetapi mereka bergantung pada model bahasa berbagai bahasa yang terkontekstualisasi besar yang secara komputasi mahal dan menjadikannya tidak dapat disembunyikan untuk aplikasi dunia nyata. Dalam kerja ini, kami menilai beberapa teknik pemampatan model untuk QE dan mendapati bahawa walaupun popularitas mereka dalam tugas NLP lain, mereka membawa kepada prestasi yang buruk dalam seting regresi ini. We observe that a full model parameterization is required to achieve SoTA results in a regression task.  Namun, kami menyangka bahawa aras ekspresif model dalam julat berterusan tidak diperlukan kerana aplikasi turun QE, dan menunjukkan bahawa mengubah QE sebagai masalah klasifikasi dan menilai model QE menggunakan metrik klasifikasi akan lebih baik mencerminkan prestasi sebenar mereka dalam aplikasi dunia nyata.</abstract_ms>
      <abstract_si>වාර්තාව-ස්තූති කුළුවත් අවශ්‍යය (QE) පද්ධතිය පද්ධතියෙන් වාර්තාව ප්‍රමාණය වෙනුවෙන් ප්‍රමාණයක් වෙනුවෙන් සූදානම් කරනවා, සහ Q මිනිස්සු විශ්වාසයෙන් වැඩි භාෂාව ප්‍රතිභාවිත විශ්වාස කරපු භාෂාව මොඩේල් වලින් පරීක්ෂණයෙන් ගොඩක් බලාපොරොත්තු වෙන්න පුළුවන මේ වැඩේ අපි QE වෙනුවෙන් මොඩල් සංකීර්ණ විද්‍යාවක් විශ්වාස කරනවා ඒ වගේම හොයාගන්නවා ඒක, අනිත් NLP වැඩේ ඔවුන්ගේ ප්‍රමාණය, ඔ අපි බලාපොරොත්තු කරනවා සම්පූර්ණ මොඩල් ප්‍රමාණයක් අවශ්‍යයි SoTA ප්‍රතිචාරයක් ප්‍රතිචාරයක් වෙන නමුත්, අපි ප්‍රශ්නය කරනවා කියලා නිතරම් ප්‍රශ්නයක් ප්‍රශ්නයක් තියෙන්නේ QE වලින් විශ්වාස කරපු ප්‍රශ්නයක් වගේ QE විශ්වාස කරපු ප්‍රශ්නයක් වගේ ප්‍රශ්න</abstract_si>
      <abstract_mt>L-istima tal-kwalità fil-livell tas-sentenza (QE) tat-traduzzjoni bil-magna tradizzjonalment hija fformulata bħala kompitu ta’ rigressjoni, u l-prestazzjoni tal-mudelli QE tipikament titkejjel bil-korrelazzjoni ta’ Pearson mat-tikketti umani. Il-mudelli reċenti tal-QE kisbu livelli ta’ korrelazzjoni li ma dehrux qabel mas-sentenzi umani, iżda jiddependu fuq mudelli kbar multilingwi kuntestwalizzati tal-lingwa li huma għaljin b’mod komputattiv u jagħmluhom infeżibbli għall-applikazzjonijiet tad-dinja reali. F’dan ix-xogħol, aħna jevalwaw diversi metodi ta’ kompressjoni mudelli għall-QE u nsibu li, minkejja l-popolarità tagħhom f’kompiti oħra tal-NLP, dawn iwasslu għal prestazzjoni ħażina f’dan l-ambjent ta’ rigressjoni. Aħna ninnota li hija meħtieġa parametrizzazzjoni sħiħa tal-mudell biex jinkiseb SoTA tirriżulta f’kompitu ta’ rigressjoni. Madankollu, a ħna jargumentaw li l-livell ta’ espressività ta’ mudell f’firxa kontinwa mhuwiex meħtieġ minħabba l-applikazzjonijiet downstream ta’ QE, u juru li r-riformulazzjoni ta’ QE bħala problem a ta’ klassifikazzjoni u l-evalwazzjoni ta’ mudelli QE bl-użu ta’ metriċi ta’ klassifikazzjoni jirriflettu aħjar il-prestazzjoni attwali tagħhom fl-applikazzjonijiet tad-dinja reali.</abstract_mt>
      <abstract_sr>Procjenjivanje kvalitete kazne (QE) prevoda mašine je tradicionalno formulisano kao zadatak regresije, a provedba modela QE obično se mjera od korelacije Pearsona sa ljudskim etiketama. Skorašnji QE modeli su postigli previše nevidljive nivoe korelacije sa ljudskim osuđivanjima, ali se oslanjaju na velike multijezičke kontekstualizacije jezičkih modela koji su kompjuterski skupi i čine ih nevidljivim za realne aplikacije. U ovom poslu, procjenjujemo nekoliko modela tehnika kompresije za QE i otkrijemo da, uprkos njihovoj popularnosti u drugim zadacima NLP-a, oni vode do lošeg izvođenja u ovom regresiji. Primišljamo da je potrebna puna modelna parameterizacija za ostvarivanje SoTA rezultata u zadatku regresije. Međutim, tvrdimo da je nivo izrazitosti model a u kontinualnom rasponu nepotrebno s obzirom na sledeće aplikacije QE-a, i pokazujemo da reframiranje QE kao problem klasifikacije i procjena modela QE koristeći klasifikacijske metrike bolje odražava njihovu stvarnu funkciju u realnim aplikacijama.</abstract_sr>
      <abstract_so>Qiimeynta qalabka qiyaastiisa (QE) ee turjumaadda machine waa sida shaqada regression, waxaana lagu qiyaasaa tusaalaha QE sida caadiga ah loo qiyaasay Pearson oo la xiriira calaamadaha dadka. Tusaalooyinka QE waxay gaadheen heerarka hore oo aan la arkaynin ee la xiriira xukummada dadka, laakiin waxay isku halleeyaan tusaalooyin badan oo luuqadaha kala duduwan oo xisaabta qaali ah, waxayna ka dhigaan kuwa aan dhibaataysan codsiga caalamiga ah. Markaas waxan, waxaynu qiimeynaynaa qalabka hoos-dhigista ee QE, waxaana helaynaa in kastoo ay popularity ku leeyihiin shaqaalaha kale ee NLP, waxay ku hogaansamaan tababar miskiin ah marka loo sameeyo qorsheyntan. Waxaynu aragnaa in lagu baahan yahay qaab sameynta oo dhan si uu SoTA u gaadho uu u sameeyo shaqada dib u celinta. Si kastaba ha ahaatee waxaynu ka fekeraynaa in heerka muusikada daboolka oo joogtada ah looma baahna in la siiyo codsiyada QE-ka hooseeya, waxaana tusinayaa in QE-ka dhigista dhibaato fasax ah iyo qiimeynaya modelalka QE-ka isticmaalka midibka fasaxa waxaa haboon in ay ka fikiraan muuqashadooda dhabta ah ee lagu sameeyo codsiyada caalamiga ah.</abstract_so>
      <abstract_sv>Kvalitetsuppskattning (QE) av maskinöversättning formuleras traditionellt som en regressionsuppgift, och prestandan hos QE-modeller mäts vanligtvis genom Pearson korrelation med mänskliga etiketter. Nya QE-modeller har uppnått tidigare okända nivåer av korrelation med mänskliga bedömningar, men de förlitar sig på stora flerspråkiga kontextuella språkmodeller som är beräkningsmässigt dyra och gör dem omöjliga för verkliga tillämpningar. I detta arbete utvärderar vi flera modellkomprimeringstekniker för QE och upptäcker att de, trots deras popularitet i andra NLP-uppgifter, leder till dålig prestanda i denna regressionsinställning. Vi observerar att en fullständig modellparametrering krävs för att uppnå SoTA resultat i en regressionsuppgift. Vi hävdar dock att nivån på uttrycksfullhet hos en modell i ett kontinuerligt intervall är onödig med tanke på efterföljande tillämpningar av QE, och visar att omdefiniering av QE som ett klassificeringsproblem och utvärdering av QE-modeller med hjälp av klassificeringsmetoder bättre skulle återspegla deras faktiska prestanda i verkliga applikationer.</abstract_sv>
      <abstract_ta>கணினி மொழிபெயர்ப்பின் வாக்கு- நிலையின் தரம் மதிப்பு (QE) பாரமிக்கப்பட்டுள்ளது மறும் QE மாதிரிகளின் செயல்பாடு மனித சிட்டைகளுடன் இணைப்பாக அளவிடப்படுக சமீபத்தில் QE மாதிரிகள் முன்னால் மறைக்கப்படாத மட்டத்தில் மனித விதிப்புகளுடன் தொடர்புடைய நிலைகளை அடைந்துவிட்டன, ஆனால் அவர்கள் பெரிய மொழி பாதிக்கப்பட்ட மாதி இந்த வேலையில், நாம் QE க்கான பல மாதிரி சுருக்கும் தொழில்நுட்பத்தை மதிப்பீடு செய்கிறோம் மற்றும் NLP பணிகளில் அவர்களுடைய மகிழ்ச்சியை  நாம் பார்க்கிறோம் ஒரு முழு மாதிரி அளபுருவை சோடா முடிவு செய்ய வேண்டும் என்று. ஆனால், நாம் தொடர்ந்து வரையில் ஒரு மாதிரியின் வெளிப்பாட்டின் நிலையின் தேவையான கியூயியின் கீழே நீர் பயன்பாடு</abstract_ta>
      <abstract_ur>ماشین کی ترجمہ کی سنت-سطح کی کیفیت کا ارزش (QE) کو سنتی طور پر ریگرس کے کام کے طور پر فرمول کیا جاتا ہے، اور QE نمڈلوں کی عملکرد عمدہ طور پر پررسون کی نسبت انسان لیبل کے ساتھ اندازہ کیا جاتا ہے. اچھی QE موڈلے پہلے لوگوں کے فیصلے کے ساتھ غیر دیکھے سطح کی تعلق پہنچ چکے ہیں، لیکن وہ بہت سی زبان کی متوسط زبان موڈلے پر بھروسہ رکھتے ہیں جو کامپیوتروں سے گران ہیں اور ان کو حقیقی دنیا کی کاربریوں کے لئے ناپسند بناتے ہیں. اس کام میں ہم نے QE کے لئے چند موڈل کمپرس ٹیکنیک کا ارزش کیا ہے اور یہ دیکھتے ہیں کہ ان کے علائم دوسرے NLP کے کاموں میں ان کے علائم بھی ہیں، وہ اس رسرسوائی تنظیمات میں کمزور عملکرد تک پہنچاتے ہیں۔ ہم دیکھتے ہیں کہ ایک پورا موڈل پارامتریزی کی ضرورت ہے سوٹا کا نتیجہ ایک دوسری روش کا کام پہنچانے کے لئے۔ لیکن ہم argue that the level of expressiveness of a model in a continuous range is unnecessary given to the downstream applications of QE, and show that QE reframing as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications.</abstract_ur>
      <abstract_uz>Name Yaqinda QE modellari odamlar bilan bog'liq bo'lgan odamlar darajaga yetishdi, lekin ular hisoblash qiymatida qiymati bo'lgan katta tillar modellariga ishlatadi va ularni halik dunyo dasturlari uchun qo'llanmagan. Bu ishda biz QE uchun bir necha model kompyuterni qiymatimiz va boshqa NLP vazifalarida jamiyatlarni ko'rib chiqarishimiz mumkin va ular bu boshqa narsalarning ko'proq bajarishga ega bo'ladi. We observe that a full model parameterization is required to achieve SoTA results in a regression task.  Lekin biz murakkab qilamiz, davom etilgan modelning darajasi QE dasturlari kerak emas, va QE dasturlarini tasdiqlash muammosi deb ko'rsatish mumkin va klassifik metrikalarni yordamida QE modellarini qiymatish mumkin, ularning haliy dunyo dasturlarida ishlashning jarayonlarini tasavvur qilishi kerak.</abstract_uz>
      <abstract_vi>Bộ đánh giá cấp đánh giá chất lượng (QE) của bộ dịch cỗ máy thường được phát triển như một nhiệm vụ hồi phục, và khả năng ứng dụng của mô hình QE thường được đo bằng cách so sánh giữa Pearson và nhãn con người. Những mô hình QE gần đây đã đạt được mức độ tương quan không ai thấy trước đây với phán quyết con người, nhưng chúng dựa trên các mô hình ngôn ngữ ngữ ngữ ngữ ngữ ngữ ngữ ngữ ngữ ngữ ngữ ngữ ngữ rộng lớn, Tính đắt đỏ và khiến chúng không thể thực tế được. Trong công việc này, chúng tôi đánh giá các kỹ thuật nén mô hình cho Qt và tìm thấy rằng, mặc dù họ nổi tiếng trong các công việc Njala khác, chúng có hiệu quả kém trong môi trường phục hồi. Chúng tôi lưu ý rằng cần thiết một mô phỏng bằng phương pháp đo lường đầy đủ để đạt được thành quả của SothA ở nhiệm vụ phục hồi. Tuy nhiên, chúng tôi cho rằng mức độ biểu lộ của mô hình trong phạm vi liên tục là không cần thiết với các ứng dụng phụ của năng lượng QE, và cho thấy rằng chất QE khúc xạ như một vấn đề phân hạng và đánh giá mô hình QE sử dụng thiết lập cấp độ phân hạng sẽ phù hợp hơn với khả năng thực tại của chúng trong các ứng dụng thế giới thực.</abstract_vi>
      <abstract_bg>Оценката на качеството на машинния превод на ниво изречение традиционно се формулира като регресионна задача, а ефективността на моделите обикновено се измерва чрез корелация на Пиърсън с човешки етикети. Съвременните модели са постигнали невиждани нива на корелация с човешките преценки, но те разчитат на големи многоезични контекстуализирани езикови модели, които са изчислително скъпи и ги правят невъзможни за приложения в реалния свят. В тази работа ние оценяваме няколко техники за компресиране на модела и откриваме, че въпреки популярността им в други задачи, те водят до лоша производителност в тази регресионна настройка. Наблюдаваме, че е необходима пълна параметризация на модела, за да се постигнат резултати в регресионна задача. Въпреки това, ние твърдим, че нивото на изразителност на един модел в непрекъснат диапазон е ненужно предвид приложенията надолу по веригата на КЕ и показват, че преразглеждането на КЕ като проблем за класификация и оценяването на КЕ модели с помощта на класификационни метрици би отразило по-добре действителното им представяне в приложения от реалния свят.</abstract_bg>
      <abstract_hr>Procjenjivanje kvalitete na razini kazne (QE) prevoda stroja tradicionalno se formulira kao zadatak regresije, a učinkovitost modela QE obično se mjera korelacija Pearsona s ljudskim etiketama. Skorašnji QE modeli su postigli ranije nevidljive razine korelacije s ljudskim osuđivanjima, ali se oslanjaju na velike multijezičke kontekstualizirane jezičke modele koje su računalno skupe i čine ih neprijateljima za realne aplikacije. U ovom poslu, procjenjujemo nekoliko modela tehnika kompresije za QE i otkrijemo da, uprkos popularnosti drugih zadataka NLP-a, oni vode do lošeg učinka u ovom regresijskom postavljanju. Primijetimo da je potrebna puna modelna parameterizacija kako bi postigla rezultate SoTA u zadatku regresije. Međutim, tvrdimo da je razina izrazitosti model a u kontinuiranom rasponu nepotrebna s obzirom na snimke primjene QE-a, i pokazujemo da je reframiranje QE kao problem klasifikacije i procjena modela QE-a koristeći klasifikacijske metrike bolje odražavalo njihovu stvarnu funkciju u primjenama stvarnog svijeta.</abstract_hr>
      <abstract_nl>Kwaliteitsschatting (QE) van machinevertaling wordt traditioneel geformuleerd als een regressietaak, en de prestaties van QE-modellen worden meestal gemeten door Pearson-correlatie met menselijke labels. Recente QE-modellen hebben eerder ongekende niveaus van correlatie met menselijke oordelen bereikt, maar ze vertrouwen op grote meertalige contextualiseerde taalmodellen die rekenkundig duur zijn en ze onuitvoerbaar maken voor toepassingen in de echte wereld. In dit werk evalueren we verschillende modelcompressietechnieken voor QE en vinden we dat, ondanks hun populariteit in andere NLP-taken, ze leiden tot slechte prestaties in deze regressie-instelling. We zien dat een volledige modelparametrisering vereist is om SotA resultaten te bereiken in een regressietaak. We stellen echter voor dat het niveau van expressiviteit van een model in een continue waaier overbodig is gezien de downstream toepassingen van QE, en tonen aan dat het herschikken van QE als een classificatieprobleem en het evalueren van QE modellen met behulp van classificatiemetrics hun werkelijke prestaties in real-world toepassingen beter zouden weerspiegelen.</abstract_nl>
      <abstract_de>Die Qualitätsschätzung (QE) der maschinellen Übersetzung wird traditionell als Regressionsaufgabe formuliert, und die Leistung von QE-Modellen wird typischerweise durch Pearson-Korrelation mit Human Labels gemessen. Neuere QE-Modelle haben bisher unbekannte Korrelationen mit menschlichen Urteilen erreicht, aber sie stützen sich auf große mehrsprachige kontextualisierte Sprachmodelle, die rechenteuer sind und sie für reale Anwendungen unmöglich machen. In dieser Arbeit evaluieren wir mehrere Modellkompressionstechniken für QE und stellen fest, dass sie trotz ihrer Popularität in anderen NLP-Aufgaben zu einer schlechten Leistung in dieser Regressionseinstellung führen. Wir beobachten, dass eine vollständige Modellparametrisierung erforderlich ist, um SoTA-Ergebnisse in einer Regressionsaufgabe zu erzielen. Wir argumentieren jedoch, dass die Expressivität eines Modells in einem kontinuierlichen Bereich angesichts der nachgelagerten Anwendungen von QE überflüssig ist, und zeigen, dass die Umrahmung von QE als Klassifizierungsproblem und die Bewertung von QE-Modellen mit Klassifizierungsmetriken ihre tatsächliche Leistung in realen Anwendungen besser widerspiegeln würde.</abstract_de>
      <abstract_ko>기계 번역의 문장급 품질 평가(QE)는 전통적으로 회귀 임무로 묘사되고 QE모델의 성능은 보통 인간 라벨의 필슨과 관련성을 통해 평가된다.최근의 양적완화 모델은 이전에 볼 수 없었던 인간의 판단과 관련된 정도에 이르렀지만 대형 다국어 어경화 언어 모델에 의존하기 때문에 이런 모델의 계산 원가가 매우 높아 실제 응용에 적용되지 않는다.이 작업에서 우리는QE에 사용되는 몇 가지 모델 압축 기술을 평가했고 다른 NLP 작업에서 인기가 많지만 이러한 회귀 설정에서 비교적 나쁜 성능을 초래할 수 있음을 발견했다.회귀 임무에서 SoTA 결과를 실현하려면 완전한 모델 매개 변수화가 필요하다는 것을 관찰했다.그러나 우리는 양적완화의 하류 응용을 감안하면 모델이 연속적인 범위 내에서 표현 수준이 불필요하다고 생각한다. 또한 양적완화를 하나의 분류 문제로 다시 정의하고 분류도량으로 양적완화모델을 평가하면 실제 응용에서의 실제 성능을 더욱 잘 반영할 수 있다고 본다.</abstract_ko>
      <abstract_sw>Takwimu ya kiwango cha kiwango cha hukumu (QE) ya tafsiri ya mashine imetengenezwa kwa kawaida kama kazi ya kudhibiti, na utendaji wa miundo mbinu za QE kwa kawaida unapimiwa na Pearson kuhusiana na mabango ya binadamu. Mradi wa hivi karibuni wa QE umepata kiwango ambacho hakifikiri kilichopita cha kuhusiana na maamuzi ya binadamu, lakini wanategemea mifano makubwa ya lugha za lugha ambazo ni ghali kwa hisabati na kuwafanya hivyo kuwa na madhara kwa matumizi halisi ya dunia. Katika kazi hii, tunatathmini mbinu kadhaa za kompyuta za kisasa kwa ajili ya QE na tunagundua kwamba, pamoja na umaarufu wao katika kazi nyingine za NLP, wanapelekea utendaji mdogo katika mazingira haya ya ukandamizaji. Tunaona kuwa upasuaji wa mifano kamili unahitajika kupata matokeo ya SoTA katika kazi ya upinzani. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications.</abstract_sw>
      <abstract_fa>ارزیابی کیفیت سخنرانی (QE) ترجمه دستگاه به طور سنتی به عنوان یک کار بازگشت فرمول می‌شود و عملکرد مدل QE معمولاً توسط ارتباط پیرسون با نقاشی انسان اندازه می‌شود. مدلهای QE اخیراً سطح متقابل قابل قابل قابل قابلیت انسانی پیش از این به دست آورده اند، ولی آنها بر مدلهای زیادی متقابل زبانی متقابل اعتماد دارند که با محاسبات گرون هستند و آنها را برای کاربردهای دنیای واقعی ناتوان سازند. در این کار، ما چندین تکنیک فشار فشار فشار مدل برای QE ارزیابی می‌کنیم و می‌بینیم که، با وجود شهرت آنها در کار های دیگر NLP، آنها به عملکرد بدی در این تنظیم افشاری رخ می‌دهند. ما مشاهده می‌کنیم که یک پارامتریزی مدل کامل برای رسیدن نتیجه‌های SoTA در یک کار تجاوز نیاز دارد. با این حال، ما بحث می‌کنیم که سطح استفاده از یک مدل در یک منطقه دائمی لازم نیست به عنوان کاربردهای پایین QE، و نشان می‌دهیم که بازسازی QE به عنوان یک مشکل شناسایی و ارزیابی مدل QE با استفاده از متریک‌های شناسایی بهتر انجام واقعیشان را در کاربردهای دنیای واقعی نشان می‌دهد.</abstract_fa>
      <abstract_tr>Maşynyň terjimesiniň derejesi Quality Taýýarlama (QE) Ýakynda QE nusgalary öňünden görä görä görünmeden adamlaryň judgamlary bilen ylalaşyk derejesine ýetip bardylar, ýöne olar kalkularyň has baglany ýagdaýda örän uly dil nusgalaryna ynanýarlar we olary hakyky dünýäde uygulamalar üçin iň ýaramaz bolýarlar. Bu işde biz QE üçin birnäçe nusga hasaplanjak tekniklerini deňleýäris we bular, başga NLP işlerinde tanyşyklarynyň ýöne, ol regressiýa düzeninde erbet etmäge sebep edýärler. Biz SoTA netijesini regresiýa buýrukynda ýetmek üçin tam nusga parameteriýasynyň gereklidigini gözləýäris. Ýöne, biz düzgün bir görnüşde bir nusgyň özüniň görnüşiniň derejesi QE'iň a şaky uygulamalaryna gerek däldir we QE nusgasyny klasifikasyýa meselesi hökmünde çykarmak we klasifikasyýa metriklerden ullanýan nusgasyny gowurak dünýä uygulamalarynda gözleýändigini görkeýäris.</abstract_tr>
      <abstract_da>Kvalitetsestimering (QE) af maskinoversættelse er traditionelt formuleret som en regressionsopgave, og ydeevnen af QE modeller måles typisk ved Pearson korrelation med menneskelige etiketter. Seneste QE-modeller har opnået hidtil usete niveauer af korrelation med menneskelige vurderinger, men de er afhængige af store flersprogede kontekstualiserede sprogmodeller, der er beregningsmæssigt dyre og gør dem umulige for virkelige applikationer. I dette arbejde evaluerer vi flere modelkomprimeringsteknikker for QE og finder ud af, at de trods deres popularitet i andre NLP-opgaver fører til dårlig ydeevne i denne regressionsindstilling. Vi observerer, at en fuld model parametrisering er nødvendig for at opnå SotA resultater i en regressionsopgave. Vi hævder dog, at niveauet af udtryksevne af en model i et kontinuerligt interval er unødvendigt i betragtning af downstream applikationer af QE, og viser, at omdannelse af QE som et klassifikationsproblem og evaluering af QE modeller ved hjælp af klassifikationsmåler bedre ville afspejle deres faktiske ydeevne i virkeligheden applikationer.</abstract_da>
      <abstract_sq>Vlerësimi i kualitetit në nivelin e gjykimeve (QE) i përkthimit të makinës është formuluar tradicionalisht si një detyrë regresive dhe performanca e modeleve QE vlerësohet tipikisht nga korrelacioni i Pearsonit me etiketat njerëzore. Modelet e fundit të QE kanë arritur nivele korrelacioni të padukshme më parë me gjykimet njerëzore, por ato mbështeten në modele të mëdha gjuhësh të kontekstualizuara shumëgjuhësore që janë llogarisht të shtrenjta dhe i bëjnë të papërshtatshëm për aplikimet e botës reale. Në këtë punë, ne vlerësojmë disa teknika të kompresimit të modelit për QE dhe zbulojmë se pavarësisht nga popullariteti i tyre në detyra të tjera NLP, ato shpien në performancë të dobët në këtë ambient regresioni. Ne vëzhgojmë se një parametrizim i plotë i modelit është i nevojshëm për të arritur rezultatet e SoTA në një detyrë regresive. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications.</abstract_sq>
      <abstract_id>Perkiraan kualitas tingkat kalimat (QE) dari terjemahan mesin secara tradisional ditentukan sebagai tugas regresi, dan prestasi model QE biasanya diukur oleh korelasi Pearson dengan label manusia. Model QE baru-baru ini telah mencapai tingkat korelasi yang belum pernah terlihat dengan penilaian manusia, tetapi mereka bergantung pada model bahasa multibahasa kontekstualisasi besar yang mahal secara komputasi dan membuat mereka tidak dapat disembunyikan untuk aplikasi dunia nyata. Dalam pekerjaan ini, kami mengevaluasi beberapa teknik kompresi model untuk QE dan menemukan bahwa, meskipun popularitas mereka dalam tugas lainnya NLP, mereka menyebabkan prestasi buruk dalam setting regresi ini. Kami memperhatikan bahwa parameterisasi model lengkap diperlukan untuk mencapai hasil SoTA dalam tugas regresi. Namun, kami menyangka bahwa tingkat ekspresivitas model dalam jangkauan terus menerus tidak diperlukan karena aplikasi turun dari QE, dan menunjukkan bahwa reframing QE sebagai masalah klasifikasi dan mengevaluasi model QE menggunakan metrik klasifikasi akan lebih baik merefleksikan prestasi mereka sebenarnya dalam aplikasi dunia nyata.</abstract_id>
      <abstract_am>የሥርዓት ደረጃ ቁጥር (QE) የmachine ትርጓሜ በተለየ ወቅት አካባቢ ትርጉም ማድረግ ነው፣ የQE ዓይነቶች አካባቢ በተለየ በፌርሶን ከሰው ማኅበረሰብ ጋር ታካክል ነው፡፡ የቀድሞው የQE ምሳሌዎች ከሰው ፍርድ ጋር ግንኙነት ያልታወቀ ደረጃዎች አግኝተዋል፤ ነገር ግን በቋንቋ ቋንቋዎች ላይ በብዛት የከበረ እና ለእውነተኛ ዓለም ፕሮግራሞች የሚታሰቡ ብዙዎችን ምሳሌዎች ይታመካሉ፡፡ በዚህ ሥራ፣ የQE የሞዴል አካባቢ ስኮቶችን እናሳውቃለን፣ እናም ብዙዎቹ የNLP ስራቶች ምንም እንኳ ቢሆን በዚህ አካባቢ ስርዓት ውስጥ ድህነትን ለማድረግ ይሄዳሉ፡፡ ሶቲ አካሄዱን ለመግኘት የሙሉ ሞዴል ምርጫዎች እንዲያስፈልጋል እናየዋለን፡፡ ምንም እንኳን፣ የሞዴል መልዕክት በዘወትር ቁጥር የQE ፕሮግራሙን ያስፈልጋል፡፡ እናም የQE መግለጫ ጉዳይ እንዲሆን እና የQE ዓይነቶችን በመግለጫው ማተሚያዎች በመጠቀም በአካባቢው አካሄዳቸውን በአዳራዊ ዓለም ፕሮግራሞች ላይ ማረጋገጥ ይሻላል፡፡</abstract_am>
      <abstract_az>Makinatın çevirilməsinin sözlər-seviyyəti qiyməti (QE) sadəcə olaraq regressiya işləri olaraq formüllənir və QE modellərin performansı insan etiketləri ilə Pearson ilə ölçülür. QE modelləri əvvəlcə görmədiyimiz insan hökmləri ilə bağlantılıq seviyelərini başa düşdülər, amma onlar çox dilli müxtəlif dil modellərinə təvəkkül edirlər ki, hesaplayaraq çox qiymətli və həqiqət dünya uyğulamalarına görə onları çox çətinlikli dəlillərə bağlı edirlər. Bu işdə QE üçün bir neçə modeli sıkıştırma tekniklərini değerləşdiririk və digər NLP işlərində məşhurluqlarına baxmayaraq, onlar bu regresiya təyin etməsində zəif performansına yol açarlar. Biz görürük ki, SoTA sonuçlarını geri çəkmək məqsədilə tamamlamaq üçün tam modeli parametrizaq lazımdır. Ancaq biz müəyyən edirik ki, QE'nin düşük uygulamalarına görə model in in ifadəsi səviyyəsi həmişəlik olmaz və QE'ni klasifikasyon problemi olaraq reframiya etmək və QE modellərini klasifikasyon metriklərini istifadə etmək üçün QE modellərinin əsl performansını gerçek dünya proqramlarında daha yaxşı reframiya edər.</abstract_az>
      <abstract_bn>মেশিন অনুবাদের শাস্তি স্তরের মান নির্ধারণ (কিউই) ঐতিহ্যবাহীতিকভাবে শাস্তি নিয়ন্ত্রণের কাজ হিসেবে গঠন করা হয় এবং কিউই মডেলের প্রাপ্ত কার্যকলাপ সাম্প্রতিক কিউই মডেল মানুষের বিচারের সাথে মানুষের অদৃশ্য পর্যায়ের সম্পর্ক অর্জন করেছে, কিন্তু তারা বিশাল মাল্টিভাষার প্রতিযোগিতার ভাষার মডেলের উপর ন এই কাজে আমরা কিউই এর জন্য বেশ কয়েকটি মডেল প্রাপ্ত কৌশলের মূল্য দিচ্ছি এবং অন্যান্য এনএলপি কাজের জনপ্রিয় সত্ত্বেও তারা এই নিয়ন্ত্রণের ব্যবস্ আমরা দেখতে পাচ্ছি যে সোটিএ পুরোপুরি মডেলের প্যারামিটারেশনের ফলাফল পাওয়ার জন্য প্রয়োজন। তবে আমরা যুক্তি দিচ্ছি যে কিউ ইউ-এর নীচের প্রয়োজনীয় প্রয়োজনীয় প্রয়োজনীয় একটি মডেলের প্রকৃতির স্তর এবং কিউই-কে ক্লাসাফিকেশনের সমস্যা হিসেবে পুনরায় বিচ্ছিন্ন করা এবং কিউ-ই মডেল ব্যবহার কর</abstract_bn>
      <abstract_af>Sentence-vlak Kwaliteit estimatie (QE) van masjien vertaling is tradisioneel formeer as 'n regresie taak, en die prestasie van QE-modelle is tipies gemeet deur Pearson korrelasie met menslike etikette. Onlangse QE-modelles het voorheen-ongesien vlakke van korrelasie met menslike oordelinge bereik, maar hulle vertrou op groot multitaalske contextualiseerde taal modelles wat rekenaasjoneel koste is en maak hulle onbeskikbaar vir reël-wêreld toepassings. In hierdie werk, ons evalueer veelvuldige model kompressie teknike vir QE en vind dat, alhoewel hulle populariteit in ander NLP-opdragte, hulle lei na arme prestasie in hierdie regresie opstelling. Ons observeer dat 'n volle model parameterisasie benodig is om Sota resultate te bereik in 'n regresie taak. Maar ons argumenteer dat die vlak van uitdrukking van 'n model in 'n voortdurende omvang onnoodsaaklik is gegee het die onderstreem toepassings van QE, en wys dat die opfrigging van QE as 'n klassifikasie probleem en die evaluering van QE modele gebruik klassifikasie metrike beter sal reflekteer hul werklike uitdrukking in regte wêreld toepassings.</abstract_af>
      <abstract_bs>Očekivanje kvalitete kazne na razini (QE) prevoda mašine tradicionalno se formira kao zadatak regresije, a učinkovitost modela QE obično se mjera korelacija Pearsona sa ljudskim etiketama. Nedavni QE modeli su postigli previše nevidljive nivoe korelacije sa ljudskim osuđivanjima, ali se oslanjaju na velike multijezičke kontekstualizirane jezičke modele koje su računalno skupe i čine ih nepodnošljivim za realne aplikacije. U ovom poslu, procjenjujemo nekoliko modela tehnika kompresije za QE i otkrijemo da, uprkos njihovoj popularnosti u drugim zadacima NLP-a, oni vode do lošeg izvođenja u ovom regresiji. Primijetimo da je potrebna puna modelna parameterizacija za ostvarivanje SoTA rezultata u zadatku regresije. Međutim, tvrdimo da je nivo izrazitosti model a u kontinuiranom rasponu nepotrebno s obzirom na sledeće aplikacije QE-a, i pokazujemo da reframiranje QE kao problem klasifikacije i procjena modela QE-a koristeći klasifikacijske metrike bolje odražava njihovu stvarnu funkciju u aplikaciji realnog svijeta.</abstract_bs>
      <abstract_hy>Մեքենայի թարգմանման արտահայտության մակարդակի որակի գնահատումը (QE) ավանդական կերպով ձևավորվում է որպես ռեգրեսիոն խնդիր, և QE մոդելների արտադրողությունը սովորաբար չափում է Փարսոնի հարաբերակցության միջոցով մարդկային պիտակների հետ: Վերջին QE մոդելները հասել են մարդկային դատողությունների հետ նախկինում անտեսանելի հաղորդակցման մակարդակին, բայց նրանք հիմնված են մեծ բազմալեզու կոնտեքստիալ լեզվի մոդելների վրա, որոնք հաշվարկների առումով թանկ են և դարձնում դրանք անհասանելի իրական աշխարհի ծրագրերի Այս աշխատանքի ընթացքում մենք գնահատում ենք QE-ի համար բազմաթիվ մոդելներ ընդգծելու մեթոդներ և հայտնաբերում ենք, որ չնայած նրանց հայտնի լինելուն այլ ՆԼՊ-ի խնդիրներում, դրանք հանգեցնում են վատ արտադրողությունների այս ռեգրեսիայի Մենք նկատում ենք, որ անհրաժեշտ է ամբողջ մոդելի պարամետրիզացիա, որպեսզի հասնենք ՍոԹԱ-ի արդյունքները վերադառնալու խնդրի մեջ: Այնուամենայնիվ, մենք փաստարկում ենք, որ շարունակական տարածքում մոդելի արտահայտության մակարդակը անհրաժեշտ է հաշվի առնելով QE-ի հետագա ծրագրերը, և ցույց են տալիս, որ QE-ի վերափոխումը որպես դասակարգման խնդիր և QE-ի մոդելների գնահատումը օգտագործելով դասակարգման մետրիկներ ավելի լա</abstract_hy>
      <abstract_ca>L'estimació de qualitat a nivell de sentits (QE) de la traducció màquina és tradicionalment formulada com una tasca de regressió, i el rendiment de models QE és normalment mesurat per la correlació de Pearson amb etiquetes humanes. Els models QE recents han aconseguit nivells de correlació previament invisibles amb els judicis humans, però confien en grans models de llenguatge contextualitzats multilingües que són costosos computacionalment i que els fan infeasibles per aplicacions del món real. En aquest treball, evaluem diverses tècniques de compressió de models per a QE i descobrim que, malgrat la seva popularitat en altres tasques de NLP, condueixen a un mal rendiment en aquest entorn de regressió. Observem que es requereix una paràmetrització completa del model per aconseguir els resultats de la SoTA en una tasca de regressió. Tot i així, argumentem que el nivell d'expressivitat d'un model en una gama continua és innecessari tenint en compte les aplicacions avall de QE, i demostram que reformar QE com un problem a de classificació i evaluar models QE utilitzant mètriques de classificació reflexionaria millor el seu rendiment real en aplicacions del món real.</abstract_ca>
      <abstract_fi>Konekäännöksen lausetason laadunarviointi (QE) on perinteisesti muotoiltu regressiotehtäväksi, ja QE-mallien suorituskykyä mitataan tyypillisesti Pearsonin korrelaatiolla inhimillisten etikettien kanssa. Viimeaikaiset laadunvarmistusmallit ovat saavuttaneet ennennäkemättömät korrelaatiotasot ihmisten arviointien kanssa, mutta ne perustuvat suuriin monikielisiin kontekstualisoituihin kielimalleihin, jotka ovat laskennallisesti kalliita ja tekevät niistä mahdottomia toteuttaa todellisia sovelluksia. Tässä työssä arvioimme useita mallikompressiotekniikoita QE:lle ja havaitsimme, että huolimatta niiden suosiosta muissa NLP-tehtävissä, ne johtavat huonoon suorituskykyyn tässä regressioasetuksessa. Havaitsemme, että koko mallin parametrisointi on tarpeen SoTA-tulosten saavuttamiseksi regressiotehtävässä. Väitämme kuitenkin, että jatkuvalla alueella olevan mallin ekspressiivisyyden taso on tarpeeton ottaen huomioon QE:n loppupään sovellukset, ja osoitamme, että QE:n uudelleenjäsentäminen luokitusongelmaksi ja QE:n mallien arviointi luokitusmittareilla heijastaisi paremmin niiden todellista suorituskykyä reaalimaailman sovelluksissa.</abstract_fi>
      <abstract_et>Masintõlke lausetaseme kvaliteedi hindamine (QE) on traditsiooniliselt sõnastatud regressiooniülesandena ja QE mudelite jõudlust mõõdetakse tavaliselt Pearsoni korrelatsiooni abil inimmärgistega. Hiljutised QE mudelid on saavutanud varem nähtamatu korrelatsiooni inimotsustega, kuid nad tuginevad suurtele mitmekeelsetele kontekstualiseeritud keelemudelitele, mis on arvutuslikult kallid ja muudavad need reaalsete rakenduste jaoks teostamatuks. Käesolevas töös hindame mitmeid mudeli kompressioonimeetodeid QE jaoks ja leiame, et vaatamata nende populaarsusele teistes NLP ülesannetes põhjustavad need halva jõudluse selles regressiooniseadises. Märgime, et SoTA tulemuste saavutamiseks regressiooniülesandes on vaja täielikku mudeli parametriseerimist. Siiski väidame, et pidevas vahemikus oleva mudeli väljendusvõime tase ei ole vajalik, arvestades QE järgnevaid rakendusi, ning näitame, et QE ümberkujundamine klassifitseerimisprobleemina ja QE mudelite hindamine klassifitseerimismõõdikute abil peegeldaks paremini nende tegelikku jõudlust reaalmaailma rakendustes.</abstract_et>
      <abstract_cs>Odhad kvality strojového překladu je tradičně formulován jako regresní úloha a výkon QE modelů je obvykle měřen Pearsonovou korelací s lidskými značkami. Nedávné QE modely dosáhly dosud neviděné úrovně korelace s lidskými úsudky, ale spoléhají na velké mnohojazyčné kontextualizované jazykové modely, které jsou výpočetně nákladné a činí je nemožnými pro reálné aplikace. V této práci hodnotíme několik modelových kompresních technik pro QE a zjišťujeme, že navzdory jejich popularitě v jiných NLP úlohách vedou k špatnému výkonu v tomto regresním nastavení. Pozorujeme, že k dosažení výsledků SOTA v regresním úkolu je nutná úplná parametrizace modelu. Nicméně argumentujeme, že úroveň expresivity modelu v kontinuálním rozsahu není zbytečná vzhledem k následným aplikacím QE, a ukazujeme, že reframování QE jako klasifikační problém a hodnocení QE modelů pomocí klasifikačních metrik by lépe odráželo jejich skutečnou výkonnost v reálných aplikacích.</abstract_cs>
      <abstract_jv>Ukuran model de R&amp;E dumateng sing perusahaan kelangan mat sampeyan karo perusahaan hukum dumateng, nguasai supoyo kuwi model multi-lenguangkap multi-lenguangkap model sing kelangan karo akeh komputasi akeh akeh dumateng, lan akeh dumateng sumulapakan kanggo aplikasi versi dumateng. Nang iki trabah, kita deweke nggawe sistem sing perusahaan kanggo ngilanggar-sistem model sing perusahaan kanggo Kemerdekaan kanggo ngerasakno. bah ngono popularno sing ngendalikno NLP sing wis ngerasakno, padha iso nggawe barang ngendalikno Regresyon iki. Monday Nanging, awak dhéwé nggalaksi banjuré kesempresan pangan ning model sing bisa dumadhi iki, nik awak dhéwé aplikasi downtream ning kE kuwi nggawe barang kelas nêmên iki bakal terus nggawe sistem sing kelas nêmên karo akeh model sing titimpen Nêmên iki dadi sing bisa nggawe barang kelas nêmên.</abstract_jv>
      <abstract_sk>Ocena kakovosti (QE) strojnega prevajanja na ravni stavka je tradicionalno oblikovana kot regresijska naloga, učinkovitost modelov QE pa se običajno meri s Pearsonovo korelacijo s človeškimi oznakami. Nedavni modeli QE so dosegli prej nevidne stopnje korelacije s človeškimi presojami, vendar se zanašajo na velike večjezične kontekstualizirane jezikovne modele, ki so računalniško dragi in jih naredijo neizvedljivi za aplikacije v realnem svetu. V tem delu smo ocenili več tehnik kompresije modelov za QE in ugotovili, da kljub njihovi priljubljenosti pri drugih nalogah NLP vodijo do slabe zmogljivosti v tej regresijski nastavitvi. Opazujemo, da je za dosego rezultatov SoTA v regresijski nalogi potrebna popolna parametrizacija modela. Vendar pa trdimo, da raven izraznosti modela v neprekinjenem obsegu ni potrebna glede na nadaljnje aplikacije QE, in pokažemo, da bi preoblikovanje QE kot problem razvrščanja in vrednotenje modelov QE z uporabo klasifikacijskih metrik bolje odražalo njihovo dejansko uspešnost v aplikacijah realnega sveta.</abstract_sk>
      <abstract_ha>Ana ƙayyade Tsarin-daraja na fassarar kwamfyutan (QU) na fassarar maɓalli an danne shi a ƙidãya kamar wani aikin regression, kuma gyaran misalin QEyi ana ƙaddara shi a bayan a ƙayyade Pearson da aka yi danganta da alama na mutum. A yanzu masu motsi na QU na sami da daraja wanda ba'a sani ba ta gabãni, sunã mãsu husũma da hukuncin mutãne, kuma sunã dõgara a kan misãlai masu yawa na harshen mulki-mulki waɗanda ke ƙidãya masu nau'i da lissafi kuma sunã sanya su an yi musamman da shiryoyin ayuka masu shiryuwa na dũniya. Daga wannan aikin, Munã ƙaddara misãlai masu ƙaranci wa QET kuma munã gane, kuma, kõ da yaushe umakinsu na cikin aikin NLP, sai su yi matalauci a cikin wannan tsarin damu. Muna ganin cewa an buƙata tsari cikakken misãlai dõmin ya sãmu matsalar SoTA cikin aikin da za'a rajista. Amma, Munã jãyayya cẽwa, gwargwadon maganar wata misalin da ke cikin hanyarsa mai daidai ba na da amfani ba a ga shirin ayuka na QEki na ƙarƙashin kwamfyuta, kuma za mu nuna cewa tsarin QEki kamar wata fitina na na fassarar kuma ana canza misãlai na QU da amfani da metrici mai fassarawa, zai fi canza mafarinsa a cikin shiryoyin-duniya.</abstract_ha>
      <abstract_he>הערכת איכות רמת המשפטים (QE) של התרגום המכונית מתייצבת במסורתית כמשימה חזרה, והביצועים של דוגמנים QE מתמדדים בדרך כלל על ידי הקשר של פירסון עם תוויות אנושיות. דוגמני QE האחרונים השיגו רמות של שיתוף בלתי נראות קודם עם שיפוטים אנושיים, אך הם סומכים על דוגמני שפת רבות-שפותיים גדולים שקשורים במחשבות יקרים והופכים אותם בלתי אפשריים לתוכניות בעולם האמיתי. בעבודה הזו, אנו מעריכים כמה טכניקות דחיסה של QE ומצאים שלמרות הפופולריות שלהם במשימות NLP אחרות, הן מובילות לביצוע גרוע בסביבה זו של גידול. We observe that a full model parameterization is required to achieve SoTA results in a regression task.  בכל אופן, אנחנו טוענים שרמה של ביטוי של מודל במטווח ממשיך אינה הכרחית בהתחשב באימונים למטה של QE, ולהראות שהשינוי QE בתור בעיה מסווג והעריכה של מודלים QE באמצעות מטריות מסווג עדיף להשקף את היציאה האמיתית שלהם באימונים בעולם האמיתי.</abstract_he>
      <abstract_bo>Sentence-level Quality estimation (QE) of machine translation is traditionally formulated as a regression task, and the performance of QE models is typically measured by Pearson correlation with human labels. It is common in modern times. འཕྲལ་མྱུར་བའི་QE མིག་དཔེ་གཞུང་གིས་མེད་པའི་སྔོན་མེད་པའི་མཐུན་རིམ In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a regression task. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications.</abstract_bo>
      </paper>
    <paper id="477">
      <title>Rule-based Morphological Inflection Improves Neural Terminology Translation</title>
      <author><first>Weijia</first><last>Xu</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>5902–5914</pages>
      <abstract>Current approaches to incorporating terminology constraints in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation (MT)</a> typically assume that the constraint terms are provided in their correct <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological forms</a>. This limits their application to real-world scenarios where <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraint terms</a> are provided as lemmas. In this paper, we introduce a modular framework for incorporating <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">lemma constraints</a> in neural MT (NMT) in which linguistic knowledge and diverse types of NMT models can be flexibly applied. It is based on a novel cross-lingual inflection module that inflects the target lemma constraints based on the source context. We explore linguistically motivated rule-based and data-driven neural-based inflection modules and design English-German health and English-Lithuanian news test suites to evaluate them in domain adaptation and low-resource MT settings. Results show that our rule-based inflection module helps NMT models incorporate <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">lemma constraints</a> more accurately than a neural module and outperforms the existing end-to-end approach with lower training costs.</abstract>
      <url hash="e31fa80c">2021.emnlp-main.477</url>
      <bibkey>xu-carpuat-2021-rule</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.477</doi>
      <pwccode url="https://github.com/izecson/terminology-translation" additional="false">izecson/terminology-translation</pwccode>
    </paper>
    <paper id="479">
      <title>Good-Enough Example Extrapolation</title>
      <author><first>Jason</first><last>Wei</last></author>
      <pages>5923–5929</pages>
      <abstract>This paper asks whether extrapolating the hidden space distribution of text examples from one class onto another is a valid <a href="https://en.wikipedia.org/wiki/Inductive_bias">inductive bias</a> for <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>. To operationalize this question, I propose a simple data augmentation protocol called good-enough example extrapolation (GE3). GE3 is lightweight and has no hyperparameters. Applied to three text classification datasets for various data imbalance scenarios, GE3 improves performance more than <a href="https://en.wikipedia.org/wiki/Upsampling">upsampling</a> and other hidden-space data augmentation methods.</abstract>
      <url hash="42d4d4b6">2021.emnlp-main.479</url>
      <attachment type="Software" hash="31382f34">2021.emnlp-main.479.Software.zip</attachment>
      <bibkey>wei-2021-good</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.479</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="489">
      <title>A Scalable Framework for Learning From Implicit User Feedback to Improve <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">Natural Language Understanding</a> in Large-Scale Conversational AI Systems<fixed-case>AI</fixed-case> Systems</title>
      <author><first>Sunghyun</first><last>Park</last></author>
      <author><first>Han</first><last>Li</last></author>
      <author><first>Ameen</first><last>Patel</last></author>
      <author><first>Sidharth</first><last>Mudgal</last></author>
      <author><first>Sungjin</first><last>Lee</last></author>
      <author><first>Young-Bum</first><last>Kim</last></author>
      <author><first>Spyros</first><last>Matsoukas</last></author>
      <author><first>Ruhi</first><last>Sarikaya</last></author>
      <pages>6054–6063</pages>
      <abstract>Natural Language Understanding (NLU) is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> and improving NLU for a large-scale production system across 10 domains.</abstract>
      <url hash="66d00b1e">2021.emnlp-main.489</url>
      <bibkey>park-etal-2021-scalable</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.489</doi>
    </paper>
    <paper id="495">
      <title>Single-dataset Experts for Multi-dataset Question Answering</title>
      <author><first>Dan</first><last>Friedman</last></author>
      <author><first>Ben</first><last>Dodge</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>6128–6137</pages>
      <abstract>Many <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> have been created for training reading comprehension models, and a natural question is whether we can combine them to build <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> that (1) perform better on all of the training datasets and (2) generalize and transfer better to new datasets. Prior work has addressed this goal by training one network simultaneously on multiple datasets, which works well on average but is prone to over- or under-fitting different sub- distributions and might transfer worse compared to source models with more overlap with the target dataset. Our approach is to model multi-dataset question answering with an ensemble of single-dataset experts, by training a collection of lightweight, dataset-specific adapter modules (Houlsby et al., 2019) that share an underlying Transformer model. We find that these Multi-Adapter Dataset Experts (MADE) outperform all our baselines in terms of in-distribution accuracy, and simple methods based on parameter-averaging lead to better zero-shot generalization and few-shot transfer performance, offering a strong and versatile starting point for building new reading comprehension systems.</abstract>
      <url hash="dc747808">2021.emnlp-main.495</url>
      <bibkey>friedman-etal-2021-single</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.495</doi>
      <pwccode url="https://github.com/princeton-nlp/made" additional="false">princeton-nlp/made</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/duorc">DuoRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="496">
      <title>Simple Entity-Centric Questions Challenge Dense Retrievers</title>
      <author><first>Christopher</first><last>Sciavolino</last></author>
      <author><first>Zexuan</first><last>Zhong</last></author>
      <author><first>Jinhyuk</first><last>Lee</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>6138–6148</pages>
      <abstract>Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed <a href="https://en.wikipedia.org/wiki/Sparse_matrix">sparse models</a> using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of <a href="https://en.wikipedia.org/wiki/Information_retrieval">retrieval</a>. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a> (e.g., Where was Arve Furset born?), and observe that dense retrievers drastically under-perform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions.</abstract>
      <url hash="3a787a5a">2021.emnlp-main.496</url>
      <bibkey>sciavolino-etal-2021-simple</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.496</doi>
      <pwccode url="https://github.com/princeton-nlp/entityquestions" additional="false">princeton-nlp/entityquestions</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/entityquestions">EntityQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paq">PAQ</pwcdataset>
    </paper>
    <paper id="497">
      <title>Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization</title>
      <author><first>Ansong</first><last>Ni</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Pradeep</first><last>Dasigi</last></author>
      <pages>6149–6161</pages>
      <abstract>Question Answering (QA) tasks requiring information from multiple documents often rely on a <a href="https://en.wikipedia.org/wiki/Information_retrieval">retrieval model</a> to identify relevant information for <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a>. The retrieval model is typically trained to maximize the likelihood of the labeled supporting evidence. However, when retrieving from large text corpora such as <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, the correct answer can often be obtained from multiple evidence candidates. Moreover, not all such candidates are labeled as positive during <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a>, rendering the training signal weak and noisy. This problem is exacerbated when the questions are unanswerable or when the answers are Boolean, since the model can not rely on lexical overlap to make a connection between the answer and supporting evidence. We develop a new parameterization of set-valued retrieval that handles unanswerable queries, and we show that marginalizing over this set during training allows a model to mitigate false negatives in supporting evidence annotations. We test our method on two multi-document QA datasets, <a href="https://en.wikipedia.org/wiki/IIRC">IIRC</a> and HotpotQA. On IIRC, we show that joint modeling with marginalization improves <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance by 5.5 F1 points and achieves a new state-of-the-art performance of 50.5 F1. We also show that retrieval marginalization results in 4.1 QA F1 improvement over a non-marginalized baseline on HotpotQA in the fullwiki setting.</abstract>
      <url hash="c6f18453">2021.emnlp-main.497</url>
      <bibkey>ni-etal-2021-mitigating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.497</doi>
      <pwccode url="https://github.com/niansong1996/retrieval_marginalization" additional="false">niansong1996/retrieval_marginalization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iirc">IIRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="500">
      <title>BiSECT : Learning to Split and Rephrase Sentences with Bitexts<fixed-case>B</fixed-case>i<fixed-case>SECT</fixed-case>: Learning to Split and Rephrase Sentences with Bitexts</title>
      <author><first>Joongwon</first><last>Kim</last></author>
      <author><first>Mounica</first><last>Maddela</last></author>
      <author><first>Reno</first><last>Kriz</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>6193–6209</pages>
      <abstract>An important task in NLP applications such as <a href="https://en.wikipedia.org/wiki/Sentence_simplification">sentence simplification</a> is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and a new <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for this ‘split and rephrase’ task. Our BiSECT training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> to convert both sides of the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> into the same language. BiSECT contains higher quality training examples than the previous Split and Rephrase corpora, with sentence splits that require more significant modifications. We categorize examples in our <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> and use these categories in a novel model that allows us to target specific regions of the input sentence to be split and edited. Moreover, we show that models trained on BiSECT can perform a wider variety of split operations and improve upon previous state-of-the-art approaches in automatic and human evaluations.</abstract>
      <url hash="6affb1a8">2021.emnlp-main.500</url>
      <bibkey>kim-etal-2021-bisect</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.500</doi>
      <pwccode url="https://github.com/mounicam/bisect" additional="false">mounicam/bisect</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bisect">BiSECT</pwcdataset>
    </paper>
    <paper id="502">
      <title>Universal Sentence Representation Learning with Conditional Masked Language Model</title>
      <author><first>Ziyi</first><last>Yang</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Daniel</first><last>Cer</last></author>
      <author><first>Jax</first><last>Law</last></author>
      <author><first>Eric</first><last>Darve</last></author>
      <pages>6216–6228</pages>
      <abstract>This paper presents a novel training method, Conditional Masked Language Modeling (CMLM), to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval, even outperforming models learned using supervised signals. As a fully <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning method</a>, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval (BR) and natural language inference (NLI) tasks outperforms the previous state-of-the-art multilingual models by a large margin, e.g. 10 % improvement upon baseline models on cross-lingual semantic search. We explore the same language bias of the learned representations, and propose a simple, post-training and model agnostic approach to remove the language identifying information from the representation while still retaining sentence semantics.</abstract>
      <url hash="706ceda5">2021.emnlp-main.502</url>
      <bibkey>yang-etal-2021-universal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.502</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="504">
      <title>Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Aadit</first><last>Trivedi</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>6247–6252</pages>
      <abstract>Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the implicit premise in an <a href="https://en.wikipedia.org/wiki/Enthymeme">enthymeme</a>, which requires not only an understanding of the stated conclusion and premise but also additional inferences that could depend on <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a>. The largest available <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for <a href="https://en.wikipedia.org/wiki/Enthymemes">enthymemes</a> (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset : <a href="https://en.wikipedia.org/wiki/Abductive_reasoning">Abductive reasoning</a> in narrative text (Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets.<i>implicit premise in an enthymeme</i>, which requires not only an understanding of the stated conclusion and premise but also additional inferences that could depend on commonsense knowledge. The largest available dataset for enthymemes (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during fine-tuning improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets.</abstract>
      <url hash="5dbdab40">2021.emnlp-main.504</url>
      <bibkey>chakrabarty-etal-2021-implicit</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.504</doi>
      <pwccode url="https://github.com/tuhinjubcse/enthymemesemnlp2021" additional="false">tuhinjubcse/enthymemesemnlp2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/art-dataset">ART Dataset</pwcdataset>
    </paper>
    <paper id="505">
      <title>Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks</title>
      <author><first>Yichen</first><last>Jiang</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>6253–6265</pages>
      <abstract>Systematic compositionality is an essential mechanism in <a href="https://en.wikipedia.org/wiki/Human_language">human language</a>, allowing the recombination of known parts to create novel <a href="https://en.wikipedia.org/wiki/Idiom">expressions</a>. However, existing neural models have been shown to lack this basic ability in learning <a href="https://en.wikipedia.org/wiki/Computer_algebra">symbolic structures</a>. Motivated by the failure of a Transformer model on the SCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing a command into actions, we propose two auxiliary sequence prediction tasks as additional training supervision. These automatically-generated sequences are more representative of the underlying compositional symbolic structures of the input data. During <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference</a>, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> jointly predicts the next action and the next tokens in the auxiliary sequences at each step. Experiments on the SCAN dataset show that our method encourages the Transformer to understand compositional structures of the command, improving its accuracy on multiple challenging splits from   10 % to 100 %. With only 418 (5 %) training instances, our <a href="https://en.wikipedia.org/wiki/Methodology">approach</a> still achieves 97.8 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on the MCD1 split. Therefore, we argue that <a href="https://en.wikipedia.org/wiki/Compositionality">compositionality</a> can be induced in <a href="https://en.wikipedia.org/wiki/Transformers_(toy_line)">Transformers</a> given minimal but proper guidance. We also show that a better result is achieved using less contextualized vectors as the attention’s query, providing insights into architecture choices in achieving systematic compositionality. Finally, we show positive generalization results on the grounded-SCAN task (Ruis et al., 2020).</abstract>
      <url hash="eb65d8de">2021.emnlp-main.505</url>
      <bibkey>jiang-bansal-2021-inducing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.505</doi>
      <pwccode url="https://github.com/jiangyctarheel/compositional-auxseq" additional="false">jiangyctarheel/compositional-auxseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cfq">CFQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="508">
      <title>Think about it ! Improving defeasible reasoning by first modeling the question scenario.</title>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Dheeraj</first><last>Rajagopal</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <author><first>Yiming</first><last>Yang</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>6291–6310</pages>
      <abstract>Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on <a href="https://en.wikipedia.org/wiki/Defeasible_reasoning">defeasible reasoning</a> suggests that a person forms a mental model of the problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the question scenario before answering a defeasible query. Our approach is, given a question, to have a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> first create a <a href="https://en.wikipedia.org/wiki/Graph_of_a_function">graph of relevant influences</a>, and then leverage that <a href="https://en.wikipedia.org/wiki/Graph_of_a_function">graph</a> as an additional input when answering the question. Our system, CURIOUS, achieves a new state-of-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by guiding a <a href="https://en.wikipedia.org/wiki/System">system</a> to think about a question and explicitly model the scenario, rather than answering reflexively.</abstract>
      <url hash="9b25d655">2021.emnlp-main.508</url>
      <bibkey>madaan-etal-2021-think</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.508</doi>
      <pwccode url="https://github.com/madaan/thinkaboutit" additional="false">madaan/thinkaboutit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wiqa">WIQA</pwcdataset>
    </paper>
    <paper id="511">
      <title>Improving Stance Detection with Multi-Dataset Learning and Knowledge Distillation</title>
      <author><first>Yingjie</first><last>Li</last></author>
      <author><first>Chenye</first><last>Zhao</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>6332–6345</pages>
      <abstract>Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as legalization of abortion. Despite significant progress on this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, one of the remaining challenges is the scarcity of <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a>. Besides, most previous works focused on a hard-label training in which meaningful similarities among categories are discarded during training. To address these challenges, first, we evaluate a multi-target and a multi-dataset training settings by training one <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> on each dataset and datasets of different domains, respectively. We show that <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can learn more universal representations with respect to targets in these settings. Second, we investigate the knowledge distillation in stance detection and observe that transferring knowledge from a teacher model to a student model can be beneficial in our proposed training settings. Moreover, we propose an Adaptive Knowledge Distillation (AKD) method that applies instance-specific temperature scaling to the teacher and student predictions. Results show that the multi-dataset model performs best on all datasets and it can be further improved by the proposed AKD, outperforming the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> by a large margin. We publicly release our code.</abstract>
      <url hash="0bb31b69">2021.emnlp-main.511</url>
      <bibkey>li-etal-2021-improving-stance</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.511</doi>
      <pwccode url="https://github.com/chuchun8/mdl-stance-distillation" additional="false">chuchun8/mdl-stance-distillation</pwccode>
    </paper>
    <paper id="513">
      <title>Improving Pre-trained Vision-and-Language Embeddings for Phrase Grounding</title>
      <author><first>Zi-Yi</first><last>Dou</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>6362–6371</pages>
      <abstract>Phrase grounding aims to map textual phrases to their associated image regions, which can be a prerequisite for multimodal reasoning and can benefit tasks requiring identifying objects based on language. With pre-trained vision-and-language models achieving impressive performance across tasks, it remains unclear if we can directly utilize their learned embeddings for phrase grounding without <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>. To this end, we propose a method to extract matched phrase-region pairs from pre-trained vision-and-language embeddings and propose four fine-tuning objectives to improve the model phrase grounding ability using image-caption data without any supervised grounding signals. Experiments on two representative datasets demonstrate the effectiveness of our objectives, outperforming baseline models in both weakly-supervised and supervised phrase grounding settings. In addition, we evaluate the aligned embeddings on several other downstream tasks and show that we can achieve better phrase grounding without sacrificing <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representation generality</a>.</abstract>
      <url hash="f851e2f4">2021.emnlp-main.513</url>
      <bibkey>dou-peng-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.513</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="515">
      <title>Hitting your MARQ : Multimodal ARgument Quality Assessment in Long Debate Video<fixed-case>MARQ</fixed-case>: Multimodal <fixed-case>AR</fixed-case>gument Quality Assessment in Long Debate Video</title>
      <author><first>Md Kamrul</first><last>Hasan</last></author>
      <author><first>James</first><last>Spann</last></author>
      <author><first>Masum</first><last>Hasan</last></author>
      <author><first>Md Saiful</first><last>Islam</last></author>
      <author><first>Kurtis</first><last>Haut</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <author><first>Ehsan</first><last>Hoque</last></author>
      <pages>6387–6397</pages>
      <abstract>The combination of <a href="https://en.wikipedia.org/wiki/Gesture">gestures</a>, <a href="https://en.wikipedia.org/wiki/Intonation_(linguistics)">intonations</a>, and <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">textual content</a> plays a key role in <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argument delivery</a>. However, the current literature mostly considers textual content while assessing the quality of an argument, and it is limited to <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> containing short sequences (18-48 words). In this paper, we study argument quality assessment in a multimodal context, and experiment on DBATES, a publicly available dataset of long debate videos. First, we propose a set of interpretable debate centric features such as clarity, content variation, body movement cues, and pauses, inspired by theories of <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argumentation quality</a>. Second, we design the Multimodal ARgument Quality assessor (MARQ)   a hierarchical neural network model that summarizes the multimodal signals on long sequences and enriches the multimodal embedding with debate centric features. Our proposed MARQ model achieves an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 81.91 % on the argument quality prediction task and outperforms established <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline models</a> with an <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">error rate reduction</a> of 22.7 %. Through ablation studies, we demonstrate the importance of multimodal cues in modeling argument quality.</abstract>
      <url hash="e8e5cbb2">2021.emnlp-main.515</url>
      <bibkey>hasan-etal-2021-hitting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.515</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/dbates">DBATES</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ibm-rank-30k">IBM-Rank-30k</pwcdataset>
    </paper>
    <paper id="516">
      <title>Mind the Context : The Impact of <a href="https://en.wikipedia.org/wiki/Contextualization">Contextualization</a> in Neural Module Networks for Grounding Visual Referring Expressions</title>
      <author><first>Arjun</first><last>Akula</last></author>
      <author><first>Spandana</first><last>Gella</last></author>
      <author><first>Keze</first><last>Wang</last></author>
      <author><first>Song-Chun</first><last>Zhu</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <pages>6398–6416</pages>
      <abstract>Neural module networks (NMN) are a popular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a> as they lack the ability to share weights and exploit associations between similar textual contexts (e.g. dark cube on the left vs. black cube on the left). In this work, we address these limitations and evaluate the impact of contextual clues in improving the performance of NMN models. First, we address the problem of fixed textual inputs by parameterizing the module arguments. This substantially reduce the number of <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a> in NMN by up to 75 % without any loss in performance. Next we propose a method to contextualize our parameterized model to enhance the module’s capacity in exploiting the visiolinguistic associations. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms the state-of-the-art NMN model on CLEVR-Ref+ dataset with +8.1 % improvement in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on the single-referent test set and +4.3 % on the full test set. Additionally, we demonstrate that contextualization provides +11.2 % and +1.7 % improvements in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> over prior NMN models on CLOSURE and NLVR2. We further evaluate the impact of our <a href="https://en.wikipedia.org/wiki/Contextualization">contextualization</a> by constructing a <a href="https://en.wikipedia.org/wiki/Contrast_(vision)">contrast set</a> for CLEVR-Ref+, which we call CC-Ref+. We significantly outperform the <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baselines</a> by as much as +10.4 % absolute <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on CC-Ref+, illustrating the generalization skills of our approach.</abstract>
      <url hash="8f7d835d">2021.emnlp-main.516</url>
      <bibkey>akula-etal-2021-mind</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.516</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr-ref">CLEVR-Ref+</pwcdataset>
    </paper>
    <paper id="524">
      <title>Knowledge Base Completion Meets Transfer Learning</title>
      <author><first>Vid</first><last>Kocijan</last></author>
      <author><first>Thomas</first><last>Lukasiewicz</last></author>
      <pages>6521–6533</pages>
      <abstract>The aim of knowledge base completion is to predict unseen facts from existing facts in <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a>. In this work, we introduce the first approach for <a href="https://en.wikipedia.org/wiki/Transfer_of_knowledge">transfer of knowledge</a> from one collection of facts to another without the need for entity or relation matching. The <a href="https://en.wikipedia.org/wiki/Methodology">method</a> works for both canonicalized knowledge bases and uncanonicalized or open knowledge bases, i.e., <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a> where more than one copy of a real-world entity or relation may exist. Such <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a> are a natural output of automated information extraction tools that extract <a href="https://en.wikipedia.org/wiki/Structured_data">structured data</a> from <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured text</a>. Our main contribution is a method that can make use of a large-scale pretraining on facts, collected from <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured text</a>, to improve predictions on <a href="https://en.wikipedia.org/wiki/Structured_data">structured data</a> from a specific domain. The introduced method is the most impactful on small datasets such as ReVerb20 K, where we obtained a 6 % absolute increase of mean reciprocal rank and 65 % relative decrease of mean rank over the previously best method, despite not relying on large pre-trained models like BERT.</abstract>
      <url hash="9a2550f8">2021.emnlp-main.524</url>
      <bibkey>kocijan-lukasiewicz-2021-knowledge</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.524</doi>
      <pwccode url="https://github.com/vid-koci/kbctransferlearning" additional="false">vid-koci/kbctransferlearning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/olpbench">OLPBENCH</pwcdataset>
    </paper>
    <paper id="526">
      <title>Towards Zero-Shot Knowledge Distillation for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a></title>
      <author><first>Ahmad</first><last>Rashid</last></author>
      <author><first>Vasileios</first><last>Lioutas</last></author>
      <author><first>Abbas</first><last>Ghaddar</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <pages>6551–6561</pages>
      <abstract>Knowledge distillation (KD) is a common <a href="https://en.wikipedia.org/wiki/Knowledge_transfer">knowledge transfer algorithm</a> used for <a href="https://en.wikipedia.org/wiki/Data_compression">model compression</a> across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher’s training data for <a href="https://en.wikipedia.org/wiki/Knowledge_transfer">knowledge transfer</a> to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such <a href="https://en.wikipedia.org/wiki/Data_(computing)">data</a>. We present, to the best of our knowledge, the first work on Zero-shot Knowledge Distillation for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, where the student learns from the much larger teacher without any task specific data. Our solution combines out-of-domain data and <a href="https://en.wikipedia.org/wiki/Adversarial_system">adversarial training</a> to learn the teacher’s output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75 % and 92 % of the teacher’s classification score (accuracy or F1) while compressing the model 30 times.</abstract>
      <url hash="01c17615">2021.emnlp-main.526</url>
      <bibkey>rashid-etal-2021-towards</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.526</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="529">
      <title>QuestEval : <a href="https://en.wikipedia.org/wiki/Summarization">Summarization</a> Asks for Fact-based Evaluation<fixed-case>Q</fixed-case>uest<fixed-case>E</fixed-case>val: Summarization Asks for Fact-based Evaluation</title>
      <author><first>Thomas</first><last>Scialom</last></author>
      <author><first>Paul-Alexis</first><last>Dray</last></author>
      <author><first>Sylvain</first><last>Lamprier</last></author>
      <author><first>Benjamin</first><last>Piwowarski</last></author>
      <author><first>Jacopo</first><last>Staiano</last></author>
      <author><first>Alex</first><last>Wang</last></author>
      <author><first>Patrick</first><last>Gallinari</last></author>
      <pages>6594–6604</pages>
      <abstract>Summarization evaluation remains an open research problem : current <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with <a href="https://en.wikipedia.org/wiki/Judgement">human judgments</a>. In this paper, we extend previous approaches and propose a unified framework, named QuestEval. In contrast to established metrics such as ROUGE or BERTScore, QuestEval does not require any <a href="https://en.wikipedia.org/wiki/Ground_truth">ground-truth reference</a>. Nonetheless, QuestEval substantially improves the correlation with human judgments over four evaluation dimensions (consistency, coherence, fluency, and relevance), as shown in extensive experiments.</abstract>
      <url hash="cf58cfa9">2021.emnlp-main.529</url>
      <bibkey>scialom-etal-2021-questeval</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.529</doi>
      <pwccode url="https://github.com/recitalAI/QuestEval" additional="true">recitalAI/QuestEval</pwccode>
    </paper>
    <paper id="531">
      <title>Finding a Balanced Degree of Automation for Summary Evaluation</title>
      <author><first>Shiyue</first><last>Zhang</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>6617–6632</pages>
      <abstract>Human evaluation for <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization tasks</a> is reliable but brings in issues of reproducibility and high costs. Automatic metrics are cheap and reproducible but sometimes poorly correlated with <a href="https://en.wikipedia.org/wiki/Judgement">human judgment</a>. In this work, we propose flexible semiautomatic to automatic summary evaluation metrics, following the Pyramid human evaluation method. Semi-automatic Lite2Pyramid retains the reusable human-labeled Summary Content Units (SCUs) for reference(s) but replaces the manual work of judging SCUs’ presence in system summaries with a natural language inference (NLI) model. Fully automatic Lite3Pyramid further substitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via a semantic role labeling (SRL) model. Finally, we propose <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">in-between metrics</a>, Lite2.xPyramid, where we use a simple <a href="https://en.wikipedia.org/wiki/Regressor">regressor</a> to predict how well the STUs can simulate SCUs and retain SCUs that are more difficult to simulate, which provides a smooth transition and balance between <a href="https://en.wikipedia.org/wiki/Automation">automation</a> and manual evaluation. Comparing to 15 existing metrics, we evaluate human-metric correlations on 3 existing meta-evaluation datasets and our newly collected PyrXSum (with 100/10 XSum examples / systems). It shows that Lite2Pyramid consistently has the best summary-level correlations ; Lite3Pyramid works better than or comparable to other automatic metrics ; Lite2.xPyramid trades off small correlation drops for larger manual effort reduction, which can reduce costs for future data collection.</abstract>
      <url hash="071dc2a3">2021.emnlp-main.531</url>
      <bibkey>zhang-bansal-2021-finding</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.531</doi>
      <pwccode url="https://github.com/zhangshiyue/lite2-3pyramid" additional="false">zhangshiyue/lite2-3pyramid</pwccode>
    </paper>
    <paper id="535">
      <title>Controlling Machine Translation for Multiple Attributes with Additive Interventions</title>
      <author><first>Andrea</first><last>Schioppa</last></author>
      <author><first>David</first><last>Vilar</last></author>
      <author><first>Artem</first><last>Sokolov</last></author>
      <author><first>Katja</first><last>Filippova</last></author>
      <pages>6676–6696</pages>
      <abstract>Fine-grained control of machine translation (MT) outputs along multiple attributes is critical for many modern MT applications and is a requirement for gaining users’ trust. A standard approach for exerting control in MT is to prepend the input with a special tag to signal the desired output attribute. Despite its simplicity, attribute tagging has several drawbacks : continuous values must be binned into discrete categories, which is unnatural for certain applications ; interference between multiple tags is poorly understood. We address these problems by introducing vector-valued interventions which allow for fine-grained control over multiple attributes simultaneously via a weighted linear combination of the corresponding vectors. For some attributes, our approach even allows for fine-tuning a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained without <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> to support such interventions. In experiments with three attributes (length, politeness and monotonicity) and two language pairs (English to German and Japanese) our models achieve better <a href="https://en.wikipedia.org/wiki/Scientific_control">control</a> over a wider range of tasks compared to <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">tagging</a>, and translation quality does not degrade when no control is requested. Finally, we demonstrate how to enable <a href="https://en.wikipedia.org/wiki/Control_theory">control</a> in an already trained <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> after a relatively cheap fine-tuning stage.</abstract>
      <url hash="9e005586">2021.emnlp-main.535</url>
      <bibkey>schioppa-etal-2021-controlling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.535</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
    </paper>
    <paper id="536">
      <title>A Generative Framework for Simultaneous Machine Translation</title>
      <author><first>Yishu</first><last>Miao</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>6697–6706</pages>
      <abstract>We propose a generative framework for simultaneous machine translation. Conventional approaches use a fixed number of source words to translate or learn dynamic policies for the number of source words by <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>. Here we formulate <a href="https://en.wikipedia.org/wiki/Simultaneous_translation">simultaneous translation</a> as a structural sequence-to-sequence learning problem. A <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a> is introduced to model read or translate actions at every time step, which is then integrated out to consider all the possible translation policies. A re-parameterised Poisson prior is used to regularise the policies which allows the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to explicitly balance translation quality and <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a>. The experiments demonstrate the effectiveness and robustness of the generative framework, which achieves the best BLEU scores given different average translation latencies on benchmark datasets.</abstract>
      <url hash="764afb8e">2021.emnlp-main.536</url>
      <bibkey>miao-etal-2021-generative</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.536</doi>
    </paper>
    <paper id="538">
      <title>Boosting Cross-Lingual Transfer via <a href="https://en.wikipedia.org/wiki/Self-learning">Self-Learning</a> with Uncertainty Estimation</title>
      <author><first>Liyan</first><last>Xu</last></author>
      <author><first>Xuchao</first><last>Zhang</last></author>
      <author><first>Xujiang</first><last>Zhao</last></author>
      <author><first>Haifeng</first><last>Chen</last></author>
      <author><first>Feng</first><last>Chen</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>6716–6723</pages>
      <abstract>Recent multilingual pre-trained language models have achieved remarkable zero-shot performance, where the model is only finetuned on one source language and directly evaluated on target languages. In this work, we propose a self-learning framework that further utilizes unlabeled data of target languages, combined with uncertainty estimation in the process to select high-quality silver labels. Three different uncertainties are adapted and analyzed specifically for the cross lingual transfer : Language Heteroscedastic / Homoscedastic Uncertainty (LEU / LOU), Evidential Uncertainty (EVI). We evaluate our framework with uncertainties on two cross-lingual tasks including Named Entity Recognition (NER) and Natural Language Inference (NLI) covering 40 languages in total, which outperforms the baselines significantly by 10 F1 for NER on average and 2.5 accuracy for NLI.</abstract>
      <url hash="9dd570d4">2021.emnlp-main.538</url>
      <bibkey>xu-etal-2021-boosting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.538</doi>
      <pwccode url="https://github.com/lxucs/multilingual-sl" additional="false">lxucs/multilingual-sl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="540">
      <title>Interactive Machine Comprehension with Dynamic Knowledge Graphs</title>
      <author><first>Xingdi</first><last>Yuan</last></author>
      <pages>6734–6750</pages>
      <abstract>Interactive machine reading comprehension (iMRC) is machine comprehension tasks where knowledge sources are partially observable. An agent must interact with an environment sequentially to gather necessary knowledge in order to answer a question. We hypothesize that <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">graph representations</a> are good inductive biases, which can serve as an agent’s memory mechanism in iMRC tasks. We explore four different categories of <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a> that can capture <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">text information</a> at various levels. We describe methods that dynamically build and update these <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a> during information gathering, as well as neural models to encode <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph representations</a> in RL agents. Extensive experiments on iSQuAD suggest that graph representations can result in significant performance improvements for RL agents.</abstract>
      <url hash="0a7f6862">2021.emnlp-main.540</url>
      <bibkey>yuan-2021-interactive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.540</doi>
      <pwccode url="https://github.com/xingdi-eric-yuan/imrc_graph_public" additional="false">xingdi-eric-yuan/imrc_graph_public</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="542">
      <title>Visual News : Benchmark and Challenges in News Image Captioning</title>
      <author><first>Fuxiao</first><last>Liu</last></author>
      <author><first>Yinghan</first><last>Wang</last></author>
      <author><first>Tianlu</first><last>Wang</last></author>
      <author><first>Vicente</first><last>Ordonez</last></author>
      <pages>6761–6771</pages>
      <abstract>We propose Visual News Captioner, an entity-aware model for the task of news image captioning. We also introduce Visual News, a large-scale benchmark consisting of more than one million news images along with associated news articles, image captions, author information, and other <a href="https://en.wikipedia.org/wiki/Metadata">metadata</a>. Unlike the standard image captioning task, news images depict situations where people, locations, and <a href="https://en.wikipedia.org/wiki/News">events</a> are of paramount importance. Our proposed method can effectively combine visual and textual features to generate captions with richer information such as <a href="https://en.wikipedia.org/wiki/Event_(philosophy)">events</a> and <a href="https://en.wikipedia.org/wiki/Non-physical_entity">entities</a>. More specifically, built upon the Transformer architecture, our model is further equipped with novel multi-modal feature fusion techniques and attention mechanisms, which are designed to generate named entities more accurately. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> utilizes much fewer parameters while achieving slightly better prediction results than competing methods. Our larger and more diverse Visual News dataset further highlights the remaining challenges in captioning news images.</abstract>
      <url hash="fea3b545">2021.emnlp-main.542</url>
      <bibkey>liu-etal-2021-visual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.542</doi>
      <pwccode url="https://github.com/FuxiaoLiu/VisualNews-Repository" additional="false">FuxiaoLiu/VisualNews-Repository</pwccode>
    </paper>
    <paper id="543">
      <title>Integrating Visuospatial, Linguistic, and Commonsense Structure into Story Visualization</title>
      <author><first>Adyasha</first><last>Maharana</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>6772–6786</pages>
      <abstract>While much research has been done in text-to-image synthesis, little work has been done to explore the usage of linguistic structure of the input text. Such information is even more important for story visualization since its inputs have an explicit <a href="https://en.wikipedia.org/wiki/Narrative_structure">narrative structure</a> that needs to be translated into an image sequence (or visual story). Prior work in this domain has shown that there is ample room for improvement in the generated image sequence in terms of visual quality, <a href="https://en.wikipedia.org/wiki/Consistency">consistency</a> and <a href="https://en.wikipedia.org/wiki/Relevance">relevance</a>. In this paper, we first explore the use of constituency parse trees using a Transformer-based recurrent architecture for encoding structured input. Second, we augment the structured input with commonsense information and study the impact of this external knowledge on the generation of visual story. Third, we also incorporate visual structure via bounding boxes and dense captioning to provide feedback about the characters / objects in generated images within a dual learning setup. We show that off-the-shelf dense-captioning models trained on Visual Genome can improve the spatial structure of images from a different target domain without needing fine-tuning. We train the model end-to-end using intra-story contrastive loss (between words and image sub-regions) and show significant improvements in visual quality. Finally, we provide an analysis of the linguistic and visuo-spatial information.</abstract>
      <url hash="7e74d4b0">2021.emnlp-main.543</url>
      <bibkey>maharana-bansal-2021-integrating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.543</doi>
      <pwccode url="https://github.com/adymaharana/vlcstorygan" additional="false">adymaharana/vlcstorygan</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="547">
      <title>Tribrid : Stance Classification with Neural Inconsistency Detection</title>
      <author><first>Song</first><last>Yang</last></author>
      <author><first>Jacopo</first><last>Urbani</last></author>
      <pages>6831–6843</pages>
      <abstract>We study the problem of performing automatic stance classification on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> with neural architectures such as <a href="https://en.wikipedia.org/wiki/BERT">BERT</a>. Although these architectures deliver impressive results, their level is not yet comparable to the one of humans and they might produce errors that have a significant impact on the downstream task (e.g., fact-checking). To improve the performance, we present a new neural architecture where the input also includes automatically generated negated perspectives over a given claim. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is jointly learned to make simultaneously multiple predictions, which can be used either to improve the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> of the original perspective or to filter out doubtful predictions. In the first case, we propose a weakly supervised method for combining the predictions into a final one. In the second case, we show that using the <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence scores</a> to remove doubtful predictions allows our method to achieve human-like performance over the retained information, which is still a sizable part of the original input.</abstract>
      <url hash="0e426dcf">2021.emnlp-main.547</url>
      <attachment type="Software" hash="42d7ad82">2021.emnlp-main.547.Software.zip</attachment>
      <bibkey>yang-urbani-2021-tribrid</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.547</doi>
      <pwccode url="https://github.com/karmaresearch/tribrid" additional="false">karmaresearch/tribrid</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/perspectrum">Perspectrum</pwcdataset>
    </paper>
    <paper id="549">
      <title>Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks</title>
      <author><first>Gaël</first><last>Guibon</last></author>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Hélène</first><last>Flamein</last></author>
      <author><first>Luce</first><last>Lefeuvre</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>6858–6870</pages>
      <abstract>Several recent studies on dyadic human-human interactions have been done on conversations without specific business objectives. However, many companies might benefit from studies dedicated to more precise environments such as after sales services or customer satisfaction surveys. In this work, we place ourselves in the scope of a live chat customer service in which we want to detect <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> and their evolution in the conversation flow. This context leads to multiple challenges that range from exploiting restricted, small and mostly unlabeled datasets to finding and adapting methods for such <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a>. We tackle these challenges by using Few-Shot Learning while making the hypothesis it can serve conversational emotion classification for different languages and sparse labels. We contribute by proposing a variation of Prototypical Networks for sequence labeling in conversation that we name ProtoSeq. We test this method on two <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> with different languages : daily conversations in English and customer service chat conversations in <a href="https://en.wikipedia.org/wiki/French_language">French</a>. When applied to emotion classification in conversations, our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> proved to be competitive even when compared to other ones.</abstract>
      <url hash="29d970cb">2021.emnlp-main.549</url>
      <bibkey>guibon-etal-2021-shot</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.549</doi>
      <pwccode url="https://github.com/gguibon/protoseq" additional="false">gguibon/protoseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="553">
      <title>When is Wall a Pared and when a Muro? : Extracting Rules Governing Lexical Selection</title>
      <author><first>Aditi</first><last>Chaudhary</last></author>
      <author><first>Kayo</first><last>Yin</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>6911–6929</pages>
      <abstract>Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun wall has different lexical manifestations in <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>   pared refers to an indoor wall while muro refers to an outside wall. However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way. In this work, we present a method for automatically identifying fine-grained lexical distinctions, and extracting rules explaining these distinctions in a human- and machine-readable format. We confirm the quality of these extracted rules in a language learning setup for two languages, <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a> and <a href="https://en.wikipedia.org/wiki/Greek_language">Greek</a>, where we use the rules to teach non-native speakers when to translate a given ambiguous word into its different possible translations.</abstract>
      <url hash="f56b9e90">2021.emnlp-main.553</url>
      <bibkey>chaudhary-etal-2021-wall</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.553</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="556">
      <title>Continuous Entailment Patterns for Lexical Inference in Context</title>
      <author><first>Martin</first><last>Schmitt</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>6952–6959</pages>
      <abstract>Combining a pretrained language model (PLM) with textual patterns has been shown to help in both zero- and few-shot settings. For zero-shot performance, it makes sense to design <a href="https://en.wikipedia.org/wiki/Pattern">patterns</a> that closely resemble the text seen during self-supervised pretraining because the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> has never seen anything else. Supervised training allows for more flexibility. If we allow for tokens outside the PLM’s vocabulary, <a href="https://en.wikipedia.org/wiki/Pattern">patterns</a> can be adapted more flexibly to a PLM’s idiosyncrasies. Contrasting patterns where a token can be any continuous vector from those where a discrete choice between vocabulary elements has to be made, we call our method CONtinous pAtterNs (CONAN). We evaluate CONAN on two established benchmarks for lexical inference in context (LIiC) a.k.a. predicate entailment, a challenging natural language understanding task with relatively <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">small training data</a>. In a direct comparison with discrete patterns, <a href="https://en.wikipedia.org/wiki/CONAN">CONAN</a> consistently leads to improved performance, setting a new state of the art. Our experiments give valuable insights on the kind of <a href="https://en.wikipedia.org/wiki/Pattern">pattern</a> that enhances a PLM’s performance on LIiC and raise important questions regarding our understanding of PLMs using text patterns.</abstract>
      <url hash="c34c3f03">2021.emnlp-main.556</url>
      <bibkey>schmitt-schutze-2021-continuous</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.556</doi>
      <pwccode url="https://github.com/mnschmit/conan" additional="false">mnschmit/conan</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sherliic">SherLIiC</pwcdataset>
    </paper>
    <paper id="557">
      <title>Numeracy enhances the Literacy of Language Models</title>
      <author><first>Avijit</first><last>Thawani</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>Filip</first><last>Ilievski</last></author>
      <pages>6960–6967</pages>
      <abstract>Specialized number representations in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction. But humans also use <a href="https://en.wikipedia.org/wiki/Numeracy">numeracy</a> to make better sense of <a href="https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences">world concepts</a>, e.g., you can seat 5 people in your ‘room’ but not 500. Does a better grasp of numbers improve a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s understanding of other concepts and words? This paper studies the effect of using six different <a href="https://en.wikipedia.org/wiki/Encoder">number encoders</a> on the task of masked word prediction (MWP), as a proxy for evaluating <a href="https://en.wikipedia.org/wiki/Literacy">literacy</a>. To support this investigation, we develop Wiki-Convert, a 900,000 sentence dataset annotated with numbers and units, to avoid conflating nominal and ordinal number occurrences. We find a significant improvement in MWP for sentences containing numbers, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a BERT baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers. We release all code at https://git.io/JuZXn.</abstract>
      <url hash="a885570e">2021.emnlp-main.557</url>
      <attachment type="Software" hash="e6eebc73">2021.emnlp-main.557.Software.zip</attachment>
      <bibkey>thawani-etal-2021-numeracy</bibkey>
      <revision id="1" href="2021.emnlp-main.557v1" hash="ecf6ae2a" />
      <revision id="2" href="2021.emnlp-main.557v2" hash="a885570e" date="2021-11-09">Added acknowledgements.</revision>
      <doi>10.18653/v1/2021.emnlp-main.557</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiconvert">WikiConvert</pwcdataset>
    </paper>
    <paper id="558">
      <title>Students Who Study Together Learn Better : On the Importance of Collective Knowledge Distillation for Domain Transfer in Fact Verification</title>
      <author><first>Mitch Paul</first><last>Mithun</last></author>
      <author><first>Sandeep</first><last>Suntwal</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>6968–6973</pages>
      <abstract>While <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> produce state-of-the- art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. Previous works have proposed delexicalization as a form of knowledge distillation to reduce the dependency on such lexical artifacts. However, a critical unsolved issue that remains is how much delexicalization to apply : a little helps reduce <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>, but too much discards useful information. We propose <a href="https://en.wikipedia.org/wiki/Group_learning">Group Learning</a>, a knowledge and model distillation approach for fact verification in which multiple student models have access to different delexicalized views of the data, but are encouraged to learn from each other through pair-wise consistency losses. In several cross-domain experiments between the FEVER and FNC fact verification datasets, we show that our approach learns the best delexicalization strategy for the given training dataset, and outperforms state-of-the-art <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> that rely on the original data.</abstract>
      <url hash="60106c55">2021.emnlp-main.558</url>
      <bibkey>mithun-etal-2021-students</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.558</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="560">
      <title>Joint Passage Ranking for Diverse Multi-Answer Retrieval</title>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Kenton</first><last>Lee</last></author>
      <author><first>Ming-Wei</first><last>Chang</last></author>
      <author><first>Kristina</first><last>Toutanova</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>6997–7008</pages>
      <abstract>We study multi-answer retrieval, an under-explored problem that requires retrieving passages to cover multiple distinct answers for a given question. This task requires joint modeling of retrieved passages, as models should not repeatedly retrieve passages containing the same answer at the cost of missing a different valid answer. Prior work focusing on single-answer retrieval is limited as it can not reason about the set of passages jointly. In this paper, we introduce JPR, a joint passage retrieval model focusing on <a href="https://en.wikipedia.org/wiki/Ranking">reranking</a>. To model the <a href="https://en.wikipedia.org/wiki/Joint_probability">joint probability</a> of the retrieved passages, JPR makes use of an autoregressive reranker that selects a sequence of passages, equipped with novel <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training and decoding algorithms</a>. Compared to prior approaches, JPR achieves significantly better answer coverage on three multi-answer datasets. When combined with downstream question answering, the improved retrieval enables larger answer generation models since they need to consider fewer passages, establishing a new state-of-the-art.</abstract>
      <url hash="b7938bc1">2021.emnlp-main.560</url>
      <bibkey>min-etal-2021-joint</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.560</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="561">
      <title>Generative Context Pair Selection for Multi-hop Question Answering</title>
      <author><first>Dheeru</first><last>Dua</last></author>
      <author><first>Cicero</first><last>Nogueira dos Santos</last></author>
      <author><first>Patrick</first><last>Ng</last></author>
      <author><first>Ben</first><last>Athiwaratkun</last></author>
      <author><first>Bing</first><last>Xiang</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>7009–7015</pages>
      <abstract>Compositional reasoning tasks such as multi-hop question answering require <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to learn how to make latent decisions using only weak supervision from the final answer. Crowdsourced datasets gathered for these tasks, however, often contain only a slice of the underlying task distribution, which can induce unanticipated biases such as shallow word overlap between the question and context. Recent works have shown that discriminative training results in models that exploit these underlying biases to achieve a better held-out performance, without learning the right way to reason. We propose a generative context selection model for multi-hop QA that reasons about how the given question could have been generated given a context pair and not just independent contexts. We show that on HotpotQA, while being comparable to the state-of-the-art answering performance, our proposed generative passage selection model has a better performance (4.9 % higher than baseline) on adversarial held-out set which tests robustness of <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s multi-hop reasoning capabilities.</abstract>
      <url hash="e38e9806">2021.emnlp-main.561</url>
      <bibkey>dua-etal-2021-generative</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.561</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="563">
      <title>Have You Seen That Number? Investigating Extrapolation in Question Answering Models</title>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Giwon</first><last>Hong</last></author>
      <author><first>Kyung-min</first><last>Kim</last></author>
      <author><first>Junmo</first><last>Kang</last></author>
      <author><first>Sung-Hyon</first><last>Myaeng</last></author>
      <pages>7031–7037</pages>
      <abstract>Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> fail to extrapolate to unseen numbers. Presenting <a href="https://en.wikipedia.org/wiki/Number">numbers</a> as digit-by-digit input to the model, we also propose the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat <a href="https://en.wikipedia.org/wiki/Number">numbers</a> differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.<i>E-digit</i> number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.</abstract>
      <url hash="64f481c2">2021.emnlp-main.563</url>
      <bibkey>kim-etal-2021-seen</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.563</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="568">
      <title>I Wish I Would Have Loved This One, But I Did n’t   A Multilingual Dataset for Counterfactual Detection in Product Review<fixed-case>I</fixed-case> Wish <fixed-case>I</fixed-case> Would Have Loved This One, But <fixed-case>I</fixed-case> Didn’t – A Multilingual Dataset for Counterfactual Detection in Product Review</title>
      <author><first>James</first><last>O’Neill</last></author>
      <author><first>Polina</first><last>Rozenshtein</last></author>
      <author><first>Ryuichi</first><last>Kiryo</last></author>
      <author><first>Motoko</first><last>Kubota</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <pages>7092–7108</pages>
      <abstract>Counterfactual statements describe events that did not or can not take place. We consider the problem of counterfactual detection (CFD) in product reviews. For this purpose, we annotate a multilingual CFD dataset from Amazon product reviews covering <a href="https://en.wikipedia.org/wiki/Counterfactual_conditional">counterfactual statements</a> written in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a>, and Japanese languages. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is unique as it contains <a href="https://en.wikipedia.org/wiki/Counterfactual_conditional">counterfactuals</a> in multiple languages, covers a new application area of <a href="https://en.wikipedia.org/wiki/Review_site">e-commerce reviews</a>, and provides high quality professional annotations. We train <a href="https://en.wikipedia.org/wiki/Computational_fluid_dynamics">CFD models</a> using different text representation methods and <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a>. We find that these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are robust against the <a href="https://en.wikipedia.org/wiki/Selection_bias">selectional biases</a> introduced due to cue phrase-based sentence selection. Moreover, our CFD dataset is compatible with prior datasets and can be merged to learn accurate CFD models. Applying <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> on English counterfactual examples to create multilingual data performs poorly, demonstrating the language-specificity of this problem, which has been ignored so far.</abstract>
      <url hash="99ac75f7">2021.emnlp-main.568</url>
      <bibkey>oneill-etal-2021-wish</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.568</doi>
      <pwccode url="https://github.com/amazon-research/amazon-multilingual-counterfactual-dataset" additional="false">amazon-research/amazon-multilingual-counterfactual-dataset</pwccode>
    </paper>
    <paper id="570">
      <title>Evaluating the Morphosyntactic Well-formedness of Generated Texts</title>
      <author><first>Adithya</first><last>Pratapa</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Shruti</first><last>Rijhwani</last></author>
      <author><first>Aditi</first><last>Chaudhary</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>7131–7150</pages>
      <abstract>Text generation systems are ubiquitous in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing applications</a>. However, evaluation of these <a href="https://en.wikipedia.org/wiki/System">systems</a> remains a challenge, especially in multilingual settings. In this paper, we propose L’AMBRE   a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> to train robust parsers. We show the effectiveness of our <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> on the task of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> through a diachronic study of systems translating into morphologically-rich languages.</abstract>
      <url hash="aab8f609">2021.emnlp-main.570</url>
      <bibkey>pratapa-etal-2021-evaluating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.570</doi>
      <pwccode url="https://github.com/adithya7/lambre" additional="false">adithya7/lambre</pwccode>
    </paper>
    <paper id="574">
      <title>ValNorm Quantifies <a href="https://en.wikipedia.org/wiki/Semantics">Semantics</a> to Reveal Consistent Valence Biases Across Languages and Over Centuries<fixed-case>V</fixed-case>al<fixed-case>N</fixed-case>orm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries</title>
      <author><first>Autumn</first><last>Toney</last></author>
      <author><first>Aylin</first><last>Caliskan</last></author>
      <pages>7203–7218</pages>
      <abstract>Word embeddings learn <a href="https://en.wikipedia.org/wiki/Implicit_stereotype">implicit biases</a> from linguistic regularities captured by word co-occurrence statistics. By extending methods that quantify human-like biases in word embeddings, we introduce ValNorm, a novel intrinsic evaluation task and method to quantify the valence dimension of affect in human-rated word sets from <a href="https://en.wikipedia.org/wiki/Social_psychology">social psychology</a>. We apply ValNorm on static word embeddings from seven languages (Chinese, <a href="https://en.wikipedia.org/wiki/English_language">English</a>, German, Polish, Portuguese, Spanish, and Turkish) and from historical English text spanning 200 years. ValNorm achieves consistently high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> in quantifying the valence of non-discriminatory, non-social group word sets. Specifically, ValNorm achieves a <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation</a> of r=0.88 for human judgment scores of valence for 399 words collected to establish pleasantness norms in <a href="https://en.wikipedia.org/wiki/English_language">English</a>. In contrast, we measure <a href="https://en.wikipedia.org/wiki/Gender_role">gender stereotypes</a> using the same set of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> and find that <a href="https://en.wikipedia.org/wiki/Bias">social biases</a> vary across languages. Our results indicate that valence associations of non-discriminatory, non-social group words represent widely-shared associations, in seven languages and over 200 years.</abstract>
      <url hash="e9e7c2b5">2021.emnlp-main.574</url>
      <bibkey>toney-caliskan-2021-valnorm</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.574</doi>
      <pwccode url="https://github.com/autumntoney/ValNorm" additional="false">autumntoney/ValNorm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="576">
      <title>Robust Open-Vocabulary Translation from Visual Text Representations</title>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>David</first><last>Etter</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <pages>7235–7252</pages>
      <abstract>Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an ‘open vocabulary.’ This approach relies on consistent and correct underlying unicode sequences, and makes <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> susceptible to degradation from common types of <a href="https://en.wikipedia.org/wiki/Noise">noise</a> and <a href="https://en.wikipedia.org/wiki/Genetic_variation">variation</a>. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows. We show that <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> using visual text representations approach or match performance of traditional text models on small and larger datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of <a href="https://en.wikipedia.org/wiki/Noise_(electronics)">noise</a>, achieving e.g., 25.9 <a href="https://en.wikipedia.org/wiki/Bitwise_operation">BLEU</a> on a character permuted GermanEnglish task where subword models degrade to 1.9.</abstract>
      <url hash="2c2bd918">2021.emnlp-main.576</url>
      <bibkey>salesky-etal-2021-robust</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.576</doi>
      <pwccode url="https://github.com/esalesky/visrep" additional="false">esalesky/visrep</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="578">
      <title>Improving Multilingual Translation by Representation and Gradient Regularization</title>
      <author><first>Yilin</first><last>Yang</last></author>
      <author><first>Akiko</first><last>Eriguchi</last></author>
      <author><first>Alexandre</first><last>Muzio</last></author>
      <author><first>Prasad</first><last>Tadepalli</last></author>
      <author><first>Stefan</first><last>Lee</last></author>
      <author><first>Hany</first><last>Hassan</last></author>
      <pages>7266–7279</pages>
      <abstract>Multilingual Neural Machine Translation (NMT) enables one <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to serve all <a href="https://en.wikipedia.org/wiki/Translation">translation directions</a>, including ones that are unseen during training, i.e. zero-shot translation. Despite being theoretically attractive, current models often produce low quality translations   commonly failing to even produce outputs in the right target language. In this work, we observe that off-target translation is dominant even in strong multilingual systems, trained on massive multilingual corpora. To address this issue, we propose a joint approach to regularize NMT models at both representation-level and gradient-level. At the representation level, we leverage an auxiliary target language prediction task to regularize decoder outputs to retain information about the target language. At the gradient level, we leverage a small amount of direct data (in thousands of sentence pairs) to regularize model gradients. Our results demonstrate that our approach is highly effective in both reducing off-target translation occurrences and improving zero-shot translation performance by +5.59 and +10.38 BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> also works well when the small amount of direct data is not available.</abstract>
      <url hash="1d4afce6">2021.emnlp-main.578</url>
      <bibkey>yang-etal-2021-improving-multilingual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.578</doi>
      <pwccode url="https://github.com/yilinyang7/fairseq_multi_fix" additional="false">yilinyang7/fairseq_multi_fix</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opus-100">OPUS-100</pwcdataset>
    </paper>
    <paper id="579">
      <title>Learning Kernel-Smoothed Machine Translation with Retrieved Examples</title>
      <author><first>Qingnan</first><last>Jiang</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Jun</first><last>Cao</last></author>
      <author><first>Shanbo</first><last>Cheng</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>7280–7290</pages>
      <abstract>How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>, updating the deployed <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, <a href="https://en.wikipedia.org/wiki/Nonparametric_statistics">non-parametric methods</a> are prone to overfit the retrieved examples. In this work, we propose to learn Kernel-Smoothed Translation with Example Retrieval (KSTER), an effective approach to adapt neural machine translation models online. Experiments on domain adaptation and multi-domain machine translation datasets show that even without expensive retraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over the best existing online adaptation methods. The code and trained <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> are released at https://github.com/jiangqn/KSTER.</abstract>
      <url hash="09faadd8">2021.emnlp-main.579</url>
      <bibkey>jiang-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.579</doi>
      <pwccode url="https://github.com/jiangqn/kster" additional="false">jiangqn/kster</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="580">
      <title>Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training</title>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Yitong</first><last>Li</last></author>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>7291–7305</pages>
      <abstract>Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the high resource ones. However, automatic balancing methods usually depend on the intra- and inter-dataset characteristics, which is usually agnostic or requires human priors. In this work, we propose an approach, MultiUAT, that dynamically adjusts the training data usage based on the model’s uncertainty on a small set of trusted clean data for multi-corpus machine translation. We experiments with two classes of uncertainty measures on multilingual (16 languages with 4 settings) and multi-domain settings (4 for in-domain and 2 for out-of-domain on English-German translation) and demonstrate our approach MultiUAT substantially outperforms its baselines, including both static and dynamic strategies. We analyze the cross-domain transfer and show the deficiency of static and similarity based methods.</abstract>
      <url hash="38f0fc7a">2021.emnlp-main.580</url>
      <bibkey>wu-etal-2021-uncertainty</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.580</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="583">
      <title>Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering</title>
      <author><first>Siddhant</first><last>Garg</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>7329–7346</pages>
      <abstract>In this paper we propose a novel approach towards improving the efficiency of Question Answering (QA) systems by filtering out questions that will not be answered by them. This is based on an interesting new finding : the answer confidence scores of state-of-the-art QA systems can be approximated well by models solely using the input question text. This enables preemptive filtering of questions that are not answered by the <a href="https://en.wikipedia.org/wiki/System">system</a> due to their answer confidence scores being lower than the <a href="https://en.wikipedia.org/wiki/System">system threshold</a>. Specifically, we learn Transformer-based question models by distilling Transformer-based answering models. Our experiments on three popular QA datasets and one industrial QA benchmark demonstrate the ability of our question models to approximate the Precision / Recall curves of the target QA system well. These question models, when used as filters, can effectively trade off lower computation cost of QA systems for lower <a href="https://en.wikipedia.org/wiki/Recall_(memory)">Recall</a>, e.g., reducing computation by ~60 %, while only losing ~3-4 % of <a href="https://en.wikipedia.org/wiki/Recall_(memory)">Recall</a>.</abstract>
      <url hash="e28c0f93">2021.emnlp-main.583</url>
      <bibkey>garg-moschitti-2021-will</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.583</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/asnq">ASNQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paralex">Paralex</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="585">
      <title>Explaining Answers with Entailment Trees</title>
      <author><first>Bhavana</first><last>Dalvi</last></author>
      <author><first>Peter</first><last>Jansen</last></author>
      <author><first>Oyvind</first><last>Tafjord</last></author>
      <author><first>Zhengnan</first><last>Xie</last></author>
      <author><first>Hannah</first><last>Smith</last></author>
      <author><first>Leighanna</first><last>Pipatanangkura</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <pages>7358–7370</pages>
      <abstract>Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a rationale). If this could be done, new opportunities for understanding and debugging the <a href="https://en.wikipedia.org/wiki/System">system</a>’s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks : generate a valid <a href="https://en.wikipedia.org/wiki/Logical_consequence">entailment tree</a> given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>. We show that a strong <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> can partially solve these <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>, in particular when the relevant sentences are included in the input (e.g., 35 % of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations.</abstract>
      <url hash="52b7078a">2021.emnlp-main.585</url>
      <bibkey>dalvi-etal-2021-explaining</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.585</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/entailmentbank">EntailmentBank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/proofwriter">ProofWriter</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/worldtree">Worldtree</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/eqasc">eQASC</pwcdataset>
    </paper>
    <paper id="593">
      <title>Effective Sequence-to-Sequence Dialogue State Tracking</title>
      <author><first>Jeffrey</first><last>Zhao</last></author>
      <author><first>Mahdis</first><last>Mahdieh</last></author>
      <author><first>Ye</first><last>Zhang</last></author>
      <author><first>Yuan</first><last>Cao</last></author>
      <author><first>Yonghui</first><last>Wu</last></author>
      <pages>7486–7493</pages>
      <abstract>Sequence-to-sequence models have been applied to a wide variety of NLP tasks, but how to properly use them for dialogue state tracking has not been systematically investigated. In this paper, we study this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> from the perspectives of pre-training objectives as well as the formats of context representations. We demonstrate that the choice of pre-training objective makes a significant difference to the state tracking quality. In particular, we find that masked span prediction is more effective than auto-regressive language modeling. We also explore using Pegasus, a span prediction-based pre-training objective for <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a>, for the state tracking model. We found that pre-training for the seemingly distant summarization task works surprisingly well for dialogue state tracking. In addition, we found that while recurrent state context representation works also reasonably well, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> may have a hard time recovering from earlier mistakes. We conducted experiments on the MultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.</abstract>
      <url hash="a821d5ba">2021.emnlp-main.593</url>
      <bibkey>zhao-etal-2021-effective-sequence</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.593</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/dialogue-state-tracking-challenge">Dialogue State Tracking Challenge</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-oz">Wizard-of-Oz</pwcdataset>
    </paper>
    <paper id="598">
      <title>RICA : Evaluating Robust Inference Capabilities Based on Commonsense Axioms<fixed-case>RICA</fixed-case>: Evaluating Robust Inference Capabilities Based on Commonsense Axioms</title>
      <author><first>Pei</first><last>Zhou</last></author>
      <author><first>Rahul</first><last>Khanna</last></author>
      <author><first>Seyeon</first><last>Lee</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Daniel</first><last>Ho</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>7560–7579</pages>
      <abstract>Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA : Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.</abstract>
      <url hash="17445795">2021.emnlp-main.598</url>
      <bibkey>zhou-etal-2021-rica</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.598</doi>
    </paper>
    <paper id="600">
      <title>MATE : Multi-view Attention for Table Transformer Efficiency<fixed-case>MATE</fixed-case>: Multi-view Attention for Table Transformer Efficiency</title>
      <author><first>Julian</first><last>Eisenschlos</last></author>
      <author><first>Maharshi</first><last>Gor</last></author>
      <author><first>Thomas</first><last>Müller</last></author>
      <author><first>William</first><last>Cohen</last></author>
      <pages>7606–7619</pages>
      <abstract>This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20 % of relational tables on the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a> have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose <a href="https://en.wikipedia.org/wiki/MATE_(software)">MATE</a>, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This <a href="https://en.wikipedia.org/wiki/Computer_architecture">architecture</a> scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate <a href="https://en.wikipedia.org/wiki/Inductive_bias">inductive bias</a> for <a href="https://en.wikipedia.org/wiki/Table_(information)">tabular data</a>, and sets a new <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> for three table reasoning datasets. For HybridQA (Chen et al., 2020), a dataset that involves large documents containing tables, we improve the best prior result by 19 points.</abstract>
      <url hash="91232325">2021.emnlp-main.600</url>
      <bibkey>eisenschlos-etal-2021-mate</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.600</doi>
      <pwccode url="https://github.com/google-research/tapas" additional="false">google-research/tapas</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hybridqa">HybridQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sqa">SQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
    </paper>
    <paper id="602">
      <title>When Attention Meets Fast Recurrence : Training <a href="https://en.wikipedia.org/wiki/Language_model">Language Models</a> with Reduced Compute</title>
      <author><first>Tao</first><last>Lei</last></author>
      <pages>7633–7648</pages>
      <abstract>Large language models have become increasingly difficult to train because of the growing <a href="https://en.wikipedia.org/wiki/Time_complexity">computation time and cost</a>. In this work, we present SRU++, a highly-efficient <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a> that combines fast recurrence and <a href="https://en.wikipedia.org/wiki/Attention">attention</a> for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an <a href="https://en.wikipedia.org/wiki/Supercomputer">8-GPU machine</a>. We further demonstrate that SRU++ requires minimal <a href="https://en.wikipedia.org/wiki/Attention">attention</a> for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.</abstract>
      <url hash="edcf9a70">2021.emnlp-main.602</url>
      <bibkey>lei-2021-attention</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.602</doi>
      <pwccode url="https://github.com/asappresearch/sru" additional="false">asappresearch/sru</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="603">
      <title>Universal-KD : Attention-based Output-Grounded Intermediate Layer Knowledge Distillation<fixed-case>KD</fixed-case>: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation</title>
      <author><first>Yimeng</first><last>Wu</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <author><first>Abbas</first><last>Ghaddar</last></author>
      <author><first>Md Akmal</first><last>Haidar</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <pages>7649–7661</pages>
      <abstract>Intermediate layer matching is shown as an effective approach for improving knowledge distillation (KD). However, this technique applies <a href="https://en.wikipedia.org/wiki/Matching_(graph_theory)">matching</a> in the hidden spaces of two different <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">networks</a> (i.e. student and teacher), which lacks clear interpretability. Moreover, intermediate layer KD can not easily deal with other problems such as layer mapping search and architecture mismatch (i.e. it requires the teacher and student to be of the same model type). To tackle the aforementioned problems all together, we propose Universal-KD to match intermediate layers of the teacher and the student in the output space (by adding pseudo classifiers on intermediate layers) via the attention-based layer projection. By doing this, our unified approach has three merits : (i) it can be flexibly combined with current intermediate layer distillation techniques to improve their results (ii) the pseudo classifiers of the teacher can be deployed instead of extra expensive teacher assistant networks to address the capacity gap problem in KD which is a common issue when the gap between the size of the teacher and student networks becomes too large ; (iii) it can be used in cross-architecture intermediate layer KD. We did comprehensive experiments in distilling BERT-base into BERT-4, RoBERTa-large into DistilRoBERTa and BERT-base into CNN and LSTM-based models. Results on the GLUE tasks show that our approach is able to outperform other KD techniques.</abstract>
      <url hash="2f8f7f56">2021.emnlp-main.603</url>
      <bibkey>wu-etal-2021-universal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.603</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="605">
      <title>Word-Level Coreference Resolution</title>
      <author><first>Vladimir</first><last>Dobrovolskii</last></author>
      <pages>7670–7675</pages>
      <abstract>Recent coreference resolution models rely heavily on span representations to find <a href="https://en.wikipedia.org/wiki/Coreference">coreference links</a> between word spans. As the number of spans is O(n^2) in the length of text and the number of potential links is O(n^4), various pruning techniques are necessary to make this approach computationally feasible. We propose instead to consider <a href="https://en.wikipedia.org/wiki/Coreference">coreference links</a> between individual words rather than word spans and then reconstruct the word spans. This reduces the <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">complexity</a> of the coreference model to O(n^2) and allows it to consider all potential mentions without pruning any of them out. We also demonstrate that, with these changes, SpanBERT for <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> will be significantly outperformed by RoBERTa. While being highly efficient, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performs competitively with recent coreference resolution systems on the OntoNotes benchmark.<tex-math>O(n^2)</tex-math> in the length of text and the number of potential links is <tex-math>O(n^4)</tex-math>, various pruning techniques are necessary to make this approach computationally feasible. We propose instead to consider coreference links between individual words rather than word spans and then reconstruct the word spans. This reduces the complexity of the coreference model to <tex-math>O(n^2)</tex-math> and allows it to consider all potential mentions without pruning any of them out. We also demonstrate that, with these changes, SpanBERT for coreference resolution will be significantly outperformed by RoBERTa. While being highly efficient, our model performs competitively with recent coreference resolution systems on the OntoNotes benchmark.</abstract>
      <url hash="37503770">2021.emnlp-main.605</url>
      <bibkey>dobrovolskii-2021-word</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.605</doi>
      <pwccode url="https://github.com/vdobrovolskii/wl-coref" additional="false">vdobrovolskii/wl-coref</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="611">
      <title>LM-Critic : Language Models for Unsupervised Grammatical Error Correction<fixed-case>LM</fixed-case>-Critic: Language Models for Unsupervised Grammatical Error Correction</title>
      <author><first>Michihiro</first><last>Yasunaga</last></author>
      <author><first>Jure</first><last>Leskovec</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <pages>7752–7763</pages>
      <abstract>Grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs for training, but obtaining such annotation can be prohibitively expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program without any labeled examples, but this relies on a perfect critic (e.g., a compiler) that returns whether an example is valid or not, which does not exist for the GEC task. In this work, we show how to leverage a pretrained language model (LM) in defining an LM-Critic, which judges a sentence to be grammatical if the LM assigns it a higher probability than its local perturbations. We apply this LM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap realistic ungrammatical / grammatical pairs for training a corrector. We evaluate our approach on GEC datasets on multiple domains (CoNLL-2014, BEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing methods in both the <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised setting</a> (+7.7 F0.5) and the <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised setting</a> (+0.5 F0.5).</abstract>
      <url hash="dd0f2b8a">2021.emnlp-main.611</url>
      <bibkey>yasunaga-etal-2021-lm</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.611</doi>
      <pwccode url="https://github.com/michiyasunaga/LM-Critic" additional="true">michiyasunaga/LM-Critic</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gmeg-wiki">GMEG-wiki</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gmeg-yahoo">GMEG-yahoo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
    </paper>
    <paper id="615">
      <title>Come hither or go away? Recognising pre-electoral coalition signals in the news</title>
      <author><first>Ines</first><last>Rehbein</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <author><first>Anna</first><last>Adendorf</last></author>
      <author><first>Oke</first><last>Bahnsen</last></author>
      <author><first>Lukas</first><last>Stoetzer</last></author>
      <author><first>Heiner</first><last>Stuckenschmidt</last></author>
      <pages>7798–7810</pages>
      <abstract>In this paper, we introduce the task of political coalition signal prediction from text, that is, the task of recognizing from the <a href="https://en.wikipedia.org/wiki/News_media">news coverage</a> leading up to an election the (un)willingness of political parties to form a <a href="https://en.wikipedia.org/wiki/Coalition_government">government coalition</a>. We decompose our problem into two related, but distinct tasks : (i) predicting whether a reported statement from a politician or a journalist refers to a potential <a href="https://en.wikipedia.org/wiki/Coalition">coalition</a> and (ii) predicting the polarity of the signal   namely, whether the speaker is in favour of or against the coalition. For this, we explore the benefits of <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> and investigate which setup and task formulation is best suited for each <a href="https://en.wikipedia.org/wiki/Task_(project_management)">sub-task</a>. We evaluate our approach, based on hand-coded newspaper articles, covering elections in three countries (Ireland, <a href="https://en.wikipedia.org/wiki/Germany">Germany</a>, Austria) and two languages (English, German). Our results show that the multi-task learning approach can further improve results over a strong monolingual transfer learning baseline.</abstract>
      <url hash="8a06a6f1">2021.emnlp-main.615</url>
      <bibkey>rehbein-etal-2021-come</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.615</doi>
    </paper>
    <paper id="616">
      <title># HowYouTagTweets : Learning User Hashtagging Preferences via Personalized Topic Attention<fixed-case>H</fixed-case>ow<fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ag<fixed-case>T</fixed-case>weets: Learning User Hashtagging Preferences via Personalized Topic Attention</title>
      <author><first>Yuji</first><last>Zhang</last></author>
      <author><first>Yubo</first><last>Zhang</last></author>
      <author><first>Chunpu</first><last>Xu</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Ziyan</first><last>Jiang</last></author>
      <author><first>Baolin</first><last>Peng</last></author>
      <pages>7811–7820</pages>
      <abstract>Millions of <a href="https://en.wikipedia.org/wiki/Hashtag">hashtags</a> are created on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> every day to cross-refer messages concerning similar topics. To help people find the topics they want to discuss, this paper characterizes a user’s hashtagging preferences via predicting how likely they will post with a <a href="https://en.wikipedia.org/wiki/Hashtag">hashtag</a>. It is hypothesized that one’s interests in a <a href="https://en.wikipedia.org/wiki/Hashtag">hashtag</a> are related with what they said before (user history) and the existing posts present the <a href="https://en.wikipedia.org/wiki/Hashtag">hashtag</a> (hashtag contexts). These factors are married in the deep semantic space built with a pre-trained BERT and a neural topic model via <a href="https://en.wikipedia.org/wiki/Multitask_learning">multitask learning</a>. In this way, user interests learned from the past can be customized to match future <a href="https://en.wikipedia.org/wiki/Hashtag">hashtags</a>, which is beyond the capability of existing <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a> assuming unchanged hashtag semantics. Furthermore, we propose a novel personalized topic attention to capture salient contents to personalize hashtag contexts. Experiments on a large-scale Twitter dataset show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> significantly outperforms the state-of-the-art recommendation approach without exploiting latent topics.</abstract>
      <url hash="76dc4a17">2021.emnlp-main.616</url>
      <bibkey>zhang-etal-2021-howyoutagtweets</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.616</doi>
      <pwccode url="https://github.com/polyusmart/personalized-hashtag-preferences" additional="false">polyusmart/personalized-hashtag-preferences</pwccode>
    </paper>
    <paper id="618">
      <title>Proxy Indicators for the Quality of Open-domain Dialogues</title>
      <author><first>Rostislav</first><last>Nedelchev</last></author>
      <author><first>Jens</first><last>Lehmann</last></author>
      <author><first>Ricardo</first><last>Usbeck</last></author>
      <pages>7834–7855</pages>
      <abstract>The automatic evaluation of open-domain dialogues remains a largely unsolved challenge. Despite the abundance of work done in the field, human judges have to evaluate dialogues’ quality. As a consequence, performing such <a href="https://en.wikipedia.org/wiki/Evaluation">evaluations</a> at scale is usually expensive. This work investigates using a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep-learning model</a> trained on the General Language Understanding Evaluation (GLUE) benchmark to serve as a quality indication of open-domain dialogues. The aim is to use the various GLUE tasks as different perspectives on judging the quality of conversation, thus reducing the need for additional training data or responses that serve as quality references. Due to this nature, the <a href="https://en.wikipedia.org/wiki/Methodology">method</a> can infer various quality metrics and can derive a component-based overall score. We achieve statistically significant <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence">correlation coefficients</a> of up to 0.7.</abstract>
      <url hash="abc74a8a">2021.emnlp-main.618</url>
      <bibkey>nedelchev-etal-2021-proxy</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.618</doi>
      <pwccode url="https://github.com/smartdataanalytics/proxy_indicators" additional="false">smartdataanalytics/proxy_indicators</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/topical-chat">Topical-Chat</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/usr-personachat">USR-PersonaChat</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/usr-topicalchat">USR-TopicalChat</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
    </paper>
    <paper id="619">
      <title>Q^2 : Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and <a href="https://en.wikipedia.org/wiki/Question_answering">Question Answering</a><tex-math>Q^{2}</tex-math>: <fixed-case>E</fixed-case>valuating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering</title>
      <author><first>Or</first><last>Honovich</last></author>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Roee</first><last>Aharoni</last></author>
      <author><first>Ella</first><last>Neeman</last></author>
      <author><first>Idan</first><last>Szpektor</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>7856–7870</pages>
      <abstract>Neural knowledge-grounded generative models for <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a> often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>. Our <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>, denoted Q^2, compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough <a href="https://en.wikipedia.org/wiki/Meta-analysis">meta-evaluation</a> of Q^2 against other <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> using this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and two others, where it consistently shows higher correlation with human judgements.<tex-math>Q^2</tex-math>, compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of <tex-math>Q^2</tex-math> against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements.</abstract>
      <url hash="b7b3f546">2021.emnlp-main.619</url>
      <bibkey>honovich-etal-2021-q2</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.619</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="622">
      <title>Zero-Shot Dialogue State Tracking via Cross-Task Transfer</title>
      <author><first>Zhaojiang</first><last>Lin</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Zhenpeng</first><last>Zhou</last></author>
      <author><first>Paul</first><last>Crook</last></author>
      <author><first>Zhiguang</first><last>Wang</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <author><first>Eunjoon</first><last>Cho</last></author>
      <author><first>Rajen</first><last>Subba</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>7890–7900</pages>
      <abstract>Zero-shot transfer learning for dialogue state tracking (DST) enables us to handle a variety of task-oriented dialogue domains without the expense of collecting in-domain data. In this work, we propose to transfer the cross-task knowledge from general question answering (QA) corpora for the zero-shot DST task. Specifically, we propose TransferQA, a transferable generative QA model that seamlessly combines extractive QA and multi-choice QA via a text-to-text transformer framework, and tracks both categorical slots and non-categorical slots in DST. In addition, we introduce two effective ways to construct unanswerable questions, namely, negative question sampling and context truncation, which enable our model to handle none value slots in the zero-shot DST setting. The extensive experiments show that our approaches substantially improve the existing zero-shot and few-shot results on MultiWoz. Moreover, compared to the fully trained <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> on the Schema-Guided Dialogue dataset, our approach shows better generalization ability in unseen domains.</abstract>
      <url hash="6efc0fb4">2021.emnlp-main.622</url>
      <bibkey>lin-etal-2021-zero</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.622</doi>
      <pwccode url="https://github.com/facebookresearch/Zero-Shot-DST" additional="false">facebookresearch/Zero-Shot-DST</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="623">
      <title>Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance</title>
      <author><first>Carel</first><last>van Niekerk</last></author>
      <author><first>Andrey</first><last>Malinin</last></author>
      <author><first>Christian</first><last>Geishauser</last></author>
      <author><first>Michael</first><last>Heck</last></author>
      <author><first>Hsien-chin</first><last>Lin</last></author>
      <author><first>Nurul</first><last>Lubis</last></author>
      <author><first>Shutong</first><last>Feng</last></author>
      <author><first>Milica</first><last>Gasic</last></author>
      <pages>7901–7914</pages>
      <abstract>The ability to identify and resolve <a href="https://en.wikipedia.org/wiki/Uncertainty">uncertainty</a> is crucial for the <a href="https://en.wikipedia.org/wiki/Robustness_(evolution)">robustness</a> of a <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue system</a>. Indeed, this has been confirmed empirically on <a href="https://en.wikipedia.org/wiki/System">systems</a> that utilise <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian approaches</a> to dialogue belief tracking. However, such <a href="https://en.wikipedia.org/wiki/System">systems</a> consider only <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence estimates</a> and have difficulty scaling to more complex settings. Neural dialogue systems, on the other hand, rarely take <a href="https://en.wikipedia.org/wiki/Uncertainty">uncertainties</a> into account. They are therefore overconfident in their decisions and less robust. Moreover, the performance of the tracking task is often evaluated in isolation, without consideration of its effect on the downstream policy optimisation. We propose the use of different uncertainty measures in neural belief tracking. The effects of these measures on the downstream task of policy optimisation are evaluated by adding selected measures of <a href="https://en.wikipedia.org/wiki/Uncertainty">uncertainty</a> to the <a href="https://en.wikipedia.org/wiki/Feature_space">feature space</a> of the policy and training policies through interaction with a user simulator. Both human and simulated user results show that incorporating these measures leads to improvements both of the performance and of the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of the downstream dialogue policy. This highlights the importance of developing neural dialogue belief trackers that take <a href="https://en.wikipedia.org/wiki/Uncertainty">uncertainty</a> into account.</abstract>
      <url hash="671b4403">2021.emnlp-main.623</url>
      <bibkey>van-niekerk-etal-2021-uncertainty</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.623</doi>
    </paper>
    <paper id="624">
      <title>Dynamic Forecasting of Conversation Derailment</title>
      <author><first>Yova</first><last>Kementchedjhieva</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>7915–7919</pages>
      <abstract>Online conversations can sometimes take a turn for the worse, either due to systematic cultural differences, accidental misunderstandings, or mere malice. Automatically forecasting derailment in public online conversations provides an opportunity to take early action to moderate it. Previous work in this space is limited, and we extend <a href="https://en.wikipedia.org/wiki/It_(pronoun)">it</a> in several ways. We apply a pretrained language encoder to the <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>, which outperforms earlier approaches. We further experiment with shifting the training paradigm for the task from a static to a dynamic one to increase the forecast horizon. This approach shows mixed results : in a high-quality data setting, a longer average forecast horizon can be achieved at the cost of a small drop in F1 ; in a low-quality data setting, however, dynamic training propagates the noise and is highly detrimental to performance.</abstract>
      <url hash="4c08ad5a">2021.emnlp-main.624</url>
      <bibkey>kementchedjhieva-sogaard-2021-dynamic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.624</doi>
    </paper>
    <paper id="628">
      <title>CAPE : Context-Aware Private Embeddings for Private Language Learning<fixed-case>CAPE</fixed-case>: Context-Aware Private Embeddings for Private Language Learning</title>
      <author><first>Richard</first><last>Plant</last></author>
      <author><first>Dimitra</first><last>Gkatzia</last></author>
      <author><first>Valerio</first><last>Giuffrida</last></author>
      <pages>7970–7978</pages>
      <abstract>Neural language models have contributed to state-of-the-art results in a number of downstream applications including <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, intent classification and others. However, obtaining text representations or embeddings using these models risks encoding <a href="https://en.wikipedia.org/wiki/Personal_data">personally identifiable information</a> learned from language and context cues that may lead to privacy leaks. To ameliorate this issue, we propose Context-Aware Private Embeddings (CAPE), a novel approach which combines <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential privacy</a> and <a href="https://en.wikipedia.org/wiki/Adversarial_learning">adversarial learning</a> to preserve <a href="https://en.wikipedia.org/wiki/Privacy">privacy</a> during training of embeddings. Specifically, CAPE firstly applies calibrated noise through <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential privacy</a> to maintain the privacy of text representations by preserving the encoded semantic links while obscuring sensitive information. Next, CAPE employs an adversarial training regime that obscures identified private variables. Experimental results demonstrate that our proposed approach is more effective in reducing private information leakage than either single intervention, with approximately a 3 % reduction in attacker performance compared to the best-performing current method.</abstract>
      <url hash="78618ae7">2021.emnlp-main.628</url>
      <bibkey>plant-etal-2021-cape</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.628</doi>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="629">
      <title>Text Detoxification using Large Pre-trained Neural Models</title>
      <author><first>David</first><last>Dale</last></author>
      <author><first>Anton</first><last>Voronov</last></author>
      <author><first>Daryna</first><last>Dementieva</last></author>
      <author><first>Varvara</first><last>Logacheva</last></author>
      <author><first>Olga</first><last>Kozlova</last></author>
      <author><first>Nikita</first><last>Semenov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>7979–7996</pages>
      <abstract>We present two novel <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a> for eliminating toxicity in text. Our first method combines two recent ideas : (1) guidance of the generation process with small style-conditional language models and (2) use of paraphrasing models to perform style transfer. We use a well-performing <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphraser</a> guided by style-trained language models to keep the text content and remove <a href="https://en.wikipedia.org/wiki/Toxicity">toxicity</a>. Our second method uses BERT to replace toxic words with their non-offensive synonyms. We make the <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> more flexible by enabling BERT to replace mask tokens with a variable number of words. Finally, we present the first large-scale comparative study of style transfer models on the task of toxicity removal. We compare our <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> with a number of <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for style transfer. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are evaluated in a reference-free way using a combination of unsupervised style transfer metrics. Both <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> we suggest yield new SOTA results.</abstract>
      <url hash="b4578d6f">2021.emnlp-main.629</url>
      <bibkey>dale-etal-2021-text</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.629</doi>
      <pwccode url="https://github.com/skoltech-nlp/detox" additional="false">skoltech-nlp/detox</pwccode>
    </paper>
    <paper id="630">
      <title>Document-Level Text Simplification : Dataset, Criteria and Baseline</title>
      <author><first>Renliang</first><last>Sun</last></author>
      <author><first>Hanqi</first><last>Jin</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>7997–8013</pages>
      <abstract>Text simplification is a valuable technique. However, current research is limited to <a href="https://en.wikipedia.org/wiki/Sentence_simplification">sentence simplification</a>. In this paper, we define and investigate a new task of document-level text simplification, which aims to simplify a document consisting of multiple sentences. Based on <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia dumps</a>, we first construct a large-scale dataset named D-Wikipedia and perform analysis and human evaluation on it to show that the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is reliable. Then, we propose a new automatic evaluation metric called D-SARI that is more suitable for the document-level simplification task. Finally, we select several representative models as baseline models for this task and perform automatic evaluation and human evaluation. We analyze the results and point out the shortcomings of the <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline models</a>.</abstract>
      <url hash="c72d2d5a">2021.emnlp-main.630</url>
      <bibkey>sun-etal-2021-document</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.630</doi>
      <pwccode url="https://github.com/rlsnlp/document-level-text-simplification" additional="false">rlsnlp/document-level-text-simplification</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilarge">WikiLarge</pwcdataset>
    </paper>
    <paper id="632">
      <title>Paraphrasing Compound Nominalizations</title>
      <author><first>John</first><last>Lee</last></author>
      <author><first>Ho Hung</first><last>Lim</last></author>
      <author><first>Carol</first><last>Webster</last></author>
      <pages>8023–8028</pages>
      <abstract>A <a href="https://en.wikipedia.org/wiki/Nominalization">nominalization</a> uses a <a href="https://en.wikipedia.org/wiki/Deverbal_noun">deverbal noun</a> to describe an event associated with its underlying verb. Commonly found in academic and formal texts, nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Our goal is to interpret <a href="https://en.wikipedia.org/wiki/Nominalization">nominalizations</a> by generating clausal paraphrases. We address compound nominalizations with both nominal and adjectival modifiers, as well as prepositional phrases. In evaluations on a number of <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a>, we obtained the strongest performance by using a pre-trained contextualized language model to re-rank paraphrase candidates identified by a textual entailment model.</abstract>
      <url hash="0093e445">2021.emnlp-main.632</url>
      <bibkey>lee-etal-2021-paraphrasing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.632</doi>
    </paper>
    <paper id="635">
      <title>TDEER : An Efficient Translating Decoding Schema for Joint Extraction of Entities and Relations<fixed-case>TDEER</fixed-case>: An Efficient Translating Decoding Schema for Joint Extraction of Entities and Relations</title>
      <author><first>Xianming</first><last>Li</last></author>
      <author><first>Xiaotian</first><last>Luo</last></author>
      <author><first>Chenghao</first><last>Dong</last></author>
      <author><first>Daichuan</first><last>Yang</last></author>
      <author><first>Beidi</first><last>Luan</last></author>
      <author><first>Zhen</first><last>He</last></author>
      <pages>8055–8064</pages>
      <abstract>Joint extraction of entities and relations from <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured texts</a> to form factual triples is a fundamental task of constructing a Knowledge Base (KB). A common method is to decode triples by predicting entity pairs to obtain the corresponding <a href="https://en.wikipedia.org/wiki/Binary_relation">relation</a>. However, it is still challenging to handle this <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> efficiently, especially for the overlapping triple problem. To address such a problem, this paper proposes a novel efficient entities and relations extraction model called TDEER, which stands for Translating Decoding Schema for Joint Extraction of Entities and Relations. Unlike the common approaches, the proposed translating decoding schema regards the relation as a translating operation from subject to objects, i.e., TDEER decodes triples as subject + relation   objects. TDEER can naturally handle the overlapping triple problem, because the translating decoding schema can recognize all possible triples, including overlapping and non-overlapping triples. To enhance <a href="https://en.wikipedia.org/wiki/Robust_statistics">model robustness</a>, we introduce negative samples to alleviate <a href="https://en.wikipedia.org/wiki/Errors-in-variables_models">error accumulation</a> at different stages. Extensive experiments on public datasets demonstrate that TDEER produces competitive results compared with the state-of-the-art (SOTA) baselines. Furthermore, the computation complexity analysis indicates that TDEER is more efficient than powerful <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>. Especially, the proposed TDEER is 2 times faster than the recent SOTA models. The code is available at.<b>TDEER</b>, which stands for <b>T</b>ranslating <b>D</b>ecoding Schema for Joint <b>E</b>xtraction of <b>E</b>ntities and <b>R</b>elations. Unlike the common approaches, the proposed translating decoding schema regards the relation as a translating operation from subject to objects, i.e., TDEER decodes triples as <tex-math>subject + relation \rightarrow objects</tex-math>. TDEER can naturally handle the overlapping triple problem, because the translating decoding schema can recognize all possible triples, including overlapping and non-overlapping triples. To enhance model robustness, we introduce negative samples to alleviate error accumulation at different stages. Extensive experiments on public datasets demonstrate that TDEER produces competitive results compared with the state-of-the-art (SOTA) baselines. Furthermore, the computation complexity analysis indicates that TDEER is more efficient than powerful baselines. Especially, the proposed TDEER is 2 times faster than the recent SOTA models. The code is available at <url>https://github.com/4AI/TDEER</url>.</abstract>
      <url hash="566e0c62">2021.emnlp-main.635</url>
      <bibkey>li-etal-2021-tdeer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.635</doi>
      <pwccode url="https://github.com/4ai/tdeer" additional="false">4ai/tdeer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nyt11-hrl">NYT11-HRL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="636">
      <title>Extracting Event Temporal Relations via <a href="https://en.wikipedia.org/wiki/Hyperbolic_geometry">Hyperbolic Geometry</a></title>
      <author><first>Xingwei</first><last>Tan</last></author>
      <author><first>Gabriele</first><last>Pergola</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>8065–8077</pages>
      <abstract>Detecting events and their evolution through time is a crucial task in <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>. Recent neural approaches to event temporal relation extraction typically map events to embeddings in the <a href="https://en.wikipedia.org/wiki/Euclidean_space">Euclidean space</a> and train a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> to detect temporal relations between event pairs. However, embeddings in the <a href="https://en.wikipedia.org/wiki/Euclidean_space">Euclidean space</a> can not capture richer <a href="https://en.wikipedia.org/wiki/Asymmetric_relation">asymmetric relations</a> such as event temporal relations. We thus propose to embed <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">events</a> into hyperbolic spaces, which are intrinsically oriented at modeling <a href="https://en.wikipedia.org/wiki/Hierarchical_organization">hierarchical structures</a>. We introduce two approaches to encode <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">events</a> and their temporal relations in hyperbolic spaces. One approach leverages hyperbolic embeddings to directly infer <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">event relations</a> through simple <a href="https://en.wikipedia.org/wiki/Operation_(mathematics)">geometrical operations</a>. In the second one, we devise an <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end architecture</a> composed of hyperbolic neural units tailored for the temporal relation extraction task. Thorough experimental assessments on widely used datasets have shown the benefits of revisiting the tasks on a different <a href="https://en.wikipedia.org/wiki/Space">geometrical space</a>, resulting in state-of-the-art performance on several standard <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>. Finally, the ablation study and several qualitative analyses highlighted the rich event semantics implicitly encoded into hyperbolic spaces.</abstract>
      <url hash="298bb25e">2021.emnlp-main.636</url>
      <bibkey>tan-etal-2021-extracting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.636</doi>
      <pwccode url="https://github.com/xingwei-warwick/hyper-event-temprel" additional="false">xingwei-warwick/hyper-event-temprel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tcr">TCR</pwcdataset>
    </paper>
    <paper id="637">
      <title>Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention</title>
      <author><first>Jiawei</first><last>Chen</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>8078–8088</pages>
      <abstract>Event detection has long been troubled by the trigger curse : overfitting the trigger will harm the generalization ability while underfitting it will hurt the <a href="https://en.wikipedia.org/wiki/Detection">detection</a> performance. This <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> is even more severe in few-shot scenario. In this paper, we identify and solve the trigger curse problem in few-shot event detection (FSED) from a causal view. By formulating FSED with a structural causal model (SCM), we found that the trigger is a confounder of the context and the result, which makes previous FSED methods much easier to overfit triggers. To resolve this problem, we propose to intervene on the context via backdoor adjustment during <a href="https://en.wikipedia.org/wiki/Training">training</a>. Experiments show that our method significantly improves the FSED on both ACE05 and MAVEN datasets.</abstract>
      <url hash="c64bb571">2021.emnlp-main.637</url>
      <bibkey>chen-etal-2021-honey</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.637</doi>
      <pwccode url="https://github.com/chen700564/causalfsed" additional="false">chen700564/causalfsed</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/maven">MAVEN</pwcdataset>
    </paper>
    <paper id="639">
      <title>Time-dependent Entity Embedding is not All You Need : A Re-evaluation of Temporal Knowledge Graph Completion Models under a Unified Framework</title>
      <author><first>Zhen</first><last>Han</last></author>
      <author><first>Gengyuan</first><last>Zhang</last></author>
      <author><first>Yunpu</first><last>Ma</last></author>
      <author><first>Volker</first><last>Tresp</last></author>
      <pages>8104–8118</pages>
      <abstract>Various temporal knowledge graph (KG) completion models have been proposed in the recent literature. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> usually contain two parts, a temporal embedding layer and a <a href="https://en.wikipedia.org/wiki/Score_(statistics)">score function</a> derived from existing static KG modeling approaches. Since the approaches differ along several dimensions, including different <a href="https://en.wikipedia.org/wiki/Score_(statistics)">score functions</a> and <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training strategies</a>, the individual contributions of different temporal embedding techniques to model performance are not always clear. In this work, we systematically study six temporal embedding approaches and empirically quantify their performance across a wide range of configurations with about 3000 experiments and 13159 GPU hours. We classify the temporal embeddings into two classes : (1) timestamp embeddings and (2) time-dependent entity embeddings. Despite the common belief that the latter is more expressive, an extensive experimental study shows that timestamp embeddings can achieve on-par or even better performance with significantly fewer parameters. Moreover, we find that when trained appropriately, the relative performance differences between various temporal embeddings often shrink and sometimes even reverse when compared to prior results. For example, TTransE (CITATION), one of the first temporal KG models, can outperform more recent architectures on ICEWS datasets. To foster further research, we provide the first unified open-source framework for temporal KG completion models with full composability, where temporal embeddings, score functions, loss functions, regularizers, and the explicit modeling of reciprocal relations can be combined arbitrarily.</abstract>
      <url hash="2f6c17f9">2021.emnlp-main.639</url>
      <bibkey>han-etal-2021-time</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.639</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/icews">ICEWS</pwcdataset>
    </paper>
    <paper id="643">
      <title>Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution</title>
      <author><first>Yi</first><last>Huang</last></author>
      <author><first>Buse</first><last>Giledereli</last></author>
      <author><first>Abdullatif</first><last>Köksal</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <author><first>Elif</first><last>Ozkirimli</last></author>
      <pages>8153–8161</pages>
      <abstract>Multi-label text classification is a challenging task because <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Resampling and re-weighting are common approaches used for addressing the <a href="https://en.wikipedia.org/wiki/Social_class">class imbalance problem</a>, however, they are not effective when there is label dependency besides <a href="https://en.wikipedia.org/wiki/Social_class">class imbalance</a> because they result in oversampling of common labels. Here, we introduce the application of balancing loss functions for multi-label text classification. We perform experiments on a general domain dataset with 90 labels (Reuters-21578) and a domain-specific dataset from <a href="https://en.wikipedia.org/wiki/PubMed">PubMed</a> with 18211 labels. We find that a distribution-balanced loss function, which inherently addresses both the class imbalance and label linkage problems, outperforms commonly used <a href="https://en.wikipedia.org/wiki/Loss_function">loss functions</a>. Distribution balancing methods have been successfully used in the image recognition field. Here, we show their effectiveness in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. Source code is available at https://github.com/blessu/BalancedLossNLP.</abstract>
      <url hash="d8f1174f">2021.emnlp-main.643</url>
      <bibkey>huang-etal-2021-balancing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.643</doi>
      <pwccode url="https://github.com/Roche/BalancedLossNLP" additional="true">Roche/BalancedLossNLP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/reuters-21578">Reuters-21578</pwcdataset>
    </paper>
    <paper id="644">
      <title>Bayesian Topic Regression for Causal Inference<fixed-case>B</fixed-case>ayesian Topic Regression for Causal Inference</title>
      <author><first>Maximilian</first><last>Ahrens</last></author>
      <author><first>Julian</first><last>Ashwin</last></author>
      <author><first>Jan-Peter</first><last>Calliess</last></author>
      <author><first>Vu</first><last>Nguyen</last></author>
      <pages>8162–8188</pages>
      <abstract>Causal inference using observational text data is becoming increasingly popular in many research areas. This paper presents the Bayesian Topic Regression (BTR) model that uses both text and numerical information to model an outcome variable. It allows estimation of both discrete and continuous treatment effects. Furthermore, it allows for the inclusion of additional numerical confounding factors next to text data. To this end, we combine a supervised Bayesian topic model with a Bayesian regression framework and perform supervised representation learning for the text features jointly with the regression parameter training, respecting the <a href="https://en.wikipedia.org/wiki/Frisch–Waugh–Lovell_theorem">Frisch-Waugh-Lovell theorem</a>. Our paper makes two main contributions. First, we provide a <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression framework</a> that allows <a href="https://en.wikipedia.org/wiki/Causal_inference">causal inference</a> in settings when both text and numerical confounders are of relevance. We show with synthetic and semi-synthetic datasets that our joint approach recovers ground truth with lower bias than any benchmark model, when text and numerical features are correlated. Second, experiments on two real-world datasets demonstrate that a joint and supervised learning strategy also yields superior prediction results compared to strategies that estimate regression weights for text and non-text features separately, being even competitive with more complex deep neural networks.</abstract>
      <url hash="be683200">2021.emnlp-main.644</url>
      <bibkey>ahrens-etal-2021-bayesian</bibkey>
      <revision id="1" href="2021.emnlp-main.644v1" hash="357e4b70" />
      <revision id="2" href="2021.emnlp-main.644v2" hash="be683200" date="2021-11-29">Corrects a small typo in the text and in an unnumbered equation.</revision>
      <doi>10.18653/v1/2021.emnlp-main.644</doi>
      <pwccode url="https://github.com/maximilianahrens/data" additional="false">maximilianahrens/data</pwccode>
    </paper>
    <paper id="646">
      <title>What’s in Your Head? Emergent Behaviour in Multi-Task Transformer Models<fixed-case>W</fixed-case>hat’s in Your Head? <fixed-case>E</fixed-case>mergent Behaviour in Multi-Task Transformer Models</title>
      <author><first>Mor</first><last>Geva</last></author>
      <author><first>Uri</first><last>Katz</last></author>
      <author><first>Aviv</first><last>Ben-Arie</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>8201–8215</pages>
      <abstract>The primary paradigm for multi-task training in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this work, we examine the behaviour of non-target heads, that is, the output of heads when given input that belongs to a different task than the one they were trained for. We find that non-target heads exhibit <a href="https://en.wikipedia.org/wiki/Emergence">emergent behaviour</a>, which may either explain the target task, or generalize beyond their original task. For example, in a numerical reasoning task, a span extraction head extracts from the input the arguments to a computation that results in a number generated by a target generative head. In addition, a summarization head that is trained with a target question answering head, outputs query-based summaries when given a question and a context from which the answer is to be extracted. This <a href="https://en.wikipedia.org/wiki/Emergence">emergent behaviour</a> suggests that multi-task training leads to non-trivial extrapolation of skills, which can be harnessed for interpretability and <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a>.</abstract>
      <url hash="d8f88eed">2021.emnlp-main.646</url>
      <bibkey>geva-etal-2021-whats</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.646</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="647">
      <title>Do n’t Search for a Search Method   Simple <a href="https://en.wikipedia.org/wiki/Heuristic">Heuristics</a> Suffice for Adversarial Text Attacks</title>
      <author><first>Nathaniel</first><last>Berger</last></author>
      <author><first>Stefan</first><last>Riezler</last></author>
      <author><first>Sebastian</first><last>Ebert</last></author>
      <author><first>Artem</first><last>Sokolov</last></author>
      <pages>8216–8224</pages>
      <abstract>Recently more attention has been given to adversarial attacks on <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP)</a>. A central research topic has been the investigation of <a href="https://en.wikipedia.org/wiki/Search_algorithm">search algorithms</a> and search constraints, accompanied by <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark algorithms</a> and <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a>. We implement an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> inspired by zeroth order optimization-based attacks and compare with the benchmark results in the TextAttack framework. Surprisingly, we find that optimization-based methods do not yield any improvement in a constrained setup and slightly benefit from approximate gradient information only in unconstrained setups where search spaces are larger. In contrast, simple <a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)">heuristics</a> exploiting <a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search">nearest neighbors</a> without querying the target function yield substantial success rates in constrained setups, and nearly full success rate in unconstrained setups, at an order of magnitude fewer queries. We conclude from these results that current TextAttack benchmark tasks are too easy and constraints are too strict, preventing meaningful research on black-box adversarial text attacks.</abstract>
      <url hash="103f9e6a">2021.emnlp-main.647</url>
      <bibkey>berger-etal-2021-dont</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.647</doi>
    </paper>
    <paper id="648">
      <title>Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods</title>
      <author><first>Peru</first><last>Bhardwaj</last></author>
      <author><first>John</first><last>Kelleher</last></author>
      <author><first>Luca</first><last>Costabello</last></author>
      <author><first>Declan</first><last>O’Sullivan</last></author>
      <pages>8225–8239</pages>
      <abstract>Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the <a href="https://en.wikipedia.org/wiki/Vulnerability_(computing)">security vulnerabilities</a> that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training time to cause model failure at test time. To select adversarial deletions, we propose to use the model-agnostic instance attribution methods from Interpretable Machine Learning, which identify the training instances that are most influential to a neural model’s predictions on test instances. We use these influential triples as adversarial deletions. We further propose a <a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)">heuristic method</a> to replace one of the two entities in each influential triple to generate adversarial additions. Our experiments show that the proposed strategies outperform the state-of-art data poisoning attacks on KGE models and improve the MRR degradation due to the attacks by up to 62 % over the baselines.</abstract>
      <url hash="96f19b38">2021.emnlp-main.648</url>
      <bibkey>bhardwaj-etal-2021-adversarial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.648</doi>
      <pwccode url="https://github.com/perubhardwaj/attributionattack" additional="false">perubhardwaj/attributionattack</pwccode>
    </paper>
    <paper id="649">
      <title>Locke’s Holiday : Belief Bias in Machine Reading</title>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>8240–8245</pages>
      <abstract>I highlight a simple <a href="https://en.wikipedia.org/wiki/Failure_mode">failure mode</a> of state-of-the-art machine reading systems : when contexts do not align with commonly shared beliefs. For example, machine reading systems fail to answer What did Elizabeth want? correctly in the context of ‘My kingdom for a cough drop, cried Queen Elizabeth.’ Biased by co-occurrence statistics in the training data of pretrained language models, systems predict my kingdom, rather than a cough drop. I argue such biases are analogous to human belief biases and present a carefully designed challenge dataset for English machine reading, called Auto-Locke, to quantify such effects. Evaluations of <a href="https://en.wikipedia.org/wiki/Machine_reading">machine reading systems</a> on Auto-Locke show the pervasiveness of <a href="https://en.wikipedia.org/wiki/Belief_bias">belief bias</a> in <a href="https://en.wikipedia.org/wiki/Machine_reading">machine reading</a>.<i>What did Elizabeth want?</i> correctly in the context of ‘My kingdom for a cough drop, cried Queen Elizabeth.’ Biased by co-occurrence statistics in the training data of pretrained language models, systems predict <i>my kingdom</i>, rather than <i>a cough drop</i>. I argue such biases are analogous to human belief biases and present a carefully designed challenge dataset for English machine reading, called Auto-Locke, to quantify such effects. Evaluations of machine reading systems on Auto-Locke show the pervasiveness of belief bias in machine reading.</abstract>
      <url hash="0494c636">2021.emnlp-main.649</url>
      <bibkey>sogaard-2021-lockes</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.649</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="650">
      <title>Sequence Length is a Domain : Length-based Overfitting in Transformer Models</title>
      <author><first>Dusan</first><last>Varis</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>8246–8257</pages>
      <abstract>Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> during training. In practice, this is usually countered either by applying <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization methods</a> (e.g. dropout, L2-regularization) or by providing huge amounts of <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data</a>. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017). We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing tasks and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.</abstract>
      <url hash="55a0b05d">2021.emnlp-main.650</url>
      <bibkey>varis-bojar-2021-sequence</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.650</doi>
    </paper>
    <paper id="658">
      <title>Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs</title>
      <author><first>Zhen</first><last>Han</last></author>
      <author><first>Zifeng</first><last>Ding</last></author>
      <author><first>Yunpu</first><last>Ma</last></author>
      <author><first>Yujia</first><last>Gu</last></author>
      <author><first>Volker</first><last>Tresp</last></author>
      <pages>8352–8364</pages>
      <abstract>There has been an increasing interest in inferring future links on temporal knowledge graphs (KG). While links on temporal KGs vary continuously over time, the existing approaches model the temporal KGs in discrete state spaces. To this end, we propose a novel continuum model by extending the idea of neural ordinary differential equations (ODEs) to multi-relational graph convolutional networks. The proposed model preserves the continuous nature of dynamic multi-relational graph data and encodes both temporal and structural information into continuous-time dynamic embeddings. In addition, a novel graph transition layer is applied to capture the transitions on the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">dynamic graph</a>, i.e., <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">edge formation</a> and dissolution. We perform extensive experiments on five benchmark datasets for temporal KG reasoning, showing our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s superior performance on the future link forecasting task.</abstract>
      <url hash="0bcf4aaf">2021.emnlp-main.658</url>
      <bibkey>han-etal-2021-learning-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.658</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/icews">ICEWS</pwcdataset>
    </paper>
    <paper id="661">
      <title>A Strong Baseline for Query Efficient Attacks in a Black Box Setting</title>
      <author><first>Rishabh</first><last>Maheshwary</last></author>
      <author><first>Saket</first><last>Maheshwary</last></author>
      <author><first>Vikram</first><last>Pudi</last></author>
      <pages>8396–8409</pages>
      <abstract>Existing black box search methods have achieved high success rate in generating adversarial attacks against NLP models. However, such search methods are inefficient as they do not consider the amount of queries required to generate adversarial attacks. Also, prior attacks do not maintain a consistent search space while comparing different <a href="https://en.wikipedia.org/wiki/Search_algorithm">search methods</a>. In this paper, we propose a query efficient attack strategy to generate plausible adversarial examples on text classification and entailment tasks. Our attack jointly leverages <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a> and locality sensitive hashing (LSH) to reduce the query count. We demonstrate the efficacy of our approach by comparing our attack with four <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> across three different <a href="https://en.wikipedia.org/wiki/Feasible_region">search spaces</a>. Further, we benchmark our results across the same <a href="https://en.wikipedia.org/wiki/Feasible_region">search space</a> used in prior attacks. In comparison to <a href="https://en.wikipedia.org/wiki/Cyberattack">attacks</a> proposed, on an average, we are able to reduce the query count by 75 % across all <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and target models. We also demonstrate that our <a href="https://en.wikipedia.org/wiki/Cyberattack">attack</a> achieves a higher success rate when compared to prior attacks in a limited query setting.</abstract>
      <url hash="950f6912">2021.emnlp-main.661</url>
      <bibkey>maheshwary-etal-2021-strong</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.661</doi>
      <pwccode url="https://github.com/rishabhmaheshwary/query-attack" additional="false">rishabhmaheshwary/query-attack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="667">
      <title>Language Modeling, Lexical Translation, Reordering : The Training Process of NMT through the Lens of Classical SMT<fixed-case>NMT</fixed-case> through the Lens of Classical <fixed-case>SMT</fixed-case></title>
      <author><first>Elena</first><last>Voita</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>8478–8491</pages>
      <abstract>Differently from the traditional <a href="https://en.wikipedia.org/wiki/Machine_translation">statistical MT</a> that decomposes the translation task into distinct separately learned components, <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> uses a single <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> to model the entire translation process. Despite <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> in traditional <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation">SMT</a>. In this work, we look at the competences related to three core SMT components and find that during training, NMT first focuses on learning target-side language modeling, then improves translation quality approaching word-by-word translation, and finally learns more complicated reordering patterns. We show that this behavior holds for several models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla non-autoregressive neural machine translation by guiding teacher model selection.</abstract>
      <url hash="c13d7926">2021.emnlp-main.667</url>
      <bibkey>voita-etal-2021-language</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.667</doi>
    </paper>
    <paper id="670">
      <title>Wino-X : Multilingual Winograd Schemas for Commonsense Reasoning and Coreference Resolution<fixed-case>X</fixed-case>: Multilingual <fixed-case>W</fixed-case>inograd Schemas for Commonsense Reasoning and Coreference Resolution</title>
      <author><first>Denis</first><last>Emelin</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>8517–8532</pages>
      <abstract>Winograd schemas are a well-established tool for evaluating coreference resolution (CoR) and commonsense reasoning (CSR) capabilities of computational models. So far, schemas remained largely confined to <a href="https://en.wikipedia.org/wiki/English_language">English</a>, limiting their utility in multilingual settings. This work presents Wino-X, a parallel dataset of German, French, and Russian schemas, aligned with their English counterparts. We use this resource to investigate whether neural machine translation (NMT) models can perform CoR that requires commonsense knowledge and whether multilingual language models (MLLMs) are capable of CSR across multiple languages. Our findings show Wino-X to be exceptionally challenging for NMT systems that are prone to undesirable biases and unable to detect disambiguating information. We quantify biases using established <a href="https://en.wikipedia.org/wiki/Statistics">statistical methods</a> and define ways to address both of these issues. We furthermore present evidence of active cross-lingual knowledge transfer in MLLMs, whereby fine-tuning models on English schemas yields CSR improvements in other languages.</abstract>
      <url hash="8c685c97">2021.emnlp-main.670</url>
      <attachment type="Software" hash="837e5225">2021.emnlp-main.670.Software.zip</attachment>
      <bibkey>emelin-sennrich-2021-wino</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.670</doi>
      <pwccode url="https://github.com/demelin/wino-x" additional="false">demelin/wino-x</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="671">
      <title>One Source, Two Targets : Challenges and Rewards of Dual Decoding<fixed-case>C</fixed-case>hallenges and Rewards of Dual Decoding</title>
      <author><first>Jitao</first><last>Xu</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>8533–8546</pages>
      <abstract>Machine translation is generally understood as generating one target text from an input source document. In this paper, we consider a stronger requirement : to jointly generate two texts so that each output side effectively depends on the other. As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text. We present an analysis of possible implementations of dual decoding, and experiment with four <a href="https://en.wikipedia.org/wiki/Application_software">applications</a>. Viewing the problem from multiple angles allows us to better highlight the challenges of dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations.</abstract>
      <url hash="20e34fa8">2021.emnlp-main.671</url>
      <bibkey>xu-yvon-2021-one</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.671</doi>
      <pwccode url="https://github.com/jitao-xu/dual-decoding" additional="false">jitao-xu/dual-decoding</pwccode>
    </paper>
    <paper id="678">
      <title>Cross-Policy Compliance Detection via Question Answering</title>
      <author><first>Marzieh</first><last>Saeidi</last></author>
      <author><first>Majid</first><last>Yazdani</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>8622–8632</pages>
      <abstract>Policy compliance detection is the task of ensuring that a scenario conforms to a <a href="https://en.wikipedia.org/wiki/Policy">policy</a> (e.g. a claim is valid according to government rules or a post in an online platform conforms to community guidelines). This task has been previously instantiated as a form of <a href="https://en.wikipedia.org/wiki/Textual_entailment">textual entailment</a>, which results in poor <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> due to the <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> of the <a href="https://en.wikipedia.org/wiki/Policy">policies</a>. In this paper we propose to address <a href="https://en.wikipedia.org/wiki/Policy">policy compliance detection</a> via decomposing it into <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, where questions check whether the conditions stated in the <a href="https://en.wikipedia.org/wiki/Policy">policy</a> apply to the scenario, and an <a href="https://en.wikipedia.org/wiki/Expression_tree">expression tree</a> combines the answers to obtain the label. Despite the initial upfront annotation cost, we demonstrate that this approach results in better <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, especially in the cross-policy setup where the <a href="https://en.wikipedia.org/wiki/Policy">policies</a> during testing are unseen in training. In addition, it allows us to use existing <a href="https://en.wikipedia.org/wiki/Question_answering">question answering models</a> pre-trained on existing <a href="https://en.wikipedia.org/wiki/Data_set">large datasets</a>. Finally, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> explicitly identifies the information missing from a scenario in case policy compliance can not be determined. We conduct our experiments using a recent dataset consisting of government policies, which we augment with expert annotations and find that the cost of annotating question answering decomposition is largely offset by improved inter-annotator agreement and speed.</abstract>
      <url hash="e4acd2d5">2021.emnlp-main.678</url>
      <bibkey>saeidi-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.678</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sharc">ShARC</pwcdataset>
    </paper>
    <paper id="680">
      <title>Unsupervised Multi-View Post-OCR Error Correction With Language Models<fixed-case>OCR</fixed-case> Error Correction With Language Models</title>
      <author><first>Harsh</first><last>Gupta</last></author>
      <author><first>Luciano</first><last>Del Corro</last></author>
      <author><first>Samuel</first><last>Broscheit</last></author>
      <author><first>Johannes</first><last>Hoffart</last></author>
      <author><first>Eliot</first><last>Brenner</last></author>
      <pages>8647–8652</pages>
      <abstract>We investigate post-OCR correction in a setting where we have access to different <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">OCR views</a> of the same document. The goal of this study is to understand if a pretrained language model (LM) can be used in an unsupervised way to reconcile the different OCR views such that their combination contains fewer errors than each individual view. This approach is motivated by scenarios in which unconstrained text generation for <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">error correction</a> is too risky. We evaluated different pretrained LMs on two <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and found significant gains in realistic scenarios with up to 15 % <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">WER</a> improvement over the best OCR view. We also show the importance of <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> for post-OCR correction on out-of-domain documents.</abstract>
      <url hash="b47ab55e">2021.emnlp-main.680</url>
      <bibkey>gupta-etal-2021-unsupervised-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.680</doi>
    </paper>
    <paper id="682">
      <title>BERT-Beta : A Proactive Probabilistic Approach to Text Moderation<fixed-case>BERT</fixed-case>-Beta: A Proactive Probabilistic Approach to Text Moderation</title>
      <author><first>Fei</first><last>Tan</last></author>
      <author><first>Yifan</first><last>Hu</last></author>
      <author><first>Kevin</first><last>Yen</last></author>
      <author><first>Changwei</first><last>Hu</last></author>
      <pages>8667–8675</pages>
      <abstract>Text moderation for <a href="https://en.wikipedia.org/wiki/User-generated_content">user generated content</a>, which helps to promote healthy interaction among users, has been widely studied and many machine learning models have been proposed. In this work, we explore an alternative perspective by augmenting reactive reviews with proactive forecasting. Specifically, we propose a new concept text toxicity propensity to characterize the extent to which a text tends to attract toxic comments. Beta regression is then introduced to do the probabilistic modeling, which is demonstrated to function well in comprehensive experiments. We also propose an explanation method to communicate the model decision clearly. Both propensity scoring and <a href="https://en.wikipedia.org/wiki/Interpretation_(logic)">interpretation</a> benefit text moderation in a novel manner. Finally, the proposed scaling mechanism for the <a href="https://en.wikipedia.org/wiki/Linear_model">linear model</a> offers useful insights beyond this work.</abstract>
      <url hash="2a0644f6">2021.emnlp-main.682</url>
      <bibkey>tan-etal-2021-bert</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.682</doi>
    </paper>
    <paper id="685">
      <title>CodeT5 : Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation<fixed-case>C</fixed-case>ode<fixed-case>T</fixed-case>5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation</title>
      <author><first>Yue</first><last>Wang</last></author>
      <author><first>Weishi</first><last>Wang</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Steven C.H.</first><last>Hoi</last></author>
      <pages>8696–8708</pages>
      <abstract>Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>. Besides, we propose a novel identifier-aware pre-training task that enables the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.</abstract>
      <url hash="56481874">2021.emnlp-main.685</url>
      <bibkey>wang-etal-2021-codet5</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.685</doi>
      <pwccode url="https://github.com/salesforce/codet5" additional="false">salesforce/codet5</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/concode">CONCODE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codexglue">CodeXGLUE</pwcdataset>
    </paper>
    <paper id="686">
      <title>Detect and Classify   Joint Span Detection and Classification for Health Outcomes</title>
      <author><first>Micheal</first><last>Abaho</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <author><first>Paula</first><last>Williamson</last></author>
      <author><first>Susanna</first><last>Dodd</last></author>
      <pages>8709–8721</pages>
      <abstract>A <a href="https://en.wikipedia.org/wiki/Outcome_(probability)">health outcome</a> is a measurement or an observation used to capture and assess the effect of a treatment. Automatic detection of health outcomes from text would undoubtedly speed up access to evidence necessary in healthcare decision making. Prior work on outcome detection has modelled this task as either (a) a sequence labelling task, where the goal is to detect which text spans describe health outcomes, or (b) a classification task, where the goal is to classify a text into a predefined set of categories depending on an outcome that is mentioned somewhere in that text. However, this decoupling of span detection and <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> is problematic from a modelling perspective and ignores global structural correspondences between sentence-level and word-level information present in a given text. To address this, we propose a method that uses both word-level and sentence-level information to simultaneously perform outcome span detection and outcome type classification. In addition to injecting contextual information to hidden vectors, we use label attention to appropriately weight both word and sentence level information. Experimental results on several benchmark datasets for health outcome detection show that our proposed method consistently outperforms decoupled methods, reporting competitive results.</abstract>
      <url hash="47258cf7">2021.emnlp-main.686</url>
      <attachment type="Software" hash="ff027604">2021.emnlp-main.686.Software.zip</attachment>
      <bibkey>abaho-etal-2021-detect</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.686</doi>
      <pwccode url="https://github.com/MichealAbaho/Label-Context-Aware-Attention-Model" additional="false">MichealAbaho/Label-Context-Aware-Attention-Model</pwccode>
    </paper>
    <paper id="687">
      <title>Multi-Class Grammatical Error Detection for Correction : A Tale of Two Systems<fixed-case>M</fixed-case>ulti-Class Grammatical Error Detection for Correction: <fixed-case>A</fixed-case> Tale of Two Systems</title>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>Shiva</first><last>Taslimipoor</last></author>
      <author><first>Christopher</first><last>Davis</last></author>
      <author><first>Christopher</first><last>Bryant</last></author>
      <pages>8722–8736</pages>
      <abstract>In this paper, we show how a multi-class grammatical error detection (GED) system can be used to improve grammatical error correction (GEC) for <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Specifically, we first develop a new state-of-the-art binary detection system based on pre-trained ELECTRA, and then extend it to multi-class detection using different error type tagsets derived from the ERRANT framework. Output from this detection system is used as auxiliary input to fine-tune a novel encoder-decoder GEC model, and we subsequently re-rank the N-best GEC output to find the hypothesis that most agrees with the GED output. Results show that fine-tuning the GEC system using 4-class GED produces the best model, but re-ranking using 55-class GED leads to the best performance overall. This suggests that different multi-class GED systems benefit GEC in different ways. Ultimately, our system outperforms all other previous work that combines <a href="https://en.wikipedia.org/wiki/General_Educational_Development">GED</a> and GEC, and achieves a new single-model NMT-based state of the art on the BEA-test benchmark.</abstract>
      <url hash="5d98ddfc">2021.emnlp-main.687</url>
      <bibkey>yuan-etal-2021-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.687</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="688">
      <title>Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models</title>
      <author><first>Tassilo</first><last>Klein</last></author>
      <author><first>Moin</first><last>Nabi</last></author>
      <pages>8737–8743</pages>
      <abstract>Can we get existing <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> and refine them for zero-shot commonsense reasoning? This paper presents an initial study exploring the feasibility of zero-shot commonsense reasoning for the <a href="https://en.wikipedia.org/wiki/Winograd_Schema_Challenge">Winograd Schema Challenge</a> by formulating the task as self-supervised refinement of a pre-trained language model. In contrast to previous studies that rely on fine-tuning annotated datasets, we seek to boost conceptualization via loss landscape refinement. To this end, we propose a novel self-supervised learning approach that refines the <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> utilizing a set of linguistic perturbations of similar concept relationships. Empirical analysis of our conceptually simple framework demonstrates the viability of zero-shot commonsense reasoning on multiple benchmarks.</abstract>
      <url hash="0f483603">2021.emnlp-main.688</url>
      <bibkey>klein-nabi-2021-towards</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.688</doi>
      <pwccode url="https://github.com/sap-samples/emnlp2021-contrastive-refinement" additional="false">sap-samples/emnlp2021-contrastive-refinement</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="689">
      <title>To Share or not to Share : Predicting Sets of Sources for Model Transfer Learning<fixed-case>P</fixed-case>redicting Sets of Sources for Model Transfer Learning</title>
      <author><first>Lukas</first><last>Lange</last></author>
      <author><first>Jannik</first><last>Strötgen</last></author>
      <author><first>Heike</first><last>Adel</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>8744–8753</pages>
      <abstract>In low-resource settings, model transfer can help to overcome a lack of labeled data for many tasks and domains. However, predicting useful transfer sources is a challenging problem, as even the most similar sources might lead to unexpected negative transfer results. Thus, ranking methods based on task and text similarity   as suggested in prior work   may not be sufficient to identify promising sources. To tackle this problem, we propose a new approach to automatically determine which and how many sources should be exploited. For this, we study the effects of model transfer on <a href="https://en.wikipedia.org/wiki/Sequence_labeling">sequence labeling</a> across various domains and tasks and show that our methods based on model similarity and support vector machines are able to predict promising sources, resulting in performance increases of up to 24 F1 points.</abstract>
      <url hash="787d85a3">2021.emnlp-main.689</url>
      <bibkey>lange-etal-2021-share</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.689</doi>
      <pwccode url="https://github.com/boschresearch/predicting_sets_of_sources" additional="false">boschresearch/predicting_sets_of_sources</pwccode>
    </paper>
    <paper id="690">
      <title>Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting : Phenotype Annotation Use Case</title>
      <author><first>Jingqing</first><last>Zhang</last></author>
      <author><first>Luis</first><last>Bolanos Trujillo</last></author>
      <author><first>Tong</first><last>Li</last></author>
      <author><first>Ashwani</first><last>Tanwar</last></author>
      <author><first>Guilherme</first><last>Freire</last></author>
      <author><first>Xian</first><last>Yang</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Vibhor</first><last>Gupta</last></author>
      <author><first>Yike</first><last>Guo</last></author>
      <pages>8754–8769</pages>
      <abstract>Contextualised word embeddings is a powerful tool to detect contextual synonyms. However, most of the current state-of-the-art (SOTA) deep learning concept extraction methods remain supervised and underexploit the potential of the context. In this paper, we propose a self-supervised pre-training approach which is able to detect contextual synonyms of concepts being training on the data created by shallow matching. We apply our <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> in the sparse multi-class setting (over 15,000 concepts) to extract phenotype information from <a href="https://en.wikipedia.org/wiki/Electronic_health_record">electronic health records</a>. We further investigate data augmentation techniques to address the problem of the class sparsity. Our approach achieves a new SOTA for the unsupervised phenotype concept annotation on clinical text on F1 and <a href="https://en.wikipedia.org/wiki/Recall_(memory)">Recall</a> outperforming the previous SOTA with a gain of up to 4.5 and 4.0 absolute points, respectively. After <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> with as little as 20 % of the labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic evaluation on three ICU benchmarks also shows the benefit of using the <a href="https://en.wikipedia.org/wiki/Phenotype">phenotypes</a> annotated by our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> as <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>.</abstract>
      <url hash="c63323dc">2021.emnlp-main.690</url>
      <bibkey>zhang-etal-2021-self</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.690</doi>
    </paper>
    <paper id="691">
      <title>ClauseRec : A Clause Recommendation Framework for AI-aided Contract Authoring<fixed-case>C</fixed-case>lause<fixed-case>R</fixed-case>ec: A Clause Recommendation Framework for <fixed-case>AI</fixed-case>-aided Contract Authoring</title>
      <author><first>Vinay</first><last>Aggarwal</last></author>
      <author><first>Aparna</first><last>Garimella</last></author>
      <author><first>Balaji Vasan</first><last>Srinivasan</last></author>
      <author><first>Anandhavelu</first><last>N</last></author>
      <author><first>Rajiv</first><last>Jain</last></author>
      <pages>8770–8776</pages>
      <abstract>Contracts are a common type of <a href="https://en.wikipedia.org/wiki/Legal_document">legal document</a> that frequent in several day-to-day business workflows. However, there has been very limited NLP research in processing such <a href="https://en.wikipedia.org/wiki/Document">documents</a>, and even lesser in generating them. These <a href="https://en.wikipedia.org/wiki/Contract">contracts</a> are made up of clauses, and the unique nature of these <a href="https://en.wikipedia.org/wiki/Clause">clauses</a> calls for specific methods to understand and generate such <a href="https://en.wikipedia.org/wiki/Document">documents</a>. In this paper, we introduce the task of clause recommendation, as a first step to aid and accelerate the authoring of contract documents. We propose a two-staged pipeline to first predict if a specific clause type is relevant to be added in a <a href="https://en.wikipedia.org/wiki/Contract">contract</a>, and then recommend the top clauses for the given type based on the contract context. We pre-train BERT on an existing library of clauses with two additional tasks and use it for our <a href="https://en.wikipedia.org/wiki/Prediction">prediction</a> and <a href="https://en.wikipedia.org/wiki/Recommender_system">recommendation</a>. We experiment with classification methods and similarity-based heuristics for clause relevance prediction, and generation-based methods for clause recommendation, and evaluate the results from various methods on several clause types. We provide analyses on the results, and further outline the limitations and future directions of this line of research.</abstract>
      <url hash="04b4c4d0">2021.emnlp-main.691</url>
      <bibkey>aggarwal-etal-2021-clauserec</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.691</doi>
    </paper>
    <paper id="692">
      <title>Finnish Dialect Identification : The Effect of Audio and Text<fixed-case>F</fixed-case>innish Dialect Identification: The Effect of Audio and Text</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <author><first>Niko</first><last>Partanen</last></author>
      <author><first>Jack</first><last>Rueter</last></author>
      <pages>8777–8783</pages>
      <abstract>Finnish is a language with multiple dialects that not only differ from each other in terms of <a href="https://en.wikipedia.org/wiki/Accent_(sociolinguistics)">accent</a> (pronunciation) but also in terms of <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological forms</a> and <a href="https://en.wikipedia.org/wiki/Lexicon">lexical choice</a>. We present the first approach to automatically detect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> is received by combining both of the modalities, as text only reaches to an overall <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 57 %, where as text and audio reach to 85 %. Our code, models and data have been released openly on Github and <a href="https://en.wikipedia.org/wiki/Zenodo">Zenodo</a>.</abstract>
      <url hash="c18020e9">2021.emnlp-main.692</url>
      <bibkey>hamalainen-etal-2021-finnish</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.692</doi>
      <pwccode url="https://github.com/rootroo-ltd/finnishdialectidentification" additional="false">rootroo-ltd/finnishdialectidentification</pwccode>
    </paper>
    <paper id="694">
      <title>Expanding End-to-End Question Answering on Differentiable Knowledge Graphs with Intersection</title>
      <author><first>Priyanka</first><last>Sen</last></author>
      <author><first>Armin</first><last>Oliya</last></author>
      <author><first>Amir</first><last>Saffari</last></author>
      <pages>8805–8812</pages>
      <abstract>End-to-end question answering using a differentiable knowledge graph is a promising technique that requires only <a href="https://en.wikipedia.org/wiki/Supervised_learning">weak supervision</a>, produces interpretable results, and is fully differentiable. Previous implementations of this technique (Cohen et al, 2020) have focused on single-entity questions using a relation following operation. In this paper, we propose a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> that explicitly handles multiple-entity questions by implementing a new intersection operation, which identifies the shared elements between two sets of <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entities</a>. We find that introducing intersection improves performance over a baseline model on two <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, WebQuestionsSP (69.6 % to 73.3 % Hits@1) and ComplexWebQuestions (39.8 % to 48.7 % Hits@1), and in particular, improves performance on questions with multiple entities by over 14 % on WebQuestionsSP and by 19 % on ComplexWebQuestions.</abstract>
      <url hash="3c5f48cd">2021.emnlp-main.694</url>
      <bibkey>sen-etal-2021-expanding</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.694</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/complexwebquestions">ComplexWebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestionssp">WebQuestionsSP</pwcdataset>
    </paper>
    <paper id="696">
      <title>Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation</title>
      <author><first>Max</first><last>Bartolo</last></author>
      <author><first>Tristan</first><last>Thrush</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Pontus</first><last>Stenetorp</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <pages>8830–8848</pages>
      <abstract>Despite recent progress, state-of-the-art question answering models remain vulnerable to a variety of <a href="https://en.wikipedia.org/wiki/Adversarial_system">adversarial attacks</a>. While dynamic adversarial data collection, in which a human annotator tries to write examples that fool a model-in-the-loop, can improve model robustness, this process is expensive which limits the scale of the collected data. In this work, we are the first to use synthetic adversarial data generation to make <a href="https://en.wikipedia.org/wiki/Question_answering">question answering models</a> more robust to <a href="https://en.wikipedia.org/wiki/Adversarial_system">human adversaries</a>. We develop a data generation pipeline that selects source passages, identifies candidate answers, generates questions, then finally filters or re-labels them to improve <a href="https://en.wikipedia.org/wiki/Quality_(business)">quality</a>. Using this approach, we amplify a smaller human-written adversarial dataset to a much larger set of synthetic question-answer pairs. By incorporating our synthetic data, we improve the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve model generalisation on nine of the twelve MRQA datasets. We further conduct a novel human-in-the-loop evaluation and show that our models are considerably more robust to new human-written adversarial examples : crowdworkers can fool our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> only 8.8 % of the time on average, compared to 17.6 % for a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained without synthetic data.</abstract>
      <url hash="3ec9154d">2021.emnlp-main.696</url>
      <bibkey>bartolo-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.696</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/adversarialqa">AdversarialQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="697">
      <title>BeliefBank : Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief<fixed-case>B</fixed-case>elief<fixed-case>B</fixed-case>ank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief</title>
      <author><first>Nora</first><last>Kassner</last></author>
      <author><first>Oyvind</first><last>Tafjord</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <pages>8849–8861</pages>
      <abstract>Although pretrained language models (PTLMs) contain significant amounts of <a href="https://en.wikipedia.org/wiki/World_knowledge">world knowledge</a>, they can still produce inconsistent answers to questions when probed, even after specialized training. As a result, it can be hard to identify what the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> actually believes about the world, making it susceptible to <a href="https://en.wikipedia.org/wiki/Consistency">inconsistent behavior</a> and simple errors. Our goal is to reduce these problems. Our approach is to embed a PTLM in a broader system that also includes an evolving, symbolic memory of beliefs   a BeliefBank   that records but then may modify the raw PTLM answers. We describe two <a href="https://en.wikipedia.org/wiki/Mechanism_(sociology)">mechanisms</a> to improve belief consistency in the overall system. First, a reasoning component   a weighted MaxSAT solver   revises beliefs that significantly clash with others. Second, a feedback component issues future queries to the PTLM using known beliefs as context. We show that, in a controlled experimental setting, these two mechanisms result in more consistent beliefs in the overall system, improving both the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and consistency of its answers over time. This is significant as it is a first step towards PTLM-based architectures with a systematic notion of belief, enabling them to construct a more coherent picture of the world, and improve over time without model retraining.</abstract>
      <url hash="5ab09962">2021.emnlp-main.697</url>
      <bibkey>kassner-etal-2021-beliefbank</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.697</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="698">
      <title>MLEC-QA : A Chinese Multi-Choice Biomedical Question Answering Dataset<fixed-case>MLEC-QA</fixed-case>: <fixed-case>A</fixed-case> <fixed-case>C</fixed-case>hinese <fixed-case>M</fixed-case>ulti-<fixed-case>C</fixed-case>hoice <fixed-case>B</fixed-case>iomedical <fixed-case>Q</fixed-case>uestion <fixed-case>A</fixed-case>nswering <fixed-case>D</fixed-case>ataset</title>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Shangping</first><last>Zhong</last></author>
      <author><first>Kaizhi</first><last>Chen</last></author>
      <pages>8862–8874</pages>
      <abstract>Question Answering (QA) has been successfully applied in scenarios of <a href="https://en.wikipedia.org/wiki/Human–computer_interaction">human-computer interaction</a> such as <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a> and <a href="https://en.wikipedia.org/wiki/Web_search_engine">search engines</a>. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields : <a href="https://en.wikipedia.org/wiki/Clinic">Clinic</a>, <a href="https://en.wikipedia.org/wiki/Oral_medicine">Stomatology</a>, <a href="https://en.wikipedia.org/wiki/Public_health">Public Health</a>, <a href="https://en.wikipedia.org/wiki/Traditional_Chinese_medicine">Traditional Chinese Medicine</a>, and <a href="https://en.wikipedia.org/wiki/Traditional_Chinese_medicine">Traditional Chinese Medicine</a> Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40 % to 55 % on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems.</abstract>
      <url hash="2a6e4581">2021.emnlp-main.698</url>
      <bibkey>li-etal-2021-mlec</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.698</doi>
      <pwccode url="https://github.com/judenpech/mlec-qa" additional="false">judenpech/mlec-qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/headqa">HeadQA</pwcdataset>
    </paper>
    <paper id="699">
      <title>IndoNLG : Benchmark and Resources for Evaluating Indonesian Natural Language Generation<fixed-case>I</fixed-case>ndo<fixed-case>NLG</fixed-case>: Benchmark and Resources for Evaluating <fixed-case>I</fixed-case>ndonesian Natural Language Generation</title>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Bryan</first><last>Wilie</last></author>
      <author><first>Karissa</first><last>Vincentio</last></author>
      <author><first>Xiaohong</first><last>Li</last></author>
      <author><first>Adhiguna</first><last>Kuncoro</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Zhi Yuan</first><last>Lim</last></author>
      <author><first>Syafri</first><last>Bahar</last></author>
      <author><first>Masayu</first><last>Khodra</last></author>
      <author><first>Ayu</first><last>Purwarianti</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>8875–8898</pages>
      <abstract>Natural language generation (NLG) benchmarks provide an important avenue to measure progress and develop better NLG systems. Unfortunately, the lack of publicly available NLG benchmarks for low-resource languages poses a challenging barrier for building NLG systems that work well for languages with limited amounts of data. Here we introduce IndoNLG, the first benchmark to measure natural language generation (NLG) progress in three low-resourceyet widely spokenlanguages of Indonesia : <a href="https://en.wikipedia.org/wiki/Indonesian_language">Indonesian</a>, <a href="https://en.wikipedia.org/wiki/Javanese_language">Javanese</a>, and <a href="https://en.wikipedia.org/wiki/Sundanese_language">Sundanese</a>. Altogether, these <a href="https://en.wikipedia.org/wiki/Language">languages</a> are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks : <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a>, <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, <a href="https://en.wikipedia.org/wiki/Chit-chat">chit-chat</a>, and three different pairs of machine translation (MT) tasks. We collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models : IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasksdespite using only one-fifth the parameters of a larger multilingual model, mBART-large (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, localized languages to achieve more efficient <a href="https://en.wikipedia.org/wiki/Learning">learning</a> and faster <a href="https://en.wikipedia.org/wiki/Inference">inference</a> at very low-resource languages like <a href="https://en.wikipedia.org/wiki/Javanese_language">Javanese</a> and <a href="https://en.wikipedia.org/wiki/Sundanese_language">Sundanese</a>.</abstract>
      <url hash="5f98a1d4">2021.emnlp-main.699</url>
      <bibkey>cahyawijaya-etal-2021-indonlg</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.699</doi>
      <pwccode url="https://github.com/indobenchmark/indonlg" additional="true">indobenchmark/indonlg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/indonlg">IndoNLG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gem">GEM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/liputan6">Liputan6</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
    </paper>
    <paper id="701">
      <title>Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors<fixed-case>BERT</fixed-case>-Based Evaluation Metrics by Disentangling along Linguistic Factors</title>
      <author><first>Marvin</first><last>Kaster</last></author>
      <author><first>Wei</first><last>Zhao</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <pages>8912–8925</pages>
      <abstract>Evaluation metrics are a key ingredient for progress of <a href="https://en.wikipedia.org/wiki/Text_generator">text generation systems</a>. In recent years, several BERT-based evaluation metrics have been proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much better with human assessment of text generation quality than <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> or ROUGE, invented two decades ago. However, little is known what these metrics, which are based on black-box language model representations, actually capture (it is typically assumed they model semantic similarity). In this work, we use a simple regression based global explainability technique to disentangle metric scores along linguistic factors, including <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a>, <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a>, <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphology</a>, and lexical overlap. We show that the different <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> capture all aspects to some degree, but that they are all substantially sensitive to <a href="https://en.wikipedia.org/wiki/Lexical_overlap">lexical overlap</a>, just like <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> and ROUGE. This exposes limitations of these novelly proposed <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>, which we also highlight in an adversarial test scenario.</abstract>
      <url hash="bf414303">2021.emnlp-main.701</url>
      <bibkey>kaster-etal-2021-global</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.701</doi>
      <pwccode url="https://github.com/steffeneger/global-explainability-metrics" additional="false">steffeneger/global-explainability-metrics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
    </paper>
    <paper id="702">
      <title>Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization<fixed-case>SQL</fixed-case> Generalization</title>
      <author><first>Yujian</first><last>Gan</last></author>
      <author><first>Xinyun</first><last>Chen</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <pages>8926–8931</pages>
      <abstract>Recently, there has been significant progress in studying <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> for translating text descriptions into SQL queries under the zero-shot cross-domain setting. Despite achieving good performance on some public benchmarks, we observe that existing text-to-SQL models do not generalize when facing <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> that does not frequently appear in the training data, which may render the worse prediction performance for unseen domains. In this work, we investigate the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of text-to-SQL models when the questions require rarely observed domain knowledge. In particular, we define five types of <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> and introduce Spider-DK (DK is the abbreviation of domain knowledge), a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-DK are selected from Spider, and we modify some samples by adding <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> that reflects real-world question paraphrases. We demonstrate that the prediction accuracy dramatically drops on samples that require such <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a>, even if the <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> appears in the training set, and the model provides the correct predictions for related training samples.</abstract>
      <url hash="4b6052c0">2021.emnlp-main.702</url>
      <bibkey>gan-etal-2021-exploring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.702</doi>
      <pwccode url="https://github.com/ygan/spider-dk" additional="false">ygan/spider-dk</pwccode>
    </paper>
    <paper id="704">
      <title>NeuTral Rewriter : A Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives<fixed-case>N</fixed-case>eu<fixed-case>T</fixed-case>ral <fixed-case>R</fixed-case>ewriter: <fixed-case>A</fixed-case> Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives</title>
      <author><first>Eva</first><last>Vanmassenhove</last></author>
      <author><first>Chris</first><last>Emmery</last></author>
      <author><first>Dimitar</first><last>Shterionov</last></author>
      <pages>8940–8948</pages>
      <abstract>Recent years have seen an increasing need for <a href="https://en.wikipedia.org/wiki/Gender-neutral_language">gender-neutral and inclusive language</a>. Within the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, there are various mono- and bilingual use cases where <a href="https://en.wikipedia.org/wiki/Gender_inclusive_language">gender inclusive language</a> is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this work, we present a rule-based and a neural approach to gender-neutral rewriting for <a href="https://en.wikipedia.org/wiki/English_language">English</a> along with manually curated synthetic data (WinoBias+) and natural data (OpenSubtitles and Reddit) benchmarks. A detailed manual and automatic evaluation highlights how our NeuTral Rewriter, trained on <a href="https://en.wikipedia.org/wiki/Data">data</a> generated by the rule-based approach, obtains <a href="https://en.wikipedia.org/wiki/Word_error_rate">word error rates (WER)</a> below 0.18 % on synthetic, in-domain and out-domain test sets.</abstract>
      <url hash="f7e486db">2021.emnlp-main.704</url>
      <bibkey>vanmassenhove-etal-2021-neutral</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.704</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="705">
      <title>Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset</title>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Weiqi</first><last>Wang</last></author>
      <author><first>Sehyun</first><last>Choi</last></author>
      <author><first>Shibo</first><last>Hao</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Bin</first><last>He</last></author>
      <pages>8949–8964</pages>
      <abstract>Reasoning over commonsense knowledge bases (CSKB) whose elements are in the form of free-text is an important yet hard task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. While CSKB completion only fills the missing links within the domain of the CSKB, CSKB population is alternatively proposed with the goal of reasoning unseen assertions from external resources. In this task, CSKBs are grounded to a large-scale eventuality (activity, state, and event) graph to discriminate whether novel triples from the eventuality graph are plausible or not. However, existing evaluations on the population task are either not accurate (automatic evaluation with randomly sampled negative examples) or of small scale (human annotation). In this paper, we benchmark the CSKB population task with a new large-scale dataset by first aligning four popular CSKBs, and then presenting a high-quality human-annotated evaluation set to probe neural models’ commonsense reasoning ability. We also propose a novel inductive commonsense reasoning model that reasons over <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a>. Experimental results show that generalizing <a href="https://en.wikipedia.org/wiki/Commonsense_reasoning">commonsense reasoning</a> on unseen assertions is inherently a hard task. Models achieving high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> during training perform poorly on the evaluation set, with a large gap between human performance. We will make the data publicly available for future contributions. Codes and data are available at https://github.com/HKUST-KnowComp/CSKB-Population.</abstract>
      <url hash="4c7070dc">2021.emnlp-main.705</url>
      <bibkey>fang-etal-2021-benchmarking</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.705</doi>
      <pwccode url="https://github.com/hkust-knowcomp/cskb-population" additional="false">hkust-knowcomp/cskb-population</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glucose">GLUCOSE</pwcdataset>
    </paper>
    <paper id="706">
      <title>Enhancing the Context Representation in Similarity-based Word Sense Disambiguation</title>
      <author><first>Ming</first><last>Wang</last></author>
      <author><first>Jianzhang</first><last>Zhang</last></author>
      <author><first>Yinglin</first><last>Wang</last></author>
      <pages>8965–8973</pages>
      <abstract>In previous similarity-based WSD systems, studies have allocated much effort on learning comprehensive sense embeddings using contextual representations and knowledge sources. However, the context embedding of an ambiguous word is learned using only the sentence where the word appears, neglecting its <a href="https://en.wikipedia.org/wiki/Context_(language_use)">global context</a>. In this paper, we investigate the contribution of both word-level and sense-level global context of an ambiguous word for <a href="https://en.wikipedia.org/wiki/Word-sense_disambiguation">disambiguation</a>. Experiments have shown that the Context-Oriented Embedding (COE) can enhance a similarity-based system’s performance on WSD by relatively large margins, achieving state-of-the-art on all-words WSD benchmarks in knowledge-based category.</abstract>
      <url hash="64971644">2021.emnlp-main.706</url>
      <attachment type="Software" hash="71b95bd0">2021.emnlp-main.706.Software.zip</attachment>
      <bibkey>wang-etal-2021-enhancing-context</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.706</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="710">
      <title>Cross-Domain Label-Adaptive Stance Detection</title>
      <author><first>Momchil</first><last>Hardalov</last></author>
      <author><first>Arnav</first><last>Arora</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>9011–9028</pages>
      <abstract>Stance detection concerns the classification of a writer’s viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the <a href="https://en.wikipedia.org/wiki/Data_collection">data collection</a>, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth analysis of 16 stance detection datasets, and we explore the possibility for cross-domain learning from them. Moreover, we propose an end-to-end unsupervised framework for out-of-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>, both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the cross-domain results, and we highlight the important factors influencing the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance.</abstract>
      <url hash="0a04bf62">2021.emnlp-main.710</url>
      <bibkey>hardalov-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.710</doi>
      <pwccode url="https://github.com/checkstep/mole-stance" additional="false">checkstep/mole-stance</pwccode>
    </paper>
    <paper id="711">
      <title>Text AutoAugment : Learning Compositional Augmentation Policy for Text Classification<fixed-case>A</fixed-case>uto<fixed-case>A</fixed-case>ugment: Learning Compositional Augmentation Policy for Text Classification</title>
      <author><first>Shuhuai</first><last>Ren</last></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>9029–9043</pages>
      <abstract>Data augmentation aims to enrich training samples for alleviating the <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting issue</a> in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previous <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a>, which decreases the diversity of the augmented data and thus restricts the performance gain. To overcome the above limitations, we propose a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> named Text AutoAugment (TAA) to establish a compositional and learnable paradigm for <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>. We regard a combination of various operations as an augmentation policy and utilize an efficient Bayesian Optimization algorithm to automatically search for the best <a href="https://en.wikipedia.org/wiki/Policy">policy</a>, which substantially improves the generalization capability of models. Experiments on six benchmark datasets show that TAA boosts <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification accuracy</a> in low-resource and class-imbalanced regimes by an average of 8.8 % and 9.7 %, respectively, outperforming strong baselines.</abstract>
      <url hash="1fddc3bb">2021.emnlp-main.711</url>
      <bibkey>ren-etal-2021-text</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.711</doi>
      <pwccode url="https://github.com/lancopku/text-autoaugment" additional="false">lancopku/text-autoaugment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="712">
      <title>Distilling Relation Embeddings from Pretrained Language Models</title>
      <author><first>Asahi</first><last>Ushio</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>9044–9062</pages>
      <abstract>Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">relation embeddings</a>, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine-grained way than is possible with <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a>. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically generated) prompt, and we fine-tune the <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, even without any task-specific fine-tuning. Source code to reproduce our experimental results and the model checkpoints are available in the following repository : https://github.com/asahi417/relbert</abstract>
      <url hash="c43be5b6">2021.emnlp-main.712</url>
      <bibkey>ushio-etal-2021-distilling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.712</doi>
      <pwccode url="https://github.com/asahi417/relbert" additional="false">asahi417/relbert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/evalution">EVALution</pwcdataset>
    </paper>
    <paper id="713">
      <title>Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning</title>
      <author><first>Prasetya</first><last>Utama</last></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last></author>
      <author><first>Victor</first><last>Sanh</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>9063–9074</pages>
      <abstract>Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting <a href="https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making">inference heuristics</a> based on <a href="https://en.wikipedia.org/wiki/Lexical_similarity">lexical overlap</a>, e.g., models incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is significantly less present in the zero-shot evaluation of the prompt-based model, indicating how <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a> can be destructive to useful knowledge learned during the pretraining. We then show that adding a <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization</a> that preserves pretraining weights is effective in mitigating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics.</abstract>
      <url hash="87d8a3f1">2021.emnlp-main.713</url>
      <bibkey>utama-etal-2021-avoiding</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.713</doi>
      <pwccode url="https://github.com/ukplab/emnlp2021-prompt-ft-heuristics" additional="false">ukplab/emnlp2021-prompt-ft-heuristics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="717">
      <title>NB-MLM : Efficient Domain Adaptation of Masked Language Models for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a><fixed-case>NB</fixed-case>-<fixed-case>MLM</fixed-case>: Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis</title>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <author><first>Dmitrii</first><last>Kharchev</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <pages>9114–9124</pages>
      <abstract>While Masked Language Models (MLM) are pre-trained on massive datasets, the additional training with the MLM objective on domain or task-specific data before fine-tuning for the final task is known to improve the final performance. This is usually referred to as the domain or task adaptation step. However, unlike the initial pre-training, this step is performed for each domain or task individually and is still rather slow, requiring several GPU days compared to several GPU hours required for the final task fine-tuning. We argue that the standard MLM objective leads to inefficiency when it is used for the adaptation step because it mostly learns to predict the most frequent words, which are not necessarily related to a final task. We propose a technique for more efficient <a href="https://en.wikipedia.org/wiki/Adaptation">adaptation</a> that focuses on predicting words with large weights of the <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes classifier</a> trained for the task at hand, which are likely more relevant than the most frequent words. The proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> provides faster adaptation and better final performance for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> compared to the standard approach.</abstract>
      <url hash="e329a0a1">2021.emnlp-main.717</url>
      <bibkey>arefyev-etal-2021-nb</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.717</doi>
      <pwccode url="https://github.com/nvanva/nb-mlm" additional="false">nvanva/nb-mlm</pwccode>
    </paper>
    <paper id="720">
      <title>Unimodal and Crossmodal Refinement Network for Multimodal Sequence Fusion</title>
      <author><first>Xiaobao</first><last>Guo</last></author>
      <author><first>Adams</first><last>Kong</last></author>
      <author><first>Huan</first><last>Zhou</last></author>
      <author><first>Xianfeng</first><last>Wang</last></author>
      <author><first>Min</first><last>Wang</last></author>
      <pages>9143–9153</pages>
      <abstract>Effective <a href="https://en.wikipedia.org/wiki/Unimodality">unimodal representation</a> and complementary crossmodal representation fusion are both important in multimodal representation learning. Prior works often modulate one modal feature to another straightforwardly and thus, underutilizing both unimodal and crossmodal representation refinements, which incurs a bottleneck of performance improvement. In this paper, Unimodal and Crossmodal Refinement Network (UCRN) is proposed to enhance both unimodal and crossmodal representations. Specifically, to improve <a href="https://en.wikipedia.org/wiki/Unimodality">unimodal representations</a>, a unimodal refinement module is designed to refine modality-specific learning via iteratively updating the distribution with transformer-based attention layers. Self-quality improvement layers are followed to generate the desired weighted representations progressively. Subsequently, those unimodal representations are projected into a common latent space, regularized by a multimodal Jensen-Shannon divergence loss for better crossmodal refinement. Lastly, a crossmodal refinement module is employed to integrate all information. By hierarchical explorations on unimodal, bimodal, and trimodal interactions, UCRN is highly robust against missing modality and noisy data. Experimental results on MOSI and MOSEI datasets illustrated that the proposed UCRN outperforms recent state-of-the-art techniques and its robustness is highly preferred in real multimodal sequence fusion scenarios. Codes will be shared publicly.</abstract>
      <url hash="cdccbf14">2021.emnlp-main.720</url>
      <bibkey>guo-etal-2021-unimodal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.720</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity">Multimodal Opinionlevel Sentiment Intensity</pwcdataset>
    </paper>
    <paper id="728">
      <title>Towards Label-Agnostic Emotion Embeddings</title>
      <author><first>Sven</first><last>Buechel</last></author>
      <author><first>Luise</first><last>Modersohn</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <pages>9231–9249</pages>
      <abstract>Research in emotion analysis is scattered across different label formats (e.g., polarity types, basic emotion categories, and affective dimensions), <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic levels</a> (word vs. sentence vs. discourse), and, of course, (few well-resourced but much more under-resourced) <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural languages and text genres</a> (e.g., product reviews, <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a>, news). The resulting heterogeneity makes data and software developed under these conflicting constraints hard to compare and challenging to integrate. To resolve this unsatisfactory state of affairs we here propose a training scheme that learns a shared latent representation of emotion independent from different label formats, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural languages</a>, and even disparate model architectures. Experiments on a wide range of datasets indicate that this approach yields the desired <a href="https://en.wikipedia.org/wiki/Interoperability">interoperability</a> without penalizing prediction quality. Code and data are archived under DOI 10.5281 / zenodo.5466068.</abstract>
      <url hash="c429039c">2021.emnlp-main.728</url>
      <bibkey>buechel-etal-2021-towards</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.728</doi>
    </paper>
    <paper id="729">
      <title>Collaborative Learning of Bidirectional Decoders for Unsupervised Text Style Transfer</title>
      <author><first>Yun</first><last>Ma</last></author>
      <author><first>Yangbin</first><last>Chen</last></author>
      <author><first>Xudong</first><last>Mao</last></author>
      <author><first>Qing</first><last>Li</last></author>
      <pages>9250–9266</pages>
      <abstract>Unsupervised text style transfer aims to alter the underlying style of the text to a desired value while keeping its style-independent semantics, without the support of parallel training corpora. Existing methods struggle to achieve both high style conversion rate and low content loss, exhibiting the over-transfer and under-transfer problems. We attribute these problems to the conflicting driving forces of the style conversion goal and content preservation goal. In this paper, we propose a collaborative learning framework for unsupervised text style transfer using a pair of bidirectional decoders, one decoding from left to right while the other decoding from right to left. In our collaborative learning mechanism, each <a href="https://en.wikipedia.org/wiki/Codec">decoder</a> is regularized by knowledge from its peer which has a different <a href="https://en.wikipedia.org/wiki/Knowledge_acquisition">knowledge acquisition process</a>. The difference is guaranteed by their opposite decoding directions and a distinguishability constraint. As a result, mutual knowledge distillation drives both decoders to a better optimum and alleviates the over-transfer and under-transfer problems. Experimental results on two benchmark datasets show that our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> achieves strong empirical results on both style compatibility and content preservation.</abstract>
      <url hash="1dd92b9b">2021.emnlp-main.729</url>
      <bibkey>ma-etal-2021-collaborative</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.729</doi>
      <pwccode url="https://github.com/sunlight-ym/cbd_style_transfer" additional="false">sunlight-ym/cbd_style_transfer</pwccode>
    </paper>
    <paper id="730">
      <title>Exploring Non-Autoregressive Text Style Transfer</title>
      <author><first>Yun</first><last>Ma</last></author>
      <author><first>Qing</first><last>Li</last></author>
      <pages>9267–9278</pages>
      <abstract>In this paper, we explore Non-AutoRegressive (NAR) decoding for unsupervised text style transfer. We first propose a base NAR model by directly adapting the common training scheme from its AutoRegressive (AR) counterpart. Despite the faster inference speed over the <a href="https://en.wikipedia.org/wiki/Autoregressive_model">AR model</a>, this NAR model sacrifices its transfer performance due to the lack of <a href="https://en.wikipedia.org/wiki/Conditional_dependence">conditional dependence</a> between output tokens. To this end, we investigate three <a href="https://en.wikipedia.org/wiki/Methodology">techniques</a>, i.e., knowledge distillation, contrastive learning, and iterative decoding, for performance enhancement. Experimental results on two benchmark datasets suggest that, although the base NAR model is generally inferior to AR decoding, their performance gap can be clearly narrowed when empowering NAR decoding with knowledge distillation, contrastive learning, and iterative decoding.</abstract>
      <url hash="78b4edf6">2021.emnlp-main.730</url>
      <bibkey>ma-li-2021-exploring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.730</doi>
      <pwccode url="https://github.com/sunlight-ym/nar_style_transfer" additional="false">sunlight-ym/nar_style_transfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="733">
      <title>Progressively Guide to Attend : An Iterative Alignment Framework for Temporal Sentence Grounding</title>
      <author><first>Daizong</first><last>Liu</last></author>
      <author><first>Xiaoye</first><last>Qu</last></author>
      <author><first>Pan</first><last>Zhou</last></author>
      <pages>9302–9311</pages>
      <abstract>A key solution to temporal sentence grounding (TSG) exists in how to learn effective alignment between vision and language features extracted from an untrimmed video and a sentence description. Existing methods mainly leverage vanilla soft attention to perform the alignment in a single-step process. However, such single-step attention is insufficient in practice, since complicated relations between inter- and intra-modality are usually obtained through multi-step reasoning. In this paper, we propose an Iterative Alignment Network (IA-Net) for TSG task, which iteratively interacts inter- and intra-modal features within multiple steps for more accurate grounding. Specifically, during the iterative reasoning process, we pad multi-modal features with learnable parameters to alleviate the nowhere-to-attend problem of non-matched frame-word pairs, and enhance the basic co-attention mechanism in a parallel manner. To further calibrate the misaligned attention caused by each reasoning step, we also devise a calibration module following each attention module to refine the alignment knowledge. With such iterative alignment scheme, our IA-Net can robustly capture the fine-grained relations between vision and language domains step-by-step for progressively reasoning the temporal boundaries. Extensive experiments conducted on three challenging benchmarks demonstrate that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performs better than the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-arts</a>.</abstract>
      <url hash="d7fa8de9">2021.emnlp-main.733</url>
      <bibkey>liu-etal-2021-progressively</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.733</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/activitynet-captions">ActivityNet Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades">Charades</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades-sta">Charades-STA</pwcdataset>
    </paper>
    <paper id="741">
      <title>ARMAN : Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization<fixed-case>ARMAN</fixed-case>: <fixed-case>P</fixed-case>re-training with <fixed-case>S</fixed-case>emantically <fixed-case>S</fixed-case>electing and <fixed-case>R</fixed-case>eordering of <fixed-case>S</fixed-case>entences for <fixed-case>P</fixed-case>ersian <fixed-case>A</fixed-case>bstractive <fixed-case>S</fixed-case>ummarization</title>
      <author><first>Alireza</first><last>Salemi</last></author>
      <author><first>Emad</first><last>Kebriaei</last></author>
      <author><first>Ghazal</first><last>Neisi Minaei</last></author>
      <author><first>Azadeh</first><last>Shakery</last></author>
      <pages>9391–9407</pages>
      <abstract>Abstractive text summarization is one of the areas influenced by the emergence of pre-trained language models. Current pre-training works in abstractive summarization give more points to the summaries with more words in common with the main text and pay less attention to the <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a> between generated sentences and the original document. We propose ARMAN, a Transformer-based encoder-decoder model pre-trained with three novel objectives to address this issue. In <a href="https://en.wikipedia.org/wiki/ARMAN">ARMAN</a>, salient sentences from a document are selected according to a modified semantic score to be masked and form a pseudo summary. To summarize more accurately and similar to human writing patterns, we applied modified sentence reordering. We evaluated our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on six downstream Persian summarization tasks. Experimental results show that our proposed model achieves state-of-the-art performance on all six summarization tasks measured by ROUGE and BERTScore. Our models also outperform prior works in <a href="https://en.wikipedia.org/wiki/Textual_entailment">textual entailment</a>, question paraphrasing, and <a href="https://en.wikipedia.org/wiki/Multiple_choice">multiple choice question answering</a>. Finally, we established a <a href="https://en.wikipedia.org/wiki/Evaluation">human evaluation</a> and show that using the semantic score significantly improves summarization results.</abstract>
      <url hash="9e52027f">2021.emnlp-main.741</url>
      <bibkey>salemi-etal-2021-arman</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.741</doi>
      <pwccode url="https://github.com/alirezasalemi7/arman" additional="false">alirezasalemi7/arman</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/perkey">PerKey</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pn-summary">pn-summary</pwcdataset>
    </paper>
    <paper id="745">
      <title>Revisiting Tri-training of Dependency Parsers</title>
      <author><first>Joachim</first><last>Wagner</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <pages>9457–9473</pages>
      <abstract>We compare two orthogonal semi-supervised learning techniques, namely tri-training and pretrained word embeddings, in the task of dependency parsing. We explore language-specific FastText and ELMo embeddings and multilingual BERT embeddings. We focus on a low resource scenario as <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised learning</a> can be expected to have the most impact here. Based on treebank size and available ELMo models, we select <a href="https://en.wikipedia.org/wiki/Hungarian_language">Hungarian</a>, Uyghur (a zero-shot language for mBERT) and <a href="https://en.wikipedia.org/wiki/Vietnamese_language">Vietnamese</a>. Furthermore, we include <a href="https://en.wikipedia.org/wiki/English_language">English</a> in a simulated low-resource setting. We find that pretrained word embeddings make more effective use of unlabelled data than tri-training but that the two approaches can be successfully combined.</abstract>
      <url hash="e899d7c6">2021.emnlp-main.745</url>
      <bibkey>wagner-foster-2021-revisiting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.745</doi>
      <pwccode url="https://github.com/jowagner/mtb-tri-training" additional="true">jowagner/mtb-tri-training</pwccode>
    </paper>
    <paper id="746">
      <title>Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion : Re-explore Zero-Shot Learning for Slot Filling</title>
      <author><first>Liwen</first><last>Wang</last></author>
      <author><first>Xuefeng</first><last>Li</last></author>
      <author><first>Jiachi</first><last>Liu</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>9474–9480</pages>
      <abstract>Zero-shot cross-domain slot filling alleviates the <a href="https://en.wikipedia.org/wiki/Data_dependence">data dependence</a> in the case of data scarcity in the target domain, which has aroused extensive research. However, as most of the existing methods do not achieve effective <a href="https://en.wikipedia.org/wiki/Knowledge_transfer">knowledge transfer</a> to the target domain, they just fit the distribution of the seen slot and show poor performance on unseen slot in the target domain. To solve this, we propose a novel approach based on prototypical contrastive learning with a dynamic label confusion strategy for zero-shot slot filling. The prototypical contrastive learning aims to reconstruct the semantic constraints of labels, and we introduce the label confusion strategy to establish the label dependence between the source domains and the target domain on-the-fly. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves significant improvement on the unseen slots, while also set new state-of-the-arts on slot filling task.</abstract>
      <url hash="5075be54">2021.emnlp-main.746</url>
      <bibkey>wang-etal-2021-bridge</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.746</doi>
      <pwccode url="https://github.com/w-lw/pclc" additional="false">w-lw/pclc</pwccode>
    </paper>
    <paper id="747">
      <title>Neuralizing Regular Expressions for Slot Filling</title>
      <author><first>Chengyue</first><last>Jiang</last></author>
      <author><first>Zijian</first><last>Jin</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>9481–9498</pages>
      <abstract>Neural models and <a href="https://en.wikipedia.org/wiki/Mathematical_logic">symbolic rules</a> such as <a href="https://en.wikipedia.org/wiki/Regular_expression">regular expressions</a> have their respective merits and weaknesses. In this paper, we study the integration of the two approaches for the slot filling task by converting <a href="https://en.wikipedia.org/wiki/Regular_expression">regular expressions</a> into <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>. Specifically, we first convert <a href="https://en.wikipedia.org/wiki/Regular_expression">regular expressions</a> into a special form of finite-state transducers, then unfold its approximate inference algorithm as a bidirectional recurrent neural model that performs slot filling via <a href="https://en.wikipedia.org/wiki/Sequence_labeling">sequence labeling</a>. Experimental results show that our model has superior zero-shot and few-shot performance and stays competitive when there are sufficient training data.</abstract>
      <url hash="07485a04">2021.emnlp-main.747</url>
      <bibkey>jiang-etal-2021-neuralizing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.747</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
    </paper>
    <paper id="755">
      <title>Case-based Reasoning for Natural Language Queries over Knowledge Bases</title>
      <author><first>Rajarshi</first><last>Das</last></author>
      <author><first>Manzil</first><last>Zaheer</last></author>
      <author><first>Dung</first><last>Thai</last></author>
      <author><first>Ameya</first><last>Godbole</last></author>
      <author><first>Ethan</first><last>Perez</last></author>
      <author><first>Jay Yoon</first><last>Lee</last></author>
      <author><first>Lizhen</first><last>Tan</last></author>
      <author><first>Lazaros</first><last>Polymenakos</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>9594–9611</pages>
      <abstract>It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions   a <a href="https://en.wikipedia.org/wiki/Paradigm">paradigm</a> known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a <a href="https://en.wikipedia.org/wiki/Parametric_model">parametric model</a> that can generate a <a href="https://en.wikipedia.org/wiki/Logical_form">logical form</a> for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the CWQ dataset, CBR-KBQA outperforms the current state of the art by 11 % on <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>. Furthermore, we show that CBR-KBQA is capable of using new cases without any further training : by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.<i>without</i> any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.</abstract>
      <url hash="c8a8afde">2021.emnlp-main.755</url>
      <bibkey>das-etal-2021-case</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.755</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cfq">CFQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/complexwebquestions">ComplexWebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestionssp">WebQuestionsSP</pwcdataset>
    </paper>
    <paper id="756">
      <title>Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation</title>
      <author><first>Chen</first><last>Zhao</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <author><first>Hal</first><last>Daumé III</last></author>
      <pages>9612–9622</pages>
      <abstract>Open-domain question answering answers a question based on evidence retrieved from a <a href="https://en.wikipedia.org/wiki/Text_corpus">large corpus</a>. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a> that rely on them can not transfer to the more common setting, where only questionanswer pairs are available. This paper investigates whether <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> can learn to find evidence from a large corpus, with only distant supervision from answer labels for model training, thereby generating no additional annotation cost. We introduce a novel approach (DistDR) that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to learn the most likely evidence. Without using any evidence labels, DistDR is on par with fully-supervised state-of-the-art methods on both multi-hop and single-hop QA benchmarks. Our analysis confirms that DistDR finds more accurate evidence over iterations, which leads to model improvements. The code is available at https://github.com/henryzhao5852/DistDR.</abstract>
      <url hash="f69c33b4">2021.emnlp-main.756</url>
      <bibkey>zhao-etal-2021-distantly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.756</doi>
      <pwccode url="https://github.com/henryzhao5852/distdr" additional="false">henryzhao5852/distdr</pwccode>
    </paper>
    <paper id="760">
      <title>Set Generation Networks for End-to-End Knowledge Base Population</title>
      <author><first>Dianbo</first><last>Sui</last></author>
      <author><first>Chenhao</first><last>Wang</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Wei</first><last>Bi</last></author>
      <pages>9650–9660</pages>
      <abstract>The task of <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base population (KBP)</a> aims to discover facts about entities from texts and expand a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a> with these facts. Previous studies shape end-to-end KBP as a machine translation task, which is required to convert unordered fact into a sequence according to a pre-specified order. However, the facts stated in a sentence are unordered in essence. In this paper, we formulate end-to-end KBP as a direct set generation problem, avoiding considering the order of multiple facts. To solve the set generation problem, we propose networks featured by <a href="https://en.wikipedia.org/wiki/Transformer">transformers</a> with non-autoregressive parallel decoding. Unlike previous approaches that use an autoregressive decoder to generate facts one by one, the proposed networks can directly output the final set of facts in one shot. Furthermore, to train the <a href="https://en.wikipedia.org/wiki/Computer_network">networks</a>, we also design a <a href="https://en.wikipedia.org/wiki/Loss_function">set-based loss</a> that forces unique predictions via <a href="https://en.wikipedia.org/wiki/Bipartite_matching">bipartite matching</a>. Compared with cross-entropy loss that highly penalizes small shifts in fact order, the proposed bipartite matching loss is invariant to any permutation of predictions. Benefiting from getting rid of the burden of predicting the order of multiple facts, our proposed networks achieve state-of-the-art (SoTA) performance on two benchmark datasets.</abstract>
      <url hash="6f25b9c5">2021.emnlp-main.760</url>
      <bibkey>sui-etal-2021-set</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.760</doi>
    </paper>
    <paper id="762">
      <title>Progressive Adversarial Learning for Bootstrapping : A Case Study on Entity Set Expansion</title>
      <author><first>Lingyong</first><last>Yan</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>9673–9682</pages>
      <abstract>Bootstrapping has become the mainstream method for entity set expansion. Conventional bootstrapping methods mostly define the expansion boundary using seed-based distance metrics, which heavily depend on the quality of selected seeds and are hard to be adjusted due to the extremely sparse supervision. In this paper, we propose BootstrapGAN, a new learning method for <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrapping</a> which jointly models the <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrapping process</a> and the boundary learning process in a GAN framework. Specifically, the expansion boundaries of different bootstrapping iterations are learned via different discriminator networks ; the bootstrapping network is the generator to generate new positive entities, and the discriminator networks identify the expansion boundaries by trying to distinguish the generated entities from known positive entities. By iteratively performing the above <a href="https://en.wikipedia.org/wiki/Adversarial_learning">adversarial learning</a>, the generator and the discriminators can reinforce each other and be progressively refined along the whole bootstrapping process. Experiments show that BootstrapGAN achieves the new state-of-the-art entity set expansion performance.</abstract>
      <url hash="a739d6d0">2021.emnlp-main.762</url>
      <bibkey>yan-etal-2021-progressive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.762</doi>
      <pwccode url="https://github.com/lingyongyan/bootstrapgan" additional="false">lingyongyan/bootstrapgan</pwccode>
    </paper>
    <paper id="765">
      <title>A Relation-Oriented Clustering Method for Open Relation Extraction</title>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Yaqian</first><last>Zhou</last></author>
      <pages>9707–9718</pages>
      <abstract>The clustering-based unsupervised relation discovery method has gradually become one of the important methods of open relation extraction (OpenRE). However, high-dimensional vectors can encode complex linguistic information which leads to the problem that the derived clusters can not explicitly align with the relational semantic classes. In this work, we propose a relation-oriented clustering model and use it to identify the novel relations in the unlabeled data. Specifically, to enable the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to learn to cluster <a href="https://en.wikipedia.org/wiki/Relational_model">relational data</a>, our method leverages the readily available labeled data of pre-defined relations to learn a relation-oriented representation. We minimize distance between the instance with same relation by gathering the instances towards their corresponding relation centroids to form a cluster structure, so that the learned representation is cluster-friendly. To reduce the clustering bias on predefined classes, we optimize the model by minimizing a joint objective on both labeled and unlabeled data. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> reduces the <a href="https://en.wikipedia.org/wiki/Error_rate">error rate</a> by 29.2 % and 15.7 %, on two datasets respectively, compared with current SOTA methods.</abstract>
      <url hash="fb4e65b2">2021.emnlp-main.765</url>
      <bibkey>zhao-etal-2021-relation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.765</doi>
      <pwccode url="https://github.com/ac-zyx/rocore" additional="false">ac-zyx/rocore</pwccode>
    </paper>
    <paper id="768">
      <title>Meta Distant Transfer Learning for Pre-trained Language Models</title>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Haojie</first><last>Pan</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <author><first>Fei</first><last>Yang</last></author>
      <author><first>Yin</first><last>Zhang</last></author>
      <pages>9742–9752</pages>
      <abstract>With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from <a href="https://en.wikipedia.org/wiki/Negative_transfer">negative transfer</a>. Inspired by <a href="https://en.wikipedia.org/wiki/Meta-learning">meta-learning</a>, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation learning to mine implicit relations among multiple tasks and classes. Based on the results, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> trains a PLM-based meta-learner to capture the transferable knowledge across tasks. The weighted maximum entropy regularizers are proposed to make meta-learner more task-agnostic and unbiased. Finally, the meta-learner can be fine-tuned to fit each <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> with better parameter initialization. We evaluate Meta-DTL using both BERT and ALBERT on seven public datasets. Experiment results confirm the superiority of Meta-DTL as it consistently outperforms strong baselines. We find that Meta-DTL is highly effective when very few data is available for the target task.</abstract>
      <url hash="86cfed8c">2021.emnlp-main.768</url>
      <bibkey>wang-etal-2021-meta-distant</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.768</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="769">
      <title>UniKER : A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference<fixed-case>U</fixed-case>ni<fixed-case>KER</fixed-case>: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference</title>
      <author><first>Kewei</first><last>Cheng</last></author>
      <author><first>Ziqing</first><last>Yang</last></author>
      <author><first>Ming</first><last>Zhang</last></author>
      <author><first>Yizhou</first><last>Sun</last></author>
      <pages>9753–9771</pages>
      <abstract>Knowledge graph inference has been studied extensively due to its wide applications. It has been addressed by two lines of research, i.e., the more traditional <a href="https://en.wikipedia.org/wiki/Logical_consequence">logical rule reasoning</a> and the more recent knowledge graph embedding (KGE). Several attempts have been made to combine KGE and <a href="https://en.wikipedia.org/wiki/Rule_of_inference">logical rules</a> for better knowledge graph inference. Unfortunately, they either simply treat logical rules as additional constraints into KGE loss or use <a href="https://en.wikipedia.org/wiki/Probabilistic_model">probabilistic model</a> to approximate the exact <a href="https://en.wikipedia.org/wiki/Logical_inference">logical inference</a> (i.e., MAX-SAT). Even worse, both approaches need to sample ground rules to tackle the scalability issue, as the total number of ground rules is intractable in practice, making them less effective in handling logical rules. In this paper, we propose a novel framework UniKER to address these challenges by restricting <a href="https://en.wikipedia.org/wiki/Rule_of_inference">logical rules</a> to be definite Horn rules, which can fully exploit the knowledge in <a href="https://en.wikipedia.org/wiki/Rule_of_inference">logical rules</a> and enable the mutual enhancement of logical rule-based reasoning and KGE in an extremely efficient way. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms in terms of both <a href="https://en.wikipedia.org/wiki/Efficiency">efficiency</a> and effectiveness.</abstract>
      <url hash="945bf38d">2021.emnlp-main.769</url>
      <bibkey>cheng-etal-2021-uniker</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.769</doi>
    </paper>
    <paper id="771">
      <title>Jointly Learning to Repair Code and Generate Commit Message</title>
      <author><first>Jiaqi</first><last>Bai</last></author>
      <author><first>Long</first><last>Zhou</last></author>
      <author><first>Ambrosio</first><last>Blanco</last></author>
      <author><first>Shujie</first><last>Liu</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>9784–9795</pages>
      <abstract>We propose a novel <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> of jointly repairing program codes and generating commit messages. Code repair and commit message generation are two essential and related tasks for <a href="https://en.wikipedia.org/wiki/Software_development">software development</a>. However, existing work usually performs the two <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> independently. We construct a multilingual triple dataset including <a href="https://en.wikipedia.org/wiki/Software_bug">buggy code</a>, fixed code, and commit messages for this novel task. We first introduce a cascaded method with two models, one is to generate the fixed code first, and the other generates the commit message based on the fixed and original codes. We enhance the cascaded method with different training approaches, including the teacher-student method, the multi-task method, and the back-translation method. To deal with the error propagation problem of the cascaded method, we also propose a joint model that can both repair the <a href="https://en.wikipedia.org/wiki/Source_code">program code</a> and generate the commit message in a unified framework. Massive experiments on our constructed buggy-fixed-commit dataset reflect the challenge of this task and that the enhanced cascaded model and the proposed joint model significantly outperform baselines in both quality of code and commit messages.</abstract>
      <url hash="4d39c31a">2021.emnlp-main.771</url>
      <bibkey>bai-etal-2021-jointly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.771</doi>
    </paper>
    <paper id="773">
      <title>On Pursuit of Designing Multi-modal Transformer for Video Grounding</title>
      <author><first>Meng</first><last>Cao</last></author>
      <author><first>Long</first><last>Chen</last></author>
      <author><first>Mike Zheng</first><last>Shou</last></author>
      <author><first>Can</first><last>Zhang</last></author>
      <author><first>Yuexian</first><last>Zou</last></author>
      <pages>9810–9823</pages>
      <abstract>Video grounding aims to localize the temporal segment corresponding to a <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence query</a> from an <a href="https://en.wikipedia.org/wiki/Video_editing">untrimmed video</a>. Almost all existing video grounding methods fall into two frameworks : 1) <a href="https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design">Top-down model</a> : It predefines a set of segment candidates and then conducts segment classification and <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression</a>. 2) Bottom-up model : It directly predicts frame-wise probabilities of the referential segment boundaries. However, all these <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> are not end-to-end, i.e., they always rely on some time-consuming post-processing steps to refine predictions. To this end, we reformulate video grounding as a set prediction task and propose a novel end-to-end multi-modal Transformer model, dubbed as GTR. Specifically, GTR has two <a href="https://en.wikipedia.org/wiki/Encoder">encoders</a> for video and language encoding, and a cross-modal decoder for grounding prediction. To facilitate the end-to-end training, we use a Cubic Embedding layer to transform the raw videos into a set of visual tokens. To better fuse these two modalities in the <a href="https://en.wikipedia.org/wiki/Codec">decoder</a>, we design a new Multi-head Cross-Modal Attention. The whole <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">GTR</a> is optimized via a Many-to-One matching loss. Furthermore, we conduct comprehensive studies to investigate different model design choices. Extensive results on three <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> have validated the superiority of GTR. All three typical GTR variants achieve record-breaking performance on all datasets and <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>, with several times faster <a href="https://en.wikipedia.org/wiki/Time_complexity">inference speed</a>.</abstract>
      <url hash="67076422">2021.emnlp-main.773</url>
      <bibkey>cao-etal-2021-pursuit</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.773</doi>
      <revision id="1" href="2021.emnlp-main.773v1" hash="2bd6c814" />
      <revision id="2" href="2021.emnlp-main.773v2" hash="67076422" date="2022-04-29">Added missing acknowledgement.</revision>
      <pwcdataset url="https://paperswithcode.com/dataset/activitynet-captions">ActivityNet Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades-sta">Charades-STA</pwcdataset>
    </paper>
    <paper id="775">
      <title>Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers</title>
      <author><first>Stella</first><last>Frank</last></author>
      <author><first>Emanuele</first><last>Bugliarello</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <pages>9847–9857</pages>
      <abstract>Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are not symmetrically cross-modal.</abstract>
      <url hash="4105ff84">2021.emnlp-main.775</url>
      <bibkey>frank-etal-2021-vision</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.775</doi>
      <pwccode url="https://github.com/e-bug/volta" additional="true">e-bug/volta</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="778">
      <title>QA-Align : Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions<fixed-case>QA</fixed-case>-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions</title>
      <author><first>Daniela</first><last>Brook Weiss</last></author>
      <author><first>Paul</first><last>Roit</last></author>
      <author><first>Ayal</first><last>Klein</last></author>
      <author><first>Ori</first><last>Ernst</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>9879–9894</pages>
      <abstract>Multi-text applications, such as <a href="https://en.wikipedia.org/wiki/Multi-document_summarization">multi-document summarization</a>, are typically required to model redundancies across related texts. Current methods confronting <a href="https://en.wikipedia.org/wiki/Consolidation_(business)">consolidation</a> struggle to fuse overlapping information. In order to explicitly represent content overlap, we propose to align predicate-argument relations across texts, providing a potential scaffold for information consolidation. We go beyond clustering coreferring mentions, and instead model overlap with respect to redundancy at a propositional level, rather than merely detecting shared referents. Our <a href="https://en.wikipedia.org/wiki/Setting_(narrative)">setting</a> exploits QA-SRL, utilizing question-answer pairs to capture predicate-argument relations, facilitating laymen annotation of cross-text alignments. We employ crowd-workers for constructing a dataset of QA-based alignments, and present a baseline QA alignment model trained over our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. Analyses show that our new task is semantically challenging, capturing content overlap beyond <a href="https://en.wikipedia.org/wiki/Lexical_similarity">lexical similarity</a> and complements cross-document coreference with proposition-level links, offering potential use for downstream tasks.</abstract>
      <url hash="6bd2ac49">2021.emnlp-main.778</url>
      <bibkey>brook-weiss-etal-2021-qa</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.778</doi>
      <pwccode url="https://github.com/danielabweiss/qa-align" additional="false">danielabweiss/qa-align</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl">QA-SRL</pwcdataset>
    </paper>
    <paper id="780">
      <title>Exploiting Twitter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings<fixed-case>T</fixed-case>witter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings</title>
      <author><first>Marco</first><last>Di Giovanni</last></author>
      <author><first>Marco</first><last>Brambilla</last></author>
      <pages>9902–9910</pages>
      <abstract>Semantic sentence embeddings are usually supervisedly built minimizing distances between pairs of embeddings of sentences labelled as semantically similar by annotators. Since big labelled datasets are rare, in particular for non-English languages, and expensive, recent studies focus on <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised approaches</a> that require not-paired input sentences. We instead propose a language-independent approach to build large datasets of pairs of informal texts weakly similar, without manual human effort, exploiting Twitter’s intrinsic powerful signals of relatedness : replies and quotes of tweets. We use the collected pairs to train a Transformer model with triplet-like structures, and we test the generated embeddings on Twitter NLP similarity tasks (PIT and TURL) and STSb. We also introduce four new sentence ranking evaluation benchmarks of informal texts, carefully extracted from the initial collections of tweets, proving not only that our best model learns classical Semantic Textual Similarity, but also excels on tasks where pairs of sentences are not exact paraphrases. Ablation studies reveal how increasing the corpus size influences positively the results, even at 2 M samples, suggesting that bigger collections of Tweets still do not contain redundant information about semantic similarities. Code available at https://github.com/marco-digio/Twitter4SSE</abstract>
      <url hash="fcbf27ea">2021.emnlp-main.780</url>
      <bibkey>di-giovanni-brambilla-2021-exploiting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.780</doi>
      <pwccode url="https://github.com/marco-digio/twitter4sse" additional="false">marco-digio/twitter4sse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pit">PIT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/twitter-news-url-corpus">TURL</pwcdataset>
    </paper>
    <paper id="781">
      <title>Guilt by Association : Emotion Intensities in Lexical Representations</title>
      <author><first>Shahab</first><last>Raji</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>9911–9917</pages>
      <abstract>What do linguistic models reveal about the <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data.</abstract>
      <url hash="640e8a71">2021.emnlp-main.781</url>
      <bibkey>raji-de-melo-2021-guilt</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.781</doi>
    </paper>
    <paper id="782">
      <title>Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender</title>
      <author><first>Sky</first><last>CH-Wang</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>9918–9938</pages>
      <abstract>Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these <a href="https://en.wikipedia.org/wiki/Choice">choices</a> in aggregate allows us to examine large-scale attitude shifts within a population. Here, we develop computational methods to study <a href="https://en.wikipedia.org/wiki/Word_choice">word choice</a> within a sociolinguistic lexical variablealternate words used to express the same conceptin order to test for change in the United States towards <a href="https://en.wikipedia.org/wiki/Human_sexuality">sexuality</a> and <a href="https://en.wikipedia.org/wiki/Gender">gender</a>. We examine two variables : i) referents to significant others, such as the word partner and ii) referents to an indefinite person, both of which could optionally be marked with gender. The linguistic choices in each variable allow us to study increased rates of acceptances of gay marriage and <a href="https://en.wikipedia.org/wiki/Gender_equality">gender equality</a>, respectively. In longitudinal analyses across <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> and <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a> over 87 M messages, we demonstrate that attitudes are changing but that these changes are driven by specific demographics within the United States. Further, in a quasi-causal analysis, we show that passages of Marriage Equality Acts in different states are drivers of <a href="https://en.wikipedia.org/wiki/Linguistic_change">linguistic change</a>.</abstract>
      <url hash="6175446a">2021.emnlp-main.782</url>
      <bibkey>ch-wang-jurgens-2021-using</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.782</doi>
    </paper>
    <paper id="785">
      <title>Assessing the Reliability of Word Embedding Gender Bias Measures</title>
      <author><first>Yupei</first><last>Du</last></author>
      <author><first>Qixiang</first><last>Fang</last></author>
      <author><first>Dong</first><last>Nguyen</last></author>
      <pages>10012–10034</pages>
      <abstract>Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these <a href="https://en.wikipedia.org/wiki/Measurement">measures</a> can suffer from <a href="https://en.wikipedia.org/wiki/Observational_error">measurement error</a>. One indication of measurement quality is <a href="https://en.wikipedia.org/wiki/Reliability_(statistics)">reliability</a>, concerning the extent to which a <a href="https://en.wikipedia.org/wiki/Measurement">measure</a> produces consistent results. In this paper, we assess three types of <a href="https://en.wikipedia.org/wiki/Reliability_(statistics)">reliability</a> of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of <a href="https://en.wikipedia.org/wiki/Random_seed">random seeds</a>, scoring rules and <a href="https://en.wikipedia.org/wiki/Word_formation">words</a>. Furthermore, we analyse the effects of various factors on these <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">measures</a>’ <a href="https://en.wikipedia.org/wiki/Reliability_(statistics)">reliability scores</a>. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">measures</a></abstract>
      <url hash="1c32913d">2021.emnlp-main.785</url>
      <bibkey>du-etal-2021-assessing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.785</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="788">
      <title>SWEAT : Scoring Polarization of Topics across Different Corpora<fixed-case>SWEAT</fixed-case>: Scoring Polarization of Topics across Different Corpora</title>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Marco</first><last>Marelli</last></author>
      <author><first>Paolo</first><last>Nicoli</last></author>
      <author><first>Matteo</first><last>Palmonari</last></author>
      <pages>10065–10072</pages>
      <abstract>Understanding differences of viewpoints across corpora is a fundamental task for <a href="https://en.wikipedia.org/wiki/Computational_social_sciences">computational social sciences</a>. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two distributional representations. To this end, <a href="https://en.wikipedia.org/wiki/SWEAT">SWEAT</a> uses two additional wordsets, deemed to have opposite valence, to represent two different <a href="https://en.wikipedia.org/wiki/Zeros_and_poles">poles</a>. We validate our approach and illustrate a case study to show the usefulness of the introduced <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">measure</a>.</abstract>
      <url hash="7b9953e7">2021.emnlp-main.788</url>
      <bibkey>bianchi-etal-2021-sweat</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.788</doi>
      <pwccode url="https://github.com/vinid/sweat" additional="false">vinid/sweat</pwccode>
    </paper>
    <paper id="791">
      <title>PAUSE : Positive and Annealed Unlabeled Sentence Embedding<fixed-case>PAUSE</fixed-case>: Positive and Annealed Unlabeled Sentence Embedding</title>
      <author><first>Lele</first><last>Cao</last></author>
      <author><first>Emil</first><last>Larsson</last></author>
      <author><first>Vilhelm</first><last>von Ehrenheim</last></author>
      <author><first>Dhiana Deva</first><last>Cavalcanti Rocha</last></author>
      <author><first>Anna</first><last>Martin</last></author>
      <author><first>Sonja</first><last>Horn</last></author>
      <pages>10096–10107</pages>
      <abstract>Sentence embedding refers to a set of effective and versatile techniques for converting raw text into numerical vector representations that can be used in a wide range of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP) applications</a>. The majority of these techniques are either <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised</a> or unsupervised. Compared to the <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a>, the supervised ones make less assumptions about <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">optimization objectives</a> and usually achieve better results. However, the training requires a large amount of labeled sentence pairs, which is not available in many industrial scenarios. To that end, we propose a generic and end-to-end approach   PAUSE (Positive and Annealed Unlabeled Sentence Embedding), capable of learning high-quality sentence embeddings from a partially labeled dataset. We experimentally show that PAUSE achieves, and sometimes surpasses, state-of-the-art results using only a small fraction of labeled sentence pairs on various benchmark tasks. When applied to a real industrial use case where labeled samples are scarce, PAUSE encourages us to extend our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> without the burden of extensive manual annotation work.</abstract>
      <url hash="51096494">2021.emnlp-main.791</url>
      <bibkey>cao-etal-2021-pause</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.791</doi>
      <pwccode url="https://github.com/eqtpartners/pause" additional="false">eqtpartners/pause</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="793">
      <title>An Information-Theoretic Characterization of Morphological Fusion</title>
      <author><first>Neil</first><last>Rathi</last></author>
      <author><first>Michael</first><last>Hahn</last></author>
      <author><first>Richard</first><last>Futrell</last></author>
      <pages>10115–10120</pages>
      <abstract>Linguistic typology generally divides <a href="https://en.wikipedia.org/wiki/Synthetic_language">synthetic languages</a> into groups based on their <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological fusion</a>. However, this <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">measure</a> has long been thought to be best considered a matter of degree. We present an information-theoretic measure, called <a href="https://en.wikipedia.org/wiki/Information_fusion">informational fusion</a>, to quantify the degree of fusion of a given set of morphological features in a surface form, which naturally provides such a graded scale. Informational fusion is able to encapsulate not only concatenative, but also nonconcatenative morphological systems (e.g. Arabic), abstracting away from any notions of morpheme segmentation. We then show, on a sample of twenty-one languages, that our measure recapitulates the usual linguistic classifications for concatenative systems, and provides new measures for nonconcatenative ones. We also evaluate the long-standing hypotheses that more frequent forms are more fusional, and that paradigm size anticorrelates with degree of fusion. We do not find evidence for the idea that <a href="https://en.wikipedia.org/wiki/Language">languages</a> have characteristic levels of fusion ; rather, the degree of fusion varies across part-of-speech within languages.</abstract>
      <url hash="3ffc2dba">2021.emnlp-main.793</url>
      <bibkey>rathi-etal-2021-information</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.793</doi>
      <pwccode url="https://github.com/neilrathi/morphological-fusion" additional="false">neilrathi/morphological-fusion</pwccode>
    </paper>
    <paper id="794">
      <title>The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning</title>
      <author><first>Yuchen</first><last>Lian</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Tessa</first><last>Verhoef</last></author>
      <pages>10121–10129</pages>
      <abstract>Natural languages display a trade-off among different strategies to convey <a href="https://en.wikipedia.org/wiki/Syntax">syntactic structure</a>, such as <a href="https://en.wikipedia.org/wiki/Word_order">word order</a> or <a href="https://en.wikipedia.org/wiki/Inflection">inflection</a>. This trade-off, however, has not appeared in recent simulations of iterated language learning with neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in light of three factors that play an important role in comparable experiments from the Language Evolution field : (i) speaker bias towards efficient messaging, (ii) non systematic input languages, and (iii) learning bottleneck. Our simulations show that neural agents mainly strive to maintain the utterance type distribution observed during <a href="https://en.wikipedia.org/wiki/Learning">learning</a>, instead of developing a more efficient or systematic language.</abstract>
      <url hash="e97aea41">2021.emnlp-main.794</url>
      <bibkey>lian-etal-2021-effect</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.794</doi>
    </paper>
    <paper id="800">
      <title>UNKs Everywhere : Adapting Multilingual Language Models to New Scripts<fixed-case>UNK</fixed-case>s Everywhere: <fixed-case>A</fixed-case>dapting Multilingual Language Models to New Scripts</title>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <pages>10186–10203</pages>
      <abstract>Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model’s embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT’s and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model.</abstract>
      <url hash="9adf6cc2">2021.emnlp-main.800</url>
      <bibkey>pfeiffer-etal-2021-unks</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.800</doi>
      <pwccode url="https://github.com/adapter-hub/unks_everywhere" additional="true">adapter-hub/unks_everywhere</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="805">
      <title>Discretized Integrated Gradients for Explaining Language Models</title>
      <author><first>Soumya</first><last>Sanyal</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>10285–10299</pages>
      <abstract>As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the <a href="https://en.wikipedia.org/wiki/Gradient">gradients</a> computed at the interpolated points and consequently, the quality of the generated explanations. Here we propose Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">embedding space</a>, yielding more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encourage reproducible research.</abstract>
      <url hash="7ad89942">2021.emnlp-main.805</url>
      <bibkey>sanyal-ren-2021-discretized</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.805</doi>
      <pwccode url="https://github.com/ink-usc/dig" additional="false">ink-usc/dig</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="814">
      <title>XLEnt : Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment<fixed-case>XLE</fixed-case>nt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment</title>
      <author><first>Ahmed</first><last>El-Kishky</last></author>
      <author><first>Adithya</first><last>Renduchintala</last></author>
      <author><first>James</first><last>Cross</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>10424–10430</pages>
      <abstract>Cross-lingual named-entity lexica are an important resource to multilingual NLP tasks such as <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> and cross-lingual wikification. While <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a> contain a large number of entities in high-resource languages such as <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/French_language">French</a>, corresponding entities for lower-resource languages are often missing. To address this, we propose Lexical-Semantic-Phonetic Align (LSP-Align), a technique to automatically mine cross-lingual entity lexica from mined web data. We demonstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community.</abstract>
      <url hash="26f17252">2021.emnlp-main.814</url>
      <bibkey>el-kishky-etal-2021-xlent</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.814</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/xlent">XLEnt</pwcdataset>
    </paper>
    <paper id="816">
      <title>Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction<fixed-case>R</fixed-case>elation <fixed-case>E</fixed-case>xtraction</title>
      <author><first>Bruno</first><last>Taillé</last></author>
      <author><first>Vincent</first><last>Guigue</last></author>
      <author><first>Geoffrey</first><last>Scoutheeten</last></author>
      <author><first>Patrick</first><last>Gallinari</last></author>
      <pages>10438–10449</pages>
      <abstract>State-of-the-art NLP models can adopt shallow heuristics that limit their generalization capability (McCoy et al., 2019). Such <a href="https://en.wikipedia.org/wiki/Heuristic">heuristics</a> include lexical overlap with the training set in Named-Entity Recognition (Taille et al., 2020) and Event or Type heuristics in Relation Extraction (Rosenman et al., 2020). In the more realistic end-to-end RE setting, we can expect yet another <a href="https://en.wikipedia.org/wiki/Heuristic">heuristic</a> : the mere retention of training relation triples. In this paper we propose two experiments confirming that retention of known facts is a key factor of performance on standard <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a>. Furthermore, one experiment suggests that a <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline model</a> able to use intermediate type representations is less prone to over-rely on retention.</abstract>
      <url hash="5e46e050">2021.emnlp-main.816</url>
      <bibkey>taille-etal-2021-separating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.816</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="817">
      <title>Automatic Text Evaluation through the Lens of Wasserstein Barycenters<fixed-case>W</fixed-case>asserstein Barycenters</title>
      <author><first>Pierre</first><last>Colombo</last></author>
      <author><first>Guillaume</first><last>Staerman</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <author><first>Pablo</first><last>Piantanida</last></author>
      <pages>10450–10466</pages>
      <abstract>A new metric BaryScore to evaluate text generation based on deep contextualized embeddings (e.g., BERT, Roberta, ELMo) is introduced. This <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> is motivated by a new <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> relying on optimal transport tools, i.e., <a href="https://en.wikipedia.org/wiki/Wasserstein_distance">Wasserstein distance</a> and <a href="https://en.wikipedia.org/wiki/Barycenter">barycenter</a>. By modelling the layer output of deep contextualized embeddings as a <a href="https://en.wikipedia.org/wiki/Probability_distribution">probability distribution</a> rather than by a vector embedding ; this framework provides a natural way to aggregate the different outputs through the Wasserstein space topology. In addition, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> provides theoretical grounds to our <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> and offers an alternative to available solutions (e.g., MoverScore and BertScore). Numerical evaluation is performed on four different tasks : <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a>, data2text generation and image captioning. Our results show that BaryScore outperforms other BERT based metrics and exhibits more consistent behaviour in particular for <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a>.<i>e.g.</i>, BERT, Roberta, ELMo) is introduced. This metric is motivated by a new framework relying on optimal transport tools, <i>i.e.</i>, Wasserstein distance and barycenter. By modelling the layer output of deep contextualized embeddings as a probability distribution rather than by a vector embedding; this framework provides a natural way to aggregate the different outputs through the Wasserstein space topology. In addition, it provides theoretical grounds to our metric and offers an alternative to available solutions (<i>e.g.</i>, MoverScore and BertScore). Numerical evaluation is performed on four different tasks: machine translation, summarization, data2text generation and image captioning. Our results show that BaryScore outperforms other BERT based metrics and exhibits more consistent behaviour in particular for text summarization.</abstract>
      <url hash="db428beb">2021.emnlp-main.817</url>
      <bibkey>colombo-etal-2021-automatic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.817</doi>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="820">
      <title>Robustness Evaluation of Entity Disambiguation Using Prior Probes : the Case of Entity Overshadowing</title>
      <author><first>Vera</first><last>Provatorova</last></author>
      <author><first>Samarth</first><last>Bhargav</last></author>
      <author><first>Svitlana</first><last>Vakulenko</last></author>
      <author><first>Evangelos</first><last>Kanoulas</last></author>
      <pages>10501–10510</pages>
      <abstract>Entity disambiguation (ED) is the last step of entity linking (EL), when candidate entities are reranked according to the context they appear in. All datasets for training and evaluating models for EL consist of convenience samples, such as news articles and <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a>, that propagate the prior probability bias of the <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity distribution</a> towards more frequently occurring entities. It was shown that the performance of the EL systems on such <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> is overestimated since it is possible to obtain higher accuracy scores by merely learning the prior. To provide a more adequate evaluation benchmark, we introduce the ShadowLink dataset, which includes 16 K short text snippets annotated with entity mentions. We evaluate and report the performance of popular EL systems on the ShadowLink benchmark. The results show a considerable difference in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> between more and less common entities for all of the EL systems under evaluation, demonstrating the effect of prior probability bias and entity overshadowing.</abstract>
      <url hash="e98cd7fe">2021.emnlp-main.820</url>
      <bibkey>provatorova-etal-2021-robustness</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.820</doi>
    </paper>
    <paper id="821">
      <title>IndoNLI : A Natural Language Inference Dataset for Indonesian<fixed-case>I</fixed-case>ndo<fixed-case>NLI</fixed-case>: A Natural Language Inference Dataset for <fixed-case>I</fixed-case>ndonesian</title>
      <author><first>Rahmad</first><last>Mahendra</last></author>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <author><first>Samuel</first><last>Louvan</last></author>
      <author><first>Fahrurrozi</first><last>Rahman</last></author>
      <author><first>Clara</first><last>Vania</last></author>
      <pages>10511–10527</pages>
      <abstract>We present IndoNLI, the first human-elicited NLI dataset for <a href="https://en.wikipedia.org/wiki/Indonesian_language">Indonesian</a>. We adapt the data collection protocol for MNLI and collect ~18 K sentence pairs annotated by crowd workers and experts. The expert-annotated data is used exclusively as a test set. It is designed to provide a challenging test-bed for Indonesian NLI by explicitly incorporating various linguistic phenomena such as numerical reasoning, structural changes, idioms, or temporal and spatial reasoning. Experiment results show that XLM-R outperforms other pre-trained models in our data. The best performance on the expert-annotated data is still far below <a href="https://en.wikipedia.org/wiki/Human_factors_and_ergonomics">human performance</a> (13.4 % accuracy gap), suggesting that this <a href="https://en.wikipedia.org/wiki/Test_set">test set</a> is especially challenging. Furthermore, our analysis shows that our expert-annotated data is more diverse and contains fewer annotation artifacts than the crowd-annotated data. We hope this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> can help accelerate progress in Indonesian NLP research.</abstract>
      <url hash="19c57970">2021.emnlp-main.821</url>
      <bibkey>mahendra-etal-2021-indonli</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.821</doi>
      <pwccode url="https://github.com/ir-nlp-csui/indonli" additional="false">ir-nlp-csui/indonli</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/indonli">IndoNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/indonlu-benchmark">IndoNLU Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ocnli">OCNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="824">
      <title>Efficient Sampling of Dependency Structure</title>
      <author><first>Ran</first><last>Zmigrod</last></author>
      <author><first>Tim</first><last>Vieira</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>10558–10569</pages>
      <abstract>Probabilistic distributions over spanning trees in <a href="https://en.wikipedia.org/wiki/Directed_graph">directed graphs</a> are a fundamental model of dependency structure in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, syntactic dependency trees. In <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, dependency trees often have an additional root constraint : only one edge may emanate from the root. However, no <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">sampling algorithm</a> has been presented in the literature to account for this additional <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraint</a>. In this paper, we adapt two spanning tree sampling algorithms to faithfully sample dependency trees from a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> subject to the root constraint. Wilson (1996 (’s sampling algorithm has a <a href="https://en.wikipedia.org/wiki/Time_complexity">running time</a> of O(H) where H is the mean hitting time of the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>. Colbourn (1996)’s sampling algorithm has a <a href="https://en.wikipedia.org/wiki/Time_complexity">running time</a> of O(N3), which is often greater than the mean hitting time of a <a href="https://en.wikipedia.org/wiki/Directed_graph">directed graph</a>. Additionally, we build upon Colbourn’s algorithm and present a novel extension that can sample K trees without replacement in O(K N3 + K2 N) time. To the best of our knowledge, no <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> has been given for sampling spanning trees without replacement from a <a href="https://en.wikipedia.org/wiki/Directed_graph">directed graph</a>.</abstract>
      <url hash="2c5b6dd2">2021.emnlp-main.824</url>
      <bibkey>zmigrod-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.824</doi>
      <pwccode url="https://github.com/rycolab/treesample" additional="false">rycolab/treesample</pwccode>
    </paper>
    <paper id="825">
      <title>Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering</title>
      <author><first>Daniel</first><last>Fernández-González</last></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last></author>
      <pages>10570–10578</pages>
      <abstract>Discontinuous constituent parsers have always lagged behind continuous approaches in terms of <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and <a href="https://en.wikipedia.org/wiki/Speed">speed</a>, as the presence of constituents with discontinuous yield introduces extra <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> to the task. However, a discontinuous tree can be converted into a continuous variant by reordering tokens. Based on that, we propose to reduce discontinuous parsing to a continuous problem, which can then be directly solved by any off-the-shelf continuous parser. To that end, we develop a Pointer Network capable of accurately generating the continuous token arrangement for a given input sentence and define a <a href="https://en.wikipedia.org/wiki/Bijection">bijective function</a> to recover the original order. Experiments on the main benchmarks with two continuous parsers prove that our approach is on par in accuracy with purely discontinuous state-of-the-art algorithms, but considerably faster.</abstract>
      <url hash="dd393819">2021.emnlp-main.825</url>
      <bibkey>fernandez-gonzalez-gomez-rodriguez-2021-reducing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.825</doi>
      <pwccode url="https://github.com/danifg/Pointer-Network-Reordering" additional="false">danifg/Pointer-Network-Reordering</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="826">
      <title>A New Representation for Span-based CCG Parsing<fixed-case>CCG</fixed-case> Parsing</title>
      <author><first>Yoshihide</first><last>Kato</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <pages>10579–10584</pages>
      <abstract>This paper proposes a new <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representation</a> for CCG derivations. CCG derivations are represented as trees whose nodes are labeled with categories strictly restricted by CCG rule schemata. This characteristic is not suitable for span-based parsing models because they predict node labels independently. In other words, span-based models may generate invalid CCG derivations that violate the rule schemata. Our proposed representation decomposes CCG derivations into several independent pieces and prevents the span-based parsing models from violating the schemata. Our experimental result shows that an off-the-shelf span-based parser with our representation is comparable with previous CCG parsers.</abstract>
      <url hash="fc23b410">2021.emnlp-main.826</url>
      <bibkey>kato-matsubara-2021-new</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.826</doi>
      <pwccode url="https://github.com/yosihide/span-based-ccg-derivation" additional="false">yosihide/span-based-ccg-derivation</pwccode>
    </paper>
    <paper id="828">
      <title>PermuteFormer : Efficient Relative Position Encoding for Long Sequences<fixed-case>P</fixed-case>ermute<fixed-case>F</fixed-case>ormer: Efficient Relative Position Encoding for Long Sequences</title>
      <author><first>Peng</first><last>Chen</last></author>
      <pages>10606–10618</pages>
      <abstract>A recent variation of <a href="https://en.wikipedia.org/wiki/Transformer">Transformer</a>, <a href="https://en.wikipedia.org/wiki/Performer">Performer</a>, scales <a href="https://en.wikipedia.org/wiki/Transformer">Transformer</a> to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to <a href="https://en.wikipedia.org/wiki/Performer_(disambiguation)">Performer</a>. Based on the analysis, we propose PermuteFormer, a Performer-based model with relative position encoding that scales linearly on long sequences. PermuteFormer applies position-dependent transformation on queries and keys to encode positional information into the attention module. This <a href="https://en.wikipedia.org/wiki/Transformation_(function)">transformation</a> is carefully crafted so that the final output of self-attention is not affected by absolute positions of tokens. PermuteFormer introduces negligible <a href="https://en.wikipedia.org/wiki/Overhead_(computing)">computational overhead</a> by design that <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> runs as fast as Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long sequences, as well as WikiText-103, a language modeling dataset. The experiments show that PermuteFormer uniformly improves the performance of Performer with almost no <a href="https://en.wikipedia.org/wiki/Overhead_(computing)">computational overhead</a> and outperforms vanilla Transformer on most of the tasks.</abstract>
      <url hash="04b30d1c">2021.emnlp-main.828</url>
      <bibkey>chen-2021-permuteformer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.828</doi>
      <pwccode url="https://github.com/cpcp1998/permuteformer" additional="false">cpcp1998/permuteformer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="829">
      <title>Block Pruning For Faster Transformers</title>
      <author><first>François</first><last>Lagunas</last></author>
      <author><first>Ella</first><last>Charlaix</last></author>
      <author><first>Victor</first><last>Sanh</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>10619–10629</pages>
      <abstract>Pre-training has improved <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">model accuracy</a> for both classification and generation tasks at the cost of introducing much larger and slower <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up <a href="https://en.wikipedia.org/wiki/Inference">inference</a>. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine-tuning. We find that this approach learns to prune out full components of the underlying <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>, such as attention heads. Experiments consider classification and generation tasks, yielding among other results a pruned model that is a 2.4x faster, 74 % smaller BERT on SQuAD v1, with a 1 % drop on F1, competitive both with distilled models in speed and pruned models in size.</abstract>
      <url hash="38185732">2021.emnlp-main.829</url>
      <bibkey>lagunas-etal-2021-block</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.829</doi>
      <pwccode url="https://github.com/huggingface/nn_pruning" additional="false">huggingface/nn_pruning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quora-question-pairs">Quora Question Pairs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="831">
      <title>How to Train BERT with an Academic Budget<fixed-case>BERT</fixed-case> with an Academic Budget</title>
      <author><first>Peter</first><last>Izsak</last></author>
      <author><first>Moshe</first><last>Berchansky</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>10644–10652</pages>
      <abstract>While large language models a la BERT are used ubiquitously in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, pretraining them is considered a luxury that only a few well-funded industry labs can afford. How can one train such <a href="https://en.wikipedia.org/wiki/Physical_model">models</a> with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a single low-end deep learning server. We demonstrate that through a combination of software optimizations, design choices, and hyperparameter tuning, it is possible to produce models that are competitive with BERT-base on GLUE tasks at a fraction of the original pretraining cost.</abstract>
      <url hash="af151510">2021.emnlp-main.831</url>
      <bibkey>izsak-etal-2021-train</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.831</doi>
      <pwccode url="https://github.com/peteriz/academic-budget-bert" additional="true">peteriz/academic-budget-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quora-question-pairs">Quora Question Pairs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rte">RTE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-benchmark">STS Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="834">
      <title>Pushing on Text Readability Assessment : A Transformer Meets Handcrafted Linguistic Features</title>
      <author><first>Bruce W.</first><last>Lee</last></author>
      <author><first>Yoo Sung</first><last>Jang</last></author>
      <author><first>Jason</first><last>Lee</last></author>
      <pages>10669–10686</pages>
      <abstract>We report two essential improvements in readability assessment : 1. three novel features in advanced semantics and 2. the timely evidence that traditional ML models (e.g. Random Forest, using handcrafted features) can combine with <a href="https://en.wikipedia.org/wiki/Transformer">transformers</a> (e.g. RoBERTa) to augment <a href="https://en.wikipedia.org/wiki/Computer_simulation">model</a> performance. First, we explore suitable <a href="https://en.wikipedia.org/wiki/Transformer">transformers</a> and traditional ML models. Then, we extract 255 handcrafted linguistic features using self-developed extraction software. Finally, we assemble those to create several hybrid models, achieving state-of-the-art (SOTA) accuracy on popular <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> in readability assessment. The use of handcrafted features help model performance on smaller datasets. Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification accuracy of 99 %, a 20.3 % increase from the previous SOTA.</abstract>
      <url hash="75ab197c">2021.emnlp-main.834</url>
      <bibkey>lee-etal-2021-pushing</bibkey>
      <revision id="1" href="2021.emnlp-main.834v1" hash="68ff98b2" />
      <revision id="2" href="2021.emnlp-main.834v2" hash="75ab197c" date="2021-11-09">Corrected a typo.</revision>
      <doi>10.18653/v1/2021.emnlp-main.834</doi>
      <pwccode url="https://github.com/brucewlee/lingfeat" additional="false">brucewlee/lingfeat</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopenglish">OneStopEnglish</pwcdataset>
    </paper>
    <paper id="837">
      <title>MTAdam : Automatic Balancing of Multiple Training Loss Terms<fixed-case>MTA</fixed-case>dam: Automatic Balancing of Multiple Training Loss Terms</title>
      <author><first>Itzik</first><last>Malkiel</last></author>
      <author><first>Lior</first><last>Wolf</last></author>
      <pages>10713–10729</pages>
      <abstract>When training neural models, it is common to combine multiple loss terms. The balancing of these <a href="https://en.wikipedia.org/wiki/Term_(logic)">terms</a> requires considerable human effort and is computationally demanding. Moreover, the optimal trade-off between the <a href="https://en.wikipedia.org/wiki/Loss_function">loss terms</a> can change as training progresses, e.g., for <a href="https://en.wikipedia.org/wiki/Loss_function">adversarial terms</a>. In this work, we generalize the Adam optimization algorithm to handle multiple loss terms. The guiding principle is that for every layer, the gradient magnitude of the terms should be balanced. To this end, the Multi-Term Adam (MTAdam) computes the <a href="https://en.wikipedia.org/wiki/Derivative">derivative</a> of each loss term separately, infers the first and second moments per parameter and loss term, and calculates a first moment for the magnitude per layer of the gradients arising from each loss. This magnitude is used to continuously balance the <a href="https://en.wikipedia.org/wiki/Gradient">gradients</a> across all layers, in a manner that both varies from one layer to the next and dynamically changes over time. Our results show that training with the new method leads to fast recovery from suboptimal initial loss weighting and to training outcomes that match or improve conventional training with the prescribed hyperparameters of each method.</abstract>
      <url hash="1b91b669">2021.emnlp-main.837</url>
      <bibkey>malkiel-wolf-2021-mtadam</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.837</doi>
      <pwccode url="https://github.com/ItzikMalkiel/MTAdam" additional="false">ItzikMalkiel/MTAdam</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bsd">BSD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/set14">Set14</pwcdataset>
    </paper>
    <paper id="839">
      <title>Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning</title>
      <author><first>Xinghua</first><last>Zhang</last></author>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Tingwen</first><last>Liu</last></author>
      <author><first>Zhenyu</first><last>Zhang</last></author>
      <author><first>Jiawei</first><last>Sheng</last></author>
      <author><first>Xue</first><last>Mengge</last></author>
      <author><first>Hongbo</first><last>Xu</last></author>
      <pages>10746–10757</pages>
      <abstract>Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotations, while most prior denoising works are only concerned with one kind of <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a> and fail to fully explore useful information in the training set. To address this issue, we propose a robust learning paradigm named Self-Collaborative Denoising Learning (SCDL), which jointly trains two teacher-student networks in a mutually-beneficial manner to iteratively perform noisy label refinery. Each network is designed to exploit reliable labels via self denoising, and two networks communicate with each other to explore unreliable annotations by collaborative denoising. Extensive experimental results on five real-world datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising methods.</abstract>
      <url hash="808956ac">2021.emnlp-main.839</url>
      <bibkey>zhang-etal-2021-improving-distantly-supervised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.839</doi>
      <pwccode url="https://github.com/airobotzhang/scdl" additional="false">airobotzhang/scdl</pwccode>
    </paper>
    <paper id="842">
      <title>VeeAlign : Multifaceted Context Representation Using Dual Attention for Ontology Alignment<fixed-case>V</fixed-case>ee<fixed-case>A</fixed-case>lign: Multifaceted Context Representation Using Dual Attention for Ontology Alignment</title>
      <author><first>Vivek</first><last>Iyer</last></author>
      <author><first>Arvind</first><last>Agarwal</last></author>
      <author id="harshit-kumar"><first>Harshit</first><last>Kumar</last></author>
      <pages>10780–10792</pages>
      <abstract>Ontology Alignment is an important research problem applied to various fields such as <a href="https://en.wikipedia.org/wiki/Data_integration">data integration</a>, <a href="https://en.wikipedia.org/wiki/Data_transmission">data transfer</a>, <a href="https://en.wikipedia.org/wiki/Data_preparation">data preparation</a>, etc. State-of-the-art (SOTA) Ontology Alignment systems typically use naive domain-dependent approaches with handcrafted rules or domain-specific architectures, making them unscalable and inefficient. In this work, we propose VeeAlign, a Deep Learning based model that uses a novel dual-attention mechanism to compute the contextualized representation of a concept which, in turn, is used to discover alignments. By doing this, not only is our approach able to exploit both syntactic and semantic information encoded in <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontologies</a>, it is also, by design, flexible and scalable to different domains with minimal effort. We evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on four different <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> from different domains and languages, and establish its superiority through these results as well as detailed ablation studies. The code and datasets used are available at https://github.com/Remorax/VeeAlign.</abstract>
      <url hash="592d6a76">2021.emnlp-main.842</url>
      <attachment type="Software" hash="cb52034e">2021.emnlp-main.842.Software.zip</attachment>
      <bibkey>iyer-etal-2021-veealign</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.842</doi>
      <pwccode url="https://github.com/remorax/veealign" additional="false">remorax/veealign</pwccode>
    </paper>
    <paper id="844">
      <title>GeneSis : A Generative Approach to Substitutes in Context<fixed-case>G</fixed-case>ene<fixed-case>S</fixed-case>is: <fixed-case>A</fixed-case> <fixed-case>G</fixed-case>enerative <fixed-case>A</fixed-case>pproach to <fixed-case>S</fixed-case>ubstitutes in <fixed-case>C</fixed-case>ontext</title>
      <author><first>Caterina</first><last>Lacerra</last></author>
      <author><first>Rocco</first><last>Tripodi</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>10810–10823</pages>
      <abstract>The lexical substitution task aims at generating a list of suitable replacements for a target word in context, ideally keeping the meaning of the modified text unchanged. While its usage has increased in recent years, the paucity of annotated data prevents the finetuning of neural models on the task, hindering the full fruition of recently introduced powerful architectures such as <a href="https://en.wikipedia.org/wiki/Language_model">language models</a>. Furthermore, <a href="https://en.wikipedia.org/wiki/Lexical_substitution">lexical substitution</a> is usually evaluated in a framework that is strictly bound to a limited vocabulary, making it impossible to credit appropriate, but out-of-vocabulary, substitutes. To assess these issues, we proposed GeneSis (Generating Substitutes in contexts), the first generative approach to <a href="https://en.wikipedia.org/wiki/Lexical_substitution">lexical substitution</a>. Thanks to a seq2seq model, we generate substitutes for a word according to the context it appears in, attaining state-of-the-art results on different <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a>. Moreover, our approach allows silver data to be produced for further improving the performances of lexical substitution systems. Along with an extensive analysis of GeneSis results, we also present a human evaluation of the generated substitutes in order to assess their quality. We release the fine-tuned models, the generated datasets, and the code to reproduce the experiments at https://github.com/SapienzaNLP/genesis.</abstract>
      <url hash="f3fb91e0">2021.emnlp-main.844</url>
      <bibkey>lacerra-etal-2021-genesis</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.844</doi>
      <pwccode url="https://github.com/sapienzanlp/genesis" additional="false">sapienzanlp/genesis</pwccode>
    </paper>
    <paper id="847">
      <title>Detecting Contact-Induced Semantic Shifts : What Can Embedding-Based Methods Do in Practice?<fixed-case>W</fixed-case>hat Can Embedding-Based Methods Do in Practice?</title>
      <author><first>Filip</first><last>Miletic</last></author>
      <author><first>Anne</first><last>Przewozny-Desriaux</last></author>
      <author><first>Ludovic</first><last>Tanguy</last></author>
      <pages>10852–10865</pages>
      <abstract>This study investigates the applicability of semantic change detection methods in descriptively oriented linguistic research. It specifically focuses on contact-induced semantic shifts in <a href="https://en.wikipedia.org/wiki/Quebec_English">Quebec English</a>. We contrast synchronic data from different regions in order to identify the meanings that are specific to <a href="https://en.wikipedia.org/wiki/Quebec">Quebec</a> and potentially related to <a href="https://en.wikipedia.org/wiki/Language_contact">language contact</a>. Type-level embeddings are used to detect new semantic shifts, and token-level embeddings to isolate regionally specific occurrences. We introduce a new 80-item test set and conduct both quantitative and qualitative evaluations. We demonstrate that diachronic word embedding methods can be applied to contact-induced semantic shifts observed in <a href="https://en.wikipedia.org/wiki/Synchrony_and_diachrony">synchrony</a>, obtaining results comparable to the state of the art on similar tasks in <a href="https://en.wikipedia.org/wiki/Diachrony">diachrony</a>. However, we show that encouraging evaluation results do not translate to practical value in detecting new semantic shifts. Finally, our application of token-level embeddings accelerates manual data exploration and provides an efficient way of scaling up sociolinguistic analyses.</abstract>
      <url hash="336fccdb">2021.emnlp-main.847</url>
      <bibkey>miletic-etal-2021-detecting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.847</doi>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</booktitle>
      <editor><first>Heike</first><last>Adel</last></editor>
      <editor><first>Shuming</first><last>Shi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online and Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="724a2c21">2021.emnlp-demo.0</url>
      <bibkey>emnlp-2021-demo</bibkey>
    </frontmatter>
    <paper id="12">
      <title>LMdiff : A Visual Diff Tool to Compare <a href="https://en.wikipedia.org/wiki/Language_model">Language Models</a><fixed-case>LM</fixed-case>diff: A Visual Diff Tool to Compare Language Models</title>
      <author><first>Hendrik</first><last>Strobelt</last></author>
      <author><first>Benjamin</first><last>Hoover</last></author>
      <author><first>Arvind</first><last>Satyanaryan</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <pages>96–105</pages>
      <abstract>While different language models are ubiquitous in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, it is hard to contrast their outputs and identify which contexts one can handle better than the other. To address this question, we introduce LMdiff, a tool that visually compares <a href="https://en.wikipedia.org/wiki/Probability_distribution">probability distributions</a> of two <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> that differ, e.g., through <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a>, <a href="https://en.wikipedia.org/wiki/Distillation">distillation</a>, or simply training with different parameter sizes. LMdiff allows the generation of hypotheses about model behavior by investigating text instances token by token and further assists in choosing these interesting text instances by identifying the most interesting phrases from large corpora. We showcase the applicability of LMdiff for hypothesis generation across multiple case studies. A demo is available at http://lmdiff.net.</abstract>
      <url hash="2bac39a1">2021.emnlp-demo.12</url>
      <bibkey>strobelt-etal-2021-lmdiff</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.12</doi>
      <pwccode url="https://github.com/hendrikstrobelt/lmdiff" additional="false">hendrikstrobelt/lmdiff</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="14">
      <title>Beyond <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">Accuracy</a> : A Consolidated Tool for Visual Question Answering Benchmarking</title>
      <author><first>Dirk</first><last>Väth</last></author>
      <author><first>Pascal</first><last>Tilli</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>114–123</pages>
      <abstract>On the way towards general Visual Question Answering (VQA) systems that are able to answer arbitrary questions, the need arises for evaluation beyond single-metric leaderboards for specific datasets. To this end, we propose a browser-based benchmarking tool for researchers and challenge organizers, with an API for easy integration of new models and datasets to keep up with the fast-changing landscape of VQA. Our tool helps test generalization capabilities of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> across multiple datasets, evaluating not just <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, but also performance in more realistic real-world scenarios such as <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> to input noise. Additionally, we include <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> that measure <a href="https://en.wikipedia.org/wiki/Bias">biases</a> and <a href="https://en.wikipedia.org/wiki/Uncertainty">uncertainty</a>, to further explain model behavior. Interactive filtering facilitates discovery of problematic behavior, down to the <a href="https://en.wikipedia.org/wiki/Sample_(statistics)">data sample level</a>. As proof of concept, we perform a case study on four <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. We find that state-of-the-art VQA models are optimized for specific tasks or datasets, but fail to generalize even to other in-domain test sets, for example they can not recognize text in images. Our <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> allow us to quantify which image and question embeddings provide most robustness to a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. All code s publicly available.</abstract>
      <url hash="d19ba3fb">2021.emnlp-demo.14</url>
      <bibkey>vath-etal-2021-beyond</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.14</doi>
      <pwccode url="https://github.com/patilli/vqa_benchmarking" additional="false">patilli/vqa_benchmarking</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ok-vqa">OK-VQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/textvqa">TextVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="15">
      <title>Athena 2.0 : Contextualized Dialogue Management for an Alexa Prize SocialBot<fixed-case>A</fixed-case>lexa <fixed-case>P</fixed-case>rize <fixed-case>S</fixed-case>ocial<fixed-case>B</fixed-case>ot</title>
      <author><first>Juraj</first><last>Juraska</last></author>
      <author><first>Kevin</first><last>Bowden</last></author>
      <author><first>Lena</first><last>Reed</last></author>
      <author><first>Vrindavan</first><last>Harrison</last></author>
      <author><first>Wen</first><last>Cui</last></author>
      <author><first>Omkar</first><last>Patil</last></author>
      <author><first>Rishi</first><last>Rajasekaran</last></author>
      <author><first>Angela</first><last>Ramirez</last></author>
      <author><first>Cecilia</first><last>Li</last></author>
      <author><first>Eduardo</first><last>Zamora</last></author>
      <author><first>Phillip</first><last>Lee</last></author>
      <author><first>Jeshwanth</first><last>Bheemanpally</last></author>
      <author><first>Rohan</first><last>Pandey</last></author>
      <author><first>Adwait</first><last>Ratnaparkhi</last></author>
      <author><first>Marilyn</first><last>Walker</last></author>
      <pages>124–133</pages>
      <abstract>Athena 2.0 is an <a href="https://en.wikipedia.org/wiki/Alexa_Internet">Alexa Prize SocialBot</a> that has been a finalist in the last two Alexa Prize Grand Challenges. One reason for Athena’s success is its novel dialogue management strategy, which allows it to dynamically construct dialogues and responses from component modules, leading to novel conversations with every interaction. Here we describe <a href="https://en.wikipedia.org/wiki/Athena">Athena</a>’s system design and performance in the <a href="https://en.wikipedia.org/wiki/Alexa_Internet">Alexa Prize</a> during the 20/21 competition. A live demo of <a href="https://en.wikipedia.org/wiki/Athena">Athena</a> as well as video recordings will provoke discussion on the state of the art in conversational AI.</abstract>
      <url hash="3c5b8a47">2021.emnlp-demo.15</url>
      <bibkey>walker-etal-2021-athena</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.15</doi>
    </paper>
    <paper id="19">
      <title>UMR-Writer : A Web Application for Annotating Uniform Meaning Representations<fixed-case>UMR</fixed-case>-Writer: A Web Application for Annotating Uniform Meaning Representations</title>
      <author><first>Jin</first><last>Zhao</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <author><first>Jens</first><last>Van Gysel</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>160–167</pages>
      <abstract>We present UMR-Writer, a web-based application for annotating Uniform Meaning Representations (UMR), a graph-based, cross-linguistically applicable semantic representation developed recently to support the development of interpretable natural language applications that require deep semantic analysis of texts. We present the functionalities of UMR-Writer and discuss the challenges in developing such a <a href="https://en.wikipedia.org/wiki/Tool">tool</a> and how they are addressed.</abstract>
      <url hash="db7c4db2">2021.emnlp-demo.19</url>
      <bibkey>zhao-etal-2021-umr</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.19</doi>
    </paper>
    <paper id="22">
      <title>Summary Explorer : Visualizing the State of the Art in Text Summarization</title>
      <author><first>Shahbaz</first><last>Syed</last></author>
      <author><first>Tariq</first><last>Yousef</last></author>
      <author><first>Khalid</first><last>Al Khatib</last></author>
      <author><first>Stefan</first><last>Jänicke</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <pages>185–194</pages>
      <abstract>This paper introduces Summary Explorer, a new tool to support the manual inspection of text summarization systems by compiling the outputs of 55 state-of-the-art single document summarization approaches on three benchmark datasets, and visually exploring them during a qualitative assessment. The underlying design of the tool considers three well-known summary quality criteria (coverage, <a href="https://en.wikipedia.org/wiki/Faithfulness">faithfulness</a>, and position bias), encapsulated in a guided assessment based on tailored visualizations. The <a href="https://en.wikipedia.org/wiki/Tool">tool</a> complements existing approaches for locally debugging summarization models and improves upon them. The <a href="https://en.wikipedia.org/wiki/Tool">tool</a> is available at https://tldr.webis.de/</abstract>
      <url hash="f2634d04">2021.emnlp-demo.22</url>
      <bibkey>syed-etal-2021-summary</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.22</doi>
      <pwccode url="https://github.com/webis-de/summary-explorer" additional="false">webis-de/summary-explorer</pwccode>
    </paper>
    <paper id="23">
      <title>MeetDot : <a href="https://en.wikipedia.org/wiki/Videotelephony">Videoconferencing</a> with Live Translation Captions<fixed-case>M</fixed-case>eet<fixed-case>D</fixed-case>ot: Videoconferencing with Live Translation Captions</title>
      <author><first>Arkady</first><last>Arkhangorodsky</last></author>
      <author><first>Christopher</first><last>Chu</last></author>
      <author><first>Scot</first><last>Fang</last></author>
      <author><first>Yiqi</first><last>Huang</last></author>
      <author><first>Denglin</first><last>Jiang</last></author>
      <author><first>Ajay</first><last>Nagesh</last></author>
      <author><first>Boliang</first><last>Zhang</last></author>
      <author><first>Kevin</first><last>Knight</last></author>
      <pages>195–202</pages>
      <abstract>We present MeetDot, a videoconferencing system with live translation captions overlaid on screen. The <a href="https://en.wikipedia.org/wiki/System">system</a> aims to facilitate conversation between people who speak different languages, thereby reducing <a href="https://en.wikipedia.org/wiki/Language_barrier">communication barriers</a> between multilingual participants. Currently, our system supports <a href="https://en.wikipedia.org/wiki/Speech">speech</a> and captions in 4 languages and combines <a href="https://en.wikipedia.org/wiki/Speech_recognition">automatic speech recognition (ASR)</a> and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation (MT)</a> in a cascade. We use the re-translation strategy to translate the streamed speech, resulting in caption flicker. Additionally, our <a href="https://en.wikipedia.org/wiki/System">system</a> has very strict latency requirements to have acceptable call quality. We implement several features to enhance <a href="https://en.wikipedia.org/wiki/User_experience">user experience</a> and reduce their <a href="https://en.wikipedia.org/wiki/Cognitive_load">cognitive load</a>, such as smooth scrolling captions and reducing caption flicker. The modular architecture allows us to integrate different ASR and MT services in our backend. Our system provides an integrated evaluation suite to optimize key intrinsic evaluation metrics such as <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a> and erasure. Finally, we present an innovative cross-lingual word-guessing game as an extrinsic evaluation metric to measure end-to-end system performance. We plan to make our system open-source for research purposes.</abstract>
      <url hash="b6a7f620">2021.emnlp-demo.23</url>
      <bibkey>arkhangorodsky-etal-2021-meetdot</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.23</doi>
    </paper>
    <paper id="25">
      <title>LexiClean : An annotation tool for rapid multi-task lexical normalisation<fixed-case>L</fixed-case>exi<fixed-case>C</fixed-case>lean: An annotation tool for rapid multi-task lexical normalisation</title>
      <author><first>Tyler</first><last>Bikaun</last></author>
      <author><first>Tim</first><last>French</last></author>
      <author><first>Melinda</first><last>Hodkiewicz</last></author>
      <author><first>Michael</first><last>Stewart</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <pages>212–219</pages>
      <abstract>NLP systems are often challenged by difficulties arising from noisy, non-standard, and domain specific corpora. The task of lexical normalisation aims to standardise such <a href="https://en.wikipedia.org/wiki/Text_corpus">corpora</a>, but currently lacks suitable tools to acquire high-quality annotated data to support <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning based approaches</a>. In this paper, we present LexiClean, the first open-source web-based annotation tool for multi-task lexical normalisation. LexiClean’s main contribution is support for simultaneous in situ token-level modification and <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> that can be rapidly applied corpus wide. We demonstrate the usefulness of our <a href="https://en.wikipedia.org/wiki/Tool">tool</a> through a case study on two sets of noisy corpora derived from the specialised-domain of industrial mining. We show that LexiClean allows for the rapid and efficient development of high-quality parallel corpora. A demo of our <a href="https://en.wikipedia.org/wiki/System">system</a> is available at : https://youtu.be/P7_ooKrQPDU.</abstract>
      <url hash="d0a345c6">2021.emnlp-demo.25</url>
      <bibkey>bikaun-etal-2021-lexiclean</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.25</doi>
      <pwccode url="https://github.com/nlp-tlp/lexiclean" additional="false">nlp-tlp/lexiclean</pwccode>
    </paper>
    <paper id="26">
      <title>T3-Vis : visual analytic for Training and fine-Tuning Transformers in NLP<fixed-case>NLP</fixed-case></title>
      <author><first>Raymond</first><last>Li</last></author>
      <author><first>Wen</first><last>Xiao</last></author>
      <author><first>Lanjun</first><last>Wang</last></author>
      <author><first>Hyeju</first><last>Jang</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>220–230</pages>
      <abstract>Transformers are the dominant architecture in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>’s intrinsic properties and behaviours. Our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> offers an intuitive overview that allows the user to explore different facets of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> (e.g., hidden states, attention) through <a href="https://en.wikipedia.org/wiki/Interactive_visualization">interactive visualization</a>, and allows a suite of built-in algorithms that compute the importance of model components and different parts of the input sequence. Case studies and feedback from a user focus group indicate that the <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> is useful, and suggest several improvements. Our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> is available at : https://github.com/raymondzmc/T3-Vis.</abstract>
      <url hash="acef0e4c">2021.emnlp-demo.26</url>
      <bibkey>li-etal-2021-t3</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.26</doi>
      <pwccode url="https://github.com/raymondzmc/t3-vis" additional="false">raymondzmc/t3-vis</pwccode>
    </paper>
    <paper id="28">
      <title>OpenFraming : Open-sourced Tool for Computational Framing Analysis of Multilingual Data<fixed-case>O</fixed-case>pen<fixed-case>F</fixed-case>raming: Open-sourced Tool for Computational Framing Analysis of Multilingual Data</title>
      <author><first>Vibhu</first><last>Bhatia</last></author>
      <author><first>Vidya Prasad</first><last>Akavoor</last></author>
      <author><first>Sejin</first><last>Paik</last></author>
      <author><first>Lei</first><last>Guo</last></author>
      <author><first>Mona</first><last>Jalal</last></author>
      <author><first>Alyssa</first><last>Smith</last></author>
      <author><first>David Assefa</first><last>Tofu</last></author>
      <author><first>Edward Edberg</first><last>Halim</last></author>
      <author><first>Yimeng</first><last>Sun</last></author>
      <author><first>Margrit</first><last>Betke</last></author>
      <author><first>Prakash</first><last>Ishwar</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>242–250</pages>
      <abstract>When journalists cover a news story, they can cover the story from multiple angles or perspectives. These <a href="https://en.wikipedia.org/wiki/Point_of_view_(philosophy)">perspectives</a> are called frames, and usage of one frame or another may influence public perception and opinion of the issue at hand. We develop a <a href="https://en.wikipedia.org/wiki/Web_application">web-based system</a> for analyzing frames in multilingual text documents. We propose and guide users through a five-step end-to-end computational framing analysis framework grounded in <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">media framing theory</a> in <a href="https://en.wikipedia.org/wiki/Communication_studies">communication research</a>. Users can use the framework to analyze multilingual text data, starting from the exploration of frames in user’s corpora and through review of previous framing literature (step 1-3) to frame classification (step 4) and prediction (step 5). The framework combines unsupervised and supervised machine learning and leverages a state-of-the-art (SoTA) multilingual language model, which can significantly enhance frame prediction performance while requiring a considerably small sample of manual annotations. Through the interactive website, anyone can perform the proposed computational framing analysis, making advanced computational analysis available to researchers without a programming background and bridging the digital divide within the communication research discipline in particular and the academic community in general. The system is available online at http://www.openframing.org, via an API http://www.openframing.org:5000/docs/, or through our GitHub page https://github.com/vibss2397/openFraming.</abstract>
      <url hash="42c2be30">2021.emnlp-demo.28</url>
      <bibkey>bhatia-etal-2021-openframing</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.28</doi>
      <pwccode url="https://github.com/vibss2397/openframing" additional="false">vibss2397/openframing</pwccode>
    </paper>
    <paper id="32">
      <title>CroAno : A Crowd Annotation Platform for Improving Label Consistency of Chinese NER Dataset<fixed-case>C</fixed-case>ro<fixed-case>A</fixed-case>no : A Crowd Annotation Platform for Improving Label Consistency of <fixed-case>C</fixed-case>hinese <fixed-case>NER</fixed-case> Dataset</title>
      <author><first>Baoli</first><last>Zhang</last></author>
      <author><first>Zhucong</first><last>Li</last></author>
      <author><first>Zhen</first><last>Gan</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Jing</first><last>Wan</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Shengping</first><last>Liu</last></author>
      <author><first>Yafei</first><last>Shi</last></author>
      <pages>275–282</pages>
      <abstract>In this paper, we introduce CroAno, a web-based crowd annotation platform for the Chinese named entity recognition (NER). Besides some basic features for crowd annotation like fast tagging and <a href="https://en.wikipedia.org/wiki/Data_management">data management</a>, CroAno provides a systematic solution for improving label consistency of Chinese NER dataset. 1) Disagreement Adjudicator : CroAno uses a multi-dimensional highlight mode to visualize instance-level inconsistent entities and makes the revision process user-friendly. 2) Inconsistency Detector : CroAno employs a detector to locate corpus-level label inconsistency and provides users an interface to correct inconsistent entities in batches. 3) Prediction Error Analyzer : We deconstruct the entity prediction error of the model to six fine-grained entity error types. Users can employ this <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">error system</a> to detect corpus-level inconsistency from a model perspective. To validate the effectiveness of our <a href="https://en.wikipedia.org/wiki/Computing_platform">platform</a>, we use CroAno to revise two public datasets. In the two revised datasets, we get an improvement of +1.96 % and +2.57 % <a href="https://en.wikipedia.org/wiki/F-number">F1</a> respectively in model performance.</abstract>
      <url hash="5ef1cf2b">2021.emnlp-demo.32</url>
      <bibkey>zhang-etal-2021-croano</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.32</doi>
    </paper>
    <paper id="35">
      <title>SeqAttack : On Adversarial Attacks for Named Entity Recognition<fixed-case>S</fixed-case>eq<fixed-case>A</fixed-case>ttack: <fixed-case>O</fixed-case>n Adversarial Attacks for Named Entity Recognition</title>
      <author><first>Walter</first><last>Simoncini</last></author>
      <author><first>Gerasimos</first><last>Spanakis</last></author>
      <pages>308–318</pages>
      <abstract>Named Entity Recognition is a fundamental task in <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a> and is an essential element for various Natural Language Processing pipelines. Adversarial attacks have been shown to greatly affect the performance of text classification systems but knowledge about their effectiveness against named entity recognition models is limited. This paper investigates the effectiveness and portability of adversarial attacks from text classification to <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> and the ability of adversarial training to counteract these attacks. We find that character-level and word-level attacks are the most effective, but adversarial training can grant significant protection at little to no expense of standard performance. Alongside our results, we also release SeqAttack, a framework to conduct adversarial attacks against token classification models (used in this work for named entity recognition) and a companion web application to inspect and cherry pick adversarial examples.</abstract>
      <url hash="874d5009">2021.emnlp-demo.35</url>
      <bibkey>simoncini-spanakis-2021-seqattack</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.35</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="40">
      <title>DRIFT : A Toolkit for Diachronic Analysis of Scientific Literature<fixed-case>DRIFT</fixed-case>: A Toolkit for Diachronic Analysis of Scientific Literature</title>
      <author><first>Abheesht</first><last>Sharma</last></author>
      <author><first>Gunjan</first><last>Chhablani</last></author>
      <author><first>Harshit</first><last>Pandey</last></author>
      <author><first>Rajaswa</first><last>Patil</last></author>
      <pages>361–371</pages>
      <abstract>In this work, we present to the NLP community, and to the wider research community as a whole, an application for the diachronic analysis of research corpora. We open source an easy-to-use tool coined DRIFT, which allows researchers to track research trends and development over the years. The analysis methods are collated from well-cited research works, with a few of our own methods added for good measure. Succinctly put, some of the analysis methods are : keyword extraction, word clouds, predicting declining / stagnant / growing trends using Productivity, tracking bi-grams using Acceleration plots, finding the Semantic Drift of words, tracking trends using similarity, etc. To demonstrate the utility and efficacy of our tool, we perform a case study on the cs. CL corpus of the arXiv repository and draw inferences from the analysis methods. The toolkit and the associated code are available here : https://github.com/rajaswa/DRIFT.</abstract>
      <url hash="46f3ab48">2021.emnlp-demo.40</url>
      <attachment type="Software" hash="1aa6ffe5">2021.emnlp-demo.40.Software.zip</attachment>
      <bibkey>sharma-etal-2021-drift</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.40</doi>
      <pwccode url="https://github.com/rajaswa/DRIFT" additional="false">rajaswa/DRIFT</pwccode>
    </paper>
    </volume>
  <volume id="tutorials" ingest-date="2021-11-04">
    <meta>
      <booktitle>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</booktitle>
      <editor><first>Jing</first><last>Jiang</last></editor>
      <editor><first>Ivan</first><last>Vulić</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic &amp; Online</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="02a2aa34">2021.emnlp-tutorials.0</url>
      <bibkey>emnlp-2021-tutorials</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Financial Opinion Mining</title>
      <author><first>Chung-Chi</first><last>Chen</last></author>
      <author><first>Hen-Hsen</first><last>Huang</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <pages>7–10</pages>
      <abstract>In this tutorial, we will show where we are and where we will be to those researchers interested in this topic. We divide this tutorial into three parts, including coarse-grained financial opinion mining, fine-grained financial opinion mining, and possible research directions. This tutorial starts by introducing the components in a financial opinion proposed in our research agenda and summarizes their related studies. We also highlight the task of mining customers’ opinions toward <a href="https://en.wikipedia.org/wiki/Financial_services">financial services</a> in the <a href="https://en.wikipedia.org/wiki/Financial_technology">FinTech industry</a>, and compare them with usual opinions. Several potential research questions will be addressed. We hope the audiences of this tutorial will gain an overview of financial opinion mining and figure out their research directions.</abstract>
      <url hash="da3e881c">2021.emnlp-tutorials.2</url>
      <bibkey>chen-etal-2021-financial</bibkey>
      <doi>10.18653/v1/2021.emnlp-tutorials.2</doi>
    </paper>
    <paper id="5">
      <title>Robustness and Adversarial Examples in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a></title>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>He</first><last>He</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>22–26</pages>
      <abstract>Recent studies show that many NLP systems are sensitive and vulnerable to a small perturbation of inputs and do not generalize well across different datasets. This lack of <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> derails the use of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP systems</a> in real-world applications. This tutorial aims at bringing awareness of practical concerns about NLP robustness. It targets NLP researchers and practitioners who are interested in building reliable NLP systems. In particular, we will review recent studies on analyzing the weakness of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP systems</a> when facing adversarial inputs and data with a <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">distribution shift</a>. We will provide the audience with a holistic view of 1) how to use adversarial examples to examine the weakness of NLP models and facilitate <a href="https://en.wikipedia.org/wiki/Debugging">debugging</a> ; 2) how to enhance the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of existing NLP models and defense against adversarial inputs ; and 3) how the consideration of <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> affects the real-world NLP applications used in our daily lives. We will conclude the tutorial by outlining future research directions in this area.</abstract>
      <url hash="32de49cb">2021.emnlp-tutorials.5</url>
      <bibkey>chang-etal-2021-robustness</bibkey>
      <doi>10.18653/v1/2021.emnlp-tutorials.5</doi>
    </paper>
    </volume>
</collection>