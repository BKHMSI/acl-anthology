<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.aacl">
  <volume id="main" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</booktitle>
      <editor><first>Kam-Fai</first><last>Wong</last></editor>
      <editor><first>Kevin</first><last>Knight</last></editor>
      <editor><first>Hua</first><last>Wu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="803dfce7">2020.aacl-main.0</url>
      <bibkey>aacl-2020-asia</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Touch Editing : A Flexible One-Time Interaction Approach for <a href="https://en.wikipedia.org/wiki/Translation">Translation</a></title>
      <author><first>Qian</first><last>Wang</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Guoping</first><last>Huang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>1–11</pages>
      <abstract>We propose a touch-based editing method for <a href="https://en.wikipedia.org/wiki/Translation">translation</a>, which is more flexible than traditional keyboard-mouse-based translation postediting. This approach relies on <a href="https://en.wikipedia.org/wiki/Somatosensory_system">touch actions</a> that users perform to indicate translation errors. We present a dual-encoder model to handle the actions and generate refined translations. To mimic the user feedback, we adopt the TER algorithm comparing between draft translations and references to automatically extract the simulated actions for training data construction. Experiments on translation datasets with simulated editing actions show that our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> significantly improves original translation of Transformer (up to 25.31 BLEU) and outperforms existing interactive translation methods (up to 16.64 BLEU). We also conduct experiments on post-editing dataset to further prove the robustness and effectiveness of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a>.</abstract>
      <url hash="69f9ea60">2020.aacl-main.1</url>
      <bibkey>wang-etal-2020-touch</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    <title_ar>التحرير باللمس: نهج تفاعل لمرة واحدة مرن للترجمة</title_ar>
      <title_pt>Edição de toque: uma abordagem flexível de interação única para tradução</title_pt>
      <title_es>Edición táctil: un enfoque flexible de interacción única para la traducción</title_es>
      <title_ja>タッチ編集：翻訳のための柔軟なワンタイムインタラクションアプローチ</title_ja>
      <title_zh>摩挲编辑:一次性译交法</title_zh>
      <title_hi>संपादन स्पर्श करें: अनुवाद के लिए एक लचीला एक-बार इंटरैक्शन दृष्टिकोण</title_hi>
      <title_ga>Eagarthóireacht Tadhaill: Cur Chuige Solúbtha Aonuaire Idirghníomhaíochta don Aistriúchán</title_ga>
      <title_ka>Name</title_ka>
      <title_el>Επεξεργασία αφής: Μια ευέλικτη προσέγγιση εφάπαξ αλληλεπίδρασης για τη μετάφραση</title_el>
      <title_hu>Érintőszerkesztés: Rugalmas egyszeri interakciós megközelítés a fordításhoz</title_hu>
      <title_it>Touch Editing: un approccio flessibile di interazione una tantum per la traduzione</title_it>
      <title_kk>Өңдеу: Аудару үшін бір- бірінші бір- бірінші интерактивтік қатынасы</title_kk>
      <title_lt>Touch Editing: A flexible one-time interaction approach for translation</title_lt>
      <title_mk>Уредување на допир: Флексибилен пристап на едновремена интеракција за превод</title_mk>
      <title_ms>Penyunting Sentuhan: Pendekatan Interaksi Satu-Kali Fleksibel untuk Terjemahan</title_ms>
      <title_ml>തൊടുക്കുന്ന ചിട്ടപ്പെടുത്തുന്നത്: ഒരു സമയത്തിനുള്ള വിവരങ്ങള്‍</title_ml>
      <title_mn>Touch Editing: A Flexible One-Time Interaction Approach for Translation</title_mn>
      <title_no>Trykk- redigering: Ein fleksibel enkelt interaksjon- tilgang for omsetjing</title_no>
      <title_pl>Edycja dotykowa: elastyczne jednorazowe podejście do interakcji w tłumaczeniu</title_pl>
      <title_ro>Editare tactilă: o abordare flexibilă de interacțiune unică pentru traducere</title_ro>
      <title_sr>Pristup za prevod</title_sr>
      <title_si>Name</title_si>
      <title_so>Touch Editing: A Flexible One-time Interaction Approach for Translation</title_so>
      <title_sv>Pekredigering: En flexibel engångsstrategi för översättning</title_sv>
      <title_ta>தொடுக்கும் தொகுப்பு: ஒரு நேரத்தில் வெளியேறும் ஒரு இடைவெளி செயல் மொழிபெயர்ப்புக்கு அருகில்</title_ta>
      <title_ur>ٹوچ ایڈیٹینگ: ترجمہ کے لئے ایک دفعہ اضافہ کی تقرب</title_ur>
      <title_mt>Editar tal-Touch: Approċċ flessibbli ta’ Interazzjoni ta’ darba għat-Traduzzjoni</title_mt>
      <title_uz>Comment</title_uz>
      <title_vi>Chạm Sửa: Một phương pháp tương tác thời gian ngắn cho dịch</title_vi>
      <title_da>Touch redigering: En fleksibel engangsinteraktionstilgang til oversættelse</title_da>
      <title_nl>Touch Editing: een flexibele eenmalige interactie aanpak voor vertaling</title_nl>
      <title_bg>Редактиране на докосване: гъвкав подход за еднократно взаимодействие за превод</title_bg>
      <title_hr>Pristup za prevod</title_hr>
      <title_de>Touch Editing: Ein flexibler, einmaliger Interaktionsansatz für Übersetzungen</title_de>
      <title_id>Penyunting Sentuhan: Pendekatan Interaksi Sekali Fleksibel untuk Terjemahan</title_id>
      <title_fa>تغییر تغییر: یک نزدیک تغییر فعالیت یک زمانی برای ترجمه</title_fa>
      <title_sw>Matukio ya Mhariri: Mtandao wa Mitandao ya Mara moja kwa mara kwa ajili ya Tafsiri</title_sw>
      <title_sq>Modifikimi i prekjes: Një metodë fleksible ndërveprimi një-herë për përkthimin</title_sq>
      <title_ko>터치 편집: 유연한 일회용 상호작용 번역 방법</title_ko>
      <title_hy>Խոսքի խմբագրում. մեկ անգամ բարդ ինտերակցիոն մոտեցում թարգմանելու համար</title_hy>
      <title_af>Touk Redigering: ' n Fleksibel Een- Tyd Interaksie toegang vir Vertaling</title_af>
      <title_az>T톛rc칲m톛 칲칞칲n Flexible One-Time Interaction Approach</title_az>
      <title_tr>Touch Editing: A Flexible One-Time Interaction Approach for Translation</title_tr>
      <title_bn>স্পর্শ সম্পাদনা: অনুবাদের জন্য একটি ফ্ল্যামেবল এক-সময় ইন্টারনেশন</title_bn>
      <title_am>ክሊፕቦርዱን ለጥፍ</title_am>
      <title_ca>Editar contactes: Un enfocament flexible d'interacció d'una vegada per a la traducció</title_ca>
      <title_bs>Pristup interakcije za prevod</title_bs>
      <title_cs>Dotykové úpravy: Flexibilní jednorázová interakce při překladu</title_cs>
      <title_fi>Kosketusmuokkaus: Joustava kertaluonteinen lähestymistapa kääntämiseen</title_fi>
      <title_et>Puudutusega redigeerimine: paindlik ühekordne suhtlemisviis tõlkimiseks</title_et>
      <title_jv>New</title_jv>
      <title_sk>Urejanje na dotik: prilagodljiv pristop za enkratno interakcijo za prevajanje</title_sk>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_he>עורך נגיעה: גישה חד פעולה נוקשה לתרגום</title_he>
      <title_bo>Touch Editing: A Flexible One-Time Interaction Approach for Translation</title_bo>
      <abstract_ar>نقترح طريقة تحرير للترجمة تعتمد على اللمس ، وهي أكثر مرونة من الترجمة التقليدية التي تعتمد على الماوس. يعتمد هذا الأسلوب على إجراءات اللمس التي يقوم بها المستخدمون للإشارة إلى أخطاء الترجمة. نقدم نموذجًا مزدوج التشفير للتعامل مع الإجراءات وإنشاء ترجمات محسّنة. لتقليد ملاحظات المستخدم ، نعتمد خوارزمية TER للمقارنة بين ترجمات المسودة والمراجع لاستخراج الإجراءات المحاكية تلقائيًا لبناء بيانات التدريب. تُظهر التجارب على مجموعات بيانات الترجمة مع إجراءات التحرير المحاكاة أن طريقتنا تحسن بشكل كبير الترجمة الأصلية لـ Transformer (حتى 25.31 BLEU) وتتفوق على طرق الترجمة التفاعلية الحالية (حتى 16.64 BLEU). نجري أيضًا تجارب على مجموعة بيانات ما بعد التحرير لإثبات متانة طريقتنا وفعاليتها بشكل أكبر.</abstract_ar>
      <abstract_pt>Propomos um método de edição baseado em toque para tradução, que é mais flexível do que a publicação de tradução tradicional baseada em mouse e teclado. Essa abordagem se baseia em ações de toque que os usuários realizam para indicar erros de tradução. Apresentamos um modelo de codificador duplo para lidar com as ações e gerar traduções refinadas. Para imitar o feedback do usuário, adotamos o algoritmo TER comparando entre rascunhos de traduções e referências para extrair automaticamente as ações simuladas para construção de dados de treinamento. Experimentos em conjuntos de dados de tradução com ações de edição simuladas mostram que nosso método melhora significativamente a tradução original do Transformer (até 25,31 BLEU) e supera os métodos de tradução interativa existentes (até 16,64 BLEU). Também realizamos experimentos no conjunto de dados pós-edição para provar ainda mais a robustez e a eficácia do nosso método.</abstract_pt>
      <abstract_es>Proponemos un método de edición táctil para la traducción, que es más flexible que la posedición tradicional basada en teclado-ratón. Este enfoque se basa en las acciones táctiles que realizan los usuarios para indicar errores de traducción. Presentamos un modelo de doble codificador para gestionar las acciones y generar traducciones refinadas. Para imitar los comentarios de los usuarios, adoptamos el algoritmo TER que compara entre borradores de traducciones y referencias para extraer automáticamente las acciones simuladas para la construcción de datos de entrenamiento. Los experimentos en conjuntos de datos de traducción con acciones de edición simuladas muestran que nuestro método mejora significativamente la traducción original de Transformer (hasta 25,31 BLEU) y supera a los métodos de traducción interactivos existentes (hasta 16,64 BLEU). También realizamos experimentos sobre la postedición de datos para demostrar aún más la solidez y eficacia de nuestro método.</abstract_es>
      <abstract_ja>従来のキーボード・マウスによる翻訳後編集よりも柔軟な、タッチベースの翻訳編集方法を提案します。このアプローチは、ユーザーが翻訳エラーを示すために実行するタッチ操作に依存します。私たちは、アクションを処理し、洗練された翻訳を生成するためのデュアルエンコーダモデルを提示します。ユーザーのフィードバックを模倣するために、トレーニングデータ構築のためのシミュレートされたアクションを自動的に抽出するために、草稿翻訳と参照の間で比較するTERアルゴリズムを採用します。模擬編集アクションを使用した翻訳データセットの実験は、当社の方法がTransformerのオリジナル翻訳を大幅に改善し（最大25.31 BLEU ）、既存のインタラクティブ翻訳方法（最大16.64 BLEU ）を上回ることを示しています。また、ポストエディットデータセットの実験を行い、メソッドの堅牢性と有効性をさらに証明しています。</abstract_ja>
      <abstract_zh>建一本于触摩之法,其变于键盘鼠标者也。 其法赖于用户行者,指授译误。 设一双编码器模样,操作精微译。 拟用户反馈, TER 算校稿译引用,自取模拟操作以练数事。 模拟编辑操实验,吾法著入 Transformer 始(高达 253.1 BLEU),优于见交互式(达 16.64 BLEU)。 又于译后编辑数据集行实验,以证吾法之鲁棒性有效性。</abstract_zh>
      <abstract_hi>हम अनुवाद के लिए एक स्पर्श-आधारित संपादन विधि का प्रस्ताव करते हैं, जो पारंपरिक कीबोर्ड-माउस-आधारित अनुवाद पोस्टिटिंग की तुलना में अधिक लचीला है। यह दृष्टिकोण उन स्पर्श क्रियाओं पर निर्भर करता है जो उपयोगकर्ता अनुवाद त्रुटियों को इंगित करने के लिए करते हैं. हम क्रियाओं को संभालने और परिष्कृत अनुवाद उत्पन्न करने के लिए एक दोहरी-एन्कोडर मॉडल प्रस्तुत करते हैं। उपयोगकर्ता प्रतिक्रिया की नकल करने के लिए, हम टीईआर एल्गोरिथ्म को प्रारूप अनुवाद और संदर्भों के बीच तुलना करते हुए स्वचालित रूप से प्रशिक्षण डेटा निर्माण के लिए नकली कार्यों को निकालने के लिए अपनाते हैं। सिम्युलेटेड संपादन कार्यों के साथ अनुवाद डेटासेट पर प्रयोगों से पता चलता है कि हमारी विधि ट्रांसफॉर्मर के मूल अनुवाद (25.31 BLEU तक) में काफी सुधार करती है और मौजूदा इंटरैक्टिव अनुवाद विधियों (16.64 BLEU तक) को मात देती है। हम अपनी विधि की मजबूती और प्रभावशीलता को और साबित करने के लिए पोस्ट-एडिटिंग डेटासेट पर प्रयोग भी करते हैं।</abstract_hi>
      <abstract_ga>Molaimid modh eagarthóireachta atá bunaithe ar theagmháil don aistriúchán, atá níos solúbtha ná posteagarthóireacht aistriúcháin thraidisiúnta méarchlár-luch-bhunaithe. Braitheann an cur chuige seo ar ghníomhartha tadhaill a dhéanann úsáideoirí chun earráidí aistriúcháin a léiriú. Cuirimid múnla dé-ionchódóra i láthair chun na gníomhartha a láimhseáil agus chun aistriúcháin scagtha a ghiniúint. Chun aithris a dhéanamh ar aiseolas na n-úsáideoirí, glacaimid leis an algartam TER i gcomparáid idir dréacht-aistriúcháin agus tagairtí chun na gníomhartha ionsamhlaithe chun sonraí oiliúna a thógáil a bhaint go huathoibríoch. Léiríonn turgnaimh ar thacair sonraí aistriúcháin le gníomhartha eagarthóireachta ionsamhailte go gcuireann ár modh feabhas suntasach ar bhunaistriúchán Transformer (suas go dtí 25.31 BLEU) agus go sáraíonn sé na modhanna aistriúcháin idirghníomhacha atá ann cheana (suas go dtí 16.64 BLEU). Déanaimid turgnaimh freisin ar thacar sonraí iar-eagarthóireachta chun láidreacht agus éifeachtacht ár modh a chruthú.</abstract_ga>
      <abstract_hu>A fordításhoz egy érintő alapú szerkesztési módszert javasolunk, amely rugalmasabb, mint a hagyományos billentyűzetes egér alapú fordítás utószerkesztés. Ez a megközelítés a felhasználók által a fordítási hibák jelzésére végrehajtott érintési műveletekre támaszkodik. Kettős kódoló modellt mutatunk be a műveletek kezelésére és finomított fordítások létrehozására. A felhasználói visszajelzések utánzása érdekében a TER algoritmust alkalmazzuk, amely összehasonlítja a vázlatok fordításait és a hivatkozásokat, hogy automatikusan kivonja a szimulált műveleteket az edzési adatok építéséhez. A szimulált szerkesztési műveletekkel végzett fordítási adatkészletekkel végzett kísérletek azt mutatják, hogy módszerünk jelentősen javítja a Transformer eredeti fordítását (legfeljebb 25,31 BLEU) és felülmúlja a meglévő interaktív fordítási módszereket (legfeljebb 16,64 BLEU). Kísérleteket végzünk utólagos adatkészletekkel is, hogy tovább bizonyítsuk módszerünk robusztusságát és hatékonyságát.</abstract_hu>
      <abstract_el>Προτείνουμε μια μέθοδο επεξεργασίας που βασίζεται στην αφή για τη μετάφραση, η οποία είναι πιο ευέλικτη από την παραδοσιακή μεταγραφή με πληκτρολόγιο-ποντίκι. Αυτή η προσέγγιση βασίζεται σε ενέργειες αφής που εκτελούν οι χρήστες για να υποδείξουν λάθη μετάφρασης. Παρουσιάζουμε ένα μοντέλο διπλού κωδικοποιητή για να χειριστεί τις ενέργειες και να δημιουργήσει εκλεπτυσμένες μεταφράσεις. Για να μιμηθούμε την ανατροφοδότηση των χρηστών, υιοθετούμε τον αλγόριθμο που συγκρίνει μεταξύ σχεδίων μεταφράσεων και αναφορών για να εξαγάγουμε αυτόματα τις προσομοιωμένες ενέργειες για την κατασκευή δεδομένων κατάρτισης. Τα πειράματα σε σύνολα δεδομένων μετάφρασης με προσομοιωμένες ενέργειες επεξεργασίας δείχνουν ότι η μέθοδος μας βελτιώνει σημαντικά την αρχική μετάφραση του μετασχηματιστή (έως 25.31 BLEU) και ξεπερνά τις υπάρχουσες διαδραστικές μεθόδους μετάφρασης (έως 16.64 BLEU). Πραγματοποιούμε επίσης πειράματα σε σύνολα δεδομένων μετά την επεξεργασία για να αποδείξουμε περαιτέρω την ανθεκτικότητα και την αποτελεσματικότητα της μεθόδου μας.</abstract_el>
      <abstract_ka>ჩვენ შეგიძლიათ შეცვლის რედაქტირების მეთოდი, რომელიც უფრო გრძნობელია, ვიდრე რედაქტირებული კლავიატურატურატურატურატურატურატურატურატურ ეს პროგრამა დახმარებულია, რომ მომხმარებელი შეცდომის შეცდომის შეცდომის გამოყენება. ჩვენ მხოლოდ ეუ-კოდერის მოდელის გამოყენება, რომელიც მოქმედება და შექმნა განსხვავებული თავსუფლებების შექმნა. მომხმარებელი გახსნა მიმიზემისთვის, ჩვენ TER ალგორიტიმას გავაკეთებთ, რომელიც პროგრამეტს გადაწყვეტილების და რეფენციების შორის შემდეგ ავტომატურად გამოვაკეთება სიმულაციული მო ჩვენი რედაქტირებული მონაცემების გამოცდილება გამოცდილობული მონაცემების გამოცდილობაში, რომელიც ჩვენი მეტი ძალიან უფრო უფრო მეტრინსტრუქტირების ორიგინალური გადაწყვეტილება (25,31 BLEU) და გადა ჩვენ ასევე ექსპერიმენტები გავაკეთებთ მონაცემების მონაცემების შესახებ, რომ ჩვენი მეთოდის ძალიან ძალიან და ეფექტიურობის შესახებ.</abstract_ka>
      <abstract_it>Proponiamo un metodo di editing touch-based per la traduzione, che è più flessibile rispetto al tradizionale post-editing basato su tastiera. Questo approccio si basa su azioni touch eseguite dagli utenti per indicare errori di traduzione. Presentiamo un modello dual-encoder per gestire le azioni e generare traduzioni raffinate. Per imitare il feedback degli utenti, adottiamo l'algoritmo TER di confronto tra traduzioni di bozze e riferimenti per estrarre automaticamente le azioni simulate per la costruzione dei dati di allenamento. Esperimenti su set di dati di traduzione con azioni di editing simulate dimostrano che il nostro metodo migliora significativamente la traduzione originale di Transformer (fino a 25,31 BLEU) e supera i metodi di traduzione interattiva esistenti (fino a 16,64 BLEU). Conduciamo anche esperimenti su set di dati post-editing per dimostrare ulteriormente la robustezza e l'efficacia del nostro metodo.</abstract_it>
      <abstract_kk>Біз аудармалардың touch- негіздеген өзгерту әдісін ұсынамыз. Бұл әдімгі пернетақта- тышқанның негіздеген аудармалардың орналасуынан артық. Бұл жағдай пайдаланушылардың аудармалардың қатесін көрсету үшін әрекеттеріне тәуелді. Біз әрекеттерді басқару және түзетілген аудармаларды құру үшін екі кодер үлгісін келтіреміз. Пайдаланушының қайталануын түсіндіру үшін, мәліметтерді құру үшін өзгертілген әрекеттерді автоматты түрде тарқату үшін TER алгоритмін қолданамыз. Орындалған өңдеу әрекеттері туралы аудару деректер жиындарының тәжірибесі Трансформацияның бастапқы аударымызды (25, 31 BLEU дегенге дейін) және интерактивті аудару әдістерінің (16, 64 BLEU дегенге дейін) жо Сонымен қатар, өзгерту деректер жиынынан кейін өзгерту тәжірибелерін өзгертіп тәжірибелеріміздің әдісіміздің құндылығын және ең әсер етіктер</abstract_kk>
      <abstract_ml>We propose a touch-based editing method for translation, which is more flexible than traditional keyboard-mouse-based translation postediting.  @ info: whatsthis പ്രവര്‍ത്തനങ്ങളെ കൈകാര്യം ചെയ്യാനും വിശദീകരിച്ച വിവരങ്ങള്‍ ഉണ്ടാക്കാനും നമ്മള്‍ രണ്ട് കോഡെര്‍ മോഡല്‍ ക ഉപയോക്താവിന്റെ ഫിഡിബ്ബാക്കിനെ മൈക്ക് ചെയ്യാന്‍ വേണ്ടി ടെആര്‍ ആല്‍ഗോരിത്മ് ഉപയോഗിക്കുന്നു. പരിശീലനത്തിനുള്ള പരിശീലനത്തിനുള് ട്രാന്‍സ്ഫോര്‍ഫിന്‍റെ അടിസ്ഥാനത്തില്‍ നിന്നും നിലവിലുള്ള interactive translation methods (16. 64 BLEU വരെ ഉള്ള) പരിശോധന നടത്തുന്നതില്‍ പരീക്ഷണങ്ങള്‍ കാണിക്കുന്നു. പിന്നീട് ചിട്ടപ്പെടുത്തുന്ന ഡാറ്റാസെറ്റില്‍ ഞങ്ങള്‍ പരീക്ഷണങ്ങള്‍ പ്രവര്‍ത്തിക്കുന്നു. നമ്മുടെ രീതിയി</abstract_ml>
      <abstract_lt>Siūlome, kad vertimo raštu redakcijos metodas būtų grindžiamas kontaktiniu būdu, kuris būtų lankstesnis nei tradicinis vertimo raštu, grindžiamu klaviatūra pele, redakcija. Šis metodas grindžiamas kontaktiniais veiksmais, kuriuos naudotojai atlieka rodydami vertimo klaidas. We present a dual-encoder model to handle the actions and generate refined translations.  Siekiant imituoti naudotojo grįžtamąją informaciją, mes priimame TER algoritmą, palyginantį vertimų projektus ir nuorodas, kad automatiškai ištrauktų modeliuojamus veiksmus rengiant duomenis. Eksperimentai dėl vertimo duomenų rinkinių su modeliuojamais redagavimo veiksmais rodo, kad mūsų metodas gerokai pagerina pradinį Transformer vertimą (iki 25,31 BLEU) ir atitinka esamus interaktyvius vertimo metodus (iki 16,64 BLEU). Taip pat atliekame eksperimentus dėl duomenų rinkinio po redakcijos, kad toliau įrodytume mūsų metodo patikimumą ir veiksmingumą.</abstract_lt>
      <abstract_ms>Kami cadangkan kaedah penyuntingan berdasarkan sentuhan untuk terjemahan, yang lebih fleksibel daripada terjemahan tradisional berdasarkan papan kekunci tetikus. pendekatan ini bergantung pada tindakan sentuhan yang pengguna lakukan untuk menunjukkan ralat terjemahan. Kami perkenalkan model pengekod-dua untuk mengendalikan tindakan dan menghasilkan terjemahan terjemahan. Untuk meniru balas balik pengguna, kami menerima algoritma TER yang membandingkan antara terjemahan drpd dan rujukan untuk secara automatik mengekstrak tindakan simulasi untuk pembangunan data latihan. Experiments on translation datasets with simulated editing actions show that our method significantly improves original translation of Transformer (up to 25.31 BLEU) and outperforms existing interactive translation methods (up to 16.64 BLEU).  Kami juga melakukan eksperimen pada set data pos-edisi untuk membuktikan lebih lanjut kepekatan dan kegunaan kaedah kami.</abstract_ms>
      <abstract_mk>We propose a touch-based editing method for translation, which is more flexible than traditional keyboard-mouse-based translation postediting.  Овој пристап се потпира на активностите кои корисниците ги извршуваат за да индицираат грешки во преводот. Презентираме модел со двојен кодер за да се справи со акциите и да се генерира рафиниран превод. За да ја имитираме реакцијата на корисникот, го усвоивме тер алгоритмот во споредба помеѓу нацрт-преводи и референции за автоматски да ги извадиме симулираните акции за обука на изградба на податоци. Експериментите на датотеките за превод со симулирани акции за уредување покажуваат дека нашиот метод значително го подобрува оригиналниот превод на Трансформерот (до 25,31 БЛЕ) и ги надминува постојните интерактивни методи за превод (до 16,64 БЛЕ). Ние, исто така, спроведуваме експерименти на постуредувачкиот податок за понатамошно да ја докажеме силноста и ефикасноста на нашиот метод.</abstract_mk>
      <abstract_no>Vi foreslår ein tekstmetode for omsetjing som er fleksibel enn tradisjonell posting av omsetjing med tastaturbasert muse. Denne tilnærminga er avhengig av kontakthandlingar som brukarar utfører for å visa omsetjingsfeilar. Vi presenterer ein dobbelkodingsmodell for å handtera handlingane og laga refinerte omsetjingar. For å mimisera brukaren tilbakemeldinga, kan vi bruka TER-algoritmen som sammenlignar mellom prosjektet av omsetjingar og referanser til å automatisk pakka ut simulerte handlingane for opplæring av datakonstruksjon. Eksperimentar på omsetjingsdata med simulerte redigeringshandlingar viser at metoden vårt betydelig forbedrar originalt omsetjing av Transformer (opp til 25,31 BLEU) og utfører eksisterande interaktive omsetjingsmetoder (opp til 16,64 BLEU). Vi gjer også eksperimenter på datasettet etter redigering for å bevise støtte og effektiviteten av metoden vårt.</abstract_no>
      <abstract_pl>Proponujemy metodę edycji dotykowej do tłumaczeń, która jest bardziej elastyczna niż tradycyjna postedycja tłumaczeń oparta na klawiaturze myszy. To podejście opiera się na działaniach dotykowych wykonywanych przez użytkowników w celu wskazania błędów tłumaczenia. Prezentujemy model dwukodera do obsługi działań i generowania dopracowanych tłumaczeń. Aby naśladować opinie użytkowników, przyjmujemy algorytm TER porównujący projekty tłumaczeń i odniesień, aby automatycznie wyodrębnić symulowane działania do budowy danych treningowych. Eksperymenty na zbiorach danych tłumaczeniowych z symulowanymi działaniami edycyjnymi pokazują, że nasza metoda znacznie poprawia oryginalne tłumaczenie Transformera (do 25.31 BLEU) i przewyższa istniejące interaktywne metody tłumaczenia (do 16.64 BLEU). Przeprowadzamy również eksperymenty na postedycji zbioru danych, aby jeszcze bardziej udowodnić solidność i skuteczność naszej metody.</abstract_pl>
      <abstract_mn>Бид өөрсдийгөө хөрөнгө оруулахын тулд суурилсан өөрчлөлтийн аргыг санал дэвшүүлнэ. Энэ нь уламжлалтын клавиатур-хулганаас суурилсан хөрөнгө оруулах арга юм. Энэ арга баримт хэрэглэгчид орчуулах алдаа гаргахад хийдэг хүчний үйл ажиллагаанд хамаарна. Бид эдгээр үйл ажиллагааг удирдаж, шинэчлэгдсэн орнуудыг бий болгох хоёр кодлогч загварыг тайлбарлаж байна. Хэрэглэгчдийн хариултыг дүрслэхийн тулд бид TER алгоритмыг өгөгдлийн барилгын сургалтын зохион байгуулалтыг автоматжуулахын тулд харьцуулахын тулд автоматжуулсан үйл ажиллагааг ашигладаг. Төгсгөл өгөгдлийн сангийн шинжлэх ухааны туршилтууд нь бидний арга нь Төгсгөлдөгч (25.31 БЛЕУ хүртэл) эхний орнуудын орнуудын орнуудын орнуудын орнуудын орнуудын хөгжүүлэлтийг илүү сайн сайжруулж, интерактив орну Мөн бид өөрсдийн арга замын хүчтэй, үр дүнтэй байдлыг баталахын тулд дараагийн өгөгдлийн сангийн дараах туршилтыг хийдэг.</abstract_mn>
      <abstract_ro>Propunem o metodă de editare tactilă pentru traducere, care este mai flexibilă decât posteditarea tradițională a traducerii bazată pe tastatură-mouse. Această abordare se bazează pe acțiuni tactile efectuate de utilizatori pentru a indica erorile de traducere. Vă prezentăm un model dual-encoder pentru a gestiona acțiunile și a genera traduceri rafinate. Pentru a imita feedback-ul utilizatorului, adoptăm algoritmul TER care compară între traducerile proiectelor și referințele pentru a extrage automat acțiunile simulate pentru construirea datelor de formare. Experimentele pe seturi de date de traducere cu acțiuni de editare simulate arată că metoda noastră îmbunătățește semnificativ traducerea originală a Transformer (până la 25,31 BLEU) și depășește metodele de traducere interactivă existente (până la 16,64 BLEU). De asemenea, efectuăm experimente pe seturi de date post-editare pentru a dovedi în continuare robustețea și eficiența metodei noastre.</abstract_ro>
      <abstract_sr>Predlažemo metodu editiranja na dodiru za prevod, koja je fleksibilnija od tradicionalnog postizanja prevoda na tastaturi na mišu. Ovaj pristup se oslanja na dodirne akcije koje korisnici izvode kako bi pokazali greške prevođenja. Predstavljamo dvokoderski model kako bi se bavili akcijama i stvorili refinansirane prevode. Da bi imitirali povratku korisnika, usvojili smo algoritam TER u usporedbi između projekta prevoda i referencija da bi automatski izvukli simulirane akcije za izgradnju podataka. Eksperimenti o setima prevođenja podataka sa simulisanim redakcijama pokazuju da naša metoda značajno poboljšava originalni prevoz transformera (do 25,31 BLEU) i iznosi postojeće interaktivne metode prevođenja (do 16,64 BLEU). Takoðe provodimo eksperimente o setu podataka za posteditanje kako bi dalje dokazali robustnost i efikasnost našeg metoda.</abstract_sr>
      <abstract_so>Waxaan soo jeedaynaa qoraal tahriri oo ku saleysan taabasho, taasoo ka sii flexin badan turjumaadda caadiga ah oo ku qoran qoraalka qoraalka muse-based. Markaas qaababkan waxay ku xiran tahay falimaha xiriirka, kuwaas oo isticmaalayaasha sameeya si ay u muujiyaan qalbiyada turjumidda. We present a dual-encoder model to handle the actions and generate refined translations.  Si aan u gaarno feedbacyada isticmaalaha, waxaynu u qaadannaa Algorithm TER oo u barbareysanaya qoraalka turjumaadda iyo soo jeedidda, si aan automatic uga soo bixino falimaha la similanayo oo loo sameynayo dhismaha danbiyada. Imtixaan ku saabsan macluumaadka turjumaadda oo simulated editing actions shows that methodeennu significantly improves the original translation of Transformer (up to 25.31 BLEU) and outperforms existing interactive translation methods (up to 16.64 BLEU). Sidoo kale waxaynu sameynaa imtixaamo ku saabsan sawirida macluumaadka ka dib si aan ugu caddeyno waxyaabaha dhaqdhaqaalaha iyo waxqabashada qaababkayaga.</abstract_so>
      <abstract_si>අපි පරිවර්තනය වෙනුවෙන් අතුරුද්ධ සංවේදනය විධානයක් ප්‍රයෝජනය කරන්න ප්‍රයෝජනය කරනවා, ඒක පාරමාන්‍ය කීබ Name අපි ක්‍රියාව පාලනය කරන්න සහ ප්‍රතික්‍රියාත්මක කරන්න දුවල් කෝඩාර් මොඩේල් එකක් පෙන්වන්න. භාවිතා ප්‍රතිචාරකය ප්‍රතිචාරයක් මයිමික් කරන්න, අපි TER ඇල්ගෝරිතම් එක ප්‍රතිචාර කරනවා ක්‍රියාක්‍ෂණය සහ ප්‍රතිචාරකය පරිවර්තන දත්ත සෙට්ටුවේ පරීක්ෂණය පෙන්වන්න පුළුවන් අපේ විධානය විශේෂයෙන් පරිවර්තන කරන්න පුළුවන් පරිවර්තන කරන්න (25.31 BLUE වලට) ස අපි පස්සේ සංපාදනය කරන්න දත්ත සූදානයට පරීක්ෂණය කරන්න ප්‍රයෝජනය කරනවා අපේ විධානයේ ශක්තිමතාවය සහ හැකිම</abstract_si>
      <abstract_sv>Vi föreslår en touch-baserad redigeringsmetod för översättning, som är mer flexibel än traditionell keyboard-musbaserad översättning postredigering. Detta tillvägagångssätt bygger på pekåtgärder som användare utför för att indikera översättningsfel. Vi presenterar en dual-encoder modell för att hantera åtgärderna och generera förfinade översättningar. För att efterlikna användarfeedback använder vi TER-algoritmen som jämför mellan utkastöversättningar och referenser för att automatiskt extrahera simulerade åtgärder för konstruktion av träningsdata. Experiment på översättningsdata med simulerade redigeringsåtgärder visar att vår metod avsevärt förbättrar originalöversättning av Transformer (upp till 25,31 BLEU) och överträffar befintliga interaktiva översättningsmetoder (upp till 16,64 BLEU). Vi genomför även experiment på efterredigeringsdata för att ytterligare bevisa robustheten och effektiviteten i vår metod.</abstract_sv>
      <abstract_ur>ہم ترجمہ کے لئے ایک ٹونچ بنیادی ایڈینگ طریقہ پیشنهاد کرتے ہیں، جو سنتی کیبوریڈ-ماوس بنیادی ترجمہ پوسٹینگ سے زیادہ مہربانی ہے. This approach relies on touch actions that users perform to indicate translation errors. ہم ایک دوئل کوڈر موڈل کو پیش کریں گے کہ عمل کو ادارہ کریں اور پاکیزہ ترجمہ پیدا کریں۔ استعمال فیڈبک کے مطابق، ہم نے ڈرافوٹ ترجمن اور ارتباطات کے درمیان تفریق کی TER الگوریتم کو استعمال کیا ہے کہ ڈیٹا بنانے کے لئے سیمولیٹ کیے ہوئے عمل کو اٹھا سکیں۔ ترجمہ ڈیٹ سٹ کی تجربیات سیمالیٹ ایڈیٹ کیٹ کیٹ کے ساتھ دکھائی جاتی ہے کہ ہمارا طریقہ معمولی طور پر ترجمہ ترجمہ کرتا ہے (25.31 BLEU تک) اور موجود مختلف ترجمہ طریقے (16.64 BLEU تک) سے زیادہ اضافہ کرتا ہے۔ ہم اس طرح اپنے طریقے کی مضبوطی اور فعالیت کی ثابت کرنے کے لئے پوسٹ ویڈیٹ ڈیٹ سٹ کے بارے میں بھی آزمائش کرتے ہیں.</abstract_ur>
      <abstract_ta>மொழிபெயர்ப்பிற்கான தொடர்பு அடிப்படையிலான தொகுப்பு முறைமையை நாம் பரிந்துரைக்கிறோம், அது மரபார்ந்த விசைப்பல @ info நாம் செயல்களை கையாளும் மற்றும் பிரதிபலிக்கப்பட்ட மொழிபெயர்ப்புகளை உருவாக்க ஒரு இரு குறியீட்டு மாத பயன்படுத்துபவர் செலுத்துவதற்கு, வரைவு மொழிபெயர்ப்புகள் மற்றும் குறிப்புகளுக்கு இடையே ஒப்பிடும் TER முறைமையை நாம் எடுத்து தானாகவே பாவன Name பின்தொகுப்பு தகவல் அமைப்பில் நாம் சோதனைகளை செய்கிறோம் மேலும் எங்கள் முறைமையின் தொகுப்பு மற்றும் விள</abstract_ta>
      <abstract_mt>Aħna nipproponu metodu ta’ editjar ibbażat fuq il-kuntatt għat-traduzzjoni, li huwa aktar flessibbli mit-traduzzjoni tradizzjonali bbażata fuq il-keyboard-mouse postediting. Dan l-approċċ jiddependi fuq azzjonijiet ta’ kuntatt li l-utenti jwettqu biex jindikaw żbalji fit-traduzzjoni. We present a dual-encoder model to handle the actions and generate refined translations.  To mimic the user feedback, we adopt the TER algorithm comparing between draft translations and references to automatically extract the simulated actions for training data construction.  Experiments on translation datasets with simulated editing actions show that our method significantly improves original translation of Transformer (up to 25.31 BLEU) and outperforms existing interactive translation methods (up to 16.64 BLEU).  Inwettqu wkoll esperimenti dwar sett ta’ dejta ta’ wara l-edizzjoni biex nipprovaw aktar ir-robustezza u l-effettività tal-metodu tagħna.</abstract_mt>
      <abstract_uz>We propose a touch-based editing method for translation, which is more flexible than traditional keyboard-mouse-based translation postediting.  @ info: whatsthis @ info: whatsthis Name @ info Biz o'zgarishdan keyingi maʼlumotlar tarkibini bajaramiz va usulidagi o'zgarishni va ishlatishni davom etish uchun.</abstract_uz>
      <abstract_vi>Chúng tôi đề xuất một phương pháp sửa chữa gấp đôi để dịch chuyển, nó còn linh hoạt hơn cả cách dịch chuyển con chuột bàn phím truyền thống. Phương pháp này dựa trên những hành động liên quan mà người dùng thực hiện để chỉ ra lỗi dịch. Chúng tôi có một mô hình mã hóa kép để xử lý các hành động và tạo ra dịch thuật phức tạp. Để bắt chước phản hồi của người dùng, chúng tôi sử dụng thuật to án TER so sánh giữa bản dịch nháp và các chỉ dẫn để tự động trích các hành động mô phỏng để đào tạo dữ liệu. Thí nghiệm trên các tập tin dịch với các thao tác soạn thảo mô phỏng cho thấy rằng phương pháp của chúng ta cải thiện bản dịch phiên bản gốc của Transformer (tới 25.31 bleU) và hoàn thành các phương pháp dịch chuyển tương tác tồn tại (tới 16.64 bleU). Chúng tôi cũng tiến hành thí nghiệm trên tập tin sau khi sửa xong để chứng minh tính bền vững và hiệu quả của phương pháp.</abstract_vi>
      <abstract_bg>Предлагаме метод за редактиране, базиран на сензор, който е по-гъвкав от традиционния постредакционен превод, базиран на клавиатура и мишка. Този подход разчита на действия с докосване, които потребителите извършват, за да показват грешки в превода. Представяме модел с двоен кодер за справяне с действията и генериране на рафинирани преводи. За да имитираме обратната връзка на потребителите, ние възприемаме алгоритъма сравняващ между чернови преводи и препратки, за да извлечем автоматично симулираните действия за изграждане на данни за обучение. Експерименти с набори от данни за преводи със симулирани действия за редактиране показват, че методът ни значително подобрява оригиналния превод на трансформатора (до 25.31) и превъзхожда съществуващите интерактивни методи за превод (до 16.64). Също така провеждаме експерименти с набор от данни след редактиране, за да докажем допълнително здравината и ефективността на нашия метод.</abstract_bg>
      <abstract_hr>Predlažemo metodu editiranja na dodiru za prevod, koja je fleksibilnija od tradicionalnog prevoda na klavijaturi na mišu. Ovaj pristup se oslanja na dodirne akcije koje korisnici čine kako bi pokazali greške prevođenja. Predstavljamo dvokoderski model kako bi se riješili akcije i stvorili rafinirane prevode. Da bi imitirali povratku korisnika, usvojili smo algoritam TER-a u usporedbi između nacrta prevoda i referencija na automatsku izvlačenje simuliranih akcija za izgradnju podataka o obuci. Eksperimenti o setima podataka prevođenja sa simuliranim redakcijama pokazuju da naša metoda značajno poboljšava originalni prevoz transformera (do 25,31 BLEU) i iznosi postojeće interaktivne metode prevođenja (do 16,64 BLEU). Također provodimo eksperimente o postavljanju podataka kako bi dalje dokazali robustnost i učinkovitost našeg metoda.</abstract_hr>
      <abstract_nl>We stellen een touch-based bewerkingsmethode voor vertaling voor, die flexibeler is dan traditionele toetsenbord-muis-gebaseerde vertaling postediting. Deze aanpak is gebaseerd op aanraakacties die gebruikers uitvoeren om vertaalfouten aan te geven. We presenteren een dual-encoder model om de acties af te handelen en verfijnde vertalingen te genereren. Om de feedback van de gebruiker na te bootsen, gebruiken we het TER-algoritme dat vergelijkt tussen conceptvertalingen en referenties om automatisch de gesimuleerde acties voor de constructie van trainingsgegevens te extraheren. Experimenten met vertaaldatasets met gesimuleerde bewerkingsacties tonen aan dat onze methode de originele vertaling van Transformer aanzienlijk verbetert (tot 25.31 BLEU) en bestaande interactieve vertaalmethoden (tot 16.64 BLEU) overtreft. We voeren ook experimenten uit op post-editing dataset om de robuustheid en effectiviteit van onze methode verder te bewijzen.</abstract_nl>
      <abstract_da>Vi foreslår en berøringsbaseret redigeringsmetode til oversættelse, som er mere fleksibel end traditionel tastatur-musebaseret oversættelse postredigering. Denne tilgang er baseret på berøringshandlinger, som brugerne udfører for at angive oversættelsesfejl. Vi præsenterer en dual-encoder model til at håndtere handlingerne og generere raffinerede oversættelser. For at efterligne brugerens feedback anvender vi TER algoritmen, der sammenligner mellem udkastsoversættelser og referencer for automatisk at udtrække de simulerede handlinger til opbygning af træningsdata. Eksperimenter med oversættelsesdatasæt med simulerede redigeringsaktioner viser, at vores metode forbedrer original oversættelse af Transformer betydeligt (op til 25,31 BLEU) og overgår eksisterende interaktive oversættelsesmetoder (op til 16,64 BLEU). Vi gennemfører også eksperimenter med post-redigering datasæt for yderligere at bevise robustheden og effektiviteten af vores metode.</abstract_da>
      <abstract_de>Wir schlagen eine Touch-basierte Bearbeitungsmethode für Übersetzungen vor, die flexibler ist als herkömmliche tastaturmausbasierte Übersetzungs-Nachbearbeitung. Dieser Ansatz basiert auf Touch-Aktionen, die Benutzer ausführen, um Übersetzungsfehler anzuzeigen. Wir präsentieren ein Dual-Encoder-Modell, um die Aktionen zu handhaben und verfeinerte Übersetzungen zu generieren. Um das Benutzerfeedback nachzuahmen, verwenden wir den TER-Algorithmus, der zwischen Entwurfsübersetzungen und Referenzen vergleicht, um die simulierten Aktionen für die Konstruktion von Trainingsdaten automatisch zu extrahieren. Experimente an Übersetzungsdatensätzen mit simulierten Bearbeitungsaktionen zeigen, dass unsere Methode die Originalübersetzung von Transformer signifikant verbessert (bis 25.31 BLEU) und bestehende interaktive Übersetzungsmethoden (bis 16.64 BLEU) übertrifft. Wir führen auch Experimente an Post-Editing-Datensätzen durch, um die Robustheit und Effektivität unserer Methode weiter zu belegen.</abstract_de>
      <abstract_id>Kami mengusulkan metode penyuntingan berdasarkan sentuhan untuk terjemahan, yang lebih fleksibel dari tradisional penyuntingan keyboard-mouse. pendekatan ini bergantung pada tindakan sentuhan yang dilakukan pengguna untuk menunjukkan kesalahan terjemahan. Kami mempersembahkan model dua pengekode untuk menangani tindakan dan menghasilkan terjemahan yang sempurna. Untuk meniru feedback pengguna, kami mengadopsi algoritma TER yang membandingkan antara draft terjemahan dan referensi untuk secara otomatis mengekstraksi tindakan simulasi untuk latihan konstruksi data. Eksperimen pada set data terjemahan dengan tindakan penyuntingan simulasi menunjukkan bahwa metode kami meningkatkan terjemahan asli Transformer (sampai 25,31 BLEU) dan melebihi metode terjemahan interaktif yang ada (sampai 16,64 BLEU). We also conduct experiments on post-editing dataset to further prove the robustness and effectiveness of our method.</abstract_id>
      <abstract_fa>ما پیشنهاد می‌کنیم روش ویرایش‌سازی بر پایه‌ی لمس برای ترجمه، که بیشتر از ترجمه‌سازی بر پایه‌ی کلید-موش‌های سنتی است. این روش بر روی کارهای لمس که کاربران برای نشان دادن خطاهای ترجمه انجام می دهند بستگی دارد. ما یک مدل دوگانه رمزگر را برای کنترل عمل و تولید ترجمه‌های پاکیزه پیشنهاد می‌کنیم. برای تصویر بازگشت کاربر، الگوریتم TER را در مقایسه بین پروژه ترجمه‌ها و ارتباطات برای استفاده از کارهای شبیه‌سازی برای ساختن داده‌های آموزش می‌کنیم. تجربه‌ها در مجموعه داده‌های ترجمه با کارهای ویرایش شبیه‌سازی نشان می‌دهند که روش ما به طور معنی ترجمه اصلی ترجمه‌کننده‌ی ترجمه‌کننده (تا ۲۵.۳۱ BLEU) را بهتر می‌کند و روش‌های ترجمه‌کننده‌ی متفاوتی موجود (تا ۱ ما همچنین آزمایش‌ها را در مجموعه‌ی داده‌های بعد از ویرایش انجام می‌دهیم تا بیشتر ثابت کنند استعداد و فعالیت روش ما.</abstract_fa>
      <abstract_sw>Tunazipendekeza mbinu ya kuhariri inayotokana na msingi wa taarifa kwa kutafsiri, ambayo ni yenye ubunifu zaidi ya tafsiri ya kitamaduni yenye msingi wa mifusi. Hatua hii inategemea vitendo vya taabu ambavyo watumiaji wanafanya kazi ili kuonyesha makosa ya kutafsiri. Tunaweza kutengeneza muundo wa kodi mbili ili kukabiliana na matendo na kutengeneza tafsiri zilizotafsiriwa. Kwa kutumia maoni ya mtumiaji, tunatumia utambulisho wa TER ukilinganisha kati ya rasimu za tafsiri na maoni yanayohusu kutengeneza hatua zilizofanana kwa ajili ya ujenzi wa data. Majaribio kuhusu taarifa za kutafsiri kwa kutumia hatua za kuhariri zinaonyesha kuwa njia yetu inaboresha tafsiri ya asili ya Tafsiri (hadi 25.31 BLEU) na inafanya mbinu za tafsiri zinazohusiana na moja kwa moja (hadi 16.64 BLEU). Pia tunafanya majaribio katika seti ya taarifa za baada ya kuhariri ili kuonesha zaidi ubakaji na ufanisi wa njia yetu.</abstract_sw>
      <abstract_tr>Biz özümize touch tabanly bir taryşma yöntemi teklip edip bilýäris. Bu däpli-täsirden, syçan tabanly terjime taýýarlamak üçin fleksibdir. Bu approach ullançylar terjime hatalaryny görkezmek üçin edip duran elleşmelere ynanýar. Biz bu işleri başarmak üçin iki ködleme modelini saýlaýrys we taýýarlanan terjimeleri döretýäris Ullançylaryň Feedbackyny mimoňlaşdyrmak üçin, biz TER algoritmini, faýllary terjime etmek üçin süýtgetmek üçin öz-özüne golaýlanýan emelleri çykarmak üçin gurlap kabul edip kabul edip görýäris. Simüle edilen edirmek emelleri bilen terjime eden veri setirlerinde örän çykyşlar görkezilýän çykyşlarymyz terjime edip görkezilýän çykyşlarymyz (25.31 BLEU we olara çevrilýän terjime metodlarymyz üstine çykarýar (16.64 BLEU çevrine çevril Mundan soňra veri setirini edip bilmek üçin synaglarymyzy çykarýarys.</abstract_tr>
      <abstract_sq>Ne propozojmë një metodë modifikimi për përkthimin bazuar në prekje, e cila është më fleksible se përkthimi tradicional bazuar në tastierë miu. Ky qasje mbështetet në veprimet e prekjes që përdoruesit kryejnë për të treguar gabimet e përkthimit. Ne paraqesim një model me dy kodues për të trajtuar veprimet dhe për të gjeneruar përkthime të rafinuara. Për të imituar reagimin e përdoruesit, ne miratojmë algoritmin TER që krahason midis projektit të përkthimeve dhe referencave për të nxjerrë automatikisht veprimet e simuluara për trajnimin e ndërtimit të të dhënave. Eksperimentet në grupet e të dhënave të përkthimit me veprime të simuliuara të ndryshimit tregojnë se metoda jonë përmirëson ndjeshëm përkthimin origjinal të Transformer (deri në 25.31 BLEU) dhe përmirëson metodat ekzistuese interaktive të përkthimit (deri në 16.64 BLEU). Ne gjithashtu kryejmë eksperimente në grupin e të dhënave të posteditimit për të provuar më tej fuqinë dhe efektshmërinë e metodës sonë.</abstract_sq>
      <abstract_am>ለትርጓሜ የተደረገውን የዳስስ ማቀናጃ ማቀናጃ ማድረግ እናዘጋጀዋለን፡፡ ይህ ሥርዓት በተርጉም ስህተት ለማሳየት የሚጠቀሙት ተጠቃሚዎች በሚያደርጉት ላይ በሚያስተካክሉ ሥራ ላይ ነው። ሁለተኛ የሆሄደውን ተርጓሚዎች እና ትርጓሜዎችን ለመቀበል እናደርጋለን፡፡ በተጠቃሚ መልዕክት ለመቀላቀል፣ በጽሑፉ ትርጉም እና በተለየ ትርጉም እና በተለየ ትምህርት መሠረት ላይ የተመሳሳይ ድርጊቶችን በራስነት ለማውጣት እናስቀሳቅሳለን፡፡ የትርጉም ዳታዎችን በመስመር ማቀናጃ ማቀናጃ ማድረግ ማድረግ እና ማድረግ ማድረግ ማድረግ ማድረግ ማድረግ ነው (እስከ 25.31 BLEU) እና outperforms existing interactive translation methods (up to 16.64 BLEU). እናም የሥርዓታችንን ጥብቅ እና ጥብቃዊውን ለመግለጽ በኋላ ማቀናጃ ዳታዎችን እናደርጋለን፡፡</abstract_am>
      <abstract_hy>Մենք առաջարկում ենք թարգմանման համար հպման հիմնված խմբագրման մեթոդ, որը ավելի ճկուն է, քան ավանդական մկնիկի վրա հիմնված թարգմանումը: This approach relies on touch actions that users perform to indicate translation errors.  Մենք ներկայացնում ենք երկու կոդերի մոդել, որպեսզի կառավարենք գործողությունները և ստեղծենք բարելավված թարգմանություններ: Որպեսզի կրկնօրինակենք օգտագործողի արձագանքը, մենք ընդունում ենք TER ալգորիթմը, որը համեմատում է թարգմանությունների նախագծերի և հղումների միջև, որպեսզի ինքնաբերաբար վերցնենք տվյալների կառուցման կրկն Սիմուլյացված խմբագրման գործողություններով թարգմանման տվյալների համակարգերի փորձարկումները ցույց են տալիս, որ մեր մեթոդը նշանակալիորեն բարելավում է Թանֆորմերի սկզբնական թարգմանությունը (մինչև 25.31 ԲԼԵՎ) և արտադրում է գոյություն ունեցող ինտե Մենք նաև փորձեր ենք կատարում տեղեկատվական համակարգի վերաբերյալ, որպեսզի ապացուցենք մեր մեթոդի կայունությունը և արդյունավետությունը:</abstract_hy>
      <abstract_af>Ons voorstel 'n raak-gebaseerde redigeringsmetode vir vertaling, wat is meer fleksibel as tradisionele sleutelbord-muis-gebaseerde vertaling. Hierdie toegang verlig op aanraak aksies wat gebruikers uitvoer om vertaling foute te wys. Ons stel 'n tweede-enkoder model om die aksies te hanteer en te genereer verrafinge vertalings. Om die gebruiker terugmelding te mimiseer, aanvaar ons die TER algoritme wat vergelyk tussen draft vertalings en verwysing tot outomaties uitpak die simuleerde aksies vir onderwerp van data konstruksie. Eksperimente op vertaling datastelle met simuleerde redigeer aksies vertoon dat ons metode betekenlik die oorspronklike vertaling van Transformer (tot 25. 31 BLEU) verbeter en uitvoer bestaande interaktiewe vertaling metodes (tot 16. 64 BLEU). Ons doen ook eksperimente op post-redigeering datastel om verder die kragtigheid en effektiviteit van ons metode te bevestig.</abstract_af>
      <abstract_ko>우리는 터치 기반의 번역 후 편집 방법을 제시했는데 전통적인 키보드 마우스 기반의 번역 후 편집보다 더욱 유연하다.이런 방법은 사용자가 실행하는 터치 동작에 의존해서 번역 오류를 표시한다.우리는 동작을 처리하고 정확한 번역을 만드는 이중 인코더 모델을 제시했다.사용자 피드백을 시뮬레이션하기 위해 우리는 TER 알고리즘을 적용하여 초고 번역과 참고 사이를 비교하고 시뮬레이션 동작을 자동으로 추출하여 훈련 데이터를 구축한다.아날로그 편집 조작을 가진 번역 데이터 집합에서의 실험에 의하면 우리의 방법은 Transformer의 원시 번역(BLEU 최고 25.31)을 현저하게 개선했고 기존의 상호작용 번역 방법(BLEU 최고 16.64)보다 우수하다는 것을 알 수 있다.우리는 또한 편집된 데이터 집합에 대해 실험을 진행하여 이 방법의 노봉성과 유효성을 한층 더 증명하였다.</abstract_ko>
      <abstract_bs>Predlažemo metodu editiranja na dodiru za prevod, koja je fleksibilnija od tradicionalnog postizanja prevoda na tastaturi na mišu. Ovaj pristup se oslanja na dodirne akcije koje korisnici izvode kako bi pokazali greške prevođenja. Predstavljamo dvokoderski model kako bi se bavili akcijama i stvorili refinansirane prevode. Da bi imitirali reakciju korisnika, usvojili smo algoritam TER-a u usporedbi između projekta prevoda i referencija da bi automatski izvukli simulirane akcije za izgradnju podataka o obuci. Eksperimenti o setima podataka prevođenja sa simuliranim redakcijama pokazuju da naša metoda značajno poboljšava originalni prevoz transformera (do 25,31 BLEU) i iznosi postojeće interaktivne metode prevođenja (do 16,64 BLEU). Također provodimo eksperimente o setu podataka nakon editiranja kako bismo dalje dokazali robustnost i učinkovitost našeg metoda.</abstract_bs>
      <abstract_cs>Navrhujeme dotykovou metodu editace pro překlad, která je flexibilnější než tradiční posteditace překladu pomocí klávesnice myší. Tento přístup spoléhá na dotykové akce, které uživatelé provádějí k označení chyb překladu. Představujeme model duálního kodéru pro zvládnutí akcí a generování rafinovaných překladů. Abychom napodobili zpětnou vazbu uživatelů, přijali jsme algoritmus TER porovnávání návrhů překladů a referencí, abychom automaticky extrahovali simulované akce pro tvorbu tréninkových dat. Experimenty na překladových datových sadách se simulovanými editačními akcemi ukazují, že naše metoda výrazně zlepšuje původní překlad Transformeru (až 25.31 BLEU) a překonává stávající interaktivní překladové metody (až 16.64 BLEU). Dále provádíme experimenty na post-editaci datových sad, abychom dále prokázali robustnost a efektivitu naší metody.</abstract_cs>
      <abstract_az>Biz təkrarlama üçün toxunma tabanlı düzenleme metodlarını təklif edirik. Bu, əvvəlki klavyə-sıçan təkrarlamasından daha fleksibildir. Bu tərzim istifadəçilərin qurğulama xətalarını göstərmək üçün etdikləri toxunmaq işlərinə təvəkkül edir. Biz işləri idarə etmək və təmizlənmiş tercümələri yaratmaq üçün iki kodlayıcı modeli göstəririk. İstifadəçilərin reaksiyonu mimiyalaşdırmaq üçün, məlumat inşaması üçün simulasyon eylemlərini avtomatik təhsil etmək üçün TER algoritmini alırıq. Simülatlı düzenleme eylemləri ilə çevirilən verilən verilənlər təcrübələrinin təcrübələrinin əsl tərcübəsini (25,31 BLEU-ə qədər ) daha yaxşı dəyişdirir və mevcut interaktif tercümə metodlarını (16,64 BLEU-ə qədər artırır). Biz həmçinin metodumuzun güclülüyünü və etkinliğini daha çox təsdiqləmək üçün verilən verilənlər qutusunda də imtahana çəkirik.</abstract_az>
      <abstract_bn>আমরা অনুবাদের জন্য একটি স্পর্শ ভিত্তিক সম্পাদক পদ্ধতি প্রস্তাব করি, যা ঐতিহ্যবাহী কীবোর্ড- মাউস ভিত্তিক অনুবাদ পোস্টিং এই পদ্ধতি অনুবাদের ত্রুটি নির্দেশ করার জন্য ব্যবহারকারীদের স্পর্শ করার উপর নির্ভর করে। আমরা একটি দুই এনকোডার মডেল উপস্থাপন করি কাজের মাধ্যমে কাজ করা এবং সংস্কার করা অনুবাদ তৈরি করার জন্য। ব্যবহারকারীর ফিডব্যাককে মাইক করার জন্য আমরা টেআর অ্যালগরিদম মেনে নিয়েছি মানচিত্র অনুবাদ এবং উল্লেখ করার মধ্যে তুলনা করেছি স্বয়ংক্রিয়ভাব অনুবাদের তথ্য সংক্রান্ত সম্পাদনার সাথে অনুবাদের পরীক্ষার পরীক্ষা দেখা যাচ্ছে যে আমাদের পদ্ধতি অনুবাদের মূল অনুবাদের গুরুত্বপূর্ণ করে দেয় (২৫. ৩১ এছাড়াও আমরা পরীক্ষা করি আমাদের পদ্ধতির রাস্তা ও কার্যকর প্রমাণের জন্য পোস্ট সম্পাদনার পরিকল্পনায়।</abstract_bn>
      <abstract_ca>Proposem un mètode d'edició basat en contacte per a la traducció, que és més flexible que la traducció tradicional basada en el ratolí. Aquest enfocament es basa en accions de contacte que els usuaris fan per indicar errors de traducció. Presentam un model de doble codificador per gestionar les accions i generar traduccions refines. Per imitar el feedback dels usuaris, adoptem l'algoritme TER comparant entre projectes de traducció i referències per extrair automàticament les accions simulades per a formar la construcció de dades. Els experiments en conjunts de dades de traducció amb accions d'edició simulades mostran que el nostre mètode millora significativament la traducció original del Transformer (fins a 25,31 BLEU) i supera els mètodes de traducció interactiu existents (fins a 16,64 BLEU). També fem experiments en un conjunt de dades post-edicionats per demostrar més la robustet i l'eficacia del nostre mètode.</abstract_ca>
      <abstract_et>Tõlkimiseks pakume välja puutepõhise redigeerimismeetodi, mis on paindlikum kui traditsiooniline klaviatuuri-hiirepõhine tõlke järeltredigeerimine. See lähenemine tugineb puutetoimingutele, mida kasutajad teevad tõlkevigade märkimiseks. Esitame topeltkodeerija mudeli toimingute käsitlemiseks ja rafineeritud tõlkete genereerimiseks. Kasutaja tagasiside jäljendamiseks võtame kasutusele TER algoritmi, mis võrdleb projekti tõlkeid ja viiteid, et automaatselt välja simuleeritud toimingud koolitusandmete ehitamiseks. Simuleeritud redigeerimistoimingutega seotud tõlkeandmekogumite eksperimendid näitavad, et meie meetod parandab oluliselt Transformeri originaaltõlget (kuni 25.31 BLEU) ja ületab olemasolevaid interaktiivseid tõlkemeetodeid (kuni 16.64 BLEU). Samuti teeme katseid järeltöötluse andmekogumiga, et veelgi tõestada meie meetodi tugevust ja efektiivsust.</abstract_et>
      <abstract_fi>Ehdotamme kosketuspohjaista k채채nn철smenetelm채채, joka on joustavampi kuin perinteinen n채pp채imist철hiiripohjainen k채채nn철ksen j채lkieditointi. T채m채 l채hestymistapa perustuu kosketustoimintoihin, joita k채ytt채j채t suorittavat k채채nn철svirheiden osoittamiseksi. Esittelemme kaksoiskooderimallin, joka k채sittelee toimintoja ja tuottaa hienostuneita k채채nn철ksi채. K채ytt채jien palautteen j채ljittelemiseksi otamme k채ytt철철n TER-algoritmin, joka vertailee luonnoksen k채채nn철ksi채 ja viittauksia, jotta simuloidut toimet voidaan automaattisesti poimia harjoitusdatan rakentamiseen. Simuloiduilla editointitoimilla tehdyt k채채nn철stietokokeet osoittavat, ett채 menetelm채mme parantaa merkitt채v채sti Transformerin alkuper채isk채채nn철st채 (jopa 25.31 BLEU) ja ylitt채채 olemassa olevat interaktiiviset k채채nn철smenetelm채t (jopa 16.64 BLEU). Teemme my철s kokeiluja j채lkimuokkausaineistosta todistaaksemme menetelm채mme luotettavuuden ja tehokkuuden.</abstract_fi>
      <abstract_sk>Predlagamo metodo urejanja na dotik za prevajanje, ki je fleksibilnejša od tradicionalnega postdediranja prevajanja na tipkovnici in miški. Ta pristop temelji na dejanjih na dotik, ki jih uporabniki izvajajo za označevanje napak pri prevodu. Predstavljamo model z dvojnim kodirnikom za upravljanje dejanj in ustvarjanje prefinjenih prevodov. Za posnemanje povratnih informacij uporabnikov uporabimo algoritem TER, ki primerja osnutke prevodov in sklicevanja, da samodejno izvlečemo simulirane akcije za gradnjo podatkov o usposabljanju. Eksperimenti na naborih podatkov o prevajanju s simuliranimi ukrepi urejanja kažejo, da naša metoda bistveno izboljša originalni prevod Transformerja (do 25.31 BLEU) in presega obstoječe interaktivne prevajalske metode (do 16.64 BLEU). Izvajamo tudi poskuse na naboru podatkov po urejanju, da bi dodatno dokazali robustnost in učinkovitost naše metode.</abstract_sk>
      <abstract_ha>Tuna goyyar da shirin taƙaitar da aka binge ta kan birarin, wanda ke fi fleksible ko kuma da fassarar fassarar da aka danne shi na zaman ayuka-mouse. Wannan durowa yana dõgara ga aikin da za'a yi amfani da shi, wanda suke aikin su nuna errors ga fassarar. Tuna gabatar da wani misali na kode-dubu dõmin su yi aiki na aikin ƙwarai kuma mu ƙiƙiro fassarar da aka fassara. To, in ƙayyade feedback na mai amfani da shi, mu zãɓi algoritm na Tsariya da ke daidaita tsakanin fassarar-fassarar da Reference zuwa ya nuna aikin da aka kamata ɗabi'a wa tsarin data. KCharselect unicode block name Kayya, Munã samun jarrabo a kan danne-haƙin bayan-editi, dõmin ka ƙara bayani ga manunufi da tayari na hanyarmu.</abstract_ha>
      <abstract_jv>Awak dhéwé ngerwih akeh basa nang dodo-basa sing tarjamahan kanggo tarjamahan, sing langgar bantuan karo perusahaan-musik sing basa tarjamahan. Name Awak dhéwé éntuk model sing duwat-koder kanggo ngebagian aksi lan nggawe tarjamahan Jejaring Name Awak dhéwé éntuk éntuk éntuk mulai perusahaan dengané dadi nggawe dadi nggawe barang nggawe barang nggawe barang nggawe sistem sing beraksi dadi.</abstract_jv>
      <abstract_he>אנו מציעים שיטת עורך מבוססת מגע לתרגום, שהיא יותר גמישה מאשר התרגום מסורתי מבוסס על עכבר. This approach relies on touch actions that users perform to indicate translation errors.  אנחנו מציגים מודל קודם כפול כדי לטפל בפעולות ולייצר תרגומות מעודפות. כדי לחקות את ההחזרה של המשתמשים, אנו מאמצים את האלגוריתם TER שיווה בין התרגשות סגנון וההתייחסות כדי לחלץ באופן אוטומטי את הפעולות הסימולציות לבניית נתונים. ניסויים על קבוצות נתונים התרגום עם פעולות עורכת סמיולציות מראים ששיטתנו משפר באופן משמעותי את התרגום המקורי של טרנספורטר (עד 25.31 BLEU) ומעביר את שיטות התרגום אינטראקטיבית קיימות (עד 16.64 BLEU). אנחנו גם מבצעים ניסויים על קבוצת נתונים לאחר העורה כדי להוכיח עוד את החזקה והיעילות של השיטה שלנו.</abstract_he>
      <abstract_bo>We propose a touch-based editing method for translation, which is more flexible than traditional keyboard-mouse-based translation postediting. གཟུགས་སྐོར་འདིས་སྤྱོད་མཁན་གྱི་སྤྲོད་ཀྱི་བྱ་འགུལ་ལ་རྟེན་འདུག We present a dual-encoder model to handle the actions and generate refined translations. སྤྱོད་མཁན དབྱིབས་སྒྱུར་བཅོས་གནད་སྡུད་གཞུང་ཚབ་དང་མཉམ་དུ་བསྡུར་བའི་བྱ་འགུལ ང་ཚོས་ཞིབ་ཕྱིར་བསྒྱུར་བཅོས་ཐབས་ལམ་གྱི་སྒྲིག་འགོད་གནད་དོན་དག་གི་ལྟ་ཞིབ་བྱེད་ཀྱི་ཡོད།</abstract_bo>
      </paper>
    <paper id="2">
      <title>Can Monolingual Pretrained Models Help Cross-Lingual Classification?</title>
      <author><first>Zewen</first><last>Chi</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Xianling</first><last>Mao</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <pages>12–17</pages>
      <abstract>Multilingual pretrained language models (such as multilingual BERT) have achieved impressive results for cross-lingual transfer. However, due to the constant model capacity, multilingual pre-training usually lags behind the monolingual competitors. In this work, we present two approaches to improve zero-shot cross-lingual classification, by transferring the knowledge from monolingual pretrained models to multilingual ones. Experimental results on two cross-lingual classification benchmarks show that our methods outperform vanilla multilingual fine-tuning.</abstract>
      <url hash="8441b124">2020.aacl-main.2</url>
      <bibkey>chi-etal-2020-monolingual</bibkey>
    </paper>
    <paper id="5">
      <title>FERNet : Fine-grained Extraction and Reasoning Network for Emotion Recognition in Dialogues<fixed-case>FERN</fixed-case>et: Fine-grained Extraction and Reasoning Network for Emotion Recognition in Dialogues</title>
      <author><first>Yingmei</first><last>Guo</last></author>
      <author><first>Zhiyong</first><last>Wu</last></author>
      <author><first>Mingxing</first><last>Xu</last></author>
      <pages>37–43</pages>
      <abstract>Unlike non-conversation scenes, <a href="https://en.wikipedia.org/wiki/Emotion_recognition">emotion recognition</a> in dialogues (ERD) poses more complicated challenges due to its interactive nature and intricate <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a>. All present methods model historical utterances without considering the content of the target utterance. However, different parts of a historical utterance may contribute differently to emotion inference of different target utterances. Therefore we propose Fine-grained Extraction and Reasoning Network (FERNet) to generate target-specific historical utterance representations. The reasoning module effectively handles both local and global sequential dependencies to reason over context, and updates target utterance representations to more informed vectors. Experiments on two <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> show that our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> achieves competitive performance compared with previous <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a>.</abstract>
      <url hash="8392b1d5">2020.aacl-main.5</url>
      <bibkey>guo-etal-2020-fernet</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="11">
      <title>Investigating Learning Dynamics of BERT Fine-Tuning<fixed-case>BERT</fixed-case> Fine-Tuning</title>
      <author><first>Yaru</first><last>Hao</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Ke</first><last>Xu</last></author>
      <pages>87–92</pages>
      <abstract>The recently introduced pre-trained language model BERT advances the state-of-the-art on many NLP tasks through the fine-tuning approach, but few studies investigate how the fine-tuning process improves the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance on downstream tasks. In this paper, we inspect the learning dynamics of BERT fine-tuning with two indicators. We use JS divergence to detect the change of the attention mode and use SVCCA distance to examine the change to the feature extraction mode during BERT fine-tuning. We conclude that BERT fine-tuning mainly changes the attention mode of the last layers and modifies the feature extraction mode of the intermediate and last layers. Moreover, we analyze the consistency of BERT fine-tuning between different <a href="https://en.wikipedia.org/wiki/Random_seed">random seeds</a> and different datasets. In summary, we provide a distinctive understanding of the learning dynamics of BERT fine-tuning, which sheds some light on improving the <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> results.</abstract>
      <url hash="d60ccad2">2020.aacl-main.11</url>
      <bibkey>hao-etal-2020-investigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="15">
      <title>A Simple Text-based Relevant Location Prediction Method using Knowledge Base</title>
      <author><first>Mei</first><last>Sasaki</last></author>
      <author><first>Shumpei</first><last>Okura</last></author>
      <author><first>Shingo</first><last>Ono</last></author>
      <pages>116–121</pages>
      <abstract>In this paper, we propose a simple method to predict salient locations from <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news article text</a> using a knowledge base (KB). The proposed method uses a dictionary of locations created from the <a href="https://en.wikipedia.org/wiki/Kibibyte">KB</a> to identify occurrences of locations in the text and uses the hierarchical information between entities in the <a href="https://en.wikipedia.org/wiki/Kibibyte">KB</a> for assigning appropriate saliency scores to regions. It allows prediction at arbitrary region units and has only a few hyperparameters that need to be tuned. We show using manually annotated news articles that the proposed method improves the <a href="https://en.wikipedia.org/wiki/F-measure">f-measure</a> by   0.12 compared to multiple baselines.</abstract>
      <url hash="0a73979d">2020.aacl-main.15</url>
      <bibkey>sasaki-etal-2020-simple</bibkey>
    </paper>
    <paper id="17">
      <title>An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks<fixed-case>K</fixed-case>orean <fixed-case>NLP</fixed-case> Tasks</title>
      <author><first>Kyubyong</first><last>Park</last></author>
      <author><first>Joohong</first><last>Lee</last></author>
      <author><first>Seongbo</first><last>Jang</last></author>
      <author><first>Dawoon</first><last>Jung</last></author>
      <pages>133–142</pages>
      <abstract>Typically, <a href="https://en.wikipedia.org/wiki/Lexical_analysis">tokenization</a> is the very first step in most <a href="https://en.wikipedia.org/wiki/Text_processing">text processing</a> works. As a token serves as an atomic unit that embeds the contextual information of text, how to define a token plays a decisive role in the performance of a model. Even though Byte Pair Encoding (BPE) has been considered the de facto standard tokenization method due to its simplicity and universality, it still remains unclear whether BPE works best across all languages and tasks. In this paper, we test several tokenization strategies in order to answer our primary research question, that is, What is the best tokenization strategy for Korean NLP tasks? Experimental results demonstrate that a hybrid approach of morphological segmentation followed by BPE works best in <a href="https://en.wikipedia.org/wiki/Korean_language">Korean</a> to / from English machine translation and natural language understanding tasks such as KorNLI, KorSTS, NSMC, and PAWS-X. As an exception, for KorQuAD, the Korean extension of SQuAD, BPE segmentation turns out to be the most effective. Our code and pre-trained models are publicly available at https://github.com/kakaobrain/kortok.</abstract>
      <url hash="5c4b3742">2020.aacl-main.17</url>
      <bibkey>park-etal-2020-empirical</bibkey>
      <pwccode url="https://github.com/kakaobrain/kortok" additional="false">kakaobrain/kortok</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kornli">KorNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/korsts">KorSTS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="22">
      <title>Named Entity Recognition in Multi-level Contexts</title>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Tao</first><last>Qi</last></author>
      <author><first>Zhigang</first><last>Yuan</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>181–190</pages>
      <abstract>Named entity recognition is a critical task in the <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing field</a>. Most existing <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> can only exploit <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> within a sentence. However, their performance on recognizing entities in limited or ambiguous sentence-level contexts is usually unsatisfactory. Fortunately, other sentences in the same document can provide supplementary document-level contexts to help recognize these entities. In addition, words themselves contain word-level contextual information since they usually have different preferences of entity type and relative position from named entities. In this paper, we propose a unified framework to incorporate <a href="https://en.wikipedia.org/wiki/Context_(computing)">multi-level contexts</a> for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>. We use TagLM as our basic <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to capture sentence-level contexts. To incorporate document-level contexts, we propose to capture interactions between sentences via a multi-head self attention network. To mine word-level contexts, we propose an auxiliary task to predict the type of each word to capture its type preference. We jointly train our model in entity recognition and the auxiliary classification task via <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>. The experimental results on several <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark datasets</a> validate the effectiveness of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a>.</abstract>
      <url hash="6e5498c7">2020.aacl-main.22</url>
      <bibkey>chen-etal-2020-named</bibkey>
    </paper>
    <paper id="28">
      <title>Generating Commonsense Explanation by Extracting Bridge Concepts from Reasoning Paths</title>
      <author><first>Haozhe</first><last>Ji</last></author>
      <author><first>Pei</first><last>Ke</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>248–257</pages>
      <abstract>Commonsense explanation generation aims to empower the machine’s sense-making capability by generating plausible explanations to statements against commonsense. While this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is easy to human, the machine still struggles to generate reasonable and informative explanations. In this work, we propose a <a href="https://en.wikipedia.org/wiki/Methodology">method</a> that first extracts the underlying concepts which are served as bridges in the reasoning chain and then integrates these <a href="https://en.wikipedia.org/wiki/Concept">concepts</a> to generate the final explanation. To facilitate the reasoning process, we utilize external commonsense knowledge to build the connection between a statement and the bridge concepts by extracting and pruning multi-hop paths to build a subgraph. We design a bridge concept extraction model that first scores the triples, routes the paths in the subgraph, and further selects bridge concepts with weak supervision at both the triple level and the concept level. We conduct experiments on the commonsense explanation generation task and our model outperforms the state-of-the-art baselines in both automatic and human evaluation.</abstract>
      <url hash="238efc0a">2020.aacl-main.28</url>
      <bibkey>ji-etal-2020-generating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="31">
      <title>All-in-One : A Deep Attentive Multi-task Learning Framework for <a href="https://en.wikipedia.org/wiki/Humour">Humour</a>, <a href="https://en.wikipedia.org/wiki/Sarcasm">Sarcasm</a>, Offensive, <a href="https://en.wikipedia.org/wiki/Motivation">Motivation</a>, and Sentiment on Memes</title>
      <author><first>Dushyant Singh</first><last>Chauhan</last></author>
      <author><first>Dhanush</first><last>S R</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>281–290</pages>
      <abstract>In this paper, we aim at learning the relationships and similarities of a variety of tasks, such as humour detection, sarcasm detection, offensive content detection, motivational content detection and <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> on a somewhat complicated form of information, i.e., <a href="https://en.wikipedia.org/wiki/Meme">memes</a>. We propose a multi-task, multi-modal deep learning framework to solve multiple tasks simultaneously. For <a href="https://en.wikipedia.org/wiki/Computer_multitasking">multi-tasking</a>, we propose two attention-like mechanisms viz., Inter-task Relationship Module (iTRM) and Inter-class Relationship Module (iCRM). The main motivation of iTRM is to learn the relationship between the tasks to realize how they help each other. In contrast, <a href="https://en.wikipedia.org/wiki/ICRM">iCRM</a> develops relations between the different classes of tasks. Finally, <a href="https://en.wikipedia.org/wiki/Representation_(arts)">representations</a> from both the attentions are concatenated and shared across the five <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> (i.e., <a href="https://en.wikipedia.org/wiki/Humour">humour</a>, <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a>, offensive, motivational, and sentiment) for <a href="https://en.wikipedia.org/wiki/Computer_multitasking">multi-tasking</a>. We use the recently released dataset in the Memotion Analysis task @ SemEval 2020, which consists of <a href="https://en.wikipedia.org/wiki/Meme">memes</a> annotated for the classes as mentioned above. Empirical results on Memotion dataset show the efficacy of our proposed approach over the existing state-of-the-art systems (Baseline and SemEval 2020 winner). The evaluation also indicates that the proposed multi-task framework yields better performance over the single-task learning.</abstract>
      <url hash="63d636f7">2020.aacl-main.31</url>
      <bibkey>chauhan-etal-2020-one</bibkey>
    </paper>
    <paper id="32">
      <title>Identifying Implicit Quotes for Unsupervised Extractive Summarization of Conversations</title>
      <author><first>Ryuji</first><last>Kano</last></author>
      <author><first>Yasuhide</first><last>Miura</last></author>
      <author><first>Tomoki</first><last>Taniguchi</last></author>
      <author><first>Tomoko</first><last>Ohkuma</last></author>
      <pages>291–302</pages>
      <abstract>We propose Implicit Quote Extractor, an end-to-end unsupervised extractive neural summarization model for conversational texts. When we reply to posts, <a href="https://en.wikipedia.org/wiki/Quotation">quotes</a> are used to highlight important part of texts. We aim to extract quoted sentences as summaries. Most replies do not explicitly include <a href="https://en.wikipedia.org/wiki/Quotation">quotes</a>, so it is difficult to use <a href="https://en.wikipedia.org/wiki/Quotation">quotes</a> as <a href="https://en.wikipedia.org/wiki/Supervisor">supervision</a>. However, even if it is not explicitly shown, replies always refer to certain parts of texts ; we call them implicit quotes. Implicit Quote Extractor aims to extract implicit quotes as summaries. The training task of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is to predict whether a reply candidate is a true reply to a post. For <a href="https://en.wikipedia.org/wiki/Prediction">prediction</a>, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> has to choose a few sentences from the post. To predict accurately, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> learns to extract sentences that replies frequently refer to. We evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on two email datasets and one social media dataset, and confirm that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is useful for extractive summarization. We further discuss two topics ; one is whether quote extraction is an important factor for summarization, and the other is whether our model can capture salient sentences that conventional methods can not.</abstract>
      <url hash="60586b7f">2020.aacl-main.32</url>
      <bibkey>kano-etal-2020-identifying</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-tifu">Reddit TIFU</pwcdataset>
    </paper>
    <paper id="37">
      <title>Chinese Content Scoring : Open-Access Datasets and Features on Different Segmentation Levels<fixed-case>C</fixed-case>hinese Content Scoring: Open-Access Datasets and Features on Different Segmentation Levels</title>
      <author><first>Yuning</first><last>Ding</last></author>
      <author><first>Andrea</first><last>Horbach</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>347–357</pages>
      <abstract>In this paper, we analyse the challenges of Chinese content scoring in comparison to <a href="https://en.wikipedia.org/wiki/English_language">English</a>. As a review of prior work for <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese content scoring</a> shows a lack of <a href="https://en.wikipedia.org/wiki/Open_access">open-access data</a> in the field, we present two short-answer data sets for <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. The Chinese Educational Short Answers data set (CESA) contains 1800 student answers for five science-related questions. As a second <a href="https://en.wikipedia.org/wiki/Data_set">data set</a>, we collected ASAP-ZH with 942 answers by re-using three existing prompts from the ASAP data set. We adapt a state-of-the-art content scoring system for <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a> and evaluate it in several settings on these <a href="https://en.wikipedia.org/wiki/Data_set">data sets</a>. Results show that <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> on lower segmentation levels such as character n-grams tend to have better performance than <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> on token level.</abstract>
      <url hash="2da0605a">2020.aacl-main.37</url>
      <attachment type="Dataset" hash="5fdfe75f">2020.aacl-main.37.Dataset.zip</attachment>
      <attachment type="Software" hash="5fdfe75f">2020.aacl-main.37.Software.zip</attachment>
      <bibkey>ding-etal-2020-chinese</bibkey>
    </paper>
    <paper id="39">
      <title>An Exploratory Study on Multilingual Quality Estimation</title>
      <author><first>Shuo</first><last>Sun</last></author>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Frédéric</first><last>Blain</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Ahmed</first><last>El-Kishky</last></author>
      <author><first>Adithya</first><last>Renduchintala</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>366–377</pages>
      <abstract>Predicting the quality of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> has traditionally been addressed with language-specific models, under the assumption that the quality label distribution or <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">linguistic features</a> exhibit traits that are not shared across languages. An obvious disadvantage of this approach is the need for <a href="https://en.wikipedia.org/wiki/Data_(computing)">labelled data</a> for each given language pair. We challenge this assumption by exploring different approaches to multilingual Quality Estimation (QE), including using scores from translation models. We show that these outperform single-language models, particularly in less balanced quality label distributions and low-resource settings. In the extreme case of zero-shot QE, we show that it is possible to accurately predict <a href="https://en.wikipedia.org/wiki/Quality_(business)">quality</a> for any given new language from <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on other languages. Our findings indicate that state-of-the-art neural QE models based on powerful pre-trained representations generalise well across languages, making them more applicable in real-world settings.</abstract>
      <url hash="de8f76ca">2020.aacl-main.39</url>
      <bibkey>sun-etal-2020-exploratory</bibkey>
    </paper>
    <paper id="40">
      <title>English-to-Chinese Transliteration with Phonetic Auxiliary Task<fixed-case>E</fixed-case>nglish-to-<fixed-case>C</fixed-case>hinese Transliteration with Phonetic Auxiliary Task</title>
      <author><first>Yuan</first><last>He</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <pages>378–388</pages>
      <abstract>Approaching named entities transliteration as a Neural Machine Translation (NMT) problem is common practice. While many have applied various NMT techniques to enhance machine transliteration models, few focus on the linguistic features particular to the relevant languages. In this paper, we investigate the effect of incorporating phonetic features for English-to-Chinese transliteration under the multi-task learning (MTL) settingwhere we define a phonetic auxiliary task aimed to improve the generalization performance of the main transliteration task. In addition to our system, we also release a new English-to-Chinese dataset and propose a novel evaluation metric which considers multiple possible <a href="https://en.wikipedia.org/wiki/Transliteration">transliterations</a> given a source name. Our results show that the multi-task model achieves similar performance as the previous state of the art with a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> of a much smaller size.</abstract>
      <url hash="2bfc4e0f">2020.aacl-main.40</url>
      <bibkey>he-cohen-2020-english</bibkey>
    </paper>
    <paper id="41">
      <title>Predicting and Using Target Length in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a></title>
      <author><first>Zijian</first><last>Yang</last></author>
      <author><first>Yingbo</first><last>Gao</last></author>
      <author><first>Weiyue</first><last>Wang</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>389–395</pages>
      <abstract>Attention-based encoder-decoder models have achieved great success in neural machine translation tasks. However, the lengths of the target sequences are not explicitly predicted in these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. This work proposes length prediction as an auxiliary task and set up a <a href="https://en.wikipedia.org/wiki/Subnetwork">sub-network</a> to obtain the length information from the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a>. Experimental results show that the length prediction sub-network brings improvements over the strong baseline system and that the predicted length can be used as an alternative to length normalization during decoding.</abstract>
      <url hash="f6b395e0">2020.aacl-main.41</url>
      <bibkey>yang-etal-2020-predicting</bibkey>
    </paper>
    <paper id="43">
      <title>Heads-up ! Unsupervised Constituency Parsing via Self-Attention Heads</title>
      <author><first>Bowen</first><last>Li</last></author>
      <author><first>Taeuk</first><last>Kim</last></author>
      <author><first>Reinald Kim</first><last>Amplayo</last></author>
      <author><first>Frank</first><last>Keller</last></author>
      <pages>409–424</pages>
      <abstract>Transformer-based pre-trained language models (PLMs) have dramatically improved the state of the art in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> across many tasks. This has led to substantial interest in analyzing the syntactic knowledge PLMs learn. Previous approaches to this question have been limited, mostly using test suites or probes. Here, we propose a novel fully unsupervised parsing approach that extracts constituency trees from PLM attention heads. We rank transformer attention heads based on their inherent properties, and create an ensemble of high-ranking heads to produce the final <a href="https://en.wikipedia.org/wiki/Tree_(graph_theory)">tree</a>. Our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> is adaptable to low-resource languages, as it does not rely on development sets, which can be expensive to annotate. Our experiments show that the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> often outperform existing approaches if there is no development set present. Our unsupervised parser can also be used as a tool to analyze the grammars PLMs learn implicitly. For this, we use the parse trees induced by our method to train a neural PCFG and compare it to a <a href="https://en.wikipedia.org/wiki/Formal_grammar">grammar</a> derived from a human-annotated treebank.</abstract>
      <url hash="c083293f">2020.aacl-main.43</url>
      <bibkey>li-etal-2020-heads</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="45">
      <title>Self-Supervised Learning for Pairwise Data Refinement</title>
      <author><first>Gustavo</first><last>Hernandez Abrego</last></author>
      <author><first>Bowen</first><last>Liang</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Zarana</first><last>Parekh</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Yunhsuan</first><last>Sung</last></author>
      <pages>435–446</pages>
      <abstract>Pairwise data automatically constructed from weakly supervised signals has been widely used for training <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning models</a>. Pairwise datasets such as parallel texts can have uneven quality levels overall, but usually contain data subsets that are more useful as learning examples. We present two methods to refine data that are aimed to obtain that kind of subsets in a self-supervised way. Our methods are based on iteratively training dual-encoder models to compute <a href="https://en.wikipedia.org/wiki/Similarity_measure">similarity scores</a>. We evaluate our methods on de-noising <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel texts</a> and training neural machine translation models. We find that : (i) The self-supervised refinement achieves most machine translation gains in the first iteration, but following iterations further improve its intrinsic evaluation. (ii) <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine translations</a> can improve the de-noising performance when combined with selection steps. (iii) Our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a> are able to reach the performance of a supervised method. Being entirely self-supervised, our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a> are well-suited to handle pairwise data without the need of prior knowledge or human annotations.</abstract>
      <url hash="d0e82ad5">2020.aacl-main.45</url>
      <bibkey>hernandez-abrego-etal-2020-self</bibkey>
    </paper>
    <paper id="46">
      <title>A Survey of the State of Explainable AI for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a><fixed-case>AI</fixed-case> for Natural Language Processing</title>
      <author><first>Marina</first><last>Danilevsky</last></author>
      <author><first>Kun</first><last>Qian</last></author>
      <author><first>Ranit</first><last>Aharonov</last></author>
      <author><first>Yannis</first><last>Katsis</last></author>
      <author><first>Ban</first><last>Kawas</last></author>
      <author><first>Prithviraj</first><last>Sen</last></author>
      <pages>447–459</pages>
      <abstract>Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing (NLP)</a>. We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.</abstract>
      <url hash="21a4d044">2020.aacl-main.46</url>
      <bibkey>danilevsky-etal-2020-survey</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="47">
      <title>Beyond Fine-tuning : Few-Sample Sentence Embedding Transfer</title>
      <author><first>Siddhant</first><last>Garg</last></author>
      <author><first>Rohit Kumar</first><last>Sharma</last></author>
      <author><first>Yingyu</first><last>Liang</last></author>
      <pages>460–469</pages>
      <abstract>Fine-tuning (FT) pre-trained sentence embedding models on small datasets has been shown to have limitations. In this paper we show that concatenating the embeddings from the pre-trained model with those from a simple sentence embedding model trained only on the target data, can improve over the performance of FT for few-sample tasks. To this end, a linear classifier is trained on the combined embeddings, either by freezing the embedding model weights or training the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> and embedding models end-to-end. We perform evaluation on seven small datasets from NLP tasks and show that our approach with end-to-end training outperforms FT with negligible <a href="https://en.wikipedia.org/wiki/Overhead_(computing)">computational overhead</a>. Further, we also show that sophisticated combination techniques like <a href="https://en.wikipedia.org/wiki/Concatenation">CCA</a> and <a href="https://en.wikipedia.org/wiki/Concatenation">KCCA</a> do not work as well in practice as <a href="https://en.wikipedia.org/wiki/Concatenation">concatenation</a>. We provide <a href="https://en.wikipedia.org/wiki/Mathematical_model">theoretical analysis</a> to explain this empirical observation.</abstract>
      <url hash="fecbf5c4">2020.aacl-main.47</url>
      <bibkey>garg-etal-2020-beyond</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
    </paper>
    <paper id="48">
      <title>Multimodal Pretraining for Dense Video Captioning</title>
      <author><first>Gabriel</first><last>Huang</last></author>
      <author><first>Bo</first><last>Pang</last></author>
      <author><first>Zhenhai</first><last>Zhu</last></author>
      <author><first>Clara</first><last>Rivera</last></author>
      <author><first>Radu</first><last>Soricut</last></author>
      <pages>470–490</pages>
      <abstract>Learning specific hands-on skills such as <a href="https://en.wikipedia.org/wiki/Cooking">cooking</a>, <a href="https://en.wikipedia.org/wiki/Service_(motor_vehicle)">car maintenance</a>, and <a href="https://en.wikipedia.org/wiki/Home_repair">home repairs</a> increasingly happens via instructional videos. The user experience with such videos is known to be improved by meta-information such as time-stamped annotations for the main steps involved. Generating such <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> automatically is challenging, and we describe here two relevant contributions. First, we construct and release a new dense video captioning dataset, Video Timeline Tags (ViTT), featuring a variety of instructional videos together with time-stamped annotations. Second, we explore several multimodal sequence-to-sequence pretraining strategies that leverage large unsupervised datasets of videos and caption-like texts. We pretrain and subsequently finetune dense video captioning models using both YouCook2 and ViTT. We show that such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> generalize well and are robust over a wide variety of instructional videos.</abstract>
      <url hash="2da7b89c">2020.aacl-main.48</url>
      <bibkey>huang-etal-2020-multimodal</bibkey>
      <pwccode url="https://github.com/google-research-datasets/Video-Timeline-Tags-ViTT" additional="false">google-research-datasets/Video-Timeline-Tags-ViTT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/vitt">ViTT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/howto100m">HowTo100M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kinetics">Kinetics</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/recipe1m-1">Recipe1M+</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/youcook2">YouCook2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/youtube-8m">YouTube-8M</pwcdataset>
    </paper>
    <paper id="54">
      <title>Point-of-Interest Oriented Question Answering with Joint Inference of Semantic Matching and Distance Correlation</title>
      <author><first>Yifei</first><last>Yuan</last></author>
      <author><first>Jingbo</first><last>Zhou</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>542–550</pages>
      <abstract>Point-of-Interest (POI) oriented question answering (QA) aims to return a list of POIs given a question issued by a user. Recent advances in intelligent virtual assistants have opened the possibility of engaging the client software more actively in the provision of location-based services, thereby showing great promise for automatic POI retrieval. Some existing <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA methods</a> can be adopted on this task such as <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA similarity calculation</a> and <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a> using pre-defined rules. The returned results, however, are subject to inherent limitations due to the lack of the ability for handling some important POI related information, including <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">tags</a>, location entities, and proximity-related terms (e.g. nearby, close). In this paper, we present a novel deep learning framework integrated with joint inference to capture both tag semantic and geographic correlation between question and POIs. One characteristic of our model is to propose a special cross attention question embedding neural network structure to obtain question-to-POI and POI-to-question information. Besides, we utilize a <a href="https://en.wikipedia.org/wiki/Skewness">skewed distribution</a> to simulate the spatial relationship between questions and POIs. By measuring the results offered by the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> against existing methods, we demonstrate its robustness and practicability, and supplement our conclusions with empirical evidence.</abstract>
      <url hash="f6e8f0a8">2020.aacl-main.54</url>
      <bibkey>yuan-etal-2020-point</bibkey>
    </paper>
    <paper id="55">
      <title>Leveraging Structured Metadata for Improving <a href="https://en.wikipedia.org/wiki/Question_answering">Question Answering</a> on the Web</title>
      <author><first>Xinya</first><last>Du</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <author><first>Adam</first><last>Fourney</last></author>
      <author><first>Robert</first><last>Sim</last></author>
      <author><first>Paul</first><last>Bennett</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <pages>551–556</pages>
      <abstract>We show that leveraging <a href="https://en.wikipedia.org/wiki/Metadata">metadata information</a> from <a href="https://en.wikipedia.org/wiki/Web_page">web pages</a> can improve the performance of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for answer passage selection / reranking. We propose a neural passage selection model that leverages <a href="https://en.wikipedia.org/wiki/Metadata">metadata information</a> with a fine-grained encoding strategy, which learns the representation for <a href="https://en.wikipedia.org/wiki/Metadata">metadata predicates</a> in a hierarchical way. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are evaluated on the MS MARCO (Nguyen et al., 2016) and Recipe-MARCO datasets. Results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> significantly outperform baseline models, which do not incorporate <a href="https://en.wikipedia.org/wiki/Metadata">metadata</a>. We also show that the fine-grained encoding’s advantage over other strategies for encoding the <a href="https://en.wikipedia.org/wiki/Metadata">metadata</a>.</abstract>
      <url hash="f361ccd8">2020.aacl-main.55</url>
      <bibkey>du-etal-2020-leveraging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="59">
      <title>Cue Me In : Content-Inducing Approaches to Interactive Story Generation</title>
      <author><first>Faeze</first><last>Brahman</last></author>
      <author><first>Alexandru</first><last>Petrusca</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <pages>588–597</pages>
      <abstract>Automatically generating stories is a challenging problem that requires producing causally related and logical sequences of events about a topic. Previous approaches in this domain have focused largely on one-shot generation, where a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> outputs a complete story based on limited initial input from a user. Here, we instead focus on the task of interactive story generation, where the user provides the model mid-level sentence abstractions in the form of cue phrases during the generation process. This provides an interface for human users to guide the <a href="https://en.wikipedia.org/wiki/Storytelling">story generation</a>. We present two content-inducing approaches to effectively incorporate this additional information. Experimental results from both automatic and human evaluations show that these <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> produce more topically coherent and personalized stories compared to baseline methods.</abstract>
      <url hash="312edeb2">2020.aacl-main.59</url>
      <bibkey>brahman-etal-2020-cue</bibkey>
    </paper>
    <paper id="60">
      <title>Liputan6 : A Large-scale Indonesian Dataset for Text Summarization<fixed-case>I</fixed-case>ndonesian Dataset for Text Summarization</title>
      <author><first>Fajri</first><last>Koto</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>598–608</pages>
      <abstract>In this paper, we introduce a large-scale Indonesian summarization dataset. We harvest articles from Liputan6.com, an online news portal, and obtain 215,827 documentsummary pairs. We leverage pre-trained language models to develop benchmark extractive and abstractive summarization methods over the dataset with multilingual and monolingual BERT-based models. We include a thorough error analysis by examining machine-generated summaries that have low ROUGE scores, and expose both issues with ROUGE itself, as well as with extractive and abstractive summarization models.</abstract>
      <url hash="46725b36">2020.aacl-main.60</url>
      <bibkey>koto-etal-2020-liputan6</bibkey>
      <pwccode url="https://github.com/fajri91/sum_liputan6" additional="false">fajri91/sum_liputan6</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/liputan6">Liputan6</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/indosum">IndoSum</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lcsts">LCSTS</pwcdataset>
    </paper>
    <paper id="61">
      <title>Generating Sports News from Live Commentary : A Chinese Dataset for Sports Game Summarization<fixed-case>C</fixed-case>hinese Dataset for Sports Game Summarization</title>
      <author><first>Kuan-Hao</first><last>Huang</last></author>
      <author><first>Chen</first><last>Li</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>609–615</pages>
      <abstract>Sports game summarization focuses on generating news articles from <a href="https://en.wikipedia.org/wiki/Sports_commentator">live commentaries</a>. Unlike traditional summarization tasks, the source documents and the target summaries for sports game summarization tasks are written in quite different writing styles. In addition, live commentaries usually contain many <a href="https://en.wikipedia.org/wiki/Named_entity">named entities</a>, which makes summarizing sports games precisely very challenging. To deeply study this task, we present SportsSum, a Chinese sports game summarization dataset which contains 5,428 soccer games of live commentaries and the corresponding <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a>. Additionally, we propose a two-step summarization model consisting of a selector and a rewriter for SportsSum. To evaluate the correctness of generated sports summaries, we design two novel score metrics : name matching score and event matching score. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performs better than other summarization baselines on ROUGE scores as well as the two designed scores.</abstract>
      <url hash="8c935819">2020.aacl-main.61</url>
      <bibkey>huang-etal-2020-generating</bibkey>
      <pwccode url="https://github.com/ej0cl6/sportssum" additional="false">ej0cl6/sportssum</pwccode>
    </paper>
    <paper id="63">
      <title>Improving <a href="https://en.wikipedia.org/wiki/Context_model">Context Modeling</a> in Neural Topic Segmentation</title>
      <author><first>Linzi</first><last>Xing</last></author>
      <author><first>Brad</first><last>Hackinen</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <author><first>Francesco</first><last>Trebbi</last></author>
      <pages>626–636</pages>
      <abstract>Topic segmentation is critical in key NLP tasks and recent works favor highly effective neural supervised approaches. However, current neural solutions are arguably limited in how they model context. In this paper, we enhance a segmenter based on a hierarchical attention BiLSTM network to better model context, by adding a coherence-related auxiliary task and restricted self-attention. Our optimized segmenter outperforms SOTA approaches when trained and tested on three datasets. We also the robustness of our proposed <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> in domain transfer setting by training a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> on a large-scale dataset and testing it on four challenging real-world benchmarks. Furthermore, we apply our proposed <a href="https://en.wikipedia.org/wiki/Strategy">strategy</a> to two other languages (German and Chinese), and show its effectiveness in multilingual scenarios.</abstract>
      <url hash="d93ad209">2020.aacl-main.63</url>
      <bibkey>xing-etal-2020-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisection">WikiSection</pwcdataset>
    </paper>
    <paper id="66">
      <title>Event Coreference Resolution with Non-Local Information</title>
      <author><first>Jing</first><last>Lu</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>653–663</pages>
      <abstract>We present two extensions to a state-of-theart joint model for event coreference resolution, which involve incorporating (1) a supervised topic model for improving trigger detection by providing global context, and (2) a preprocessing module that seeks to improve event coreference by discarding unlikely candidate antecedents of an event mention using discourse contexts computed based on salient entities. The resulting <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> yields the best results reported to date on the KBP 2017 English and Chinese datasets.</abstract>
      <url hash="dfadac5c">2020.aacl-main.66</url>
      <bibkey>lu-ng-2020-event</bibkey>
    </paper>
    <paper id="68">
      <title>Asking Crowdworkers to Write Entailment Examples : The Best of Bad Options<fixed-case>A</fixed-case>sking <fixed-case>C</fixed-case>rowdworkers to <fixed-case>W</fixed-case>rite <fixed-case>E</fixed-case>ntailment <fixed-case>E</fixed-case>xamples: <fixed-case>T</fixed-case>he <fixed-case>B</fixed-case>est of <fixed-case>B</fixed-case>ad Options</title>
      <author><first>Clara</first><last>Vania</last></author>
      <author><first>Ruijie</first><last>Chen</last></author>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <pages>672–686</pages>
      <abstract>Large-scale natural language inference (NLI) datasets such as SNLI or MNLI have been created by asking crowdworkers to read a premise and write three new hypotheses, one for each possible semantic relationships (entailment, contradiction, and neutral). While this <a href="https://en.wikipedia.org/wiki/Communication_protocol">protocol</a> has been used to create useful benchmark data, it remains unclear whether the writing-based annotation protocol is optimal for any purpose, since it has not been evaluated directly. Furthermore, there is ample evidence that crowdworker writing can introduce artifacts in the data. We investigate two alternative <a href="https://en.wikipedia.org/wiki/Communication_protocol">protocols</a> which automatically create candidate (premise, hypothesis) pairs for annotators to label. Using these protocols and a writing-based baseline, we collect several new English NLI datasets of over 3k examples each, each using a fixed amount of annotator time, but a varying number of examples to fit that time budget. Our experiments on NLI and <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> show negative results : None of the alternative protocols outperforms the baseline in evaluations of <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a> within NLI or on transfer to outside target tasks. We conclude that crowdworker writing still the best known option for entailment data, highlighting the need for further data collection work to focus on improving writing-based annotation processes.</abstract>
      <url hash="310210a4">2020.aacl-main.68</url>
      <bibkey>vania-etal-2020-asking</bibkey>
      <pwccode url="https://github.com/nyu-mll/semi-automatic-nli" additional="false">nyu-mll/semi-automatic-nli</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="70">
      <title>Answering Product-related Questions with Heterogeneous Information</title>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Qian</first><last>Yu</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>696–705</pages>
      <abstract>Providing <a href="https://en.wikipedia.org/wiki/Instant_messaging">instant response</a> for product-related questions in E-commerce question answering platforms can greatly improve users’ online shopping experience. However, existing product question answering (PQA) methods only consider a single information source such as user reviews and/or require large amounts of labeled data. In this paper, we propose a novel framework to tackle the PQA task via exploiting heterogeneous information including natural language text and attribute-value pairs from two information sources of the concerned product, namely product details and user reviews. A heterogeneous information encoding component is then designed for obtaining unified representations of information with different formats. The sources of the candidate snippets are also incorporated when measuring the question-snippet relevance. Moreover, the <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> is trained with a specifically designed weak supervision paradigm making use of available answers in the training phase. Experiments on a real-world dataset show that our proposed <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> achieves superior performance over state-of-the-art models.</abstract>
      <url hash="5f93d8d0">2020.aacl-main.70</url>
      <bibkey>zhang-etal-2020-answering</bibkey>
    </paper>
    <paper id="73">
      <title>Multi-view Classification Model for Knowledge Graph Completion</title>
      <author><first>Wenbin</first><last>Jiang</last></author>
      <author><first>Mengfei</first><last>Guo</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Ying</first><last>Li</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <author><first>Yong</first><last>Zhu</last></author>
      <pages>726–734</pages>
      <abstract>Most previous work on knowledge graph completion conducted single-view prediction or calculation for candidate triple evaluation, based only on the content information of the candidate triples. This paper describes a novel multi-view classification model for knowledge graph completion, where multiple classification views are performed based on both content and context information for candidate triple evaluation. Each classification view evaluates the validity of a candidate triple from a specific viewpoint, based on the content information inside the candidate triple and the context information nearby the triple. These classification views are implemented by a unified neural network and the classification predictions are weightedly integrated to obtain the final evaluation. Experiments show that, the multi-view model brings very significant improvements over previous methods, and achieves the new state-of-the-art on two representative datasets. We believe that, the flexibility and the scalability of the multi-view classification model facilitates the introduction of additional information and resources for better performance.</abstract>
      <url hash="1427616c">2020.aacl-main.73</url>
      <bibkey>jiang-etal-2020-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="77">
      <title>ExpanRL : Hierarchical Reinforcement Learning for Course Concept Expansion in MOOCs<fixed-case>E</fixed-case>xpan<fixed-case>RL</fixed-case>: Hierarchical Reinforcement Learning for Course Concept Expansion in <fixed-case>MOOC</fixed-case>s</title>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Chenyu</first><last>Wang</last></author>
      <author><first>Gan</first><last>Luo</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Jie</first><last>Tang</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <pages>770–780</pages>
      <abstract>Within the prosperity of Massive Open Online Courses (MOOCs), the education applications that automatically provide extracurricular knowledge for MOOC users become rising research topics. However, MOOC courses’ diversity and rapid updates make it more challenging to find suitable new knowledge for students. In this paper, we present ExpanRL, an end-to-end hierarchical reinforcement learning (HRL) model for concept expansion in <a href="https://en.wikipedia.org/wiki/Massive_open_online_course">MOOCs</a>. Employing a two-level HRL mechanism of seed selection and concept expansion, ExpanRL is more feasible to adjust the expansion strategy to find new concepts based on the students’ feedback on expansion results. Our experiments on nine novel datasets from real MOOCs show that ExpanRL achieves significant improvements over existing methods and maintain competitive performance under different settings.</abstract>
      <url hash="a180d3b0">2020.aacl-main.77</url>
      <bibkey>yu-etal-2020-expanrl</bibkey>
    </paper>
    <paper id="89">
      <title>You May Like This Hotel Because... : Identifying Evidence for Explainable Recommendations</title>
      <author><first>Shin</first><last>Kanouchi</last></author>
      <author><first>Masato</first><last>Neishi</last></author>
      <author><first>Yuta</first><last>Hayashibe</last></author>
      <author><first>Hiroki</first><last>Ouchi</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>890–899</pages>
      <abstract>Explainable recommendation is a good way to improve user satisfaction. However, explainable recommendation in <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a> is challenging since it has to handle <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> as both input and output. To tackle the challenge, this paper proposes a novel and practical task to explain evidences in recommending hotels given vague requests expressed freely in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>. We decompose the process into two subtasks on hotel reviews : Evidence Identification and Evidence Explanation. The former predicts whether or not a sentence contains evidence that expresses why a given request is satisfied. The latter generates a <a href="https://en.wikipedia.org/wiki/Sentence_(law)">recommendation sentence</a> given a request and an <a href="https://en.wikipedia.org/wiki/Sentence_(law)">evidence sentence</a>. In order to address these subtasks, we build an Evidence-based Explanation dataset, which is the largest <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for explaining evidences in recommending hotels for vague requests. The experimental results demonstrate that the BERT model can find evidence sentences with respect to various vague requests and that the LSTM-based model can generate recommendation sentences.</abstract>
      <url hash="da69c7ae">2020.aacl-main.89</url>
      <bibkey>kanouchi-etal-2020-may</bibkey>
    </paper>
    <paper id="90">
      <title>A Unified Framework for Multilingual and Code-Mixed Visual Question Answering</title>
      <author><first>Deepak</first><last>Gupta</last></author>
      <author><first>Pabitra</first><last>Lenka</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>900–913</pages>
      <abstract>In this paper, we propose an effective deep learning framework for multilingual and code- mixed visual question answering. The pro- posed model is capable of predicting answers from the questions in Hindi, English or Code- mixed (Hinglish : Hindi-English) languages. The majority of the existing techniques on Vi- sual Question Answering (VQA) focus on En- glish questions only. However, many applica- tions such as <a href="https://en.wikipedia.org/wiki/Medical_imaging">medical imaging</a>, <a href="https://en.wikipedia.org/wiki/Tourism">tourism</a>, visual assistants require a multilinguality-enabled module for their widespread usages. As there is no available dataset in English-Hindi VQA, we firstly create Hindi and Code-mixed VQA datasets by exploiting the linguistic properties of these languages. We propose a robust tech- nique capable of handling the multilingual and code-mixed question to provide the answer against the visual information (image). To better encode the multilingual and code-mixed questions, we introduce a hierarchy of shared layers. We control the behaviour of these shared layers by an attention-based soft layer sharing mechanism, which learns how shared layers are applied in different ways for the dif- ferent languages of the question. Further, our model uses bi-linear attention with a residual connection to fuse the language and image fea- tures. We perform extensive evaluation and ablation studies for English, Hindi and Code- mixed VQA. The evaluation shows that the proposed multilingual model achieves state-of- the-art performance in all these settings.</abstract>
      <url hash="55f2bbad">2020.aacl-main.90</url>
      <bibkey>gupta-etal-2020-unified</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mcvqa">MCVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="92">
      <title>Measuring What Counts : The Case of Rumour Stance Classification</title>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Diego</first><last>Silva</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <pages>925–932</pages>
      <abstract>Stance classification can be a powerful tool for understanding whether and which users believe in <a href="https://en.wikipedia.org/wiki/Rumor">online rumours</a>. The task aims to automatically predict the stance of replies towards a given <a href="https://en.wikipedia.org/wiki/Rumor">rumour</a>, namely support, deny, question, or comment. Numerous methods have been proposed and their performance compared in the RumourEval shared tasks in 2017 and 2019. Results demonstrated that this is a challenging problem since naturally occurring rumour stance data is highly imbalanced. This paper specifically questions the evaluation metrics used in these shared <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. We re-evaluate the systems submitted to the two RumourEval tasks and show that the two widely adopted metrics   <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and macro-F1   are not robust for the four-class imbalanced task of rumour stance classification, as they wrongly favour systems with highly skewed accuracy towards the majority class. To overcome this problem, we propose new evaluation metrics for rumour stance detection. These are not only robust to imbalanced data but also score higher systems that are capable of recognising the two most informative minority classes (support and deny).</abstract>
      <url hash="551d0d20">2020.aacl-main.92</url>
      <bibkey>scarton-etal-2020-measuring</bibkey>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop</booktitle>
      <editor><first>Boaz</first><last>Shmueli</last></editor>
      <editor><first>Yin Jou</first><last>Huang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="3ddc0644">2020.aacl-srw.0</url>
      <bibkey>aacl-2020-asia-pacific</bibkey>
    </frontmatter>
    <paper id="5">
      <title>Automatic Classification of Students on Twitter Using Simple Profile Information<fixed-case>T</fixed-case>witter Using Simple Profile Information</title>
      <author><first>Lili-Michal</first><last>Wilson</last></author>
      <author><first>Christopher</first><last>Wun</last></author>
      <pages>30–36</pages>
      <abstract>Obtaining social media demographic information using <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> is important for efficient computational social science research. Automatic age classification has been accomplished with relative success and allows for the study of youth populations, but student classificationdetermining which users are currently attending an academic institutionhas not been thoroughly studied. Previous work (He et al., 2016) proposes a model which utilizes 3 tweet-content features to classify users as students or non-students. This <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 84 %, but is restrictive and time intensive because it requires accessing and processing many user tweets. In this study, we propose <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification models</a> which use 7 <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">numerical features</a> and 10 <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">text-based features</a> drawn from simple profile information. These profile-based features allow for faster, more accessible data collection and enable the classification of users without needing access to their tweets. Compared to previous <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> identify students with greater <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> ; our best model obtains an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 88.1 % and an F1 score of.704. This improved student identification tool has the potential to facilitate research on topics ranging from <a href="https://en.wikipedia.org/wiki/Professional_network">professional networking</a> to the impact of <a href="https://en.wikipedia.org/wiki/Education">education</a> on Twitter behaviors.</abstract>
      <url hash="e37d868b">2020.aacl-srw.5</url>
      <bibkey>wilson-wun-2020-automatic</bibkey>
    </paper>
    <paper id="16">
      <title>A Review of Cross-Domain Text-to-SQL Models<fixed-case>SQL</fixed-case> Models</title>
      <author><first>Yujian</first><last>Gan</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <author><first>John R.</first><last>Woodward</last></author>
      <pages>108–115</pages>
      <abstract>WikiSQL and Spider, the large-scale cross-domain text-to-SQL datasets, have attracted much attention from the research community. The leaderboards of WikiSQL and Spider show that many researchers propose their models trying to solve the text-to-SQL problem. This paper first divides the top <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> in these two leaderboards into two paradigms. We then present details not mentioned in their original paper by evaluating the key components, including schema linking, pretrained word embeddings, and reasoning assistance modules. Based on the analysis of these models, we want to promote understanding of the text-to-SQL field and find out some interesting future works, for example, it is worth studying the text-to-SQL problem in an environment where it is more challenging to build schema linking and also worth studying combing the advantage of each model toward text-to-SQL.</abstract>
      <url hash="88fe2201">2020.aacl-srw.16</url>
      <bibkey>gan-etal-2020-review</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="20">
      <title>Exploring Statistical and Neural Models for Noun Ellipsis Detection and Resolution in English<fixed-case>E</fixed-case>nglish</title>
      <author><first>Payal</first><last>Khullar</last></author>
      <pages>139–145</pages>
      <abstract>Computational approaches to noun ellipsis resolution has been sparse, with only a naive rule-based approach that uses syntactic feature constraints for marking noun ellipsis licensors and selecting their antecedents. In this paper, we further the ellipsis research by exploring several statistical and neural models for both the subtasks involved in the ellipsis resolution process and addressing the representation and contribution of manual features proposed in previous research. Using the best performing <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>, we build an end-to-end supervised Machine Learning (ML) framework for this task that improves the existing <a href="https://en.wikipedia.org/wiki/F-number">F1 score</a> by 16.55 % for the <a href="https://en.wikipedia.org/wiki/Detection">detection</a> and 14.97 % for the resolution subtask. Our experiments demonstrate robust scores through pretrained BERT (Bidirectional Encoder Representations from Transformers) embeddings for word representation, and more so the importance of manual features once again highlighting the syntactic and semantic characteristics of the ellipsis phenomenon. For the classification decision, we notice that a simple Multilayar Perceptron (MLP) works well for the detection of ellipsis ; however, Recurrent Neural Networks (RNN) are a better choice for the much harder resolution step.</abstract>
      <url hash="d6cea3e7">2020.aacl-srw.20</url>
      <bibkey>khullar-2020-exploring</bibkey>
    </paper>
    <paper id="22">
      <title>Text Simplification with <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a> Using <a href="https://en.wikipedia.org/wiki/Supervised_learning">Supervised Rewards</a> on <a href="https://en.wikipedia.org/wiki/Grammaticality">Grammaticality</a>, Meaning Preservation, and <a href="https://en.wikipedia.org/wiki/Simplicity">Simplicity</a></title>
      <author><first>Akifumi</first><last>Nakamachi</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Yuki</first><last>Arase</last></author>
      <pages>153–159</pages>
      <abstract>We optimize rewards of <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> in <a href="https://en.wikipedia.org/wiki/Text_simplification">text simplification</a> using <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> that are highly correlated with human-perspectives. To address problems of <a href="https://en.wikipedia.org/wiki/Exposure_bias">exposure bias</a> and loss-evaluation mismatch, text-to-text generation tasks employ <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> that rewards task-specific metrics. Previous studies in <a href="https://en.wikipedia.org/wiki/Text_simplification">text simplification</a> employ the weighted sum of sub-rewards from three perspectives : <a href="https://en.wikipedia.org/wiki/Grammaticality">grammaticality</a>, meaning preservation, and <a href="https://en.wikipedia.org/wiki/Simplicity">simplicity</a>. However, the previous rewards do not align with <a href="https://en.wikipedia.org/wiki/Human_psychology">human-perspectives</a> for these <a href="https://en.wikipedia.org/wiki/Point_of_view_(philosophy)">perspectives</a>. In this study, we propose to use BERT regressors fine-tuned for <a href="https://en.wikipedia.org/wiki/Grammaticality">grammaticality</a>, meaning preservation, and <a href="https://en.wikipedia.org/wiki/Simplicity">simplicity</a> as reward estimators to achieve text simplification conforming to human-perspectives. Experimental results show that <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> with our <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">rewards</a> balances meaning preservation and <a href="https://en.wikipedia.org/wiki/Simplicity">simplicity</a>. Additionally, human evaluation confirmed that simplified texts by our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> are preferred by humans compared to previous studies.</abstract>
      <url hash="ecbdd029">2020.aacl-srw.22</url>
      <bibkey>nakamachi-etal-2020-text</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="23">
      <title>Label Representations in Modeling Classification as Text Generation</title>
      <author><first>Xinyi</first><last>Chen</last></author>
      <author><first>Jingxian</first><last>Xu</last></author>
      <author><first>Alex</first><last>Wang</last></author>
      <pages>160–164</pages>
      <abstract>Several recent state-of-the-art transfer learning methods model classification tasks as text generation, where labels are represented as strings for the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> to generate. We investigate the effect that the choice of strings used to represent labels has on how effectively the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> learns the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. For four standard text classification tasks, we design a diverse set of possible <a href="https://en.wikipedia.org/wiki/String_(computer_science)">string representations</a> for labels, ranging from canonical label definitions to random strings. We experiment with T5 on these tasks, varying the label representations as well as the amount of training data. We find that, in the low data setting, label representation impacts task performance on some tasks, with task-related labels being most effective, but fails to have an impact on others. In the full data setting, our results are largely negative : Different label representations do not affect overall task performance.</abstract>
      <url hash="7729d846">2020.aacl-srw.23</url>
      <bibkey>chen-etal-2020-label</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    </volume>
  <volume id="demo" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations</booktitle>
      <editor><first>Derek</first><last>Wong</last></editor>
      <editor><first>Douwe</first><last>Kiela</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="4f1457ff">2020.aacl-demo.0</url>
      <bibkey>aacl-2020-asia-pacific-chapter</bibkey>
    </frontmatter>
    <paper id="2">
      <title>AutoNLU : An On-demand Cloud-based Natural Language Understanding System for Enterprises<fixed-case>A</fixed-case>uto<fixed-case>NLU</fixed-case>: An On-demand Cloud-based Natural Language Understanding System for Enterprises</title>
      <author><first>Nham</first><last>Le</last></author>
      <author><first>Tuan</first><last>Lai</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Doo Soon</first><last>Kim</last></author>
      <pages>8–13</pages>
      <abstract>With the renaissance of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>, <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> have achieved promising results on many natural language understanding (NLU) tasks. Even though the source codes of many neural network models are publicly available, there is still a large gap from open-sourced models to solving real-world problems in enterprises. Therefore, to fill this gap, we introduce AutoNLU, an on-demand cloud-based system with an easy-to-use interface that covers all common use-cases and steps in developing an NLU model. AutoNLU has supported many product teams within Adobe with different use-cases and datasets, quickly delivering them working models. To demonstrate the effectiveness of AutoNLU, we present two case studies. i) We build a practical NLU model for handling various image-editing requests in <a href="https://en.wikipedia.org/wiki/Adobe_Photoshop">Photoshop</a>. ii) We build powerful keyphrase extraction models that achieve state-of-the-art results on two public benchmarks. In both cases, end users only need to write a small amount of code to convert their datasets into a common format used by AutoNLU.</abstract>
      <url hash="e81242b0">2020.aacl-demo.2</url>
      <bibkey>le-etal-2020-autonlu</bibkey>
    </paper>
    <paper id="3">
      <title>ISA : An Intelligent Shopping Assistant<fixed-case>ISA</fixed-case>: An Intelligent Shopping Assistant</title>
      <author><first>Tuan</first><last>Lai</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Nedim</first><last>Lipka</last></author>
      <pages>14–19</pages>
      <abstract>Despite the growth of <a href="https://en.wikipedia.org/wiki/E-commerce">e-commerce</a>, brick-and-mortar stores are still the preferred destinations for many people. In this paper, we present ISA, a mobile-based intelligent shopping assistant that is designed to improve shopping experience in physical stores. ISA assists users by leveraging advanced techniques in <a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a>, <a href="https://en.wikipedia.org/wiki/Speech_processing">speech processing</a>, and <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. An in-store user only needs to take a picture or scan the barcode of the product of interest, and then the user can talk to the assistant about the product. The assistant can also guide the user through the purchase process or recommend other similar products to the user. We take a data-driven approach in building the engines of <a href="https://en.wikipedia.org/wiki/Industry_Standard_Architecture">ISA</a>’s natural language processing component, and the <a href="https://en.wikipedia.org/wiki/Engine">engines</a> achieve good performance.</abstract>
      <url hash="3e17504c">2020.aacl-demo.3</url>
      <bibkey>lai-etal-2020-isa</bibkey>
    </paper>
    </volume>
</collection>