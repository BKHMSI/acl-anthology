<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.mrqa">
  <volume id="1" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</booktitle>
      <editor><first>Adam</first><last>Fisch</last></editor>
      <editor><first>Alon</first><last>Talmor</last></editor>
      <editor><first>Danqi</first><last>Chen</last></editor>
      <editor><first>Eunsol</first><last>Choi</last></editor>
      <editor><first>Minjoon</first><last>Seo</last></editor>
      <editor><first>Patrick</first><last>Lewis</last></editor>
      <editor><first>Robin</first><last>Jia</last></editor>
      <editor><first>Sewon</first><last>Min</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="13500ebb">2021.mrqa-1.0</url>
      <bibkey>mrqa-2021-machine</bibkey>
    </frontmatter>
    <paper id="1">
      <title>MFAQ : a Multilingual FAQ Dataset<fixed-case>MFAQ</fixed-case>: a Multilingual <fixed-case>FAQ</fixed-case> Dataset</title>
      <author><first>Maxime</first><last>De Bruyn</last></author>
      <author><first>Ehsan</first><last>Lotfi</last></author>
      <author><first>Jeska</first><last>Buhmann</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>1–13</pages>
      <abstract>In this paper, we present the first multilingual FAQ dataset publicly available. We collected around 6 M FAQ pairs from the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a>, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges : duplication of content and uneven distribution of topics. We adopt a similar setup as Dense Passage Retrieval (DPR) and test various bi-encoders on this dataset. Our experiments reveal that a multilingual model based on XLM-RoBERTa achieves the best results, except for <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Lower resources languages seem to learn from one another as a multilingual model achieves a higher MRR than language-specific ones. Our qualitative analysis reveals the brittleness of the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> on simple word changes. We publicly release our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>, and training script.</abstract>
      <url hash="e9c21c43">2021.mrqa-1.1</url>
      <bibkey>de-bruyn-etal-2021-mfaq</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.1</doi>
      <pwccode url="https://github.com/clips/mfaq" additional="false">clips/mfaq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mfaq">MFAQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paq">PAQ</pwcdataset>
    </paper>
    <paper id="6">
      <title>Can Question Generation Debias <a href="https://en.wikipedia.org/wiki/Question_answering">Question Answering Models</a>? A Case Study on QuestionContext Lexical Overlap</title>
      <author><first>Kazutoshi</first><last>Shinoda</last></author>
      <author><first>Saku</first><last>Sugawara</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>63–72</pages>
      <abstract>Question answering (QA) models for <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> have been demonstrated to exploit unintended dataset biases such as questioncontext lexical overlap. This hinders QA models from generalizing to <a href="https://en.wikipedia.org/wiki/Underrepresented_group">under-represented samples</a> such as questions with low lexical overlap. Question generation (QG), a method for augmenting QA datasets, can be a solution for such performance degradation if QG can properly debias QA datasets. However, we discover that recent neural QG models are biased towards generating questions with high lexical overlap, which can amplify the dataset bias. Moreover, our analysis reveals that data augmentation with these QG models frequently impairs the performance on questions with low <a href="https://en.wikipedia.org/wiki/Lexical_overlap">lexical overlap</a>, while improving that on questions with high <a href="https://en.wikipedia.org/wiki/Lexical_overlap">lexical overlap</a>. To address this problem, we use a synonym replacement-based approach to augment questions with low lexical overlap. We demonstrate that the proposed data augmentation approach is simple yet effective to mitigate the degradation problem with only 70k synthetic examples.</abstract>
      <url hash="361d679b">2021.mrqa-1.6</url>
      <bibkey>shinoda-etal-2021-question</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.6</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="9">
      <title>Eliciting Bias in Question Answering Models through Ambiguity</title>
      <author><first>Andrew</first><last>Mao</last></author>
      <author><first>Naveen</first><last>Raman</last></author>
      <author><first>Matthew</first><last>Shu</last></author>
      <author><first>Eric</first><last>Li</last></author>
      <author><first>Franklin</first><last>Yang</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>92–99</pages>
      <abstract>Question answering (QA) models use retriever and reader systems to answer questions. Reliance on training data by <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA systems</a> can amplify or reflect inequity through their responses. Many QA models, such as those for the SQuAD dataset, are trained and tested on a subset of Wikipedia articles which encode their own biases and also reproduce real-world inequality. Understanding how training data affects bias in <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA systems</a> can inform methods to mitigate <a href="https://en.wikipedia.org/wiki/Equity_(economics)">inequity</a>. We develop two sets of questions for closed and open domain questions respectively, which use ambiguous questions to probe QA models for <a href="https://en.wikipedia.org/wiki/Bias">bias</a>. We feed three deep-learning-based QA systems with our question sets and evaluate responses for <a href="https://en.wikipedia.org/wiki/Bias">bias</a> via the <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>. Using our metrics, we find that open-domain QA models amplify biases more than their closed-domain counterparts and propose that biases in the retriever surface more readily due to greater <a href="https://en.wikipedia.org/wiki/Freedom_of_choice">freedom of choice</a>.</abstract>
      <url hash="66250172">2021.mrqa-1.9</url>
      <bibkey>mao-etal-2021-eliciting</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.9</doi>
      <pwccode url="https://github.com/axz5fy3e6fq07q13/emnlp_bias" additional="false">axz5fy3e6fq07q13/emnlp_bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    <title_ar>استخلاص التحيز في الإجابة على الأسئلة من خلال الغموض</title_ar>
      <title_pt>Elicitando o viés em modelos de resposta a perguntas por meio da ambiguidade</title_pt>
      <title_es>Provocar sesgos en los modelos de respuesta a preguntas mediante la ambigüedad</title_es>
      <title_ja>あいまいさを通じて質問回答モデルに偏見を引き起こす</title_ja>
      <title_zh>因模糊性于问答模引偏差</title_zh>
      <title_hi>अस्पष्टता के माध्यम से प्रश्न का उत्तर देने वाले मॉडल में पूर्वाग्रह को प्राप्त करना</title_hi>
      <title_ga>Laofacht á lorg i Múnlaí Freagartha Ceisteanna trí Athbhrí</title_ga>
      <title_ka>კითხვის გასაგების მოდელების გამოყენება</title_ka>
      <title_hu>A kérdésekre vonatkozó modellek kétségtelen megválaszolása</title_hu>
      <title_el>Εξάλειψη προκαταλήψεων στα μοντέλα απάντησης ερωτήσεων μέσω αμφιβολίας</title_el>
      <title_kk>Сұрақ жауап беру үлгілерінде шектеу</title_kk>
      <title_it>Eliminare Bias nei modelli di risposta alle domande attraverso l'ambiguità</title_it>
      <title_ml>ചോദ്യത്തിന്റെ ഉത്തരം മോഡലുകളില്‍ ബിയാസുകള്‍ എലിയാസ് ചെയ്യുന്നു</title_ml>
      <title_lt>Eliciting Bias in Question Answering Models through Ambiguity</title_lt>
      <title_ms>Memilih Bias dalam Model Jawab soalan melalui Ambiguity</title_ms>
      <title_mn>Биас асуулт хариултын загваруудыг хадгалах</title_mn>
      <title_no>Gjennomsiktige oppgåver i spørsmålmodular</title_no>
      <title_mt>L-eliġibbiltà tal-ħsara fil-mudelli tat-tweġiba għall-mistoqsijiet permezz tal-Ambigurtà</title_mt>
      <title_ro>Eliberarea Bias în modelele de răspuns la întrebări prin ambiguitate</title_ro>
      <title_sr>Eliminiranje Bija u modelima odgovora na pitanje kroz ambicioznost</title_sr>
      <title_pl>Wyeliminowanie uprzedzeń w modelach odpowiadania na pytania poprzez niejasność</title_pl>
      <title_si>ප්‍රශ්න ප්‍රතික්‍රියාවට ප්‍රතික්‍රියාවට ප්‍රතික්‍රියාවට ප්‍රතික්‍රියාවක් නිර්මාණය</title_si>
      <title_so>Eliciting Bias in Question answering Models through Ambiguity</title_so>
      <title_sv>Eliminera missförhållanden i fråga Svarsmodeller genom Ambiguity</title_sv>
      <title_ta>கேள்வி பதில் மாதிரிகளில் பியாஸ்களை நீக்குகிறது அமைப்பு வழியாக</title_ta>
      <title_ur>سوال کے جواب دینے کے موڈل کے ذریعہ دوسری باتوں کو ٹال دیا جاتا ہے</title_ur>
      <title_mk>Одбивање на непријатности во моделите за одговори на прашања преку амбигија</title_mk>
      <title_vi>KCharselect unicode block name</title_vi>
      <title_uz>Name</title_uz>
      <title_bg>Елиминиране на предразсъдъци в моделите за отговор на въпроси чрез амбициозност</title_bg>
      <title_da>Eliminering af bias i spørgsmål besvarelse modeller gennem Ambiguity</title_da>
      <title_hr>Eliminiranje Bia u modelima odgovora na pitanje kroz obilaznost</title_hr>
      <title_nl>Het elimineren van vooroordelen in vragenbeantwoordingsmodellen door dubbelzinnigheid</title_nl>
      <title_id>Memiliki gangguan dalam Model Jawab Pertanyaan melalui Ambiguity</title_id>
      <title_de>Beseitigung von Vorurteilen bei Fragebeantworter Modelle durch Ambiguität</title_de>
      <title_ko>잘못된 뜻을 통해 문답 모델 중의 편견을 끌어내다</title_ko>
      <title_fa>پاک کردن بیها در مدل جواب سوال از طریق آزمایش</title_fa>
      <title_sw>Akiondoa Bias katika maswali ya maswali kupitia Ubalozi</title_sw>
      <title_tr>Sorag jogabaty nusgala hillerden çykar</title_tr>
      <title_sq>Duke hequr dëmet në modelet e përgjigjes për pyetje nëpërmjet ambigjitetit</title_sq>
      <title_af>Verwyder Bias in Vrag Antwoord Modelle deur Ambiguity</title_af>
      <title_hy>Eliciting Bias in Question Answering Models through Ambiguity</title_hy>
      <title_am>Bias in Question Answering Models through Ambiguity</title_am>
      <title_az>Söylə cavab vermək modelləri məhşərə olaraq</title_az>
      <title_bn>প্রশ্নের উত্তর প্রশ্ন মোডেলে বিয়াস তুলে ধরা হচ্ছে</title_bn>
      <title_bs>Eliminiranje Bija u modelima odgovora na pitanje kroz ambicioznost</title_bs>
      <title_ca>Eliminar els biais en els models de resposta a preguntes a través de l'ambigüitat</title_ca>
      <title_cs>Odstraňování předsudků v modelech zodpovědí na otázky prostřednictvím nejasnosti</title_cs>
      <title_et>Küsimustele vastamise mudelite eelarvamuste kõrvaldamine ambiguity kaudu</title_et>
      <title_fi>Kyselymallien ennakkoluulojen poistaminen epäselvyyden avulla</title_fi>
      <title_jv>echoH e l l o space w o r l d periodHelloworldHello world</title_jv>
      <title_sk>Odpravljanje pristranskosti pri modelih odgovarjanja na vprašanja prek ambiguitete</title_sk>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_he>ניתן להוציא סבלנות במודלים לענות על שאלות באמצעות אמביגיות</title_he>
      <title_bo>གནས་ཚུལ་གྱི་གནས་ཚུལ་ནང་གི་བྲི་ཚིག་Eliciting Bias in Question Answering Models through Ambiguity</title_bo>
      <abstract_ar>تستخدم نماذج الإجابة على الأسئلة (QA) أنظمة المسترد والقارئ للإجابة على الأسئلة. الاعتماد على بيانات التدريب بواسطة أنظمة ضمان الجودة يمكن أن يضخم أو يعكس عدم المساواة من خلال استجاباتهم. يتم تدريب العديد من نماذج ضمان الجودة ، مثل تلك الخاصة بمجموعة بيانات SQuAD ، واختبارها على مجموعة فرعية من مقالات ويكيبيديا التي تشفر تحيزاتها وتعيد إنتاج عدم المساواة في العالم الحقيقي. يمكن أن يساعد فهم كيفية تأثير بيانات التدريب على التحيز في أنظمة ضمان الجودة على توجيه طرق التخفيف من عدم المساواة. نقوم بتطوير مجموعتين من الأسئلة لأسئلة المجال المغلق والمفتوح على التوالي ، والتي تستخدم أسئلة غامضة لفحص نماذج ضمان الجودة للتحيز. نقوم بتغذية ثلاثة أنظمة لضمان الجودة قائمة على التعلم العميق بمجموعات الأسئلة الخاصة بنا وتقييم الاستجابات للتحيز عبر المقاييس. باستخدام مقاييسنا ، وجدنا أن نماذج ضمان الجودة ذات المجال المفتوح تضخم التحيزات أكثر من نظيراتها في المجال المغلق وتقترح أن التحيزات في سطح المسترد أكثر سهولة بسبب حرية أكبر في الاختيار.</abstract_ar>
      <abstract_pt>Os modelos de resposta a perguntas (QA) usam sistemas de recuperação e leitura para responder a perguntas. A confiança nos dados de treinamento dos sistemas de controle de qualidade pode amplificar ou refletir a desigualdade por meio de suas respostas. Muitos modelos de controle de qualidade, como os do conjunto de dados SQuAD, são treinados e testados em um subconjunto de artigos da Wikipedia que codificam seus próprios preconceitos e também reproduzem a desigualdade do mundo real. Compreender como os dados de treinamento afetam o viés em sistemas de controle de qualidade pode informar métodos para mitigar a desigualdade. Desenvolvemos dois conjuntos de perguntas para perguntas de domínio fechado e aberto, respectivamente, que usam perguntas ambíguas para sondar modelos de controle de qualidade quanto a vieses. Alimentamos três sistemas de controle de qualidade baseados em aprendizado profundo com nossos conjuntos de perguntas e avaliamos as respostas por viés por meio das métricas. Usando nossas métricas, descobrimos que os modelos de controle de qualidade de domínio aberto amplificam mais os vieses do que suas contrapartes de domínio fechado e propomos que os vieses no retriever apareçam mais prontamente devido à maior liberdade de escolha.</abstract_pt>
      <abstract_es>Los modelos de respuesta a preguntas (QA) utilizan sistemas de recuperación y lectura para responder preguntas. La confianza en los datos de capacitación por parte de los sistemas de control de calidad puede amplificar o reflejar la desigualdad a través de sus respuestas. Muchos modelos de control de calidad, como los del conjunto de datos sQuad, se entrenan y prueban en un subconjunto de artículos de Wikipedia que codifican sus propios sesgos y también reproducen la desigualdad del mundo real. Comprender cómo los datos de entrenamiento afectan el sesgo en los sistemas de control de calidad puede informar los métodos para mitigar la desigualdad. Desarrollamos dos conjuntos de preguntas para preguntas de dominio cerrado y abierto, respectivamente, que utilizan preguntas ambiguas para investigar los modelos de control de calidad en busca de sesgos. Alimentamos tres sistemas de control de calidad basados en el aprendizaje profundo con nuestros conjuntos de preguntas y evaluamos las respuestas para detectar sesgos a través de las métricas. Utilizando nuestras métricas, descubrimos que los modelos de control de calidad de dominio abierto amplifican los sesgos más que sus homólogos de dominio cerrado y proponemos que los sesgos en el recuperador surjan más fácilmente debido a una mayor libertad de elección.</abstract_es>
      <abstract_ja>質疑応答（ QA ）モデルは、リトリーバーとリーダーシステムを使用して質問に答えます。QAシステムによるトレーニングデータへの依存は、応答を通じて不公平性を増幅または反映する可能性があります。SQuADデータセットのような多くのQAモデルは、独自のバイアスを符号化し、現実世界の不平等も再現するウィキペディアの記事のサブセットで訓練され、テストされています。トレーニングデータがQAシステムのバイアスにどのように影響するかを理解することは、不公平を軽減する方法に役立ちます。私たちは、クローズドドメイン質問とオープンドメイン質問の2つの質問セットをそれぞれ開発しています。これらの質問は、曖昧な質問を使用して、バイアスについてQAモデルを調査します。私たちは、3つのディープラーニングベースのQAシステムに質問セットをフィードバックし、メトリクスを介してバイアスに対する応答を評価します。当社の指標を使用して、オープンドメインのQAモデルは、クローズドドメインの対応モデルよりもバイアスを増幅し、より自由な選択により、リトリーバー表面のバイアスがより容易に増幅することを提案しています。</abstract_ja>
      <abstract_zh>问答 (QA) 模用检索器与阅读器系统以对。 QA系统对培训数者,所以应大与不平也。 诸QA模形,如SQuAD数集者,皆于维基百科文子集上习试,编码己偏见,复现世界不等。 知训练之数 QA 偏差可以为缓不平之法。 各为封闭开放域开两组问,以模棱测QA差。 吾等用问集为三基于深学 QA 系统供反馈,并以指标评估应有差。 用我之指标,见开放域QA大于闭域对应物,而大择自由,检索器中之差易浮也。</abstract_zh>
      <abstract_hi>प्रश्न उत्तर (क्यूए) मॉडल प्रश्नों के उत्तर देने के लिए रिट्रीवर और रीडर सिस्टम का उपयोग करते हैं। क्यूए सिस्टम द्वारा प्रशिक्षण डेटा पर निर्भरता उनकी प्रतिक्रियाओं के माध्यम से असमानता को बढ़ा या प्रतिबिंबित कर सकती है। कई क्यूए मॉडल, जैसे कि SQuAD डेटासेट के लिए, विकिपीडिया लेखों के सबसेट पर प्रशिक्षित और परीक्षण किए जाते हैं जो अपने स्वयं के पूर्वाग्रहों को एन्कोड करते हैं और वास्तविक दुनिया की असमानता को भी पुन: पेश करते हैं। यह समझना कि प्रशिक्षण डेटा क्यूए सिस्टम में पूर्वाग्रह को कैसे प्रभावित करता है, असमानता को कम करने के तरीकों को सूचित कर सकता है। हम क्रमशः बंद और खुले डोमेन प्रश्नों के लिए प्रश्नों के दो सेट विकसित करते हैं, जो पूर्वाग्रह के लिए क्यूए मॉडल की जांच करने के लिए अस्पष्ट प्रश्नों का उपयोग करते हैं। हम अपने प्रश्न सेट के साथ तीन गहरी-सीखने-आधारित क्यूए प्रणालियों को खिलाते हैं और मीट्रिक के माध्यम से पूर्वाग्रह के लिए प्रतिक्रियाओं का मूल्यांकन करते हैं। हमारे मैट्रिक्स का उपयोग करते हुए, हम पाते हैं कि ओपन-डोमेन क्यूए मॉडल अपने बंद-डोमेन समकक्षों की तुलना में पूर्वाग्रहों को अधिक बढ़ाते हैं और प्रस्ताव करते हैं कि पसंद की अधिक स्वतंत्रता के कारण पुनर्प्राप्ति सतह में पूर्वाग्रह अधिक आसानी से।</abstract_hi>
      <abstract_ga>Úsáideann samhlacha freagartha ceisteanna (QA) córais aisghabhálaí agus léitheora chun ceisteanna a fhreagairt. Má bhíonn córais QA ag brath ar shonraí oiliúna, féadtar éagothroime a mhéadú nó a léiriú trína gcuid freagraí. Déantar go leor samhlacha QA, mar iad siúd do thacair sonraí SQuAD, a oiliúint agus a thástáil ar fho-thacar d'ailt Vicipéid a ionchódaíonn a gcuid laofachta féin agus a atáirgeann freisin éagothroime sa saol fíor. Má thuigtear conas a théann sonraí oiliúna i bhfeidhm ar laofacht i gcórais QA, féadtar eolas a chur ar mhodhanna chun éagothroime a mhaolú. Forbraímid dhá shraith ceisteanna do cheisteanna fearainn iata agus oscailte faoi seach, a úsáideann ceisteanna débhríoch chun samhlacha QA a fhiosrú le haghaidh laofachta. Cuirimid trí chóras QA domhainfhoghlama lenár dtacar ceisteanna agus measúnaítear freagraí maidir le claonadh tríd an méadracht. Agus ár méadracht á úsáid againn, feicimid go méadaíonn samhlacha QA fearainn oscailte laofachtaí níos mó ná a gcomhghleacaithe d’fhearann dúnta agus molann siad go mbeidh laofachtaí sa dromchla aisghabhála níos éasca mar gheall ar níos mó saoirse rogha.</abstract_ga>
      <abstract_ka>Name QA სისტემების შესახებ მონაცემების შესახებ შესაძლებელია გააკეთება ან გააკეთება განსხვავებას მათი განსხვავებებით. QA მოდელები, როგორც SQuAD მონაცემებისთვის მონაცემებისთვის, უკეთესოდიის მონაცემებისთვის სუბსეტიდან შემოწმება და შემოწმება, რომლებიც საკუთარი წინასწორებების კოდირებით და მ გავიგოთ, როგორ განათლების მონაცემები QA სისტემებში შეუძლიათ ინფორმაციო მეტისები განათლების განათლებისთვის. ჩვენ განვითარებთ ორი კითხვების კითხვები დახურებული და გახურებული დემომინის კითხვებისთვის, რომლებიც გამოყენებენ უცნობიერი კითხვები QA მოდელებისთვის წინასწორებ ჩვენ სამი ძალიან სწავლებული QA სისტემი ჩვენი კითხვის კონფიგურაციებისთვის გავუმუშავებთ და მეტრიკის გამოყენებისთვის გაუმუშავება. ჩვენი მეტრიკის გამოყენება, ჩვენ აღმოვაჩნეთ, რომ გახსნა დიომინული QA მოდელები უფრო მეტად გახსენებს, ვიდრე ჩვენი გახსენებული დიომინის კონტეპორტები და დავიწყებთ, რომ გახსენებელი საფუძ</abstract_ka>
      <abstract_hu>A kérdésre válaszoló modellek retriever és olvasó rendszereket használnak a kérdések megválaszolására. A minőségbiztosítási rendszerek képzési adataira való támaszkodás válaszaik révén fokozhatja vagy tükrözheti az igazságtalanságot. Számos minőségbiztosítási modellt, mint például az SQUAD adatkészletet, képeznek és tesztelnek a Wikipédia cikkek egy részhalmazán, amelyek kódolják saját előítéleteiket és reprodukálják a valós világbeli egyenlőtlenségeket. Annak megértése, hogy a képzési adatok hogyan befolyásolják a minőségbiztosítási rendszerekben előforduló elfogultságokat, segíthet az igazságtalanságok enyhítésére szolgáló módszereket. Két kérdéscsomagot dolgozunk ki zárt és nyílt domain kérdésekhez, amelyek kétértelmű kérdéseket használnak fel a minőségbiztosítási modellek vizsgálatára. Kérdéskészleteinkkel három mélytanulási alapú minőségbiztosítási rendszert táplálunk, és a mutatók segítségével értékeljük a válaszokat az elfogultságra. Metrikáink segítségével megállapítjuk, hogy a nyílt domain minőségbiztosítási modellek jobban erősítik az elfogultságokat, mint a zárt domain társaik, és javasoljuk, hogy a retriever felületén a nagyobb választási szabadság miatt könnyebben előforduljanak.</abstract_hu>
      <abstract_kk>Сұрақ жауап беру (QA) үлгілері сұрақтарды жауап беру үшін алу және оқу жүйелерін қолданады. QA жүйелерінің оқыту деректерінің қатынасы өзінің жауаптары арқылы бұл қатынасын көтеруге немесе қатынасыздығын көтеруге болады. SQuAD деректер қорларының көп QA үлгілері, мысалы, Wikipedia мақалаларының ішінде оқылған және тексеріледі. Олар өзінің өзінің қарсылығын кодтамасыз және сондай-ақ әлемдегі қасиеттерді қайта жаса QA жүйелерінде оқыту деректері қалай әсер ететінін түсіндіруге болады. Біз доменге жабылған және ашылған сұрақтар үшін екі сұрақ жасаймыз. Бұл QA үлгілерін бақылау үшін бұл сұрақтар қолданылады. Біз біздің сұрақ жиынымыз менен үш түсіндірілген QA жүйелерін метрикалық арқылы жауаптарды бағалаймыз. Метрикаларымызды қолдану үшін ашық домендық QA үлгілері өзінің жабылған домендық партнерінен артық көбейту үшін көбейту үшін көбейту үшін көбейту үшін қолданып, алу үшін таңдау мүмкіндіктерінің арты</abstract_kk>
      <abstract_it>I modelli di risposta alle domande (QA) utilizzano sistemi di recupero e lettura per rispondere alle domande. L'affidamento sui dati di formazione da parte dei sistemi QA può amplificare o riflettere l'iniquità attraverso le loro risposte. Molti modelli di QA, come quelli per il dataset SQUAD, sono addestrati e testati su un sottoinsieme di articoli di Wikipedia che codificano i propri pregiudizi e riproducono anche disuguaglianze nel mondo reale. Capire come i dati di formazione influiscono sui pregiudizi nei sistemi di QA può informare i metodi per mitigare l'iniquità. Sviluppiamo due serie di domande rispettivamente per domande a dominio chiuso e aperto, che utilizzano domande ambigue per sondare i modelli QA per il bias. Alimentamo tre sistemi di QA basati su deep learning con i nostri set di domande e valutiamo le risposte per il bias tramite le metriche. Utilizzando le nostre metriche, scopriamo che i modelli di QA a dominio aperto amplificano i bias più delle loro controparti a dominio chiuso e propongono che i bias nella superficie del retriever siano più facilmente a causa di una maggiore libertà di scelta.</abstract_it>
      <abstract_el>Τα μοντέλα απάντησης ερωτήσεων (QA) χρησιμοποιούν συστήματα ανάκτησης και ανάγνωσης για να απαντήσουν σε ερωτήσεις. Η εμπιστοσύνη στα δεδομένα κατάρτισης από τα συστήματα QS μπορεί να ενισχύσει ή να αντικατοπτρίζει την ανισότητα μέσω των απαντήσεών τους. Πολλά μοντέλα QA, όπως αυτά για το σύνολο δεδομένων SQuAD, εκπαιδεύονται και δοκιμάζονται σε ένα υποσύνολο άρθρων της Βικιπαίδειας που κωδικοποιούν τις δικές τους προκαταλήψεις και επίσης αναπαράγουν την πραγματική ανισότητα. Η κατανόηση του τρόπου με τον οποίο τα δεδομένα κατάρτισης επηρεάζουν την προκατάληψη στα συστήματα QS μπορεί να ενημερώσει τις μεθόδους μετριασμού της ανισότητας. Αναπτύσσουμε δύο σύνολα ερωτήσεων για κλειστές και ανοικτές ερωτήσεις αντίστοιχα, τα οποία χρησιμοποιούν ασάφειες ερωτήσεις για να εξετάσουν μοντέλα QA για προκατάληψη. Τροφοδοτούμε τρία συστήματα ποιοτικής αξιολόγησης βασισμένα σε βαθιά μάθηση με τα σύνολα ερωτήσεων μας και αξιολογούμε τις απαντήσεις για προκατάληψη μέσω των μετρήσεων. Χρησιμοποιώντας τις μετρήσεις μας, διαπιστώνουμε ότι τα μοντέλα QA ανοικτού τομέα ενισχύουν τις προκατάληψη περισσότερο από ό,τι τα ομόλογά τους κλειστού τομέα και προτείνουν ότι οι προκατάληψη στην επιφάνεια του ανακτητή είναι πιο εύκολα λόγω της μεγαλύτερης ελευθερίας επιλογής.</abstract_el>
      <abstract_mk>Question answering (QA) models use retriever and reader systems to answer questions.  Reliance on training data by QA systems can amplify or reflect inequity through their responses.  Многу QA модели, како што се оние за SQuAD податоците, се тренирани и тестирани на подгрупа на статии од Википедија кои ги кодираат своите предрасуди и исто така ја репродуктираат нееднаквоста во реалниот свет. Разбирање како податоците за обука влијаат на предрасудите во системите на QA може да ги информира методите за намалување на нееднаквоста. Развиваме два набора прашања за затворени и отворени прашања на домен, кои користат двогледни прашања за проверка на моделите на QA за пристрасност. Ние храниме три системи на длабоко учење на QA со нашите поставувања прашања и оценуваме одговори за предрасуди преку метриката. Користејќи ги нашите метрики, откриваме дека моделите на отворен домен QA ги зголемуваат предрасудите повеќе од нивните колеги на затворен домен и предложуваме предрасудите на површината на преземачот да бидат полесни поради поголемата слобода на избор.</abstract_mk>
      <abstract_lt>Klausimų atsakymo (QA) modeliai, atsakydami į klausimus, naudoja paieškos ir skaitymo sistemas. Pasikliauti QA sistemų mokymo duomenimis gali sustiprinti arba atspindėti nelygybę jų atsakymais. Daugelis QA modelių, pavyzdžiui, SQuAD duomenų rinkinio modelių, rengiami ir išbandomi Wikipedia straipsnių pogrupyje, kuriame koduojami jų nusikaltimai ir taip pat atkuriama realaus pasaulio nelygybė. Understanding how training data affects bias in QA systems can inform methods to mitigate inequity.  Mes parengiame du klausimų rinkinius atitinkamai uždarytiems ir atviriems domeniniams klausimams, kurie naudoja dviprasmiškus klausimus, kad ištirtų QA modelius, kad būtų galima nustatyti sąžiningumą. Mes naudojame tris giliai mokymosi pagrindu grindžiamas QA sistemas savo klausimų rinkiniais ir vertiname atsakymus į sąžiningumą taikant metrinius rodiklius. Naudodami mūsų metrinius rodiklius nustatome, kad atvirojo domeno QA modeliai labiau sustiprina šoninius rodiklius nei jų uždarojo domeno kolegos ir siūlo, kad šoniniai rodikliai atkūrimo paviršiuje būtų lengviau nukreipti dėl didesnės pasirinkimo laisvės.</abstract_lt>
      <abstract_ms>Model menjawab soalan (QA) menggunakan sistem pemulihan dan pembaca untuk menjawab soalan. Percayaan pada data latihan oleh sistem QA boleh memperkuat atau merefleksikan ketidakadilan melalui balasan mereka. Banyak model QA, seperti yang untuk set data SQuAD, dilatih dan diuji pada subset artikel Wikipedia yang mengekodkan biases mereka sendiri dan juga mengembalikan ketidaksamaan dunia nyata. Memahami bagaimana data latihan mempengaruhi bias dalam sistem QA boleh maklumkan kaedah untuk mengurangi ketidaksamaan. Kami mengembangkan dua set soalan untuk soalan domain tertutup dan terbuka secara berdasarkan, yang menggunakan soalan yang ambiguh untuk menguji model QA untuk bias. Kami memberi makan tiga sistem QA berasaskan belajar dalam dengan set soalan kami dan menilai jawapan untuk bias melalui metrik. Dengan menggunakan metrik kita, kita mendapati bahawa model QA domain terbuka amplifikasi biases lebih daripada rakan-rakan domain tertutup mereka dan melamar biases di permukaan pemulihan lebih mudah disebabkan kebebasan pilihan yang lebih besar.</abstract_ms>
      <abstract_ml>ചോദ്യം ഉത്തരം (ക്യൂഎ) മോഡലുകള്‍ ചോദ്യങ്ങള്‍ക്ക് ഉത്തരം നല്‍കാനും വായിക്കുന്ന സിസ്റ്റം ഉപയോഗിക്കു ക്യൂഎ സിസ്റ്റമുകളില്‍ നിന്നുള്ള പരിശീലനത്തിന്റെ വിശ്വാസം അവരുടെ ഉത്തരങ്ങളിലൂടെ കൂടുതല്‍ കൂടുതലാക സ്കുവാഡ് ഡാറ്റാസെറ്റിന്റെ പോലുള്ള ക്യൂഎ മോഡലുകളില്‍ പലരും വിക്കിപിഡിയയുടെ ലേഖനങ്ങളില്‍ പരിശീലിക്കപ്പെടുകയും ചെയ്തിരിക്കുന്നു.  പരിശീലന വിവരങ്ങള്‍ ക്യൂഎ സിസ്റ്റത്തിലെ പിശാചുക്കളെ എങ്ങനെ ബാധിക്കുന്നുവെന്ന് മനസ്സിലാക്കുന്നു എന് നമ്മള്‍ രണ്ടു ചോദ്യങ്ങള്‍ നിര്‍മ്മിക്കുന്നു. നിര്‍ണ്ണയിക്കുന്നത് പൂര്‍ണ്ണമായും തുറന്ന ഡൊമെയിന്‍ ചോദ്യങ്ങള്‍ക്ക് വ ഞങ്ങള്‍ മൂന്നു ആഴത്തില്‍ പഠിക്കുന്ന ക്യൂഎ സിസ്റ്റമുണ്ടാക്കുന്നത് നമ്മുടെ ചോദ്യ സജ്ജീകരണങ്ങള്‍ കൊണ്ടാണ്  നമ്മുടെ മെറ്റിക്കുകള്‍ ഉപയോഗിച്ച്, തുറന്ന ഡൊമെയിന്‍ ക്യൂഎ മോഡലുകള്‍ അവരുടെ അടച്ചുമൂടിയ ഡോമെയിന്‍ കൂടുതല്‍ പ്രതിയോഗിക്കുന്നതാണ്, തിരിച്ചുവര</abstract_ml>
      <abstract_mn>асуулт хариулт (QA) загварууд асуултад хариулт өгөхийн тулд авагч болон уншигч системийг ашигладаг. QA системийн сургалтын өгөгдлийн тухай хариу үйлдэл нь тэдний хариу үйлдэл бус байдлыг нэмэгдүүлж эсвэл харуулж чадна. SQuAD өгөгдлийн сангийн олон QA загварууд нь Wikipedia өгөгдлийн багц дээр суралцаж, шалгалт хийгддэг. Энэ нь өөрсдийн өрөөсгөл байдлыг дүрслэж, мөн бодит ертөнцийн тэгш байдлыг үржүүлдэг. QA системд сургалтын өгөгдлийн мэдээллийг хэрхэн нөлөөлж байгааг ойлгохын тулд тэгш байдлыг багасгах арга замыг мэдэх боломжтой. Бид хоёр олон асуултуудыг бүтээж, нээлттэй холбоотой. Энэ нь QA загварын талаар шалгахын тулд гайхалтай асуултуудыг ашигладаг. Бид суралцаж суралцах 3 гүнзгий QA системийг суралцах асуулт хэмжээтэй хандуулж, метрийн аргаар хариултыг үнэлдэг. Манай метрикийг ашиглан, QA-ын нээлттэй загварын загвар нь түүний холбоотой холбоотой холбоотой хүмүүсээс илүү ойлголтыг нэмэгдүүлдэг ба түүний сонголтын эрх чөлөөтэй учраас хадгалагч гадаргуудад байдал илүү амархан байдлы</abstract_mn>
      <abstract_mt>Il-mudelli tat-tweġiba għall-mistoqsijiet (QA) jużaw sistemi ta’ ġbir lura u qarrejja biex iwieġbu mistoqsijiet. Id-dipendenza fuq id-dejta tat-taħriġ mis-sistemi QA tista’ tamplifika jew tirrifletti l-inugwaljanza permezz tar-reazzjonijiet tagħhom. Ħafna mudelli QA, bħal dawk għas-sett tad-dejta SQuAD, huma mħarrġa u ttestjati fuq sottosett ta’ oġġetti tal-Wikipedia li jikkodifikaw il-preġudizzji tagħhom stess u jirriproduċu wkoll l-inugwaljanza fid-dinja reali. Il-fehim ta’ kif id-dejta tat-taħriġ taffettwa l-preġudizzju fis-sistemi QA jista’ jinforma metodi biex itaffi l-inugwaljanza. Aħna niżviluppaw żewġ settijiet ta’ mistoqsijiet għal mistoqsijiet magħluqa u miftuħa ta’ dominju rispettivament, li jużaw mistoqsijiet ambigwi biex jinstabu mudelli ta’ QA għal preġudizzju. Aħna nużaw tliet sistemi ta’ QA bbażati fuq tagħlim profond bis-settijiet ta’ mistoqsijiet tagħna u jivvalutaw ir-risposti għal preġudizzju permezz tal-metriċi. Bl-użu tal-metriċi tagħna, isibu li mudelli ta’ QA ta’ dominju miftuħ jimplifikaw il-preġudizzji aktar mill-kontropartijiet tagħhom ta’ dominju magħluq u jipproponu li l-preġudizzji fis-superfiċje tal-irkupru jkunu aktar faċli minħabba libertà akbar ta’ għażla.</abstract_mt>
      <abstract_no>Name Tilsvar på treningsdata av QA-systemet kan styrke eller refleksera ulikheten gjennom dei svara. Mange QA-modeller, som dei for SQuAD-datasettet, vert trengte og testa på ein undergruppe av Wikipedia-artiklar som kodar sine eige forsikt og også gjenopprettar ulikheten i verden. For å forstå korleis opplæringsdata påvirkar bias i QA-systemet kan informera metodar for å gjere ulikhet. Vi utviklar to sett spørsmål for lukka og opna domenespørsmål respectivt, som brukar uavhengige spørsmål for å prøve QA-modeller for forsiktighet. Vi får tre dype læringsbaserte QA-systemar med våre spørsmålssett og evaluerer svar for forsiktighet via metrikane. Bruk metrikane våre finn vi at open-domain QA-modeller forstørrar forstørringar meir enn dei lukka-domenekontorene og foreslår at forstørringane i tilhendingsfarget meir lett på grunn av større frihet av valet.</abstract_no>
      <abstract_pl>Modele odpowiedzi na pytania (QA) wykorzystują systemy retrievera i czytnika do odpowiedzi na pytania. Poleganie na danych szkoleniowych przez systemy QA może zwiększyć lub odzwierciedlać nierówność poprzez ich reakcje. Wiele modeli jakości, takich jak te dla zbioru danych SQuAD, są trenowane i testowane na podzbiorze artykułów Wikipedii, które kodują własne uprzedzenia, a także reprodukują nierówności w świecie rzeczywistym. Zrozumienie, w jaki sposób dane szkoleniowe wpływają na stronniczość w systemach jakości, może pomóc metodom łagodzenia nierówności. Opracowujemy dwa zestawy pytań odpowiednio dla pytań zamkniętych i otwartych, które wykorzystują niejednoznaczne pytania do badania modeli jakości pod kątem uprzedzeń. Naszymi zestawami pytań wykorzystujemy trzy systemy jakości oparte na głębokim uczeniu i oceniamy odpowiedzi pod kątem uprzedzeń za pomocą wskaźników. Korzystając z naszych wskaźników, stwierdzimy, że modele QA otwartej domeny wzmacniają uprzedzenia bardziej niż ich odpowiedniki w zamkniętej domenie i proponują, że uprzedzenia w powierzchni retrievera łatwiej ze względu na większą swobodę wyboru.</abstract_pl>
      <abstract_si>Name QA පද්ධතියෙන් ප්‍රශ්නය දත්තේ සම්බන්ධ වෙන්න පුළුවන් ඔවුන්ගේ ප්‍රතික්‍රියාවයෙන් අනිවාර්ය වි ගොඩක් QA මොඩල්, හරියට SQuAD දත්ත සෙට් වෙනුවෙන්, විකිපිඩියා ප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රති QA පද්ධතියේ ප්‍රශ්නය දත්ත කොහොමද ප්‍රශ්නය කරන්නේ කියලා තේරුම් ගන්න පුළුවන් විදිහට අනිවාර්ය අපි ප්‍රශ්නයක් දෙකක් විස්තර කරනවා සහ ප්‍රශ්නයක් ප්‍රශ්නයක් ප්‍රශ්නයක් විස්තර කරනවා, ඒ වගේම QA මොඩේල් එක අපි ගොඩක් ඉගෙන ගන්නේ QA පද්ධතිය තුනක් පද්ධතිය අපේ ප්‍රශ්න සෙට් එක්ක කැමති කරනවා ඒ වගේම ප්‍රශ්නයක් ප්‍ර අපේ මෙට්‍රික්ස් එක භාවිතා කරන්න, අපි හොයාගන්නවා ඒ විවෘත්ත QA මෝඩේල් එක්ක ඔවුන්ගේ වහල් ඩොමේන් එක්ක ප්‍රතිකාරයෝ වඩා වඩා ප්‍රතිකාරය</abstract_si>
      <abstract_ro>Modelele de răspuns la întrebări (QA) utilizează sisteme de retriever și cititor pentru a răspunde la întrebări. Baza pe datele de formare de către sistemele de asigurare a calității poate amplifica sau reflecta inegalitatea prin răspunsurile lor. Multe modele QA, cum ar fi cele pentru setul de date SQUAD, sunt instruite și testate pe un subset de articole Wikipedia care codează propriile prejudecăți și reproduc, de asemenea, inegalitatea din lumea reală. Înțelegerea modului în care datele de formare afectează părtinirea în sistemele de calitate poate informa metode de atenuare a inegalității. Dezvoltăm două seturi de întrebări pentru întrebări cu domeniu închis și, respectiv, deschis, care utilizează întrebări ambigue pentru a analiza modelele QA pentru părtinire. Furnizăm trei sisteme de calitate bazate pe învățare profundă cu seturile noastre de întrebări și evaluăm răspunsurile pentru părtinire prin intermediul măsurătorilor. Folosind metricile noastre, constatăm că modelele QA cu domeniu deschis amplifică prejudecățile mai mult decât omologii lor cu domeniu închis și propun ca prejudecățile din suprafața retrieverului să fie mai ușor datorită unei mai mari libertăți de alegere.</abstract_ro>
      <abstract_sr>Modeli odgovora na pitanja (QA) koriste povratničke i čitačke sisteme za odgovor na pitanja. Odstupanje na podatke o obuci podataka QA sistema može pojačati ili odražavati nepravednost kroz njihove odgovore. Mnogi QA modeli, kao što su oni za SQuAD dataset, obučeni su i testirani na podskupu Wikipedijskih članaka koji kodiraju sopstvene predrasude i takođe reproduciraju nepravednost stvarnog svijeta. Razumejući kako podaci obuke utiču na pristrasnost u QA sistemima mogu obavijestiti metode za smanjenje neispravnosti. Razvijamo dve sete pitanja za zatvorene i otvorene pitanja domena, koje koriste ambigutne pitanja za probu modela QA za predrasude. Hranimo tri sistema na dubokom učenju QA sa našim setima pitanja i procjenjujemo odgovore na predrasude putem metrika. Koristeći našu metriku, našli smo da modeli otvorenog domena QA pojačaju predrasude više od njihovih kolega zatvorenih domena i predlažemo da predrasude na površini površine povratnika lakše zbog veće slobode izbora.</abstract_sr>
      <abstract_so>Isku jawaabta su'aalaha su'aalaha (QA) waxay isticmaaliyaan dib u qaadista iyo nidaamka akhriska si ay ugu jawaabaan su'aalaha. Kaalmada waxbarashada ee nidaamka QA waxay ku kordhi karaan ama ka fikiri karaan sinnaanta jawaabahooda. Tusaale badan QA, tusaale ahaan kuwa SQuAD dataset, waxaa lagu tababaray oo lagu tijaabiyey qeyb ka mid ah warqadaha Wikipedia oo ku qoran baaritaankooda islamarkaasna lagu soo bandhigaa sinnaanta dunida ee halis ah. Waan garashada sida macluumaadka waxbarashadu ay u saameyn karaan hababka nidaamka QA, waxay ogeysiin karaan qaababka ku qiyaasta sinnaanta. Waxaan horumarinaa laba su'aalood oo su'aalo ah si qarsoon oo furan, kuwaas oo lagu isticmaalaa su'aalo qalloocan ah si aan u caddeyno tusaalooyin QA ah oo baas ah. Saddex nidaam ee QA ee hoose-u-barashada ayaannu ku quudinnaa kooxaha su'aalahana, waxaynu qiimeynaynaa jawaabayaasha baaritaanka xagga metrikada. Isku isticmaalidda qaababkayada QA ee furan waxaynu aragnaa in qaababka QA ay ku kordhiyaan khilaafka ay ka badnaan yihiin saaxiibbadooda degmooyinka qarsoonka ah, waxaana soo jeedinayaa in dabeecada dib u soo celinta ay si fudud ugu fududaato xorriyadda doorashada badan darteed.</abstract_so>
      <abstract_ta>Question answering (QA) models use retriever and reader systems to answer questions.  QA அமைப்புகள் மூலம் பயிற்சி தரவுகள் மீது மாற்றலாம் அல்லது அவர்களுடைய பதில்கள் மூலம் நியாயத்தை பார்க்கலாம். Many QA models, like those for the SQuAD dataset, are trained and tested on a subset of Wikipedia articles which encode their own biases and also reproduce real- world inequality. பயிற்சி தரவு எவ்வாறு QA முறைமைகளில் பிரச்சனைகளை பாதிக்கும் என்பதை புரிந்து கொள்வது எவ்வாறு நியாயத்த நாம் மூடப்பட்டுள்ள மற்றும் திறந்த களம் கேள்விகளுக்கு இரண்டு கேள்விகளை உருவாக்குகிறோம். அது கியூஏ மாதிரிகளை பாதி நாங்கள் மூன்று ஆழமான கற்றல் அடிப்படையிலான கியூஏ அமைப்புகளை எங்கள் கேள்வி அமைப்புகளுடன் உணவளிக்கிறோம் மெட்ரிக் எங்கள் மெட்ரிக்களை பயன்படுத்தி, திறந்த களம் கியூஏ மாதிரிகள் அவர்களுடைய மூடிய களஞ்சியத்தை விட பெரிதாக்குகிறது மற்றும் திரும்ப திருப்பும் மேல்</abstract_ta>
      <abstract_sv>Frågor besvarande modeller använder retriever- och läsarsystem för att svara på frågor. Att kvalitetssäkringssystem förlitar sig på träningsdata kan förstärka eller återspegla orättvisor genom deras svar. Många QA-modeller, till exempel de för SQUAD-datauppsättningen, är utbildade och testade på en delmängd av Wikipediaartiklar som kodar deras egna fördomar och även reproducerar verklig ojämlikhet. Att förstå hur träningsdata påverkar bias i QA-system kan bidra till metoder för att mildra orättvisor. Vi utvecklar två uppsättningar frågor för stängda respektive öppna domänfrågor, som använder tvetydiga frågor för att undersöka QA-modeller för bias. Vi matar tre djupinlärningsbaserade QA-system med våra frågeuppsättningar och utvärderar svar för bias via mätvärdena. Med hjälp av våra mätvärden ser vi att QA-modeller med öppen domän förstärker fördomar mer än deras motsvarigheter med sluten domän och föreslår att fördomar i retriever ytan lättare på grund av större valfrihet.</abstract_sv>
      <abstract_ur>سوال جواب دینے کے لئے (QA) موڈل استعمال کرتے ہیں سوال جواب دینے کے لئے پھیرنے اور پڑھنے کی سیسٹم. QA سیستموں کے ذریعہ تربیت ڈیٹوں پر اعتبار ہے کہ ان کے جواب کے ذریعہ انصاف کو بڑھا یا دکھا سکتا ہے. بہت سے QA موڈل، جیسے SQuAD ڈیٹ سٹ کے لئے، ویکیپیڈیا لکھائیوں کے سپسٹ پر آموزش اور آزمائش کی جاتی ہیں جو اپنی مخالفت کو Encoding کرتے ہیں اور اصل دنیا کی نابرابری کو بھی دوبارہ پیدا کرتے ہیں۔ تعلیم دیٹا کیسے QA سیستموں میں غیر انصافی کو کمزور کرنے کے لئے روش بتائی جاتی ہے۔ ہم دو سٹ سوالوں کو بند اور کھول دینے کے لئے مختلف سوالوں کے لئے تخلیق کریں گے، جو QA موڈل کو بغیر غیر مشکلات کے لئے استعمال کرتے ہیں۔ ہم تین عمیق سیکھنے کی QA سیستموں کو پوچھنے کے ساتھ کھلاتے ہیں اور منٹریک کے ذریعے انعام کے لئے جواب دیتے ہیں۔ ہمارے مٹریک کے استعمال سے ہم دیکھتے ہیں کہ open-domain QA موڈل ان کے بند ڈومین کے کنٹرپارتوں سے زیادہ غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر</abstract_ur>
      <abstract_uz>Name Name Koʻp QA modellari, SQuAD maʼlumotlari uchun o'rganilgan va Wikipedia maqolalarining tub qismlarida ishlatilgan va ularning o'zimlarini kodlash va asl dunyodagi sinflikni qaytadi. @ info Biz boshqa yopilgan va ochiq domen savollari uchun ikkita savollarni yaratishmiz. Bu savollar QA modellarini bias uchun ishlab chiqaradi. Biz uchta ta ta'lim bo'lgan QA tizimlarimizni soʻrovimiz moslamalarimiz bilan ishlatamiz va metriklar orqali bizning qiymatlarimizni qiymatimiz. Bizning metriklarimizdan foydalanishimiz mumkin, open domain QA modellari yopilgan domen kompyuterlaridan ko'proq ishlarni oshirishni ko'rsatamiz va o'zgartirib chiqishni o'zgartirish xolosi sababini ko'proq o'zgartirib turadi.</abstract_uz>
      <abstract_vi>Các mẫu câu hỏi (QA) sử dụng hệ thống phục hồi và đọc để trả lời câu hỏi. Sự tin tưởng vào dữ liệu huấn luyện của hệ thống QA có thể khuếch đại hay phản ánh bất công qua các phản ứng. Nhiều mô hình QA, như hệ thống dữ liệu SCOAD, được đào tạo và thử nghiệm trên một nhóm các tạp chí Wikipedia đã mã hóa các giả của chính họ và cũng tạo ra sự bất công trong thế giới thực. Sự hiểu tại sao dữ liệu về huấn luyện ảnh hưởng đến khuynh hướng trong hệ thống QA có thể cung cấp phương pháp chống lại bất công. Chúng tôi phát triển hai bộ câu hỏi cho hai câu hỏi riêng lẻ và mở, hai câu hỏi dùng những câu mơ hồ để thăm dò biểu tượng thiên vị. Chúng tôi cung cấp ba hệ thống QA dựa vào sâu học với các bộ câu hỏi và đánh giá các phản ứng về khuynh hướng thông qua âm lượng. Sử dụng âm lượng của chúng tôi, chúng tôi tìm thấy các mô hình QA mở rộng hơn so với các chủng giả trong vùng kín và đề xuất khả năng biến dạng trên bề mặt phục hồi nhanh hơn nhờ vào sự tự do lựa chọn nhiều hơn.</abstract_vi>
      <abstract_bg>Моделите за отговор на въпроси използват системи за изтегляне и четене, за да отговорят на въпроси. Разчитането на данните за обучението от системите за контрол на качеството може да увеличи или отрази неравенството чрез техните отговори. Много модели за оценка на качеството, като тези за набора от данни SQuAD, са обучени и тествани върху поднабор от статии в Уикипедия, които кодират собствените си пристрастия и също възпроизвеждат неравенството в реалния свят. Разбирането как данните от обучението влияят на пристрастията в системите за контрол на качеството може да информира методите за смекчаване на неравенството. Разработваме два комплекта въпроси съответно за въпроси със затворен и отворен домейн, които използват двусмислени въпроси за проучване на модели за оценка на качеството за пристрастия. Ние подхранваме три системи за оценка на качеството, базирани на дълбоко обучение, с нашите набори от въпроси и оценяваме отговорите за пристрастия чрез показателите. Използвайки нашите показатели, откриваме, че моделите с отворен домейн увеличават пристрастията повече от техните затворени домейни колеги и предлагат пристрастията в повърхността на ретривера по-лесно поради по-голяма свобода на избор.</abstract_bg>
      <abstract_hr>Modeli odgovora na pitanja (QA) koriste sustave prikupljača i čitača za odgovor na pitanja. Odstupanje na podatke o obuci sustava QA-a može povećati ili odražiti nejednakost kroz njihove odgovore. Mnogi QA modeli, kao što su oni za SQuAD dataset, obučeni su i testirani na podskupu Wikipedijskih članaka koji kodiraju vlastite predrasude i reproduktiraju nepravednost stvarnog svijeta. Razumijeti kako podaci obuke utječu na pristrasnost u QA sustavima mogu obavijestiti metode za smanjenje neispravnosti. Razvijamo dvije vrste pitanja za zatvorena i otvorena pitanja domena, koje koriste dvosmislene pitanja za probu modela QA za predrasude. Hranimo tri sustava na dubokom učenju QA sa našim postavima pitanja i procjenjujemo odgovore na predrasude putem metrika. Koristeći našu metriku, nalazimo se da modeli otvorenog domena QA pojačaju predrasude više od njihovih kolega zatvorenih domena i predlažemo da predrasude na površini površine povlačenja lakše zbog veće slobode izbora.</abstract_hr>
      <abstract_nl>Vragen beantwoorden (QA) modellen gebruiken retriever- en readersystemen om vragen te beantwoorden. Vertrouwen op trainingsgegevens door QA-systemen kan ongelijkheid vergroten of weerspiegelen door hun reacties. Veel QA modellen, zoals die voor de SQuAD dataset, worden getraind en getest op een subset Wikipedia artikelen die hun eigen vooroordelen coderen en ook echte ongelijkheid reproduceren. Inzicht in hoe trainingsdata bias in QA-systemen beïnvloedt, kan informatie geven over methoden om ongelijkheid te verminderen. We ontwikkelen twee sets vragen voor respectievelijk gesloten en open domeinvragen, die dubbelzinnige vragen gebruiken om QA modellen te onderzoeken op bias. We voeden drie op deep learning gebaseerde QA-systemen met onze vragensets en evalueren antwoorden op bias via de statistieken. Met behulp van onze metrics, ontdekken we dat open-domein QA modellen biases meer versterken dan hun closed-domein tegenhangers en stellen we voor dat biases in de retriever gemakkelijker opduiken als gevolg van een grotere keuzevrijheid.</abstract_nl>
      <abstract_da>Spørgsmålsbesvarelsesmodeller bruger retriever- og læsersystemer til at besvare spørgsmål. Afhængighed af træningsdata fra QA-systemer kan forstærke eller afspejle ulighed gennem deres reaktioner. Mange QA modeller, såsom dem for SQUAD datasættet, er trænet og testet på en del af Wikipedia artikler, der koder deres egne fordomme og også reproducerer ulighed i den virkelige verden. At forstå, hvordan træningsdata påvirker bias i QA-systemer, kan danne grundlag for metoder til at afhjælpe uligheder. Vi udvikler to sæt spørgsmål til henholdsvis lukkede og åbne domæne spørgsmål, som bruger tvetydige spørgsmål til at undersøge QA modeller for bias. Vi fodrer tre deep-learning-baserede QA-systemer med vores spørgsmålsæt og evaluerer svar for bias via målene. Ved hjælp af vores metrics finder vi, at open-domain QA modeller forstærker bias mere end deres lukkede domæne modstykker og foreslår, at bias i retriever overfladen lettere på grund af større valgfrihed.</abstract_da>
      <abstract_de>Fragebeantworter (QA)-Modelle verwenden Retriever- und Lesesysteme, um Fragen zu beantworten. Die Abhängigkeit von Trainingsdaten durch QS-Systeme kann Ungleichheit durch ihre Reaktionen verstärken oder widerspiegeln. Viele QA-Modelle, wie die für den SQuAD-Datensatz, werden an einer Teilmenge von Wikipedia-Artikeln trainiert und getestet, die ihre eigenen Verzerrungen kodieren und auch reale Ungleichheiten reproduzieren. Zu verstehen, wie Trainingsdaten Verzerrungen in QS-Systemen beeinflussen, kann Methoden zur Minderung von Ungleichheiten unterstützen. Wir entwickeln zwei Sätze von Fragen für geschlossene und offene Fragen, die mehrdeutige Fragen verwenden, um QA-Modelle auf Bias zu untersuchen. Wir speisen drei Deep-Learning-basierte QA-Systeme mit unseren Fragesets und bewerten Antworten auf Bias anhand der Metriken. Anhand unserer Metriken finden wir heraus, dass offene QA-Modelle Verzerrungen stärker verstärken als ihre geschlossenen Pendants und schlagen vor, dass Verzerrungen in der Retrieveroberfläche aufgrund größerer Wahlfreiheit leichter auftreten.</abstract_de>
      <abstract_ko>Q&amp;amp;A 모델은 검색기와 카드 리더기 시스템을 사용하여 질문에 대답합니다.QA시스템의 교육 데이터에 대한 의존도는 그들의 반응을 통해 불공평함을 확대하거나 반영할 수 있다.많은 품질 보증 모델, 예를 들어 단체 데이터 집합에 사용되는 모델은 모두 위키백과 문장의 서브집합에서 훈련과 테스트를 실시한 것이다. 이런 문장들은 자신의 편견을 인코딩하고 현실 세계의 불평등을 재현했다.교육 데이터가 QA 시스템의 편차에 어떻게 영향을 미치는지 이해하면 불공평함을 완화할 수 있는 방법을 제공할 수 있다.폐쇄적 영역 문제와 개방적 영역 문제에 대해 모호한 문제로 QA모델의 편견을 탐색하는 두 그룹을 개발했다.우리는 깊이 있는 학습을 바탕으로 하는 세 개의 QA 시스템에 문제집을 제공하고 이러한 지표를 통해 피드백에 편차가 있는지 평가한다.우리의 지표를 사용하면 개방역 QA모델이 폐쇄역 QA모델보다 편차를 확대하기 쉽다는 것을 알 수 있고 더 큰 선택의 자유로 인해 검색기에서의 편차가 더욱 쉽게 나타난다는 것을 알 수 있다.</abstract_ko>
      <abstract_id>Model menjawab pertanyaan (QA) menggunakan sistem retriever dan pembaca untuk menjawab pertanyaan. Bergantung pada data pelatihan oleh sistem QA dapat memperkuat atau merefleksikan ketidakadilan melalui respon mereka. Banyak model QA, seperti yang untuk set data SQuAD, dilatih dan diuji pada subset artikel Wikipedia yang mengekodikan biases mereka sendiri dan juga mereproduksi ketidaksamaan dunia nyata. Memahami bagaimana data pelatihan mempengaruhi bias dalam sistem QA dapat memberitahu metode untuk mengurangi ketidaksetara. Kami mengembangkan dua set pertanyaan untuk pertanyaan domain tertutup dan terbuka secara sesuai, yang menggunakan pertanyaan ambiguh untuk memeriksa model QA untuk bias. Kami memberi makan tiga sistem QA berbasis belajar dalam dengan set pertanyaan kami dan mengevaluasi respon untuk bias melalui metrik. Menggunakan metrik kami, kami menemukan bahwa model QA domain terbuka amplifikasi biases lebih dari rekan domain tertutup mereka dan menyarankan biases di permukaan retriever lebih mudah karena kebebasan pilihan yang lebih besar.</abstract_id>
      <abstract_fa>مدلهای پاسخ سوال (QA) برای پاسخ از سوالات استفاده از سیستم گیرنده و خواننده استفاده می‌کنند. ارتباط به داده آموزش توسط سیستم‌های QA می‌تواند به وسیله پاسخ‌هایشان عدالتی را افزایش یا توضیح دهد. بسیاری از مدل QA، مانند آن‌ها که برای مجموعه داده‌های SQuAD، روی زیر مجموعه‌های ویکیپدییا آموزش داده می‌شوند و آزمایش می‌شوند که همچنین تغییرات خود را رمز می‌کنند و همچنین نابرابری دنیای واقعی را بازسازی می‌کنند. درک کردن اطلاعات آموزش چگونه در سیستم‌های QA تحت تاثیر قرار می‌گیرد می‌تواند روش‌هایی را برای کاهش دادن عدالتی اطلاع دهد. ما دو مجموعه سوال برای سوال های بسته و باز دامنی را به صورت مختلف توسعه می کنیم، که از سوال های مختلف استفاده می کنند تا مدل های QA را برای تحقیق کردن طبیعی تحقیق کنند. ما سه سیستم QA بنیاد عمیق یادگیری را با مجموعه سوال‌هایمان تغذیه می‌کنیم و پاسخ‌هایی برای تغییرات در طول متریک ارزیابی می‌کنیم. با استفاده از متریک‌هایمان، ما پیدا می‌کنیم که مدل‌های باز دامنه‌ای QA از همکاران دامنه‌های بسته‌شان بیشتری را افزایش می‌دهد و پیشنهاد می‌دهیم که توسط آزادی‌های انتخاب بیشتری در سطح بازیابی‌کننده‌ها بسیار آسانتر است.</abstract_fa>
      <abstract_tr>Sorag jogap (QA) nusgalary almak we okamak sistemlerini soraglary jogaplamak üçin ulanýarlar. QA sistemalary tarapyndan okuwçylyk maglumatynda ynamlyk çykyşlygyny ýagdaýlaşdyryp biler. SQuAD maglumatlary üçin köp QA modelleri, Wikipediýanyň bir toparyny kodýan we dünýäde ýetişiklikleri diýip test edilýär. QA sistemalarda öwrenme maglumatyň nähili nähili ýeterlik edýändigini biljek bolýar. Qapyk we açyk domena soraglary üçin iki topar soragy düzenleýäris. Bu ýerler QA modellerini bias üçin öňünde örän möhüm soraglary ulanýarlar. Biz 3 derin öwrenme tabanly QA sistemalary soraglarymyz bilen üýtgedýäris we metriýa görä jogaplarymyzy deňleýäris. Metriklerimizi kullanarak, open-domain QA modelleri kapalı domeny karşındakilerinden ötesini arttırmak ve daha kolay çekişin yüzeyinde bu sowukları saýlamak özgürlüğüne göre daha kolay düşürmeyi teklif ediyoruz.</abstract_tr>
      <abstract_sw>Swali likijibu mifano (QA) hutumia mfumo wa kurejea na wasomaji kujibu maswali. Uimani wa taarifa za mafunzo na mfumo wa QA unaweza kuongeza au kutafakari kutokuwepo usawa kupitia majibu yao. Mfano mwingi wa QA, kama vile wale wa seti ya data ya SQuAD, wanafundishwa na kuchapishwa kwenye mfululizo wa makala za Wikipedia ambazo hujumuisha upendeleo wao wenyewe na pia kuonyesha usawa wa kimataifa halisi. Understanding how training data affects bias in QA systems can inform methods to mitigate inequity.  Tunaendeleza vitu viwili vya maswali kwa ajili ya kufungwa na wazi maswali ya ndani, ambavyo hutumia maswali yasiyo na matatizo ya kuthibitisha mifano ya QA kwa upendeleo. Tunawalisha mfumo wa QA wenye msingi wa elimu tatu kwa maswali yetu na kutathmini majibu ya upendeleo kupitia mitindo. Kwa kutumia mbinu zetu, tunagundua kuwa mifano ya wazi ya QA inaongezea upendeleo zaidi ya wapinzani wao waliofungwa ndani na kupendekeza kwamba upendeleo katika upinzani wa kurejea kwa urahisi zaidi kwa sababu ya uhuru mkubwa wa uchaguzi.</abstract_sw>
      <abstract_sq>Modelet që përgjigjen pyetjeve (QA) përdorin sistemet e marrjes dhe lexuesit për të përgjigjur pyetjeve. Reliance on training data by QA systems can amplify or reflect inequity through their responses.  Shumë modele QA, të tillë si ato për grupin e të dhënave SQuAD, janë trajnuar dhe testuar në një nëngrup artikujsh të Wikipedias që kodojnë paragjykimet e tyre dhe gjithashtu riprodhojnë pabarazinë e botës reale. Duke kuptuar se si treinimi i të dhënave ndikon në paragjykimet në sistemet QA mund të informojë metodat për të lehtësuar pabarazinë. Ne zhvillojmë dy grupe pyetjesh për pyetje të mbyllura dhe të hapura respektivisht, të cilat përdorin pyetje të qarta për të hetuar modelet QA për paragjykim. Ne ushqejmë tre sisteme QA bazuar në mësim të thellë me grupet tona të pyetjeve dhe vlerësojmë përgjigjet për paragjykimet nëpërmjet metrikave. Duke përdorur metrikat tona, ne gjejmë se modelet QA në domeni të hapur amplifikojnë paragjykimet më shumë se homologët e tyre në domeni të mbyllur dhe propozojnë që paragjykimet në sipërfaqen e ripërmarrësit më lehtë për shkak të lirisë më të madhe të zgjedhjes.</abstract_sq>
      <abstract_af>Name Verwantigheid op onderwerp data deur QA stelsels kan onregtigheid versterk of reflekteer deur hul antwoordes. Baie QA-modelles, soos dié vir die SQuAD-datastel, word onderwerp en toets op 'n subgrup van Wikipedia-artikels wat hul eie voorspoedies kodeer en ook reël-wêreld-inekwaliteit herhaal. Om te verstaan hoe onderwerking data beïnvloor bias in QA stelsels kan metodes inlig om onregtigheid te verminder. Ons ontwikkel twee stel vrae vir toegesluit en oop domein vrae respektief, wat gebruik onbepaalde vrae om QA-modele vir bias te probeer. Ons voer drie diep-leer-gebaseerde QA stelsels met ons vraagstellings en evalueer antwoordes vir bias deur die metriek. By die gebruik van ons metries, vind ons dat open-domain QA-modeller versterk biasies meer as hulle toegesluit-domein-kenaars en voorstel dat voorspoedies in die ontvanger-oortjie meer leeg weens groot vryheid van keuse.</abstract_af>
      <abstract_az>Soru cavab vermək (QA) modelləri suallarına cavab vermək üçün alıcı və oxuyan sistemləri kullanır. QA sistemlərinin təhsil məlumatlarının bağlılığı onların cavab vermələri ilə ədalətliyini artıra və ya göstərə bilər. SQuAD veri qutusu üçün bir çox QA modelləri Wikipedia maddələrinin subgruplarında təhsil edilir və təhsil edilir ki, özlərinin önlərini kodlayar və həmçinin həqiqət dünyanın ədalətini təhsil edirlər. QA sistemlərində təhsil məlumatları necə təsir edir ki, ədaləti azaltmaq üçün metodları bilər. Qapılmış və açıq domena sualları üçün iki sorğu inşa edirik ki, QA modellərini bias üçün təşkil etmək üçün münasibətli suallara istifadə edirlər. Biz 3 dərin öyrənmə sistemi QA sistemlərini sorğu qurularımızla besləyirik və metrik vasitəsilə təqsirlərin cavablarını değerləyirik. Metriklərimizi istifadə edirək, açıq-domani QA modelləri onların qapılmış domenin qardaşlarından daha çox təsirlərini artırar və seçim özgürlüyünün səbəbi üçün bu təsirlərin geri almaq üzərində daha asanlıqla təsirlərini təbliğ edirik.</abstract_az>
      <abstract_bn>প্রশ্নের উত্তর (QA) মডেল প্রশ্নের উত্তর দেয়ার জন্য পুনরুদ্ধার এবং পাঠক সিস্টেম ব্যবহার করে। কিউএ সিস্টেমের প্রশিক্ষণের তথ্য বিশ্বাস তাদের প্রতিক্রিয়ার মাধ্যমে বৃদ্ধি বা অসমর্থতা প্রতিফলিত করত অনেক কিউএ মডেল, যেমন এসকুয়াড ডাটাসেটের জন্য প্রশিক্ষণ ও পরীক্ষা করা হয় উইকিপিডিয়া প্রবন্ধের একটি সাবস্কেটে যা তাদের নিজেদের বিরুদ্ধে প্রতি বুঝতে পারলাম কিভাবে প্রশিক্ষণের তথ্য কিউএ সিস্টেমের বিভ্রান্তির উপর প্রভাব ফেলে দেয়া যায়, যাতে নৈত আমরা প্রত্যেক ভাবে বন্ধ এবং খোলা ডোমেইন প্রশ্নের জন্য দুই ধরনের প্রশ্ন তৈরি করি, যা কিউএ মডেল ব্যবহার করে বিভিন্ন প্রশ্নের জন আমরা তিনটি গভীর শিক্ষার ভিত্তিক কিউএ সিস্টেম খাওয়াচ্ছি আমাদের প্রশ্নের সেট দিয়ে এবং মেট্রিকের মাধ্যমে বিভ্রান আমাদের মেট্রিক ব্যবহার করে আমরা দেখতে পাচ্ছি যে উন্মুক্ত ডোমেইন কিউএ মডেল তাদের বন্ধ-ডোমেইন বিরোধীদের চেয়ে বেশী প্রতিযোগিতা বৃদ্ধি করে এবং প্রস্তা</abstract_bn>
      <abstract_am>ጥያቄ መልስ የQA ስርዓት ድምፅ በማድረግ ወይም በመስጠታቸው በጥቅነት ማድረግ ይችላል፡፡ ብዙዎች የQA ምሳሌዎች፣ እንደነዚህ SQuAD ዳታ ሰርቨሮች፣ የራሳቸውን ጥያቄ የሆኑትን እና እውነተኛውን ዓለም ትክክል በማድረግ በWikipedia ጽሑፎች ክፍል ተማርተዋል፡፡ የትምህርት ዳታዎች በQA ስርዓቶች ውስጥ የውስደትን ማቀናቀል እንዴት እንዲያስጨንቃቸው እንዳስተውሉ ነው፡፡ ሁለት ጥያቄዎችን ለመዘጋጀት እና ለመክፈት የዶሜን ጥያቄዎች እናስገድዳለን፡፡ የጥልቅ ትምህርት መሆኑን የQA ሲስተማርን በመጠየቃችን ሰርሰናል እና የመተማሪዎችን መልስ እናስተዋልታለን፡፡ ማተሚያዎቻችንን በመጠቀም የክፈት ዲሜইন የQA ሙሉ አካባቢዎችን ከመዘጋጀት የሚጨምር ሁኔታዎችን እና የመለስ ነጻነት ምክንያት በጥቅልነት የሚደረገውን እናሳውቃለን፡፡</abstract_am>
      <abstract_bs>Modeli odgovora na pitanja (QA) koriste povratničke i čitačke sisteme za odgovor na pitanja. Odstupanje na podatke o obuci podataka QA sustava može pojačati ili odražavati nepravednost kroz njihove odgovore. Mnogi QA modeli, kao što su oni za SQuAD dataset, obučeni su i testirani na podskupu Wikipedijskih članaka koji kodiraju sopstvene predrasude i također reproduktuju nepravednost stvarnog svijeta. Razumijeti kako podaci obuke utiču na pristrasnost u QA sistemima mogu obavijestiti metode za smanjenje neispravnosti. Razvijamo dvije vrste pitanja za zatvorene i otvorene pitanja domena, koje koriste ambigutne pitanja za probu modela QA za predrasude. Hranimo tri sistema na dubokom učenju QA sa našim setima pitanja i procjenjujemo odgovore na predrasude putem metrika. Koristeći našu metriku, našli smo da modeli otvorenog domena QA pojačaju predrasude više od njihovih kolega zatvorenih domena i predlažemo da predrasude na površini površine povlačenja lakše zbog veće slobode izbora.</abstract_bs>
      <abstract_ca>Question answering (QA) models use retriever and reader systems to answer questions.  La confiança en les dades d'entrenament dels sistemes QA pot amplificar o reflexionar la desigualtat a través de les seves respostes. Molts models QA, com els del conjunt de dades SQuAD, són entrenats i testats en un subconjunt d'articles de Wikipedia que codifiquen les seves pròpies tendències i també reprodueixen la desigualtat del món real. Entendre com les dades d'entrenament afecten els bias dels sistemes QA poden informar mètodes per mitigar la desigualtat. Desenvolvem dos conjunts de preguntes per preguntes de domini tancat i obert, respectivament, que utilitzen preguntes ambigues per investigar els models QA per al bias. Ens alimentem tres sistemes de QA basats en aprenentatge profund amb els nostres conjunts de preguntes i evaluem les respostes de bias a través de les mètriques. Utilitzant les nostres mètriques, trobem que els models de QA de domini obert amplifiquen les biases més que els seus homosexuals de domini tancat i proponem que les biases a la superfície del recuperador siguin més fàcilment degut a una llibertat d'elecció més gran.</abstract_ca>
      <abstract_hy>Հարցերին պատասխանելու (QA) մոդելները օգտագործում են վերադարձնող և կարդացնող համակարգեր հարցերին պատասխանելու համար: Հաշվի առնելը QA համակարգերի կողմից տեղեկատվության ուսումնասիրության վրա կարող է աճեցնել կամ արտացոլում անհավասարությունը իրենց արձագանքների միջոցով: Շատ QA մոդելներ, ինչպիսիք են SQUADի տվյալների համակարգի մոդելները, սովորեցվում են և փորձարկում են Վիքիփեդիայի հոդվածների մի ենթահամակարգի վրա, որոնք կոդավորում են իրենց սեփական կողմնականությունները և նաև վերարտադրում են իրական աշխարհ Հասկանալ, թե ինչպես են կրթության տվյալները ազդում QA համակարգերի կողմնականության վրա, կարող են տեղեկացնել անհավասարության նվազեցնելու մեթոդները: Մենք զարգանում ենք երկու հարցեր փակ և բաց դաշտային հարցերի համար, որոնք օգտագործում են երկու հարցեր QA մոդելների ուսումնասիրելու համար: Մենք կերակրում ենք երեք խորը ուսումնասիրության հիմնված QA համակարգեր մեր հարցերի համակարգերի միջոցով և գնահատում ենք կողմնականության պատասխանները մետրիկայի միջոցով: Օգտագործելով մեր մետրիկները, մենք հայտնաբերում ենք, որ բաց տիեզերքի QA մոդելները ամպլիֆիացնում են կողմնականությունները ավելի շատ, քան իրենց փակ տիեզերքի հակառակը և առաջարկում են, որ կողմնականությունները վերադարձնող մակերևույթի վրա ավելի հեշտ ընտրության</abstract_hy>
      <abstract_cs>Modely zodpovězení otázek (QA) využívají k odpovědi na otázky systémy retriever a čtečky. Spoléhání na výcviková data systémy QA může prostřednictvím jejich reakcí zesílit nebo odrážet nerovnost. Mnoho modelů QA, například modelů pro datovou sadu SQuAD, je trénováno a testováno na podmnožině článků Wikipedie, které kódují své vlastní předsudky a také reprodukují nerovnost v reálném světě. Pochopení toho, jak tréninková data ovlivňují zaujatost v systémech QA, může informovat o metodách ke zmírnění nerovnosti. Vyvíjíme dvě sady otázek pro otázky uzavřené a otevřené domény, které používají nejednoznačné otázky k zkoumání modelů kvality kvality pro zaujatost. Našimi sadami otázek krmíme tři systémy QA založené na hlubokém učení a vyhodnocujeme odpovědi na zaujatost prostřednictvím metrik. Pomocí našich metrik zjišťujeme, že modely QA otevřené domény zesilují předsudky více než jejich protějšky v uzavřené doméně a navrhujeme, že předsudky v retrieveru povrchu snadněji díky větší svobodě volby.</abstract_cs>
      <abstract_et>Küsimustele vastamise mudelid kasutavad küsimustele vastamiseks otsija- ja lugejasüsteeme. Kvaliteedi tagamise süsteemide tuginemine koolitusandmetele võib nende vastuste kaudu võimendada või kajastada ebavõrdsust. Paljud kvaliteedi tagamise mudelid, näiteks SQuAD andmekogumi mudelid, on koolitatud ja testitud Vikipeedia artiklite alamhulga alusel, mis kodeerivad nende enda kallakud ja reprodutseerivad ka reaalset ebavõrdsust. Mõistmine, kuidas koolitusandmed mõjutavad kvaliteedi tagamise süsteemide eelarvamusi, võib anda teavet ebavõrdsuse leevendamise meetoditest. Töötame välja kaks küsimuste komplekti vastavalt suletud ja avatud domeeni küsimustele, mis kasutavad ebaselgeid küsimusi kvaliteedi tagamise mudelite uurimiseks erapooletuse suhtes. Me lisame oma küsimustega kolme sügavõppepõhist kvaliteedi tagamise süsteemi ja hindame mõõdikute kaudu vastuseid erapooletuse suhtes. Meie mõõdikute abil leiame, et avatud domeeni kvaliteedi tagamise mudelid võimendavad kallakuid rohkem kui nende suletud domeeni kolleegid ja pakuvad välja, et kallakud retriiveri pinnal kergemini tänu suuremale valikuvabadusele.</abstract_et>
      <abstract_fi>Kysymysten vastausmallit kﾃ､yttﾃ､vﾃ､t hakuja ja lukijoita kysymyksiin vastaamiseen. Luottamus laadunvarmistusjﾃ､rjestelmien koulutustietoihin voi lisﾃ､tﾃ､ tai heijastaa eriarvoisuutta vastauksissaan. Monet laadunvarmistusmallit, kuten SQuAD-aineiston mallit, on koulutettu ja testattu Wikipedia-artikkeleiden alaryhmﾃ､llﾃ､, joka koodaa omia ennakkoluulojaan ja myﾃｶs toistaa reaalimaailman eriarvoisuutta. Sen ymmﾃ､rtﾃ､minen, miten koulutustiedot vaikuttavat laadunvarmistusjﾃ､rjestelmien ennakkoluuloihin, voi auttaa vﾃ､hentﾃ､mﾃ､ﾃ､n eriarvoisuutta. Kehitﾃ､mme kaksi kysymystﾃ､ suljetuille ja avoimille kysymyksille, jotka kﾃ､yttﾃ､vﾃ､t epﾃ､selviﾃ､ kysymyksiﾃ､ kartoittaakseen laadunvarmistusmalleja puolueellisuuden varalta. Syﾃｶtﾃ､mme kysymyksiﾃ､ kolmelle syvﾃ､oppimiseen perustuvalle laadunvarmistusjﾃ､rjestelmﾃ､lle ja arvioimme vastauksia vinoutumisen varalta mittarien avulla. Mittareidemme avulla havaitsemme, ettﾃ､ avoimen verkkotunnuksen laadunvarmistusmallit vahvistavat vﾃ､ﾃ､ristymiﾃ､ enemmﾃ､n kuin suljetun verkkotunnuksen vastineet ja ehdottavat, ettﾃ､ vﾃ､ﾃ､ristymﾃ､t noutajan pinnalla helpommin johtuen suuremmasta valinnanvapaudesta.</abstract_fi>
      <abstract_jv>Name Lakan nganggo data nggawe barang kelas Daftar model Tulung ngerasai perangkat kuwi nggambar dadi kanggo biasane ning sistem KA iso nggawe barang nggawe barang kanggo mbelikasi layang. We create 2 set of question for Clock and open domain question responsibly, that use ambguent question to likely qA modes for bias. Awak dhéwé éntuk telu sistem sing basa-ingkang dipun-ingkang kelas barang awak dhéwé kuwi nggawe barang nggambar barang maning. Ngawe gunakake meter, kita bukane open-domain qA model amplify bias liyane karo hal-domain count nggawe lan supoyo bias kanggo awak dhéwé iso nggawe aturan tambah kebebasan perusahaan langgar-sistem sing luwih akeh lani kanggo kebebasané perusahaan winih dhéwé.</abstract_jv>
      <abstract_sk>Modeli za odgovarjanje na vprašanja (QA) uporabljajo sisteme za iskanje in branje za odgovore na vprašanja. Zanašanje na podatke o usposabljanju s strani sistemov za zagotavljanje kakovosti lahko poveča ali odraža neenakost z njihovimi odzivi. Mnogi modeli kakovosti, kot so modeli za nabor podatkov SQuAD, so usposobljeni in testirani na podskupini Wikipedijskih člankov, ki kodirajo njihove pristranskosti in tudi reproducirajo neenakost v realnem svetu. Razumevanje, kako podatki o usposabljanju vplivajo na pristranskost v sistemih za zagotavljanje kakovosti, lahko upošteva metode za ublažitev neenakosti. Razvijamo dva sklopa vprašanj za zaprta in odprta vprašanja, ki uporabljata dvoumna vprašanja za proučevanje pristranskosti modelov kakovosti. Z našimi nabori vprašanj hranimo tri sisteme za zagotavljanje kakovosti, ki temeljijo na globokem učenju, in ocenjujemo odgovore na pristranskost prek meritev. Z uporabo naših meritev ugotavljamo, da modeli odprtega domena kakovosti bolj povečujejo pristranske pristranske kot njihove zaprte domene in predlagamo, da pristranske na površini iskalnika lažje zaradi večje svobode izbire.</abstract_sk>
      <abstract_he>דוגמנים לענות על שאלות (QA) משתמשים במערכות מערכות מערכת מערכת מערכת מערכת מערכת מערכת מערכת מערכת מערכת מערכת מערכת מערכת מערכת מערכת מערכת מערכת מערכת מע תלויה במידע האימוני על ידי מערכות QA יכולות להגדיל או לשקף אי-שוויון דרך התגובות שלהם. דוגמנים רבים של QA, כמו אלה עבור קבוצת מידע SQuAD, מאומנים ומבחנים על תחת קבוצת מאמרים בוויקיפדיה שמקודדים את ההתמחות שלהם, וגם משחזרים את אי-שוויון בעולם האמיתי. Understanding how training data affects bias in QA systems can inform methods to mitigate inequity.  אנחנו מפתחים שתי סדרות של שאלות לשאלות סגורות ופתוחות בתחום, בהתאם, אשר משתמשות בשאלות סביבות כדי לחקור דוגמנים QA למיוחד. אנחנו מאכילים שלושה מערכות QA מבוססות על למידה עמוקה עם קבוצות השאלות שלנו ולעריך תגובות למחיות דרך המטריות. בשימוש במטריקה שלנו, אנו מוצאים שדוגמנים QA של שטח פתוח מגבירים את ההתמחות יותר מאשר שותפיהם של שטח סגור ולהציע שהתמחות בשטח המחזיר בקלות יותר בגלל חופש בחירה גדול יותר.</abstract_he>
      <abstract_ha>Socket error code ConnectionTimeout Mutarin da za'a sami data na tsari da QA na iya ƙara ko kuma zai yi rabo da ɓarna a bayan majiɓincinsu. masu yawa na QA misãlai, kamar waɗand a ke tsaron SQuAD data, an karanta kuma aka jarraba a ƙarƙashin makala na Wikimedia wanda ke kodi sigarinsu da kuma yana dubuɗe masu daidaita duniya. Ana gane yadda data na ƙidãya za'a yi amfani da ɓangare cikin QA's system, za'a iya sanar da shiryoyin su cire kasancẽwa. Tuna buɗe sau biyu masu tambayi ga aka rufe da kuma aka buɗe masu tambayar kwamfyuta, wanda ke amfani da su masu sauna wa'anar QA dõmin ya jarraba misalin QA dõmin ya yi kuskure. Ana ciyar da QA's system uku masu ƙaranci da ke samun tambayarinmu kuma munã qiimata majiɓintan sauri a kan misalin ta. Using our metrics, we find that open-domain QA models amplify biases more than their closed-domain counterparts and propose that biases in the retriever surface more readily due to greater freedom of choice.</abstract_ha>
      <abstract_bo>འདྲི་ཚིག་ལན་གསལ(QA)མ་དཔེ་གཞུང་གིས་ལྟ་ཀློག་བྱེད་པ་དང་ལྟ་ཀློག་བྱེད་མི་ལག་ལེན་འཐབ་པ་ཡིན། QA་རིམ་ལག་གི་སྒེར་གྱི་གཙོ་སློང་གི་གནས་ཚུལ་དང་མཐུན་རྐྱེན་ཚད་ལ་ཆེ་རུ་གཏོང་དང་འགྱུར་བ་ཡིན། QA མ་དབྱིབས་མང་པོ་ཞིག་ཡིན། དཔེར་ན། SQuAD གནད་སྡུད་ཆ་འཕྲིན་གྱི་ནང་དུ་གཞི་སྒྲིག་ནས་བརྟག་ཞིབ་བྱེད་ཀྱི་ཡོད། QA་མ་ལག་གི་དབྱིབས་གཙོ་སློང་ཆ་ཡིག་ཆ་གིས་ཇི་ལྟར་འགྱུར་བའི་ནད་ཅིག་ནི་རྒྱུན་ལྡན་ཞིག We develop two sets of questions for closed and open domain questions respectively, which use ambiguous questions to probe QA models for bias. We feed three deep-learning-based QA systems with our question sets and evaluate responses for bias via the metrics. Using our metrics, we find that open-domain QA models amplify biases more than their closed-domain counterparts and propose that biases in the retriever surface more readily due to greater freedom of choice.</abstract_bo>
      </paper>
    <paper id="10">
      <title>Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer</title>
      <author><first>Ziqing</first><last>Yang</last></author>
      <author><first>Wentao</first><last>Ma</last></author>
      <author><first>Yiming</first><last>Cui</last></author>
      <author><first>Jiani</first><last>Ye</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Shijin</first><last>Wang</last></author>
      <pages>100–105</pages>
      <abstract>Multilingual pre-trained models have achieved remarkable performance on cross-lingual transfer learning. Some multilingual models such as mBERT, have been pre-trained on unlabeled corpora, therefore the embeddings of different languages in the <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> may not be aligned very well. In this paper, we aim to improve the zero-shot cross-lingual transfer performance by proposing a pre-training task named Word-Exchange Aligning Model (WEAM), which uses the statistical alignment information as the prior knowledge to guide cross-lingual word prediction. We evaluate our model on multilingual machine reading comprehension task MLQA and natural language interface task XNLI. The results show that <a href="https://en.wikipedia.org/wiki/WEAM">WEAM</a> can significantly improve the zero-shot performance.</abstract>
      <url hash="649f1b24">2021.mrqa-1.10</url>
      <bibkey>yang-etal-2021-bilingual</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.10</doi>
    </paper>
    <paper id="14">
      <title>Investigating Post-pretraining Representation Alignment for Cross-Lingual Question Answering</title>
      <author><first>Fahim</first><last>Faisal</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>133–148</pages>
      <abstract>Human knowledge is collectively encoded in the roughly 6500 languages spoken around the world, but <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> is not distributed equally across languages. Hence, for information-seeking question answering (QA) systems to adequately serve speakers of all languages, they need to operate cross-lingually. In this work we investigate the capabilities of multilingually pretrained language models on cross-lingual QA. We find that explicitly aligning the <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representations</a> across languages with a post-hoc finetuning step generally leads to improved performance. We additionally investigate the effect of data size as well as the language choice in this fine-tuning step, also releasing a dataset for evaluating cross-lingual QA systems.</abstract>
      <url hash="b36f7425">2021.mrqa-1.14</url>
      <bibkey>faisal-anastasopoulos-2021-investigating</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.14</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="15">
      <title>Semantic Answer Similarity for Evaluating Question Answering Models</title>
      <author><first>Julian</first><last>Risch</last></author>
      <author><first>Timo</first><last>Möller</last></author>
      <author><first>Julian</first><last>Gutsch</last></author>
      <author><first>Malte</first><last>Pietsch</last></author>
      <pages>149–157</pages>
      <abstract>The evaluation of question answering models compares <a href="https://en.wikipedia.org/wiki/Ground_truth">ground-truth annotations</a> with <a href="https://en.wikipedia.org/wiki/Prediction">model predictions</a>. However, as of today, this comparison is mostly lexical-based and therefore misses out on answers that have no lexical overlap but are still semantically similar, thus treating correct answers as false. This underestimation of the true performance of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> hinders user acceptance in applications and complicates a fair comparison of different <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Therefore, there is a need for an evaluation metric that is based on <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> instead of pure <a href="https://en.wikipedia.org/wiki/String_similarity">string similarity</a>. In this short paper, we present SAS, a cross-encoder-based metric for the estimation of semantic answer similarity, and compare it to seven existing <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>. To this end, we create an English and a German three-way annotated evaluation dataset containing pairs of answers along with human judgment of their <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a>, which we release along with an implementation of the SAS metric and the experiments. We find that semantic similarity metrics based on recent transformer models correlate much better with human judgment than traditional lexical similarity metrics on our two newly created datasets and one dataset from related work.</abstract>
      <url hash="af261719">2021.mrqa-1.15</url>
      <bibkey>risch-etal-2021-semantic</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.15</doi>
    </paper>
    </volume>
</collection>