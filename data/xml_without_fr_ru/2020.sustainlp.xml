<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.sustainlp">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</booktitle>
      <editor><first>Nafise Sadat</first><last>Moosavi</last></editor>
      <editor><first>Angela</first><last>Fan</last></editor>
      <editor><first>Vered</first><last>Shwartz</last></editor>
      <editor><first>Goran</first><last>Glavaš</last></editor>
      <editor><first>Shafiq</first><last>Joty</last></editor>
      <editor><first>Alex</first><last>Wang</last></editor>
      <editor><first>Thomas</first><last>Wolf</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="29071254">2020.sustainlp-1.0</url>
      <bibkey>sustainlp-2020-sustainlp</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Knowing Right from Wrong : Should We Use More Complex <a href="https://en.wikipedia.org/wiki/Mathematical_model">Models</a> for Automatic Short-Answer Scoring in Bahasa Indonesia?<fixed-case>B</fixed-case>ahasa <fixed-case>I</fixed-case>ndonesia?</title>
      <author><first>Ali Akbar</first><last>Septiandri</last></author>
      <author><first>Yosef Ardhito</first><last>Winatmoko</last></author>
      <author><first>Ilham Firdausi</first><last>Putra</last></author>
      <pages>1–7</pages>
      <abstract>We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring : single classical, <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble classical</a>, and <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>. The task is to classify given answers to two questions, whether they are right or wrong. While recent development shows increasing model complexity to push the benchmark performances, they tend to be resource-demanding with mundane improvement. For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble models</a> and Bi-LSTM model with pre-trained word2vec embedding from 200 million words. In this case, the single classical machine learning achieved less than 2 % difference in <a href="https://en.wikipedia.org/wiki/F-number">F1</a> compared to the deep learning approach with 1/18 time for model training.</abstract>
      <url hash="737436b0">2020.sustainlp-1.1</url>
      <doi>10.18653/v1/2020.sustainlp-1.1</doi>
      <video href="https://slideslive.com/38939419" />
      <bibkey>septiandri-etal-2020-knowing</bibkey>
    </paper>
    <paper id="3">
      <title>Learning Informative Representations of Biomedical Relations with Latent Variable Models</title>
      <author><first>Harshil</first><last>Shah</last></author>
      <author><first>Julien</first><last>Fauqueur</last></author>
      <pages>19–28</pages>
      <abstract>Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire corpus (pair-level). In both cases, recent methods have achieved strong results by learning a <a href="https://en.wikipedia.org/wiki/Point_estimation">point estimate</a> to represent the <a href="https://en.wikipedia.org/wiki/Binary_relation">relation</a> ; this is then used as the input to a relation classifier. However, the <a href="https://en.wikipedia.org/wiki/Binary_relation">relation</a> expressed in text between a pair of biomedical entities is often more complex than can be captured by a <a href="https://en.wikipedia.org/wiki/Point_estimate">point estimate</a>. To address this issue, we propose a <a href="https://en.wikipedia.org/wiki/Latent_variable_model">latent variable model</a> with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.</abstract>
      <url hash="cbaf98a3">2020.sustainlp-1.3</url>
      <doi>10.18653/v1/2020.sustainlp-1.3</doi>
      <video href="https://slideslive.com/38939422" />
      <bibkey>shah-fauqueur-2020-learning</bibkey>
      <pwccode url="https://github.com/BenevolentAI/RELVM" additional="false">BenevolentAI/RELVM</pwccode>
    </paper>
    <paper id="4">
      <title>End to End Binarized Neural Networks for Text Classification</title>
      <author><first>Kumar</first><last>Shridhar</last></author>
      <author><first>Harshil</first><last>Jain</last></author>
      <author><first>Akshat</first><last>Agarwal</last></author>
      <author><first>Denis</first><last>Kleyko</last></author>
      <pages>29–34</pages>
      <abstract>Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these <a href="https://en.wikipedia.org/wiki/Computer_network">networks</a> pose high requirements for <a href="https://en.wikipedia.org/wiki/Computer_hardware">computing hardware</a> and training budgets. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a <a href="https://en.wikipedia.org/wiki/Computer_network">network</a> is one way of addressing the issue of the increasing <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">complexity</a>. In this paper, we propose an end to end binarized neural network for the task of intent and text classification. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> are binarized. We demonstrate the efficiency of such a network on the intent classification of short texts over three <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and text classification with a larger dataset. On the considered <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, the proposed <a href="https://en.wikipedia.org/wiki/Computer_network">network</a> achieves comparable to the state-of-the-art results while utilizing 20-40 % lesser <a href="https://en.wikipedia.org/wiki/Computer_memory">memory</a> and training time compared to the benchmarks.</abstract>
      <url hash="ab07164c">2020.sustainlp-1.4</url>
      <attachment type="OptionalSupplementaryMaterial" hash="1f601b55">2020.sustainlp-1.4.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.sustainlp-1.4</doi>
      <video href="https://slideslive.com/38939423" />
      <bibkey>shridhar-etal-2020-end</bibkey>
    </paper>
    <paper id="5">
      <title>Exploring the Boundaries of Low-Resource BERT Distillation<fixed-case>BERT</fixed-case> Distillation</title>
      <author><first>Moshe</first><last>Wasserblat</last></author>
      <author><first>Oren</first><last>Pereg</last></author>
      <author><first>Peter</first><last>Izsak</last></author>
      <pages>35–40</pages>
      <abstract>In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on devices with limited resources is challenging due to the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>’ large <a href="https://en.wikipedia.org/wiki/Computation">computational consumption</a> and <a href="https://en.wikipedia.org/wiki/Computer_memory">memory requirements</a>. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios. Model distillation has shown promising results for reducing model size, <a href="https://en.wikipedia.org/wiki/Load_(computing)">computational load</a> and <a href="https://en.wikipedia.org/wiki/Data_efficiency">data efficiency</a>. In this paper we test the boundaries of BERT model distillation in terms of <a href="https://en.wikipedia.org/wiki/Data_compression">model compression</a>, inference efficiency and data scarcity. We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data. We also show that the <a href="https://en.wikipedia.org/wiki/Distillation">distillation</a> of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available.</abstract>
      <url hash="fd7284b7">2020.sustainlp-1.5</url>
      <doi>10.18653/v1/2020.sustainlp-1.5</doi>
      <video href="https://slideslive.com/38939426" />
      <bibkey>wasserblat-etal-2020-exploring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emotion">CARER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="6">
      <title>Efficient Estimation of Influence of a Training Instance</title>
      <author><first>Sosuke</first><last>Kobayashi</last></author>
      <author><first>Sho</first><last>Yokoi</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>41–47</pages>
      <abstract>Understanding the influence of a training instance on a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network model</a> leads to improving <a href="https://en.wikipedia.org/wiki/Interpretability">interpretability</a>. However, it is difficult and inefficient to evaluate the influence, which shows how a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s prediction would be changed if a training instance were not used. In this paper, we propose an efficient <a href="https://en.wikipedia.org/wiki/Methodology">method</a> for estimating the influence. Our method is inspired by dropout, which zero-masks a <a href="https://en.wikipedia.org/wiki/Subnetwork">sub-network</a> and prevents the <a href="https://en.wikipedia.org/wiki/Subnetwork">sub-network</a> from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a>.</abstract>
      <url hash="472a0c08">2020.sustainlp-1.6</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2924916b">2020.sustainlp-1.6.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.sustainlp-1.6</doi>
      <video href="https://slideslive.com/38939427" />
      <bibkey>kobayashi-etal-2020-efficient</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cifar-10">CIFAR-10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="7">
      <title>Efficient Inference For Neural Machine Translation</title>
      <author><first>Yi-Te</first><last>Hsu</last></author>
      <author><first>Sarthak</first><last>Garg</last></author>
      <author><first>Yi-Hsiu</first><last>Liao</last></author>
      <author><first>Ilya</first><last>Chatsviorkin</last></author>
      <pages>48–53</pages>
      <abstract>Large Transformer models have achieved state-of-the-art results in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize <a href="https://en.wikipedia.org/wiki/Time_complexity">inference speed</a> without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109 % and 84 % speedup on <a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a> and <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a> respectively and reduce the number of parameters by 25 % while maintaining the same translation quality in terms of BLEU.</abstract>
      <url hash="7ad2b4df">2020.sustainlp-1.7</url>
      <doi>10.18653/v1/2020.sustainlp-1.7</doi>
      <video href="https://slideslive.com/38939429" />
      <bibkey>hsu-etal-2020-efficient</bibkey>
    <title_ar>الاستدلال الفعال لترجمة الآلة العصبية</title_ar>
      <title_pt>Inferência eficiente para tradução automática neural</title_pt>
      <title_es>Inferencia eficiente para la traducción automática neuronal</title_es>
      <title_ja>神経機械翻訳のための効率的な推論</title_ja>
      <title_zh>神经机器翻译高效推理</title_zh>
      <title_hi>तंत्रिका मशीन अनुवाद के लिए कुशल अनुमान</title_hi>
      <title_ga>Tátal Éifeachtach d'Aistriúchán Meaisín Néarach</title_ga>
      <title_ka>Name</title_ka>
      <title_hu>Hatékony fertőzés a neurális gépi fordításhoz</title_hu>
      <title_it>Inferenza efficiente per la traduzione automatica neurale</title_it>
      <title_kk>Нейрондық машинаны аудару үшін әсер етілген қасиеттер</title_kk>
      <title_mk>Ефикасна инференција за превод на неврална машина</title_mk>
      <title_el>Αποτελεσματικό συμπέρασμα για τη νευρωνική μηχανική μετάφραση</title_el>
      <title_lt>Veiksmingas nervinių mašinų vertimas</title_lt>
      <title_ml>നെയുറല്‍ മെഷീന്‍ പരിഭാഷപ്പെടുത്തുന്നതിനുള്ള പ്രയോജനപ്പെടുത്തല്‍</title_ml>
      <title_mt>Efficient Inference For Neural Machine Translation</title_mt>
      <title_ms>Inferensi Efisien untuk Terjemahan Mesin Neural</title_ms>
      <title_mn>Сэтгэл машины хөрөнгө оруулалтын үр дүнтэй нөлөөлдөг</title_mn>
      <title_no>Effektiv forskjell for neuralmaskinsomsetjing</title_no>
      <title_pl>Skuteczne wnioski dla neuronowego tłumaczenia maszynowego</title_pl>
      <title_ro>Inferență eficientă pentru traducerea mașinii neurale</title_ro>
      <title_so>Efficient Inference For Neural machine Translation</title_so>
      <title_sr>Efektan pogodak za neuronski prevod mašine</title_sr>
      <title_si>න්‍යූරල් මැෂින් පරිවර්තනය වෙනුවෙන් ප්‍රශ්ණ විශ්වාස කරන්න</title_si>
      <title_ur>نیورال ماشین ترجمہ کے لئے اثر انفارنس</title_ur>
      <title_sv>Effektiv inferens för neural maskinöversättning</title_sv>
      <title_ta>புதிய இயந்திரத்தின் மொழிபெயர்ப்புக்கான விளைவு</title_ta>
      <title_uz>Tarjima qilish</title_uz>
      <title_vi>Sự liên hệ hiệu quả cho phiên dịch máy thần kinh</title_vi>
      <title_da>Effektiv inferens til neural maskinoversættelse</title_da>
      <title_nl>Efficiënte Inferentie voor Neurale Machine Translation</title_nl>
      <title_hr>Učinjena šteta za neuronski prevod strojeva</title_hr>
      <title_bg>Ефективно заключение за неврален машинен превод</title_bg>
      <title_de>Effiziente Schlussfolgerung für neuronale maschinelle Übersetzung</title_de>
      <title_fa>تفاوت فعالی برای ترجمه ماشین عصبی</title_fa>
      <title_id>Inferensi Efisien untuk Translation Mesin Neural</title_id>
      <title_ko>신경 기계 번역 중의 효율적인 추리</title_ko>
      <title_sq>Inferencë Efikase për Translacionin e Makinës Neurale</title_sq>
      <title_sw>Tafsiri yenye ufanisi</title_sw>
      <title_am>Efficient Inference For Neural Machine Translation</title_am>
      <title_af>Name</title_af>
      <title_hy>Նյարդային մեքենայի թարգմանման արդյունավետ ինֆերանսը</title_hy>
      <title_az>NĂ¶ral maĹźÄ±na Ă§evirilmÉ™si ĂĽĂ§ĂĽn ehtiyacÄ±</title_az>
      <title_bn>নিউরাল মেশিন অনুবাদের জন্য কার্যকর ইনফেরেন্স</title_bn>
      <title_tr>Neural Maşynyň terjimesine ýeterlik ýeterlik</title_tr>
      <title_bs>Učinjena šteta za neuronski prevod mašine</title_bs>
      <title_cs>Efektivní závěr pro neuronový strojový překlad</title_cs>
      <title_ca>Inferència eficient per a la traducció de màquines neurones</title_ca>
      <title_et>Tõhus järeldus neuroaalse masintõlke jaoks</title_et>
      <title_fi>Tehokas pĂ¤Ă¤telmĂ¤ hermojen konekĂ¤Ă¤nnĂ¶kselle</title_fi>
      <title_jv>layer-mode-effects</title_jv>
      <title_he>אינפרנסה יעילה לתרגום מכונת נוירולית</title_he>
      <title_sk>Učinkovita ugotovitev za strojno prevajanje nevronov</title_sk>
      <title_ha>@ action</title_ha>
      <title_bo>ནུས་ཡོད་པའི་མེ་འཁོར་གྱི་ལག་འཁྱེར་ལ་བསྒྱུར་ནུས་ཡོད་པ</title_bo>
      <abstract_ar>حققت نماذج المحولات الكبيرة أحدث النتائج في الترجمة الآلية العصبية وأصبحت قياسية في هذا المجال. في هذا العمل ، نبحث عن مزيج مثالي من الأساليب المعروفة لتحسين سرعة الاستدلال دون التضحية بجودة الترجمة. نجري دراسة تجريبية تجمع بين الأساليب المختلفة وتوضح أن الجمع بين استبدال الاهتمام الذاتي لوحدة فك التشفير بوحدات متكررة مبسطة ، واعتماد مشفر عميق وبنية وحدة فك ترميز ضحلة وتقليم الانتباه متعدد الرؤوس يمكن أن يحقق ما يصل إلى 109٪ و 84٪ تسريع على CPU و GPU على التوالي وتقليل عدد المعلمات بنسبة 25٪ مع الحفاظ على نفس جودة الترجمة من حيث BLEU.</abstract_ar>
      <abstract_pt>Grandes modelos de transformadores alcançaram resultados de última geração em tradução automática neural e se tornaram padrão no campo. Neste trabalho, procuramos a combinação ideal de técnicas conhecidas para otimizar a velocidade de inferência sem sacrificar a qualidade da tradução. Conduzimos um estudo empírico que empilha várias abordagens e demonstra que a combinação da substituição da autoatenção do decodificador por unidades recorrentes simplificadas, a adoção de um codificador profundo e uma arquitetura de decodificador rasa e a poda de atenção multi-head podem atingir até 109% e 84% de aceleração em CPU e GPU, respectivamente, e reduzem o número de parâmetros em 25%, mantendo a mesma qualidade de tradução em termos de BLEU.</abstract_pt>
      <abstract_es>Los modelos Transformer de gran tamaño han logrado resultados de vanguardia en la traducción automática neuronal y se han convertido en un estándar en el campo. En este trabajo, buscamos la combinación óptima de técnicas conocidas para optimizar la velocidad de inferencia sin sacrificar la calidad de la traducción. Llevamos a cabo un estudio empírico que combina varios enfoques y demuestra que la combinación de reemplazar la autoatención del decodificador con unidades recurrentes simplificadas, la adopción de un codificador profundo y una arquitectura de decodificador poco profunda y la reducción de atención de múltiples cabezales puede lograr una aceleración de hasta el 109% y el 84% en CPU y GPU respectivamente, y reducir el número de parámetros en un 25% manteniendo la misma calidad de traducción en términos de BLEU.</abstract_es>
      <abstract_ja>大型変圧器モデルは、ニューラル機械翻訳で最先端の結果を達成し、この分野で標準となっています。この研究では、翻訳品質を犠牲にすることなく推論速度を最適化するために、既知の技術の最適な組み合わせを探します。様々なアプローチを積み重ね、デコーダの自己注目を簡略化されたリカレントユニットに置き換え、深いエンコーダと浅いデコーダアーキテクチャを採用し、マルチヘッド注目の枝刈りを行うことで、CPUとGPUでそれぞれ最大109 ％と84 ％のスピードアップを達成し、BLEUの点で同じ翻訳品質を維持しながらパラメータの数を25 ％削減できることを実証する実証研究を行っています。</abstract_ja>
      <abstract_zh>大Transformer先神经机器翻译,以为率土。 于此之中,求已知之最,以不牺牲译优化推理速度。 一实考之,累累其方,证将解码器自意代为循环单元,用深度编码器浅层解码器架构及多头意修合,可于CPUGPU上各得达109%84%之速,并减参数数25%,兼同BLEU转质。</abstract_zh>
      <abstract_hi>बड़े ट्रांसफॉर्मर मॉडल ने तंत्रिका मशीन अनुवाद में अत्याधुनिक परिणाम प्राप्त किए हैं और क्षेत्र में मानक बन गए हैं। इस काम में, हम अनुवाद की गुणवत्ता का त्याग किए बिना अनुमान की गति को अनुकूलित करने के लिए ज्ञात तकनीकों के इष्टतम संयोजन की तलाश करते हैं। हम एक अनुभवजन्य अध्ययन करते हैं जो विभिन्न दृष्टिकोणों को ढेर करता है और यह दर्शाता है कि सरलीकृत आवर्तक इकाइयों के साथ डिकोडर आत्म-ध्यान को बदलने का संयोजन, एक गहरी एन्कोडर और एक उथले डिकोडर आर्किटेक्चर और मल्टी-हेड ध्यान प्रूनिंग को अपनाने से सीपीयू और जीपीयू पर क्रमशः 109% और 84% स्पीडअप प्राप्त हो सकता है और BLEU के संदर्भ में एक ही अनुवाद गुणवत्ता को बनाए रखते हुए पैरामीटर की संख्या को 25% तक कम किया जा सकता है।</abstract_hi>
      <abstract_ga>Tá torthaí úrscothacha bainte amach ag samhlacha Trasfhoirmeoirí Móra san aistriúchán meaisín néarach agus tá siad tar éis éirí caighdeánach sa réimse. Sa saothar seo, féachaimid don chomhcheangal is fearr de theicnící aitheanta chun an luas tátail a bharrfheabhsú gan caighdeán an aistriúcháin a íobairt. Déanaimid staidéar eimpíreach a chruann cineálacha cur chuige éagsúla agus a thaispeánann gur féidir suas le 109% agus 84% luas suas le 109% agus 84% a bhaint amach le hionadú féin-aird an díchódóra le haonaid athfhillteacha simplithe, trí ionchódóir domhain agus ailtireacht díchódóra éadomhain a ghlacadh. LAP agus GPU faoi seach agus laghdaítear líon na bparaiméadar faoi 25% agus an caighdeán aistriúcháin céanna á choinneáil i dtéarmaí BLEU.</abstract_ga>
      <abstract_ka>დიდი ტრანფორმენტერის მოდელები გავაკეთეთ სიცოცხლის შედეგები ნეიროლური მაქინის გაგრძელებაში და გავაკეთეთ სტანდარტულება. ამ სამუშაოში, ჩვენ ძირებთ უცნობილი ტექნოგიების ოპტიმალური კომბინეცია, რომ ინფრენციის სიჩქარე ოპტიმიზრებად დავიწყებთ, უცნობიერებელი გა ჩვენ ემპერიკალური სწავლობას, რომელიც განსხვავებული მიზეზების შესაძლებლობა დავწყება და გამოჩვენება, რომ სწორედ განსხვავებული განსხვავებული განსხვავებული განსხვავებული ერთეზ შეიძლება გავაკეთოთ დიბოლო კოდერი და დიბოლო კოდერის აქტიქტურაცია და მრავალთან დაახლოების აღმოჩენა 109% და 84% სიჩქარე CPU და GPU-ზე და 25% პარამეტრის რაოდენობას დაახლოებით, როცა BLEU-ის განმავლობაში იგივე გადაწყვეტილების</abstract_ka>
      <abstract_el>Τα μεγάλα μοντέλα μετασχηματιστών έχουν επιτύχει αποτελέσματα τελευταίας τεχνολογίας στη νευρωνική μηχανική μετάφραση και έχουν γίνει πρότυπο στον τομέα. Σε αυτή την εργασία, αναζητούμε τον βέλτιστο συνδυασμό γνωστών τεχνικών για τη βελτιστοποίηση της ταχύτητας συμπερασμάτων χωρίς να θυσιάζουμε την ποιότητα της μετάφρασης. Διεξάγουμε μια εμπειρική μελέτη που συσσωρεύει διάφορες προσεγγίσεις και αποδεικνύει ότι ο συνδυασμός αντικατάστασης της αυτοπροσοχής του αποκωδικοποιητή με απλοποιημένες επαναλαμβανόμενες μονάδες, υιοθετώντας έναν βαθύ κωδικοποιητή και μια ρηχή αρχιτεκτονική αποκωδικοποιητή και κλαδέματος προσοχής πολλαπλών κεφαλών μπορούν να επιτύχουν έως 109% και 84% επιτάχυνση στην ΚΜΕ και τη GPU αντίστοιχα και να μειώσουν τον αριθμό των παραμέτρων κατά 25% διατηρώντας την ίδια ποιότητα μετάφρασης όσον αφορά την BLEU.</abstract_el>
      <abstract_hu>A nagy transzformátor modellek korszerű eredményeket értek el a neurális gépi fordításban, és szabványossá váltak a területen. Ebben a munkában az ismert technikák optimális kombinációját keressük, hogy optimalizáljuk a következtetési sebességet anélkül, hogy feláldoznánk a fordítási minőséget. Egy empirikus tanulmányt végzünk, amely különböző megközelítéseket halmoz össze, és bebizonyítja, hogy a dekóder önfigyelem helyettesítése egyszerűsített visszatérő egységekkel, A mélykódoló és a sekély dekódoló architektúra alkalmazása, valamint a többfejű figyelem levágása akár 109%-os, illetve 84%-os gyorsulást érhet el a CPU és a GPU esetében, és 25%-kal csökkentheti a paraméterek számát, miközben ugyanazon fordítási minőséget tart fenn a BLEU tekintetében.</abstract_hu>
      <abstract_lt>Dideli transformatoriai pasiekė pažangiausius rezultatus, susijusius su nervinių mašin ų vertimu, ir tapo standartiniais šioje srityje. Šiame darbe siekiame optimalaus žinomų metodų derinio siekiant optimizuoti išvados greitį nepažeidžiant vertimo kokybės. Atliekame empirinį tyrimą, kuriame nustatomi įvairūs metodai ir įrodoma, kad dekoderių savarankiškumo pakeitimas supaprastintais pakartotiniais vienetais, Priėmus gilų kodavimo kodą ir plokščią dekoderių architektūrą bei daugiakalbį dėmesį, galima atitinkamai pasiekti iki 109 % ir 84 % greičio CPU ir GPU ir sumažinti parametrų skaičių 25 %, išlaikant tą pačią vertimo kokybę BLEU atžvilgiu.</abstract_lt>
      <abstract_mk>Големите трансформски модели постигнаа најсовремени резултати во преводот на невропските машини и станаа стандардни на теренот. Во оваа работа бараме оптимална комбинација на познати техники за оптимизација на брзината на конференцијата без жртвување на квалитетот на превод. Правиме емпириска студија која собира различни пристапи и демонстрира дека комбинацијата на замена на самото внимание на декодерот со едноставни рецидентни единици, Примената на длабокиот кодер и ниска архитектура на декодерот и привлекувањето на повеќето глави на вниманието може да достигне до 109 отсто и 84 отсто брзина на процесорот и GPU, односно, и да го намали бројот на параметри за 25 отсто, при што ќе се одржи истиот квалитет на превод во поглед на</abstract_mk>
      <abstract_kk>Үлкен түрлендіру үлгілері невралдық компьютердің аудармасының күйін жеткізді және өрісте стандартты болды. Бұл жұмыста біз белгілі техникалардың оптималдығын іздейміз, аудармалардың сапасын көмектесу үшін инференциялық жылдамдығын оптимизациялау үшін. Біз әртүрлі жағдайларды топтастырып, декодтардың өзіне қайталанатын бірліктермен ауыстыруын көрсетеді. Тіпті кодерді және көпшілік декодер архитектурасын қолдану және көпшілік басып тұрғысын қолдану мүмкін процессорды және GPU арқылы 109% және 84% жылдамдығына жеткізе алады және бір аудармалы сапатты BLEU арқылы қалаған параметрлердің санын 25% деп аза</abstract_kk>
      <abstract_ms>Model Transformer Besar telah mencapai keputusan-state-of-the-art dalam terjemahan mesin saraf dan telah menjadi piawai dalam medan. Dalam kerja ini, kita mencari kombinasi optimal teknik yang diketahui untuk optimize kelajuan kesimpulan tanpa mengorbankan kualiti terjemahan. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, Mengadopsi pengekod dalam dan arkitektur pengekod rendah dan pemotongan perhatian berbilang-kepala boleh mencapai hingga 109% dan 84% kecepatan pada CPU dan GPU secara berdasarkan dan mengurangkan bilangan parameter dengan 25% sementara menjaga kualiti terjemahan yang sama dalam terma BLEU.</abstract_ms>
      <abstract_it>I modelli Large Transformer hanno raggiunto risultati all'avanguardia nella traduzione automatica neurale e sono diventati standard nel campo. In questo lavoro, cerchiamo la combinazione ottimale di tecniche conosciute per ottimizzare la velocità di inferenza senza sacrificare la qualità della traduzione. Conduciamo uno studio empirico che impila vari approcci e dimostra che la combinazione di sostituire l'auto-attenzione decodificatore con unità ricorrenti semplificate, L'adozione di un encoder profondo e di un'architettura decodificatore poco profonda e la potatura dell'attenzione multi-testa può raggiungere una velocità fino al 109% e all'84% rispettivamente su CPU e GPU e ridurre il numero di parametri del 25% mantenendo la stessa qualità di traduzione in termini di BLEU.</abstract_it>
      <abstract_ml>വലിയ ട്രാന്‍സ്ഫോര്‍മാന്‍സ്ഫോര്‍മാറ്റര്‍ മോഡലുകള്‍ ന്യൂറല്‍ മെഷീന്‍ പരിഭാഷണത്തിന്റെ അവസ്ഥ നേരിട്ടുണ്ട്. പിന്നെ  ഈ ജോലിയില്‍, പരിചയപ്പെട്ട ട ടെക്നിക്കങ്ങളുടെ ഐപ്റ്റമില്ലാത്ത ഒരുമിച്ചിട്ടുണ്ടാക്കാന്‍ ഞങ്ങള്‍ നോക്കുന്നു,  വ്യത്യസ്ത വഴികള്‍ സ്ഥാപിക്കുന്നു എന്നിട്ട് സ്വയം ആത്മാര്‍ത്ഥ്യം മാറ്റുന്നതിനുള്ള സ്വയം ശ്രദ്ധ കൂട്ടുന്നതിനെ കൂട്ടി ഒരു ആഴത്തെ കോഡെര്‍ എടുക്കുന്നതും ഒരു തണുത്ത ഡെക്കോഡേര്‍ ആര്‍ക്കെക്ട്രെക്റ്റിക്കേറ്റര്‍ ശ്രദ്ധ കാണിക്കുന്നതും സിപിയുവിലും 84% വേഗത്തില്‍ എത്തുന്നതും ബിലിയുവിന്റെ വിഭാ</abstract_ml>
      <abstract_no>Stor transformeringsmodeller har oppnådd tilstanden av kunsten i omsetjinga av neuralmaskina og har blitt standard i feltet. I dette arbeidet ser vi etter optimalt kombinasjon av kjende teknikk for å optimalisera infeksjonsfartet utan å oftasta omsetjingskvalitet. Vi gjer eit empirisk studie som stakkar ulike tilnærmingar og demonstrerer at kombinasjonen av å byta ut dekoder selvmerksomhet med enkelte rekurserte einingar, Eit dyp koder og ein sårba dekoderarkitektur og fleire hovuddekoder kan oppnå opp til 109 % og 84 % raskare på CPU og GPU, og redusere talet på parametrar med 25 % mens det gjeld samme omsetjingskvalitet under BLEU.</abstract_no>
      <abstract_mt>Il-mudelli tat-Transformer il-kbar kisbu riżultati l-aktar avvanzati fit-traduzzjoni tal-magni newrali u saru standard fil-qasam. F’dan ix-xogħol, aħna qed ifittxu l-aħjar kombinazzjoni ta’ tekniki magħrufa biex nimmassimizzaw il-veloċità ta’ inferenza mingħajr ma nisakrifikaw il-kwalità tat-traduzzjoni. Għandna nagħmlu studju empiriku li jimpjega diversi approċċi u juri li l-kombinazzjoni ta’ sostituzzjoni tal-awtonomija tad-dekoder b’unitajiet rikorrenti ssimplifikati, L-adozzjoni ta’ kodifikatur profond u arkitettura ta’ dekoder baxx u pruning ta’ attenzjoni b’ħafna ras jistgħu jilħqu sa 109% u 84% velodup fuq CPU u GPU rispettivament u jnaqqsu n-numru ta’ parametri b’25% filwaqt li jżommu l-istess kwalità ta’ traduzzjoni f’termini ta’ BLEU.</abstract_mt>
      <abstract_mn>Том шилжүүлэгч загварууд мэдрэлийн машины хөрөнгө оруулалт болон стандарт болж байна. Энэ ажлын тулд бид мэддэг техникуудын хамтдаа халдварын хурдыг илүү сайжруулахын тулд илүү сайжруулагддаг. Бид өөр өөр арга барилгыг багтаж, өөрийн анхаарлыг хялбарчлан дахин дахин дахин дахин дахин дахин дахин дахин анхаарлаа орлуулж, Гүн гүнзгий кодер болон гүнзгий декодер архитектур болон олон толгой анхаарлын удирдлага нь CPU болон GPU дээр хурдан 109% болон 84% хүртэл хүрэх боломжтой болно. БЛЕУ-ын хувьд адилхан орчуулах чадварыг 25% багасгаж байна.</abstract_mn>
      <abstract_pl>Duże modele transformatorów osiągnęły najnowocześniejsze wyniki w neuronowym tłumaczeniu maszynowym i stały się standardem w tej dziedzinie. W niniejszej pracy poszukujemy optymalnego połączenia znanych technik w celu optymalizacji szybkości wnioskowania bez obniżania jakości tłumaczenia. Przeprowadzamy badanie empiryczne, które łączy różne podejścia i pokazuje, że połączenie zastępowania samoobserwacji dekodera uproszczonymi jednostkami powtarzającymi się, Zastosowanie głębokiego kodera i płytkiej architektury dekodera oraz wielogłowicowego przycinania uwagi może osiągnąć do 109% i 84% przyspieszenie procesora i GPU odpowiednio oraz zmniejszyć liczbę parametrów o 25% przy zachowaniu tej samej jakości tłumaczenia pod względem BLEU.</abstract_pl>
      <abstract_ro>Modelele de transformare mari au obținut rezultate de ultimă oră în traducerea mașinii neurale și au devenit standard în domeniu. În această lucrare, căutăm combinația optimă de tehnici cunoscute pentru a optimiza viteza inferenței fără a sacrifica calitatea traducerii. Realizăm un studiu empiric care stivuiește diferite abordări și demonstrează că combinația de înlocuire a auto-atenției decodorului cu unități recurente simplificate, Adoptarea unui encoder profund și a unei arhitecturi de decodare superficială și reducerea atenției cu mai multe capete pot atinge o viteză de până la 109% și, respectiv, 84% pe CPU și GPU și reduce numărul de parametri cu 25%, menținând în același timp aceeași calitate a traducerii în ceea ce privește BLEU.</abstract_ro>
      <abstract_so>Tusaale wayn oo turjubaal ah waxay gaadheen xaalad-farshaxan, waxayna ka heleen tarjumaadda maskinada neurada ah, waxayna noqdeen standard duurka. Shaqodaas waxaynu raadinnaa iskuulka suurtagalka ah ee loo yaqaan si aan u faa’iideyn karin dhaqdhaqaaq la’aanta tarjumaadda. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, Waxyaabaha aad u dheer kartid iyo dhismaha aad u deynta iyo xannaaneynta madaxa kala duduwanba waxay gaadhi karaan ugu badnaan 109% iyo 84% si gaar ah CPU iyo GPU, waxayna hoos u dhigi karaan tirada tirada ah 25% inta ay xajisanayso isku qiimaha tarjumaadka oo ah BLEU.</abstract_so>
      <abstract_si>විශාල ප්‍රවර්තනයක් විදිහට ප්‍රමාණයක් ලැබුනා න්‍යූරල් මැෂින් පරිවර්තනයේ ස්ථිතිය- of- the- art ප්‍රතිප මේ වැඩේ අපි හොයාගෙන ඉන්නේ දන්න තාක්ෂණිකාවගේ හොඳම සම්බන්ධයක් හොයාගෙන ඉන්නේ අනුවාර්ථ වේගයක් නැති  අපි ඉම්පිරිකාලික අධ්‍යානයක් කරනවා වගේම විවිදියට ප්‍රතික්‍රියා කරනවා ඒ වගේම ප්‍රතික්‍රියා කරනවා කියලා සාමාන්‍ය ව ගොඩක් ඇන්කෝඩර් එකක් සහ ගොඩක් ඩිකෝඩර් ස්ථාපනයක් සහ ගොඩක් හෙඩක් අවධානයක් ඉන්න පුළුවන් CPU සහ GPU වලින් ඉක්මනට 109% සහ 84% ඉක්මනට සම්පූර්ණයෙන් ඉන්න සහ 25%</abstract_si>
      <abstract_ta>பெரிய மாற்று மாற்றும் மாதிரிகள் நிலைமை- கலை மாற்றியமைத்து புதிய இயந்திரம் மொழிமாற்றி புலத்தில் இயல்பான மாத இந்த வேலையில், நாம் தெரியும் தொழில்நுட்பத்தின் வேகத்தை அதிகப்படுத்த வேண்டும் என்று தேடுகிறோம் மொழிபெயர்ப் நாம் சுலபமான திரும்ப அலகுகளை மாற்றும் குறியீட்டு தன்னுடைய கவனத்தை மாற்றும் குறியீட்டை மாற்றும் சுலபமான திரும்ப அலக ஒரு ஆழமான குறியீட்டு மற்றும் ஒரு மழுமையான குறியீட்டாளர் அமைப்பு மற்றும் பல தலைப்பு கவனத்தை புரிந்து பிபியு மற்றும் GPU மற்றும் 84% வேகத்தை பெற முடியும் மற்றும் பிலியூவி</abstract_ta>
      <abstract_sr>Veliki modeli transformera postigli su rezultate umetnosti u prevodu neuralne mašine i postali su standardni na terenu. U ovom poslu tražimo optimalnu kombinaciju poznatih tehnika da optimiziramo brzinu infekcije bez žrtvovanja kvalitete prevoda. Vodimo empiričko ispitivanje koje sastavlja različite pristupe i pokazuje da kombinacija zamjene samopouzdanja dekodera sa jednostavnim povratnim jedinicama, Prihvaćanje dubokog kodera i plitkog arhitektura dekodera i višeglavnog obrezanja pažnje može postići do 109% i 84% ubrzanja na CPU i GPU, te smanjiti broj parametara za 25% dok održavaju istu kvalitet prevođenja u smislu BLEU-a.</abstract_sr>
      <abstract_sv>Stora Transformermodeller har uppnått toppmoderna resultat inom neural maskinöversättning och har blivit standard inom området. I detta arbete letar vi efter den optimala kombinationen av kända tekniker för att optimera inferenshastigheten utan att offra översättningskvaliteten. Vi genomför en empirisk studie som staplar olika tillvägagångssätt och visar att kombinationen av att ersätta avkodare självuppmärksamhet med förenklade återkommande enheter, Genom att använda en djup kodare och en ytlig avkodningsarkitektur och flerskalig uppmärksamhet kan man uppnå upp till 109% respektive 84% snabbare på CPU respektive GPU och minska antalet parametrar med 25% samtidigt som man bibehåller samma översättningskvalitet när det gäller BLEU.</abstract_sv>
      <abstract_ur>بڑے ترنسفورر نمڈلوں نے نئورل ماشین ترجمہ کے نتیجے پہنچ گئے ہیں اور کھیل میں استاندارڈ ہوگئے ہیں. اس کام میں، ہم جانے والی تکنیک کی اچھی ترکیب کے لئے تلاش کرتے ہیں کہ اس کے ذریعہ مہربانی کی سرعت مہربانی کریں بغیر ترکیب کی کیفیت کے۔ ہم ایک مصریح تحقیق کرتے ہیں جو مختلف طریقوں کو ٹکڑے رکھتا ہے اور دکھاتے ہیں کہ دکور کی اپنا توجه سادھا دوبارہ واحدوں سے بدل دینے کی ترکیب ہے، سی پی یو اور جی پی یو پر چڑھا ہوا تھا اور ایک گہرے ڈیکوڈر معماری اور بہت سی سروں کی توجه پرینگ کے ذریعے سی پی یو اور جی پی یو پر چڑھا ہوا تھا اور اس کی تعداد 25% کے ذریعے کم کر سکتا ہے جب کہ بلیو کے اندر ایک ہی ترجمہ کیفیت کی حفاظت کرتی ہے۔</abstract_ur>
      <abstract_uz>Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field.  Bu vazifanda, biz tanlangan teknikalarning optimal birlashtirishni istaysizmi, tarjima sifatini tozalash mumkin. Biz bir tashkilotni bajaramiz, har xil usullarni qo'yish va o'sha narsalarni o'zgartirish qo'shimcha o'zimni o'zgartirish bilan soddalashtirish uchun o'zgartirish mumkin, AQSH</abstract_uz>
      <abstract_vi>Các mô hình biến hình lớn đã đạt được kết quả tối tân trong việc dịch chuyển cỗ máy thần kinh và trở thành tiêu chuẩn trên chiến trường. Trong công việc này, chúng tôi tìm kiếm sự kết hợp tối ưu tiên của kỹ thuật được biết để tối ưu tiên tốc độ nhận biết mà không hy sinh chất lượng dịch. Chúng tôi thực hiện một nghiên cứu có kinh nghiệm mang theo nhiều phương pháp khác nhau và chứng minh rằng kết hợp của việc thay thế bộ lọc bằng đơn vị thường xuyên, Một máy mã hóa sâu và một cấu trúc giải mã nông cạn và việc cắt giảm tập trung đa đầu có thể tăng tốc lên đến 109=và84=.='trên hai bộ vi xử lý cộng cộng đồng và giảm bớt số lượng các tham số bằng 25t. Trong khi vẫn giữ nguyên chất dịch bản phân loại tương tự.</abstract_vi>
      <abstract_da>Store Transformermodeller har opnået state-of-the-art resultater inden for neural maskinoversættelse og er blevet standard på området. I dette arbejde leder vi efter den optimale kombination af kendte teknikker til at optimere inferencehastigheden uden at gå på kompromis med oversættelseskvaliteten. Vi gennemfører en empirisk undersøgelse, der stabler forskellige tilgange og demonstrerer, at kombinationen af at erstatte dekoder selvopmærksomhed med forenklede tilbagevendende enheder, Vedtagelse af en dyb encoder og en lavvandet dekoderkarkitektur og opmærksomhedsbeskæring med flere hoveder kan opnå op til 109% og 84% hastighed på henholdsvis CPU og GPU og reducere antallet af parametre med 25% samtidig med at den samme oversættelseskvalitet med hensyn til BLEU opretholdes.</abstract_da>
      <abstract_de>Große Transformatormodelle haben modernste Ergebnisse in der neuronalen maschinellen Übersetzung erzielt und sind in diesem Bereich Standard geworden. In dieser Arbeit suchen wir nach der optimalen Kombination bekannter Techniken, um die Inferenzgeschwindigkeit zu optimieren, ohne die Übersetzungsqualität zu beeinträchtigen. Wir führen eine empirische Studie durch, die verschiedene Ansätze stapelt und zeigt, dass die Kombination aus dem Ersetzen der Selbstaufmerksamkeit des Decoders durch vereinfachte wiederkehrende Einheiten, Durch die Verwendung eines tiefen Encoders und einer flachen Decoderarchitektur und des Aufmerksamkeitsschnitts mit mehreren Köpfen kann eine Beschleunigung von bis zu 109% bzw. 84% auf CPU und GPU erreicht und die Anzahl der Parameter um 25% reduziert werden, während die Übersetzungsqualität in Bezug auf BLEU beibehalten wird.</abstract_de>
      <abstract_bg>Големите трансформаторни модели са постигнали най-съвременни резултати в невронния машинен превод и са станали стандарт в областта. В тази работа търсим оптималната комбинация от известни техники за оптимизиране на скоростта на заключение, без да жертваме качеството на превода. Извършваме емпирично проучване, което подрежда различни подходи и демонстрира, че комбинацията от заместване на декодерното самовнимание с опростени повтарящи се единици, приемането на дълбок кодер и повърхностна декодерна архитектура и подрязването на вниманието с няколко глави може да постигне до 109% и 84% ускорение съответно на процесора и графичния процесор и да намали броя на параметрите с 25%, като същевременно поддържа същото качество на превода по отношение на Блеу.</abstract_bg>
      <abstract_id>Model Transformer Besar telah mencapai hasil terbaik dalam terjemahan mesin saraf dan telah menjadi standar di lapangan. Dalam pekerjaan ini, kita mencari kombinasi optimal dari teknik yang dikenal untuk optimisasi kecepatan inferensi tanpa mengorbankan kualitas terjemahan. Kami melakukan sebuah studi empiris yang mengumpulkan berbagai pendekatan dan menunjukkan bahwa kombinasi menggantikan perhatian diri dekoder dengan unit rekuren sederhana, Mengadopsi koder dalam dan arsitektur dekoder rendah dan pemotongan perhatian multi-kepala dapat mencapai sampai 109% dan 84% speedup pada CPU dan GPU secara respektif dan mengurangi jumlah parameter dengan 25% sementara mempertahankan kualitas terjemahan yang sama dalam terma BLEU.</abstract_id>
      <abstract_nl>Grote Transformatormodellen hebben state-of-the-art resultaten behaald in neurale machinevertaling en zijn standaard in het veld geworden. In dit werk zoeken we naar de optimale combinatie van bekende technieken om de inferentiesnelheid te optimaliseren zonder afbreuk te doen aan de vertaalkwaliteit. We voeren een empirische studie uit die verschillende benaderingen stapelt en aantoont dat de combinatie van het vervangen van decoder zelfaandacht door vereenvoudigde terugkerende eenheden, Door gebruik te maken van een diepe encoder en een ondiepe decoderarchitectuur en multi-head attentie snoeien kan een versnelling tot 109% en 84% van respectievelijk CPU en GPU worden bereikt en het aantal parameters met 25% worden verminderd terwijl dezelfde vertaalkwaliteit in termen van BLEU wordt gehandhaafd.</abstract_nl>
      <abstract_hr>Veliki modeli transformera postigli su rezultate umjetnosti u prevodu neuralnih strojeva i postali su standardni na terenu. U ovom poslu tražimo optimalnu kombinaciju poznatih tehnika da optimiziramo brzinu infekcije bez žrtvovanja kvalitete prevoda. Provodimo empiričko ispitivanje koje sastavlja različite pristupe i pokazuje da kombinacija zamjene samopouzdanja dekodera sa jednostavnim povratnim jedinicama, Prihvaćanje dubokog kodera i plitkog arhitektura dekodera i višeglavnog obrezanja pažnje može postići do 109% i 84% ubrzanja na CPU i GPU, te smanjiti broj parametara za 25% dok održavaju istu kvalitet prevoda u smislu BLEU-a.</abstract_hr>
      <abstract_sw>Mradi mkubwa wa Tafsiri umefanikiwa kuwa na hali ya sanaa na matokeo ya utafsiri wa mashine ya kisasa na yamekuwa kiwango cha kawaida katika uwanja. Katika kazi hii, tunatafuta muunganiko bora wa mbinu zinazofahamika kuboresha kiwango cha uchunguzi bila kutoa sifa za tafsiri. Tunafanya utafiti wa msisitizo ambao unaweka hatua mbalimbali na kuonyesha kuwa muunganiko wa kubadilisha nafuu kwa vitengo rahisi vya kurudi, Kwa kuchukua kiwango cha ndani na ujenzi mdogo wa decodi na uchunguzi wa kichwa vingi unaweza kufikia asilimia 109 na asilimia 84 kwa kiwango cha CPU na GPU na kupunguza idadi ya parameter kwa asilimia 25 wakati wakiendelea kutangaza kiwango hicho cha tafsiri kwa mujibu wa BLEU.</abstract_sw>
      <abstract_af>Name In hierdie werk, ons soek na die optimale kombinasie van bekende teknike om inferensie spoed te optimaliseer sonder om vertaling kwaliteit te offer. Ons doen 'n empiriese studie wat verskeie toegange stap en wys dat kombinasie van vervanging van dekoder self-aandag met eenvoudige herhaalde eenhede, Die aanvaar van 'n diep enkoder en 'n skaal dekoder-arkitektuur en meer-kop aanmerking kan tot 109% en 84% speeduig op CPU en GPU aanvaar en die nommer van parameters met 25% verduur terwyl die selfde vertaling-kwaliteit in terms van BLEU onderhou.</abstract_af>
      <abstract_tr>Ullakan Transformer nusgalary neural maşynyň terjimesinde ýetip bardylar we sahypada standart boldy. Bu işde, biz bilinen teknikleriň optimal birleşigini terjime etmek üçin azalyş ýigrimizi bejermek üçin gözleýäris. Biz empirik bir aralygy ýerine ýetirýäris we munuň birnäçe nusgalaryny ýerleşdirýändigini görkezýäris. Garyp ködleme we çukultyk arhitektura we köp kelläp üns berişi 109% we 84% CPU we GPU-a süýtgelip biler we ayn terjime howpsaly BLEU-a garaşyp biler parameterleriň sanyny 25% tarapyndan azaltyp biler.</abstract_tr>
      <abstract_fa>مدل‌های تغییر‌پذیر بزرگ به حالت هنر نتیجه‌های تغییر‌پذیر ماشین عصبی رسیده‌اند و در زمینه استاندارد شده‌اند. در این کار، ما دنبال ترکیب بهترین تکنیک‌های شناخته می‌گردیم تا سرعت آلودگی را بدون قربانی کیفیت ترکیب بهترین کنیم. ما یک مطالعه امپراتیک را انجام می دهیم که تقریبا مختلف را جمع می کند و نشان می دهد که ترکیب توجه خود را با واحدهای ساده تکرار می دهد، با پذیرفتن یک کودهر عمیق و یک معماری دکوردر عمیق و حفظ توجه بسیاری از سرها می تواند تا 109 درصد و 84 درصد سرعت بر CPU و GPU را به طور مستقل رسید و تعداد پارامتر را به 25 درصد کاهش دهد در حالی که با حفظ یک کیفیت ترجمه را به عنوان BLEU نگه می دارد.</abstract_fa>
      <abstract_ko>대형 변압기 모형은 신경기계 번역 분야에서 가장 선진적인 성과를 거두었고 이미 이 분야의 표준이 되었다.이 작업에서 우리는 이미 알고 있는 기술의 가장 좋은 조합을 찾아 추리 속도를 최적화하는 동시에 번역의 질을 희생하지 않는다.우리는 실증 연구를 실시하여 각종 방법을 총결하고 간소화된 중복 단원으로 자신의 주의를 대체하는 것을 증명했다.깊이 인코더와 얕은 디코더 구조, 다중 주의 가지치기로 각각 CPU와 GPU에서 109%와 84%의 가속을 실현하고 파라미터 수량을 25% 줄이는 동시에 BLEU에서 같은 번역 품질을 유지할 수 있다.</abstract_ko>
      <abstract_az>Büyük Transformer modelləri nöral maşına çevirilməsi ilə mümkün olduğu və sahədə standart oldular. Bu işdə, biz bilinmiş tekniklərin optimal kombinatsiyasını istəyirik ki, dəyişiklik sürətini qurbanlıq etmədən optimizləsin. Biz müxtəlif yaxınlıqları birləşdirən empirik təhsil etdik və dekoderin özünü təhsil etməsini basit təhsil edilən biriklərlə dəyişdiririk. Dərzini kodlayıcı və çətinli dekoder arhitektarını və çoxlu başlıqların gözləməsi CPU və GPU ilə müqayisədə 109%-ə və 84%-ə hızlandıra bilər və aynı tercümə keyfiyyətini BLEU ilə qoruyarkən parametru sayını 25%-ə əskilə bilər.</abstract_az>
      <abstract_sq>Modelet e mëdha të Transformës kanë arritur rezultate më të larta në përkthimin e makinave nervore dhe janë bërë standarde në fushë. Në këtë punë, ne kërkojmë kombinimin optimal të teknikave të njohura për të optimizuar shpejtësinë e përfundimit pa sakrifikuar cilësinë e përkthimit. Ne kryejmë një studim empirik që grumbullon metoda të ndryshme dhe demonstron se kombinimi i zëvendësimit të vetëvëmendjes së dekoderit me njësitë e thjeshta të përsëritura, - miratimi i një kodifikuesi të thellë dhe një arkitekture dekoderi të thellë dhe shtrëngimi i vëmendjes me shumë koka mund të arrijë deri në 109% dhe 84% shpejtësi respektivisht në CPU dhe GPU dhe të reduktojë numrin e parametrave me 25% duke mbajtur të njëjtën cilësi përkthimi në termat e BLEU.</abstract_sq>
      <abstract_bn>Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field.  এই কাজে আমরা পরিচিত প্রযুক্তির সম্মিলনের অপেক্ষা করছি অনুবাদের মান ছাড়া ইনভেন্সের গতি বৃদ্ধি করার জন্য। আমরা একটি সম্মানিত গবেষণা করি যা বিভিন্ন উপায় স্থাপন করে এবং প্রদর্শন করে যে সুস্পষ্ট পুনরাবর্তন ইউনিটের সাথে নিজেকে আত্মমনোযোগ প্রতিস একটি গভীর এনকোডার গ্রহণ করা এবং একটি ধুলো কোডার আর্কিডেক্টার এবং বহুমাথার মনোযোগ প্রদান করা যায়, সিপিউ এবং জিপিউ-তে প্রায় ১০৯% এবং ৮৪% বেড়ে যাবে এবং বিলিউ এর মাধ্যমে একই অনুবাদের মান</abstract_bn>
      <abstract_am>ትልቅ ትርጉም ሚድልቶች የ-የ-አርእስት ግንኙነትን አግኝተዋል እና በሜዳው የተመሳሳይ ሆኖአል፡፡ በዚህ ስራ፣ የተታወቀ ስህተት ማቀናጃ ጥቅም ሳይያሳርፍ ፍጥረትን ማሻሻል እናስፈልጋለን፡፡ የተለየ ልዩ ልቦችን የሚቆርጥ እና የድምፅ አካባቢ ተቃውሞ የሚለውጥ የራሱን ትኩረት በመለስ እናሳያልን፡፡ የጥልቅ ኮድ እና የጥቁር አካባቢ መሠረት እና የብዙራዊ አካባቢ ጉዳይ ጉዳይ በCPU እና GPU ላይ አቅራቢያ 109 በመቶ እና 84 በመቶው ይደርሳል፡፡</abstract_am>
      <abstract_hy>Մեծ տրանֆերմերների մոդելները հասել են նորագույն արդյունքներին նյարդային մեքենայի թարգմանման մեջ և դառնում են դաշտում ստանդարտ: Այս աշխատանքում մենք փնտրում ենք հայտնի մեթոդների օպտիմալ համադրումը, որպեսզի օպտիմացվի եզրակացության արագությունը առանց թարգմանման որակի զոհաբերելու: We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, Խելամիտ կոդերի, մակերեսային կոդերի ճարտարապետության և բազմագլխավոր ուշադրության կրճատման ընդունելը կարող է հասնել մինչև 109 և 84 տոկոսի արագություն համակարգչային համակարգի և GPU-ի վրա և նվազեցնել պարամետրերի թիվը 25 տոկոսով, մինչդեռ պահպանել նույն թարգմանման որակ</abstract_hy>
      <abstract_bs>Veliki modeli transformera postigli su rezultate umjetnosti u prevodu neuralnih strojeva i postali su standardni na terenu. U ovom poslu tražimo optimalnu kombinaciju poznatih tehnika da optimiziramo brzinu infekcije bez žrtvovanja kvalitete prevoda. Provodimo empiričko ispitivanje koje sastavlja različite pristupe i pokazuje da kombinacija zamjene samopouzdanja dekodera sa jednostavnim povratnim jedinicama, Prihvaćanje dubokog kodera i plitkog arhitektura dekodera i višeglavnog obrezanja pažnje može postići do 109% i 84% ubrzanja na CPU i GPU, te smanjiti broj parametara za 25% dok održavaju istu kvalitet prevoda u smislu BLEU-a.</abstract_bs>
      <abstract_ca>Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field.  En aquest treball busquem la combinació optima de tècniques conegudes per optimitzar la velocitat de inferència sense sacrificar la qualitat de traducció. Realitzem un estudi empíric que agrupa diversos enfocaments i demostra que la combinació de substituir l'autoatenció del decodificador per unitats recurrents simplificades, L'adopció d'un codificador profund i d'una arquitectura de decodificació superficial i una pruning d'atenció multicapa poden aconseguir fins al 109% i l'84% de velocitat en CPU i GPU respectivament i reduir el nombre de paràmetres un 25% mantenint la mateixa qualitat de traducció en termes de BLEU.</abstract_ca>
      <abstract_cs>Modely velkých transformátorů dosáhly nejmodernějších výsledků v oblasti neuronového strojového překladu a staly se standardem v oboru. V této práci hledáme optimální kombinaci známých technik pro optimalizaci rychlosti inference bez snížení kvality překladu. Provádíme empirickou studii, která shromažďuje různé přístupy a demonstruje, že kombinace nahrazení sebepozornosti dekodéru zjednodušenými opakujícími se jednotkami, Přijetím hlubokého snímače a mělké architektury dekodéru a vícehlavového prořezávání pozornosti lze dosáhnout až 109% a 84% urychlení procesoru a GPU a snížit počet parametrů o 25% při zachování stejné kvality překladu z hlediska BLEU.</abstract_cs>
      <abstract_et>Suurte transformaatorite mudelid on saavutanud tipptasemel tulemusi neuromasintõlkes ja muutunud valdkonnas standardiks. Selles töös otsime teadaolevate tehnikate optimaalset kombinatsiooni järelduste kiiruse optimeerimiseks ilma tõlkekvaliteeti ohverdamata. Viime läbi empiirilise uuringu, mis koondab erinevaid lähenemisviise ja näitab, et dekooderi enesetähelepanu asendamise kombinatsioon lihtsustatud korduvate üksustega, Sügava kodeerija ja madala dekooderi arhitektuuri kasutuselevõtmine ning mitme peaga tähelepanu vähendamine võib saavutada protsessori ja graafikaprotsessori kiiruse vastavalt 109% ja 84% ning vähendada parameetrite arvu 25% võrra, säilitades samal ajal BLEU-s sama tõlkekvaliteedi.</abstract_et>
      <abstract_fi>Suuret muuntajamallit ovat saavuttaneet huippuluokan tuloksia neurokonekäännöksessä ja niistä on tullut alan standardi. Tässä työssä etsimme tunnettujen tekniikoiden optimaalista yhdistelmää päättelynopeuden optimoimiseksi kääntämisen laadusta tinkimättä. Teemme empiirisen tutkimuksen, joka pinoaa erilaisia lähestymistapoja ja osoittaa, että yhdistelmä dekooderin itsetunnon korvaaminen yksinkertaistetuilla toistuvilla yksiköillä, Syväkooderin ja matalan dekooderiarkkitehtuurin käyttöönotto ja monipäinen huomioleikkaus voivat nopeuttaa suoritinta 109% ja grafiikkasuoritinta 84% ja vähentää parametrien määrää 25% säilyttäen samalla BLEU:n käännöslaadun.</abstract_fi>
      <abstract_jv>string" in "context_BAR_stringLink Nang barêng-barêng iki, kita sampeyan kanggo ngerasakno ampliwat karo teknik sing berarti ujian kanggo ngerasakno luwih apik, lan akeh nyong ngerasakno. Awak dhéwé éntuk éntuk empir sing nggawe barang nggawe gerakan sampeyan karo ngono nggambar perusahaan winih dhéwé kuwi nggawe gerakan kelas perusahaan karo perusahaan sugih deep</abstract_jv>
      <abstract_sk>Modeli velikih transformatorjev so dosegli najsodobnejše rezultate v nevronskem strojnem prevajanju in postali standardni na tem področju. V tem delu iščemo optimalno kombinacijo znanih tehnik za optimizacijo hitrosti sklepanja brez žrtvovanja kakovosti prevajanja. Izvedli smo empirično študijo, ki zloži različne pristope in dokazuje, da kombinacija zamenjave samopozornosti dekoderja s poenostavljenimi ponavljajočimi enotami, S sprejetjem globokega kodirnika in plitve arhitekture dekodirnika ter večglavnega obrezovanja pozornosti lahko dosežete do 109% oziroma 84% pospešitev na CPU oziroma GPU ter zmanjšate število parametrov za 25%, hkrati pa ohranjate enako kakovost prevajanja v smislu BLEU.</abstract_sk>
      <abstract_ha>@ action: button Daga wannan aikin, munã dãkin komai mai amfani da shiryoyin ayuka da aka sani wajen kwamfyuta saukarwa na kasancẽwa idan ba da tsarin fassarar ta ba. Tuna sami wani littãfi na tamkar da ke samun hanyõyi dabam-daban kuma ke nuna cewa da za'a bada sauri-rayi kanaga-raye da sunayen da aka sauce, @ info: whatsthis</abstract_ha>
      <abstract_he>דוגמנים גדולים של טרנספורר השיגו תוצאות חדשות בתרגום מכונות עצביות והפכו לסטנדרטים בשטח. בעבודה הזו, אנו מחפשים שילוב אופטימלי של טכניקות ידועות כדי לאופטימיזם מהירות המסקנה בלי להקריב איכות התרגום. אנו מבצעים מחקר אמפירי שמעריך גישות שונות ומוכיח שילוב של החלפת תשומת לב עצמית של מפענח עם יחידות חדשות פשוטות, באמצעות קודד עמוק וארכיטקטורת קודד גבוהה ומעטפת תשומת לב רב-ראשית יכולה להשיג עד 109% ו-84% מהירות על CPU ו-GPU בהתאם ולפחות את מספר הפרמטרים ב-25% בזמן לשמור על אותו איכות התרגום במונחים של BLEU.</abstract_he>
      <abstract_bo>རྩིས་པ་ཆེ་བའི་དབྱིབས་བཟོ་བྱེད་མ་དབྱིབས་སྣང་བའི་གནས་སྟངས་དང་མཐུན་རྐྱེན་གྱིས འོན་ཀྱང་། ང་ཚོས་གནས་ཚུལ་འདིའི་ནང་དུ་ཆེས་ཤུགས་ཀྱི་མཉམ་དུ་མཐུན་རྐྱེན་ཚད་མེད་སྤྲོད་ཀྱི་མཚམས་མཐུན་དང་། ང་ཚོས་རང་ཉིད་ཀྱི་གནད་དོན་དག་གི་ཐབས་ལམ་མ་འདྲ་བརྩལ་བ་ཞིག་བྱེད་ཀྱི་རྩོལ་ཞིག་བྱས་ནས། - adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.</abstract_bo>
      </paper>
    <paper id="8">
      <title>Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm</title>
      <author><first>Alicia</first><last>Tsai</last></author>
      <author><first>Laurent</first><last>El Ghaoui</last></author>
      <pages>54–62</pages>
      <abstract>We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem. We solve it using a dedicated <a href="https://en.wikipedia.org/wiki/Frank-Wolfe_algorithm">Frank-Wolfe algorithm</a>. To generate a summary with k sentences, the <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> only needs to execute approximately k iterations, making it very efficient for a long document. We evaluate our approach against two other <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a> using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones. Our method achieves better results with both <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and works especially well when combined with <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> for highly paraphrased summaries.</abstract>
      <url hash="b22dc89c">2020.sustainlp-1.8</url>
      <attachment type="OptionalSupplementaryMaterial" hash="957e4570">2020.sustainlp-1.8.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.sustainlp-1.8</doi>
      <video href="https://slideslive.com/38939430" />
      <bibkey>tsai-el-ghaoui-2020-sparse</bibkey>
    </paper>
    <paper id="10">
      <title>A Two-stage Model for Slot Filling in Low-resource Settings : Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings</title>
      <author><first>Cennet</first><last>Oguz</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>73–82</pages>
      <abstract>Learning-based slot filling-a key component of spoken language understanding systems-typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples. The first step is designed to remove non-slot tokens (i.e., O labeled tokens), as they introduce <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a> in the input of slot filling models. This step is domain-agnostic and therefore, can be trained by exploiting out-of-domain data. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as <a href="https://en.wikipedia.org/wiki/ELMO">ELMO</a> and BERT. We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.</abstract>
      <url hash="ab915359">2020.sustainlp-1.10</url>
      <doi>10.18653/v1/2020.sustainlp-1.10</doi>
      <video href="https://slideslive.com/38939432" />
      <bibkey>oguz-vu-2020-two</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="11">
      <title>Early Exiting BERT for Efficient Document Ranking<fixed-case>BERT</fixed-case> for Efficient Document Ranking</title>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Rodrigo</first><last>Nogueira</last></author>
      <author><first>Yaoliang</first><last>Yu</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>83–88</pages>
      <abstract>Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for <a href="https://en.wikipedia.org/wiki/Document_ranking">document ranking</a>. With a slight modification, BERT becomes a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with multiple output paths, and each inference sample can exit early from these <a href="https://en.wikipedia.org/wiki/Path_(graph_theory)">paths</a>. In this way, <a href="https://en.wikipedia.org/wiki/Computation">computation</a> can be effectively allocated among samples, and overall <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">system latency</a> is significantly reduced while the original <a href="https://en.wikipedia.org/wiki/Quality_(business)">quality</a> is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x <a href="https://en.wikipedia.org/wiki/Time_complexity">inference speedup</a> with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.</abstract>
      <url hash="58f3e0f0">2020.sustainlp-1.11</url>
      <doi>10.18653/v1/2020.sustainlp-1.11</doi>
      <video href="https://slideslive.com/38939433" />
      <bibkey>xin-etal-2020-early</bibkey>
      <pwccode url="https://github.com/castorini/earlyexiting-monobert" additional="false">castorini/earlyexiting-monobert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="14">
      <title>A Little Bit Is Worse Than None : Ranking with Limited Training Data</title>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Andrew</first><last>Yates</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>107–112</pages>
      <abstract>Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from <a href="https://en.wikipedia.org/wiki/Keyword_search">keyword search</a>. In this work, we tackle the challenge of fine-tuning these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> using corpus-specific labeled data from sources such as TREC. We first answer the question : How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that some labeled in-domain data can be worse than none at all.</abstract>
      <url hash="8f1f5096">2020.sustainlp-1.14</url>
      <doi>10.18653/v1/2020.sustainlp-1.14</doi>
      <video href="https://slideslive.com/38939436" />
      <bibkey>zhang-etal-2020-little</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="16">
      <title>Load What You Need : Smaller Versions of Mutililingual BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Amine</first><last>Abdaoui</last></author>
      <author><first>Camille</first><last>Pradel</last></author>
      <author><first>Grégoire</first><last>Sigel</last></author>
      <pages>119–123</pages>
      <abstract>Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> is often a drawback for their deployment in <a href="https://en.wikipedia.org/wiki/Real-time_computing">real production applications</a>. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to extract smaller <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that keep comparable results, while reducing up to 45 % of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, <a href="https://en.wikipedia.org/wiki/Distillation">distillation</a> induced a 1.7 % to 6 % drop in the overall accuracy on the XNLI data set. The presented <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> and code are publicly available.</abstract>
      <url hash="f74dd40a">2020.sustainlp-1.16</url>
      <doi>10.18653/v1/2020.sustainlp-1.16</doi>
      <video href="https://slideslive.com/38939438" />
      <bibkey>abdaoui-etal-2020-load</bibkey>
      <pwccode url="https://github.com/Geotrend-research/smaller-transformers" additional="false">Geotrend-research/smaller-transformers</pwccode>
    </paper>
    <paper id="19">
      <title>Towards Accurate and Reliable Energy Measurement of NLP Models<fixed-case>NLP</fixed-case> Models</title>
      <author><first>Qingqing</first><last>Cao</last></author>
      <author><first>Aruna</first><last>Balasubramanian</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <pages>141–148</pages>
      <abstract>Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects <a href="https://en.wikipedia.org/wiki/Energy_consumption">energy consumption</a>. We conduct energy measurement experiments with four different <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for a <a href="https://en.wikipedia.org/wiki/Question_answering">question answering task</a>. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and <a href="https://en.wikipedia.org/wiki/Energy_consumption">energy consumption</a>. We release the code and data at https://github.com/csarron/sustainlp2020-energy.</abstract>
      <url hash="8fd7694b">2020.sustainlp-1.19</url>
      <doi>10.18653/v1/2020.sustainlp-1.19</doi>
      <video href="https://slideslive.com/38939441" />
      <bibkey>cao-etal-2020-towards</bibkey>
      <pwccode url="https://github.com/csarron/sustainlp2020-energy" additional="false">csarron/sustainlp2020-energy</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="24">
      <title>Overview of the SustaiNLP 2020 Shared Task<fixed-case>S</fixed-case>ustai<fixed-case>NLP</fixed-case> 2020 Shared Task</title>
      <author><first>Alex</first><last>Wang</last></author>
      <author><first>Thomas</first><last>Wolf</last></author>
      <pages>174–178</pages>
      <abstract>We describe the SustaiNLP 2020 shared task : efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmark</a> as well as energy consumed in making predictions on the test sets. We describe the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, its organization, and the submitted <a href="https://en.wikipedia.org/wiki/System">systems</a>. Across the six submissions to the shared task, participants achieved efficiency gains of 20 over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.</abstract>
      <url hash="79668c0d">2020.sustainlp-1.24</url>
      <doi>10.18653/v1/2020.sustainlp-1.24</doi>
      <bibkey>wang-wolf-2020-overview</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/record">ReCoRD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
  </volume>
</collection>