<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.ngt">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Neural Generation and Translation</booktitle>
      <editor><first>Alexandra</first><last>Birch</last></editor>
      <editor><first>Andrew</first><last>Finch</last></editor>
      <editor><first>Hiroaki</first><last>Hayashi</last></editor>
      <editor><first>Kenneth</first><last>Heafield</last></editor>
      <editor><first>Marcin</first><last>Junczys-Dowmunt</last></editor>
      <editor><first>Ioannis</first><last>Konstas</last></editor>
      <editor><first>Xian</first><last>Li</last></editor>
      <editor><first>Graham</first><last>Neubig</last></editor>
      <editor><first>Yusuke</first><last>Oda</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="35b7d8df">2020.ngt-1</url>
    </meta>
    <frontmatter>
      <url hash="ce0cb7a8">2020.ngt-1.0</url>
      <bibkey>ngt-2020-neural</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Learning to Generate Multiple Style Transfer Outputs for an Input Sentence</title>
      <author><first>Kevin</first><last>Lin</last></author>
      <author><first>Ming-Yu</first><last>Liu</last></author>
      <author><first>Ming-Ting</first><last>Sun</last></author>
      <author><first>Jan</first><last>Kautz</last></author>
      <pages>10–23</pages>
      <abstract>Text style transfer refers to the task of rephrasing a given text in a different style. While various methods have been proposed to advance the state of the art, they often assume the transfer output follows a <a href="https://en.wikipedia.org/wiki/Delta_distribution">delta distribution</a>, and thus their <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can not generate different style transfer results for a given input text. To address the limitation, we propose a one-to-many text style transfer framework. In contrast to prior works that learn a <a href="https://en.wikipedia.org/wiki/One-to-one_mapping">one-to-one mapping</a> that converts an input sentence to one output sentence, our approach learns a one-to-many mapping that can convert an input sentence to multiple different output sentences, while preserving the input content. This is achieved by applying <a href="https://en.wikipedia.org/wiki/Adversarial_system">adversarial training</a> with a latent decomposition scheme. Specifically, we decompose the latent representation of the input sentence to a style code that captures the language style variation and a content code that encodes the language style-independent content. We then combine the <a href="https://en.wikipedia.org/wiki/Content_(media)">content code</a> with the <a href="https://en.wikipedia.org/wiki/Style_sheet_(web_development)">style code</a> for generating a style transfer output. By combining the same <a href="https://en.wikipedia.org/wiki/Content_(media)">content code</a> with a different <a href="https://en.wikipedia.org/wiki/Style_sheet_(web_development)">style code</a>, we generate a different style transfer output. Extensive experimental results with comparisons to several text style transfer approaches on multiple public datasets using a diverse set of performance metrics validate effectiveness of the proposed approach.</abstract>
      <url hash="0ad9ac22">2020.ngt-1.2</url>
      <doi>10.18653/v1/2020.ngt-1.2</doi>
      <video href="http://slideslive.com/38929815" />
      <bibkey>lin-etal-2020-learning</bibkey>
    </paper>
    <paper id="6">
      <title>Automatically Ranked Russian Paraphrase Corpus for Text Generation<fixed-case>R</fixed-case>ussian Paraphrase Corpus for Text Generation</title>
      <author><first>Vadim</first><last>Gudkov</last></author>
      <author><first>Olga</first><last>Mitrofanova</last></author>
      <author><first>Elizaveta</first><last>Filippskikh</last></author>
      <pages>54–59</pages>
      <abstract>The article is focused on automatic development and ranking of a large corpus for Russian paraphrase generation which proves to be the first <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> of such type in Russian computational linguistics. Existing manually annotated paraphrase datasets for Russian are limited to small-sized ParaPhraser corpus and ParaPlag which are suitable for a set of NLP tasks, such as paraphrase and plagiarism detection, sentence similarity and relatedness estimation, etc. Due to size restrictions, these <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> can hardly be applied in end-to-end text generation solutions. Meanwhile, <a href="https://en.wikipedia.org/wiki/Paraphrase_generation">paraphrase generation</a> requires a large amount of training data. In our study we propose a solution to the problem : we collect, rank and evaluate a new publicly available headline paraphrase corpus (ParaPhraser Plus), and then perform text generation experiments with manual evaluation on automatically ranked corpora using the Universal Transformer architecture.</abstract>
      <url hash="88a4de10">2020.ngt-1.6</url>
      <doi>10.18653/v1/2020.ngt-1.6</doi>
      <video href="http://slideslive.com/38929819" />
      <bibkey>gudkov-etal-2020-automatically</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opusparcus">Opusparcus</pwcdataset>
    </paper>
    <paper id="12">
      <title>Distill, Adapt, Distill : Training Small, In-Domain Models for Neural Machine Translation</title>
      <author><first>Mitchell</first><last>Gordon</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <pages>110–118</pages>
      <abstract>We explore best practices for training small, memory efficient <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation models</a> with sequence-level knowledge distillation in the domain adaptation setting. While both <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> and knowledge distillation are widely-used, their interaction remains little understood. Our large-scale empirical results in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> (on three language pairs with three domains each) suggest distilling twice for best performance : once using general-domain data and again using in-domain data with an adapted teacher.</abstract>
      <url hash="9cd128f3">2020.ngt-1.12</url>
      <doi>10.18653/v1/2020.ngt-1.12</doi>
      <video href="http://slideslive.com/38929825" />
      <bibkey>gordon-duh-2020-distill</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="17">
      <title>The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task<fixed-case>ADAPT</fixed-case> System Description for the <fixed-case>STAPLE</fixed-case> 2020 <fixed-case>E</fixed-case>nglish-to-<fixed-case>P</fixed-case>ortuguese Translation Task</title>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Yasmin</first><last>Moslem</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>144–152</pages>
      <abstract>This paper describes the ADAPT Centre’s submission to STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, a shared task of the 4th Workshop on Neural Generation and Translation (WNGT), for the English-to-Portuguese translation task. In this shared task, the participants were asked to produce high-coverage sets of plausible translations given English prompts (input source sentences). We present our English-to-Portuguese machine translation (MT) models that were built applying various strategies, e.g. data and sentence selection, monolingual MT for generating alternative translations, and combining multiple n-best translations. Our experiments show that adding the aforementioned techniques to the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> yields an excellent performance in the English-to-Portuguese translation task.</abstract>
      <url hash="ba8bc9ae">2020.ngt-1.17</url>
      <doi>10.18653/v1/2020.ngt-1.17</doi>
      <video href="http://slideslive.com/38929831" />
      <bibkey>haque-etal-2020-adapt</bibkey>
    </paper>
    <paper id="25">
      <title>Efficient and High-Quality <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a> with OpenNMT<fixed-case>O</fixed-case>pen<fixed-case>NMT</fixed-case></title>
      <author><first>Guillaume</first><last>Klein</last></author>
      <author><first>Dakun</first><last>Zhang</last></author>
      <author><first>Clément</first><last>Chouteau</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <pages>211–217</pages>
      <abstract>This paper describes the OpenNMT submissions to the WNGT 2020 efficiency shared task. We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies. By combining additional <a href="https://en.wikipedia.org/wiki/Optimizing_compiler">optimizations</a> and <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallelization techniques</a>, we create small, efficient, and high-quality neural machine translation models.</abstract>
      <url hash="65454d9c">2020.ngt-1.25</url>
      <doi>10.18653/v1/2020.ngt-1.25</doi>
      <video href="http://slideslive.com/38929839" />
      <bibkey>klein-etal-2020-efficient</bibkey>
    </paper>
    <paper id="26">
      <title>Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task<fixed-case>E</fixed-case>dinburgh’s Submissions to the 2020 Machine Translation Efficiency Task</title>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <author><first>Maximiliana</first><last>Behnke</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <author><first>Sidharth</first><last>Kashyap</last></author>
      <author><first>Emmanouil-Ioannis</first><last>Farsarakis</last></author>
      <author><first>Mateusz</first><last>Chudyk</last></author>
      <pages>218–224</pages>
      <abstract>We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task : <a href="https://en.wikipedia.org/wiki/Single-core">single-core CPU</a>, <a href="https://en.wikipedia.org/wiki/Multi-core_processor">multi-core CPU</a>, and <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a>. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. On <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPUs</a>, we used 16-bit floating-point tensor cores. On <a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPUs</a>, we customized 8-bit quantization and <a href="https://en.wikipedia.org/wiki/Multiprocessing">multiple processes</a> with affinity for the <a href="https://en.wikipedia.org/wiki/Multi-core_processor">multi-core setting</a>. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.</abstract>
      <url hash="b267cea9">2020.ngt-1.26</url>
      <doi>10.18653/v1/2020.ngt-1.26</doi>
      <attachment type="Dataset" hash="ffd898b7">2020.ngt-1.26.Dataset.txt</attachment>
      <video href="http://slideslive.com/38929840" />
      <bibkey>bogoychev-etal-2020-edinburghs</bibkey>
    <title_ar>تقديمات إدنبرة لمهمة كفاءة الترجمة الآلية لعام 2020</title_ar>
      <title_pt>Submissões de Edimburgo para a Tarefa de Eficiência da Tradução Automática de 2020</title_pt>
      <title_es>Presentaciones de Edimburgo para la tarea de eficiencia de la traducción automática de 2020</title_es>
      <title_ja>2020年の機械翻訳効率化タスクへのエディンバラの提出</title_ja>
      <title_zh>爱丁堡提交 2020 年机器翻译效率任</title_zh>
      <title_hi>2020 मशीन अनुवाद दक्षता कार्य के लिए एडिनबर्ग की प्रस्तुतियाँ</title_hi>
      <title_ga>Aighneachtaí Dhún Éideann don Tasc Éifeachtúlachta um Aistriú Meaisín 2020</title_ga>
      <title_ka>ექინდონის მისამართები 2020-ის მაქინის გადაწყვეტილების ეფექტურება</title_ka>
      <title_el>Υποβολές του Εδιμβούργου στο έργο αποδοτικότητας μηχανικής μετάφρασης 2020</title_el>
      <title_hu>Edinburgh beadványai a 2020-as gépi fordítás hatékonysági feladathoz</title_hu>
      <title_kk>Эдинбург 2020 жылы машинаны аудару мүмкіндігінің тапсырмасы</title_kk>
      <title_it>Contributi di Edimburgo al compito 2020 di efficienza della traduzione automatica</title_it>
      <title_lt>Edinburgo pasiūlymai dėl 2020 m. mašinų vertimo efektyvumo užduoties</title_lt>
      <title_mk>Предлозите на Единбург на задачата за ефикасност на преведувањето на машините во 2020 година</title_mk>
      <title_ml>എഡിന്‍ബര്‍ഗിന്‍റെ അബ്രിമിഷന്‍ 2020 മെഷീന്‍ പരിഭാഷപ്പെടുത്തുന്നതിന്‍റെ പ്രവര്‍ത്തനങ്ങള്‍</title_ml>
      <title_ms>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</title_ms>
      <title_mn>Эдинбургийн 2020 оны Машин хөрөнгө оруулах чадварын даалгавар</title_mn>
      <title_mt>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</title_mt>
      <title_no>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</title_no>
      <title_ro>Transmiterile de la Edinburgh la sarcina 2020 privind eficiența traducerii automate</title_ro>
      <title_pl>Zgłoszenia Edynburga do zadania związanego z efektywnością tłumaczenia maszynowego 2020</title_pl>
      <title_sr>Edinburgovi podaci na zadatak za prevod mašine 2020.</title_sr>
      <title_si>ඩිවන්බින්ග්රින්ග් සාමිෂ්ණයෝ 2020වාර්ථාන පරිවර්තනය සක්‍රියාත්මක කාර්ය</title_si>
      <title_so>Edinburgh Submissions to the 2020 machine Translation Effective Task</title_so>
      <title_ta>2020 இயந்திரத்தின் மொழிபெயர்ப்பு விளைவுகள்</title_ta>
      <title_sv>Edinburghs bidrag till 2020-uppgiften för effektivitet i maskinöversättning</title_sv>
      <title_ur>۲۰۰۲ ماشین ترجمہ فعالیت ٹاکس کے لئے ادینڈینبور کے سرماشین</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Sự đệ trình của Edinburgh vào Nhiệm vụ Độ khẩn máy 2020</title_vi>
      <title_bg>Докладите на Единбург за задачата за ефективност на машинния превод през 2020 г.</title_bg>
      <title_nl>Edinburghs inzendingen aan de 2020 Machine Translation Efficiency Task</title_nl>
      <title_da>Edinburghs indlæg til opgaven om effektiv maskinoversættelse i 2020</title_da>
      <title_hr>Edinburgovi podaci na zadatak za praćenje strojeva 2020.</title_hr>
      <title_ko>에든버러 2020년 기계번역 효율 임무 제출</title_ko>
      <title_de>Edinburghs Einreichungen zur 2020-Aufgabe zur Effizienz maschineller Übersetzung</title_de>
      <title_fa>مسئله‌های ادینبورگ برای تاثیر تغییرات ماشین ۲۰۰۲</title_fa>
      <title_sw>Mipango ya Edinburgh kwenye kazi ya Tafsiri ya Mashiniki 2020</title_sw>
      <title_af>Edinburgh se Submissions na die 2020 Masjien Vertaling Efficiency Task</title_af>
      <title_sq>Paraqitjet e Edinburgut në detyrën 2020 për efektshmërinë e përkthimit të makinave</title_sq>
      <title_am>Edinburgh's Submissions to the 2020 machine Translation Effective Task</title_am>
      <title_hy>Էդինբուրգի ներկայացումները 2020 թվականի մեքենային թարգմանման արդյունավետության խնդիրը</title_hy>
      <title_az>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</title_az>
      <title_bn>২০২০০ মেশিন অনুবাদের কাজে এডিনবার্গের সাবমিশন</title_bn>
      <title_id>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</title_id>
      <title_tr>Edinburg's Submissions to the 2020 Machine Translation Efficiency Task</title_tr>
      <title_bs>Edinburgovi podaci na zadatak za praćenje mašine 2020.</title_bs>
      <title_ca>Les Submissions d'Edimburgue a la tasca d'eficiència en la traducció màquina del 2020</title_ca>
      <title_cs>Edinburghské příspěvky k úkolu efektivity strojového překladu 2020</title_cs>
      <title_et>Edinburghi ettepanekud 2020. aasta masintõlke tõhususe ülesandele</title_et>
      <title_fi>Edinburghin ehdotukset vuoden 2020 konekäännöksen tehokkuustehtävään</title_fi>
      <title_jv>Sub-misi nang Edgar 2020 Majin Terjamahan Efeffisisi Job</title_jv>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_sk>Edinburgovi prispevki k nalogi učinkovitosti strojnega prevajanja za leto 2020</title_sk>
      <title_bo>Edinburgh's Submissions to the 2020 Machine Translation Efficiency Task</title_bo>
      <title_he>הגישות של אדינבורג למשימה של יעילות התרגום של מכונות 2020</title_he>
      <abstract_ar>شاركنا في جميع مسارات ورشة العمل الخاصة بالجيل العصبي والترجمة 2020 Efficiency Shared Task: وحدة المعالجة المركزية أحادية النواة ، ووحدة المعالجة المركزية متعددة النواة ، ووحدة معالجة الرسومات. على مستوى النموذج ، نستخدم تدريب المعلم والطالب مع مجموعة متنوعة من أحجام الطلاب ، وربطة عنق وأحيانًا طبقات ، ونستخدم وحدة أبسط بسيطة متكررة ، ونقدم تقليم الرأس. في وحدات معالجة الرسومات ، استخدمنا نوى موتر ذات فاصلة عائمة 16 بت. على وحدات المعالجة المركزية (CPU) ، قمنا بتخصيص 8 بت تكميم وعمليات متعددة مع تقارب لإعداد متعدد النواة. لتقليل حجم النموذج ، جربنا تكميمًا لسجل 4 بت ولكننا استخدمنا عوامات في وقت التشغيل. في المهمة المشتركة ، كانت معظم عمليات الإرسال لدينا مثالية باريتو فيما يتعلق بالمفاضلة بين الوقت والجودة.</abstract_ar>
      <abstract_es>Participamos en todos los temas del Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: CPU de un solo núcleo, CPU multinúcleo y GPU. A nivel de modelo, utilizamos la capacitación de maestros y estudiantes con una variedad de tamaños de estudiantes, incrustaciones de ataduras y, a veces, capas, utilizamos la Unidad Recurrente Simple Más Simple e introducimos la poda de cabezas. En las GPU, utilizamos núcleos tensoriales de punto flotante de 16 bits. En las CPU, personalizamos la cuantificación de 8 bits y los procesos múltiples con afinidad por la configuración multinúcleo. Para reducir el tamaño del modelo, experimentamos con la cuantificación logarítmica de 4 bits, pero utilizamos flotantes en tiempo de ejecución. En la tarea compartida, la mayoría de nuestras presentaciones fueron óptimas en términos de Pareto con respecto a la compensación entre tiempo y calidad.</abstract_es>
      <abstract_pt>Participamos de todas as trilhas do Workshop sobre Geração Neural e Tarefa Compartilhada de Eficiência de Tradução 2020: CPU single-core, CPU multi-core e GPU. No nível do modelo, usamos o treinamento professor-aluno com uma variedade de tamanhos de alunos, incorporações de gravata e, às vezes, camadas, usamos a Unidade Recorrente Simples Mais Simples e introduzimos a poda de cabeça. Em GPUs, usamos núcleos tensores de ponto flutuante de 16 bits. Em CPUs, personalizamos a quantização de 8 bits e vários processos com afinidade para a configuração multi-core. Para reduzir o tamanho do modelo, experimentamos a quantização de log de 4 bits, mas usamos floats em tempo de execução. Na tarefa compartilhada, a maioria de nossas submissões foram ótimas de Pareto em relação ao compromisso entre tempo e qualidade.</abstract_pt>
      <abstract_ja>私たちは、シングルコアCPU、マルチコアCPU、およびGPUという、ニューラルジェネレーションと翻訳に関するワークショップ2020効率共有タスクのすべてのトラックに参加しました。モデルレベルでは、さまざまな生徒のサイズの教師と生徒のトレーニング、タイの埋め込み、時にはレイヤー、シンプルなリカレントユニットを使用し、頭の刈り込みを導入します。GPUでは、16ビットの浮動小数点テンソルコアを使用しました。CPUでは、マルチコア設定に親和性のある8ビット量子化と複数のプロセスをカスタマイズしました。モデルサイズを小さくするために、4ビットのログ量子化を実験しましたが、実行時にフロートを使用します。共有タスクでは、パレートの提出物のほとんどは、時間と品質のトレードオフに関して最適でした。</abstract_ja>
      <abstract_zh>2020年神经成转研讨会效率共事分会场单核CPU多核CPUGPU。 凡模形等级,用诸生规模师生培训,绑扎嵌之,或为层层,用更简循环单元,引入头剪。 于 GPU ,我用 16 位浮点张量心。 于 CPU 之上,自定义 8 位量化多进,与多核设有关联性。 为小大,试 4 位日志量化,浮数于行。 凡所共任,多为帕累托最,时质之权衡。</abstract_zh>
      <abstract_hi>हमने न्यूरल जनरेशन और अनुवाद 2020 दक्षता साझा कार्य पर कार्यशाला के सभी पटरियों में भाग लिया: एकल-कोर सीपीयू, मल्टी-कोर सीपीयू और जीपीयू। मॉडल स्तर पर, हम विभिन्न प्रकार के छात्र आकारों के साथ शिक्षक-छात्र प्रशिक्षण का उपयोग करते हैं, एम्बेडिंग और कभी-कभी परतों को बांधते हैं, सरल सरल आवर्तक इकाई का उपयोग करते हैं, और सिर की छंटाई का परिचय देते हैं। जीपीयू पर, हमने 16-बिट फ्लोटिंग-पॉइंट टेंसर कोर का उपयोग किया। सीपीयू पर, हमने मल्टी-कोर सेटिंग के लिए आत्मीयता के साथ 8-बिट परिमाणीकरण और कई प्रक्रियाओं को अनुकूलित किया। मॉडल आकार को कम करने के लिए, हमने 4-बिट लॉग परिमाणीकरण के साथ प्रयोग किया लेकिन रनटाइम पर फ्लोट का उपयोग करें। साझा कार्य में, हमारे अधिकांश सबमिशन समय और गुणवत्ता के बीच व्यापार-बंद के संबंध में पारेटो इष्टतम थे।</abstract_hi>
      <abstract_ga>Ghlacamar páirt i ngach rian den Cheardlann ar Thasc Comhroinnte Éifeachtúlachta Giniúint Néar agus Aistriúcháin 2020: LAP aon-lárnach, LAP il-lárnach, agus GPU. Ag leibhéal an mhúnla, bainimid úsáid as oiliúint múinteoirí-mac léinn le méideanna éagsúla mac léinn, leabaithe ceangail agus uaireanta sraitheanna, úsáidimid an tAonad Athfhillteach Níos Simplí, agus tugtar isteach bearradh cinn. Ar GPUanna, d’úsáideamar croíleacáin tensor snámhphointe 16-giotán. Ar LAPanna, rinneamar cainníochtú 8-giotán agus próisis iolracha a shaincheapadh le cleamhnas don suíomh il-lárnach. Chun méid an mhúnla a laghdú, rinneamar turgnamh le cainníochtú loga 4-giotán ach úsáidimid snámháin ag am rite. Sa tasc roinnte, bhí an chuid is mó dár n-aighneachtaí Pareto optamach maidir leis an gcomhbhabhtáil idir am agus cáilíocht.</abstract_ga>
      <abstract_hu>Részt vettünk a 2020-as Neural Generation and Translation Efficiency Shared Task Workshop összes pályáján: egymagos processzor, többmagos processzor és GPU. Modellszinten tanár-diák képzést használunk különböző diákméretekkel, kötőbeágyazásokkal és néha rétegekkel, az Egyszerűbb Egyszerű Ismétlődő Egységet használjuk, és bevezetjük a fejmetszést. GPU-kon 16 bites lebegőpontos tenzormagokat használtunk. A processzorokon testre szabtuk a 8 bites kvantizálást és a több folyamatot, amelyek a többmagos beállításhoz hasonlóak. A modell méretének csökkentése érdekében 4 bites log kvantizálással kísérleteztünk, de futási időben float-okat használtunk. A közös feladat során a beadványok többsége Pareto optimális volt az idő és a minőség közötti kompromisszum tekintetében.</abstract_hu>
      <abstract_el>Συμμετείχαμε σε όλα τα κομμάτια του Εργαστηρίου Νευρικής Παραγωγής και Μετάφρασης 2020 Κοινή Εργασία: μονοπυρήνων CPU, πολυπυρήνων CPU και GPU. Στο επίπεδο μοντέλου, χρησιμοποιούμε εκπαίδευση δασκάλου-μαθητή με ποικίλα μεγέθη μαθητών, ενσωμάτωση δεσμών και μερικές φορές στρώματα, χρησιμοποιούμε την απλούστερη απλή επαναλαμβανόμενη μονάδα και εισάγουμε κλάδεμα κεφαλής. Σε GPU, χρησιμοποιήσαμε 16bits κυμαινόμενου σημείου πυρήνες Tensor. Στους επεξεργαστές, προσαρμόσαμε 8-κβαντισμό και πολλαπλάσιες διαδικασίες με συγγένεια για τη ρύθμιση πολλαπλών πυρήνων. Για να μειώσουμε το μέγεθος του μοντέλου, πειραματιστήκαμε με 4-bit κβαντισμό καταγραφής, αλλά χρησιμοποιούμε επιπλέες κατά τη διάρκεια εκτέλεσης. Στο κοινό έργο, οι περισσότερες από τις υποβολές μας ήταν βέλτιστες όσον αφορά το συμβιβασμό μεταξύ χρόνου και ποιότητας.</abstract_el>
      <abstract_ka>ჩვენ მივიღეთ ყველა სამუშაო სამუშაო ნეიროლური განვითარება და განვითარება 2020 ეფექციენტის სამუშაო სამუშაო პარამეტრებში: ერთ-core CPU, მრავალ-core CPU და GPU. მოდელური დონეში, ჩვენ სტუდენტის სტუდენტის სტუდენტის სტუდენტის განმავლობას გამოყენებთ სტუდენტის განმავლობას, დაკავშირებას და ზოგჯერ სტუდენტის განმავლობას, გამოყ GPUs-ში ჩვენ გამოყენეთ 16-ბიტური კონტაქტის ტენსორის კონტაქტი. პროცესების შესახებ, ჩვენ 8- ბიტის კვანტიზაციას და მრავალ პროცესების განმავლობაში, რამდენიმე კვანტიზაციას დავაკეთებდით. მოდელური ზომის შემცირებისთვის, ჩვენ 4- ბიტური ლოგური კვანტიზაციით ექსპერიმენტირებდით, მაგრამ გამოყენეთ წარმოდგენების დროს. ოჲგვფვრჲ ჲრ ნაქთრვ ოპჲეყლზვნთწ ბვქვ ოაპვრჲ ჲორთმალნთ ჟ გყჱდლვზეანვრჲ ნა რპყდგარა между გპვმვრჲ თ კალთრვრჲ.</abstract_ka>
      <abstract_it>Abbiamo partecipato a tutte le tracce del Workshop sulla Generazione Neurale e Translation 2020 Efficiency Shared Task: CPU single-core, CPU multi-core e GPU. A livello di modello, utilizziamo la formazione insegnante-studente con una varietà di dimensioni studentesche, incorporazioni di cravatte e talvolta strati, usiamo l'unità ricorrente semplice più semplice e introduciamo la potatura della testa. Sulle GPU, abbiamo utilizzato core tensori a 16 bit a virgola mobile. Sulle CPU, abbiamo personalizzato la quantizzazione a 8 bit e più processi con affinità per l'impostazione multi-core. Per ridurre le dimensioni del modello, abbiamo sperimentato la quantizzazione di log a 4 bit ma abbiamo utilizzato float al runtime. Nel compito condiviso, la maggior parte delle nostre proposte sono state Pareto ottimali nel rispetto del compromesso tra tempo e qualità.</abstract_it>
      <abstract_lt>Dalyvavome visuose seminaro „Neural in ės generacijos ir 2020 m. vertimo efektyvumo klausimais“ etapuose: vienas pagrindinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis procesinis Modelio lygmeniu mes naudojame mokytojų ir student ų mokymą su įvairiais studentų dydžiais, sąsajų įdėjimais ir kartais sluoksniais, naudojame paprastesnį paprastesnį pakartotinį vienetą ir įdiegiame galvos smulkinimą. GPU naudojome 16 bit ų svyravimo taško tempimo jėgas. CPU, mes pritaikėme 8 bit ų kiekybinį nustatymą ir kelis procesus su afinitetu daugiašaliam nustatymui. Siekdami sumažinti modelio dydį, eksperimentavome su 4 bit ų log kiekybiškumu, bet naudojame plaukiojimo metu. Bendroje užduotyje dauguma mūsų pareiškimų buvo Pareto optimalus laikas ir kokybė.</abstract_lt>
      <abstract_kk>Біз 2020 жылы нейралық жасау және аудару және аудару жұмысының барлық жолдарына қатысушы болдық: бір негізгі процессор, бірнеше негізгі процессор және GPU. Үлгі деңгейінде мұғалім- студенттердің оқытуын көптеген студенттердің өлшемі, көптеген ендіру және кейбірде қабаттарды қолданып, Қарапайым қайталану бірлігін қолдану және басының көптеген GPУ үшін 16- биттік жылжымалы нүктелген тензер тензерін қолдандық. Процессорларда біз 8- бит квантизациясын және бірнеше процестерді бірнеше негізгі параметрлердің көпшілігімен өзгертдік. Үлгі өлшемін азайту үшін 4- биттік журналды квантизациялау арқылы тәжірибедік, бірақ жегу уақытында жылжымыз. Ортақ тапсырманың көпшілігіміз уақыт мен сапа арасындағы тәжірибесіне қатынасыз керек болды.</abstract_kk>
      <abstract_mk>Учествувавме на сите траги од работилницата за неурална генерација и превод 2020 ефикасност споделена задача: еднојадрен процесор, мултијадрен процесор и GPU. На моделно ниво, користиме обука на учител-студент со различни студентски големини, врзувања и понекогаш слоеви, користиме едноставна единица за повторно повторно, и воведуваме кревање на главата. На GPU, користевме 16-битни тензорски јадра. На процесорите, ја прилагодивме квантизацијата од 8 бити и многуте процеси со афинитет за поставувањето на многуте јадра. За да ја намалиме големината на моделот, експериментиравме со квантизација на логот од 4 бити, но употребувавме пливачки на време на бегство. Во заедничката задача, повеќето од нашите предлози беа Парето оптимални во однос на разликата помеѓу времето и квалитетот.</abstract_mk>
      <abstract_ms>Kami berpartisipasi dalam semua trek Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning.  Pada GPU, kami menggunakan inti tensor titik-terapung 16-bit. Pada CPU, kami menyesuaikan kuantisasi 8-bit dan proses berbilang dengan afini untuk tetapan berbilang-inti. Untuk mengurangi saiz model, kami eksperimen dengan kuantisasi log 4-bit tetapi guna float pada masa berjalan. Dalam tugas berkongsi, kebanyakan penghantaran kami adalah Pareto optimal dengan menghormati perdagangan antara masa dan kualiti.</abstract_ms>
      <abstract_mt>Parteċipajna fil-binarji kollha tal-Workshop dwar il-Ġenerazzjoni Newrali u t-Traduzzjoni 2020 Kompitu Konġunt dwar l-Effiċjenza: CPU uniku, CPU multi ċentrali, u GPU. Fil-livell tal-mudell, a ħna nużaw taħriġ bejn l-għalliema u l-istudenti b’varjetà ta’ daqsijiet tal-istudenti, inkorporazzjonijiet tal-irbit u xi kultant saffi, nużaw l-Unit à Simpli u Rikorrenti, u nintroduċu l-pruning tar-ras. Fuq GPUs, użajna ċentri tat-tensur b’punt floating ta’ 16-il bit. Fuq is-CPUs, aħna addattajna kwantifikazzjoni ta’ 8 bits u proċessi multipli b’affinità għall-issettjar multi-core. Biex tnaqqas id-daqs tal-mudell, esperimentajna b’kwantifikazzjoni ta’ logaritmu ta’ 4 bits iżda użajna floats waqt ir-runtime. Fil-kompitu kondiviż, il-biċċa l-kbira tas-sottomissjonijiet tagħna kienu Pareto ottimali fir-rigward tal-kompromess bejn iż-żmien u l-kwalità.</abstract_mt>
      <abstract_ml>ഞങ്ങള്‍ നെയുറല്‍ ജനറനും പരിഭാഷവും 2020 സാധാരണ പങ്കാളികള്‍ പങ്കുചേര്‍ക്കുന്ന പണിയില്‍ പങ്കാളികളില്‍ പങ്കാളികളായി പങ്കുചേര്‍ത്തിരിക്കുന്നു. ഒര മോഡല്‍ നിലയില്‍, ഞങ്ങള്‍ വിദ്യാര്‍ത്ഥികളുടെ വലിപ്പം ഉപയോഗിക്കുന്നു, വിദ്യാര്‍ത്ഥികളുടെ വലിപ്പമുള്ള പരിശീലനം ഉപയോഗിക്കുന്നു, ചിലപ്പോള്‍ തട്ട ജിപിയുസില്‍ ഞങ്ങള്‍ 16- ബിറ്റ് നീലുന്ന ടെന്‍സര്‍ കോര്‍ ഉപയോഗിച്ചു. സിപിയുസില്‍ ഞങ്ങള്‍ 8 ബിറ്റ് വ്യവസ്ഥയും പല പ്രക്രിയകളും കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ കൂടുതല്‍ സംവിധാ മോഡലിന്റെ വലിപ്പം കുറയ്ക്കാന്‍ ഞങ്ങള്‍ 4- ബിറ്റ് ലോഗിന്റെ പരീക്ഷണത്തില്‍ പരീക്ഷിച്ചിരിക്കുന്നു. പക്ഷെ റ പങ്കാളിയുള്ള ജോലിയില്‍, നമ്മുടെ കീഴ്പ്പെടുത്തുന്നവരില്‍ മിക്കവാറും സമയവും ഗുണവും തമ്മിലുള്ള വ്യാപാര്‍ത്</abstract_ml>
      <abstract_mn>Бид 2020 оны мэдрэлийн бүтээлч, хөгжлийн үр дүнтэй хуваалцах үйл ажиллагаанд оролцсон. Нэг төвөгтэй CPU, олон төвөгтэй CPU, GPU. Загварын түвшинд бид багш нарын оюутнуудын сургалтын сургалтыг олон төрлийн хэмжээтэй хэрэглэдэг. Заримдаа холбоотой холбоотой холбоотой, заримдаа давхар холбоотой холбоотой холбоотой. Энгийн энгийн давхар GPUs дээр бид 16-бит floating point tensor cores ашигласан. Процессорын хувьд бид 8-бит хэмжээст болон олон төвөгтэй олон төвөгтэй процессүүдийг хувьд зөвшөөрсөн. Загварын хэмжээг багасгахын тулд бид 4-бит лог квантизацийг туршиж, гэхдээ ажиллах цаг хугацаанд хөдөлгөөн ашиглаж байна. Бидний хуваалцах ажлын ихэнх нь хугацаа болон сайн чанарын хоорондох худалдааны тухай Парето зөвхөн эерэг байсан.</abstract_mn>
      <abstract_no>Vi delta i alle spor av arbeidsområdet på Neuralgenerasjon og omsetjing 2020 delt delt effektivt delt oppgåve: enkelkjerne CPU, fleire kjerneCPU og GPU. På modellnivået bruker vi lærer-studenttrening med forskjellige studentstorleik, knyttingar og noen ganger lag, bruker den enklare gjentakingseininga og introduserer hodeprinning. På GPUs brukte vi 16- bits flytande- punkttensorer. På prosessarar har vi tilpassa 8- bit kvantisering og fleire prosessar med affinitet for multikjerneinnstillingane. For å redusera modellstorleiken, eksperimenterte vi med 4- bit loggkvantisering, men bruk flyttar ved køyringsdato. I den delte oppgåva var dei fleste av våre søknader Pareto optimalt med respekt av utviklinga mellom tid og kvalitet.</abstract_no>
      <abstract_pl>Uczestniczyliśmy we wszystkich utworach Warsztatu Generacji Neuralnej i Translation 2020 Efektywność Wspólnego Zadania: procesor jednorzeniowy, procesor wielorzeniowy i GPU. Na poziomie modelu stosujemy szkolenia nauczyciela-ucznia z różnymi rozmiarami uczniów, osadzeniami wiązań, a czasem warstwami, używamy prostszej jednostki powtarzającej i wprowadzamy przycinanie głowy. Na procesorach graficznych używaliśmy 16-bitowych rdzeni tensorów zmiennoprzecinkowych. Na procesorach dostosowaliśmy 8-bitową kwantyzację i wiele procesów z powinowactwem do ustawienia wielorzeniowego. Aby zmniejszyć rozmiar modelu, eksperymentowaliśmy z 4-bitową kwantyzacją logów, ale używaliśmy floatów w czasie uruchomienia. W ramach wspólnego zadania większość naszych zgłoszeń była optymalna w Pareto pod względem kompromisu między czasem a jakością.</abstract_pl>
      <abstract_ro>Am participat la toate piesele Workshop-ului privind generarea neurală și traducerea eficienței 2020 Sarcină partajată: procesor single-core, procesor multi-core și GPU. La nivelul modelului, folosim instruirea profesor-elev cu o varietate de dimensiuni de elev, încorporări de cravată și uneori straturi, folosim Unitatea Recurentă Simplă și introducem tăierea capului. Pe GPU-uri, am folosit nuclee tensoare cu punct plutitor pe 16 biți. Pe procesoare, am personalizat cuantificarea pe 8 biți și procese multiple cu afinitate pentru setarea multi-core. Pentru a reduce dimensiunea modelului, am experimentat cuantificarea logului pe 4 biți, dar am folosit flotoare la rulare. În sarcina partajată, majoritatea depunerilor noastre au fost Pareto optime în ceea ce privește compromisul dintre timp și calitate.</abstract_ro>
      <abstract_sr>Učestvovali smo u svim tragovima radionice o Neuralnoj generaciji i prevodi 2020. zajedničkom zadatku učinkovitosti: jedinstvenog procesora, multi core CPU i GPU. Na nivou model a, koristimo trening učitelja i učitelja sa raznim veličinama studenata, vezama i ponekad slojevima, koristimo jednostavniju jednostavnu povratnu jedinicu i predstavljamo glavnu pružanje. Na GPU, koristili smo 16 bitnih tenzorskih koraka. Na procesorima smo prilagodili kvantizaciju od 8 bita i višestruke procese sa afinitetom za višecore postavljanje. Da bismo smanjili veličinu modela, eksperimentirali smo sa kvantizacijom 4-bitnih dnevnika, ali koristili smo plove u provoznom vremenu. U zajedničkom zadatku, većina naših podataka je bila Pareto optimalna s poštovanjem trgovine između vremena i kvaliteta.</abstract_sr>
      <abstract_si>අපි සියළුම ප්‍රවේශනය සහ පරිවර්තනය සම්ප්‍රවේශනයේ සියළුම ප්‍රවේශකයේ සියළුම ප්‍රවේශකයේ සම්පූර්ණය කරලා: එක-කෝර් CPU මොඩල් ස්ථානයේදී, අපි ගුරුවර්ති-විද්‍යාණික ප්‍රශ්නයක් පාවිච්චි කරනවා, විද්‍යාණික ප්‍රශ්නයක් සමග විද්‍යාණික ප්‍රශ GPUSට, අපි 16-බිට් ප්‍රවාහනය ප්‍රවාහනය කරලා තියෙන්නේ. CPU වලින්, අපි 8- බිට් ක්වාන්තිකරණය සහ ගොඩක් පරීක්ෂණය සඳහා ගොඩක් ක්‍රියාත්මක සැකසුම් වෙනුවෙන්. මොඩල් ප්‍රමාණය අඩු කරන්න, අපි 4- බිට් ලොග් ක්වාන්ටිස් එක්ක පරීක්ෂණය කරලා තියෙන්නේ නමුත් රන්ටිම් ව සමාගත වැඩේ ඉන්නේ, අපේ ගොඩක් පිළිගන්නේ පැරෙටෝ විශේෂය සහ කුළුතිය අතර ව්‍යාපාරයක් ගැන ගෞරවයෙන</abstract_si>
      <abstract_so>We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU.  Iskuulka tusaale ahaan waxaynu isticmaalnaa waxbarashada waxbarashada ardayda oo kala duduwan, qashinka xidhan iyo qasnada qaarkood, waxaynu isticmaalnaa qaybta soo deganaanshaha ee fudud, waxaana soo bandhigaynaa caqliga madaxa. GPUs, waxaynu isticmaalnay 16-bit oo ku socota barta tensor. CPUs, waxaynu u isticmaalnay qiyaastii 8 bit oo kala duduwan, waxaana la xiriirnay kooxaha kala duduwan. Si aan u hooseeyo tirada modellka, waxaan ku tijaabiyey qiyaastii qoriga 4-bit, laakiin waxaynu isticmaalnaa goobta lagu soconayo. Shaqada la qaybsan, inta badan ka soo dhiibeyno waxay ahaayeen Pareto mid aad u wanaagsan inay ka heshiiyaan ganacsiga u dhexeeya wakhtiga iyo qiimo.</abstract_so>
      <abstract_sv>Vi deltog i alla spår av Workshopen om Neural Generation and Translation 2020 Efficiency Shared Task: enkärnig processor, flerkärnig processor och GPU. På modellnivå använder vi lärar-elevutbildning med olika elevstorlekar, slipsbäddning och ibland lager, använder Simpler Simple Recurrent Unit och introducerar huvudbeskärning. På GPU:er använde vi 16-bitars flytande tensorkärnor. På processorer anpassade vi 8-bitars kvantisering och flera processer med affinitet för flerkärnig inställning. För att minska modellstorleken experimenterade vi med 4-bitars loggkvantisering men använde floats vid körning. I den delade uppgiften var de flesta av våra bidrag Pareto optimala med hänsyn till avvägningen mellan tid och kvalitet.</abstract_sv>
      <abstract_ta>2020 மொழிபெயர்ப்புகள் பங்கிடப்பட்ட பணியில் நாங்கள் நெயுரல் உருவாக்குதல் மற்றும் மொழிபெயர்ப்புகளின் அனைத்து தடங்களிலும் பங்கிடப்பட்டோம்: ஒ At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning.  ஜிபியூஸில், நாங்கள் 16 பிட்டு மிதவை புள்ளி டென்சார் கோர்களை பயன்படுத்தினோம். சிபியூஸ் மீது, நாம் 8 பிட் அளவு மற்றும் பல உறுப்பு அமைப்புகளுக்கு தொடர்புடன் பல செயல்களை தனிப்பயனாக்கினோம். மாதிரி அளவை குறைக்க, நாம் 4- பிட் பதிவு மதிப்புடன் பரிசோதித்தோம் ஆனால் ஓடும் நேரத்தில் மிதவைகளை பயன்படுத்து. பங்கிடப்பட்ட பணியில், எங்கள் பெரும்பாலானவர்கள் பார்டோ விருப்பத்தேர்வாக இருந்தனர் நேரம் மற்றும் தரம் இடையே வணி</abstract_ta>
      <abstract_ur>ہم نے نورول پیدائش اور ترجمہ 2020 کے تمام ٹراکیوں میں شامل ہوا: single-core CPU, multicore CPU اور GPU. نمڈل سطح میں، ہم استاد-استاد کی تعلیم کے مطابق مختلف استاد کے ساتھ استعمال کرتے ہیں، ٹائی ابڈینگ اور کبھی لہروں کے مطابق، ساده ساده دوبارہ یونیٹ کے مطابق استعمال کرتے ہیں، اور سر پرینگ کو معلوم کرتے ہیں. جی پی یوس پر ہم 16 بیٹ فلانٹ پوینٹ ٹینسٹر کور استعمال کرتے تھے۔ CPUs پر ہم نے 8-bit quantization اور بہت سی پروسسوں کو مثبت کے ساتھ مطابق کیا۔ ماڈل کی اندازہ کم کرنے کے لئے، ہم نے 4-بیٹ لاگ کوانتیزی کے ساتھ آزمائش کی لیکن رونٹ زمانہ میں فلاٹ استعمال کریں۔ مشترک کام میں، ہماری اکثریت مسلمانوں کو وقت اور کیفیت کے درمیان تجارت کے معاملہ سے پاریٹا بہترین تھا.</abstract_ur>
      <abstract_uz>2020 Tafsilotlar bilan bir necha CPU, bir nechta CPU, va GPU va bir nechta tarjima qilgan Workshopining hamma yo'plalariga ega bo'lgan. Model darajada, biz o'qituvchi o'qituvchi o'quvchi o'quvchi o'quvchi o'quvchi o'quvchi o'rganishdan foydalanamiz, o'zlarining ko'plab o'quvchilari sizlari bilan boshlanamiz, va ba'zida qachon qachon qatlamlar bilan boshlanamiz, od GPUs'da, biz 16-bit floating-point tensorning tugmalaridan foydalanamiz. CPU bilan 8- bit qiymatni moslash va bir nechta vazifalarni bir nechta boshqarish. Modelning oʻlchamini kamaytirish uchun biz 4- bitta logning tizimini aniqlashni istadik lekin ishga tushganda floatsiyalarni ishlatish. Bizning ko'pchiligimizning qismlarimiz vaqt va сифат орасидаги гуруҳларни муҳим қилиш учун Pareto.</abstract_uz>
      <abstract_vi>Chúng tôi đã tham gia vào mọi dấu vết của "Workshop in Neural generation and translation 2020 efficity shared Task: single-core CPU, đa-core CPU, và GPU. Ở mức mô hình, chúng tôi dùng khóa giáo viên-sinh viên với nhiều kích thước sinh viên, sự ghép thắt cà vạt và đôi khi nhiều lớp, dùng đơn giản đơn vị phục hồi, và giới thiệu việc tỉa đầu. Trên GPU, chúng tôi dùng lõi tensor ở tư khoản 16-cắn. Trên CPU, chúng tôi đã tùy chỉnh lượng 8-cắn và nhiều tiến trình có sự đồng thuận với thiết lập đa lõi. Để giảm cỡ mô hình, chúng tôi thí nghiệm với phân lượng 4-cắn bản ghi, nhưng sử dụng nổi ở thời gian chạy. Trong công việc chia sẻ, hầu hết tài liệu của chúng tôi là "Pareto" tối ưu với tôn trọng sự trao đổi giữa thời gian và chất lượng.</abstract_vi>
      <abstract_bg>Участвахме във всички песни на семинара за генериране и превод на неврони 2020 Споделена задача за ефективност: едноядрен процесор, многоядрен процесор и графичен процесор. На ниво модел използваме обучение учител-студент с различни размери на учениците, вграждания на вратовръзки и понякога слоеве, използваме по-простата обикновена повтаряща се единица и въвеждаме подрязване на главата. На GPU използвахме 16-битови тензорни ядра с плаваща точка. На процесорите персонализирахме 8-битова квантизация и множество процеси с афинитет към многоядрената настройка. За да намалим размера на модела, експериментирахме с 4-битова логова квантизация, но използвахме плувки по време на изпълнение. В споделената задача повечето от нашите предложения бяха оптимални по отношение на компромиса между време и качество.</abstract_bg>
      <abstract_da>Vi deltog i alle spor af Workshoppen om Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU og GPU. På modelniveau bruger vi lærer-studerende træning med en række forskellige studerende størrelser, bindeindlejringer og undertiden lag, bruger Simpler Simple Recurrent Unit og introducerer hovedbeskæring. På GPU'er brugte vi 16-bit floating-point tensor kerner. På CPU'er tilpassede vi 8-bit kvantisering og flere processer med affinitet til multi-core indstillingen. For at reducere modelstørrelsen eksperimenterede vi med 4-bit logkvantisering, men bruger floats ved kørselstid. I den fælles opgave var de fleste af vores indlæg Pareto optimale med hensyn til afstemningen mellem tid og kvalitet.</abstract_da>
      <abstract_hr>Učinili smo se u svim tragovima radionice o Neuralnoj generaciji i prevodi 2020. zajedničkom zadatku učinkovitosti: jedinstvenog procesora, višecore CPU i GPU. Na razini model a, koristimo trening učitelja i učitelja s raznim veličinama učenika, ugrađenjem vezama i ponekad slojevima, koristimo jednostavniju jednostavnu povratnu jedinicu i predstavljamo glavnu pružanje. Na GPU, koristili smo 16-bit plivajućih tenzorskih kabla. Na procesorima smo prilagodili kvantizaciju od 8 bita i višestruke procese s afinitetom za višecore postavku. Da bismo smanjili veličinu modela, eksperimentirali smo s kvantizacijom 4-bit dnevnika, ali koristili smo plove u provozu. U zajedničkom zadatku, većina naših podataka je bila Pareto optimalna s poštovanjem trgovine između vremena i kvalitete.</abstract_hr>
      <abstract_nl>We hebben deelgenomen aan alle tracks van de Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU en GPU. Op modelniveau maken we gebruik van leraar-student training met een verscheidenheid aan studentengrootten, bindbindingen en soms lagen, gebruiken we de Simpler Simple Recurrent Unit en introduceren we hoofdsnijden. Op GPU's gebruikten we 16-bit floating-point tensor cores. Op CPU's hebben we 8-bits kwantisatie en meerdere processen aangepast met affiniteit voor de multi-core instelling. Om de modelgrootte te verkleinen, experimenteerden we met 4-bits logquantisatie, maar gebruikten floats tijdens runtime. In de gedeelde taak waren de meeste van onze inzendingen Pareto optimaal met respect voor de afweging tussen tijd en kwaliteit.</abstract_nl>
      <abstract_de>Wir haben an allen Tracks des Workshops zur Neuralen Generation und Übersetzung 2020 Efficiency Shared Task teilgenommen: Single-Core CPU, Multi-Core CPU und GPU. Auf Modellebene verwenden wir Lehrer-Schüler-Training mit einer Vielzahl von Schülergrößen, Bindeeinbettungen und manchmal Schichten, verwenden die Simpler Simple Recurrent Unit und führen Kopfschnitt ein. Auf GPUs verwendeten wir 16-Bit Gleitkomma-Tensorkerne. Auf CPUs haben wir 8-Bit-Quantisierung und mehrere Prozesse mit Affinität für die Multi-Core-Einstellung angepasst. Um die Modellgröße zu reduzieren, experimentierten wir mit 4-Bit Log Quantisierung, verwenden aber Floats zur Laufzeit. In der gemeinsamen Aufgabe waren die meisten unserer Einreichungen Pareto optimal hinsichtlich des Kompromisses zwischen Zeit und Qualität.</abstract_de>
      <abstract_id>Kami berpartisipasi dalam semua jejak Workshop tentang Generasi Neural dan Translation 2020 Efisiensi Bersama Tugas: CPU satu-inti, CPU multi-inti, dan GPU. Pada tingkat model, kami menggunakan pelatihan guru-siswa dengan berbagai ukuran siswa, dasi embedding dan kadang-kadang lapisan, menggunakan Simpler Simple Recurrent Unit, dan memperkenalkan pemotong kepala. Pada GPU, kami menggunakan inti tensor titik berenang 16 bit. Pada CPU, kami menyesuaikan kuantisasi 8 bit dan proses berbilang dengan afinitas untuk seting multi-core. Untuk mengurangi ukuran model, kami eksperimen dengan kuantisasi log 4-bit tetapi menggunakan float pada waktu berjalan. Dalam tugas bersama, kebanyakan pengiriman kami adalah Pareto optimal dengan menghormati perdagangan antara waktu dan kualitas.</abstract_id>
      <abstract_fa>ما در تمام رده‌های کارگاه روی نسل‌های عصبی و ترجمه‌های فعالیت‌های مشترک در سال ۲۰۰۲ شرکت کردیم: CPU single-core, CPU multicore, and GPU. در سطح مدل، ما از آموزش آموزش آموزش دانش آموزش استفاده می کنیم با اندازه های مختلف دانش آموزشی، وسیله‌های قالب و گاهی طبقه‌ها، از واحد ساده‌ترین دوباره استفاده می‌کنیم، و سر را معرفی می‌کنیم. روی جی پی یوس، ما از ۱۶ بیت تنسور نقطه شناورن استفاده کردیم. در CPUs، ما 8 بیت کوانتیزی و فرایند چندین را با تعادل برای تنظیم مجموعه هسته‌های زیادی تنظیم کردیم. برای کاهش اندازه مدل، ما با کوانتیزی چهار بیت لیگ آزمایش کردیم ولی از پرواز در زمان چرخش استفاده کردیم. در وظیفه مشترک، بیشتر تسلیم‌های ما پارتو بهترین بود با احترام تجارت بین زمان و کیفیت.</abstract_fa>
      <abstract_sw>Tumeshiriki katika tafiti zote za Warsha ya Uzalishaji na Tafsiri ya Neurali 2020 Kushirikishwa na kazi: CPU moja kwa moja, CPU na GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning.  Kwenye GPUs, tulitumia viungo vya viungo vya sekta 16 vya ndege. Kwenye CPU, tulitumia kiasi cha takwimu 8 na michakato mengi yenye uhusiano wa mazingira mengi. Ili kupunguza ukubwa wa mifano, tulijaribu kwa kiasi kikubwa cha log 4-bit lakini tunatumia mafua wakati wa runtime. Katika jukumu lililoshirikishwa, maoni mengi yetu yalikuwa bora zaidi ya Pareto kwa kuheshimu biashara kati ya muda na kiwango.</abstract_sw>
      <abstract_ko>우리는 신경 생성 및 번역 2020 효율 공유 작업 세미나의 모든 궤도인 단핵 CPU, 다핵 CPU와 GPU에 참가했다.모델 차원에서 우리는 교사-학생 교육을 사용하는데 각종 학생 규모, 넥타이를 포함하고 때로는 차원도 있다. 더욱 간단한 귀속 단원을 사용하고 머리 커팅을 도입한다.GPU에서 우리는 16비트 부동 소수점 장량 핵을 사용한다.CPU에서는 멀티 코어 설정을 위한 8비트 계량화 및 멀티 프로세스를 맞춤형으로 구성했습니다.모델의 크기를 줄이기 위해서, 우리는 4개의 로그 양화를 시도했지만, 실행할 때 부동점을 사용했다.공유 작업 중 시간과 품질 사이의 균형을 보면, 우리의 대다수 제출은 파르토리코가 가장 좋다.</abstract_ko>
      <abstract_tr>Biz Nural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, we GPU'yň ähli hatlaryna goşuldyk. Model derejesinde mugallymlary okuwçylar üçin birnäçe topar ölçüsi, baglaýyşlar we käwagt katlar bilen ulanýarys, Kiçiräk ýeterlik Birlikden ullan we kellämizi süýtgetmek üçin ullanýarys. GPU'da 16 bit ýüzlik noktalar esnesörlerini ulandyk CPU-lerde, multi-core düzenlemek üçin 8-bit küntatiýasyny we köp prosesleri bejerdik. Model ölçüsini azaltmak üçin 4-bit küçümseme ile denedik ama eserdeki çizgileri kullandık. Paýlaşan zadyň köpüsi wagt we kwalitet arasyndaky syýahat alyp Pareto optimaldyr.</abstract_tr>
      <abstract_am>በኔural Generation እና ትርጓሜ 2020 ተርጓሚዎች የስራ ትርጓሜ ሰርቨርስቲ ሁሉ ተጋርተናል: አንድ-core CPU፣ ብዙ-core CPU እና GPU. በሞዴል ደረጃው፣ አስተማሪ-ተማሪ ትምህርት ትምህርት በተለይ መጠን፣ እየቆረጥ እና አንዳንድ ጊዜ ደረጃዎች፣ ቀላል የቀረበውን ተማሪ እኩል እናስታውቃለን፡፡ በGPU ላይ 16 ቢትር እየነጥፍ ቆንጆር እየቆረጥን ነበር፡፡ በCPU ላይ 8 ቢትር መጠን እና በብዙ አካባቢ ማህበረሰብ ጋር አብዛኛዎችን ፍጥረቶች አስቀምጠን፡፡ የሞዴል መጠን ለማሳነስ፣ 4 ቢትር የሎግ ማሰናከል ፈተናል ግን በሮን ጊዜ መፍጠርን ተጠቀምን፡፡ In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.</abstract_am>
      <abstract_af>Ons het gedeel in alle snitte van die Werkshop op Neurale Generasie en Vertaling 2020 Gedeelde Opdrag: enkelcore CPU, multi core CPU en GPU. Op die model vlak gebruik ons onderwysers-studente onderwyseling met 'n verskillende studente grootte, tie inbêding en soms laag, gebruik ons die Eenvoudige Herhaalde Eenheid en introduseer hoof aandrukking. Op GPUs gebruik ons 16- bit floating- point tensor cores. Op CPUs, ons pasmaak 8- bit quantisasie en veelvuldige prosesse met affinity vir die multi core instelling. Om model grootte te verminder, het ons eksperimenteer met 4- bit log quantiseering, maar gebruik vliewe op looptyd. In die gedeelde taak was die meeste van ons ondersoek Pareto optimal met respek van die handel tussen tyd en kwaliteit.</abstract_af>
      <abstract_hy>Մենք մասնակցեցինք նյարդային սերունդի և 2020-ի թարգմանման արդյունավետության բոլոր գործընթացներին' մեկ հիմնական պրոցեբույսը, բազմահիմնական պրոցեբույսը և GPU-ը: Մոդելի մակարդակում մենք օգտագործում ենք ուսուցիչներ-ուսանողներ ուսուցիչներ ուսանողների տարբեր չափսերով, կապերով և երբեմն շերտերով, օգտագործում ենք պարզ կրկնվող միավորը և ներկայացնում գլխի կտրտումը: GPU-ում մենք օգտագործեցինք 16-բիթ լողացող կետի տենսորի հիմքեր: Համակարգչային համակարգերի վրա մենք պատրաստեցինք 8-բիտի քանակությամբ և բազմաթիվ գործընթացներով, որոնք բազմահիմնական սահմանափակում են աֆինիտիվություն: Մոդելի չափսի կրճատելու համար մենք փորձեցինք 4-բիտ լոգ քվանտիզացիայի միջոցով, բայց օգտագործեցինք լոգակներ ընթացքում: Մեր ընդհանուր խնդրի մեծամասնությունը Pareto-ն օպտիմալ էր ժամանակի և որակի միջև հակամարտության հարցում:</abstract_hy>
      <abstract_sq>Ne morëm pjesë në të gjitha gjurmët e Workshop mbi Gjenerimin Neural dhe Efikasitetin e Përkthimit 2020 Detyrën e Përbashkët: CPU me një qendër, CPU me shumë qendër dhe GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning.  Në GPU, kemi përdorur 16-bit pikë-fluturimi në bazë të tensorit. Në CPU, ne personalizuam kuantizimin 8-bit dhe proceset e shumëfishtë me afinitet për përcaktimin e shumëbërthamës. Për të reduktuar madhësinë e modelit, ne eksperimentuam me kuantizim logaritmi 4-bit por përdorim fluturime në kohën e funksionimit. Në detyrën e përbashkët, shumica e paraqitjeve tona ishin Pareto optimale me respektin e kompromisit midis kohës dhe cilësisë.</abstract_sq>
      <abstract_bn>আমরা নিউরাল জেনারেশন এবং অনুবাদের ২০২০ টি প্রভাব শেয়ার করার কার্যকর কর্মশালায় অংশগ্রহণ করেছি: একক মূল সিপিউ, বহুমূল সিপিউ এবং জিপিউ। মডেল পর্যায়ে আমরা শিক্ষক-ছাত্রের প্রশিক্ষণ ব্যবহার করি বিভিন্ন ধরনের ছাত্রের আকার, বাড়ি বাঁধা এবং মাঝে মাঝে মাঝে মাঝে মাঝে সাধারণ পুনরাবার ইউনিট জিপিউসে আমরা ১৬ বিট ফ্লানিং পয়েন্ট ট টেনসার কোর ব্যবহার করেছিলাম। সিপিউসে আমরা ৮ বিটের পরিমাণ ব্যবহার করেছি এবং অনেক প্রক্রিয়ার সাথে মাল্টিক মূল সেটের সাথে সাথে যোগাযোগ করেছি। মডেলের আকার কমানোর জন্য আমরা ৪ বিট লগের পরীক্ষা করেছি কিন্তু রান্টাইমে ফ্লোটগুলো ব্যবহার করেছি। শেয়ার কর্মসূচীতে আমাদের বেশীরভাগ উপস্থাপন ছিল প্যারেটো অপেক্ষায়, সময় এবং মানের মধ্যে ব্যবসায়িক সম্মানের</abstract_bn>
      <abstract_az>Biz Nural Generation and Translation 2020 İşçin in Bütün İşçinin İşçilərinə daxil olduq: single-core CPU, multi-core CPU və GPU. Model səviyyəsində, müəllimlər-öğrencilər təhsilini müxtəlif ölçülərlə istifadə edirik, körpüsü və bəzi səviyyələr istifadə edirik, sadəcə olaraq təhsil təhsilini istifadə edirik və başlığı təhsil edirik. GPUlar üzerində 16-bit yüksək nöqtələr tenzeri kullandıq. CPUlar barəsində, çoxlu-core ayarlarının bağlılığı ilə 8-bit kvantifikasyonu və çoxlu prosesləri müəyyən etdik. Model böyüklüyünü azaltmaq üçün 4-bit log kvantifikasiyası ilə imtahana çəkdik, amma hərəkət vaxtında floats kullandıq. Bu paylaşdığımız işdə, müsəlmanlarımızın çoxu zaman və keyfiyyəti arasındakı ticarət haqqında Pareto optimal idi.</abstract_az>
      <abstract_bs>Učestvovali smo u svim tragovima radionice o Neuralnoj generaciji i prevodi 2020. zajedničkom zadatku učinkovitosti: jedinstvenog procesora, multi core CPU i GPU. Na razini model a, koristimo trening učitelja i učitelja sa raznim veličinama učenika, ugrađenjem vezama i ponekad slojevima, koristimo Jednostavnu Jednostavnu povratnu jedinicu i predstavljamo glavnu pružanje. Na GPU, koristili smo 16-bit plivajućih tenzora. Na procesorima smo prilagodili kvantizaciju od 8 bita i višestruke procese sa afinitetom za višecore postavku. Da bismo smanjili veličinu modela, eksperimentirali smo sa kvantizacijom 4-bit dnevnika, ali koristili smo plove u provozu. U zajedničkom zadatku, većina naših podataka je bila Pareto optimalna s poštovanjem trgovine između vremena i kvalitete.</abstract_bs>
      <abstract_cs>Účastnili jsme se všech stop workshopu o neuronové generaci a překladu 2020 Efficiency Shared Task: jednojádrový CPU, vícejádrový CPU a GPU. Na úrovni modelu používáme školení učitelů-studentů s různými velikostmi studentů, vkládáním vazeb a někdy vrstvami, používáme jednodušší jednoduché opakované jednotky a zavádíme prořezávání hlavy. Na GPU jsme použili 16bitová tenzorová jádra s plovoucím bodem. U procesorů jsme přizpůsobili 8bitovou kvantizaci a více procesů s afinitou pro vícejádrové nastavení. Pro snížení velikosti modelu jsme experimentovali s 4-bitovou kvantizací log, ale používali jsme plováky za runtime. Ve společném úkolu byla většina našich podání Pareto optimální s ohledem na kompromis mezi časem a kvalitou.</abstract_cs>
      <abstract_et>Osalesime 2020. aasta neurogeneratsiooni ja tõlkimise töötoa kõikidel radadel: ühetuumaline protsessor, mitmetuumaline protsessor ja GPU. Mudeli tasandil kasutame õpetaja-õpilase koolitust erinevate õpilaste suuruste, lipsude manustamise ja mõnikord kihtidega, kasutame lihtsamat lihtsat korduvat üksust ja tutvustame pea lõikamist. GPU-del kasutasime 16-bitist ujuvpunktiga tensorsüdamikku. Protsessoritel kohandasime 8-bitist kvantiseerimist ja mitut protsessi mitme tuumaga seadistusega. Mudeli suuruse vähendamiseks katsetasime 4-bitist logikvantiseerimist, kuid kasutasime käivitusajal ujuvuid. Ühise ülesande puhul olid enamik meie ettepanekuid Pareto optimaalsed aja ja kvaliteedi vahelise kompromisse osas.</abstract_et>
      <abstract_fi>Osallistuimme kaikkiin hermojen tuottamista ja kﾃ､ﾃ､ntﾃ､mistﾃ､ kﾃ､sittelevﾃ､n tyﾃｶpajan kappaleisiin 2020 Efficiency Shared Task: yksiytiminen suoritin, moniytiminen suoritin ja grafiikkasuoritin. Mallitasolla kﾃ､ytﾃ､mme opettaja-opiskelijakoulutusta, jossa on erilaisia opiskelijakokoja, solmioita ja joskus kerroksia, kﾃ､ytﾃ､mme Simpler Simple Toistuva yksikkﾃｶ ja esittelemme pﾃ､ﾃ､n karsimista. Grafiikkanﾃ､ytﾃｶnohjaimilla kﾃ､ytimme 16-bittisiﾃ､ kelluvapistetensoriytimiﾃ､. Prosessoreilla rﾃ､ﾃ､tﾃ､lﾃｶimme 8-bittistﾃ､ kvantisointia ja useita prosesseja moniytimisiin asetuksiin. Mallin koon pienentﾃ､miseksi kokeilimme 4-bittistﾃ､ lokikivantointia, mutta kﾃ､ytﾃ､mme floatseja ajoaikana. Yhteisessﾃ､ tehtﾃ､vﾃ､ssﾃ､ suurin osa ehdotuksistamme oli Pareto-optimaalista ajan ja laadun vﾃ､lisen kompromissin suhteen.</abstract_fi>
      <abstract_ca>Vam participar en totes les pistes de la Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU i GPU. A nivell model, fem servir entrenament professor-estudiant amb una varietat de mida d'estudiants, enganxats i de vegades capes, fem servir l'Unitat Simple Recurrent, i introduïm pruning del cap. En GPU, vam utilitzar núcles de tensor flutuants de 16 bits. En els CPU, vam personalitzar la quantificació de 8 bits i múltiples processos amb afinitat a l'ajustament multinucli. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime.  En la tasca compartida, la majoria de les nostres proposicionseren Pareto óptims en relació amb el compromís entre temps i qualitat.</abstract_ca>
      <abstract_jv>Awak dhéwé wis ngubah barêng-barêng ning arep Workspace nang Generation Neral lan translation 2020 Efeffectness Joined tasks: single-coral Nang kudu model, kita gambar nggawe aturan guru kelas karo akeh pisan umut, ditambah barang nggawe layang lan sampek Layer, kuwi iso nggawe Simple Jejaring section Laptop" and "Desktop Nang ngomongke karo hal-hal nganggo kesempatan, akeh sing paling awak dhéwé Peretono sing paling nggawe ngupakan negoro kejahatan ning terakhir lan kaliwat</abstract_jv>
      <abstract_ha>@ info: whatsthis At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning.  Ga GPU, mun yi amfani da karatun tsumarni na guda na guda. Ga CPU, mun ƙayyade lissafar 8-bitan sami masu yawa da masu hushi wa daidaita mulki-nufi. Ga mu ƙara girmar shirin ayuka, sai muka jarraba tsarin logogin 4-bits kuma amma, mun yi amfani da floatsu idan na yi tafiya. Kuma a cikin aikin da aka raba, mafi yawanku da musuluntu sun kasance Parato mafificiya game da cinikin fara tsakanin lokaci da sifa.</abstract_ha>
      <abstract_sk>Sodelovali smo na vseh skladbah delavnice o nevralni generaciji in prevajanju 2020 Efficiency Shared Task: enojedrni procesor, večjedrni procesor in GPU. Na ravni modela uporabljamo usposabljanje učiteljev-učencev z različnimi velikostmi študentov, vgradnjo kravat in včasih plastmi, uporabljamo enoto enostavnejše ponavljajoče se enote in uvajamo obrezovanje glave. Na GPU smo uporabili 16-bitna tenzorska jedra s plavajočo točko. Na procesih smo prilagodili 8-bitno kvantizacijo in več procesov z afiniteto za večjedrno nastavitev. Za zmanjšanje velikosti modela smo eksperimentirali s 4-bitno kvantizacijo log, vendar uporabljali plovce v času delovanja. V skupni nalogi je bila večina naših prispevkov Pareto optimalna glede kompromisa med časom in kakovostjo.</abstract_sk>
      <abstract_he>השתתפנו בכל העקבות של הלימודים על דנרציה נוירולית ותרגום 2020 יעילות משימה משותפת: ברמה של המודל, אנו משתמשים באימונים מורים-סטודנטים עם מגוון של גודלים סטודנטים, קישורים קשרים ולפעמים שכבות, להשתמש ביחידה חדשה פשוטה יותר, ולהציג שיקוי ראש. על GPUs, השתמשנו בגרעני טנסור נקודת צף 16-ביט. על CPUs, אנחנו מתאימים 8-ביט קוונטיזציה ומספר תהליכים עם affinity לסטה multi-core. כדי להפחית את גודל המודל, ניסונו עם קיוונטיזציה של לוג 4-ביטים אבל להשתמש בציפורים בזמן הריצה. במשימה המשותפת, רוב ההצעות שלנו היו Pareto אופטימליים בכבוד הסחר בין זמן לאיכות.</abstract_he>
      <abstract_bo>We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. མ་དབུལ་གྱི་ཚད་ལྟར། ང་ཚོས་སློབ་ཆེན་པོ་སྣ་ཚོགས་ལས་སྦྱོར་བྱེད་སྐབས་ཡིག་ཆ་རྐྱེན་བྱས་ནས། ང་ཚོས་GPUདུ་16-bit floating-point tensor cores་སྤྱོད་པ་ཡིན། On CPUs, we customized 8-bit quantization and multiple processes with affinity for the multi-core setting. རྣམ་གྲངས་ཚད་དམའ་དགོས་བྱས་ན། ང་ཚོས་དྲན་ཐིག་གི་ཚད་ལྡན་བཞིན་པའི་བརྡ་སྟོན་བྱས་མིན་འདུག དབྱེ་སྤྱོད་མཁན་གྱི་ལས་འགུལ་དུ་མང་ཆེ་བ་ཡིན་ན་ང་ཚོའི་བསམ་འཆར་པ་ནི་དུས་ཚོད་དང་ཁྱད་པར་བརྗོད་སྤེལ་གྱི་</abstract_bo>
      </paper>
    <paper id="27">
      <title>Improving Document-Level Neural Machine Translation with Domain Adaptation</title>
      <author><first>Sami</first><last>Ul Haq</last></author>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>Arslan</first><last>Shoukat</last></author>
      <author><first>Noor-e-</first><last>Hira</last></author>
      <pages>225–231</pages>
      <abstract>Recent studies have shown that translation quality of NMT systems can be improved by providing document-level contextual information. In general sentence-based NMT models are extended to capture <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> from large-scale document-level corpora which are difficult to acquire. Domain adaptation on the other hand promises adapting components of already developed <a href="https://en.wikipedia.org/wiki/System">systems</a> by exploiting limited in-domain data. This paper presents FJWU’s system submission at WNGT, we specifically participated in Document level MT task for German-English translation. Our system is based on context-aware Transformer model developed on top of original NMT architecture by integrating contextual information using attention networks. Our experimental results show providing previous sentences as context significantly improves the BLEU score as compared to a strong NMT baseline. We also studied the impact of domain adaptation on document level translationand were able to improve results by adaptingthe <a href="https://en.wikipedia.org/wiki/System">systems</a> according to the testing domain.</abstract>
      <url hash="7a0ce3cc">2020.ngt-1.27</url>
      <doi>10.18653/v1/2020.ngt-1.27</doi>
      <bibkey>ul-haq-etal-2020-improving</bibkey>
    </paper>
    </volume>
</collection>