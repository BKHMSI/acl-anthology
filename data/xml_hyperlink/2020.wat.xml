<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.wat">
  <volume id="1" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the 7th Workshop on Asian Translation</booktitle>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Hideki</first><last>Nakayama</last></editor>
      <editor><first>Chenchen</first><last>Ding</last></editor>
      <editor><first>Raj</first><last>Dabre</last></editor>
      <editor><first>Anoop</first><last>Kunchukuttan</last></editor>
      <editor><first>Win Pa</first><last>Pa</last></editor>
      <editor><first>Ondřej</first><last>Bojar</last></editor>
      <editor><first>Shantipriya</first><last>Parida</last></editor>
      <editor><first>Isao</first><last>Goto</last></editor>
      <editor><first>Hidaya</first><last>Mino</last></editor>
      <editor><first>Hiroshi</first><last>Manabe</last></editor>
      <editor><first>Katsuhito</first><last>Sudoh</last></editor>
      <editor><first>Sadao</first><last>Kurohashi</last></editor>
      <editor><first>Pushpak</first><last>Bhattacharyya</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="e4d5969c">2020.wat-1.0</url>
      <bibkey>wat-2020-asian</bibkey>
    </frontmatter>
    <paper id="5">
      <title>Meta Ensemble for Japanese-Chinese Neural Machine Translation : Kyoto-U+ECNU Participation to WAT 2020<fixed-case>J</fixed-case>apanese-<fixed-case>C</fixed-case>hinese Neural Machine Translation: <fixed-case>K</fixed-case>yoto-<fixed-case>U</fixed-case>+<fixed-case>ECNU</fixed-case> Participation to <fixed-case>WAT</fixed-case> 2020</title>
      <author><first>Zhuoyuan</first><last>Mao</last></author>
      <author><first>Yibin</first><last>Shen</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <author><first>Cheqing</first><last>Jin</last></author>
      <pages>64–71</pages>
      <abstract>This paper describes the Japanese-Chinese Neural Machine Translation (NMT) system submitted by the joint team of Kyoto University and East China Normal University (Kyoto-U+ECNU) to WAT 2020 (Nakazawa et al.,2020). We participate in APSEC Japanese-Chinese translation task. We revisit several techniques for NMT including various architectures, different data selection and augmentation methods, denoising pre-training, and also some specific tricks for Japanese-Chinese translation. We eventually perform a meta ensemble to combine all of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> into a single <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese-Chinese translation.</abstract>
      <url hash="e7252315">2020.wat-1.5</url>
      <bibkey>mao-etal-2020-meta</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="8">
      <title>HW-TSC’s Participation in the WAT 2020 Indic Languages Multilingual Task<fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation in the <fixed-case>WAT</fixed-case> 2020 Indic Languages Multilingual Task</title>
      <author><first>Zhengzhe</first><last>Yu</last></author>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Xiaoyu</first><last>Chen</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <pages>92–97</pages>
      <abstract>This paper describes our work in the WAT 2020 Indic Multilingual Translation Task. We participated in all 7 language pairs (En-Bn / Hi / Gu / Ml / Mr / Ta / Te) in both directions under the constrained conditionusing only the officially provided data. Using <a href="https://en.wikipedia.org/wiki/Transformer">transformer</a> as a baseline, our Multi-En and En-Multi translation systems achieve the best performances. Detailed data filtering and data domain selection are the keys to performance enhancement in our experiment, with an average improvement of 2.6 BLEU scores for each language pair in the En-Multi system and an average improvement of 4.6 BLEU scores regarding the Multi-En. In addition, we employed language independent adapter to further improve the <a href="https://en.wikipedia.org/wiki/System">system</a> performances. Our submission obtains competitive results in the final evaluation.</abstract>
      <url hash="3d3cf08c">2020.wat-1.8</url>
      <bibkey>yu-etal-2020-hw</bibkey>
    </paper>
    <paper id="11">
      <title>Multimodal Neural Machine Translation for English to Hindi<fixed-case>E</fixed-case>nglish to <fixed-case>H</fixed-case>indi</title>
      <author><first>Sahinur Rahman</first><last>Laskar</last></author>
      <author><first>Abdullah Faiz Ur Rahman</first><last>Khilji</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>109–113</pages>
      <abstract>Machine translation (MT) focuses on the <a href="https://en.wikipedia.org/wiki/Machine_translation">automatic translation</a> of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-the-art results in the task of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> because of utilizing advanced deep learning techniques and handles issues like long-term dependency, and context-analysis. Nevertheless, NMT still suffers low translation quality for <a href="https://en.wikipedia.org/wiki/Linguistic_conservatism">low resource languages</a>. To encounter this challenge, the multi-modal concept comes in. The multi-modal concept combines textual and visual features to improve the translation quality of low resource languages. Moreover, the utilization of monolingual data in the pre-training step can improve the performance of the <a href="https://en.wikipedia.org/wiki/System">system</a> for low resource language translations. Workshop on Asian Translation 2020 (WAT2020) organized a translation task for multimodal translation in <a href="https://en.wikipedia.org/wiki/English_language">English</a> to <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>. We have participated in the same in two-track submission, namely text-only and multi-modal translation with team name CNLP-NITS. The evaluated results are declared at the WAT2020 translation task, which reports that our multi-modal NMT system attained higher scores than our text-only NMT on both challenge and evaluation test set. For the challenge test data, our multi-modal neural machine translation system achieves <a href="https://en.wikipedia.org/wiki/Bilingual_Evaluation_Understudy">Bilingual Evaluation Understudy (BLEU) score</a> of 33.57, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.754141, Adequacy-Fluency Metrics (AMFM) score 0.787320 and for evaluation test data, BLEU, RIBES, and, AMFM score of 40.51, 0.803208, and 0.820980 for English to Hindi translation respectively.</abstract>
      <url hash="aa3f66e3">2020.wat-1.11</url>
      <bibkey>laskar-etal-2020-multimodal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hindi-visual-genome">Hindi Visual Genome</pwcdataset>
    </paper>
    <paper id="14">
      <title>WT : Wipro AI Submissions to the WAT 2020<fixed-case>WT</fixed-case>: Wipro <fixed-case>AI</fixed-case> Submissions to the <fixed-case>WAT</fixed-case> 2020</title>
      <author><first>Santanu</first><last>Pal</last></author>
      <pages>122–126</pages>
      <abstract>In this paper we present an EnglishHindi and HindiEnglish neural machine translation (NMT) system, submitted to the Translation shared Task organized at WAT 2020. We trained a multilingual NMT system based on transformer architecture. In this paper we show : (i) how effective pre-processing helps to improve performance, (ii) how synthetic data through <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a> from available monolingual data can help in overall translation performance, (iii) how language similarity can aid more onto it. Our submissions ranked 1st in both English to Hindi and Hindi to English translation achieving <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> 20.80 and 29.59 respectively.</abstract>
      <url hash="d5c0308a">2020.wat-1.14</url>
      <bibkey>pal-2020-wt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="17">
      <title>The ADAPT Centre’s Neural MT Systems for the WAT 2020 Document-Level Translation Task<fixed-case>ADAPT</fixed-case> Centre’s Neural <fixed-case>MT</fixed-case> Systems for the <fixed-case>WAT</fixed-case> 2020 Document-Level Translation Task</title>
      <author><first>Wandri</first><last>Jooste</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>142–146</pages>
      <abstract>In this paper we describe the ADAPT Centre’s submissions to the WAT 2020 document-level Business Scene Dialogue (BSD) Translation task. We only consider translating from <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a> to <a href="https://en.wikipedia.org/wiki/English_language">English</a> for this task and we use the MarianNMT toolkit to train Transformer models. In order to improve the translation quality, we made use of both in-domain and out-of-domain data for training our Machine Translation (MT) systems, as well as various data augmentation techniques for fine-tuning the model parameters. This paper outlines the experiments we ran to train our <a href="https://en.wikipedia.org/wiki/System">systems</a> and report the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> achieved through these various experiments.</abstract>
      <url hash="5ac897bd">2020.wat-1.17</url>
      <bibkey>jooste-etal-2020-adapt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/business-scene-dialogue">Business Scene Dialogue</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="19">
      <title>Improving NMT via Filtered Back Translation<fixed-case>NMT</fixed-case> via Filtered Back Translation</title>
      <author><first>Nikhil</first><last>Jaiswal</last></author>
      <author><first>Mayur</first><last>Patidar</last></author>
      <author><first>Surabhi</first><last>Kumari</last></author>
      <author><first>Manasi</first><last>Patwardhan</last></author>
      <author><first>Shirish</first><last>Karande</last></author>
      <author><first>Puneet</first><last>Agarwal</last></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <pages>154–159</pages>
      <abstract>Document-Level Machine Translation (MT) has become an active research area among the NLP community in recent years. Unlike sentence-level MT, which translates the sentences independently, document-level MT aims to utilize <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> while translating a given source sentence. This paper demonstrates our submission (Team ID-DEEPNLP) to the Document-Level Translation task organized by WAT 2020. This task focuses on translating texts from a business dialog corpus while optionally utilizing the context present in the dialog. In our proposed approach, we utilize publicly available <a href="https://en.wikipedia.org/wiki/Parallel_corpus">parallel corpus</a> from different domains to train an open domain base NMT model. We then use monolingual target data to create filtered pseudo parallel data and employ <a href="https://en.wikipedia.org/wiki/Back-translation">Back-Translation</a> to fine-tune the base model. This is further followed by <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> on the domain-specific corpus. We also ensemble various <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> to improvise the <a href="https://en.wikipedia.org/wiki/Translation">translation</a> performance. Our best <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> achieve a <a href="https://en.wikipedia.org/wiki/BLEU">BLEU score</a> of 26.59 and 22.83 in an <a href="https://en.wikipedia.org/wiki/BLEU">unconstrained setting</a> and 15.10 and 10.91 in the <a href="https://en.wikipedia.org/wiki/BLEU">constrained settings</a> for <a href="https://en.wikipedia.org/wiki/BLEU">En-Ja &amp; Ja-En direction</a>, respectively.</abstract>
      <url hash="29250156">2020.wat-1.19</url>
      <bibkey>jaiswal-etal-2020-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="20">
      <title>A Parallel Evaluation Data Set of <a href="https://en.wikipedia.org/wiki/Software_documentation">Software Documentation</a> with Document Structure Annotation</title>
      <author><first>Bianka</first><last>Buschbeck</last></author>
      <author><first>Miriam</first><last>Exel</last></author>
      <pages>160–169</pages>
      <abstract>This paper accompanies the software documentation data set for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, a parallel evaluation data set of data originating from the SAP Help Portal, that we released to the machine translation community for research purposes. It offers the possibility to tune and evaluate <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a> in the domain of corporate software documentation and contributes to the availability of a wider range of evaluation scenarios. The data set comprises of the language pairs English to Hindi, <a href="https://en.wikipedia.org/wiki/Indonesian_language">Indonesian</a>, <a href="https://en.wikipedia.org/wiki/Malay_language">Malay</a> and <a href="https://en.wikipedia.org/wiki/Thai_language">Thai</a>, and thus also increases the test coverage for the many low-resource language pairs. Unlike most evaluation data sets that consist of plain parallel text, the segments in this <a href="https://en.wikipedia.org/wiki/Data_set">data set</a> come with additional metadata that describes structural information of the document context. We provide insights into the origin and creation, the particularities and characteristics of the <a href="https://en.wikipedia.org/wiki/Data_set">data set</a> as well as <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> results.</abstract>
      <url hash="9f2ed0b7">2020.wat-1.20</url>
      <bibkey>buschbeck-exel-2020-parallel</bibkey>
      <pwccode url="https://github.com/SAP/software-documentation-data-set-for-machine-translation" additional="false">SAP/software-documentation-data-set-for-machine-translation</pwccode>
    </paper>
    </volume>
</collection>