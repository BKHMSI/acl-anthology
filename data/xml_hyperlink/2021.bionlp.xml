<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.bionlp">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the 20th Workshop on Biomedical Language Processing</booktitle>
      <editor><first>Dina</first><last>Demner-Fushman</last></editor>
      <editor><first>Kevin Bretonnel</first><last>Cohen</last></editor>
      <editor><first>Sophia</first><last>Ananiadou</last></editor>
      <editor><first>Junichi</first><last>Tsujii</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="9d6b408e">2021.bionlp-1</url>
    </meta>
    <frontmatter>
      <url hash="9670b428">2021.bionlp-1.0</url>
      <bibkey>bionlp-2021-biomedical</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Scalable Few-Shot Learning of Robust Biomedical Name Representations</title>
      <author><first>Pieter</first><last>Fivez</last></author>
      <author><first>Simon</first><last>Suster</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>23–29</pages>
      <abstract>Recent research on robust representations of biomedical names has focused on modeling large amounts of fine-grained conceptual distinctions using complex neural encoders. In this paper, we explore the opposite <a href="https://en.wikipedia.org/wiki/Paradigm">paradigm</a> : training a simple encoder architecture using only small sets of <a href="https://en.wikipedia.org/wiki/Name">names</a> sampled from high-level biomedical concepts. Our encoder post-processes pretrained representations of biomedical names, and is effective for various types of input representations, both domain-specific or unsupervised. We validate our proposed few-shot learning approach on multiple biomedical relatedness benchmarks, and show that it allows for continual learning, where we accumulate information from various conceptual hierarchies to consistently improve encoder performance. Given these findings, we propose our approach as a low-cost alternative for exploring the impact of conceptual distinctions on robust biomedical name representations.</abstract>
      <url hash="15454166">2021.bionlp-1.3</url>
      <doi>10.18653/v1/2021.bionlp-1.3</doi>
      <bibkey>fivez-etal-2021-scalable</bibkey>
      <pwccode url="https://github.com/clips/fewshot-biomedical-names" additional="false">clips/fewshot-biomedical-names</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ehr-rel">EHR-Rel</pwcdataset>
    </paper>
    <paper id="4">
      <title>SAFFRON : tranSfer leArning For Food-disease RelatiOn extractioN<fixed-case>SAFFRON</fixed-case>: tran<fixed-case>S</fixed-case>fer le<fixed-case>A</fixed-case>rning For Food-disease <fixed-case>R</fixed-case>elati<fixed-case>O</fixed-case>n extractio<fixed-case>N</fixed-case></title>
      <author><first>Gjorgjina</first><last>Cenikj</last></author>
      <author><first>Tome</first><last>Eftimov</last></author>
      <author><first>Barbara</first><last>Koroušić Seljak</last></author>
      <pages>30–40</pages>
      <abstract>The accelerating growth of <a href="https://en.wikipedia.org/wiki/Big_data">big data</a> in the biomedical domain, with an endless amount of electronic health records and more than 30 million citations and abstracts in <a href="https://en.wikipedia.org/wiki/PubMed">PubMed</a>, introduces the need for automatic structuring of textual biomedical data. In this paper, we develop a method for detecting relations between food and disease entities from <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">raw text</a>. Due to the lack of annotated data on <a href="https://en.wikipedia.org/wiki/Food">food</a> with respect to health, we explore the feasibility of <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> by training BERT-based models on existing datasets annotated for the presence of cause and treat relations among different types of biomedical entities, and using them to recognize the same relations between <a href="https://en.wikipedia.org/wiki/Food">food and disease entities</a> in a dataset created for the purposes of this study. The best <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> achieve macro averaged F1 scores of 0.847 and 0.900 for the <a href="https://en.wikipedia.org/wiki/Causality">cause and treat relations</a>, respectively.</abstract>
      <url hash="b447feb6">2021.bionlp-1.4</url>
      <doi>10.18653/v1/2021.bionlp-1.4</doi>
      <bibkey>cenikj-etal-2021-saffron</bibkey>
    </paper>
    <paper id="8">
      <title>Overview of the MEDIQA 2021 Shared Task on <a href="https://en.wikipedia.org/wiki/Summarization">Summarization</a> in the Medical Domain<fixed-case>MEDIQA</fixed-case> 2021 Shared Task on Summarization in the Medical Domain</title>
      <author><first>Asma</first><last>Ben Abacha</last></author>
      <author><first>Yassine</first><last>Mrabet</last></author>
      <author><first>Yuhao</first><last>Zhang</last></author>
      <author><first>Chaitanya</first><last>Shivade</last></author>
      <author><first>Curtis</first><last>Langlotz</last></author>
      <author><first>Dina</first><last>Demner-Fushman</last></author>
      <pages>74–85</pages>
      <abstract>The MEDIQA 2021 shared tasks at the BioNLP 2021 workshop addressed three tasks on <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a> for medical text : (i) a question summarization task aimed at exploring new approaches to understanding complex real-world consumer health queries, (ii) a multi-answer summarization task that targeted aggregation of multiple relevant answers to a biomedical question into one concise and relevant answer, and (iii) a radiology report summarization task addressing the development of clinically relevant impressions from radiology report findings. Thirty-five teams participated in these shared tasks with sixteen working notes submitted (fifteen accepted) describing a wide variety of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> developed and tested on the shared and external datasets. In this paper, we describe the tasks, the <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, the <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> and techniques developed by various teams, the results of the <a href="https://en.wikipedia.org/wiki/Evaluation">evaluation</a>, and a study of correlations among various summarization evaluation measures. We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation.</abstract>
      <url hash="6915890f">2021.bionlp-1.8</url>
      <doi>10.18653/v1/2021.bionlp-1.8</doi>
      <bibkey>ben-abacha-etal-2021-overview</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mediqa-ans">MEDIQA-AnS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meqsum">MeQSum</pwcdataset>
    </paper>
    <paper id="9">
      <title>WBI at MEDIQA 2021 : Summarizing Consumer Health Questions with Generative Transformers<fixed-case>WBI</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2021: Summarizing Consumer Health Questions with Generative Transformers</title>
      <author><first>Mario</first><last>Sänger</last></author>
      <author><first>Leon</first><last>Weber</last></author>
      <author><first>Ulf</first><last>Leser</last></author>
      <pages>86–95</pages>
      <abstract>This paper describes our contribution for the MEDIQA-2021 Task 1 question summarization competition. We model the <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> as conditional generation problem. Our concrete pipeline performs a finetuning of the large pretrained generative transformers PEGASUS (Zhang et al.,2020a) and BART (Lewis et al.,2020). We used the resulting <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> as strong baselines and experimented with (i) integrating structured knowledge via entity embeddings, (ii) ensembling multiple generative models with the generator-discriminator framework and (iii) disentangling summarization and interrogative prediction to achieve further improvements. Our best performing model, a fine-tuned vanilla PEGASUS, reached the second place in the competition with an ROUGE-2-F1 score of 15.99. We observed that all of our additional measures hurt performance (up to 5.2 pp) on the official test set. In course of a post-hoc experimental analysis which uses a larger validation set results indicate slight performance improvements through the proposed extensions. However, further analysis is need to provide stronger evidence.</abstract>
      <url hash="b5ba736a">2021.bionlp-1.9</url>
      <doi>10.18653/v1/2021.bionlp-1.9</doi>
      <bibkey>sanger-etal-2021-wbi</bibkey>
    </paper>
    <paper id="11">
      <title>BDKG at MEDIQA 2021 : System Report for the Radiology Report Summarization Task<fixed-case>BDKG</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2021: System Report for the Radiology Report Summarization Task</title>
      <author><first>Songtai</first><last>Dai</last></author>
      <author><first>Quan</first><last>Wang</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <author><first>Yong</first><last>Zhu</last></author>
      <pages>103–111</pages>
      <abstract>This paper presents our winning system at the Radiology Report Summarization track of the MEDIQA 2021 shared task. Radiology report summarization automatically summarizes radiology findings into free-text impressions. This year’s task emphasizes the generalization and transfer ability of participating systems. Our <a href="https://en.wikipedia.org/wiki/System">system</a> is built upon a pre-trained Transformer encoder-decoder architecture, i.e., <a href="https://en.wikipedia.org/wiki/PEGASUS">PEGASUS</a>, deployed with an additional domain adaptation module to particularly handle the transfer and generalization issue. Heuristics like <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble</a> and <a href="https://en.wikipedia.org/wiki/Text_normalization">text normalization</a> are also used. Our system is conceptually simple yet highly effective, achieving a ROUGE-2 score of 0.436 on test set and ranked the 1st place among all participating systems.</abstract>
      <url hash="0ca5ce23">2021.bionlp-1.11</url>
      <doi>10.18653/v1/2021.bionlp-1.11</doi>
      <bibkey>dai-etal-2021-bdkg</bibkey>
    </paper>
    <paper id="12">
      <title>damo_nlp at MEDIQA 2021 : Knowledge-based Preprocessing and Coverage-oriented Reranking for Medical Question Summarization<fixed-case>MEDIQA</fixed-case> 2021: Knowledge-based Preprocessing and Coverage-oriented Reranking for Medical Question Summarization</title>
      <author><first>Yifan</first><last>He</last></author>
      <author><first>Mosha</first><last>Chen</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <pages>112–118</pages>
      <abstract>Medical question summarization is an important but difficult task, where the input is often complex and erroneous while annotated data is expensive to acquire. We report our participation in the MEDIQA 2021 question summarization task in which we are required to address these challenges. We start from pre-trained conditional generative language models, use <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a> to help correct input errors, and rerank single system outputs to boost coverage. Experimental results show significant improvement in string-based metrics.</abstract>
      <url hash="1040da26">2021.bionlp-1.12</url>
      <doi>10.18653/v1/2021.bionlp-1.12</doi>
      <bibkey>he-etal-2021-damo</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/meqsum">MeQSum</pwcdataset>
    </paper>
    <paper id="13">
      <title>Stress Test Evaluation of Biomedical Word Embeddings</title>
      <author><first>Vladimir</first><last>Araujo</last></author>
      <author><first>Andrés</first><last>Carvallo</last></author>
      <author><first>Carlos</first><last>Aspillaga</last></author>
      <author><first>Camilo</first><last>Thorne</last></author>
      <author><first>Denis</first><last>Parra</last></author>
      <pages>119–125</pages>
      <abstract>The success of pretrained word embeddings has motivated their use in the biomedical domain, with contextualized embeddings yielding remarkable results in several biomedical NLP tasks. However, there is a lack of research on quantifying their behavior under <a href="https://en.wikipedia.org/wiki/Stress_(biology)">severe stress scenarios</a>. In this work, we systematically evaluate three language models with adversarial examples   automatically constructed tests that allow us to examine how robust the <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> are. We propose two types of stress scenarios focused on the biomedical named entity recognition (NER) task, one inspired by spelling errors and another based on the use of synonyms for medical terms. Our experiments with three <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> show that the performance of the original <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> decreases considerably, in addition to revealing their weaknesses and strengths. Finally, we show that adversarial training causes the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to improve their robustness and even to exceed the original performance in some cases.</abstract>
      <url hash="34b6a8cc">2021.bionlp-1.13</url>
      <doi>10.18653/v1/2021.bionlp-1.13</doi>
      <bibkey>araujo-etal-2021-stress</bibkey>
      <pwccode url="https://github.com/ialab-puc/BioNLP-StressTest" additional="false">ialab-puc/BioNLP-StressTest</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
    </paper>
    <paper id="16">
      <title>BioELECTRA : Pretrained Biomedical text Encoder using Discriminators<fixed-case>B</fixed-case>io<fixed-case>ELECTRA</fixed-case>:Pretrained Biomedical text Encoder using Discriminators</title>
      <author><first>Kamal raj</first><last>Kanakarajan</last></author>
      <author><first>Bhuvana</first><last>Kundumani</last></author>
      <author><first>Malaikannan</first><last>Sankarasubbu</last></author>
      <pages>143–154</pages>
      <abstract>Recent advancements in pretraining strategies in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> have shown a significant improvement in the performance of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on various text mining tasks. We apply ‘replaced token detection’ pretraining technique proposed by ELECTRA and pretrain a biomedical language model from scratch using biomedical text and vocabulary. We introduce BioELECTRA, a biomedical domain-specific language encoder model that adapts ELECTRA for the Biomedical domain. WE evaluate our model on the BLURB and BLUE biomedical NLP benchmarks. BioELECTRA outperforms the previous models and achieves state of the art (SOTA) on all the 13 datasets in BLURB benchmark and on all the 4 Clinical datasets from BLUE Benchmark across 7 different NLP tasks. BioELECTRA pretrained on PubMed and PMC full text articles performs very well on Clinical datasets as well. BioELECTRA achieves new SOTA 86.34%(1.39 % accuracy improvement) on MedNLI and 64 % (2.98 % accuracy improvement) on PubMedQA dataset.</abstract>
      <url hash="eeff0c07">2021.bionlp-1.16</url>
      <doi>10.18653/v1/2021.bionlp-1.16</doi>
      <bibkey>kanakarajan-etal-2021-bioelectra</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/biosses">BIOSSES</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/blurb">BLURB</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/chemprot">ChemProt</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ddi">DDI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hoc-1">HOC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mednli">MedNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed-pico-element-detection-dataset">PubMed PICO Element Detection Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmedqa">PubMedQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="17">
      <title>Word centrality constrained representation for keyphrase extraction</title>
      <author><first>Zelalem</first><last>Gero</last></author>
      <author><first>Joyce</first><last>Ho</last></author>
      <pages>155–161</pages>
      <abstract>To keep pace with the increased generation and digitization of documents, automated methods that can improve <a href="https://en.wikipedia.org/wiki/Search_engine_technology">search</a>, discovery and mining of the vast body of literature are essential. Keyphrases provide a concise representation by identifying salient concepts in a document. Various <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised approaches</a> model <a href="https://en.wikipedia.org/wiki/Keyphrase_extraction">keyphrase extraction</a> using local context to predict the label for each token and perform much better than the <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised counterparts</a>. Unfortunately, this <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> fails for short documents where the context is unclear. Moreover, <a href="https://en.wikipedia.org/wiki/Keyword_(linguistics)">keyphrases</a>, which are usually the gist of a document, need to be the central theme. We propose a new extraction model that introduces a <a href="https://en.wikipedia.org/wiki/Centrality">centrality constraint</a> to enrich the word representation of a Bidirectional long short-term memory. Performance evaluation on 2 publicly available datasets demonstrate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms existing state-of-the art approaches.</abstract>
      <url hash="ab295abd">2021.bionlp-1.17</url>
      <doi>10.18653/v1/2021.bionlp-1.17</doi>
      <bibkey>gero-ho-2021-word</bibkey>
      <pwccode url="https://github.com/zhgero/keyphrases_centrality" additional="false">zhgero/keyphrases_centrality</pwccode>
    </paper>
    <paper id="18">
      <title>End-to-end Biomedical Entity Linking with Span-based Dictionary Matching</title>
      <author><first>Shogo</first><last>Ujiie</last></author>
      <author><first>Hayate</first><last>Iso</last></author>
      <author><first>Shuntaro</first><last>Yada</last></author>
      <author><first>Shoko</first><last>Wakamiya</last></author>
      <author><first>Eiji</first><last>Aramaki</last></author>
      <pages>162–167</pages>
      <abstract>Disease name recognition and <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalization</a> is a fundamental process in <a href="https://en.wikipedia.org/wiki/Biomedical_text_mining">biomedical text mining</a>. Recently, neural joint learning of both <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> has been proposed to utilize the mutual benefits. While this approach achieves high performance, disease concepts that do not appear in the training dataset can not be accurately predicted. This study introduces a novel <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end approach</a> that combines span representations with dictionary-matching features to address this problem. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> handles unseen concepts by referring to a <a href="https://en.wikipedia.org/wiki/Dictionary">dictionary</a> while maintaining the performance of neural network-based models. Experiments using two major datasaets demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieved competitive results with strong baselines, especially for unseen concepts during training.</abstract>
      <url hash="21860744">2021.bionlp-1.18</url>
      <doi>10.18653/v1/2021.bionlp-1.18</doi>
      <bibkey>ujiie-etal-2021-end</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
    </paper>
    <paper id="26">
      <title>Context-aware query design combines knowledge and data for efficient reading and reasoning</title>
      <author><first>Emilee</first><last>Holtzapple</last></author>
      <author><first>Brent</first><last>Cochran</last></author>
      <author><first>Natasa</first><last>Miskov-Zivanov</last></author>
      <pages>238–246</pages>
      <abstract>The amount of biomedical literature has vastly increased over the past few decades. As a result, the sheer quantity of accessible information is overwhelming, and complicates manual information retrieval. Automated methods seek to speed up <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a> from <a href="https://en.wikipedia.org/wiki/Medical_literature">biomedical literature</a>. However, such automated methods are still too time-intensive to survey all existing biomedical literature. We present a <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> for automatically generating literature queries that select relevant papers based on <a href="https://en.wikipedia.org/wiki/Data">biological data</a>. By using <a href="https://en.wikipedia.org/wiki/Gene_expression">differentially expressed genes</a> to inform our literature searches, we focus <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a> on mechanistic signaling details that are crucial for the disease or context of interest.</abstract>
      <url hash="b3448888">2021.bionlp-1.26</url>
      <doi>10.18653/v1/2021.bionlp-1.26</doi>
      <bibkey>holtzapple-etal-2021-context</bibkey>
    </paper>
    <paper id="27">
      <title>Measuring the relative importance of full text sections for <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a> from <a href="https://en.wikipedia.org/wiki/Scientific_literature">scientific literature</a>.</title>
      <author><first>Lana</first><last>Yeganova</last></author>
      <author><first>Won Gyu</first><last>Kim</last></author>
      <author><first>Donald</first><last>Comeau</last></author>
      <author><first>W John</first><last>Wilbur</last></author>
      <author><first>Zhiyong</first><last>Lu</last></author>
      <pages>247–256</pages>
      <abstract>With the growing availability of full-text articles, integrating <a href="https://en.wikipedia.org/wiki/Abstract_(summary)">abstracts</a> and full texts of documents into a unified representation is essential for comprehensive search of scientific literature. However, previous studies have shown that navely merging <a href="https://en.wikipedia.org/wiki/Abstract_(summary)">abstracts</a> with full texts of articles does not consistently yield better performance. Balancing the contribution of query terms appearing in the abstract and in sections of different importance in full text articles remains a challenge both with traditional bag-of-words IR approaches and for neural retrieval methods. In this work we establish the connection between the BM25 score of a query term appearing in a section of a full text document and the probability of that document being clicked or identified as relevant. Probability is computed using Pool Adjacent Violators (PAV), an isotonic regression algorithm, providing a <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimate</a> based on the observed data. Using this probabilistic transformation of BM25 scores we show an improved performance on the PubMed Click dataset developed and presented in this study, as well as the 2007 TREC Genomics collection.</abstract>
      <url hash="7ed457fb">2021.bionlp-1.27</url>
      <doi>10.18653/v1/2021.bionlp-1.27</doi>
      <bibkey>yeganova-etal-2021-measuring</bibkey>
    </paper>
    <paper id="31">
      <title>SB_NITK at MEDIQA 2021 : Leveraging Transfer Learning for Question Summarization in Medical Domain<fixed-case>SB</fixed-case>_<fixed-case>NITK</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2021: Leveraging Transfer Learning for Question Summarization in Medical Domain</title>
      <author><first>Spandana</first><last>Balumuri</last></author>
      <author><first>Sony</first><last>Bachina</last></author>
      <author><first>Sowmya</first><last>Kamath S</last></author>
      <pages>273–279</pages>
      <abstract>Recent strides in the healthcare domain, have resulted in vast quantities of <a href="https://en.wikipedia.org/wiki/Streaming_data">streaming data</a> available for use for building intelligent knowledge-based applications. However, the challenges introduced to the huge volume, velocity of generation, variety and variability of this medical data have to be adequately addressed. In this paper, we describe the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and results for our submission at MEDIQA 2021 Question Summarization shared task. In order to improve the performance of summarization of consumer health questions, our method explores the use of <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> to utilize the knowledge of NLP transformers like BART, T5 and <a href="https://en.wikipedia.org/wiki/PEGASUS">PEGASUS</a>. The proposed models utilize the knowledge of pre-trained NLP transformers to achieve improved results when compared to conventional deep learning models such as LSTM, RNN etc. Our team SB_NITK ranked 12th among the total 22 submissions in the official final rankings. Our BART based model achieved a ROUGE-2 F1 score of 0.139.</abstract>
      <url hash="88abd0d7">2021.bionlp-1.31</url>
      <doi>10.18653/v1/2021.bionlp-1.31</doi>
      <bibkey>balumuri-etal-2021-sb</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meqsum">MeQSum</pwcdataset>
    </paper>
    <paper id="33">
      <title>QIAI at MEDIQA 2021 : Multimodal Radiology Report Summarization<fixed-case>QIAI</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2021: Multimodal Radiology Report Summarization</title>
      <author><first>Jean-Benoit</first><last>Delbrouck</last></author>
      <author><first>Cassie</first><last>Zhang</last></author>
      <author><first>Daniel</first><last>Rubin</last></author>
      <pages>285–290</pages>
      <abstract>This paper describes the solution of the QIAI lab sent to the Radiology Report Summarization (RRS) challenge at MEDIQA 2021. This paper aims to investigate whether using <a href="https://en.wikipedia.org/wiki/Multimodality">multimodality</a> during training improves the summarizing performances of the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> at test-time. Our preliminary results shows that taking advantage of the visual features from the <a href="https://en.wikipedia.org/wiki/X-ray">x-rays</a> associated to the radiology reports leads to higher evaluation metrics compared to a text-only baseline system. These improvements are reported according to the automatic evaluation metrics METEOR, <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> and ROUGE scores. Our experiments can be fully replicated at the following address : https:// github.com/jbdel/vilmedic.</abstract>
      <url hash="776cfe25">2021.bionlp-1.33</url>
      <doi>10.18653/v1/2021.bionlp-1.33</doi>
      <bibkey>delbrouck-etal-2021-qiai</bibkey>
      <pwccode url="https://github.com/jbdel/vilmedic" additional="false">jbdel/vilmedic</pwccode>
    </paper>
    <paper id="37">
      <title>MNLP at MEDIQA 2021 : Fine-Tuning PEGASUS for Consumer Health Question Summarization<fixed-case>MNLP</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2021: Fine-Tuning <fixed-case>PEGASUS</fixed-case> for Consumer Health Question Summarization</title>
      <author><first>Jooyeon</first><last>Lee</last></author>
      <author><first>Huong</first><last>Dang</last></author>
      <author><first>Ozlem</first><last>Uzuner</last></author>
      <author><first>Sam</first><last>Henry</last></author>
      <pages>320–327</pages>
      <abstract>This paper details a Consumer Health Question (CHQ) summarization model submitted to MEDIQA 2021 for shared task 1 : Question Summarization. Many CHQs are composed of multiple sentences with typos or unnecessary information, which can interfere with automated question answering systems. Question summarization mitigates this issue by removing this unnecessary information, aiding automated systems in generating a more accurate summary. Our summarization approach focuses on applying multiple pre-processing techniques, including question focus identification on the input and the development of an ensemble method to combine question focus with an abstractive summarization method. We use the state-of-art abstractive summarization model, PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization), to generate abstractive summaries. Our experiments show that using our ensemble method, which combines abstractive summarization with question focus identification, improves performance over using <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a> alone. Our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> shows a ROUGE-2 F-measure of 11.14 % against the official test dataset.</abstract>
      <url hash="95c2c2b9">2021.bionlp-1.37</url>
      <doi>10.18653/v1/2021.bionlp-1.37</doi>
      <bibkey>lee-etal-2021-mnlp</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/meqsum">MeQSum</pwcdataset>
    </paper>
    </volume>
</collection>