<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.ngt">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Neural Generation and Translation</booktitle>
      <editor><first>Alexandra</first><last>Birch</last></editor>
      <editor><first>Andrew</first><last>Finch</last></editor>
      <editor><first>Hiroaki</first><last>Hayashi</last></editor>
      <editor><first>Kenneth</first><last>Heafield</last></editor>
      <editor><first>Marcin</first><last>Junczys-Dowmunt</last></editor>
      <editor><first>Ioannis</first><last>Konstas</last></editor>
      <editor><first>Xian</first><last>Li</last></editor>
      <editor><first>Graham</first><last>Neubig</last></editor>
      <editor><first>Yusuke</first><last>Oda</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="35b7d8df">2020.ngt-1</url>
    </meta>
    <frontmatter>
      <url hash="ce0cb7a8">2020.ngt-1.0</url>
      <bibkey>ngt-2020-neural</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Learning to Generate Multiple Style Transfer Outputs for an Input Sentence</title>
      <author><first>Kevin</first><last>Lin</last></author>
      <author><first>Ming-Yu</first><last>Liu</last></author>
      <author><first>Ming-Ting</first><last>Sun</last></author>
      <author><first>Jan</first><last>Kautz</last></author>
      <pages>10–23</pages>
      <abstract>Text style transfer refers to the task of rephrasing a given text in a different style. While various methods have been proposed to advance the state of the art, they often assume the transfer output follows a <a href="https://en.wikipedia.org/wiki/Delta_distribution">delta distribution</a>, and thus their <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can not generate different style transfer results for a given input text. To address the limitation, we propose a one-to-many text style transfer framework. In contrast to prior works that learn a <a href="https://en.wikipedia.org/wiki/One-to-one_mapping">one-to-one mapping</a> that converts an input sentence to one output sentence, our approach learns a one-to-many mapping that can convert an input sentence to multiple different output sentences, while preserving the input content. This is achieved by applying <a href="https://en.wikipedia.org/wiki/Adversarial_system">adversarial training</a> with a latent decomposition scheme. Specifically, we decompose the latent representation of the input sentence to a style code that captures the language style variation and a content code that encodes the language style-independent content. We then combine the <a href="https://en.wikipedia.org/wiki/Content_(media)">content code</a> with the <a href="https://en.wikipedia.org/wiki/Style_sheet_(web_development)">style code</a> for generating a style transfer output. By combining the same <a href="https://en.wikipedia.org/wiki/Content_(media)">content code</a> with a different <a href="https://en.wikipedia.org/wiki/Style_sheet_(web_development)">style code</a>, we generate a different style transfer output. Extensive experimental results with comparisons to several text style transfer approaches on multiple public datasets using a diverse set of performance metrics validate effectiveness of the proposed approach.</abstract>
      <url hash="0ad9ac22">2020.ngt-1.2</url>
      <doi>10.18653/v1/2020.ngt-1.2</doi>
      <video href="http://slideslive.com/38929815" />
      <bibkey>lin-etal-2020-learning</bibkey>
    </paper>
    <paper id="6">
      <title>Automatically Ranked Russian Paraphrase Corpus for Text Generation<fixed-case>R</fixed-case>ussian Paraphrase Corpus for Text Generation</title>
      <author><first>Vadim</first><last>Gudkov</last></author>
      <author><first>Olga</first><last>Mitrofanova</last></author>
      <author><first>Elizaveta</first><last>Filippskikh</last></author>
      <pages>54–59</pages>
      <abstract>The article is focused on automatic development and ranking of a large corpus for Russian paraphrase generation which proves to be the first <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> of such type in Russian computational linguistics. Existing manually annotated paraphrase datasets for Russian are limited to small-sized ParaPhraser corpus and ParaPlag which are suitable for a set of NLP tasks, such as paraphrase and plagiarism detection, sentence similarity and relatedness estimation, etc. Due to size restrictions, these <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> can hardly be applied in end-to-end text generation solutions. Meanwhile, <a href="https://en.wikipedia.org/wiki/Paraphrase_generation">paraphrase generation</a> requires a large amount of training data. In our study we propose a solution to the problem : we collect, rank and evaluate a new publicly available headline paraphrase corpus (ParaPhraser Plus), and then perform text generation experiments with manual evaluation on automatically ranked corpora using the Universal Transformer architecture.</abstract>
      <url hash="88a4de10">2020.ngt-1.6</url>
      <doi>10.18653/v1/2020.ngt-1.6</doi>
      <video href="http://slideslive.com/38929819" />
      <bibkey>gudkov-etal-2020-automatically</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opusparcus">Opusparcus</pwcdataset>
    </paper>
    <paper id="12">
      <title>Distill, Adapt, Distill : Training Small, In-Domain Models for Neural Machine Translation</title>
      <author><first>Mitchell</first><last>Gordon</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <pages>110–118</pages>
      <abstract>We explore best practices for training small, memory efficient <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation models</a> with sequence-level knowledge distillation in the domain adaptation setting. While both <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> and knowledge distillation are widely-used, their interaction remains little understood. Our large-scale empirical results in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> (on three language pairs with three domains each) suggest distilling twice for best performance : once using general-domain data and again using in-domain data with an adapted teacher.</abstract>
      <url hash="9cd128f3">2020.ngt-1.12</url>
      <doi>10.18653/v1/2020.ngt-1.12</doi>
      <video href="http://slideslive.com/38929825" />
      <bibkey>gordon-duh-2020-distill</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="17">
      <title>The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task<fixed-case>ADAPT</fixed-case> System Description for the <fixed-case>STAPLE</fixed-case> 2020 <fixed-case>E</fixed-case>nglish-to-<fixed-case>P</fixed-case>ortuguese Translation Task</title>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Yasmin</first><last>Moslem</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>144–152</pages>
      <abstract>This paper describes the ADAPT Centre’s submission to STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, a shared task of the 4th Workshop on Neural Generation and Translation (WNGT), for the English-to-Portuguese translation task. In this shared task, the participants were asked to produce high-coverage sets of plausible translations given English prompts (input source sentences). We present our English-to-Portuguese machine translation (MT) models that were built applying various strategies, e.g. data and sentence selection, monolingual MT for generating alternative translations, and combining multiple n-best translations. Our experiments show that adding the aforementioned techniques to the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> yields an excellent performance in the English-to-Portuguese translation task.</abstract>
      <url hash="ba8bc9ae">2020.ngt-1.17</url>
      <doi>10.18653/v1/2020.ngt-1.17</doi>
      <video href="http://slideslive.com/38929831" />
      <bibkey>haque-etal-2020-adapt</bibkey>
    </paper>
    <paper id="25">
      <title>Efficient and High-Quality <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a> with OpenNMT<fixed-case>O</fixed-case>pen<fixed-case>NMT</fixed-case></title>
      <author><first>Guillaume</first><last>Klein</last></author>
      <author><first>Dakun</first><last>Zhang</last></author>
      <author><first>Clément</first><last>Chouteau</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <pages>211–217</pages>
      <abstract>This paper describes the OpenNMT submissions to the WNGT 2020 efficiency shared task. We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies. By combining additional <a href="https://en.wikipedia.org/wiki/Optimizing_compiler">optimizations</a> and <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallelization techniques</a>, we create small, efficient, and high-quality neural machine translation models.</abstract>
      <url hash="65454d9c">2020.ngt-1.25</url>
      <doi>10.18653/v1/2020.ngt-1.25</doi>
      <video href="http://slideslive.com/38929839" />
      <bibkey>klein-etal-2020-efficient</bibkey>
    </paper>
    <paper id="26">
      <title>Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task<fixed-case>E</fixed-case>dinburgh’s Submissions to the 2020 Machine Translation Efficiency Task</title>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <author><first>Maximiliana</first><last>Behnke</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <author><first>Sidharth</first><last>Kashyap</last></author>
      <author><first>Emmanouil-Ioannis</first><last>Farsarakis</last></author>
      <author><first>Mateusz</first><last>Chudyk</last></author>
      <pages>218–224</pages>
      <abstract>We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task : <a href="https://en.wikipedia.org/wiki/Single-core">single-core CPU</a>, <a href="https://en.wikipedia.org/wiki/Multi-core_processor">multi-core CPU</a>, and <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a>. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. On <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPUs</a>, we used 16-bit floating-point tensor cores. On <a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPUs</a>, we customized 8-bit quantization and <a href="https://en.wikipedia.org/wiki/Multiprocessing">multiple processes</a> with affinity for the <a href="https://en.wikipedia.org/wiki/Multi-core_processor">multi-core setting</a>. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.</abstract>
      <url hash="b267cea9">2020.ngt-1.26</url>
      <doi>10.18653/v1/2020.ngt-1.26</doi>
      <attachment type="Dataset" hash="ffd898b7">2020.ngt-1.26.Dataset.txt</attachment>
      <video href="http://slideslive.com/38929840" />
      <bibkey>bogoychev-etal-2020-edinburghs</bibkey>
    </paper>
    <paper id="27">
      <title>Improving Document-Level Neural Machine Translation with Domain Adaptation</title>
      <author><first>Sami</first><last>Ul Haq</last></author>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>Arslan</first><last>Shoukat</last></author>
      <author><first>Noor-e-</first><last>Hira</last></author>
      <pages>225–231</pages>
      <abstract>Recent studies have shown that translation quality of NMT systems can be improved by providing document-level contextual information. In general sentence-based NMT models are extended to capture <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> from large-scale document-level corpora which are difficult to acquire. Domain adaptation on the other hand promises adapting components of already developed <a href="https://en.wikipedia.org/wiki/System">systems</a> by exploiting limited in-domain data. This paper presents FJWU’s system submission at WNGT, we specifically participated in Document level MT task for German-English translation. Our system is based on context-aware Transformer model developed on top of original NMT architecture by integrating contextual information using attention networks. Our experimental results show providing previous sentences as context significantly improves the BLEU score as compared to a strong NMT baseline. We also studied the impact of domain adaptation on document level translationand were able to improve results by adaptingthe <a href="https://en.wikipedia.org/wiki/System">systems</a> according to the testing domain.</abstract>
      <url hash="7a0ce3cc">2020.ngt-1.27</url>
      <doi>10.18653/v1/2020.ngt-1.27</doi>
      <bibkey>ul-haq-etal-2020-improving</bibkey>
    </paper>
    </volume>
</collection>