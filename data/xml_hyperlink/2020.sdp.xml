<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.sdp">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the First Workshop on Scholarly Document Processing</booktitle>
      <editor><first>Muthu Kumar</first><last>Chandrasekaran</last></editor>
      <editor><first>Anita</first><last>de Waard</last></editor>
      <editor><first>Guy</first><last>Feigenblat</last></editor>
      <editor><first>Dayne</first><last>Freitag</last></editor>
      <editor><first>Tirthankar</first><last>Ghosal</last></editor>
      <editor><first>Eduard</first><last>Hovy</last></editor>
      <editor><first>Petr</first><last>Knoth</last></editor>
      <editor><first>David</first><last>Konopnicki</last></editor>
      <editor><first>Philipp</first><last>Mayr</last></editor>
      <editor><first>Robert M.</first><last>Patton</last></editor>
      <editor><first>Michal</first><last>Shmueli-Scheuer</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="a32476e4">2020.sdp-1.0</url>
      <bibkey>sdp-2020-scholarly</bibkey>
    </frontmatter>
    <paper id="2">
      <title>The future of <a href="https://en.wikipedia.org/wiki/ArXiv">arXiv</a> and <a href="https://en.wikipedia.org/wiki/Discovery_(observation)">knowledge discovery</a> in open science<fixed-case>X</fixed-case>iv and knowledge discovery in open science</title>
      <author><first>Steinn</first><last>Sigurdsson</last></author>
      <pages>7–9</pages>
      <abstract>arXiv, the preprint server for the physical and mathematical sciences, is in its third decade of operation. As the flow of new, open access research increases inexorably, the challenges to keep up with and discover research content also become greater. I will discuss the status and future of <a href="https://en.wikipedia.org/wiki/ArXiv">arXiv</a>, and possibilities and plans to make more effective use of the research database to enhance ongoing research efforts.</abstract>
      <url hash="5babaa19">2020.sdp-1.2</url>
      <doi>10.18653/v1/2020.sdp-1.2</doi>
      <bibkey>sigurdsson-2020-future</bibkey>
    </paper>
    <paper id="3">
      <title>Acknowledgement Entity Recognition in CORD-19 Papers<fixed-case>CORD</fixed-case>-19 Papers</title>
      <author><first>Jian</first><last>Wu</last></author>
      <author><first>Pei</first><last>Wang</last></author>
      <author><first>Xin</first><last>Wei</last></author>
      <author><first>Sarah</first><last>Rajtmajer</last></author>
      <author><first>C. Lee</first><last>Giles</last></author>
      <author><first>Christopher</first><last>Griffin</last></author>
      <pages>10–19</pages>
      <abstract>Acknowledgements are ubiquitous in <a href="https://en.wikipedia.org/wiki/Academic_publishing">scholarly papers</a>. Existing acknowledgement entity recognition methods assume all named entities are acknowledged. Here, we examine the nuances between acknowledged and named entities by analyzing <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence structure</a>. We develop an acknowledgement extraction system, AckExtract based on open-source text mining software and evaluate our method using manually labeled data. AckExtract uses the PDF of a scholarly paper as input and outputs <a href="https://en.wikipedia.org/wiki/Acknowledgement_(data_networks)">acknowledgement entities</a>. Results show an overall performance of F_1=0.92. We built a supplementary database by linking CORD-19 papers with acknowledgement entities extracted by AckExtract including persons and organizations and find that only up to 5060 % of named entities are actually acknowledged. We further analyze chronological trends of acknowledgement entities in CORD-19 papers. All codes and labeled data are publicly available at https://github.com/lamps-lab/ackextract.</abstract>
      <url hash="dc7c6b4d">2020.sdp-1.3</url>
      <doi>10.18653/v1/2020.sdp-1.3</doi>
      <video href="https://slideslive.com/38940712" />
      <bibkey>wu-etal-2020-acknowledgement</bibkey>
      <pwccode url="https://github.com/lamps-lab/ackextract" additional="false">lamps-lab/ackextract</pwccode>
    </paper>
    <paper id="7">
      <title>Effective <a href="https://en.wikipedia.org/wiki/Distributed_representation">distributed representations</a> for academic expert search</title>
      <author><first>Mark</first><last>Berger</last></author>
      <author><first>Jakub</first><last>Zavrel</last></author>
      <author><first>Paul</first><last>Groth</last></author>
      <pages>56–71</pages>
      <abstract>Expert search aims to find and rank experts based on a user’s query. In <a href="https://en.wikipedia.org/wiki/Academy">academia</a>, retrieving experts is an efficient way to navigate through a large amount of <a href="https://en.wikipedia.org/wiki/Outline_of_academic_disciplines">academic knowledge</a>. Here, we study how different distributed representations of academic papers (i.e. embeddings) impact academic expert retrieval. We use the Microsoft Academic Graph dataset and experiment with different configurations of a document-centric voting model for retrieval. In particular, we explore the impact of the use of contextualized embeddings on <a href="https://en.wikipedia.org/wiki/Web_search_engine">search</a> performance. We also present results for paper embeddings that incorporate <a href="https://en.wikipedia.org/wiki/Citation">citation information</a> through <a href="https://en.wikipedia.org/wiki/Retrofitting">retrofitting</a>. Additionally, experiments are conducted using different <a href="https://en.wikipedia.org/wiki/Scientific_technique">techniques</a> for assigning author weights based on <a href="https://en.wikipedia.org/wiki/Author_order">author order</a>. We observe that using contextual embeddings produced by a transformer model trained for sentence similarity tasks produces the most effective paper representations for document-centric expert retrieval. However, retrofitting the paper embeddings and using elaborate author contribution weighting strategies did not improve retrieval performance.</abstract>
      <url hash="b792b177">2020.sdp-1.7</url>
      <doi>10.18653/v1/2020.sdp-1.7</doi>
      <video href="https://slideslive.com/38940716" />
      <bibkey>berger-etal-2020-effective</bibkey>
      <pwccode url="https://github.com/mabergerx/SDP500_expert_search" additional="false">mabergerx/SDP500_expert_search</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/microsoft-academic-graph">Microsoft Academic Graph</pwcdataset>
    </paper>
    <paper id="14">
      <title>Multi-task Peer-Review Score Prediction</title>
      <author><first>Jiyi</first><last>Li</last></author>
      <author><first>Ayaka</first><last>Sato</last></author>
      <author><first>Kazuya</first><last>Shimura</last></author>
      <author><first>Fumiyo</first><last>Fukumoto</last></author>
      <pages>121–126</pages>
      <abstract>Automatic prediction on the peer-review aspect scores of <a href="https://en.wikipedia.org/wiki/Academic_publishing">academic papers</a> can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi-task approach to leverage additional information from other aspects of scores for improving the performance of the target. Because one of the problems of building multi-task models is how to select the proper resources of auxiliary tasks and how to select the proper shared structures. We propose a multi-task shared structure encoding approach which automatically selects good shared network structures as well as good auxiliary resources. The experiments based on peer-review datasets show that our approach is effective and has better performance on the target scores than the single-task method and naive multi-task methods.</abstract>
      <url hash="b5f69f40">2020.sdp-1.14</url>
      <doi>10.18653/v1/2020.sdp-1.14</doi>
      <video href="https://slideslive.com/38940727" />
      <bibkey>li-etal-2020-multi-task</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/peerread">PeerRead</pwcdataset>
    </paper>
    <paper id="15">
      <title>ERLKG : Entity Representation Learning and Knowledge Graph based association analysis of COVID-19 through mining of unstructured biomedical corpora<fixed-case>ERLKG</fixed-case>: Entity Representation Learning and Knowledge Graph based association analysis of <fixed-case>COVID</fixed-case>-19 through mining of unstructured biomedical corpora</title>
      <author><first>Sayantan</first><last>Basu</last></author>
      <author><first>Sinchani</first><last>Chakraborty</last></author>
      <author><first>Atif</first><last>Hassan</last></author>
      <author><first>Sana</first><last>Siddique</last></author>
      <author><first>Ashish</first><last>Anand</last></author>
      <pages>127–137</pages>
      <abstract>We introduce a generic, human-out-of-the-loop pipeline, ERLKG, to perform rapid association analysis of any biomedical entity with other existing entities from a corpora of the same domain. Our pipeline consists of a Knowledge Graph (KG) created from the Open Source CORD-19 dataset by fully automating the procedure of <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a> using SciBERT. The best latent entity representations are then found by benchnmarking different KG embedding techniques on the task of link prediction using a Graph Convolution Network Auto Encoder (GCN-AE). We demonstrate the utility of ERLKG with respect to COVID-19 through multiple qualitative evaluations. Due to the lack of a gold standard, we propose a relatively large intrinsic evaluation dataset for COVID-19 and use it for validating the top two performing KG embedding techniques. We find TransD to be the best performing KG embedding technique with Pearson and Spearman correlation scores of 0.4348 and 0.4570 respectively. We demonstrate that a considerable number of ERLKG’s top protein, chemical and disease predictions are currently in consideration for COVID-19 related research.</abstract>
      <url hash="a7ae7621">2020.sdp-1.15</url>
      <doi>10.18653/v1/2020.sdp-1.15</doi>
      <video href="https://slideslive.com/38940725" />
      <bibkey>basu-etal-2020-erlkg</bibkey>
      <pwccode url="https://github.com/sayantanbasu05/ERKLG" additional="false">sayantanbasu05/ERKLG</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
    </paper>
    <paper id="21">
      <title>Scaling Systematic Literature Reviews with Machine Learning Pipelines</title>
      <author><first>Seraphina</first><last>Goldfarb-Tarrant</last></author>
      <author><first>Alexander</first><last>Robertson</last></author>
      <author><first>Jasmina</first><last>Lazic</last></author>
      <author><first>Theodora</first><last>Tsouloufi</last></author>
      <author><first>Louise</first><last>Donnison</last></author>
      <author><first>Karen</first><last>Smyth</last></author>
      <pages>184–195</pages>
      <abstract>Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>. They are vital to many fields of science and philanthropy, but are very time-consuming and require experts. Yet the three main stages of a <a href="https://en.wikipedia.org/wiki/Systematic_review">systematic review</a> are easily done automatically : searching for documents can be done via <a href="https://en.wikipedia.org/wiki/Application_programming_interface">APIs</a> and scrapers, selection of relevant documents can be done via <a href="https://en.wikipedia.org/wiki/Binary_classification">binary classification</a>, and extraction of data can be done via sequence-labelling classification. Despite the promise of automation for this field, little research exists that examines the various ways to automate each of these <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. We construct a <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a> that automates each of these aspects, and experiment with many human-time vs. system quality trade-offs. We test the ability of <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> to work well on small amounts of data and to generalise to data from countries not represented in the training data. We test different types of <a href="https://en.wikipedia.org/wiki/Data_extraction">data extraction</a> with varying difficulty in <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a>, and five different neural architectures to do the <a href="https://en.wikipedia.org/wiki/Data_extraction">extraction</a>. We find that we can get surprising <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and generalisability of the whole <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline system</a> with only 2 weeks of human-expert annotation, which is only 15 % of the time it takes to do the whole review manually and can be repeated and extended to new data with no additional effort.</abstract>
      <url hash="2240aaf3">2020.sdp-1.21</url>
      <doi>10.18653/v1/2020.sdp-1.21</doi>
      <bibkey>goldfarb-tarrant-etal-2020-scaling</bibkey>
      <pwccode url="https://github.com/seraphinatarrant/systematic_reviews" additional="false">seraphinatarrant/systematic_reviews</pwccode>
    </paper>
    <paper id="22">
      <title>Document-Level Definition Detection in Scholarly Documents : Existing Models, Error Analyses, and Future Directions</title>
      <author><first>Dongyeop</first><last>Kang</last></author>
      <author><first>Andrew</first><last>Head</last></author>
      <author><first>Risham</first><last>Sidhu</last></author>
      <author><first>Kyle</first><last>Lo</last></author>
      <author><first>Daniel</first><last>Weld</last></author>
      <author><first>Marti A.</first><last>Hearst</last></author>
      <pages>196–206</pages>
      <abstract>The task of definition detection is important for <a href="https://en.wikipedia.org/wiki/Academic_publishing">scholarly papers</a>, because papers often make use of <a href="https://en.wikipedia.org/wiki/Jargon">technical terminology</a> that may be unfamiliar to readers. Despite prior work on definition detection, current approaches are far from being accurate enough to use in realworld applications. In this paper, we first perform in-depth error analysis of the current best performing definition detection system and discover major causes of errors. Based on this analysis, we develop a new definition detection system, HEDDEx, that utilizes syntactic features, transformer encoders, and <a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)">heuristic filters</a>, and evaluate it on a standard sentence-level benchmark. Because current <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmarks</a> evaluate <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">randomly sampled sentences</a>, we propose an alternative <a href="https://en.wikipedia.org/wiki/Evaluation">evaluation</a> that assesses every sentence within a document. This allows for evaluating <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> in addition to <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a>. HEDDEx outperforms the leading system on both the sentence-level and the document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively. We note that performance on the high-recall document-level task is much lower than in the standard evaluation approach, due to the necessity of incorporation of document structure as <a href="https://en.wikipedia.org/wiki/Software_feature">features</a>. We discuss remaining challenges in document-level definition detection, ideas for improvements, and potential issues for the development of reading aid applications.</abstract>
      <url hash="f37ba031">2020.sdp-1.22</url>
      <attachment type="OptionalSupplementaryMaterial" hash="00000000">2020.sdp-1.22.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.sdp-1.22</doi>
      <video href="https://slideslive.com/38940724" />
      <bibkey>kang-etal-2020-document</bibkey>
      <pwccode url="https://github.com/allenai/scholarphi" additional="false">allenai/scholarphi</pwccode>
    </paper>
    <paper id="23">
      <title>A New Neural Search and Insights Platform for Navigating and Organizing AI Research<fixed-case>AI</fixed-case> Research</title>
      <author><first>Marzieh</first><last>Fadaee</last></author>
      <author><first>Olga</first><last>Gureenkova</last></author>
      <author><first>Fernando</first><last>Rejon Barrera</last></author>
      <author><first>Carsten</first><last>Schnober</last></author>
      <author><first>Wouter</first><last>Weerkamp</last></author>
      <author><first>Jakub</first><last>Zavrel</last></author>
      <pages>207–213</pages>
      <abstract>To provide AI researchers with modern tools for dealing with the explosive growth of the research literature in their field, we introduce a new platform, AI Research Navigator, that combines classical <a href="https://en.wikipedia.org/wiki/Keyword_search">keyword search</a> with neural retrieval to discover and organize relevant literature. The system provides <a href="https://en.wikipedia.org/wiki/Search_engine_technology">search</a> at multiple levels of textual granularity, from sentences to aggregations across documents, both in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a> and through navigation in a domain specific Knowledge Graph. We give an overview of the overall architecture of the system and of the components for document analysis, <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, <a href="https://en.wikipedia.org/wiki/Search_engine_technology">search</a>, <a href="https://en.wikipedia.org/wiki/Analytics">analytics</a>, expert search, and recommendations.</abstract>
      <url hash="23b1319d">2020.sdp-1.23</url>
      <doi>10.18653/v1/2020.sdp-1.23</doi>
      <bibkey>fadaee-etal-2020-new</bibkey>
    </paper>
    <paper id="24">
      <title>Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020 : CL-SciSumm, LaySumm and LongSumm<fixed-case>CL</fixed-case>-<fixed-case>S</fixed-case>ci<fixed-case>S</fixed-case>umm, <fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm and <fixed-case>L</fixed-case>ong<fixed-case>S</fixed-case>umm</title>
      <author><first>Muthu Kumar</first><last>Chandrasekaran</last></author>
      <author><first>Guy</first><last>Feigenblat</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <author><first>Abhilasha</first><last>Ravichander</last></author>
      <author><first>Michal</first><last>Shmueli-Scheuer</last></author>
      <author><first>Anita</first><last>de Waard</last></author>
      <pages>214–224</pages>
      <abstract>We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020 : CL-SciSumm, LaySumm and LongSumm. We report on each of the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>, which received 18 submissions in total, with some submissions addressing two or three of the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. In summary, the quality and quantity of the submissions show that there is ample interest in scholarly document summarization, and the state of the art in this domain is at a midway point between being an impossible task and one that is fully resolved.</abstract>
      <url hash="30019fc2">2020.sdp-1.24</url>
      <doi>10.18653/v1/2020.sdp-1.24</doi>
      <revision id="1" href="2020.sdp-1.24v1" hash="38d9bf57" />
      <revision id="2" href="2020.sdp-1.24v2" hash="30019fc2" date="2021-01-01">Added missing reference to DUCS (Chaturvedi et al., 2020).</revision>
      <bibkey>chandrasekaran-etal-2020-overview-insights</bibkey>
    </paper>
    <paper id="32">
      <title>Team MLU@CL-SciSumm20 : Methods for Computational Linguistics Scientific Citation Linkage<fixed-case>MLU</fixed-case>@<fixed-case>CL</fixed-case>-<fixed-case>S</fixed-case>ci<fixed-case>S</fixed-case>umm20: Methods for Computational Linguistics Scientific Citation Linkage</title>
      <author><first>Rong</first><last>Huang</last></author>
      <author><first>Kseniia</first><last>Krylova</last></author>
      <pages>282–287</pages>
      <abstract>This paper describes our approach to the CL-SciSumm 2020 shared task toward the problem of identifying reference span of the citing article in the referred article. In Task 1a, we apply and compare different methods in combination with similarity scores to identify spans of the reference text for the given citance. In Task 1b, we use a <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> to classifying the discourse facets.</abstract>
      <url hash="717d0e75">2020.sdp-1.32</url>
      <doi>10.18653/v1/2020.sdp-1.32</doi>
      <bibkey>huang-krylova-2020-team</bibkey>
    </paper>
    <paper id="36">
      <title>ARTU / TU Wien and Artificial Researcher@ LongSumm 20<fixed-case>ARTU</fixed-case> / <fixed-case>TU</fixed-case> <fixed-case>W</fixed-case>ien and Artificial Researcher@ <fixed-case>L</fixed-case>ong<fixed-case>S</fixed-case>umm 20</title>
      <author><first>Alaa</first><last>El-Ebshihy</last></author>
      <author><first>Annisa Maulida</first><last>Ningtyas</last></author>
      <author><first>Linda</first><last>Andersson</last></author>
      <author><first>Florina</first><last>Piroi</last></author>
      <author><first>Andreas</first><last>Rauber</last></author>
      <pages>310–317</pages>
      <abstract>In this paper, we present our approach to solve the LongSumm 2020 Shared Task, at the 1st Workshop on Scholarly Document Processing. The objective of the long summaries task is to generate long summaries that cover salient information in <a href="https://en.wikipedia.org/wiki/Scientific_literature">scientific articles</a>. The task is to generate abstractive and extractive summaries of a given <a href="https://en.wikipedia.org/wiki/Scientific_literature">scientific article</a>. In the proposed approach, we are inspired by the concept of Argumentative Zoning (AZ) that de- fines the main rhetorical structure in scientific articles. We define two aspects that should be covered in scientific paper summary, namely Claim / Method and Conclusion / Result aspects. We use Solr index to expand the sentences of the paper abstract. We formulate each abstract sentence in a given publication as query to retrieve similar sentences from the text body of the document itself. We utilize a sentence selection algorithm described in previous literature to select sentences for the final summary that covers the two aforementioned aspects.</abstract>
      <url hash="1b9d1bba">2020.sdp-1.36</url>
      <doi>10.18653/v1/2020.sdp-1.36</doi>
      <bibkey>el-ebshihy-etal-2020-artu</bibkey>
    </paper>
    <paper id="40">
      <title>Divide and Conquer : From <a href="https://en.wikipedia.org/wiki/Complexity">Complexity</a> to <a href="https://en.wikipedia.org/wiki/Simplicity">Simplicity</a> for Lay Summarization</title>
      <author><first>Rochana</first><last>Chaturvedi</last></author>
      <author><first>Saachi</first><last>.</last></author>
      <author><first>Jaspreet Singh</first><last>Dhani</last></author>
      <author><first>Anurag</first><last>Joshi</last></author>
      <author><first>Ankush</first><last>Khanna</last></author>
      <author><first>Neha</first><last>Tomar</last></author>
      <author><first>Swagata</first><last>Duari</last></author>
      <author><first>Alka</first><last>Khurana</last></author>
      <author><first>Vasudha</first><last>Bhatnagar</last></author>
      <pages>344–355</pages>
      <abstract>We describe our approach for the 1st Computational Linguistics Lay Summary Shared Task CL-LaySumm20. The task is to produce non-technical summaries of scholarly documents. The summary should be within easy grasp of a layman who may not be well versed with the domain of the research article. We propose a two step divide-and-conquer approach. First, we judiciously select segments of the documents that are not overly pedantic and are likely to be of interest to the laity, and over-extract sentences from each segment using an unsupervised network based method. Next, we perform abstractive summarization on these <a href="https://en.wikipedia.org/wiki/Abstraction_(computer_science)">extractions</a> and systematically merge the <a href="https://en.wikipedia.org/wiki/Abstraction_(computer_science)">abstractions</a>. We run ablation studies to establish that each step in our pipeline is critical for improvement in the quality of lay summary. Our approach leverages state-of-the-art pre-trained deep neural network based models as zero-shot learners to achieve high scores on the task.</abstract>
      <url hash="d1fa233a">2020.sdp-1.40</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b1c1bf76">2020.sdp-1.40.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.sdp-1.40</doi>
      <bibkey>chaturvedi-etal-2020-divide</bibkey>
      <pwccode url="https://github.com/anuragjoshi3519/laysumm20" additional="false">anuragjoshi3519/laysumm20</pwccode>
    </paper>
    </volume>
</collection>