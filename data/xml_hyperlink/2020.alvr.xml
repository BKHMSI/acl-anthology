<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.alvr">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the First Workshop on Advances in Language and Vision Research</booktitle>
      <editor><first>Xin</first><last>Wang</last></editor>
      <editor><first>Jesse</first><last>Thomason</last></editor>
      <editor><first>Ronghang</first><last>Hu</last></editor>
      <editor><first>Xinlei</first><last>Chen</last></editor>
      <editor><first>Peter</first><last>Anderson</last></editor>
      <editor><first>Qi</first><last>Wu</last></editor>
      <editor><first>Asli</first><last>Celikyilmaz</last></editor>
      <editor><first>Jason</first><last>Baldridge</last></editor>
      <editor><first>William Yang</first><last>Wang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="7f4feb1f">2020.alvr-1</url>
    </meta>
    <frontmatter>
      <url hash="30a10a5a">2020.alvr-1.0</url>
      <bibkey>alvr-2020-advances</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Visual Question Generation from Radiology Images</title>
      <author><first>Mourad</first><last>Sarrouti</last></author>
      <author><first>Asma</first><last>Ben Abacha</last></author>
      <author><first>Dina</first><last>Demner-Fushman</last></author>
      <pages>12–18</pages>
      <abstract>Visual Question Generation (VQG), the task of generating a question based on image contents, is an increasingly important area that combines <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> and <a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a>. Although there are some recent works that have attempted to generate questions from images in the <a href="https://en.wikipedia.org/wiki/Open_domain">open domain</a>, the task of VQG in the <a href="https://en.wikipedia.org/wiki/Medical_imaging">medical domain</a> has not been explored so far. In this paper, we introduce an approach to generation of visual questions about radiology images called VQGR, i.e. an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> that is able to ask a question when shown an image. VQGR first generates new training data from the existing examples, based on contextual word embeddings and image augmentation techniques. It then uses the variational auto-encoders model to encode images into a latent space and decode natural language questions. Experimental automatic evaluations performed on the VQA-RAD dataset of clinical visual questions show that VQGR achieves good performances compared with the baseline system. The source code is available at https://github.com/sarrouti/vqgr.</abstract>
      <url hash="0dcc0d39">2020.alvr-1.3</url>
      <doi>10.18653/v1/2020.alvr-1.3</doi>
      <video href="http://slideslive.com/38929760" />
      <bibkey>sarrouti-etal-2020-visual</bibkey>
      <pwccode url="https://github.com/sarrouti/vqgr" additional="false">sarrouti/vqgr</pwccode>
    </paper>
    <paper id="4">
      <title>On the role of effective and referring questions in GuessWhat? !<fixed-case>G</fixed-case>uess<fixed-case>W</fixed-case>hat?!</title>
      <author><first>Mauricio</first><last>Mazuecos</last></author>
      <author><first>Alberto</first><last>Testoni</last></author>
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <author><first>Luciana</first><last>Benotti</last></author>
      <pages>19–25</pages>
      <abstract>Task success is the standard <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> used to evaluate referential visual dialogue systems. In this paper we propose two new <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> that evaluate how each question contributes to the goal. First, we measure how effective each question is by evaluating whether the question discards objects that are not the referent. Second, we define referring questions as those that univocally identify one object in the image. We report the new <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> for human dialogues and for state of the art publicly available <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on GuessWhat? !. Regarding our first <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>, we find that successful dialogues do not have a higher percentage of effective questions for most <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. With respect to the second <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>, humans make questions at the end of the dialogue that are referring, confirming their guess before guessing. Human dialogues that use this <a href="https://en.wikipedia.org/wiki/Strategy">strategy</a> have a higher task success but models do not seem to learn it.</abstract>
      <url hash="61627744">2020.alvr-1.4</url>
      <doi>10.18653/v1/2020.alvr-1.4</doi>
      <bibkey>mazuecos-etal-2020-role</bibkey>
    </paper>
    <paper id="5">
      <title>Latent Alignment of Procedural Concepts in Multimodal Recipes</title>
      <author><first>Hossein</first><last>Rajaby Faghihi</last></author>
      <author><first>Roshanak</first><last>Mirzaee</last></author>
      <author><first>Sudarshan</first><last>Paliwal</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>26–31</pages>
      <abstract>We propose a novel alignment mechanism to deal with procedural reasoning on a newly released multimodal QA dataset, named RecipeQA. Our model is solving the textual cloze task which is a <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> on a <a href="https://en.wikipedia.org/wiki/Recipe">recipe</a> containing images and instructions. We exploit the power of attention networks, cross-modal representations, and a latent alignment space between instructions and candidate answers to solve the problem. We introduce constrained max-pooling which refines the max pooling operation on the alignment matrix to impose disjoint constraints among the outputs of the model. Our evaluation result indicates a 19 % improvement over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="1aa3a317">2020.alvr-1.5</url>
      <doi>10.18653/v1/2020.alvr-1.5</doi>
      <video href="http://slideslive.com/38929759" />
      <bibkey>rajaby-faghihi-etal-2020-latent</bibkey>
      <pwccode url="https://github.com/HLR/LatentAlignmentProcedural" additional="false">HLR/LatentAlignmentProcedural</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/recipeqa">RecipeQA</pwcdataset>
    </paper>
  </volume>
</collection>