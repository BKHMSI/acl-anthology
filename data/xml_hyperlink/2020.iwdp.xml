<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.iwdp">
  <volume id="1" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the Second International Workshop of Discourse Processing</booktitle>
      <editor><first>Qun</first><last>Liu</last></editor>
      <editor><first>Deyi</first><last>Xiong</last></editor>
      <editor><first>Shili</first><last>Ge</last></editor>
      <editor><first>Xiaojun</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="64d9f206">2020.iwdp-1.0</url>
      <bibkey>iwdp-2020-international</bibkey>
    </frontmatter>
    <paper id="5">
      <title>Context-Aware Word Segmentation for Chinese Real-World Discourse<fixed-case>C</fixed-case>hinese Real-World Discourse</title>
      <author><first>Kaiyu</first><last>Huang</last></author>
      <author><first>Junpeng</first><last>Liu</last></author>
      <author><first>Jingxiang</first><last>Cao</last></author>
      <author><first>Degen</first><last>Huang</last></author>
      <pages>22–28</pages>
      <abstract>Previous neural approaches achieve significant progress for Chinese word segmentation (CWS) as a sentence-level task, but it suffers from limitations on real-world scenario. In this paper, we address this issue with a context-aware method and optimize the <a href="https://en.wikipedia.org/wiki/Solution">solution</a> at document-level. This paper proposes a three-step strategy to improve the performance for discourse CWS. First, the method utilizes an auxiliary segmenter to remedy the limitation on pre-segmenter. Then the context-aware algorithm computes the <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence</a> of each split. The maximum probability path is reconstructed via this <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a>. Besides, in order to evaluate the performance in <a href="https://en.wikipedia.org/wiki/Discourse">discourse</a>, we build a new <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmark</a> consisting of the latest news and Chinese medical articles. Extensive experiments on this <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark</a> show that our proposed method achieves a competitive performance on a document-level real-world scenario for CWS.</abstract>
      <url hash="061c4146">2020.iwdp-1.5</url>
      <attachment type="Dataset" hash="ba4de41c">2020.iwdp-1.5.Dataset.rar</attachment>
      <bibkey>huang-etal-2020-context</bibkey>
    </paper>
    <paper id="6">
      <title>Neural Abstractive Multi-Document Summarization : Hierarchical or Flat Structure?</title>
      <author><first>Ye</first><last>Ma</last></author>
      <author><first>Lu</first><last>Zong</last></author>
      <pages>29–37</pages>
      <abstract>With regards to WikiSum (CITATION) that empowers applicative explorations of Neural Multi-Document Summarization (MDS) to learn from large scale dataset, this study develops two hierarchical Transformers (HT) that describe both the cross-token and cross-document dependencies, at the same time allow extended length of input documents. By incorporating word- and paragraph-level multi-head attentions in the decoder based on the parallel and vertical architectures, the proposed parallel and vertical hierarchical Transformers (PHT &amp; VHT) generate summaries utilizing context-aware word embeddings together with static and dynamics paragraph embeddings, respectively. A comprehensive evaluation is conducted on WikiSum to compare PHT &amp; VHT with established models and to answer the question whether hierarchical structures offer more promising performances than flat structures in the MDS task. The results suggest that our hierarchical models generate summaries of higher quality by better capturing cross-document relationships, and save more memory spaces in comparison to flat-structure models. Moreover, we recommend PHT given its practical value of higher <a href="https://en.wikipedia.org/wiki/Time_complexity">inference speed</a> and greater memory-saving capacity.</abstract>
      <url hash="0b7d8d4d">2020.iwdp-1.6</url>
      <bibkey>ma-zong-2020-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisum">WikiSum</pwcdataset>
    </paper>
    </volume>
</collection>