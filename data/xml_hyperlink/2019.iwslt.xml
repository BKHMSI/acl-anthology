<?xml version='1.0' encoding='utf-8'?>
<collection id="2019.iwslt">
  <volume id="1" ingest-date="2022-02-17">
    <meta>
      <booktitle>Proceedings of the 16th International Conference on Spoken Language Translation</booktitle>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong</address>
      <month>November 2-3</month>
      <year>2019</year>
      <editor><first>Jan</first><last>Niehues</last></editor>
      <editor><first>Rolando</first><last>Cattoni</last></editor>
      <editor><first>Sebastian</first><last>Stüker</last></editor>
      <editor><first>Matteo</first><last>Negri</last></editor>
      <editor><first>Marco</first><last>Turchi</last></editor>
      <editor><first>Thanh-Le</first><last>Ha</last></editor>
      <editor><first>Elizabeth</first><last>Salesky</last></editor>
      <editor><first>Ramon</first><last>Sanabria</last></editor>
      <editor><first>Loic</first><last>Barrault</last></editor>
      <editor><first>Lucia</first><last>Specia</last></editor>
      <editor><first>Marcello</first><last>Federico</last></editor>
    </meta>
    <paper id="1">
      <title>The IWSLT 2019 Evaluation Campaign<fixed-case>IWSLT</fixed-case> 2019 Evaluation Campaign</title>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Rolando</first><last>Cattoni</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Thanh-Le</first><last>Ha</last></author>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>Ramon</first><last>Sanabria</last></author>
      <author><first>Loic</first><last>Barrault</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <abstract>The IWSLT 2019 evaluation campaign featured three tasks : speech translation of (i) <a href="https://en.wikipedia.org/wiki/TED_(conference)">TED talks</a> and (ii) How2 instructional videos from <a href="https://en.wikipedia.org/wiki/English_language">English</a> into <a href="https://en.wikipedia.org/wiki/German_language">German</a> and <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese</a>, and (iii) text translation of <a href="https://en.wikipedia.org/wiki/TED_(conference)">TED talks</a> from <a href="https://en.wikipedia.org/wiki/English_language">English</a> into <a href="https://en.wikipedia.org/wiki/Czech_language">Czech</a>. For the first two tasks we encouraged submissions of end- to-end speech-to-text systems, and for the second task participants could also use the video as additional input. We received submissions by 12 research teams. This overview provides detailed descriptions of the data and evaluation conditions of each task and reports results of the participating systems.</abstract>
      <url hash="96ae3b28">2019.iwslt-1.1</url>
      <bibkey>niehues-etal-2019-iwslt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="4">
      <title>ESPnet How2 Speech Translation System for IWSLT 2019 : Pre-training, Knowledge Distillation, and Going Deeper<fixed-case>ESP</fixed-case>net How2 Speech Translation System for <fixed-case>IWSLT</fixed-case> 2019: Pre-training, Knowledge Distillation, and Going Deeper</title>
      <author><first>Hirofumi</first><last>Inaguma</last></author>
      <author><first>Shun</first><last>Kiyono</last></author>
      <author><first>Nelson Enrique Yalta</first><last>Soplin</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <abstract>This paper describes the ESPnet submissions to the How2 Speech Translation task at IWSLT2019. In this year, we mainly build our systems based on Transformer architectures in all tasks and focus on the end-to-end speech translation (E2E-ST). We first compare RNN-based models and Transformer, and then confirm Transformer models significantly and consistently outperform RNN models in all tasks and corpora. Next, we investigate pre-training of E2E-ST models with the ASR and MT tasks. On top of the pre-training, we further explore knowledge distillation from the NMT model and the deeper speech encoder, and confirm drastic improvements over the baseline model. All of our codes are publicly available in ESPnet.</abstract>
      <url hash="be527f4e">2019.iwslt-1.4</url>
      <bibkey>inaguma-etal-2019-espnet</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="5">
      <title>ON-TRAC Consortium End-to-End Speech Translation Systems for the IWSLT 2019 Shared Task<fixed-case>ON</fixed-case>-<fixed-case>TRAC</fixed-case> Consortium End-to-End Speech Translation Systems for the <fixed-case>IWSLT</fixed-case> 2019 Shared Task</title>
      <author><first>Ha</first><last>Nguyen</last></author>
      <abstract>This paper describes the ON-TRAC Consortium translation systems developed for the end-to-end model task of IWSLT Evaluation 2019 for the English Portuguese language pair. ON-TRAC Consortium is composed of researchers from three French academic laboratories : LIA (Avignon Universit), LIG (Universit Grenoble Alpes), and LIUM (Le Mans Universit). A single end-to-end model built as a neural encoder-decoder architecture with attention mechanism was used for two primary submissions corresponding to the two EN-PT evaluations sets : (1) TED (MuST-C) and (2) How2. In this paper, we notably investigate impact of pooling heterogeneous corpora for training, impact of target tokenization (characters or BPEs), impact of <a href="https://en.wikipedia.org/wiki/Speech_segmentation">speech input segmentation</a> and we also compare our best <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end model</a> (BLEU of 26.91 on MuST-C and 43.82 on How2 validation sets) to a pipeline (ASR+MT) approach.</abstract>
      <url hash="4b4966fb">2019.iwslt-1.5</url>
      <bibkey>nguyen-2019-trac</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ted-lium-3">TED-LIUM 3</pwcdataset>
    </paper>
    <paper id="6">
      <title>Transformer-based Cascaded Multimodal Speech Translation</title>
      <author><first>Zixiu</first><last>Wu</last></author>
      <author><first>Ozan</first><last>Caglayan</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Josiah</first><last>Wang</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <abstract>This paper describes the cascaded multimodal speech translation systems developed by Imperial College London for the IWSLT 2019 evaluation campaign. The architecture consists of an automatic speech recognition (ASR) system followed by a Transformer-based multimodal machine translation (MMT) system. While the ASR component is identical across the experiments, the MMT model varies in terms of the way of integrating the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">visual context</a> (simple conditioning vs. attention), the type of <a href="https://en.wikipedia.org/wiki/Visual_system">visual features</a> exploited (pooled, convolutional, action categories) and the underlying architecture. For the latter, we explore both the canonical transformer and its deliberation version with additive and cascade variants which differ in how they integrate the textual attention. Upon conducting extensive experiments, we found that (i) the explored visual integration schemes often harm the translation performance for the transformer and additive deliberation, but considerably improve the cascade deliberation ; (ii) the transformer and cascade deliberation integrate the visual modality better than the additive deliberation, as shown by the incongruence analysis.</abstract>
      <url hash="3a1fe36d">2019.iwslt-1.6</url>
      <bibkey>wu-etal-2019-transformer</bibkey>
    </paper>
    <paper id="11">
      <title>The LIG system for the English-Czech Text Translation Task of IWSLT 2019<fixed-case>LIG</fixed-case> system for the <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>zech Text Translation Task of <fixed-case>IWSLT</fixed-case> 2019</title>
      <author><first>Loïc</first><last>Vial</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Hang</first><last>Le</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <abstract>In this paper, we present our submission for the English to Czech Text Translation Task of IWSLT 2019. Our system aims to study how pre-trained language models, used as input embeddings, can improve a specialized machine translation system trained on few data. Therefore, we implemented a Transformer-based encoder-decoder neural system which is able to use the output of a pre-trained language model as input embeddings, and we compared its performance under three configurations : 1) without any pre-trained language model (constrained), 2) using a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> trained on the monolingual parts of the allowed English-Czech data (constrained), and 3) using a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> trained on a large quantity of external monolingual data (unconstrained). We used BERT as external pre-trained language model (configuration 3), and BERT architecture for training our own <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> (configuration 2). Regarding the training data, we trained our MT system on a small quantity of parallel text : one set only consists of the provided MuST-C corpus, and the other set consists of the MuST-C corpus and the News Commentary corpus from WMT. We observed that using the external pre-trained BERT improves the scores of our <a href="https://en.wikipedia.org/wiki/System">system</a> by +0.8 to +1.5 of BLEU on our development set, and +0.97 to +1.94 of BLEU on the test set. However, using our own <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> trained only on the allowed parallel data seems to improve the <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> performances only when the system is trained on the smallest dataset.</abstract>
      <url hash="61d6c2fb">2019.iwslt-1.11</url>
      <bibkey>vial-etal-2019-lig</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="13">
      <title>KIT’s Submission to the IWSLT 2019 Shared Task on Text Translation<fixed-case>KIT</fixed-case>’s Submission to the <fixed-case>IWSLT</fixed-case> 2019 Shared Task on Text Translation</title>
      <author><first>Felix</first><last>Schneider</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <abstract>In this paper, we describe KIT’s submission for the IWSLT 2019 shared task on text translation. Our <a href="https://en.wikipedia.org/wiki/System">system</a> is based on the transformer model [ 1 ] using our in-house implementation. We augment the available training data using <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a> and employ <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> for the final <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>. For our best results, we used a 12-layer transformer-big config- uration, achieving state-of-the-art results on the WMT2018 test set. We also experiment with student-teacher models to improve performance of smaller <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a>.</abstract>
      <url hash="a61bab92">2019.iwslt-1.13</url>
      <bibkey>schneider-waibel-2019-kits</bibkey>
    </paper>
    <paper id="16">
      <title>Adapting Multilingual Neural Machine Translation to Unseen Languages</title>
      <author><first>Surafel M.</first><last>Lakew</last></author>
      <author><first>Alina</first><last>Karakanta</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <abstract>Multilingual Neural Machine Translation (MNMT) for low- resource languages (LRL) can be enhanced by the presence of related high-resource languages (HRL), but the relatedness of HRL usually relies on predefined linguistic assumptions about language similarity. Recently, adapting MNMT to a <a href="https://en.wikipedia.org/wiki/Linear_regression">LRL</a> has shown to greatly improve performance. In this work, we explore the problem of adapting an MNMT model to an unseen <a href="https://en.wikipedia.org/wiki/Linear_regression">LRL</a> using data selection and model adapta- tion. In order to improve NMT for <a href="https://en.wikipedia.org/wiki/Linguistic_description">LRL</a>, we employ perplexity to select HRL data that are most similar to the <a href="https://en.wikipedia.org/wiki/Linguistic_description">LRL</a> on the basis of <a href="https://en.wikipedia.org/wiki/Language_distance">language distance</a>. We extensively explore data selection in popular multilingual NMT settings, namely in (zero-shot) translation, and in adaptation from a multilingual pre-trained model, for both directions (LRLen). We further show that dynamic adaptation of the model’s vocabulary results in a more favourable segmentation for the LRL in comparison with direct adaptation. Experiments show re- ductions in training time and significant performance gains over LRL baselines, even with zero LRL data (+13.0 BLEU), up to +17.0 BLEU for pre-trained multilingual model dynamic adaptation with related data selection. Our method outperforms current approaches, such as massively multilingual models and <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, on four <a href="https://en.wikipedia.org/wiki/Linear_regression">LRL</a>.</abstract>
      <url hash="1b241b83">2019.iwslt-1.16</url>
      <bibkey>lakew-etal-2019-adapting</bibkey>
      <pwccode url="https://github.com/surafelml/adapt-mnmt" additional="false">surafelml/adapt-mnmt</pwccode>
    </paper>
    <paper id="17">
      <title>Transformers without Tears : Improving the Normalization of Self-Attention</title>
      <author><first>Toan Q.</first><last>Nguyen</last></author>
      <author><first>Julian</first><last>Salazar</last></author>
      <abstract>We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PRENORM) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose l2 normalization with a single scale parameter (SCALENORM) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FIXNORM). On five low-resource translation pairs from <a href="https://en.wikipedia.org/wiki/TED_(conference)">TED Talks-based corpora</a>, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT’ 15 English-Vietnamese. We ob- serve sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT’ 14 English-German), SCALENORM and FIXNORM remain competitive but PRENORM degrades performance.</abstract>
      <url hash="7cc09ad6">2019.iwslt-1.17</url>
      <bibkey>nguyen-salazar-2019-transformers</bibkey>
      <pwccode url="https://github.com/tnq177/transformers_without_tears" additional="true">tnq177/transformers_without_tears</pwccode>
    </paper>
    <paper id="18">
      <title>Harnessing Indirect Training Data for End-to-End Automatic Speech Translation : Tricks of the Trade</title>
      <author><first>Juan</first><last>Pino</last></author>
      <author><first>Liezl</first><last>Puzon</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Xutai</first><last>Ma</last></author>
      <author><first>Arya D.</first><last>McCarthy</last></author>
      <author><first>Deepak</first><last>Gopinath</last></author>
      <abstract>For automatic speech translation (AST), end-to-end approaches are outperformed by cascaded models that transcribe with automatic speech recognition (ASR), then trans- late with machine translation (MT). A major cause of the performance gap is that, while existing AST corpora are small, massive datasets exist for both the ASR and MT subsystems. In this work, we evaluate several data augmentation and pretraining approaches for <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a>, by comparing all on the same <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>. Simple <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> by translating ASR transcripts proves most effective on the EnglishFrench augmented LibriSpeech dataset, closing the performance gap from 8.2 to 1.4 BLEU, compared to a very strong cascade that could directly utilize copious ASR and MT data. The same <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end approach</a> plus <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> closes the gap on the EnglishRomanian MuST-C dataset from 6.7 to 3.7 <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>. In addition to these results, we present practical rec- ommendations for augmentation and pretraining approaches. Finally, we decrease the performance gap to 0.01 BLEU us- ing a Transformer-based architecture.</abstract>
      <url hash="2c55855c">2019.iwslt-1.18</url>
      <bibkey>pino-etal-2019-harnessing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="22">
      <title>On Using SpecAugment for End-to-End Speech Translation<fixed-case>S</fixed-case>pec<fixed-case>A</fixed-case>ugment for End-to-End Speech Translation</title>
      <author><first>Parnia</first><last>Bahar</last></author>
      <author><first>Albert</first><last>Zeyer</last></author>
      <author><first>Ralf</first><last>Schlüter</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <abstract>This work investigates a simple data augmentation technique, SpecAugment, for end-to-end speech translation. SpecAugment is a low-cost implementation method applied directly to the audio input features and it consists of masking blocks of frequency channels, and/or time steps. We apply SpecAugment on end-to-end speech translation tasks and achieve up to +2.2 % BLEU on LibriSpeech Audiobooks EnFr and +1.2 % on IWSLT TED-talks EnDe by alleviating <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> to some extent. We also examine the effectiveness of the <a href="https://en.wikipedia.org/wiki/Methodology">method</a> in a variety of data scenarios and show that the <a href="https://en.wikipedia.org/wiki/Methodology">method</a> also leads to significant improvements in various data conditions irrespective of the amount of training data.</abstract>
      <url hash="e3ecaec4">2019.iwslt-1.22</url>
      <bibkey>bahar-etal-2019-using</bibkey>
    </paper>
    <paper id="23">
      <title>Estimating post-editing effort : a study on human judgements, task-based and reference-based metrics of MT quality<fixed-case>MT</fixed-case> quality</title>
      <author><first>Scarton</first><last>Scarton</last></author>
      <author><first>Mikel L.</first><last>Forcada</last></author>
      <author><first>Miquel</first><last>Esplà-Gomis</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <abstract>Devising <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> to assess translation quality has always been at the core of machine translation (MT) research. Traditional automatic reference-based metrics, such as <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>, have shown correlations with human judgements of adequacy and fluency and have been paramount for the advancement of MT system development. Crowd-sourcing has popularised and enabled the scalability of <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> based on human judgments, such as subjective direct assessments (DA) of adequacy, that are believed to be more reliable than reference-based automatic metrics. Finally, task-based measurements, such as post-editing time, are expected to provide a more de- tailed evaluation of the usefulness of translations for a specific task. Therefore, while DA averages adequacy judgements to obtain an appraisal of (perceived) quality independently of the task, and reference-based automatic metrics try to objectively estimate quality also in a task-independent way, task-based metrics are measurements obtained either during or after performing a specific task. In this paper we argue that, although expensive, task-based measurements are the most reliable when estimating MT quality in a specific task ; in our case, this task is <a href="https://en.wikipedia.org/wiki/Post-editing">post-editing</a>. To that end, we report experiments on a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> with newly-collected post-editing indicators and show their usefulness when estimating post-editing effort. Our results show that task-based metrics comparing machine-translated and post-edited versions are the best at tracking post-editing effort, as expected.</abstract>
      <url hash="8a61b1b6">2019.iwslt-1.23</url>
      <bibkey>scarton-etal-2019-estimating</bibkey>
      <pwccode url="https://github.com/carolscarton/iwslt2019" additional="false">carolscarton/iwslt2019</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iwslt-2019">IWSLT 2019</pwcdataset>
    </paper>
    <paper id="24">
      <title>Exploring Kernel Functions in the Softmax Layer for Contextual Word Classification</title>
      <author><first>Yingbo</first><last>Gao</last></author>
      <author><first>Christian</first><last>Herold</last></author>
      <author><first>Weiyue</first><last>Wang</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <abstract>Prominently used in support vector machines and <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic re-gressions</a>, kernel functions (kernels) can implicitly map data points into high dimensional spaces and make it easier to learn complex decision boundaries. In this work, by replacing the inner product function in the softmax layer, we explore the use of <a href="https://en.wikipedia.org/wiki/Kernel_method">kernels</a> for contextual word classification. In order to compare the individual kernels, experiments are conducted on standard language modeling and machine translation tasks. We observe a wide range of performances across different <a href="https://en.wikipedia.org/wiki/Kernel_(operating_system)">kernel settings</a>. Extending the results, we look at the gradient properties, investigate various mixture strategies and examine the disambiguation abilities.</abstract>
      <url hash="f68150f6">2019.iwslt-1.24</url>
      <bibkey>gao-etal-2019-exploring</bibkey>
    </paper>
    <paper id="26">
      <title>Generic and Specialized Word Embeddings for Multi-Domain Machine Translation</title>
      <author><first>MinhQuang</first><last>Pham</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <abstract>Supervised machine translation works well when the train and test data are sampled from the same distribution. When this is not the case, adaptation techniques help ensure that the knowledge learned from out-of-domain texts generalises to in-domain sentences. We study here a related setting, multi-domain adaptation, where the number of domains is potentially large and adapting separately to each domain would waste training resources. Our proposal transposes to <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> the feature expansion technique of (Daum III, 2007): it isolates domain-agnostic from domain-specific lexical representations, while sharing the most of the network across domains. Our experiments use two architectures and two language pairs : they show that our approach, while simple and computationally inexpensive, outperforms several strong baselines and delivers a multi-domain system that successfully translates texts from diverse sources.</abstract>
      <url hash="c5a76260">2019.iwslt-1.26</url>
      <bibkey>pham-etal-2019-generic</bibkey>
    </paper>
    <paper id="27">
      <title>Lexical Micro-adaptation for Neural Machine Translation</title>
      <author><first>Jitao</first><last>Xu</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <abstract>This work is inspired by a typical machine translation industry scenario in which translators make use of in-domain data for facilitating translation of similar or repeating sentences. We introduce a generic framework applied at <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference</a> in which a subset of segment pairs are first extracted from training data according to their similarity to the input sentences. These segments are then used to dynamically update the parameters of a generic NMT network, thus performing a lexical micro-adaptation. Our approach demonstrates strong adaptation performance to new and existing <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> including pseudo in-domain data. We evaluate our approach on a heterogeneous English-French training dataset showing accuracy gains on all evaluated domains when compared to strong adaptation baselines.</abstract>
      <url hash="74fc87c1">2019.iwslt-1.27</url>
      <bibkey>xu-etal-2019-lexical</bibkey>
    </paper>
    <paper id="28">
      <title>Efficient Bilingual Generalization from Neural Transduction Grammar Induction</title>
      <author><first>Yuchen</first><last>Yan</last></author>
      <author><first>Dekai</first><last>Wu</last></author>
      <author><first>Serkan</first><last>Kumyol</last></author>
      <abstract>We introduce (1) a novel neural network structure for bilingual modeling of sentence pairs that allows efficient capturing of bilingual relationship via biconstituent composition, (2) the concept of neural network biparsing, which applies to not only machine translation (MT) but also to a variety of other bilingual research areas, and (3) the concept of a biparsing-backpropagation training loop, which we hypothesize that can efficiently learn complex biparse tree patterns. Our work distinguishes from sequential attention-based models, which are more traditionally found in neural machine translation (NMT) in three aspects. First, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> enforces compositional constraints. Second, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> has a smaller search space in terms of discovering bilingual relationships from bilingual sentence pairs. Third, our model produces explicit biparse trees, which enable transparent error analysis during evaluation and external tree constraints during training.</abstract>
      <url hash="2cfa24d9">2019.iwslt-1.28</url>
      <bibkey>yan-etal-2019-efficient</bibkey>
    </paper>
    <paper id="29">
      <title>Breaking the Data Barrier : Towards Robust Speech Translation via Adversarial Stability Training</title>
      <author><first>Qiao</first><last>Cheng</last></author>
      <author><first>Meiyuan</first><last>Fan</last></author>
      <author><first>Yaqian</first><last>Han</last></author>
      <author><first>Jin</first><last>Huang</last></author>
      <author><first>Yitao</first><last>Duan</last></author>
      <abstract>In a pipeline speech translation system, automatic speech recognition (ASR) system will transmit errors in recognition to the downstream machine translation (MT) system. A standard <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation system</a> is usually trained on <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpus</a> composed of clean text and will perform poorly on text with recognition noise, a gap well known in speech translation community. In this paper, we propose a <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training architecture</a> which aims at making a neural machine translation model more robust against speech recognition errors. Our approach addresses the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and the <a href="https://en.wikipedia.org/wiki/Code">decoder</a> simultaneously using <a href="https://en.wikipedia.org/wiki/Adversarial_learning">adversarial learning</a> and <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, respectively. Experimental results on IWSLT2018 speech translation task show that our approach can bridge the gap between the ASR output and the MT input, outperforms the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> by up to 2.83 BLEU on noisy ASR output, while maintaining close performance on clean text.</abstract>
      <url hash="c688bc26">2019.iwslt-1.29</url>
      <bibkey>cheng-etal-2019-breaking</bibkey>
    </paper>
    <paper id="31">
      <title>Controlling the Output Length of Neural Machine Translation</title>
      <author><first>Surafel Melaku</first><last>Lakew</last></author>
      <author><first>Mattia</first><last>Di Gangi</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <abstract>The recent advances introduced by neural machine translation (NMT) are rapidly expanding the application fields of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, as well as reshaping the quality level to be targeted. In particular, if translations have to fit some given layout, <a href="https://en.wikipedia.org/wiki/Quality_(business)">quality</a> should not only be measured in terms of adequacy and <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a>, but also length. Exemplary cases are the translation of document files, <a href="https://en.wikipedia.org/wiki/Subtitle_(titling)">subtitles</a>, and scripts for dubbing, where the output length should ideally be as close as possible to the length of the input text. This pa-per addresses for the first time, to the best of our knowledge, the problem of controlling the output length in NMT. We investigate two methods for biasing the output length with a transformer architecture : i) conditioning the output to a given target-source length-ratio class and ii) enriching the transformer positional embedding with length information. Our experiments show that both methods can induce the <a href="https://en.wikipedia.org/wiki/Social_network">network</a> to generate shorter translations, as well as acquiring inter- pretable linguistic skills.</abstract>
      <url hash="e032ec61">2019.iwslt-1.31</url>
      <bibkey>lakew-etal-2019-controlling</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    </volume>
</collection>