<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.iwcs">
  <volume id="1" ingest-date="2021-10-26">
    <meta>
      <booktitle>Proceedings of the 14th International Conference on Computational Semantics (IWCS)</booktitle>
      <editor><first>Sina</first><last>Zarrieß</last></editor>
      <editor><first>Johan</first><last>Bos</last></editor>
      <editor><first>Rik</first><last>van Noord</last></editor>
      <editor><first>Lasha</first><last>Abzianidze</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Groningen, The Netherlands (online)</address>
      <month>June</month>
      <year>2021</year>
      <url hash="8f88d3fb">2021.iwcs-1</url>
    </meta>
    <frontmatter>
      <url hash="8c78ef9d">2021.iwcs-1.0</url>
      <bibkey>iwcs-2021-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Switching Contexts : Transportability Measures for NLP<fixed-case>NLP</fixed-case></title>
      <author><first>Guy</first><last>Marshall</last></author>
      <author><first>Mokanarangan</first><last>Thayaparan</last></author>
      <author><first>Philip</first><last>Osborne</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <pages>1–10</pages>
      <abstract>This paper explores the topic of transportability, as a sub-area of generalisability. By proposing the utilisation of <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> based on well-established statistics, we are able to estimate the change in performance of <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP models</a> in new contexts. Defining a new measure for transportability may allow for better estimation of NLP system performance in new domains, and is crucial when assessing the performance of NLP systems in new tasks and domains. Through several instances of increasing <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">complexity</a>, we demonstrate how lightweight domain similarity measures can be used as estimators for the transportability in NLP applications. The proposed transportability measures are evaluated in the context of <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entity Recognition</a> and Natural Language Inference tasks.</abstract>
      <url hash="a46138ff">2021.iwcs-1.1</url>
      <bibkey>marshall-etal-2021-switching</bibkey>
      <pwccode url="https://github.com/ai-systems/transportability" additional="false">ai-systems/transportability</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="2">
      <title>Applied Temporal Analysis : A Complete Run of the FraCaS Test Suite<fixed-case>F</fixed-case>ra<fixed-case>C</fixed-case>a<fixed-case>S</fixed-case> Test Suite</title>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
      <pages>11–20</pages>
      <abstract>In this paper, we propose an implementation of temporal semantics that translates <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">syntax trees</a> to <a href="https://en.wikipedia.org/wiki/Well-formed_formula">logical formulas</a>, suitable for consumption by the <a href="https://en.wikipedia.org/wiki/Coq">Coq proof assistant</a>. The analysis supports a wide range of phenomena including : temporal references, temporal adverbs, aspectual classes and progressives. The new semantics are built on top of a previous system handling all sections of the FraCaS test suite except the temporal reference section, and we obtain an accuracy of 81 percent overall and 73 percent for the problems explicitly marked as related to temporal reference. To the best of our knowledge, this is the best performance of a <a href="https://en.wikipedia.org/wiki/Formal_system">logical system</a> on the whole of the FraCaS.</abstract>
      <url hash="39ab80fd">2021.iwcs-1.2</url>
      <bibkey>bernardy-chatzikyriakidis-2021-applied</bibkey>
    </paper>
    <paper id="7">
      <title>Critical Thinking for <a href="https://en.wikipedia.org/wiki/Language_model">Language Models</a></title>
      <author><first>Gregor</first><last>Betz</last></author>
      <author><first>Christian</first><last>Voigt</last></author>
      <author><first>Kyle</first><last>Richardson</last></author>
      <pages>63–75</pages>
      <abstract>This paper takes a first step towards a critical thinking curriculum for neural auto-regressive language models. We introduce a synthetic corpus of deductively valid arguments, and generate artificial argumentative texts to train CRiPT : a <a href="https://en.wikipedia.org/wiki/Critical_thinking">critical thinking</a> intermediarily pre-trained transformer based on GPT-2. Significant transfer learning effects can be observed : Trained on three simple core schemes, CRiPT accurately completes conclusions of different, and more complex types of arguments, too. CRiPT generalizes the core argument schemes in a correct way. Moreover, we obtain consistent and promising results for NLU benchmarks. In particular, CRiPT’s zero-shot accuracy on the GLUE diagnostics exceeds GPT-2’s performance by 15 percentage points. The findings suggest that intermediary pre-training on texts that exemplify basic reasoning abilities (such as typically covered in critical thinking textbooks) might help <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> to acquire a broad range of reasoning skills. The synthetic argumentative texts presented in this paper are a promising starting point for building such a critical thinking curriculum for <a href="https://en.wikipedia.org/wiki/Language_model">language models</a>.</abstract>
      <url hash="781d8e32">2021.iwcs-1.7</url>
      <bibkey>betz-etal-2021-critical</bibkey>
      <pwccode url="https://github.com/debatelab/aacorpus" additional="false">debatelab/aacorpus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/logiqa">LogiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="12">
      <title>Monotonicity Marking from Universal Dependency Trees<fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Trees</title>
      <author><first>Zeming</first><last>Chen</last></author>
      <author><first>Qiyue</first><last>Gao</last></author>
      <pages>121–131</pages>
      <abstract>Dependency parsing is a tool widely used in the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural language processing</a> and <a href="https://en.wikipedia.org/wiki/Computational_linguistics">computational linguistics</a>. However, there is hardly any work that connects dependency parsing to <a href="https://en.wikipedia.org/wiki/Monotonic_function">monotonicity</a>, which is an essential part of logic and linguistic semantics. In this paper, we present a system that automatically annotates <a href="https://en.wikipedia.org/wiki/Monotonic_function">monotonicity information</a> based on Universal Dependency parse trees. Our system utilizes surface-level monotonicity facts about <a href="https://en.wikipedia.org/wiki/Quantifier_(logic)">quantifiers</a>, <a href="https://en.wikipedia.org/wiki/Lexical_item">lexical items</a>, and token-level polarity information. We compared our <a href="https://en.wikipedia.org/wiki/System">system</a>’s performance with existing systems in the literature, including NatLog and ccg2mono, on a small evaluation dataset. Results show that our <a href="https://en.wikipedia.org/wiki/System">system</a> outperforms NatLog and ccg2mono.</abstract>
      <url hash="c2a48d21">2021.iwcs-1.12</url>
      <attachment type="Attachment" hash="15ca85bb">2021.iwcs-1.12.Attachment.zip</attachment>
      <bibkey>chen-gao-2021-monotonicity</bibkey>
      <award>Outstanding Paper</award>
      <pwccode url="https://github.com/eric11eca/Udep2Mono" additional="false">eric11eca/Udep2Mono</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="13">
      <title>Is that really a question? Going beyond factoid questions in NLP<fixed-case>NLP</fixed-case></title>
      <author><first>Aikaterini-Lida</first><last>Kalouli</last></author>
      <author><first>Rebecca</first><last>Kehlbeck</last></author>
      <author><first>Rita</first><last>Sevastjanova</last></author>
      <author><first>Oliver</first><last>Deussen</last></author>
      <author><first>Daniel</first><last>Keim</last></author>
      <author><first>Miriam</first><last>Butt</last></author>
      <pages>132–143</pages>
      <abstract>Research in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> has mainly focused on factoid questions, with the goal of finding quick and reliable ways of matching a query to an answer. However, human discourse involves more than that : <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> contains non-canonical questions deployed to achieve specific communicative goals. In this paper, we investigate this under-studied aspect of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> by introducing a targeted <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, creating an appropriate <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> for the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> and providing baseline models of diverse nature. With this, we are also able to generate useful insights on the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> and open the way for future research in this direction.</abstract>
      <url hash="4989e886">2021.iwcs-1.13</url>
      <bibkey>kalouli-etal-2021-really</bibkey>
      <award>Outstanding Paper</award>
      <pwccode url="https://github.com/kkalouli/rquet" additional="false">kkalouli/rquet</pwccode>
    </paper>
    <paper id="15">
      <title>Breeding Fillmore’s Chickens and Hatching the Eggs : Recombining Frames and Roles in Frame-Semantic Parsing<fixed-case>F</fixed-case>illmore’s Chickens and Hatching the Eggs: Recombining Frames and Roles in Frame-Semantic Parsing</title>
      <author><first>Gosse</first><last>Minnema</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>155–165</pages>
      <abstract>Frame-semantic parsers traditionally predict <a href="https://en.wikipedia.org/wiki/Predicate_(grammar)">predicates</a>, <a href="https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)">frames</a>, and semantic roles in a fixed order. This paper explores the ‘chicken-or-egg’ problem of interdependencies between these components theoretically and practically. We introduce a flexible BERT-based sequence labeling architecture that allows for predicting frames and roles independently from each other or combining them in several ways. Our results show that our setups can approximate more complex traditional <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>’ performance, while allowing for a clearer view of the interdependencies between the pipeline’s components, and of how frame and role prediction models make different use of BERT’s layers.</abstract>
      <url hash="e8fd313a">2021.iwcs-1.15</url>
      <bibkey>minnema-nissim-2021-breeding</bibkey>
    </paper>
    <paper id="16">
      <title>Large-scale text pre-training helps with dialogue act recognition, but not without <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a></title>
      <author><first>Bill</first><last>Noble</last></author>
      <author><first>Vladislav</first><last>Maraev</last></author>
      <pages>166–172</pages>
      <abstract>We use dialogue act recognition (DAR) to investigate how well BERT represents utterances in dialogue, and how fine-tuning and large-scale pre-training contribute to its performance. We find that while both the standard BERT pre-training and pretraining on dialogue-like data are useful, task-specific fine-tuning is essential for good performance.</abstract>
      <url hash="96175fb0">2021.iwcs-1.16</url>
      <bibkey>noble-maraev-2021-large</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="22">
      <title>Variation in framing as a function of temporal reporting distance</title>
      <author><first>Levi</first><last>Remijnse</last></author>
      <author><first>Marten</first><last>Postma</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <pages>228–238</pages>
      <abstract>In this paper, we measure variation in <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">framing</a> as a function of <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">foregrounding</a> and <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">backgrounding</a> in a co-referential corpus with a range of temporal distance. In one type of experiment, frame-annotated corpora grouped under event types were contrasted, resulting in a ranking of frames with typicality rates. In contrasting between publication dates, a different ranking of frames emerged for documents that are close to or far from the event instance. In the second type of analysis, we trained a diagnostic classifier with frame occurrences in order to let it differentiate documents based on their temporal distance class (close to or far from the event instance). The <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifier</a> performs above chance and outperforms <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> with words.</abstract>
      <url hash="8b299bc5">2021.iwcs-1.22</url>
      <bibkey>remijnse-etal-2021-variation</bibkey>
      <pwccode url="https://github.com/cltl/hdd_analysis" additional="false">cltl/hdd_analysis</pwccode>
    </paper>
    </volume>
</collection>