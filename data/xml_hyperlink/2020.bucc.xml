<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.bucc">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 13th Workshop on Building and Using Comparable Corpora</booktitle>
      <editor><first>Reinhard</first><last>Rapp</last></editor>
      <editor><first>Pierre</first><last>Zweigenbaum</last></editor>
      <editor><first>Serge</first><last>Sharoff</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-42-9</isbn>
    </meta>
    <frontmatter>
      <url hash="a7cecf05">2020.bucc-1.0</url>
      <bibkey>bucc-2020-building</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Constructing a Bilingual Corpus of Parallel Tweets</title>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Sabit</first><last>Hassan</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <pages>14–21</pages>
      <abstract>In a bid to reach a larger and more diverse audience, Twitter users often post parallel tweetstweets that contain the same content but are written in different languages. Parallel tweets can be an important resource for developing <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation (MT) systems</a> among other <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP) tasks</a>. In this paper, we introduce a generic <a href="https://en.wikipedia.org/wiki/Methodology">method</a> for collecting parallel tweets. Using this method, we collect a bilingual corpus of English-Arabic parallel tweets and a list of Twitter accounts who post English-Arabictweets regularly. Since our method is generic, it can also be used for collecting parallel tweets that cover less-resourced languages such as <a href="https://en.wikipedia.org/wiki/Serbian_language">Serbian</a> and <a href="https://en.wikipedia.org/wiki/Urdu">Urdu</a>. Additionally, we annotate a subset of Twitter accounts with their countries of origin and topic of interest, which provides insights about the population who post parallel tweets. This latter information can also be useful for author profiling tasks.</abstract>
      <url hash="b3ac5e2f">2020.bucc-1.3</url>
      <language>eng</language>
      <bibkey>mubarak-etal-2020-constructing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bilingual-corpus-of-arabic-english-parallel">Bilingual Corpus of Arabic-English Parallel Tweets</pwcdataset>
    </paper>
    <paper id="4">
      <title>Automatic Creation of Correspondence Table of Meaning Tags from Two Dictionaries in One Language Using Bilingual Word Embedding</title>
      <author><first>Teruo</first><last>Hirabayashi</last></author>
      <author><first>Kanako</first><last>Komiya</last></author>
      <author><first>Masayuki</first><last>Asahara</last></author>
      <author><first>Hiroyuki</first><last>Shinnou</last></author>
      <pages>22–28</pages>
      <abstract>In this paper, we show how to use bilingual word embeddings (BWE) to automatically create a corresponding table of meaning tags from two dictionaries in one language and examine the effectiveness of the method. To do this, we had a problem : the meaning tags do not always correspond one-to-one because the granularities of the <a href="https://en.wikipedia.org/wiki/Word_sense">word senses</a> and the <a href="https://en.wikipedia.org/wiki/Concept">concepts</a> are different from each other. Therefore, we regarded the concept tag that corresponds to a <a href="https://en.wikipedia.org/wiki/Word_sense">word sense</a> the most as the correct concept tag corresponding the <a href="https://en.wikipedia.org/wiki/Word_sense">word sense</a>. We used two BWE methods, a <a href="https://en.wikipedia.org/wiki/Linear_map">linear transformation matrix</a> and VecMap. We evaluated the most frequent sense (MFS) method and the corpus concatenation method for comparison. The accuracies of the proposed methods were higher than the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of the random baseline but lower than those of the MFS and corpus concatenation methods. However, because our method utilized the embedding vectors of the word senses, the relations of the sense tags corresponding to concept tags could be examined by mapping the sense embeddings to the vector space of the concept tags. Also, our methods could be performed when we have only concept or word sense embeddings whereas the MFS method requires a <a href="https://en.wikipedia.org/wiki/Parallel_corpus">parallel corpus</a> and the corpus concatenation method needs two tagged corpora.</abstract>
      <url hash="3b5027b0">2020.bucc-1.4</url>
      <language>eng</language>
      <bibkey>hirabayashi-etal-2020-automatic</bibkey>
    </paper>
    <paper id="6">
      <title>Benchmarking Multidomain English-Indonesian Machine Translation<fixed-case>E</fixed-case>nglish-<fixed-case>I</fixed-case>ndonesian Machine Translation</title>
      <author><first>Tri Wahyu</first><last>Guntara</last></author>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <author><first>Radityo Eko</first><last>Prasojo</last></author>
      <pages>35–43</pages>
      <abstract>In the context of Machine Translation (MT) from-and-to English, <a href="https://en.wikipedia.org/wiki/Indonesian_language">Bahasa Indonesia</a> has been considered a low-resource language, and therefore applying Neural Machine Translation (NMT) which typically requires large training dataset proves to be problematic. In this paper, we show otherwise by collecting large, publicly-available datasets from the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">Web</a>, which we split into several domains : <a href="https://en.wikipedia.org/wiki/News">news</a>, <a href="https://en.wikipedia.org/wiki/Religion">religion</a>, general, and <a href="https://en.wikipedia.org/wiki/Conversation">conversation</a>, to train and benchmark some variants of transformer-based NMT models across the domains. We show using BLEU that our models perform well across them, outperform the baseline Statistical Machine Translation (SMT) models, and perform comparably with <a href="https://en.wikipedia.org/wiki/Google_Translate">Google Translate</a>. Our datasets (with the standard split for training, validation, and testing), code, and models are available on<url>https://github.com/gunnxx/indonesian-mt-data</url>
      </abstract>
      <url hash="7513e58c">2020.bucc-1.6</url>
      <language>eng</language>
      <bibkey>guntara-etal-2020-benchmarking</bibkey>
      <pwccode url="https://github.com/gunnxx/indonesian-mt-data" additional="false">gunnxx/indonesian-mt-data</pwccode>
    </paper>
    <paper id="7">
      <title>Reducing the Search Space for Parallel Sentences in Comparable Corpora</title>
      <author><first>Rémi</first><last>Cardon</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <pages>44–48</pages>
      <abstract>This paper describes and evaluates simple techniques for reducing the research space for parallel sentences in monolingual comparable corpora. Initially, when searching for parallel sentences between two comparable documents, all the possible sentence pairs between the documents have to be considered, which introduces a great degree of imbalance between parallel pairs and non-parallel pairs. This is a problem because even with a high performing <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a>, a lot of <a href="https://en.wikipedia.org/wiki/Noise">noise</a> will be present in the extracted results, thus introducing a need for an extensive and costly manual check phase. We work on a manually annotated subset obtained from a French comparable corpus and show how we can drastically reduce the number of sentence pairs that have to be fed to a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> so that the results can be manually handled.</abstract>
      <url hash="fe2455e4">2020.bucc-1.7</url>
      <language>eng</language>
      <bibkey>cardon-grabar-2020-reducing</bibkey>
    </paper>
    <paper id="9">
      <title>TALN / LS2N Participation at the BUCC Shared Task : Bilingual Dictionary Induction from Comparable Corpora<fixed-case>TALN</fixed-case>/<fixed-case>LS</fixed-case>2<fixed-case>N</fixed-case> Participation at the <fixed-case>BUCC</fixed-case> Shared Task: Bilingual Dictionary Induction from Comparable Corpora</title>
      <author><first>Martin</first><last>Laville</last></author>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <pages>56–60</pages>
      <abstract>This paper describes the TALN / LS2N system participation at the Building and Using Comparable Corpora (BUCC) shared task. We first introduce three strategies : (i) a word embedding approach based on fastText embeddings ; (ii) a concatenation approach using both character Skip-Gram and character CBOW models, and finally (iii) a cognates matching approach based on an exact match string similarity. Then, we present the applied <a href="https://en.wikipedia.org/wiki/Strategy_(game_theory)">strategy</a> for the shared task which consists in the combination of the embeddings concatenation and the cognates matching approaches. The covered languages are <a href="https://en.wikipedia.org/wiki/French_language">French</a>, <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a>, <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a> and <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>. Overall, our system mixing embeddings concatenation and perfect cognates matching obtained the best results while compared to individual strategies, except for English-Russian and Russian-English language pairs for which the concatenation approach was preferred.</abstract>
      <url hash="c0189300">2020.bucc-1.9</url>
      <language>eng</language>
      <bibkey>laville-etal-2020-taln</bibkey>
    </paper>
    <paper id="11">
      <title>BUCC2020 : Bilingual Dictionary Induction using Cross-lingual Embedding<fixed-case>BUCC</fixed-case>2020: Bilingual Dictionary Induction using Cross-lingual Embedding</title>
      <author><first>Sanjanasri</first><last>JP</last></author>
      <author><first>Vijay Krishna</first><last>Menon</last></author>
      <author><first>Soman</first><last>KP</last></author>
      <pages>65–68</pages>
      <abstract>This paper presents a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning system</a> for the BUCC 2020 shared task : Bilingual dictionary induction from comparable corpora. We have submitted two runs for this shared Task, German (de) and English (en) language pair for closed track and Tamil (ta) and English (en) for the open track. Our core approach focuses on quantifying the <a href="https://en.wikipedia.org/wiki/Semantics">semantics of the language pairs</a>, so that <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> of two different language pairs can be compared or transfer learned. With the advent of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>, it is possible to quantify this. In this paper, we propose a deep learning approach which makes use of the supplied training data, to generate cross-lingual embedding. This is later used for inducting <a href="https://en.wikipedia.org/wiki/Bilingual_dictionary">bilingual dictionary</a> from comparable corpora.</abstract>
      <url hash="2e3ec71e">2020.bucc-1.11</url>
      <language>eng</language>
      <bibkey>jp-etal-2020-bucc2020</bibkey>
    </paper>
  </volume>
</collection>