<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.wmt">
  <volume id="1" ingest-date="2020-12-22">
    <meta>
      <booktitle>Proceedings of the Fifth Conference on Machine Translation</booktitle>
      <editor><first>Loïc</first><last>Barrault</last></editor>
      <editor><first>Ondřej</first><last>Bojar</last></editor>
      <editor><first>Fethi</first><last>Bougares</last></editor>
      <editor><first>Rajen</first><last>Chatterjee</last></editor>
      <editor><first>Marta R.</first><last>Costa-jussà</last></editor>
      <editor><first>Christian</first><last>Federmann</last></editor>
      <editor><first>Mark</first><last>Fishel</last></editor>
      <editor><first>Alexander</first><last>Fraser</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Paco</first><last>Guzman</last></editor>
      <editor><first>Barry</first><last>Haddow</last></editor>
      <editor><first>Matthias</first><last>Huck</last></editor>
      <editor><first>Antonio Jimeno</first><last>Yepes</last></editor>
      <editor><first>Philipp</first><last>Koehn</last></editor>
      <editor><first>André</first><last>Martins</last></editor>
      <editor><first>Makoto</first><last>Morishita</last></editor>
      <editor><first>Christof</first><last>Monz</last></editor>
      <editor><first>Masaaki</first><last>Nagata</last></editor>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Matteo</first><last>Negri</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="42919a30">2020.wmt-1.0</url>
      <bibkey>wmt-2020-machine</bibkey>
    </frontmatter>
    <paper id="12">
      <title>Tohoku-AIP-NTT at WMT 2020 News Translation Task<fixed-case>AIP</fixed-case>-<fixed-case>NTT</fixed-case> at <fixed-case>WMT</fixed-case> 2020 News Translation Task</title>
      <author><first>Shun</first><last>Kiyono</last></author>
      <author><first>Takumi</first><last>Ito</last></author>
      <author><first>Ryuto</first><last>Konno</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <pages>145–155</pages>
      <abstract>In this paper, we describe the submission of Tohoku-AIP-NTT to the WMT’20 news translation task. We participated in this task in two language pairs and four language directions : <a href="https://en.wikipedia.org/wiki/German_language">English   German</a> and <a href="https://en.wikipedia.org/wiki/Japanese_language">English   Japanese</a>. Our system consists of techniques such as <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a> and <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>, which are already widely adopted in translation tasks. We attempted to develop new <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for both synthetic data filtering and <a href="https://en.wikipedia.org/wiki/Ranking">reranking</a>. However, the <a href="https://en.wikipedia.org/wiki/Scientific_method">methods</a> turned out to be ineffective, and they provided us with no significant improvement over the <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline</a>. We analyze these negative results to provide insights for future studies.</abstract>
      <url hash="e5b6c4c9">2020.wmt-1.12</url>
      <video href="https://slideslive.com/38939545" />
      <bibkey>kiyono-etal-2020-tohoku</bibkey>
    </paper>
    <paper id="13">
      <title>NRC Systems for the 2020 Inuktitut-English News Translation Task<fixed-case>NRC</fixed-case> Systems for the 2020 <fixed-case>I</fixed-case>nuktitut-<fixed-case>E</fixed-case>nglish News Translation Task</title>
      <author><first>Rebecca</first><last>Knowles</last></author>
      <author><first>Darlene</first><last>Stewart</last></author>
      <author><first>Samuel</first><last>Larkin</last></author>
      <author><first>Patrick</first><last>Littell</last></author>
      <pages>156–170</pages>
      <abstract>We describe the National Research Council of Canada (NRC) submissions for the 2020 Inuktitut-English shared task on news translation at the Fifth Conference on Machine Translation (WMT20). Our submissions consist of ensembled domain-specific finetuned transformer models, trained using the Nunavut Hansard and news data and, in the case of Inuktitut-English, backtranslated news and parliamentary data. In this work we explore challenges related to the relatively small amount of parallel data, morphological complexity, and domain shifts.</abstract>
      <url hash="a834ee2f">2020.wmt-1.13</url>
      <video href="https://slideslive.com/38939639" />
      <bibkey>knowles-etal-2020-nrc</bibkey>
    </paper>
    <paper id="14">
      <title>CUNI Submission for the Inuktitut Language in WMT News 2020<fixed-case>CUNI</fixed-case> Submission for the <fixed-case>I</fixed-case>nuktitut Language in <fixed-case>WMT</fixed-case> News 2020</title>
      <author><first>Tom</first><last>Kocmi</last></author>
      <pages>171–174</pages>
      <abstract>This paper describes CUNI submission to the WMT 2020 News Translation Shared Task for the low-resource scenario InuktitutEnglish in both translation directions. Our system combines <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> from a CzechEnglish high-resource language pair and backtranslation. We notice surprising behaviour when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. We are using the Transformer model in a constrained submission.</abstract>
      <url hash="bb9e6059">2020.wmt-1.14</url>
      <video href="https://slideslive.com/38939666" />
      <bibkey>kocmi-2020-cuni</bibkey>
    </paper>
    <paper id="17">
      <title>Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model : the UEDIN-CUNI Submission to the WMT 2020 News Translation Task<fixed-case>UEDIN</fixed-case>-<fixed-case>CUNI</fixed-case> Submission to the <fixed-case>WMT</fixed-case> 2020 News Translation Task</title>
      <author><first>Ulrich</first><last>Germann</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Radina</first><last>Dobreva</last></author>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>191–196</pages>
      <abstract>We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech / English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference efficiency</a>. On the WMT 2020 Czech   English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single <a href="https://en.wikipedia.org/wiki/Thread_(computing)">CPU thread</a>, thus making neural translation feasible on consumer hardware without a <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a>.</abstract>
      <url hash="511f1989">2020.wmt-1.17</url>
      <video href="https://slideslive.com/38939661" />
      <bibkey>germann-etal-2020-speed</bibkey>
    </paper>
    <paper id="18">
      <title>The University of Edinburgh’s submission to the German-to-English and English-to-German Tracks in the WMT 2020 News Translation and Zero-shot Translation Robustness Tasks<fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh’s submission to the <fixed-case>G</fixed-case>erman-to-<fixed-case>E</fixed-case>nglish and <fixed-case>E</fixed-case>nglish-to-<fixed-case>G</fixed-case>erman Tracks in the <fixed-case>WMT</fixed-case> 2020 News Translation and Zero-shot Translation Robustness Tasks</title>
      <author><first>Ulrich</first><last>Germann</last></author>
      <pages>197–201</pages>
      <abstract>This paper describes the University of Edinburgh’s submission of German-English systems to the WMT2020 Shared Tasks on News Translation and Zero-shot Robustness.</abstract>
      <url hash="9fa30b19">2020.wmt-1.18</url>
      <video href="https://slideslive.com/38939663" />
      <bibkey>germann-2020-university</bibkey>
    </paper>
    <paper id="22">
      <title>SJTU-NICT’s Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task<fixed-case>SJTU</fixed-case>-<fixed-case>NICT</fixed-case>’s Supervised and Unsupervised Neural Machine Translation Systems for the <fixed-case>WMT</fixed-case>20 News Translation Task</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Kehai</first><last>Chen</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>218–229</pages>
      <abstract>In this paper, we introduced our joint team SJTU-NICT ‘s participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs : English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques : document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a>. In our submissions, the primary systems won the first place on <a href="https://en.wikipedia.org/wiki/English_language">English</a> to <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, <a href="https://en.wikipedia.org/wiki/Polish_language">Polish</a> to <a href="https://en.wikipedia.org/wiki/English_language">English</a>, and German to Upper Sorbian translation directions.</abstract>
      <url hash="65d90d87">2020.wmt-1.22</url>
      <video href="https://slideslive.com/38939657" />
      <bibkey>li-etal-2020-sjtu</bibkey>
    </paper>
    <paper id="28">
      <title>CUNI English-Czech and English-Polish Systems in WMT20 : Robust Document-Level Training<fixed-case>CUNI</fixed-case> <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>zech and <fixed-case>E</fixed-case>nglish-<fixed-case>P</fixed-case>olish Systems in <fixed-case>WMT</fixed-case>20: Robust Document-Level Training</title>
      <author><first>Martin</first><last>Popel</last></author>
      <pages>269–273</pages>
      <abstract>We describe our two NMT systems submitted to the WMT 2020 shared task in English-Czech and English-Polish news translation. One <a href="https://en.wikipedia.org/wiki/System">system</a> is sentence level, translating each sentence independently. The second system is document level, translating multiple sentences, trained on <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">multi-sentence sequences</a> up to 3000 characters long.</abstract>
      <url hash="fdb9dba0">2020.wmt-1.28</url>
      <video href="https://slideslive.com/38939668" />
      <bibkey>popel-2020-cuni</bibkey>
    </paper>
    <paper id="30">
      <title>OPPO’s Machine Translation Systems for WMT20<fixed-case>OPPO</fixed-case>’s Machine Translation Systems for <fixed-case>WMT</fixed-case>20</title>
      <author><first>Tingxun</first><last>Shi</last></author>
      <author><first>Shiyu</first><last>Zhao</last></author>
      <author><first>Xiaopu</first><last>Li</last></author>
      <author><first>Xiaoxue</first><last>Wang</last></author>
      <author><first>Qian</first><last>Zhang</last></author>
      <author><first>Di</first><last>Ai</last></author>
      <author><first>Dawei</first><last>Dang</last></author>
      <author><first>Xue</first><last>Zhengshan</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <pages>282–292</pages>
      <abstract>In this paper we demonstrate our (OPPO’s) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts : the data preprocessing part will show how the data are preprocessed and filtered, and the system part will show our models architecture and the techniques we followed. Detailed information, such as training hyperparameters and the results generated by each technique will be depicted in the corresponding subsections. Our final submissions ranked top in 6 directions (English   Czech, English   Russian, French   German and Tamil   English), third in 2 directions (English   German, English   Japanese), and fourth in 2 directions (English   Pashto and and English   Tamil).<tex-math>\leftrightarrow</tex-math> Czech, English <tex-math>\leftrightarrow</tex-math> Russian, French <tex-math>\rightarrow</tex-math> German and Tamil <tex-math>\rightarrow</tex-math> English), third in 2 directions (English <tex-math>\rightarrow</tex-math> German, English <tex-math>\rightarrow</tex-math> Japanese), and fourth in 2 directions (English <tex-math>\rightarrow</tex-math> Pashto and and English <tex-math>\rightarrow</tex-math> Tamil).</abstract>
      <url hash="b7ee25aa">2020.wmt-1.30</url>
      <video href="https://slideslive.com/38939558" />
      <bibkey>shi-etal-2020-oppos</bibkey>
    </paper>
    <paper id="31">
      <title>HW-TSC’s Participation in the WMT 2020 News Translation Shared Task<fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation in the <fixed-case>WMT</fixed-case> 2020 News Translation Shared Task</title>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Zhengzhe</first><last>Yu</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <author><first>Shiliang</first><last>Sun</last></author>
      <pages>293–299</pages>
      <abstract>This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh / En, Km / En, and Ps / En and in both directions under the constrained condition. We use the standard Transformer-Big model as the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> and obtain the best performance via two variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual dataset. Several commonly used strategies are used to train our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> such as <a href="https://en.wikipedia.org/wiki/Back_translation">Back Translation</a>, Ensemble Knowledge Distillation, etc. We also conduct experiment with similar language augmentation, which lead to positive results, although not used in our submission. Our submission obtains remarkable results in the final evaluation.</abstract>
      <url hash="db78d297">2020.wmt-1.31</url>
      <video href="https://slideslive.com/38939573" />
      <bibkey>wei-etal-2020-hw</bibkey>
    </paper>
    <paper id="33">
      <title>The Volctrans Machine Translation System for WMT20<fixed-case>WMT</fixed-case>20</title>
      <author><first>Liwei</first><last>Wu</last></author>
      <author><first>Xiao</first><last>Pan</last></author>
      <author><first>Zehui</first><last>Lin</last></author>
      <author><first>Yaoming</first><last>Zhu</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>305–312</pages>
      <abstract>This paper describes our submission systems for VolcTrans for WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer (CITATION), into which we also employed new architectures (bigger or deeper Transformers, dynamic convolution). The final systems include text pre-process, subword(a.k.a. BPE(CITATION)), baseline model training, iterative back-translation, model ensemble, knowledge distillation and multilingual pre-training.</abstract>
      <url hash="58264a1d">2020.wmt-1.33</url>
      <video href="https://slideslive.com/38939581" />
      <bibkey>wu-etal-2020-volctrans</bibkey>
    </paper>
    <paper id="37">
      <title>The NiuTrans Machine Translation Systems for WMT20<fixed-case>N</fixed-case>iu<fixed-case>T</fixed-case>rans Machine Translation Systems for <fixed-case>WMT</fixed-case>20</title>
      <author><first>Yuhao</first><last>Zhang</last></author>
      <author><first>Ziyang</first><last>Wang</last></author>
      <author><first>Runzhe</first><last>Cao</last></author>
      <author><first>Binghao</first><last>Wei</last></author>
      <author><first>Weiqiao</first><last>Shan</last></author>
      <author><first>Shuhan</first><last>Zhou</last></author>
      <author><first>Abudurexiti</first><last>Reheman</last></author>
      <author><first>Tao</first><last>Zhou</last></author>
      <author><first>Xin</first><last>Zeng</last></author>
      <author><first>Laohu</first><last>Wang</last></author>
      <author><first>Yongyu</first><last>Mu</last></author>
      <author><first>Jingnan</first><last>Zhang</last></author>
      <author><first>Xiaoqian</first><last>Liu</last></author>
      <author><first>Xuanjun</first><last>Zhou</last></author>
      <author><first>Yinqiao</first><last>Li</last></author>
      <author><first>Bei</first><last>Li</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>338–345</pages>
      <abstract>This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese-English, English-Chinese, Inuktitut-English and Tamil-English total five tasks and rank first in Japanese-English both sides. We mainly utilized iterative back-translation, different depth and widen model architectures, iterative knowledge distillation and iterative fine-tuning. And we find that adequately widened and deepened the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> simultaneously, the performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut-English and Tamil-English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance.</abstract>
      <url hash="d2a127ab">2020.wmt-1.37</url>
      <video href="https://slideslive.com/38939572" />
      <bibkey>zhang-etal-2020-niutrans</bibkey>
    </paper>
    <paper id="39">
      <title>Gender Coreference and Bias Evaluation at WMT 2020<fixed-case>WMT</fixed-case> 2020</title>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Tomasz</first><last>Limisiewicz</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <pages>357–364</pages>
      <abstract>Gender bias in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> become more popular and deployed within commercial systems. Our work presents the largest evidence for the phenomenon in more than 19 systems submitted to the WMT over four diverse target languages : <a href="https://en.wikipedia.org/wiki/Czech_language">Czech</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a>, <a href="https://en.wikipedia.org/wiki/Polish_language">Polish</a>, and <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a>. To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to languages with <a href="https://en.wikipedia.org/wiki/Grammatical_gender">grammatical gender</a>. We extend WinoMT to handle two new <a href="https://en.wikipedia.org/wiki/Language">languages</a> tested in WMT : <a href="https://en.wikipedia.org/wiki/Polish_language">Polish</a> and <a href="https://en.wikipedia.org/wiki/Czech_language">Czech</a>. We find that all <a href="https://en.wikipedia.org/wiki/System">systems</a> consistently use spurious correlations in the data rather than meaningful <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a>.</abstract>
      <url hash="7a9c379f">2020.wmt-1.39</url>
      <video href="https://slideslive.com/38939659" />
      <bibkey>kocmi-etal-2020-gender</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="42">
      <title>Translating Similar Languages : Role of <a href="https://en.wikipedia.org/wiki/Mutual_intelligibility">Mutual Intelligibility</a> in Multilingual Transformers</title>
      <author><first>Ife</first><last>Adebara</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>Muhammad</first><last>Abdul Mageed</last></author>
      <pages>381–386</pages>
      <abstract>In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. We participated in all language pairs and performed various experiments. We used a transformer architecture for all the <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> and used <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a> for one of the language pairs. We explore both bilingual and multi-lingual approaches. We describe the pre-processing, training, translation and results for each <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>. We also investigate the role of <a href="https://en.wikipedia.org/wiki/Mutual_intelligibility">mutual intelligibility</a> in <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance.</abstract>
      <url hash="56245539">2020.wmt-1.42</url>
      <video href="https://slideslive.com/38939640" />
      <bibkey>adebara-etal-2020-translating</bibkey>
    </paper>
    <paper id="47">
      <title>The IPN-CIC team system submission for the WMT 2020 similar language task<fixed-case>IPN</fixed-case>-<fixed-case>CIC</fixed-case> team system submission for the <fixed-case>WMT</fixed-case> 2020 similar language task</title>
      <author><first>Luis A.</first><last>Menéndez-Salazar</last></author>
      <author><first>Grigori</first><last>Sidorov</last></author>
      <author><first>Marta R.</first><last>Costa-Jussà</last></author>
      <pages>409–413</pages>
      <abstract>This paper describes the participation of the NLP research team of the IPN Computer Research center in the WMT 2020 Similar Language Translation Task. We have submitted <a href="https://en.wikipedia.org/wiki/Linguistic_system">systems</a> for the Spanish-Portuguese language pair (in both directions). The three submitted systems are based on the Transformer architecture and used fine tuning for <a href="https://en.wikipedia.org/wiki/Domain_Adaptation">domain Adaptation</a>.</abstract>
      <url hash="553ba031">2020.wmt-1.47</url>
      <video href="https://slideslive.com/38939595" />
      <bibkey>menendez-salazar-etal-2020-ipn</bibkey>
    </paper>
    <paper id="49">
      <title>NUIG-Panlingua-KMI Hindi-Marathi MT Systems for Similar Language Translation Task @ WMT 2020<fixed-case>NUIG</fixed-case>-Panlingua-<fixed-case>KMI</fixed-case> <fixed-case>H</fixed-case>indi-<fixed-case>M</fixed-case>arathi <fixed-case>MT</fixed-case> Systems for Similar Language Translation Task @ <fixed-case>WMT</fixed-case> 2020</title>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <author><first>Priya</first><last>Rani</last></author>
      <author><first>Akanksha</first><last>Bansal</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Ritesh</first><last>Kumar</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <pages>418–423</pages>
      <abstract>NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in Similar Language Translation Task for HindiMarathi language pair. As part of these efforts, we conducteda series of experiments to address the challenges for translation between similar languages. Among the 4 MT systems prepared under this task, 1 PBSMT systems were prepared for HindiMarathi each and 1 NMT systems were developed for <a href="https://en.wikipedia.org/wiki/Marathi_language">HindiMarathi</a> using Byte PairEn-coding (BPE) into subwords. The results show that different architectures NMT could be an effective method for developing MT systems for closely related languages. Our Hindi-Marathi NMT system was ranked 8th among the 14 teams that participated and our Marathi-Hindi NMT system was ranked 8th among the 11 teams participated for the task.</abstract>
      <url hash="4d53a2ad">2020.wmt-1.49</url>
      <video href="https://slideslive.com/38939638" />
      <bibkey>ojha-etal-2020-nuig</bibkey>
    </paper>
    <paper id="53">
      <title>Document Level NMT of Low-Resource Languages with Backtranslation<fixed-case>NMT</fixed-case> of Low-Resource Languages with Backtranslation</title>
      <author><first>Sami</first><last>Ul Haq</last></author>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>Arsalan</first><last>Shaukat</last></author>
      <author><first>Abdullah</first><last>Saeed</last></author>
      <pages>442–446</pages>
      <abstract>This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair MarathiHindi. Our system is an extension of state-of-the-art Transformer architecture with hierarchical attention networks to incorporate <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a>. Since, NMT requires large amount of parallel data which is not available for this task, our approach is focused on utilizing <a href="https://en.wikipedia.org/wiki/Monolingualism">monolingual data</a> with <a href="https://en.wikipedia.org/wiki/Back_translation">back translation</a> to train our <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>. Our experiments reveal that document-level NMT can be a reasonable alternative to sentence-level NMT for improving translation quality of low resourced languages even when used with synthetic data.</abstract>
      <url hash="a32a4c70">2020.wmt-1.53</url>
      <video href="https://slideslive.com/38939608" />
      <bibkey>ul-haq-etal-2020-document</bibkey>
    </paper>
    <paper id="56">
      <title>The University of Maryland’s Submissions to the WMT20 Chat Translation Task : Searching for More Data to Adapt Discourse-Aware Neural Machine Translation<fixed-case>U</fixed-case>niversity of <fixed-case>M</fixed-case>aryland’s Submissions to the <fixed-case>WMT</fixed-case>20 Chat Translation Task: Searching for More Data to Adapt Discourse-Aware Neural Machine Translation</title>
      <author><first>Calvin</first><last>Bao</last></author>
      <author><first>Yow-Ting</first><last>Shiue</last></author>
      <author><first>Chujun</first><last>Song</last></author>
      <author><first>Jie</first><last>Li</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>456–461</pages>
      <abstract>This paper describes the University of Maryland’s submissions to the WMT20 Shared Task on Chat Translation. We focus on translating agent-side utterances from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to <a href="https://en.wikipedia.org/wiki/German_language">German</a>. We started from an off-the-shelf BPE-based standard transformer model trained with WMT17 news and fine-tuned it with the provided in-domain training data. In addition, we augment the training set with its best matches in the WMT19 news dataset. Our primary submission uses a standard <a href="https://en.wikipedia.org/wiki/Transformers_(toy_line)">Transformer</a>, while our contrastive submissions use multi-encoder Transformers to attend to previous utterances. Our primary submission achieves 56.7 <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> on the agent side (ende), outperforming a baseline system provided by the task organizers by more than 13 BLEU points. Moreover, according to an evaluation on a set of carefully-designed examples, the multi-encoder architecture is able to generate more coherent translations.</abstract>
      <url hash="aab797e5">2020.wmt-1.56</url>
      <video href="https://slideslive.com/38939647" />
      <bibkey>bao-etal-2020-university</bibkey>
    </paper>
    <paper id="62">
      <title>Fast Interleaved Bidirectional Sequence Generation</title>
      <author><first>Biao</first><last>Zhang</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>503–515</pages>
      <abstract>Independence assumptions during sequence generation can speed up <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference</a>, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-to-left directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and selfattention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of ~2x compared to autoregressive decoding with comparable quality. Notably, it outperforms left-to-right SA because the <a href="https://en.wikipedia.org/wiki/Independence_(probability_theory)">independence assumptions</a> in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4x11x across different <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> at the cost of 1 <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> or 0.5 ROUGE (on average)</abstract>
      <url hash="1b00560c">2020.wmt-1.62</url>
      <video href="https://slideslive.com/38939588" />
      <bibkey>zhang-etal-2020-fast</bibkey>
      <pwccode url="https://github.com/bzhangGo/zero" additional="false">bzhangGo/zero</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="70">
      <title>Towards Multimodal Simultaneous Neural Machine Translation</title>
      <author><first>Aizhan</first><last>Imankulova</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Tosho</first><last>Hirasawa</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>594–603</pages>
      <abstract>Simultaneous translation involves translating a sentence before the speaker’s utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT), which leverages visual information as an additional modality. Our experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency. Furthermore, we verified the importance of visual information during decoding by performing an adversarial evaluation of MSNMT, where we studied how models behaved with incongruent input modality and analyzed the effect of different word order between source and target languages.</abstract>
      <url hash="6f2582e1">2020.wmt-1.70</url>
      <video href="https://slideslive.com/38939559" />
      <bibkey>imankulova-etal-2020-towards</bibkey>
      <pwccode url="https://github.com/toshohirasawa/mst" additional="false">toshohirasawa/mst</pwccode>
    </paper>
    <paper id="74">
      <title>Document-aligned Japanese-English Conversation Parallel Corpus<fixed-case>J</fixed-case>apanese-<fixed-case>E</fixed-case>nglish Conversation Parallel Corpus</title>
      <author><first>Matīss</first><last>Rikters</last></author>
      <author><first>Ryokan</first><last>Ri</last></author>
      <author><first>Tong</first><last>Li</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <pages>639–645</pages>
      <abstract>Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data ; and 2) evaluate, as the main methods and data sets focus on SL evaluation. To address the first issue, we present a document-aligned Japanese-English conversation corpus, including balanced, high-quality business conversation data for tuning and testing. As for the second issue, we manually identify the main areas where SL MT fails to produce adequate translations in lack of context. We then create an evaluation set where these phenomena are annotated to alleviate automatic evaluation of DL systems. We train MT models using our <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> to demonstrate how using <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a> leads to improvements.</abstract>
      <url hash="1ab7229f">2020.wmt-1.74</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b789c2b8">2020.wmt-1.74.OptionalSupplementaryMaterial.pdf</attachment>
      <video href="https://slideslive.com/38939560" />
      <bibkey>rikters-etal-2020-document</bibkey>
      <pwccode url="https://github.com/tsuruoka-lab/AMI-Meeting-Parallel-Corpus" additional="false">tsuruoka-lab/AMI-Meeting-Parallel-Corpus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/business-scene-dialogue">Business Scene Dialogue</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jparacrawl">JParaCrawl</pwcdataset>
    </paper>
    <paper id="75">
      <title>Findings of the WMT 2020 Shared Task on Automatic Post-Editing<fixed-case>WMT</fixed-case> 2020 Shared Task on Automatic Post-Editing</title>
      <author><first>Rajen</first><last>Chatterjee</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>646–659</pages>
      <abstract>We present the results of the 6th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a black-box machine translation system by learning from existing human corrections of different sentences. This year, the challenge consisted of fixing the errors present in English Wikipedia pages translated into <a href="https://en.wikipedia.org/wiki/German_language">German</a> and <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a> by state-ofthe-art, not domain-adapted neural MT (NMT) systems unknown to participants. Six teams participated in the English-German task, submitting a total of 11 runs. Two teams participated in the English-Chinese task submitting 2 runs each. Due to i) the different source / domain of data compared to the past (Wikipedia vs Information Technology), ii) the different quality of the initial translations to be corrected and iii) the introduction of a new language pair (English-Chinese), this year’s results are not directly comparable with last year’s round. However, on both language directions, participants’ submissions show considerable improvements over the baseline results. On <a href="https://en.wikipedia.org/wiki/German_language">English-German</a>, the top ranked system improves over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> by -11.35 TER and +16.68 BLEU points, while on <a href="https://en.wikipedia.org/wiki/Chinese_language">EnglishChinese</a> the improvements are respectively up to -12.13 TER and +14.57 BLEU points. Overall, coherent gains are also highlighted by the outcomes of human evaluation, which confirms the effectiveness of APE to improve MT quality, especially in the new generic domain selected for this year’s round.</abstract>
      <url hash="a77d9210">2020.wmt-1.75</url>
      <video href="https://slideslive.com/38939672" />
      <bibkey>chatterjee-etal-2020-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="77">
      <title>Results of the WMT20 Metrics Shared Task<fixed-case>WMT</fixed-case>20 Metrics Shared Task</title>
      <author><first>Nitika</first><last>Mathur</last></author>
      <author><first>Johnny</first><last>Wei</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Qingsong</first><last>Ma</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>688–725</pages>
      <abstract>This paper presents the results of the WMT20 Metrics Shared Task. Participants were asked to score the outputs of the translation systems competing in the WMT20 News Translation Task with <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">automatic metrics</a>. Ten research groups submitted 27 metrics, four of which are <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">reference-less metrics</a>. In addition, we computed five baseline metrics, including sentBLEU, <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>, TER and using the SacreBLEU scorer. All <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> were evaluated on how well they correlate at the system-, document- and segment-level with the WMT20 official human scores. We present an extensive analysis on influence of different reference translations on <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric reliability</a>, how well automatic metrics score human translations, and we also flag major discrepancies between <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> and human scores when evaluating MT systems. Finally, we investigate whether we can use <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">automatic metrics</a> to flag incorrect human ratings.</abstract>
      <url hash="8830e5cf">2020.wmt-1.77</url>
      <bibkey>mathur-etal-2020-results</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="81">
      <title>Cross-Lingual Transformers for Neural Automatic Post-Editing</title>
      <author><first>Dongjun</first><last>Lee</last></author>
      <pages>772–776</pages>
      <abstract>In this paper, we describe the Bering Lab’s submission to the WMT 2020 Shared Task on Automatic Post-Editing (APE). First, we propose a cross-lingual Transformer architecture that takes a concatenation of a source sentence and a machine-translated (MT) sentence as an input to generate the post-edited (PE) output. For further improvement, we mask incorrect or missing words in the PE output based on word-level quality estimation and then predict the actual word for each mask based on the fine-tuned cross-lingual language model (XLM-RoBERTa). Finally, to address the over-correction problem, we select the final output among the PE outputs and the original MT sentence based on a sentence-level quality estimation. When evaluated on the WMT 2020 English-German APE test dataset, our <a href="https://en.wikipedia.org/wiki/System">system</a> improves the NMT output by -3.95 and +4.50 in terms of <a href="https://en.wikipedia.org/wiki/Terminology_of_the_Low_Countries">TER</a> and BLEU, respectively.</abstract>
      <url hash="08284caf">2020.wmt-1.81</url>
      <video href="https://slideslive.com/38939547" />
      <bibkey>lee-2020-cross</bibkey>
    </paper>
    <paper id="82">
      <title>POSTECH-ETRI’s Submission to the WMT2020 APE Shared Task : Automatic Post-Editing with Cross-lingual Language Model<fixed-case>POSTECH</fixed-case>-<fixed-case>ETRI</fixed-case>’s Submission to the <fixed-case>WMT</fixed-case>2020 <fixed-case>APE</fixed-case> Shared Task: Automatic Post-Editing with Cross-lingual Language Model</title>
      <author><first>Jihyung</first><last>Lee</last></author>
      <author><first>WonKee</first><last>Lee</last></author>
      <author><first>Jaehun</first><last>Shin</last></author>
      <author><first>Baikjin</first><last>Jung</last></author>
      <author><first>Young-Kil</first><last>Kim</last></author>
      <author><first>Jong-Hyeok</first><last>Lee</last></author>
      <pages>777–782</pages>
      <abstract>This paper describes POSTECH-ETRI’s submission to WMT2020 for the shared task on automatic post-editing (APE) for 2 language pairs : English-German (En-De) and English-Chinese (En-Zh). We propose APE systems based on a cross-lingual language model, which jointly adopts translation language modeling (TLM) and masked language modeling (MLM) training objectives in the pre-training stage ; the APE models then utilize jointly learned language representations between the source language and the target language. In addition, we created 19 million new sythetic triplets as additional training data for our final ensemble model. According to experimental results on the WMT2020 APE development data set, our models showed an improvement over the baseline by <a href="https://en.wikipedia.org/wiki/Terminology">TER</a> of -3.58 and a BLEU score of +5.3 for the <a href="https://en.wikipedia.org/wiki/Terminology">En-De subtask</a> ; and <a href="https://en.wikipedia.org/wiki/Terminology">TER</a> of -5.29 and a BLEU score of +7.32 for the <a href="https://en.wikipedia.org/wiki/Terminology">En-Zh subtask</a>.</abstract>
      <url hash="f1249c8d">2020.wmt-1.82</url>
      <video href="https://slideslive.com/38939561" />
      <bibkey>lee-etal-2020-postech</bibkey>
    </paper>
    <paper id="84">
      <title>Alibaba’s Submission for the WMT 2020 APE Shared Task : Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual BERT<fixed-case>A</fixed-case>libaba’s Submission for the <fixed-case>WMT</fixed-case> 2020 <fixed-case>APE</fixed-case> Shared Task: Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual <fixed-case>BERT</fixed-case></title>
      <author><first>Jiayi</first><last>Wang</last></author>
      <author><first>Ke</first><last>Wang</last></author>
      <author><first>Kai</first><last>Fan</last></author>
      <author><first>Yuqi</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Lu</last></author>
      <author><first>Xin</first><last>Ge</last></author>
      <author><first>Yangbin</first><last>Shi</last></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <pages>789–796</pages>
      <abstract>The goal of Automatic Post-Editing (APE) is basically to examine the automatic methods for correcting translation errors generated by an unknown machine translation (MT) system. This paper describes Alibaba’s submissions to the WMT 2020 APE Shared Task for the English-German language pair. We design a two-stage training pipeline. First, a BERT-like cross-lingual language model is pre-trained by randomly masking target sentences alone. Then, an additional neural decoder on the top of the pre-trained model is jointly fine-tuned for the APE task. We also apply an imitation learning strategy to augment a reasonable amount of pseudo APE training data, potentially preventing the model to overfit on the limited real training data and boosting the performance on held-out data. To verify our proposed model and <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, we examine our approach with the well-known benchmarking English-German dataset from the WMT 2017 APE task. The experiment results demonstrate that our <a href="https://en.wikipedia.org/wiki/System">system</a> significantly outperforms all other <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> and achieves the state-of-the-art performance. The final results on the WMT 2020 test dataset show that our <a href="https://en.wikipedia.org/wiki/Subscription_business_model">submission</a> can achieve +5.56 <a href="https://en.wikipedia.org/wiki/British_thermal_unit">BLEU</a> and -4.57 TER with respect to the official MT baseline.</abstract>
      <url hash="42810c42">2020.wmt-1.84</url>
      <video href="https://slideslive.com/38939622" />
      <bibkey>wang-etal-2020-alibabas</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="86">
      <title>LIMSI @ WMT 2020<fixed-case>LIMSI</fixed-case> @ <fixed-case>WMT</fixed-case> 2020</title>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>José Carlos</first><last>Rosales Núñez</last></author>
      <author><first>Minh Quang</first><last>Pham</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>803–812</pages>
      <abstract>This paper describes LIMSI’s submissions to the translation shared tasks at WMT’20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from <a href="https://en.wikipedia.org/wiki/English_language">English</a> into <a href="https://en.wikipedia.org/wiki/French_language">French</a>, using back-translated texts, terminological resources as well as multiple pre-processing pipelines, including pre-trained representations. Systems were also prepared for the robustness task for translating from <a href="https://en.wikipedia.org/wiki/English_language">English</a> into <a href="https://en.wikipedia.org/wiki/German_language">German</a> ; for this large-scale task we developed multi-domain, noise-robust, translation systems aim to handle the two test conditions : zero-shot and few-shot domain adaptation.</abstract>
      <url hash="02a6bd30">2020.wmt-1.86</url>
      <video href="https://slideslive.com/38939618" />
      <bibkey>abdul-rauf-etal-2020-limsi</bibkey>
    </paper>
    <paper id="87">
      <title>Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation</title>
      <author><first>Ander</first><last>Corral</last></author>
      <author><first>Xabier</first><last>Saralegi</last></author>
      <pages>813–819</pages>
      <abstract>This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and English-Spanish. In all cases a Transformer architecture was chosen and we studied different strategies to combine <a href="https://en.wikipedia.org/wiki/Open_data">open domain data</a> with biomedical domain data for building the training corpora. For the English-Basque pair, given the scarcity of parallel corpora in the biomedical domain, we set out to create domain training data in a synthetic way. The <a href="https://en.wikipedia.org/wiki/System">systems</a> presented in the terminology and abstract translation subtasks for the English-Basque language pair ranked first in their respective tasks among four participants, achieving 0.78 accuracy for terminology translation and a <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> of 0.1279 for the translation of abstracts. In the abstract translation task for the English-Spanish pair our team ranked second (BLEU=0.4498) in the case of OK sentences.</abstract>
      <url hash="1120ae61">2020.wmt-1.87</url>
      <video href="https://slideslive.com/38939591" />
      <bibkey>corral-saralegi-2020-elhuyar</bibkey>
    </paper>
    <paper id="88">
      <title>YerevaNN’s Systems for WMT20 Biomedical Translation Task : The Effect of Fixing Misaligned Sentence Pairs<fixed-case>Y</fixed-case>ereva<fixed-case>NN</fixed-case>’s Systems for <fixed-case>WMT</fixed-case>20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs</title>
      <author><first>Karen</first><last>Hambardzumyan</last></author>
      <author><first>Hovhannes</first><last>Tamoyan</last></author>
      <author><first>Hrant</first><last>Khachatrian</last></author>
      <pages>820–825</pages>
      <abstract>This report describes YerevaNN’s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide <a href="https://en.wikipedia.org/wiki/Language_planning">systems</a> for English-Russian and English-German language pairs. For the English-Russian pair, our submissions achieve the best BLEU scores, with enru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.<tex-math>\rightarrow</tex-math>ru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.</abstract>
      <url hash="76c0c7c9">2020.wmt-1.88</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c2ea9401">2020.wmt-1.88.OptionalSupplementaryMaterial.tgz</attachment>
      <video href="https://slideslive.com/38939644" />
      <bibkey>hambardzumyan-etal-2020-yerevanns</bibkey>
    </paper>
    <paper id="89">
      <title>Pretrained Language Models and Backtranslation for English-Basque Biomedical Neural Machine Translation<fixed-case>E</fixed-case>nglish-<fixed-case>B</fixed-case>asque Biomedical Neural Machine Translation</title>
      <author><first>Inigo</first><last>Jauregi Unanue</last></author>
      <author><first>Massimo</first><last>Piccardi</last></author>
      <pages>826–832</pages>
      <abstract>This paper describes the machine translation systems proposed by the University of Technology Sydney Natural Language Processing (UTS_NLP) team for the WMT20 English-Basque biomedical translation tasks. Due to the limited parallel corpora available, we have proposed to train a BERT-fused NMT model that leverages the use of pretrained language models. Furthermore, we have augmented the training corpus by backtranslating monolingual data. Our experiments show that NMT models in low-resource scenarios can benefit from combining these two <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training techniques</a>, with improvements of up to 6.16 BLEU percentual points in the case of biomedical abstract translations.</abstract>
      <url hash="85596cf6">2020.wmt-1.89</url>
      <video href="https://slideslive.com/38939562" />
      <bibkey>jauregi-unanue-piccardi-2020-pretrained</bibkey>
    </paper>
    <paper id="90">
      <title>Lite Training Strategies for Portuguese-English and English-Portuguese Translation<fixed-case>P</fixed-case>ortuguese-<fixed-case>E</fixed-case>nglish and <fixed-case>E</fixed-case>nglish-<fixed-case>P</fixed-case>ortuguese Translation</title>
      <author><first>Alexandre</first><last>Lopes</last></author>
      <author><first>Rodrigo</first><last>Nogueira</last></author>
      <author><first>Roberto</first><last>Lotufo</last></author>
      <author><first>Helio</first><last>Pedrini</last></author>
      <pages>833–840</pages>
      <abstract>Despite the widespread adoption of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Portuguese translation tasks using low-cost hardware. We explore the use of Portuguese and English pre-trained language models and propose an adaptation of the English tokenizer to represent Portuguese characters, such as <a href="https://en.wikipedia.org/wiki/Diaeresis_(diacritic)">diaeresis</a>, acute and grave accents. We compare our models to the Google Translate API and MarianMT on a subset of the ParaCrawl dataset, as well as to the winning submission to the WMT19 Biomedical Translation Shared Task. We also describe our submission to the WMT20 Biomedical Translation Shared Task. Our results show that our <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a> have a competitive performance to state-of-the-art <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a> while being trained on modest hardware (a single 8 GB gaming GPU for nine days). Our <a href="https://en.wikipedia.org/wiki/Data">data</a>, models and code are available in our GitHub repository.</abstract>
      <url hash="6264f17a">2020.wmt-1.90</url>
      <video href="https://slideslive.com/38939645" />
      <bibkey>lopes-etal-2020-lite</bibkey>
      <pwccode url="https://github.com/unicamp-dl/Lite-T5-Translation" additional="false">unicamp-dl/Lite-T5-Translation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/capes">capes</pwcdataset>
    </paper>
    <paper id="94">
      <title>Addressing Exposure Bias With Document Minimum Risk Training : Cambridge at the WMT20 Biomedical Translation Task<fixed-case>C</fixed-case>ambridge at the <fixed-case>WMT</fixed-case>20 Biomedical Translation Task</title>
      <author><first>Danielle</first><last>Saunders</last></author>
      <author><first>Bill</first><last>Byrne</last></author>
      <pages>862–869</pages>
      <abstract>The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such <a href="https://en.wikipedia.org/wiki/Data">data</a> are susceptible to <a href="https://en.wikipedia.org/wiki/Exposure_bias">exposure bias effects</a>, particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during <a href="https://en.wikipedia.org/wiki/Inference">inference</a> if the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> learns to neglect the source sentence. The UNICAM entry addresses this problem during <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove ‘problem’ training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result, despite using only single models with no ensembling.</abstract>
      <url hash="dcb9d8e6">2020.wmt-1.94</url>
      <video href="https://slideslive.com/38939583" />
      <bibkey>saunders-byrne-2020-addressing</bibkey>
    </paper>
    <paper id="101">
      <title>Unbabel’s Participation in the WMT20 Metrics Shared Task<fixed-case>WMT</fixed-case>20 Metrics Shared Task</title>
      <author><first>Ricardo</first><last>Rei</last></author>
      <author><first>Craig</first><last>Stewart</last></author>
      <author><first>Ana C</first><last>Farinha</last></author>
      <author><first>Alon</first><last>Lavie</last></author>
      <pages>911–920</pages>
      <abstract>We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segmentlevel, document-level and system-level tracks on all language pairs, as well as the QE as a Metric track. Accordingly, we illustrate results of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> in these <a href="https://en.wikipedia.org/wiki/Track_(rail_transport)">tracks</a> with reference to test sets from the previous year. Our submissions build upon the recently proposed COMET framework : we train several estimator models to regress on different humangenerated quality scores and a novel ranking model trained on relative ranks obtained from Direct Assessments. We also propose a simple technique for converting segment-level predictions into a document-level score. Overall, our <a href="https://en.wikipedia.org/wiki/System">systems</a> achieve strong results for all language pairs on previous test sets and in many cases set a new <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>.</abstract>
      <url hash="362e34a3">2020.wmt-1.101</url>
      <attachment type="OptionalSupplementaryMaterial" hash="828c2126">2020.wmt-1.101.OptionalSupplementaryMaterial.pdf</attachment>
      <video href="https://slideslive.com/38939604" />
      <bibkey>rei-etal-2020-unbabels</bibkey>
    </paper>
    <paper id="104">
      <title>Incorporate Semantic Structures into Machine Translation Evaluation via <a href="https://en.wikipedia.org/wiki/UCCA">UCCA</a><fixed-case>UCCA</fixed-case></title>
      <author><first>Jin</first><last>Xu</last></author>
      <author><first>Yinuo</first><last>Guo</last></author>
      <author><first>Junfeng</first><last>Hu</last></author>
      <pages>934–939</pages>
      <abstract>Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, we notice that there are certain words or phrases appearing in all good translations of one source text, and these words tend to convey important semantic information. Therefore, in this work, we define words carrying important semantic meanings in sentences as semantic core words. Moreover, we propose an MT evaluation approach named Semantically Weighted Sentence Similarity (SWSS). It leverages the power of UCCA to identify semantic core words, and then calculates sentence similarity scores on the overlap of semantic core words. Experimental results show that SWSS can consistently improve the performance of popular MT evaluation metrics which are based on <a href="https://en.wikipedia.org/wiki/Lexical_similarity">lexical similarity</a>.</abstract>
      <url hash="23fd41d7">2020.wmt-1.104</url>
      <video href="https://slideslive.com/38939565" />
      <bibkey>xu-etal-2020-incorporate</bibkey>
    </paper>
    <paper id="105">
      <title>Filtering Noisy Parallel Corpus using Transformers with Proxy Task Learning</title>
      <author><first>Haluk</first><last>Açarçiçek</last></author>
      <author><first>Talha</first><last>Çolakoğlu</last></author>
      <author><first>Pınar Ece</first><last>Aktan Hatipoğlu</last></author>
      <author><first>Chong Hsuan</first><last>Huang</last></author>
      <author><first>Wei</first><last>Peng</last></author>
      <pages>940–946</pages>
      <abstract>This paper illustrates Huawei’s submission to the WMT20 low-resource parallel corpus filtering shared task. Our approach focuses on developing a proxy task learner on top of a transformer-based multilingual pre-trained language model to boost the filtering capability for noisy parallel corpora. Such a supervised task also helps us to iterate much more quickly than using an existing <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation system</a> to perform the same <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>. After performing empirical analyses of the finetuning task, we benchmark our approach by comparing the results with past years’ state-of-theart records. This paper wraps up with a discussion of limitations and future work. The scripts for this study will be made publicly available.</abstract>
      <url hash="5dc0edc3">2020.wmt-1.105</url>
      <video href="https://slideslive.com/38939606" />
      <bibkey>acarcicek-etal-2020-filtering</bibkey>
      <pwccode url="https://github.com/wpti/proxy-filter" additional="false">wpti/proxy-filter</pwccode>
    </paper>
    <paper id="106">
      <title>Score Combination for Improved Parallel Corpus Filtering for Low Resource Conditions</title>
      <author><first>Muhammad</first><last>ElNokrashy</last></author>
      <author><first>Amr</first><last>Hendy</last></author>
      <author><first>Mohamed</first><last>Abdelghaffar</last></author>
      <author><first>Mohamed</first><last>Afify</last></author>
      <author><first>Ahmed</first><last>Tawfik</last></author>
      <author><first>Hany</first><last>Hassan Awadalla</last></author>
      <pages>947–951</pages>
      <abstract>This paper presents the description of our submission to WMT20 sentence filtering task. We combine scores from custom LASER built for each source language, a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> built to distinguish positive and negative pairs and the original scores provided with the task. For the mBART setup, provided by the organizers, our method shows 7 % and 5 % relative improvement, over the baseline, in sacreBLEU score on the test set for <a href="https://en.wikipedia.org/wiki/Pashto">Pashto</a> and <a href="https://en.wikipedia.org/wiki/Khmer_language">Khmer</a> respectively.</abstract>
      <url hash="f7f93dee">2020.wmt-1.106</url>
      <video href="https://slideslive.com/38939612" />
      <bibkey>elnokrashy-etal-2020-score</bibkey>
    </paper>
    <paper id="108">
      <title>An exploratory approach to the Parallel Corpus Filtering shared task WMT20<fixed-case>WMT</fixed-case>20</title>
      <author><first>Ankur</first><last>Kejriwal</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>959–965</pages>
      <abstract>In this document we describe our submission to the parallel corpus filtering task using multilingual word embedding, <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> and an ensemble of pre and post filtering rules. We use the norms of embedding and the perplexities of language models along with pre / post filtering rules to complement the LASER baseline scores and in the end get an improvement on the dev set in both language pairs.</abstract>
      <url hash="86dd458a">2020.wmt-1.108</url>
      <video href="https://slideslive.com/38939649" />
      <bibkey>kejriwal-koehn-2020-exploratory</bibkey>
    </paper>
    <paper id="113">
      <title>PATQUEST : Papago Translation Quality Estimation<fixed-case>PATQUEST</fixed-case>: Papago Translation Quality Estimation</title>
      <author><first>Yujin</first><last>Baek</last></author>
      <author><first>Zae Myung</first><last>Kim</last></author>
      <author><first>Jihyung</first><last>Moon</last></author>
      <author><first>Hyunjoong</first><last>Kim</last></author>
      <author><first>Eunjeong</first><last>Park</last></author>
      <pages>991–998</pages>
      <abstract>This paper describes the <a href="https://en.wikipedia.org/wiki/System">system</a> submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation : (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of <a href="https://en.wikipedia.org/wiki/Errors_and_residuals">errors</a> that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a>, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment ; EN-DE only), and Task 3 (Document-Level Score).</abstract>
      <url hash="ca5662d5">2020.wmt-1.113</url>
      <video href="https://slideslive.com/38939610" />
      <bibkey>baek-etal-2020-patquest</bibkey>
    </paper>
    <paper id="118">
      <title>Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation</title>
      <author><first>Dongjun</first><last>Lee</last></author>
      <pages>1024–1028</pages>
      <abstract>In this paper, we describe the Bering Lab’s submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language model, with a few additional parameters. Model training consists of two phases. We first pre-train our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> on a huge artificially generated QE dataset, and then we fine-tune the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> with a human-labeled dataset. When evaluated on the WMT 2020 English-German QE test set, our systems achieve the best result on the target-side of word-level QE and the second best results on the source-side of word-level QE and sentence-level QE among all submissions.</abstract>
      <url hash="4beb1299">2020.wmt-1.118</url>
      <video href="https://slideslive.com/38939546" />
      <bibkey>lee-2020-two</bibkey>
    </paper>
    <paper id="119">
      <title>IST-Unbabel Participation in the WMT20 Quality Estimation Shared Task<fixed-case>IST</fixed-case>-Unbabel Participation in the <fixed-case>WMT</fixed-case>20 Quality Estimation Shared Task</title>
      <author><first>João</first><last>Moura</last></author>
      <author><first>Miguel</first><last>Vera</last></author>
      <author><first>Daan</first><last>van Stigt</last></author>
      <author><first>Fabio</first><last>Kepler</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>1029–1036</pages>
      <abstract>We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitted systems were developed by extending the OpenKiwi framework to a transformer-based predictor-estimator architecture, and to cope with glass-box, uncertainty-based features coming from neural machine translation systems.</abstract>
      <url hash="1dfb6e6c">2020.wmt-1.119</url>
      <video href="https://slideslive.com/38939643" />
      <bibkey>moura-etal-2020-ist</bibkey>
    </paper>
    <paper id="122">
      <title>TransQuest at WMT2020 : Sentence-Level Direct Assessment<fixed-case>T</fixed-case>rans<fixed-case>Q</fixed-case>uest at <fixed-case>WMT</fixed-case>2020: Sentence-Level Direct Assessment</title>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>1049–1055</pages>
      <abstract>This paper presents the team TransQuest’s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. The proposed methods achieve state-of-the-art results surpassing the results obtained by OpenKiwi, the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> used in the shared task. We further fine tune the QE framework by performing ensemble and <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>. Our approach is the winning solution in all of the language pairs according to the WMT 2020 official results.</abstract>
      <url hash="32d15a4f">2020.wmt-1.122</url>
      <video href="https://slideslive.com/38939607" />
      <bibkey>ranasinghe-etal-2020-transquest-wmt2020</bibkey>
      <pwccode url="https://github.com/tharindudr/transQuest" additional="false">tharindudr/transQuest</pwccode>
    </paper>
    <paper id="124">
      <title>Tencent submission for WMT20 Quality Estimation Shared Task<fixed-case>WMT</fixed-case>20 Quality Estimation Shared Task</title>
      <author><first>Haijiang</first><last>Wu</last></author>
      <author><first>Zixuan</first><last>Wang</last></author>
      <author><first>Qingsong</first><last>Ma</last></author>
      <author><first>Xinjie</first><last>Wen</last></author>
      <author><first>Ruichen</first><last>Wang</last></author>
      <author><first>Xiaoli</first><last>Wang</last></author>
      <author><first>Yulin</first><last>Zhang</last></author>
      <author><first>Zhipeng</first><last>Yao</last></author>
      <author><first>Siyao</first><last>Peng</last></author>
      <pages>1062–1067</pages>
      <abstract>This paper presents Tencent’s submission to the WMT20 Quality Estimation (QE) Shared Task : Sentence-Level Post-editing Effort for English-Chinese in Task 2. Our system ensembles two <a href="https://en.wikipedia.org/wiki/Computer_architecture">architectures</a>, XLM-based and Transformer-based Predictor-Estimator models. For the XLM-based Predictor-Estimator architecture, the predictor produces two types of contextualized token representations, i.e., masked XLM and non-masked XLM ; the LSTM-estimator and Transformer-estimator employ two effective strategies, top-K and multi-head attention, to enhance the sentence feature representation. For Transformer-based Predictor-Estimator architecture, we improve a top-performing model by conducting three modifications : using multi-decoding in machine translation module, creating a new model by replacing the transformer-based predictor with XLM-based predictor, and finally integrating two models by a weighted average. Our submission achieves a <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation</a> of 0.664, ranking first (tied) on <a href="https://en.wikipedia.org/wiki/Standard_Chinese">English-Chinese</a>.</abstract>
      <url hash="193481f0">2020.wmt-1.124</url>
      <video href="https://slideslive.com/38939609" />
      <bibkey>wu-etal-2020-tencent-submission</bibkey>
    </paper>
    <paper id="126">
      <title>NLPRL System for Very Low Resource Supervised Machine Translation<fixed-case>NLPRL</fixed-case> System for Very Low Resource Supervised Machine Translation</title>
      <author><first>Rupjyoti</first><last>Baruah</last></author>
      <author><first>Rajesh Kumar</first><last>Mundotiya</last></author>
      <author><first>Amit</first><last>Kumar</last></author>
      <author><first>Anil kumar</first><last>Singh</last></author>
      <pages>1075–1078</pages>
      <abstract>This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try to address the Out of Vocabulary (OOV) word problem by performing <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a> so that segments correspond to morphological units. They are also reported to work across different languages, especially similar languages due to their sub-word nature. Based on BLEU cased score, our NLPRL systems ranked ninth for HSB to GER and tenth in GER to HSB translation scenario.</abstract>
      <url hash="671b4450">2020.wmt-1.126</url>
      <video href="https://slideslive.com/38939625" />
      <bibkey>baruah-etal-2020-nlprl</bibkey>
    </paper>
    <paper id="129">
      <title>UdS-DFKI@WMT20 : Unsupervised MT and Very Low Resource Supervised MT for German-Upper Sorbian<fixed-case>U</fixed-case>d<fixed-case>S</fixed-case>-<fixed-case>DFKI</fixed-case>@<fixed-case>WMT</fixed-case>20: Unsupervised <fixed-case>MT</fixed-case> and Very Low Resource Supervised <fixed-case>MT</fixed-case> for <fixed-case>G</fixed-case>erman-<fixed-case>U</fixed-case>pper <fixed-case>S</fixed-case>orbian</title>
      <author><first>Sourav</first><last>Dutta</last></author>
      <author><first>Jesujoba</first><last>Alabi</last></author>
      <author><first>Saptarashmi</first><last>Bandyopadhyay</last></author>
      <author><first>Dana</first><last>Ruiter</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>1092–1098</pages>
      <abstract>This paper describes the UdS-DFKI submission to the shared task for unsupervised machine translation (MT) and very low-resource supervised MT between German (de) and Upper Sorbian (hsb) at the Fifth Conference of Machine Translation (WMT20). We submit <a href="https://en.wikipedia.org/wiki/System">systems</a> for both the supervised and unsupervised tracks. Apart from various experimental approaches like bitext mining, model pre-training, and iterative back-translation, we employ a factored machine translation approach on a small BPE vocabulary.</abstract>
      <url hash="70031bea">2020.wmt-1.129</url>
      <video href="https://slideslive.com/38939584" />
      <bibkey>dutta-etal-2020-uds</bibkey>
    </paper>
    <paper id="133">
      <title>CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20<fixed-case>CUNI</fixed-case> Systems for the Unsupervised and Very Low Resource Translation Task in <fixed-case>WMT</fixed-case>20</title>
      <author><first>Ivana</first><last>Kvapilíková</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>1123–1128</pages>
      <abstract>This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between <a href="https://en.wikipedia.org/wiki/German_language">German</a> and Upper Sorbian. We experimented with training on <a href="https://en.wikipedia.org/wiki/Synthetic_data">synthetic data</a> and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.</abstract>
      <url hash="a9c59155">2020.wmt-1.133</url>
      <video href="https://slideslive.com/38939641" />
      <bibkey>kvapilikova-etal-2020-cuni</bibkey>
    </paper>
    <paper id="134">
      <title>The University of Helsinki and Aalto University submissions to the WMT 2020 news and low-resource translation tasks<fixed-case>U</fixed-case>niversity of <fixed-case>H</fixed-case>elsinki and Aalto University submissions to the <fixed-case>WMT</fixed-case> 2020 news and low-resource translation tasks</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Stig-Arne</first><last>Grönroos</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <pages>1129–1138</pages>
      <abstract>This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020 : the news translation between Inuktitut and <a href="https://en.wikipedia.org/wiki/English_language">English</a> and the low-resource translation between <a href="https://en.wikipedia.org/wiki/German_language">German</a> and Upper Sorbian. For both tasks, our efforts concentrate on efficient use of monolingual and related bilingual corpora with scheduled multi-task learning as well as an optimized subword segmentation with <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">sampling</a>. Our submission obtained the highest score for <a href="https://en.wikipedia.org/wiki/Upper_Sorbian">Upper Sorbian-German</a> and was ranked second for German-Upper Sorbian according to <a href="https://en.wikipedia.org/wiki/BLEU">BLEU scores</a>. For EnglishInuktitut, we reached ranks 8 and 10 out of 11 according to BLEU scores.</abstract>
      <url hash="eb48442e">2020.wmt-1.134</url>
      <video href="https://slideslive.com/38939589" />
      <bibkey>scherrer-etal-2020-university</bibkey>
    </paper>
    <paper id="135">
      <title>The NITS-CNLP System for the Unsupervised MT Task at WMT 2020<fixed-case>NITS</fixed-case>-<fixed-case>CNLP</fixed-case> System for the Unsupervised <fixed-case>MT</fixed-case> Task at <fixed-case>WMT</fixed-case> 2020</title>
      <author><first>Salam Michael</first><last>Singh</last></author>
      <author><first>Thoudam Doren</first><last>Singh</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>1139–1143</pages>
      <abstract>We describe NITS-CNLP’s submission to WMT 2020 unsupervised machine translation shared task for German language (de) to Upper Sorbian (hsb) in a constrained setting i.e, using only the data provided by the organizers. We train our <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised model</a> using monolingual data from both the languages by jointly pre-training the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and <a href="https://en.wikipedia.org/wiki/Code">decoder</a> and fine-tune using backtranslation loss. The final model uses the source side (de) monolingual data and the target side (hsb) synthetic data as a pseudo-parallel data to train a pseudo-supervised system which is tuned using the provided development set(dev set).</abstract>
      <url hash="64ef4f2f">2020.wmt-1.135</url>
      <video href="https://slideslive.com/38939575" />
      <bibkey>singh-etal-2020-nits</bibkey>
    </paper>
    <paper id="136">
      <title>Adobe AMPS’s Submission for Very Low Resource Supervised Translation Task at WMT20<fixed-case>AMPS</fixed-case>’s Submission for Very Low Resource Supervised Translation Task at <fixed-case>WMT</fixed-case>20</title>
      <author><first>Keshaw</first><last>Singh</last></author>
      <pages>1144–1149</pages>
      <abstract>In this paper, we describe our <a href="https://en.wikipedia.org/wiki/System">systems</a> submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transformer-based neural machine translation model trained on original training bitext. We also conduct several experiments with backtranslation using limited monolingual data in our post-submission work and include our results for the same. In one such experiment, we observe jumps of up to 2.6 BLEU points over the primary system by <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">pretraining</a> on a synthetic, backtranslated corpus followed by <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> on the original parallel training data.</abstract>
      <url hash="7d02bb70">2020.wmt-1.136</url>
      <video href="https://slideslive.com/38939621" />
      <bibkey>singh-2020-adobe</bibkey>
    </paper>
    <paper id="140">
      <title>Human-Paraphrased References Improve Neural Machine Translation</title>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>George</first><last>Foster</last></author>
      <author><first>David</first><last>Grangier</last></author>
      <author><first>Colin</first><last>Cherry</last></author>
      <pages>1183–1192</pages>
      <abstract>Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by freitag2020bleu. When used in place of original references, the paraphrased versions produce <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric scores</a> that correlate better with <a href="https://en.wikipedia.org/wiki/Judgement">human judgment</a>. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal (translationese) ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is ignificantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.</abstract>
      <url hash="3dccb6eb">2020.wmt-1.140</url>
      <video href="https://slideslive.com/38939593" />
      <bibkey>freitag-etal-2020-human</bibkey>
    </paper>
    <paper id="141">
      <title>Incorporating Terminology Constraints in Automatic Post-Editing</title>
      <author><first>David</first><last>Wan</last></author>
      <author><first>Chris</first><last>Kedzie</last></author>
      <author><first>Faisal</first><last>Ladhak</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>1193–1204</pages>
      <abstract>Users of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation (MT)</a> may want to ensure the use of specific <a href="https://en.wikipedia.org/wiki/Terminology">lexical terminologies</a>. While there exist techniques for incorporating terminology constraints during <a href="https://en.wikipedia.org/wiki/Inference">inference</a> for MT, current APE approaches can not ensure that they will appear in the final translation. In this paper, we present both autoregressive and non-autoregressive models for lexically constrained APE, demonstrating that our approach enables preservation of 95 % of the terminologies and also improves translation quality on English-German benchmarks. Even when applied to lexically constrained MT output, our approach is able to improve preservation of the terminologies. However, we show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> do not learn to copy constraints systematically and suggest a simple data augmentation technique that leads to improved performance and <a href="https://en.wikipedia.org/wiki/Robust_statistics">robustness</a>.</abstract>
      <url hash="e3b0f9c7">2020.wmt-1.141</url>
      <video href="https://slideslive.com/38939650" />
      <bibkey>wan-etal-2020-incorporating</bibkey>
      <pwccode url="https://github.com/zerocstaker/constrained_ape" additional="false">zerocstaker/constrained_ape</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
  </volume>
</collection>