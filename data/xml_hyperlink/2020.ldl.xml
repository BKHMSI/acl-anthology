<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.ldl">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 7th Workshop on Linked Data in Linguistics (LDL-2020)</booktitle>
      <editor><first>Maxim</first><last>Ionov</last></editor>
      <editor><first>John P.</first><last>McCrae</last></editor>
      <editor><first>Christian</first><last>Chiarcos</last></editor>
      <editor><first>Thierry</first><last>Declerck</last></editor>
      <editor><first>Julia</first><last>Bosque-Gil</last></editor>
      <editor><first>Jorge</first><last>Gracia</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-36-8</isbn>
    </meta>
    <frontmatter>
      <url hash="f15e978d">2020.ldl-1.0</url>
      <bibkey>ldl-2020-linked</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Transforming the Cologne Digital Sanskrit Dictionaries into OntoLex-Lemon<fixed-case>S</fixed-case>anskrit Dictionaries into <fixed-case>O</fixed-case>nto<fixed-case>L</fixed-case>ex-Lemon</title>
      <author><first>Francisco</first><last>Mondaca</last></author>
      <author><first>Felix</first><last>Rau</last></author>
      <pages>11–14</pages>
      <abstract>The Cologne Digital Sanskrit Dictionaries (CDSD) is a large collection of complex digitized Sanskrit dictionaries, consisting of over thirty-five works, and is the most prominent collection of Sanskrit dictionaries worldwide. In this paper we evaluate two methods for transforming the <a href="https://en.wikipedia.org/wiki/Conformational_isomerism">CDSD</a> into Ontolex-Lemon based on a modelling exercise. The first method that we evaluate consists of applying <a href="https://en.wikipedia.org/wiki/RDFa">RDFa</a> to the existent TEI-P5 files. The second method consists of transforming the TEI-encoded dictionaries into new files containing RDF triples modelled in OntoLex-Lemon. As a result of the modelling exercise we choose the second method : to transform TEI-encoded lexical data into Ontolex-Lemon by creating new files containing exclusively RDF triples.</abstract>
      <url hash="7bb4408c">2020.ldl-1.2</url>
      <language>eng</language>
      <bibkey>mondaca-rau-2020-transforming</bibkey>
    </paper>
    <paper id="7">
      <title>Challenges of Word Sense Alignment : Portuguese Language Resources<fixed-case>P</fixed-case>ortuguese Language Resources</title>
      <author><first>Ana</first><last>Salgado</last></author>
      <author><first>Sina</first><last>Ahmadi</last></author>
      <author><first>Alberto</first><last>Simões</last></author>
      <author><first>John Philip</first><last>McCrae</last></author>
      <author><first>Rute</first><last>Costa</last></author>
      <pages>45–51</pages>
      <abstract>This paper reports on an ongoing task of monolingual word sense alignment in which a comparative study between the Portuguese Academy of Sciences Dictionary and the Dicionrio Aberto is carried out in the context of the ELEXIS (European Lexicographic Infrastructure) project. Word sense alignment involves searching for matching senses within dictionary entries of different lexical resources and linking them, which poses significant challenges. The lexicographic criteria are not always entirely consistent within individual dictionaries and even less so across different projects where different options may have been assumed in terms of structure and especially wording techniques of <a href="https://en.wikipedia.org/wiki/Gloss_(annotation)">lexicographic glosses</a>. This hinders the task of <a href="https://en.wikipedia.org/wiki/Sensory_nervous_system">matching senses</a>. We aim to present our annotation workflow in <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese</a> using the <a href="https://en.wikipedia.org/wiki/Semantic_Web">Semantic Web technologies</a>. The results obtained are useful for the discussion within the community.</abstract>
      <url hash="71ecc019">2020.ldl-1.7</url>
      <language>eng</language>
      <bibkey>salgado-etal-2020-challenges</bibkey>
    </paper>
    <paper id="12">
      <title>Lexemes in <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a> : 2020 status<fixed-case>W</fixed-case>ikidata: 2020 status</title>
      <author><first>Finn</first><last>Nielsen</last></author>
      <pages>82–86</pages>
      <abstract>Wikidata now records data about <a href="https://en.wikipedia.org/wiki/Lexeme">lexemes</a>, senses and lexical forms and exposes them as <a href="https://en.wikipedia.org/wiki/Linguistic_Linked_Open_Data">Linguistic Linked Open Data</a>. Since <a href="https://en.wikipedia.org/wiki/Lexeme">lexemes</a> in <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a> was first established in 2018, this <a href="https://en.wikipedia.org/wiki/Data">data</a> has grown considerable in size. Links between <a href="https://en.wikipedia.org/wiki/Lexeme">lexemes</a> in different languages can be made, e.g., through a <a href="https://en.wikipedia.org/wiki/Morphological_derivation">derivation property</a> or <a href="https://en.wikipedia.org/wiki/Word_sense">senses</a>. We present some descriptive statistics about the lexemes of <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a>, focusing on the multilingual aspects and show that there are still relatively few multilingual links.</abstract>
      <url hash="0d0fdd66">2020.ldl-1.12</url>
      <language>eng</language>
      <bibkey>nielsen-2020-lexemes</bibkey>
    </paper>
  </volume>
</collection>