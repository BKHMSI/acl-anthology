<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.nlp4convai">
  <volume id="1" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</booktitle>
      <editor><first>Alexandros</first><last>Papangelis</last></editor>
      <editor><first>Paweł</first><last>Budzianowski</last></editor>
      <editor><first>Bing</first><last>Liu</last></editor>
      <editor><first>Elnaz</first><last>Nouri</last></editor>
      <editor><first>Abhinav</first><last>Rastogi</last></editor>
      <editor><first>Yun-Nung</first><last>Chen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="e583b414">2021.nlp4convai-1.0</url>
      <bibkey>nlp4convai-2021-natural</bibkey>
    </frontmatter>
    <paper id="6">
      <title>Not So Fast, Classifier   Accuracy and Entropy Reduction in Incremental Intent Classification</title>
      <author><first>Lianna</first><last>Hrycyk</last></author>
      <author><first>Alessandra</first><last>Zarcone</last></author>
      <author><first>Luzian</first><last>Hahn</last></author>
      <pages>52–67</pages>
      <abstract>Incremental intent classification requires the assignment of intent labels to partial utterances. However, partial utterances do not necessarily contain enough information to be mapped to the intent class of their complete utterance (correctly and with a certain degree of confidence). Using the final interpretation as the ground truth to measure a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier’s accuracy</a> during intent classification of partial utterances is thus problematic. We release inCLINC, a dataset of partial and full utterances with human annotations of plausible intent labels for different portions of each utterance, as an upper (human) baseline for incremental intent classification. We analyse the incremental annotations and propose entropy reduction as a measure of human annotators’ convergence on an interpretation (i.e. intent label). We argue that, when the annotators do not converge to one or a few possible interpretations and yet the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> already identifies the final intent class early on, it is a sign of <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> that can be ascribed to artefacts in the dataset.</abstract>
      <url hash="f0275347">2021.nlp4convai-1.6</url>
      <bibkey>hrycyk-etal-2021-fast</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.6</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
    </paper>
    <paper id="8">
      <title>Amendable Generation for Dialogue State Tracking</title>
      <author><first>Xin</first><last>Tian</last></author>
      <author><first>Liankai</first><last>Huang</last></author>
      <author><first>Yingzhan</first><last>Lin</last></author>
      <author><first>Siqi</first><last>Bao</last></author>
      <author><first>Huang</first><last>He</last></author>
      <author><first>Yunyi</first><last>Yang</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Fan</first><last>Wang</last></author>
      <author><first>Shuqi</first><last>Sun</last></author>
      <pages>80–92</pages>
      <abstract>In task-oriented dialogue systems, recent dialogue state tracking methods tend to perform one-pass generation of the dialogue state based on the previous dialogue state. The mistakes of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> made at the current turn are prone to be carried over to the next turn, causing <a href="https://en.wikipedia.org/wiki/Error_propagation">error propagation</a>. In this paper, we propose a novel Amendable Generation for Dialogue State Tracking (AG-DST), which contains a two-pass generation process : (1) generating a primitive dialogue state based on the dialogue of the current turn and the previous dialogue state, and (2) amending the primitive dialogue state from the first pass. With the additional amending generation pass, our model is tasked to learn more robust dialogue state tracking by amending the errors that still exist in the primitive dialogue state, which plays the role of reviser in the double-checking process and alleviates unnecessary error propagation. Experimental results show that AG-DST significantly outperforms previous works in two active DST datasets (MultiWOZ 2.2 and WOZ 2.0), achieving new state-of-the-art performances.</abstract>
      <url hash="37682d1b">2021.nlp4convai-1.8</url>
      <bibkey>tian-etal-2021-amendable</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.8</doi>
      <pwccode url="https://github.com/PaddlePaddle/Knover/tree/develop/projects/AG-DST" additional="false">PaddlePaddle/Knover</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-oz">Wizard-of-Oz</pwcdataset>
    </paper>
    <paper id="9">
      <title>What Went Wrong? Explaining Overall Dialogue Quality through Utterance-Level Impacts</title>
      <author><first>James D.</first><last>Finch</last></author>
      <author><first>Sarah E.</first><last>Finch</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>93–101</pages>
      <abstract>Improving user experience of a <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue system</a> often requires intensive developer effort to read conversation logs, run statistical analyses, and intuit the relative importance of system shortcomings. This paper presents a novel approach to automated analysis of conversation logs that learns the relationship between user-system interactions and overall dialogue quality. Unlike prior work on utterance-level quality prediction, our approach learns the impact of each interaction from the overall user rating without utterance-level annotation, allowing resultant model conclusions to be derived on the basis of empirical evidence and at low cost. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> identifies <a href="https://en.wikipedia.org/wiki/Interaction">interactions</a> that have a strong correlation with the overall dialogue quality in a chatbot setting. Experiments show that the automated analysis from our model agrees with expert judgments, making this work the first to show that such weakly-supervised learning of utterance-level quality prediction is highly achievable.</abstract>
      <url hash="e6426f79">2021.nlp4convai-1.9</url>
      <bibkey>finch-etal-2021-went</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.9</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/topical-chat">Topical-Chat</pwcdataset>
    </paper>
    <paper id="12">
      <title>Semi-supervised Intent Discovery with Contrastive Learning</title>
      <author><first>Xiang</first><last>Shen</last></author>
      <author><first>Yinge</first><last>Sun</last></author>
      <author><first>Yao</first><last>Zhang</last></author>
      <author><first>Mani</first><last>Najmabadi</last></author>
      <pages>120–129</pages>
      <abstract>User intent discovery is a key step in developing a Natural Language Understanding (NLU) module at the core of any modern Conversational AI system. Typically, human experts review a representative sample of user input data to discover new intents, which is subjective, costly, and error-prone. In this work, we aim to assist the NLU developers by presenting a novel method for discovering new intents at scale given a corpus of utterances. Our method utilizes supervised contrastive learning to leverage information from a domain-relevant, already labeled dataset and identifies new intents in the corpus at hand using unsupervised K-means clustering. Our method outperforms the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> by a large margin up to 2 % and 13 % on two benchmark datasets, measured by clustering accuracy. Furthermore, we apply our method on a large dataset from the travel domain to demonstrate its effectiveness on a real-world use case.</abstract>
      <url hash="8cbac7a3">2021.nlp4convai-1.12</url>
      <bibkey>shen-etal-2021-semi</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.12</doi>
    </paper>
    <paper id="13">
      <title>CS-BERT : a pretrained model for customer service dialogues<fixed-case>CS</fixed-case>-<fixed-case>BERT</fixed-case>: a pretrained model for customer service dialogues</title>
      <author><first>Peiyao</first><last>Wang</last></author>
      <author><first>Joyce</first><last>Fang</last></author>
      <author><first>Julia</first><last>Reinspach</last></author>
      <pages>130–142</pages>
      <abstract>Large-scale pretrained transformer models have demonstrated state-of-the-art (SOTA) performance in a variety of NLP tasks. Nowadays, numerous pretrained models are available in different model flavors and different languages, and can be easily adapted to one’s downstream task. However, only a limited number of <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> are available for dialogue tasks, and in particular, goal-oriented dialogue tasks. In addition, the available pretrained models are trained on general domain language, creating a mismatch between the pretraining language and the downstream domain launguage. In this contribution, we present CS-BERT, a BERT model pretrained on millions of dialogues in the customer service domain. We evaluate CS-BERT on several downstream customer service dialogue tasks, and demonstrate that our in-domain pretraining is advantageous compared to other pretrained models in both zero-shot experiments as well as in finetuning experiments, especially in a low-resource data setting.</abstract>
      <url hash="b4a6387c">2021.nlp4convai-1.13</url>
      <bibkey>wang-etal-2021-cs</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.13</doi>
    </paper>
    <paper id="14">
      <title>PLATO-KAG : Unsupervised Knowledge-Grounded Conversation via Joint Modeling<fixed-case>PLATO</fixed-case>-<fixed-case>KAG</fixed-case>: Unsupervised Knowledge-Grounded Conversation via Joint Modeling</title>
      <author><first>Xinxian</first><last>Huang</last></author>
      <author><first>Huang</first><last>He</last></author>
      <author><first>Siqi</first><last>Bao</last></author>
      <author><first>Fan</first><last>Wang</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>143–154</pages>
      <abstract>Large-scale conversation models are turning to leveraging <a href="https://en.wikipedia.org/wiki/Knowledge">external knowledge</a> to improve the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">factual accuracy</a> in response generation. Considering the infeasibility to annotate the external knowledge for large-scale dialogue corpora, it is desirable to learn the knowledge selection and response generation in an unsupervised manner. In this paper, we propose PLATO-KAG (Knowledge-Augmented Generation), an unsupervised learning approach for end-to-end knowledge-grounded conversation modeling. For each dialogue context, the top-k relevant knowledge elements are selected and then employed in knowledge-grounded response generation. The two components of knowledge selection and response generation are optimized jointly and effectively under a balanced objective. Experimental results on two publicly available datasets validate the superiority of PLATO-KAG.</abstract>
      <url hash="8ce43e25">2021.nlp4convai-1.14</url>
      <bibkey>huang-etal-2021-plato</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.14</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/holl-e">Holl-E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="17">
      <title>Personalized Search-based Query Rewrite System for Conversational AI<fixed-case>AI</fixed-case></title>
      <author><first>Eunah</first><last>Cho</last></author>
      <author><first>Ziyan</first><last>Jiang</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <author><first>Zheng</first><last>Chen</last></author>
      <author><first>Saurabh</first><last>Gupta</last></author>
      <author><first>Xing</first><last>Fan</last></author>
      <author><first>Chenlei</first><last>Guo</last></author>
      <pages>179–188</pages>
      <abstract>Query rewrite (QR) is an emerging component in conversational AI systems, reducing user defect. User defect is caused by various reasons, such as errors in the spoken dialogue system, users’ slips of the tongue or their abridged language. Many of the user defects stem from personalized factors, such as user’s speech pattern, <a href="https://en.wikipedia.org/wiki/Dialect">dialect</a>, or preferences. In this work, we propose a personalized search-based QR framework, which focuses on automatic reduction of user defect. We build a personalized index for each user, which encompasses diverse affinity layers to reflect personal preferences for each user in the conversational AI. Our personalized QR system contains retrieval and ranking layers. Supported by user feedback based learning, training our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> does not require hand-annotated data. Experiments on personalized test set showed that our personalized QR system is able to correct systematic and user errors by utilizing phonetic and semantic inputs.</abstract>
      <url hash="8a3b1465">2021.nlp4convai-1.17</url>
      <bibkey>cho-etal-2021-personalized</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.17</doi>
    </paper>
    <paper id="19">
      <title>AuGPT : Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models<fixed-case>AuGPT</fixed-case>: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models</title>
      <author><first>Jonáš</first><last>Kulhánek</last></author>
      <author><first>Vojtěch</first><last>Hudeček</last></author>
      <author><first>Tomáš</first><last>Nekvinda</last></author>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <pages>198–210</pages>
      <abstract>Attention-based pre-trained language models such as GPT-2 brought considerable progress to end-to-end dialogue modelling. However, they also present considerable risks for task-oriented dialogue, such as lack of knowledge grounding or diversity. To address these issues, we introduce modified training objectives for language model finetuning, and we employ massive data augmentation via <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a> to increase the diversity of the training data. We further examine the possibilities of combining data from multiples sources to improve performance on the target <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. We carefully evaluate our <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">contributions</a> with both <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">human and automatic methods</a>. Our model substantially outperforms the baseline on the MultiWOZ data and shows competitive performance with state of the art in both automatic and human evaluation.</abstract>
      <url hash="5050e18c">2021.nlp4convai-1.19</url>
      <bibkey>kulhanek-etal-2021-augpt</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.19</doi>
      <pwccode url="https://github.com/ufal/augpt" additional="false">ufal/augpt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/taskmaster-1">Taskmaster-1</pwcdataset>
    </paper>
    <paper id="22">
      <title>Using Pause Information for More Accurate Entity Recognition</title>
      <author><first>Sahas</first><last>Dendukuri</last></author>
      <author><first>Pooja</first><last>Chitkara</last></author>
      <author><first>Joel Ruben Antony</first><last>Moniz</last></author>
      <author><first>Xiao</first><last>Yang</last></author>
      <author><first>Manos</first><last>Tsagkias</last></author>
      <author><first>Stephen</first><last>Pulman</last></author>
      <pages>243–250</pages>
      <abstract>Entity tags in human-machine dialog are integral to natural language understanding (NLU) tasks in conversational assistants. However, current systems struggle to accurately parse spoken queries with the typical use of text input alone, and often fail to understand the user intent. Previous work in <a href="https://en.wikipedia.org/wiki/Linguistics">linguistics</a> has identified a cross-language tendency for longer speech pauses surrounding <a href="https://en.wikipedia.org/wiki/Noun">nouns</a> as compared to <a href="https://en.wikipedia.org/wiki/Verb">verbs</a>. We demonstrate that the linguistic observation on pauses can be used to improve <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> in machine-learnt language understanding tasks. Analysis of pauses in French and English utterances from a commercial voice assistant shows the statistically significant difference in pause duration around multi-token entity span boundaries compared to within entity spans. Additionally, in contrast to text-based NLU, we apply pause duration to enrich contextual embeddings to improve shallow parsing of entities. Results show that our proposed novel embeddings improve the relative error rate by up to 8 % consistently across three domains for <a href="https://en.wikipedia.org/wiki/French_language">French</a>, without any added annotation or alignment costs to the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a>.</abstract>
      <url hash="df826e0f">2021.nlp4convai-1.22</url>
      <bibkey>dendukuri-etal-2021-using</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.22</doi>
    </paper>
    <paper id="24">
      <title>Teach Me What to Say and I Will Learn What to Pick : Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models<fixed-case>I</fixed-case> Will Learn What to Pick: Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models</title>
      <author><first>Ehsan</first><last>Lotfi</last></author>
      <author><first>Maxime</first><last>De Bruyn</last></author>
      <author><first>Jeska</first><last>Buhmann</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>254–262</pages>
      <abstract>Knowledge Grounded Conversation Models are usually based on a selection / retrieval module and a generation module, trained separately or simultaneously, with or without having access to a ‘gold’ knowledge option. With the introduction of large pre-trained generative models, the selection and generation part have become more and more entangled, shifting the focus towards enhancing knowledge incorporation (from multiple sources) instead of trying to pick the best knowledge option. These approaches however depend on knowledge labels and/or a separate dense retriever for their best performance. In this work we study the unsupervised selection abilities of pre-trained generative models (e.g. BART) and show that by adding a score-and-aggregate module between <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and <a href="https://en.wikipedia.org/wiki/Codec">decoder</a>, they are capable of learning to pick the proper knowledge through minimising the language modelling loss (i.e. without having access to knowledge labels). Trained as such, our model-K-Mine-shows competitive selection and generation performance against models that benefit from knowledge labels and/or separate dense retriever.</abstract>
      <url hash="6fbace53">2021.nlp4convai-1.24</url>
      <bibkey>lotfi-etal-2021-teach</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.24</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/holl-e">Holl-E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="25">
      <title>Influence of user personality on dialogue task performance : A case study using a rule-based dialogue system</title>
      <author><first>Ao</first><last>Guo</last></author>
      <author><first>Atsumoto</first><last>Ohashi</last></author>
      <author><first>Ryu</first><last>Hirai</last></author>
      <author><first>Yuya</first><last>Chiba</last></author>
      <author><first>Yuiko</first><last>Tsunomori</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <pages>263–270</pages>
      <abstract>Endowing a task-oriented dialogue system with adaptiveness to <a href="https://en.wikipedia.org/wiki/User_(computing)">user personality</a> can greatly help improve the performance of a dialogue task. However, such a <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue system</a> can be practically challenging to implement, because it is unclear how <a href="https://en.wikipedia.org/wiki/User_(computing)">user personality</a> influences dialogue task performance. To explore the relationship between user personality and dialogue task performance, we enrolled participants via <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a> to first answer specified personality questionnaires and then chat with a dialogue system to accomplish assigned tasks. A rule-based dialogue system on the prevalent Multi-Domain Wizard-of-Oz (MultiWOZ) task was used. A total of 211 participants’ personalities and their 633 dialogues were collected and analyzed. The results revealed that sociable and extroverted people tended to fail the task, whereas neurotic people were more likely to succeed. We extracted <a href="https://en.wikipedia.org/wiki/Software_feature">features</a> related to user dialogue behaviors and performed further analysis to determine which kind of <a href="https://en.wikipedia.org/wiki/Behavior">behavior</a> influences task performance. As a result, we identified that average utterance length and slots per utterance are the key features of dialogue behavior that are highly correlated with both task performance and user personality.</abstract>
      <url hash="8a9c940d">2021.nlp4convai-1.25</url>
      <bibkey>guo-etal-2021-influence</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.25</doi>
    </paper>
    <paper id="27">
      <title>Towards Zero and Few-shot Knowledge-seeking Turn Detection in Task-orientated Dialogue Systems</title>
      <author><first>Di</first><last>Jin</last></author>
      <author><first>Shuyang</first><last>Gao</last></author>
      <author><first>Seokhwan</first><last>Kim</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>281–288</pages>
      <abstract>Most prior work on task-oriented dialogue systems is restricted to supporting domain APIs. However, users may have requests that are out of the scope of these <a href="https://en.wikipedia.org/wiki/Application_programming_interface">APIs</a>. This work focuses on identifying such <a href="https://en.wikipedia.org/wiki/User_(computing)">user requests</a>. Existing methods for this task mainly rely on fine-tuning pre-trained models on large annotated data. We propose a novel method, REDE, based on adaptive representation learning and <a href="https://en.wikipedia.org/wiki/Density_estimation">density estimation</a>. REDE can be applied to zero-shot cases, and quickly learns a high-performing detector with only a few shots by updating less than 3 K parameters. We demonstrate <a href="https://en.wikipedia.org/wiki/Rede">REDE</a>’s competitive performance on DSTC9 data and our newly collected test set.</abstract>
      <url hash="bc4804fa">2021.nlp4convai-1.27</url>
      <bibkey>jin-etal-2021-towards</bibkey>
      <doi>10.18653/v1/2021.nlp4convai-1.27</doi>
      <pwccode url="https://github.com/jind11/rede" additional="false">jind11/rede</pwccode>
    </paper>
  </volume>
</collection>