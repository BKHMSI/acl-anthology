<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.iwslt">
  <volume id="1" ingest-date="2021-07-22">
    <meta>
      <booktitle>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</booktitle>
      <editor><first>Marcello</first><last>Federico</last></editor>
      <editor><first>Alex</first><last>Waibel</last></editor>
      <editor><first>Marta R.</first><last>Costa-jussà</last></editor>
      <editor><first>Jan</first><last>Niehues</last></editor>
      <editor><first>Sebastian</first><last>Stuker</last></editor>
      <editor><first>Elizabeth</first><last>Salesky</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand (online)</address>
      <month>August</month>
      <year>2021</year>
      <url hash="3a0b28e8">2021.iwslt-1</url>
    </meta>
    <frontmatter>
      <url hash="aeeec0cb">2021.iwslt-1.0</url>
      <bibkey>iwslt-2021-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN<fixed-case>FINDINGS</fixed-case> <fixed-case>OF</fixed-case> <fixed-case>THE</fixed-case> <fixed-case>IWSLT</fixed-case> 2021 <fixed-case>EVALUATION</fixed-case> <fixed-case>CAMPAIGN</fixed-case></title>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Jacob</first><last>Bremerman</last></author>
      <author><first>Roldano</first><last>Cattoni</last></author>
      <author><first>Maha</first><last>Elbayad</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <author><first>Xutai</first><last>Ma</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Juan</first><last>Pino</last></author>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <author><first>Changhan</first><last>Wang</last></author>
      <author><first>Matthew</first><last>Wiesner</last></author>
      <pages>1–29</pages>
      <abstract>The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2021) featured this year four shared tasks : (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Multilingual speech translation, (iv) Low-resource speech translation. A total of 22 teams participated in at least one of the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. This paper describes each shared task, data and evaluation metrics, and reports results of the received submissions.</abstract>
      <url hash="d04d3be0">2021.iwslt-1.1</url>
      <doi>10.18653/v1/2021.iwslt-1.1</doi>
      <bibkey>anastasopoulos-etal-2021-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/covost">CoVoST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="3">
      <title>NAIST English-to-Japanese Simultaneous Translation System for IWSLT 2021 Simultaneous Text-to-text Task<fixed-case>NAIST</fixed-case> <fixed-case>E</fixed-case>nglish-to-<fixed-case>J</fixed-case>apanese Simultaneous Translation System for <fixed-case>IWSLT</fixed-case> 2021 Simultaneous Text-to-text Task</title>
      <author><first>Ryo</first><last>Fukuda</last></author>
      <author><first>Yui</first><last>Oka</last></author>
      <author><first>Yasumasa</first><last>Kano</last></author>
      <author><first>Yuki</first><last>Yano</last></author>
      <author><first>Yuka</first><last>Ko</last></author>
      <author><first>Hirotaka</first><last>Tokuyama</last></author>
      <author><first>Kosuke</first><last>Doi</last></author>
      <author><first>Sakriani</first><last>Sakti</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>39–45</pages>
      <abstract>This paper describes NAIST’s system for the English-to-Japanese Simultaneous Text-to-text Translation Task in IWSLT 2021 Evaluation Campaign. Our primary submission is based on wait-k neural machine translation with sequence-level knowledge distillation to encourage <a href="https://en.wikipedia.org/wiki/Literal_translation">literal translation</a>.</abstract>
      <url hash="1929bf0a">2021.iwslt-1.3</url>
      <doi>10.18653/v1/2021.iwslt-1.3</doi>
      <bibkey>fukuda-etal-2021-naist</bibkey>
    </paper>
    <paper id="7">
      <title>THE IWSLT 2021 BUT SPEECH TRANSLATION SYSTEMS<fixed-case>THE</fixed-case> <fixed-case>IWSLT</fixed-case> 2021 <fixed-case>BUT</fixed-case> <fixed-case>SPEECH</fixed-case> <fixed-case>TRANSLATION</fixed-case> <fixed-case>SYSTEMS</fixed-case></title>
      <author><first>hari Krishna</first><last>Vydana</last></author>
      <author><first>Martin</first><last>Karafiat</last></author>
      <author><first>Lukas</first><last>Burget</last></author>
      <author><first>Jan</first><last>Černocký</last></author>
      <pages>75–83</pages>
      <abstract>The paper describes BUT’s English to German offline speech translation (ST) systems developed for IWSLT2021. They are based on jointly trained Automatic Speech Recognition-Machine Translation models. Their performances is evaluated on MustC-Common test set. In this work, we study their efficiency from the perspective of having a large amount of separate ASR training data and MT training data, and a smaller amount of speech-translation training data. Large amounts of ASR and MT training data are utilized for pre-training the ASR and MT models. Speech-translation data is used to jointly optimize ASR-MT models by defining an end-to-end differentiable path from speech to translations. For this purpose, we use the internal continuous representations from the ASR-decoder as the input to MT module. We show that <a href="https://en.wikipedia.org/wiki/Speech_translation">speech translation</a> can be further improved by training the ASR-decoder jointly with the MT-module using large amount of text-only MT training data. We also show significant improvements by training an ASR module capable of generating punctuated text, rather than leaving the punctuation task to the MT module.</abstract>
      <url hash="5a20ce0f">2021.iwslt-1.7</url>
      <doi>10.18653/v1/2021.iwslt-1.7</doi>
      <bibkey>vydana-etal-2021-iwslt</bibkey>
    </paper>
    <paper id="8">
      <title>Dealing with training and test segmentation mismatch : FBK@IWSLT2021<fixed-case>FBK</fixed-case>@<fixed-case>IWSLT</fixed-case>2021</title>
      <author><first>Sara</first><last>Papi</last></author>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>84–91</pages>
      <abstract>This paper describes FBK’s system submission to the IWSLT 2021 Offline Speech Translation task. We participated with a direct model, which is a Transformer-based architecture trained to translate English speech audio data into German texts. The training pipeline is characterized by knowledge distillation and a two-step fine-tuning procedure. Both knowledge distillation and the first fine-tuning step are carried out on manually segmented real and synthetic data, the latter being generated with an MT system trained on the available corpora. Differently, the second fine-tuning step is carried out on a random segmentation of the MuST-C v2 En-De dataset. Its main goal is to reduce the performance drops occurring when a <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech translation model</a> trained on manually segmented data (i.e. an ideal, sentence-like segmentation) is evaluated on <a href="https://en.wikipedia.org/wiki/Audio_signal_processing">automatically segmented audio</a> (i.e. actual, more realistic testing conditions). For the same purpose, a custom hybrid segmentation procedure that accounts for both audio content (pauses) and for the length of the produced segments is applied to the test data before passing them to the system. At inference time, we compared this <a href="https://en.wikipedia.org/wiki/Procedure_(term)">procedure</a> with a baseline segmentation method based on Voice Activity Detection (VAD). Our results indicate the effectiveness of the proposed hybrid approach, shown by a reduction of the gap with manual segmentation from 8.3 to 1.4 BLEU points.</abstract>
      <url hash="7f4c6e97">2021.iwslt-1.8</url>
      <doi>10.18653/v1/2021.iwslt-1.8</doi>
      <bibkey>papi-etal-2021-dealing</bibkey>
    </paper>
    <paper id="11">
      <title>End-to-End Speech Translation with Pre-trained Models and Adapters : UPC at IWSLT 2021<fixed-case>UPC</fixed-case> at <fixed-case>IWSLT</fixed-case> 2021</title>
      <author><first>Gerard I.</first><last>Gállego</last></author>
      <author><first>Ioannis</first><last>Tsiamas</last></author>
      <author><first>Carlos</first><last>Escolano</last></author>
      <author><first>José A. R.</first><last>Fonollosa</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <pages>110–119</pages>
      <abstract>This paper describes the submission to the IWSLT 2021 offline speech translation task by the UPC Machine Translation group. The task consists of building a <a href="https://en.wikipedia.org/wiki/System">system</a> capable of translating <a href="https://en.wikipedia.org/wiki/English_language">English audio recordings</a> extracted from <a href="https://en.wikipedia.org/wiki/TED_(conference)">TED talks</a> into <a href="https://en.wikipedia.org/wiki/German_language">German text</a>. Submitted systems can be either cascade or end-to-end and use a custom or given segmentation. Our submission is an end-to-end speech translation system, which combines pre-trained models (Wav2Vec 2.0 and mBART) with coupling modules between the encoder and decoder, and uses an efficient fine-tuning technique, which trains only 20 % of its total parameters. We show that adding an Adapter to the system and pre-training it, can increase the <a href="https://en.wikipedia.org/wiki/Convergence_rate">convergence speed</a> and the final result, with which we achieve a BLEU score of 27.3 on the MuST-C test set. Our final <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is an <a href="https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)">ensemble</a> that obtains 28.22 BLEU score on the same set. Our submission also uses a custom segmentation algorithm that employs pre-trained Wav2Vec 2.0 for identifying periods of untranscribable text and can bring improvements of 2.5 to 3 BLEU score on the IWSLT 2019 test set, as compared to the result with the given segmentation.</abstract>
      <url hash="31d80234">2021.iwslt-1.11</url>
      <doi>10.18653/v1/2021.iwslt-1.11</doi>
      <bibkey>gallego-etal-2021-end</bibkey>
      <pwccode url="https://github.com/mt-upc/iwslt-2021" additional="false">mt-upc/iwslt-2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/covost">CoVoST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/europarl-st">Europarl-ST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iwslt-2019">IWSLT 2019</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="15">
      <title>Maastricht University’s Multilingual Speech Translation System for IWSLT 2021<fixed-case>IWSLT</fixed-case> 2021</title>
      <author><first>Danni</first><last>Liu</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>138–143</pages>
      <abstract>This paper describes Maastricht University’s participation in the IWSLT 2021 multilingual speech translation track. The task in this track is to build multilingual speech translation systems in supervised and zero-shot directions. Our primary system is an <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end model</a> that performs both <a href="https://en.wikipedia.org/wiki/Transcription_(linguistics)">speech transcription</a> and <a href="https://en.wikipedia.org/wiki/Translation">translation</a>. We observe that the joint training for the two tasks is complementary especially when the speech translation data is scarce. On the source and target side, we use <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> and pseudo-labels respectively to improve the performance of our <a href="https://en.wikipedia.org/wiki/System">systems</a>. We also introduce an ensembling technique that consistently improves the quality of transcriptions and <a href="https://en.wikipedia.org/wiki/Translation">translations</a>. The experiments show that the <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end system</a> is competitive with its cascaded counterpart especially in zero-shot conditions.</abstract>
      <url hash="dff7b25d">2021.iwslt-1.15</url>
      <doi>10.18653/v1/2021.iwslt-1.15</doi>
      <bibkey>liu-niehues-2021-maastricht</bibkey>
    </paper>
    <paper id="16">
      <title>ZJU’s IWSLT 2021 Speech Translation System<fixed-case>ZJU</fixed-case>’s <fixed-case>IWSLT</fixed-case> 2021 Speech Translation System</title>
      <author><first>Linlin</first><last>Zhang</last></author>
      <pages>144–148</pages>
      <abstract>In this paper, we describe Zhejiang University’s submission to the IWSLT2021 Multilingual Speech Translation Task. This task focuses on speech translation (ST) research across many non-English source languages. Participants can decide whether to work on constrained systems or unconstrained systems which can using external data. We create both cascaded and end-to-end speech translation constrained systems, using the provided data only. In the cascaded approach, we combine Conformer-based automatic speech recognition (ASR) with the Transformer-based neural machine translation (NMT). Our end-to-end direct speech translation systems use ASR pretrained encoder and multi-task decoders. The submitted <a href="https://en.wikipedia.org/wiki/System">systems</a> are ensembled by different cascaded models.</abstract>
      <url hash="244ce8e4">2021.iwslt-1.16</url>
      <doi>10.18653/v1/2021.iwslt-1.16</doi>
      <bibkey>zhang-2021-zjus</bibkey>
    </paper>
    <paper id="17">
      <title>Multilingual Speech Translation with Unified Transformer : Huawei Noah’s Ark Lab at IWSLT 2021<fixed-case>N</fixed-case>oah’s Ark Lab at <fixed-case>IWSLT</fixed-case> 2021</title>
      <author><first>Xingshan</first><last>Zeng</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>149–153</pages>
      <abstract>This paper describes the system submitted to the IWSLT 2021 Multilingual Speech Translation (MultiST) task from Huawei Noah’s Ark Lab. We use a unified transformer architecture for our MultiST model, so that the data from different modalities (i.e., speech and text) and different <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> (i.e., <a href="https://en.wikipedia.org/wiki/Speech_recognition">Speech Recognition</a>, <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a>, and Speech Translation) can be exploited to enhance the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>’s ability. Specifically, speech and text inputs are firstly fed to different feature extractors to extract acoustic and textual features, respectively. Then, these <a href="https://en.wikipedia.org/wiki/Software_feature">features</a> are processed by a shared encoderdecoder architecture. We apply several training techniques to improve the performance, including <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>, task-level curriculum learning, <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, etc. Our final system achieves significantly better results than bilingual baselines on supervised language pairs and yields reasonable results on zero-shot language pairs.</abstract>
      <url hash="83dc2a5f">2021.iwslt-1.17</url>
      <doi>10.18653/v1/2021.iwslt-1.17</doi>
      <bibkey>zeng-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="18">
      <title>Multilingual Speech Translation KIT @ IWSLT2021<fixed-case>KIT</fixed-case> @ <fixed-case>IWSLT</fixed-case>2021</title>
      <author><first>Ngoc-Quan</first><last>Pham</last></author>
      <author><first>Tuan Nam</first><last>Nguyen</last></author>
      <author><first>Thanh-Le</first><last>Ha</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <author><first>Dan</first><last>He</last></author>
      <pages>154–159</pages>
      <abstract>This paper contains the description for the submission of Karlsruhe Institute of Technology (KIT) for the multilingual TEDx translation task in the IWSLT 2021 evaluation campaign. Our main approach is to develop both cascade and end-to-end systems and eventually combine them together to achieve the best possible results for this extremely low-resource setting. The report also confirms certain consistent architectural improvement added to the Transformer architecture, for all tasks : <a href="https://en.wikipedia.org/wiki/Translation">translation</a>, <a href="https://en.wikipedia.org/wiki/Transcription_(linguistics)">transcription</a> and speech translation.</abstract>
      <url hash="125bdb8e">2021.iwslt-1.18</url>
      <doi>10.18653/v1/2021.iwslt-1.18</doi>
      <bibkey>pham-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="26">
      <title>Between Flexibility and Consistency : Joint Generation of Captions and Subtitles</title>
      <author><first>Alina</first><last>Karakanta</last></author>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>215–225</pages>
      <abstract>Speech translation (ST) has lately received growing interest for the generation of subtitles without the need for an intermediate source language transcription and timing (i.e. captions). However, the joint generation of source captions and target subtitles does not only bring potential output quality advantages when the two decoding processes inform each other, but it is also often required in multilingual scenarios. In this work, we focus on ST models which generate consistent captions-subtitles in terms of <a href="https://en.wikipedia.org/wiki/Structure">structure</a> and <a href="https://en.wikipedia.org/wiki/Content_(media)">lexical content</a>. We further introduce new <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> for evaluating subtitling consistency. Our findings show that joint decoding leads to increased performance and consistency between the generated captions and <a href="https://en.wikipedia.org/wiki/Subtitle_(titling)">subtitles</a> while still allowing for sufficient flexibility to produce <a href="https://en.wikipedia.org/wiki/Subtitle_(titling)">subtitles</a> conforming to language-specific needs and norms.</abstract>
      <url hash="bc462a33">2021.iwslt-1.26</url>
      <doi>10.18653/v1/2021.iwslt-1.26</doi>
      <bibkey>karakanta-etal-2021-flexibility</bibkey>
      <pwccode url="https://github.com/mgaido91/FBK-fairseq-ST" additional="false">mgaido91/FBK-fairseq-ST</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/must-cinema">MuST-Cinema</pwcdataset>
    </paper>
    <paper id="28">
      <title>Inverted Projection for Robust Speech Translation</title>
      <author><first>Dirk</first><last>Padfield</last></author>
      <author><first>Colin</first><last>Cherry</last></author>
      <pages>236–244</pages>
      <abstract>Traditional translation systems trained on written documents perform well for text-based translation but not as well for speech-based applications. We aim to adapt translation models to speech by introducing actual lexical errors from ASR and segmentation errors from automatic punctuation into our translation training data. We introduce an inverted projection approach that projects automatically detected system segments onto human transcripts and then re-segments the gold translations to align with the projected human transcripts. We demonstrate that this overcomes the train-test mismatch present in other training approaches. The new projection approach achieves gains of over 1 BLEU point over a baseline that is exposed to the human transcripts and segmentations, and these gains hold for both IWSLT data and <a href="https://en.wikipedia.org/wiki/YouTube">YouTube data</a>.</abstract>
      <url hash="cf49d38e">2021.iwslt-1.28</url>
      <doi>10.18653/v1/2021.iwslt-1.28</doi>
      <bibkey>padfield-cherry-2021-inverted</bibkey>
    </paper>
    <paper id="29">
      <title>Towards the evaluation of automatic simultaneous speech translation from a communicative perspective</title>
      <author><first>Claudio</first><last>Fantinuoli</last></author>
      <author><first>Bianca</first><last>Prandi</last></author>
      <pages>245–254</pages>
      <abstract>In recent years, automatic speech-to-speech and speech-to-text translation has gained momentum thanks to advances in <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial intelligence</a>, especially in the domains of <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition</a> and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. The quality of such <a href="https://en.wikipedia.org/wiki/Application_software">applications</a> is commonly tested with automatic metrics, such as <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>, primarily with the goal of assessing improvements of releases or in the context of evaluation campaigns. However, little is known about how the output of such <a href="https://en.wikipedia.org/wiki/System">systems</a> is perceived by end users or how they compare to human performances in similar communicative tasks. In this paper, we present the results of an experiment aimed at evaluating the quality of a real-time speech translation engine by comparing it to the performance of professional simultaneous interpreters. To do so, we adopt a <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> developed for the assessment of <a href="https://en.wikipedia.org/wiki/Language_interpretation">human interpreters</a> and use it to perform a manual evaluation on both <a href="https://en.wikipedia.org/wiki/Human–computer_interaction">human and machine performances</a>. In our sample, we found better performance for the <a href="https://en.wikipedia.org/wiki/Language_interpretation">human interpreters</a> in terms of <a href="https://en.wikipedia.org/wiki/Intelligibility_(communication)">intelligibility</a>, while the <a href="https://en.wikipedia.org/wiki/Machine">machine</a> performs slightly better in terms of <a href="https://en.wikipedia.org/wiki/Informatics">informativeness</a>. The limitations of the study and the possible enhancements of the chosen <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> are discussed. Despite its intrinsic limitations, the use of this <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> represents a first step towards a user-centric and communication-oriented methodology for evaluating real-time automatic speech translation.</abstract>
      <url hash="52d3d64a">2021.iwslt-1.29</url>
      <doi>10.18653/v1/2021.iwslt-1.29</doi>
      <bibkey>fantinuoli-prandi-2021-towards</bibkey>
    </paper>
    </volume>
</collection>