<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.wnut">
  <volume id="1" ingest-date="2021-11-12">
    <meta>
      <booktitle>Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)</booktitle>
      <editor><first>Wei</first><last>Xu</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <editor><first>Tim</first><last>Baldwin</last></editor>
      <editor><first>Afshin</first><last>Rahimi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="375a0bef">2021.wnut-1.0</url>
      <bibkey>wnut-2021-noisy</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Text Simplification for Comprehension-based Question-Answering</title>
      <author><first>Tanvi</first><last>Dadu</last></author>
      <author><first>Kartikey</first><last>Pant</last></author>
      <author><first>Seema</first><last>Nagar</last></author>
      <author><first>Ferdous</first><last>Barbhuiya</last></author>
      <author><first>Kuntal</first><last>Dey</last></author>
      <pages>1–10</pages>
      <abstract>Text simplification is the process of splitting and rephrasing a sentence to a sequence of sentences making it easier to read and understand while preserving the content and approximating the original meaning. Text simplification has been exploited in NLP applications like <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a>, <a href="https://en.wikipedia.org/wiki/Semantic_role_labeling">semantic role labeling</a>, and <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a>, opening a broad avenue for its exploitation in comprehension-based question-answering downstream tasks. In this work, we investigate the effect of <a href="https://en.wikipedia.org/wiki/Text_simplification">text simplification</a> in the task of <a href="https://en.wikipedia.org/wiki/Question_answering">question-answering</a> using a <a href="https://en.wikipedia.org/wiki/Context_(language_use)">comprehension context</a>. We release Simple-SQuAD, a simplified version of the widely-used SQuAD dataset. Firstly, we outline each step in the dataset creation pipeline, including style transfer, thresholding of sentences showing correct transfer, and offset finding for each answer. Secondly, we verify the quality of the transferred sentences through various <a href="https://en.wikipedia.org/wiki/Methodology">methodologies</a> involving both automated and human evaluation. Thirdly, we benchmark the newly created <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> and perform an ablation study for examining the effect of the simplification process in the SQuAD-based question answering task. Our experiments show that simplification leads to up to 2.04 % and 1.74 % increase in <a href="https://en.wikipedia.org/wiki/Exact_Match">Exact Match</a> and F1, respectively. Finally, we conclude with an analysis of the transfer process, investigating the types of edits made by the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, and the effect of <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence length</a> on the transfer model.</abstract>
      <url hash="7f5d9cfb">2021.wnut-1.1</url>
      <bibkey>dadu-etal-2021-text</bibkey>
      <doi>10.18653/v1/2021.wnut-1.1</doi>
      <pwccode url="https://github.com/kartikeypant/text-simplification-qa-www2021" additional="false">kartikeypant/text-simplification-qa-www2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisplit">WikiSplit</pwcdataset>
    </paper>
    <paper id="4">
      <title>Keyphrase Extraction with Incomplete Annotated Training Data</title>
      <author><first>Yanfei</first><last>Lei</last></author>
      <author><first>Chunming</first><last>Hu</last></author>
      <author><first>Guanghui</first><last>Ma</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <pages>26–34</pages>
      <abstract>Extracting keyphrases that summarize the main points of a document is a fundamental task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. Supervised approaches to keyphrase extraction(KPE) are largely developed based on the assumption that the training data is fully annotated. However, due to the difficulty of keyphrase annotating, KPE models severely suffer from incomplete annotated problem in many scenarios. To this end, we propose a more robust <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training method</a> that learns to mitigate the misguidance brought by unlabeled keyphrases. We introduce negative sampling to adjust training loss, and conduct experiments under different scenarios. Empirical studies on synthetic datasets and open domain dataset show that our model is robust to incomplete annotated problem and surpasses prior baselines. Extensive experiments on five scientific domain datasets of different scales demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is competitive with the state-of-the-art method.</abstract>
      <url hash="f5db7d3a">2021.wnut-1.4</url>
      <attachment type="Software" hash="429484a1">2021.wnut-1.4.Software.zip</attachment>
      <bibkey>lei-etal-2021-keyphrase</bibkey>
      <doi>10.18653/v1/2021.wnut-1.4</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="5">
      <title>Fine-grained Temporal Relation Extraction with Ordered-Neuron LSTM and Graph Convolutional Networks<fixed-case>LSTM</fixed-case> and Graph Convolutional Networks</title>
      <author><first>Minh</first><last>Tran Phu</last></author>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>35–45</pages>
      <abstract>Fine-grained temporal relation extraction (FineTempRel) aims to recognize the durations and timeline of event mentions in text. A missing part in the current deep learning models for FineTempRel is their failure to exploit the <a href="https://en.wikipedia.org/wiki/Syntax">syntactic structures</a> of the input sentences to enrich the <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representation vectors</a>. In this work, we propose to fill this gap by introducing novel methods to integrate the <a href="https://en.wikipedia.org/wiki/Syntax">syntactic structures</a> into the <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning models</a> for FineTempRel. The proposed <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> focuses on two types of syntactic information from the <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">dependency trees</a>, i.e., the syntax-based importance scores for representation learning of the words and the syntactic connections to identify important <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context words</a> for the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">event mentions</a>. We also present two novel techniques to facilitate the knowledge transfer between the subtasks of FineTempRel, leading to a novel model with the state-of-the-art performance for this task.</abstract>
      <url hash="81e1333c">2021.wnut-1.5</url>
      <bibkey>tran-phu-etal-2021-fine</bibkey>
      <doi>10.18653/v1/2021.wnut-1.5</doi>
    </paper>
    <paper id="9">
      <title>A Text Editing Approach to Joint Japanese Word Segmentation, <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">POS Tagging</a>, and Lexical Normalization<fixed-case>J</fixed-case>apanese Word Segmentation, <fixed-case>POS</fixed-case> Tagging, and Lexical Normalization</title>
      <author><first>Shohei</first><last>Higashiyama</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>67–80</pages>
      <abstract>Lexical normalization, in addition to <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a> and <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a>, is a fundamental task for Japanese user-generated text processing. In this paper, we propose a text editing model to solve the three task jointly and methods of pseudo-labeled data generation to overcome the problem of data deficiency. Our experiments showed that the proposed <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> achieved better <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalization</a> performance when trained on more diverse pseudo-labeled data.</abstract>
      <url hash="16c82883">2021.wnut-1.9</url>
      <bibkey>higashiyama-etal-2021-text</bibkey>
      <doi>10.18653/v1/2021.wnut-1.9</doi>
    </paper>
    <paper id="10">
      <title>Intrinsic evaluation of language models for <a href="https://en.wikipedia.org/wiki/Code-switching">code-switching</a></title>
      <author><first>Sik Feng</first><last>Cheong</last></author>
      <author><first>Hai Leong</first><last>Chieu</last></author>
      <author><first>Jing</first><last>Lim</last></author>
      <pages>81–86</pages>
      <abstract>Language models used in <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition</a> are often either evaluated intrinsically using <a href="https://en.wikipedia.org/wiki/Perplexity">perplexity</a> on test data, or extrinsically with an automatic speech recognition (ASR) system. The former <a href="https://en.wikipedia.org/wiki/Evaluation">evaluation</a> does not always correlate well with <a href="https://en.wikipedia.org/wiki/Signaling_(telecommunications)">ASR</a> performance, while the latter could be specific to particular <a href="https://en.wikipedia.org/wiki/Signaling_(telecommunications)">ASR systems</a>. Recent work proposed to evaluate language models by using them to classify ground truth sentences among alternative phonetically similar sentences generated by a fine state transducer. Underlying such an <a href="https://en.wikipedia.org/wiki/Evaluation">evaluation</a> is the assumption that the generated sentences are linguistically incorrect. In this paper, we first put this assumption into question, and observe that alternatively generated sentences could often be linguistically correct when they differ from the ground truth by only one edit. Secondly, we showed that by using multi-lingual BERT, we can achieve better performance than previous work on two code-switching data sets. Our implementation is publicly available on Github at https://github.com/sikfeng/language-modelling-for-code-switching.</abstract>
      <url hash="dab0802c">2021.wnut-1.10</url>
      <bibkey>cheong-etal-2021-intrinsic</bibkey>
      <doi>10.18653/v1/2021.wnut-1.10</doi>
      <pwccode url="https://github.com/sikfeng/language-modelling-for-code-switching" additional="false">sikfeng/language-modelling-for-code-switching</pwccode>
    </paper>
    <paper id="12">
      <title>Perceived and Intended Sarcasm Detection with Graph Attention Networks</title>
      <author><first>Joan</first><last>Plepi</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>97–105</pages>
      <abstract>Existing sarcasm detection systems focus on exploiting <a href="https://en.wikipedia.org/wiki/Marker_(linguistics)">linguistic markers</a>, <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a>, or user-level priors. However, social studies suggest that the relationship between the author and the audience can be equally relevant for the sarcasm usage and interpretation. In this work, we propose a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> jointly leveraging (1) a user context from their historical tweets together with (2) the social information from a user’s conversational neighborhood in an interaction graph, to contextualize the interpretation of the post. We use graph attention networks (GAT) over users and tweets in a conversation thread, combined with dense user history representations. Apart from achieving state-of-the-art results on the recently published dataset of 19k Twitter users with 30 K labeled tweets, adding 10 M unlabeled tweets as context, our results indicate that the model contributes to interpreting the sarcastic intentions of an author more than to predicting the sarcasm perception by others.</abstract>
      <url hash="fc846cee">2021.wnut-1.12</url>
      <bibkey>plepi-flek-2021-perceived</bibkey>
      <doi>10.18653/v1/2021.wnut-1.12</doi>
      <pwccode url="https://github.com/caisa-lab/sarcasm_detection" additional="false">caisa-lab/sarcasm_detection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/spirs">SPIRS</pwcdataset>
    </paper>
    <paper id="18">
      <title>Comparing Grammatical Theories of Code-Mixing</title>
      <author><first>Adithya</first><last>Pratapa</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>158–167</pages>
      <abstract>Code-mixed text generation systems have found applications in many downstream tasks, including <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition</a>, <a href="https://en.wikipedia.org/wiki/Translation">translation</a> and <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a>. A paradigm of these generation systems relies on well-defined grammatical theories of code-mixing, and there is a lack of comparison of these <a href="https://en.wikipedia.org/wiki/Theory">theories</a>. We present a large-scale human evaluation of two popular grammatical theories, Matrix-Embedded Language (ML) and Equivalence Constraint (EC). We compare them against three heuristic-based models and quantitatively demonstrate the effectiveness of the two <a href="https://en.wikipedia.org/wiki/Grammatical_theory">grammatical theories</a>.</abstract>
      <url hash="251fcc03">2021.wnut-1.18</url>
      <bibkey>pratapa-choudhury-2021-comparing</bibkey>
      <doi>10.18653/v1/2021.wnut-1.18</doi>
    </paper>
    <paper id="21">
      <title>Mitigation of Diachronic Bias in Fake News Detection Dataset</title>
      <author><first>Taichi</first><last>Murayama</last></author>
      <author><first>Shoko</first><last>Wakamiya</last></author>
      <author><first>Eiji</first><last>Aramaki</last></author>
      <pages>182–188</pages>
      <abstract>Fake news causes significant damage to society. To deal with these <a href="https://en.wikipedia.org/wiki/Fake_news">fake news</a>, several studies on building detection models and arranging datasets have been conducted. Most of the fake news datasets depend on a specific time period. Consequently, the detection models trained on such a dataset have difficulty detecting novel <a href="https://en.wikipedia.org/wiki/Fake_news">fake news</a> generated by political changes and social changes ; they may possibly result in biased output from the input, including specific person names and organizational names. We refer to this problem as Diachronic Bias because it is caused by the creation date of news in each dataset. In this study, we confirm the <a href="https://en.wikipedia.org/wiki/Bias">bias</a>, especially <a href="https://en.wikipedia.org/wiki/Proper_noun">proper nouns</a> including <a href="https://en.wikipedia.org/wiki/Personal_name">person names</a>, from the deviation of phrase appearances in each dataset. Based on these findings, we propose masking methods using <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a> to mitigate the influence of person names and validate whether they make fake news detection models robust through experiments with in-domain and out-of-domain data.</abstract>
      <url hash="6b3d5998">2021.wnut-1.21</url>
      <bibkey>murayama-etal-2021-mitigation</bibkey>
      <doi>10.18653/v1/2021.wnut-1.21</doi>
    </paper>
    <paper id="24">
      <title>Changes in Twitter geolocations : Insights and suggestions for future usage<fixed-case>T</fixed-case>witter geolocations: Insights and suggestions for future usage</title>
      <author><first>Anna</first><last>Kruspe</last></author>
      <author><first>Matthias</first><last>Häberle</last></author>
      <author><first>Eike J.</first><last>Hoffmann</last></author>
      <author><first>Samyo</first><last>Rode-Hasinger</last></author>
      <author><first>Karam</first><last>Abdulahhad</last></author>
      <author><first>Xiao Xiang</first><last>Zhu</last></author>
      <pages>212–221</pages>
      <abstract>Twitter data has become established as a valuable source of data for various application scenarios in the past years. For many such <a href="https://en.wikipedia.org/wiki/Application_software">applications</a>, it is necessary to know where Twitter posts (tweets) were sent from or what location they refer to. Researchers have frequently used exact coordinates provided in a small percentage of tweets, but <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> removed the option to share these <a href="https://en.wikipedia.org/wiki/Coordinate_system">coordinates</a> in mid-2019. Moreover, there is reason to suspect that a large share of the provided coordinates did not correspond to <a href="https://en.wikipedia.org/wiki/Global_Positioning_System">GPS coordinates</a> of the user even before that. In this paper, we explain the situation and the 2019 policy change and shed light on the various options of still obtaining location information from <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a>. We provide usage statistics including changes over time, and analyze what the removal of exact coordinates means for various common research tasks performed with Twitter data. Finally, we make suggestions for future research requiring geolocated tweets.</abstract>
      <url hash="80627891">2021.wnut-1.24</url>
      <bibkey>kruspe-etal-2021-changes</bibkey>
      <doi>10.18653/v1/2021.wnut-1.24</doi>
    </paper>
    <paper id="32">
      <title>Coping with Noisy Training Data Labels in Paraphrase Detection</title>
      <author><first>Teemu</first><last>Vahtola</last></author>
      <author><first>Mathias</first><last>Creutz</last></author>
      <author><first>Eetu</first><last>Sjöblom</last></author>
      <author><first>Sami</first><last>Itkonen</last></author>
      <pages>291–296</pages>
      <abstract>We present new state-of-the-art benchmarks for <a href="https://en.wikipedia.org/wiki/Paraphrase_detection">paraphrase detection</a> on all six languages in the Opusparcus sentential paraphrase corpus : <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Finnish_language">Finnish</a>, <a href="https://en.wikipedia.org/wiki/French_language">French</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a>, <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a>, and <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a>. We reach these <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> by fine-tuning BERT. The best results are achieved on smaller and cleaner subsets of the training sets than was observed in previous research. Additionally, we study a translation-based approach that is competitive for the languages with more limited and noisier training data.</abstract>
      <url hash="d302747c">2021.wnut-1.32</url>
      <bibkey>vahtola-etal-2021-coping</bibkey>
      <doi>10.18653/v1/2021.wnut-1.32</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opusparcus">Opusparcus</pwcdataset>
    </paper>
    <paper id="35">
      <title>Detecting Cross-Geographic Biases in Toxicity Modeling on <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a></title>
      <author><first>Sayan</first><last>Ghosh</last></author>
      <author><first>Dylan</first><last>Baker</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <pages>313–328</pages>
      <abstract>Online social media platforms increasingly rely on Natural Language Processing (NLP) techniques to detect abusive content at scale in order to mitigate the harms it causes to their users. However, these techniques suffer from various sampling and association biases present in training data, often resulting in sub-par performance on content relevant to <a href="https://en.wikipedia.org/wiki/Social_exclusion">marginalized groups</a>, potentially furthering disproportionate harms towards them. Studies on such biases so far have focused on only a handful of axes of disparities and subgroups that have annotations / lexicons available. Consequently, biases concerning non-Western contexts are largely ignored in the literature. In this paper, we introduce a weakly supervised method to robustly detect lexical biases in broader geo-cultural contexts. Through a case study on a publicly available toxicity detection model, we demonstrate that our method identifies salient groups of cross-geographic errors, and, in a follow up, demonstrate that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts. We also conduct analysis of a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained on a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> with ground truth labels to better understand these biases, and present preliminary mitigation experiments.</abstract>
      <url hash="66cc0450">2021.wnut-1.35</url>
      <bibkey>ghosh-etal-2021-detecting</bibkey>
      <doi>10.18653/v1/2021.wnut-1.35</doi>
    </paper>
    <paper id="36">
      <title>Detection of Puffery on the <a href="https://en.wikipedia.org/wiki/English_Wikipedia">English Wikipedia</a><fixed-case>E</fixed-case>nglish <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Amanda</first><last>Bertsch</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <pages>329–333</pages>
      <abstract>On <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, an online crowdsourced encyclopedia, volunteers enforce the encyclopedia’s editorial policies. Wikipedia’s policy on maintaining a neutral point of view has inspired recent research on bias detection, including <a href="https://en.wikipedia.org/wiki/Weasel_word">weasel words</a> and <a href="https://en.wikipedia.org/wiki/Hedge_(finance)">hedges</a>. Yet to date, little work has been done on identifying <a href="https://en.wikipedia.org/wiki/Puffery">puffery</a>, phrases that are overly positive without a verifiable source. We demonstrate that collecting training data for this task requires some care, and construct a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> by combining <a href="https://en.wikipedia.org/wiki/Wikipedia_community">Wikipedia editorial annotations</a> and <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval techniques</a>. We compare several approaches to predicting puffery, and achieve 0.963 <a href="https://en.wikipedia.org/wiki/F-number">f1 score</a> by incorporating <a href="https://en.wikipedia.org/wiki/Citation">citation features</a> into a RoBERTa model. Finally, we demonstrate how to integrate our model with Wikipedia’s public infrastructure to give back to the Wikipedia editor community.</abstract>
      <url hash="17a682c2">2021.wnut-1.36</url>
      <bibkey>bertsch-bethard-2021-detection</bibkey>
      <doi>10.18653/v1/2021.wnut-1.36</doi>
      <pwccode url="https://github.com/abertsch72/wikipedia-puffery-detection" additional="false">abertsch72/wikipedia-puffery-detection</pwccode>
    </paper>
    <paper id="37">
      <title>Robustness and Sensitivity of BERT Models Predicting Alzheimer’s Disease from Text<fixed-case>BERT</fixed-case> Models Predicting <fixed-case>A</fixed-case>lzheimer’s Disease from Text</title>
      <author><first>Jekaterina</first><last>Novikova</last></author>
      <pages>334–339</pages>
      <abstract>Understanding robustness and <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">sensitivity</a> of BERT models predicting Alzheimer’s disease from text is important for both developing better classification models and for understanding their capabilities and limitations. In this paper, we analyze how a controlled amount of desired and undesired text alterations impacts performance of BERT. We show that BERT is robust to natural linguistic variations in text. On the other hand, we show that BERT is not sensitive to removing clinically important information from text.</abstract>
      <url hash="02c68688">2021.wnut-1.37</url>
      <bibkey>novikova-2021-robustness</bibkey>
      <doi>10.18653/v1/2021.wnut-1.37</doi>
    </paper>
    <paper id="39">
      <title>CIDEr-R : Robust Consensus-based Image Description Evaluation<fixed-case>CIDE</fixed-case>r-<fixed-case>R</fixed-case>: Robust Consensus-based Image Description Evaluation</title>
      <author><first>Gabriel</first><last>Oliveira dos Santos</last></author>
      <author><first>Esther Luna</first><last>Colombini</last></author>
      <author><first>Sandra</first><last>Avila</last></author>
      <pages>351–360</pages>
      <abstract>This paper shows that CIDEr-D, a traditional evaluation metric for image description, does not work properly on datasets where the number of words in the sentence is significantly greater than those in the MS COCO Captions dataset. We also show that CIDEr-D has performance hampered by the lack of multiple reference sentences and high variance of <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence length</a>. To bypass this problem, we introduce CIDEr-R, which improves CIDEr-D, making it more flexible in dealing with <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> with high sentence length variance. We demonstrate that CIDEr-R is more accurate and closer to human judgment than CIDEr-D ; CIDEr-R is more robust regarding the number of available references. Our results reveal that using Self-Critical Sequence Training to optimize CIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized, the generated captions’ length tends to be similar to the reference length. However, the <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> also repeat several times the same word to increase the <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence length</a>.</abstract>
      <url hash="1e750212">2021.wnut-1.39</url>
      <bibkey>oliveira-dos-santos-etal-2021-cider</bibkey>
      <doi>10.18653/v1/2021.wnut-1.39</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco-captions">COCO Captions</pwcdataset>
    </paper>
    <paper id="42">
      <title>Improved Multilingual Language Model Pretraining for Social Media Text via Translation Pair Prediction</title>
      <author><first>Shubhanshu</first><last>Mishra</last></author>
      <author><first>Aria</first><last>Haghighi</last></author>
      <pages>381–388</pages>
      <abstract>We evaluate a simple approach to improving zero-shot multilingual transfer of mBERT on social media corpus by adding a pretraining task called translation pair prediction (TPP), which predicts whether a pair of cross-lingual texts are a valid translation. Our approach assumes access to translations (exact or approximate) between source-target language pairs, where we fine-tune a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on source language task data and evaluate the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> in the target language. In particular, we focus on language pairs where <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> is difficult for mBERT : those where source and target languages are different in script, <a href="https://en.wikipedia.org/wiki/Vocabulary">vocabulary</a>, and <a href="https://en.wikipedia.org/wiki/Linguistic_typology">linguistic typology</a>. We show improvements from TPP pretraining over mBERT alone in zero-shot transfer from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>, <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, and <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a> on two social media tasks : NER (a 37 % average relative improvement in F1 across target languages) and sentiment classification (12 % relative improvement in F1) on social media text, while also benchmarking on a non-social media task of Universal Dependency POS tagging (6.7 % relative improvement in accuracy). Our results are promising given the lack of social media bitext corpus. Our code can be found at : https://github.com/twitter-research/multilingual-alignment-tpp.</abstract>
      <url hash="d9d16d6b">2021.wnut-1.42</url>
      <bibkey>mishra-haghighi-2021-improved</bibkey>
      <doi>10.18653/v1/2021.wnut-1.42</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="46">
      <title>Character Transformations for Non-Autoregressive GEC Tagging<fixed-case>GEC</fixed-case> Tagging</title>
      <author><first>Milan</first><last>Straka</last></author>
      <author><first>Jakub</first><last>Náplava</last></author>
      <author><first>Jana</first><last>Straková</last></author>
      <pages>417–422</pages>
      <abstract>We propose a character-based non-autoregressive GEC approach, with automatically generated character transformations. Recently, per-word classification of correction edits has proven an efficient, parallelizable alternative to current encoder-decoder GEC systems. We show that word replacement edits may be suboptimal and lead to explosion of rules for <a href="https://en.wikipedia.org/wiki/Spelling">spelling</a>, diacritization and errors in morphologically rich languages, and propose a method for generating character transformations from GEC corpus. Finally, we train character transformation models for <a href="https://en.wikipedia.org/wiki/Czech_language">Czech</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a> and <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a>, reaching solid results and dramatic speedup compared to autoregressive systems. The source code is released at https://github.com/ufal/wnut2021_character_transformations_gec.</abstract>
      <url hash="23583507">2021.wnut-1.46</url>
      <bibkey>straka-etal-2021-character</bibkey>
      <doi>10.18653/v1/2021.wnut-1.46</doi>
      <pwccode url="https://github.com/ufal/wnut2021_character_transformations_gec" additional="false">ufal/wnut2021_character_transformations_gec</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/akces-gec">AKCES-GEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
    </paper>
    <paper id="47">
      <title>Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?</title>
      <author><first>Arij</first><last>Riabi</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>423–436</pages>
      <abstract>Recent impressive improvements in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages remains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Arabic written using an extension of the <a href="https://en.wikipedia.org/wiki/Latin_script">Latin script</a>, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a> and dependency parsing to that of monolingual and multilingual models. We show that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> in low-resource and high language variability set- tings.</abstract>
      <url hash="0dee8f63">2021.wnut-1.47</url>
      <bibkey>riabi-etal-2021-character</bibkey>
      <doi>10.18653/v1/2021.wnut-1.47</doi>
    </paper>
    <paper id="48">
      <title>Something Something Hota Hai ! An Explainable Approach towards <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> on Indian Code-Mixed Data<fixed-case>I</fixed-case>ndian Code-Mixed Data</title>
      <author><first>Aman</first><last>Priyanshu</last></author>
      <author><first>Aleti</first><last>Vardhan</last></author>
      <author><first>Sudarshan</first><last>Sivakumar</last></author>
      <author><first>Supriti</first><last>Vijay</last></author>
      <author><first>Nipuna</first><last>Chhabra</last></author>
      <pages>437–444</pages>
      <abstract>The increasing use of social media sites in countries like India has given rise to large volumes of code-mixed data. Sentiment analysis of this <a href="https://en.wikipedia.org/wiki/Data">data</a> can provide integral insights into people’s perspectives and opinions. Code-mixed data is often noisy in nature due to multiple spellings for the same word, lack of definite order of words in a sentence, and random abbreviations. Thus, working with code-mixed data is more challenging than monolingual data. Interpreting a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s predictions allows us to determine the <a href="https://en.wikipedia.org/wiki/Robust_statistics">robustness</a> of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> against different forms of <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a>. In this paper, we propose a <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> to integrate explainable approaches into code-mixed sentiment analysis. By interpreting the predictions of <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis models</a> we evaluate how well the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is able to adapt to the implicit noises present in code-mixed data.</abstract>
      <url hash="8350c695">2021.wnut-1.48</url>
      <bibkey>priyanshu-etal-2021-something</bibkey>
      <doi>10.18653/v1/2021.wnut-1.48</doi>
    </paper>
    <paper id="49">
      <title>BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets<fixed-case>BERT</fixed-case>weet<fixed-case>FR</fixed-case> : Domain Adaptation of Pre-Trained Language Models for <fixed-case>F</fixed-case>rench Tweets</title>
      <author><first>Yanzhu</first><last>Guo</last></author>
      <author><first>Virgile</first><last>Rennard</last></author>
      <author><first>Christos</first><last>Xypolopoulos</last></author>
      <author><first>Michalis</first><last>Vazirgiannis</last></author>
      <pages>445–450</pages>
      <abstract>We introduce BERTweetFR, the first large-scale pre-trained language model for French tweets. Our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is initialised using a general-domain French language model CamemBERT which follows the base architecture of BERT. Experiments show that BERTweetFR outperforms all previous general-domain French language models on two downstream Twitter NLP tasks of offensiveness identification and <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> used in the offensiveness detection task is first created and annotated by our team, filling in the gap of such analytic datasets in <a href="https://en.wikipedia.org/wiki/French_language">French</a>. We make our model publicly available in the transformers library with the aim of promoting future research in analytic tasks for French tweets.</abstract>
      <url hash="594c8dad">2021.wnut-1.49</url>
      <bibkey>guo-etal-2021-bertweetfr</bibkey>
      <doi>10.18653/v1/2021.wnut-1.49</doi>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="50">
      <title>To What Extent Does Lexical Normalization Help English-as-a-Second Language Learners to Read Noisy English Texts?<fixed-case>E</fixed-case>nglish-as-a-Second Language Learners to Read Noisy <fixed-case>E</fixed-case>nglish Texts?</title>
      <author><first>Yo</first><last>Ehara</last></author>
      <pages>451–456</pages>
      <abstract>How difficult is it for English-as-a-second language (ESL) learners to read noisy English texts? Do <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">ESL learners</a> need lexical normalization to read noisy English texts? These questions may also affect community formation on <a href="https://en.wikipedia.org/wiki/Social_networking_service">social networking sites</a> where differences can be attributed to <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">ESL learners</a> and <a href="https://en.wikipedia.org/wiki/First_language">native English speakers</a>. However, few studies have addressed these questions. To this end, we built highly accurate readability assessors to evaluate the readability of texts for <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">ESL learners</a>. We then applied these assessors to noisy English texts to further assess the readability of the texts. The experimental results showed that although intermediate-level ESL learners can read most noisy English texts in the first place, lexical normalization significantly improves the readability of noisy English texts for <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">ESL learners</a>.</abstract>
      <url hash="4c6e5af1">2021.wnut-1.50</url>
      <bibkey>ehara-2021-extent-lexical</bibkey>
      <doi>10.18653/v1/2021.wnut-1.50</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopenglish">OneStopEnglish</pwcdataset>
    </paper>
    <paper id="51">
      <title>Multilingual Sequence Labeling Approach to solve Lexical Normalization</title>
      <author><first>Divesh</first><last>Kubal</last></author>
      <author><first>Apurva</first><last>Nagvenkar</last></author>
      <pages>457–464</pages>
      <abstract>The task of converting a <a href="https://en.wikipedia.org/wiki/Nonstandard_dialect">nonstandard text</a> to a standard and readable text is known as lexical normalization. Almost all the Natural Language Processing (NLP) applications require the text data in normalized form to build quality task-specific models. Hence, lexical normalization has been proven to improve the performance of numerous natural language processing tasks on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. This study aims to solve the problem of Lexical Normalization by formulating the Lexical Normalization task as a Sequence Labeling problem. This paper proposes a sequence labeling approach to solve the problem of Lexical Normalization in combination with the word-alignment technique. The goal is to use a single model to normalize text in various languages namely <a href="https://en.wikipedia.org/wiki/Croatian_language">Croatian</a>, <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>, <a href="https://en.wikipedia.org/wiki/Dutch_language">Dutch</a>, <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Indonesian_language">Indonesian-English</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a>, <a href="https://en.wikipedia.org/wiki/Italian_language">Italian</a>, <a href="https://en.wikipedia.org/wiki/Serbian_language">Serbian</a>, <a href="https://en.wikipedia.org/wiki/Slovene_language">Slovenian</a>, <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>, <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a>, and <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish-German</a>. This is a shared task in 2021 The 7th Workshop on Noisy User-generated Text (W-NUT) in which the participants are expected to create a system / model that performs lexical normalization, which is the translation of non-canonical texts into their canonical equivalents, comprising data from over 12 languages. The proposed single multilingual model achieves an overall ERR score of 43.75 on intrinsic evaluation and an overall Labeled Attachment Score (LAS) score of 63.12 on extrinsic evaluation. Further, the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> achieves the highest Error Reduction Rate (ERR) score of 61.33 among the participants in the shared task.</abstract>
      <url hash="8be67a39">2021.wnut-1.51</url>
      <bibkey>kubal-nagvenkar-2021-multilingual</bibkey>
      <doi>10.18653/v1/2021.wnut-1.51</doi>
    </paper>
    <paper id="52">
      <title>Sesame Street to Mount Sinai : BERT-constrained character-level Moses models for multilingual lexical normalization<fixed-case>BERT</fixed-case>-constrained character-level <fixed-case>M</fixed-case>oses models for multilingual lexical normalization</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <pages>465–472</pages>
      <abstract>This paper describes the HEL-LJU submissions to the MultiLexNorm shared task on multilingual lexical normalization. Our system is based on a BERT token classification preprocessing step, where for each token the type of the necessary transformation is predicted (none, uppercase, lowercase, capitalize, modify), and a character-level SMT step where the text is translated from original to normalized given the BERT-predicted transformation constraints. For some languages, depending on the results on development data, the <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data</a> was extended by back-translating OpenSubtitles data. In the final ordering of the ten participating teams, the HEL-LJU team has taken the second place, scoring better than the previous <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>.</abstract>
      <url hash="255f5f11">2021.wnut-1.52</url>
      <bibkey>scherrer-ljubesic-2021-sesame</bibkey>
      <doi>10.18653/v1/2021.wnut-1.52</doi>
    </paper>
    <paper id="53">
      <title>Sequence-to-Sequence Lexical Normalization with Multilingual Transformers</title>
      <author><first>Ana-Maria</first><last>Bucur</last></author>
      <author><first>Adrian</first><last>Cosma</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <pages>473–482</pages>
      <abstract>Current benchmark tasks for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> contain text that is qualitatively different from the text used in informal day to day digital communication. This discrepancy has led to severe performance degradation of state-of-the-art NLP models when fine-tuned on real-world data. One way to resolve this issue is through lexical normalization, which is the process of transforming non-standard text, usually from <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, into a more standardized form. In this work, we propose a sentence-level sequence-to-sequence model based on mBART, which frames the problem as a <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation problem</a>. As the noisy text is a pervasive problem across languages, not just <a href="https://en.wikipedia.org/wiki/English_language">English</a>, we leverage the multi-lingual pre-training of mBART to fine-tune it to our data. While current approaches mainly operate at the word or subword level, we argue that this approach is straightforward from a technical standpoint and builds upon existing pre-trained transformer networks. Our results show that while word-level, intrinsic, performance evaluation is behind other methods, our model improves performance on extrinsic, downstream tasks through <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalization</a> compared to models operating on raw, unprocessed, social media text.</abstract>
      <url hash="1fbf78c4">2021.wnut-1.53</url>
      <bibkey>bucur-etal-2021-sequence</bibkey>
      <doi>10.18653/v1/2021.wnut-1.53</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="54">
      <title>FAL at MultiLexNorm 2021 : Improving Multilingual Lexical Normalization by Fine-tuning ByT5<fixed-case>ÚFAL</fixed-case> at <fixed-case>M</fixed-case>ulti<fixed-case>L</fixed-case>ex<fixed-case>N</fixed-case>orm 2021: Improving Multilingual Lexical Normalization by Fine-tuning <fixed-case>B</fixed-case>y<fixed-case>T</fixed-case>5</title>
      <author><first>David</first><last>Samuel</last></author>
      <author><first>Milan</first><last>Straka</last></author>
      <pages>483–492</pages>
      <abstract>We present the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021 (van der Goot et al., 2021a), which evaluates lexical-normalization systems on 12 social media datasets in 11 languages. We base our solution on a pre-trained byte-level language model, ByT5 (Xue et al., 2021a), which we further pre-train on synthetic data and then fine-tune on authentic normalization data. Our system achieves the best performance by a wide margin in intrinsic evaluation, and also the best performance in extrinsic evaluation through dependency parsing. The source code is released at https://github.com/ufal/multilexnorm2021 and the fine-tuned models at https://huggingface.co/ufal.</abstract>
      <url hash="ba0bc551">2021.wnut-1.54</url>
      <bibkey>samuel-straka-2021-ufal</bibkey>
      <doi>10.18653/v1/2021.wnut-1.54</doi>
      <pwccode url="https://github.com/ufal/multilexnorm2021" additional="false">ufal/multilexnorm2021</pwccode>
    </paper>
    </volume>
</collection>