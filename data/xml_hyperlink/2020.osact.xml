<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.osact">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection</booktitle>
      <editor><first>Hend</first><last>Al-Khalifa</last></editor>
      <editor><first>Walid</first><last>Magdy</last></editor>
      <editor><first>Kareem</first><last>Darwish</last></editor>
      <editor><first>Tamer</first><last>Elsayed</last></editor>
      <editor><first>Hamdy</first><last>Mubarak</last></editor>
      <publisher>European Language Resource Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-51-1</isbn>
    </meta>
    <frontmatter>
      <url hash="60cb224b">2020.osact-1.0</url>
      <bibkey>osact-2020-open</bibkey>
    </frontmatter>
    <paper id="2">
      <title>AraBERT : Transformer-based Model for Arabic Language Understanding<fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case>: Transformer-based Model for <fixed-case>A</fixed-case>rabic Language Understanding</title>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Fady</first><last>Baly</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>9–15</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Arabic">Arabic language</a> is a morphologically rich language with relatively few resources and a less explored syntax compared to <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">language understanding</a>, provided they are pre-trained on a very large corpus. Such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the <a href="https://en.wikipedia.org/wiki/Arabic">Arabic language</a> in the pursuit of achieving the same success that BERT did for the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>. The performance of AraBERT is compared to multilingual BERT from <a href="https://en.wikipedia.org/wiki/Google">Google</a> and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/araBERT hoping to encourage research and applications for Arabic NLP.</abstract>
      <url hash="31a4db61">2020.osact-1.2</url>
      <language>eng</language>
      <bibkey>antoun-etal-2020-arabert</bibkey>
      <pwccode url="https://github.com/aub-mind/araBERT" additional="true">aub-mind/araBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arcd">ARCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/astd">ASTD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arsentd-lev">ArSentD-LEV</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hard">HARD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/labr">LABR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="5">
      <title>From Arabic Sentiment Analysis to Sarcasm Detection : The ArSarcasm Dataset<fixed-case>A</fixed-case>rabic Sentiment Analysis to Sarcasm Detection: The <fixed-case>A</fixed-case>r<fixed-case>S</fixed-case>arcasm Dataset</title>
      <author><first>Ibrahim</first><last>Abu Farha</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>32–39</pages>
      <abstract>Sarcasm is one of the main challenges for sentiment analysis systems. Its <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> comes from the <a href="https://en.wikipedia.org/wiki/Opinion">expression of opinion</a> using implicit indirect phrasing. In this paper, we present ArSarcasm, an Arabic sarcasm detection dataset, which was created through the reannotation of available Arabic sentiment analysis datasets. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> contains 10,547 tweets, 16 % of which are <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcastic</a>. In addition to <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> the <a href="https://en.wikipedia.org/wiki/Data">data</a> was annotated for sentiment and dialects. Our analysis shows the highly subjective nature of these tasks, which is demonstrated by the shift in sentiment labels based on annotators’ biases. Experiments show the degradation of state-of-the-art <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysers</a> when faced with sarcastic content. Finally, we train a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning model</a> for sarcasm detection using BiLSTM. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves an <a href="https://en.wikipedia.org/wiki/F-number">F1 score</a> of 0.46, which shows the challenging nature of the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, and should act as a basic baseline for future research on our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>.</abstract>
      <url hash="be33a23f">2020.osact-1.5</url>
      <language>eng</language>
      <bibkey>abu-farha-magdy-2020-arabic</bibkey>
    </paper>
    <paper id="9">
      <title>ALT Submission for OSACT Shared Task on Offensive Language Detection<fixed-case>ALT</fixed-case> Submission for <fixed-case>OSACT</fixed-case> Shared Task on Offensive Language Detection</title>
      <author><first>Sabit</first><last>Hassan</last></author>
      <author><first>Younes</first><last>Samih</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Ammar</first><last>Rashed</last></author>
      <author><first>Shammur Absar</first><last>Chowdhury</last></author>
      <pages>61–65</pages>
      <abstract>In this paper, we describe our efforts at OSACT Shared Task on Offensive Language Detection. The shared <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> consists of two subtasks : offensive language detection (Subtask A) and hate speech detection (Subtask B). For offensive language detection, a system combination of Support Vector Machines (SVMs) and <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Neural Networks (DNNs)</a> achieved the best results on development set, which ranked 1st in the official results for Subtask A with F1-score of 90.51 % on the test set. For hate speech detection, DNNs were less effective and a system combination of multiple SVMs with different parameters achieved the best results on development set, which ranked 4th in official results for Subtask B with F1-macro score of 80.63 % on the test set.</abstract>
      <url hash="43a38c26">2020.osact-1.9</url>
      <language>eng</language>
      <bibkey>hassan-etal-2020-alt</bibkey>
    </paper>
    <paper id="10">
      <title>ASU_OPTO at OSACT4-Offensive Language Detection for Arabic text<fixed-case>ASU</fixed-case>_<fixed-case>OPTO</fixed-case> at <fixed-case>OSACT</fixed-case>4 - Offensive Language Detection for <fixed-case>A</fixed-case>rabic text</title>
      <author><first>Amr</first><last>Keleg</last></author>
      <author><first>Samhaa R.</first><last>El-Beltagy</last></author>
      <author><first>Mahmoud</first><last>Khalil</last></author>
      <pages>66–70</pages>
      <abstract>In the past years, toxic comments and offensive speech are polluting the internet and manual inspection of these comments is becoming a tiresome task to manage. Having a <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning based model</a> that is able to filter offensive Arabic content is of high need nowadays. In this paper, we describe the model that was submitted to the Shared Task on Offensive Language Detection that is organized by (The 4th Workshop on Open-Source Arabic Corpora and Processing Tools). Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> makes use transformer based model (BERT) to detect offensive content. We came in the fourth place in subtask A (detecting Offensive Speech) and in the third place in subtask B (detecting Hate Speech).</abstract>
      <url hash="7727dc00">2020.osact-1.10</url>
      <language>eng</language>
      <bibkey>keleg-etal-2020-asu</bibkey>
    </paper>
    <paper id="16">
      <title>Multi-Task Learning using AraBert for Offensive Language Detection<fixed-case>A</fixed-case>ra<fixed-case>B</fixed-case>ert for Offensive Language Detection</title>
      <author><first>Marc</first><last>Djandji</last></author>
      <author><first>Fady</first><last>Baly</last></author>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>97–101</pages>
      <abstract>The use of <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a> has become more prevalent, which has provided tremendous opportunities for people to connect but has also opened the door for misuse with the spread of <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a> and <a href="https://en.wikipedia.org/wiki/Profanity">offensive language</a>. This phenomenon has been driving more and more people to more extreme reactions and online aggression, sometimes causing physical harm to individuals or groups of people. There is a need to control and prevent such misuse of <a href="https://en.wikipedia.org/wiki/Social_media">online social media</a> through automatic detection of profane language. The shared task on Offensive Language Detection at the OSACT4 has aimed at achieving state of art profane language detection methods for Arabic social media. Our team BERTologists tackled this problem by leveraging state of the art pretrained Arabic language model, AraBERT, that we augment with the addition of <a href="https://en.wikipedia.org/wiki/Multi-task_learning">Multi-task learning</a> to enable our model to learn efficiently from little data. Our Multitask AraBERT approach achieved the second place in both subtasks A &amp; B, which shows that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performs consistently across different tasks.</abstract>
      <url hash="72541388">2020.osact-1.16</url>
      <language>eng</language>
      <bibkey>djandji-etal-2020-multi</bibkey>
    </paper>
    </volume>
</collection>