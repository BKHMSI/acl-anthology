<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.sigmorphon">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
      <editor><first>Garrett</first><last>Nicolai</last></editor>
      <editor><first>Kyle</first><last>Gorman</last></editor>
      <editor><first>Ryan</first><last>Cotterell</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="52a1928d">2020.sigmorphon-1</url>
    </meta>
    <frontmatter>
      <url hash="348ca84d">2020.sigmorphon-1.0</url>
      <bibkey>sigmorphon-2020-sigmorphon</bibkey>
    </frontmatter>
    <paper id="6">
      <title>The CMU-LTI submission to the SIGMORPHON 2020 Shared Task 0 : Language-Specific Cross-Lingual Transfer<fixed-case>CMU</fixed-case>-<fixed-case>LTI</fixed-case> submission to the <fixed-case>SIGMORPHON</fixed-case> 2020 Shared Task 0: Language-Specific Cross-Lingual Transfer</title>
      <author><first>Nikitha</first><last>Murikinati</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>79–84</pages>
      <abstract>This paper describes the CMU-LTI submission to the SIGMORPHON 2020 Shared Task 0 on typologically diverse morphological inflection. The (unrestricted) submission uses the cross-lingual approach of our last year’s winning submission (Anastasopoulos and Neubig, 2019), but adapted to use specific transfer languages for each test language. Our <a href="https://en.wikipedia.org/wiki/System">system</a>, with fixed non-tuned hyperparameters, achieved a macro-averaged accuracy of 80.65 ranking 20th among 31 systems, but it was still tied for best <a href="https://en.wikipedia.org/wiki/System">system</a> in 25 of the 90 total languages.</abstract>
      <url hash="78fecf87">2020.sigmorphon-1.6</url>
      <doi>10.18653/v1/2020.sigmorphon-1.6</doi>
      <bibkey>murikinati-anastasopoulos-2020-cmu</bibkey>
    </paper>
    <paper id="7">
      <title>Grapheme-to-Phoneme Conversion with a Multilingual Transformer Model</title>
      <author><first>Omnia</first><last>ElSaadany</last></author>
      <author><first>Benjamin</first><last>Suter</last></author>
      <pages>85–89</pages>
      <abstract>In this paper, we describe our three submissions to the SIGMORPHON 2020 shared task 1 on grapheme-to-phoneme conversion for 15 languages. We experimented with a single multilingual transformer model. We observed that the multilingual model achieves results on par with our separately trained <a href="https://en.wikipedia.org/wiki/Monolingualism">monolingual models</a> and is even able to avoid a few of the errors made by the <a href="https://en.wikipedia.org/wiki/Monolingualism">monolingual models</a>.</abstract>
      <url hash="f2153579">2020.sigmorphon-1.7</url>
      <doi>10.18653/v1/2020.sigmorphon-1.7</doi>
      <bibkey>elsaadany-suter-2020-grapheme</bibkey>
    </paper>
    <paper id="9">
      <title>The IMSCUBoulder System for the SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm Completion<fixed-case>IMS</fixed-case>–<fixed-case>CUB</fixed-case>oulder System for the <fixed-case>SIGMORPHON</fixed-case> 2020 Shared Task on Unsupervised Morphological Paradigm Completion</title>
      <author><first>Manuel</first><last>Mager</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>99–105</pages>
      <abstract>In this paper, we present the systems of the University of Stuttgart IMS and the University of Colorado Boulder (IMSCUBoulder) for SIGMORPHON 2020 Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). The task consists of generating the <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological paradigms</a> of a set of lemmas, given only the lemmas themselves and unlabeled text. Our proposed <a href="https://en.wikipedia.org/wiki/System">system</a> is a modified version of the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> introduced together with the <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>. In particular, we experiment with substituting the inflection generation component with an LSTM sequence-to-sequence model and an LSTM pointer-generator network. Our pointer-generator system obtains the best score of all seven submitted systems on average over all languages, and outperforms the official baseline, which was best overall, on <a href="https://en.wikipedia.org/wiki/Bulgarian_language">Bulgarian</a> and <a href="https://en.wikipedia.org/wiki/Kannada">Kannada</a>.</abstract>
      <url hash="4704eca8">2020.sigmorphon-1.9</url>
      <doi>10.18653/v1/2020.sigmorphon-1.9</doi>
      <bibkey>mager-kann-2020-ims</bibkey>
    </paper>
    <paper id="14">
      <title>Exploring Neural Architectures And Techniques For Typologically Diverse Morphological Inflection</title>
      <author><first>Pratik</first><last>Jayarao</last></author>
      <author><first>Siddhanth</first><last>Pillay</last></author>
      <author><first>Pranav</first><last>Thombre</last></author>
      <author><first>Aditi</first><last>Chaudhary</last></author>
      <pages>128–136</pages>
      <abstract>Morphological inflection in low resource languages is critical to augment existing corpora in Low Resource Languages, which can help develop several applications in these languages with very good social impact. We describe our attention-based encoder-decoder approach that we implement using <a href="https://en.wikipedia.org/wiki/Light-emitting_diode">LSTMs</a> and <a href="https://en.wikipedia.org/wiki/Transformers_(video_game)">Transformers</a> as the base units. We also describe the ancillary techniques that we experimented with, such as <a href="https://en.wikipedia.org/wiki/Hallucination">hallucination</a>, language vector injection, sparsemax loss and adversarial language network alongside our approach to select the related language(s) for training. We present the results we generated on the constrained as well as unconstrained SIGMORPHON 2020 dataset (CITATION). One of the primary goals of our paper was to study the contribution varied components described above towards the performance of our <a href="https://en.wikipedia.org/wiki/System">system</a> and perform an analysis on the same.</abstract>
      <url hash="1ac67b74">2020.sigmorphon-1.14</url>
      <doi>10.18653/v1/2020.sigmorphon-1.14</doi>
      <bibkey>jayarao-etal-2020-exploring</bibkey>
    </paper>
    <paper id="17">
      <title>Leveraging Principal Parts for Morphological Inflection</title>
      <author><first>Ling</first><last>Liu</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>153–161</pages>
      <abstract>This paper presents the submission by the CU Ling team from the University of Colorado to SIGMORPHON 2020 shared task 0 on morphological inflection. The task is to generate the target <a href="https://en.wikipedia.org/wiki/Inflection">inflected word form</a> given a <a href="https://en.wikipedia.org/wiki/Lemma_(morphology)">lemma form</a> and a target <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphosyntactic description</a>. Our <a href="https://en.wikipedia.org/wiki/System">system</a> uses the Transformer architecture. Our overall approach is to treat the morphological inflection task as a paradigm cell filling problem and to design the system to leverage principal parts information for better morphological inflection when the training data is limited. We train one <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for each language separately without <a href="https://en.wikipedia.org/wiki/Data">external data</a>. The overall average performance of our submission ranks the first in both <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">average accuracy</a> and <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein distance</a> from the gold inflection among all submissions including those using external resources.</abstract>
      <url hash="e1277421">2020.sigmorphon-1.17</url>
      <doi>10.18653/v1/2020.sigmorphon-1.17</doi>
      <bibkey>liu-hulden-2020-leveraging</bibkey>
    </paper>
    <paper id="29">
      <title>Multi-Tiered Strictly Local Functions</title>
      <author><first>Phillip</first><last>Burness</last></author>
      <author><first>Kevin</first><last>McMullin</last></author>
      <pages>245–255</pages>
      <abstract>Tier-based Strictly Local functions, as they have so far been defined, are equipped with just a single tier. In light of this fact, they are currently incapable of modelling simultaneous phonological processes that would require different tiers. In this paper we consider whether and how we can allow a single function to operate over more than one tier. We conclude that multiple tiers can and should be permitted, but that the relationships between them must be restricted in some way to avoid overgeneration. The particular restriction that we propose comes in two parts. First, each input element is associated with a set of tiers that on their own can fully determine what the element is mapped to. Second, the set of tiers associated to a given input element must form a strict superset-subset hierarchy. In this way, we can track multiple, related sources of information when deciding how to process a particular input element. We demonstrate that doing so enables simple and intuitive analyses to otherwise challenging <a href="https://en.wikipedia.org/wiki/Phonology">phonological phenomena</a>.</abstract>
      <url hash="da360ed7">2020.sigmorphon-1.29</url>
      <revision id="1" href="2020.sigmorphon-1.29v1" hash="cde00479" />
      <revision id="2" href="2020.sigmorphon-1.29v2" hash="da360ed7" date="2020-07-08">Corrected bibliography entry</revision>
      <doi>10.18653/v1/2020.sigmorphon-1.29</doi>
      <video href="http://slideslive.com/38929874" />
      <bibkey>burness-mcmullin-2020-multi</bibkey>
    </paper>
  </volume>
</collection>