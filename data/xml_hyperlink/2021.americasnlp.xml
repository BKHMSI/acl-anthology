<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.americasnlp">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</booktitle>
      <editor><first>Manuel</first><last>Mager</last></editor>
      <editor><first>Arturo</first><last>Oncevay</last></editor>
      <editor><first>Annette</first><last>Rios</last></editor>
      <editor><first>Ivan Vladimir Meza</first><last>Ruiz</last></editor>
      <editor><first>Alexis</first><last>Palmer</last></editor>
      <editor><first>Graham</first><last>Neubig</last></editor>
      <editor><first>Katharina</first><last>Kann</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.americasnlp-1</url>
    </meta>
    <frontmatter>
      <url hash="beb9eed7">2021.americasnlp-1.0</url>
      <bibkey>americasnlp-2021-natural</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Investigating variation in written forms of Nahuatl using character-based language models<fixed-case>N</fixed-case>ahuatl using character-based language models</title>
      <author><first>Robert</first><last>Pugh</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>21–27</pages>
      <abstract>We describe experiments with character-based language modeling for written variants of Nahuatl. Using a standard LSTM model and publicly available <a href="https://en.wikipedia.org/wiki/Bible_translations">Bible translations</a>, we explore how character language models can be applied to the tasks of estimating mutual intelligibility, identifying genetic similarity, and distinguishing written variants. We demonstrate that these simple <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> are able to capture similarities and differences that have been described in the <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic literature</a>.</abstract>
      <url hash="ba4bb55f">2021.americasnlp-1.3</url>
      <doi>10.18653/v1/2021.americasnlp-1.3</doi>
      <bibkey>pugh-tyers-2021-investigating</bibkey>
      <pwccode url="https://github.com/lguyogiro/nahuatl-variant-charlms-americasnlp" additional="false">lguyogiro/nahuatl-variant-charlms-americasnlp</pwccode>
    </paper>
    <paper id="10">
      <title>Morphological Segmentation for Seneca<fixed-case>S</fixed-case>eneca</title>
      <author><first>Zoey</first><last>Liu</last></author>
      <author><first>Robert</first><last>Jimerson</last></author>
      <author><first>Emily</first><last>Prud’hommeaux</last></author>
      <pages>90–101</pages>
      <abstract>This study takes up the task of low-resource morphological segmentation for <a href="https://en.wikipedia.org/wiki/Seneca_language">Seneca</a>, a critically endangered and morphologically complex Native American language primarily spoken in what is now New York State and <a href="https://en.wikipedia.org/wiki/Ontario">Ontario</a>. The labeled data in our experiments comes from two sources : one digitized from a publicly available grammar book and the other collected from informal sources. We treat these two sources as distinct domains and investigate different evaluation designs for <a href="https://en.wikipedia.org/wiki/Model_selection">model selection</a>. The first design abides by standard practices and evaluate models with the in-domain development set, while the second one carries out evaluation using a development domain, or the out-of-domain development set. Across a series of monolingual and crosslinguistic training settings, our results demonstrate the utility of neural encoder-decoder architecture when coupled with <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>.</abstract>
      <url hash="371b8485">2021.americasnlp-1.10</url>
      <doi>10.18653/v1/2021.americasnlp-1.10</doi>
      <bibkey>liu-etal-2021-morphological</bibkey>
      <pwccode url="https://github.com/zoeyliu18/seneca" additional="false">zoeyliu18/seneca</pwccode>
    </paper>
    <paper id="11">
      <title>Representation of Yine [ Arawak ] Morphology by Finite State Transducer Formalism<fixed-case>Y</fixed-case>ine [<fixed-case>A</fixed-case>rawak] Morphology by Finite State Transducer Formalism</title>
      <author><first>Adriano</first><last>Ingunza Torres</last></author>
      <author><first>John</first><last>Miller</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <author><first>Roberto</first><last>Zariquiey Biondi</last></author>
      <pages>102–112</pages>
      <abstract>We represent the complexity of Yine (Arawak) morphology with a finite state transducer (FST) based morphological analyzer. Yine is a low-resource indigenous polysynthetic Peruvian language spoken by approximately 3,000 people and is classified as ‘definitely endangered’ by UNESCO. We review Yine morphology focusing on <a href="https://en.wikipedia.org/wiki/Morphophonology">morphophonology</a>, <a href="https://en.wikipedia.org/wiki/Possessive">possessive constructions</a> and <a href="https://en.wikipedia.org/wiki/Predicate_(grammar)">verbal predicates</a>. Then we develop FSTs to model these components proposing techniques to solve challenging problems such as complex patterns of incorporating open and closed category arguments. This is a work in progress and we still have more to do in the development and verification of our <a href="https://en.wikipedia.org/wiki/Analyzer">analyzer</a>. Our analyzer will serve both as a tool to better document the <a href="https://en.wikipedia.org/wiki/Yine">Yine language</a> and as a component of natural language processing (NLP) applications such as <a href="https://en.wikipedia.org/wiki/Spell_checker">spell checking</a> and correction.</abstract>
      <url hash="fa1d4d59">2021.americasnlp-1.11</url>
      <attachment type="OptionalSupplementaryCode" hash="3b960847">2021.americasnlp-1.11.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.americasnlp-1.11</doi>
      <bibkey>ingunza-torres-etal-2021-representation</bibkey>
    </paper>
    <paper id="14">
      <title>Expanding Universal Dependencies for <a href="https://en.wikipedia.org/wiki/Polysynthetic_language">Polysynthetic Languages</a> : A Case of St. Lawrence Island Yupik<fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies for Polysynthetic Languages: A Case of <fixed-case>S</fixed-case>t. <fixed-case>L</fixed-case>awrence <fixed-case>I</fixed-case>sland <fixed-case>Y</fixed-case>upik</title>
      <author><first>Hyunji Hayley</first><last>Park</last></author>
      <author><first>Lane</first><last>Schwartz</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>131–142</pages>
      <abstract>This paper describes the development of the first Universal Dependencies (UD) treebank for St. Lawrence Island Yupik, an <a href="https://en.wikipedia.org/wiki/Endangered_language">endangered language</a> spoken in the <a href="https://en.wikipedia.org/wiki/Beringia">Bering Strait region</a>. While the UD guidelines provided a general framework for our annotations, language-specific decisions were made necessary by the rich morphology of the <a href="https://en.wikipedia.org/wiki/Polysynthetic_language">polysynthetic language</a>. Most notably, we annotated a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> at the <a href="https://en.wikipedia.org/wiki/Morpheme">morpheme level</a> as well as the <a href="https://en.wikipedia.org/wiki/Word">word level</a>. The morpheme level annotation was conducted using an existing <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological analyzer</a> and manual disambiguation. By comparing the two resulting annotation schemes, we argue that morpheme-level annotation is essential for polysynthetic languages like St. Lawrence Island Yupik. Word-level annotation results in degenerate trees for some Yupik sentences and often fails to capture syntactic relations that can be manifested at the morpheme level. Dependency parsing experiments provide further support for morpheme-level annotation. Implications for UD annotation of other <a href="https://en.wikipedia.org/wiki/Polysynthetic_language">polysynthetic languages</a> are discussed.</abstract>
      <url hash="49a8ac83">2021.americasnlp-1.14</url>
      <doi>10.18653/v1/2021.americasnlp-1.14</doi>
      <bibkey>park-etal-2021-expanding</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="23">
      <title>Findings of the AmericasNLP 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas<fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2021 Shared Task on Open Machine Translation for Indigenous Languages of the <fixed-case>A</fixed-case>mericas</title>
      <author><first>Manuel</first><last>Mager</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <author><first>Abteen</first><last>Ebrahimi</last></author>
      <author><first>John</first><last>Ortega</last></author>
      <author><first>Annette</first><last>Rios</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Ximena</first><last>Gutierrez-Vasques</last></author>
      <author><first>Luis</first><last>Chiruzzo</last></author>
      <author><first>Gustavo</first><last>Giménez-Lugo</last></author>
      <author><first>Ricardo</first><last>Ramos</last></author>
      <author><first>Ivan Vladimir</first><last>Meza Ruiz</last></author>
      <author><first>Rolando</first><last>Coto-Solano</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <author><first>Elisabeth</first><last>Mager-Hois</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>202–217</pages>
      <abstract>This paper presents the results of the 2021 Shared Task on Open Machine Translation for <a href="https://en.wikipedia.org/wiki/Indigenous_languages_of_the_Americas">Indigenous Languages of the Americas</a>. The shared task featured two independent tracks, and participants submitted <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a> for up to 10 <a href="https://en.wikipedia.org/wiki/Indigenous_language">indigenous languages</a>. Overall, 8 teams participated with a total of 214 submissions. We provided <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training sets</a> consisting of data collected from various sources, as well as manually translated sentences for the <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">development and test sets</a>. An official <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline</a> trained on this <a href="https://en.wikipedia.org/wiki/Data">data</a> was also provided. Team submissions featured a variety of <a href="https://en.wikipedia.org/wiki/Computer_architecture">architectures</a>, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>. The best performing <a href="https://en.wikipedia.org/wiki/System">systems</a> achieved 12.97 ChrF higher than <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>, when averaged across languages.</abstract>
      <url hash="ac069c84">2021.americasnlp-1.23</url>
      <doi>10.18653/v1/2021.americasnlp-1.23</doi>
      <bibkey>mager-etal-2021-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="27">
      <title>The REPU CS’ SpanishQuechua Submission to the AmericasNLP 2021 Shared Task on Open Machine Translation<fixed-case>REPU</fixed-case> <fixed-case>CS</fixed-case>’ <fixed-case>S</fixed-case>panish–<fixed-case>Q</fixed-case>uechua Submission to the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2021 Shared Task on Open Machine Translation</title>
      <author><first>Oscar</first><last>Moreno</last></author>
      <pages>241–247</pages>
      <abstract>We present the submission of REPUcs to the AmericasNLP machine translation shared task for the low resource language pair SpanishQuechua. Our neural machine translation system ranked first in Track two (development set not used for training) and third in Track one (training includes development data). Our contribution is focused on : (i) the collection of new parallel data from different web sources (poems, lyrics, lexicons, handbooks), and (ii) using large SpanishEnglish data for pre-training and then fine-tuning the SpanishQuechua system. This paper describes the new <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpora</a> and our approach in detail.</abstract>
      <url hash="0d16c89a">2021.americasnlp-1.27</url>
      <doi>10.18653/v1/2021.americasnlp-1.27</doi>
      <bibkey>moreno-2021-repu</bibkey>
    </paper>
    <paper id="29">
      <title>The Helsinki submission to the AmericasNLP shared task<fixed-case>H</fixed-case>elsinki submission to the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> shared task</title>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>255–264</pages>
      <abstract>The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs. Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects : (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data. This paper describes our efforts in detail.</abstract>
      <url hash="c92b8108">2021.americasnlp-1.29</url>
      <doi>10.18653/v1/2021.americasnlp-1.29</doi>
      <bibkey>vazquez-etal-2021-helsinki</bibkey>
    </paper>
    </volume>
</collection>