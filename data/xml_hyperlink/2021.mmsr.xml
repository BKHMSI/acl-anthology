<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.mmsr">
  <volume id="1" ingest-date="2021-10-27">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR)</booktitle>
      <editor><first>Lucia</first><last>Donatelli</last></editor>
      <editor><first>Nikhil</first><last>Krishnaswamy</last></editor>
      <editor><first>Kenneth</first><last>Lai</last></editor>
      <editor><first>James</first><last>Pustejovsky</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Groningen, Netherlands (Online)</address>
      <month>June</month>
      <year>2021</year>
      <url hash="d8c17df0">2021.mmsr-1</url>
    </meta>
    <frontmatter>
      <url hash="a804a9ef">2021.mmsr-1.0</url>
      <bibkey>mmsr-2021-multimodal</bibkey>
    </frontmatter>
    <paper id="1">
      <title>What is Multimodality?</title>
      <author><first>Letitia</first><last>Parcalabescu</last></author>
      <author><first>Nils</first><last>Trost</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>1–10</pages>
      <abstract>The last years have shown rapid developments in the field of multimodal machine learning, combining e.g., <a href="https://en.wikipedia.org/wiki/Computer_vision">vision</a>, <a href="https://en.wikipedia.org/wiki/Written_language">text</a> or <a href="https://en.wikipedia.org/wiki/Speech">speech</a>. In this position paper we explain how the <a href="https://en.wikipedia.org/wiki/Field_(mathematics)">field</a> uses outdated definitions of <a href="https://en.wikipedia.org/wiki/Multimodality">multimodality</a> that prove unfit for the <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning era</a>. We propose a new task-relative definition of (multi)modality in the context of multimodal machine learning that focuses on representations and information that are relevant for a given machine learning task. With our new definition of <a href="https://en.wikipedia.org/wiki/Multimodality">multimodality</a> we aim to provide a missing foundation for multimodal research, an important component of language grounding and a crucial milestone towards <a href="https://en.wikipedia.org/wiki/Natural_language_understanding">NLU</a>.</abstract>
      <url hash="0b1961ab">2021.mmsr-1.1</url>
      <bibkey>parcalabescu-etal-2021-multimodality</bibkey>
    </paper>
    <paper id="4">
      <title>Seeing past words : Testing the cross-modal capabilities of pretrained V&amp;L models on counting tasks<fixed-case>V</fixed-case>&amp;<fixed-case>L</fixed-case> models on counting tasks</title>
      <author><first>Letitia</first><last>Parcalabescu</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <author><first>Iacer</first><last>Calixto</last></author>
      <pages>32–44</pages>
      <abstract>We investigate the reasoning ability of pretrained vision and language (V&amp;L) models in two tasks that require multimodal integration : (1) discriminating a correct image-sentence pair from an incorrect one, and (2) counting entities in an image. We evaluate three pretrained V&amp;L models on these tasks : ViLBERT, ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results show that <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> solve task (1) very well, as expected, since all <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are pretrained on task (1). However, none of the pretrained V&amp;L models is able to adequately solve task (2), our counting probe, and they can not generalise to out-of-distribution quantities. We propose a number of explanations for these findings : LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of catastrophic forgetting on task (1). Concerning our results on the counting probe, we find evidence that all models are impacted by dataset bias, and also fail to individuate entities in the visual input. While a selling point of pretrained V&amp;L models is their ability to solve complex tasks, our findings suggest that understanding their reasoning and grounding capabilities requires more targeted investigations on specific phenomena.</abstract>
      <url hash="f4fa7033">2021.mmsr-1.4</url>
      <bibkey>parcalabescu-etal-2021-seeing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/counting-probe">Counting Probe</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual7w">Visual7W</pwcdataset>
    </paper>
    <paper id="5">
      <title>How Vision Affects Language : Comparing Masked Self-Attention in Uni-Modal and Multi-Modal Transformer</title>
      <author><first>Nikolai</first><last>Ilinykh</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>45–55</pages>
      <abstract>The problem of interpretation of knowledge learned by multi-head self-attention in transformers has been one of the central questions in <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a>. However, a lot of work mainly focused on <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> trained for uni-modal tasks, e.g. machine translation. In this paper, we examine masked self-attention in a multi-modal transformer trained for the task of image captioning. In particular, we test whether the multi-modality of the task objective affects the learned attention patterns. Our visualisations of masked self-attention demonstrate that (i) it can learn general linguistic knowledge of the textual input, and (ii) its <a href="https://en.wikipedia.org/wiki/Attention">attention patterns</a> incorporate artefacts from <a href="https://en.wikipedia.org/wiki/Visual_system">visual modality</a> even though it has never accessed it directly. We compare our transformer’s attention patterns with masked attention in distilgpt-2 tested for uni-modal text generation of image captions. Based on the maps of extracted attention weights, we argue that masked self-attention in image captioning transformer seems to be enhanced with semantic knowledge from images, exemplifying joint language-and-vision information in its attention patterns.</abstract>
      <url hash="5e433943">2021.mmsr-1.5</url>
      <bibkey>ilinykh-dobnik-2021-vision</bibkey>
    </paper>
    <paper id="6">
      <title>EMISSOR : A platform for capturing multimodal interactions as Episodic Memories and Interpretations with Situated Scenario-based Ontological References<fixed-case>EMISSOR</fixed-case>: A platform for capturing multimodal interactions as Episodic Memories and Interpretations with Situated Scenario-based Ontological References</title>
      <author><first>Selene</first><last>Baez Santamaria</last></author>
      <author><first>Thomas</first><last>Baier</last></author>
      <author><first>Taewoon</first><last>Kim</last></author>
      <author><first>Lea</first><last>Krause</last></author>
      <author><first>Jaap</first><last>Kruijt</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <pages>56–77</pages>
      <abstract>We present EMISSOR : a platform to capture multimodal interactions as recordings of episodic experiences with explicit referential interpretations that also yield an episodic Knowledge Graph (eKG). The <a href="https://en.wikipedia.org/wiki/Computing_platform">platform</a> stores streams of multiple modalities as <a href="https://en.wikipedia.org/wiki/Parallel_communication">parallel signals</a>. Each signal is segmented and annotated independently with interpretation. Annotations are eventually mapped to explicit identities and relations in the <a href="https://en.wikipedia.org/wiki/Electrocardiography">eKG</a>. As we ground signal segments from different modalities to the same instance representations, we also ground different modalities across each other. Unique to our <a href="https://en.wikipedia.org/wiki/Electrocardiography">eKG</a> is that it accepts different interpretations across modalities, sources and experiences and supports reasoning over conflicting information and uncertainties that may result from multimodal experiences. EMISSOR can record and annotate experiments in virtual and real-world, combine data, evaluate system behavior and their performance for preset goals but also model the accumulation of knowledge and interpretations in the Knowledge Graph as a result of these episodic experiences.</abstract>
      <url hash="6bb1d51b">2021.mmsr-1.6</url>
      <bibkey>baez-santamaria-etal-2021-emissor</bibkey>
      <pwccode url="https://github.com/cltl/EMISSOR" additional="false">cltl/EMISSOR</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/alfred">ALFRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reccon">RECCON</pwcdataset>
    </paper>
    <paper id="8">
      <title>Incremental Unit Networks for Multimodal, Fine-grained Information State Representation</title>
      <author><first>Casey</first><last>Kennington</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>89–94</pages>
      <abstract>We offer a fine-grained information state annotation scheme that follows directly from the Incremental Unit abstract model of dialogue processing when used within a multimodal, co-located, interactive setting. We explain the Incremental Unit model and give an example application using the Localized Narratives dataset, then offer avenues for future research.</abstract>
      <url hash="7a8151bd">2021.mmsr-1.8</url>
      <bibkey>kennington-schlangen-2021-incremental</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/localized-narratives">Localized Narratives</pwcdataset>
    </paper>
    <paper id="9">
      <title>Teaching Arm and Head Gestures to a Humanoid Robot through Interactive Demonstration and Spoken Instruction</title>
      <author><first>Michael</first><last>Brady</last></author>
      <author><first>Han</first><last>Du</last></author>
      <pages>95–101</pages>
      <abstract>We describe work in progress for training a <a href="https://en.wikipedia.org/wiki/Humanoid_robot">humanoid robot</a> to produce iconic arm and head gestures as part of task-oriented dialogue interaction. This involves the development and use of a multimodal dialog manager for non-experts to quickly ‘program’ the robot through <a href="https://en.wikipedia.org/wiki/Speech">speech</a> and <a href="https://en.wikipedia.org/wiki/Visual_system">vision</a>. Using this <a href="https://en.wikipedia.org/wiki/Dialog_manager">dialog manager</a>, videos of <a href="https://en.wikipedia.org/wiki/Gesture_recognition">gesture demonstrations</a> are collected. Motor positions are extracted from these videos to specify motor trajectories where collections of motor trajectories are used to produce robot gestures following a Gaussian mixtures approach. Concluding discussion considers how learned representations may be used for <a href="https://en.wikipedia.org/wiki/Gesture_recognition">gesture recognition</a> by the robot, and how the framework may mature into a system to address language grounding and semantic representation.</abstract>
      <url hash="6e2b7173">2021.mmsr-1.9</url>
      <bibkey>brady-du-2021-teaching</bibkey>
    </paper>
    </volume>
</collection>