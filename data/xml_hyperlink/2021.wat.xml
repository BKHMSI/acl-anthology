<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.wat">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</booktitle>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Hideki</first><last>Nakayama</last></editor>
      <editor><first>Isao</first><last>Goto</last></editor>
      <editor><first>Hideya</first><last>Mino</last></editor>
      <editor><first>Chenchen</first><last>Ding</last></editor>
      <editor><first>Raj</first><last>Dabre</last></editor>
      <editor><first>Anoop</first><last>Kunchukuttan</last></editor>
      <editor><first>Shohei</first><last>Higashiyama</last></editor>
      <editor><first>Hiroshi</first><last>Manabe</last></editor>
      <editor><first>Win Pa</first><last>Pa</last></editor>
      <editor><first>Shantipriya</first><last>Parida</last></editor>
      <editor><first>Ondřej</first><last>Bojar</last></editor>
      <editor><first>Chenhui</first><last>Chu</last></editor>
      <editor><first>Akiko</first><last>Eriguchi</last></editor>
      <editor><first>Kaori</first><last>Abe</last></editor>
      <editor><first>Yusuke</first><last>Oda</last></editor>
      <editor><first>Katsuhito</first><last>Sudoh</last></editor>
      <editor><first>Sadao</first><last>Kurohashi</last></editor>
      <editor><first>Pushpak</first><last>Bhattacharyya</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="949b3b17">2021.wat-1</url>
    </meta>
    <frontmatter>
      <url hash="659551df">2021.wat-1.0</url>
      <bibkey>wat-2021-asian</bibkey>
    </frontmatter>
    <paper id="4">
      <title>NICT’s Neural Machine Translation Systems for the WAT21 Restricted Translation Task<fixed-case>NICT</fixed-case>’s Neural Machine Translation Systems for the <fixed-case>WAT</fixed-case>21 Restricted Translation Task</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>62–67</pages>
      <abstract>This paper describes our <a href="https://en.wikipedia.org/wiki/System">system</a> (Team ID : nictrb) for participating in the WAT’21 restricted machine translation task. In our submitted <a href="https://en.wikipedia.org/wiki/System">system</a>, we designed a new <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training approach</a> for restricted machine translation. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary. With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, as well as <a href="https://en.wikipedia.org/wiki/Mathematical_model">model ensembling</a>, which further improved the final translation performance.</abstract>
      <url hash="d801187b">2021.wat-1.4</url>
      <doi>10.18653/v1/2021.wat-1.4</doi>
      <bibkey>li-etal-2021-nicts</bibkey>
    </paper>
    <paper id="6">
      <title>NECTEC’s Participation in WAT-2021<fixed-case>NECTEC</fixed-case>’s Participation in <fixed-case>WAT</fixed-case>-2021</title>
      <author><first>Zar Zar</first><last>Hlaing</last></author>
      <author><first>Ye Kyaw</first><last>Thu</last></author>
      <author><first>Thazin</first><last>Myint Oo</last></author>
      <author><first>Mya</first><last>Ei San</last></author>
      <author><first>Sasiporn</first><last>Usanavasin</last></author>
      <author><first>Ponrudee</first><last>Netisopakul</last></author>
      <author><first>Thepchai</first><last>Supnithi</last></author>
      <pages>74–82</pages>
      <abstract>In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021. Basically, our models are based on <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural methods</a> for both directions of English-Myanmar and Myanmar-English language pairs. Most of the existing Neural Machine Translation (NMT) models mainly focus on the conversion of sequential data and do not directly use syntactic information. However, we conduct multi-source neural machine translation (NMT) models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus. The multi-source translation is an approach to exploit multiple inputs (e.g. in two different formats) to increase translation accuracy. The RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment. The experimental results showed that the proposed models of RNN-based architecture outperform the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline model</a> for English-to-Myanmar translation task, and the multi-source and shared-multi-source transformer models yield better translation results than the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>.</abstract>
      <url hash="70ed53ac">2021.wat-1.6</url>
      <doi>10.18653/v1/2021.wat-1.6</doi>
      <bibkey>hlaing-etal-2021-nectecs</bibkey>
    </paper>
    <paper id="11">
      <title>Zero-pronoun Data Augmentation for Japanese-to-English Translation<fixed-case>J</fixed-case>apanese-to-<fixed-case>E</fixed-case>nglish Translation</title>
      <author><first>Ryokan</first><last>Ri</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Yoshimasa</first><last>Tsuruoka</last></author>
      <pages>117–123</pages>
      <abstract>For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding <a href="https://en.wikipedia.org/wiki/Pronoun">pronoun</a> in the target side of the English sentence. However, although fully resolving zero pronouns often needs <a href="https://en.wikipedia.org/wiki/Context_(language_use)">discourse context</a>, in some cases, the local context within a sentence gives clues to the inference of the zero pronoun. In this study, we propose a data augmentation method that provides additional training signals for the translation model to learn correlations between <a href="https://en.wikipedia.org/wiki/Context_(language_use)">local context</a> and zero pronouns. We show that the proposed method significantly improves the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of zero pronoun translation with <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> experiments in the conversational domain.</abstract>
      <url hash="a0e5659f">2021.wat-1.11</url>
      <doi>10.18653/v1/2021.wat-1.11</doi>
      <bibkey>ri-etal-2021-zero</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/business-scene-dialogue">Business Scene Dialogue</pwcdataset>
    </paper>
    <paper id="13">
      <title>TMU NMT System with Japanese BART for the Patent task of WAT 2021<fixed-case>TMU</fixed-case> <fixed-case>NMT</fixed-case> System with <fixed-case>J</fixed-case>apanese <fixed-case>BART</fixed-case> for the Patent task of <fixed-case>WAT</fixed-case> 2021</title>
      <author><first>Hwichan</first><last>Kim</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>133–137</pages>
      <abstract>In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. One of the pre-trained models, BART (Lewis et al., 2020), was shown to improve translation accuracy via fine-tuning with bilingual data. However, they experimented only Romanian!English translation using <a href="https://en.wikipedia.org/wiki/BART">English BART</a>. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</abstract>
      <url hash="3aba8b3d">2021.wat-1.13</url>
      <doi>10.18653/v1/2021.wat-1.13</doi>
      <bibkey>kim-komachi-2021-tmu</bibkey>
    </paper>
    <paper id="18">
      <title>IITP at WAT 2021 : System description for English-Hindi Multimodal Translation Task<fixed-case>IITP</fixed-case> at <fixed-case>WAT</fixed-case> 2021: System description for <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi Multimodal Translation Task</title>
      <author><first>Baban</first><last>Gain</last></author>
      <author><first>Dibyanayan</first><last>Bandyopadhyay</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>161–165</pages>
      <abstract>Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the <a href="https://en.wikipedia.org/wiki/Translation">translation</a> by removing <a href="https://en.wikipedia.org/wiki/Ambiguity">ambiguity</a> on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT-2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 <a href="https://en.wikipedia.org/wiki/BLEU">BLEU points</a> for Evaluation and Challenge subset, respectively.</abstract>
      <url hash="bf4e62ef">2021.wat-1.18</url>
      <doi>10.18653/v1/2021.wat-1.18</doi>
      <bibkey>gain-etal-2021-iitp</bibkey>
    </paper>
    <paper id="20">
      <title>TMEKU System for the WAT2021 Multimodal Translation Task<fixed-case>TMEKU</fixed-case> System for the <fixed-case>WAT</fixed-case>2021 Multimodal Translation Task</title>
      <author><first>Yuting</first><last>Zhao</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <pages>174–180</pages>
      <abstract>We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT). The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use.</abstract>
      <url hash="8c1f0d63">2021.wat-1.20</url>
      <doi>10.18653/v1/2021.wat-1.20</doi>
      <bibkey>zhao-etal-2021-tmeku</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k-entities">Flickr30K Entities</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="21">
      <title>Optimal Word Segmentation for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a> into Dravidian Languages<fixed-case>D</fixed-case>ravidian Languages</title>
      <author><first>Prajit</first><last>Dhar</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>181–190</pages>
      <abstract>Dravidian languages, such as <a href="https://en.wikipedia.org/wiki/Kannada">Kannada</a> and <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a>, are notoriously difficult to translate by state-of-the-art neural models. This stems from the fact that these <a href="https://en.wikipedia.org/wiki/Language">languages</a> are morphologically very rich as well as being low-resourced. In this paper, we focus on subword segmentation and evaluate Linguistically Motivated Vocabulary Reduction (LMVR) against the more commonly used SentencePiece (SP) for the task of translating from <a href="https://en.wikipedia.org/wiki/English_language">English</a> into four different <a href="https://en.wikipedia.org/wiki/Dravidian_languages">Dravidian languages</a>. Additionally we investigate the optimal subword vocabulary size for each language. We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality.</abstract>
      <url hash="61626921">2021.wat-1.21</url>
      <doi>10.18653/v1/2021.wat-1.21</doi>
      <bibkey>dhar-etal-2021-optimal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
    </paper>
    <paper id="22">
      <title>Itihasa : A <a href="https://en.wikipedia.org/wiki/Text_corpus">large-scale corpus</a> for Sanskrit to English translation<fixed-case>S</fixed-case>anskrit to <fixed-case>E</fixed-case>nglish translation</title>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>191–197</pages>
      <abstract>This work introduces <a href="https://en.wikipedia.org/wiki/Itihasa">Itihasa</a>, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The <a href="https://en.wikipedia.org/wiki/Shloka">shlokas</a> are extracted from two <a href="https://en.wikipedia.org/wiki/Indian_epic_poetry">Indian epics</a> viz., The <a href="https://en.wikipedia.org/wiki/Ramayana">Ramayana</a> and The <a href="https://en.wikipedia.org/wiki/Mahabharata">Mahabharata</a>. We first describe the motivation behind the curation of such a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.</abstract>
      <url hash="544fabc2">2021.wat-1.22</url>
      <doi>10.18653/v1/2021.wat-1.22</doi>
      <bibkey>aralikatte-etal-2021-itihasa</bibkey>
      <revision id="1" href="2021.wat-1.22v1" hash="9f1bffa5" />
      <revision id="2" href="2021.wat-1.22v2" hash="544fabc2" date="2021-10-11">Fixed typo</revision>
      <pwcdataset url="https://paperswithcode.com/dataset/itihasa">Itihasa</pwcdataset>
    </paper>
    <paper id="23">
      <title>NICT-5’s Submission To WAT 2021 : MBART Pre-training And In-Domain Fine Tuning For Indic Languages<fixed-case>NICT</fixed-case>-5’s Submission To <fixed-case>WAT</fixed-case> 2021: <fixed-case>MBART</fixed-case> Pre-training And In-Domain Fine Tuning For Indic Languages</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Abhisek</first><last>Chakrabarty</last></author>
      <pages>198–204</pages>
      <abstract>In this paper we describe our submission to the multilingual Indic language translation wtask MultiIndicMT under the team name NICT-5. This task involves <a href="https://en.wikipedia.org/wiki/Translation">translation</a> from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training.</abstract>
      <url hash="284ad0ea">2021.wat-1.23</url>
      <doi>10.18653/v1/2021.wat-1.23</doi>
      <bibkey>dabre-chakrabarty-2021-nict</bibkey>
    </paper>
    <paper id="24">
      <title>How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task<fixed-case>GPU</fixed-case> in 100 hours? <fixed-case>C</fixed-case>o<fixed-case>AS</fixed-case>ta<fixed-case>L</fixed-case> at <fixed-case>M</fixed-case>ulti<fixed-case>I</fixed-case>ndic<fixed-case>MT</fixed-case> Shared Task</title>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>Héctor Ricardo</first><last>Murrieta Bello</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Marcel</first><last>Bollmann</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>205–211</pages>
      <abstract>This work shows that competitive <a href="https://en.wikipedia.org/wiki/Translation_(geometry)">translation</a> results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a> for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>.</abstract>
      <url hash="397e106b">2021.wat-1.24</url>
      <doi>10.18653/v1/2021.wat-1.24</doi>
      <bibkey>aralikatte-etal-2021-far</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
    </paper>
    <paper id="26">
      <title>Language Relatedness and Lexical Closeness can help Improve Multilingual NMT : IITBombay@MultiIndicNMT WAT2021<fixed-case>NMT</fixed-case>: <fixed-case>IITB</fixed-case>ombay@<fixed-case>M</fixed-case>ulti<fixed-case>I</fixed-case>ndic<fixed-case>NMT</fixed-case> <fixed-case>WAT</fixed-case>2021</title>
      <author><first>Jyotsana</first><last>Khatri</last></author>
      <author><first>Nikhil</first><last>Saini</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>217–223</pages>
      <abstract>Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for <a href="https://en.wikipedia.org/wiki/Multilingualism">multiple languages</a>. This paper describes our submission (Team ID : CFILT-IITB) for the MultiIndicMT : An Indic Language Multilingual Task at WAT 2021. We train multilingual NMT systems by sharing <a href="https://en.wikipedia.org/wiki/Encoder">encoder and decoder parameters</a> with language embedding associated with each token in both <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and decoder. Furthermore, we demonstrate the use of transliteration (script conversion) for <a href="https://en.wikipedia.org/wiki/Indo-Aryan_languages">Indic languages</a> in reducing the lexical gap for training a multilingual NMT system. Further, we show improvement in performance by training a multilingual NMT system using languages of the same family, i.e., <a href="https://en.wikipedia.org/wiki/Lingua_franca">related languages</a>.</abstract>
      <url hash="6bfbb574">2021.wat-1.26</url>
      <doi>10.18653/v1/2021.wat-1.26</doi>
      <bibkey>khatri-etal-2021-language</bibkey>
    </paper>
    <paper id="27">
      <title>Samsung R&amp;D Institute Poland submission to WAT 2021 Indic Language Multilingual Task<fixed-case>S</fixed-case>amsung <fixed-case>R</fixed-case>&amp;<fixed-case>D</fixed-case> Institute <fixed-case>P</fixed-case>oland submission to <fixed-case>WAT</fixed-case> 2021 Indic Language Multilingual Task</title>
      <author><first>Adam</first><last>Dobrowolski</last></author>
      <author><first>Marcin</first><last>Szymański</last></author>
      <author><first>Marcin</first><last>Chochowski</last></author>
      <author><first>Paweł</first><last>Przybysz</last></author>
      <pages>224–232</pages>
      <abstract>This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&amp;D Institute Poland. The task covered translation between 10 Indic Languages (Bengali, <a href="https://en.wikipedia.org/wiki/Gujarati_language">Gujarati</a>, <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>, <a href="https://en.wikipedia.org/wiki/Kannada">Kannada</a>, <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a>, <a href="https://en.wikipedia.org/wiki/Marathi_language">Marathi</a>, <a href="https://en.wikipedia.org/wiki/Odia_language">Oriya</a>, <a href="https://en.wikipedia.org/wiki/Punjabi_language">Punjabi</a>, <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a> and <a href="https://en.wikipedia.org/wiki/Telugu_language">Telugu</a>) and <a href="https://en.wikipedia.org/wiki/English_language">English</a>. We combined a variety of techniques : <a href="https://en.wikipedia.org/wiki/Transliteration">transliteration</a>, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models. We applied an effective approach to low-resource training that consist of pretraining on backtranslations and tuning on parallel corpora. We experimented with two different domain-adaptation techniques which significantly improved translation quality when applied to monolingual corpora. We researched and applied a novel approach for finding the best <a href="https://en.wikipedia.org/wiki/Hyperparameter">hyperparameters</a> for ensembling a number of <a href="https://en.wikipedia.org/wiki/Translation_(biology)">translation models</a>. All techniques combined gave significant improvement-up to +8 <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> over baseline results. The quality of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> has been confirmed by the human evaluation where SRPOL models scored best for all 5 manually evaluated languages.</abstract>
      <url hash="27e2db9d">2021.wat-1.27</url>
      <doi>10.18653/v1/2021.wat-1.27</doi>
      <bibkey>dobrowolski-etal-2021-samsung</bibkey>
    </paper>
    <paper id="28">
      <title>Multilingual Machine Translation Systems at WAT 2021 : One-to-Many and Many-to-One Transformer based NMT<fixed-case>WAT</fixed-case> 2021: One-to-Many and Many-to-One Transformer based <fixed-case>NMT</fixed-case></title>
      <author><first>Shivam</first><last>Mhaskar</last></author>
      <author><first>Aditya</first><last>Jain</last></author>
      <author><first>Aakash</first><last>Banerjee</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>233–237</pages>
      <abstract>In this paper, we present the details of the systems that we have submitted for the WAT 2021 MultiIndicMT : An Indic Language Multilingual Task. We have submitted two separate multilingual NMT models : one for <a href="https://en.wikipedia.org/wiki/English_language">English</a> to 10 Indic languages and another for 10 Indic languages to English. We discuss the implementation details of two separate multilingual NMT approaches, namely one-to-many and many-to-one, that makes use of a shared decoder and a shared encoder, respectively. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration.</abstract>
      <url hash="57129c8f">2021.wat-1.28</url>
      <doi>10.18653/v1/2021.wat-1.28</doi>
      <bibkey>mhaskar-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="30">
      <title>ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task<fixed-case>ANVITA</fixed-case> Machine Translation System for <fixed-case>WAT</fixed-case> 2021 <fixed-case>M</fixed-case>ulti<fixed-case>I</fixed-case>ndic<fixed-case>MT</fixed-case> Shared Task</title>
      <author><first>Pavanpankaj</first><last>Vegi</last></author>
      <author><first>Sivabhavani</first><last>J</last></author>
      <author><first>Biswajit</first><last>Paul</last></author>
      <author><first>Chitra</first><last>Viswanathan</last></author>
      <author><first>Prasanna Kumar</first><last>K R</last></author>
      <pages>244–249</pages>
      <abstract>This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions : EnglishIndic and IndicEnglish ; Indic set comprised of 10 Indian languages. ANVITA-1.0 MT system comprised of two multi-lingual NMT models one for the EnglishIndic directions and other for the IndicEnglish directions with shared encoder-decoder, catering 10 language pairs and twenty translation directions. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization. Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training. ANVITA-1.0 achieved highest AM-FM score for <a href="https://en.wikipedia.org/wiki/Bengali_language">EnglishBengali</a>, 2nd for <a href="https://en.wikipedia.org/wiki/Tamil_language">EnglishTamil</a> and 3rd for <a href="https://en.wikipedia.org/wiki/Indian_English">EnglishHindi</a>, BengaliEnglish directions on official test set. In general, performance achieved by ANVITA for the IndicEnglish directions are relatively better than that of EnglishIndic directions for all the 10 language pairs when evaluated using BLEU and RIBES, although the same trend is not observed consistently when AM-FM based evaluation was carried out. As compared to <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>, RIBES and AM-FM based scoring placed ANVITA relatively better among all the task participants.</abstract>
      <url hash="04b835ef">2021.wat-1.30</url>
      <doi>10.18653/v1/2021.wat-1.30</doi>
      <bibkey>vegi-etal-2021-anvita</bibkey>
    </paper>
  </volume>
</collection>