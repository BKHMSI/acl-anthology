<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.privatenlp">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Privacy in Natural Language Processing</booktitle>
      <editor><first>Oluwaseyi</first><last>Feyisetan</last></editor>
      <editor><first>Sepideh</first><last>Ghanavati</last></editor>
      <editor><first>Shervin</first><last>Malmasi</last></editor>
      <editor><first>Patricia</first><last>Thaine</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.privatenlp-1</url>
    </meta>
    <frontmatter>
      <url hash="8fef7924">2021.privatenlp-1.0</url>
      <bibkey>privatenlp-2021-privacy</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Learning and Evaluating a Differentially Private Pre-trained Language Model</title>
      <author><first>Shlomo</first><last>Hoory</last></author>
      <author><first>Amir</first><last>Feder</last></author>
      <author><first>Avichai</first><last>Tendler</last></author>
      <author><first>Alon</first><last>Cohen</last></author>
      <author><first>Sofia</first><last>Erell</last></author>
      <author><first>Itay</first><last>Laish</last></author>
      <author><first>Hootan</first><last>Nakhost</last></author>
      <author><first>Uri</first><last>Stemmer</last></author>
      <author><first>Ayelet</first><last>Benjamini</last></author>
      <author><first>Avinatan</first><last>Hassidim</last></author>
      <author><first>Yossi</first><last>Matias</last></author>
      <pages>21–29</pages>
      <abstract>Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to <a href="https://en.wikipedia.org/wiki/Information_leakage">information leakage</a> and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the <a href="https://en.wikipedia.org/wiki/Privacy">privacy</a> of such individuals is to train a differentially-private model, but this usually comes at the expense of model performance. Moreover, it is hard to tell given a privacy parameter $ \epsilon$ what was the effect on the trained <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a>. In this work we aim to guide future practitioners and researchers on how to improve <a href="https://en.wikipedia.org/wiki/Privacy">privacy</a> while maintaining good model performance. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $ \epsilon=1 $ and with only a small degradation in performance. We experiment on a dataset of clinical notes with a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained on a target entity extraction task, and compare it to a similar <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained without differential privacy. Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.</abstract>
      <url hash="804e3a52">2021.privatenlp-1.3</url>
      <doi>10.18653/v1/2021.privatenlp-1.3</doi>
      <bibkey>hoory-etal-2021-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
    </paper>
    <paper id="6">
      <title>Using Confidential Data for <a href="https://en.wikipedia.org/wiki/Domain_adaptation">Domain Adaptation</a> of Neural Machine Translation</title>
      <author><first>Sohyung</first><last>Kim</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Fatih</first><last>Turkmen</last></author>
      <pages>46–52</pages>
      <abstract>We study the problem of <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> in Neural Machine Translation (NMT) when domain-specific data can not be shared due to confidentiality or copyright issues. As a first step, we propose to fragment data into phrase pairs and use a <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">random sample</a> to fine-tune a generic NMT model instead of the full sentences. Despite the loss of long segments for the sake of confidentiality protection, we find that NMT quality can considerably benefit from this adaptation, and that further gains can be obtained with a simple tagging technique.</abstract>
      <url hash="86e00d83">2021.privatenlp-1.6</url>
      <attachment type="OptionalSupplementaryData" hash="dbf8a8e7">2021.privatenlp-1.6.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.privatenlp-1.6</doi>
      <bibkey>kim-etal-2021-using</bibkey>
      <pwccode url="https://github.com/sohyo/using-confidential-data-for-nmt" additional="false">sohyo/using-confidential-data-for-nmt</pwccode>
    </paper>
    <paper id="7">
      <title>Private Text Classification with <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a></title>
      <author><first>Samuel</first><last>Adams</last></author>
      <author><first>David</first><last>Melanson</last></author>
      <author><first>Martine</first><last>De Cock</last></author>
      <pages>53–58</pages>
      <abstract>Text classifiers are regularly applied to personal texts, leaving users of these classifiers vulnerable to <a href="https://en.wikipedia.org/wiki/Information_privacy">privacy breaches</a>. We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC). Our method enables the inference of a class label for a personal text in such a way that (1) the owner of the personal text does not have to disclose their text to anyone in an unencrypted manner, and (2) the owner of the text classifier does not have to reveal the trained model parameters to the text owner or to anyone else. To demonstrate the feasibility of our protocol for practical private text classification, we implemented it in the PyTorch-based MPC framework CrypTen, using a well-known additive secret sharing scheme in the honest-but-curious setting. We test the <a href="https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)">runtime</a> of our privacy-preserving text classifier, which is fast enough to be used in practice.</abstract>
      <url hash="a36b03e1">2021.privatenlp-1.7</url>
      <doi>10.18653/v1/2021.privatenlp-1.7</doi>
      <bibkey>adams-etal-2021-private</bibkey>
    </paper>
  </volume>
</collection>