<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.multilingualbio">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020)</booktitle>
      <editor><first>Maite</first><last>Melero</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-65-8</isbn>
    </meta>
    <frontmatter>
      <url hash="8a0c7457">2020.multilingualbio-1.0</url>
      <bibkey>multilingualbio-2020-lrec</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Detecting Adverse Drug Events from Swedish Electronic Health Records using <a href="https://en.wikipedia.org/wiki/Text_mining">Text Mining</a><fixed-case>S</fixed-case>wedish Electronic Health Records using Text Mining</title>
      <author><first>Maria</first><last>Bampa</last></author>
      <author><first>Hercules</first><last>Dalianis</last></author>
      <pages>1–8</pages>
      <abstract>Electronic Health Records are a valuable source of patient information which can be leveraged to detect Adverse Drug Events (ADEs) and aid post-mark drug-surveillance. The overall aim of this study is to scrutinize text written by clinicians in the <a href="https://en.wikipedia.org/wiki/Electronic_health_record">EHRs</a> and build a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for ADE detection that produces medically relevant predictions. Natural Language Processing techniques will be exploited to create important <a href="https://en.wikipedia.org/wiki/Dependent_and_independent_variables">predictors</a> and incorporate them into the <a href="https://en.wikipedia.org/wiki/Learning">learning process</a>. The study focuses on the 5 most frequent ADE cases found ina Swedish electronic patient record corpus. The results indicate that considering textual features, rather than the structured, can improve the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> performance by 15 % in some ADE cases. Additionally, variable patient history lengths are incorporated in the models, demonstrating the importance of the above decision rather than using an arbitrary number for a history length. The experimental findings suggest that the clinical text in <a href="https://en.wikipedia.org/wiki/Electronic_health_record">EHRs</a> includes information that can capture data beyond the ones that are found in a structured format.</abstract>
      <url hash="335c5670">2020.multilingualbio-1.1</url>
      <language>eng</language>
      <bibkey>bampa-dalianis-2020-detecting</bibkey>
    </paper>
    <paper id="3">
      <title>Localising the Clinical Terminology SNOMED CT by Semi-automated Creation of a German Interface Vocabulary<fixed-case>SNOMED</fixed-case> <fixed-case>CT</fixed-case> by Semi-automated Creation of a <fixed-case>G</fixed-case>erman Interface Vocabulary</title>
      <author><first>Stefan</first><last>Schulz</last></author>
      <author><first>Larissa</first><last>Hammer</last></author>
      <author><first>David</first><last>Hashemian-Nik</last></author>
      <author><first>Markus</first><last>Kreuzthaler</last></author>
      <pages>15–20</pages>
      <abstract>Medical language exhibits great variations regarding users, institutions and language registers. With large parts of clinical documents in free text, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> is playing a more and more important role in unlocking re-usable and interoperable meaning from <a href="https://en.wikipedia.org/wiki/Medical_record">medical records</a>. This study describes the architectural principles and the evolution of a German interface vocabulary, combining <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> with human annotation and rule-based term generation, yielding a resource with 7.7 million raw entries, each of which linked to the reference terminology <a href="https://en.wikipedia.org/wiki/SNOMED_CT">SNOMED CT</a>, an international standard with about 350 thousand concepts. The purpose is to offer a high coverage of medical jargon in order to optimise terminology grounding of <a href="https://en.wikipedia.org/wiki/Medicine">clinical texts</a> by <a href="https://en.wikipedia.org/wiki/Text_mining">text mining systems</a>. The core resource is a manually curated table of English-to-German word and chunk translations, supported by a set of language generation rules. The work describes a <a href="https://en.wikipedia.org/wiki/Workflow">workflow</a> consisting the enrichment and modification of this table with human and machine efforts, manually enriched by grammarspecific tags. Top-down and bottom-up methods for terminology population used in parallel. The final interface terms are produced by a term generator, which creates one-to-many German variants per SNOMED CT English description. Filtering against a large collection of domain terminologies and corpora drastically reduces the size of the vocabulary in favour of more realistic terms or terms that can reasonably be expected to match clinical text passages within a text-mining pipeline. An evaluation was performed by a comparison between the current version of the German interface vocabulary and the English description table of the SNOMED CT International release. An exact term matching was performed with a small <a href="https://en.wikipedia.org/wiki/Parallel_corpus">parallel corpus</a> constituted by text snippets from different clinical documents. With overall low retrieval parameters (with F-values around 30 %), the performance of the German language scenario reaches 80   90 % of the English one. Interestingly, annotations are slightly better with machine-translated (German   English) texts, using the International SNOMED CT resource only.</abstract>
      <url hash="310802e5">2020.multilingualbio-1.3</url>
      <language>eng</language>
      <bibkey>schulz-etal-2020-localising</bibkey>
    </paper>
    <paper id="4">
      <title>Multilingual enrichment of disease biomedical ontologies</title>
      <author><first>Léo</first><last>Bouscarrat</last></author>
      <author><first>Antoine</first><last>Bonnefoy</last></author>
      <author><first>Cécile</first><last>Capponi</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <pages>21–28</pages>
      <abstract>Translating biomedical ontologies is an important challenge, but doing it manually requires much time and money. We study the possibility to use open-source knowledge bases to translate biomedical ontologies. We focus on two aspects : <a href="https://en.wikipedia.org/wiki/Coverage_(telecommunication)">coverage</a> and <a href="https://en.wikipedia.org/wiki/Quality_(business)">quality</a>. We look at the coverage of two biomedical ontologies focusing on diseases with respect to <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a> for 9 European languages (Czech, Dutch, English, French, German, Italian, Polish, Portuguese and Spanish) for both, plus <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a> and <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a> for the second. We first use direct links between <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a> and the studied <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontologies</a> and then use second-order links by going through other intermediate ontologies. We then compare the quality of the translations obtained thanks to <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a> with a commercial machine translation tool, here Google Cloud Translation.</abstract>
      <url hash="bcd74b95">2020.multilingualbio-1.4</url>
      <language>eng</language>
      <bibkey>bouscarrat-etal-2020-multilingual</bibkey>
      <pwccode url="https://github.com/euranova/orphanet_translation" additional="true">euranova/orphanet_translation</pwccode>
    </paper>
    <paper id="6">
      <title>Automated Processing of Multilingual Online News for the Monitoring of Animal Infectious Diseases</title>
      <author><first>Sarah</first><last>Valentin</last></author>
      <author><first>Renaud</first><last>Lancelot</last></author>
      <author><first>Mathieu</first><last>Roche</last></author>
      <pages>33–36</pages>
      <abstract>The Platform for Automated extraction of animal Disease Information from the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a> (PADI-web) is an automated system which monitors the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a> for monitoring and detecting emerging animal infectious diseases. The <a href="https://en.wikipedia.org/wiki/Tool">tool</a> automatically collects news via customised multilingual queries, classifies them and extracts <a href="https://en.wikipedia.org/wiki/Epidemiology">epidemiological information</a>. We detail the processing of multilingual online sources by PADI-web and analyse the translated outputs in a case study</abstract>
      <url hash="4ed57f23">2020.multilingualbio-1.6</url>
      <language>eng</language>
      <bibkey>valentin-etal-2020-automated</bibkey>
    </paper>
  </volume>
</collection>