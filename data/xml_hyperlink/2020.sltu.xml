<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.sltu">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)</booktitle>
      <editor><first>Dorothee</first><last>Beermann</last></editor>
      <editor><first>Laurent</first><last>Besacier</last></editor>
      <editor><first>Sakriani</first><last>Sakti</last></editor>
      <editor><first>Claudia</first><last>Soria</last></editor>
      <publisher>European Language Resources association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-35-1</isbn>
    </meta>
    <frontmatter>
      <url hash="f2d4a780">2020.sltu-1.0</url>
      <bibkey>sltu-2020-joint</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Open-Source High Quality Speech Datasets for Basque, Catalan and Galician<fixed-case>B</fixed-case>asque, <fixed-case>C</fixed-case>atalan and <fixed-case>G</fixed-case>alician</title>
      <author><first>Oddur</first><last>Kjartansson</last></author>
      <author><first>Alexander</first><last>Gutkin</last></author>
      <author><first>Alena</first><last>Butryna</last></author>
      <author><first>Isin</first><last>Demirsahin</last></author>
      <author><first>Clara</first><last>Rivera</last></author>
      <pages>21–27</pages>
      <abstract>This paper introduces new open speech datasets for three of the languages of Spain : <a href="https://en.wikipedia.org/wiki/Basque_language">Basque</a>, <a href="https://en.wikipedia.org/wiki/Catalan_language">Catalan</a> and <a href="https://en.wikipedia.org/wiki/Galician_language">Galician</a>. Catalan is furthermore the official language of the Principality of Andorra. The <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> consist of high-quality multi-speaker recordings of the three languages along with the associated transcriptions. The resulting <a href="https://en.wikipedia.org/wiki/Text_corpus">corpora</a> include over 33 hours of crowd-sourced recordings of 132 male and female native speakers. The recording scripts also include material for elicitation of global and local place names, personal and business names. The <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> are released under a permissive license and are available for free download for commercial, academic and personal use. The high-quality annotated speech datasets described in this paper can be used to, among other things, build <a href="https://en.wikipedia.org/wiki/Speech_synthesis">text-to-speech systems</a>, serve as adaptation data in <a href="https://en.wikipedia.org/wiki/Speech_recognition">automatic speech recognition</a> and provide useful phonetic and phonological insights in <a href="https://en.wikipedia.org/wiki/Corpus_linguistics">corpus linguistics</a>.</abstract>
      <url hash="7c050393">2020.sltu-1.3</url>
      <language>eng</language>
      <bibkey>kjartansson-etal-2020-open</bibkey>
    </paper>
    <paper id="5">
      <title>Morphological Disambiguation of South Smi with FSTs and <a href="https://en.wikipedia.org/wiki/Neural_network">Neural Networks</a><fixed-case>S</fixed-case>outh <fixed-case>S</fixed-case>ámi with <fixed-case>FST</fixed-case>s and Neural Networks</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Linda</first><last>Wiechetek</last></author>
      <pages>36–40</pages>
      <abstract>We present a method for conducting morphological disambiguation for South Smi, which is an <a href="https://en.wikipedia.org/wiki/Endangered_language">endangered language</a>. Our method uses an FST-based morphological analyzer to produce an ambiguous set of morphological readings for each word in a sentence. These readings are disambiguated with a Bi-RNN model trained on the related North Smi UD Treebank and some synthetically generated South Smi data. The disambiguation is done on the level of morphological tags ignoring word forms and <a href="https://en.wikipedia.org/wiki/Lemma_(morphology)">lemmas</a> ; this makes it possible to use North Smi training data for South Smi without the need for a <a href="https://en.wikipedia.org/wiki/Bilingual_dictionary">bilingual dictionary</a> or aligned word embeddings. Our approach requires only minimal resources for South Smi, which makes it usable and applicable in the contexts of any other <a href="https://en.wikipedia.org/wiki/Endangered_language">endangered language</a> as well.</abstract>
      <url hash="f80e78b8">2020.sltu-1.5</url>
      <language>eng</language>
      <bibkey>hamalainen-wiechetek-2020-morphological</bibkey>
    </paper>
    <paper id="8">
      <title>Neural Text-to-Speech Synthesis for an Under-Resourced Language in a Diglossic Environment : the Case of Gascon Occitan<fixed-case>G</fixed-case>ascon <fixed-case>O</fixed-case>ccitan</title>
      <author><first>Ander</first><last>Corral</last></author>
      <author><first>Igor</first><last>Leturia</last></author>
      <author><first>Aure</first><last>Séguier</last></author>
      <author><first>Michäel</first><last>Barret</last></author>
      <author><first>Benaset</first><last>Dazéas</last></author>
      <author><first>Philippe</first><last>Boula de Mareüil</last></author>
      <author><first>Nicolas</first><last>Quint</last></author>
      <pages>53–60</pages>
      <abstract>Occitan is a <a href="https://en.wikipedia.org/wiki/Minority_language">minority language</a> spoken in Southern France, some Alpine Valleys of Italy, and the Val d’Aran in Spain, which only very recently started developing language and speech technologies. This paper describes the first project for designing a Text-to-Speech synthesis system for one of its main <a href="https://en.wikipedia.org/wiki/Gascon_language">regional varieties</a>, namely <a href="https://en.wikipedia.org/wiki/Gascon_language">Gascon</a>. We used a state-of-the-art deep neural network approach, the Tacotron2-WaveGlow system. However, we faced two additional difficulties or challenges : on the one hand, we wanted to test if it was possible to obtain good quality results with fewer recording hours than is usually reported for such systems ; on the other hand, we needed to achieve a standard, non-Occitan pronunciation of French proper names, therefore we needed to record French words and test phoneme-based approaches. The evaluation carried out over the various developed <a href="https://en.wikipedia.org/wiki/System">systems</a> and <a href="https://en.wikipedia.org/wiki/Software_development_process">approaches</a> shows promising results with near production-ready quality. It has also allowed us to detect the phenomena for which some flaws or fall of quality occur, pointing at the direction of future work to improve the quality of the actual system and for new systems for other <a href="https://en.wikipedia.org/wiki/Variety_(linguistics)">language varieties</a> and voices.</abstract>
      <url hash="51050726">2020.sltu-1.8</url>
      <language>eng</language>
      <bibkey>corral-etal-2020-neural</bibkey>
    </paper>
    <paper id="14">
      <title>Poio Text Prediction : Lessons on the Development and Sustainability of LTs for Endangered Languages<fixed-case>LT</fixed-case>s for Endangered Languages</title>
      <author><first>Gema</first><last>Zamora Fernández</last></author>
      <author><first>Vera</first><last>Ferreira</last></author>
      <author><first>Pedro</first><last>Manha</last></author>
      <pages>106–110</pages>
      <abstract>2019, the International Year of Indigenous Languages (IYIL), marked a crucial milestone for a diverse community united by a strong sense of urgency. In this presentation, we evaluate the impact of IYIL’s outcomes in the development of LTs for <a href="https://en.wikipedia.org/wiki/Endangered_language">endangered languages</a>. We give a brief description of the field of <a href="https://en.wikipedia.org/wiki/Language_documentation">Language Documentation</a>, whose experts have led the research and data collection efforts surrounding <a href="https://en.wikipedia.org/wiki/Endangered_language">endangered languages</a> for the past 30 years. We introduce the work of the Interdisciplinary Centre for Social and Language Documentation and we look at Poio as an example of an LT developed specifically with speakers of endangered languages in mind. This example illustrates how the deeper systemic causes of <a href="https://en.wikipedia.org/wiki/Endangered_language">language endangerment</a> are reflected in the development of <a href="https://en.wikipedia.org/wiki/Language_proficiency">LTs</a>. Additionally, we share some of the strategic decisions that have led the development of this project. Finally, we advocate the importance of bridging the divide between research and activism, pushing for the inclusion of threatened languages in the world of LTs, and doing so in close collaboration with the speaker community.</abstract>
      <url hash="9a0d1bde">2020.sltu-1.14</url>
      <language>eng</language>
      <bibkey>zamora-fernandez-etal-2020-poio</bibkey>
    </paper>
    <paper id="16">
      <title>Scaling Language Data Import / Export with a Data Transformer Interface</title>
      <author><first>Nicholas</first><last>Buckeridge</last></author>
      <author><first>Ben</first><last>Foley</last></author>
      <pages>121–125</pages>
      <abstract>This paper focuses on the technical improvement of <a href="https://en.wikipedia.org/wiki/Elpis">Elpis</a>, a <a href="https://en.wikipedia.org/wiki/Language_technology">language technology</a> which assists people in the process of transcription, particularly for low-resource language documentation situations. To provide better support for the diversity of file formats encountered by people working to document the world’s languages, a Data Transformer interface has been developed to abstract the complexities of designing individual data import scripts. This work took place as part of a larger project of <a href="https://en.wikipedia.org/wiki/Code_quality">code quality improvement</a> and the publication of <a href="https://en.wikipedia.org/wiki/Template_processor">template code</a> that can be used for development of other <a href="https://en.wikipedia.org/wiki/Programming_language">language technologies</a>.</abstract>
      <url hash="007f0046">2020.sltu-1.16</url>
      <language>eng</language>
      <bibkey>buckeridge-foley-2020-scaling</bibkey>
    </paper>
    <paper id="21">
      <title>Natural Language Processing Chains Inside a Cross-lingual Event-Centric Knowledge Pipeline for European Union Under-resourced Languages<fixed-case>E</fixed-case>uropean <fixed-case>U</fixed-case>nion Under-resourced Languages</title>
      <author><first>Diego</first><last>Alves</last></author>
      <author><first>Gaurish</first><last>Thakkar</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <pages>153–158</pages>
      <abstract>This article presents the strategy for developing a platform containing Language Processing Chains for European Union languages, consisting of Tokenization to <a href="https://en.wikipedia.org/wiki/Parsing">Parsing</a>, also including Named Entity recognition and with addition of <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a>. These chains are part of the first step of an event-centric knowledge processing pipeline whose aim is to process multilingual media information about major events that can cause an impact in Europe and the rest of the world. Due to the differences in terms of availability of language resources for each language, we have built this strategy in three steps, starting with processing chains for the well-resourced languages and finishing with the development of new modules for the under-resourced ones. In order to classify all <a href="https://en.wikipedia.org/wiki/Languages_of_the_European_Union">European Union official languages</a> in terms of resources, we have analysed the size of annotated corpora as well as the existence of pre-trained models in mainstream Language Processing tools, and we have combined this information with the proposed classification published at META-NET whitepaper series.</abstract>
      <url hash="c953eb1b">2020.sltu-1.21</url>
      <language>eng</language>
      <bibkey>alves-etal-2020-natural</bibkey>
    </paper>
    <paper id="23">
      <title>Acoustic-Phonetic Approach for ASR of Less Resourced Languages Using Monolingual and Cross-Lingual Information<fixed-case>ASR</fixed-case> of Less Resourced Languages Using Monolingual and Cross-Lingual Information</title>
      <author><first>Shweta</first><last>Bansal</last></author>
      <pages>167–171</pages>
      <abstract>The exploration of <a href="https://en.wikipedia.org/wiki/Speech_processing">speech processing</a> for <a href="https://en.wikipedia.org/wiki/Endangered_language">endangered languages</a> has substantially increased in the past epoch of time. In this paper, we present the acoustic-phonetic approach for automatic speech recognition (ASR) using monolingual and cross-lingual information with application to under-resourced Indian languages, <a href="https://en.wikipedia.org/wiki/Punjabi_language">Punjabi</a>, <a href="https://en.wikipedia.org/wiki/Nepali_language">Nepali</a> and <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>. The challenging task while developing the ASR was the collection of the acoustic corpus for under-resourced languages. We have described here, in brief, the strategies used for designing the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> and also highlighted the issues pertaining while collecting data for these <a href="https://en.wikipedia.org/wiki/Language">languages</a>. The bootstrap GMM-UBM based approach is used, which integrates pronunciation lexicon, <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> and <a href="https://en.wikipedia.org/wiki/Acoustic_phonetics">acoustic-phonetic model</a>. Mel Frequency Cepstral Coefficients were used for extracting the acoustic signal features for training in monolingual and cross-lingual settings. The experimental result shows the overall performance of ASR for cross-lingual and monolingual. The phone substitution plays a key role in the cross-lingual as well as monolingual recognition. The result obtained by cross-lingual recognition compared with other <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline system</a> and it has been found that the performance of the <a href="https://en.wikipedia.org/wiki/Speech_recognition">recognition system</a> is based on <a href="https://en.wikipedia.org/wiki/Phoneme">phonemic units</a>. The recognition rate of cross-lingual generally declines as compared with the monolingual.</abstract>
      <url hash="7fa62cee">2020.sltu-1.23</url>
      <language>eng</language>
      <bibkey>bansal-2020-acoustic</bibkey>
    </paper>
    <paper id="25">
      <title>A Sentiment Analysis Dataset for Code-Mixed Malayalam-English<fixed-case>M</fixed-case>alayalam-<fixed-case>E</fixed-case>nglish</title>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Navya</first><last>Jose</last></author>
      <author><first>Shardul</first><last>Suryawanshi</last></author>
      <author><first>Elizabeth</first><last>Sherly</last></author>
      <author><first>John Philip</first><last>McCrae</last></author>
      <pages>177–184</pages>
      <abstract>There is an increasing demand for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> of text from <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> which are mostly code-mixed. Systems trained on monolingual data fail for code-mixed data due to the complexity of mixing at different levels of the text. However, very few resources are available for code-mixed data to create <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> specific for this <a href="https://en.wikipedia.org/wiki/Data">data</a>. Although much research in multilingual and cross-lingual sentiment analysis has used <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised or unsupervised methods</a>, <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised methods</a> still performs better. Only a few datasets for popular languages such as English-Spanish, <a href="https://en.wikipedia.org/wiki/English_language_in_India">English-Hindi</a>, and <a href="https://en.wikipedia.org/wiki/Standard_Chinese">English-Chinese</a> are available. There are no resources available for Malayalam-English code-mixed data. This paper presents a new gold standard corpus for sentiment analysis of code-mixed text in <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam-English</a> annotated by voluntary annotators. This gold standard <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> obtained a <a href="https://en.wikipedia.org/wiki/Krippendorff’s_alpha">Krippendorff’s alpha</a> above 0.8 for the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. We use this new <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> to provide the benchmark for sentiment analysis in Malayalam-English code-mixed texts.</abstract>
      <url hash="59bc1465">2020.sltu-1.25</url>
      <language>eng</language>
      <bibkey>chakravarthi-etal-2020-sentiment</bibkey>
      <pwccode url="https://github.com/bharathichezhiyan/MalayalamMixSentiment" additional="false">bharathichezhiyan/MalayalamMixSentiment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/malayalammixsentiment">MalayalamMixSentiment</pwcdataset>
    </paper>
    <paper id="27">
      <title>Macsen : A Voice Assistant for Speakers of a Lesser Resourced Language<fixed-case>M</fixed-case>acsen: A Voice Assistant for Speakers of a Lesser Resourced Language</title>
      <author><first>Dewi</first><last>Jones</last></author>
      <pages>194–201</pages>
      <abstract>This paper reports on the development of a voice assistant mobile app for speakers of a lesser resourced language   Welsh. An assistant with a smaller set of effective but useful skills is both desirable and urgent for the wider Welsh speaking community. Descriptions of the <a href="https://en.wikipedia.org/wiki/Mobile_app">app</a>’s skills, architecture, design decisions and <a href="https://en.wikipedia.org/wiki/User_interface">user interface</a> is provided before elaborating on the most recent research and activities in open source speech technology for <a href="https://en.wikipedia.org/wiki/Welsh_language">Welsh</a>. The paper reports on the progress to date on crowdsourcing Welsh speech data in Mozilla Common Voice and of its suitability for training Mozilla’s DeepSpeech speech recognition for a voice assistant application according to conventional and transfer learning methods. We demonstrate that with smaller datasets of speech data, <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> and a domain specific language model, acceptable <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition</a> is achievable that facilitates, as confirmed by beta users, a practical and useful voice assistant for Welsh speakers. We hope that this work informs and serves as a model to researchers and developers in other lesser-resourced linguistic communities and helps bring into being voice assistant apps for their languages.</abstract>
      <url hash="21215a5f">2020.sltu-1.27</url>
      <language>eng</language>
      <bibkey>jones-2020-macsen</bibkey>
      <pwccode url="https://github.com/techiaith/docker-deepspeech-cy" additional="true">techiaith/docker-deepspeech-cy</pwccode>
    </paper>
    <paper id="29">
      <title>Gender Detection from <a href="https://en.wikipedia.org/wiki/Human_voice">Human Voice</a> Using Tensor Analysis</title>
      <author><first>Prasanta</first><last>Roy</last></author>
      <author><first>Parabattina</first><last>Bhagath</last></author>
      <author><first>Pradip</first><last>Das</last></author>
      <pages>211–217</pages>
      <abstract>Speech-based communication is one of the most preferred modes of communication for humans. The <a href="https://en.wikipedia.org/wiki/Human_voice">human voice</a> contains several important information and clues that help in interpreting the voice message. The gender of the speaker can be accurately guessed by a person based on the received voice of a speaker. The knowledge of the speaker’s gender can be a great aid to design accurate <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition systems</a>. GMM based classifier is a popular choice used for gender detection. In this paper, we propose a Tensor-based approach for detecting the gender of a speaker and discuss its implementation details for low resourceful languages. Experiments were conducted using the TIMIT and SHRUTI dataset. An average gender detection accuracy of 91 % is recorded. Analysis of the results with the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is presented in this paper.</abstract>
      <url hash="d708a748">2020.sltu-1.29</url>
      <language>eng</language>
      <bibkey>roy-etal-2020-gender</bibkey>
    </paper>
    <paper id="30">
      <title>Data-Driven Parametric Text Normalization : Rapidly Scaling Finite-State Transduction Verbalizers to New Languages</title>
      <author><first>Sandy</first><last>Ritchie</last></author>
      <author><first>Eoin</first><last>Mahon</last></author>
      <author><first>Kim</first><last>Heiligenstein</last></author>
      <author><first>Nikos</first><last>Bampounis</last></author>
      <author><first>Daan</first><last>van Esch</last></author>
      <author><first>Christian</first><last>Schallhart</last></author>
      <author><first>Jonas</first><last>Mortensen</last></author>
      <author><first>Benoit</first><last>Brard</last></author>
      <pages>218–225</pages>
      <abstract>This paper presents a methodology for rapidly generating FST-based verbalizers for ASR and TTS systems by efficiently sourcing language-specific data. We describe a questionnaire which collects the necessary data to bootstrap the number grammar induction system and parameterize the verbalizer templates described in Ritchie et al. (2019), and a machine-readable data store which allows the data collected through the questionnaire to be supplemented by additional data from other sources. This system allows us to rapidly scale technologies such as <a href="https://en.wikipedia.org/wiki/Autonomous_system_(Internet)">ASR</a> and <a href="https://en.wikipedia.org/wiki/Text-based_user_interface">TTS</a> to more languages, including low-resource languages.</abstract>
      <url hash="2fb9504b">2020.sltu-1.30</url>
      <language>eng</language>
      <bibkey>ritchie-etal-2020-data</bibkey>
    </paper>
    <paper id="32">
      <title>Adapting a Welsh Terminology Tool to Develop a Cornish Dictionary<fixed-case>W</fixed-case>elsh Terminology Tool to Develop a <fixed-case>C</fixed-case>ornish Dictionary</title>
      <author><first>Delyth</first><last>Prys</last></author>
      <pages>235–239</pages>
      <abstract>Cornish and <a href="https://en.wikipedia.org/wiki/Welsh_language">Welsh</a> are closely related <a href="https://en.wikipedia.org/wiki/Celtic_languages">Celtic languages</a> and this paper provides a brief description of a recent project to publish an online bilingual English / Cornish dictionary, the Gerlyver Kernewek, based on similar work previously undertaken for <a href="https://en.wikipedia.org/wiki/Welsh_language">Welsh</a>. Both languages are endangered, Cornish critically so, but both can benefit from the use of <a href="https://en.wikipedia.org/wiki/Language_technology">language technology</a>. Welsh has previous experience of using language technologies for <a href="https://en.wikipedia.org/wiki/Language_revitalization">language revitalization</a>, and this is now being used to help the <a href="https://en.wikipedia.org/wiki/Cornish_language">Cornish language</a> create new tools and resources, including lexicographical ones, helping a dispersed team of language specialists and editors, many of them in a voluntary capacity, to work collaboratively online. Details are given of the Maes T dictionary writing and publication platform, originally developed for <a href="https://en.wikipedia.org/wiki/Welsh_language">Welsh</a>, and of some of the adaptations that had to be made to accommodate the specific needs of Cornish, including their use of Middle and Late varieties due to its development as a revived language.</abstract>
      <url hash="48eef228">2020.sltu-1.32</url>
      <language>eng</language>
      <bibkey>prys-2020-adapting</bibkey>
    </paper>
    <paper id="40">
      <title>On the Exploration of English to Urdu Machine Translation<fixed-case>E</fixed-case>nglish to <fixed-case>U</fixed-case>rdu Machine Translation</title>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>Syeda</first><last>Abida</last></author>
      <author><first>Noor-e-</first><last>Hira</last></author>
      <author><first>Syeda</first><last>Zahra</last></author>
      <author><first>Dania</first><last>Parvez</last></author>
      <author><first>Javeria</first><last>Bashir</last></author>
      <author><first>Qurat-ul-ain</first><last>Majid</last></author>
      <pages>285–293</pages>
      <abstract>Machine Translation is the inevitable technology to reduce communication barriers in today’s world. It has made substantial progress in recent years and is being widely used in commercial as well as non-profit sectors. Such is only the case for European and other high resource languages. For English-Urdu language pair, the <a href="https://en.wikipedia.org/wiki/Technology">technology</a> is in its infancy stage due to scarcity of resources. Present research is an important milestone in English-Urdu machine translation, as we present results for four major domains including Biomedical, Religious, Technological and General using Statistical and Neural Machine Translation. We performed series of experiments in attempts to optimize the performance of each <a href="https://en.wikipedia.org/wiki/System">system</a> and also to study the impact of data sources on the <a href="https://en.wikipedia.org/wiki/System">systems</a>. Finally, we established a comparison of the data sources and the effect of language model size on <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation">statistical machine translation</a> performance.</abstract>
      <url hash="d45e0926">2020.sltu-1.40</url>
      <language>eng</language>
      <bibkey>abdul-rauf-etal-2020-exploration</bibkey>
    </paper>
    <paper id="42">
      <title>Adapting Language Specific Components of Cross-Media Analysis Frameworks to Less-Resourced Languages : the Case of Amharic<fixed-case>A</fixed-case>mharic</title>
      <author><first>Yonas</first><last>Woldemariam</last></author>
      <author><first>Adam</first><last>Dahlgren</last></author>
      <pages>298–305</pages>
      <abstract>We present an ASR based pipeline for <a href="https://en.wikipedia.org/wiki/Amharic">Amharic</a> that orchestrates NLP components within a cross media analysis framework (CMAF). One of the major challenges that are inherently associated with CMAFs is effectively addressing multi-lingual issues. As a result, many languages remain under-resourced and fail to leverage out of available media analysis solutions. Although spoken natively by over 22 million people and there is an ever-increasing amount of Amharic multimedia content on the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">Web</a>, querying them with simple text search is difficult. Searching for, especially audio / video content with simple key words, is even hard as they exist in their raw form. In this study, we introduce a spoken and textual content processing workflow into a CMAF for <a href="https://en.wikipedia.org/wiki/Amharic">Amharic</a>. We design an ASR-named entity recognition (NER) pipeline that includes three main components : ASR, a transliterator and NER. We explore various acoustic modeling techniques and develop an OpenNLP-based NER extractor along with a transliterator that interfaces between ASR and <a href="https://en.wikipedia.org/wiki/Near-infrared_spectroscopy">NER</a>. The designed ASR-NER pipeline for <a href="https://en.wikipedia.org/wiki/Amharic">Amharic</a> promotes the multi-lingual support of CMAFs. Also, the state-of-the art design principles and techniques employed in this study shed light for other less-resourced languages, particularly the <a href="https://en.wikipedia.org/wiki/Semitic_languages">Semitic ones</a>.</abstract>
      <url hash="d9c253ee">2020.sltu-1.42</url>
      <language>eng</language>
      <bibkey>woldemariam-dahlgren-2020-adapting</bibkey>
    </paper>
    <paper id="45">
      <title>Owksape-An Online Language Learning Platform for Lakota<fixed-case>L</fixed-case>akota</title>
      <author><first>Jan</first><last>Ullrich</last></author>
      <author><first>Elliot</first><last>Thornton</last></author>
      <author><first>Peter</first><last>Vieira</last></author>
      <author><first>Logan</first><last>Swango</last></author>
      <author><first>Marek</first><last>Kupiec</last></author>
      <pages>321–329</pages>
      <abstract>This paper presents Owksape, an online language learning platform for the under-resourced language Lakota. The Lakota language (Laktiyapi) is a Siouan language native to the United States with fewer than 2000 fluent speakers. Owksape was developed by The Language Conservancy to support revitalization efforts, including reaching younger generations and providing a tool to complement traditional teaching methods. This project grew out of various multimedia resources in order to combine their most effective aspects into a single, self-paced learning tool. The first section of this paper discusses the motivation for and background of Owksape. Section two details the <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic features</a> and language documentation principles that form the backbone of the <a href="https://en.wikipedia.org/wiki/Computing_platform">platform</a>. Section three lays out the unique integration of cultural aspects of the Lakota people into the <a href="https://en.wikipedia.org/wiki/Graphic_design">visual design</a> of the <a href="https://en.wikipedia.org/wiki/Application_software">application</a>. Section four explains the pedagogical principles of Owksape. Application features and exercise types are then discussed in detail with visual examples, followed by an overview of the <a href="https://en.wikipedia.org/wiki/Software_design">software design</a>, as well as the effort required to develop the <a href="https://en.wikipedia.org/wiki/Computing_platform">platform</a>. Finally, a description of future features and considerations is presented.</abstract>
      <url hash="26a7b211">2020.sltu-1.45</url>
      <language>eng</language>
      <bibkey>ullrich-etal-2020-owoksape</bibkey>
    </paper>
    <paper id="51">
      <title>Speech Transcription Challenges for Resource Constrained Indigenous Language Cree<fixed-case>C</fixed-case>ree</title>
      <author><first>Vishwa</first><last>Gupta</last></author>
      <author><first>Gilles</first><last>Boulianne</last></author>
      <pages>362–367</pages>
      <abstract>Cree is one of the most spoken Indigenous languages in Canada. From a speech recognition perspective, it is a low-resource language, since very little data is available for either acoustic or language modeling. This has prevented development of <a href="https://en.wikipedia.org/wiki/Speech_technology">speech technology</a> that could help revitalize the <a href="https://en.wikipedia.org/wiki/Language">language</a>. We describe our experiments with available Cree data to improve <a href="https://en.wikipedia.org/wiki/Transcription_(biology)">automatic transcription</a> both in speaker- independent and dependent scenarios. While it was difficult to get low speaker-independent word error rates with only six speakers, we were able to get low word and phoneme error rates in the speaker-dependent scenario. We compare our phoneme recognition with two state-of-the-art open-source phoneme recognition toolkits, which use end-to-end training and sequence-to-sequence modeling. Our phoneme error rate (8.7 %) is significantly lower than that achieved by the best of these systems (15.1 %). With these systems and varying amounts of transcribed and text data, we show that pre-training on other languages is important for speaker-independent recognition, and even small amounts of additional text-only documents are useful. These results can guide practical language documentation work, when deciding how much transcribed and text data is needed to achieve useful phoneme accuracies.</abstract>
      <url hash="5d9bcbea">2020.sltu-1.51</url>
      <language>eng</language>
      <bibkey>gupta-boulianne-2020-speech</bibkey>
    </paper>
    </volume>
</collection>