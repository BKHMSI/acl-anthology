<?xml version='1.0' encoding='utf-8'?>
<collection id="2018.gwc">
  <volume id="1" ingest-date="2021-02-07">
    <meta>
      <booktitle>Proceedings of the 9th Global Wordnet Conference</booktitle>
      <editor><first>Francis</first><last>Bond</last></editor>
      <editor><first>Piek</first><last>Vossen</last></editor>
      <editor><first>Christiane</first><last>Fellbaum</last></editor>
      <publisher>Global Wordnet Association</publisher>
      <address>Nanyang Technological University (NTU), Singapore</address>
      <month>January</month>
      <year>2018</year>
      <url hash="9e9cc8ba">2018.gwc-1</url>
    </meta>
    <frontmatter>
      <url hash="15c94917">2018.gwc-1.0</url>
      <bibkey>gwc-2018-global</bibkey>
    </frontmatter>
    <paper id="1">
      <title>BanglaNet : Towards a WordNet for Bengali Language<fixed-case>B</fixed-case>angla<fixed-case>N</fixed-case>et: Towards a <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et for <fixed-case>B</fixed-case>engali Language</title>
      <author><first>K.M. Tahsin Hassan</first><last>Rahit</last></author>
      <author><first>Khandaker Tabin</first><last>Hasan</last></author>
      <author><first>Md. Al-</first><last>Amin</last></author>
      <author><first>Zahiduddin</first><last>Ahmed</last></author>
      <pages>1–9</pages>
      <abstract>Despite being a popular language in the world, the <a href="https://en.wikipedia.org/wiki/Bengali_language">Bengali language</a> lacks in having a good <a href="https://en.wikipedia.org/wiki/Wordnet">wordnet</a>. This restricts us to do NLP related research work in <a href="https://en.wikipedia.org/wiki/Bengali_language">Bengali</a>. Most of the today’s <a href="https://en.wikipedia.org/wiki/Wordnet">wordnets</a> are developed by following expand approach. One of the key challenges of this approach is the cross-lingual word-sense disambiguation. In our research work, we make semantic relation between Bengali wordnet and <a href="https://en.wikipedia.org/wiki/Princeton_WordNet">Princeton WordNet</a> based on well-established research work in other languages. The <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> will derive relations between concepts as well. One of our key objectives is to provide a panel for lexicographers so that they can validate and contribute to the <a href="https://en.wikipedia.org/wiki/Wordnet">wordnet</a>.</abstract>
      <url hash="67837898">2018.gwc-1.1</url>
      <bibkey>rahit-etal-2018-banglanet</bibkey>
    </paper>
    <paper id="4">
      <title>Towards Cross-checking <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a> and SUMO Using Meronymy<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et and <fixed-case>SUMO</fixed-case> Using Meronymy</title>
      <author><first>Javier</first><last>Álvez</last></author>
      <author><first>German</first><last>Rigau</last></author>
      <pages>25–33</pages>
      <abstract>We describe the practical application of a black-box testing methodology for the validation of the knowledge encoded in <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a>, SUMO and their mapping by using <a href="https://en.wikipedia.org/wiki/Automated_theorem_proving">automated theorem provers</a>. In this paper, weconcentrateonthepart-whole information provided by <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a> and create a large set of tests on the basis of few question patterns. From our preliminary evaluation results, we report on some of the detected inconsistencies.</abstract>
      <url hash="a32aa073">2018.gwc-1.4</url>
      <bibkey>alvez-rigau-2018-towards</bibkey>
    </paper>
    <paper id="7">
      <title>Investigating English Affixes and their Productivity with <a href="https://en.wikipedia.org/wiki/Princeton_WordNet">Princeton WordNet</a><fixed-case>E</fixed-case>nglish Affixes and their Productivity with <fixed-case>P</fixed-case>rinceton <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Verginica</first><last>Mititelu</last></author>
      <pages>53–60</pages>
      <abstract>Such a rich language resource like <a href="https://en.wikipedia.org/wiki/Princeton_WordNet">Princeton WordNet</a>, containing linguistic information of different types (semantic, lexical, syntactic, derivational, dialectal, etc.), is a thesaurus which is worth both being used in various language-enabled applications and being explored in order to study a language. In this paper we show how we used Princeton WordNet version 3.0 to study the English affixes. We extracted pairs of base-derived words and identified the <a href="https://en.wikipedia.org/wiki/Affix">affixes</a> by means of which the derived words were created from their bases. We distinguished among four types of <a href="https://en.wikipedia.org/wiki/Morphological_derivation">derivation</a> depending on the type of overlapping between the senses of the base word and those of the derived word that are linked by derivational relations in Princeton WordNet. We studied the behaviour of <a href="https://en.wikipedia.org/wiki/Affix">affixes</a> with respect to these derivation types. Drawing on these data, we inferred about their productivity.</abstract>
      <url hash="2b712ffd">2018.gwc-1.7</url>
      <bibkey>mititelu-2018-investigating</bibkey>
    </paper>
    <paper id="9">
      <title>Mapping WordNet Concepts with CPA Ontology<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Concepts with <fixed-case>CPA</fixed-case> Ontology</title>
      <author><first>Svetla</first><last>Koeva</last></author>
      <author><first>Cvetana</first><last>Dimitrova</last></author>
      <author><first>Valentina</first><last>Stefanova</last></author>
      <author><first>Dimitar</first><last>Hristov</last></author>
      <pages>69–76</pages>
      <abstract>The paper discusses the enrichment of WordNet data through merging of WordNet concepts and Corpus Pattern Analysis (CPA) semantic types. The 253 CPA semantic types are mapped to the respective WordNet concepts. As a result of mapping, the <a href="https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy">hyponyms</a> of a synset to which a CPA semantic type is mapped inherit not only the respective WordNet semantic primitive but also the CPA semantic type.</abstract>
      <url hash="291226b5">2018.gwc-1.9</url>
      <bibkey>koeva-etal-2018-mapping</bibkey>
    </paper>
    <paper id="11">
      <title>Semantic Feature Structure Extraction From Documents Based on Extended Lexical Chains</title>
      <author><first>Terry</first><last>Ruas</last></author>
      <author><first>William</first><last>Grosky</last></author>
      <pages>87–96</pages>
      <abstract>The meaning of a sentence in a document is more easily determined if its constituent words exhibit <a href="https://en.wikipedia.org/wiki/Cohesion_(linguistics)">cohesion</a> with respect to their individual semantics. This paper explores the degree of cohesion among a document’s words using lexical chains as a semantic representation of its meaning. Using a combination of diverse types of <a href="https://en.wikipedia.org/wiki/Lexical_chain">lexical chains</a>, we develop a text document representation that can be used for semantic document retrieval. For our approach, we develop two kinds of lexical chains : (i) a multilevel flexible chain representation of the extracted semantic values, which is used to construct a fixed segmentation of these chains and constituent words in the text ; and (ii) a fixed <a href="https://en.wikipedia.org/wiki/Lexical_chain">lexical chain</a> obtained directly from the initial semantic representation from a document. The extraction and processing of concepts is performed using <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a> as a <a href="https://en.wikipedia.org/wiki/Lexical_database">lexical database</a>. The segmentation then uses these lexical chains to model the dispersion of concepts in the document. Representing each document as a <a href="https://en.wikipedia.org/wiki/Dimension_(vector_space)">high-dimensional vector</a>, we use spherical k-means clustering to demonstrate that our approach performs better than previous techniques.</abstract>
      <url hash="efbee64e">2018.gwc-1.11</url>
      <bibkey>ruas-grosky-2018-semantic</bibkey>
    </paper>
    <paper id="12">
      <title>Toward a Semantic Concordancer</title>
      <author><first>Adam</first><last>Pease</last></author>
      <author><first>Andrew</first><last>Cheung</last></author>
      <pages>97–104</pages>
      <abstract>Concordancers are an accepted and valuable part of the tool set of linguists and lexicographers. They allow the user to see the context of use of a word or phrase in a corpus. A large enough corpus, such as the Corpus Of Contemporary American English, provides the data needed to enumerate all common uses or meanings. One challenge is that there may be too many results for short search phrases or common words when only a specific context is desired. However, finding meaningful groupings of usage may be impractical if it entails enumerating long lists of possible values, such as city names. If a tool existed that could create some semantic abstractions, it would free the lexicographer from the need to resort to customized development of analysis software. To address this need, we have developed a Semantic Concordancer that uses dependency parsing and the Suggested Upper Merged Ontology (SUMO) to support linguistic analysis at a level of semantic abstraction above the original textual elements. We show how this facility can be employed to analyze the use of <a href="https://en.wikipedia.org/wiki/English_prepositions">English prepositions</a> by non-native speakers. We briefly introduce condordancers and then describe the corpora on which we applied this work. Next we provide a detailed description of the NLP pipeline followed by how this captures detailed <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a>. We show how the <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> can be used to analyze errors in the use of <a href="https://en.wikipedia.org/wiki/English_language">English prepositions</a> by non-native speakers of <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Then we provide a description of a tool that allows users to build semantic search specifications from a set of English examples and how those results can be employed to build rules that translate sentences into logical forms.</abstract>
      <url hash="45acde09">2018.gwc-1.12</url>
      <bibkey>pease-cheung-2018-toward</bibkey>
    </paper>
    <paper id="13">
      <title>Using OpenWordnet-PT for <a href="https://en.wikipedia.org/wiki/Question_answering">Question Answering</a> on Legal Domain<fixed-case>O</fixed-case>pen<fixed-case>W</fixed-case>ordnet-<fixed-case>PT</fixed-case> for Question Answering on Legal Domain</title>
      <author><first>Pedro</first><last>Delfino</last></author>
      <author><first>Bruno</first><last>Cuconato</last></author>
      <author><first>Guilherme</first><last>Paulino-Passos</last></author>
      <author><first>Gerson</first><last>Zaverucha</last></author>
      <author><first>Alexandre</first><last>Rademaker</last></author>
      <pages>105–112</pages>
      <abstract>In order to practice a legal profession in Brazil, law graduates must be approved in the OAB national unified bar exam. For their topic coverage and national reach, the OAB exams provide an excellent benchmark for the performance of legal information systems, as it provides objective metrics and are challenging even for humans, as only 20 % of its candidates are approved. After constructing a new <a href="https://en.wikipedia.org/wiki/Data_set">data set</a> on the exams and doing shallow experiments on it, we now employ the OpenWordnet-PT to verify whether using <a href="https://en.wikipedia.org/wiki/Word_sense">word senses</a> and <a href="https://en.wikipedia.org/wiki/Binary_relation">relations</a> we can improve previous results. We discuss the results, possible future ideas and the additions to the OpenWordnet-PT that we made.</abstract>
      <url hash="f4846440">2018.gwc-1.13</url>
      <bibkey>delfino-etal-2018-using</bibkey>
    </paper>
    <paper id="14">
      <title>Implementation of the Verb Model in plWordNet 4.0<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et 4.0</title>
      <author><first>Agnieszka</first><last>Dziob</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <pages>113–122</pages>
      <abstract>The paper presents an expansion of the verb model for plWordNet   the wordnet of Polish. A modified system of constitutive features (register, aspect and verb classes), synset and lexical relations is presented. A special attention is given to the proposed new relations and changes in the <a href="https://en.wikipedia.org/wiki/Grammatical_category">verb classification</a>. We discuss also the results of its verification by application to the description of a relatively large sample of <a href="https://en.wikipedia.org/wiki/Polish_verbs">Polish verbs</a>. The model introduces a new class of <a href="https://en.wikipedia.org/wiki/Binary_relation">relations</a>, namely non-constitutive synset relations that are shared among lexical units, but describe, not define <a href="https://en.wikipedia.org/wiki/Synset">synsets</a>. The proposed <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is compared to the <a href="https://en.wikipedia.org/wiki/Logical_consequence">entailment relations</a> in other wordnets, and the description of verbs based on <a href="https://en.wikipedia.org/wiki/Valency_(linguistics)">valency frames</a>.</abstract>
      <url hash="86e782d7">2018.gwc-1.14</url>
      <bibkey>dziob-piasecki-2018-implementation</bibkey>
    </paper>
    <paper id="15">
      <title>Public Apologies in India-Semantics, <a href="https://en.wikipedia.org/wiki/Sentimentality">Sentiment</a> and Emotion<fixed-case>I</fixed-case>ndia - Semantics, Sentiment and Emotion</title>
      <author><first>Sangeeta</first><last>Shukla</last></author>
      <author><first>Rajita</first><last>Shukla</last></author>
      <pages>123–135</pages>
      <abstract>This paper reports a pilot study related to public apologies in India, with reference to certain <a href="https://en.wikipedia.org/wiki/Index_term">keywords</a> found in them. The study is of importance as the choice of lexical items holds importance which goes beyond the surface meaning of the words. The analysis of the lexical items has been done using interlinked digital lexical resources which, in future, can lend this study to computational tasks related to <a href="https://en.wikipedia.org/wiki/Opinion_mining">opinion mining</a>, <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> and <a href="https://en.wikipedia.org/wiki/Document_classification">document classification</a>. The study attempts an in-depth psycholinguistic analysis of whether the <a href="https://en.wikipedia.org/wiki/Apology_(act)">apology</a> conveys a sincerity of intent or is it a mere ritualistic exercise to control and repair damage.</abstract>
      <url hash="3d2e81e3">2018.gwc-1.15</url>
      <bibkey>shukla-shukla-2018-public</bibkey>
    </paper>
    <paper id="16">
      <title>Derivational Relations in Arabic WordNet<fixed-case>A</fixed-case>rabic <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Mohamed Ali</first><last>Batita</last></author>
      <author><first>Mounir</first><last>Zrigui</last></author>
      <pages>136–144</pages>
      <abstract>When derivational relations deficiency exists in a <a href="https://en.wikipedia.org/wiki/WordNet">wordnet</a>, such as the <a href="https://en.wikipedia.org/wiki/Arabic_WordNet">Arabic WordNet</a>, it makes it very difficult to exploit in the natural language processing community. Such deficiency is raised when many wordnets follow the same development path of <a href="https://en.wikipedia.org/wiki/Princeton_WordNet">Princeton WordNet</a>. A rule-based approach for Arabic derivational relations is proposed in this paper to deal with this deficiency. The proposed <a href="https://en.wikipedia.org/wiki/Tactic_(method)">approach</a> is explained step by step. It involves the gathering of lexical entries that share the same <a href="https://en.wikipedia.org/wiki/Root_(linguistics)">root</a>, into a bag of words. Rules are then used to affect the appropriate derivational relations, i.e. to relate existing words in the AWN, involving part-of-speech switch. The <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> is implemented using <a href="https://en.wikipedia.org/wiki/Java_(programming_language)">Java</a>. Manual verification by a <a href="https://en.wikipedia.org/wiki/Lexicography">lexicographer</a> takes place to ensure good results. The described approach gave good results. It could be useful for other <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphologically complex languages</a> as well.</abstract>
      <url hash="e331ee56">2018.gwc-1.16</url>
      <bibkey>batita-zrigui-2018-derivational</bibkey>
    </paper>
    <paper id="19">
      <title>The Company They Keep : Extracting Japanese Neologisms Using Language Patterns<fixed-case>J</fixed-case>apanese Neologisms Using Language Patterns</title>
      <author><first>James</first><last>Breen</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <pages>163–171</pages>
      <abstract>We describe an investigation into the identification and extraction of unrecorded potential lexical items in <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese text</a> by detecting text passages containing selected language patterns typically associated with such items. We identified a set of suitable <a href="https://en.wikipedia.org/wiki/Pattern">patterns</a>, then tested them with two large collections of text drawn from the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">WWW</a> and <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>. Samples of the extracted items were evaluated, and it was demonstrated that the approach has considerable potential for identifying terms for later <a href="https://en.wikipedia.org/wiki/Lexicography">lexicographic analysis</a>.</abstract>
      <url hash="cbf2108e">2018.gwc-1.19</url>
      <bibkey>breen-etal-2018-company</bibkey>
    </paper>
    <paper id="20">
      <title>Lexical-semantic resources : yet powerful resources for automatic personality classification</title>
      <author><first>Xuan-Son</first><last>Vu</last></author>
      <author><first>Lucie</first><last>Flekova</last></author>
      <author><first>Lili</first><last>Jiang</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>172–181</pages>
      <abstract>In this paper, we aim to reveal the impact of lexical-semantic resources, used in particular for <a href="https://en.wikipedia.org/wiki/Word-sense_disambiguation">word sense disambiguation</a> and sense-level semantic categorization, on automatic personality classification task. While <a href="https://en.wikipedia.org/wiki/Style_(sociolinguistics)">stylistic features</a> (e.g., part-of-speech counts) have been shown their power in this task, the impact of <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> beyond targeted word lists is relatively unexplored. We propose and extract three types of lexical-semantic features, which capture high-level concepts and <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a>, overcoming the lexical gap of word n-grams. Our experimental results are comparable to state-of-the-art methods, while no personality-specific resources are required.</abstract>
      <url hash="d41596fa">2018.gwc-1.20</url>
      <bibkey>vu-etal-2018-lexical</bibkey>
    </paper>
    <paper id="21">
      <title>Towards a principled approach to sense clustering   a case study of <a href="https://en.wikipedia.org/wiki/Wordnet">wordnet</a> and dictionary senses in <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a><fixed-case>D</fixed-case>anish</title>
      <author><first>Bolette</first><last>Pedersen</last></author>
      <author><first>Manex</first><last>Agirrezabal</last></author>
      <author><first>Sanni</first><last>Nimb</last></author>
      <author><first>Ida</first><last>Olsen</last></author>
      <author><first>Sussi</first><last>Olsen</last></author>
      <pages>182–189</pages>
      <abstract>Our aim is to develop principled methods for sense clustering which can make existing lexical resources practically useful in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>   not too fine-grained to be operational and yet finegrained enough to be worth the trouble. Where traditional <a href="https://en.wikipedia.org/wiki/Dictionary">dictionaries</a> have a highly structured sense inventory typically describing the vocabulary by means of mainand subsenses, wordnets are generally fine-grained and unstructured. We present a series of clustering and annotation experiments with 10 of the most polysemous nouns in <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>. We combine the structured information of a traditional Danish dictionary with the ontological types found in the Danish wordnet, DanNet. This constellation enables us to automatically cluster senses in a principled way and improve <a href="https://en.wikipedia.org/wiki/Inter-annotator_agreement">inter-annotator agreement</a> and wsd performance.</abstract>
      <url hash="f6658855">2018.gwc-1.21</url>
      <bibkey>pedersen-etal-2018-towards</bibkey>
    </paper>
    <paper id="22">
      <title>WordnetLoom   a Multilingual Wordnet Editing System Focused on Graph-based Presentation<fixed-case>W</fixed-case>ordnet<fixed-case>L</fixed-case>oom – a Multilingual <fixed-case>W</fixed-case>ordnet Editing System Focused on Graph-based Presentation</title>
      <author><first>Tomasz</first><last>Naskręt</last></author>
      <author><first>Agnieszka</first><last>Dziob</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <author><first>Chakaveh</first><last>Saedi</last></author>
      <author><first>António</first><last>Branco</last></author>
      <pages>190–199</pages>
      <abstract>The paper presents a new re-built and expanded, version 2.0 of WordnetLoom   an open wordnet editor. It facilitates work on a multilingual system of wordnets, is based on efficient <a href="https://en.wikipedia.org/wiki/Software_architecture">software architecture</a> of <a href="https://en.wikipedia.org/wiki/Thin_client">thin client</a>, and offers more flexibility in enriching wordnet representation. This new version is built on the experience collected during the use of the previous one for more than 10 years of plWordNet development. We discuss its extensions motivated by the collected experience. A special focus is given to the development of a variant for the needs of MultiWordnet of Portuguese, which is based on a very different wordnet development model.</abstract>
      <url hash="1ac23b8c">2018.gwc-1.22</url>
      <bibkey>naskret-etal-2018-wordnetloom</bibkey>
    </paper>
    <paper id="24">
      <title>Lexical Perspective on <a href="https://en.wikipedia.org/wiki/Wordnet">Wordnet</a> to Wordnet Mapping<fixed-case>W</fixed-case>ordnet to <fixed-case>W</fixed-case>ordnet Mapping</title>
      <author><first>Ewa</first><last>Rudnicka</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <author><first>Łukasz</first><last>Grabowski</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <author><first>Tadeusz</first><last>Piotrowski</last></author>
      <pages>209–218</pages>
      <abstract>The paper presents a feature-based model of equivalence targeted at (manual) sense linking between <a href="https://en.wikipedia.org/wiki/Princeton_WordNet">Princeton WordNet</a> and <a href="https://en.wikipedia.org/wiki/PlWordNet">plWordNet</a>. The model incorporates insights from lexicographic and translation theories on bilingual equivalence and draws on the results of earlier synset-level mapping of nouns between <a href="https://en.wikipedia.org/wiki/Princeton_WordNet">Princeton WordNet</a> and <a href="https://en.wikipedia.org/wiki/PlWordNet">plWordNet</a>. It takes into account all basic aspects of <a href="https://en.wikipedia.org/wiki/Language">language</a> such as form, <a href="https://en.wikipedia.org/wiki/Meaning_(linguistics)">meaning</a> and function and supplements them with (parallel) corpus frequency and <a href="https://en.wikipedia.org/wiki/Transliteration">translatability</a>. Three types of <a href="https://en.wikipedia.org/wiki/Equivalence_relation">equivalence</a> are distinguished, namely strong, regular and weak depending on the conformity with the proposed <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a>. The presented solutions are language-neutral and they can be easily applied to language pairs other than <a href="https://en.wikipedia.org/wiki/Polish_language">Polish</a> and <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Sense-level mapping is a more fine-grained mapping than the existing synset mappings and is thus of great potential to <a href="https://en.wikipedia.org/wiki/Translation">human and machine translation</a>.</abstract>
      <url hash="92ebe48f">2018.gwc-1.24</url>
      <bibkey>rudnicka-etal-2018-lexical</bibkey>
    </paper>
    <paper id="26">
      <title>Wordnet-based Evaluation of Large Distributional Models for Polish<fixed-case>W</fixed-case>ordnet-based Evaluation of Large Distributional Models for <fixed-case>P</fixed-case>olish</title>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <author><first>Gabriela</first><last>Czachor</last></author>
      <author><first>Arkadiusz</first><last>Janz</last></author>
      <author><first>Dominik</first><last>Kaszewski</last></author>
      <author><first>Paweł</first><last>Kędzia</last></author>
      <pages>229–238</pages>
      <abstract>The paper presents construction of large scale test datasets for <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> on the basis of a very large wordnet. They were next applied for evaluation of word embedding models and used to assess and compare the usefulness of different word embeddings extracted from a very large corpus of Polish. We analysed also and compared several publicly available <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> described in literature. In addition, several large word embeddings models built on the basis of a very large Polish corpus are presented.</abstract>
      <url hash="8e7189f3">2018.gwc-1.26</url>
      <bibkey>piasecki-etal-2018-wordnet</bibkey>
    </paper>
    <paper id="27">
      <title>Distant Supervision for Relation Extraction with Multi-sense Word Embedding</title>
      <author><first>Sangha</first><last>Nam</last></author>
      <author><first>Kijong</first><last>Han</last></author>
      <author><first>Eun-Kyung</first><last>Kim</last></author>
      <author><first>Key-Sun</first><last>Choi</last></author>
      <pages>239–244</pages>
      <abstract>Distant supervision can automatically generate labeled data between a large-scale corpus and a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a> without utilizing human efforts. Therefore, many studies have used the distant supervision approach in relation extraction tasks. However, existing studies have a disadvantage in that they do not reflect the <a href="https://en.wikipedia.org/wiki/Homograph">homograph</a> in the <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> used as an input of the relation extraction model. Thus, it can be seen that the relation extraction model learns without grasping the meaning of the word accurately. In this paper, we propose a relation extraction model with multi-sense word embedding. We learn multi-sense word embedding using a word sense disambiguation module. In addition, we use <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural network</a> and piecewise max pooling convolutional neural network relation extraction models that efficiently grasp key features in sentences. To evaluate the performance of the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, two additional methods of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> were learned and compared. Accordingly, our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> showed the highest performance among them.</abstract>
      <url hash="461fc3cb">2018.gwc-1.27</url>
      <bibkey>nam-etal-2018-distant</bibkey>
    </paper>
    <paper id="28">
      <title>Cross-Lingual and Supervised Learning Approach for Indonesian Word Sense Disambiguation Task<fixed-case>I</fixed-case>ndonesian Word Sense Disambiguation Task</title>
      <author><first>Rahmad</first><last>Mahendra</last></author>
      <author><first>Heninggar</first><last>Septiantri</last></author>
      <author><first>Haryo Akbarianto</first><last>Wibowo</last></author>
      <author><first>Ruli</first><last>Manurung</last></author>
      <author><first>Mirna</first><last>Adriani</last></author>
      <pages>245–250</pages>
      <abstract>Ambiguity is a problem we frequently face in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a>. Word Sense Disambiguation (WSD) is a task to determine the correct sense of an ambiguous word. However, research in WSD for <a href="https://en.wikipedia.org/wiki/Indonesian_language">Indonesian</a> is still rare to find. The availability of English-Indonesian parallel corpora and <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a> for both languages can be used as training data for WSD by applying Cross-Lingual WSD method. This training data is used as an input to build a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> using supervised machine learning algorithms. Our research also examines the use of Word Embedding features to build the WSD model.</abstract>
      <url hash="667fe9f1">2018.gwc-1.28</url>
      <bibkey>mahendra-etal-2018-cross</bibkey>
    </paper>
    <paper id="30">
      <title>Simple Embedding-Based Word Sense Disambiguation</title>
      <author><first>Dieke</first><last>Oele</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>259–265</pages>
      <abstract>We present a simple knowledge-based WSD method that uses word and sense embeddings to compute the similarity between the gloss of a sense and the context of the word. Our method is inspired by the <a href="https://en.wikipedia.org/wiki/Lesk_algorithm">Lesk algorithm</a> as it exploits both the context of the words and the definitions of the senses. It only requires large unlabeled corpora and a sense inventory such as <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a>, and therefore does not rely on annotated data. We explore whether additional extensions to Lesk are compatible with our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a>. The results of our experiments show that by lexically extending the amount of words in the gloss and context, although it works well for other implementations of Lesk, harms our method. Using a lexical selection method on the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context words</a>, on the other hand, improves it. The combination of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> with lexical selection enables our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> to outperform state-of the art <a href="https://en.wikipedia.org/wiki/Knowledge-based_systems">knowledge-based systems</a>.</abstract>
      <url hash="c19f0f0c">2018.gwc-1.30</url>
      <bibkey>oele-noord-2018-simple</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="31">
      <title>Semi-automatic WordNet Linking using Word Embeddings<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Linking using Word Embeddings</title>
      <author><first>Kevin</first><last>Patel</last></author>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>266–271</pages>
      <abstract>Wordnets are rich lexico-semantic resources. Linked wordnets are extensions of <a href="https://en.wikipedia.org/wiki/Wordnet">wordnets</a>, which link similar concepts in <a href="https://en.wikipedia.org/wiki/Wordnet">wordnets</a> of different languages. Such <a href="https://en.wikipedia.org/wiki/Resource_(computer_science)">resources</a> are extremely useful in many Natural Language Processing (NLP) applications, primarily those based on knowledge-based approaches. In such approaches, these <a href="https://en.wikipedia.org/wiki/Natural_resource">resources</a> are considered as gold standard / oracle. Thus, it is crucial that these <a href="https://en.wikipedia.org/wiki/Resource_(computer_science)">resources</a> hold correct information. Thereby, they are created by human experts. However, manual maintenance of such <a href="https://en.wikipedia.org/wiki/Resource_(project_management)">resources</a> is a tedious and costly affair. Thus techniques that can aid the experts are desirable. In this paper, we propose an approach to link <a href="https://en.wikipedia.org/wiki/Wordnet">wordnets</a>. Given a synset of the source language, the approach returns a ranked list of potential candidate synsets in the target language from which the human expert can choose the correct one(s). Our technique is able to retrieve a winner synset in the top 10 ranked list for 60 % of all synsets and 70 % of noun synsets.</abstract>
      <url hash="ba85ab63">2018.gwc-1.31</url>
      <bibkey>patel-etal-2018-semi</bibkey>
    </paper>
    <paper id="32">
      <title>Multilingual Wordnet sense Ranking using nearest context<fixed-case>W</fixed-case>ordnet sense Ranking using nearest context</title>
      <author><first>E Umamaheswari</first><last>Vasanthakumar</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <pages>272–283</pages>
      <abstract>In this paper, we combine methods to estimate sense rankings from raw text with recent work on <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> to provide sense ranking estimates for the entries in the Open Multilingual WordNet (OMW). The existing Word2Vec pre-trained models from Polygot2 are only built for single word entries, we, therefore, re-train them with multiword expressions from the wordnets, so that multiword expressions can also be ranked. Thus this trained <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> gives <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> for both single words and <a href="https://en.wikipedia.org/wiki/Interlingue">multiwords</a>. The resulting <a href="https://en.wikipedia.org/wiki/Lexicon">lexicon</a> gives a WSD baseline for five languages. The results are evaluated for Semcor sense corpora for 5 languages using Word2Vec and Glove models. The Glove model achieves an average accuracy of 0.47 and Word2Vec achieves 0.31 for languages such as <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Italian_language">Italian</a>, <a href="https://en.wikipedia.org/wiki/Indonesian_language">Indonesian</a>, <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a> and <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a>. The experimentation on OMW sense ranking proves that the <a href="https://en.wikipedia.org/wiki/Rank_correlation">rank correlation</a> is generally similar to the human ranking. Hence <a href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional semantics</a> can aid in Wordnet Sense Ranking.</abstract>
      <url hash="3ea89375">2018.gwc-1.32</url>
      <bibkey>vasanthakumar-bond-2018-multilingual</bibkey>
    </paper>
    <paper id="34">
      <title>An Iterative Approach for Unsupervised Most Frequent Sense Detection using <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a> and Word Embeddings<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et and Word Embeddings</title>
      <author><first>Kevin</first><last>Patel</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>293–297</pages>
      <abstract>Given a word, what is the most frequent sense in which it occurs in a given corpus? Most Frequent Sense (MFS) is a strong baseline for unsupervised word sense disambiguation. If we have large amounts of sense-annotated corpora, MFS can be trivially created. However, sense-annotated corpora are a rarity. In this paper, we propose a method which can compute MFS from raw corpora. Our approach iteratively exploits the semantic congruity among related words in corpus. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> performs better compared to another similar work.</abstract>
      <url hash="82a1e150">2018.gwc-1.34</url>
      <bibkey>patel-bhattacharyya-2018-iterative</bibkey>
    </paper>
    <paper id="35">
      <title>Automatic Identification of Basic-Level Categories</title>
      <author><first>Chad</first><last>Mills</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <author><first>Gina-Anne</first><last>Levow</last></author>
      <pages>298–305</pages>
      <abstract>Basic-level categories have been shown to be both psychologically significant and useful in a wide range of practical applications. We build a rule-based system to identify basic-level categories in <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a>, achieving 77 % accuracy on a test set derived from prior psychological experiments. With additional annotations we found our system also has low precision, in part due to the existence of many categories that do not fit into the three classes (superordinate, basic-level, and subordinate) relied on in basic-level category research.</abstract>
      <url hash="8056cd88">2018.gwc-1.35</url>
      <bibkey>mills-etal-2018-automatic</bibkey>
    </paper>
    <paper id="36">
      <title>African Wordnet : facilitating language learning in <a href="https://en.wikipedia.org/wiki/Languages_of_Africa">African languages</a><fixed-case>A</fixed-case>frican <fixed-case>W</fixed-case>ordnet: facilitating language learning in <fixed-case>A</fixed-case>frican languages</title>
      <author><first>Sonja</first><last>Bosch</last></author>
      <author><first>Marissa</first><last>Griesel</last></author>
      <pages>306–313</pages>
      <abstract>The development of the African Wordnet (AWN) has reached a stage of maturity where the first steps towards an <a href="https://en.wikipedia.org/wiki/Application_software">application</a> can be attempted. The AWN is based on the expand method, and to compensate for the general resource scarceness of the <a href="https://en.wikipedia.org/wiki/Languages_of_Africa">African languages</a>, various development strategies were used. The aim of this paper is to investigate the usefulness of the current isiZulu Wordnet in an <a href="https://en.wikipedia.org/wiki/Application_software">application</a> such as <a href="https://en.wikipedia.org/wiki/Language_acquisition">language learning</a>. The advantage of incorporating the <a href="https://en.wikipedia.org/wiki/Wordnet">wordnet</a> of a language into a language learning system is that it provides learners with an integrated application to enhance their learning experience by means of the unique sense identification features of <a href="https://en.wikipedia.org/wiki/Wordnet">wordnets</a>. In this paper it will be demonstrated by means of a variety of examples within the context of a basic free online course how the isiZulu Wordnet can offer the language learner improved decision support.</abstract>
      <url hash="0f476253">2018.gwc-1.36</url>
      <bibkey>bosch-griesel-2018-african</bibkey>
    </paper>
    <paper id="37">
      <title>Hindi Wordnet for <a href="https://en.wikipedia.org/wiki/Language_pedagogy">Language Teaching</a> : Experiences and Lessons Learnt<fixed-case>H</fixed-case>indi <fixed-case>W</fixed-case>ordnet for Language Teaching: Experiences and Lessons Learnt</title>
      <author><first>Hanumant</first><last>Redkar</last></author>
      <author><first>Rajita</first><last>Shukla</last></author>
      <author><first>Sandhya</first><last>Singh</last></author>
      <author><first>Jaya</first><last>Saraswati</last></author>
      <author><first>Laxmi</first><last>Kashyap</last></author>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Preethi</first><last>Jyothi</last></author>
      <author><first>Malhar</first><last>Kulkarni</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>314–323</pages>
      <abstract>This paper reports the work related to making Hindi Wordnet1 available as a digital resource for language learning and teaching, and the experiences and lessons that were learnt during the process. The <a href="https://en.wikipedia.org/wiki/Data_(computing)">language data</a> of the Hindi Wordnet has been suitably modified and enhanced to make it into a <a href="https://en.wikipedia.org/wiki/Language_acquisition">language learning aid</a>. This aid is based on modern pedagogical axioms and is aligned to the learning objectives of the syllabi of the school education in India. To make it into a comprehensive language tool, grammatical information has also been encoded, as far as these can be marked on the lexical items. The delivery of information is multi-layered, multi-sensory and is available across multiple <a href="https://en.wikipedia.org/wiki/Computing_platform">digital platforms</a>. The front end has been designed to offer an eye-catching user-friendly interface which is suitable for learners starting from age six onward. Preliminary testing of the <a href="https://en.wikipedia.org/wiki/Tool">tool</a> has been done and it has been modified as per the feedbacks that were received. Above all, the entire exercise has offered gainful insights into learning based on associative networks and how knowledge based on such <a href="https://en.wikipedia.org/wiki/Social_network">networks</a> can be made available to modern learners.</abstract>
      <url hash="add72dd2">2018.gwc-1.37</url>
      <bibkey>redkar-etal-2018-hindi</bibkey>
    </paper>
    <paper id="41">
      <title>Enchancing the Collaborative Interlingual Index for <a href="https://en.wikipedia.org/wiki/Digital_humanities">Digital Humanities</a> : Cross-linguistic Analysis in the Domain of Theology</title>
      <author><first>Laura</first><last>Slaughter</last></author>
      <author><first>Wenjie</first><last>Wang</last></author>
      <author><first>Luis Morgado Da</first><last>Costa</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <pages>341–346</pages>
      <abstract>We aim to support <a href="https://en.wikipedia.org/wiki/Digital_humanities">digital humanities</a> work related to the study of <a href="https://en.wikipedia.org/wiki/Religious_text">sacred texts</a>. To do this, we propose to build a cross-lingual wordnet within the do-main of theology. We target the Collaborative Interlingual Index (CILI) directly instead of each individual <a href="https://en.wikipedia.org/wiki/Wordnet">wordnet</a>. The paper presents background for this proposal : (1) an overview of concepts relevant to theology and (2) a summary of the domain-associated issues observed in the Princeton WordNet (PWN). We have found that definitions for concepts in this <a href="https://en.wikipedia.org/wiki/Domain_of_discourse">domain</a> can be too restrictive, inconsistent, and unclear. Necessary synsets are missing, with the PWN being skewed towards <a href="https://en.wikipedia.org/wiki/Christianity">Christianity</a>. We argue that tackling problems in a <a href="https://en.wikipedia.org/wiki/Domain-specific_language">single domain</a> is a better method for improving CILI. By focusing on a single topic rather than a single language, this will result in the proper construction of definitions, romanization / translation of lemmas, and also improvements in use of / creation of a cross-lingual domain hierarchy.</abstract>
      <url hash="5c84f182">2018.gwc-1.41</url>
      <bibkey>slaughter-etal-2018-enchancing</bibkey>
    </paper>
    <paper id="43">
      <title>Further expansion of the Croatian WordNet<fixed-case>C</fixed-case>roatian <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Krešimir</first><last>Šojat</last></author>
      <author><first>Matea</first><last>Filko</last></author>
      <author><first>Antoni</first><last>Oliver</last></author>
      <pages>352–357</pages>
      <abstract>In this paper a semi-automatic procedure for the expansion of the Croatian Wordnet (CroWN) is presented. An English-Croatian dictionary was used in order to translate monosemous PWN 3.0 English variants. The <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a> values of the <a href="https://en.wikipedia.org/wiki/Automation">automatic process</a> is low (about 30 %), but the results proved valuable for the enlargment of CroWN. After manual validation, 10,884 new <a href="https://en.wikipedia.org/wiki/Synchrony_and_diachrony">synset-variant pairs</a> were added to CroWN, achieving a total of 62,075 <a href="https://en.wikipedia.org/wiki/Synchrony_and_diachrony">synset-variant pairs</a>.</abstract>
      <url hash="f0c21982">2018.gwc-1.43</url>
      <bibkey>sojat-etal-2018-expansion</bibkey>
    </paper>
    <paper id="48">
      <title>Sinitic Wordnet : Laying the Groundwork with Chinese Varieties Written in Traditional Characters<fixed-case>W</fixed-case>ordnet: Laying the Groundwork with <fixed-case>C</fixed-case>hinese Varieties Written in Traditional Characters</title>
      <author><first>Chih-Yao</first><last>Lee</last></author>
      <author><first>Shu-Kai</first><last>Hsieh</last></author>
      <pages>384–387</pages>
      <abstract>The present work seeks to make the logographic nature of <a href="https://en.wikipedia.org/wiki/Chinese_script">Chinese script</a> a relevant research ground in wordnet studies. While wordnets are not so much about words as about the concepts represented in words, synset formation inevitably involves the use of orthographic and/or phonetic representations to serve as headword for a given concept. For wordnets of Chinese languages, if their synsets are mapped with each other, the connection from logographic forms to lexicalized concepts can be explored backwards to, for instance, help trace the development of cognates in different varieties of <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. The Sinitic Wordnet project is an attempt to construct such an integrated wordnet that aggregates three <a href="https://en.wikipedia.org/wiki/Varieties_of_Chinese">Chinese varieties</a> that are widely spoken in Taiwan and all written in traditional Chinese characters.</abstract>
      <url hash="533fd25f">2018.gwc-1.48</url>
      <bibkey>lee-hsieh-2018-sinitic</bibkey>
    </paper>
    <paper id="50">
      <title>Toward Constructing the National Cancer Institute Thesaurus Derived WordNet (ncitWN)<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et (ncit<fixed-case>WN</fixed-case>)</title>
      <author><first>Amanda</first><last>Hicks</last></author>
      <author><first>Selja</first><last>Seppälä</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <pages>394–400</pages>
      <abstract>We describe preliminary work in the creation of the first specialized vocabulary to be integrated into the Open Multilingual Wordnet (OMW). The NCIt Derived WordNet (ncitWN) is based on the National Cancer Institute Thesaurus (NCIt), a controlled biomedical terminology that includes formal class restrictions and English definitions developed by groups of clinicians and terminologists. The ncitWN is created by converting the NCIt to the WordNet Lexical Markup Framework and adding semantic types. We report the development of a prototype ncitWN and first steps towards integrating <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> into the OMW.</abstract>
      <url hash="e166e33e">2018.gwc-1.50</url>
      <bibkey>hicks-etal-2018-toward</bibkey>
    </paper>
    <paper id="51">
      <title>Towards a Crowd-Sourced WordNet for Colloquial English<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et for Colloquial <fixed-case>E</fixed-case>nglish</title>
      <author><first>John P.</first><last>McCrae</last></author>
      <author><first>Ian</first><last>Wood</last></author>
      <author><first>Amanda</first><last>Hicks</last></author>
      <pages>401–406</pages>
      <abstract>Princeton WordNet is one of the most widely-used resources for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, but is updated only infrequently and can not keep up with the fast-changing usage of the <a href="https://en.wikipedia.org/wiki/English_language">English language</a> on <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a> such as <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>. The Colloquial WordNet aims to provide an open platform whereby anyone can contribute, while still following the structure of <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a>. Many crowd-sourced lexical resources often have significant quality issues, and as such care must be taken in the design of the <a href="https://en.wikipedia.org/wiki/User_interface">interface</a> to ensure quality. In this paper, we present the development of a <a href="https://en.wikipedia.org/wiki/Computing_platform">platform</a> that can be opened on the Web to any lexicographer who wishes to contribute to this resource and the lexicographic methodology applied by this <a href="https://en.wikipedia.org/wiki/Interface_(computing)">interface</a>.</abstract>
      <url hash="89d89ebb">2018.gwc-1.51</url>
      <bibkey>mccrae-etal-2018-towards</bibkey>
    </paper>
    <paper id="53">
      <title>SardaNet : a Linguistic Resource for <a href="https://en.wikipedia.org/wiki/Sardinian_language">Sardinian Language</a><fixed-case>S</fixed-case>arda<fixed-case>N</fixed-case>et: a Linguistic Resource for <fixed-case>S</fixed-case>ardinian Language</title>
      <author><first>Manuela</first><last>Angioni</last></author>
      <author><first>Franco</first><last>Tuveri</last></author>
      <author><first>Maurizio</first><last>Virdis</last></author>
      <author><first>Laura Lucia</first><last>Lai</last></author>
      <author><first>Micol Elisa</first><last>Maltesi</last></author>
      <pages>412–419</pages>
      <abstract>This paper describes the process of building SardaNet, a linguistic resource for <a href="https://en.wikipedia.org/wiki/Sardinian_language">Sardinian language</a> including the different <a href="https://en.wikipedia.org/wiki/Variety_(linguistics)">linguistic varieties</a> in Sardinia. SardaNet aims at identifying the <a href="https://en.wikipedia.org/wiki/Semantics">semantic relations</a> between Sardinian terms, by manually mapping existing WordNet entries to <a href="https://en.wikipedia.org/wiki/Sardinian_language">Sardinian word senses</a>. The work, still in progress, has been developed in collaboration with the University of Cagliari. After discussing some linguistic peculiarities, the paper presents the basic steps of the construction process, the method and the tools involved, the issues encountered during the development and the current version of SardaNet.</abstract>
      <url hash="01c9a3ee">2018.gwc-1.53</url>
      <bibkey>angioni-etal-2018-sardanet</bibkey>
    </paper>
    </volume>
</collection>