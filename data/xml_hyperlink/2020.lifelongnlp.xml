<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.lifelongnlp">
  <volume id="1" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems</booktitle>
      <editor><first>William M.</first><last>Campbell</last></editor>
      <editor><first>Alex</first><last>Waibel</last></editor>
      <editor><first>Dilek</first><last>Hakkani-Tur</last></editor>
      <editor><first>Timothy J.</first><last>Hazen</last></editor>
      <editor><first>Kevin</first><last>Kilgour</last></editor>
      <editor><first>Eunah</first><last>Cho</last></editor>
      <editor><first>Varun</first><last>Kumar</last></editor>
      <editor><first>Hadrien</first><last>Glaude</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="407cd6f4">2020.lifelongnlp-1.0</url>
      <bibkey>lifelongnlp-2020-life</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Supervised Adaptation of Sequence-to-Sequence Speech Recognition Systems using Batch-Weighting</title>
      <author><first>Christian</first><last>Huber</last></author>
      <author><first>Juan</first><last>Hussain</last></author>
      <author><first>Tuan-Nam</first><last>Nguyen</last></author>
      <author><first>Kaihang</first><last>Song</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>9–17</pages>
      <abstract>When training <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition systems</a>, one often faces the situation that sufficient amounts of training data for the language in question are available but only small amounts of data for the domain in question. This problem is even bigger for end-to-end speech recognition systems that only accept transcribed speech as training data, which is harder and more expensive to obtain than text data. In this paper we present experiments in adapting end-to-end speech recognition systems by a method which is called batch-weighting and which we contrast against <a href="https://en.wikipedia.org/wiki/Fine-tuning">regular fine-tuning</a>, i.e., to continue to train existing neural speech recognition models on <a href="https://en.wikipedia.org/wiki/Adaptation_data">adaptation data</a>. We perform experiments using these s techniques in adapting to topic, accent and <a href="https://en.wikipedia.org/wiki/Vocabulary">vocabulary</a>, showing that batch-weighting consistently outperforms <a href="https://en.wikipedia.org/wiki/Musical_tuning">fine-tuning</a>. In order to show the generalization capabilities of batch-weighting we perform experiments in several languages, i.e., <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/German_language">German</a>. Due to its relatively small computational requirements <a href="https://en.wikipedia.org/wiki/Batch_processing">batch-weighting</a> is a suitable technique for <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised life-long learning</a> during the life-time of a <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition system</a>, e.g., from user corrections.</abstract>
      <url hash="e8e4ade1">2020.lifelongnlp-1.2</url>
      <bibkey>huber-etal-2020-supervised</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/how2">How2</pwcdataset>
    </paper>
    </volume>
</collection>