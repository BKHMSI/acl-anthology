<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.ltedi">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion</booktitle>
      <editor><first>Bharathi Raja</first><last>Chakravarthi</last></editor>
      <editor><first>John P.</first><last>McCrae</last></editor>
      <editor><first>Manel</first><last>Zarrouk</last></editor>
      <editor><first>Kalika</first><last>Bali</last></editor>
      <editor><first>Paul</first><last>Buitelaar</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Kyiv</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="bdaa4076">2021.ltedi-1.0</url>
      <bibkey>ltedi-2021-language</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Impact of COVID-19 in Natural Language Processing Publications : a Disaggregated Study in Gender, Contribution and Experience<fixed-case>COVID</fixed-case>-19 in Natural Language Processing Publications: a Disaggregated Study in Gender, Contribution and Experience</title>
      <author><first>Christine</first><last>Basta</last></author>
      <author><first>Marta R.</first><last>Costa-jussa</last></author>
      <pages>1–6</pages>
      <abstract>This study sheds light on the effects of COVID-19 in the particular field of <a href="https://en.wikipedia.org/wiki/Computational_linguistics">Computational Linguistics</a> and <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a> within <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence</a>. We provide an inter-sectional study on gender, contribution, and experience that considers one school year (from August 2019 to August 2020) as a pandemic year. August is included twice for the purpose of an inter-annual comparison. While the trend in publications increased with the crisis, the results show that the ratio between female and male publications decreased. This only helps to reduce the importance of the female role in the scientific contributions of <a href="https://en.wikipedia.org/wiki/Computational_linguistics">computational linguistics</a> (it is now far below its peak of 0.24). The pandemic has a particularly negative effect on the production of female senior researchers in the first position of authors (maximum work), followed by the female junior researchers in the last position of authors (supervision or collaborative work).</abstract>
      <url hash="3e3f01c0">2021.ltedi-1.1</url>
      <bibkey>basta-costa-jussa-2021-impact</bibkey>
    </paper>
    <paper id="4">
      <title>hBERT + BiasCorp-Fighting Racism on the Web<fixed-case>BERT</fixed-case> + <fixed-case>B</fixed-case>ias<fixed-case>C</fixed-case>orp - Fighting Racism on the Web</title>
      <author><first>Olawale</first><last>Onabola</last></author>
      <author><first>Zhuang</first><last>Ma</last></author>
      <author><first>Xie</first><last>Yang</last></author>
      <author><first>Benjamin</first><last>Akera</last></author>
      <author><first>Ibraheem</first><last>Abdulrahman</last></author>
      <author><first>Jia</first><last>Xue</last></author>
      <author><first>Dianbo</first><last>Liu</last></author>
      <author><first>Yoshua</first><last>Bengio</last></author>
      <pages>26–33</pages>
      <abstract>Subtle and overt racism is still present both in physical and online communities today and has impacted many lives in different segments of the society. In this short piece of work, we present how we’re tackling this societal issue with <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a>. We are releasing BiasCorp, a dataset containing 139,090 comments and news segment from three specific sources-Fox News, <a href="https://en.wikipedia.org/wiki/Breitbart_News">BreitbartNews</a> and <a href="https://en.wikipedia.org/wiki/YouTube">YouTube</a>. The first batch (45,000 manually annotated) is ready for publication. We are currently in the final phase of manually labeling the remaining <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> using <a href="https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk">Amazon Mechanical Turk</a>. BERT has been used widely in several downstream tasks. In this work, we present hBERT, where we modify certain layers of the pretrained BERT model with the new Hopfield Layer. hBert generalizes well across different distributions with the added advantage of a reduced model complexity. We are also releasing a JavaScript library 3 and a Chrome Extension Application, to help developers make use of our trained model in web applications (say chat application) and for users to identify and report racially biased contents on the web respectively</abstract>
      <url hash="d39effc3">2021.ltedi-1.4</url>
      <bibkey>onabola-etal-2021-hbert</bibkey>
    </paper>
    <paper id="5">
      <title>An Overview of Fairness in Data   Illuminating the Bias in Data Pipeline</title>
      <author><first>Senthil Kumar</first><last>B</last></author>
      <author><first>Aravindan</first><last>Chandrabose</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <pages>34–45</pages>
      <abstract>Data in general encodes human biases by default ; being aware of this is a good start, and the research around how to handle it is ongoing. The term ‘<a href="https://en.wikipedia.org/wiki/Bias">bias</a>’ is extensively used in various contexts in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP systems</a>. In our research the focus is specific to biases such as <a href="https://en.wikipedia.org/wiki/Gender">gender</a>, <a href="https://en.wikipedia.org/wiki/Racism">racism</a>, <a href="https://en.wikipedia.org/wiki/Religion">religion</a>, demographic and other intersectional views on biases that prevail in text processing systems responsible for systematically discriminating specific population, which is not ethical in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. These <a href="https://en.wikipedia.org/wiki/Bias">biases</a> exacerbate the lack of <a href="https://en.wikipedia.org/wiki/Social_equality">equality</a>, <a href="https://en.wikipedia.org/wiki/Multiculturalism">diversity</a> and inclusion of specific population while utilizing the <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP applications</a>. The tools and technology at the intermediate level utilize <a href="https://en.wikipedia.org/wiki/Bias_(statistics)">biased data</a>, and transfer or amplify this <a href="https://en.wikipedia.org/wiki/Bias_(statistics)">bias</a> to the downstream applications. However, it is not enough to be colourblind, gender-neutral alone when designing a unbiased technology   instead, we should take a conscious effort by designing a unified framework to measure and benchmark the <a href="https://en.wikipedia.org/wiki/Bias">bias</a>. In this paper, we recommend six measures and one augment measure based on the observations of the bias in data, annotations, text representations and debiasing techniques.</abstract>
      <url hash="6b831c7c">2021.ltedi-1.5</url>
      <bibkey>b-etal-2021-overview</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="6">
      <title>GEPSA, a tool for monitoring social challenges in <a href="https://en.wikipedia.org/wiki/Digital_media">digital press</a><fixed-case>GEPSA</fixed-case>, a tool for monitoring social challenges in digital press</title>
      <author><first>Iñaki</first><last>San Vicente</last></author>
      <author><first>Xabier</first><last>Saralegi</last></author>
      <author><first>Nerea</first><last>Zubia</last></author>
      <pages>46–50</pages>
      <abstract>This papers presents a platform for monitoring press narratives with respect to several social challenges, including <a href="https://en.wikipedia.org/wiki/Gender_equality">gender equality</a>, <a href="https://en.wikipedia.org/wiki/Human_migration">migrations</a> and <a href="https://en.wikipedia.org/wiki/Minority_language">minority languages</a>. As narratives are encoded in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a>, we have to use natural processing techniques to automate their analysis. Thus, crawled news are processed by means of several NLP modules, including <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, keyword extraction, <a href="https://en.wikipedia.org/wiki/Document_classification">document classification</a> for social challenge detection, and <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. A Flask powered interface provides <a href="https://en.wikipedia.org/wiki/Data_visualization">data visualization</a> for a user-based analysis of the data. This paper presents the architecture of the <a href="https://en.wikipedia.org/wiki/System">system</a> and describes in detail its different components. Evaluation is provided for the <a href="https://en.wikipedia.org/wiki/Modular_design">modules</a> related to extraction and classification of information regarding <a href="https://en.wikipedia.org/wiki/Social_issue">social challenges</a>.</abstract>
      <url hash="f8e7ae96">2021.ltedi-1.6</url>
      <bibkey>san-vicente-etal-2021-gepsa</bibkey>
    </paper>
    <paper id="7">
      <title>Finding Spoiler Bias in Tweets by Zero-shot Learning and Knowledge Distilling from Neural Text Simplification</title>
      <author><first>Avi</first><last>Bleiweiss</last></author>
      <pages>51–60</pages>
      <abstract>Automatic detection of critical plot information in reviews of media items poses unique challenges to both <a href="https://en.wikipedia.org/wiki/Social_computing">social computing</a> and <a href="https://en.wikipedia.org/wiki/Computational_linguistics">computational linguistics</a>. In this paper we propose to cast the problem of discovering spoiler bias in online discourse as a text simplification task. We conjecture that for an item-user pair, the simpler the user review we learn from an item summary the higher its likelihood to present a spoiler. Our neural model incorporates the advanced transformer network to rank the severity of a spoiler in user tweets. We constructed a sustainable high-quality movie dataset scraped from unsolicited review tweets and paired with a title summary and meta-data extracted from a movie specific domain. To a large extent, our quantitative and qualitative results weigh in on the performance impact of named entity presence in <a href="https://en.wikipedia.org/wiki/Plot_(graphics)">plot summaries</a>. Pretrained on a split-and-rephrase corpus with knowledge distilled from <a href="https://en.wikipedia.org/wiki/English_Wikipedia">English Wikipedia</a> and fine-tuned on our movie dataset, our neural model shows to outperform both a language modeler and monolingual translation baselines.</abstract>
      <url hash="cb3c8c3f">2021.ltedi-1.7</url>
      <attachment type="Dataset" hash="2e3925d5">2021.ltedi-1.7.Dataset.zip</attachment>
      <bibkey>bleiweiss-2021-finding</bibkey>
      <pwccode url="https://github.com/bshalem/mst" additional="false">bshalem/mst</pwccode>
    </paper>
    <paper id="13">
      <title>IIITT@LT-EDI-EACL2021-Hope Speech Detection : There is always hope in Transformers<fixed-case>IIITT</fixed-case>@<fixed-case>LT</fixed-case>-<fixed-case>EDI</fixed-case>-<fixed-case>EACL</fixed-case>2021-Hope Speech Detection: There is always hope in Transformers</title>
      <author><first>Karthik</first><last>Puranik</last></author>
      <author><first>Adeep</first><last>Hande</last></author>
      <author><first>Ruba</first><last>Priyadharshini</last></author>
      <author><first>Sajeetha</first><last>Thavareesan</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <pages>98–106</pages>
      <abstract>In a world with serious challenges like <a href="https://en.wikipedia.org/wiki/Climate_change">climate change</a>, religious and political conflicts, global pandemics, <a href="https://en.wikipedia.org/wiki/Terrorism">terrorism</a>, and racial discrimination, an <a href="https://en.wikipedia.org/wiki/Internet">internet</a> full of <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a>, abusive and offensive content is the last thing we desire for. In this paper, we work to identify and promote positive and supportive content on these <a href="https://en.wikipedia.org/wiki/Computing_platform">platforms</a>. We work with several transformer-based models to classify social media comments as hope speech or not hope speech in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a>, and Tamil languages. This paper portrays our work for the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at LT-EDI 2021- EACL 2021. The codes for our best submission can be viewed.</abstract>
      <url hash="35a0ebf4">2021.ltedi-1.13</url>
      <attachment type="Software" hash="b54b3da8">2021.ltedi-1.13.Software.zip</attachment>
      <bibkey>puranik-etal-2021-iiitt</bibkey>
      <pwccode url="https://github.com/karthikpuranik11/Hope-Speech-Detection-" additional="false">karthikpuranik11/Hope-Speech-Detection-</pwccode>
    </paper>
    <paper id="16">
      <title>ZYJ@LT-EDI-EACL2021 : XLM-RoBERTa-Based Model with Attention for Hope Speech Detection<fixed-case>ZYJ</fixed-case>@<fixed-case>LT</fixed-case>-<fixed-case>EDI</fixed-case>-<fixed-case>EACL</fixed-case>2021:<fixed-case>XLM</fixed-case>-<fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a-Based Model with Attention for Hope Speech Detection</title>
      <author><first>Yingjia</first><last>Zhao</last></author>
      <author><first>Xin</first><last>Tao</last></author>
      <pages>118–121</pages>
      <abstract>Due to the development of modern computer technology and the increase in the number of online media users, we can see all kinds of posts and comments everywhere on the internet. Hope speech can not only inspire the creators but also make other viewers pleasant. It is necessary to effectively and automatically detect hope speech. This paper describes the approach of our team in the task of hope speech detection. We use the attention mechanism to adjust the weight of all the output layers of XLM-RoBERTa to make full use of the information extracted from each layer, and use the weighted sum of all the output layers to complete the classification task. And we use the Stratified-K-Fold method to enhance the <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data set</a>. We achieve a weighted average F1-score of 0.59, 0.84, and 0.92 for <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a>, <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a>, and <a href="https://en.wikipedia.org/wiki/English_language">English language</a>, ranked 3rd, 2nd, and 2nd.</abstract>
      <url hash="9c6805ad">2021.ltedi-1.16</url>
      <attachment type="Software" hash="ce5760dc">2021.ltedi-1.16.Software.zip</attachment>
      <bibkey>zhao-tao-2021-zyj</bibkey>
    </paper>
    <paper id="20">
      <title>TeamUNCC@LT-EDI-EACL2021 : Hope Speech Detection using Transfer Learning with Transformers<fixed-case>T</fixed-case>eam<fixed-case>UNCC</fixed-case>@<fixed-case>LT</fixed-case>-<fixed-case>EDI</fixed-case>-<fixed-case>EACL</fixed-case>2021: Hope Speech Detection using Transfer Learning with Transformers</title>
      <author><first>Khyati</first><last>Mahajan</last></author>
      <author><first>Erfan</first><last>Al-Hossami</last></author>
      <author><first>Samira</first><last>Shaikh</last></author>
      <pages>136–142</pages>
      <abstract>In this paper, we describe our approach towards utilizing pre-trained models for the task of hope speech detection. We participated in Task 2 : Hope Speech Detection for Equality, Diversity and Inclusion at LT-EDI-2021 @ EACL2021. The goal of this task is to predict the presence of hope speech, along with the presence of samples that do not belong to the same language in the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. We describe our approach to fine-tuning RoBERTa for Hope Speech detection in <a href="https://en.wikipedia.org/wiki/English_language">English</a> and our approach to fine-tuning XLM-RoBERTa for Hope Speech detection in <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a> and <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a>, two low resource Indic languages. We demonstrate the performance of our approach on classifying text into hope-speech, non-hope and not-language. Our approach ranked 1st in <a href="https://en.wikipedia.org/wiki/English_language">English</a> (F1 = 0.93), 1st in <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a> (F1 = 0.61) and 3rd in <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a> (F1 = 0.83).</abstract>
      <url hash="b7e23c3d">2021.ltedi-1.20</url>
      <attachment type="Software" hash="dea413ca">2021.ltedi-1.20.Software.zip</attachment>
      <bibkey>mahajan-etal-2021-teamuncc</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hopeedi">HopeEDI</pwcdataset>
    </paper>
    <paper id="21">
      <title>Autobots@LT-EDI-EACL2021 : One World, One Family : Hope Speech Detection with BERT Transformer Model<fixed-case>LT</fixed-case>-<fixed-case>EDI</fixed-case>-<fixed-case>EACL</fixed-case>2021: One World, One Family: Hope Speech Detection with <fixed-case>BERT</fixed-case> Transformer Model</title>
      <author><first>Sunil</first><last>Gundapu</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>143–148</pages>
      <abstract>The rapid rise of <a href="https://en.wikipedia.org/wiki/List_of_social_networking_websites">online social networks</a> like <a href="https://en.wikipedia.org/wiki/YouTube">YouTube</a>, <a href="https://en.wikipedia.org/wiki/Facebook">Facebook</a>, <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> allows people to express their views more widely online. However, at the same time, it can lead to an increase in <a href="https://en.wikipedia.org/wiki/Conflict_(process)">conflict</a> and hatred among consumers in the form of <a href="https://en.wikipedia.org/wiki/Freedom_of_speech">freedom of speech</a>. Therefore, it is essential to take a positive strengthening method to research on encouraging, positive, helping, and supportive social media content. In this paper, we describe a Transformer-based BERT model for Hope speech detection for equality, diversity, and inclusion, submitted for LT-EDI-2021 Task 2. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves a weighted averaged f1-score of 0.93 on the test set.</abstract>
      <url hash="ca01867b">2021.ltedi-1.21</url>
      <attachment type="Software" hash="d95ae8fa">2021.ltedi-1.21.Software.zip</attachment>
      <bibkey>gundapu-mamidi-2021-autobots</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hopeedi">HopeEDI</pwcdataset>
    </paper>
    <paper id="24">
      <title>Hopeful NLP@LT-EDI-EACL2021 : Finding Hope in YouTube Comment Section<fixed-case>NLP</fixed-case>@<fixed-case>LT</fixed-case>-<fixed-case>EDI</fixed-case>-<fixed-case>EACL</fixed-case>2021: Finding Hope in <fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ube Comment Section</title>
      <author><first>Vasudev</first><last>Awatramani</last></author>
      <pages>164–167</pages>
      <abstract>The proliferation of <a href="https://en.wikipedia.org/wiki/Hate_speech">Hate Speech</a> and <a href="https://en.wikipedia.org/wiki/Misinformation">misinformation</a> in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> is fast becoming a menace to society. In compliment, the dissemination of hate-diffusing, promising and anti-oppressive messages become a unique alternative. Unfortunately, due to its complex nature as well as the relatively limited manifestation in comparison to hostile and neutral content, the identification of Hope Speech becomes a challenge. This work revolves around the detection of Hope Speech in Youtube comments, for the Shared Task on Hope Speech Detection for <a href="https://en.wikipedia.org/wiki/Social_equality">Equality</a>, <a href="https://en.wikipedia.org/wiki/Multiculturalism">Diversity</a>, and <a href="https://en.wikipedia.org/wiki/Inclusion_(disability_rights)">Inclusion</a>. We achieve an <a href="https://en.wikipedia.org/wiki/F-score">f-score</a> of 0.93, ranking 1st on the <a href="https://en.wikipedia.org/wiki/Glossary_of_French_expressions_in_English">leaderboard</a> for English comments.</abstract>
      <url hash="9ea81a68">2021.ltedi-1.24</url>
      <bibkey>awatramani-2021-hopeful</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hopeedi">HopeEDI</pwcdataset>
    </paper>
    <paper id="25">
      <title>NLP-CUET@LT-EDI-EACL2021 : Multilingual Code-Mixed Hope Speech Detection using Cross-lingual Representation Learner<fixed-case>NLP</fixed-case>-<fixed-case>CUET</fixed-case>@<fixed-case>LT</fixed-case>-<fixed-case>EDI</fixed-case>-<fixed-case>EACL</fixed-case>2021: Multilingual Code-Mixed Hope Speech Detection using Cross-lingual Representation Learner</title>
      <author><first>Eftekhar</first><last>Hossain</last></author>
      <author><first>Omar</first><last>Sharif</last></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last></author>
      <pages>168–174</pages>
      <abstract>In recent years, several systems have been developed to regulate the spread of negativity and eliminate aggressive, offensive or abusive contents from the online platforms. Nevertheless, a limited number of researches carried out to identify positive, encouraging and supportive contents. In this work, our goal is to identify whether a social media post / comment contains hope speech or not. We propose three distinct models to identify hope speech in English, Tamil and Malayalam language to serve this purpose. To attain this goal, we employed various machine learning (SVM, LR, ensemble), deep learning (CNN+BiLSTM) and transformer (m-BERT, Indic-BERT, XLNet, XLM-R) based methods. Results indicate that XLM-R outdoes all other techniques by gaining a <a href="https://en.wikipedia.org/wiki/Weighted_arithmetic_mean">weighted f_1-score</a> of 0.93, 0.60 and 0.85 respectively for English, Tamil and Malayalam language. Our team has achieved 1st, 2nd and 1st rank in these three <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> respectively.</abstract>
      <url hash="11b16ff6">2021.ltedi-1.25</url>
      <attachment type="Software" hash="53b2858d">2021.ltedi-1.25.Software.zip</attachment>
      <bibkey>hossain-etal-2021-nlp-cuet</bibkey>
      <pwccode url="https://github.com/eftekhar-hossain/CUET_NLP-EACL_2021" additional="false">eftekhar-hossain/CUET_NLP-EACL_2021</pwccode>
    </paper>
    <paper id="28">
      <title>Spartans@LT-EDI-EACL2021 : Inclusive Speech Detection using Pretrained Language Models<fixed-case>LT</fixed-case>-<fixed-case>EDI</fixed-case>-<fixed-case>EACL</fixed-case>2021: Inclusive Speech Detection using Pretrained Language Models</title>
      <author><first>Megha</first><last>Sharma</last></author>
      <author><first>Gaurav</first><last>Arora</last></author>
      <pages>188–192</pages>
      <abstract>We describe our system that ranked first in Hope Speech Detection (HSD) shared task and fourth in Offensive Language Identification (OLI) shared task, both in <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil language</a>. The goal of HSD and OLI is to identify if a code-mixed comment or post contains hope speech or offensive content respectively. We pre-train a transformer-based model RoBERTa using synthetically generated code-mixed data and use it in an ensemble along with their pre-trained ULMFiT model available from iNLTK.</abstract>
      <url hash="0c7a2996">2021.ltedi-1.28</url>
      <attachment type="Software" hash="6b9ae7c3">2021.ltedi-1.28.Software.zip</attachment>
      <bibkey>sharma-arora-2021-spartans</bibkey>
    </paper>
    </volume>
</collection>