<?xml version='1.0' encoding='utf-8'?>
<collection id="U19">
  <volume id="1" ingest-date="2019-12-16">
    <meta>
      <booktitle>Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association</booktitle>
      <url hash="f09ad8ed">U19-1</url>
      <editor><first>Meladel</first><last>Mistica</last></editor>
      <editor><first>Massimo</first><last>Piccardi</last></editor>
      <editor><first>Andrew</first><last>MacKinlay</last></editor>
      <publisher>Australasian Language Technology Association</publisher>
      <address>Sydney, Australia</address>
      <month>4--6 December</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="4886300f">U19-1000</url>
      <bibkey>alta-2019-australasian</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Towards A Robust Morphological Analyzer for Kunwinjku</title>
      <author><first>William</first><last>Lane</last></author>
      <author><first>Steven</first><last>Bird</last></author>
      <pages>1–9</pages>
      <abstract>Kunwinjku is an <a href="https://en.wikipedia.org/wiki/Australian_Aboriginal_languages">indigenous Australian language</a> spoken in northern Australia which exhibits agglutinative and polysynthetic properties. Members of the community have expressed interest in co-developing language applications that promote their values and priorities. Modeling the <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphology</a> of the <a href="https://en.wikipedia.org/wiki/Kunwinjku_language">Kunwinjku language</a> is an important step towards accomplishing the community’s goals. Finite State Transducers have long been the go-to method for modeling morphologically rich languages, and in this paper we discuss some of the distinct modeling challenges present in the <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphosyntax</a> of verbs in <a href="https://en.wikipedia.org/wiki/Kunwinjku">Kunwinjku</a>. We show that a fairly straightforward implementation using standard features of the foma toolkit can account for much of the verb structure. Continuing challenges include <a href="https://en.wikipedia.org/wiki/Robustness_(evolution)">robustness</a> in the face of <a href="https://en.wikipedia.org/wiki/Variation_(linguistics)">variation</a> and unseen vocabulary, as well as how to handle complex reduplicative processes. Our future work will build off the baseline and challenges presented here.</abstract>
      <url hash="d6978dd8">U19-1001</url>
      <bibkey>lane-bird-2019-towards</bibkey>
    </paper>
    <paper id="3">
      <title>Readability of Twitter Tweets for Second Language Learners<fixed-case>T</fixed-case>witter Tweets for Second Language Learners</title>
      <author><first>Patrick</first><last>Jacob</last></author>
      <author><first>Alexandra</first><last>Uitdenbogerd</last></author>
      <pages>19–27</pages>
      <abstract>Optimal <a href="https://en.wikipedia.org/wiki/Language_acquisition">language acquisition</a> via <a href="https://en.wikipedia.org/wiki/Reading">reading</a> requires the learners to read slightly above their current <a href="https://en.wikipedia.org/wiki/Language_proficiency">language skill level</a>. Identifying material at the right level is the essential role of automatic readability measurement. Short message platforms such as <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> offer the opportunity for <a href="https://en.wikipedia.org/wiki/Language_acquisition">language practice</a> while reading about current topics and engaging in conversation in small doses, and can be filtered according to linguistic criteria to suit the learner. In this research, we explore how readable tweets are for <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">English language learners</a> and which factors contribute to their <a href="https://en.wikipedia.org/wiki/Readability">readability</a>. With participants from six language groups, we collected 14,659 data points, each representing a tweet from a pool of 4100 tweets, and a judgement of perceived readability. Traditional readability measures and <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> failed on the data-set, but <a href="https://en.wikipedia.org/wiki/Demography">demographic data</a> showed that judgements were largely genuine and reflected reported language skill, which is consistent with other recent studies. We report on the properties of the data set and implications for future research.</abstract>
      <url hash="f153dffd">U19-1003</url>
      <bibkey>jacob-uitdenbogerd-2019-readability</bibkey>
    </paper>
    <paper id="10">
      <title>Improved <a href="https://en.wikipedia.org/wiki/Document_modelling">Document Modelling</a> with a Neural Discourse Parser</title>
      <author><first>Fajri</first><last>Koto</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>67–76</pages>
      <abstract>Despite the success of attention-based neural models for natural language generation and classification tasks, they are unable to capture the discourse structure of larger documents. We hypothesize that explicit discourse representations have utility for NLP tasks over longer documents or document sequences, which sequence-to-sequence models are unable to capture. For abstractive summarization, for instance, conventional neural models simply match source documents and the summary in a latent space without explicit representation of text structure or relations. In this paper, we propose to use neural discourse representations obtained from a rhetorical structure theory (RST) parser to enhance document representations. Specifically, document representations are generated for discourse spans, known as the elementary discourse units (EDUs). We empirically investigate the benefit of the proposed approach on two different tasks : abstractive summarization and popularity prediction of online petitions. We find that the proposed approach leads to substantial improvements in all cases.</abstract>
      <url hash="1e04d100">U19-1010</url>
      <bibkey>koto-etal-2019-improved</bibkey>
      <pwccode url="https://github.com/fajri91/RSTExtractor" additional="false">fajri91/RSTExtractor</pwccode>
    </paper>
    <paper id="11">
      <title>Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a><fixed-case>LSTM</fixed-case> forget more than a <fixed-case>CNN</fixed-case>? An empirical study of catastrophic forgetting in <fixed-case>NLP</fixed-case></title>
      <author><first>Gaurav</first><last>Arora</last></author>
      <author><first>Afshin</first><last>Rahimi</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>77–86</pages>
      <abstract>Catastrophic forgetting   whereby a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> trained on one <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is fine-tuned on a second, and in doing so, suffers a catastrophic drop in performance over the first <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>   is a hurdle in the development of better transfer learning techniques. Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different <a href="https://en.wikipedia.org/wiki/Network_architecture">architectures</a> and hyper-parameters affect <a href="https://en.wikipedia.org/wiki/Forgetting">forgetting</a> in a <a href="https://en.wikipedia.org/wiki/Computer_network">network</a>. With this study, we aim to understand factors which cause <a href="https://en.wikipedia.org/wiki/Forgetting">forgetting</a> during sequential training. Our primary finding is that CNNs forget less than LSTMs. We show that max-pooling is the underlying operation which helps CNNs alleviate forgetting compared to LSTMs. We also found that curriculum learning, placing a hard task towards the end of task sequence, reduces <a href="https://en.wikipedia.org/wiki/Forgetting">forgetting</a>. We analysed the effect of <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning contextual embeddings</a> on catastrophic forgetting and found that using embeddings as feature extractor is preferable to <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> in continual learning setup.</abstract>
      <url hash="5d63513f">U19-1011</url>
      <bibkey>arora-etal-2019-lstm</bibkey>
    </paper>
    <paper id="14">
      <title>Detecting Chemical Reactions in Patents</title>
      <author><first>Hiyori</first><last>Yoshikawa</last></author>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <author><first>Zenan</first><last>Zhai</last></author>
      <author><first>Christian</first><last>Druckenbrodt</last></author>
      <author><first>Camilo</first><last>Thorne</last></author>
      <author><first>Saber A.</first><last>Akhondi</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>100–110</pages>
      <abstract>Extracting <a href="https://en.wikipedia.org/wiki/Chemical_reaction">chemical reactions</a> from <a href="https://en.wikipedia.org/wiki/Patent">patents</a> is a crucial task for chemists working on <a href="https://en.wikipedia.org/wiki/Chemical_engineering">chemical exploration</a>. In this paper we introduce the novel task of detecting the textual spans that describe or refer to <a href="https://en.wikipedia.org/wiki/Chemical_reaction">chemical reactions</a> within patents. We formulate this task as a paragraph-level sequence tagging problem, where the system is required to return a sequence of paragraphs which contain a description of a reaction. To address this new task, we construct an annotated dataset from an existing proprietary database of chemical reactions manually extracted from <a href="https://en.wikipedia.org/wiki/Patent">patents</a>. We introduce several baseline methods for the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> and evaluate them over our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. Through error analysis, we discuss what makes the task complex and challenging, and suggest possible directions for future research.</abstract>
      <url hash="748e8b9c">U19-1014</url>
      <bibkey>yoshikawa-etal-2019-detecting</bibkey>
    </paper>
    <paper id="15">
      <title>Identifying Patients with Pain in Emergency Departments using Conventional <a href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning</a> and Deep Learning</title>
      <author><first>Thanh</first><last>Vu</last></author>
      <author><first>Anthony</first><last>Nguyen</last></author>
      <author><first>Nathan</first><last>Brown</last></author>
      <author><first>James</first><last>Hughes</last></author>
      <pages>111–119</pages>
      <abstract>Pain is the main symptom that patients present with to the emergency department (ED). Pain management, however, is often poorly done aspect of <a href="https://en.wikipedia.org/wiki/Emergency_medicine">emergency care</a> and patients with painful conditions can endure long waits before their pain is assessed or treated. To improve pain management quality, identifying whether or not an ED patient presents with pain is an important task and allows for further investigation of the quality of care provided. In this paper, <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> was utilised to handle the task of automatically detecting patients who present at EDs with pain from retrospective data. Experimental results on a manually annotated dataset show that our proposed <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning models</a> achieve high performances, in which the highest <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and macro-averaged F1 are 91.00 % and 90.96 %, respectively.</abstract>
      <url hash="ad494e6e">U19-1015</url>
      <bibkey>vu-etal-2019-identifying</bibkey>
    </paper>
    <paper id="24">
      <title>An Improved Coarse-to-Fine Method for Solving Generation Tasks</title>
      <author><first>Wenyv</first><last>Guan</last></author>
      <author><first>Qianying</first><last>Liu</last></author>
      <author><first>Guangzhi</first><last>Han</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <pages>178–185</pages>
      <abstract>The coarse-to-fine (coarse2fine) methods have recently been widely used in the generation tasks. The methods first generate a rough sketch in the coarse stage and then use the sketch to get the final result in the fine stage. However, <a href="https://en.wikipedia.org/wiki/They_(2017_film)">they</a> usually lack the correction ability when getting a wrong sketch. To solve this problem, in this paper, we propose an improved coarse2fine model with a control mechanism, with which our method can control the influence of the sketch on the final results in the fine stage. Even if the sketch is wrong, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> still has the opportunity to get a correct result. We have experimented our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> on the tasks of <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a> and math word problem solving. The results have shown the effectiveness of our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>.</abstract>
      <url hash="c185b3e4">U19-1024</url>
      <bibkey>guan-etal-2019-improved</bibkey>
    </paper>
    </volume>
</collection>