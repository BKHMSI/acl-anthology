<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.bucc">
  <volume id="1" ingest-date="2021-11-09">
    <meta>
      <booktitle>Proceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021)</booktitle>
      <editor><first>Reinhard</first><last>Rapp</last></editor>
      <editor><first>Serge</first><last>Sharoff</last></editor>
      <editor><first>Pierre</first><last>Zweigenbaum</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Online (Virtual Mode)</address>
      <month>September</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="f525bf72">2021.bucc-1.0</url>
      <bibkey>bucc-2021-building</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Syntax-aware Transformers for Neural Machine Translation : The Case of Text to Sign Gloss Translation</title>
      <author><first>Santiago</first><last>Egea Gómez</last></author>
      <author><first>Euan</first><last>McGill</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>18–27</pages>
      <abstract>It is well-established that the preferred mode of communication of the deaf and hard of hearing (DHH) community are Sign Languages (SLs), but they are considered low resource languages where natural language processing technologies are of concern. In this paper we study the problem of text to SL gloss Machine Translation (MT) using Transformer-based architectures. Despite the significant advances of MT for spoken languages in the recent couple of decades, MT is in its infancy when it comes to <a href="https://en.wikipedia.org/wiki/Standard_language">SLs</a>. We enrich a Transformer-based architecture aggregating syntactic information extracted from a dependency parser to word-embeddings. We test our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on a well-known dataset showing that the syntax-aware model obtains performance gains in terms of MT evaluation metrics.</abstract>
      <url hash="2ddf419a">2021.bucc-1.4</url>
      <bibkey>egea-gomez-etal-2021-syntax</bibkey>
      <pwccode url="https://github.com/lastus-taln-upf/syntax-aware-transformer-text2gloss" additional="false">lastus-taln-upf/syntax-aware-transformer-text2gloss</pwccode>
    </paper>
    <paper id="5">
      <title>Employing <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> as a resource for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entity Recognition</a> in Morphologically complex under-resourced languages<fixed-case>W</fixed-case>ikipedia as a resource for Named Entity Recognition in Morphologically complex under-resourced languages</title>
      <author><first>Aravind</first><last>Krishnan</last></author>
      <author><first>Stefan</first><last>Ziehe</last></author>
      <author><first>Franziska</first><last>Pannach</last></author>
      <author><first>Caroline</first><last>Sporleder</last></author>
      <pages>28–39</pages>
      <abstract>We propose a novel approach for rapid prototyping of <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognisers</a> through the development of semi-automatically annotated datasets. We demonstrate the proposed pipeline on two under-resourced agglutinating languages : the <a href="https://en.wikipedia.org/wiki/Malayalam">Dravidian language Malayalam</a> and the <a href="https://en.wikipedia.org/wiki/Zulu_language">Bantu language isiZulu</a>. Our approach is weakly supervised and bootstraps training data from <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> and <a href="https://en.wikipedia.org/wiki/Google_Knowledge_Graph">Google Knowledge Graph</a>. Moreover, our approach is relatively language independent and can consequently be ported quickly (and hence cost-effectively) from one language to another, requiring only minor language-specific tailoring.</abstract>
      <url hash="949c0b5b">2021.bucc-1.5</url>
      <bibkey>krishnan-etal-2021-employing</bibkey>
    </paper>
    <paper id="7">
      <title>Majority Voting with Bidirectional Pre-translation For Bitext Retrieval</title>
      <author><first>Alexander</first><last>Jones</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>46–59</pages>
      <abstract>Obtaining high-quality parallel corpora is of paramount importance for training NMT systems. However, as many language pairs lack adequate gold-standard training data, a popular approach has been to mine so-called pseudo-parallel sentences from paired documents in two languages. In this paper, we outline some drawbacks with current methods that rely on an embedding similarity threshold, and propose a <a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)">heuristic method</a> in its place. Our method involves translating both halves of a paired corpus before mining, and then performing a majority vote on sentence pairs mined in three ways : after translating documents in language x to language y, after translating language y to x, and using the original documents in languages x and y. We demonstrate success with this novel approach on the Tatoeba similarity search benchmark in 64 low-resource languages, and on NMT in <a href="https://en.wikipedia.org/wiki/Kazakh_language">Kazakh</a> and Gujarati. We also uncover the effect of <a href="https://en.wikipedia.org/wiki/Resource_(biology)">resource-related factors</a> (i.e. how much monolingual / bilingual data is available for a given language) on the optimal choice of bitext mining method, demonstrating that there is currently no one-size-fits-all approach for this task. We make the code and data used in our experiments publicly available.</abstract>
      <url hash="9e9ffbc4">2021.bucc-1.7</url>
      <bibkey>jones-wijaya-2021-majority</bibkey>
      <pwccode url="https://github.com/AlexJonesNLP/alt-bitexts" additional="false">AlexJonesNLP/alt-bitexts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bucc">BUCC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tatoeba">Tatoeba</pwcdataset>
    </paper>
    <paper id="8">
      <title>EM Corpus : a comparable corpus for a less-resourced language pair Manipuri-English<fixed-case>EM</fixed-case> Corpus: a comparable corpus for a less-resourced language pair <fixed-case>M</fixed-case>anipuri-<fixed-case>E</fixed-case>nglish</title>
      <author><first>Rudali</first><last>Huidrom</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <author><first>Khogendra</first><last>Khomdram</last></author>
      <pages>60–67</pages>
      <abstract>In this paper, we introduce a sentence-level comparable text corpus crawled and created for the less-resourced language pair, Manipuri(mni) and English (eng). Our monolingual corpora comprise 1.88 million Manipuri sentences and 1.45 million English sentences, and our <a href="https://en.wikipedia.org/wiki/Parallel_corpus">parallel corpus</a> comprises 124,975 Manipuri-English sentence pairs. These data were crawled and collected over a year from August 2020 to March 2021 from a local newspaper website called ‘The Sangai Express.’ The resources reported in this paper are made available to help the low-resourced languages community for MT / NLP tasks.</abstract>
      <url hash="a4e1b714">2021.bucc-1.8</url>
      <bibkey>huidrom-etal-2021-em</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
    </paper>
    <paper id="9">
      <title>On Pronunciations in <a href="https://en.wikipedia.org/wiki/Wiktionary">Wiktionary</a> : Extraction and Experiments on Multilingual Syllabification and Stress Prediction<fixed-case>W</fixed-case>iktionary: Extraction and Experiments on Multilingual Syllabification and Stress Prediction</title>
      <author><first>Winston</first><last>Wu</last></author>
      <author><first>David</first><last>Yarowsky</last></author>
      <pages>68–74</pages>
      <abstract>We constructed <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a> for five non-English editions of <a href="https://en.wikipedia.org/wiki/Wiktionary">Wiktionary</a>, which combined with <a href="https://en.wikipedia.org/wiki/Pronunciation">pronunciations</a> from the English edition, comprises over 5.3 million IPA pronunciations, the largest pronunciation lexicon of its kind. This <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is a unique comparable corpus of IPA pronunciations annotated from multiple sources. We analyze the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, noting the presence of machine-generated pronunciations. We develop a novel <a href="https://en.wikipedia.org/wiki/Visualization_(graphics)">visualization method</a> to quantify <a href="https://en.wikipedia.org/wiki/Syllabification">syllabification</a>. We experiment on the new combined task of multilingual IPA syllabification and stress prediction, finding that training a massively multilingual neural sequence-to-sequence model with copy attention can improve performance on both high- and low-resource languages, and multi-task training on stress prediction helps with <a href="https://en.wikipedia.org/wiki/Syllabification">syllabification</a>.</abstract>
      <url hash="0a8e6918">2021.bucc-1.9</url>
      <bibkey>wu-yarowsky-2021-pronunciations</bibkey>
    </paper>
    <paper id="10">
      <title>A Dutch Dataset for Cross-lingual Multilabel Toxicity Detection<fixed-case>D</fixed-case>utch Dataset for Cross-lingual Multilabel Toxicity Detection</title>
      <author><first>Ben</first><last>Burtenshaw</last></author>
      <author><first>Mike</first><last>Kestemont</last></author>
      <pages>75–79</pages>
      <abstract>Multi-label toxicity detection is highly prominent, with many research groups, companies, and individuals engaging with it through shared tasks and dedicated venues. This paper describes a cross-lingual approach to annotating multi-label text classification on a newly developed Dutch language dataset, using a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained on English data. We present an ensemble model of one Transformer model and an LSTM using Multilingual embeddings. The combination of multilingual embeddings and the Transformer model improves performance in a cross-lingual setting.</abstract>
      <url hash="ae0ef078">2021.bucc-1.10</url>
      <bibkey>burtenshaw-kestemont-2021-dutch</bibkey>
    </paper>
  </volume>
</collection>