<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.bea">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</booktitle>
      <editor><first>Jill</first><last>Burstein</last></editor>
      <editor><first>Ekaterina</first><last>Kochmar</last></editor>
      <editor><first>Claudia</first><last>Leacock</last></editor>
      <editor><first>Nitin</first><last>Madnani</last></editor>
      <editor><first>Ildikó</first><last>Pilán</last></editor>
      <editor><first>Helen</first><last>Yannakoudakis</last></editor>
      <editor><first>Torsten</first><last>Zesch</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, WA, USA → Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="24dd231d">2020.bea-1</url>
    </meta>
    <frontmatter>
      <url hash="d9b0a62a">2020.bea-1.0</url>
      <bibkey>bea-2020-innovative</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Complementary Systems for Off-Topic Spoken Response Detection</title>
      <author><first>Vatsal</first><last>Raina</last></author>
      <author><first>Mark</first><last>Gales</last></author>
      <author><first>Kate</first><last>Knill</last></author>
      <pages>41–51</pages>
      <abstract>Increased demand to learn English for business and education has led to growing interest in automatic spoken language assessment and teaching systems. With this shift to automated approaches it is important that systems reliably assess all aspects of a candidate’s responses. This paper examines one form of spoken language assessment ; whether the response from the candidate is relevant to the prompt provided. This will be referred to as off-topic spoken response detection. Two forms of previously proposed approaches are examined in this work : the hierarchical attention-based topic model (HATM) ; and the similarity grid model (SGM). The work focuses on the scenario when the prompt, and associated responses, have not been seen in the training data, enabling the <a href="https://en.wikipedia.org/wiki/System">system</a> to be applied to new test scripts without the need to collect data or retrain the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>. To improve the performance of the systems for unseen prompts, <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> based on easy data augmentation (EDA) and translation based approaches are applied. Additionally for the HATM, a form of prompt dropout is described. The <a href="https://en.wikipedia.org/wiki/System">systems</a> were evaluated on both seen and unseen prompts from Linguaskill Business and General English tests. For unseen data the performance of the HATM was improved using <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, in contrast to the <a href="https://en.wikipedia.org/wiki/SGM">SGM</a> where no gains were obtained. The two <a href="https://en.wikipedia.org/wiki/Computer_simulation">approaches</a> were found to be complementary to one another, yielding a combined <a href="https://en.wikipedia.org/wiki/F-number">F0.5 score</a> of 0.814 for off-topic response detection where the prompts have not been seen in training.</abstract>
      <url hash="a7612ffd">2020.bea-1.4</url>
      <doi>10.18653/v1/2020.bea-1.4</doi>
      <bibkey>raina-etal-2020-complementary</bibkey>
    </paper>
    <paper id="5">
      <title>CIMA : A Large Open Access Dialogue Dataset for Tutoring<fixed-case>CIMA</fixed-case>: A Large Open Access Dialogue Dataset for Tutoring</title>
      <author><first>Katherine</first><last>Stasaski</last></author>
      <author><first>Kimberly</first><last>Kao</last></author>
      <author><first>Marti A.</first><last>Hearst</last></author>
      <pages>52–64</pages>
      <abstract>One-to-one tutoring is often an effective means to help students learn, and recent experiments with neural conversation systems are promising. However, large open datasets of tutoring conversations are lacking. To remedy this, we propose a novel asynchronous method for collecting tutoring dialogue via crowdworkers that is both amenable to the needs of deep learning algorithms and reflective of pedagogical concerns. In this approach, extended conversations are obtained between crowdworkers role-playing as both students and tutors. The CIMA collection, which we make publicly available, is novel in that students are exposed to overlapping grounded concepts between exercises and multiple relevant tutoring responses are collected for the same input. CIMA contains several compelling properties from an educational perspective : student role-players complete exercises in fewer turns during the course of the conversation and tutor players adopt strategies that conform with some educational conversational norms, such as providing hints versus asking questions in appropriate contexts. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> enables a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> to be trained to generate the next tutoring utterance in a conversation, conditioned on a provided action strategy.</abstract>
      <url hash="01bc5caa">2020.bea-1.5</url>
      <doi>10.18653/v1/2020.bea-1.5</doi>
      <bibkey>stasaski-etal-2020-cima</bibkey>
    </paper>
    <paper id="6">
      <title>Becoming Linguistically Mature : Modeling English and German Children’s Writing Development Across School Grades<fixed-case>E</fixed-case>nglish and <fixed-case>G</fixed-case>erman Children’s Writing Development Across School Grades</title>
      <author><first>Elma</first><last>Kerz</last></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Daniel</first><last>Wiechmann</last></author>
      <author><first>Marcus</first><last>Ströbel</last></author>
      <pages>65–74</pages>
      <abstract>In this paper we employ a novel approach to advancing our understanding of the development of writing in English and German children across school grades using classification tasks. The <a href="https://en.wikipedia.org/wiki/Data">data</a> used come from two recently compiled corpora : The English data come from the the GiC corpus (983 school children in second-, sixth-, ninth- and eleventh-grade) and the German data are from the FD-LEX corpus (930 school children in fifth- and ninth-grade). The key to this paper is the combined use of what we refer to as ‘complexity contours’, i.e. series of measurements that capture the progression of linguistic complexity within a text, and Recurrent Neural Network (RNN) classifiers that adequately capture the sequential information in those contours. Our experiments demonstrate that RNN classifiers trained on complexity contours achieve higher classification accuracy than one trained on text-average complexity scores. In a second step, we determine the relative importance of the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> from four distinct categories through a Sensitivity-Based Pruning approach.</abstract>
      <url hash="b544b2f5">2020.bea-1.6</url>
      <doi>10.18653/v1/2020.bea-1.6</doi>
      <attachment type="Dataset" hash="6275ac98">2020.bea-1.6.Dataset.pdf</attachment>
      <bibkey>kerz-etal-2020-becoming</bibkey>
    </paper>
    <paper id="8">
      <title>Can <a href="https://en.wikipedia.org/wiki/Neural_network">Neural Networks</a> Automatically Score Essay Traits?</title>
      <author><first>Sandeep</first><last>Mathias</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>85–91</pages>
      <abstract>Essay traits are attributes of an essay that can help explain how well written (or badly written) the essay is. Examples of traits include <a href="https://en.wikipedia.org/wiki/Content_(media)">Content</a>, Organization, <a href="https://en.wikipedia.org/wiki/Language">Language</a>, Sentence Fluency, <a href="https://en.wikipedia.org/wiki/Word_choice">Word Choice</a>, etc. A lot of research in the last decade has dealt with automatic holistic essay scoring-where a machine rates an essay and gives a score for the essay. However, writers need feedback, especially if they want to improve their writing-which is why trait-scoring is important. In this paper, we show how a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep-learning based system</a> can outperform feature-based machine learning systems, as well as a string kernel system in scoring <a href="https://en.wikipedia.org/wiki/Essay">essay traits</a>.</abstract>
      <url hash="7665989d">2020.bea-1.8</url>
      <doi>10.18653/v1/2020.bea-1.8</doi>
      <bibkey>mathias-bhattacharyya-2020-neural</bibkey>
    </paper>
    <paper id="12">
      <title>Applications of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a> in Bilingual Language Teaching : An Indonesian-English Case Study<fixed-case>I</fixed-case>ndonesian-<fixed-case>E</fixed-case>nglish Case Study</title>
      <author><first>Zara</first><last>Maxwelll-Smith</last></author>
      <author><first>Simón</first><last>González Ochoa</last></author>
      <author><first>Ben</first><last>Foley</last></author>
      <author><first>Hanna</first><last>Suominen</last></author>
      <pages>124–134</pages>
      <abstract>Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this <a href="https://en.wikipedia.org/wiki/Data">data</a> so rich and problematic to classify. In this paper, we set out methodological considerations of using <a href="https://en.wikipedia.org/wiki/Speech_recognition">automated speech recognition</a> to build a <a href="https://en.wikipedia.org/wiki/Speech_corpus">corpus of teacher speech</a> in an Indonesian language classroom. Our preliminary results (64 % word error rate) suggest these tools have the potential to speed <a href="https://en.wikipedia.org/wiki/Data_collection">data collection</a> in this context. We provide practical examples of our data structure, details of our piloted computer-assisted processes, and fine-grained error analysis. Our study is informed and directed by genuine research questions and discussion in both the education and computational linguistics fields. We highlight some of the benefits and risks of using these emerging <a href="https://en.wikipedia.org/wiki/Technology">technologies</a> to analyze the complex work of <a href="https://en.wikipedia.org/wiki/Language_education">language teachers</a> and in <a href="https://en.wikipedia.org/wiki/Education">education</a> more generally.</abstract>
      <url hash="dd31f883">2020.bea-1.12</url>
      <doi>10.18653/v1/2020.bea-1.12</doi>
      <bibkey>maxwelll-smith-etal-2020-applications</bibkey>
    </paper>
    <paper id="13">
      <title>An empirical investigation of neural methods for content scoring of science explanations</title>
      <author><first>Brian</first><last>Riordan</last></author>
      <author><first>Sarah</first><last>Bichler</last></author>
      <author><first>Allison</first><last>Bradford</last></author>
      <author><first>Jennifer</first><last>King Chen</last></author>
      <author><first>Korah</first><last>Wiley</last></author>
      <author><first>Libby</first><last>Gerard</last></author>
      <author><first>Marcia</first><last>C. Linn</last></author>
      <pages>135–144</pages>
      <abstract>With the widespread adoption of the Next Generation Science Standards (NGSS), science teachers and online learning environments face the challenge of evaluating students’ integration of different dimensions of <a href="https://en.wikipedia.org/wiki/Science_education">science learning</a>. Recent advances in representation learning in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> have proven effective across many <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing tasks</a>, but a rigorous evaluation of the relative merits of these methods for scoring complex constructed response formative assessments has not previously been carried out. We present a detailed empirical investigation of feature-based, recurrent neural network, and pre-trained transformer models on scoring content in real-world formative assessment data. We demonstrate that recent <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural methods</a> can rival or exceed the performance of feature-based methods. We also provide evidence that different classes of neural models take advantage of different learning cues, and pre-trained transformer models may be more robust to spurious, dataset-specific learning cues, better reflecting scoring rubrics.</abstract>
      <url hash="c81cf5e9">2020.bea-1.13</url>
      <doi>10.18653/v1/2020.bea-1.13</doi>
      <bibkey>riordan-etal-2020-empirical</bibkey>
    </paper>
    <paper id="16">
      <title>GECToR   Grammatical Error Correction : Tag, Not Rewrite<fixed-case>GECT</fixed-case>o<fixed-case>R</fixed-case> – Grammatical Error Correction: Tag, Not Rewrite</title>
      <author><first>Kostiantyn</first><last>Omelianchuk</last></author>
      <author><first>Vitaliy</first><last>Atrasevych</last></author>
      <author><first>Artem</first><last>Chernodub</last></author>
      <author><first>Oleksandr</first><last>Skurzhanskyi</last></author>
      <pages>163–170</pages>
      <abstract>In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages : first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model / ensemble GEC tagger achieves an F_0.5 of 65.3/66.5 on CONLL-2014 (test) and F_0.5 of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.</abstract>
      <url hash="7ed8a32f">2020.bea-1.16</url>
      <doi>10.18653/v1/2020.bea-1.16</doi>
      <bibkey>omelianchuk-etal-2020-gector</bibkey>
      <pwccode url="https://github.com/grammarly/gector" additional="true">grammarly/gector</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
    </paper>
    <paper id="17">
      <title>Interpreting Neural CWI Classifiers’ Weights as Vocabulary Size<fixed-case>CWI</fixed-case> Classifiers’ Weights as Vocabulary Size</title>
      <author><first>Yo</first><last>Ehara</last></author>
      <pages>171–176</pages>
      <abstract>Complex Word Identification (CWI) is a task for the identification of words that are challenging for <a href="https://en.wikipedia.org/wiki/Second-language_acquisition">second-language learners</a> to read. Even though the use of neural classifiers is now common in CWI, the interpretation of their parameters remains difficult. This paper analyzes neural CWI classifiers and shows that some of their parameters can be interpreted as <a href="https://en.wikipedia.org/wiki/Vocabulary_size">vocabulary size</a>. We present a novel formalization of vocabulary size measurement methods that are practiced in the applied linguistics field as a kind of neural classifier. We also contribute to building a novel <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for validating vocabulary testing and readability via <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a>.</abstract>
      <url hash="f7390621">2020.bea-1.17</url>
      <doi>10.18653/v1/2020.bea-1.17</doi>
      <attachment type="Dataset" hash="de00ecff">2020.bea-1.17.Dataset.zip</attachment>
      <bibkey>ehara-2020-interpreting</bibkey>
    </paper>
    <paper id="20">
      <title>Predicting the Difficulty and Response Time of Multiple Choice Questions Using Transfer Learning</title>
      <author><first>Kang</first><last>Xue</last></author>
      <author><first>Victoria</first><last>Yaneva</last></author>
      <author><first>Christopher</first><last>Runyon</last></author>
      <author><first>Peter</first><last>Baldwin</last></author>
      <pages>193–197</pages>
      <abstract>This paper investigates whether <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> can improve the prediction of the difficulty and response time parameters for 18,000 multiple-choice questions from a high-stakes medical exam. The type the signal that best predicts difficulty and <a href="https://en.wikipedia.org/wiki/Response_time_(technology)">response time</a> is also explored, both in terms of <a href="https://en.wikipedia.org/wiki/Abstraction_(computer_science)">representation abstraction</a> and item component used as input (e.g., whole item, answer options only, etc.). The results indicate that, for our sample, <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> can improve the prediction of item difficulty when <a href="https://en.wikipedia.org/wiki/Mental_chronometry">response time</a> is used as an auxiliary task but not the other way around. In addition, difficulty was best predicted using signal from the item stem (the description of the clinical case), while all parts of the item were important for predicting the response time.</abstract>
      <url hash="1f9e95f0">2020.bea-1.20</url>
      <doi>10.18653/v1/2020.bea-1.20</doi>
      <bibkey>xue-etal-2020-predicting</bibkey>
    </paper>
    </volume>
</collection>