<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.vardial">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects</booktitle>
      <editor><first>Marcos</first><last>Zampieri</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Nikola</first><last>Ljubešić</last></editor>
      <editor><first>Jörg</first><last>Tiedemann</last></editor>
      <editor><first>Yves</first><last>Scherrer</last></editor>
      <publisher>International Committee on Computational Linguistics (ICCL)</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="3484dcef">2020.vardial-1.0</url>
      <bibkey>vardial-2020-nlp</bibkey>
    </frontmatter>
    <paper id="2">
      <title>ASR for Non-standardised Languages with Dialectal Variation : the case of <a href="https://en.wikipedia.org/wiki/Swiss_German">Swiss German</a><fixed-case>ASR</fixed-case> for Non-standardised Languages with Dialectal Variation: the case of <fixed-case>S</fixed-case>wiss <fixed-case>G</fixed-case>erman</title>
      <author><first>Iuliia</first><last>Nigmatulina</last></author>
      <author><first>Tannon</first><last>Kew</last></author>
      <author><first>Tanja</first><last>Samardzic</last></author>
      <pages>15–24</pages>
      <abstract>Strong regional variation, together with the lack of standard <a href="https://en.wikipedia.org/wiki/Orthography">orthography</a>, makes Swiss German automatic speech recognition (ASR) particularly difficult in a multi-dialectal setting. This paper focuses on one of the many challenges, namely, the choice of the output text to represent non-standardised Swiss German. We investigate two potential options : a) dialectal writing   approximate phonemic transcriptions that provide close correspondence between grapheme labels and the acoustic signal but are highly inconsistent and b) normalised writing   transcriptions resembling standard <a href="https://en.wikipedia.org/wiki/German_language">German</a> that are relatively consistent but distant from the acoustic signal. To find out which <a href="https://en.wikipedia.org/wiki/Writing">writing</a> facilitates Swiss German ASR, we build several systems using the <a href="https://en.wikipedia.org/wiki/Kaldi_(software)">Kaldi toolkit</a> and a dataset covering 14 regional varieties. A formal comparison shows that the system trained on the <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalised transcriptions</a> achieves better results in word error rate (WER) (29.39 %) but underperforms at the <a href="https://en.wikipedia.org/wiki/Character_(computing)">character level</a>, suggesting <a href="https://en.wikipedia.org/wiki/Transcription_(linguistics)">dialectal transcriptions</a> offer a viable solution for downstream applications where <a href="https://en.wikipedia.org/wiki/Dialect">dialectal differences</a> are important. To better assess word-level performance for dialectal transcriptions, we use a flexible WER measure (FlexWER). When evaluated with this <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>, the system trained on <a href="https://en.wikipedia.org/wiki/Transcription_(linguistics)">dialectal transcriptions</a> outperforms that trained on the normalised writing. Besides establishing a benchmark for Swiss German multi-dialectal ASR, our findings can be helpful in designing ASR systems for other languages without standard <a href="https://en.wikipedia.org/wiki/Orthography">orthography</a>.</abstract>
      <url hash="7bab8437">2020.vardial-1.2</url>
      <bibkey>nigmatulina-etal-2020-asr</bibkey>
    </paper>
    <paper id="4">
      <title>Machine-oriented NMT Adaptation for Zero-shot NLP tasks : Comparing the Usefulness of Close and Distant Languages<fixed-case>NMT</fixed-case> Adaptation for Zero-shot <fixed-case>NLP</fixed-case> tasks: Comparing the Usefulness of Close and Distant Languages</title>
      <author><first>Amirhossein</first><last>Tebbifakhr</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>36–46</pages>
      <abstract>Neural Machine Translation (NMT) models are typically trained by considering humans as end-users and maximizing human-oriented objectives. However, in some scenarios, their output is consumed by automatic NLP components rather than by humans. In these scenarios, translations’ quality is measured in terms of their fitness for purpose (i.e. maximizing performance of external NLP tools) rather than in terms of standard human fluency / adequacy criteria. Recently, <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning techniques</a> exploiting the feedback from downstream NLP tools have been proposed for machine-oriented NMT adaptation. In this work, we tackle the problem in a multilingual setting where a single NMT model translates from multiple languages for downstream automatic processing in the target language. Knowledge sharing across close and distant languages allows to apply our machine-oriented approach in the zero-shot setting where no labeled data for the test language is seen at training time. Moreover, we incorporate multi-lingual BERT in the source side of our NMT system to benefit from the knowledge embedded in this model. Our experiments show coherent performance gains, for different language directions over both i) generic NMT models (trained for human consumption), and ii) fine-tuned multilingual BERT. This gain for zero-shot language directions (e.g. SpanishEnglish) is higher when the models are fine-tuned on a closely-related source language (Italian) than a distant one (German).</abstract>
      <url hash="935d1a6a">2020.vardial-1.4</url>
      <bibkey>tebbifakhr-etal-2020-machine</bibkey>
    </paper>
    <paper id="5">
      <title>Character Alignment in Morphologically Complex Translation Sets for Related Languages</title>
      <author><first>Michael</first><last>Gasser</last></author>
      <author><first>Binyam Ephrem</first><last>Seyoum</last></author>
      <author><first>Nazareth Amlesom</first><last>Kifle</last></author>
      <pages>47–56</pages>
      <abstract>For languages with complex morphology, word-to-word translation is a task with various potential applications, for example, in <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a>, <a href="https://en.wikipedia.org/wiki/Language_education">language instruction</a>, and dictionary creation, as well as in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. In this paper, we confine ourselves to the subtask of <a href="https://en.wikipedia.org/wiki/Character_alignment">character alignment</a> for the particular case of <a href="https://en.wikipedia.org/wiki/Language_family">families of related languages</a> with very few resources for most or all members. There are many such <a href="https://en.wikipedia.org/wiki/Language_family">families</a> ; we focus on the subgroup of Semitic languages spoken in <a href="https://en.wikipedia.org/wiki/Ethiopia">Ethiopia</a> and <a href="https://en.wikipedia.org/wiki/Eritrea">Eritrea</a>. We begin with an adaptation of the familiar <a href="https://en.wikipedia.org/wiki/Sequence_alignment">alignment algorithms</a> behind <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation">statistical machine translation</a>, modifying them as appropriate for our task. We show how <a href="https://en.wikipedia.org/wiki/Character_alignment">character alignment</a> can reveal morphological, phonological, and orthographic correspondences among related languages.</abstract>
      <url hash="637f0829">2020.vardial-1.5</url>
      <bibkey>gasser-etal-2020-character</bibkey>
    </paper>
    <paper id="6">
      <title>Bilingual Lexicon Induction across Orthographically-distinct Under-Resourced Dravidian Languages<fixed-case>D</fixed-case>ravidian Languages</title>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Navaneethan</first><last>Rajasekaran</last></author>
      <author><first>Mihael</first><last>Arcan</last></author>
      <author><first>Kevin</first><last>McGuinness</last></author>
      <author><first>Noel</first><last>E. O’Connor</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <pages>57–69</pages>
      <abstract>Bilingual lexicons are a vital tool for under-resourced languages and recent state-of-the-art approaches to this leverage pretrained monolingual word embeddings using supervised or semi-supervised approaches. However, these approaches require cross-lingual information such as seed dictionaries to train the model and find a <a href="https://en.wikipedia.org/wiki/Linear_map">linear transformation</a> between the word embedding spaces. Especially in the case of low-resourced languages, seed dictionaries are not readily available, and as such, these methods produce extremely weak results on these <a href="https://en.wikipedia.org/wiki/Programming_language">languages</a>. In this work, we focus on the <a href="https://en.wikipedia.org/wiki/Dravidian_languages">Dravidian languages</a>, namely <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a>, <a href="https://en.wikipedia.org/wiki/Telugu_language">Telugu</a>, <a href="https://en.wikipedia.org/wiki/Kannada">Kannada</a>, and <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a>, which are even more challenging as they are written in unique scripts. To take advantage of <a href="https://en.wikipedia.org/wiki/Orthography">orthographic information</a> and <a href="https://en.wikipedia.org/wiki/Cognate">cognates</a> in these <a href="https://en.wikipedia.org/wiki/Language">languages</a>, we bring the related languages into a single script. Previous approaches have used linguistically sub-optimal measures such as the Levenshtein edit distance to detect cognates, whereby we demonstrate that the longest common sub-sequence is linguistically more sound and improves the performance of bilingual lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these <a href="https://en.wikipedia.org/wiki/Language">languages</a> many times, making bilingual lexicon induction approaches feasible for such under-resourced languages.</abstract>
      <url hash="99a051cf">2020.vardial-1.6</url>
      <bibkey>chakravarthi-etal-2020-bilingual</bibkey>
    </paper>
    <paper id="9">
      <title>Recycling and Comparing Morphological Annotation Models for Armenian Diachronic-Variational Corpus Processing<fixed-case>A</fixed-case>rmenian Diachronic-Variational Corpus Processing</title>
      <author><first>Chahan</first><last>Vidal-Gorène</last></author>
      <author><first>Victoria</first><last>Khurshudyan</last></author>
      <author><first>Anaïd</first><last>Donabédian-Demopoulos</last></author>
      <pages>90–101</pages>
      <abstract>Armenian is a language with significant variation and unevenly distributed NLP resources for different varieties. An attempt is made to process an <a href="https://en.wikipedia.org/wiki/Radio-frequency_identification">RNN model</a> for morphological annotation on the basis of different Armenian data (provided or not with morphologically annotated corpora), and to compare the annotation results of <a href="https://en.wikipedia.org/wiki/Radio-frequency_identification">RNN</a> and rule-based models. Different tests were carried out to evaluate the reuse of an unspecialized model of lemmatization and POS-tagging for under-resourced language varieties. The research focused on three dialects and further extended to <a href="https://en.wikipedia.org/wiki/Western_Armenian">Western Armenian</a> with a <a href="https://en.wikipedia.org/wiki/Mean">mean accuracy</a> of 94,00 % in <a href="https://en.wikipedia.org/wiki/Lemmatization">lemmatization</a> and 97,02 % in POS-tagging, as well as a possible reusability of models to cover different other <a href="https://en.wikipedia.org/wiki/Armenian_language">Armenian varieties</a>. Interestingly, the comparison of an RNN model trained on <a href="https://en.wikipedia.org/wiki/Eastern_Armenian">Eastern Armenian</a> with the <a href="https://en.wikipedia.org/wiki/Eastern_Armenian">Eastern Armenian National Corpus rule-based model</a> applied to <a href="https://en.wikipedia.org/wiki/Western_Armenian">Western Armenian</a> showed an enhancement of 19 % in parsing. This <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> covers 88,79 % of a short heterogeneous dataset in <a href="https://en.wikipedia.org/wiki/Western_Armenian">Western Armenian</a>, and could be a baseline for a massive corpus annotation in that standard. It is argued that an RNN-based model can be a valid alternative to a rule-based one giving consideration to such factors as time-consumption, <a href="https://en.wikipedia.org/wiki/Reusability">reusability</a> for different varieties of a target language and significant qualitative results in morphological annotation.</abstract>
      <url hash="167087bd">2020.vardial-1.9</url>
      <bibkey>vidal-gorene-etal-2020-recycling</bibkey>
    </paper>
    <paper id="16">
      <title>Uralic Language Identification (ULI) 2020 shared task dataset and the Wanca 2017 corpora<fixed-case>ULI</fixed-case>) 2020 shared task dataset and the Wanca 2017 corpora</title>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Heidi</first><last>Jauhiainen</last></author>
      <author><first>Niko</first><last>Partanen</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <pages>173–185</pages>
      <abstract>This article introduces the Wanca 2017 web corpora from which the sentences written in minor Uralic languages were collected for the test set of the Uralic Language Identification (ULI) 2020 shared task. We describe the ULI shared task and how the <a href="https://en.wikipedia.org/wiki/Test_set">test set</a> was constructed using the Wanca 2017 corpora and texts in different languages from the Leipzig corpora collection. We also provide the results of a baseline language identification experiment conducted using the ULI 2020 dataset.</abstract>
      <url hash="c77a4cfd">2020.vardial-1.16</url>
      <bibkey>jauhiainen-etal-2020-uralic</bibkey>
    </paper>
    <paper id="19">
      <title>HeLju@VarDial 2020 : Social Media Variety Geolocation with BERT Models<fixed-case>H</fixed-case>e<fixed-case>L</fixed-case>ju@<fixed-case>V</fixed-case>ar<fixed-case>D</fixed-case>ial 2020: Social Media Variety Geolocation with <fixed-case>BERT</fixed-case> Models</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <pages>202–211</pages>
      <abstract>This paper describes the Helsinki-Ljubljana contribution to the VarDial shared task on social media variety geolocation. Our solutions are based on the BERT Transformer models, the constrained versions of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> reaching 1st place in two subtasks and 3rd place in one subtask, while our unconstrained models outperform all the constrained systems by a large margin. We show in our analyses that Transformer-based models outperform traditional <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> by far, and that improvements obtained by pre-training <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> on large quantities of (mostly standard) text are significant, but not drastic, with single-language models also outperforming multilingual models. Our manual analysis shows that two types of signals are the most crucial for a (mis)prediction : <a href="https://en.wikipedia.org/wiki/Named_entity">named entities</a> and <a href="https://en.wikipedia.org/wiki/Dialect">dialectal features</a>, both of which are handled well by our <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>.</abstract>
      <url hash="8f99a4bf">2020.vardial-1.19</url>
      <bibkey>scherrer-ljubesic-2020-helju</bibkey>
    </paper>
    <paper id="21">
      <title>Experiments in Language Variety Geolocation and Dialect Identification</title>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Heidi</first><last>Jauhiainen</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <pages>220–231</pages>
      <abstract>In this paper we describe the systems we used when participating in the VarDial Evaluation Campaign organized as part of the 7th workshop on <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> for similar languages, varieties and dialects. The shared tasks we participated in were the second edition of the Romanian Dialect Identification (RDI) and the first edition of the Social Media Variety Geolocation (SMG). The submissions of our SUKI team used generative language models based on <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> and character n-grams.</abstract>
      <url hash="3c7daedc">2020.vardial-1.21</url>
      <bibkey>jauhiainen-etal-2020-experiments</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/moroco">MOROCO</pwcdataset>
    </paper>
    <paper id="22">
      <title>Exploring the Power of Romanian BERT for Dialect Identification<fixed-case>R</fixed-case>omanian <fixed-case>BERT</fixed-case> for Dialect Identification</title>
      <author><first>George-Eduard</first><last>Zaharia</last></author>
      <author><first>Andrei-Marius</first><last>Avram</last></author>
      <author><first>Dumitru-Clementin</first><last>Cercel</last></author>
      <author><first>Traian</first><last>Rebedea</last></author>
      <pages>232–241</pages>
      <abstract>Dialect identification represents a key aspect for improving a series of tasks, for example, <a href="https://en.wikipedia.org/wiki/Opinion_mining">opinion mining</a>, considering that the location of the speaker can greatly influence the attitude towards a subject. In this work, we describe the systems developed by our team for VarDial 2020 : Romanian Dialect Identification, a task specifically created for challenging participants to solve the previously mentioned issue. More specifically, we introduce a series of neural systems based on Transformers, that combine a BERT model exclusively pre-trained on the <a href="https://en.wikipedia.org/wiki/Romanian_language">Romanian language</a> with techniques such as <a href="https://en.wikipedia.org/wiki/Adversarial_system">adversarial training</a> or character-level embeddings. By using these approaches, we were able to obtain a 0.6475 macro F1 score on the test dataset, thus allowing us to be ranked 5th out of 8 participant teams.</abstract>
      <url hash="ba7812fd">2020.vardial-1.22</url>
      <bibkey>zaharia-etal-2020-exploring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/moroco">MOROCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ronec">RONEC</pwcdataset>
    </paper>
    <paper id="27">
      <title>Geolocation of Tweets with a BiLSTM Regression Model<fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> Regression Model</title>
      <author><first>Piyush</first><last>Mishra</last></author>
      <pages>283–289</pages>
      <abstract>Identifying a user’s location can be useful for <a href="https://en.wikipedia.org/wiki/Recommender_system">recommendation systems</a>, <a href="https://en.wikipedia.org/wiki/Demography">demographic analyses</a>, and <a href="https://en.wikipedia.org/wiki/Emergency_management">disaster outbreak monitoring</a>. Although <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> allows users to voluntarily reveal their location, such information is n’t universally available. Analyzing a tweet can provide a general estimation of a tweet location while giving insight into the dialect of the user and other <a href="https://en.wikipedia.org/wiki/Marker_(linguistics)">linguistic markers</a>. Such <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic attributes</a> can be used to provide a regional approximation of tweet origins. In this paper, we present a neural regression model that can identify the linguistic intricacies of a tweet to predict the location of the user. The final <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> identifies the <a href="https://en.wikipedia.org/wiki/Dialect">dialect</a> embedded in the tweet and predicts the location of the tweet.</abstract>
      <url hash="e9990a78">2020.vardial-1.27</url>
      <bibkey>mishra-2020-geolocation</bibkey>
    </paper>
  </volume>
</collection>