<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.cogalex">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Workshop on the Cognitive Aspects of the Lexicon</booktitle>
      <editor><first>Michael</first><last>Zock</last></editor>
      <editor><first>Emmanuele</first><last>Chersoni</last></editor>
      <editor><first>Alessandro</first><last>Lenci</last></editor>
      <editor><first>Enrico</first><last>Santus</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="54df1291">2020.cogalex-1.0</url>
      <bibkey>cogalex-2020-cognitive</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Individual corpora predict fast memory retrieval during reading</title>
      <author><first>Markus J.</first><last>Hofmann</last></author>
      <author><first>Lara</first><last>Müller</last></author>
      <author><first>Andre</first><last>Rölke</last></author>
      <author><first>Ralph</first><last>Radach</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>1–11</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>, from which a <a href="https://en.wikipedia.org/wiki/Predictive_modelling">predictive language model</a> is trained, can be considered the experience of a semantic system. We recorded everyday reading of two participants for two months on a tablet, generating individual corpus samples of 300/500 K tokens. Then we trained word2vec models from individual corpora and a 70 million-sentence newspaper corpus to obtain individual and norm-based long-term memory structure. To test whether individual corpora can make better predictions for a cognitive task of long-term memory retrieval, we generated stimulus materials consisting of 134 sentences with uncorrelated individual and norm-based word probabilities. For the subsequent eye tracking study 1-2 months later, our <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression analyses</a> revealed that individual, but not norm-corpus-based word probabilities can account for first-fixation duration and first-pass gaze duration. Word length additionally affected gaze duration and total viewing duration. The results suggest that corpora representative for an individual’s <a href="https://en.wikipedia.org/wiki/Long-term_memory">long-term memory structure</a> can better explain reading performance than a norm corpus, and that recently acquired information is lexically accessed rapidly.</abstract>
      <url hash="fc19dd91">2020.cogalex-1.1</url>
      <bibkey>hofmann-etal-2020-individual</bibkey>
    </paper>
    <paper id="4">
      <title>Less is Better : A cognitively inspired unsupervised model for language segmentation</title>
      <author><first>Jinbiao</first><last>Yang</last></author>
      <author><first>Stefan L.</first><last>Frank</last></author>
      <author><first>Antal</first><last>van den Bosch</last></author>
      <pages>33–45</pages>
      <abstract>Language users process utterances by segmenting them into many cognitive units, which vary in their sizes and <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic levels</a>. Although we can do such unitization / segmentation easily, its <a href="https://en.wikipedia.org/wiki/Cognition">cognitive mechanism</a> is still not clear. This paper proposes an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised model</a>, Less-is-Better (LiB), to simulate the human cognitive process with respect to language unitization / segmentation. LiB follows the principle of least effort and aims to build a lexicon which minimizes the number of unit tokens (alleviating the effort of analysis) and number of unit types (alleviating the effort of storage) at the same time on any given corpus. LiB’s workflow is inspired by empirical cognitive phenomena. The design makes the mechanism of LiB cognitively plausible and the computational requirement light-weight. The lexicon generated by LiB performs the best among different types of lexicons (e.g. ground-truth words) both from an information-theoretical view and a cognitive view, which suggests that the LiB lexicon may be a plausible proxy of the <a href="https://en.wikipedia.org/wiki/Mental_lexicon">mental lexicon</a>.</abstract>
      <url hash="57dc6bea">2020.cogalex-1.4</url>
      <bibkey>yang-etal-2020-less</bibkey>
      <pwccode url="https://github.com/ray306/lib" additional="false">ray306/lib</pwccode>
    </paper>
    <paper id="5">
      <title>The CogALex Shared Task on Monolingual and Multilingual Identification of Semantic Relations<fixed-case>C</fixed-case>og<fixed-case>AL</fixed-case>ex Shared Task on Monolingual and Multilingual Identification of Semantic Relations</title>
      <author><first>Rong</first><last>Xiang</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Luca</first><last>Iacoponi</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <pages>46–53</pages>
      <abstract>The shared task of the CogALex-VI workshop focuses on the monolingual and multilingual identification of semantic relations. We provided training and validation data for the following languages : <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a> and <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. Given a word pair, systems had to be trained to identify which relation holds between them, with possible choices being <a href="https://en.wikipedia.org/wiki/Synonym">synonymy</a>, <a href="https://en.wikipedia.org/wiki/Opposite_(semantics)">antonymy</a>, <a href="https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy">hypernymy</a> and no relation at all. Two test sets were released for evaluating the participating <a href="https://en.wikipedia.org/wiki/System">systems</a>. One containing pairs for each of the training languages (systems were evaluated in a monolingual fashion) and the other proposing a surprise language to test the crosslingual transfer capabilities of the systems. Among the submitted systems, top performance was achieved by a transformer-based model in both the monolingual and in the multilingual setting, for all the tested languages, proving the potentials of this recently-introduced neural architecture. The shared task description and the results are available at https://sites.google.com/site/cogalexvisharedtask/.</abstract>
      <url hash="179b23b8">2020.cogalex-1.5</url>
      <bibkey>xiang-etal-2020-cogalex</bibkey>
    </paper>
    <paper id="7">
      <title>CogALex-VI Shared Task : Transrelation-A Robust Multilingual Language Model for Multilingual Relation Identification<fixed-case>C</fixed-case>og<fixed-case>AL</fixed-case>ex-<fixed-case>VI</fixed-case> Shared Task: Transrelation - A Robust Multilingual Language Model for Multilingual Relation Identification</title>
      <author><first>Lennart</first><last>Wachowiak</last></author>
      <author><first>Christian</first><last>Lang</last></author>
      <author><first>Barbara</first><last>Heinisch</last></author>
      <author><first>Dagmar</first><last>Gromann</last></author>
      <pages>59–64</pages>
      <abstract>We describe our submission to the CogALex-VI shared task on the identification of multilingual paradigmatic relations building on XLM-RoBERTa (XLM-R), a robustly optimized and multilingual BERT model. In spite of several experiments with data augmentation, data addition and ensemble methods with a Siamese Triple Net, Translrelation, the XLM-R model with a <a href="https://en.wikipedia.org/wiki/Linear_classifier">linear classifier</a> adapted to this specific task, performed best in testing and achieved the best results in the final evaluation of the shared task, even for a previously unseen language.</abstract>
      <url hash="f6c6dda6">2020.cogalex-1.7</url>
      <bibkey>wachowiak-etal-2020-cogalex</bibkey>
      <pwccode url="https://github.com/Text2TCS/Transrelation" additional="false">Text2TCS/Transrelation</pwccode>
    </paper>
    <paper id="14">
      <title>Translating Collocations : The Need for Task-driven Word Associations</title>
      <author><first>Oi Yee</first><last>Kwong</last></author>
      <pages>112–116</pages>
      <abstract>Existing <a href="https://en.wikipedia.org/wiki/Dictionary">dictionaries</a> may help collocation translation by suggesting associated words in the form of collocations, <a href="https://en.wikipedia.org/wiki/Thesaurus">thesaurus</a>, and example sentences. We propose to enhance them with task-driven word associations, illustrating the need by a few scenarios and outlining a possible approach based on <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a>. An example is given, using pre-trained word embedding, while more extensive investigation with more refined methods and resources is underway.</abstract>
      <url hash="ac742e0a">2020.cogalex-1.14</url>
      <bibkey>kwong-2020-translating</bibkey>
    </paper>
    <paper id="15">
      <title>Characterizing Dynamic Word Meaning Representations in the Brain</title>
      <author><first>Nora</first><last>Aguirre-Celis</last></author>
      <author><first>Risto</first><last>Miikkulainen</last></author>
      <pages>117–128</pages>
      <abstract>During <a href="https://en.wikipedia.org/wiki/Sentence_comprehension">sentence comprehension</a>, humans adjust <a href="https://en.wikipedia.org/wiki/Meaning_(linguistics)">word meanings</a> according to the combination of the concepts that occur in the sentence. This paper presents a neural network model called CEREBRA (Context-dEpendent meaning REpresentation in the BRAin) that demonstrates this process based on fMRI sentence patterns and the Concept Attribute Rep-resentation (CAR) theory. In several experiments, CEREBRA is used to quantify conceptual combination effect and demonstrate that it matters to humans. Such context-based representations could be used in future <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing systems</a> allowing them to mirror human performance more accurately.</abstract>
      <url hash="274a0291">2020.cogalex-1.15</url>
      <bibkey>aguirre-celis-miikkulainen-2020-characterizing</bibkey>
    </paper>
    <paper id="16">
      <title>Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense Knowledge</title>
      <author><first>Sathvik</first><last>Nair</last></author>
      <author><first>Mahesh</first><last>Srinivasan</last></author>
      <author><first>Stephan</first><last>Meylan</last></author>
      <pages>129–141</pages>
      <abstract>Understanding context-dependent variation in word meanings is a key aspect of <a href="https://en.wikipedia.org/wiki/Sentence_processing">human language comprehension</a> supported by the lexicon. Lexicographic resources (e.g., WordNet) capture only some of this context-dependent variation ; for example, they often do not encode how closely senses, or <a href="https://en.wikipedia.org/wiki/Semantic_change">discretized word meanings</a>, are related to one another. Our work investigates whether recent advances in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, specifically contextualized word embeddings, capture human-like distinctions between English word senses, such as <a href="https://en.wikipedia.org/wiki/Polysemy">polysemy</a> and <a href="https://en.wikipedia.org/wiki/Homonym">homonymy</a>. We collect data from a behavioral, web-based experiment, in which participants provide judgments of the relatedness of multiple WordNet senses of a word in a two-dimensional spatial arrangement task. We find that participants’ judgments of the relatedness between senses are correlated with distances between senses in the BERT embedding space. Specifically, homonymous senses (e.g., bat as mammal vs. bat as sports equipment) are reliably more distant from one another in the embedding space than polysemous ones (e.g., chicken as animal vs. chicken as meat). Our findings point towards the potential utility of continuous-space representations of sense meanings.</abstract>
      <url hash="57e59d12">2020.cogalex-1.16</url>
      <bibkey>nair-etal-2020-contextualized</bibkey>
    </paper>
    </volume>
</collection>