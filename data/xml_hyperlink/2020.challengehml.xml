<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.challengehml">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)</booktitle>
      <editor><first>Amir</first><last>Zadeh</last></editor>
      <editor><first>Louis-Philippe</first><last>Morency</last></editor>
      <editor><first>Paul Pu</first><last>Liang</last></editor>
      <editor><first>Soujanya</first><last>Poria</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, USA</address>
      <month>July</month>
      <year>2020</year>
      <url hash="ce85a8c6">2020.challengehml-1</url>
    </meta>
    <frontmatter>
      <url hash="1197a09f">2020.challengehml-1.0</url>
      <bibkey>challenge-hml-2020-grand</bibkey>
    </frontmatter>
    <paper id="2">
      <title>A Multi-modal Approach to Fine-grained Opinion Mining on Video Reviews</title>
      <author><first>Edison</first><last>Marrese-Taylor</last></author>
      <author><first>Cristian</first><last>Rodriguez</last></author>
      <author><first>Jorge</first><last>Balazs</last></author>
      <author><first>Stephen</first><last>Gould</last></author>
      <author><first>Yutaka</first><last>Matsuo</last></author>
      <pages>8â€“18</pages>
      <abstract>Despite the recent advances in <a href="https://en.wikipedia.org/wiki/Opinion_mining">opinion mining</a> for written reviews, few works have tackled the <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> on other sources of reviews. In light of this issue, we propose a multi-modal approach for mining fine-grained opinions from video reviews that is able to determine the aspects of the item under review that are being discussed and the sentiment orientation towards them. Our approach works at the sentence level without the need for time annotations and uses features derived from the audio, video and language transcriptions of its contents. We evaluate our approach on two datasets and show that leveraging the video and audio modalities consistently provides increased performance over text-only baselines, providing evidence these extra modalities are key in better understanding video reviews.</abstract>
      <url hash="40d762b5">2020.challengehml-1.2</url>
      <doi>10.18653/v1/2020.challengehml-1.2</doi>
      <video href="http://slideslive.com/38931259" />
      <bibkey>marrese-taylor-etal-2020-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/youtubean">Youtubean</pwcdataset>
    </paper>
    </volume>
</collection>