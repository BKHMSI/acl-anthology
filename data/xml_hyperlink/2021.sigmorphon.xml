<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.sigmorphon">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
      <editor><first>Garrett</first><last>Nicolai</last></editor>
      <editor><first>Kyle</first><last>Gorman</last></editor>
      <editor><first>Ryan</first><last>Cotterell</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="872e9305">2021.sigmorphon-1</url>
    </meta>
    <frontmatter>
      <url hash="620a81ae">2021.sigmorphon-1.0</url>
      <bibkey>sigmorphon-2021-sigmorphon</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Recursive prosody is not finite-state</title>
      <author><first>Hossep</first><last>Dolatian</last></author>
      <author><first>Aniello</first><last>De Santo</last></author>
      <author><first>Thomas</first><last>Graf</last></author>
      <pages>11–22</pages>
      <abstract>This paper investigates bounds on the generative capacity of <a href="https://en.wikipedia.org/wiki/Prosody_(linguistics)">prosodic processes</a>, by focusing on the complexity of recursive prosody in coordination contexts in <a href="https://en.wikipedia.org/wiki/English_language">English</a> (Wagner, 2010). Although all <a href="https://en.wikipedia.org/wiki/Phonology">phonological processes</a> and most <a href="https://en.wikipedia.org/wiki/Prosody_(linguistics)">prosodic processes</a> are computationally regular string languages, we show that recursive prosody is not. The output string language is instead parallel multiple context-free (Seki et al., 1991). We evaluate the complexity of the pattern over strings, and then move on to a characterization over <a href="https://en.wikipedia.org/wiki/Tree_(graph_theory)">trees</a> that requires the expressivity of multi bottom-up tree transducers. In doing so, we provide a foundation for future mathematically grounded investigations of the syntax-prosody interface.</abstract>
      <url hash="cdaa26e5">2021.sigmorphon-1.2</url>
      <doi>10.18653/v1/2021.sigmorphon-1.2</doi>
      <bibkey>dolatian-etal-2021-recursive</bibkey>
    </paper>
    <paper id="3">
      <title>The Match-Extend serialization algorithm in Multiprecedence</title>
      <author><first>Maxime</first><last>Papillon</last></author>
      <pages>23–31</pages>
      <abstract>Raimy (1999 ; 2000a ; 2000b) proposed a graphical formalism for modeling <a href="https://en.wikipedia.org/wiki/Reduplication">reduplication</a>, originallymostly focused on phonological overapplication in a derivational framework. This <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> is now known as Precedence-based phonology or Multiprecedence phonology. Raimy’s idea is that the segments at the input to the <a href="https://en.wikipedia.org/wiki/Phonology">phonology</a> are not totally ordered by precedence. This paper tackles a challenge that arose with Raimy’s work, the development of a deterministic serialization algorithm as part of the derivation of surface forms. The Match-Extend algorithm introduced here requires fewer assumptions and sticks tighter to the attested typology. The <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> also contains no parameter or <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraint</a> specific to individual <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a> or <a href="https://en.wikipedia.org/wiki/Topological_space">topologies</a>, unlike previous proposals. Match-Extend requires nothing except knowing the last added set of links.</abstract>
      <url hash="ceeb3a26">2021.sigmorphon-1.3</url>
      <attachment type="OptionalSupplementaryMaterial" hash="776b6b1f">2021.sigmorphon-1.3.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.sigmorphon-1.3</doi>
      <bibkey>papillon-2021-match</bibkey>
    </paper>
    <paper id="6">
      <title>A Study of Morphological Robustness of Neural Machine Translation</title>
      <author><first>Sai Muralidhar</first><last>Jayanthi</last></author>
      <author><first>Adithya</first><last>Pratapa</last></author>
      <pages>49–59</pages>
      <abstract>In this work, we analyze the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of neural machine translation systems towards grammatical perturbations in the source. In particular, we focus on morphological inflection related perturbations. While this has been recently studied for EnglishFrench (MORPHEUS) (Tan et al., 2020), it is unclear how this extends to AnyEnglish translation systems. We propose MORPHEUS-MULTILINGUAL that utilizes UniMorph dictionaries to identify morphological perturbations to source that adversely affect the translation models. Along with an analysis of state-of-the-art pretrained MT systems, we train and analyze systems for 11 language pairs using the multilingual TED corpus (Qi et al., 2018). We also compare this to actual errors of non-native speakers using Grammatical Error Correction datasets. Finally, we present a qualitative and quantitative analysis of the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of AnyEnglish translation systems.</abstract>
      <url hash="f9b65799">2021.sigmorphon-1.6</url>
      <doi>10.18653/v1/2021.sigmorphon-1.6</doi>
      <bibkey>jayanthi-pratapa-2021-study</bibkey>
      <pwccode url="https://github.com/murali1996/morpheus_multilingual" additional="false">murali1996/morpheus_multilingual</pwccode>
    </paper>
    <paper id="7">
      <title>Sample-efficient Linguistic Generalizations through <a href="https://en.wikipedia.org/wiki/Program_synthesis">Program Synthesis</a> : Experiments with Phonology Problems</title>
      <author><first>Saujas</first><last>Vaduguru</last></author>
      <author><first>Aalok</first><last>Sathe</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>60–71</pages>
      <abstract>Neural models excel at extracting statistical patterns from large amounts of data, but struggle to learn <a href="https://en.wikipedia.org/wiki/Pattern">patterns</a> or reason about <a href="https://en.wikipedia.org/wiki/Language">language</a> from only a few examples. In this paper, we ask : Can we learn explicit <a href="https://en.wikipedia.org/wiki/Rule_of_inference">rules</a> that generalize well from only a few examples? We explore this question using <a href="https://en.wikipedia.org/wiki/Program_synthesis">program synthesis</a>. We develop a synthesis model to learn <a href="https://en.wikipedia.org/wiki/Phonology">phonology rules</a> as <a href="https://en.wikipedia.org/wiki/Computer_program">programs</a> in a <a href="https://en.wikipedia.org/wiki/Domain-specific_language">domain-specific language</a>. We test the ability of our models to generalize from few training examples using our new dataset of problems from the <a href="https://en.wikipedia.org/wiki/International_Linguistic_Olympiad">Linguistics Olympiad</a>, a challenging set of tasks that require strong linguistic reasoning ability. In addition to being highly sample-efficient, our approach generates <a href="https://en.wikipedia.org/wiki/Human-readable_medium">human-readable programs</a>, and allows control over the generalizability of the learnt programs.</abstract>
      <url hash="2e4fd440">2021.sigmorphon-1.7</url>
      <doi>10.18653/v1/2021.sigmorphon-1.7</doi>
      <bibkey>vaduguru-etal-2021-sample</bibkey>
      <pwccode url="https://github.com/saujasv/phonological-generalizations" additional="false">saujasv/phonological-generalizations</pwccode>
    </paper>
    <paper id="9">
      <title>Adaptor Grammars for Unsupervised Paradigm Clustering<fixed-case>A</fixed-case>daptor <fixed-case>G</fixed-case>rammars for Unsupervised Paradigm Clustering</title>
      <author><first>Kate</first><last>McCurdy</last></author>
      <author><first>Sharon</first><last>Goldwater</last></author>
      <author><first>Adam</first><last>Lopez</last></author>
      <pages>82–89</pages>
      <abstract>This work describes the Edinburgh submission to the SIGMORPHON 2021 Shared Task 2 on unsupervised morphological paradigm clustering. Given raw text input, the task was to assign each token to a cluster with other tokens from the same paradigm. We use Adaptor Grammar segmentations combined with frequency-based heuristics to predict paradigm clusters. Our system achieved the highest average F1 score across 9 test languages, placing first out of 15 submissions.</abstract>
      <url hash="14af72d0">2021.sigmorphon-1.9</url>
      <doi>10.18653/v1/2021.sigmorphon-1.9</doi>
      <bibkey>mccurdy-etal-2021-adaptor</bibkey>
    </paper>
    <paper id="17">
      <title>CLUZH at SIGMORPHON 2021 Shared Task on Multilingual Grapheme-to-Phoneme Conversion : Variations on a Baseline<fixed-case>CLUZH</fixed-case> at <fixed-case>SIGMORPHON</fixed-case> 2021 Shared Task on Multilingual Grapheme-to-Phoneme Conversion: Variations on a Baseline</title>
      <author><first>Simon</first><last>Clematide</last></author>
      <author><first>Peter</first><last>Makarov</last></author>
      <pages>148–153</pages>
      <abstract>This paper describes the submission by the team from the Department of Computational Linguistics, Zurich University, to the Multilingual Grapheme-to-Phoneme Conversion (G2P) Task 1 of the SIGMORPHON 2021 challenge in the low and medium settings. The submission is a variation of our 2020 G2P system, which serves as the baseline for this year’s challenge. The system is a neural transducer that operates over explicit edit actions and is trained with imitation learning. For this challenge, we experimented with the following changes : a) emitting phoneme segments instead of single character phonemes, b) input character dropout, c) a mogrifier LSTM decoder (Melis et al., 2019), d) enriching the decoder input with the currently attended input character, e) parallel BiLSTM encoders, and f) an adaptive batch size scheduler. In the low setting, our best <a href="https://en.wikipedia.org/wiki/Ensemble_cast">ensemble</a> improved over the <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline</a>, however, in the medium setting, the <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline</a> was stronger on average, although for certain languages improvements could be observed.</abstract>
      <url hash="2f4238ab">2021.sigmorphon-1.17</url>
      <doi>10.18653/v1/2021.sigmorphon-1.17</doi>
      <bibkey>clematide-makarov-2021-cluzh</bibkey>
    </paper>
    <paper id="20">
      <title>Recognizing Reduplicated Forms : Finite-State Buffered Machines</title>
      <author><first>Yang</first><last>Wang</last></author>
      <pages>177–187</pages>
      <abstract>Total reduplication is common in <a href="https://en.wikipedia.org/wiki/Phonology">natural language phonology</a> and <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphology</a>. However, formally as copying on reduplicants of unbounded size, unrestricted total reduplication requires computational power beyond context-free, while other phonological and morphological patterns are regular, or even sub-regular. Thus, existing <a href="https://en.wikipedia.org/wiki/Class_(computer_programming)">language classes</a> characterizing reduplicated strings inevitably include typologically unattested context-free patterns, such as reversals. This paper extends <a href="https://en.wikipedia.org/wiki/Regular_language">regular languages</a> to incorporate <a href="https://en.wikipedia.org/wiki/Reduplication">reduplication</a> by introducing a new computational device : finite state buffered machine (FSBMs). We give its mathematical definitions and discuss some <a href="https://en.wikipedia.org/wiki/Closure_(mathematics)">closure properties</a> of the corresponding set of languages. As a result, the class of <a href="https://en.wikipedia.org/wiki/Regular_language">regular languages</a> and <a href="https://en.wikipedia.org/wiki/Formal_language">languages</a> derived from them through a copying mechanism is characterized. Suggested by previous literature, this <a href="https://en.wikipedia.org/wiki/Class_(set_theory)">class of languages</a> should approach the characterization of natural language word sets.</abstract>
      <url hash="6b070b36">2021.sigmorphon-1.20</url>
      <doi>10.18653/v1/2021.sigmorphon-1.20</doi>
      <bibkey>wang-2021-recognizing</bibkey>
    </paper>
    <paper id="24">
      <title>Improved pronunciation prediction accuracy using morphology</title>
      <author><first>Dravyansh</first><last>Sharma</last></author>
      <author><first>Saumya</first><last>Sahai</last></author>
      <author><first>Neha</first><last>Chaudhari</last></author>
      <author><first>Antoine</first><last>Bruguier</last></author>
      <pages>222–228</pages>
      <abstract>Pronunciation lexicons and <a href="https://en.wikipedia.org/wiki/Speech_recognition">prediction models</a> are a key component in several <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech synthesis and recognition systems</a>. We know that morphologically related words typically follow a fixed pattern of <a href="https://en.wikipedia.org/wiki/Pronunciation">pronunciation</a> which can be described by language-specific paradigms. In this work we explore how deep recurrent neural networks can be used to automatically learn and exploit this pattern to improve the pronunciation prediction quality of words related by morphological inflection. We propose two novel approaches for supplying morphological information, using the word’s morphological class and its lemma, which are typically annotated in standard lexicons. We report improvements across a number of European languages with varying degrees of phonological and morphological complexity, and two <a href="https://en.wikipedia.org/wiki/Language_family">language families</a>, with greater improvements for languages where the pronunciation prediction task is inherently more challenging. We also observe that combining bidirectional LSTM networks with attention mechanisms is an effective neural approach for the computational problem considered, across languages. Our approach seems particularly beneficial in the low resource setting, both by itself and in conjunction with <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>.</abstract>
      <url hash="d0d96573">2021.sigmorphon-1.24</url>
      <doi>10.18653/v1/2021.sigmorphon-1.24</doi>
      <bibkey>sharma-etal-2021-improved</bibkey>
    </paper>
    </volume>
</collection>