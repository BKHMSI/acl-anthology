<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.nodalida">
  <volume id="main" ingest-date="2021-05-31">
    <meta>
      <booktitle>Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)</booktitle>
      <editor><first>Simon</first><last>Dobnik</last></editor>
      <editor><first>Lilja</first><last>Øvrelid</last></editor>
      <publisher>Linköping University Electronic Press, Sweden</publisher>
      <address>Reykjavik, Iceland (Online)</address>
      <month>May 31--2 June</month>
      <year>2021</year>
      <url hash="d102b425">2021.nodalida-main</url>
    </meta>
    <frontmatter>
      <url hash="4da07233">2021.nodalida-main.0</url>
      <bibkey>nodalida-2021-nordic</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Operationalizing a National Digital Library : The Case for a Norwegian Transformer Model<fixed-case>N</fixed-case>orwegian Transformer Model</title>
      <author><first>Per E</first><last>Kummervold</last></author>
      <author><first>Javier</first><last>De la Rosa</last></author>
      <author><first>Freddy</first><last>Wetjen</last></author>
      <author><first>Svein Arne</first><last>Brygfjeld</last></author>
      <pages>20–29</pages>
      <abstract>In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library. The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian</a> outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian Bokml</a> and <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian Nynorsk</a>. Our model also improves the mBERT performance for other languages present in the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> such as <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a>, and <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>. For <a href="https://en.wikipedia.org/wiki/Language">languages</a> not included in the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>, the weights degrade moderately while keeping strong multilingual properties. Therefore, we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.</abstract>
      <url hash="51be5fb2">2021.nodalida-main.3</url>
      <bibkey>kummervold-etal-2021-operationalizing</bibkey>
    </paper>
    <paper id="4">
      <title>Large-Scale Contextualised Language Modelling for <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian</a><fixed-case>N</fixed-case>orwegian</title>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <pages>30–40</pages>
      <abstract>We present the ongoing NorLM initiative to support the creation and use of very large contextualised language models for <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian</a> (and in principle other Nordic languages), including a ready-to-use software environment, as well as an experience report for data preparation and training. This paper introduces the first large-scale monolingual language models for <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian</a>, based on both the ELMo and BERT frameworks. In addition to detailing the training process, we present contrastive benchmark results on a suite of NLP tasks for <a href="https://en.wikipedia.org/wiki/Norwegian_language">Norwegian</a>. For additional background and access to the data, <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, and software, please see : http://norlm.nlpl.eu</abstract>
      <url hash="a9eecc98">2021.nodalida-main.4</url>
      <bibkey>kutuzov-etal-2021-large</bibkey>
      <pwccode url="https://github.com/ltgoslo/NorBERT" additional="true">ltgoslo/NorBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/norec-fine">NoReC_fine</pwcdataset>
    </paper>
    <paper id="11">
      <title>A Baseline Document Planning Method for <a href="https://en.wikipedia.org/wiki/Automated_journalism">Automated Journalism</a></title>
      <author><first>Leo</first><last>Leppänen</last></author>
      <author><first>Hannu</first><last>Toivonen</last></author>
      <pages>101–111</pages>
      <abstract>In this work, we present a method for content selection and document planning for automated news and report generation from structured statistical data such as that offered by the European Union’s statistical agency, <a href="https://en.wikipedia.org/wiki/EuroStat">EuroStat</a>. The <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is driven by the data and is highly topic-independent within the <a href="https://en.wikipedia.org/wiki/Data_set">statistical dataset domain</a>. As our approach is not based on <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>, it is suitable for introducing news automation to the wide variety of domains where no <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data</a> is available. As such, it is suitable as a low-cost (in terms of implementation effort) baseline for document structuring prior to introduction of domain-specific knowledge.</abstract>
      <url hash="0f5954ac">2021.nodalida-main.11</url>
      <bibkey>leppanen-toivonen-2021-baseline</bibkey>
    </paper>
    <paper id="16">
      <title>Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification</title>
      <author><first>Samuel</first><last>Rönnqvist</last></author>
      <author><first>Valtteri</first><last>Skantsi</last></author>
      <author><first>Miika</first><last>Oinonen</last></author>
      <author><first>Veronika</first><last>Laippala</last></author>
      <pages>157–165</pages>
      <abstract>This article studies register classification of documents from the unrestricted web, such as news articles or opinion blogs, in a multilingual setting, exploring both the benefit of training on multiple languages and the capabilities for zero-shot cross-lingual transfer. While the wide range of linguistic variation found on the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a> poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved. In this study, we show that training on multiple languages 1) benefits languages with limited amounts of register-annotated data, 2) on average achieves performance on par with monolingual models, and 3) greatly improves upon previous zero-shot results in <a href="https://en.wikipedia.org/wiki/Finnish_language">Finnish</a>, <a href="https://en.wikipedia.org/wiki/French_language">French</a> and <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a>. The best results are achieved with the multilingual XLM-R model. As data, we use the CORE corpus series featuring register annotated data from the unrestricted web.</abstract>
      <url hash="271cadbf">2021.nodalida-main.16</url>
      <bibkey>ronnqvist-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="21">
      <title>De-identification of Privacy-related Entities in Job Postings</title>
      <author><first>Kristian Nørgaard</first><last>Jensen</last></author>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>210–221</pages>
      <abstract>De-identification is the task of detecting privacy-related entities in text, such as person names, <a href="https://en.wikipedia.org/wiki/Email">emails</a> and contact data. It has been well-studied within the <a href="https://en.wikipedia.org/wiki/Medicine">medical domain</a>. The need for de-identification technology is increasing, as privacy-preserving data handling is in high demand in many domains. In this paper, we focus on <a href="https://en.wikipedia.org/wiki/Employment_website">job postings</a>. We present JobStack, a new <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> for de-identification of personal data in job vacancies on <a href="https://en.wikipedia.org/wiki/Stackoverflow">Stackoverflow</a>. We introduce <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>, comparing Long-Short Term Memory (LSTM) and Transformer models. To improve these baselines, we experiment with BERT representations, and distantly related auxiliary data via <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>. Our results show that auxiliary data helps to improve <a href="https://en.wikipedia.org/wiki/De-identification">de-identification</a> performance. While BERT representations improve performance, surprisingly vanilla BERT turned out to be more effective than BERT trained on <a href="https://en.wikipedia.org/wiki/Stackoverflow">Stackoverflow-related data</a>.</abstract>
      <url hash="a59b2641">2021.nodalida-main.21</url>
      <bibkey>jensen-etal-2021-de</bibkey>
      <pwccode url="https://github.com/kris927b/JobStack" additional="false">kris927b/JobStack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/jobstack">JobStack</pwcdataset>
    </paper>
    <paper id="28">
      <title>NLI Data Sanity Check : Assessing the Effect of <a href="https://en.wikipedia.org/wiki/Data_corruption">Data Corruption</a> on Model Performance<fixed-case>NLI</fixed-case> Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance</title>
      <author><first>Aarne</first><last>Talman</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>276–287</pages>
      <abstract>Pre-trained neural language models give high performance on natural language inference (NLI) tasks. But whether they actually understand the meaning of the processed sequences is still unclear. We propose a new diagnostics test suite which allows to assess whether a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> constitutes a good testbed for evaluating the models’ meaning understanding capabilities. We specifically apply controlled corruption transformations to widely used <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs. If model accuracy on the corrupted data remains high, then the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is likely to contain <a href="https://en.wikipedia.org/wiki/Bias_(statistics)">statistical biases</a> and artefacts that guide <a href="https://en.wikipedia.org/wiki/Prediction">prediction</a>. Inversely, a large decrease in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">model accuracy</a> indicates that the original <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> provides a proper challenge to the models’ reasoning capabilities. Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks.</abstract>
      <url hash="c358f7c4">2021.nodalida-main.28</url>
      <bibkey>talman-etal-2021-nli</bibkey>
      <pwccode url="https://github.com/Helsinki-NLP/nli-data-sanity-check" additional="false">Helsinki-NLP/nli-data-sanity-check</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="33">
      <title>Towards cross-lingual application of language-specific PoS tagging schemes<fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> tagging schemes</title>
      <author><first>Hinrik</first><last>Hafsteinsson</last></author>
      <author><first>Anton Karl</first><last>Ingason</last></author>
      <pages>321–325</pages>
      <abstract>We describe the process of conversion between the PoS tagging schemes of two languages, the Icelandic MIM-GOLD tagging scheme and the Faroese Sosialurin tagging scheme. These tagging schemes are functionally similar but use separate ways to encode fine-grained morphological information on tokenised text. As Faroese and Icelandic are lexically and grammatically similar, having a systematic method to convert between these two tagging schemes would be beneficial in the field of <a href="https://en.wikipedia.org/wiki/Language_technology">language technology</a>, specifically in research on <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> between the two languages. As a product of our work, we present a provisional version of Icelandic corpora, prepared in the Faroese PoS tagging scheme, ready for use in cross-lingual NLP applications.</abstract>
      <url hash="a590242a">2021.nodalida-main.33</url>
      <bibkey>hafsteinsson-ingason-2021-towards</bibkey>
    </paper>
    <paper id="34">
      <title>Exploring the Importance of Source Text in Automatic Post-Editing for Context-Aware Machine Translation</title>
      <author><first>Chaojun</first><last>Wang</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>326–335</pages>
      <abstract>Accurate <a href="https://en.wikipedia.org/wiki/Translation">translation</a> requires document-level information, which is ignored by sentence-level machine translation. Recent work has demonstrated that document-level consistency can be improved with automatic post-editing (APE) using only target-language (TL) information. We study an extended APE model that additionally integrates <a href="https://en.wikipedia.org/wiki/Context_(language_use)">source context</a>. A human evaluation of fluency and adequacy in EnglishRussian translation reveals that the model with access to source context significantly outperforms monolingual APE in terms of adequacy, an effect largely ignored by automatic evaluation metrics. Our results show that TL-only modelling increases <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a> without improving adequacy, demonstrating the need for conditioning on source text for automatic post-editing. They also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably.</abstract>
      <url hash="1e1ba1c4">2021.nodalida-main.34</url>
      <bibkey>wang-etal-2021-exploring</bibkey>
      <pwccode url="https://github.com/zippotju/context-aware-bilingual-repair-for-neural-machine-translation" additional="false">zippotju/context-aware-bilingual-repair-for-neural-machine-translation</pwccode>
    </paper>
    <paper id="36">
      <title>Grapheme-Based Cross-Language Forced Alignment : Results with <a href="https://en.wikipedia.org/wiki/Uralic_languages">Uralic Languages</a></title>
      <author><first>Juho</first><last>Leinonen</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <pages>345–350</pages>
      <abstract>Forced alignment is an effective <a href="https://en.wikipedia.org/wiki/Process_(engineering)">process</a> to speed up <a href="https://en.wikipedia.org/wiki/Linguistics">linguistic research</a>. However, most forced aligners are language-dependent, and under-resourced languages rarely have enough resources to train an <a href="https://en.wikipedia.org/wiki/Acoustic_model">acoustic model</a> for an aligner. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple <a href="https://en.wikipedia.org/wiki/Uralic_languages">Uralic languages</a> and <a href="https://en.wikipedia.org/wiki/English_language">English</a> as an unrelated language. We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments.</abstract>
      <url hash="091b5464">2021.nodalida-main.36</url>
      <bibkey>leinonen-etal-2021-grapheme</bibkey>
      <pwccode url="https://github.com/aalto-speech/finnish-forced-alignment" additional="false">aalto-speech/finnish-forced-alignment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="40">
      <title>Decentralized Word2Vec Using Gossip Learning<fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec Using Gossip Learning</title>
      <author><first>Abdul Aziz</first><last>Alkathiri</last></author>
      <author><first>Lodovico</first><last>Giaretta</last></author>
      <author><first>Sarunas</first><last>Girdzijauskas</last></author>
      <author><first>Magnus</first><last>Sahlgren</last></author>
      <pages>373–377</pages>
      <abstract>Advanced NLP models require huge amounts of data from various domains to produce high-quality representations. It is useful then for a few large public and private organizations to join their corpora during training. However, factors such as legislation and user emphasis on <a href="https://en.wikipedia.org/wiki/Information_privacy">data privacy</a> may prevent centralized orchestration and <a href="https://en.wikipedia.org/wiki/Data_sharing">data sharing</a> among these organizations. Therefore, for this specific scenario, we investigate how gossip learning, a massively-parallel, data-private, decentralized protocol, compares to a shared-dataset solution. We find that the application of Word2Vec in a gossip learning framework is viable. Without any tuning, the results are comparable to a traditional centralized setting, with a loss of quality as low as 4.3 %. Furthermore, the results are up to 54.8 % better than independent local training.</abstract>
      <url hash="58d0d53e">2021.nodalida-main.40</url>
      <bibkey>alkathiri-etal-2021-decentralized</bibkey>
    </paper>
    <paper id="41">
      <title>Multilingual ELMo and the Effects of Corpus Sampling<fixed-case>ELM</fixed-case>o and the Effects of Corpus Sampling</title>
      <author><first>Vinit</first><last>Ravishankar</last></author>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>378–384</pages>
      <abstract>Multilingual pretrained language models are rapidly gaining popularity in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP systems</a> for non-English languages. Most of these models feature an important corpus sampling step in the process of accumulating training data in different languages, to ensure that the signal from better resourced languages does not drown out poorly resourced ones. In this study, we train multiple multilingual recurrent language models, based on the ELMo architecture, and analyse both the effect of varying corpus size ratios on downstream performance, as well as the performance difference between monolingual models for each language, and broader multilingual language models. As part of this effort, we also make these trained <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> available for public use.</abstract>
      <url hash="c73e3d0a">2021.nodalida-main.41</url>
      <bibkey>ravishankar-etal-2021-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="46">
      <title>The Danish Gigaword Corpus<fixed-case>D</fixed-case>anish <fixed-case>G</fixed-case>igaword Corpus</title>
      <author><first>Leon</first><last>Strømberg-Derczynski</last></author>
      <author><first>Manuel</first><last>Ciosici</last></author>
      <author><first>Rebekah</first><last>Baglini</last></author>
      <author><first>Morten H.</first><last>Christiansen</last></author>
      <author><first>Jacob Aarup</first><last>Dalsgaard</last></author>
      <author><first>Riccardo</first><last>Fusaroli</last></author>
      <author><first>Peter Juel</first><last>Henrichsen</last></author>
      <author><first>Rasmus</first><last>Hvingelby</last></author>
      <author><first>Andreas</first><last>Kirkedal</last></author>
      <author><first>Alex Speed</first><last>Kjeldsen</last></author>
      <author><first>Claus</first><last>Ladefoged</last></author>
      <author><first>Finn Årup</first><last>Nielsen</last></author>
      <author><first>Jens</first><last>Madsen</last></author>
      <author><first>Malte Lau</first><last>Petersen</last></author>
      <author><first>Jonathan Hvithamar</first><last>Rystrøm</last></author>
      <author><first>Daniel</first><last>Varab</last></author>
      <pages>413–421</pages>
      <abstract>Danish language technology has been hindered by a lack of broad-coverage corpora at the scale modern <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> prefers. This paper describes the Danish Gigaword Corpus, the result of a focused effort to provide a diverse and freely-available one billion word corpus of <a href="https://en.wikipedia.org/wiki/Danish_language">Danish text</a>. The Danish Gigaword corpus covers a wide array of time periods, domains, speakers’ socio-economic status, and <a href="https://en.wikipedia.org/wiki/Danish_dialects">Danish dialects</a>.</abstract>
      <url hash="d90398be">2021.nodalida-main.46</url>
      <revision id="1" href="2021.nodalida-main.46v1" hash="ae5c62ef" />
      <revision id="2" href="2021.nodalida-main.46v2" hash="d90398be" date="2021-06-04">This revision amends an incorrect name in one of the cited works.</revision>
      <bibkey>stromberg-derczynski-etal-2021-danish</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dagw">DAGW</pwcdataset>
    </paper>
    <paper id="47">
      <title>DanFEVER : claim verification dataset for Danish<fixed-case>D</fixed-case>an<fixed-case>FEVER</fixed-case>: claim verification dataset for <fixed-case>D</fixed-case>anish</title>
      <author><first>Jeppe</first><last>Nørregaard</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <pages>422–428</pages>
      <abstract>We present a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, DanFEVER, intended for multilingual misinformation research. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is in <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a> and has the same format as the well-known English FEVER dataset. It can be used for testing methods in multilingual settings, as well as for creating models in production for the <a href="https://en.wikipedia.org/wiki/Danish_language">Danish language</a>.</abstract>
      <url hash="b787c42b">2021.nodalida-main.47</url>
      <bibkey>norregaard-derczynski-2021-danfever</bibkey>
      <pwccode url="https://github.com/StrombergNLP/danfever" additional="false">StrombergNLP/danfever</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/danfever">DanFEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
    </paper>
    <paper id="51">
      <title>NorDial : A Preliminary Corpus of Written Norwegian Dialect Use<fixed-case>N</fixed-case>or<fixed-case>D</fixed-case>ial: A Preliminary Corpus of Written <fixed-case>N</fixed-case>orwegian Dialect Use</title>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>Samia</first><last>Touileb</last></author>
      <pages>445–451</pages>
      <abstract>Norway has a large amount of dialectal variation, as well as a general tolerance to its use in the public sphere. There are, however, few available resources to study this variation and its change over time and in more informal areas, on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. In this paper, we propose a first step to creating a <a href="https://en.wikipedia.org/wiki/Corpus_linguistics">corpus of dialectal variation</a> of <a href="https://en.wikipedia.org/wiki/Norwegian_language">written Norwegian</a>. We collect a small corpus of <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a> and manually annotate them as Bokml, <a href="https://en.wikipedia.org/wiki/Nynorsk">Nynorsk</a>, any dialect, or a mix. We further perform preliminary experiments with state-of-the-art models, as well as an analysis of the data to expand this <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> in the future. Finally, we make the annotations available for future work.</abstract>
      <url hash="1f13a7c8">2021.nodalida-main.51</url>
      <bibkey>barnes-etal-2021-nordial</bibkey>
      <pwccode url="https://github.com/jerbarnes/norwegian_dialect" additional="false">jerbarnes/norwegian_dialect</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nordial">NorDial</pwcdataset>
    </paper>
    <paper id="52">
      <title>The Swedish Winogender Dataset<fixed-case>S</fixed-case>wedish <fixed-case>W</fixed-case>inogender Dataset</title>
      <author><first>Saga</first><last>Hansson</last></author>
      <author><first>Konstantinos</first><last>Mavromatakis</last></author>
      <author><first>Yvonne</first><last>Adesam</last></author>
      <author><first>Gerlof</first><last>Bouma</last></author>
      <author><first>Dana</first><last>Dannélls</last></author>
      <pages>452–459</pages>
      <abstract>We introduce the SweWinogender test set, a diagnostic dataset to measure gender bias in <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>. It is modelled after the English Winogender benchmark, and is released with reference statistics on the distribution of men and women between occupations and the association between gender and occupation in modern corpus material. The paper discusses the design and creation of the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, and presents a small investigation of the supplementary statistics.</abstract>
      <url hash="4924bf48">2021.nodalida-main.52</url>
      <bibkey>hansson-etal-2021-swedish</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    </volume>
</collection>