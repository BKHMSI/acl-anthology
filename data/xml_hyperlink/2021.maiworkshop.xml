<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.maiworkshop">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Multimodal Artificial Intelligence</booktitle>
      <editor><first>Amir</first><last>Zadeh</last></editor>
      <editor><first>Louis-Philippe</first><last>Morency</last></editor>
      <editor><first>Paul Pu</first><last>Liang</last></editor>
      <editor><first>Candace</first><last>Ross</last></editor>
      <editor><first>Ruslan</first><last>Salakhutdinov</last></editor>
      <editor><first>Soujanya</first><last>Poria</last></editor>
      <editor><first>Erik</first><last>Cambria</last></editor>
      <editor><first>Kelly</first><last>Shi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2021</year>
      <url hash="f2f0aab4">2021.maiworkshop-1</url>
    </meta>
    <frontmatter>
      <url hash="46cec279">2021.maiworkshop-1.0</url>
      <bibkey>maiworkshop-2021-multimodal</bibkey>
    </frontmatter>
    <paper id="5">
      <title>Multi Task Learning based Framework for Multimodal Classification</title>
      <author><first>Danting</first><last>Zeng</last></author>
      <pages>30–35</pages>
      <abstract>Large-scale multi-modal classification aim to distinguish between different multi-modal data, and it has drawn dramatically attentions since last decade. In this paper, we propose a multi-task learning-based framework for the multimodal classification task, which consists of two branches : multi-modal autoencoder branch and attention-based multi-modal modeling branch. Multi-modal autoencoder can receive multi-modal features and obtain the interactive information which called multi-modal encoder feature, and use this feature to reconstitute all the input data. Besides, multi-modal encoder feature can be used to enrich the raw dataset, and improve the performance of downstream tasks (such as classification task). As for attention-based multimodal modeling branch, we first employ attention mechanism to make the model focused on important features, then we use the multi-modal encoder feature to enrich the input information, achieve a better performance. We conduct extensive experiments on different dataset, the results demonstrate the effectiveness of proposed <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a>.</abstract>
      <url hash="e12103f7">2021.maiworkshop-1.5</url>
      <doi>10.18653/v1/2021.maiworkshop-1.5</doi>
      <bibkey>zeng-2021-multi</bibkey>
    </paper>
    <paper id="10">
      <title>A Package for Learning on Tabular and Text Data with Transformers</title>
      <author><first>Ken</first><last>Gu</last></author>
      <author><first>Akshay</first><last>Budhkar</last></author>
      <pages>69–73</pages>
      <abstract>Recent progress in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> has led to Transformer architectures becoming the predominant model used for natural language tasks. However, in many real- world datasets, additional modalities are included which the <a href="https://en.wikipedia.org/wiki/Transformer">Transformer</a> does not directly leverage. We present Multimodal- Toolkit, an open-source Python package to incorporate text and tabular (categorical and numerical) data with Transformers for downstream applications. Our toolkit integrates well with Hugging Face’s existing API such as tokenization and the model hub which allows easy download of different pre-trained models.</abstract>
      <url hash="2665785e">2021.maiworkshop-1.10</url>
      <doi>10.18653/v1/2021.maiworkshop-1.10</doi>
      <bibkey>gu-budhkar-2021-package</bibkey>
    </paper>
    <paper id="13">
      <title>Learning to Select Question-Relevant Relations for Visual Question Answering</title>
      <author><first>Jaewoong</first><last>Lee</last></author>
      <author><first>Heejoon</first><last>Lee</last></author>
      <author><first>Hwanhee</first><last>Lee</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>87–96</pages>
      <abstract>Previous existing visual question answering (VQA) systems commonly use graph neural networks(GNNs) to extract visual relationships such as semantic relations or spatial relations. However, studies that use GNNs typically ignore the importance of each relation and simply concatenate outputs from multiple relation encoders. In this paper, we propose a novel layer architecture that fuses multiple visual relations through an attention mechanism to address this issue. Specifically, we develop a model that uses question embedding and joint embedding of the encoders to obtain dynamic attention weights with regard to the type of questions. Using the learnable attention weights, the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can efficiently use the necessary visual relation features for a given question. Experimental results on the VQA 2.0 dataset demonstrate that the proposed model outperforms existing graph attention network-based architectures. Additionally, we visualize the attention weight and show that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> assigns a higher weight to relations that are more relevant to the question.</abstract>
      <url hash="e2e33677">2021.maiworkshop-1.13</url>
      <doi>10.18653/v1/2021.maiworkshop-1.13</doi>
      <bibkey>lee-etal-2021-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
  </volume>
</collection>