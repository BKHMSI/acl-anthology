<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.vardial">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects</booktitle>
      <editor><first>Marcos</first><last>Zampieri</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Nikola</first><last>Ljubešić</last></editor>
      <editor><first>Jörg</first><last>Tiedemann</last></editor>
      <editor><first>Yves</first><last>Scherrer</last></editor>
      <editor><first>Tommi</first><last>Jauhiainen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Kiyv, Ukraine</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="5dcc52a6">2021.vardial-1.0</url>
      <bibkey>vardial-2021-nlp</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Hierarchical Transformer for Multilingual Machine Translation</title>
      <author><first>Albina</first><last>Khusainova</last></author>
      <author><first>Adil</first><last>Khan</last></author>
      <author><first>Adín Ramírez</first><last>Rivera</last></author>
      <author><first>Vitaly</first><last>Romanov</last></author>
      <pages>12–20</pages>
      <abstract>The choice of parameter sharing strategy in multilingual machine translation models determines how optimally <a href="https://en.wikipedia.org/wiki/Parameter_space">parameter space</a> is used and hence, directly influences ultimate translation quality. Inspired by <a href="https://en.wikipedia.org/wiki/Tree_(graph_theory)">linguistic trees</a> that show the degree of relatedness between different languages, the new general approach to parameter sharing in multilingual machine translation was suggested recently. The main idea is to use these expert language hierarchies as a basis for multilingual architecture : the closer two languages are, the more parameters they share. In this work, we test this idea using the Transformer architecture and show that despite the success in previous work there are problems inherent to training such hierarchical models. We demonstrate that in case of carefully chosen training strategy the hierarchical architecture can outperform bilingual models and multilingual models with full parameter sharing.</abstract>
      <url hash="9548e505">2021.vardial-1.2</url>
      <bibkey>khusainova-etal-2021-hierarchical</bibkey>
    </paper>
    <paper id="4">
      <title>Representations of Language Varieties Are Reliable Given Corpus Similarity Measures</title>
      <author><first>Jonathan</first><last>Dunn</last></author>
      <pages>28–38</pages>
      <abstract>This paper measures <a href="https://en.wikipedia.org/wiki/Similarity_measure">similarity</a> both within and between 84 <a href="https://en.wikipedia.org/wiki/Variety_(linguistics)">language varieties</a> across nine languages. These <a href="https://en.wikipedia.org/wiki/Corpus_linguistics">corpora</a> are drawn from digital sources (the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a> and tweets), allowing us to evaluate whether such geo-referenced corpora are reliable for modelling linguistic variation. The basic idea is that, if each source adequately represents a single underlying <a href="https://en.wikipedia.org/wiki/Variety_(linguistics)">language variety</a>, then the similarity between these sources should be stable across all languages and countries. The paper shows that there is a consistent agreement between these <a href="https://en.wikipedia.org/wiki/Source_text">sources</a> using frequency-based corpus similarity measures. This provides further evidence that digital geo-referenced corpora consistently represent <a href="https://en.wikipedia.org/wiki/Dialect">local language varieties</a>.</abstract>
      <url hash="b64e4e4d">2021.vardial-1.4</url>
      <bibkey>dunn-2021-representations</bibkey>
    </paper>
    <paper id="5">
      <title>Whit’s the Richt Pairt o Speech : PoS tagging for Scots<fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> tagging for <fixed-case>S</fixed-case>cots</title>
      <author><first>Harm</first><last>Lameris</last></author>
      <author><first>Sara</first><last>Stymne</last></author>
      <pages>39–48</pages>
      <abstract>In this paper we explore PoS tagging for the <a href="https://en.wikipedia.org/wiki/Scots_language">Scots language</a>. Scots is spoken in <a href="https://en.wikipedia.org/wiki/Scotland">Scotland</a> and Northern Ireland, and is closely related to <a href="https://en.wikipedia.org/wiki/English_language">English</a>. As no linguistically annotated Scots data were available, we manually PoS tagged a small set that is used for evaluation and training. We use <a href="https://en.wikipedia.org/wiki/English_language">English</a> as a transfer language to examine zero-shot transfer and transfer learning methods. We find that training on a very small amount of Scots data was superior to zero-shot transfer from <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Combining the Scots and English data led to further improvements, with a <a href="https://en.wikipedia.org/wiki/Concatenation">concatenation method</a> giving the best results. We also compared the use of two different English treebanks and found that a treebank containing web data was superior in the zero-shot setting, while it was outperformed by a <a href="https://en.wikipedia.org/wiki/Treebank">treebank</a> containing a mix of genres when combined with Scots data.</abstract>
      <url hash="ce78f7c0">2021.vardial-1.5</url>
      <bibkey>lameris-stymne-2021-whits</bibkey>
    </paper>
    <paper id="8">
      <title>Discriminating Between Similar Nordic Languages</title>
      <author><first>René</first><last>Haas</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <pages>67–75</pages>
      <abstract>Automatic language identification is a challenging problem. Discriminating between closely related languages is especially difficult. This paper presents a machine learning approach for <a href="https://en.wikipedia.org/wiki/Automatic_language_identification">automatic language identification</a> for the <a href="https://en.wikipedia.org/wiki/North_Germanic_languages">Nordic languages</a>, which often suffer <a href="https://en.wikipedia.org/wiki/Miscategorization">miscategorisation</a> by existing state-of-the-art tools. Concretely we will focus on discrimination between six Nordic languages : <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>, <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a>, Norwegian (Nynorsk), Norwegian (Bokml), <a href="https://en.wikipedia.org/wiki/Faroese_language">Faroese</a> and <a href="https://en.wikipedia.org/wiki/Icelandic_language">Icelandic</a>.</abstract>
      <url hash="19384313">2021.vardial-1.8</url>
      <bibkey>haas-derczynski-2021-discriminating</bibkey>
      <pwccode url="https://github.com/StrombergNLP/NordicDSL" additional="true">StrombergNLP/NordicDSL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nordic-langid">nordic_langid</pwcdataset>
    </paper>
    <paper id="11">
      <title>Optimizing a Supervised Classifier for a Difficult Language Identification Problem</title>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>96–101</pages>
      <abstract>This paper describes the system developed by the Laboratoire d’analyse statistique des textes for the Dravidian Language Identification (DLI) shared task of VarDial 2021. This task is particularly difficult because the materials consists of short YouTube comments, written in <a href="https://en.wikipedia.org/wiki/Latin_script">Roman script</a>, from three closely related <a href="https://en.wikipedia.org/wiki/Dravidian_languages">Dravidian languages</a>, and a fourth category consisting of several other languages in varying proportions, all mixed with English. The proposed system is made up of a <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression model</a> which uses as only features n-grams of characters with a maximum length of 5. After its optimization both in terms of the feature weighting and the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier parameters</a>, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> ranked first in the challenge. The additional analyses carried out underline the importance of <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">optimization</a>, especially when the measure of effectiveness is the Macro-F1.</abstract>
      <url hash="2f5da910">2021.vardial-1.11</url>
      <bibkey>bestgen-2021-optimizing</bibkey>
    </paper>
    <paper id="14">
      <title>Comparing <a href="https://en.wikipedia.org/wiki/Dialectic">Approaches</a> to Dravidian Language Identification<fixed-case>D</fixed-case>ravidian Language Identification</title>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>120–127</pages>
      <abstract>This paper describes the submissions by team HWR to the Dravidian Language Identification (DLI) shared task organized at VarDial 2021 workshop. The DLI training set includes 16,674 YouTube comments written in Roman script containing code-mixed text with English and one of the three South Dravidian languages : <a href="https://en.wikipedia.org/wiki/Kannada">Kannada</a>, <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a>, and <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a>. We submitted results generated using two models, a <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes classifier</a> with adaptive language models, which has shown to obtain competitive performance in many language and dialect identification tasks, and a transformer-based model which is widely regarded as the state-of-the-art in a number of NLP tasks. Our first submission was sent in the closed submission track using only the training set provided by the shared task organisers, whereas the second submission is considered to be open as it used a pretrained model trained with external data. Our team attained shared second position in the shared task with the submission based on Naive Bayes. Our results reinforce the idea that deep learning methods are not as competitive in language identification related tasks as they are in many other text classification tasks.</abstract>
      <url hash="19975f07">2021.vardial-1.14</url>
      <bibkey>jauhiainen-etal-2021-comparing</bibkey>
    </paper>
    </volume>
</collection>