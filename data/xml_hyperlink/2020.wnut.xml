<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.wnut">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</booktitle>
      <editor><first>Wei</first><last>Xu</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <editor><first>Tim</first><last>Baldwin</last></editor>
      <editor><first>Afshin</first><last>Rahimi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="626d85d3">2020.wnut-1.0</url>
      <bibkey>wnut-2020-noisy</bibkey>
    </frontmatter>
    <paper id="5">
      <title>Combining BERT with Static Word Embeddings for Categorizing Social Media<fixed-case>BERT</fixed-case> with Static Word Embeddings for Categorizing Social Media</title>
      <author><first>Israa</first><last>Alghanmi</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>28–33</pages>
      <abstract>Pre-trained neural language models (LMs) have achieved impressive results in various natural language processing tasks, across different languages. Surprisingly, this extends to the <a href="https://en.wikipedia.org/wiki/Social_media">social media genre</a>, despite the fact that <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> often has very different characteristics from the language that LMs have seen during training. A particularly striking example is the performance of AraBERT, an LM for the <a href="https://en.wikipedia.org/wiki/Arabic">Arabic language</a>, which is successful in categorizing social media posts in <a href="https://en.wikipedia.org/wiki/Arabic">Arabic dialects</a>, despite only having been trained on Modern Standard <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>. Our hypothesis in this paper is that the performance of LMs for <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> can nonetheless be improved by incorporating static word vectors that have been specifically trained on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. We show that a simple method for incorporating such word vectors is indeed successful in several Arabic and English benchmarks. Curiously, however, we also find that similar improvements are possible with word vectors that have been trained on traditional text sources (e.g. Wikipedia).</abstract>
      <url hash="87599cd4">2020.wnut-1.5</url>
      <doi>10.18653/v1/2020.wnut-1.5</doi>
      <bibkey>alghanmi-etal-2020-combining</bibkey>
    </paper>
    <paper id="7">
      <title>PHINC : A Parallel Hinglish Social Media Code-Mixed Corpus for <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a><fixed-case>PHINC</fixed-case>: A Parallel <fixed-case>H</fixed-case>inglish Social Media Code-Mixed Corpus for Machine Translation</title>
      <author><first>Vivek</first><last>Srivastava</last></author>
      <author><first>Mayank</first><last>Singh</last></author>
      <pages>41–49</pages>
      <abstract>Code-mixing is the phenomenon of using more than one language in a sentence. In the <a href="https://en.wikipedia.org/wiki/Multilingualism">multilingual communities</a>, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> is a very frequently observed pattern of communication on <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a>. Flexibility to use multiple languages in one text message might help to communicate efficiently with the target audience. But, the noisy user-generated code-mixed text adds to the challenge of processing and understanding <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> to a much larger extent. Machine translation from monolingual source to the target language is a well-studied research problem. Here, we demonstrate that widely popular and sophisticated translation systems such as <a href="https://en.wikipedia.org/wiki/Google_Translate">Google Translate</a> fail at times to translate code-mixed text effectively. To address this challenge, we present a <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpus</a> of the 13,738 code-mixed Hindi-English sentences and their corresponding human translation in <a href="https://en.wikipedia.org/wiki/English_language">English</a>. In addition, we also propose a translation pipeline build on top of <a href="https://en.wikipedia.org/wiki/Google_Translate">Google Translate</a>. The evaluation of the proposed <a href="https://en.wikipedia.org/wiki/Pipeline_transport">pipeline</a> on PHINC demonstrates an increase in the performance of the underlying <a href="https://en.wikipedia.org/wiki/System">system</a>. With minimal effort, we can extend the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and the proposed approach to other code-mixing language pairs.<tex-math>PHINC</tex-math> demonstrates an increase in the performance of the underlying system. With minimal effort, we can extend the dataset and the proposed approach to other code-mixing language pairs.</abstract>
      <url hash="7a0cbddf">2020.wnut-1.7</url>
      <doi>10.18653/v1/2020.wnut-1.7</doi>
      <bibkey>srivastava-singh-2020-phinc</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/phinc">PHINC</pwcdataset>
    </paper>
    <paper id="11">
      <title>Non-ingredient Detection in User-generated Recipes using the Sequence Tagging Approach</title>
      <author><first>Yasuhiro</first><last>Yamaguchi</last></author>
      <author><first>Shintaro</first><last>Inuzuka</last></author>
      <author><first>Makoto</first><last>Hiramatsu</last></author>
      <author><first>Jun</first><last>Harashima</last></author>
      <pages>76–80</pages>
      <abstract>Recently, the number of <a href="https://en.wikipedia.org/wiki/User-generated_content">user-generated recipes</a> on the Internet has increased. In such <a href="https://en.wikipedia.org/wiki/Recipe">recipes</a>, users are generally supposed to write a title, an ingredient list, and steps to create a dish. However, some items in an ingredient list in a <a href="https://en.wikipedia.org/wiki/User-generated_content">user-generated recipe</a> are not actually edible ingredients. For example, headings, comments, and <a href="https://en.wikipedia.org/wiki/Kitchenware">kitchenware</a> sometimes appear in an ingredient list because users can freely write the list in their recipes. Such noise makes it difficult for computers to use <a href="https://en.wikipedia.org/wiki/Recipe">recipes</a> for a variety of <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a>, such as calorie estimation. To address this issue, we propose a non-ingredient detection method inspired by a neural sequence tagging model. In our experiment, we annotated 6,675 ingredients in 600 user-generated recipes and showed that our proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> achieved a 93.3 <a href="https://en.wikipedia.org/wiki/F-number">F1 score</a>.</abstract>
      <url hash="b13595b3">2020.wnut-1.11</url>
      <doi>10.18653/v1/2020.wnut-1.11</doi>
      <bibkey>yamaguchi-etal-2020-non</bibkey>
    </paper>
    <paper id="14">
      <title>An Empirical Analysis of Human-Bot Interaction on Reddit<fixed-case>R</fixed-case>eddit</title>
      <author><first>Ming-Cheng</first><last>Ma</last></author>
      <author><first>John P.</first><last>Lalor</last></author>
      <pages>101–106</pages>
      <abstract>Automated agents (bots) have emerged as an ubiquitous and influential presence on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. Bots engage on <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a> by posting content and replying to other users on the platform. In this work we conduct an empirical analysis of the activity of a single <a href="https://en.wikipedia.org/wiki/Internet_bot">bot</a> on <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a>. Our goal is to determine whether bot activity (in the form of posted comments on the website) has an effect on how humans engage on <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a>. We find that (1) the sentiment of a bot comment has a significant, positive effect on the subsequent human reply, and (2) human Reddit users modify their comment behaviors to overlap with the text of the bot, similar to how humans modify their text to mimic other humans in conversation. Understanding human-bot interactions on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> with relatively simple <a href="https://en.wikipedia.org/wiki/Internet_bot">bots</a> is important for preparing for more advanced <a href="https://en.wikipedia.org/wiki/Internet_bot">bots</a> in the future.</abstract>
      <url hash="2142851b">2020.wnut-1.14</url>
      <doi>10.18653/v1/2020.wnut-1.14</doi>
      <bibkey>ma-lalor-2020-empirical</bibkey>
    </paper>
    <paper id="15">
      <title>Detecting Trending Terms in Cybersecurity Forum Discussions</title>
      <author><first>Jack</first><last>Hughes</last></author>
      <author><first>Seth</first><last>Aycock</last></author>
      <author><first>Andrew</first><last>Caines</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <author><first>Alice</first><last>Hutchings</last></author>
      <pages>107–115</pages>
      <abstract>We present a lightweight method for identifying currently trending terms in relation to a known prior of terms, using a weighted log-odds ratio with an informative prior. We apply this method to a dataset of posts from an English-language underground hacking forum, spanning over ten years of activity, with posts containing misspellings, <a href="https://en.wikipedia.org/wiki/Orthography">orthographic variation</a>, <a href="https://en.wikipedia.org/wiki/Acronym">acronyms</a>, and <a href="https://en.wikipedia.org/wiki/Slang">slang</a>. Our statistical approach supports analysis of linguistic change and discussion topics over time, without a requirement to train a <a href="https://en.wikipedia.org/wiki/Topic_model">topic model</a> for each time interval for analysis. We evaluate the approach by comparing the results to TF-IDF using the discounted cumulative gain metric with human annotations, finding our method outperforms TF-IDF on <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a>.</abstract>
      <url hash="1f5b8b49">2020.wnut-1.15</url>
      <doi>10.18653/v1/2020.wnut-1.15</doi>
      <bibkey>hughes-etal-2020-detecting</bibkey>
    </paper>
    <paper id="18">
      <title>Punctuation Restoration using Transformer Models for High-and Low-Resource Languages</title>
      <author><first>Tanvirul</first><last>Alam</last></author>
      <author><first>Akib</first><last>Khan</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <pages>132–142</pages>
      <abstract>Punctuation restoration is a common post-processing problem for Automatic Speech Recognition (ASR) systems. It is important to improve the readability of the transcribed text for the human reader and facilitate NLP tasks. Current state-of-art address this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> using different <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning models</a>. Recently, transformer models have proven their success in downstream NLP tasks, and these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> have been explored very little for the punctuation restoration problem. In this work, we explore different transformer based models and propose an augmentation strategy for this task, focusing on high-resource (English) and low-resource (Bangla) languages. For <a href="https://en.wikipedia.org/wiki/English_language">English</a>, we obtain comparable state-of-the-art results, while for <a href="https://en.wikipedia.org/wiki/Bengali_language">Bangla</a>, it is the first reported work, which can serve as a strong baseline for future work. We have made our developed Bangla dataset publicly available for the research community.</abstract>
      <url hash="267337be">2020.wnut-1.18</url>
      <doi>10.18653/v1/2020.wnut-1.18</doi>
      <bibkey>alam-etal-2020-punctuation</bibkey>
      <pwccode url="https://github.com/xashru/punctuation-restoration" additional="false">xashru/punctuation-restoration</pwccode>
    </paper>
    <paper id="20">
      <title>Fine-Tuning MT systems for Robustness to Second-Language Speaker Variations<fixed-case>MT</fixed-case> systems for Robustness to Second-Language Speaker Variations</title>
      <author><first>Md Mahfuz Ibn</first><last>Alam</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>149–158</pages>
      <abstract>The performance of neural machine translation (NMT) systems only trained on a single language variant degrades when confronted with even slightly different language variations. With this work, we build upon previous work to explore how to mitigate this issue. We show that <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> using naturally occurring noise along with pseudo-references (i.e. corrected non-native inputs translated using the baseline NMT system) is a promising solution towards systems robust to such type of input variations. We focus on four translation pairs, from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>, <a href="https://en.wikipedia.org/wiki/Italian_language">Italian</a>, <a href="https://en.wikipedia.org/wiki/French_language">French</a>, and <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese</a>, with our system achieving improvements of up to 3.1 BLEU points compared to the baselines, establishing a new state-of-the-art on the JFLEG-ES dataset. All datasets and code are publicly available here : https://github.com/mahfuzibnalam/finetuning_for_robustness.</abstract>
      <url hash="f665096a">2020.wnut-1.20</url>
      <doi>10.18653/v1/2020.wnut-1.20</doi>
      <bibkey>alam-anastasopoulos-2020-fine</bibkey>
      <pwccode url="https://github.com/mahfuzibnalam/finetuning_for_robustness" additional="false">mahfuzibnalam/finetuning_for_robustness</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="21">
      <title>Impact of ASR on Alzheimer’s Disease Detection : All Errors are Equal, but Deletions are More Equal than Others<fixed-case>ASR</fixed-case> on <fixed-case>A</fixed-case>lzheimer’s Disease Detection: All Errors are Equal, but Deletions are More Equal than Others</title>
      <author><first>Aparna</first><last>Balagopalan</last></author>
      <author><first>Ksenia</first><last>Shkaruta</last></author>
      <author><first>Jekaterina</first><last>Novikova</last></author>
      <pages>159–164</pages>
      <abstract>Automatic Speech Recognition (ASR) is a critical component of any fully-automated speech-based dementia detection model. However, despite years of speech recognition research, little is known about the impact of ASR accuracy on dementia detection. In this paper, we experiment with controlled amounts of artificially generated ASR errors and investigate their influence on dementia detection. We find that deletion errors affect <a href="https://en.wikipedia.org/wiki/Detection_theory">detection</a> performance the most, due to their impact on the features of syntactic complexity and discourse representation in speech. We show the trend to be generalisable across two different datasets for cognitive impairment detection. As a conclusion, we propose optimising the ASR to reflect a higher penalty for <a href="https://en.wikipedia.org/wiki/Deletion_(genetics)">deletion errors</a> in order to improve dementia detection performance.</abstract>
      <url hash="0914b233">2020.wnut-1.21</url>
      <attachment type="OptionalSupplementaryMaterial" hash="84821a87">2020.wnut-1.21.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.wnut-1.21</doi>
      <bibkey>balagopalan-etal-2020-impact</bibkey>
    </paper>
    <paper id="22">
      <title>Detecting Entailment in Code-Mixed Hindi-English Conversations<fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Conversations</title>
      <author><first>Sharanya</first><last>Chakravarthy</last></author>
      <author><first>Anjana</first><last>Umapathy</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>165–170</pages>
      <abstract>The presence of large-scale corpora for Natural Language Inference (NLI) has spurred deep learning research in this area, though much of this research has focused solely on monolingual data. Code-mixing is the intertwined usage of multiple languages, and is commonly seen in informal conversations among <a href="https://en.wikipedia.org/wiki/Multilingualism">polyglots</a>. Given the rising importance of dialogue agents, it is imperative that they understand <a href="https://en.wikipedia.org/wiki/Code-mixing">code-mixing</a>, but the scarcity of code-mixed Natural Language Understanding (NLU) datasets has precluded research in this area. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> by Khanuja et. al. for detecting conversational entailment in code-mixed Hindi-English text is the first of its kind. We investigate the effectiveness of <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a>, <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, <a href="https://en.wikipedia.org/wiki/Translation">translation</a>, and architectural approaches to address the code-mixed, conversational, and low-resource aspects of this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. We obtain an 8.09 % increase in test set accuracy over the current state of the art.</abstract>
      <url hash="c3b2840b">2020.wnut-1.22</url>
      <doi>10.18653/v1/2020.wnut-1.22</doi>
      <bibkey>chakravarthy-etal-2020-detecting</bibkey>
      <pwccode url="https://github.com/sharanyarc96/hinglishnli" additional="false">sharanyarc96/hinglishnli</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="24">
      <title>Annotation Efficient <a href="https://en.wikipedia.org/wiki/Language_identification">Language Identification</a> from Weak Labels</title>
      <author><first>Shriphani</first><last>Palakodety</last></author>
      <author><first>Ashiqur</first><last>KhudaBukhsh</last></author>
      <pages>181–192</pages>
      <abstract>India is home to several languages with more than 30 m speakers. These <a href="https://en.wikipedia.org/wiki/Language">languages</a> exhibit significant presence on <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a>. However, several of these widely-used languages are under-addressed by current Natural Language Processing (NLP) models and resources. User generated social media content in these <a href="https://en.wikipedia.org/wiki/Language">languages</a> is also typically authored in the <a href="https://en.wikipedia.org/wiki/Latin_script">Roman script</a> as opposed to the traditional native script further contributing to resource scarcity. In this paper, we leverage a minimally supervised NLP technique to obtain weak language labels from a large-scale Indian social media corpus leading to a robust and annotation-efficient language-identification technique spanning nine Romanized Indian languages. In fast-spreading pandemic situations such as the current COVID-19 situation, information processing objectives might be heavily tilted towards under-served languages in densely populated regions. We release our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to facilitate downstream analyses in these low-resource languages. Experiments across multiple <a href="https://en.wikipedia.org/wiki/Social_media">social media corpora</a> demonstrate the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>’s robustness and provide several interesting insights on Indian language usage patterns on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. We release an annotated data set of 1,000 comments in ten <a href="https://en.wikipedia.org/wiki/Romanization_(cultural)">Romanized languages</a> as a social media evaluation benchmark.</abstract>
      <url hash="a803b951">2020.wnut-1.24</url>
      <doi>10.18653/v1/2020.wnut-1.24</doi>
      <bibkey>palakodety-khudabukhsh-2020-annotation</bibkey>
    </paper>
    <paper id="25">
      <title>Fantastic Features and Where to Find Them : Detecting Cognitive Impairment with a Subsequence Classification Guided Approach</title>
      <author><first>Ben</first><last>Eyre</last></author>
      <author><first>Aparna</first><last>Balagopalan</last></author>
      <author><first>Jekaterina</first><last>Novikova</last></author>
      <pages>193–199</pages>
      <abstract>Despite the widely reported success of embedding-based machine learning methods on natural language processing tasks, the use of more easily interpreted engineered features remains common in fields such as cognitive impairment (CI) detection. Manually engineering features from <a href="https://en.wikipedia.org/wiki/Noisy_text">noisy text</a> is time and resource consuming, and can potentially result in <a href="https://en.wikipedia.org/wiki/Feature_(computer_vision)">features</a> that do not enhance <a href="https://en.wikipedia.org/wiki/Computer_simulation">model</a> performance. To combat this, we describe a new approach to <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a> that leverages sequential machine learning models and <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> to predict which <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> help enhance performance. We provide a concrete example of this <a href="https://en.wikipedia.org/wiki/Methodology">method</a> on a standard data set of CI speech and demonstrate that CI classification accuracy improves by 2.3 % over a strong baseline when using <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> produced by this <a href="https://en.wikipedia.org/wiki/Methodology">method</a>. This demonstration provides an example of how this method can be used to assist <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> in fields where <a href="https://en.wikipedia.org/wiki/Interpretability">interpretability</a> is important, such as <a href="https://en.wikipedia.org/wiki/Health_care">health care</a>.</abstract>
      <url hash="fb9c2b7d">2020.wnut-1.25</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8564b332">2020.wnut-1.25.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.wnut-1.25</doi>
      <bibkey>eyre-etal-2020-fantastic</bibkey>
    </paper>
    <paper id="28">
      <title>Civil Unrest on Twitter (CUT): A Dataset of Tweets to Support Research on Civil Unrest<fixed-case>T</fixed-case>witter (<fixed-case>CUT</fixed-case>): A Dataset of Tweets to Support Research on Civil Unrest</title>
      <author><first>Justin</first><last>Sech</last></author>
      <author><first>Alexandra</first><last>DeLucia</last></author>
      <author><first>Anna L.</first><last>Buczak</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>215–221</pages>
      <abstract>We present CUT, a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for studying <a href="https://en.wikipedia.org/wiki/Civil_disorder">Civil Unrest</a> on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>. Our dataset includes 4,381 tweets related to <a href="https://en.wikipedia.org/wiki/Civil_disorder">civil unrest</a>, hand-annotated with information related to the study of civil unrest discussion and events. Our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is drawn from 42 countries from 2014 to 2019. We present <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline systems</a> trained on this <a href="https://en.wikipedia.org/wiki/Data">data</a> for the identification of tweets related to <a href="https://en.wikipedia.org/wiki/Civil_disorder">civil unrest</a>. We include a discussion of ethical issues related to research on this topic.</abstract>
      <url hash="49aba556">2020.wnut-1.28</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c311282c">2020.wnut-1.28.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.wnut-1.28</doi>
      <bibkey>sech-etal-2020-civil</bibkey>
      <pwccode url="https://github.com/aadelucia/jhu-cut" additional="false">aadelucia/jhu-cut</pwccode>
    </paper>
    <paper id="30">
      <title>Representation learning of writing style</title>
      <author><first>Julien</first><last>Hay</last></author>
      <author><first>Bich-Lien</first><last>Doan</last></author>
      <author><first>Fabrice</first><last>Popineau</last></author>
      <author><first>Ouassim</first><last>Ait Elhara</last></author>
      <pages>232–243</pages>
      <abstract>In this paper, we introduce a new method of <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning</a> that aims to embed documents in a stylometric space. Previous studies in the field of <a href="https://en.wikipedia.org/wiki/Authorship_analysis">authorship analysis</a> focused on feature engineering techniques in order to represent document styles and to enhance <a href="https://en.wikipedia.org/wiki/Computer_simulation">model</a> performance in specific tasks. Instead, we directly embed documents in a stylometric space by relying on a reference set of authors and the intra-author consistency property which is one of two components in our definition of <a href="https://en.wikipedia.org/wiki/Writing_style">writing style</a>. The main intuition of this paper is that we can define a general stylometric space from a set of reference authors such that, in this space, the coordinates of different documents will be close when the documents are by the same author, and spread away when they are by different authors, even for documents by authors who are not in the set of reference authors. The method we propose allows for the clustering of documents based on stylistic clues reflecting the authorship of documents. For the empirical validation of the method, we train a deep neural network model to predict authors of a large reference dataset consisting of news and blog articles. Albeit the learning process is supervised, it does not require a dedicated labeling of the data but <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> relies only on the metadata of the articles which are available in huge amounts. We evaluate the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> on multiple <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, on both the authorship clustering and the authorship attribution tasks.</abstract>
      <url hash="9682d7d0">2020.wnut-1.30</url>
      <doi>10.18653/v1/2020.wnut-1.30</doi>
      <bibkey>hay-etal-2020-representation</bibkey>
      <pwccode url="https://github.com/hayj/deepstyle" additional="false">hayj/deepstyle</pwccode>
    </paper>
    <paper id="31">
      <title>A Little Birdie Told Me...  -Inductive Biases for Rumour Stance Detection on <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a></title>
      <author><first>Karthik</first><last>Radhakrishnan</last></author>
      <author><first>Tushar</first><last>Kanakagiri</last></author>
      <author><first>Sharanya</first><last>Chakravarthy</last></author>
      <author><first>Vidhisha</first><last>Balachandran</last></author>
      <pages>244–248</pages>
      <abstract>The rise in the usage of <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> has placed <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> in a central position for <a href="https://en.wikipedia.org/wiki/Dissemination">news dissemination</a> and consumption. This greatly increases the potential for proliferation of <a href="https://en.wikipedia.org/wiki/Rumor">rumours</a> and <a href="https://en.wikipedia.org/wiki/Misinformation">misinformation</a>. In an effort to mitigate the spread of rumours, we tackle the related task of identifying the stance (Support, Deny, Query, Comment) of a <a href="https://en.wikipedia.org/wiki/Social_media_marketing">social media post</a>. Unlike previous works, we impose <a href="https://en.wikipedia.org/wiki/Inductive_reasoning">inductive biases</a> that capture platform specific user behavior. These <a href="https://en.wikipedia.org/wiki/Bias">biases</a>, coupled with social media fine-tuning of BERT allow for better <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">language understanding</a>, thus yielding an F1 score of 58.7 on the SemEval 2019 task on rumour stance detection.</abstract>
      <url hash="07e94742">2020.wnut-1.31</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b721a636">2020.wnut-1.31.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.wnut-1.31</doi>
      <bibkey>radhakrishnan-etal-2020-little</bibkey>
    </paper>
    <paper id="34">
      <title>IITKGP at W-NUT 2020 Shared Task-1 : Domain specific BERT representation for Named Entity Recognition of lab protocol<fixed-case>IITKGP</fixed-case> at <fixed-case>W</fixed-case>-<fixed-case>NUT</fixed-case> 2020 Shared Task-1: Domain specific <fixed-case>BERT</fixed-case> representation for Named Entity Recognition of lab protocol</title>
      <author><first>Tejas</first><last>Vaidhya</last></author>
      <author><first>Ayush</first><last>Kaushal</last></author>
      <pages>268–272</pages>
      <abstract>Supervised models trained to predict properties from representations have been achieving high accuracy on a variety of tasks. For in-stance, the BERT family seems to work exceptionally well on the downstream task from NER tagging to the range of other linguistictasks. But the vocabulary used in the <a href="https://en.wikipedia.org/wiki/Medicine">medical field</a> contains a lot of different tokens used only in the <a href="https://en.wikipedia.org/wiki/Healthcare_industry">medical industry</a> such as the name of different diseases, devices, organisms, medicines, etc. that makes it difficult for traditional BERT model to create contextualized embedding. In this paper, we are going to illustrate the <a href="https://en.wikipedia.org/wiki/System">System</a> for Named Entity Tagging based on Bio-Bert. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> gives substantial improvements over the baseline and stood the fourth runner up in terms of <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a>, and first runner up in terms of <a href="https://en.wikipedia.org/wiki/Recall_(memory)">Recall</a> with just 2.21 <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> behind the best one.</abstract>
      <url hash="60747a41">2020.wnut-1.34</url>
      <doi>10.18653/v1/2020.wnut-1.34</doi>
      <bibkey>vaidhya-kaushal-2020-iitkgp</bibkey>
      <pwccode url="https://github.com/tejasvaidhyadev/NER_Lab_Protocols" additional="false">tejasvaidhyadev/NER_Lab_Protocols</pwccode>
    </paper>
    <paper id="38">
      <title>mgsohrab at WNUT 2020 Shared Task-1 : Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols<fixed-case>WNUT</fixed-case> 2020 Shared Task-1: Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols</title>
      <author><first>Mohammad Golam</first><last>Sohrab</last></author>
      <author><first>Anh-Khoa</first><last>Duong Nguyen</last></author>
      <author><first>Makoto</first><last>Miwa</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <pages>290–298</pages>
      <abstract>We present a neural exhaustive approach that addresses named entity recognition (NER) and relation recognition (RE), for the entity and re- lation recognition over the wet-lab protocols shared task. We introduce BERT-based neural exhaustive approach that enumerates all pos- sible spans as potential entity mentions and classifies them into entity types or no entity with deep neural networks to address NER. To solve relation extraction task, based on the NER predictions or given gold mentions we create all possible trigger-argument pairs and classify them into relation types or no relation. In NER task, we achieved 76.60 % in terms of <a href="https://en.wikipedia.org/wiki/F-score">F-score</a> as third rank system among the partic- ipated systems. In relation extraction task, we achieved 80.46 % in terms of <a href="https://en.wikipedia.org/wiki/F-score">F-score</a> as the top system in the relation extraction or recognition task. Besides we compare our model based on the wet lab protocols corpus (WLPC) with the WLPC baseline and dynamic graph-based in- formation extraction (DyGIE) systems.</abstract>
      <url hash="5e97f082">2020.wnut-1.38</url>
      <doi>10.18653/v1/2020.wnut-1.38</doi>
      <bibkey>sohrab-etal-2020-mgsohrab</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-20-task-1-extracting-entities-and">WNUT 2020</pwcdataset>
    </paper>
    <paper id="41">
      <title>WNUT-2020 Task 2 : Identification of Informative COVID-19 English Tweets<fixed-case>WNUT</fixed-case>-2020 Task 2: Identification of Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <author><first>Thanh</first><last>Vu</last></author>
      <author><first>Afshin</first><last>Rahimi</last></author>
      <author><first>Mai Hoang</first><last>Dao</last></author>
      <author><first>Linh The</first><last>Nguyen</last></author>
      <author><first>Long</first><last>Doan</last></author>
      <pages>314–318</pages>
      <abstract>In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus of 10 K Tweets</a> and organize the development and evaluation phases for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline <a href="https://en.wikipedia.org/wiki/FastText">fastText</a> (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised training</a> performs well in this task.</abstract>
      <url hash="fc2a48b6">2020.wnut-1.41</url>
      <doi>10.18653/v1/2020.wnut-1.41</doi>
      <bibkey>nguyen-etal-2020-wnut</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2020-task-2">WNUT-2020 Task 2</pwcdataset>
    </paper>
    <paper id="45">
      <title>Siva at WNUT-2020 Task 2 : Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets<fixed-case>WNUT</fixed-case>-2020 Task 2: Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets</title>
      <author><first>Siva</first><last>Sai</last></author>
      <pages>337–341</pages>
      <abstract>Social media witnessed vast amounts of <a href="https://en.wikipedia.org/wiki/Misinformation">misinformation</a> being circulated every day during the Covid-19 pandemic so much so that the WHO Director-General termed the phenomenon as infodemic. The ill-effects of such <a href="https://en.wikipedia.org/wiki/Misinformation">misinformation</a> are multifarious. Thus, identifying and eliminating the sources of <a href="https://en.wikipedia.org/wiki/Misinformation">misinformation</a> becomes very crucial, especially when <a href="https://en.wikipedia.org/wiki/Mass_psychogenic_illness">mass panic</a> can be controlled only through the right information. However, manual identification is arduous, with such large amounts of data being generated every day. This shows the importance of automatic identification of misinformative posts on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. WNUT-2020 Task 2 aims at building <a href="https://en.wikipedia.org/wiki/System">systems</a> for automatic identification of informative tweets. In this paper, I discuss my approach to WNUT-2020 Task 2. I fine-tuned eleven variants of four transformer networks -BERT, RoBERTa, XLM-RoBERTa, ELECTRA, on top of two different preprocessing techniques to reap good results. My top submission achieved an F1-score of 85.3 % in the final evaluation.</abstract>
      <url hash="40268765">2020.wnut-1.45</url>
      <doi>10.18653/v1/2020.wnut-1.45</doi>
      <bibkey>sai-2020-siva</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2020-task-2">WNUT-2020 Task 2</pwcdataset>
    </paper>
    <paper id="48">
      <title>CXP949 at WNUT-2020 Task 2 : Extracting Informative COVID-19 Tweets-RoBERTa Ensembles and The Continued Relevance of Handcrafted Features<fixed-case>CXP</fixed-case>949 at <fixed-case>WNUT</fixed-case>-2020 Task 2: Extracting Informative <fixed-case>COVID</fixed-case>-19 Tweets - <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a Ensembles and The Continued Relevance of Handcrafted Features</title>
      <author><first>Calum</first><last>Perrio</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <pages>352–358</pages>
      <abstract>This paper presents our submission to Task 2 of the Workshop on Noisy User-generated Text. We explore improving the performance of a pre-trained transformer-based language model fine-tuned for text classification through an ensemble implementation that makes use of corpus level information and a handcrafted feature. We test the effectiveness of including the aforementioned <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> in accommodating the challenges of a noisy data set centred on a specific subject outside the remit of the pre-training data. We show that inclusion of additional <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> can improve <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> results and achieve a score within 2 points of the top performing team.</abstract>
      <url hash="2b799cf4">2020.wnut-1.48</url>
      <doi>10.18653/v1/2020.wnut-1.48</doi>
      <bibkey>perrio-tayyar-madabushi-2020-cxp949</bibkey>
    </paper>
    <paper id="55">
      <title>CSECU-DSG at WNUT-2020 Task 2 : Exploiting Ensemble of Transfer Learning and Hand-crafted Features for Identification of Informative COVID-19 English Tweets<fixed-case>CSECU</fixed-case>-<fixed-case>DSG</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Exploiting Ensemble of Transfer Learning and Hand-crafted Features for Identification of Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Fareen</first><last>Tasneem</last></author>
      <author><first>Jannatun</first><last>Naim</last></author>
      <author><first>Radiathun</first><last>Tasnia</last></author>
      <author><first>Tashin</first><last>Hossain</last></author>
      <author><first>Abu Nowshed</first><last>Chy</last></author>
      <pages>394–398</pages>
      <abstract>COVID-19 pandemic has become the trending topic on twitter and people are interested in sharing diverse information ranging from new cases, healthcare guidelines, medicine, and vaccine news. Such information assists the people to be updated about the situation as well as beneficial for public safety personnel for decision making. However, the informal nature of <a href="https://en.wikipedia.org/wiki/Twitter">twitter</a> makes it challenging to refine the informative tweets from the huge tweet streams. To address these challenges WNUT-2020 introduced a shared task focusing on COVID-19 related informative tweet identification. In this paper, we describe our participation in this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. We propose a neural model that adopts the strength of <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> and hand-crafted features in a unified architecture. To extract the transfer learning features, we utilize the state-of-the-art pre-trained sentence embedding model BERT, RoBERTa, and InferSent, whereas various twitter characteristics are exploited to extract the hand-crafted features. Next, various feature combinations are utilized to train a set of multilayer perceptron (MLP) as the base-classifier. Finally, a majority voting based fusion approach is employed to determine the informative tweets. Our approach achieved competitive performance and outperformed the baseline by 7 % (approx.</abstract>
      <url hash="f5edb100">2020.wnut-1.55</url>
      <doi>10.18653/v1/2020.wnut-1.55</doi>
      <bibkey>tasneem-etal-2020-csecu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2020-task-2">WNUT-2020 Task 2</pwcdataset>
    </paper>
    <paper id="56">
      <title>IRLab@IITBHU at WNUT-2020 Task 2 : Identification of informative COVID-19 English Tweets using BERT<fixed-case>IRL</fixed-case>ab@<fixed-case>IITBHU</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Identification of informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets using <fixed-case>BERT</fixed-case></title>
      <author><first>Supriya</first><last>Chanda</last></author>
      <author><first>Eshita</first><last>Nandy</last></author>
      <author><first>Sukomal</first><last>Pal</last></author>
      <pages>399–403</pages>
      <abstract>This paper reports our submission to the shared Task 2 : Identification of informative COVID-19 English tweets at W-NUT 2020. We attempted a few techniques, and we briefly explain here two models that showed promising results in tweet classification tasks : DistilBERT and <a href="https://en.wikipedia.org/wiki/FastText">FastText</a>. DistilBERT achieves a F1 score of 0.7508 on the test set, which is the best of our submissions.</abstract>
      <url hash="813d202a">2020.wnut-1.56</url>
      <doi>10.18653/v1/2020.wnut-1.56</doi>
      <bibkey>chanda-etal-2020-irlab</bibkey>
      <pwccode url="https://github.com/VinAIResearch/COVID19Tweet" additional="false">VinAIResearch/COVID19Tweet</pwccode>
    </paper>
    <paper id="58">
      <title>DSC-IIT ISM at WNUT-2020 Task 2 : Detection of COVID-19 informative tweets using RoBERTa<fixed-case>DSC</fixed-case>-<fixed-case>IIT</fixed-case> <fixed-case>ISM</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Detection of <fixed-case>COVID</fixed-case>-19 informative tweets using <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a</title>
      <author><first>Sirigireddy</first><last>Dhana Laxmi</last></author>
      <author><first>Rohit</first><last>Agarwal</last></author>
      <author><first>Aman</first><last>Sinha</last></author>
      <pages>409–413</pages>
      <abstract>Social media such as <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> is a hotspot of user-generated information. In this ongoing Covid-19 pandemic, there has been an abundance of data on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> which can be classified as informative and uninformative content. In this paper, we present our work to detect informative Covid-19 English tweets using RoBERTa model as a part of the W-NUT workshop 2020. We show the efficacy of our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> on a public dataset with an <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> of 0.89 on the validation dataset and 0.87 on the <a href="https://en.wikipedia.org/wiki/Score_(statistics)">leaderboard</a>.</abstract>
      <url hash="3853e316">2020.wnut-1.58</url>
      <doi>10.18653/v1/2020.wnut-1.58</doi>
      <bibkey>dhana-laxmi-etal-2020-dsc</bibkey>
    </paper>
    <paper id="60">
      <title>NLPRL at WNUT-2020 Task 2 : ELMo-based System for Identification of COVID-19 Tweets<fixed-case>NLPRL</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: <fixed-case>ELM</fixed-case>o-based System for Identification of <fixed-case>COVID</fixed-case>-19 Tweets</title>
      <author><first>Rajesh Kumar</first><last>Mundotiya</last></author>
      <author><first>Rupjyoti</first><last>Baruah</last></author>
      <author><first>Bhavana</first><last>Srivastava</last></author>
      <author><first>Anil Kumar</first><last>Singh</last></author>
      <pages>419–422</pages>
      <abstract>The Coronavirus pandemic has been a dominating news on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people and their related symptoms, as available on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, such as <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>, can help in <a href="https://en.wikipedia.org/wiki/Preventive_healthcare">prevention</a> and taking precautions. This is an example of using noisy text processing for <a href="https://en.wikipedia.org/wiki/Emergency_management">disaster management</a>. This paper discusses the NLPRL results in Shared Task-2 of WNUT-2020 workshop. We have considered this problem as a binary classification problem and have used a pre-trained ELMo embedding with GRU units. This approach helps classify the tweets with <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> as 80.85 % and 78.54 % as <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> on the provided test dataset. The experimental code is available online.</abstract>
      <url hash="9dfb4503">2020.wnut-1.60</url>
      <doi>10.18653/v1/2020.wnut-1.60</doi>
      <bibkey>mundotiya-etal-2020-nlprl</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2020-task-2">WNUT-2020 Task 2</pwcdataset>
    </paper>
    <paper id="63">
      <title>ComplexDataLab at W-NUT 2020 Task 2 : Detecting Informative COVID-19 Tweets by Attending over Linked Documents<fixed-case>C</fixed-case>omplex<fixed-case>D</fixed-case>ata<fixed-case>L</fixed-case>ab at <fixed-case>W</fixed-case>-<fixed-case>NUT</fixed-case> 2020 Task 2: Detecting Informative <fixed-case>COVID</fixed-case>-19 Tweets by Attending over Linked Documents</title>
      <author><first>Kellin</first><last>Pelrine</last></author>
      <author><first>Jacob</first><last>Danovitch</last></author>
      <author><first>Albert</first><last>Orozco Camacho</last></author>
      <author><first>Reihaneh</first><last>Rabbany</last></author>
      <pages>434–439</pages>
      <abstract>Given the global scale of COVID-19 and the flood of social media content related to it, how can we find informative discussions? We present Gapformer, which effectively classifies content as informative or not. It reformulates the problem as <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph classification</a>, drawing on not only the tweet but connected webpages and entities. We leverage a pre-trained language model as well as the connections between nodes to learn a pooled representation for each document network. We show it outperforms several competitive baselines and present ablation studies supporting the benefit of the linked information. Code is available on Github.</abstract>
      <url hash="c1693783">2020.wnut-1.63</url>
      <doi>10.18653/v1/2020.wnut-1.63</doi>
      <bibkey>pelrine-etal-2020-complexdatalab</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2020-task-2">WNUT-2020 Task 2</pwcdataset>
    </paper>
    <paper id="65">
      <title>LynyrdSkynyrd at WNUT-2020 Task 2 : Semi-Supervised Learning for Identification of Informative COVID-19 English Tweets<fixed-case>L</fixed-case>ynyrd<fixed-case>S</fixed-case>kynyrd at <fixed-case>WNUT</fixed-case>-2020 Task 2: Semi-Supervised Learning for Identification of Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Abhilasha</first><last>Sancheti</last></author>
      <author><first>Kushal</first><last>Chawla</last></author>
      <author><first>Gaurav</first><last>Verma</last></author>
      <pages>444–449</pages>
      <abstract>In this work, we describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well as recent advances in pre-trained language models that help in capturing the syntactic, semantic, and contextual features from the tweets. We further employ pseudo-labelling to incorporate the unlabelled Twitter data released on the pandemic. Our best performing <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> achieves an F1-score of 0.9179 on the provided validation set and 0.8805 on the blind test-set.</abstract>
      <url hash="29e1d30b">2020.wnut-1.65</url>
      <doi>10.18653/v1/2020.wnut-1.65</doi>
      <bibkey>sancheti-etal-2020-lynyrdskynyrd</bibkey>
    </paper>
    <paper id="73">
      <title>SunBear at WNUT-2020 Task 2 : Improving BERT-Based Noisy Text Classification with Knowledge of the Data domain<fixed-case>S</fixed-case>un<fixed-case>B</fixed-case>ear at <fixed-case>WNUT</fixed-case>-2020 Task 2: Improving <fixed-case>BERT</fixed-case>-Based Noisy Text Classification with Knowledge of the Data domain</title>
      <author><first>Linh</first><last>Doan Bao</last></author>
      <author><first>Viet Anh</first><last>Nguyen</last></author>
      <author><first>Quang</first><last>Pham Huu</last></author>
      <pages>485–490</pages>
      <abstract>This paper proposes an improved custom model for WNUT task 2 : Identification of Informative COVID-19 English Tweet. We improve experiment with the effectiveness of <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning methodologies</a> for state-of-the-art <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> RoBERTa. We make a preliminary instantiation of this formal <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for the text classification approaches. With appropriate training techniques, our model is able to achieve 0.9218 <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> on public validation set and the ensemble version settles at top 9 <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> (0.9005) and top 2 Recall (0.9301) on private test set.</abstract>
      <url hash="59778d2f">2020.wnut-1.73</url>
      <doi>10.18653/v1/2020.wnut-1.73</doi>
      <bibkey>doan-bao-etal-2020-sunbear</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2020-task-2">WNUT-2020 Task 2</pwcdataset>
    </paper>
    <paper id="75">
      <title>COVCOR20 at WNUT-2020 Task 2 : An Attempt to Combine <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> and Expert rules<fixed-case>COVCOR</fixed-case>20 at <fixed-case>WNUT</fixed-case>-2020 Task 2: An Attempt to Combine Deep Learning and Expert rules</title>
      <author><first>Ali</first><last>Hürriyetoğlu</last></author>
      <author><first>Ali</first><last>Safaya</last></author>
      <author><first>Osman</first><last>Mutlu</last></author>
      <author><first>Nelleke</first><last>Oostdijk</last></author>
      <author><first>Erdem</first><last>Yörük</last></author>
      <pages>495–498</pages>
      <abstract>In the scope of WNUT-2020 Task 2, we developed various text classification systems, using <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning models</a> and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically informed rules, we found that through the integration of (the output of) the three systems a better performance could be achieved than the standalone performance of each approach in a cross-validation setting. However, on the test data the performance of the <a href="https://en.wikipedia.org/wiki/Integral">integration</a> was slightly lower than our best performing deep learning model. These results hardly indicate any progress in line of integrating <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> and expert rules driven systems. We expect that the release of the annotation manuals and gold labels of the test data after this workshop will shed light on these perplexing results.</abstract>
      <url hash="5ba041ce">2020.wnut-1.75</url>
      <doi>10.18653/v1/2020.wnut-1.75</doi>
      <bibkey>hurriyetoglu-etal-2020-covcor20</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2020-task-2">WNUT-2020 Task 2</pwcdataset>
    </paper>
    <paper id="76">
      <title>TEST_POSITIVE at W-NUT 2020 Shared Task-3 : Cross-task modeling<fixed-case>TEST</fixed-case>_<fixed-case>POSITIVE</fixed-case> at <fixed-case>W</fixed-case>-<fixed-case>NUT</fixed-case> 2020 Shared Task-3: Cross-task modeling</title>
      <author><first>Chacha</first><last>Chen</last></author>
      <author><first>Chieh-Yang</first><last>Huang</last></author>
      <author><first>Yaqi</first><last>Hou</last></author>
      <author><first>Yang</first><last>Shi</last></author>
      <author><first>Enyan</first><last>Dai</last></author>
      <author><first>Jiaqi</first><last>Wang</last></author>
      <pages>499–504</pages>
      <abstract>The competition of extracting COVID-19 events from <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> is to develop systems that can automatically extract related events from <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a>. The built <a href="https://en.wikipedia.org/wiki/System">system</a> should identify different pre-defined slots for each event, in order to answer important questions (e.g., Who is tested positive? What is the age of the person? Where is he / she?). To tackle these challenges, we propose the Joint Event Multi-task Learning (JOELIN) model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>. Moreover, we implement a type-aware post-processing procedure using named entity recognition (NER) to further filter the predictions. JOELIN outperforms the BERT baseline by 17.2 % in micro F1.</abstract>
      <url hash="54d58088">2020.wnut-1.76</url>
      <doi>10.18653/v1/2020.wnut-1.76</doi>
      <bibkey>chen-etal-2020-test</bibkey>
    </paper>
    <paper id="80">
      <title>HLTRI at W-NUT 2020 Shared Task-3 : COVID-19 Event Extraction from Twitter Using Multi-Task Hopfield Pooling<fixed-case>HLTRI</fixed-case> at <fixed-case>W</fixed-case>-<fixed-case>NUT</fixed-case> 2020 Shared Task-3: <fixed-case>COVID</fixed-case>-19 Event Extraction from <fixed-case>T</fixed-case>witter Using Multi-Task Hopfield Pooling</title>
      <author><first>Maxwell</first><last>Weinzierl</last></author>
      <author><first>Sanda</first><last>Harabagiu</last></author>
      <pages>530–538</pages>
      <abstract>Extracting structured knowledge involving self-reported events related to the COVID-19 pandemic from <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> has the potential to inform surveillance systems that play a critical role in <a href="https://en.wikipedia.org/wiki/Public_health">public health</a>. The event extraction challenge presented by the W-NUT 2020 Shared Task 3 focused on the identification of five types of events relevant to the COVID-19 pandemic and their respective set of pre-defined slots encoding demographic, epidemiological, clinical as well as spatial, temporal or subjective knowledge. Our participation in the challenge led to the design of a neural architecture for jointly identifying all Event Slots expressed in a tweet relevant to an event of interest. This <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a> uses COVID-Twitter-BERT as the pre-trained language model. In addition, to learn text span embeddings for each Event Slot, we relied on a special case of <a href="https://en.wikipedia.org/wiki/Hopfield_network">Hopfield Networks</a>, namely Hopfield pooling. The results of the shared task evaluation indicate that our system performs best when it is trained on a larger dataset, while <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> remains competitive when training on smaller datasets.</abstract>
      <url hash="86654687">2020.wnut-1.80</url>
      <doi>10.18653/v1/2020.wnut-1.80</doi>
      <bibkey>weinzierl-harabagiu-2020-hltri</bibkey>
    </paper>
  </volume>
</collection>