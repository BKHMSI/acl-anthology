<?xml version='1.0' encoding='utf-8'?>
<collection id="R19">
  <volume id="1" ingest-date="2020-01-15">
    <meta>
      <booktitle>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</booktitle>
      <url hash="ff24b8c4">R19-1</url>
      <editor><first>Ruslan</first><last>Mitkov</last></editor>
      <editor><first>Galia</first><last>Angelova</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="e25a755c">R19-1000</url>
      <bibkey>ranlp-2019-international</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Identification of Good and Bad News on Twitter<fixed-case>T</fixed-case>witter</title>
      <author><first>Piush</first><last>Aggarwal</last></author>
      <author><first>Ahmet</first><last>Aker</last></author>
      <pages>9–17</pages>
      <abstract>Social media plays a great role in <a href="https://en.wikipedia.org/wiki/Dissemination">news dissemination</a> which includes <a href="https://en.wikipedia.org/wiki/News">good and bad news</a>. However, studies show that <a href="https://en.wikipedia.org/wiki/News">news</a>, in general, has a significant impact on our mental stature and that this influence is more in bad news. An ideal situation would be that we have a tool that can help to filter out the type of news we do not want to consume. In this paper, we provide the basis for such a <a href="https://en.wikipedia.org/wiki/Tool">tool</a>. In our work, we focus on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>. We release a manually annotated dataset containing 6,853 tweets from 5 different topical categories. Each tweet is annotated with good and bad labels. We also investigate various <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning systems</a> and <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> and evaluate their performance on the newly generated <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. We also perform a comparative analysis with <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiments</a> showing that <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiment</a> alone is not enough to distinguish between good and bad news.</abstract>
      <url hash="86b5e65f">R19-1002</url>
      <doi>10.26615/978-954-452-056-4_002</doi>
      <bibkey>aggarwal-aker-2019-identification</bibkey>
    </paper>
    <paper id="3">
      <title>Bilingual Low-Resource Neural Machine Translation with Round-Tripping : The Case of Persian-Spanish<fixed-case>P</fixed-case>ersian-<fixed-case>S</fixed-case>panish</title>
      <author><first>Benyamin</first><last>Ahmadnia</last></author>
      <author><first>Bonnie</first><last>Dorr</last></author>
      <pages>18–24</pages>
      <abstract>The quality of Neural Machine Translation (NMT), as a data-driven approach, massively depends on quantity, quality, and relevance of the training dataset. Such approaches have achieved promising results for bilingually high-resource scenarios but are inadequate for low-resource conditions. This paper describes a round-trip training approach to bilingual low-resource NMT that takes advantage of monolingual datasets to address training data scarcity, thus augmenting translation quality. We conduct detailed experiments on <a href="https://en.wikipedia.org/wiki/Persian_language">Persian-Spanish</a> as a bilingually low-resource scenario. Experimental results demonstrate that this competitive approach outperforms the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="7f7b0b45">R19-1003</url>
      <doi>10.26615/978-954-452-056-4_003</doi>
      <bibkey>ahmadnia-dorr-2019-bilingual</bibkey>
    </paper>
    <paper id="11">
      <title>Diachronic Analysis of Entities by Exploiting Wikipedia Page revisions<fixed-case>W</fixed-case>ikipedia Page revisions</title>
      <author><first>Pierpaolo</first><last>Basile</last></author>
      <author><first>Annalina</first><last>Caputo</last></author>
      <author><first>Seamus</first><last>Lawless</last></author>
      <author><first>Giovanni</first><last>Semeraro</last></author>
      <pages>84–91</pages>
      <abstract>In the last few years, the increasing availability of large corpora spanning several time periods has opened new opportunities for the diachronic analysis of language. This type of analysis can bring to the light not only linguistic phenomena related to the shift of word meanings over time, but it can also be used to study the impact that societal and cultural trends have on this language change. This paper introduces a new resource for performing the diachronic analysis of named entities built upon Wikipedia page revisions. This resource enables the analysis over time of changes in the relations between entities (concepts), surface forms (words), and the contexts surrounding entities and surface forms, by analysing the whole history of Wikipedia internal links. We provide some useful use cases that prove the impact of this <a href="https://en.wikipedia.org/wiki/Resource">resource</a> on diachronic studies and delineate some possible future usage.</abstract>
      <url hash="9a1416ed">R19-1011</url>
      <doi>10.26615/978-954-452-056-4_011</doi>
      <bibkey>basile-etal-2019-diachronic</bibkey>
    </paper>
    <paper id="12">
      <title>Using a Lexical Semantic Network for the Ontology Building</title>
      <author><first>Nadia</first><last>Bebeshina-Clairet</last></author>
      <author><first>Sylvie</first><last>Despres</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <pages>92–101</pages>
      <abstract>Building multilingual ontologies is a hard task as <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontologies</a> are often data-rich resources. We introduce an approach which allows exploiting structured lexical semantic knowledge for the <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology building</a>. Given a multilingual lexical semantic (non ontological) resource and an <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology model</a>, it allows mining relevant semantic knowledge and make the ontology building and enhancement process faster.</abstract>
      <url hash="6666a491">R19-1012</url>
      <doi>10.26615/978-954-452-056-4_012</doi>
      <bibkey>bebeshina-clairet-etal-2019-using</bibkey>
    </paper>
    <paper id="16">
      <title>Evaluating the Consistency of Word Embeddings from <a href="https://en.wikipedia.org/wiki/Small_data">Small Data</a></title>
      <author><first>Jelke</first><last>Bloem</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <author><first>Aurélie</first><last>Herbelot</last></author>
      <pages>132–141</pages>
      <abstract>In this work, we address the evaluation of distributional semantic models trained on smaller, domain-specific texts, specifically, philosophical text. Specifically, we inspect the behaviour of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> using a pre-trained background space in <a href="https://en.wikipedia.org/wiki/Machine_learning">learning</a>. We propose a <a href="https://en.wikipedia.org/wiki/Measurement">measure</a> of <a href="https://en.wikipedia.org/wiki/Consistency">consistency</a> which can be used as an evaluation metric when no in-domain gold-standard data is available. This <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">measure</a> simply computes the ability of a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to learn similar embeddings from different parts of some <a href="https://en.wikipedia.org/wiki/Homogeneity_(statistics)">homogeneous data</a>. We show that in spite of being a simple evaluation, consistency actually depends on various combinations of factors, including the nature of the data itself, the model used to train the <a href="https://en.wikipedia.org/wiki/Semantic_space">semantic space</a>, and the frequency of the learnt terms, both in the background space and in the in-domain data of interest.</abstract>
      <url hash="a9cbe167">R19-1016</url>
      <doi>10.26615/978-954-452-056-4_016</doi>
      <bibkey>bloem-etal-2019-evaluating</bibkey>
    </paper>
    <paper id="18">
      <title>Learning Sentence Embeddings for Coherence Modelling and Beyond</title>
      <author><first>Tanner</first><last>Bohn</last></author>
      <author><first>Yining</first><last>Hu</last></author>
      <author><first>Jinhang</first><last>Zhang</last></author>
      <author><first>Charles</first><last>Ling</last></author>
      <pages>151–160</pages>
      <abstract>We present a novel and effective technique for performing text coherence tasks while facilitating deeper insights into the data. Despite obtaining ever-increasing task performance, modern deep-learning approaches to NLP tasks often only provide users with the final network decision and no additional understanding of the data. In this work, we show that a new type of <a href="https://en.wikipedia.org/wiki/Sentence_embedding">sentence embedding</a> learned through self-supervision can be applied effectively to text coherence tasks while serving as a window through which deeper understanding of the data can be obtained. To produce these sentence embeddings, we train a <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network</a> to take individual sentences and predict their location in a document in the form of a distribution over locations. We demonstrate that these embeddings, combined with simple visual heuristics, can be used to achieve performance competitive with <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> on multiple text coherence tasks, outperforming more complex and specialized approaches. Additionally, we demonstrate that these embeddings can provide insights useful to writers for improving writing quality and informing document structuring, and assisting readers in summarizing and locating information.</abstract>
      <url hash="b9dba296">R19-1018</url>
      <doi>10.26615/978-954-452-056-4_018</doi>
      <bibkey>bohn-etal-2019-learning</bibkey>
    </paper>
    <paper id="21">
      <title>Classifying Author Intention for Writer Feedback in Related Work</title>
      <author><first>Arlene</first><last>Casey</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <author><first>Dorota</first><last>Glowacka</last></author>
      <pages>178–187</pages>
      <abstract>The ability to produce high-quality publishable material is critical to academic success but many Post-Graduate students struggle to learn to do so. While recent years have seen an increase in tools designed to provide feedback on aspects of writing, one aspect that has so far been neglected is the Related Work section of <a href="https://en.wikipedia.org/wiki/Academic_publishing">academic research papers</a>. To address this, we have trained a <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised classifier</a> on a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> of 94 Related Work sections and evaluated it against a manually annotated gold standard. The <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifier</a> uses novel features pertaining to citation types and <a href="https://en.wikipedia.org/wiki/Co-reference">co-reference</a>, along with patterns found from studying Related Works. We show that these novel features contribute to <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> performance with performance being favourable compared to other similar works that classify author intentions and consider <a href="https://en.wikipedia.org/wiki/Feedback">feedback</a> for <a href="https://en.wikipedia.org/wiki/Academic_writing">academic writing</a>.</abstract>
      <url hash="ce3c6934">R19-1021</url>
      <doi>10.26615/978-954-452-056-4_021</doi>
      <bibkey>casey-etal-2019-classifying</bibkey>
    </paper>
    <paper id="22">
      <title>Sparse Victory   A Large Scale Systematic Comparison of count-based and prediction-based vectorizers for text classification</title>
      <author><first>Rupak</first><last>Chakraborty</last></author>
      <author><first>Ashima</first><last>Elhence</last></author>
      <author><first>Kapil</first><last>Arora</last></author>
      <pages>188–197</pages>
      <abstract>In this paper we study the performance of several text vectorization algorithms on a diverse collection of 73 publicly available datasets. Traditional sparse vectorizers like Tf-Idf and Feature Hashing have been systematically compared with the latest state of the art neural word embeddings like Word2Vec, GloVe, FastText and character embeddings like ELMo, Flair. We have carried out an extensive analysis of the performance of these vectorizers across different dimensions like classification metrics (.i.e. precision, recall, accuracy), dataset-size, and imbalanced data (in terms of the distribution of the number of class labels). Our experiments reveal that the sparse vectorizers beat the neural word and character embedding models on 61 of the 73 datasets by an average margin of 3-5 % (in terms of macro f1 score) and this performance is consistent across the different dimensions of comparison.</abstract>
      <url hash="2e9efb5f">R19-1022</url>
      <doi>10.26615/978-954-452-056-4_022</doi>
      <bibkey>chakraborty-etal-2019-sparse</bibkey>
      <pwccode url="https://github.com/opennlp/Large-Scale-Text-Classification" additional="false">opennlp/Large-Scale-Text-Classification</pwccode>
    </paper>
    <paper id="24">
      <title>Personality-dependent Neural Text Summarization</title>
      <author><first>Pablo</first><last>Costa</last></author>
      <author><first>Ivandré</first><last>Paraboni</last></author>
      <pages>205–212</pages>
      <abstract>In Natural Language Generation systems, personalization strategies-i.e, the use of information about a target author to generate text that (more) closely resembles human-produced language-have long been applied to improve results. The present work addresses one such strategy-namely, the use of <a href="https://en.wikipedia.org/wiki/Big_Five_personality_traits">Big Five personality information</a> about the target author-applied to the case of abstractive text summarization using neural sequence-to-sequence models. Initial results suggest that having access to <a href="https://en.wikipedia.org/wiki/Personality_type">personality information</a> does lead to more accurate (or human-like) text summaries, and paves the way for more robust systems of this kind.</abstract>
      <url hash="07a9836c">R19-1024</url>
      <doi>10.26615/978-954-452-056-4_024</doi>
      <bibkey>costa-paraboni-2019-personality</bibkey>
    </paper>
    <paper id="29">
      <title>Detecting Toxicity in <a href="https://en.wikipedia.org/wiki/Article_(publishing)">News Articles</a> : Application to Bulgarian<fixed-case>B</fixed-case>ulgarian</title>
      <author><first>Yoan</first><last>Dinkov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>247–258</pages>
      <abstract>Online media aim for reaching ever bigger audience and for attracting ever longer attention span. This competition creates an environment that rewards sensational, fake, and toxic news. To help limit their spread and impact, we propose and develop a news toxicity detector that can recognize various types of toxic content. While previous research primarily focused on <a href="https://en.wikipedia.org/wiki/English_language">English</a>, here we target <a href="https://en.wikipedia.org/wiki/Bulgarian_language">Bulgarian</a>. We created a new dataset by crawling a website that for five years has been collecting Bulgarian news articles that were manually categorized into eight toxicity groups. Then we trained a multi-class classifier with nine categories : eight toxic and one non-toxic. We experimented with different representations based on ElMo, BERT, and XLM, as well as with a variety of domain-specific features. Due to the small size of our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, we created a separate <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> for each feature type, and we ultimately combined these <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> into a meta-classifier. The evaluation results show an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 59.0 % and a macro-F1 score of 39.7 %, which represent sizable improvements over the majority-class baseline (Acc=30.3 %, macro-F1=5.2 %).</abstract>
      <url hash="a06b5392">R19-1029</url>
      <doi>10.26615/978-954-452-056-4_029</doi>
      <bibkey>dinkov-etal-2019-detecting</bibkey>
      <pwccode url="https://github.com/yoandinkov/ranlp-2019" additional="false">yoandinkov/ranlp-2019</pwccode>
    </paper>
    <paper id="30">
      <title>De-Identification of Emails : Pseudonymizing Privacy-Sensitive Data in a German Email Corpus<fixed-case>G</fixed-case>erman Email Corpus</title>
      <author><first>Elisabeth</first><last>Eder</last></author>
      <author><first>Ulrike</first><last>Krieg-Holz</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <pages>259–269</pages>
      <abstract>We deal with the <a href="https://en.wikipedia.org/wiki/Pseudonymization">pseudonymization</a> of those stretches of text in emails that might allow to identify real individual persons. This <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> is decomposed into two steps. First, <a href="https://en.wikipedia.org/wiki/Legal_person">named entities</a> carrying privacy-sensitive information (e.g., names of persons, locations, phone numbers or dates) are identified, and, second, these <a href="https://en.wikipedia.org/wiki/Legal_person">privacy-bearing entities</a> are replaced by <a href="https://en.wikipedia.org/wiki/Legal_person">synthetically generated surrogates</a> (e.g., a person originally named ‘John Doe’ is renamed as ‘Bill Powers’). We describe a <a href="https://en.wikipedia.org/wiki/Systems_architecture">system architecture</a> for surrogate generation and evaluate our approach on CodeAlltag, a German email corpus.</abstract>
      <url hash="43043d7d">R19-1030</url>
      <doi>10.26615/978-954-452-056-4_030</doi>
      <bibkey>eder-etal-2019-de</bibkey>
    </paper>
    <paper id="31">
      <title>Lexical Quantile-Based Text Complexity Measure</title>
      <author><first>Maksim</first><last>Eremeev</last></author>
      <author><first>Konstantin</first><last>Vorontsov</last></author>
      <pages>270–275</pages>
      <abstract>This paper introduces a new <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> to estimating the text document complexity. Common readability indices are based on average length of sentences and words. In contrast to these methods, we propose to count the number of rare words occurring abnormally often in the document. We use the reference corpus of texts and the quantile approach in order to determine what words are rare, and what frequencies are abnormal. We construct a general text complexity model, which can be adjusted for the specific <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, and introduce two special <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>. The experimental design is based on a set of thematically similar pairs of Wikipedia articles, labeled using <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a>. The experiments demonstrate the competitiveness of the proposed <a href="https://en.wikipedia.org/wiki/Methodology">approach</a>.</abstract>
      <url hash="553b414f">R19-1031</url>
      <doi>10.26615/978-954-452-056-4_031</doi>
      <bibkey>eremeev-vorontsov-2019-lexical</bibkey>
    </paper>
    <paper id="32">
      <title>Demo Application for LETO : Learning Engine Through Ontologies<fixed-case>LETO</fixed-case>: Learning Engine Through Ontologies</title>
      <author><first>Suilan</first><last>Estevez-Velarde</last></author>
      <author><first>Andrés</first><last>Montoyo</last></author>
      <author><first>Yudivian</first><last>Almeida-Cruz</last></author>
      <author><first>Yoan</first><last>Gutiérrez</last></author>
      <author><first>Alejandro</first><last>Piad-Morffis</last></author>
      <author><first>Rafael</first><last>Muñoz</last></author>
      <pages>276–284</pages>
      <abstract>The massive amount of multi-formatted information available on the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">Web</a> necessitates the design of <a href="https://en.wikipedia.org/wiki/Software_system">software systems</a> that leverage this information to obtain knowledge that is valid and useful. The main challenge is to discover relevant information and continuously update, enrich and integrate knowledge from various sources of structured and unstructured data. This paper presents the Learning Engine Through Ontologies(LETO) framework, an architecture for the continuous and incremental discovery of knowledge from multiple sources of unstructured and structured data. We justify the main design decision behind LETO’s architecture and evaluate the <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a>’s feasibility using the Internet Movie Data Base(IMDB) and <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> as a practical application.</abstract>
      <url hash="2aea80b4">R19-1032</url>
      <doi>10.26615/978-954-452-056-4_032</doi>
      <bibkey>estevez-velarde-etal-2019-demo</bibkey>
    </paper>
    <paper id="33">
      <title>Sentence Simplification for Semantic Role Labelling and Information Extraction</title>
      <author><first>Richard</first><last>Evans</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <pages>285–294</pages>
      <abstract>In this paper, we report on the extrinsic evaluation of an automatic sentence simplification method with respect to two NLP tasks : semantic role labelling (SRL) and information extraction (IE). The paper begins with our observation of challenges in the intrinsic evaluation of sentence simplification systems, which motivates the use of extrinsic evaluation of these systems with respect to other NLP tasks. We describe the two NLP systems and the test data used in the extrinsic evaluation, and present arguments and evidence motivating the integration of a sentence simplification step as a means of improving the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of these <a href="https://en.wikipedia.org/wiki/System">systems</a>. Our evaluation reveals that their performance is improved by the simplification step : the SRL system is better able to assign semantic roles to the majority of the arguments of verbs and the IE system is better able to identify fillers for all IE template slots.</abstract>
      <url hash="8704ae51">R19-1033</url>
      <doi>10.26615/978-954-452-056-4_033</doi>
      <bibkey>evans-orasan-2019-sentence</bibkey>
    </paper>
    <paper id="34">
      <title>OlloBot-Towards A Text-Based Arabic Health Conversational Agent : Evaluation and Results<fixed-case>O</fixed-case>llo<fixed-case>B</fixed-case>ot - Towards A Text-Based <fixed-case>A</fixed-case>rabic Health Conversational Agent: Evaluation and Results</title>
      <author><first>Ahmed</first><last>Fadhil</last></author>
      <author><first>Ahmed</first><last>AbuRa’ed</last></author>
      <pages>295–303</pages>
      <abstract>We introduce OlloBot, an Arabic conversational agent that assists physicians and supports patients with the care process. It does n’t replace the physicians, instead provides health tracking and support and assists physicians with the <a href="https://en.wikipedia.org/wiki/Health_care">care delivery</a> through a conversation medium. The current model comprises <a href="https://en.wikipedia.org/wiki/Healthy_diet">healthy diet</a>, <a href="https://en.wikipedia.org/wiki/Physical_activity">physical activity</a>, <a href="https://en.wikipedia.org/wiki/Mental_health">mental health</a>, in addition to food logging. Not only OlloBot tracks user daily food, it also offers useful tips for healthier living. We will discuss the design, development and testing of OlloBot, and highlight the findings and limitations arose from the testing.</abstract>
      <url hash="4f533371">R19-1034</url>
      <doi>10.26615/978-954-452-056-4_034</doi>
      <bibkey>fadhil-aburaed-2019-ollobot</bibkey>
    </paper>
    <paper id="36">
      <title>Summarizing Legal Rulings : Comparative Experiments</title>
      <author><first>Diego</first><last>Feijo</last></author>
      <author><first>Viviane</first><last>Moreira</last></author>
      <pages>313–322</pages>
      <abstract>In the context of <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a>, texts in the legal domain have peculiarities related to their length and to their specialized vocabulary. Recent neural network-based approaches can achieve high-quality scores for <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a>. However, these approaches have been used mostly for generating very short abstracts for <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a>. Thus, their applicability to the legal domain remains an open issue. In this work, we experimented with ten extractive and four abstractive models in a real dataset of <a href="https://en.wikipedia.org/wiki/Judgment_(law)">legal rulings</a>. These models were compared with an extractive baseline based on <a href="https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making">heuristics</a> to select the most relevant parts of the text. Our results show that <a href="https://en.wikipedia.org/wiki/Abstraction">abstractive approaches</a> significantly outperform extractive methods in terms of ROUGE scores.</abstract>
      <url hash="fe363e37">R19-1036</url>
      <doi>10.26615/978-954-452-056-4_036</doi>
      <bibkey>feijo-moreira-2019-summarizing</bibkey>
    </paper>
    <paper id="39">
      <title>Comparing Automated Methods to Detect Explicit Content in Song Lyrics</title>
      <author><first>Michael</first><last>Fell</last></author>
      <author><first>Elena</first><last>Cabrio</last></author>
      <author><first>Michele</first><last>Corazza</last></author>
      <author><first>Fabien</first><last>Gandon</last></author>
      <pages>338–344</pages>
      <abstract>The Parental Advisory Label (PAL) is a warning label that is placed on <a href="https://en.wikipedia.org/wiki/Sound_recording_and_reproduction">audio recordings</a> in recognition of profanity or inappropriate references, with the intention of alerting parents of material potentially unsuitable for children. Since 2015, digital providers   such as <a href="https://en.wikipedia.org/wiki/ITunes">iTunes</a>, <a href="https://en.wikipedia.org/wiki/Spotify">Spotify</a>, <a href="https://en.wikipedia.org/wiki/Amazon_Music">Amazon Music</a> and Deezer   also follow PAL guidelines and tag such tracks as explicit. Nowadays, such <a href="https://en.wikipedia.org/wiki/Labelling">labelling</a> is carried out mainly manually on voluntary basis, with the drawbacks of being time consuming and therefore costly, error prone and partly a subjective task. In this paper, we compare automated methods ranging from dictionary-based lookup to state-of-the-art <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a> to automatically detect explicit contents in English lyrics. We show that more complex <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> perform only slightly better on this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, and relying on a qualitative analysis of the data, we discuss the inherent hardness and subjectivity of the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>.</abstract>
      <url hash="5aff7ac8">R19-1039</url>
      <doi>10.26615/978-954-452-056-4_039</doi>
      <bibkey>fell-etal-2019-comparing</bibkey>
    </paper>
    <paper id="40">
      <title>Linguistic classification : dealing jointly with irrelevance and inconsistency</title>
      <author><first>Laura</first><last>Franzoi</last></author>
      <author><first>Andrea</first><last>Sgarro</last></author>
      <author><first>Anca</first><last>Dinu</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <pages>345–352</pages>
      <abstract>In this paper, we present new methods for <a href="https://en.wikipedia.org/wiki/Language_classification">language classification</a> which put to good use both syntax and fuzzy tools, and are capable of dealing with irrelevant linguistic features (i.e. features which should not contribute to the classification) and even inconsistent features (which do not make sense for specific languages). We introduce a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric distance</a>, based on the generalized Steinhaus transform, which allows one to deal jointly with irrelevance and inconsistency. To evaluate our methods, we test them on a syntactic data set, due to the linguist G. Longobardi and his school. We obtain <a href="https://en.wikipedia.org/wiki/Phylogenetic_tree">phylogenetic trees</a> which sometimes outperform the ones obtained by Atkinson and Gray.</abstract>
      <url hash="5026a46f">R19-1040</url>
      <doi>10.26615/978-954-452-056-4_040</doi>
      <bibkey>franzoi-etal-2019-linguistic</bibkey>
    </paper>
    <paper id="43">
      <title>Two Discourse Tree-Based Approaches to Indexing Answers</title>
      <author><first>Boris</first><last>Galitsky</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <pages>367–372</pages>
      <abstract>We explore anatomy of answers with respect to which text fragments from an answer are worth matching with a question and which should not be matched. We apply the <a href="https://en.wikipedia.org/wiki/Rhetorical_structure_theory">Rhetorical Structure Theory</a> to build a discourse tree of an answer and select elementary discourse units that are suitable for indexing. Manual rules for selection of these discourse units as well as automated classification based on web search engine mining are evaluated con-cerning improving search accuracy. We form two sets of question-answer pairs for FAQ and community QA search domains and use them for evaluation of the proposed indexing methodology, which delivers up to 16 percent improvement in search recall.</abstract>
      <url hash="c20ac7e9">R19-1043</url>
      <doi>10.26615/978-954-452-056-4_043</doi>
      <bibkey>galitsky-ilvovsky-2019-two</bibkey>
    </paper>
    <paper id="45">
      <title>On a <a href="https://en.wikipedia.org/wiki/Chatbot">Chatbot</a> Providing Virtual Dialogues</title>
      <author><first>Boris</first><last>Galitsky</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <author><first>Elizaveta</first><last>Goncharova</last></author>
      <pages>382–387</pages>
      <abstract>We present a <a href="https://en.wikipedia.org/wiki/Chatbot">chatbot</a> that delivers content in the form of virtual dialogues automatically produced from the plain texts that are extracted and selected from the documents. This virtual dialogue content is provided in the form of answers derived from the found and selected documents split into fragments, and questions that are automatically generated for these answers based on the initial text.</abstract>
      <url hash="45b7af73">R19-1045</url>
      <doi>10.26615/978-954-452-056-4_045</doi>
      <bibkey>galitsky-etal-2019-chatbot</bibkey>
    </paper>
    <paper id="46">
      <title>Assessing socioeconomic status of Twitter users : A survey<fixed-case>T</fixed-case>witter users: A survey</title>
      <author><first>Dhouha</first><last>Ghazouani</last></author>
      <author><first>Luigi</first><last>Lancieri</last></author>
      <author><first>Habib</first><last>Ounelli</last></author>
      <author><first>Chaker</first><last>Jebari</last></author>
      <pages>388–398</pages>
      <abstract>Every day, the emotion and opinion of different people across the world are reflected in the form of short messages using <a href="https://en.wikipedia.org/wiki/Microblogging">microblogging platforms</a>. Despite the existence of enormous potential introduced by this data source, the <a href="https://en.wikipedia.org/wiki/Twitter_community">Twitter community</a> is still ambiguous and is not fully explored yet. While there are a huge number of studies examining the possibilities of inferring gender and age, there exist hardly researches on socioeconomic status (SES) inference of Twitter users. As <a href="https://en.wikipedia.org/wiki/Socioeconomic_status">socioeconomic status</a> is essential to treating diverse questions linked to <a href="https://en.wikipedia.org/wiki/Human_behavior">human behavior</a> in several fields (sociology, <a href="https://en.wikipedia.org/wiki/Demography">demography</a>, <a href="https://en.wikipedia.org/wiki/Public_health">public health</a>, etc.), we conducted a comprehensive literature review of SES studies, <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference methods</a>, and <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>. With reference to the research on literature’s results, we came to outline the most critical challenges for researchers. To the best of our knowledge, this paper is the first review that introduces the different aspects of SES inference. Indeed, this article provides the benefits for practitioners who aim to process and explore Twitter SES inference.</abstract>
      <url hash="607725ab">R19-1046</url>
      <doi>10.26615/978-954-452-056-4_046</doi>
      <bibkey>ghazouani-etal-2019-assessing</bibkey>
    </paper>
    <paper id="47">
      <title>Divide and Extract   Disentangling Clause Splitting and Proposition Extraction</title>
      <author><first>Darina</first><last>Gold</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>399–408</pages>
      <abstract>Proposition extraction from sentences is an important task for information extraction systems Evaluation of such <a href="https://en.wikipedia.org/wiki/System">systems</a> usually conflates two aspects : splitting complex sentences into clauses and the extraction of propositions. It is thus difficult to independently determine the quality of the proposition extraction step. We create a manually annotated proposition dataset from sentences taken from restaurant reviews that distinguishes between clauses that need to be split and those that do not. The resulting proposition evaluation dataset allows us to independently compare the performance of proposition extraction systems on simple and complex clauses. Although performance drastically drops on more complex sentences, we show that the same <a href="https://en.wikipedia.org/wiki/System">systems</a> perform best on both simple and complex clauses. Furthermore, we show that specific kinds of <a href="https://en.wikipedia.org/wiki/Dependent_clause">subordinate clauses</a> pose difficulties to most systems.</abstract>
      <url hash="a6559902">R19-1047</url>
      <doi>10.26615/978-954-452-056-4_047</doi>
      <bibkey>gold-zesch-2019-divide</bibkey>
    </paper>
    <paper id="49">
      <title>Automatic Question Answering for Medical MCQs : Can It go Further than Information Retrieval?<fixed-case>MCQ</fixed-case>s: Can It go Further than Information Retrieval?</title>
      <author><first>Le An</first><last>Ha</last></author>
      <author><first>Victoria</first><last>Yaneva</last></author>
      <pages>418–422</pages>
      <abstract>We present a novel approach to automatic question answering that does not depend on the performance of an information retrieval (IR) system and does not require that the training data come from the same source as the questions. We evaluate the <a href="https://en.wikipedia.org/wiki/System">system</a> performance on a challenging set of university-level medical science multiple-choice questions. Best performance is achieved when combining a <a href="https://en.wikipedia.org/wiki/Neural_circuit">neural approach</a> with an <a href="https://en.wikipedia.org/wiki/Information_theory">IR approach</a>, both of which work independently. Unlike previous approaches, the <a href="https://en.wikipedia.org/wiki/System">system</a> achieves statistically significant improvement over the random guess baseline even for questions that are labeled as challenging based on the performance of baseline solvers.</abstract>
      <url hash="3f6f31c4">R19-1049</url>
      <doi>10.26615/978-954-452-056-4_049</doi>
      <bibkey>ha-yaneva-2019-automatic</bibkey>
    </paper>
    <paper id="52">
      <title>Investigating Terminology Translation in Statistical and Neural Machine Translation : A Case Study on English-to-Hindi and Hindi-to-English<fixed-case>E</fixed-case>nglish-to-<fixed-case>H</fixed-case>indi and <fixed-case>H</fixed-case>indi-to-<fixed-case>E</fixed-case>nglish</title>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Md</first><last>Hasanuzzaman</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>437–446</pages>
      <abstract>Terminology translation plays a critical role in domain-specific machine translation (MT). In this paper, we conduct a comparative qualitative evaluation on terminology translation in phrase-based statistical MT (PB-SMT) and neural MT (NMT) in two translation directions : English-to-Hindi and Hindi-to-English. For this, we select a test set from a legal domain corpus and create a gold standard for evaluating terminology translation in MT. We also propose an error typology taking the terminology translation errors into consideration. We evaluate the MT systems’ performance on terminology translation, and demonstrate our findings, unraveling strengths, weaknesses, and similarities of PB-SMT and NMT in the area of term translation.</abstract>
      <url hash="22b4276a">R19-1052</url>
      <doi>10.26615/978-954-452-056-4_052</doi>
      <bibkey>haque-etal-2019-investigating</bibkey>
    </paper>
    <paper id="53">
      <title>Beyond English-Only Reading Comprehension : Experiments in Zero-shot Multilingual Transfer for Bulgarian<fixed-case>E</fixed-case>nglish-Only Reading Comprehension: Experiments in Zero-shot Multilingual Transfer for <fixed-case>B</fixed-case>ulgarian</title>
      <author><first>Momchil</first><last>Hardalov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>447–459</pages>
      <abstract>Recently, reading comprehension models achieved near-human performance on large-scale datasets such as SQuAD, CoQA, MS Macro, RACE, etc. This is largely due to the release of pre-trained contextualized representations such as <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> and ELMo, which can be fine-tuned for the target task. Despite those advances and the creation of more challenging datasets, most of the work is still done for <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Here, we study the effectiveness of multilingual BERT fine-tuned on large-scale English datasets for <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> (e.g., for RACE), and we apply it to Bulgarian multiple-choice reading comprehension. We propose a new dataset containing 2,221 questions from <a href="https://en.wikipedia.org/wiki/Matriculation_examination">matriculation exams</a> for twelfth grade in various subjects history, biology, geography and philosophy, and 412 additional questions from online quizzes in history. While the quiz authors gave no relevant context, we incorporate knowledge from <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, retrieving documents matching the combination of question + each answer option. Moreover, we experiment with different indexing and pre-training strategies. The evaluation results show accuracy of 42.23 %, which is well above the <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline</a> of 24.89 %.</abstract>
      <url hash="21539b59">R19-1053</url>
      <doi>10.26615/978-954-452-056-4_053</doi>
      <bibkey>hardalov-etal-2019-beyond</bibkey>
      <pwccode url="https://github.com/mhardalov/bg-reason-BERT" additional="false">mhardalov/bg-reason-BERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bulgarian-reading-comprehension-dataset">Bulgarian Reading Comprehension Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
    </paper>
    <paper id="56">
      <title>Emoji Powered Capsule Network to Detect Type and Target of Offensive Posts in <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a></title>
      <author><first>Hansi</first><last>Hettiarachchi</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <pages>474–480</pages>
      <abstract>This paper describes a novel research approach to detect type and target of offensive posts in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> using a capsule network. The input to the <a href="https://en.wikipedia.org/wiki/Computer_network">network</a> was <a href="https://en.wikipedia.org/wiki/Character_encoding">character embeddings</a> combined with <a href="https://en.wikipedia.org/wiki/Emoji">emoji embeddings</a>. The approach was evaluated on all three subtasks in Task 6-SemEval 2019 : OffensEval : Identifying and Categorizing Offensive Language in Social Media. The evaluation also showed that even though the capsule networks have not been used commonly in natural language processing tasks, they can outperform existing state of the art solutions for offensive language detection in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>.</abstract>
      <url hash="3f0f15b8">R19-1056</url>
      <doi>10.26615/978-954-452-056-4_056</doi>
      <bibkey>hettiarachchi-ranasinghe-2019-emoji</bibkey>
    </paper>
    <paper id="63">
      <title>Using Syntax to Resolve NPE in English<fixed-case>NPE</fixed-case> in <fixed-case>E</fixed-case>nglish</title>
      <author><first>Payal</first><last>Khullar</last></author>
      <author><first>Allen</first><last>Antony</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>534–540</pages>
      <abstract>This paper describes a novel, syntax-based system for automatic detection and resolution of Noun Phrase Ellipsis (NPE) in <a href="https://en.wikipedia.org/wiki/English_language">English</a>. The <a href="https://en.wikipedia.org/wiki/System">system</a> takes in free input English text, detects the site of nominal elision, and if present, selects potential antecedent candidates. The rules are built using the syntactic information on <a href="https://en.wikipedia.org/wiki/Ellipsis_(linguistics)">ellipsis</a> and its antecedent discussed in previous theoretical linguistics literature on NPE. Additionally, we prepare a curated dataset of 337 sentences from well-known, reliable sources, containing positive and negative samples of <a href="https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values">NPE</a>. We split this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> into two parts, and use one part to refine our rules and the other to test the performance of our final <a href="https://en.wikipedia.org/wiki/System">system</a>. We get an <a href="https://en.wikipedia.org/wiki/Fluorescence_in_situ_hybridization">F1-score</a> of 76.47 % for <a href="https://en.wikipedia.org/wiki/Fluorescence_in_situ_hybridization">detection</a> and 70.27 % for <a href="https://en.wikipedia.org/wiki/Fluorescence_in_situ_hybridization">NPE resolution</a> on the testset. To the best of our knowledge, ours is the first <a href="https://en.wikipedia.org/wiki/System">system</a> that detects and resolves <a href="https://en.wikipedia.org/wiki/Non-player_character">NPE</a> in <a href="https://en.wikipedia.org/wiki/English_language">English</a>. The curated dataset used for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, albeit small, covers a wide variety of NPE cases and will be made public for future work.</abstract>
      <url hash="8894d46e">R19-1063</url>
      <doi>10.26615/978-954-452-056-4_063</doi>
      <bibkey>khullar-etal-2019-using</bibkey>
    </paper>
    <paper id="64">
      <title>Is Similarity Visually Grounded? Computational Model of Similarity for the Estonian language<fixed-case>E</fixed-case>stonian language</title>
      <author><first>Claudia</first><last>Kittask</last></author>
      <author><first>Eduard</first><last>Barbu</last></author>
      <pages>541–549</pages>
      <abstract>Researchers in <a href="https://en.wikipedia.org/wiki/Computational_linguistics">Computational Linguistics</a> build <a href="https://en.wikipedia.org/wiki/Conceptual_model">models of similarity</a> and test them against <a href="https://en.wikipedia.org/wiki/Judgement">human judgments</a>. Although there are many empirical studies of the computational models of similarity for the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>, the similarity for other languages is less explored. In this study we are chiefly interested in two aspects. In the first place we want to know how much of the human similarity is grounded in the <a href="https://en.wikipedia.org/wiki/Visual_perception">visual perception</a>. To answer this question two neural computer vision models are used and their correlation with the human derived similarity scores is computed. In the second place we investigate if <a href="https://en.wikipedia.org/wiki/Language">language</a> influences the similarity computation. To this purpose diverse <a href="https://en.wikipedia.org/wiki/Computational_model">computational models</a> trained on Estonian resources are evaluated against <a href="https://en.wikipedia.org/wiki/Judgement">human judgments</a></abstract>
      <url hash="89cbffa5">R19-1064</url>
      <doi>10.26615/978-954-452-056-4_064</doi>
      <bibkey>kittask-barbu-2019-similarity</bibkey>
    </paper>
    <paper id="65">
      <title>Language-Agnostic Twitter-Bot Detection<fixed-case>T</fixed-case>witter-Bot Detection</title>
      <author><first>Jürgen</first><last>Knauth</last></author>
      <pages>550–558</pages>
      <abstract>In this paper we address the problem of detecting Twitter bots. We analyze a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 8385 <a href="https://en.wikipedia.org/wiki/Twitter">Twitter accounts</a> and their <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a> consisting of both humans and different kinds of <a href="https://en.wikipedia.org/wiki/Internet_bot">bots</a>. We use this <a href="https://en.wikipedia.org/wiki/Data">data</a> to train <a href="https://en.wikipedia.org/wiki/Statistical_classification">machine learning classifiers</a> that distinguish between real and bot accounts. We identify <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> that are easy to extract while still providing good results. We analyze different feature groups based on account specific, tweet specific and behavioral specific features and measure their performance compared to other state of the art bot detection methods. For easy future portability of our work we focus on language-agnostic features. With <a href="https://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a>, the best performing <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a>, we achieve an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 0.988 and an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">AUC</a> of 0.995. As the creation of good training data in <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> is often difficult-especially in the domain of Twitter bot detection-we additionally analyze to what extent smaller amounts of training data lead to useful results by reviewing cross-validated learning curves. Our results indicate that using few but expressive features already has a good practical benefit for bot detection, especially if only a small amount of <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data</a> is available.</abstract>
      <url hash="59f6fcbc">R19-1065</url>
      <doi>10.26615/978-954-452-056-4_065</doi>
      <bibkey>knauth-2019-language</bibkey>
    </paper>
    <paper id="70">
      <title>Question Similarity in Community Question Answering : A Systematic Exploration of Preprocessing Methods and Models</title>
      <author><first>Florian</first><last>Kunneman</last></author>
      <author><first>Thiago Castro</first><last>Ferreira</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <author><first>Antal</first><last>van den Bosch</last></author>
      <pages>593–601</pages>
      <abstract>Community Question Answering forums are popular among Internet users, and a basic problem they encounter is trying to find out if their question has already been posed before. To address this issue, NLP researchers have developed methods to automatically detect question-similarity, which was one of the shared tasks in <a href="https://en.wikipedia.org/wiki/SemEval">SemEval</a>. The best performing systems for this task made use of Syntactic Tree Kernels or the SoftCosine metric. However, it remains unclear why these methods seem to work, whether their performance can be improved by better preprocessing methods and what kinds of errors they (and other methods) make. In this paper, we therefore systematically combine and compare these two approaches with the more traditional BM25 and translation-based models. Moreover, we analyze the impact of preprocessing steps (lowercasing, suppression of punctuation and stop words removal) and word meaning similarity based on different distributions (word translation probability, Word2Vec, fastText and ELMo) on the performance of the task. We conduct an error analysis to gain insight into the differences in performance between the system set-ups. The implementation is made publicly available from https://github.com/fkunneman/DiscoSumo/tree/master/ranlp.</abstract>
      <url hash="85b47cba">R19-1070</url>
      <doi>10.26615/978-954-452-056-4_070</doi>
      <bibkey>kunneman-etal-2019-question</bibkey>
      <pwccode url="https://github.com/fkunneman/DiscoSumo" additional="false">fkunneman/DiscoSumo</pwccode>
    </paper>
    <paper id="72">
      <title>Resolving Pronouns for a Resource-Poor Language, <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a> Using Resource-Rich Language, Tamil.<fixed-case>M</fixed-case>alayalam Using Resource-Rich Language, <fixed-case>T</fixed-case>amil.</title>
      <author><first>Sobha</first><last>Lalitha Devi</last></author>
      <pages>611–618</pages>
      <abstract>In this paper we give in detail how a resource rich language can be used for resolving <a href="https://en.wikipedia.org/wiki/Pronoun">pronouns</a> for a less resource language. The source language, which is resource rich language in this study, is <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a> and the resource poor language is <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a>, both belonging to the same language family, <a href="https://en.wikipedia.org/wiki/Dravidian_languages">Dravidian</a>. The Pronominal resolution developed for <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a> uses CRFs. Our approach is to leverage the Tamil language model to test Malayalam data and the processing required for Malayalam data is detailed. The similarity at the <a href="https://en.wikipedia.org/wiki/Syntax">syntactic level</a> between the languages is exploited in identifying the <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> for developing the Tamil language model. The <a href="https://en.wikipedia.org/wiki/Word_form">word form</a> or the <a href="https://en.wikipedia.org/wiki/Lexical_item">lexical item</a> is not considered as a <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">feature</a> for training the CRFs. Evaluation on Malayalam Wikipedia data shows that our approach is correct and the results, though not as good as <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a>, but comparable.</abstract>
      <url hash="a192769d">R19-1072</url>
      <doi>10.26615/978-954-452-056-4_072</doi>
      <bibkey>lalitha-devi-2019-resolving</bibkey>
    </paper>
    <paper id="73">
      <title>Semantic Role Labeling with Pretrained Language Models for Known and Unknown Predicates</title>
      <author><first>Daniil</first><last>Larionov</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Elena</first><last>Chistova</last></author>
      <author><first>Ivan</first><last>Smirnov</last></author>
      <pages>619–628</pages>
      <abstract>We build the first full <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a> for semantic role labelling of Russian texts. The <a href="https://en.wikipedia.org/wiki/Pipeline_transport">pipeline</a> implements predicate identification, argument extraction, argument classification (labeling), and global scoring via <a href="https://en.wikipedia.org/wiki/Integer_linear_programming">integer linear programming</a>. We train supervised neural network models for argument classification using Russian semantically annotated corpus   FrameBank. However, we note that this resource provides <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> only to a very limited set of predicates. We combat the problem of annotation scarcity by introducing two models that rely on different sets of <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> : one for known predicates that are present in the training set and one for unknown predicates that are not. We show that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for unknown predicates can alleviate the lack of <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> by using pretrained embeddings. We perform experiments with various types of embeddings including the ones generated by deep pretrained language models : word2vec, FastText, ELMo, BERT, and show that embeddings generated by deep pretrained language models are superior to classical shallow embeddings for argument classification of both known and unknown predicates.</abstract>
      <url hash="ce2927a0">R19-1073</url>
      <doi>10.26615/978-954-452-056-4_073</doi>
      <bibkey>larionov-etal-2019-semantic</bibkey>
    </paper>
    <paper id="76">
      <title>The Impact of Semantic Linguistic Features in <a href="https://en.wikipedia.org/wiki/Relation_extraction">Relation Extraction</a> : A Logical Relational Learning Approach</title>
      <author><first>Rinaldo</first><last>Lima</last></author>
      <author><first>Bernard</first><last>Espinasse</last></author>
      <author><first>Frederico</first><last>Freitas</last></author>
      <pages>648–654</pages>
      <abstract>Relation Extraction (RE) consists in detecting and classifying semantic relations between entities in a sentence. The vast majority of the state-of-the-art RE systems relies on morphosyntactic features and supervised machine learning algorithms. This paper tries to answer important questions concerning both the impact of semantic based features, and the integration of external linguistic knowledge resources on RE performance. For that, a RE system based on a logical and relational learning algorithm was used and evaluated on three reference datasets from two distinct domains. The yielded results confirm that the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> induced using the proposed richer feature set outperformed the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> built with morphosyntactic features in average 4 % (F1-measure).</abstract>
      <url hash="962d0616">R19-1076</url>
      <doi>10.26615/978-954-452-056-4_076</doi>
      <bibkey>lima-etal-2019-impact</bibkey>
    </paper>
    <paper id="77">
      <title>Detecting Anorexia in Spanish Tweets<fixed-case>S</fixed-case>panish Tweets</title>
      <author><first>Pilar</first><last>López Úbeda</last></author>
      <author><first>Flor Miriam</first><last>Plaza del Arco</last></author>
      <author><first>Manuel Carlos</first><last>Díaz Galiano</last></author>
      <author><first>L. Alfonso</first><last>Urena Lopez</last></author>
      <author><first>Maite</first><last>Martin</last></author>
      <pages>655–663</pages>
      <abstract>Mental health is one of the main concerns of today’s society. Early detection of symptoms can greatly help people with <a href="https://en.wikipedia.org/wiki/Mental_disorder">mental disorders</a>. People are using <a href="https://en.wikipedia.org/wiki/List_of_social_networking_websites">social networks</a> more and more to express emotions, <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiments</a> and <a href="https://en.wikipedia.org/wiki/Mental_state">mental states</a>. Thus, the treatment of this information using NLP technologies can be applied to the automatic detection of mental problems such as <a href="https://en.wikipedia.org/wiki/Eating_disorder">eating disorders</a>. However, the first step to solving the problem should be to provide a corpus in order to evaluate our <a href="https://en.wikipedia.org/wiki/System">systems</a>. In this paper, we specifically focus on detecting <a href="https://en.wikipedia.org/wiki/Anorexia_(symptom)">anorexia messages</a> on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>. Firstly, we have generated a new corpus of tweets extracted from different accounts including anorexia and non-anorexia messages in <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>. The <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> is called SAD : Spanish Anorexia Detection corpus. In order to validate the effectiveness of the SAD corpus, we also propose several machine learning approaches for automatically detecting anorexia symptoms in the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>. The good results obtained show that the application of textual classification methods is a promising option for developing this kind of system demonstrating that these tools could be used by professionals to help in the early detection of mental problems.</abstract>
      <url hash="48173a51">R19-1077</url>
      <doi>10.26615/978-954-452-056-4_077</doi>
      <bibkey>lopez-ubeda-etal-2019-detecting</bibkey>
    </paper>
    <paper id="79">
      <title>v-trel : Vocabulary Trainer for Tracing Word Relations-An Implicit Crowdsourcing Approach</title>
      <author><first>Verena</first><last>Lyding</last></author>
      <author><first>Christos</first><last>Rodosthenous</last></author>
      <author><first>Federico</first><last>Sangati</last></author>
      <author><first>Umair</first><last>ul Hassan</last></author>
      <author><first>Lionel</first><last>Nicolas</last></author>
      <author><first>Alexander</first><last>König</last></author>
      <author><first>Jolita</first><last>Horbacauskiene</last></author>
      <author><first>Anisia</first><last>Katinskaia</last></author>
      <pages>674–683</pages>
      <abstract>In this paper, we present our work on developing a vocabulary trainer that uses exercises generated from language resources such as <a href="https://en.wikipedia.org/wiki/ConceptNet">ConceptNet</a> and crowdsources the responses of the learners to enrich the language resource. We performed an empirical evaluation of our approach with 60 non-native speakers over two days, which shows that new entries to expand Concept-Net can efficiently be gathered through vocabulary exercises on word relations. We also report on the feedback gathered from the users and an expert from <a href="https://en.wikipedia.org/wiki/Language_acquisition">language teaching</a>, and discuss the potential of the vocabulary trainer application from the user and language learner perspective. The feedback suggests that v-trel has educational potential, while in its current state some shortcomings could be identified.</abstract>
      <url hash="e619259f">R19-1079</url>
      <doi>10.26615/978-954-452-056-4_079</doi>
      <bibkey>lyding-etal-2019-v</bibkey>
    </paper>
    <paper id="80">
      <title>Jointly Learning Author and Annotated Character N-gram Embeddings : A Case Study in Literary Text</title>
      <author><first>Suraj</first><last>Maharjan</last></author>
      <author><first>Deepthi</first><last>Mave</last></author>
      <author><first>Prasha</first><last>Shrestha</last></author>
      <author><first>Manuel</first><last>Montes</last></author>
      <author><first>Fabio A.</first><last>González</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>684–692</pages>
      <abstract>An author’s way of presenting a story through his / her writing style has a great impact on whether the story will be liked by readers or not. In this paper, we learn <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representations</a> for authors of literary texts together with <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representations</a> for character n-grams annotated with their functional roles. We train a neural character n-gram based language model using an external corpus of literary texts and transfer learned representations for use in downstream tasks. We show that augmenting the knowledge from external works of authors produces results competitive with other style-based methods for book likability prediction, genre classification, and authorship attribution.</abstract>
      <url hash="949d78fa">R19-1080</url>
      <doi>10.26615/978-954-452-056-4_080</doi>
      <bibkey>maharjan-etal-2019-jointly</bibkey>
    </paper>
    <paper id="81">
      <title>Generating Challenge Datasets for Task-Oriented Conversational Agents through Self-Play</title>
      <author><first>Sourabh</first><last>Majumdar</last></author>
      <author><first>Serra Sinem</first><last>Tekiroglu</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <pages>693–702</pages>
      <abstract>End-to-end neural approaches are becoming increasingly common in conversational scenarios due to their promising performances when provided with sufficient amount of data. In this paper, we present a novel <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> to address the interpretability of neural approaches in such scenarios by creating challenge datasets using dialogue self-play over multiple tasks / intents. Dialogue self-play allows generating large amount of synthetic data ; by taking advantage of the complete control over the generation process, we show how neural approaches can be evaluated in terms of unseen dialogue patterns. We propose several out-of-pattern test cases each of which introduces a natural and unexpected user utterance phenomenon. As a proof of concept, we built a single and a multiple memory network, and show that these two architectures have diverse performances depending on the peculiar dialogue patterns.</abstract>
      <url hash="364417be">R19-1081</url>
      <doi>10.26615/978-954-452-056-4_081</doi>
      <bibkey>majumdar-etal-2019-generating</bibkey>
    </paper>
    <paper id="90">
      <title>Unsupervised Data Augmentation for Less-Resourced Languages with no Standardized Spelling</title>
      <author><first>Alice</first><last>Millour</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <pages>776–784</pages>
      <abstract>Building representative linguistic resources and NLP tools for non-standardized languages is challenging : when <a href="https://en.wikipedia.org/wiki/Spelling">spelling</a> is not determined by a norm, multiple written forms can be encountered for a given word, inducing a large proportion of out-of-vocabulary words. To embrace this diversity, we propose a <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> based on crowdsourced alternative spellings we use to extract rules applied to match OOV words with one of their spelling variants. This virtuous process enables the unsupervised augmentation of multi-variant lexicons without expert rule definition. We apply this multilingual methodology on <a href="https://en.wikipedia.org/wiki/Alsatian_dialect">Alsatian</a>, a <a href="https://en.wikipedia.org/wiki/Languages_of_France">French regional language</a> and provide an intrinsic evaluation of the correctness of the variants pairs, and an extrinsic evaluation on a downstream task. We show that in a low-resource scenario, 145 inital pairs can lead to the generation of 876 additional variant pairs, and a diminution of OOV words improving the <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a> performance by 1 to 4 %.</abstract>
      <url hash="4873d2ab">R19-1090</url>
      <doi>10.26615/978-954-452-056-4_090</doi>
      <bibkey>millour-fort-2019-unsupervised</bibkey>
    </paper>
    <paper id="91">
      <title>Neural Feature Extraction for Contextual Emotion Detection</title>
      <author><first>Elham</first><last>Mohammadi</last></author>
      <author><first>Hessam</first><last>Amini</last></author>
      <author><first>Leila</first><last>Kosseim</last></author>
      <pages>785–794</pages>
      <abstract>This paper describes a new approach for the task of contextual emotion detection. The approach is based on a neural feature extractor, composed of a <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network</a> with an <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a>, followed by a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a>, that can be neural or SVM-based. We evaluated the model with the dataset of the task 3 of SemEval 2019 (EmoContext), which includes short 3-turn conversations, tagged with 4 emotion classes. The best performing setup was achieved using ELMo word embeddings and POS tags as input, bidirectional GRU as hidden units, and an <a href="https://en.wikipedia.org/wiki/Symmetric_multiprocessing">SVM</a> as the final classifier. This configuration reached 69.93 % in terms of micro-average F1 score on the main 3 emotion classes, a score that outperformed the baseline system by 11.25 %.</abstract>
      <url hash="7c5ca82a">R19-1091</url>
      <doi>10.26615/978-954-452-056-4_091</doi>
      <bibkey>mohammadi-etal-2019-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/emocontext">EmoContext</pwcdataset>
    </paper>
    <paper id="93">
      <title>A Fast and Accurate Partially Deterministic Morphological Analysis</title>
      <author><first>Hajime</first><last>Morita</last></author>
      <author><first>Tomoya</first><last>Iwakura</last></author>
      <pages>804–809</pages>
      <abstract>This paper proposes a partially deterministic morphological analysis method for improved processing speed. Maximum matching is a fast deterministic method for <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological analysis</a>. However, the <a href="https://en.wikipedia.org/wiki/Methodology">method</a> tends to decrease performance due to lack of consideration of <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a>. In order to use <a href="https://en.wikipedia.org/wiki/Maximum_matching">maximum matching</a> safely, we propose the use of Context Independent Strings (CISs), which are strings that do not have <a href="https://en.wikipedia.org/wiki/Ambiguity">ambiguity</a> in terms of <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological analysis</a>. Our method first identifies CISs in a sentence using <a href="https://en.wikipedia.org/wiki/Maximum_matching">maximum matching</a> without <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a>, then analyzes the unprocessed part of the sentence using a bi-gram-based morphological analysis model. We evaluate the <a href="https://en.wikipedia.org/wiki/Methodology">method</a> on a Japanese morphological analysis task. The experimental results show a 30 % reduction of <a href="https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)">running time</a> while maintaining improved <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>.</abstract>
      <url hash="4c660793">R19-1093</url>
      <doi>10.26615/978-954-452-056-4_093</doi>
      <bibkey>morita-iwakura-2019-fast</bibkey>
    </paper>
    <paper id="94">
      <title>incom.py-A Toolbox for Calculating Linguistic Distances and Asymmetries between Related Languages</title>
      <author><first>Marius</first><last>Mosbach</last></author>
      <author><first>Irina</first><last>Stenger</last></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>810–818</pages>
      <abstract>Languages may be differently distant from each other and their <a href="https://en.wikipedia.org/wiki/Mutual_intelligibility">mutual intelligibility</a> may be asymmetric. In this paper we introduce incom.py, a toolbox for calculating <a href="https://en.wikipedia.org/wiki/Linguistic_distance">linguistic distances</a> and asymmetries between related languages. incom.py allows linguist experts to quickly and easily perform <a href="https://en.wikipedia.org/wiki/Statistics">statistical analyses</a> and compare those with experimental results. We demonstrate the efficacy of incom.py in an incomprehension experiment on two <a href="https://en.wikipedia.org/wiki/Slavic_languages">Slavic languages</a> : <a href="https://en.wikipedia.org/wiki/Bulgarian_language">Bulgarian</a> and <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a>. Using incom.py we were able to validate three methods to measure linguistic distances and asymmetries : <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein distance</a>, word adaptation surprisal, and <a href="https://en.wikipedia.org/wiki/Conditional_entropy">conditional entropy</a> as predictors of success in a reading intercomprehension experiment.</abstract>
      <url hash="45a8521a">R19-1094</url>
      <doi>10.26615/978-954-452-056-4_094</doi>
      <bibkey>mosbach-etal-2019-incom</bibkey>
    </paper>
    <paper id="95">
      <title>A Holistic Natural Language Generation Framework for the <a href="https://en.wikipedia.org/wiki/Semantic_Web">Semantic Web</a></title>
      <author><first>Axel-Cyrille</first><last>Ngonga Ngomo</last></author>
      <author><first>Diego</first><last>Moussallem</last></author>
      <author><first>Lorenz</first><last>Bühmann</last></author>
      <pages>819–828</pages>
      <abstract>With the ever-growing generation of data for the <a href="https://en.wikipedia.org/wiki/Semantic_Web">Semantic Web</a> comes an increasing demand for this <a href="https://en.wikipedia.org/wiki/Data">data</a> to be made available to non-semantic Web experts. One way of achieving this goal is to translate the languages of the <a href="https://en.wikipedia.org/wiki/Semantic_Web">Semantic Web</a> into <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>. We present LD2NL, a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> that allows verbalizing the three key languages of the <a href="https://en.wikipedia.org/wiki/Semantic_Web">Semantic Web</a>, i.e., <a href="https://en.wikipedia.org/wiki/Resource_Description_Framework">RDF</a>, <a href="https://en.wikipedia.org/wiki/Web_Ontology_Language">OWL</a>, and <a href="https://en.wikipedia.org/wiki/SPARQL">SPARQL</a>. Our <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> is based on a <a href="https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design">bottom-up approach</a> to <a href="https://en.wikipedia.org/wiki/Verbalization">verbalization</a>. We evaluated LD2NL in an open survey with 86 persons. Our results suggest that our <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> can generate verbalizations that are close to <a href="https://en.wikipedia.org/wiki/Natural_language">natural languages</a> and that can be easily understood by non-experts. Therewith, it enables non-domain experts to interpret Semantic Web data with more than 91 % of the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of domain experts.</abstract>
      <url hash="acb5bff8">R19-1095</url>
      <doi>10.26615/978-954-452-056-4_095</doi>
      <bibkey>ngonga-ngomo-etal-2019-holistic</bibkey>
    </paper>
    <paper id="98">
      <title>Large-Scale Hierarchical Alignment for Data-driven Text Rewriting</title>
      <author><first>Nikola I.</first><last>Nikolov</last></author>
      <author><first>Richard</first><last>Hahnloser</last></author>
      <pages>844–853</pages>
      <abstract>We propose a simple <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised method</a> for extracting pseudo-parallel monolingual sentence pairs from comparable corpora representative of two different text styles, such as <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> and <a href="https://en.wikipedia.org/wiki/Scientific_literature">scientific papers</a>. Our approach does not require a seed <a href="https://en.wikipedia.org/wiki/Parallel_corpus">parallel corpus</a>, but instead relies solely on hierarchical search over pre-trained embeddings of documents and sentences. We demonstrate the effectiveness of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> through automatic and extrinsic evaluation on text simplification from the normal to the Simple Wikipedia. We show that pseudo-parallel sentences extracted with our method not only supplement existing parallel data, but can even lead to competitive performance on their own.</abstract>
      <url hash="74dab92f">R19-1098</url>
      <doi>10.26615/978-954-452-056-4_098</doi>
      <bibkey>nikolov-hahnloser-2019-large</bibkey>
      <pwccode url="https://github.com/ninikolov/lha" additional="false">ninikolov/lha</pwccode>
    </paper>
    <paper id="99">
      <title>Dependency-Based Relative Positional Encoding for Transformer NMT<fixed-case>NMT</fixed-case></title>
      <author><first>Yutaro</first><last>Omote</last></author>
      <author><first>Akihiro</first><last>Tamura</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <pages>854–861</pages>
      <abstract>This paper proposes a new Transformer neural machine translation model that incorporates syntactic distances between two source words into the relative position representations of the self-attention mechanism. In particular, the proposed model encodes pair-wise relative depths on a source dependency tree, which are differences between the depths of the two source words, in the encoder’s self-attention. The experiments show that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves 0.5 point gain in BLEU on the Asian Scientific Paper Excerpt Corpus Japanese-to-English translation task.</abstract>
      <url hash="9365c240">R19-1099</url>
      <doi>10.26615/978-954-452-056-4_099</doi>
      <bibkey>omote-etal-2019-dependency</bibkey>
    </paper>
    <paper id="101">
      <title>Building a <a href="https://en.wikipedia.org/wiki/Morphological_analysis">Morphological Analyser</a> for Laz<fixed-case>L</fixed-case>az</title>
      <author><first>Esra</first><last>Onal</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>869–877</pages>
      <abstract>This study is an attempt to contribute to documentation and revitalization efforts of endangered Laz language, a member of South Caucasian language family mainly spoken on northeastern coastline of Turkey. It constitutes the first steps to create a general <a href="https://en.wikipedia.org/wiki/Computational_model">computational model</a> for word form recognition and production for Laz by building a rule-based morphological analyser using Helsinki Finite-State Toolkit (HFST). The evaluation results show that the <a href="https://en.wikipedia.org/wiki/Analyser">analyser</a> has a 64.9 % coverage over a corpus collected for this study with 111,365 tokens. We have also performed an error analysis on randomly selected 100 tokens from the corpus which are not covered by the analyser, and these results show that the errors mostly result from Turkish words in the corpus and missing stems in our lexicon.</abstract>
      <url hash="b0663f1f">R19-1101</url>
      <doi>10.26615/978-954-452-056-4_101</doi>
      <bibkey>onal-tyers-2019-building</bibkey>
    </paper>
    <paper id="103">
      <title>Quotation Detection and Classification with a Corpus-Agnostic Model</title>
      <author><first>Sean</first><last>Papay</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>888–894</pages>
      <abstract>The detection of quotations (i.e., reported speech, <a href="https://en.wikipedia.org/wiki/Thought">thought</a>, and writing) has established itself as an <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP analysis task</a>. However, state-of-the-art models have been developed on the basis of specific corpora and incorpo- rate a high degree of corpus-specific assumptions and knowledge, which leads to fragmentation. In the spirit of task-agnostic modeling, we present a corpus-agnostic neural model for quotation detection and evaluate it on three <a href="https://en.wikipedia.org/wiki/Text_corpus">corpora</a> that vary in language, text genre, and structural assumptions. The model (a) approaches the state-of-the-art on the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpora</a> when using established feature sets and (b) shows reasonable performance even when us- ing solely word forms, which makes it applicable for non-standard (i.e., historical) corpora.</abstract>
      <url hash="208966a6">R19-1103</url>
      <doi>10.26615/978-954-452-056-4_103</doi>
      <bibkey>papay-pado-2019-quotation</bibkey>
    </paper>
    <paper id="104">
      <title>Validation of Facts Against Textual Sources</title>
      <author><first>Vamsi Krishna</first><last>Pendyala</last></author>
      <author><first>Simran</first><last>Sinha</last></author>
      <author><first>Satya</first><last>Prakash</last></author>
      <author><first>Shriya</first><last>Reddy</last></author>
      <author><first>Anupam</first><last>Jamatia</last></author>
      <pages>895–903</pages>
      <abstract>In today’s digital world of information, a fact verification system to disprove assertions made in <a href="https://en.wikipedia.org/wiki/Public_speaking">speech</a>, <a href="https://en.wikipedia.org/wiki/Mass_media">print media</a> or <a href="https://en.wikipedia.org/wiki/Online_content">online content</a> is the need of the hour. We propose a <a href="https://en.wikipedia.org/wiki/System">system</a> which would verify a claim against a source and classify the claim to be true, false, out-of-context or an inappropriate claim with respect to the textual source provided to the <a href="https://en.wikipedia.org/wiki/System">system</a>. A true label is used if the claim is true, false if it is false, if the claim has no relation with the source then it is classified as out-of-context and if the claim can not be verified at all then it is classified as inappropriate. This would help us to verify a claim or a fact as well as know about the source or our knowledge base against which we are trying to verify our facts. We used a two-step approach to achieve our goal. At first, we retrieved evidence related to the claims from the textual source using the Term Frequency-Inverse Document Frequency(TF-IDF) vectors. Later we classified the claim-evidence pairs as true, false, inappropriate and out of context using a modified version of textual entailment module. Textual entailment module calculates the probability of each sentence supporting the claim, contradicting the claim or not providing any relevant information using Bi-LSTM network to assess the veracity of the claim. The <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of the best performing <a href="https://en.wikipedia.org/wiki/System">system</a> is 64.49 %</abstract>
      <url hash="0245c7d7">R19-1104</url>
      <doi>10.26615/978-954-452-056-4_104</doi>
      <bibkey>pendyala-etal-2019-validation</bibkey>
    </paper>
    <paper id="105">
      <title>A Neural Network Component for Knowledge-Based Semantic Representations of Text</title>
      <author><first>Alejandro</first><last>Piad-Morffis</last></author>
      <author><first>Rafael</first><last>Muñoz</last></author>
      <author><first>Yoan</first><last>Gutiérrez</last></author>
      <author><first>Yudivian</first><last>Almeida-Cruz</last></author>
      <author><first>Suilan</first><last>Estevez-Velarde</last></author>
      <author><first>Andrés</first><last>Montoyo</last></author>
      <pages>904–911</pages>
      <abstract>This paper presents Semantic Neural Networks (SNNs), a knowledge-aware component based on <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>. SNNs can be trained to encode explicit semantic knowledge from an arbitrary <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a>, and can subsequently be combined with other deep learning architectures. At prediction time, SNNs provide a semantic encoding extracted from the input data, which can be exploited by other neural network components to build extended representation models that can face alternative problems. The SNN architecture is defined in terms of the concepts and relations present in a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a>. Based on this <a href="https://en.wikipedia.org/wiki/Architecture">architecture</a>, a training procedure is developed. Finally, an experimental setup is presented to illustrate the behaviour and performance of a SNN for a specific NLP problem, in this case, <a href="https://en.wikipedia.org/wiki/Opinion_mining">opinion mining</a> for the classification of movie reviews.</abstract>
      <url hash="be4f8802">R19-1105</url>
      <doi>10.26615/978-954-452-056-4_105</doi>
      <bibkey>piad-morffis-etal-2019-neural</bibkey>
    </paper>
    <paper id="108">
      <title>Unsupervised dialogue intent detection via hierarchical topic model</title>
      <author><first>Artem</first><last>Popov</last></author>
      <author><first>Victor</first><last>Bulatov</last></author>
      <author><first>Darya</first><last>Polyudova</last></author>
      <author><first>Eugenia</first><last>Veselova</last></author>
      <pages>932–938</pages>
      <abstract>One of the challenges during a task-oriented chatbot development is the scarce availability of the <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">labeled training data</a>. The best way of getting one is to ask the assessors to tag each dialogue according to its intent. Unfortunately, performing <a href="https://en.wikipedia.org/wiki/Label">labeling</a> without any provisional collection structure is difficult since the very notion of the intent is ill-defined. In this paper, we propose a hierarchical multimodal regularized topic model to obtain a first approximation of the intent set. Our rationale for hierarchical models usage is their ability to take into account several degrees of the dialogues relevancy. We attempt to build a <a href="https://en.wikipedia.org/wiki/Scientific_modelling">model</a> that can distinguish between subject-based (e.g. medicine and transport topics) and action-based (e.g. filing of an application and tracking application status) similarities. In order to achieve this, we divide set of all <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> into several groups according to part-of-speech analysis. Various feature groups are treated differently on different <a href="https://en.wikipedia.org/wiki/Hierarchy">hierarchy levels</a>.</abstract>
      <url hash="9022d139">R19-1108</url>
      <doi>10.26615/978-954-452-056-4_108</doi>
      <bibkey>popov-etal-2019-unsupervised</bibkey>
    </paper>
    <paper id="111">
      <title>Are ambiguous conjunctions problematic for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>?</title>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Sheila</first><last>Castilho</last></author>
      <pages>959–966</pages>
      <abstract>The translation of ambiguous words still poses challenges for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. In this work, we carry out a systematic quantitative analysis regarding the ability of different <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a> to disambiguate the source language conjunctions but and and. We evaluate specialised test sets focused on the <a href="https://en.wikipedia.org/wiki/Translation_(geometry)">translation</a> of these two <a href="https://en.wikipedia.org/wiki/Logical_conjunction">conjunctions</a>. The test sets contain source languages that do not distinguish different variants of the given <a href="https://en.wikipedia.org/wiki/Logical_conjunction">conjunction</a>, whereas the target languages do. In total, we evaluate the <a href="https://en.wikipedia.org/wiki/Logical_conjunction">conjunction</a> but on 20 translation outputs, and the <a href="https://en.wikipedia.org/wiki/Logical_conjunction">conjunction</a> and on 10. All <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a> almost perfectly recognise one variant of the target conjunction, especially for the source conjunction but. The other target variant, however, represents a challenge for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a>, with <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> varying from 50 % to 95 % for but and from 20 % to 57 % for and. The major error for all <a href="https://en.wikipedia.org/wiki/System">systems</a> is replacing the correct target variant with the opposite one.</abstract>
      <url hash="441c540b">R19-1111</url>
      <doi>10.26615/978-954-452-056-4_111</doi>
      <bibkey>popovic-castilho-2019-ambiguous</bibkey>
    </paper>
    <paper id="114">
      <title>NE-Table : A Neural key-value table for Named Entities<fixed-case>NE</fixed-case>-Table: A Neural key-value table for Named Entities</title>
      <author><first>Janarthanan</first><last>Rajendran</last></author>
      <author><first>Jatin</first><last>Ganhotra</last></author>
      <author><first>Xiaoxiao</first><last>Guo</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Satinder</first><last>Singh</last></author>
      <author><first>Lazaros</first><last>Polymenakos</last></author>
      <pages>980–993</pages>
      <abstract>Many Natural Language Processing (NLP) tasks depend on using Named Entities (NEs) that are contained in texts and in external knowledge sources. While this is easy for humans, the present neural methods that rely on learned word embeddings may not perform well for these NLP tasks, especially in the presence of Out-Of-Vocabulary (OOV) or rare NEs. In this paper, we propose a solution for this problem, and present empirical evaluations on : a) a structured Question-Answering task, b) three related Goal-Oriented dialog tasks, and c) a Reading-Comprehension task, which show that the proposed method can be effective in dealing with both in-vocabulary and OOV NEs. We create extended versions of dialog bAbI tasks 1,2 and 4 and OOV versions of the CBT test set which are available at-https://github.com/IBM/ne-table-datasets/</abstract>
      <url hash="6442b059">R19-1114</url>
      <doi>10.26615/978-954-452-056-4_114</doi>
      <bibkey>rajendran-etal-2019-ne</bibkey>
      <pwccode url="https://github.com/IBM/ne-table-datasets" additional="false">IBM/ne-table-datasets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cbt">CBT</pwcdataset>
    </paper>
    <paper id="116">
      <title>Semantic Textual Similarity with Siamese Neural Networks<fixed-case>S</fixed-case>iamese Neural Networks</title>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>1004–1011</pages>
      <abstract>Calculating the Semantic Textual Similarity (STS) is an important research area in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> which plays a significant role in many applications such as <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, document summarisation, <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a> and <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a>. This paper evaluates Siamese recurrent architectures, a special type of <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>, which are used here to measure STS. Several variants of the <a href="https://en.wikipedia.org/wiki/Architecture">architecture</a> are compared with existing methods</abstract>
      <url hash="40527a78">R19-1116</url>
      <doi>10.26615/978-954-452-056-4_116</doi>
      <bibkey>ranasinghe-etal-2019-semantic</bibkey>
    </paper>
    <paper id="119">
      <title>Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems</title>
      <author><first>Mansour</first><last>Saffar Mehrjardi</last></author>
      <author><first>Amine</first><last>Trabelsi</last></author>
      <author><first>Osmar R.</first><last>Zaiane</last></author>
      <pages>1031–1040</pages>
      <abstract>Self-attentional models are a new paradigm for sequence modelling tasks which differ from common sequence modelling methods, such as recurrence-based and convolution-based sequence learning, in the way that their architecture is only based on the attention mechanism. Self-attentional models have been used in the creation of the state-of-the-art models in many NLP task such as <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>, but their usage has not been explored for the task of training end-to-end task-oriented dialogue generation systems yet. In this study, we apply these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on the DSTC2 dataset for training task-oriented chatbots. Our finding shows that self-attentional models can be exploited to create end-to-end task-oriented chatbots which not only achieve higher evaluation scores compared to recurrence-based models, but also do so more efficiently.</abstract>
      <url hash="fdfbdb79">R19-1119</url>
      <doi>10.26615/978-954-452-056-4_119</doi>
      <bibkey>saffar-mehrjardi-etal-2019-self</bibkey>
    </paper>
    <paper id="121">
      <title>Persistence pays off : Paying Attention to What the LSTM Gating Mechanism Persists<fixed-case>LSTM</fixed-case> Gating Mechanism Persists</title>
      <author><first>Giancarlo</first><last>Salton</last></author>
      <author><first>John</first><last>Kelleher</last></author>
      <pages>1052–1059</pages>
      <abstract>Recurrent Neural Network Language Models composed of LSTM units, especially those augmented with an external memory, have achieved state-of-the-art results in <a href="https://en.wikipedia.org/wiki/Language_model">Language Modeling</a>. However, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> still struggle to process long sequences which are more likely to contain long-distance dependencies because of information fading. In this paper we demonstrate an effective <a href="https://en.wikipedia.org/wiki/Mechanism_design">mechanism</a> for retrieving information in a memory augmented LSTM LM based on attending to information in memory in proportion to the number of timesteps the LSTM gating mechanism persisted the information.</abstract>
      <url hash="44a99322">R19-1121</url>
      <doi>10.26615/978-954-452-056-4_121</doi>
      <bibkey>salton-kelleher-2019-persistence</bibkey>
    </paper>
    <paper id="122">
      <title>Development and Evaluation of Three Named Entity Recognition Systems for Serbian-The Case of Personal Names<fixed-case>S</fixed-case>erbian - The Case of Personal Names</title>
      <author><first>Branislava</first><last>Šandrih</last></author>
      <author><first>Cvetana</first><last>Krstev</last></author>
      <author><first>Ranka</first><last>Stankovic</last></author>
      <pages>1060–1068</pages>
      <abstract>In this paper we present a rule- and lexicon-based system for the recognition of Named Entities (NE) in Serbian newspaper texts that was used to prepare a gold standard annotated with personal names. It was further used to prepare training sets for four different levels of annotation, which were further used to train two Named Entity Recognition (NER) systems : Stanford and <a href="https://en.wikipedia.org/wiki/SpaCy">spaCy</a>. All obtained models, together with a rule- and lexicon-based system were evaluated on two sample texts : a part of the gold standard and an independent newspaper text of approximately the same size. The results show that rule- and lexicon-based system outperforms trained <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> in all four scenarios (measured by F1), while Stanford models has the highest <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a>. All systems obtain best results in recognizing <a href="https://en.wikipedia.org/wiki/Personal_name">full names</a>, while the recognition of first names only is rather poor. The produced <a href="https://en.wikipedia.org/wiki/Physical_model">models</a> are incorporated into a Web platform NER&amp;Beyond that provides various NE-related functions.</abstract>
      <url hash="87989f6a">R19-1122</url>
      <doi>10.26615/978-954-452-056-4_122</doi>
      <bibkey>sandrih-etal-2019-development</bibkey>
    </paper>
    <paper id="123">
      <title>Moral Stance Recognition and Polarity Classification from <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> and Elicited Text<fixed-case>T</fixed-case>witter and Elicited Text</title>
      <author><first>Wesley</first><last>Santos</last></author>
      <author><first>Ivandré</first><last>Paraboni</last></author>
      <pages>1069–1075</pages>
      <abstract>We introduce a labelled corpus of stances about moral issues for the <a href="https://en.wikipedia.org/wiki/Brazilian_Portuguese">Brazilian Portuguese language</a>, and present reference results for both the stance recognition and polarity classification tasks. The <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> is built from <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> and further expanded with data elicited through <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowd sourcing</a> and labelled by their own authors. Put together, the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> and reference results are expected to be taken as a baseline for further studies in the field of stance recognition and polarity classification from text.</abstract>
      <url hash="7ed1ff6b">R19-1123</url>
      <doi>10.26615/978-954-452-056-4_123</doi>
      <bibkey>santos-paraboni-2019-moral</bibkey>
    </paper>
    <paper id="125">
      <title>Offence in Dialogues : A Corpus-Based Study</title>
      <author><first>Johannes</first><last>Schäfer</last></author>
      <author><first>Ben</first><last>Burtenshaw</last></author>
      <pages>1085–1093</pages>
      <abstract>In recent years an increasing number of analyses of offensive language has been published, however, dealing mainly with the automatic detection and classification of isolated instances. In this paper we aim to understand the impact of offensive messages in online conversations diachronically, and in particular the change in offensiveness of dialogue turns. In turn, we aim to measure the progression of offence level as well as its direction-For example, whether a conversation is escalating or declining in offence. We present our method of extracting linear dialogues from tree-structured conversations in social media data and make our code publicly available. Furthermore, we discuss methods to analyse this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> through changes in discourse offensiveness. Our paper includes two main contributions ; first, using a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> to measure the level of offensiveness in conversations ; and second, the analysis of conversations around offensive comments using decoupling functions.</abstract>
      <url hash="b9d26c8a">R19-1125</url>
      <doi>10.26615/978-954-452-056-4_125</doi>
      <bibkey>schafer-burtenshaw-2019-offence</bibkey>
    </paper>
    <paper id="127">
      <title>A Morpho-Syntactically Informed LSTM-CRF Model for Named Entity Recognition<fixed-case>LSTM</fixed-case>-<fixed-case>CRF</fixed-case> Model for Named Entity Recognition</title>
      <author><first>Lilia</first><last>Simeonova</last></author>
      <author><first>Kiril</first><last>Simov</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1104–1113</pages>
      <abstract>We propose a morphologically informed model for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, which is based on LSTM-CRF architecture and combines word embeddings, Bi-LSTM character embeddings, part-of-speech (POS) tags, and morphological information. While previous work has focused on learning from raw word input, using word and character embeddings only, we show that for morphologically rich languages, such as <a href="https://en.wikipedia.org/wiki/Bulgarian_language">Bulgarian</a>, access to POS information contributes more to the performance gains than the detailed morphological information. Thus, we show that <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> needs only coarse-grained POS tags, but at the same time it can benefit from simultaneously using some POS information of different granularity. Our evaluation results over a standard <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> show sizeable improvements over the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> for Bulgarian NER.</abstract>
      <url hash="74886e28">R19-1127</url>
      <doi>10.26615/978-954-452-056-4_127</doi>
      <bibkey>simeonova-etal-2019-morpho</bibkey>
    </paper>
    <paper id="131">
      <title>Automated Text Simplification as a Preprocessing Step for <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a> into an Under-resourced Language</title>
      <author><first>Sanja</first><last>Štajner</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <pages>1141–1150</pages>
      <abstract>In this work, we investigate the possibility of using fully automatic text simplification system on the English source in machine translation (MT) for improving its translation into an under-resourced language. We use the state-of-the-art automatic text simplification (ATS) system for lexically and syntactically simplifying source sentences, which are then translated with two state-of-the-art English-to-Serbian MT systems, the phrase-based MT (PBMT) and the neural MT (NMT). We explore three different scenarios for using the ATS in MT : (1) using the raw output of the ATS ; (2) automatically filtering out the sentences with low grammaticality and meaning preservation scores ; and (3) performing a minimal manual correction of the ATS output. Our results show improvement in fluency of the <a href="https://en.wikipedia.org/wiki/Translation">translation</a> regardless of the chosen scenario, and difference in success of the three scenarios depending on the MT approach used (PBMT or NMT) with regards to improving <a href="https://en.wikipedia.org/wiki/Translation">translation fluency</a> and post-editing effort.</abstract>
      <url hash="39f85c13">R19-1131</url>
      <doi>10.26615/978-954-452-056-4_131</doi>
      <bibkey>stajner-popovic-2019-automated</bibkey>
    </paper>
    <paper id="132">
      <title>Investigating Multilingual Abusive Language Detection : A Cautionary Tale</title>
      <author><first>Kenneth</first><last>Steimel</last></author>
      <author><first>Daniel</first><last>Dakota</last></author>
      <author><first>Yue</first><last>Chen</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <pages>1151–1160</pages>
      <abstract>Abusive language detection has received much attention in the last years, and recent approaches perform the task in a number of different languages. We investigate which factors have an effect on multilingual settings, focusing on the compatibility of data and annotations. In the current paper, we focus on <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/German_language">German</a>. Our findings show large differences in performance between the two languages. We find that the best performance is achieved by different <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification algorithms</a>. Sampling to address class imbalance issues is detrimental for <a href="https://en.wikipedia.org/wiki/German_language">German</a> and beneficial for <a href="https://en.wikipedia.org/wiki/English_language">English</a>. The only similarity that we find is that neither data set shows clear topics when we compare the results of <a href="https://en.wikipedia.org/wiki/Topic_modeling">topic modeling</a> to the gold standard. Based on our findings, we can conclude that a multilingual optimization of classifiers is not possible even in settings where comparable data sets are used.</abstract>
      <url hash="26796c6e">R19-1132</url>
      <doi>10.26615/978-954-452-056-4_132</doi>
      <bibkey>steimel-etal-2019-investigating</bibkey>
    </paper>
    <paper id="138">
      <title>SenZi : A Sentiment Analysis Lexicon for the Latinised Arabic (Arabizi)<fixed-case>S</fixed-case>en<fixed-case>Z</fixed-case>i: A Sentiment Analysis Lexicon for the Latinised <fixed-case>A</fixed-case>rabic (<fixed-case>A</fixed-case>rabizi)</title>
      <author><first>Taha</first><last>Tobaili</last></author>
      <author><first>Miriam</first><last>Fernandez</last></author>
      <author><first>Harith</first><last>Alani</last></author>
      <author><first>Sanaa</first><last>Sharafeddine</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>1203–1211</pages>
      <abstract>Arabizi is an informal written form of <a href="https://en.wikipedia.org/wiki/Varieties_of_Arabic">dialectal Arabic</a> transcribed in <a href="https://en.wikipedia.org/wiki/Latin_script">Latin alphanumeric characters</a>. It has a proven popularity on chat platforms and <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, yet <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> suffers from a severe lack of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP) resources</a>. As such, texts written in <a href="https://en.wikipedia.org/wiki/Arabizi">Arabizi</a> are often disregarded in sentiment analysis tasks for <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>. In this paper we describe the creation of a sentiment lexicon for <a href="https://en.wikipedia.org/wiki/Arabizi">Arabizi</a> that was enriched with <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. The result is a new Arabizi lexicon consisting of 11.3 K positive and 13.3 K negative words. We evaluated this <a href="https://en.wikipedia.org/wiki/Lexicon">lexicon</a> by classifying the sentiment of Arabizi tweets achieving an F1-score of 0.72. We provide a detailed error analysis to present the challenges that impact the <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> of <a href="https://en.wikipedia.org/wiki/Arabizi">Arabizi</a>.</abstract>
      <url hash="0f6b016e">R19-1138</url>
      <doi>10.26615/978-954-452-056-4_138</doi>
      <bibkey>tobaili-etal-2019-senzi</bibkey>
    </paper>
    <paper id="140">
      <title>Cross-Lingual Word Embeddings for Morphologically Rich Languages</title>
      <author><first>Ahmet</first><last>Üstün</last></author>
      <author><first>Gosse</first><last>Bouma</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>1222–1228</pages>
      <abstract>Cross-lingual word embedding models learn a shared vector space for two or more languages so that words with similar meaning are represented by similar vectors regardless of their language. Although the existing models achieve high performance on pairs of morphologically simple languages, they perform very poorly on morphologically rich languages such as <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a> and <a href="https://en.wikipedia.org/wiki/Finnish_language">Finnish</a>. In this paper, we propose a morpheme-based model in order to increase the performance of cross-lingual word embeddings on morphologically rich languages. Our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> includes a simple extension which enables us to exploit <a href="https://en.wikipedia.org/wiki/Morpheme">morphemes</a> for cross-lingual mapping. We applied our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> for the <a href="https://en.wikipedia.org/wiki/Finnish_language">Turkish-Finnish language pair</a> on the bilingual word translation task. Results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms the baseline models by 2 % in the <a href="https://en.wikipedia.org/wiki/Nearest_neighbour_search">nearest neighbour ranking</a>.</abstract>
      <url hash="a6ad34bc">R19-1140</url>
      <doi>10.26615/978-954-452-056-4_140</doi>
      <bibkey>ustun-etal-2019-cross</bibkey>
    </paper>
    <paper id="142">
      <title>Deep learning contextual models for prediction of sport event outcome from sportsman’s interviews</title>
      <author><first>Boris</first><last>Velichkov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Svetla</first><last>Boytcheva</last></author>
      <pages>1240–1246</pages>
      <abstract>This paper presents an approach for prediction of results for <a href="https://en.wikipedia.org/wiki/Sport">sport events</a>. Usually the sport forecasting approaches are based on <a href="https://en.wikipedia.org/wiki/Structured_data">structured data</a>. We test the hypothesis that the sports results can be predicted by using <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> and machine learning techniques applied over interviews with the players shortly before the sport events. The proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> uses deep learning contextual models, applied over <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured textual documents</a>. Several experiments were performed for interviews with players in individual sports like <a href="https://en.wikipedia.org/wiki/Boxing">boxing</a>, <a href="https://en.wikipedia.org/wiki/Martial_arts">martial arts</a>, and <a href="https://en.wikipedia.org/wiki/Tennis">tennis</a>. The results from the conducted experiment confirmed our initial assumption that an interview from a sportsman before a match contains information that can be used for prediction the outcome from it. Furthermore, the results provide strong evidence in support of our research hypothesis, that is, we can predict the outcome from a sport match analyzing an interview, given before it.</abstract>
      <url hash="54dbd560">R19-1142</url>
      <doi>10.26615/978-954-452-056-4_142</doi>
      <bibkey>velichkov-etal-2019-deep</bibkey>
    </paper>
    <paper id="144">
      <title>Exploiting <a href="https://en.wikipedia.org/wiki/Open_IE">Open IE</a> for Deriving Multiple Premises Entailment Corpus<fixed-case>IE</fixed-case> for Deriving Multiple Premises Entailment Corpus</title>
      <author><first>Martin</first><last>Víta</last></author>
      <author><first>Jakub</first><last>Klímek</last></author>
      <pages>1257–1264</pages>
      <abstract>Natural language inference (NLI) is a key part of <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>. The NLI task is defined as a <a href="https://en.wikipedia.org/wiki/Decision_problem">decision problem</a> whether a given sentence   hypothesis   can be inferred from a given text. Typically, we deal with a text consisting of just a single premise / single sentence, which is called a single premise entailment (SPE) task. Recently, a derived task of NLI from multiple premises (MPE) was introduced together with the first annotated corpus and corresponding several strong baselines. Nevertheless, the further development in MPE field requires accessibility of huge amounts of annotated data. In this paper we introduce a novel method for rapid deriving of MPE corpora from an existing NLI (SPE) annotated data that does not require any additional annotation work. This proposed approach is based on using an open <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction system</a>. We demonstrate the application of the <a href="https://en.wikipedia.org/wiki/Methodology">method</a> on a well known SNLI corpus. Over the obtained <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>, we provide the first evaluations as well as we state a strong <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>.</abstract>
      <url hash="43509211">R19-1144</url>
      <doi>10.26615/978-954-452-056-4_144</doi>
      <bibkey>vita-klimek-2019-exploiting</bibkey>
    </paper>
    <paper id="147">
      <title>ETNLP : A Visual-Aided Systematic Approach to Select Pre-Trained Embeddings for a Downstream Task<fixed-case>ETNLP</fixed-case>: A Visual-Aided Systematic Approach to Select Pre-Trained Embeddings for a Downstream Task</title>
      <author><first>Son</first><last>Vu Xuan</last></author>
      <author><first>Thanh</first><last>Vu</last></author>
      <author><first>Son</first><last>Tran</last></author>
      <author><first>Lili</first><last>Jiang</last></author>
      <pages>1285–1294</pages>
      <abstract>Given many recent advanced embedding models, selecting pre-trained word representation (i.e., word embedding) models best fit for a specific downstream NLP task is non-trivial. In this paper, we propose a systematic approach to extracting, evaluating, and visualizing multiple sets of pre-trained word embed- dings to determine which embeddings should be used in a downstream task. First, for extraction, we provide a method to extract a subset of the <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> to be used in the downstream NLP tasks. Second, for evaluation, we analyse the quality of pre-trained embeddings using an input word analogy list. Finally, we visualize the <a href="https://en.wikipedia.org/wiki/Embedding">embedding space</a> to explore the embedded words interactively. We demonstrate the effectiveness of the proposed approach on our pre-trained word embedding models in <a href="https://en.wikipedia.org/wiki/Vietnamese_language">Vietnamese</a> to select which <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> are suitable for a named entity recogni- tion (NER) task. Specifically, we create a large Vietnamese word analogy list to evaluate and select the pre-trained embedding models for the task. We then utilize the selected embed- dings for the NER task and achieve the new state-of-the-art results on the task benchmark dataset. We also apply the approach to another downstream task of privacy-guaranteed embedding selection, and show that it helps users quickly select the most suitable embeddings. In addition, we create an <a href="https://en.wikipedia.org/wiki/Open-source_software">open-source system</a> using the proposed systematic approach to facilitate similar studies on other NLP tasks. The source code and data are available at https : //github.com / vietnlp / etnlp.</abstract>
      <url hash="9ed67059">R19-1147</url>
      <doi>10.26615/978-954-452-056-4_147</doi>
      <bibkey>vu-xuan-etal-2019-etnlp</bibkey>
      <pwccode url="https://github.com/vietnlp/etnlp" additional="true">vietnlp/etnlp</pwccode>
    </paper>
    <paper id="150">
      <title>Bigger versus Similar : Selecting a Background Corpus for First Story Detection Based on Distributional Similarity</title>
      <author><first>Fei</first><last>Wang</last></author>
      <author><first>Robert J.</first><last>Ross</last></author>
      <author><first>John D.</first><last>Kelleher</last></author>
      <pages>1312–1320</pages>
      <abstract>The current state of the art for First Story Detection (FSD) are nearest neighbour-based models with traditional term vector representations ; however, one challenge faced by FSD models is that the document representation is usually defined by the vocabulary and term frequency from a background corpus. Consequently, the ideal background corpus should arguably be both large-scale to ensure adequate term coverage, and similar to the target domain in terms of the <a href="https://en.wikipedia.org/wiki/Frequency_distribution">language distribution</a>. However, given these two factors can not always be mutually satisfied, in this paper we examine whether the distributional similarity of common terms is more important than the scale of common terms for FSD. As a basis for our analysis we propose a set of <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> to quantitatively measure the scale of common terms and the distributional similarity between corpora. Using these <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> we rank different background corpora relative to a target corpus. We also apply <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> based on different background corpora to the FSD task. Our results show that term distributional similarity is more predictive of good FSD performance than the scale of common terms ; and, thus we demonstrate that a smaller recent domain-related corpus will be more suitable than a very large-scale general corpus for FSD.</abstract>
      <url hash="6dbbf3d3">R19-1150</url>
      <doi>10.26615/978-954-452-056-4_150</doi>
      <bibkey>wang-etal-2019-bigger</bibkey>
    </paper>
    <paper id="151">
      <title>Predicting Sentiment of Polish Language Short Texts<fixed-case>P</fixed-case>olish Language Short Texts</title>
      <author><first>Aleksander</first><last>Wawer</last></author>
      <author><first>Julita</first><last>Sobiczewska</last></author>
      <pages>1321–1327</pages>
      <abstract>The goal of this paper is to use all available Polish language data sets to seek the best possible performance in supervised sentiment analysis of short texts. We use text collections with labelled sentiment such as <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a>, <a href="https://en.wikipedia.org/wiki/Film_criticism">movie reviews</a> and a sentiment treebank, in three comparison modes. In the first, we examine the performance of <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained and tested on the same <a href="https://en.wikipedia.org/wiki/Text_corpus">text collection</a> using standard cross-validation (in-domain). In the second we train <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> on all available <a href="https://en.wikipedia.org/wiki/Data">data</a> except the given test collection, which we use for testing (one vs rest cross-domain). In the third, we train a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> on one <a href="https://en.wikipedia.org/wiki/Data_set">data set</a> and apply it to another one (one vs one cross-domain). We compare wide range of methods including <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> on bag-of-words representation, bidirectional recurrent neural networks as well as the most recent pre-trained architectures <a href="https://en.wikipedia.org/wiki/ELMO">ELMO</a> and BERT. We formulate conclusions as to cross-domain and in-domain performance of each <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a>. Unsurprisingly, BERT turned out to be a strong performer, especially in the cross-domain setting. What is surprising however, is solid performance of the relatively simple multinomial Naive Bayes classifier, which performed equally well as BERT on several data sets.</abstract>
      <url hash="809e9378">R19-1151</url>
      <doi>10.26615/978-954-452-056-4_151</doi>
      <bibkey>wawer-sobiczewska-2019-predicting</bibkey>
    </paper>
    <paper id="152">
      <title>Improving Named Entity Linking Corpora Quality</title>
      <author><first>Albert</first><last>Weichselbraun</last></author>
      <author><first>Adrian M.P.</first><last>Brasoveanu</last></author>
      <author><first>Philipp</first><last>Kuntschik</last></author>
      <author><first>Lyndon J.B.</first><last>Nixon</last></author>
      <pages>1328–1337</pages>
      <abstract>Gold standard corpora and competitive evaluations play a key role in benchmarking named entity linking (NEL) performance and driving the development of more sophisticated NEL systems. The quality of the used corpora and the used evaluation metrics are crucial in this process. We, therefore, assess the quality of three popular evaluation corpora, identifying four major issues which affect these gold standards : (i) the use of different annotation styles, (ii) incorrect and missing annotations, (iii) Knowledge Base evolution, (iv) and differences in annotating co-occurrences. This paper addresses these issues by formalizing NEL annotations and corpus versioning which allows standardizing corpus creation, supports corpus evolution, and paves the way for the use of lenses to automatically transform between different corpus configurations. In addition, the use of clearly defined scoring rules and evaluation metrics ensures a better comparability of evaluation results.</abstract>
      <url hash="85cd3078">R19-1152</url>
      <doi>10.26615/978-954-452-056-4_152</doi>
      <bibkey>weichselbraun-etal-2019-improving</bibkey>
    </paper>
    <paper id="156">
      <title>An Open, Extendible, and Fast Turkish Morphological Analyzer<fixed-case>T</fixed-case>urkish Morphological Analyzer</title>
      <author><first>Olcay Taner</first><last>Yıldız</last></author>
      <author><first>Begüm</first><last>Avar</last></author>
      <author><first>Gökhan</first><last>Ercan</last></author>
      <pages>1364–1372</pages>
      <abstract>In this paper, we present a two-level morphological analyzer for <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a>. The morphological analyzer consists of five main components : finite state transducer, rule engine for suffixation, <a href="https://en.wikipedia.org/wiki/Lexicon">lexicon</a>, trie data structure, and <a href="https://en.wikipedia.org/wiki/LRU_cache">LRU cache</a>. We use <a href="https://en.wikipedia.org/wiki/Java_(programming_language)">Java language</a> to implement finite state machine logic and rule engine, Xml language to describe the finite state transducer rules of the <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish language</a>, which makes the morphological analyzer both easily extendible and easily applicable to other languages. Empowered with the comprehensiveness of a lexicon of 54,000 bare-forms including 19,000 proper nouns, our morphological analyzer presents one of the most reliable analyzers produced so far. The analyzer is compared with Turkish morphological analyzers in the literature. By using <a href="https://en.wikipedia.org/wiki/LRU_cache">LRU cache</a> and a <a href="https://en.wikipedia.org/wiki/Trie">trie data structure</a>, the <a href="https://en.wikipedia.org/wiki/System">system</a> can analyze 100,000 words per second, which enables users to analyze huge corpora in a few hours.</abstract>
      <url hash="b85c84ca">R19-1156</url>
      <doi>10.26615/978-954-452-056-4_156</doi>
      <bibkey>yildiz-etal-2019-open</bibkey>
    </paper>
    <paper id="159">
      <title>Multilingual Dynamic Topic Model</title>
      <author><first>Elaine</first><last>Zosa</last></author>
      <author><first>Mark</first><last>Granroth-Wilding</last></author>
      <pages>1388–1396</pages>
      <abstract>Dynamic topic models (DTMs) capture the evolution of topics and trends in <a href="https://en.wikipedia.org/wiki/Time_series">time series data</a>. Current DTMs are applicable only to monolingual datasets. In this paper we present the multilingual dynamic topic model (ML-DTM), a novel <a href="https://en.wikipedia.org/wiki/Topic_model">topic model</a> that combines DTM with an existing multilingual topic modeling method to capture cross-lingual topics that evolve across time. We present results of this model on a parallel German-English corpus of news articles and a comparable corpus of Finnish and Swedish news articles. We demonstrate the capability of ML-DTM to track significant events related to a topic and show that it finds distinct topics and performs as well as existing multilingual topic models in aligning cross-lingual topics.</abstract>
      <url hash="cc8f6d5d">R19-1159</url>
      <doi>10.26615/978-954-452-056-4_159</doi>
      <bibkey>zosa-granroth-wilding-2019-multilingual</bibkey>
    </paper>
    <paper id="160">
      <title>A Wide-Coverage Context-Free Grammar for Icelandic and an Accompanying Parsing System<fixed-case>I</fixed-case>celandic and an Accompanying Parsing System</title>
      <author><first>Vilhjálmur</first><last>Þorsteinsson</last></author>
      <author><first>Hulda</first><last>Óladóttir</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <pages>1397–1404</pages>
      <abstract>We present an open-source, wide-coverage context-free grammar (CFG) for <a href="https://en.wikipedia.org/wiki/Icelandic_language">Icelandic</a>, and an accompanying parsing system. The grammar has over 5,600 <a href="https://en.wikipedia.org/wiki/Terminal_and_nonterminal_symbols">nonterminals</a>, 4,600 <a href="https://en.wikipedia.org/wiki/Terminal_and_nonterminal_symbols">terminals</a> and 19,000 productions in fully expanded form, with feature agreement constraints for <a href="https://en.wikipedia.org/wiki/Grammatical_case">case</a>, <a href="https://en.wikipedia.org/wiki/Grammatical_gender">gender</a>, number and person. The parsing system consists of an enhanced <a href="https://en.wikipedia.org/wiki/Earley_parser">Earley-based parser</a> and a mechanism to select best-scoring parse trees from shared packed parse forests. Our parsing system is able to parse about 90 % of all sentences in articles published on the main Icelandic news websites. Preliminary evaluation with evalb shows an <a href="https://en.wikipedia.org/wiki/F-measure">F-measure</a> of 70.72 % on parsed sentences. Our system demonstrates that parsing a morphologically rich language using a wide-coverage CFG can be practical.</abstract>
      <url hash="69ae74cc">R19-1160</url>
      <doi>10.26615/978-954-452-056-4_160</doi>
      <bibkey>thorsteinsson-etal-2019-wide</bibkey>
    </paper>
  </volume>
  <volume id="2" ingest-date="2020-01-16">
    <meta>
      <booktitle>Proceedings of the Student Research Workshop Associated with RANLP 2019</booktitle>
      <url hash="c2fdf0f9">R19-2</url>
      <editor><first>Venelin</first><last>Kovatchev</last></editor>
      <editor><first>Irina</first><last>Temnikova</last></editor>
      <editor><first>Branislava</first><last>Šandrih</last></editor>
      <editor><first>Ivelina</first><last>Nikolova</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="dbf59d4d">R19-2000</url>
      <bibkey>ranlp-2019-student</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Classification Approaches to Identify Informative Tweets</title>
      <author><first>Piush</first><last>Aggarwal</last></author>
      <pages>7–15</pages>
      <abstract>Social media platforms have become prime forums for reporting news, with users sharing what they saw, heard or read on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. News from <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> is potentially useful for various stakeholders including aid organizations, news agencies, and individuals. However, <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> also contains a vast amount of non-news content. For users to be able to draw on benefits from news reported on social media it is necessary to reliably identify news content and differentiate <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> from non-news. In this paper, we tackle the challenge of classifying a social post as news or not. To this end, we provide a new manually annotated dataset containing 2,992 tweets from 5 different topical categories. Unlike earlier <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, it includes postings posted by personal users who do not promote a business or a product and are not affiliated with any organization. We also investigate various <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline systems</a> and evaluate their performance on the newly generated <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. Our results show that the best <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> are the SVM and BERT models.</abstract>
      <url hash="a93d448e">R19-2002</url>
      <doi>10.26615/issn.2603-2821.2019_002</doi>
      <bibkey>aggarwal-2019-classification</bibkey>
    </paper>
    <paper id="4">
      <title>Multilingual Language Models for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entity Recognition</a> in <a href="https://en.wikipedia.org/wiki/German_language">German</a> and English<fixed-case>G</fixed-case>erman and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Antonia</first><last>Baumann</last></author>
      <pages>21–27</pages>
      <abstract>We assess the language specificity of recent <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> by exploring the potential of a multilingual language model. In particular, we evaluate Google’s multilingual BERT (mBERT) model on Named Entity Recognition (NER) in <a href="https://en.wikipedia.org/wiki/German_language">German</a> and <a href="https://en.wikipedia.org/wiki/English_language">English</a>. We expand the work on language model fine-tuning by Howard and Ruder (2018), applying it to the BERT architecture. We successfully reproduce the NER results published by Devlin et al. (2019).Our results show that the multilingual language model generalises well for <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">NER</a> in the chosen languages, matching the native model in <a href="https://en.wikipedia.org/wiki/English_language">English</a> and comparing well with recent approaches for <a href="https://en.wikipedia.org/wiki/German_language">German</a>. However, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> does not benefit from the added fine-tuning methods.</abstract>
      <url hash="2c98c8ea">R19-2004</url>
      <doi>10.26615/issn.2603-2821.2019_004</doi>
      <bibkey>baumann-2019-multilingual</bibkey>
    </paper>
    <paper id="6">
      <title>Cross-Lingual Coreference : The Case of <a href="https://en.wikipedia.org/wiki/Bulgarian_language">Bulgarian</a> and English<fixed-case>B</fixed-case>ulgarian and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Zara</first><last>Kancheva</last></author>
      <pages>32–38</pages>
      <abstract>The paper presents several common approaches towards cross- and multi-lingual coreference resolution in a search of the most effective practices to be applied within the work on Bulgarian-English manual coreference annotation of a short story. The work aims at outlining the typology of the differences in the annotated parallel texts. The results of the research prove to be comparable with the tendencies observed in similar works on other <a href="https://en.wikipedia.org/wiki/Slavic_languages">Slavic languages</a> and show surprising differences between the types of markables and their frequency in <a href="https://en.wikipedia.org/wiki/Bulgarian_language">Bulgarian</a> and <a href="https://en.wikipedia.org/wiki/English_language">English</a>.</abstract>
      <url hash="c2dcc62d">R19-2006</url>
      <doi>10.26615/issn.2603-2821.2019_006</doi>
      <bibkey>kancheva-2019-cross</bibkey>
    </paper>
    <paper id="8">
      <title>Evaluation of Stacked Embeddings for <a href="https://en.wikipedia.org/wiki/Bulgarian_language">Bulgarian</a> on the Downstream Tasks POS and NERC<fixed-case>B</fixed-case>ulgarian on the Downstream Tasks <fixed-case>POS</fixed-case> and <fixed-case>NERC</fixed-case></title>
      <author><first>Iva</first><last>Marinova</last></author>
      <pages>48–54</pages>
      <abstract>This paper reports on experiments with different stacks of word embeddings and evaluation of their usefulness for Bulgarian downstream tasks such as Named Entity Recognition and Classification (NERC) and Part-of-speech (POS) Tagging. Word embeddings stay in the core of the development of NLP, with several key language models being created over the last two years like FastText (CITATION), ElMo (CITATION), BERT (CITATION) and Flair (CITATION). Stacking or combining different word embeddings is another technique used in this paper and still not reported for Bulgarian NERC. Well-established architecture is used for the sequence tagging task such as BI-LSTM-CRF, and different pre-trained language models are combined in the embedding layer to decide which combination of them scores better.</abstract>
      <url hash="9c3bb300">R19-2008</url>
      <doi>10.26615/issn.2603-2821.2019_008</doi>
      <bibkey>marinova-2019-evaluation</bibkey>
    </paper>
    <paper id="9">
      <title>Overview on NLP Techniques for Content-based Recommender Systems for Books<fixed-case>NLP</fixed-case> Techniques for Content-based Recommender Systems for Books</title>
      <author><first>Melania</first><last>Berbatova</last></author>
      <pages>55–61</pages>
      <abstract>Recommender systems are an essential part of today’s largest websites. Without <a href="https://en.wikipedia.org/wiki/Microsoft_Windows">them</a>, it would be hard for users to find the right products and content. One of the most popular <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for <a href="https://en.wikipedia.org/wiki/Recommender_system">recommendations</a> is <a href="https://en.wikipedia.org/wiki/Content-based_filtering">content-based filtering</a>. It relies on analysing product metadata, a great part of which is <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">textual data</a>. Despite their frequent use, there is still no standard procedure for developing and evaluating content-based recommenders. In this paper, we will first examine current approaches for designing, training and evaluating <a href="https://en.wikipedia.org/wiki/Recommender_system">recommender systems</a> based on <a href="https://en.wikipedia.org/wiki/Text-based_user_interface">textual data</a> for <a href="https://en.wikipedia.org/wiki/Recommender_system">books recommendations</a> for <a href="https://en.wikipedia.org/wiki/GoodReads">GoodReads’ website</a>. We will give critiques on existing methods and suggest how <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language techniques</a> can be employed for the improvement of content-based recommenders.</abstract>
      <url hash="515d0024">R19-2009</url>
      <doi>10.26615/issn.2603-2821.2019_009</doi>
      <bibkey>berbatova-2019-overview</bibkey>
    </paper>
    <paper id="13">
      <title>Multilingual Complex Word Identification : <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a> with Morphological and Linguistic Features</title>
      <author><first>Kim Cheng</first><last>Sheang</last></author>
      <pages>83–89</pages>
      <abstract>The paper is about our experiments with Complex Word Identification system using deep learning approach with <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> and engineered features.</abstract>
      <url hash="b2792b26">R19-2013</url>
      <doi>10.26615/issn.2603-2821.2019_013</doi>
      <bibkey>sheang-2019-multilingual</bibkey>
    </paper>
    </volume>
</collection>