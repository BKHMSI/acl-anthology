<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.rocling">
  <volume id="1" ingest-date="2021-10-13">
    <meta>
      <booktitle>Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021)</booktitle>
      <editor><first>Lung-Hao</first><last>Lee</last></editor>
      <editor><first>Chia-Hui</first><last>Chang</last></editor>
      <editor><first>Kuan-Yu</first><last>Chen</last></editor>
      <publisher>The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)</publisher>
      <address>Taoyuan, Taiwan</address>
      <month>October</month>
      <year>2021</year>
      <url hash="1ae3f8cb">2021.rocling-1</url>
    </meta>
    <frontmatter>
      <url hash="0cc424c0">2021.rocling-1.0</url>
      <bibkey>rocling-2021-linguistics</bibkey>
    </frontmatter>
    <paper id="2">
      <title>A Study on Using <a href="https://en.wikipedia.org/wiki/Transfer_learning">Transfer Learning</a> to Improve BERT Model for Emotional Classification of <a href="https://en.wikipedia.org/wiki/Chinese_poetry">Chinese Lyrics</a><fixed-case>BERT</fixed-case> Model for Emotional Classification of <fixed-case>C</fixed-case>hinese Lyrics</title>
      <author><first>Jia-Yi</first><last>Liao</last></author>
      <author><first>Ya-Hsuan</first><last>Lin</last></author>
      <author><first>Kuan-Cheng</first><last>Lin</last></author>
      <author><first>Jia-Wei</first><last>Chang</last></author>
      <pages>13–17</pages>
      <abstract>The explosive growth of <a href="https://en.wikipedia.org/wiki/Music_library">music libraries</a> has made <a href="https://en.wikipedia.org/wiki/Music_information_retrieval">music information retrieval</a> and recommendation a critical issue. Recommendation systems based on music emotion recognition are gradually gaining attention. Most of the studies focus on <a href="https://en.wikipedia.org/wiki/Audio_signal_processing">audio data</a> rather than <a href="https://en.wikipedia.org/wiki/Lyrics">lyrics</a> to build models of music emotion classification. In addition, because of the richness of <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">English language resources</a>, most of the existing studies are focused on <a href="https://en.wikipedia.org/wiki/English_language">English lyrics</a> but rarely on <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. For this reason, We propose an approach that uses the BERT pretraining model and <a href="https://en.wikipedia.org/wiki/Transfer_learning">Transfer learning</a> to improve the emotion classification task of Chinese lyrics. The following approaches were used without any specific training for the Chinese lyrics emotional classification task : (a) Using BERT, only can reach 50 % of the classification accuracy. (b) Using BERT with <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> of CVAW, CVAP, and CVAT datasets can achieve 71 % classification accuracy.</abstract>
      <url hash="51b2a637">2021.rocling-1.2</url>
      <bibkey>liao-etal-2021-study</bibkey>
    </paper>
    <paper id="3">
      <title>Nested Named Entity Recognition for Chinese Electronic Health Records with QA-based Sequence Labeling<fixed-case>C</fixed-case>hinese Electronic Health Records with <fixed-case>QA</fixed-case>-based Sequence Labeling</title>
      <author><first>Yu-Lun</first><last>Chiang</last></author>
      <author><first>Chih-Hao</first><last>Lin</last></author>
      <author><first>Cheng-Lung</first><last>Sung</last></author>
      <author><first>Keh-Yih</first><last>Su</last></author>
      <pages>18–25</pages>
      <abstract>This study presents a novel QA-based sequence labeling (QASL) approach to naturally tackle both flat and nested Named Entity Recogntion (NER) tasks on a Chinese Electronic Health Records (CEHRs) dataset. This proposed QASL approach parallelly asks a corresponding natural language question for each specific named entity type, and then identifies those associated NEs of the same specified type with the BIO tagging scheme. The associated nested NEs are then formed by overlapping the results of various types. In comparison with those pure sequence-labeling (SL) approaches, since the given question includes significant prior knowledge about the specified entity type and the capability of extracting NEs with different types, the performance for nested NER task is thus improved, obtaining 90.70 % of F1-score. Besides, in comparison with the pure QA-based approach, our proposed approach retains the SL features, which could extract multiple NEs with the same types without knowing the exact number of NEs in the same passage in advance. Eventually, experiments on our CEHR dataset demonstrate that QASL-based models greatly outperform the SL-based models by 6.12 % to 7.14 % of F1-score.</abstract>
      <url hash="668e1cd9">2021.rocling-1.3</url>
      <bibkey>chiang-etal-2021-nested</bibkey>
    </paper>
    <paper id="4">
      <title>AI Clerk Platform : Information Extraction DIY Platform<fixed-case>AI</fixed-case> Clerk Platform : Information Extraction <fixed-case>DIY</fixed-case> Platform</title>
      <author><first>Ru-Yng</first><last>Chang</last></author>
      <author><first>Wen-Lun</first><last>Chen</last></author>
      <author><first>Cheng-Ju</first><last>Kao</last></author>
      <pages>26–32</pages>
      <abstract>Information extraction is a core technology of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, which extracts some meaningful phrases / clauses from unstructured or semistructured content to a particular topic. It can be said to be the core technology of many <a href="https://en.wikipedia.org/wiki/List_of_programming_languages_by_type">language technologies</a> and applications. This paper introduces AI Clerk Platform, which aims to accelerate and improve the entire process and convenience of the development of information extraction tools. AI Clerk Platform provides a friendly and intuitive visualized manual labeling interface, sets suitable semantic label in need, and implements, distributes and controls manual labeling tasks, so that users can complete customized information extraction models without programming and view the automatically predict results of models by three method. AI Clerk Platform further assists in the development of other natural language processing technologies and the derivation of application services.</abstract>
      <url hash="b722d718">2021.rocling-1.4</url>
      <bibkey>chang-etal-2021-ai</bibkey>
    </paper>
    <paper id="6">
      <title>Speech Emotion Recognition Based on CNN+LSTM Model<fixed-case>CNN</fixed-case>+<fixed-case>LSTM</fixed-case> Model</title>
      <author><first>Wei</first><last>Mou</last></author>
      <author><first>Pei-Hsuan</first><last>Shen</last></author>
      <author><first>Chu-Yun</first><last>Chu</last></author>
      <author><first>Yu-Cheng</first><last>Chiu</last></author>
      <author><first>Tsung-Hsien</first><last>Yang</last></author>
      <author><first>Ming-Hsiang</first><last>Su</last></author>
      <pages>43–47</pages>
      <abstract>Due to the popularity of intelligent dialogue assistant services, speech emotion recognition has become more and more important. In the communication between humans and machines, <a href="https://en.wikipedia.org/wiki/Emotion_recognition">emotion recognition</a> and emotion analysis can enhance the interaction between machines and humans. This study uses the CNN+LSTM model to implement speech emotion recognition (SER) processing and prediction. From the experimental results, it is known that using the CNN+LSTM model achieves better performance than using the traditional NN model.</abstract>
      <url hash="b3f45be8">2021.rocling-1.6</url>
      <bibkey>mou-etal-2021-speech</bibkey>
    </paper>
    <paper id="7">
      <title>A Study on Contextualized Language Modeling for Machine Reading Comprehension</title>
      <author><first>Chin-Ying</first><last>Wu</last></author>
      <author><first>Yung-Chang</first><last>Hsu</last></author>
      <author><first>Berlin</first><last>Chen</last></author>
      <pages>48–57</pages>
      <abstract>With the recent breakthrough of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning technologies</a>, research on machine reading comprehension (MRC) has attracted much attention and found its versatile applications in many use cases. MRC is an important natural language processing (NLP) task aiming to assess the ability of a machine to understand natural language expressions, which is typically operationalized by first asking questions based on a given text paragraph and then receiving machine-generated answers in accordance with the given context paragraph and questions. In this paper, we leverage two novel pretrained language models built on top of Bidirectional Encoder Representations from Transformers (BERT), namely BERT-wwm and MacBERT, to develop effective MRC methods. In addition, we also seek to investigate whether additional incorporation of the categorical information about a context paragraph can benefit MRC or not, which is achieved based on performing context paragraph clustering on the training dataset. On the other hand, an ensemble learning approach is proposed to harness the synergistic power of the aforementioned two BERT-based models so as to further promote MRC performance.</abstract>
      <url hash="584721fa">2021.rocling-1.7</url>
      <bibkey>wu-etal-2021-study</bibkey>
    </paper>
    <paper id="12">
      <title>Discussion on domain generalization in the cross-device speaker verification system</title>
      <author><first>Wei-Ting</first><last>Lin</last></author>
      <author><first>Yu-Jia</first><last>Zhang</last></author>
      <author><first>Chia-Ping</first><last>Chen</last></author>
      <author><first>Chung-Li</first><last>Lu</last></author>
      <author><first>Bo-Cheng</first><last>Chan</last></author>
      <pages>87–94</pages>
      <abstract>In this paper, we use domain generalization to improve the performance of the cross-device speaker verification system. Based on a trainable speaker verification system, we use domain generalization algorithms to fine-tune the model parameters. First, we use the VoxCeleb2 dataset to train ECAPA-TDNN as a baseline model. Then, use the CHT-TDSV dataset and the following domain generalization algorithms to fine-tune it : DANN, CDNN, Deep CORAL. Our proposed system tests 10 different scenarios in the NSYSU-TDSV dataset, including a single device and multiple devices. Finally, in the scenario of multiple devices, the best equal error rate decreased from 18.39 in the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> to 8.84. Successfully achieved cross-device identification on the speaker verification system.</abstract>
      <url hash="06a4b1b0">2021.rocling-1.12</url>
      <bibkey>lin-etal-2021-discussion</bibkey>
    </paper>
    <paper id="13">
      <title>Integrated Semantic and Phonetic Post-correction for Chinese Speech Recognition<fixed-case>C</fixed-case>hinese Speech Recognition</title>
      <author><first>Yi-Chang</first><last>Chen</last></author>
      <author><first>Chun-Yen</first><last>Cheng</last></author>
      <author><first>Chien-An</first><last>Chen</last></author>
      <author><first>Ming-Chieh</first><last>Sung</last></author>
      <author><first>Yi-Ren</first><last>Yeh</last></author>
      <pages>95–102</pages>
      <abstract>Due to the recent advances of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, several works have applied the pre-trained masked language model (MLM) of BERT to the post-correction of speech recognition. However, existing pre-trained models only consider the <a href="https://en.wikipedia.org/wiki/Semantic_change">semantic correction</a> while the <a href="https://en.wikipedia.org/wiki/Phoneme">phonetic features</a> of words is neglected. The semantic-only post-correction will consequently decrease the performance since homophonic errors are fairly common in Chinese ASR. In this paper, we proposed a novel approach to collectively exploit the contextualized representation and the phonetic information between the error and its replacing candidates to alleviate the <a href="https://en.wikipedia.org/wiki/Error_rate">error rate</a> of Chinese ASR. Our experiment results on real world speech recognition datasets showed that our proposed method has evidently lower <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence">CER</a> than the baseline model, which utilized a pre-trained BERT MLM as the corrector.</abstract>
      <url hash="331c74b7">2021.rocling-1.13</url>
      <bibkey>chen-etal-2021-integrated</bibkey>
      <pwccode url="https://github.com/gitycc/phonetic_mlm" additional="false">gitycc/phonetic_mlm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aishell-3">AISHELL-3</pwcdataset>
    </paper>
    <paper id="14">
      <title>A Preliminary Study on Environmental Sound Classification Leveraging Large-Scale Pretrained Model and <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">Semi-Supervised Learning</a></title>
      <author><first>You-Sheng</first><last>Tsao</last></author>
      <author><first>Tien-Hong</first><last>Lo</last></author>
      <author><first>Jiun-Ting</first><last>Li</last></author>
      <author><first>Shi-Yan</first><last>Weng</last></author>
      <author><first>Berlin</first><last>Chen</last></author>
      <pages>103–110</pages>
      <abstract>With the widespread commercialization of smart devices, research on environmental sound classification has gained more and more attention in recent years. In this paper, we set out to make effective use of large-scale audio pretrained model and <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised model training paradigm</a> for environmental sound classification. To this end, an environmental sound classification method is first put forward, whose <a href="https://en.wikipedia.org/wiki/Component_model">component model</a> is built on top a large-scale audio pretrained model. Further, to simulate a low-resource sound classification setting where only limited supervised examples are made available, we instantiate the notion of <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> with a recently proposed training algorithm (namely, FixMatch) and a data augmentation method (namely, SpecAugment) to achieve the goal of semi-supervised model training. Experiments conducted on bench-mark dataset UrbanSound8 K reveal that our classification method can lead to an accuracy improvement of 2.4 % in relation to a current <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline method</a>.</abstract>
      <url hash="d33f8df7">2021.rocling-1.14</url>
      <bibkey>tsao-etal-2021-preliminary</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/urbansound8k-1">UrbanSound8K</pwcdataset>
    </paper>
    <paper id="15">
      <title>Mining Commonsense and Domain Knowledge from Math Word Problems</title>
      <author><first>Shih-Hung</first><last>Tsai</last></author>
      <author><first>Chao-Chun</first><last>Liang</last></author>
      <author><first>Hsin-Min</first><last>Wang</last></author>
      <author><first>Keh-Yih</first><last>Su</last></author>
      <pages>111–117</pages>
      <abstract>Current neural math solvers learn to incorporate commonsense or domain knowledge by utilizing pre-specified constants or formulas. However, as these constants and formulas are mainly human-specified, the generalizability of the <a href="https://en.wikipedia.org/wiki/Solver">solvers</a> is limited. In this paper, we propose to explicitly retrieve the required knowledge from math problemdatasets. In this way, we can determinedly characterize the required knowledge andimprove the explainability of solvers. Our two <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> take the problem text andthe <a href="https://en.wikipedia.org/wiki/Equation_solving">solution equations</a> as input. Then, they try to deduce the required commonsense and domain knowledge by integrating information from both parts. We construct two math datasets and show the effectiveness of our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> that they can retrieve the required knowledge for <a href="https://en.wikipedia.org/wiki/Problem_solving">problem-solving</a>.</abstract>
      <url hash="6979fdda">2021.rocling-1.15</url>
      <bibkey>tsai-etal-2021-mining</bibkey>
    </paper>
    <paper id="22">
      <title>A BERT-based Siamese-structured Retrieval Model<fixed-case>BERT</fixed-case>-based <fixed-case>S</fixed-case>iamese-structured Retrieval Model</title>
      <author><first>Hung-Yun</first><last>Chiang</last></author>
      <author><first>Kuan-Yu</first><last>Chen</last></author>
      <pages>163–172</pages>
      <abstract>Due to the development of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>, the natural language processing tasks have made great progresses by leveraging the bidirectional encoder representations from Transformers (BERT). The goal of <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a> is to search the most relevant results for the user’s query from a large set of documents. Although BERT-based retrieval models have shown excellent results in many studies, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> usually suffer from the need for large amounts of computations and/or additional storage spaces. In view of the flaws, a BERT-based Siamese-structured retrieval model (BESS) is proposed in this paper. BESS not only inherits the merits of pre-trained language models, but also can generate extra information to compensate the original query automatically. Besides, the <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning strategy</a> is introduced to make the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> more robust. Accordingly, we evaluate BESS on three public-available corpora, and the experimental results demonstrate the efficiency of the proposed retrieval model.</abstract>
      <url hash="57365409">2021.rocling-1.22</url>
      <bibkey>chiang-chen-2021-bert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/movieqa">MovieQA</pwcdataset>
    </paper>
    <paper id="27">
      <title>Using Valence and Arousal-infused Bi-LSTM for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> in Social Media Product Reviews<fixed-case>B</fixed-case>i-<fixed-case>LSTM</fixed-case> for Sentiment Analysis in Social Media Product Reviews</title>
      <author><first>Yu-Ya</first><last>Cheng</last></author>
      <author><first>Wen-Chao</first><last>Yeh</last></author>
      <author><first>Yan-Ming</first><last>Chen</last></author>
      <author><first>Yung-Chun</first><last>Chang</last></author>
      <pages>210–217</pages>
      <abstract>With the popularity of the current Internet age, online social platforms have provided a bridge for communication between private companies, public organizations, and the public. The purpose of this research is to understand the user’s experience of the product by analyzing product review data in different fields. We propose a BiLSTM-based neural network which infused rich emotional information. In addition to consider <a href="https://en.wikipedia.org/wiki/Valence_(psychology)">Valence</a> and Arousal which is the smallest morpheme of emotional information, the dependence relationship between texts is also integrated into the deep learning model to analyze the <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment</a>. The experimental results show that this <a href="https://en.wikipedia.org/wiki/Research">research</a> can achieve good performance in predicting the vocabulary Valence and Arousal. In addition, the integration of VA and dependency information into the BiLSTM model can have excellent performance for social text sentiment analysis, which verifies that this <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is effective in emotion recognition of social medial short text.</abstract>
      <url hash="9fa9c73f">2021.rocling-1.27</url>
      <bibkey>cheng-etal-2021-using</bibkey>
    </paper>
    <paper id="29">
      <title>Aggregating User-Centric and Post-Centric Sentiments from <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a> for Topical Stance Prediction</title>
      <author><first>Jenq-Haur</first><last>Wang</last></author>
      <author><first>Kuan-Ting</first><last>Chen</last></author>
      <pages>226–235</pages>
      <abstract>Conventional <a href="https://en.wikipedia.org/wiki/Opinion_poll">opinion polls</a> were usually conducted via questionnaires or phone interviews, which are time-consuming and error-prone. With the advances in <a href="https://en.wikipedia.org/wiki/Social_networking_service">social networking platforms</a>, it’s easier for the general public to express their opinions on popular topics. Given the huge amount of user opinions, it would be useful if we can automatically collect and aggregate the overall topical stance for a specific topic. In this paper, we propose to predict topical stances from <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> by concept expansion, sentiment classification, and stance aggregation based on <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. For concept expansion of a given topic, related posts are collected from <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> and clustered by <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. Then, major <a href="https://en.wikipedia.org/wiki/Index_term">keywords</a> are extracted by <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a> and named entity recognition methods. For sentiment classification and aggregation, <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">machine learning methods</a> are used to train sentiment lexicon with <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. Then, the <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment scores</a> from user-centric and post-centric views are aggregated as the total stance on the topic. In the experiments, we evaluated the performance of our proposed approach using <a href="https://en.wikipedia.org/wiki/Social_media">social media data</a> from <a href="https://en.wikipedia.org/wiki/Internet_forum">online forums</a>. The experimental results for 2016 Taiwan Presidential Election showed that our proposed method can effectively expand keywords and aggregate topical stances from the public for accurate prediction of election results. The best performance is 0.52 % in terms of <a href="https://en.wikipedia.org/wiki/Mean_absolute_error">mean absolute error (MAE)</a>. Further investigation is needed to evaluate the performance of the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> in larger scales.</abstract>
      <url hash="84d25a12">2021.rocling-1.29</url>
      <bibkey>wang-chen-2021-aggregating</bibkey>
    </paper>
    <paper id="31">
      <title>Hidden Advertorial Detection on <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a> in Chinese<fixed-case>C</fixed-case>hinese</title>
      <author><first>Meng-Ching</first><last>Ho</last></author>
      <author><first>Ching-Yun</first><last>Chuang</last></author>
      <author><first>Yi-Chun</first><last>Hsu</last></author>
      <author><first>Yu-Yun</first><last>Chang</last></author>
      <pages>243–251</pages>
      <abstract>Nowadays, there are a lot of <a href="https://en.wikipedia.org/wiki/Advertising">advertisements</a> hiding as normal posts or experience sharing in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. There is little research of advertorial detection on <a href="https://en.wikipedia.org/wiki/Mandarin_Chinese">Mandarin Chinese texts</a>. This paper thus aimed to focus on hidden advertorial detection of <a href="https://en.wikipedia.org/wiki/Online_advertising">online posts</a> in <a href="https://en.wikipedia.org/wiki/Taiwanese_Mandarin">Taiwan Mandarin Chinese</a>. We inspected seven <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual features</a> based on <a href="https://en.wikipedia.org/wiki/Linguistics">linguistic theories</a> in <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse level</a>. These features can be further grouped into three schemas under the general advertorial writing structure. We further implemented these <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> to train a multi-task BERT model to detect <a href="https://en.wikipedia.org/wiki/Advertorial">advertorials</a>. The results suggested that specific <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">linguistic features</a> would help extract <a href="https://en.wikipedia.org/wiki/Advertorial">advertorials</a>.</abstract>
      <url hash="5a98c5ad">2021.rocling-1.31</url>
      <bibkey>ho-etal-2021-hidden</bibkey>
    </paper>
    <paper id="32">
      <title>Automatic Extraction of English Grammar Pattern Correction Rules<fixed-case>E</fixed-case>nglish Grammar Pattern Correction Rules</title>
      <author><first>Kuan-Yu</first><last>Shen</last></author>
      <author><first>Yi-Chien</first><last>Lin</last></author>
      <author><first>Jason S.</first><last>Chang</last></author>
      <pages>252–256</pages>
      <abstract>We introduce a method for generating error-correction rules for grammar pattern errors in a given annotated learner corpus. In our approach, annotated edits in the learner corpus are converted into edit rules for correcting common writing errors. The method involves automatic extraction of grammar patterns, and automatic alignment of the erroneous patterns and correct patterns. At run-time, <a href="https://en.wikipedia.org/wiki/Pattern_recognition">grammar patterns</a> are extracted from the grammatically correct sentences, and correction rules are retrieved by aligning the extracted <a href="https://en.wikipedia.org/wiki/Pattern_recognition">grammar patterns</a> with the erroneous patterns. Using the proposed method, we generate 1,499 high-quality correction rules related to 232 <a href="https://en.wikipedia.org/wiki/Headword">headwords</a>. The method can be used to assist <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">ESL students</a> in avoiding <a href="https://en.wikipedia.org/wiki/Grammatical_error">grammatical errors</a>, and aid teachers in correcting students’ essays. Additionally, the method can be used in the compilation of collocation error dictionaries and the construction of grammar error correction systems.</abstract>
      <url hash="fec89013">2021.rocling-1.32</url>
      <bibkey>shen-etal-2021-automatic</bibkey>
    </paper>
    <paper id="39">
      <title>Learning to Find Translation of Grammar Patterns in Parallel Corpus</title>
      <author><first>Kai-Wen</first><last>Tuan</last></author>
      <author><first>Yi-Jyun</first><last>Chen</last></author>
      <author><first>Yi-Chien</first><last>Lin</last></author>
      <author><first>Chun-Ho</first><last>Kwok</last></author>
      <author><first>Hai-Lun</first><last>Tu</last></author>
      <author><first>Jason S.</first><last>Chang</last></author>
      <pages>301–309</pages>
      <abstract>We introduce a method for assisting English as Second Language (ESL) learners by providing translations of Collins COBUILD grammar patterns(GP) for a given word. In our approach, bilingual parallel corpus is transformed into bilingual GP pairs aimed at providing native language support for learning word usage through GPs. The method involves automatically parsing sentences to extract GPs, automatically generating translation GP pairs from bilingual sentences, and automatically extracting common bilingual GPs. At run-time, the target word is used for lookup GPs and translations, and the retrieved common GPs and their example sentences are shown to the user. We present a prototype phrase search engine, Linggle GPTrans, that implements the methods to assist <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">ESL learners</a>. Preliminary evaluation on a set of more than 300 GP-translation pairs shows that the methods achieve 91 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>.</abstract>
      <url hash="eb624eb3">2021.rocling-1.39</url>
      <bibkey>tuan-etal-2021-learning</bibkey>
    </paper>
    <paper id="49">
      <title>SoochowDS at ROCLING-2021 Shared Task : Text Sentiment Analysis Using BERT and LSTM<fixed-case>S</fixed-case>oochow<fixed-case>DS</fixed-case> at <fixed-case>ROCLING</fixed-case>-2021 Shared Task: Text Sentiment Analysis Using <fixed-case>BERT</fixed-case> and <fixed-case>LSTM</fixed-case></title>
      <author><first>Ruei-Cyuan</first><last>Su</last></author>
      <author><first>Sig-Seong</first><last>Chong</last></author>
      <author><first>Tzu-En</first><last>Su</last></author>
      <author><first>Ming-Hsiang</first><last>Su</last></author>
      <pages>375–379</pages>
      <abstract>In this shared task, this paper proposes a method to combine the BERT-based word vector model and the LSTM prediction model to predict the Valence and Arousal values in the text. Among them, the BERT-based word vector is 768-dimensional, and each word vector in the sentence is sequentially fed to the LSTM model for prediction. The experimental results show that the performance of our proposed method is better than the results of the <a href="https://en.wikipedia.org/wiki/Lasso_regression">Lasso Regression model</a>.</abstract>
      <url hash="f182d649">2021.rocling-1.49</url>
      <bibkey>su-etal-2021-soochowds</bibkey>
    </paper>
    <paper id="50">
      <title>NCU-NLP at ROCLING-2021 Shared Task : Using MacBERT Transformers for Dimensional Sentiment Analysis<fixed-case>NCU</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>ROCLING</fixed-case>-2021 Shared Task: Using <fixed-case>M</fixed-case>ac<fixed-case>BERT</fixed-case> Transformers for Dimensional Sentiment Analysis</title>
      <author><first>Man-Chen</first><last>Hung</last></author>
      <author><first>Chao-Yi</first><last>Chen</last></author>
      <author><first>Pin-Jung</first><last>Chen</last></author>
      <author><first>Lung-Hao</first><last>Lee</last></author>
      <pages>380–384</pages>
      <abstract>We use the MacBERT transformers and fine-tune them to ROCLING-2021 shared tasks using the CVAT and CVAS data. We compare the performance of MacBERT with the other two transformers BERT and RoBERTa in the valence and arousal dimensions, respectively. MAE and correlation coefficient (r) were used as evaluation metrics. On ROCLING-2021 test set, our used MacBERT model achieves 0.611 of <a href="https://en.wikipedia.org/wiki/Major_histocompatibility_complex">MAE</a> and 0.904 of r in the <a href="https://en.wikipedia.org/wiki/Valence_(psychology)">valence dimensions</a> ; and 0.938 of <a href="https://en.wikipedia.org/wiki/Major_histocompatibility_complex">MAE</a> and 0.549 of r in the arousal dimension.</abstract>
      <url hash="3a90ccd4">2021.rocling-1.50</url>
      <bibkey>hung-etal-2021-ncu</bibkey>
    </paper>
    <paper id="51">
      <title>ROCLING-2021 Shared Task : <a href="https://en.wikipedia.org/wiki/Dimensional_analysis">Dimensional Sentiment Analysis</a> for Educational Texts<fixed-case>ROCLING</fixed-case>-2021 Shared Task: Dimensional Sentiment Analysis for Educational Texts</title>
      <author><first>Liang-Chih</first><last>Yu</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Bo</first><last>Peng</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <pages>385–388</pages>
      <abstract>This paper presents the ROCLING 2021 shared task on dimensional sentiment analysis for educational texts which seeks to identify a real-value sentiment score of self-evaluation comments written by Chinese students in the both valence and arousal dimensions. Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and <a href="https://en.wikipedia.org/wiki/Arousal">arousal</a> represents the degree of excitement and calm. Of the 7 teams registered for this shared task for two-dimensional sentiment analysis, 6 submitted results. We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques for the educational domain. All data sets with gold standards and scoring script are made publicly available to researchers.</abstract>
      <url hash="130a986c">2021.rocling-1.51</url>
      <bibkey>yu-etal-2021-rocling</bibkey>
    </paper>
  </volume>
</collection>