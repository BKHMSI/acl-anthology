<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.woah">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)</booktitle>
      <editor><first>Aida</first><last>Mostafazadeh Davani</last></editor>
      <editor><first>Douwe</first><last>Kiela</last></editor>
      <editor><first>Mathias</first><last>Lambert</last></editor>
      <editor><first>Bertie</first><last>Vidgen</last></editor>
      <editor><first>Vinodkumar</first><last>Prabhakaran</last></editor>
      <editor><first>Zeerak</first><last>Waseem</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="747477a4">2021.woah-1</url>
    </meta>
    <frontmatter>
      <url hash="7df1a565">2021.woah-1.0</url>
      <bibkey>woah-2021-online</bibkey>
    </frontmatter>
    <paper id="10">
      <title>Improving Counterfactual Generation for Fair Hate Speech Detection</title>
      <author><first>Aida</first><last>Mostafazadeh Davani</last></author>
      <author><first>Ali</first><last>Omrani</last></author>
      <author><first>Brendan</first><last>Kennedy</last></author>
      <author><first>Mohammad</first><last>Atari</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Morteza</first><last>Dehghani</last></author>
      <pages>92–101</pages>
      <abstract>Bias mitigation approaches reduce models’ dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features. In <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech detection</a>, however, equalizing model predictions may ignore important differences among targeted social groups, as <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a> can contain stereotypical language specific to each SGT. Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs. Our method evaluates the similarity in sentence likelihoods (via pre-trained language models) among <a href="https://en.wikipedia.org/wiki/Counterfactual_conditional">counterfactuals</a>, to treat SGTs equally only within interchangeable contexts. By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection.</abstract>
      <url hash="288ee55e">2021.woah-1.10</url>
      <doi>10.18653/v1/2021.woah-1.10</doi>
      <bibkey>mostafazadeh-davani-etal-2021-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="15">
      <title>Context Sensitivity Estimation in Toxicity Detection</title>
      <author><first>Alexandros</first><last>Xenos</last></author>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <pages>140–145</pages>
      <abstract>User posts whose perceived toxicity depends on the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">conversational context</a> are rare in current toxicity detection datasets. Hence, toxicity detectors trained on current datasets will also disregard <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a>, making the detection of context-sensitive toxicity a lot harder when it occurs. We constructed and publicly release a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 10k posts with two kinds of toxicity labels per post, obtained from annotators who considered (i) both the current post and the previous one as context, or (ii) only the current post. We introduce a new task, context-sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered. Using the new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, we show that <a href="https://en.wikipedia.org/wiki/System">systems</a> can be developed for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce additional costs.</abstract>
      <url hash="c94e092b">2021.woah-1.15</url>
      <doi>10.18653/v1/2021.woah-1.15</doi>
      <bibkey>xenos-etal-2021-context</bibkey>
    </paper>
    <paper id="18">
      <title>When the Echo Chamber Shatters : Examining the Use of Community-Specific Language Post-Subreddit Ban</title>
      <author><first>Milo</first><last>Trujillo</last></author>
      <author><first>Sam</first><last>Rosenblatt</last></author>
      <author><first>Guillermo</first><last>de Anda Jáuregui</last></author>
      <author><first>Emily</first><last>Moog</last></author>
      <author><first>Briane Paul V.</first><last>Samson</last></author>
      <author><first>Laurent</first><last>Hébert-Dufresne</last></author>
      <author><first>Allison M.</first><last>Roth</last></author>
      <pages>164–178</pages>
      <abstract>Community-level bans are a common tool against groups that enable <a href="https://en.wikipedia.org/wiki/Cyberbullying">online harassment</a> and <a href="https://en.wikipedia.org/wiki/Cyberbullying">harmful speech</a>. Unfortunately, the efficacy of community bans has only been partially studied and with mixed results. Here, we provide a flexible <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methodology</a> to identify in-group language and track user activity on <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a> both before and after the ban of a community (subreddit). We use a simple word frequency divergence to identify uncommon words overrepresented in a given <a href="https://en.wikipedia.org/wiki/Community">community</a>, not as a proxy for harmful speech but as a linguistic signature of the community. We apply our method to 15 banned subreddits, and find that community response is heterogeneous between <a href="https://en.wikipedia.org/wiki/Reddit">subreddits</a> and between users of a <a href="https://en.wikipedia.org/wiki/Reddit">subreddit</a>. Top users were more likely to become less active overall, while random users often reduced use of in-group language without decreasing activity. Finally, we find some evidence that the effectiveness of <a href="https://en.wikipedia.org/wiki/Ban_(law)">bans</a> aligns with the content of a community. Users of dark humor communities were largely unaffected by bans while users of communities organized around <a href="https://en.wikipedia.org/wiki/White_supremacy">white supremacy</a> and <a href="https://en.wikipedia.org/wiki/Fascism">fascism</a> were the most affected. Altogether, our results show that <a href="https://en.wikipedia.org/wiki/Ban_(law)">bans</a> do not affect all groups or users equally, and pave the way to understanding the effect of <a href="https://en.wikipedia.org/wiki/Ban_(law)">bans</a> across communities.</abstract>
      <url hash="018d8659">2021.woah-1.18</url>
      <attachment type="OptionalSupplementaryMaterial" hash="96cf3135">2021.woah-1.18.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.woah-1.18</doi>
      <bibkey>trujillo-etal-2021-echo</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="19">
      <title>Targets and Aspects in <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a> Hate Speech</title>
      <author><first>Alexander</first><last>Shvets</last></author>
      <author><first>Paula</first><last>Fortuna</last></author>
      <author><first>Juan</first><last>Soler</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <pages>179–190</pages>
      <abstract>Mainstream research on <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a> focused so far predominantly on the task of classifying mainly social media posts with respect to predefined typologies of rather coarse-grained hate speech categories. This may be sufficient if the goal is to detect and delete abusive language posts. However, removal is not always possible due to the legislation of a country. Also, there is evidence that <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a> can not be successfully combated by merely removing <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech posts</a> ; they should be countered by <a href="https://en.wikipedia.org/wiki/Education">education</a> and counter-narratives. For this purpose, we need to identify (i) who is the target in a given hate speech post, and (ii) what aspects (or characteristics) of the target are attributed to the target in the post. As the first approximation, we propose to adapt a generic state-of-the-art concept extraction model to the hate speech domain. The outcome of the experiments is promising and can serve as inspiration for further work on the task</abstract>
      <url hash="ff060c2c">2021.woah-1.19</url>
      <doi>10.18653/v1/2021.woah-1.19</doi>
      <bibkey>shvets-etal-2021-targets</bibkey>
      <pwccode url="https://github.com/talnupf/hatespeechtargetsaspects" additional="false">talnupf/hatespeechtargetsaspects</pwccode>
    </paper>
    <paper id="23">
      <title>Racist or Sexist Meme? Classifying Memes beyond Hateful</title>
      <author><first>Haris Bin</first><last>Zia</last></author>
      <author><first>Ignacio</first><last>Castro</last></author>
      <author><first>Gareth</first><last>Tyson</last></author>
      <pages>215–219</pages>
      <abstract>Memes are the combinations of text and images that are often humorous in nature. But, that may not always be the case, and certain combinations of <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">texts</a> and <a href="https://en.wikipedia.org/wiki/Image">images</a> may depict <a href="https://en.wikipedia.org/wiki/Hatred">hate</a>, referred to as hateful memes. This work presents a multimodal pipeline that takes both visual and textual features from <a href="https://en.wikipedia.org/wiki/Meme">memes</a> into account to (1) identify the protected category (e.g. race, sex etc.) that has been attacked ; and (2) detect the type of attack (e.g. contempt, <a href="https://en.wikipedia.org/wiki/Pejorative">slurs</a> etc.). Our <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a> uses state-of-the-art pre-trained visual and textual representations, followed by a simple <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression classifier</a>. We employ our <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a> on the Hateful Memes Challenge dataset with additional newly created fine-grained labels for protected category and type of attack. Our best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves an AUROC of 0.96 for identifying the protected category, and 0.97 for detecting the type of attack. We release our code at https://github.com/harisbinzia/HatefulMemes</abstract>
      <url hash="24ecb5e6">2021.woah-1.23</url>
      <doi>10.18653/v1/2021.woah-1.23</doi>
      <bibkey>zia-etal-2021-racist</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hateful-memes">Hateful Memes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hateful-memes-challenge">Hateful Memes Challenge</pwcdataset>
    </paper>
    </volume>
</collection>