<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.latechclfl">
  <volume id="1" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
      <editor><first>Stefania</first><last>Degaetano-Ortlieb</last></editor>
      <editor><first>Anna</first><last>Kazantseva</last></editor>
      <editor><first>Nils</first><last>Reiter</last></editor>
      <editor><first>Stan</first><last>Szpakowicz</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic (online)</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="3e1699e7">2021.latechclfl-1.0</url>
      <bibkey>latechclfl-2021-joint</bibkey>
    </frontmatter>
    <paper id="2">
      <title>FrameNet-like Annotation of Olfactory Information in Texts<fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et-like Annotation of Olfactory Information in Texts</title>
      <author><first>Sara</first><last>Tonelli</last></author>
      <author><first>Stefano</first><last>Menini</last></author>
      <pages>11–20</pages>
      <abstract>Although olfactory references play a crucial role in our <a href="https://en.wikipedia.org/wiki/Cultural_memory">cultural memory</a>, only few works in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> have tried to capture them from a computational perspective. Currently, the main challenge is not much the development of technological components for olfactory information extraction, given recent advances in <a href="https://en.wikipedia.org/wiki/Semantic_processing">semantic processing</a> and <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>, but rather the lack of a theoretical framework to capture this <a href="https://en.wikipedia.org/wiki/Information">information</a> from a linguistic point of view, as a preliminary step towards the development of automated systems. Therefore, in this work we present the annotation guidelines, developed with the help of history scholars and domain experts, aimed at capturing all the relevant elements involved in olfactory situations or events described in texts. These guidelines have been inspired by FrameNet annotation, but underwent some adaptations, which are detailed in this paper. Furthermore, we present a case study concerning the annotation of olfactory situations in English historical travel writings describing trips to <a href="https://en.wikipedia.org/wiki/Italy">Italy</a>. An analysis of the most frequent role fillers show that olfactory descriptions pertain to some typical domains such as <a href="https://en.wikipedia.org/wiki/Religion">religion</a>, <a href="https://en.wikipedia.org/wiki/Food">food</a>, <a href="https://en.wikipedia.org/wiki/Nature">nature</a>, ancient past, poor sanitation, all supporting the creation of a stereotypical imagery related to <a href="https://en.wikipedia.org/wiki/Italy">Italy</a>. On the other hand, positive feelings triggered by smells are prevalent, and contribute to framing travels to Italy as an exciting experience involving all senses.</abstract>
      <url hash="09467409">2021.latechclfl-1.2</url>
      <bibkey>tonelli-menini-2021-framenet</bibkey>
      <doi>10.18653/v1/2021.latechclfl-1.2</doi>
    </paper>
    <paper id="3">
      <title>Batavia asked for advice. Pretrained language models for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entity Recognition</a> in historical texts.</title>
      <author><first>Sophie I.</first><last>Arnoult</last></author>
      <author><first>Lodewijk</first><last>Petram</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <pages>21–30</pages>
      <abstract>Pretrained language models like <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> have advanced the state of the art for many NLP tasks. For resource-rich languages, one has the choice between a number of language-specific models, while multilingual models are also worth considering. These <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> are well known for their crosslingual performance, but have also shown competitive in-language performance on some tasks. We consider monolingual and multilingual models from the perspective of historical texts, and in particular for <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">texts</a> enriched with editorial notes : how do language models deal with the historical and editorial content in these texts? We present a new Named Entity Recognition dataset for <a href="https://en.wikipedia.org/wiki/Dutch_language">Dutch</a> based on 17th and 18th century United East India Company (VOC) reports extended with modern editorial notes. Our experiments with multilingual and Dutch pretrained language models confirm the crosslingual abilities of multilingual models while showing that all <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> can leverage mixed-variant data. In particular, <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> successfully incorporate <a href="https://en.wikipedia.org/wiki/Note_(typography)">notes</a> for the prediction of entities in <a href="https://en.wikipedia.org/wiki/History">historical texts</a>. We also find that multilingual models outperform monolingual models on our data, but that this superiority is linked to the task at hand : multilingual models lose their advantage when confronted with more semantical tasks.</abstract>
      <url hash="469f2a04">2021.latechclfl-1.3</url>
      <bibkey>arnoult-etal-2021-batavia</bibkey>
      <doi>10.18653/v1/2021.latechclfl-1.3</doi>
    </paper>
    <paper id="8">
      <title>Emotion Classification in <a href="https://en.wikipedia.org/wiki/German_language">German</a> Plays with Transformer-based Language Models Pretrained on Historical and Contemporary Language<fixed-case>G</fixed-case>erman Plays with Transformer-based Language Models Pretrained on Historical and Contemporary Language</title>
      <author><first>Thomas</first><last>Schmidt</last></author>
      <author><first>Katrin</first><last>Dennerlein</last></author>
      <author><first>Christian</first><last>Wolff</last></author>
      <pages>67–79</pages>
      <abstract>We present results of a project on emotion classification on historical German plays of <a href="https://en.wikipedia.org/wiki/Age_of_Enlightenment">Enlightenment</a>, <a href="https://en.wikipedia.org/wiki/Storm_and_Stress">Storm and Stress</a>, and <a href="https://en.wikipedia.org/wiki/German_Classicism">German Classicism</a>. We have developed a hierarchical annotation scheme consisting of 13 sub-emotions like <a href="https://en.wikipedia.org/wiki/Suffering">suffering</a>, love and joy that sum up to 6 main and 2 polarity classes (positive / negative). We have conducted <a href="https://en.wikipedia.org/wiki/Annotation">textual annotations</a> on 11 German plays and have acquired over 13,000 <a href="https://en.wikipedia.org/wiki/Annotation">emotion annotations</a> by two annotators per play. We have evaluated multiple traditional machine learning approaches as well as transformer-based models pretrained on historical and contemporary language for a single-label text sequence emotion classification for the different <a href="https://en.wikipedia.org/wiki/Emotion_classification">emotion categories</a>. The evaluation is carried out on three different instances of the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> : (1) taking all annotations, (2) filtering overlapping annotations by annotators, (3) applying a <a href="https://en.wikipedia.org/wiki/Heuristic">heuristic</a> for speech-based analysis. Best results are achieved on the filtered corpus with the best <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> being large transformer-based models pretrained on contemporary German language. For the polarity classification accuracies of up to 90 % are achieved. The accuracies become lower for settings with a higher number of classes, achieving 66 % for 13 sub-emotions. Further pretraining of a historical model with a corpus of dramatic texts led to no improvements.</abstract>
      <url hash="8f74a762">2021.latechclfl-1.8</url>
      <bibkey>schmidt-etal-2021-emotion</bibkey>
      <doi>10.18653/v1/2021.latechclfl-1.8</doi>
    </paper>
    <paper id="9">
      <title>Automating the Detection of Poetic Features : The Limerick as Model Organism</title>
      <author><first>Almas</first><last>Abdibayev</last></author>
      <author><first>Yohei</first><last>Igarashi</last></author>
      <author><first>Allen</first><last>Riddell</last></author>
      <author><first>Daniel</first><last>Rockmore</last></author>
      <pages>80–90</pages>
      <abstract>In this paper we take up the problem of limerick detection and describe a <a href="https://en.wikipedia.org/wiki/System">system</a> to identify five-line poems as <a href="https://en.wikipedia.org/wiki/Limerick_(poetry)">limericks</a> or not. This turns out to be a surprisingly difficult challenge with many subtleties. More precisely, we produce an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> which focuses on the structural aspects of the <a href="https://en.wikipedia.org/wiki/Limerick_(poetry)">limerick   rhyme scheme</a> and <a href="https://en.wikipedia.org/wiki/Rhythm">rhythm</a> (i.e., stress patterns)   and when tested on a a culled data set of 98,454 publicly available <a href="https://en.wikipedia.org/wiki/Limerick_(poetry)">limericks</a>, our limerick filter accepts 67 % as <a href="https://en.wikipedia.org/wiki/Limerick_(poetry)">limericks</a>. The primary failure of our <a href="https://en.wikipedia.org/wiki/Filter_(signal_processing)">filter</a> is on the detection of non-standard rhymes, which we highlight as an outstanding challenge in computational poetics. Our accent detection algorithm proves to be very robust. Our main contributions are (1) a novel rhyme detection algorithm that works on English words including rare proper nouns and made-up words (and thus, words not in the widely used CMUDict database) ; (2) a novel rhythm-identifying heuristic that is robust to language noise at moderate levels and comparable in accuracy to state-of-the-art scansion algorithms. As a third significant contribution (3) we make publicly available a large corpus of <a href="https://en.wikipedia.org/wiki/Limerick_(poetry)">limericks</a> that includes tags of limerick or not-limerick as determined by our identification software, thereby providing a benchmark for the community. The poetic tasks that we have identified as challenges for machines suggest that the <a href="https://en.wikipedia.org/wiki/Limerick_(poetry)">limerick</a> is a useful model organism for the study of machine capabilities in <a href="https://en.wikipedia.org/wiki/Poetry">poetry</a> and more broadly literature and language. We include a list of open challenges as well.</abstract>
      <url hash="d4fe781d">2021.latechclfl-1.9</url>
      <bibkey>abdibayev-etal-2021-automating</bibkey>
      <doi>10.18653/v1/2021.latechclfl-1.9</doi>
    </paper>
    <paper id="12">
      <title>Translationese in Russian Literary Texts<fixed-case>R</fixed-case>ussian Literary Texts</title>
      <author><first>Maria</first><last>Kunilovskaya</last></author>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>101–112</pages>
      <abstract>The paper reports the results of a translationese study of <a href="https://en.wikipedia.org/wiki/Literature">literary texts</a> based on translated and non-translated Russian. We aim to find out if <a href="https://en.wikipedia.org/wiki/Translation">translations</a> deviate from non-translated literary texts, and if the established differences can be attributed to <a href="https://en.wikipedia.org/wiki/Linguistic_typology">typological relations</a> between source and target languages. We expect that literary translations from typologically distant languages should exhibit more translationese, and the fingerprints of individual source languages (and their families) are traceable in translations. We explore linguistic properties that distinguish non-translated Russian literature from translations into <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a>. Our results show that non-translated fiction is different from <a href="https://en.wikipedia.org/wiki/Translation">translations</a> to the degree that these two <a href="https://en.wikipedia.org/wiki/Variety_(linguistics)">language varieties</a> can be automatically classified. As expected, <a href="https://en.wikipedia.org/wiki/Linguistic_typology">language typology</a> is reflected in translations of literary texts. We identified features that point to linguistic specificity of Russian non-translated literature and to shining-through effects. Some of translationese features cut across all language pairs, while others are characteristic of literary translations from languages belonging to specific language families.</abstract>
      <url hash="b3e4c2f2">2021.latechclfl-1.12</url>
      <bibkey>kunilovskaya-etal-2021-translationese</bibkey>
      <doi>10.18653/v1/2021.latechclfl-1.12</doi>
    </paper>
    <paper id="15">
      <title>A Pilot Study for BERT Language Modelling and <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">Morphological Analysis</a> for Ancient and Medieval Greek<fixed-case>BERT</fixed-case> Language Modelling and Morphological Analysis for Ancient and Medieval <fixed-case>G</fixed-case>reek</title>
      <author><first>Pranaydeep</first><last>Singh</last></author>
      <author><first>Gorik</first><last>Rutten</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <pages>128–137</pages>
      <abstract>This paper presents a pilot study to automatic linguistic preprocessing of <a href="https://en.wikipedia.org/wiki/Ancient_Greek">Ancient and Byzantine Greek</a>, and <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological analysis</a> more specifically. To this end, a novel subword-based BERT language model was trained on the basis of a varied corpus of Modern, Ancient and Post-classical Greek texts. Consequently, the obtained BERT embeddings were incorporated to train a fine-grained Part-of-Speech tagger for Ancient and Byzantine Greek. In addition, a corpus of Greek Epigrams was manually annotated and the resulting gold standard was used to evaluate the performance of the <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological analyser</a> on <a href="https://en.wikipedia.org/wiki/Medieval_Greek">Byzantine Greek</a>. The experimental results show very good perplexity scores (4.9) for the BERT language model and state-of-the-art performance for the fine-grained Part-of-Speech tagger for in-domain data (treebanks containing a mixture of Classical and Medieval Greek), as well as for the newly created Byzantine Greek gold standard data set. The language models and associated code are made available for use at https://github.com/pranaydeeps/Ancient-Greek-BERT</abstract>
      <url hash="864c05aa">2021.latechclfl-1.15</url>
      <bibkey>singh-etal-2021-pilot</bibkey>
      <doi>10.18653/v1/2021.latechclfl-1.15</doi>
      <pwccode url="https://github.com/pranaydeeps/ancient-greek-bert" additional="false">pranaydeeps/ancient-greek-bert</pwccode>
    </paper>
    <paper id="16">
      <title>Zero-Shot Information Extraction to Enhance a <a href="https://en.wikipedia.org/wiki/Knowledge_graph">Knowledge Graph</a> Describing Silk Textiles</title>
      <author><first>Thomas</first><last>Schleider</last></author>
      <author><first>Raphael</first><last>Troncy</last></author>
      <pages>138–146</pages>
      <abstract>The knowledge of the European silk textile production is a typical case for which the information collected is heterogeneous, spread across many museums and sparse since rarely complete. Knowledge Graphs for this cultural heritage domain, when being developed with appropriate <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontologies</a> and vocabularies, enable to integrate and reconcile this diverse information. However, many of these original <a href="https://en.wikipedia.org/wiki/Collection_(artwork)">museum records</a> still have some metadata gaps. In this paper, we present a zero-shot learning approach that leverages the ConceptNet common sense knowledge graph to predict categorical metadata informing about the silk objects production. We compared the performance of our approach with traditional supervised deep learning-based methods that do require <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data</a>. We demonstrate promising and competitive performance for similar datasets and circumstances and the ability to predict sometimes more fine-grained information. Our results can be reproduced using the code and datasets published at https://github.com/silknow/ZSL-KG-silk.</abstract>
      <url hash="603bc3b6">2021.latechclfl-1.16</url>
      <bibkey>schleider-troncy-2021-zero</bibkey>
      <doi>10.18653/v1/2021.latechclfl-1.16</doi>
      <pwccode url="https://github.com/silknow/zsl-kg-silk" additional="false">silknow/zsl-kg-silk</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="19">
      <title>Period Classification in Chinese Historical Texts<fixed-case>C</fixed-case>hinese Historical Texts</title>
      <author><first>Zuoyu</first><last>Tian</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <pages>168–177</pages>
      <abstract>In this study, we study <a href="https://en.wikipedia.org/wiki/Language_change">language change</a> in Chinese Biji by using a classification task : classifying Ancient Chinese texts by time periods. Specifically, we focus on a unique genre in <a href="https://en.wikipedia.org/wiki/Classical_Chinese_literature">classical Chinese literature</a> : <a href="https://en.wikipedia.org/wiki/Biji_(Chinese_literature)">Biji</a> (literally notebook or brush notes), i.e., collections of anecdotes, quotations, etc., anything authors consider noteworthy, <a href="https://en.wikipedia.org/wiki/Biji_(Chinese_literature)">Biji</a> span hundreds of years across many dynasties and conserve informal language in written form. For these reasons, they are regarded as a good resource for investigating <a href="https://en.wikipedia.org/wiki/Language_change">language change</a> in <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a> (Fang, 2010). In this paper, we create a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 108 Biji across four dynasties. Based on the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, we first introduce a time period classification task for <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. Then we investigate different <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">feature representation methods</a> for <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>. The results show that <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> using contextualized embeddings perform best. An analysis of the top <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> chosen by the word n-gram model (after bleaching proper nouns) confirms that these <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> are informative and correspond to observations and assumptions made by historical linguists.</abstract>
      <url hash="b030a9f1">2021.latechclfl-1.19</url>
      <bibkey>tian-kubler-2021-period</bibkey>
      <doi>10.18653/v1/2021.latechclfl-1.19</doi>
    </paper>
    <paper id="21">
      <title>Stylometric Literariness Classification : the Case of Stephen King</title>
      <author><first>Andreas</first><last>van Cranenburgh</last></author>
      <author><first>Erik</first><last>Ketzan</last></author>
      <pages>189–197</pages>
      <abstract>This paper applies <a href="https://en.wikipedia.org/wiki/Stylometry">stylometry</a> to quantify the literariness of 73 novels and novellas by American author Stephen King, chosen as an extraordinary case of a writer who has been dubbed both high and low in literariness in critical reception. We operationalize <a href="https://en.wikipedia.org/wiki/Literariness">literariness</a> using a measure of stylistic distance (Cosine Delta) based on the 1000 most frequent words in two bespoke comparison corpora used as proxies for <a href="https://en.wikipedia.org/wiki/Literariness">literariness</a> : one of popular genre fiction, another of National Book Award-winning authors. We report that a <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised model</a> is highly effective in distinguishing the two categories, with 94.6 % macro average in a <a href="https://en.wikipedia.org/wiki/Binary_classification">binary classification</a>. We define two subsets of texts by Kinghigh and low literariness works as suggested by critics and ourselvesand find that a predictive model does identify King’s Dark Tower series and novels such as Dolores Claiborne as among his most literary texts, consistent with critical reception, which has also ascribed postmodern qualities to the Dark Tower novels. Our results demonstrate the efficacy of Cosine Delta-based stylometry in quantifying the literariness of texts, while also highlighting the methodological challenges of <a href="https://en.wikipedia.org/wiki/Literariness">literariness</a>, especially in the case of <a href="https://en.wikipedia.org/wiki/Stephen_King">Stephen King</a>. The code and data to reproduce our results are available at https://github.com/andreasvc/kinglit</abstract>
      <url hash="7b93ab8a">2021.latechclfl-1.21</url>
      <bibkey>van-cranenburgh-ketzan-2021-stylometric</bibkey>
      <doi>10.18653/v1/2021.latechclfl-1.21</doi>
      <pwccode url="https://github.com/andreasvc/kinglit" additional="false">andreasvc/kinglit</pwccode>
    </paper>
  </volume>
</collection>