<?xml version='1.0' encoding='utf-8'?>
<collection id="2018.lilt">
  <volume id="16">
    <meta>
      <booktitle>Linguistic Issues in Language Technology, Volume 16, 2018</booktitle>
      <publisher>CSLI Publications</publisher>
      <year>2018</year>
      <month>July</month>
    </meta>
    <paper id="1">
      <title>Can <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural Networks</a> Learn Nested Recursion?</title>
      <author><first>Jean-Phillipe</first><last>Bernardy</last></author>
      <abstract>Context-free grammars (CFG) were one of the first formal tools used to model <a href="https://en.wikipedia.org/wiki/Natural_language">natural languages</a>, and they remain relevant today as the basis of several frameworks. A key ingredient of CFG is the presence of nested recursion. In this paper, we investigate experimentally the capability of several <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks (RNNs)</a> to learn nested recursion. More precisely, we measure an upper bound of their capability to do so, by simplifying the task to learning a generalized Dyck language, namely one composed of matching parentheses of various kinds. To do so, we present the RNNs with a set of random strings having a given maximum nesting depth and test its ability to predict the kind of closing parenthesis when facing deeper nested strings. We report mixed results : when generalizing to deeper nesting levels, the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of standard RNNs is significantly higher than <a href="https://en.wikipedia.org/wiki/Randomness">random</a>, but still far from perfect. Additionally, we propose some non-standard stack-based models which can approach perfect <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, at the cost of <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a>.</abstract>
      <issue>1</issue>
      <url hash="a5d8242e">2018.lilt-16.1</url>
      <bibkey>bernardy-2018-recurrent</bibkey>
    </paper>
  </volume>
</collection>