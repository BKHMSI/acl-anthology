<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.sigdial">
  <volume id="1" ingest-date="2021-08-12">
    <meta>
      <booktitle>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</booktitle>
      <editor><first>Haizhou</first><last>Li</last></editor>
      <editor><first>Gina-Anne</first><last>Levow</last></editor>
      <editor><first>Zhou</first><last>Yu</last></editor>
      <editor><first>Chitralekha</first><last>Gupta</last></editor>
      <editor><first>Berrak</first><last>Sisman</last></editor>
      <editor><first>Siqi</first><last>Cai</last></editor>
      <editor><first>David</first><last>Vandyke</last></editor>
      <editor><first>Nina</first><last>Dethlefs</last></editor>
      <editor><first>Yan</first><last>Wu</last></editor>
      <editor><first>Junyi Jessy</first><last>Li</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore and Online</address>
      <month>July</month>
      <year>2021</year>
      <url hash="605ef8f6">2021.sigdial-1</url>
    </meta>
    <frontmatter>
      <url hash="c6cff5ff">2021.sigdial-1.0</url>
      <bibkey>sigdial-2021-special</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Individual Interaction Styles : Evidence from a Spoken Chat Corpus</title>
      <author><first>Nigel</first><last>Ward</last></author>
      <pages>27–31</pages>
      <abstract>here is increasing interest in modeling style choices in <a href="https://en.wikipedia.org/wiki/Dialogue">dialog</a>, for example for enabling <a href="https://en.wikipedia.org/wiki/Dialogue">dialog systems</a> to adapt to their users. It is commonly assumed that each user has his or her own stable characteristics, but for interaction style the truth of this assumption has not been well examined. I investigated using a vector-space model of interaction styles, derived from the Switchboard corpus of telephone conversations and a broad set of prosodic-behavior features. While most individuals exhibited interaction style tendencies, these were generally far from stable, with a <a href="https://en.wikipedia.org/wiki/Predictive_modelling">predictive model</a> based on individual tendencies outperforming a speaker-independent model by only 3.6 %. The tendencies were somewhat stronger for some speakers, including generally males, and for some dimensions of variation.</abstract>
      <url hash="269878c8">2021.sigdial-1.4</url>
      <bibkey>ward-2021-individual</bibkey>
      <video href="https://www.youtube.com/watch?v=cSNGdDL-MVY" />
    </paper>
    <paper id="6">
      <title>Improving Named Entity Recognition in Spoken Dialog Systems by Context and Speech Pattern Modeling</title>
      <author><first>Minh</first><last>Nguyen</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>45–55</pages>
      <abstract>While named entity recognition (NER) from <a href="https://en.wikipedia.org/wiki/Speech">speech</a> has been around as long as NER from written text has, the accuracy of NER from <a href="https://en.wikipedia.org/wiki/Speech">speech</a> has generally been much lower than that of NER from <a href="https://en.wikipedia.org/wiki/Written_language">text</a>. The rise in popularity of spoken dialog systems such as <a href="https://en.wikipedia.org/wiki/Siri">Siri</a> or <a href="https://en.wikipedia.org/wiki/Amazon_Alexa">Alexa</a> highlights the need for more accurate <a href="https://en.wikipedia.org/wiki/Natural_language_understanding">NER</a> from speech because <a href="https://en.wikipedia.org/wiki/Natural_language_understanding">NER</a> is a core component for understanding what users said in dialogs. Deployed spoken dialog systems receive user input in the form of automatic speech recognition (ASR) transcripts, and simply applying NER model trained on written text to ASR transcripts often leads to low accuracy because compared to written text, ASR transcripts lack important cues such as <a href="https://en.wikipedia.org/wiki/Punctuation">punctuation</a> and <a href="https://en.wikipedia.org/wiki/Capitalization">capitalization</a>. Besides, errors in ASR transcripts also make <a href="https://en.wikipedia.org/wiki/Near-infrared_spectroscopy">NER</a> from <a href="https://en.wikipedia.org/wiki/Speech">speech</a> challenging. We propose two models that exploit dialog context and speech pattern clues to extract named entities more accurately from open-domain dialogs in spoken dialog systems. Our results show the benefit of modeling dialog context and speech patterns in two settings : a standard setting with random partition of data and a more realistic but also more difficult setting where many named entities encountered during deployment are unseen during training.</abstract>
      <url hash="ab415bae">2021.sigdial-1.6</url>
      <bibkey>nguyen-yu-2021-improving</bibkey>
      <video href="https://www.youtube.com/watch?v=JIGvcylPvPI" />
    </paper>
    <paper id="7">
      <title>SoDA : On-device Conversational Slot Extraction<fixed-case>S</fixed-case>o<fixed-case>DA</fixed-case>: On-device Conversational Slot Extraction</title>
      <author><first>Sujith</first><last>Ravi</last></author>
      <author><first>Zornitsa</first><last>Kozareva</last></author>
      <pages>56–65</pages>
      <abstract>We propose a novel on-device neural sequence labeling model which uses embedding-free projections and character information to construct compact word representations to learn a sequence model using a combination of bidirectional LSTM with self-attention and CRF. Unlike typical dialog models that rely on huge, complex neural network architectures and large-scale pre-trained Transformers to achieve state-of-the-art results, our method achieves comparable results to BERT and even outperforms its smaller variant DistilBERT on conversational slot extraction tasks. Our method is faster than BERT models while achieving significant model size reductionour model requires 135x and 81x fewer model parameters than BERT and DistilBERT, respectively. We conduct experiments on multiple conversational datasets and show significant improvements over existing methods including recent on-device models. Experimental results and ablation studies also show that our neural models preserve tiny memory footprint necessary to operate on smart devices, while still maintaining high performance.</abstract>
      <url hash="6d8b59c4">2021.sigdial-1.7</url>
      <bibkey>ravi-kozareva-2021-soda</bibkey>
      <video href="https://www.youtube.com/watch?v=0hDaafkctwI" />
    </paper>
    <paper id="9">
      <title>ARTA : Collection and Classification of Ambiguous Requests and Thoughtful Actions<fixed-case>ARTA</fixed-case>: Collection and Classification of Ambiguous Requests and Thoughtful Actions</title>
      <author><first>Shohei</first><last>Tanaka</last></author>
      <author><first>Koichiro</first><last>Yoshino</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>77–88</pages>
      <abstract>Human-assisting systems such as dialogue systems must take thoughtful, appropriate actions not only for clear and unambiguous user requests, but also for ambiguous user requests, even if the users themselves are not aware of their potential requirements. To construct such a dialogue agent, we collected a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> and developed a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> that classifies ambiguous user requests into corresponding system actions. In order to collect a high-quality corpus, we asked workers to input antecedent user requests whose pre-defined actions could be regarded as thoughtful. Although multiple actions could be identified as thoughtful for a single <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol">user request</a>, annotating all combinations of <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol">user requests</a> and <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol">system actions</a> is impractical. For this reason, we fully annotated only the test data and left the annotation of the training data incomplete. In order to train the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification model</a> on such training data, we applied the positive / unlabeled (PU) learning method, which assumes that only a part of the data is labeled with positive examples. The experimental results show that the PU learning method achieved better performance than the general positive / negative (PN) learning method to classify thoughtful actions given an ambiguous user request.</abstract>
      <url hash="8119cb08">2021.sigdial-1.9</url>
      <bibkey>tanaka-etal-2021-arta</bibkey>
      <video href="https://www.youtube.com/watch?v=Y4OAaQzoIhA" />
      <pwccode url="https://github.com/ahclab/arta_corpus" additional="false">ahclab/arta_corpus</pwccode>
    </paper>
    <paper id="14">
      <title>Velocidapter : Task-oriented Dialogue Comprehension Modeling Pairing Synthetic Text Generation with Domain Adaptation</title>
      <author><first>Ibrahim Taha</first><last>Aksu</last></author>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>133–143</pages>
      <abstract>We introduce a synthetic dialogue generation framework, Velocidapter, which addresses the corpus availability problem for dialogue comprehension. Velocidapter augments datasets by simulating synthetic conversations for a task-oriented dialogue domain, requiring a small amount of bootstrapping work for each new domain. We evaluate the efficacy of our framework on a task-oriented dialogue comprehension dataset, MRCWOZ, which we curate by annotating questions for slots in the restaurant, taxi, and hotel domains of the MultiWOZ 2.2 dataset (Zang et al., 2020). We run experiments within a low-resource setting, where we pretrain a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on SQuAD, fine-tuning it on either a small original data or on the synthetic data generated by our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a>. Velocidapter shows significant improvements using both the transformer-based BERTBase and BiDAF as base models. We further show that the <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> is easy to use by novice users and conclude that Velocidapter can greatly help training over task-oriented dialogues, especially for low-resourced emerging domains.</abstract>
      <url hash="88cab37f">2021.sigdial-1.14</url>
      <bibkey>aksu-etal-2021-velocidapter</bibkey>
      <video href="https://www.youtube.com/watch?v=2BkbrFFGTFA" />
      <pwccode url="https://github.com/cuthalionn/velocidapter" additional="false">cuthalionn/velocidapter</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="16">
      <title>A Simple yet Effective Method for Sentence Ordering</title>
      <author><first>Aili</first><last>Shen</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>154–160</pages>
      <abstract>Sentence ordering is the task of arranging a given bag of sentences so as to maximise the coherence of the overall text. In this work, we propose a simple yet effective <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training method</a> that improves the capacity of models to capture overall text coherence based on training over pairs of sentences / segments. Experimental results show the superiority of our proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> in in- and cross-domain settings. The utility of our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> is also verified over a multi-document summarisation task.</abstract>
      <url hash="e2aab5e3">2021.sigdial-1.16</url>
      <bibkey>shen-baldwin-2021-simple</bibkey>
      <video href="https://www.youtube.com/watch?v=HcurPPqHcrY" />
    </paper>
    <paper id="18">
      <title>Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair Coherence Scoring</title>
      <author><first>Linzi</first><last>Xing</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>167–177</pages>
      <abstract>Dialogue topic segmentation is critical in several dialogue modeling problems. However, popular <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised approaches</a> only exploit surface features in assessing topical coherence among utterances. In this work, we address this limitation by leveraging supervisory signals from the utterance-pair coherence scoring task. First, we present a simple yet effective strategy to generate a <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training corpus</a> for utterance-pair coherence scoring. Then, we train a BERT-based neural utterance-pair coherence model with the obtained <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training corpus</a>. Finally, such <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is used to measure the topical relevance between utterances, acting as the basis of the segmentation inference. Experiments on three public datasets in <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a> demonstrate that our proposal outperforms the state-of-the-art baselines.</abstract>
      <url hash="284d742f">2021.sigdial-1.18</url>
      <bibkey>xing-carenini-2021-improving</bibkey>
      <video href="https://www.youtube.com/watch?v=04Urc5LRBlk" />
      <pwccode url="https://github.com/lxing532/Dialogue-Topic-Segmenter" additional="false">lxing532/Dialogue-Topic-Segmenter</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial-1">Doc2Dial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial">doc2dial</pwcdataset>
    </paper>
    <paper id="21">
      <title>Contrastive Response Pairs for Automatic Evaluation of Non-task-oriented Neural Conversational Models</title>
      <author><first>Koshiro</first><last>Okano</last></author>
      <author><first>Yu</first><last>Suzuki</last></author>
      <author><first>Masaya</first><last>Kawamura</last></author>
      <author><first>Tsuneo</first><last>Kato</last></author>
      <author><first>Akihiro</first><last>Tamura</last></author>
      <author><first>Jianming</first><last>Wu</last></author>
      <pages>202–207</pages>
      <abstract>Responses generated by neural conversational models (NCMs) for non-task-oriented systems are difficult to evaluate. We propose contrastive response pairs (CRPs) for automatically evaluating responses from non-task-oriented NCMs. We conducted an error analysis on responses generated by an encoder-decoder recurrent neural network (RNN) type NCM and created three types of CRPs corresponding to the three most frequent errors found in the analysis. Three NCMs of different response quality were objectively evaluated with the CRPs and compared to a subjective assessment. The correctness obtained by the three types of CRPs were consistent with the results of the subjective assessment.</abstract>
      <url hash="6ccd3ce7">2021.sigdial-1.21</url>
      <bibkey>okano-etal-2021-contrastive</bibkey>
      <video href="https://www.youtube.com/watch?v=Wtma3lm9AMc" />
    </paper>
    <paper id="25">
      <title>Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems : A Survey</title>
      <author><first>Vevake</first><last>Balaraman</last></author>
      <author><first>Seyedmostafa</first><last>Sheikhalishahi</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <pages>239–251</pages>
      <abstract>This paper aims at providing a comprehensive overview of recent developments in dialogue state tracking (DST) for task-oriented conversational systems. We introduce the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, the main <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> that have been exploited as well as their evaluation metrics, and we analyze several proposed approaches. We distinguish between static ontology DST models, which predict a fixed set of dialogue states, and dynamic ontology models, which can predict dialogue states even when the <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a> changes. We also discuss the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s ability to track either single or multiple domains and to scale to new domains, both in terms of <a href="https://en.wikipedia.org/wiki/Knowledge_transfer">knowledge transfer</a> and zero-shot learning. We cover a period from 2013 to 2020, showing a significant increase of multiple domain methods, most of them utilizing pre-trained language models.</abstract>
      <url hash="6722fe65">2021.sigdial-1.25</url>
      <bibkey>balaraman-etal-2021-recent</bibkey>
      <video href="https://www.youtube.com/watch?v=zQuaI9czmJk" />
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="26">
      <title>Scikit-talk : A toolkit for processing real-world conversational speech data<fixed-case>S</fixed-case>cikit-talk: A toolkit for processing real-world conversational speech data</title>
      <author><first>Andreas</first><last>Liesenfeld</last></author>
      <author><first>Gabor</first><last>Parti</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <pages>252–256</pages>
      <abstract>We present Scikit-talk, an open-source toolkit for processing collections of real-world conversational speech in <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a>. First of its kind, the <a href="https://en.wikipedia.org/wiki/List_of_toolkits">toolkit</a> equips those interested in studying or modeling conversations with an easy-to-use interface to build and explore large collections of transcriptions and annotations of talk-in-interaction. Designed for applications in <a href="https://en.wikipedia.org/wiki/Speech_processing">speech processing</a> and Conversational AI, Scikit-talk provides tools to custom-build datasets for tasks such as intent prototyping, dialog flow testing, and conversation design. Its preprocessor module comes with several pre-built interfaces for common transcription formats, which aim to make working across multiple data sources more accessible. The explorer module provides a collection of tools to explore and analyse this data type via <a href="https://en.wikipedia.org/wiki/String_matching">string matching</a> and unsupervised machine learning techniques. Scikit-talk serves as a platform to collect and connect different transcription formats and representations of talk, enabling the user to quickly build multilingual datasets of varying detail and granularity. Thus, the toolkit aims to make working with authentic conversational speech data in <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a> more accessible and to provide the user with comprehensive options to work with representations of talk in appropriate detail for any downstream task. For the latest updates and information on currently supported languages and language resources, please refer to : https://pypi.org/project/scikit-talk/<i>preprocessor</i> module comes with several pre-built interfaces for common transcription formats, which aim to make working across multiple data sources more accessible. The <i>explorer</i> module provides a collection of tools to explore and analyse this data type via string matching and unsupervised machine learning techniques. Scikit-talk serves as a platform to collect and connect different transcription formats and representations of talk, enabling the user to quickly build multilingual datasets of varying detail and granularity. Thus, the toolkit aims to make working with authentic conversational speech data in Python more accessible and to provide the user with comprehensive options to work with representations of talk in appropriate detail for any downstream task. For the latest updates and information on currently supported languages and language resources, please refer to: https://pypi.org/project/scikit-talk/</abstract>
      <url hash="c9f37c38">2021.sigdial-1.26</url>
      <bibkey>liesenfeld-etal-2021-scikit</bibkey>
      <video href="https://www.youtube.com/watch?v=yNtYLKCo3xI" />
    </paper>
    <paper id="31">
      <title>Summarizing Behavioral Change Goals from SMS Exchanges to Support Health Coaches<fixed-case>SMS</fixed-case> Exchanges to Support Health Coaches</title>
      <author><first>Itika</first><last>Gupta</last></author>
      <author><first>Barbara</first><last>Di Eugenio</last></author>
      <author><first>Brian D.</first><last>Ziebart</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Ben S.</first><last>Gerber</last></author>
      <author><first>Lisa K.</first><last>Sharp</last></author>
      <pages>276–289</pages>
      <abstract>Regular physical activity is associated with a reduced risk of chronic diseases such as type 2 diabetes and improved <a href="https://en.wikipedia.org/wiki/Mental_health">mental well-being</a>. Yet, more than half of the US population is insufficiently active. Health coaching has been successful in promoting healthy behaviors. In this paper, we present our work towards assisting <a href="https://en.wikipedia.org/wiki/Coaching">health coaches</a> by extracting the physical activity goal the user and coach negotiate via <a href="https://en.wikipedia.org/wiki/Text_messaging">text messages</a>. We show that information captured by dialogue acts can help to improve the goal extraction results. We employ both traditional and transformer-based machine learning models for dialogue acts prediction and find them statistically indistinguishable in performance on our health coaching dataset. Moreover, we discuss the feedback provided by the health coaches when evaluating the correctness of the extracted goal summaries. This work is a step towards building a virtual assistant health coach to promote a healthy lifestyle.</abstract>
      <url hash="99e5467b">2021.sigdial-1.31</url>
      <bibkey>gupta-etal-2021-summarizing</bibkey>
      <video href="https://www.youtube.com/watch?v=0FxAJvs93WA" />
    </paper>
    <paper id="33">
      <title>CIDER : Commonsense Inference for Dialogue Explanation and Reasoning<fixed-case>CIDER</fixed-case>: Commonsense Inference for Dialogue Explanation and Reasoning</title>
      <author><first>Deepanway</first><last>Ghosal</last></author>
      <author><first>Pengfei</first><last>Hong</last></author>
      <author><first>Siqi</first><last>Shen</last></author>
      <author><first>Navonil</first><last>Majumder</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>301–313</pages>
      <abstract>Commonsense inference to understand and explain <a href="https://en.wikipedia.org/wiki/Human_language">human language</a> is a fundamental research problem in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. Explaining human conversations poses a great challenge as it requires contextual understanding, planning, <a href="https://en.wikipedia.org/wiki/Inference">inference</a>, and several aspects of <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a> including causal, temporal, and commonsense reasoning. In this work, we introduce CIDER   a manually curated dataset that contains dyadic dialogue explanations in the form of implicit and explicit knowledge triplets inferred using contextual commonsense inference. Extracting such rich explanations from <a href="https://en.wikipedia.org/wiki/Conversation">conversations</a> can be conducive to improving several downstream <a href="https://en.wikipedia.org/wiki/Application_software">applications</a>. The annotated triplets are categorized by the type of <a href="https://en.wikipedia.org/wiki/Common_knowledge_(logic)">commonsense knowledge</a> present (e.g., causal, conditional, temporal). We set up three different tasks conditioned on the annotated dataset : Dialogue-level Natural Language Inference, Span Extraction, and Multi-choice Span Selection. Baseline results obtained with transformer-based models reveal that the tasks are difficult, paving the way for promising future research. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and the baseline implementations are publicly available at https://github.com/declare-lab/CIDER.</abstract>
      <url hash="ec76dd0d">2021.sigdial-1.33</url>
      <bibkey>ghosal-etal-2021-cider</bibkey>
      <video href="https://www.youtube.com/watch?v=vSNq0OOGRc0" />
      <pwccode url="https://github.com/declare-lab/CIDER" additional="false">declare-lab/CIDER</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glucose">GLUCOSE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mutual">MuTual</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
    </paper>
    <paper id="37">
      <title>How Should Agents Ask Questions For <a href="https://en.wikipedia.org/wiki/Situated_learning">Situated Learning</a>? An Annotated Dialogue Corpus</title>
      <author><first>Felix</first><last>Gervits</last></author>
      <author><first>Antonio</first><last>Roque</last></author>
      <author><first>Gordon</first><last>Briggs</last></author>
      <author><first>Matthias</first><last>Scheutz</last></author>
      <author><first>Matthew</first><last>Marge</last></author>
      <pages>353–359</pages>
      <abstract>Intelligent agents that are confronted with novel concepts in situated environments will need to ask their human teammates questions to learn about the <a href="https://en.wikipedia.org/wiki/Universe">physical world</a>. To better understand this problem, we need data about asking questions in situated task-based interactions. To this end, we present the Human-Robot Dialogue Learning (HuRDL) Corpus-a novel dialogue corpus collected in an online interactive virtual environment in which human participants play the role of a robot performing a collaborative tool-organization task. We describe the <a href="https://en.wikipedia.org/wiki/Corpus_linguistics">corpus data</a> and a corresponding <a href="https://en.wikipedia.org/wiki/Annotation">annotation scheme</a> to offer insight into the form and content of questions that humans ask to facilitate learning in a situated environment. We provide the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> as an empirically-grounded resource for improving question generation in situated intelligent agents.</abstract>
      <url hash="c1163e34">2021.sigdial-1.37</url>
      <bibkey>gervits-etal-2021-agents</bibkey>
      <video href="https://www.youtube.com/watch?v=9IAwjDa0Wp0" />
      <pwccode url="https://github.com/USArmyResearchLab/ARL-HuRDL" additional="false">USArmyResearchLab/ARL-HuRDL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hurdl">HuRDL</pwcdataset>
    </paper>
    <paper id="40">
      <title>What to Fact-Check : Guiding Check-Worthy Information Detection in News Articles through Argumentative Discourse Structure</title>
      <author><first>Tariq</first><last>Alhindi</last></author>
      <author><first>Brennan</first><last>McManus</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>380–391</pages>
      <abstract>Most existing <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for automatic fact-checking start with a precompiled list of claims to verify. We investigate the understudied problem of determining what statements in <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> are worthy to fact-check. We annotate the <a href="https://en.wikipedia.org/wiki/Argument_structure">argument structure</a> of 95 <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> in the <a href="https://en.wikipedia.org/wiki/Global_warming">climate change domain</a> that are fact-checked by climate scientists at climatefeedback.org. We release the first multi-layer annotated corpus for both argumentative discourse structure (argument types and relations) and for fact-checked statements in news articles. We discuss the connection between <a href="https://en.wikipedia.org/wiki/Argument_structure">argument structure</a> and check-worthy statements and develop several baseline models for detecting check-worthy statements in the <a href="https://en.wikipedia.org/wiki/Climate_change_modeling">climate change domain</a>. Our preliminary results show that using information about argumentative discourse structure shows slight but statistically significant improvement over a baseline of local discourse structure.</abstract>
      <url hash="c1a4cf3d">2021.sigdial-1.40</url>
      <bibkey>alhindi-etal-2021-fact</bibkey>
      <video href="https://www.youtube.com/watch?v=oBT795ipFFM" />
    </paper>
    <paper id="41">
      <title>How open are the conversations with open-domain chatbots? A proposal for Speech Event based evaluation</title>
      <author><first>A. Seza</first><last>Doğruöz</last></author>
      <author><first>Gabriel</first><last>Skantze</last></author>
      <pages>392–402</pages>
      <abstract>Open-domain chatbots are supposed to converse freely with humans without being restricted to a topic, task or domain. However, the boundaries and/or contents of open-domain conversations are not clear. To clarify the boundaries of openness, we conduct two studies : First, we classify the types of <a href="https://en.wikipedia.org/wiki/Speech">speech events</a> encountered in a chatbot evaluation data set (i.e., Meena by Google) and find that these conversations mainly cover the small talk category and exclude the other speech event categories encountered in real life human-human communication. Second, we conduct a small-scale pilot study to generate <a href="https://en.wikipedia.org/wiki/Online_chat">online conversations</a> covering a wider range of <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech event categories</a> between two humans vs. a human and a state-of-the-art <a href="https://en.wikipedia.org/wiki/Chatbot">chatbot</a> (i.e., Blender by Facebook). A human evaluation of these generated <a href="https://en.wikipedia.org/wiki/Conversation">conversations</a> indicates a preference for human-human conversations, since the human-chatbot conversations lack coherence in most speech event categories. Based on these results, we suggest (a) using the term <a href="https://en.wikipedia.org/wiki/Small_talk">small talk</a> instead of <a href="https://en.wikipedia.org/wiki/Open_domain">open-domain</a> for the current <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a> which are not that open in terms of conversational abilities yet, and (b) revising the evaluation methods to test the chatbot conversations against other speech events.</abstract>
      <url hash="aa2871ba">2021.sigdial-1.41</url>
      <bibkey>dogruoz-skantze-2021-open</bibkey>
      <video href="https://www.youtube.com/watch?v=bYXcZg_VWiE" />
    </paper>
    <paper id="44">
      <title>DTAFA : Decoupled Training Architecture for Efficient FAQ Retrieval<fixed-case>DTAFA</fixed-case>: Decoupled Training Architecture for Efficient <fixed-case>FAQ</fixed-case> Retrieval</title>
      <author><first>Haytham</first><last>Assem</last></author>
      <author><first>Sourav</first><last>Dutta</last></author>
      <author><first>Edward</first><last>Burgin</last></author>
      <pages>423–430</pages>
      <abstract>Automated Frequently Asked Question (FAQ) retrieval provides an effective procedure to provide prompt responses to natural language based queries, providing an efficient platform for large-scale service-providing companies for presenting readily available information pertaining to customers’ questions. We propose DTAFA, a novel multi-lingual FAQ retrieval system that aims at improving the top-1 retrieval accuracy with the least number of parameters. We propose two decoupled deep learning architectures trained for (i) candidate generation via text classification for a user question, and (ii) learning fine-grained semantic similarity between user questions and the FAQ repository for candidate refinement. We validate our <a href="https://en.wikipedia.org/wiki/System">system</a> using real-life enterprise data as well as <a href="https://en.wikipedia.org/wiki/Open-source_data">open source dataset</a>. Empirically we show that DTAFA achieves better <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> compared to existing <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> while requiring nearly 30 lesser number of <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training parameters</a>.</abstract>
      <url hash="ff972d8c">2021.sigdial-1.44</url>
      <bibkey>assem-etal-2021-dtafa</bibkey>
      <video href="https://www.youtube.com/watch?v=_tJhKdtu8EM" />
    </paper>
    <paper id="45">
      <title>Projection of Turn Completion in Incremental Spoken Dialogue Systems</title>
      <author><first>Erik</first><last>Ekstedt</last></author>
      <author><first>Gabriel</first><last>Skantze</last></author>
      <pages>431–437</pages>
      <abstract>The ability to take turns in a fluent way (i.e., without long response delays or frequent interruptions) is a fundamental aspect of any spoken dialog system. However, practical speech recognition services typically induce a long response delay, as it takes time before the processing of the user’s utterance is complete. There is a considerable amount of research indicating that humans achieve fast response times by projecting what the interlocutor will say and estimating upcoming turn completions. In this work, we implement this mechanism in an incremental spoken dialog system, by using a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> that generates possible futures to project upcoming completion points. In theory, this could make the <a href="https://en.wikipedia.org/wiki/System">system</a> more responsive, while still having access to <a href="https://en.wikipedia.org/wiki/Semantics">semantic information</a> not yet processed by the <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognizer</a>. We conduct a small study which indicates that this is a viable approach for practical dialog systems, and that this is a promising direction for future research.</abstract>
      <url hash="cbe97965">2021.sigdial-1.45</url>
      <bibkey>ekstedt-skantze-2021-projection</bibkey>
      <video href="https://www.youtube.com/watch?v=jfB1gE1wP6Y" />
    </paper>
    <paper id="50">
      <title>Do Encoder Representations of Generative Dialogue Models have sufficient summary of the Information about the task?</title>
      <author><first>Prasanna</first><last>Parthasarathi</last></author>
      <author><first>Joelle</first><last>Pineau</last></author>
      <author><first>Sarath</first><last>Chandar</last></author>
      <pages>477–488</pages>
      <abstract>Predicting the next utterance in dialogue is contingent on encoding of users’ input text to generate appropriate and relevant response in data-driven approaches. Although the semantic and syntactic quality of the language generated is evaluated, more often than not, the encoded representation of input is not evaluated. As the representation of the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> is essential for predicting the appropriate response, evaluation of <a href="https://en.wikipedia.org/wiki/Encoder">encoder representation</a> is a challenging yet important problem. In this work, we showcase evaluating the text generated through human or automatic metrics is not sufficient to appropriately evaluate soundness of the language understanding of dialogue models and, to that end, propose a set of probe tasks to evaluate encoder representation of different <a href="https://en.wikipedia.org/wiki/Encoder">language encoders</a> commonly used in dialogue models. From experiments, we observe that some of the probe tasks are easier and some are harder for even sophisticated model architectures to learn. And, through experiments we observe that RNN based architectures have lower performance on automatic metrics on text generation than transformer model but perform better than the transformer model on the probe tasks indicating that RNNs might preserve task information better than the Transformers.</abstract>
      <url hash="582efccf">2021.sigdial-1.50</url>
      <bibkey>parthasarathi-etal-2021-encoder</bibkey>
      <video href="https://www.youtube.com/watch?v=AwHuUPEpJFA" />
      <pwccode url="https://github.com/ppartha03/Dialogue-Probe-Tasks-Public" additional="false">ppartha03/Dialogue-Probe-Tasks-Public</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="52">
      <title>Schema-Guided Paradigm for Zero-Shot Dialog</title>
      <author><first>Shikib</first><last>Mehri</last></author>
      <author><first>Maxine</first><last>Eskenazi</last></author>
      <pages>499–508</pages>
      <abstract>Developing mechanisms that flexibly adapt dialog systems to unseen tasks and domains is a major challenge in dialog research. Neural models implicitly memorize task-specific dialog policies from the training data. We posit that this implicit memorization has precluded zero-shot transfer learning. To this end, we leverage the schema-guided paradigm, wherein the task-specific dialog policy is explicitly provided to the model. We introduce the Schema Attention Model (SAM) and improved schema representations for the STAR corpus. SAM obtains significant improvement in zero-shot settings, with a +22 F1 score improvement over prior work. These results validate the feasibility of zero-shot generalizability in <a href="https://en.wikipedia.org/wiki/Dialogue">dialog</a>. Ablation experiments are also presented to demonstrate the efficacy of <a href="https://en.wikipedia.org/wiki/Samarium_selenide">SAM</a>.</abstract>
      <url hash="e42940e6">2021.sigdial-1.52</url>
      <bibkey>mehri-eskenazi-2021-schema</bibkey>
      <video href="https://www.youtube.com/watch?v=usZQulwdOZs" />
      <pwccode url="https://github.com/Shikib/schema_attention_model" additional="false">Shikib/schema_attention_model</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/star">STAR</pwcdataset>
    </paper>
    <paper id="53">
      <title>Coreference-Aware Dialogue Summarization</title>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Ke</first><last>Shi</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>509–519</pages>
      <abstract>Summarizing conversations via neural approaches has been gaining research traction lately, yet it is still challenging to obtain practical solutions. Examples of such challenges include unstructured information exchange in dialogues, informal interactions between speakers, and dynamic role changes of speakers as the dialogue evolves. Many of such challenges result in complex coreference links. Therefore, in this work, we investigate different approaches to explicitly incorporate coreference information in neural abstractive dialogue summarization models to tackle the aforementioned challenges. Experimental results show that the proposed approaches achieve state-of-the-art performance, implying it is useful to utilize coreference information in dialogue summarization. Evaluation results on factual correctness suggest such coreference-aware models are better at tracing the information flow among interlocutors and associating accurate status / actions with the corresponding <a href="https://en.wikipedia.org/wiki/Interlocutor_(linguistics)">interlocutors</a> and person mentions.</abstract>
      <url hash="9be8ac79">2021.sigdial-1.53</url>
      <bibkey>liu-etal-2021-coreference</bibkey>
      <revision id="1" href="2021.sigdial-1.53v1" hash="a9c373ea" />
      <revision id="2" href="2021.sigdial-1.53v2" hash="9be8ac79" date="2022-01-24">Fixed typos in Section 3 and updated Table 3.</revision>
      <video href="https://www.youtube.com/watch?v=XNiUdhaW6LI" />
      <pwccode url="https://github.com/seq-to-mind/coref_dial_summ" additional="false">seq-to-mind/coref_dial_summ</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="54">
      <title>Weakly Supervised Extractive Summarization with Attention</title>
      <author><first>Yingying</first><last>Zhuang</last></author>
      <author><first>Yichao</first><last>Lu</last></author>
      <author><first>Simi</first><last>Wang</last></author>
      <pages>520–529</pages>
      <abstract>Automatic summarization aims to extract important information from large amounts of <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">textual data</a> in order to create a shorter version of the original texts while preserving its information. Training traditional extractive summarization models relies heavily on human-engineered labels such as sentence-level annotations of summary-worthiness. However, in many use cases, such human-engineered labels do not exist and manually annotating thousands of documents for the purpose of training models may not be feasible. On the other hand, indirect signals for summarization are often available, such as agent actions for customer service dialogues, headlines for news articles, <a href="https://en.wikipedia.org/wiki/Diagnosis">diagnosis</a> for <a href="https://en.wikipedia.org/wiki/Electronic_health_record">Electronic Health Records</a>, etc. In this paper, we develop a general framework that generates extractive summarization as a byproduct of <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning tasks</a> for indirect signals via the help of attention mechanism. We test our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on customer service dialogues and experimental results demonstrated that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can reliably select informative sentences and words for <a href="https://en.wikipedia.org/wiki/Automatic_summarization">automatic summarization</a>.</abstract>
      <url hash="96373a53">2021.sigdial-1.54</url>
      <bibkey>zhuang-etal-2021-weakly</bibkey>
      <video href="https://www.youtube.com/watch?v=0xiQe0OPwBA" />
    </paper>
    <paper id="55">
      <title>Incremental temporal summarization in multi-party meetings</title>
      <author><first>Ramesh</first><last>Manuvinakurike</last></author>
      <author><first>Saurav</first><last>Sahay</last></author>
      <author><first>Wenda</first><last>Chen</last></author>
      <author><first>Lama</first><last>Nachman</last></author>
      <pages>530–541</pages>
      <abstract>In this work, we develop a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for incremental temporal summarization in a multiparty dialogue. We use crowd-sourcing paradigm with a model-in-loop approach for collecting the summaries and compare the data with the expert summaries. We leverage the question generation paradigm to automatically generate questions from the <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a>, which can be used to validate the <a href="https://en.wikipedia.org/wiki/Participation_(decision_making)">user participation</a> and potentially also draw attention of the user towards the contents then need to summarize. We then develop several <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for abstractive summary generation in the Incremental temporal scenario. We perform a detailed analysis of the results and show that including the past context into the summary generation yields better summaries.</abstract>
      <url hash="484cf7ac">2021.sigdial-1.55</url>
      <bibkey>manuvinakurike-etal-2021-incremental</bibkey>
      <video href="https://www.youtube.com/watch?v=CnHqotO89jQ" />
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="58">
      <title>Large-Scale Quantitative Evaluation of Dialogue Agents’ Response Strategies against Offensive Users</title>
      <author><first>Haojun</first><last>Li</last></author>
      <author><first>Dilara</first><last>Soylu</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>556–561</pages>
      <abstract>As voice assistants and dialogue agents grow in popularity, so does the abuse they receive. We conducted a large-scale quantitative evaluation of the effectiveness of 4 response types (avoidance, why, empathetic, and counter), and 2 additional factors (using a redirect or a voluntarily provided name) that have not been tested by prior work. We measured their direct effectiveness on real users in-the-wild by the re-offense ratio, length of conversation after the initial response, and number of turns until the next re-offense. Our experiments confirm prior lab studies in showing that <a href="https://en.wikipedia.org/wiki/Empathy">empathetic responses</a> perform better than generic avoidance responses as well as counter responses. We show that dialogue agents should almost always guide offensive users to a new topic through the use of redirects and use the user’s name if provided. As compared to a baseline avoidance strategy employed by commercial agents, our best <a href="https://en.wikipedia.org/wiki/Strategy">strategy</a> is able to reduce the re-offense ratio from 92 % to 43 %.</abstract>
      <url hash="8fd5350b">2021.sigdial-1.58</url>
      <bibkey>li-etal-2021-large</bibkey>
      <video href="https://www.youtube.com/watch?v=FLsqwyGx4zM" />
      <pwccode url="https://github.com/lithiumh/offensive" additional="false">lithiumh/offensive</pwccode>
    </paper>
  </volume>
</collection>