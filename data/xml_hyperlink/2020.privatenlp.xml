<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.privatenlp">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Privacy in NLP</booktitle>
      <editor><first>Oluwaseyi</first><last>Feyisetan</last></editor>
      <editor><first>Sepideh</first><last>Ghanavati</last></editor>
      <editor><first>Shervin</first><last>Malmasi</last></editor>
      <editor><first>Patricia</first><last>Thaine</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="aa9e39bf">2020.privatenlp-1.0</url>
      <bibkey>privatenlp-2020-privacy</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Surfacing Privacy Settings Using Semantic Matching</title>
      <author><first>Rishabh</first><last>Khandelwal</last></author>
      <author><first>Asmit</first><last>Nayak</last></author>
      <author id="yao-yao-uwisc"><first>Yao</first><last>Yao</last></author>
      <author><first>Kassem</first><last>Fawaz</last></author>
      <pages>28–38</pages>
      <abstract>Online services utilize privacy settings to provide users with control over their data. However, these privacy settings are often hard to locate, causing the user to rely on provider-chosen default values. In this work, we train privacy-settings-centric encoders and leverage them to create an interface that allows users to search for privacy settings using free-form queries. In order to achieve this goal, we create a custom Semantic Similarity dataset, which consists of real user queries covering various privacy settings. We then use this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> to fine-tune a state of the art <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a>. Using this fine-tuned encoder, we perform <a href="https://en.wikipedia.org/wiki/Semantic_matching">semantic matching</a> between the <a href="https://en.wikipedia.org/wiki/Information_retrieval">user queries</a> and the privacy settings to retrieve the most relevant setting. Finally, we also use the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> to generate embeddings of privacy settings from the top 100 websites and perform unsupervised clustering to learn about the online privacy settings types. We find that the most common type of privacy settings are ‘Personalization’ and ‘Notifications’, with coverage of 35.8 % and 34.4 %, respectively, in our dataset.</abstract>
      <url hash="af9252e1">2020.privatenlp-1.4</url>
      <doi>10.18653/v1/2020.privatenlp-1.4</doi>
      <video href="https://slideslive.com/38939773" />
      <bibkey>khandelwal-etal-2020-surfacing</bibkey>
      <pwccode url="https://github.com/wi-pi/surface_privacy_dataset" additional="false">wi-pi/surface_privacy_dataset</pwccode>
    </paper>
    <paper id="5">
      <title>Differentially Private Language Models Benefit from Public Pre-training</title>
      <author><first>Gavin</first><last>Kerrigan</last></author>
      <author><first>Dylan</first><last>Slack</last></author>
      <author><first>Jens</first><last>Tuyls</last></author>
      <pages>39–45</pages>
      <abstract>Language modeling is a keystone task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. When training a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> on <a href="https://en.wikipedia.org/wiki/Information_sensitivity">sensitive information</a>, differential privacy (DP) allows us to quantify the degree to which our <a href="https://en.wikipedia.org/wiki/Personal_data">private data</a> is protected. However, training algorithms which enforce <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential privacy</a> often lead to degradation in model quality. We study the feasibility of learning a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> possible.</abstract>
      <url hash="85aa5c44">2020.privatenlp-1.5</url>
      <doi>10.18653/v1/2020.privatenlp-1.5</doi>
      <video href="https://slideslive.com/38939774" />
      <bibkey>kerrigan-etal-2020-differentially</bibkey>
    </paper>
  </volume>
</collection>