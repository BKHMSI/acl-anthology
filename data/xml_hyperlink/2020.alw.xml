<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.alw">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Online Abuse and Harms</booktitle>
      <editor><first>Seyi</first><last>Akiwowo</last></editor>
      <editor><first>Bertie</first><last>Vidgen</last></editor>
      <editor><first>Vinodkumar</first><last>Prabhakaran</last></editor>
      <editor><first>Zeerak</first><last>Waseem</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="4daefd0c">2020.alw-1.0</url>
      <bibkey>alw-2020-online</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Fine-tuning for multi-domain and multi-label uncivil language detection</title>
      <author><first>Kadir Bulut</first><last>Ozler</last></author>
      <author><first>Kate</first><last>Kenski</last></author>
      <author><first>Steve</first><last>Rains</last></author>
      <author><first>Yotam</first><last>Shmargad</last></author>
      <author><first>Kevin</first><last>Coe</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <pages>28–33</pages>
      <abstract>Incivility is a problem on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, and it comes in many forms (name-calling, <a href="https://en.wikipedia.org/wiki/Vulgarity">vulgarity</a>, <a href="https://en.wikipedia.org/wiki/Threat">threats</a>, etc.) and domains (microblog posts, <a href="https://en.wikipedia.org/wiki/Online_newspaper">online news comments</a>, <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia edits</a>, etc.). Training <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning models</a> to detect such <a href="https://en.wikipedia.org/wiki/Incivility">incivility</a> must handle the multi-label and multi-domain nature of the problem. We present a BERT-based model for incivility detection and propose several approaches for training it for multi-label and multi-domain datasets. We find that individual binary classifiers outperform a joint multi-label classifier, and that simply combining multiple domains of training data outperforms other recently-proposed fine tuning strategies. We also establish new state-of-the-art performance on several incivility detection datasets.</abstract>
      <url hash="3f75913b">2020.alw-1.4</url>
      <doi>10.18653/v1/2020.alw-1.4</doi>
      <video href="https://slideslive.com/38939522" />
      <bibkey>ozler-etal-2020-fine</bibkey>
    </paper>
    <paper id="5">
      <title>HurtBERT : Incorporating Lexical Features with BERT for the Detection of Abusive Language<fixed-case>H</fixed-case>urt<fixed-case>BERT</fixed-case>: Incorporating Lexical Features with <fixed-case>BERT</fixed-case> for the Detection of Abusive Language</title>
      <author><first>Anna</first><last>Koufakou</last></author>
      <author><first>Endang Wahyu</first><last>Pamungkas</last></author>
      <author><first>Valerio</first><last>Basile</last></author>
      <author><first>Viviana</first><last>Patti</last></author>
      <pages>34–43</pages>
      <abstract>The detection of abusive or offensive remarks in <a href="https://en.wikipedia.org/wiki/Social_text">social texts</a> has received significant attention in research. In several related <a href="https://en.wikipedia.org/wiki/Task_(computing)">shared tasks</a>, BERT has been shown to be the state-of-the-art. In this paper, we propose to utilize lexical features derived from a hate lexicon towards improving the performance of BERT in such tasks. We explore different ways to utilize the <a href="https://en.wikipedia.org/wiki/Lexicon">lexical features</a> in the form of lexicon-based encodings at the <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence level</a> or embeddings at the <a href="https://en.wikipedia.org/wiki/Word_(linguistics)">word level</a>. We provide an extensive dataset evaluation that addresses in-domain as well as cross-domain detection of abusive content to render a complete picture. Our results indicate that our proposed models combining BERT with lexical features help improve over a baseline BERT model in many of our in-domain and cross-domain experiments.</abstract>
      <url hash="b29ab768">2020.alw-1.5</url>
      <doi>10.18653/v1/2020.alw-1.5</doi>
      <video href="https://slideslive.com/38939530" />
      <bibkey>koufakou-etal-2020-hurtbert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="10">
      <title>Attending the Emotions to Detect Online Abusive Language</title>
      <author><first>Niloofar</first><last>Safi Samghabadi</last></author>
      <author><first>Afsheen</first><last>Hatami</last></author>
      <author><first>Mahsa</first><last>Shafaei</last></author>
      <author><first>Sudipta</first><last>Kar</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>79–88</pages>
      <abstract>In recent years, <a href="https://en.wikipedia.org/wiki/Abuse">abusive behavior</a> has become a serious issue in <a href="https://en.wikipedia.org/wiki/List_of_social_networking_websites">online social networks</a>. In this paper, we present a new <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> for the task of abusive language detection that is collected from a semi-anonymous online platform, and unlike the majority of other available resources, is not created based on a specific list of bad words. We also develop <a href="https://en.wikipedia.org/wiki/Computational_model">computational models</a> to incorporate <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> into <a href="https://en.wikipedia.org/wiki/Sensory_cue">textual cues</a> to improve <a href="https://en.wikipedia.org/wiki/Aggression">aggression identification</a>. We evaluate our proposed <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> on a set of corpora related to the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> and show promising results with respect to abusive language detection.</abstract>
      <url hash="2aacb0ad">2020.alw-1.10</url>
      <doi>10.18653/v1/2020.alw-1.10</doi>
      <video href="https://slideslive.com/38939534" />
      <bibkey>safi-samghabadi-etal-2020-attending</bibkey>
    </paper>
    <paper id="13">
      <title>Countering hate on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> : Large scale classification of hate and counter speech</title>
      <author><first>Joshua</first><last>Garland</last></author>
      <author><first>Keyan</first><last>Ghazi-Zahedi</last></author>
      <author><first>Jean-Gabriel</first><last>Young</last></author>
      <author><first>Laurent</first><last>Hébert-Dufresne</last></author>
      <author><first>Mirta</first><last>Galesic</last></author>
      <pages>102–112</pages>
      <abstract>Hateful rhetoric is plaguing <a href="https://en.wikipedia.org/wiki/Online_discourse">online discourse</a>, fostering <a href="https://en.wikipedia.org/wiki/Extremism">extreme societal movements</a> and possibly giving rise to <a href="https://en.wikipedia.org/wiki/Violence">real-world violence</a>. A potential solution to this growing global problem is citizen-generated counter speech where citizens actively engage with <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a> to restore <a href="https://en.wikipedia.org/wiki/Civil_discourse">civil non-polarized discourse</a>. However, its actual effectiveness in curbing the spread of <a href="https://en.wikipedia.org/wiki/Hatred">hatred</a> is unknown and hard to quantify. One major obstacle to researching this question is a lack of large labeled data sets for training <a href="https://en.wikipedia.org/wiki/Statistical_classification">automated classifiers</a> to identify counter speech. Here we use a unique situation in <a href="https://en.wikipedia.org/wiki/Germany">Germany</a> where self-labeling groups engaged in organized online hate and counter speech. We use an ensemble learning algorithm which pairs a variety of paragraph embeddings with regularized logistic regression functions to classify both hate and counter speech in a corpus of millions of relevant tweets from these two groups. Our <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a> achieves macro F1 scores on out of sample balanced test sets ranging from 0.76 to 0.97accuracy in line and even exceeding the state of the art. We then use the classifier to discover hate and counter speech in more than 135,000 fully-resolved Twitter conversations occurring from 2013 to 2018 and study their frequency and interaction. Altogether, our results highlight the potential of automated methods to evaluate the impact of coordinated counter speech in stabilizing conversations on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>.</abstract>
      <url hash="799042bc">2020.alw-1.13</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b2840f42">2020.alw-1.13.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.alw-1.13</doi>
      <video href="https://slideslive.com/38939518" />
      <bibkey>garland-etal-2020-countering</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-counter">Hate Counter</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="14">
      <title>Moderating Our (Dis)Content : Renewing the Regulatory Approach</title>
      <author><first>Claire</first><last>Pershan</last></author>
      <pages>113</pages>
      <abstract>As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and interconnectedness of <a href="https://en.wikipedia.org/wiki/Content-control_software">content moderation</a> across a fragmented online landscape. This report urges regulators and legislators to consider a range of <a href="https://en.wikipedia.org/wiki/Computing_platform">platforms</a> and moderation approaches in the <a href="https://en.wikipedia.org/wiki/Regulation">regulation</a>. In particular, it calls for a holistic, process-oriented regulatory approach that accounts for actors beyond the handful of dominant platforms that currently shape public debate.</abstract>
      <url hash="dc387124">2020.alw-1.14</url>
      <doi>10.18653/v1/2020.alw-1.14</doi>
      <video href="https://slideslive.com/38939516" />
      <bibkey>pershan-2020-moderating</bibkey>
    </paper>
    <paper id="19">
      <title>Detecting East Asian Prejudice on Social Media<fixed-case>E</fixed-case>ast <fixed-case>A</fixed-case>sian Prejudice on Social Media</title>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <author><first>Scott</first><last>Hale</last></author>
      <author><first>Ella</first><last>Guest</last></author>
      <author><first>Helen</first><last>Margetts</last></author>
      <author><first>David</first><last>Broniatowski</last></author>
      <author><first>Zeerak</first><last>Waseem</last></author>
      <author><first>Austin</first><last>Botelho</last></author>
      <author><first>Matthew</first><last>Hall</last></author>
      <author><first>Rebekah</first><last>Tromble</last></author>
      <pages>162–172</pages>
      <abstract>During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> into four classes : Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> struggles with <a href="https://en.wikipedia.org/wiki/Edge_case">edge cases</a> and <a href="https://en.wikipedia.org/wiki/Ambiguity">ambiguous content</a>. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.</abstract>
      <url hash="439658bd">2020.alw-1.19</url>
      <attachment type="OptionalSupplementaryMaterial" hash="53df7520">2020.alw-1.19.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.alw-1.19</doi>
      <video href="https://slideslive.com/38939526" />
      <bibkey>vidgen-etal-2020-detecting</bibkey>
    </paper>
    <paper id="20">
      <title>On Cross-Dataset Generalization in Automatic Detection of Online Abuse</title>
      <author><first>Isar</first><last>Nejadgholi</last></author>
      <author><first>Svetlana</first><last>Kiritchenko</last></author>
      <pages>173–183</pages>
      <abstract>NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training and test datasets</a> are usually obtained from similar data samples, in practice systems are often applied on data that are different from the training set in topic and class distributions. Also, the ambiguity in class definitions inherited in this <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> aggravates the discrepancies between source and target datasets. We explore the topic bias and the task formulation bias in cross-dataset generalization. We show that the benign examples in the Wikipedia Detox dataset are biased towards platform-specific topics. We identify these examples using unsupervised topic modeling and manual inspection of topics’ keywords. Removing these topics increases cross-dataset generalization, without reducing in-domain classification performance. For a robust dataset design, we suggest applying inexpensive <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a> to inspect the collected data and downsize the non-generalizable content before manually annotating for class labels.</abstract>
      <url hash="91b1f621">2020.alw-1.20</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8dbcbfc5">2020.alw-1.20.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.alw-1.20</doi>
      <video href="https://slideslive.com/38939537" />
      <bibkey>nejadgholi-kiritchenko-2020-cross</bibkey>
    </paper>
    <paper id="22">
      <title>Investigating Annotator Bias with a Graph-Based Approach</title>
      <author><first>Maximilian</first><last>Wich</last></author>
      <author><first>Hala</first><last>Al Kuwatly</last></author>
      <author><first>Georg</first><last>Groh</last></author>
      <pages>191–199</pages>
      <abstract>A challenge that many online platforms face is <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a> or any other form of online abuse. To cope with this, hate speech detection systems are developed based on <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> to reduce manual work for monitoring these <a href="https://en.wikipedia.org/wiki/Computing_platform">platforms</a>. Unfortunately, <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> is vulnerable to unintended bias in training data, which could have severe consequences, such as a decrease in <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification performance</a> or unfair behavior (e.g., discriminating minorities). In the scope of this study, we want to investigate annotator bias   a form of <a href="https://en.wikipedia.org/wiki/Bias">bias</a> that annotators cause due to different knowledge in regards to the task and their subjective perception. Our goal is to identify annotation bias based on similarities in the <a href="https://en.wikipedia.org/wiki/Annotation">annotation behavior</a> from annotators. To do so, we build a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> based on the annotations from the different annotators, apply a community detection algorithm to group the annotators, and train for each group classifiers whose performances we compare. By doing so, we are able to identify annotator bias within a <a href="https://en.wikipedia.org/wiki/Data_set">data set</a>. The proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> and collected insights can contribute to developing fairer and more reliable hate speech classification models.</abstract>
      <url hash="ac0b25f9">2020.alw-1.22</url>
      <attachment type="OptionalSupplementaryMaterial" hash="f968f6da">2020.alw-1.22.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.alw-1.22</doi>
      <video href="https://slideslive.com/38939539" />
      <bibkey>wich-etal-2020-investigating</bibkey>
      <revision id="1" href="2020.alw-1.22v1" hash="a7eb4a69" />
      <revision id="2" href="2020.alw-1.22v2" hash="ac0b25f9" date="2021-08-12">Results content fix</revision>
      <pwccode url="https://github.com/mawic/graph-based-method-annotator-bias" additional="false">mawic/graph-based-method-annotator-bias</pwccode>
    </paper>
  </volume>
</collection>