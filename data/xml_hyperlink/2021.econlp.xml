<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.econlp">
  <volume id="1" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Economics and Natural Language Processing</booktitle>
      <editor><first>Udo</first><last>Hahn</last></editor>
      <editor><first>Veronique</first><last>Hoste</last></editor>
      <editor><first>Amanda</first><last>Stent</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="4e91a557">2021.econlp-1.0</url>
      <bibkey>econlp-2021-economics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Fine-Grained Annotated Corpus for Target-Based Opinion Analysis of Economic and Financial Narratives</title>
      <author><first>Jiahui</first><last>Hu</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>1–12</pages>
      <abstract>In this paper about aspect-based sentiment analysis (ABSA), we present the first version of a fine-grained annotated corpus for target-based opinion analysis (TBOA) to analyze <a href="https://en.wikipedia.org/wiki/Economy">economic activities</a> or <a href="https://en.wikipedia.org/wiki/Financial_market">financial markets</a>. We have annotated, at an intra-sentential level, a corpus of sentences extracted from documents representative of financial analysts’ most-read materials by considering how financial actors communicate about the evolution of event trends and analyze related publications (news, official communications, etc.). Since we focus on identifying the expressions of opinions related to the <a href="https://en.wikipedia.org/wiki/Economy">economy</a> and financial markets, we annotated the sentences that contain at least one subjective expression about a domain-specific term. Candidate sentences for annotations were randomly chosen from texts of specialized press and professional information channels over a period ranging from 1986 to 2021. Our annotation scheme relies on various linguistic markers like domain-specific vocabulary, <a href="https://en.wikipedia.org/wiki/Syntax">syntactic structures</a>, and rhetorical relations to explicitly describe the author’s subjective stance. We investigated and evaluated the recourse to automatic pre-annotation with existing <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing technologies</a> to alleviate the annotation workload. Our aim is to propose a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> usable on the one hand as training material for the automatic detection of the opinions expressed on an extensive range of domain-specific aspects and on the other hand as a gold standard for evaluation TBOA. In this paper, we present our pre-annotation models and evaluations of their performance, introduce our annotation scheme and report on the main characteristics of our <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>.</abstract>
      <url hash="06570a64">2021.econlp-1.1</url>
      <bibkey>hu-paroubek-2021-fine</bibkey>
      <doi>10.18653/v1/2021.econlp-1.1</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="2">
      <title>EDGAR-CORPUS : Billions of Tokens Make The World Go Round<fixed-case>EDGAR</fixed-case>-<fixed-case>CORPUS</fixed-case>: Billions of Tokens Make The World Go Round</title>
      <author><first>Lefteris</first><last>Loukas</last></author>
      <author><first>Manos</first><last>Fergadiotis</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <author><first>Prodromos</first><last>Malakasiotis</last></author>
      <pages>13–18</pages>
      <abstract>We release EDGAR-CORPUS, a novel <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> comprising annual reports from all the publicly traded companies in the US spanning a period of more than 25 years. To the best of our knowledge, EDGAR-CORPUS is the largest financial NLP corpus available to date. All the reports are downloaded, split into their corresponding items (sections), and provided in a clean, easy-to-use JSON format. We use EDGAR-CORPUS to train and release EDGAR-W2V, which are WORD2VEC embeddings for the financial domain. We employ these <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> in a battery of financial NLP tasks and showcase their superiority over generic GloVe embeddings and other existing financial word embeddings. We also open-source EDGAR-CRAWLER, a toolkit that facilitates downloading and extracting future <a href="https://en.wikipedia.org/wiki/Annual_report">annual reports</a>.</abstract>
      <url hash="7cc0391b">2021.econlp-1.2</url>
      <bibkey>loukas-etal-2021-edgar</bibkey>
      <doi>10.18653/v1/2021.econlp-1.2</doi>
      <pwccode url="https://github.com/nlpaueb/edgar-crawler" additional="false">nlpaueb/edgar-crawler</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/edgar-corpus">EDGAR-CORPUS</pwcdataset>
    </paper>
    <paper id="3">
      <title>The Global Banking Standards QA Dataset (GBS-QA)<fixed-case>QA</fixed-case> Dataset (<fixed-case>GBS</fixed-case>-<fixed-case>QA</fixed-case>)</title>
      <author><first>Kyunghwan</first><last>Sohn</last></author>
      <author><first>Sunjae</first><last>Kwon</last></author>
      <author><first>Jaesik</first><last>Choi</last></author>
      <pages>19–25</pages>
      <abstract>A domain specific question answering (QA) dataset dramatically improves the machine comprehension performance. This paper presents a new Global Banking Standards QA dataset (GBS-QA) in the banking regulation domain. The GBS-QA has three values. First, it contains actual questions from market players and answers from global rule setter, the Basel Committee on Banking Supervision (BCBS) in the middle of creating and revising banking regulations. Second, financial regulation experts analyze and verify pairs of questions and answers in the annotation process. Lastly, the GBS-QA is a totally different dataset with existing datasets in <a href="https://en.wikipedia.org/wiki/Finance">finance</a> and is applicable to stimulate transfer learning research in the banking regulation domain.</abstract>
      <url hash="cc80d688">2021.econlp-1.3</url>
      <bibkey>sohn-etal-2021-global</bibkey>
      <doi>10.18653/v1/2021.econlp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Corporate Bankruptcy Prediction with Domain-Adapted BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Alex Gunwoo</first><last>Kim</last></author>
      <author><first>Sangwon</first><last>Yoon</last></author>
      <pages>26–36</pages>
      <abstract>This study performs BERT-based analysis, which is a representative contextualized language model, on corporate disclosure data to predict impending bankruptcies. Prior literature on <a href="https://en.wikipedia.org/wiki/Bankruptcy_prediction">bankruptcy prediction</a> mainly focuses on developing more sophisticated prediction methodologies with financial variables. However, in our study, we focus on improving the quality of input dataset. Specifically, we employ BERT model to perform <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> on MD&amp;A disclosures. We show that BERT outperforms dictionary-based predictions and Word2Vec-based predictions in terms of adjusted R-square in <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>, k-nearest neighbor (kNN-5), and linear kernel support vector machine (SVM). Further, instead of pre-training the BERT model from scratch, we apply <a href="https://en.wikipedia.org/wiki/Autodidacticism">self-learning</a> with confidence-based filtering to corporate disclosure data (10-K). We achieve the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy rate</a> of 91.56 % and demonstrate that the domain adaptation procedure brings a significant improvement in prediction accuracy.</abstract>
      <url hash="570b9d58">2021.econlp-1.4</url>
      <bibkey>kim-yoon-2021-corporate</bibkey>
      <doi>10.18653/v1/2021.econlp-1.4</doi>
    </paper>
    <paper id="9">
      <title>To What Extent Can English-as-a-Second Language Learners Read Economic News Texts?<fixed-case>E</fixed-case>nglish-as-a-Second Language Learners Read Economic News Texts?</title>
      <author><first>Yo</first><last>Ehara</last></author>
      <pages>62–68</pages>
      <abstract>In <a href="https://en.wikipedia.org/wiki/Decision-making">decision making</a> in the <a href="https://en.wikipedia.org/wiki/Economy">economic field</a>, an especially important requirement is to rapidly understand news to absorb ever-changing economic situations. Given that most economic news is written in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, the ability to read such information without waiting for a translation is particularly valuable in <a href="https://en.wikipedia.org/wiki/Economics">economics</a> in contrast to other fields. In consideration of this issue, this research investigated the extent to which <a href="https://en.wikipedia.org/wiki/Foreign_language">non-native English speakers</a> are able to read economic news to make decisions accordingly   an issue that has been rarely addressed in previous studies. Using an existing standard <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> as training data, we created a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> that automatically evaluates the <a href="https://en.wikipedia.org/wiki/Readability">readability</a> of text with high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> for English learners. Our assessment of the <a href="https://en.wikipedia.org/wiki/Readability">readability</a> of an economic news corpus revealed that most news texts can be read by <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">intermediate English learners</a>. We also found that in some cases, <a href="https://en.wikipedia.org/wiki/Readability">readability</a> varies considerably depending on the knowledge of certain words specific to the <a href="https://en.wikipedia.org/wiki/Economics">economic field</a>.</abstract>
      <url hash="fd7d735f">2021.econlp-1.9</url>
      <bibkey>ehara-2021-extent</bibkey>
      <doi>10.18653/v1/2021.econlp-1.9</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopenglish">OneStopEnglish</pwcdataset>
    </paper>
    </volume>
</collection>