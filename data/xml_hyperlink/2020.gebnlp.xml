<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.gebnlp">
  <volume id="1" ingest-date="2020-12-10">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Gender Bias in Natural Language Processing</booktitle>
      <editor><first>Marta R.</first><last>Costa-jussà</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Will</first><last>Radford</last></editor>
      <editor><first>Kellie</first><last>Webster</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="2ab94dbb">2020.gebnlp-1.0</url>
      <bibkey>gebnlp-2020-gender</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Interdependencies of Gender and Race in Contextualized Word Embeddings</title>
      <author><first>May</first><last>Jiang</last></author>
      <author><first>Christiane</first><last>Fellbaum</last></author>
      <pages>17–25</pages>
      <abstract>Recent years have seen a surge in research on the biases in word embeddings with respect to <a href="https://en.wikipedia.org/wiki/Gender">gender</a> and, to a lesser extent, <a href="https://en.wikipedia.org/wiki/Race_(human_categorization)">race</a>. Few of these studies, however, have given attention to the critical intersection of <a href="https://en.wikipedia.org/wiki/Race_(human_categorization)">race</a> and <a href="https://en.wikipedia.org/wiki/Gender">gender</a>. In this case study, we analyze the dimensions of <a href="https://en.wikipedia.org/wiki/Gender">gender</a> and <a href="https://en.wikipedia.org/wiki/Race_(human_categorization)">race</a> in contextualized word embeddings of given names, taken from BERT, and investigate the nature and nuance of their interaction. We find that these demographic axes, though typically treated as physically and conceptually separate, are in fact interdependent and thus inadvisable to consider in isolation. Further, we show that demographic dimensions predicated on default settings in language, such as in <a href="https://en.wikipedia.org/wiki/Pronoun">pronouns</a>, may risk rendering groups with multiple marginalized identities invisible. We conclude by discussing the importance and implications of <a href="https://en.wikipedia.org/wiki/Intersectionality">intersectionality</a> for future studies on <a href="https://en.wikipedia.org/wiki/Bias">bias</a> and debiasing in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>.</abstract>
      <url hash="cc5715e8">2020.gebnlp-1.2</url>
      <bibkey>jiang-fellbaum-2020-interdependencies</bibkey>
    </paper>
    <paper id="3">
      <title>Fine-tuning Neural Machine Translation on Gender-Balanced Datasets</title>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Adrià</first><last>de Jorge</last></author>
      <pages>26–34</pages>
      <abstract>Misrepresentation of certain communities in datasets is causing big disruptions in <a href="https://en.wikipedia.org/wiki/List_of_applications_of_artificial_intelligence">artificial intelligence applications</a>. In this paper, we propose using an automatically extracted gender-balanced dataset parallel corpus from <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>. This <a href="https://en.wikipedia.org/wiki/Balanced_set">balanced set</a> is used to perform fine-tuning techniques from a bigger model trained on unbalanced datasets to mitigate gender biases in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>.</abstract>
      <url hash="9ce7ac05">2020.gebnlp-1.3</url>
      <bibkey>costa-jussa-de-jorge-2020-fine</bibkey>
    </paper>
    <paper id="7">
      <title>Conversational Assistants and <a href="https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States">Gender Stereotypes</a> : Public Perceptions and Desiderata for Voice Personas</title>
      <author><first>Amanda</first><last>Cercas Curry</last></author>
      <author><first>Judy</first><last>Robertson</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>72–78</pages>
      <abstract>Conversational voice assistants are rapidly developing from purely transactional systems to social companions with personality. UNESCO recently stated that the female and submissive personality of current <a href="https://en.wikipedia.org/wiki/Digital_assistant">digital assistants</a> gives rise for concern as it reinforces <a href="https://en.wikipedia.org/wiki/Gender_role">gender stereotypes</a>. In this work, we present results from a participatory design workshop, where we invite people to submit their preferences for a what their ideal persona might look like, both in drawings as well as in a multiple choice questionnaire. We find no clear consensus which suggests that one possible solution is to let people configure / personalise their assistants. We then outline a multi-disciplinary project of how we plan to address the complex question of gender and stereotyping in digital assistants.</abstract>
      <url hash="1ebcee76">2020.gebnlp-1.7</url>
      <bibkey>cercas-curry-etal-2020-conversational</bibkey>
    </paper>
    </volume>
</collection>