<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.mmtlrl">
  <volume id="1" ingest-date="2021-11-09">
    <meta>
      <booktitle>Proceedings of the First Workshop on Multimodal Machine Translation for Low Resource Languages (MMTLRL 2021)</booktitle>
      <editor><first>Thoudam</first><last>Doren Singh</last></editor>
      <editor><first>Cristina</first><last>España i Bonet</last></editor>
      <editor><first>Sivaji</first><last>Bandyopadhyay</last></editor>
      <editor><first>Josef</first><last>van Genabith</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Online (Virtual Mode)</address>
      <month>September</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="cd3e0b50">2021.mmtlrl-1.0</url>
      <bibkey>mmtlrl-2021-multimodal</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Models and Tasks for Human-Centered Machine Translation</title>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>1</pages>
      <abstract>In this talk, I will describe current research directions in my group that aim to make machine translation (MT) more human-centered. Instead of viewing <a href="https://en.wikipedia.org/wiki/Machine_to_machine">MT</a> solely as a task that aims to transduce a source sentence into a well-formed target language equivalent, we revisit all steps of the <a href="https://en.wikipedia.org/wiki/Machine_to_machine">MT research and development lifecycle</a> with the goal of designing <a href="https://en.wikipedia.org/wiki/Machine_to_machine">MT systems</a> that are able to help people communicate across language barriers. I will present methods to better characterize the parallel training data that powers MT systems, and how the degree of equivalence impacts translation quality. I will introduce models that enable flexible conditional language generation, and will discuss recent work on framing machine translation tasks and evaluation to center human factors.</abstract>
      <url hash="1210d2dd">2021.mmtlrl-1.1</url>
      <bibkey>carpuat-2021-models</bibkey>
    </paper>
    <paper id="5">
      <title>Multimodal Simultaneous Machine Translation</title>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>30</pages>
      <abstract>Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a> and highest quality possible. Therefore, <a href="https://en.wikipedia.org/wiki/Translation">translation</a> has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this talk I will present work where we seek to understand whether the addition of <a href="https://en.wikipedia.org/wiki/Visual_system">visual information</a> can compensate for the missing source context. We analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks, including fixed and dynamic policy approaches using <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>. Our results show that <a href="https://en.wikipedia.org/wiki/Context_(language_use)">visual context</a> is helpful and that visually-grounded models based on explicit object region information perform the best. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from <a href="https://en.wikipedia.org/wiki/English_language">English</a> into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/French_language">French</a>.</abstract>
      <url hash="9eebbc4e">2021.mmtlrl-1.5</url>
      <bibkey>specia-2021-multimodal</bibkey>
    </paper>
    <paper id="6">
      <title>Multimodal Neural Machine Translation System for <a href="https://en.wikipedia.org/wiki/English_language">English</a> to Bengali<fixed-case>E</fixed-case>nglish to <fixed-case>B</fixed-case>engali</title>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Subhadarshi</first><last>Panda</last></author>
      <author><first>Satya Prakash</first><last>Biswal</last></author>
      <author><first>Ketan</first><last>Kotwal</last></author>
      <author><first>Arghyadeep</first><last>Sen</last></author>
      <author><first>Satya Ranjan</first><last>Dash</last></author>
      <author><first>Petr</first><last>Motlicek</last></author>
      <pages>31–39</pages>
      <abstract>Multimodal Machine Translation (MMT) systems utilize additional information from other modalities beyond text to improve the quality of machine translation (MT). The additional <a href="https://en.wikipedia.org/wiki/Modality_(semiotics)">modality</a> is typically in the form of <a href="https://en.wikipedia.org/wiki/Digital_image">images</a>. Despite proven advantages, it is indeed difficult to develop an MMT system for various languages primarily due to the lack of a suitable multimodal dataset. In this work, we develop an MMT for <a href="https://en.wikipedia.org/wiki/Bengali_language">English- Bengali</a> using a recently published Bengali Visual Genome (BVG) dataset that contains images with associated bilingual textual descriptions. Through a comparative study of the developed MMT system vis-a-vis a Text-to-text translation, we demonstrate that the use of multimodal data not only improves the <a href="https://en.wikipedia.org/wiki/Translation">translation</a> performance improvement in BLEU score of +1.3 on the development set, +3.9 on the evaluation test, and +0.9 on the challenge test set but also helps to resolve ambiguities in the pure text description. As per best of our knowledge, our English-Bengali MMT system is the first attempt in this direction, and thus, can act as a baseline for the subsequent research in <a href="https://en.wikipedia.org/wiki/Multilingualism">MMT</a> for low resource languages.</abstract>
      <url hash="bdc63cae">2021.mmtlrl-1.6</url>
      <bibkey>parida-etal-2021-multimodal</bibkey>
    </paper>
    </volume>
</collection>