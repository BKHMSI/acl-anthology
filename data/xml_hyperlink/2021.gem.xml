<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.gem">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)</booktitle>
      <editor><first>Antoine</first><last>Bosselut</last></editor>
      <editor><first>Esin</first><last>Durmus</last></editor>
      <editor><first>Varun Prashant</first><last>Gangal</last></editor>
      <editor><first>Sebastian</first><last>Gehrmann</last></editor>
      <editor><first>Yacine</first><last>Jernite</last></editor>
      <editor><first>Laura</first><last>Perez-Beltrachini</last></editor>
      <editor><first>Samira</first><last>Shaikh</last></editor>
      <editor><first>Wei</first><last>Xu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="ab60ad50">2021.gem-1</url>
    </meta>
    <frontmatter>
      <url hash="13027448">2021.gem-1.0</url>
      <bibkey>gem-2021-natural</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Semantic Similarity Based Evaluation for Abstractive News Summarization</title>
      <author><first>Figen</first><last>Beken Fikri</last></author>
      <author><first>Kemal</first><last>Oflazer</last></author>
      <author><first>Berrin</first><last>Yanikoglu</last></author>
      <pages>24–33</pages>
      <abstract>ROUGE is a widely used evaluation metric in <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a>. However, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> is not suitable for the evaluation of abstractive summarization systems as <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> relies on lexical overlap between the gold standard and the generated summaries. This limitation becomes more apparent for <a href="https://en.wikipedia.org/wiki/Agglutinative_language">agglutinative languages</a> with very large vocabularies and high type / token ratios. In this paper, we present semantic similarity models for <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a> and apply them as evaluation metrics for an abstractive summarization task. To achieve this, we translated the English STSb dataset into <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a> and presented the first semantic textual similarity dataset for <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a> as well. We showed that our best similarity models have better alignment with average human judgments compared to ROUGE in both Pearson and Spearman correlations.</abstract>
      <url hash="e7d5e247">2021.gem-1.3</url>
      <doi>10.18653/v1/2021.gem-1.3</doi>
      <bibkey>beken-fikri-etal-2021-semantic</bibkey>
      <pwccode url="https://github.com/verimsu/stsb-tr" additional="false">verimsu/stsb-tr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlsum">MLSUM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nli-tr">NLI-TR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="10">
      <title>The GEM Benchmark : <a href="https://en.wikipedia.org/wiki/Natural-language_generation">Natural Language Generation</a>, its Evaluation and Metrics<fixed-case>GEM</fixed-case> Benchmark: Natural Language Generation, its Evaluation and Metrics</title>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Tosin</first><last>Adewumi</last></author>
      <author><first>Karmanya</first><last>Aggarwal</last></author>
      <author><first>Pawan Sasanka</first><last>Ammanamanchi</last></author>
      <author><first>Anuoluwapo</first><last>Aremu</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <author><first>Khyathi Raghavi</first><last>Chandu</last></author>
      <author><first>Miruna-Adriana</first><last>Clinciu</last></author>
      <author><first>Dipanjan</first><last>Das</last></author>
      <author><first>Kaustubh</first><last>Dhole</last></author>
      <author><first>Wanyu</first><last>Du</last></author>
      <author><first>Esin</first><last>Durmus</last></author>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <author><first>Chris Chinenye</first><last>Emezue</last></author>
      <author><first>Varun</first><last>Gangal</last></author>
      <author><first>Cristina</first><last>Garbacea</last></author>
      <author><first>Tatsunori</first><last>Hashimoto</last></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Yacine</first><last>Jernite</last></author>
      <author><first>Harsh</first><last>Jhamtani</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <author><first>Shailza</first><last>Jolly</last></author>
      <author><first>Mihir</first><last>Kale</last></author>
      <author><first>Dhruv</first><last>Kumar</last></author>
      <author><first>Faisal</first><last>Ladhak</last></author>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Mounica</first><last>Maddela</last></author>
      <author><first>Khyati</first><last>Mahajan</last></author>
      <author><first>Saad</first><last>Mahamood</last></author>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last></author>
      <author><first>Pedro Henrique</first><last>Martins</last></author>
      <author><first>Angelina</first><last>McMillan-Major</last></author>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <author><first>Moin</first><last>Nadeem</last></author>
      <author><first>Shashi</first><last>Narayan</last></author>
      <author><first>Vitaly</first><last>Nikolaev</last></author>
      <author><first>Andre</first><last>Niyongabo Rubungo</last></author>
      <author><first>Salomey</first><last>Osei</last></author>
      <author><first>Ankur</first><last>Parikh</last></author>
      <author><first>Laura</first><last>Perez-Beltrachini</last></author>
      <author><first>Niranjan Ramesh</first><last>Rao</last></author>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Juan Diego</first><last>Rodriguez</last></author>
      <author><first>Sashank</first><last>Santhanam</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Thibault</first><last>Sellam</last></author>
      <author><first>Samira</first><last>Shaikh</last></author>
      <author><first>Anastasia</first><last>Shimorina</last></author>
      <author><first>Marco Antonio</first><last>Sobrevilla Cabezudo</last></author>
      <author><first>Hendrik</first><last>Strobelt</last></author>
      <author><first>Nishant</first><last>Subramani</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <author><first>Akhila</first><last>Yerukola</last></author>
      <author><first>Jiawei</first><last>Zhou</last></author>
      <pages>96–120</pages>
      <abstract>We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, and human evaluation standards. Due to this moving target, new <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> often still evaluate on divergent anglo-centric corpora with well-established, but flawed, <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>. This disconnect makes it challenging to identify the limitations of current <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> and opportunities for progress. Addressing this limitation, <a href="https://en.wikipedia.org/wiki/Graphics_Environment_Manager">GEM</a> provides an environment in which <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> can easily be applied to a wide set of <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> and in which <a href="https://en.wikipedia.org/wiki/Evaluation_strategy">evaluation strategies</a> can be tested. Regular updates to the <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark</a> will help NLG research become more multilingual and evolve the challenge alongside <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.</abstract>
      <url hash="0df4bca5">2021.gem-1.10</url>
      <doi>10.18653/v1/2021.gem-1.10</doi>
      <bibkey>gehrmann-etal-2021-gem</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gem">GEM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/asset">ASSET</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commongen">CommonGen</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/czech-restaurant-information">Czech restaurant information</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dart">DART</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e2e">E2E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlsum">MLSUM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/totto">ToTTo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/turkcorpus">TurkCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilingua">WikiLingua</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xsum">XSum</pwcdataset>
    </paper>
    <paper id="11">
      <title>Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation : A Case Study of the HuggingFace and GEM Data and Model Cards<fixed-case>H</fixed-case>ugging<fixed-case>F</fixed-case>ace and <fixed-case>GEM</fixed-case> Data and Model Cards</title>
      <author><first>Angelina</first><last>McMillan-Major</last></author>
      <author><first>Salomey</first><last>Osei</last></author>
      <author><first>Juan Diego</first><last>Rodriguez</last></author>
      <author><first>Pawan Sasanka</first><last>Ammanamanchi</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Yacine</first><last>Jernite</last></author>
      <pages>121–135</pages>
      <abstract>Developing documentation guidelines and easy-to-use templates for datasets and models is a challenging task, especially given the variety of backgrounds, skills, and incentives of the people involved in the building of natural language processing (NLP) tools. Nevertheless, the adoption of standard documentation practices across the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> promotes more accessible and detailed descriptions of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP datasets</a> and models, while supporting researchers and developers in reflecting on their work. To help with the standardization of documentation, we present two case studies of efforts that aim to develop reusable documentation templates   the HuggingFace data card, a general purpose card for datasets in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, and the GEM benchmark data and model cards with a focus on natural language generation. We describe our process for developing these <a href="https://en.wikipedia.org/wiki/Template_(word_processing)">templates</a>, including the identification of relevant stakeholder groups, the definition of a set of guiding principles, the use of existing <a href="https://en.wikipedia.org/wiki/Template_(word_processing)">templates</a> as our foundation, and iterative revisions based on feedback.</abstract>
      <url hash="5259f8bd">2021.gem-1.11</url>
      <doi>10.18653/v1/2021.gem-1.11</doi>
      <bibkey>mcmillan-major-etal-2021-reusable</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/asset">ASSET</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gem">GEM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="16">
      <title>Decoding Methods for Neural Narrative Generation</title>
      <author><first>Alexandra</first><last>DeLucia</last></author>
      <author><first>Aaron</first><last>Mueller</last></author>
      <author><first>Xiang Lisa</first><last>Li</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <pages>166–185</pages>
      <abstract>Narrative generation is an open-ended NLP task in which a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> generates a story given a prompt. The task is similar to neural response generation for <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a> ; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparametersspecifically, maximum mutual informationanalyzing results over multiple criteria with automatic and human evaluation. We find that (1) <a href="https://en.wikipedia.org/wiki/Nucleic_acid_test">nucleus sampling</a> is generally best with <a href="https://en.wikipedia.org/wiki/Statistical_threshold">thresholds</a> between 0.7 and 0.9 ; (2) a maximum mutual information objective can improve the quality of generated stories ; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any <a href="https://en.wikipedia.org/wiki/Qualitative_property">qualitative metric</a>.</abstract>
      <url hash="aa1c5b4f">2021.gem-1.16</url>
      <doi>10.18653/v1/2021.gem-1.16</doi>
      <bibkey>delucia-etal-2021-decoding</bibkey>
      <pwccode url="https://github.com/AADeLucia/gpt2-narrative-decoding" additional="false">AADeLucia/gpt2-narrative-decoding</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
  </volume>
</collection>