<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.wanlp">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Fifth Arabic Natural Language Processing Workshop</booktitle>
      <editor><first>Imed</first><last>Zitouni</last></editor>
      <editor><first>Muhammad</first><last>Abdul-Mageed</last></editor>
      <editor><first>Houda</first><last>Bouamor</last></editor>
      <editor><first>Fethi</first><last>Bougares</last></editor>
      <editor><first>Mahmoud</first><last>El-Haj</last></editor>
      <editor><first>Nadi</first><last>Tomeh</last></editor>
      <editor><first>Wajdi</first><last>Zaghouani</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="2ae5a718">2020.wanlp-1.0</url>
      <bibkey>wanlp-2020-arabic</bibkey>
    </frontmatter>
    <paper id="5">
      <title>A Semi-Supervised BERT Approach for Arabic Named Entity Recognition<fixed-case>BERT</fixed-case> Approach for <fixed-case>A</fixed-case>rabic Named Entity Recognition</title>
      <author><first>Chadi</first><last>Helwe</last></author>
      <author><first>Ghassan</first><last>Dib</last></author>
      <author><first>Mohsen</first><last>Shamas</last></author>
      <author><first>Shady</first><last>Elbassuoni</last></author>
      <pages>49–57</pages>
      <abstract>Named entity recognition (NER) plays a significant role in many applications such as <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a>, <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a>, <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, and even <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. Most of the work on NER using <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> was done for non-Arabic languages like <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/French_language">French</a>, and only few studies focused on <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>. This paper proposes a semi-supervised learning approach to train a BERT-based NER model using labeled and semi-labeled datasets. We compared our approach against various baselines, and state-of-the-art Arabic NER tools on three datasets : AQMAR, NEWS, and TWEETS. We report a significant improvement in <a href="https://en.wikipedia.org/wiki/F-measure">F-measure</a> for the AQMAR and the NEWS datasets, which are written in Modern Standard Arabic (MSA), and competitive results for the TWEETS dataset, which contains tweets that are mostly in the Egyptian dialect and contain many mistakes or misspellings.</abstract>
      <url hash="1a068e1d">2020.wanlp-1.5</url>
      <bibkey>helwe-etal-2020-semi</bibkey>
    </paper>
    <paper id="14">
      <title>MANorm : A Normalization Dictionary for Moroccan Arabic Dialect Written in Latin Script<fixed-case>MAN</fixed-case>orm: A Normalization Dictionary for <fixed-case>M</fixed-case>oroccan <fixed-case>A</fixed-case>rabic Dialect Written in <fixed-case>L</fixed-case>atin Script</title>
      <author><first>Randa</first><last>Zarnoufi</last></author>
      <author><first>Hamid</first><last>Jaafar</last></author>
      <author><first>Walid</first><last>Bachri</last></author>
      <author><first>Mounia</first><last>Abik</last></author>
      <pages>155–166</pages>
      <abstract>Social media user generated text is actually the main resource for many NLP tasks. This text, however, does not follow the standard <a href="https://en.wikipedia.org/wiki/Writing_system">rules of writing</a>. Moreover, the use of <a href="https://en.wikipedia.org/wiki/Dialect">dialect</a> such as <a href="https://en.wikipedia.org/wiki/Moroccan_Arabic">Moroccan Arabic</a> in <a href="https://en.wikipedia.org/wiki/Writing">written communications</a> increases further NLP tasks complexity. A <a href="https://en.wikipedia.org/wiki/Dialect">dialect</a> is a <a href="https://en.wikipedia.org/wiki/Spoken_language">verbal language</a> that does not have a standard <a href="https://en.wikipedia.org/wiki/Orthography">orthography</a>. The <a href="https://en.wikipedia.org/wiki/Dialect">written dialect</a> is based on the phonetic transliteration of spoken words which leads users to improvise spelling while writing. Thus, for the same word we can find multiple forms of <a href="https://en.wikipedia.org/wiki/Transliteration">transliterations</a>. Subsequently, it is mandatory to normalize these different <a href="https://en.wikipedia.org/wiki/Transliteration">transliterations</a> to one canonical word form. To reach this goal, we have exploited the powerfulness of word embedding models generated with a corpus of YouTube comments. Besides, using a Moroccan Arabic dialect dictionary that provides the canonical forms, we have built a normalization dictionary that we refer to as MANorm. We have conducted several experiments to demonstrate the efficiency of MANorm, which have shown its usefulness in dialect normalization. We made MANorm freely available online.</abstract>
      <url hash="223258b3">2020.wanlp-1.14</url>
      <bibkey>zarnoufi-etal-2020-manorm</bibkey>
    </paper>
    <paper id="17">
      <title>AraWEAT : Multidimensional Analysis of Biases in Arabic Word Embeddings<fixed-case>A</fixed-case>ra<fixed-case>WEAT</fixed-case>: Multidimensional Analysis of Biases in <fixed-case>A</fixed-case>rabic Word Embeddings</title>
      <author><first>Anne</first><last>Lauscher</last></author>
      <author><first>Rafik</first><last>Takieddin</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>192–199</pages>
      <abstract>Recent work has shown that distributional word vector spaces often encode <a href="https://en.wikipedia.org/wiki/Bias">human biases</a> like <a href="https://en.wikipedia.org/wiki/Sexism">sexism</a> or <a href="https://en.wikipedia.org/wiki/Racism">racism</a>. In this work, we conduct an extensive analysis of biases in Arabic word embeddings by applying a range of recently introduced bias tests on a variety of embedding spaces induced from corpora in Arabic. We measure the presence of biases across several dimensions, namely : embedding models (Skip-Gram, CBOW, and FastText) and vector sizes, types of text (encyclopedic text, and news vs. user-generated content), dialects (Egyptian Arabic vs. Modern Standard Arabic), and time (diachronic analyses over corpora from different time periods). Our analysis yields several interesting findings, e.g., that implicit gender bias in <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> trained on Arabic news corpora steadily increases over time (between 2007 and 2017). We make the Arabic bias specifications (AraWEAT) publicly available.</abstract>
      <url hash="190afe95">2020.wanlp-1.17</url>
      <bibkey>lauscher-etal-2020-araweat</bibkey>
    </paper>
    <paper id="18">
      <title>Parallel resources for Tunisian Arabic Dialect Translation<fixed-case>T</fixed-case>unisian <fixed-case>A</fixed-case>rabic Dialect Translation</title>
      <author><first>Saméh</first><last>Kchaou</last></author>
      <author><first>Rahma</first><last>Boujelbane</last></author>
      <author><first>Lamia</first><last>Hadrich-Belguith</last></author>
      <pages>200–206</pages>
      <abstract>The difficulty of processing <a href="https://en.wikipedia.org/wiki/Dialect">dialects</a> is clearly observed in the high cost of building <a href="https://en.wikipedia.org/wiki/Text_corpus">representative corpus</a>, in particular for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. Indeed, all <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a> require a huge amount and good management of training data, which represents a challenge in a low-resource setting such as the <a href="https://en.wikipedia.org/wiki/Tunisian_Arabic">Tunisian Arabic dialect</a>. In this paper, we present a data augmentation technique to create a parallel corpus for Tunisian Arabic dialect written in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> and standard <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> in order to build a Machine Translation (MT) model. The created <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> was used to build a sentence-based translation model. This <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> reached a BLEU score of 15.03 % on a test set, while it was limited to 13.27 % utilizing the corpus without augmentation.</abstract>
      <url hash="69de3825">2020.wanlp-1.18</url>
      <bibkey>kchaou-etal-2020-parallel</bibkey>
    </paper>
    <paper id="21">
      <title>Improving Arabic Text Categorization Using Transformer Training Diversification<fixed-case>A</fixed-case>rabic Text Categorization Using Transformer Training Diversification</title>
      <author><first>Shammur Absar</first><last>Chowdhury</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <author><first>Jung</first><last>Soon-Gyo</last></author>
      <author><first>Joni</first><last>Salminen</last></author>
      <author><first>Bernard J.</first><last>Jansen</last></author>
      <pages>226–236</pages>
      <abstract>Automatic categorization of short texts, such as <a href="https://en.wikipedia.org/wiki/Headline">news headlines</a> and <a href="https://en.wikipedia.org/wiki/Social_media">social media posts</a>, has many applications ranging from <a href="https://en.wikipedia.org/wiki/Content_analysis">content analysis</a> to <a href="https://en.wikipedia.org/wiki/Recommender_system">recommendation systems</a>. In this paper, we use such <a href="https://en.wikipedia.org/wiki/Categorization">text categorization</a> i.e., labeling the social media posts to categories like ‘sports’, ‘politics’, ‘human-rights’ among others, to showcase the efficacy of <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> across different sources and varieties of <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>. In doing so, we show that diversifying the training data, whether by using diverse training data for the specific task (an increase of 21 % macro F1) or using diverse data to pre-train a BERT model (26 % macro F1), leads to overall improvements in classification effectiveness. In our work, we also introduce two new Arabic text categorization datasets, where the first is composed of social media posts from a popular Arabic news channel that cover <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>, <a href="https://en.wikipedia.org/wiki/Facebook">Facebook</a>, and <a href="https://en.wikipedia.org/wiki/YouTube">YouTube</a>, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and <a href="https://en.wikipedia.org/wiki/Varieties_of_Arabic">dialectal Arabic</a>.</abstract>
      <url hash="ab2499d3">2020.wanlp-1.21</url>
      <bibkey>chowdhury-etal-2020-improving-arabic</bibkey>
      <pwccode url="https://github.com/shammur/arabic_news_text_classification_datasets" additional="false">shammur/arabic_news_text_classification_datasets</pwccode>
    </paper>
    <paper id="22">
      <title>Team Alexa at NADI Shared Task<fixed-case>A</fixed-case>lexa at <fixed-case>NADI</fixed-case> Shared Task</title>
      <author><first>Mutaz</first><last>Younes</last></author>
      <author><first>Nour</first><last>Al-khdour</last></author>
      <author><first>Mohammad</first><last>AL-Smadi</last></author>
      <pages>237–242</pages>
      <abstract>In this paper, we discuss our team’s work on the NADI Shared Task. The <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> requires classifying Arabic tweets among 21 dialects. We tested out different <a href="https://en.wikipedia.org/wiki/Methodology">approaches</a>, and the best <a href="https://en.wikipedia.org/wiki/One_(pronoun)">one</a> was the simplest one. Our best submission was using Multinational Naive Bayes (MNB) classifier (Small and Hsiao, 1985) with <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a> as <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a>. Despite its simplicity, this <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> shows better results than complicated <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> such as BERT. Our best submitted score was 17 % <a href="https://en.wikipedia.org/wiki/F1_score">F1-score</a> and 35 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>.</abstract>
      <url hash="4568a554">2020.wanlp-1.22</url>
      <bibkey>younes-etal-2020-team</bibkey>
    </paper>
    <paper id="27">
      <title>Weighted combination of BERT and N-GRAM features for Nuanced Arabic Dialect Identification<fixed-case>BERT</fixed-case> and N-<fixed-case>GRAM</fixed-case> features for Nuanced <fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Abdellah</first><last>El Mekki</last></author>
      <author><first>Ahmed</first><last>Alami</last></author>
      <author><first>Hamza</first><last>Alami</last></author>
      <author><first>Ahmed</first><last>Khoumsi</last></author>
      <author><first>Ismail</first><last>Berrada</last></author>
      <pages>268–274</pages>
      <abstract>Around the Arab world, different <a href="https://en.wikipedia.org/wiki/Varieties_of_Arabic">Arabic dialects</a> are spoken by more than 300 M persons, and are increasingly popular in social media texts. However, <a href="https://en.wikipedia.org/wiki/Varieties_of_Arabic">Arabic dialects</a> are considered to be low-resource languages, limiting the development of machine-learning based systems for these <a href="https://en.wikipedia.org/wiki/Dialect">dialects</a>. In this paper, we investigate the Arabic dialect identification task, from two perspectives : country-level dialect identification from 21 Arab countries, and province-level dialect identification from 100 provinces. We introduce an unified pipeline of state-of-the-art models, that can handle the two subtasks. Our experimental studies applied to the NADI shared task, show promising results both at the country-level (F1-score of 25.99 %) and the province-level (F1-score of 6.39 %), and thus allow us to be ranked 2nd for the country-level subtask, and 1st in the province-level subtask.</abstract>
      <url hash="519b307c">2020.wanlp-1.27</url>
      <bibkey>el-mekki-etal-2020-weighted</bibkey>
    </paper>
    <paper id="29">
      <title>Faheem at NADI shared task : Identifying the dialect of Arabic tweet<fixed-case>NADI</fixed-case> shared task: Identifying the dialect of <fixed-case>A</fixed-case>rabic tweet</title>
      <author><first>Nouf</first><last>AlShenaifi</last></author>
      <author><first>Aqil</first><last>Azmi</last></author>
      <pages>282–287</pages>
      <abstract>This paper describes Faheem (adj. of understand), our submission to NADI (Nuanced Arabic Dialect Identification) shared task. With so many <a href="https://en.wikipedia.org/wiki/Varieties_of_Arabic">Arabic dialects</a> being under-studied due to the scarcity of the resources, the objective is to identify the <a href="https://en.wikipedia.org/wiki/Varieties_of_Arabic">Arabic dialect</a> used in the tweet, country wise. We propose a machine learning approach where we utilize word-level n-gram (n = 1 to 3) and tf-idf features and feed them to six different <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a>. We train the <a href="https://en.wikipedia.org/wiki/System">system</a> using a <a href="https://en.wikipedia.org/wiki/Data_set">data set</a> of 21,000 tweetsprovided by the organizerscovering twenty-one Arab countries. Our top performing classifiers are : <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic Regression</a>, <a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machines</a>, and Multinomial Na ve Bayes.</abstract>
      <url hash="d83329d1">2020.wanlp-1.29</url>
      <bibkey>alshenaifi-azmi-2020-faheem</bibkey>
    </paper>
    <paper id="31">
      <title>The QMUL / HRBDT contribution to the NADI Arabic Dialect Identification Shared Task<fixed-case>QMUL</fixed-case>/<fixed-case>HRBDT</fixed-case> contribution to the <fixed-case>NADI</fixed-case> <fixed-case>A</fixed-case>rabic Dialect Identification Shared Task</title>
      <author><first>Abdulrahman</first><last>Aloraini</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <author><first>Ayman</first><last>Alhelbawy</last></author>
      <pages>295–301</pages>
      <abstract>We present the Arabic dialect identification system that we used for the country-level subtask of the NADI challenge. Our model consists of three components : BiLSTM-CNN, character-level TF-IDF, and topic modeling features. We represent each tweet using these <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> and feed them into a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural network</a>. We then add an effective <a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)">heuristic</a> that improves the overall performance. We achieved an F1-Macro score of 20.77 % and an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 34.32 % on the test set. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> was also evaluated on the Arabic Online Commentary dataset, achieving results better than the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>.</abstract>
      <url hash="fa2a8391">2020.wanlp-1.31</url>
      <bibkey>aloraini-etal-2020-qmul</bibkey>
    </paper>
    </volume>
</collection>