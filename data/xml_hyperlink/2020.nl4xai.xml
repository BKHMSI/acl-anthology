<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.nl4xai">
  <volume id="1" ingest-date="2021-01-18">
    <meta>
      <booktitle>2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence</booktitle>
      <editor><first>Jose M.</first><last>Alonso</last></editor>
      <editor><first>Alejandro</first><last>Catala</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>November</month>
      <year>2020</year>
      <url hash="19c711f4">2020.nl4xai-1</url>
    </meta>
    <frontmatter>
      <url hash="0ca3b076">2020.nl4xai-1.0</url>
      <bibkey>nl4xai-2020-interactive</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Bias in <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI-systems</a> : A multi-step approach<fixed-case>AI</fixed-case>-systems: A multi-step approach</title>
      <author><first>Eirini</first><last>Ntoutsi</last></author>
      <pages>3–4</pages>
      <abstract>Algorithmic-based decision making powered via AI and (big) data has already penetrated into almost all spheres of human life, from <a href="https://en.wikipedia.org/wiki/Recommender_system">content recommendation</a> and <a href="https://en.wikipedia.org/wiki/Health_care">healthcare</a> to <a href="https://en.wikipedia.org/wiki/Predictive_policing">predictive policing</a> and <a href="https://en.wikipedia.org/wiki/Self-driving_car">autonomous driving</a>, deeply affecting everyone, anywhere, anytime. While <a href="https://en.wikipedia.org/wiki/Technology">technology</a> allows previously unthinkable optimizations in the automation of expensive human decision making, the risks that the <a href="https://en.wikipedia.org/wiki/Technology">technology</a> can pose are also high, leading to an ever increasing public concern about the impact of the <a href="https://en.wikipedia.org/wiki/Technology">technology</a> in our lives. The area of responsible AI has recently emerged in an attempt to put humans at the center of AI-based systems by considering aspects, such as <a href="https://en.wikipedia.org/wiki/Social_justice">fairness</a>, <a href="https://en.wikipedia.org/wiki/Reliability_engineering">reliability</a> and <a href="https://en.wikipedia.org/wiki/Privacy">privacy of decision-making systems</a>. In this talk, we will focus on the fairness aspect. We will start with understanding the many sources of <a href="https://en.wikipedia.org/wiki/Bias">bias</a> and how <a href="https://en.wikipedia.org/wiki/Bias">biases</a> can enter at each step of the learning process and even get propagated / amplified from previous steps. We will continue with methods for mitigating <a href="https://en.wikipedia.org/wiki/Bias">bias</a> which typically focus on some step of the pipeline (data, algorithms or results) and why it is important to target bias in each step and collectively, in the whole (machine) learning pipeline. We will conclude this talk by discussing accountability issues in connection to <a href="https://en.wikipedia.org/wiki/Bias">bias</a> and in particular, proactive consideration via bias-aware data collection, processing and algorithmic selection and retroactive consideration via explanations.</abstract>
      <url hash="4aecb088">2020.nl4xai-1.2</url>
      <bibkey>ntoutsi-2020-bias</bibkey>
    </paper>
    <paper id="3">
      <title>Content Selection for Explanation Requests in Customer-Care Domain</title>
      <author><first>Luca</first><last>Anselma</last></author>
      <author><first>Mirko</first><last>Di Lascio</last></author>
      <author><first>Dario</first><last>Mana</last></author>
      <author><first>Alessandro</first><last>Mazzei</last></author>
      <author><first>Manuela</first><last>Sanguinetti</last></author>
      <pages>5–10</pages>
      <abstract>This paper describes a content selection module for the generation of explanations in a <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue system</a> designed for customer care domain. First we describe the construction of a corpus of a dialogues containing explanation requests from customers to a virtual agent of a telco, and second we study and formalize the importance of a specific information content for the generated message. In particular, we adapt the notions of <a href="https://en.wikipedia.org/wiki/Importance">importance</a> and <a href="https://en.wikipedia.org/wiki/Relevance">relevance</a> in the case of schematic knowledge bases.</abstract>
      <url hash="c092c1a7">2020.nl4xai-1.3</url>
      <bibkey>anselma-etal-2020-content</bibkey>
    </paper>
    <paper id="5">
      <title>The Natural Language Pipeline, Neural Text Generation and Explainability</title>
      <author><first>Juliette</first><last>Faille</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>16–21</pages>
      <abstract>End-to-end encoder-decoder approaches to data-to-text generation are often black boxes whose predictions are difficult to explain. Breaking up the end-to-end model into sub-modules is a natural way to address this problem. The traditional pre-neural Natural Language Generation (NLG) pipeline provides a framework for breaking up the end-to-end encoder-decoder. We survey recent papers that integrate traditional NLG submodules in neural approaches and analyse their explainability. Our survey is a first step towards building explainable neural NLG models.</abstract>
      <url hash="fc3dbb07">2020.nl4xai-1.5</url>
      <bibkey>faille-etal-2020-natural</bibkey>
    </paper>
    <paper id="11">
      <title>Toward Natural Language Mitigation Strategies for Cognitive Biases in Recommender Systems</title>
      <author><first>Alisa</first><last>Rieger</last></author>
      <author><first>Mariët</first><last>Theune</last></author>
      <author><first>Nava</first><last>Tintarev</last></author>
      <pages>50–54</pages>
      <abstract>Cognitive biases in the context of consuming online information filtered by <a href="https://en.wikipedia.org/wiki/Recommender_system">recommender systems</a> may lead to sub-optimal choices. One approach to mitigate such <a href="https://en.wikipedia.org/wiki/Bias">biases</a> is through interface and interaction design. This survey reviews studies focused on cognitive bias mitigation of recommender system users during two processes : 1) item selection and 2) preference elicitation. It highlights a number of promising directions for Natural Language Generation research for mitigating <a href="https://en.wikipedia.org/wiki/Cognitive_bias">cognitive bias</a> including : the need for <a href="https://en.wikipedia.org/wiki/Personalization">personalization</a>, as well as for transparency and control.</abstract>
      <url hash="f27efe74">2020.nl4xai-1.11</url>
      <bibkey>rieger-etal-2020-toward</bibkey>
    </paper>
    <paper id="13">
      <title>Learning from Explanations and Demonstrations : A Pilot Study</title>
      <author><first>Silvia</first><last>Tulli</last></author>
      <author><first>Sebastian</first><last>Wallkötter</last></author>
      <author><first>Ana</first><last>Paiva</last></author>
      <author><first>Francisco S.</first><last>Melo</last></author>
      <author><first>Mohamed</first><last>Chetouani</last></author>
      <pages>61–66</pages>
      <abstract>AI has become prominent in a growing number of <a href="https://en.wikipedia.org/wiki/System">systems</a>, and, as a direct consequence, the desire for explainability in such <a href="https://en.wikipedia.org/wiki/System">systems</a> has become prominent as well. To build explainable systems, a large portion of existing research uses various kinds of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language technologies</a>, e.g., <a href="https://en.wikipedia.org/wiki/Speech_synthesis">text-to-speech mechanisms</a>, or string visualizations. Here, we provide an overview of the challenges associated with <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language explanations</a> by reviewing existing literature. Additionally, we discuss the relationship between explainability and <a href="https://en.wikipedia.org/wiki/Knowledge_transfer">knowledge transfer</a> in <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>. We argue that explainability methods, in particular <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> that model the recipient of an explanation, might help increasing sample efficiency. For this, we present a computational approach to optimize the learner’s performance using explanations of another agent and discuss our results in light of effective natural language explanations for humans.</abstract>
      <url hash="80ec638a">2020.nl4xai-1.13</url>
      <bibkey>tulli-etal-2020-learning</bibkey>
    </paper>
    <paper id="14">
      <title>Generating Explanations of Action Failures in a <a href="https://en.wikipedia.org/wiki/Cognitive_robotics">Cognitive Robotic Architecture</a></title>
      <author><first>Ravenna</first><last>Thielstrom</last></author>
      <author><first>Antonio</first><last>Roque</last></author>
      <author><first>Meia</first><last>Chita-Tegmark</last></author>
      <author><first>Matthias</first><last>Scheutz</last></author>
      <pages>67–72</pages>
      <abstract>We describe an approach to generating explanations about why robot actions fail, focusing on the considerations of robots that are run by cognitive robotic architectures. We define a set of Failure Types and Explanation Templates, motivating them by the needs and constraints of cognitive architectures that use action scripts and interpretable belief states, and describe content realization and surface realization in this context. We then describe an <a href="https://en.wikipedia.org/wiki/Evaluation">evaluation</a> that can be extended to further study the effects of varying the explanation templates.</abstract>
      <url hash="fd005fb3">2020.nl4xai-1.14</url>
      <bibkey>thielstrom-etal-2020-generating</bibkey>
    </paper>
  </volume>
</collection>