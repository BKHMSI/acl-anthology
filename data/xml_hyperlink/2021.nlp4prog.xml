<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.nlp4prog">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)</booktitle>
      <editor><first>Royi</first><last>Lachmy</last></editor>
      <editor><first>Ziyu</first><last>Yao</last></editor>
      <editor><first>Greg</first><last>Durrett</last></editor>
      <editor><first>Milos</first><last>Gligoric</last></editor>
      <editor><first>Junyi Jessy</first><last>Li</last></editor>
      <editor><first>Ray</first><last>Mooney</last></editor>
      <editor><first>Graham</first><last>Neubig</last></editor>
      <editor><first>Yu</first><last>Su</last></editor>
      <editor><first>Huan</first><last>Sun</last></editor>
      <editor><first>Reut</first><last>Tsarfaty</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="f287d5ec">2021.nlp4prog-1</url>
    </meta>
    <frontmatter>
      <url hash="a66f82be">2021.nlp4prog-1.0</url>
      <bibkey>nlp4prog-2021-natural</bibkey>
    </frontmatter>
    <paper id="2">
      <title>ConTest : A Unit Test Completion Benchmark featuring Context<fixed-case>C</fixed-case>on<fixed-case>T</fixed-case>est: A Unit Test Completion Benchmark featuring Context</title>
      <author><first>Johannes</first><last>Villmow</last></author>
      <author><first>Jonas</first><last>Depoix</last></author>
      <author><first>Adrian</first><last>Ulges</last></author>
      <pages>17–25</pages>
      <abstract>We introduce CONTEST, a <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark</a> for NLP-based unit test completion, the task of predicting a test’s assert statements given its setup and focal method, i.e. the <a href="https://en.wikipedia.org/wiki/Methodology">method</a> to be tested. ConTest is large-scale (with 365k datapoints). Besides the test code and tested code, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> also features context code called by either. We found <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a> to be crucial for accurately predicting assertions. We also introduce baselines based on transformer encoder-decoders, and study the effects of including <a href="https://en.wikipedia.org/wiki/Syntax">syntactic information</a> and <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a>. Overall, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> achieve a BLEU score of 38.2, while only generating unparsable code in 1.92 % of cases.</abstract>
      <url hash="0bb865de">2021.nlp4prog-1.2</url>
      <doi>10.18653/v1/2021.nlp4prog-1.2</doi>
      <bibkey>villmow-etal-2021-contest</bibkey>
    </paper>
    <paper id="3">
      <title>CommitBERT : Commit Message Generation Using Pre-Trained Programming Language Model<fixed-case>C</fixed-case>ommit<fixed-case>BERT</fixed-case>: Commit Message Generation Using Pre-Trained Programming Language Model</title>
      <author><first>Tae Hwan</first><last>Jung</last></author>
      <pages>26–33</pages>
      <abstract>Commit message is a document that summarizes source code changes in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a>. A good commit message clearly shows the source code changes, so this enhances collaboration between developers. Therefore, our work is to develop a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> that automatically writes the commit message. To this end, we release 345 K datasets consisting of code modification and commit messages in six programming languages (Python, PHP, <a href="https://en.wikipedia.org/wiki/Go_(programming_language)">Go</a>, <a href="https://en.wikipedia.org/wiki/Java_(programming_language)">Java</a>, <a href="https://en.wikipedia.org/wiki/JavaScript">JavaScript</a>, and Ruby). Similar to the neural machine translation (NMT) model, using our dataset, we feed the code modification to the encoder input and the commit message to the decoder input and measure the result of the generated commit message with BLEU-4. Also, we propose the following two training methods to improve the result of generating the commit message : (1) A method of preprocessing the input to feed the code modification to the encoder input. (2) A method that uses an initial weight suitable for the code domain to reduce the gap in contextual representation between programming language (PL) and natural language (NL).</abstract>
      <url hash="dd9303f1">2021.nlp4prog-1.3</url>
      <doi>10.18653/v1/2021.nlp4prog-1.3</doi>
      <bibkey>jung-2021-commitbert</bibkey>
      <pwccode url="https://github.com/graykode/commit-autosuggestions" additional="false">graykode/commit-autosuggestions</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
    </paper>
    <paper id="4">
      <title>Time-Efficient Code Completion Model for the <a href="https://en.wikipedia.org/wiki/R_(programming_language)">R Programming Language</a><fixed-case>R</fixed-case> Programming Language</title>
      <author><first>Artem</first><last>Popov</last></author>
      <author><first>Dmitrii</first><last>Orekhov</last></author>
      <author><first>Denis</first><last>Litvinov</last></author>
      <author><first>Nikolay</first><last>Korolev</last></author>
      <author><first>Gleb</first><last>Morgachev</last></author>
      <pages>34–39</pages>
      <abstract>In this paper we present a deep learning code completion model for the <a href="https://en.wikipedia.org/wiki/R_(programming_language)">R language</a>. We introduce several <a href="https://en.wikipedia.org/wiki/Software_development_process">techniques</a> to utilize <a href="https://en.wikipedia.org/wiki/Language_model">language modeling based architecture</a> in the <a href="https://en.wikipedia.org/wiki/Autocomplete">code completion task</a>. With these techniques, the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> requires low resources, but still achieves high quality. We also present an evaluation dataset for the <a href="https://en.wikipedia.org/wiki/R_(programming_language)">R language completion task</a>. Our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> contains multiple autocompletion usage contexts that provides robust validation results. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is publicly available.</abstract>
      <url hash="319fba13">2021.nlp4prog-1.4</url>
      <doi>10.18653/v1/2021.nlp4prog-1.4</doi>
      <bibkey>popov-etal-2021-time</bibkey>
      <pwccode url="https://github.com/arti32lehtonen/rcompletion_evaluation_dataset" additional="false">arti32lehtonen/rcompletion_evaluation_dataset</pwccode>
    </paper>
    <paper id="7">
      <title>Shellcode_IA32 : A Dataset for Automatic Shellcode Generation<fixed-case>S</fixed-case>hellcode_<fixed-case>IA</fixed-case>32: A Dataset for Automatic Shellcode Generation</title>
      <author><first>Pietro</first><last>Liguori</last></author>
      <author><first>Erfan</first><last>Al-Hossami</last></author>
      <author><first>Domenico</first><last>Cotroneo</last></author>
      <author><first>Roberto</first><last>Natella</last></author>
      <author><first>Bojan</first><last>Cukic</last></author>
      <author><first>Samira</first><last>Shaikh</last></author>
      <pages>58–64</pages>
      <abstract>We take the first step to address the task of <a href="https://en.wikipedia.org/wiki/Shellcode">automatically generating shellcodes</a>, i.e., small pieces of code used as a payload in the exploitation of a <a href="https://en.wikipedia.org/wiki/Vulnerability_(computing)">software vulnerability</a>, starting from <a href="https://en.wikipedia.org/wiki/Comment_(computer_programming)">natural language comments</a>. We assemble and release a novel dataset (Shellcode_IA32), consisting of challenging but common assembly instructions with their natural language descriptions. We experiment with standard methods in neural machine translation (NMT) to establish baseline performance levels on this task.</abstract>
      <url hash="6895273b">2021.nlp4prog-1.7</url>
      <doi>10.18653/v1/2021.nlp4prog-1.7</doi>
      <bibkey>liguori-etal-2021-shellcode</bibkey>
      <pwccode url="https://github.com/dessertlab/Shellcode_IA32" additional="false">dessertlab/Shellcode_IA32</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/shellcode-ia32">Shellcode_IA32</pwcdataset>
    </paper>
    <paper id="8">
      <title>Reading StackOverflow Encourages Cheating : Adding Question Text Improves Extractive Code Generation<fixed-case>S</fixed-case>tack<fixed-case>O</fixed-case>verflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation</title>
      <author><first>Gabriel</first><last>Orlanski</last></author>
      <author><first>Alex</first><last>Gittens</last></author>
      <pages>65–76</pages>
      <abstract>Answering a programming question with only its title is difficult as salient contextual information is left out. To address this, we present a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> of over 40,000 StackOverflow question texts to be used in conjunction with the corresponding intents from the CoNaLa dataset (Yin et al., 2018). Using both the <a href="https://en.wikipedia.org/wiki/Intention">intent</a> and the question body, we use <a href="https://en.wikipedia.org/wiki/Bay_Area_Rapid_Transit">BART</a> to establish a baseline BLEU score of 34.35 for this new task. We then find further improvements of 2.8 % by combining the mined CoNaLa data with the labeled data to achieve a 35.32 BLEU score. We then evaluate the prior state-of-the-art CoNaLa models with this additional <a href="https://en.wikipedia.org/wiki/Data">data</a>. We find that our proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> of using the body and mined data beats that of the previous <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> by a 71.96 % BLEU score. Finally, we perform ablations that prove that BART is an unsupervised multimodal learner and examine its extractive behavior.</abstract>
      <url hash="959970d3">2021.nlp4prog-1.8</url>
      <doi>10.18653/v1/2021.nlp4prog-1.8</doi>
      <bibkey>orlanski-gittens-2021-reading</bibkey>
      <pwccode url="https://github.com/gabeorlanski/stackoverflow-encourages-cheating" additional="false">gabeorlanski/stackoverflow-encourages-cheating</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conala-ext">CoNaLa-Ext</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conala">CoNaLa</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/juice">JuICe</pwcdataset>
    </paper>
    <paper id="9">
      <title>Text-to-SQL in the Wild : A Naturally-Occurring Dataset Based on Stack Exchange Data<fixed-case>SQL</fixed-case> in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data</title>
      <author><first>Moshe</first><last>Hazoom</last></author>
      <author><first>Vibhor</first><last>Malik</last></author>
      <author><first>Ben</first><last>Bogin</last></author>
      <pages>77–87</pages>
      <abstract>Most available semantic parsing datasets, comprising of pairs of natural utterances and logical forms, were collected solely for the purpose of training and evaluation of <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding systems</a>. As a result, they do not contain any of the richness and variety of natural-occurring utterances, where humans ask about data they need or are curious about. In this work, we release <a href="https://en.wikipedia.org/wiki/Stack_Exchange">SEDE</a>, a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> with 12,023 pairs of utterances and <a href="https://en.wikipedia.org/wiki/SQL">SQL queries</a> collected from real usage on the <a href="https://en.wikipedia.org/wiki/Stack_Exchange">Stack Exchange website</a>. We show that these pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset, propose an evaluation metric based on comparison of partial query clauses that is more suitable for real-world queries, and conduct experiments with strong baselines, showing a large gap between the performance on SEDE compared to other common datasets.</abstract>
      <url hash="54167235">2021.nlp4prog-1.9</url>
      <doi>10.18653/v1/2021.nlp4prog-1.9</doi>
      <bibkey>hazoom-etal-2021-text</bibkey>
      <pwccode url="https://github.com/hirupert/sede" additional="false">hirupert/sede</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sede">SEDE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/spider-1">SPIDER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="10">
      <title>Bag-of-Words Baselines for Semantic Code Search</title>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Andrew</first><last>Yates</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>88–94</pages>
      <abstract>The task of semantic code search is to retrieve <a href="https://en.wikipedia.org/wiki/Snippet_(programming)">code snippets</a> from a <a href="https://en.wikipedia.org/wiki/Text_corpus">source code corpus</a> based on an information need expressed in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>. The semantic gap between natural language and programming languages has for long been regarded as one of the most significant obstacles to the effectiveness of keyword-based information retrieval (IR) methods. It is a common assumption that traditional bag-of-words IR methods are poorly suited for semantic code search : our work empirically investigates this assumption. Specifically, we examine the effectiveness of two traditional IR methods, namely <a href="https://en.wikipedia.org/wiki/BM25">BM25</a> and <a href="https://en.wikipedia.org/wiki/RM3">RM3</a>, on the CodeSearchNet Corpus, which consists of natural language queries paired with relevant code snippets. We find that the two keyword-based methods outperform several pre-BERT neural models. We also compare several code-specific data pre-processing strategies and find that specialized tokenization improves effectiveness.</abstract>
      <url hash="3f972535">2021.nlp4prog-1.10</url>
      <doi>10.18653/v1/2021.nlp4prog-1.10</doi>
      <bibkey>zhang-etal-2021-bag</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
    </paper>
  </volume>
</collection>