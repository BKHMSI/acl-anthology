<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.argmining">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the 7th Workshop on Argument Mining</booktitle>
      <editor><first>Elena</first><last>Cabrio</last></editor>
      <editor><first>Serena</first><last>Villata</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="9d500249">2020.argmining-1.0</url>
      <bibkey>argmining-2020-argument</bibkey>
    </frontmatter>
    <paper id="1">
      <title>DebateSum : A large-scale argument mining and summarization dataset<fixed-case>D</fixed-case>ebate<fixed-case>S</fixed-case>um: A large-scale argument mining and summarization dataset</title>
      <author><first>Allen</first><last>Roush</last></author>
      <author><first>Arvind</first><last>Balaji</last></author>
      <pages>1–7</pages>
      <abstract>Prior work in <a href="https://en.wikipedia.org/wiki/Argument_mining">Argument Mining</a> frequently alludes to its potential applications in automatic debating systems. Despite this focus, almost no <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> or <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> exist which apply <a href="https://en.wikipedia.org/wiki/Natural-language_processing">natural language processing techniques</a> to problems found within competitive formal debate. To remedy this, we present the DebateSum dataset. DebateSum consists of 187,386 unique pieces of evidence with corresponding <a href="https://en.wikipedia.org/wiki/Argument">argument</a> and extractive summaries. DebateSum was made using <a href="https://en.wikipedia.org/wiki/Data">data</a> compiled by competitors within the <a href="https://en.wikipedia.org/wiki/National_Speech_and_Debate_Association">National Speech and Debate Association</a> over a 7year period. We train several transformer summarization models to benchmark <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a> performance on DebateSum. We also introduce a set of fasttext word-vectors trained on DebateSum called debate2vec. Finally, we present a <a href="https://en.wikipedia.org/wiki/Web_search_engine">search engine</a> for this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> which is utilized extensively by members of the <a href="https://en.wikipedia.org/wiki/National_Speech_and_Debate_Association">National Speech and Debate Association</a> today. The DebateSum search engine is available to the public here : http://www.debate.cards</abstract>
      <url hash="3352bff5">2020.argmining-1.1</url>
      <bibkey>roush-balaji-2020-debatesum</bibkey>
      <pwccode url="https://github.com/arvind-balaji/debate-cards" additional="true">arvind-balaji/debate-cards</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/debatesum">DebateSum</pwcdataset>
    </paper>
    <paper id="2">
      <title>Annotating Topics, Stance, Argumentativeness and Claims in Dutch Social Media Comments : A Pilot Study<fixed-case>D</fixed-case>utch Social Media Comments: A Pilot Study</title>
      <author><first>Nina</first><last>Bauwelinck</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <pages>8–18</pages>
      <abstract>One of the major challenges currently facing the field of argumentation mining is the lack of consensus on how to analyse argumentative user-generated texts such as online comments. The theoretical motivations underlying the annotation guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. This pilot study reports on the <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> of a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> of 100 Dutch user comments made in response to politically-themed news articles on <a href="https://en.wikipedia.org/wiki/Facebook">Facebook</a>. The annotation covers topic and aspect labelling, stance labelling, argumentativeness detection and claim identification. Our IAA study reports substantial agreement scores for argumentativeness detection (0.76 Fleiss’ kappa) and moderate agreement for claim labelling (0.45 Fleiss’ kappa). We provide a clear justification of the theories and definitions underlying the design of our <a href="https://en.wikipedia.org/wiki/Guideline">guidelines</a>. Our analysis of the annotations signal the importance of adjusting our guidelines to include allowances for missing context information and defining the concept of argumentativeness in connection with <a href="https://en.wikipedia.org/wiki/List_of_human_positions">stance</a>. Our annotated corpus and associated guidelines are made publicly available.</abstract>
      <url hash="bb2eccd1">2020.argmining-1.2</url>
      <bibkey>bauwelinck-lefever-2020-annotating</bibkey>
    </paper>
    <paper id="5">
      <title>Aspect-Based Argument Mining</title>
      <author><first>Dietrich</first><last>Trautmann</last></author>
      <pages>41–52</pages>
      <abstract>Computational Argumentation in general and <a href="https://en.wikipedia.org/wiki/Argument_mining">Argument Mining</a> in particular are important research fields. In previous works, many of the challenges to automatically extract and to some degree reason over natural language arguments were addressed. The tools to extract <a href="https://en.wikipedia.org/wiki/Argument_of_a_function">argument units</a> are increasingly available and further open problems can be addressed. In this work, we are presenting the task of Aspect-Based Argument Mining (ABAM), with the essential subtasks of Aspect Term Extraction (ATE) and Nested Segmentation (NS). At the first instance, we create and release an annotated corpus with aspect information on the token-level. We consider <a href="https://en.wikipedia.org/wiki/Element_(mathematics)">aspects</a> as the main point(s) argument units are addressing. This information is important for further downstream tasks such as argument ranking, argument summarization and generation, as well as the search for counter-arguments on the aspect-level. We present several experiments using state-of-the-art supervised architectures and demonstrate their performance for both of the subtasks. The annotated benchmark is available at https://github.com/trtm/ABAM.</abstract>
      <url hash="12cdc26e">2020.argmining-1.5</url>
      <bibkey>trautmann-2020-aspect</bibkey>
      <pwccode url="https://github.com/trtm/ABAM" additional="false">trtm/ABAM</pwccode>
    </paper>
    <paper id="6">
      <title>Annotation and Detection of Arguments in Tweets</title>
      <author><first>Robin</first><last>Schaefer</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>53–58</pages>
      <abstract>Notwithstanding the increasing role <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> plays in modern political and social discourse, resources built for conducting <a href="https://en.wikipedia.org/wiki/Argument_mining">argument mining</a> on <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a> remain limited. In this paper, we present a new <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> of <a href="https://en.wikipedia.org/wiki/Twitter">German tweets</a> annotated for <a href="https://en.wikipedia.org/wiki/Argument_(linguistics)">argument components</a>. To the best of our knowledge, this is the first <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> containing not only annotated full tweets but also argumentative spans within tweets. We further report first promising results using <a href="https://en.wikipedia.org/wiki/Supervised_classification">supervised classification</a> (F1 : 0.82) and sequence labeling (F1 : 0.72) approaches.</abstract>
      <url hash="e177aeab">2020.argmining-1.6</url>
      <bibkey>schaefer-stede-2020-annotation</bibkey>
      <pwccode url="https://github.com/robinschaefer/climate-tweet-corpus" additional="false">robinschaefer/climate-tweet-corpus</pwccode>
    </paper>
    <paper id="8">
      <title>ECHR : <a href="https://en.wikipedia.org/wiki/Corpus_Juris_Civilis">Legal Corpus</a> for Argument Mining<fixed-case>ECHR</fixed-case>: Legal Corpus for Argument Mining</title>
      <author><first>Prakash</first><last>Poudyal</last></author>
      <author><first>Jaromir</first><last>Savelka</last></author>
      <author><first>Aagje</first><last>Ieven</last></author>
      <author><first>Marie Francine</first><last>Moens</last></author>
      <author><first>Teresa</first><last>Goncalves</last></author>
      <author><first>Paulo</first><last>Quaresma</last></author>
      <pages>67–75</pages>
      <abstract>In this paper, we publicly release an annotated corpus of 42 decisions of the <a href="https://en.wikipedia.org/wiki/European_Court_of_Human_Rights">European Court of Human Rights (ECHR)</a>. The <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> is annotated in terms of three types of <a href="https://en.wikipedia.org/wiki/Clause_(logic)">clauses</a> useful in <a href="https://en.wikipedia.org/wiki/Argument_mining">argument mining</a> : premise, conclusion, and non-argument parts of the text. Furthermore, relationships among the premises and conclusions are mapped. We present baselines for three <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> that lead from <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured texts</a> to structured arguments. The tasks are argument clause recognition, clause relation prediction, and premise / conclusion recognition. Despite a straightforward application of the bidirectional encoders from Transformers (BERT), we obtained very promising results F1 0.765 on argument recognition, 0.511 on relation prediction, and 0.859/0.628 on premise / conclusion recognition). The results suggest the usefulness of pre-trained language models based on <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural network architectures</a> in <a href="https://en.wikipedia.org/wiki/Argument_mining">argument mining</a>. Because of the simplicity of the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>, there is ample space for improvement in future work based on the released corpus.</abstract>
      <url hash="efde1688">2020.argmining-1.8</url>
      <bibkey>poudyal-etal-2020-echr</bibkey>
    </paper>
    <paper id="11">
      <title>Annotating argumentation in Swedish social media<fixed-case>S</fixed-case>wedish social media</title>
      <author><first>Anna</first><last>Lindahl</last></author>
      <pages>100–105</pages>
      <abstract>This paper presents a small study of annotating <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argumentation</a> in Swedish social media. Annotators were asked to annotate spans of <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argumentation</a> in 9 threads from two <a href="https://en.wikipedia.org/wiki/Internet_forum">discussion forums</a>. At the post level, Cohen’s k and Krippendorff’s alpha 0.48 was achieved. When manually inspecting the <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> the annotators seemed to agree when conditions in the guidelines were explicitly met, but implicit argumentation and opinions, resulting in annotators having to interpret what’s missing in the text, caused disagreements.</abstract>
      <url hash="587b0cf2">2020.argmining-1.11</url>
      <bibkey>lindahl-2020-annotating</bibkey>
    </paper>
    </volume>
</collection>