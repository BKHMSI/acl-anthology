<?xml version='1.0' encoding='utf-8'?>
<collection id="2019.icon">
  <volume id="1" ingest-date="2021-05-10">
    <meta>
      <booktitle>Proceedings of the 16th International Conference on Natural Language Processing</booktitle>
      <editor><first>Dipti Misra</first><last>Sharma</last></editor>
      <editor><first>Pushpak</first><last>Bhattacharya</last></editor>
      <publisher>NLP Association of India</publisher>
      <address>International Institute of Information Technology, Hyderabad, India</address>
      <month>December</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="8847b7d4">2019.icon-1.0</url>
      <bibkey>icon-2019-international</bibkey>
    </frontmatter>
    <paper id="2">
      <title>A Deep Ensemble Framework for Fake News Detection and Multi-Class Classification of Short Political Statements</title>
      <author><first>Arjun</first><last>Roy</last></author>
      <author><first>Kingshuk</first><last>Basak</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>9–17</pages>
      <abstract>Fake news, <a href="https://en.wikipedia.org/wiki/Rumor">rumor</a>, incorrect information, and misinformation detection are nowadays crucial issues as these might have serious consequences for our social fabrics. Such information is increasing rapidly due to the availability of enormous web information sources including <a href="https://en.wikipedia.org/wiki/Social_media">social media feeds</a>, <a href="https://en.wikipedia.org/wiki/Blog">news blogs</a>, <a href="https://en.wikipedia.org/wiki/Online_newspaper">online newspapers</a> etc. In this paper, we develop various deep learning models for detecting <a href="https://en.wikipedia.org/wiki/Fake_news">fake news</a> and classifying them into the pre-defined fine-grained categories. At first, we develop individual models based on Convolutional Neural Network (CNN), and Bi-directional Long Short Term Memory (Bi-LSTM) networks. The <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> obtained from these two <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are fed into a Multi-layer Perceptron Model (MLP) for the final <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>. Our experiments on a <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmark dataset</a> show promising results with an overall <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 44.87 %, which outperforms the current state of the arts.</abstract>
      <url hash="d3430c1c">2019.icon-1.2</url>
      <bibkey>roy-etal-2019-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/liar">LIAR</pwcdataset>
    </paper>
    <paper id="4">
      <title>Introducing Aspects of <a href="https://en.wikipedia.org/wiki/Creativity">Creativity</a> in Automatic Poetry Generation</title>
      <author><first>Brendan</first><last>Bena</last></author>
      <author><first>Jugal</first><last>Kalita</last></author>
      <pages>26–35</pages>
      <abstract>Poetry Generation involves teaching systems to automatically generate text that resembles <a href="https://en.wikipedia.org/wiki/Poetry">poetic work</a>. A <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning system</a> can learn to generate <a href="https://en.wikipedia.org/wiki/Poetry">poetry</a> on its own by training on a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus of poems</a> and modeling the particular style of language. In this paper, we propose taking an approach that fine-tunes GPT-2, a pre-trained language model, to our downstream task of poetry generation. We extend prior work on poetry generation by introducing creative elements. Specifically, we generate <a href="https://en.wikipedia.org/wiki/Poetry">poems</a> that express emotion and elicit the same in readers, and <a href="https://en.wikipedia.org/wiki/Poetry">poems</a> that use the language of dreamscalled dream poetry. We are able to produce <a href="https://en.wikipedia.org/wiki/Poetry">poems</a> that correctly elicit the emotions of sadness and joy 87.5 and 85 percent, respectively, of the time. We produce dreamlike poetry by training on a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus of texts</a> that describe <a href="https://en.wikipedia.org/wiki/Dream">dreams</a>. Poems from this <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> are shown to capture elements of <a href="https://en.wikipedia.org/wiki/Dream_vision">dream poetry</a> with scores of no less than 3.2 on the <a href="https://en.wikipedia.org/wiki/Likert_scale">Likert scale</a>. We perform crowdsourced human-evaluation for all our <a href="https://en.wikipedia.org/wiki/Poetry">poems</a>. We also make use of the Coh-Metrix tool, outlining metrics we use to gauge the quality of text generated.</abstract>
      <url hash="2200f374">2019.icon-1.4</url>
      <bibkey>bena-kalita-2019-introducing</bibkey>
    </paper>
    <paper id="5">
      <title>Incorporating Sub-Word Level Information in Language Invariant Neural Event Detection</title>
      <author><first>Suhan</first><last>Prabhu</last></author>
      <author><first>Pranav</first><last>Goel</last></author>
      <author><first>Alok</first><last>Debnath</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>36–44</pages>
      <abstract>Detection of TimeML events in text have traditionally been done on <a href="https://en.wikipedia.org/wiki/Text_corpus">corpora</a> such as TimeBanks. However, <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning methods</a> have not been applied to these <a href="https://en.wikipedia.org/wiki/Corpus_linguistics">corpora</a>, because these <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> seldom contain more than 10,000 <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">event mentions</a>. Traditional architectures revolve around highly feature engineered, language specific statistical models. In this paper, we present a Language Invariant Neural Event Detection (ALINED) architecture. ALINED uses an aggregation of both sub-word level features as well as lexical and structural information. This is achieved by combining convolution over character embeddings, with recurrent layers over contextual word embeddings. We find that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> extracts relevant <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> for event span identification without relying on language specific features. We compare the performance of our language invariant model to the current state-of-the-art in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>, <a href="https://en.wikipedia.org/wiki/Italian_language">Italian</a> and <a href="https://en.wikipedia.org/wiki/French_language">French</a>. We outperform the F1-score of the state of the art in <a href="https://en.wikipedia.org/wiki/English_language">English</a> by 1.65 points. We achieve <a href="https://en.wikipedia.org/wiki/International_Federation_of_the_Phonographic_Industry">F1-scores</a> of 84.96, 80.87 and 74.81 on <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>, <a href="https://en.wikipedia.org/wiki/Italian_language">Italian</a> and <a href="https://en.wikipedia.org/wiki/French_language">French</a> respectively which is comparable to the current states of the art for these languages. We also introduce the automatic annotation of events in <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>, a low resource language, with an F1-Score of 77.13.</abstract>
      <url hash="dea3fc5f">2019.icon-1.5</url>
      <bibkey>prabhu-etal-2019-incorporating</bibkey>
    </paper>
    <paper id="6">
      <title>Event Centric Entity Linking for Hindi News Articles : A Knowledge Graph Based Approach<fixed-case>H</fixed-case>indi News Articles: A Knowledge Graph Based Approach</title>
      <author><first>Pranav</first><last>Goel</last></author>
      <author><first>Suhan</first><last>Prabhu</last></author>
      <author><first>Alok</first><last>Debnath</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>45–55</pages>
      <abstract>We describe the development of a <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> from an event annotated corpus by presenting a <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a> that identifies and extracts the relations between entities and events from <a href="https://en.wikipedia.org/wiki/Hindi">Hindi news articles</a>. Due to the semantic implications of argument identification for events in <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>, we use a combined syntactic argument and semantic role identification methodology. To the best of our knowledge, no other <a href="https://en.wikipedia.org/wiki/Architecture">architecture</a> exists for this purpose. The extracted combined role information is incorporated in a <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> that can be queried via subgraph extraction for basic questions. The architectures presented in this paper can be used for participant extraction and event-entity linking in most <a href="https://en.wikipedia.org/wiki/Indo-Aryan_languages">Indo-Aryan languages</a>, due to similar syntactic and semantic properties of event arguments.</abstract>
      <url hash="fad8c55e">2019.icon-1.6</url>
      <bibkey>goel-etal-2019-event</bibkey>
    </paper>
    <paper id="8">
      <title>Non-native Accent Partitioning for Speakers of Indian Regional Languages<fixed-case>I</fixed-case>ndian Regional Languages</title>
      <author><first>Radha Krishna</first><last>Guntur</last></author>
      <author><first>Krishnan</first><last>Ramakrishnan</last></author>
      <author><first>Vinay Kumar</first><last>Mittal</last></author>
      <pages>65–74</pages>
      <abstract>Acoustic features extracted from the speech signal can help in identifying speaker related multiple information such as geographical origin, <a href="https://en.wikipedia.org/wiki/Regional_accents_of_English">regional accent</a> and <a href="https://en.wikipedia.org/wiki/Nativity_of_Jesus">nativity</a>. In this paper, classification of native speakers of South Indian languages is carried out based upon the accent of their non-native language, i.e., <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Four South Indian languages : <a href="https://en.wikipedia.org/wiki/Kannada">Kannada</a>, <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a>, <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a>, and <a href="https://en.wikipedia.org/wiki/Telugu_language">Telugu</a> are examined. A database of English speech from the native speakers of these languages, along with the native language speech data was collected, from a non-overlapping set of speakers. Segment level acoustic features F0 and Mel-frequency cepstral coefficients (MFCCs) are used. Accent partitioning of non-native English speech data is carried out using multiple classifiers : k-nearest neighbour (KNN), <a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">linear discriminant analysis (LDA)</a> and <a href="https://en.wikipedia.org/wiki/Support_vector_machine">support vector machine (SVM)</a>, for validation and comparison of results. Classification accuracies of 86.6 % are observed using KNN, and 89.2 % or more than 90 % using SVM classifier. A study of acoustic feature F0 contour, related to L2 intonation, showed that native speakers of <a href="https://en.wikipedia.org/wiki/Kannada">Kannada language</a> are quite distinct as compared to those of Tamil or Telugu languages. It is also observed that identification of Malayalam and Kannada speakers from their English speech accent is relatively easier than Telugu or Tamil speakers.</abstract>
      <url hash="fbe59409">2019.icon-1.8</url>
      <attachment type="OptionalSupplementaryMaterial" hash="65c25af3">2019.icon-1.8.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>guntur-etal-2019-non</bibkey>
    </paper>
    <paper id="9">
      <title>A little perturbation makes a difference : Treebank augmentation by perturbation improves transfer parsing</title>
      <author><first>Ayan</first><last>Das</last></author>
      <author><first>Sudeshna</first><last>Sarkar</last></author>
      <pages>75–84</pages>
      <abstract>We present an approach for cross-lingual transfer of dependency parser so that the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> trained on a single source language can more effectively cater to diverse target languages. In this work, we show that the cross-lingual performance of the <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a> can be enhanced by over-generating the source language treebank. For this, the source language treebank is augmented with its perturbed version in which controlled perturbation is introduced in the <a href="https://en.wikipedia.org/wiki/Parse_tree">parse trees</a> by stochastically reordering the positions of the dependents with respect to their heads while keeping the structure of the <a href="https://en.wikipedia.org/wiki/Parse_tree">parse trees</a> unchanged. This enables the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> to capture diverse syntactic patterns in addition to those that are found in the source language. The resulting <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> is found to more effectively parse target languages with different <a href="https://en.wikipedia.org/wiki/Syntax_(programming_languages)">syntactic structures</a>. With <a href="https://en.wikipedia.org/wiki/English_language">English</a> as the source language, our system shows an average improvement of 6.7 % and 7.7 % in terms of <a href="https://en.wikipedia.org/wiki/Universal_asynchronous_receiver-transmitter">UAS</a> and LAS over 29 target languages compared to the baseline single source parser trained using unperturbed source language treebank. This also results in significant improvement over the transfer parser proposed by (CITATION) that involves an order-free parser algorithm.</abstract>
      <url hash="22d7c447">2019.icon-1.9</url>
      <bibkey>das-sarkar-2019-little</bibkey>
    </paper>
    <paper id="12">
      <title>Sanskrit Segmentation revisited<fixed-case>S</fixed-case>anskrit Segmentation revisited</title>
      <author><first>Sriram</first><last>Krishnan</last></author>
      <author><first>Amba</first><last>Kulkarni</last></author>
      <pages>105–114</pages>
      <abstract>Computationally analyzing Sanskrit texts requires proper segmentation in the initial stages. There have been various <a href="https://en.wikipedia.org/wiki/Tool">tools</a> developed for Sanskrit text segmentation. Of these, Grard Huet’s Reader in the Sanskrit Heritage Engine analyzes the input text and segments it based on the word parameters-phases like iic, ifc, Pr, Subst, etc., and sandhi (or transition) that takes place at the end of a word with the initial part of the next word. And it enlists all the possible solutions differentiating them with the help of the <a href="https://en.wikipedia.org/wiki/Phase_(matter)">phases</a>. The <a href="https://en.wikipedia.org/wiki/Phase_(matter)">phases</a> and their analyses have their use in the domain of <a href="https://en.wikipedia.org/wiki/Parsing">sentential parsers</a>. In segmentation, though, they are not used beyond deciding whether the words formed with the phases are morphologically valid. This paper tries to modify the above segmenter by ignoring the phase details (except for a few cases), and also proposes a <a href="https://en.wikipedia.org/wiki/Probability_function">probability function</a> to prioritize the list of solutions to bring up the most valid solutions at the top.</abstract>
      <url hash="b4688758">2019.icon-1.12</url>
      <bibkey>krishnan-kulkarni-2019-sanskrit</bibkey>
    </paper>
    <paper id="15">
      <title>Dataset for Aspect Detection on Mobile reviews in Hindi<fixed-case>H</fixed-case>indi</title>
      <author><first>Pruthwik</first><last>Mishra</last></author>
      <author><first>Ayush</first><last>Joshi</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>130–134</pages>
      <abstract>In recent years <a href="https://en.wikipedia.org/wiki/Opinion_mining">Opinion Mining</a> has become one of the very interesting fields of <a href="https://en.wikipedia.org/wiki/Language_processing">Language Processing</a>. To extract the gist of a sentence in a shorter and efficient manner is what <a href="https://en.wikipedia.org/wiki/Opinion_mining">opinion mining</a> provides. In this paper we focus on detecting aspects for a particular <a href="https://en.wikipedia.org/wiki/Domain_(mathematical_analysis)">domain</a>. While relevant research work has been done in aspect detection in resource rich languages like <a href="https://en.wikipedia.org/wiki/English_language">English</a>, we are trying to do the same in a relatively resource poor Hindi language. Here we present a corpus of mobile reviews which are labelled with carefully curated aspects. The motivation behind Aspect detection is to get information on a finer level about the data. In this paper we identify all aspects related to the gadget which are present on the reviews given online on various websites. We also propose <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline models</a> to detect <a href="https://en.wikipedia.org/wiki/Grammatical_aspect">aspects</a> in <a href="https://en.wikipedia.org/wiki/Hindi">Hindi text</a> after conducting various experiments.</abstract>
      <url hash="74a9e224">2019.icon-1.15</url>
      <bibkey>mishra-etal-2019-dataset</bibkey>
    </paper>
    <paper id="18">
      <title>Towards Handling Verb Phrase Ellipsis in English-Hindi Machine Translation<fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi Machine Translation</title>
      <author><first>Niyati</first><last>Bafna</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>150–159</pages>
      <abstract>English-Hindi machine translation systems have difficulty interpreting verb phrase ellipsis (VPE) in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, and commit errors in translating sentences with VPE. We present a solution and theoretical backing for the treatment of English VPE, with the specific scope of enabling English-Hindi MT, based on an understanding of the syntactical phenomenon of verb-stranding verb phrase ellipsis in Hindi (VVPE). We implement a rule-based system to perform the following sub-tasks : 1) Verb ellipsis identification in the English source sentence, 2) Elided verb phrase head identification 3) Identification of verb segment which needs to be induced at the site of <a href="https://en.wikipedia.org/wiki/Ellipsis_(linguistics)">ellipsis</a> 4) Modify input sentence ; i.e. resolving <a href="https://en.wikipedia.org/wiki/Verb–object–subject">VPE</a> and inducing the required <a href="https://en.wikipedia.org/wiki/Verb–object–subject">verb segment</a>. This <a href="https://en.wikipedia.org/wiki/System">system</a> obtains 94.83 percent <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a> and 83.04 percent <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> on subtask (1), tested on 3900 sentences from the BNC corpus. This is competitive with state-of-the-art results. We measure <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of subtasks (2) and (3) together, and obtain a 91 percent <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on 200 sentences taken from the WSJ corpus. Finally, in order to indicate the relevance of ellipsis handling to MT, we carried out a manual analysis of the English-Hindi MT outputs of 100 sentences after passing it through our system. We set up a basic metric (1-5) for this evaluation, where 5 indicates drastic improvement, and obtained an average of 3.55. As far as we know, this is the first attempt to target ellipsis resolution in the context of improving English-Hindi machine translation.</abstract>
      <url hash="7965028d">2019.icon-1.18</url>
      <bibkey>bafna-sharma-2019-towards</bibkey>
    </paper>
    <paper id="22">
      <title>Kunji : A Resource Management System for Higher Productivity in Computer Aided Translation Tools</title>
      <author><first>Priyank</first><last>Gupta</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Dipti Misra</first><last>Sharma</last></author>
      <author><first>Rashid</first><last>Ahmad</last></author>
      <pages>184–192</pages>
      <abstract>Complex NLP applications, such as machine translation systems, utilize various kinds of resources namely lexical, multiword, domain dictionaries, maps and rules etc. Similarly, translators working on Computer Aided Translation workbenches, also require help from various kinds of resources-glossaries, <a href="https://en.wikipedia.org/wiki/Terminology">terminologies</a>, <a href="https://en.wikipedia.org/wiki/Concordance_(publishing)">concordances</a> and <a href="https://en.wikipedia.org/wiki/Translation_memory">translation memory</a> in the workbenches in order to increase their productivity. Additionally, translators have to look away from the workbenches for linguistic resources like Named Entities, Multiwords, lexical and lexeme dictionaries in order to get help, as the available resources like concordances, terminologies and glossaries are often not enough. In this paper we present <a href="https://en.wikipedia.org/wiki/Kunji">Kunji</a>, a <a href="https://en.wikipedia.org/wiki/Resource_management_(computing)">resource management system</a> for translation workbenches and MT modules. This <a href="https://en.wikipedia.org/wiki/System">system</a> can be easily integrated in translation workbenches and can also be used as a management tool for resources for MT systems. The described <a href="https://en.wikipedia.org/wiki/Resource_management_(computing)">resource management system</a> has been integrated in a translation workbench Transzaar. We also study the impact of providing this <a href="https://en.wikipedia.org/wiki/Resource_management">resource management system</a> along with linguistic resources on the productivity of translators for English-Hindi language pair. When the linguistic resources like <a href="https://en.wikipedia.org/wiki/Lexeme">lexeme</a>, NER and MWE dictionaries were made available to translators in addition to their regular translation memories, <a href="https://en.wikipedia.org/wiki/Concordance_(publishing)">concordances</a> and <a href="https://en.wikipedia.org/wiki/Terminology">terminologies</a>, their productivity increased by 15.61 %.</abstract>
      <url hash="a20f5438">2019.icon-1.22</url>
      <bibkey>gupta-etal-2019-kunji</bibkey>
    </paper>
    <paper id="25">
      <title>Unsung Challenges of Building and Deploying Language Technologies for Low Resource Language Communities</title>
      <author><first>Pratik</first><last>Joshi</last></author>
      <author><first>Christain</first><last>Barnes</last></author>
      <author><first>Sebastin</first><last>Santy</last></author>
      <author><first>Simran</first><last>Khanuja</last></author>
      <author><first>Sanket</first><last>Shah</last></author>
      <author><first>Anirudh</first><last>Srinivasan</last></author>
      <author><first>Satwik</first><last>Bhattamishra</last></author>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <pages>211–219</pages>
      <abstract>In this paper, we examine and analyze the challenges associated with developing and introducing <a href="https://en.wikipedia.org/wiki/Language_technology">language technologies</a> to low-resource language communities. While doing so we bring to light the successes and failures of past work in this area, challenges being faced in doing so, and what have they achieved. Throughout this paper, we take a problem-facing approach and describe essential factors which the success of such <a href="https://en.wikipedia.org/wiki/Technology">technologies</a> hinges upon. We present the various aspects in a manner which clarify and lay out the different tasks involved, which can aid organizations looking to make an impact in this area. We take the example of <a href="https://en.wikipedia.org/wiki/Gondi_language">Gondi</a>, an extremely-low resource Indian language, to reinforce and complement our discussion.</abstract>
      <url hash="ceb13465">2019.icon-1.25</url>
      <bibkey>joshi-etal-2019-unsung</bibkey>
    </paper>
    <paper id="26">
      <title>DRCoVe : An Augmented Word Representation Approach using Distributional and Relational Context<fixed-case>DRC</fixed-case>o<fixed-case>V</fixed-case>e: An Augmented Word Representation Approach using Distributional and Relational Context</title>
      <author><first>Md. Aslam</first><last>Parwez</last></author>
      <author><first>Muhammad</first><last>Abulaish</last></author>
      <author><first>Mohd</first><last>Fazil</last></author>
      <pages>220–229</pages>
      <abstract>Word representation using the distributional information of words from a sizeable corpus is considered efficacious in many natural language processing and text mining applications. However, distributional representation of a word is unable to capture distant relational knowledge, representing the relational semantics. In this paper, we propose a novel word representation approach using distributional and relational contexts, DRCoVe, which augments the distributional representation of a word using the relational semantics extracted as syntactic and semantic association among entities from the underlying corpus. Unlike existing approaches that use external knowledge bases representing the relational semantics for enhanced word representation, DRCoVe uses typed dependencies (aka syntactic dependencies) to extract relational knowledge from the underlying corpus. The proposed approach is applied over a biomedical text corpus to learn <a href="https://en.wikipedia.org/wiki/Word_processor_(electronic_device)">word representation</a> and compared with <a href="https://en.wikipedia.org/wiki/GloVe_(machine_learning)">GloVe</a>, which is one of the most popular word embedding approaches. The evaluation results on various benchmark datasets for word similarity and word categorization tasks demonstrate the effectiveness of DRCoVe over the <a href="https://en.wikipedia.org/wiki/GloVe">GloVe</a>.</abstract>
      <url hash="e9f8c9f8">2019.icon-1.26</url>
      <bibkey>parwez-etal-2019-drcove</bibkey>
    </paper>
    <paper id="27">
      <title>A Deep Learning Approach for Automatic Detection of Fake News</title>
      <author><first>Tanik</first><last>Saikh</last></author>
      <author><first>Arkadipta</first><last>De</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>230–238</pages>
      <abstract>Fake news detection is a very prominent and essential task in the field of <a href="https://en.wikipedia.org/wiki/Journalism">journalism</a>. This challenging <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> is seen so far in the field of <a href="https://en.wikipedia.org/wiki/Politics">politics</a>, but it could be even more challenging when it is to be determined in the multi-domain platform. In this paper, we propose two effective models based on <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> for solving fake news detection problem in online news contents of multiple domains. We evaluate our techniques on the two recently released datasets, namely Fake News AMT and Celebrity for fake news detection. The proposed <a href="https://en.wikipedia.org/wiki/System">systems</a> yield encouraging performance, outperforming the current hand-crafted feature engineering based state-of-the-art system with a significant margin of 3.08 % and 9.3 % by the two <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>, respectively. In order to exploit the datasets, available for the related tasks, we perform cross-domain analysis (model trained on FakeNews AMT and tested on <a href="https://en.wikipedia.org/wiki/Celebrity">Celebrity</a> and vice versa) to explore the applicability of our systems across the domains.</abstract>
      <url hash="166cb9aa">2019.icon-1.27</url>
      <bibkey>saikh-etal-2019-deep</bibkey>
    </paper>
    <paper id="28">
      <title>Samajh-Boojh : A Reading Comprehension system in Hindi<fixed-case>H</fixed-case>indi</title>
      <author><first>Shalaka</first><last>Vaidya</last></author>
      <author><first>Hiranmai</first><last>Sri Adibhatla</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>239–248</pages>
      <abstract>This paper presents a novel approach designed to answer questions on a reading comprehension passage. It is an end-to-end system which first focuses on comprehending the given passage wherein it converts <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured passage</a> into a <a href="https://en.wikipedia.org/wiki/Structured_data">structured data</a> and later proceeds to answer the questions related to the passage using solely the aforementioned <a href="https://en.wikipedia.org/wiki/Structured_data">structured data</a>. To the best of our knowledge, the proposed <a href="https://en.wikipedia.org/wiki/Design">design</a> is first of its kind which accounts for entire process of comprehending the passage and then answering the questions associated with the passage. The comprehension stage converts the passage into a Discourse Collection that comprises of the relation shared amongst logical sentences in given passage along with the key characteristics of each sentence. This <a href="https://en.wikipedia.org/wiki/Design">design</a> has its applications in academic domain, query comprehension in speech systems among others.</abstract>
      <url hash="c8c6e93b">2019.icon-1.28</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2325c695">2019.icon-1.28.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>vaidya-etal-2019-samajh</bibkey>
    </paper>
  </volume>
</collection>