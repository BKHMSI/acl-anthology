<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.gebnlp">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing</booktitle>
      <editor><first>Marta</first><last>Costa-jussa</last></editor>
      <editor><first>Hila</first><last>Gonen</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Kellie</first><last>Webster</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="38ae9296">2021.gebnlp-1</url>
    </meta>
    <frontmatter>
      <url hash="9df1219b">2021.gebnlp-1.0</url>
      <bibkey>gebnlp-2021-gender</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Gender Bias Hidden Behind Chinese Word Embeddings : The Case of Chinese Adjectives<fixed-case>C</fixed-case>hinese Word Embeddings: The Case of <fixed-case>C</fixed-case>hinese Adjectives</title>
      <author><first>Meichun</first><last>Jiao</last></author>
      <author><first>Ziyang</first><last>Luo</last></author>
      <pages>8–15</pages>
      <abstract>Gender bias in word embeddings gradually becomes a vivid research field in recent years. Most studies in this field aim at measurement and debiasing methods with <a href="https://en.wikipedia.org/wiki/English_language">English</a> as the target language. This paper investigates <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> in static word embeddings from a unique perspective, <a href="https://en.wikipedia.org/wiki/Chinese_adjectives">Chinese adjectives</a>. By training word representations with different <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>, the <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> behind the vectors of adjectives is assessed. Through a comparison between the produced results and a human scored data set, we demonstrate how gender bias encoded in word embeddings differentiates from people’s attitudes.</abstract>
      <url hash="509ba7f6">2021.gebnlp-1.2</url>
      <doi>10.18653/v1/2021.gebnlp-1.2</doi>
      <bibkey>jiao-luo-2021-gender</bibkey>
    </paper>
    <paper id="3">
      <title>Evaluating Gender Bias in Hindi-English Machine Translation<fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Machine Translation</title>
      <author><first>Krithika</first><last>Ramesh</last></author>
      <author><first>Gauri</first><last>Gupta</last></author>
      <author><first>Sanjay</first><last>Singh</last></author>
      <pages>16–23</pages>
      <abstract>With <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for <a href="https://en.wikipedia.org/wiki/Indo-Aryan_languages">Indic languages</a>. In our work, we attempt to evaluate and quantify the <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>. We also compare and contrast the resulting bias measurements across multiple <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> for pre-trained embeddings and the ones learned by our machine translation model.</abstract>
      <url hash="899b8bf2">2021.gebnlp-1.3</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5b68eee7">2021.gebnlp-1.3.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.gebnlp-1.3</doi>
      <bibkey>ramesh-etal-2021-evaluating</bibkey>
    </paper>
    <paper id="4">
      <title>Alexa, Google, Siri : What are Your Pronouns? Gender and Anthropomorphism in the Design and Perception of Conversational Assistants<fixed-case>A</fixed-case>lexa, <fixed-case>G</fixed-case>oogle, <fixed-case>S</fixed-case>iri: What are Your Pronouns? Gender and Anthropomorphism in the Design and Perception of Conversational Assistants</title>
      <author><first>Gavin</first><last>Abercrombie</last></author>
      <author><first>Amanda</first><last>Cercas Curry</last></author>
      <author><first>Mugdha</first><last>Pandya</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>24–33</pages>
      <abstract>Technology companies have produced varied responses to concerns about the effects of the design of their conversational AI systems. Some have claimed that their voice assistants are in fact not gendered or human-likedespite design features suggesting the contrary. We compare these claims to user perceptions by analysing the pronouns they use when referring to AI assistants. We also examine systems’ responses and the extent to which they generate output which is gendered and anthropomorphic. We find that, while some companies appear to be addressing the ethical concerns raised, in some cases, their claims do not seem to hold true. In particular, our results show that system outputs are ambiguous as to the humanness of the systems, and that users tend to personify and gender them as a result.</abstract>
      <url hash="37fbde43">2021.gebnlp-1.4</url>
      <doi>10.18653/v1/2021.gebnlp-1.4</doi>
      <bibkey>abercrombie-etal-2021-alexa</bibkey>
      <pwccode url="https://github.com/GavinAbercrombie/GeBNLP2021" additional="false">GavinAbercrombie/GeBNLP2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="5">
      <title>Gender Bias in Text : Origin, Taxonomy, and Implications</title>
      <author><first>Jad</first><last>Doughman</last></author>
      <author><first>Wael</first><last>Khreich</last></author>
      <author><first>Maya</first><last>El Gharib</last></author>
      <author><first>Maha</first><last>Wiss</last></author>
      <author><first>Zahraa</first><last>Berjawi</last></author>
      <pages>34–44</pages>
      <abstract>Gender inequality represents a considerable loss of human potential and perpetuates a culture of violence, higher gender wage gaps, and a lack of representation of women in higher and leadership positions. Applications powered by Artificial Intelligence (AI) are increasingly being used in the real world to provide critical decisions about who is going to be hired, granted a loan, admitted to college, etc. However, the main pillars of <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI</a>, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing (NLP)</a> and <a href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning (ML)</a> have been shown to reflect and even amplify <a href="https://en.wikipedia.org/wiki/Gender_bias">gender biases</a> and stereotypes, which are mainly inherited from historical training data. In an effort to facilitate the identification and mitigation of <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> in English text, we develop a comprehensive taxonomy that relies on the following <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias types</a> : Generic Pronouns, <a href="https://en.wikipedia.org/wiki/Sexism">Sexism</a>, Occupational Bias, Exclusionary Bias, and <a href="https://en.wikipedia.org/wiki/Semantics">Semantics</a>. We also provide a bottom-up overview of <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a>, from its societal origin to its spillover onto language. Finally, we link the societal implications of <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> to their corresponding type(s) in the proposed <a href="https://en.wikipedia.org/wiki/Taxonomy_(biology)">taxonomy</a>. The underlying motivation of our work is to help enable the technical community to identify and mitigate relevant biases from training corpora for improved fairness in NLP systems.</abstract>
      <url hash="8a6bc4f6">2021.gebnlp-1.5</url>
      <doi>10.18653/v1/2021.gebnlp-1.5</doi>
      <bibkey>doughman-etal-2021-gender</bibkey>
    </paper>
    <paper id="6">
      <title>Sexism in the Judiciary : The Importance of Bias Definition in <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a> and In Our Courts<fixed-case>NLP</fixed-case> and In Our Courts</title>
      <author><first>Noa</first><last>Baker Gillis</last></author>
      <pages>45–54</pages>
      <abstract>We analyze 6.7 million case law documents to determine the presence of <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> within our <a href="https://en.wikipedia.org/wiki/Judiciary">judicial system</a>. We find that current bias detection methods in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> are insufficient to determine gender bias in our case law database and propose an alternative approach. We show that existing algorithms’ inconsistent results are consequences of prior research’s inconsistent definitions of biases themselves. Bias detection algorithms rely on groups of words to represent <a href="https://en.wikipedia.org/wiki/Bias">bias</a> (e.g., ‘salary,’ ‘job,’ and ‘boss’ to represent <a href="https://en.wikipedia.org/wiki/Employment">employment</a> as a potentially biased theme against women in text). However, the methods to build these groups of words have several weaknesses, primarily that the word lists are based on the researchers’ own intuitions. We suggest two new methods of automating the creation of word lists to represent <a href="https://en.wikipedia.org/wiki/Bias">biases</a>. We find that our <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> outperform current NLP bias detection methods. Our research improves the capabilities of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP technology</a> to detect <a href="https://en.wikipedia.org/wiki/Bias">bias</a> and highlights <a href="https://en.wikipedia.org/wiki/Sexism">gender biases</a> present in influential case law. In order to test our NLP bias detection method’s performance, we regress our results of bias in case law against <a href="https://en.wikipedia.org/wiki/United_States_Census">U.S census data</a> of women’s participation in the workforce in the last 100 years.</abstract>
      <url hash="0cc4e192">2021.gebnlp-1.6</url>
      <doi>10.18653/v1/2021.gebnlp-1.6</doi>
      <bibkey>baker-gillis-2021-sexism</bibkey>
    </paper>
    <paper id="10">
      <title>Investigating the Impact of <a href="https://en.wikipedia.org/wiki/Gender_representation">Gender Representation</a> in ASR Training Data : a Case Study on Librispeech<fixed-case>ASR</fixed-case> Training Data: a Case Study on Librispeech</title>
      <author><first>Mahault</first><last>Garnerin</last></author>
      <author><first>Solange</first><last>Rossato</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>86–92</pages>
      <abstract>In this paper we question the impact of <a href="https://en.wikipedia.org/wiki/Gender_representation">gender representation</a> in training data on the performance of an end-to-end ASR system. We create an experiment based on the Librispeech corpus and build 3 different training corpora varying only the proportion of data produced by each gender category. We observe that if our system is overall robust to the gender balance or imbalance in training data, it is nonetheless dependant of the adequacy between the individuals present in the training and testing sets.</abstract>
      <url hash="d224b621">2021.gebnlp-1.10</url>
      <doi>10.18653/v1/2021.gebnlp-1.10</doi>
      <bibkey>garnerin-etal-2021-investigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="11">
      <title>Generating Gender Augmented Data for NLP<fixed-case>NLP</fixed-case></title>
      <author><first>Nishtha</first><last>Jain</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Declan</first><last>Groves</last></author>
      <author><first>Eva</first><last>Vanmassenhove</last></author>
      <pages>93–102</pages>
      <abstract>Gender bias is a frequent occurrence in NLP-based applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of <a href="https://en.wikipedia.org/wiki/Bias">bias</a> becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable re-writing approach for short conversational sentences. The <a href="https://en.wikipedia.org/wiki/Rewriting">rewriting method</a> can be applied to sentences that, without <a href="https://en.wikipedia.org/wiki/Context_(language_use)">extra-sentential context</a>, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation system trained to ‘translate’ from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results with respect to the automatic generation of gender alternatives for conversational sentences in Spanish.</abstract>
      <url hash="552391e1">2021.gebnlp-1.11</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2de0cc30">2021.gebnlp-1.11.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2021.gebnlp-1.11</doi>
      <bibkey>jain-etal-2021-generating</bibkey>
      <pwccode url="https://github.com/awslabs/sockeye" additional="false">awslabs/sockeye</pwccode>
    </paper>
    <paper id="12">
      <title>Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias Detection in Coreference Resolution<fixed-case>W</fixed-case>ino<fixed-case>B</fixed-case>ias (<fixed-case>S</fixed-case>o<fixed-case>W</fixed-case>ino<fixed-case>B</fixed-case>ias) Test Set for Latent Gender Bias Detection in Coreference Resolution</title>
      <author><first>Hillary</first><last>Dawkins</last></author>
      <pages>103–111</pages>
      <abstract>We observe an instance of gender-induced bias in a downstream application, despite the absence of explicit gender words in the test cases. We provide a test set, SoWinoBias, for the purpose of measuring such latent gender bias in coreference resolution systems. We evaluate the performance of current debiasing methods on the SoWinoBias test set, especially in reference to the method’s design and altered embedding space properties. See https://github.com/hillary-dawkins/SoWinoBias.</abstract>
      <url hash="4a3028b8">2021.gebnlp-1.12</url>
      <doi>10.18653/v1/2021.gebnlp-1.12</doi>
      <bibkey>dawkins-2021-second</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
  </volume>
</collection>