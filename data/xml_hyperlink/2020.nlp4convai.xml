<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.nlp4convai">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI</booktitle>
      <editor><first>Tsung-Hsien</first><last>Wen</last></editor>
      <editor><first>Asli</first><last>Celikyilmaz</last></editor>
      <editor><first>Zhou</first><last>Yu</last></editor>
      <editor><first>Alexandros</first><last>Papangelis</last></editor>
      <editor><first>Mihail</first><last>Eric</last></editor>
      <editor><first>Anuj</first><last>Kumar</last></editor>
      <editor><first>Iñigo</first><last>Casanueva</last></editor>
      <editor><first>Rushin</first><last>Shah</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="ce634d6f">2020.nlp4convai-1</url>
    </meta>
    <frontmatter>
      <url hash="a9c1804c">2020.nlp4convai-1.0</url>
      <bibkey>nlp4convai-2020-natural</bibkey>
    </frontmatter>
    <paper id="4">
      <title>How to Tame Your Data : <a href="https://en.wikipedia.org/wiki/Data_augmentation">Data Augmentation</a> for Dialog State Tracking</title>
      <author><first>Adam</first><last>Summerville</last></author>
      <author><first>Jordan</first><last>Hashemi</last></author>
      <author><first>James</first><last>Ryan</last></author>
      <author><first>William</first><last>Ferguson</last></author>
      <pages>32–37</pages>
      <abstract>Dialog State Tracking (DST) is a problem space in which the effective vocabulary is practically limitless. For example, the domain of possible <a href="https://en.wikipedia.org/wiki/Film_title_design">movie titles</a> or restaurant names is bound only by the limits of language. As such, DST systems often encounter out-of-vocabulary words at inference time that were never encountered during training. To combat this issue, we present a targeted data augmentation process, by which a practitioner observes the types of errors made on held-out evaluation data, and then modifies the training data with additional corpora to increase the vocabulary size at training time. Using this with a RoBERTa-based Transformer architecture, we achieve state-of-the-art results in comparison to systems that only mask trouble slots with special tokens. Additionally, we present a data-representation scheme for seamlessly retargeting DST architectures to new domains.</abstract>
      <url hash="31dafe5a">2020.nlp4convai-1.4</url>
      <doi>10.18653/v1/2020.nlp4convai-1.4</doi>
      <video href="http://slideslive.com/38929634" />
      <bibkey>summerville-etal-2020-tame</bibkey>
    </paper>
    <paper id="5">
      <title>Efficient Intent Detection with Dual Sentence Encoders</title>
      <author><first>Iñigo</first><last>Casanueva</last></author>
      <author><first>Tadas</first><last>Temčinas</last></author>
      <author><first>Daniela</first><last>Gerz</last></author>
      <author><first>Matthew</first><last>Henderson</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <pages>38–45</pages>
      <abstract>Building conversational systems in new domains and with added functionality requires resource-efficient models that work under low-data regimes (i.e., in few-shot setups). Motivated by these requirements, we introduce intent detection methods backed by pretrained dual sentence encoders such as USE and ConveRT. We demonstrate the usefulness and wide applicability of the proposed intent detectors, showing that : 1) they outperform intent detectors based on fine-tuning the full BERT-Large model or using BERT as a fixed black-box encoder on three diverse intent detection data sets ; 2) the gains are especially pronounced in few-shot setups (i.e., with only 10 or 30 annotated examples per intent) ; 3) our intent detectors can be trained in a matter of minutes on a single CPU ; and 4) they are stable across different hyperparameter settings. In hope of facilitating and democratizing research focused on intention detection, we release our code, as well as a new challenging single-domain intent detection dataset comprising 13,083 annotated examples over 77 intents.</abstract>
      <url hash="c3636fef">2020.nlp4convai-1.5</url>
      <doi>10.18653/v1/2020.nlp4convai-1.5</doi>
      <attachment type="Dataset" hash="0666d8ca">2020.nlp4convai-1.5.Dataset.zip</attachment>
      <video href="http://slideslive.com/38929632" />
      <bibkey>casanueva-etal-2020-efficient</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="6">
      <title>Accelerating <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">Natural Language Understanding</a> in Task-Oriented Dialog</title>
      <author><first>Ojas</first><last>Ahuja</last></author>
      <author><first>Shrey</first><last>Desai</last></author>
      <pages>46–53</pages>
      <abstract>Task-oriented dialog models typically leverage complex neural architectures and large-scale, pre-trained Transformers to achieve state-of-the-art performance on popular natural language understanding benchmarks. However, these models frequently have in excess of tens of millions of parameters, making them impossible to deploy on-device where <a href="https://en.wikipedia.org/wiki/Resource_efficiency">resource-efficiency</a> is a major concern. In this work, we show that a simple convolutional model compressed with structured pruning achieves largely comparable results to BERT on <a href="https://en.wikipedia.org/wiki/Automatic_terminal_information_service">ATIS</a> and Snips, with under 100 K parameters. Moreover, we perform acceleration experiments on <a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPUs</a>, where we observe our multi-task model predicts intents and slots nearly 63x faster than even DistilBERT.</abstract>
      <url hash="686fcd1b">2020.nlp4convai-1.6</url>
      <doi>10.18653/v1/2020.nlp4convai-1.6</doi>
      <bibkey>ahuja-desai-2020-accelerating</bibkey>
      <pwccode url="https://github.com/oja/pruned-nlu" additional="false">oja/pruned-nlu</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="9">
      <title>Automating Template Creation for Ranking-Based Dialogue Models</title>
      <author><first>Jingxiang</first><last>Chen</last></author>
      <author><first>Heba</first><last>Elfardy</last></author>
      <author><first>Simi</first><last>Wang</last></author>
      <author><first>Andrea</first><last>Kahn</last></author>
      <author><first>Jared</first><last>Kramer</last></author>
      <pages>71–78</pages>
      <abstract>Dialogue response generation models that use template ranking rather than direct sequence generation allow model developers to limit generated responses to pre-approved messages. However, manually creating templates is time-consuming and requires domain expertise. To alleviate this problem, we explore automating the process of creating dialogue templates by using <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a> to cluster historical utterances and selecting representative utterances from each <a href="https://en.wikipedia.org/wiki/Cluster_analysis">cluster</a>. Specifically, we propose an end-to-end model called Deep Sentence Encoder Clustering (DSEC) that uses an auto-encoder structure to jointly learn the utterance representation and construct template clusters. We compare this method to a random baseline that randomly assigns templates to clusters as well as a strong baseline that performs the sentence encoding and the utterance clustering sequentially. To evaluate the performance of the proposed method, we perform an automatic evaluation with two annotated customer service datasets to assess clustering effectiveness, and a human-in-the-loop experiment using a live customer service application to measure the acceptance rate of the generated templates. DSEC performs best in the automatic evaluation, beats both the sequential and random baselines on most metrics in the human-in-the-loop experiment, and shows promising results when compared to gold / manually created templates.</abstract>
      <url hash="e2fc267c">2020.nlp4convai-1.9</url>
      <attachment type="Software" hash="d3cfb0f6">2020.nlp4convai-1.9.Software.txt</attachment>
      <doi>10.18653/v1/2020.nlp4convai-1.9</doi>
      <attachment type="Software" hash="b5003fb1">2020.nlp4convai-1.9.Software.zip</attachment>
      <video href="http://slideslive.com/38929630" />
      <bibkey>chen-etal-2020-automating</bibkey>
    </paper>
    <paper id="13">
      <title>MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines<fixed-case>M</fixed-case>ulti<fixed-case>WOZ</fixed-case> 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines</title>
      <author><first>Xiaoxue</first><last>Zang</last></author>
      <author><first>Abhinav</first><last>Rastogi</last></author>
      <author><first>Srinivas</first><last>Sunkara</last></author>
      <author><first>Raghav</first><last>Gupta</last></author>
      <author><first>Jianguo</first><last>Zhang</last></author>
      <author><first>Jindong</first><last>Chen</last></author>
      <pages>109–117</pages>
      <abstract>MultiWOZ is a well-known task-oriented dialogue dataset containing over 10,000 annotated dialogues spanning 8 domains. It is extensively used as a benchmark for dialogue state tracking. However, recent works have reported presence of substantial noise in the dialogue state annotations. MultiWOZ 2.1 identified and fixed many of these erroneous <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> and user utterances, resulting in an improved version of this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. This work introduces MultiWOZ 2.2, which is a yet another improved version of this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. Firstly, we identify and fix dialogue state annotation errors across 17.3 % of the utterances on top of MultiWOZ 2.1. Secondly, we redefine the <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a> by disallowing vocabularies of slots with a large number of possible values (e.g., restaurant name, time of booking). In addition, we introduce slot span annotations for these slots to standardize them across recent models, which previously used custom string matching heuristics to generate them. We also benchmark a few state of the art dialogue state tracking models on the corrected dataset to facilitate comparison for future work. In the end, we discuss best practices for dialogue data collection that can help avoid annotation errors.</abstract>
      <url hash="5dc5d275">2020.nlp4convai-1.13</url>
      <doi>10.18653/v1/2020.nlp4convai-1.13</doi>
      <video href="http://slideslive.com/38929641" />
      <bibkey>zang-etal-2020-multiwoz</bibkey>
      <pwccode url="https://github.com/budzianowski/multiwoz" additional="false">budzianowski/multiwoz</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/100doh">100DOH</pwcdataset>
    </paper>
    <paper id="15">
      <title>Probing Neural Dialog Models for Conversational Understanding</title>
      <author><first>Abdelrhman</first><last>Saleh</last></author>
      <author><first>Tovly</first><last>Deutsch</last></author>
      <author><first>Stephen</first><last>Casper</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <author><first>Stuart</first><last>Shieber</last></author>
      <pages>132–143</pages>
      <abstract>The predominant approach to open-domain dialog generation relies on end-to-end training of neural models on chat datasets. However, this approach provides little insight as to what these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> learn (or do not learn) about engaging in <a href="https://en.wikipedia.org/wiki/Dialogue">dialog</a>. In this study, we analyze the internal representations learned by neural open-domain dialog systems and evaluate the quality of these <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> for learning basic conversational skills. Our results suggest that standard open-domain dialog systems struggle with answering questions, inferring <a href="https://en.wikipedia.org/wiki/Contradiction">contradiction</a>, and determining the topic of conversation, among other tasks. We also find that the dyadic, turn-taking nature of <a href="https://en.wikipedia.org/wiki/Dialogue">dialog</a> is not fully leveraged by these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. By exploring these limitations, we highlight the need for additional research into <a href="https://en.wikipedia.org/wiki/Computer_architecture">architectures</a> and <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training methods</a> that can better capture high-level information about dialog.</abstract>
      <url hash="9360d36d">2020.nlp4convai-1.15</url>
      <doi>10.18653/v1/2020.nlp4convai-1.15</doi>
      <video href="http://slideslive.com/38929635" />
      <bibkey>saleh-etal-2020-probing</bibkey>
      <pwccode url="https://github.com/AbdulSaleh/dialog-probing" additional="false">AbdulSaleh/dialog-probing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
  </volume>
</collection>