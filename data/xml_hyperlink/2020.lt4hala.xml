<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.lt4hala">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages</booktitle>
      <editor><first>Rachele</first><last>Sprugnoli</last></editor>
      <editor><first>Marco</first><last>Passarotti</last></editor>
      <publisher>European Language Resources Association (ELRA)</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-53-5</isbn>
    </meta>
    <frontmatter>
      <url hash="cbe27ee8">2020.lt4hala-1.0</url>
      <bibkey>lt4hala-2020-lt4hala</bibkey>
    </frontmatter>
    <paper id="6">
      <title>Using LatInfLexi for an Entropy-Based Assessment of Predictability in Latin Inflection<fixed-case>L</fixed-case>at<fixed-case>I</fixed-case>nf<fixed-case>L</fixed-case>exi for an Entropy-Based Assessment of Predictability in <fixed-case>L</fixed-case>atin Inflection</title>
      <author><first>Matteo</first><last>Pellegrini</last></author>
      <pages>37–46</pages>
      <abstract>This paper presents LatInfLexi, a large inflected lexicon of Latin providing information on all the <a href="https://en.wikipedia.org/wiki/List_of_Latin-script_digraphs">inflected wordforms</a> of 3,348 verbs and 1,038 nouns. After a description of the structure of the resource and some data on its size, the procedure followed to obtain the lexicon from the database of the Lemlat 3.0 morphological analyzer is detailed, as well as the choices made regarding overabundant and defective cells. The way in which the data of LatInfLexi can be exploited in order to perform a quantitative assessment of predictability in Latin verb inflection is then illustrated : results obtained by computing the conditional entropy of guessing the content of a paradigm cell assuming knowledge of one wordform or multiple wordforms are presented in turn, highlighting the descriptive and theoretical relevance of the analysis. Lastly, the paper envisages the advantages of an inclusion of LatInfLexi into the LiLa knowledge base, both for the presented resource and for the knowledge base itself.</abstract>
      <url hash="04497ef1">2020.lt4hala-1.6</url>
      <language>eng</language>
      <bibkey>pellegrini-2020-using</bibkey>
    </paper>
    <paper id="7">
      <title>A Tool for Facilitating OCR Postediting in Historical Documents<fixed-case>OCR</fixed-case> Postediting in Historical Documents</title>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <author><first>Mohammad</first><last>Aboomar</last></author>
      <author><first>Jan</first><last>Buts</last></author>
      <author><first>James</first><last>Hadley</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>47–51</pages>
      <abstract>Optical character recognition (OCR) for <a href="https://en.wikipedia.org/wiki/Historical_document">historical documents</a> is a complex procedure subject to a unique set of material issues, including inconsistencies in <a href="https://en.wikipedia.org/wiki/Typeface">typefaces</a> and low quality scanning. Consequently, even the most sophisticated <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">OCR engines</a> produce errors. This paper reports on a tool built for postediting the output of Tesseract, more specifically for correcting common errors in digitized historical documents. The proposed <a href="https://en.wikipedia.org/wiki/Tool">tool</a> suggests alternatives for <a href="https://en.wikipedia.org/wiki/Linguistic_description">word forms</a> not found in a specified vocabulary. The assumed error is replaced by a presumably correct alternative in the post-edition based on the scores of a Language Model (LM). The <a href="https://en.wikipedia.org/wiki/Tool">tool</a> is tested on a chapter of the book An Essay Towards Regulating the Trade and Employing the Poor of this Kingdom (Cary, 1719). As demonstrated below, the <a href="https://en.wikipedia.org/wiki/Tool">tool</a> is successful in correcting a number of common errors. If sometimes unreliable, <a href="https://en.wikipedia.org/wiki/It_(2017_film)">it</a> is also transparent and subject to human intervention.</abstract>
      <url hash="c046e2f0">2020.lt4hala-1.7</url>
      <language>eng</language>
      <bibkey>poncelas-etal-2020-tool</bibkey>
      <pwccode url="https://github.com/alberto-poncelas/tesseract_postprocess" additional="false">alberto-poncelas/tesseract_postprocess</pwccode>
    </paper>
    <paper id="10">
      <title>A Thesaurus for Biblical Hebrew<fixed-case>H</fixed-case>ebrew</title>
      <author><first>Miriam</first><last>Azar</last></author>
      <author><first>Aliza</first><last>Pahmer</last></author>
      <author><first>Joshua</first><last>Waxman</last></author>
      <pages>68–73</pages>
      <abstract>We built a thesaurus for <a href="https://en.wikipedia.org/wiki/Biblical_Hebrew">Biblical Hebrew</a>, with connections between roots based on phonetic, semantic, and distributional similarity. To this end, we apply established <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> to find connections between <a href="https://en.wikipedia.org/wiki/Headword">headwords</a> based on existing lexicons and other digital resources. For semantic similarity, we utilize the cosine-similarity of tf-idf vectors of English gloss text of Hebrew headwords from Ernest Klein’s A Comprehensive Etymological Dictionary of the Hebrew Language for Readers of English as well as to Brown-Driver-Brigg’s Hebrew Lexicon. For phonetic similarity, we digitize part of Matityahu Clark’s Etymological Dictionary of Biblical Hebrew, grouping Hebrew roots into phonemic classes, and establish phonetic relationships between headwords in Klein’s Dictionary. For distributional similarity, we consider the cosine similarity of PPMI vectors of Hebrew roots and also, in a somewhat novel approach, apply Word2Vec to a Biblical corpus reduced to its lexemes. The resulting resource is helpful to those trying to understand <a href="https://en.wikipedia.org/wiki/Biblical_Hebrew">Biblical Hebrew</a>, and also stands as a good basis for programs trying to process the Biblical text.</abstract>
      <url hash="06b60503">2020.lt4hala-1.10</url>
      <language>eng</language>
      <bibkey>azar-etal-2020-thesaurus</bibkey>
    </paper>
    <paper id="12">
      <title>Comparing Statistical and Neural Models for Learning Sound Correspondences</title>
      <author><first>Clémentine</first><last>Fourrier</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>79–83</pages>
      <abstract>Cognate prediction and proto-form reconstruction are key tasks in computational historical linguistics that rely on the study of sound change regularity. Solving these tasks appears to be very similar to <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, though methods from that field have barely been applied to <a href="https://en.wikipedia.org/wiki/Historical_linguistics">historical linguistics</a>. Therefore, in this paper, we investigate the learnability of sound correspondences between a proto-language and daughter languages for two machine-translation-inspired models, one statistical, the other neural. We first carry out our experiments on plausible artificial languages, without <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a>, in order to study the role of each parameter on the algorithms respective performance under almost perfect conditions. We then study real languages, namely <a href="https://en.wikipedia.org/wiki/Latin">Latin</a>, <a href="https://en.wikipedia.org/wiki/Italian_language">Italian</a> and <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>, to see if those performances generalise well. We show that both model types manage to learn sound changes despite data scarcity, although the best performing model type depends on several parameters such as the size of the training data, the ambiguity, and the prediction direction.</abstract>
      <url hash="50e462e3">2020.lt4hala-1.12</url>
      <language>eng</language>
      <bibkey>fourrier-sagot-2020-comparing</bibkey>
      <pwccode url="https://github.com/clefourrier/PLexGen" additional="false">clefourrier/PLexGen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/etymdb-2-0">EtymDB 2.0</pwcdataset>
    </paper>
    <paper id="14">
      <title>Latin-Spanish Neural Machine Translation : from the Bible to Saint Augustine<fixed-case>L</fixed-case>atin-<fixed-case>S</fixed-case>panish Neural Machine Translation: from the <fixed-case>B</fixed-case>ible to Saint Augustine</title>
      <author><first>Eva</first><last>Martínez Garcia</last></author>
      <author><first>Álvaro</first><last>García Tejedor</last></author>
      <pages>94–99</pages>
      <abstract>Although there are several sources where to find historical texts, they usually are available in the original language that makes them generally inaccessible. This paper presents the development of state-of-the-art Neural Machine Systems for the low-resourced Latin-Spanish language pair. First, we build a Transformer-based Machine Translation system on the Bible parallel corpus. Then, we build a comparable corpus from <a href="https://en.wikipedia.org/wiki/Augustine_of_Hippo">Saint Augustine texts</a> and their translations. We use this <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> to study the domain adaptation case from the <a href="https://en.wikipedia.org/wiki/Bible">Bible texts</a> to Saint Augustine’s works. Results show the difficulties of handling a low-resourced language as Latin. First, we noticed the importance of having enough data, since the <a href="https://en.wikipedia.org/wiki/System">systems</a> do not achieve high BLEU scores. Regarding <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a>, results show how using in-domain data helps <a href="https://en.wikipedia.org/wiki/System">systems</a> to achieve a better quality translation. Also, we observed that it is needed a higher amount of data to perform an effective vocabulary extension that includes in-domain vocabulary.</abstract>
      <url hash="662ea4ca">2020.lt4hala-1.14</url>
      <language>eng</language>
      <bibkey>martinez-garcia-garcia-tejedor-2020-latin</bibkey>
    </paper>
    <paper id="19">
      <title>A Gradient Boosting-Seq2Seq System for Latin POS Tagging and <a href="https://en.wikipedia.org/wiki/Lemmatization">Lemmatization</a><fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq System for <fixed-case>L</fixed-case>atin <fixed-case>POS</fixed-case> Tagging and Lemmatization</title>
      <author><first>Giuseppe G. A.</first><last>Celano</last></author>
      <pages>119–123</pages>
      <abstract>The paper presents the system used in the EvaLatin shared task to POS tag and lemmatize Latin. It consists of two components. A gradient boosting machine (LightGBM) is used for POS tagging, mainly fed with pre-computed word embeddings of a window of seven contiguous tokensthe token at hand plus the three preceding and following onesper target feature value. Word embeddings are trained on the texts of the <a href="https://en.wikipedia.org/wiki/Perseus_Digital_Library">Perseus Digital Library</a>, <a href="https://en.wikipedia.org/wiki/Patrologia_Latina">Patrologia Latina</a>, and Biblioteca Digitale di Testi Tardo Antichi, which together comprise a high number of texts of different genres from the Classical Age to Late Antiquity. Word forms plus the outputted POS labels are used to feed a seq2seq algorithm implemented in <a href="https://en.wikipedia.org/wiki/Keras">Keras</a> to predict lemmas. The final shared-task accuracies measured for Classical Latin texts are in line with state-of-the-art POS taggers (0.96) and lemmatizers (0.95).</abstract>
      <url hash="179587e2">2020.lt4hala-1.19</url>
      <language>eng</language>
      <bibkey>celano-2020-gradient</bibkey>
    </paper>
    </volume>
</collection>