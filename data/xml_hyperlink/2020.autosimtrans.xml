<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.autosimtrans">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the First Workshop on Automatic Simultaneous Translation</booktitle>
      <editor><first>Hua</first><last>Wu</last></editor>
      <editor><first>Collin</first><last>Cherry</last></editor>
      <editor><first>Liang</first><last>Huang</last></editor>
      <editor><first>Zhongjun</first><last>He</last></editor>
      <editor><first>Mark</first><last>Liberman</last></editor>
      <editor><first>James</first><last>Cross</last></editor>
      <editor id="yang-liu-ict"><first>Yang</first><last>Liu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, Washington</address>
      <month>July</month>
      <year>2020</year>
      <url hash="acf5340f">2020.autosimtrans-1</url>
    </meta>
    <frontmatter>
      <url hash="fdc9613d">2020.autosimtrans-1.0</url>
      <bibkey>autosimtrans-2020-automatic</bibkey>
    </frontmatter>
    <paper id="5">
      <title>Modeling Discourse Structure for Document-level Neural Machine Translation</title>
      <author><first>Junxuan</first><last>Chen</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Jiarui</first><last>Zhang</last></author>
      <author><first>Chulun</first><last>Zhou</last></author>
      <author><first>Jianwei</first><last>Cui</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <pages>30â€“36</pages>
      <abstract>Recently, document-level neural machine translation (NMT) has become a hot topic in the community of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> is based on a hierarchical attention network (HAN) (Miculicich et al., 2018). Specifically, we first parse the input document to obtain its discourse structure. Then, we introduce a Transformer-based path encoder to embed the discourse structure information of each word. Finally, we combine the discourse structure information with the <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> before it is fed into the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a>. Experimental results on the English-to-German dataset show that our model can significantly outperform both Transformer and Transformer+HAN.</abstract>
      <url hash="9016f8ab">2020.autosimtrans-1.5</url>
      <doi>10.18653/v1/2020.autosimtrans-1.5</doi>
      <video href="http://slideslive.com/38929921" />
      <bibkey>chen-etal-2020-modeling</bibkey>
    </paper>
    </volume>
</collection>