<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.figlang">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Figurative Language Processing</booktitle>
      <editor><first>Beata Beigman</first><last>Klebanov</last></editor>
      <editor><first>Ekaterina</first><last>Shutova</last></editor>
      <editor><first>Patricia</first><last>Lichtenstein</last></editor>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Chee</first><last>Wee</last></editor>
      <editor><first>Anna</first><last>Feldman</last></editor>
      <editor><first>Debanjan</first><last>Ghosh</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="5a671364">2020.figlang-1</url>
    </meta>
    <frontmatter>
      <url hash="567b9601">2020.figlang-1.0</url>
      <bibkey>fig-lang-2020-figurative</bibkey>
    </frontmatter>
    <paper id="7">
      <title>Sarcasm Detection in Tweets with BERT and GloVe Embeddings<fixed-case>BERT</fixed-case> and <fixed-case>G</fixed-case>lo<fixed-case>V</fixed-case>e Embeddings</title>
      <author><first>Akshay</first><last>Khatri</last></author>
      <author><first>Pranav</first><last>P</last></author>
      <pages>56–60</pages>
      <abstract>Sarcasm is a form of communication in which the person states opposite of what he actually means. In this paper, we propose using machine learning techniques with BERT and GloVe embeddings to detect <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> in <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a>. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is preprocessed before extracting the <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a>. The proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> also uses all of the context provided in the dataset to which the user is reacting along with his actual response.</abstract>
      <url hash="060f0261">2020.figlang-1.7</url>
      <doi>10.18653/v1/2020.figlang-1.7</doi>
      <video href="http://slideslive.com/38929697" />
      <bibkey>khatri-p-2020-sarcasm</bibkey>
    </paper>
    <paper id="8">
      <title>C-Net : Contextual Network for Sarcasm Detection<fixed-case>C</fixed-case>-Net: Contextual Network for Sarcasm Detection</title>
      <author><first>Amit</first><last>Kumar Jena</last></author>
      <author><first>Aman</first><last>Sinha</last></author>
      <author><first>Rohit</first><last>Agarwal</last></author>
      <pages>61–66</pages>
      <abstract>Automatic Sarcasm Detection in <a href="https://en.wikipedia.org/wiki/Conversation">conversations</a> is a difficult and tricky task. Classifying an utterance as sarcastic or not in isolation can be futile since most of the time the sarcastic nature of a sentence heavily relies on its context. This paper presents our proposed model, <a href="https://en.wikipedia.org/wiki/C-Net">C-Net</a>, which takes contextual information of a sentence in a sequential manner to classify it as sarcastic or non-sarcastic. Our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> showcases competitive performance in the Sarcasm Detection shared task organised on CodaLab and achieved 75.0 % <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> on the Twitter dataset and 66.3 % <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> on Reddit dataset.</abstract>
      <url hash="5ca7edd9">2020.figlang-1.8</url>
      <doi>10.18653/v1/2020.figlang-1.8</doi>
      <video href="http://slideslive.com/38929698" />
      <bibkey>kumar-jena-etal-2020-c</bibkey>
    </paper>
    <paper id="10">
      <title>Sarcasm Identification and Detection in Conversion Context using BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Kalaivani</first><last>A.</last></author>
      <author><first>Thenmozhi</first><last>D.</last></author>
      <pages>72–76</pages>
      <abstract>Sarcasm analysis in user conversion text is automatic detection of any irony, insult, hurting, painful, caustic, <a href="https://en.wikipedia.org/wiki/Humour">humour</a>, vulgarity that degrades an individual. It is helpful in the field of sentimental analysis and <a href="https://en.wikipedia.org/wiki/Cyberbullying">cyberbullying</a>. As an immense growth of <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, sarcasm analysis helps to avoid insult, hurts and <a href="https://en.wikipedia.org/wiki/Humour">humour</a> to affect someone. In this paper, we present traditional machine learning approaches, deep learning approach (LSTM -RNN) and BERT (Bidirectional Encoder Representations from Transformers) for identifying <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a>. We have used the approaches to build the model, to identify and categorize how much conversion context or response is needed for sarcasm detection and evaluated on the two social media forums that is twitter conversation dataset and reddit conversion dataset. We compare the performance based on the approaches and obtained the best F1 scores as 0.722, 0.679 for the <a href="https://en.wikipedia.org/wiki/Twitter">twitter forums</a> and <a href="https://en.wikipedia.org/wiki/Reddit">reddit forums</a> respectively.</abstract>
      <url hash="4c82ea89">2020.figlang-1.10</url>
      <doi>10.18653/v1/2020.figlang-1.10</doi>
      <video href="http://slideslive.com/38929700" />
      <bibkey>a-d-2020-sarcasm</bibkey>
    </paper>
    <paper id="11">
      <title>Neural Sarcasm Detection using Conversation Context</title>
      <author><first>Nikhil</first><last>Jaiswal</last></author>
      <pages>77–82</pages>
      <abstract>Social media platforms and <a href="https://en.wikipedia.org/wiki/Internet_forum">discussion forums</a> such as <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a>, <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>, etc. are filled with <a href="https://en.wikipedia.org/wiki/Literal_and_figurative_language">figurative languages</a>. Sarcasm is one such category of <a href="https://en.wikipedia.org/wiki/Literal_and_figurative_language">figurative language</a> whose presence in a conversation makes <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">language understanding</a> a challenging task. In this paper, we present a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural architecture</a> for sarcasm detection. We investigate various pre-trained language representation models (PLRMs) like BERT, RoBERTa, etc. and fine-tune it on the Twitter dataset. We experiment with a variety of PLRMs either on the twitter utterance in isolation or utilizing the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> along with the utterance. Our findings indicate that by taking into consideration the previous three most recent utterances, the model is more accurately able to classify a conversation as being sarcastic or not. Our best performing <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble model</a> achieves an overall <a href="https://en.wikipedia.org/wiki/F-number">F1 score</a> of 0.790, which ranks us second on the leaderboard of the Sarcasm Shared Task 2020.</abstract>
      <url hash="64c8a878">2020.figlang-1.11</url>
      <doi>10.18653/v1/2020.figlang-1.11</doi>
      <video href="http://slideslive.com/38929701" />
      <bibkey>jaiswal-2020-neural</bibkey>
    </paper>
    <paper id="14">
      <title>A Novel Hierarchical BERT Architecture for Sarcasm Detection<fixed-case>BERT</fixed-case> Architecture for Sarcasm Detection</title>
      <author><first>Himani</first><last>Srivastava</last></author>
      <author><first>Vaibhav</first><last>Varshney</last></author>
      <author><first>Surabhi</first><last>Kumari</last></author>
      <author><first>Saurabh</first><last>Srivastava</last></author>
      <pages>93–97</pages>
      <abstract>Online discussion platforms are often flooded with opinions from users across the world on a variety of topics. Many such posts, comments, or utterances are often sarcastic in nature, i.e., the actual intent is hidden in the sentence and is different from its literal meaning, making the detection of such utterances challenging without additional context. In this paper, we propose a novel deep learning-based approach to detect whether an utterance is sarcastic or non-sarcastic by utilizing the given contexts ina hierarchical manner. We have used <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> from two online discussion platforms-Twitter and Reddit1for our experiments. Experimental and error analysis shows that the hierarchical models can make full use of history to obtain a better representation of contexts and thus, in turn, can outperform their sequential counterparts.</abstract>
      <url hash="3be522ec">2020.figlang-1.14</url>
      <doi>10.18653/v1/2020.figlang-1.14</doi>
      <video href="http://slideslive.com/38929704" />
      <bibkey>srivastava-etal-2020-novel</bibkey>
    </paper>
    <paper id="15">
      <title>Detecting Sarcasm in Conversation Context Using Transformer-Based Models<fixed-case>D</fixed-case>etecting <fixed-case>S</fixed-case>arcasm in <fixed-case>C</fixed-case>onversation <fixed-case>C</fixed-case>ontext <fixed-case>U</fixed-case>sing <fixed-case>T</fixed-case>ransformer-<fixed-case>B</fixed-case>ased <fixed-case>M</fixed-case>odels</title>
      <author><first>Adithya</first><last>Avvaru</last></author>
      <author><first>Sanath</first><last>Vobilisetty</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>98–103</pages>
      <abstract>Sarcasm detection, regarded as one of the sub-problems of <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, is a very typical task because the introduction of sarcastic words can flip the sentiment of the sentence itself. To date, many research works revolve around detecting <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> in one single sentence and there is very limited research to detect <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> resulting from multiple sentences. Current models used Long Short Term Memory (LSTM) variants with or without <a href="https://en.wikipedia.org/wiki/Attention">attention</a> to detect <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> in conversations. We showed that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> using state-of-the-art Bidirectional Encoder Representations from Transformers (BERT), to capture syntactic and semantic information across conversation sentences, performed better than the current <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Based on the data analysis, we estimated that the number of sentences in the conversation that can contribute to the <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> and the results agrees to this estimation. We also perform a comparative study of our different versions of BERT-based model with other variants of LSTM model and XLNet (both using the estimated number of conversation sentences) and find out that BERT-based models outperformed them.</abstract>
      <url hash="f21683bd">2020.figlang-1.15</url>
      <doi>10.18653/v1/2020.figlang-1.15</doi>
      <bibkey>avvaru-etal-2020-detecting</bibkey>
    </paper>
    <paper id="16">
      <title>Using Conceptual Norms for Metaphor Detection</title>
      <author><first>Mingyu</first><last>Wan</last></author>
      <author><first>Kathleen</first><last>Ahrens</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Menghan</first><last>Jiang</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <author><first>Rong</first><last>Xiang</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <pages>104–109</pages>
      <abstract>This paper reports a linguistically-enriched method of detecting token-level metaphors for the second shared task on Metaphor Detection. We participate in all four phases of competition with both <a href="https://en.wikipedia.org/wiki/Digital_data">datasets</a>, i.e. Verbs and AllPOS on the VUA and the TOFEL datasets. We use the modality exclusivity and embodiment norms for constructing a conceptual representation of the nodes and the context. Our <a href="https://en.wikipedia.org/wiki/System">system</a> obtains an <a href="https://en.wikipedia.org/wiki/International_Federation_of_the_Phonographic_Industry">F-score</a> of 0.652 for the VUA Verbs track, which is 5 % higher than the strong baselines. The experimental results across models and datasets indicate the salient contribution of using modality exclusivity and modality shift information for predicting <a href="https://en.wikipedia.org/wiki/Metaphor">metaphoricity</a>.</abstract>
      <url hash="d539dd53">2020.figlang-1.16</url>
      <doi>10.18653/v1/2020.figlang-1.16</doi>
      <video href="http://slideslive.com/38929723" />
      <bibkey>wan-etal-2020-using</bibkey>
    </paper>
    <paper id="18">
      <title>Character aware models with <a href="https://en.wikipedia.org/wiki/Similarity_learning">similarity learning</a> for metaphor detection</title>
      <author><first>Tarun</first><last>Kumar</last></author>
      <author><first>Yashvardhan</first><last>Sharma</last></author>
      <pages>116–125</pages>
      <abstract>Recent work on automatic sequential metaphor detection has involved <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a> initialized with different pre-trained word embeddings and which are sometimes combined with hand engineered features. To capture lexical and orthographic information automatically, in this paper we propose to add character based word representation. Also, to contrast the difference between <a href="https://en.wikipedia.org/wiki/Literal_and_figurative_language">literal and contextual meaning</a>, we utilize a similarity network. We explore these components via two different architectures-a BiLSTM model and a Transformer Encoder model similar to BERT to perform metaphor identification. We participate in the Second Shared Task on Metaphor Detection on both the VUA and TOFEL datasets with the above models. The experimental results demonstrate the effectiveness of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> as it outperforms all the <a href="https://en.wikipedia.org/wiki/System">systems</a> which participated in the previous shared task.</abstract>
      <url hash="50d48c2c">2020.figlang-1.18</url>
      <doi>10.18653/v1/2020.figlang-1.18</doi>
      <video href="http://slideslive.com/38929724" />
      <bibkey>kumar-sharma-2020-character</bibkey>
    </paper>
    <paper id="20">
      <title>Recognizing Euphemisms and Dysphemisms Using <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a></title>
      <author><first>Christian</first><last>Felt</last></author>
      <author><first>Ellen</first><last>Riloff</last></author>
      <pages>136–145</pages>
      <abstract>This paper presents the first research aimed at recognizing euphemistic and dysphemistic phrases with <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. Euphemisms soften references to topics that are sensitive, disagreeable, or taboo. Conversely, <a href="https://en.wikipedia.org/wiki/Dysphemism">dysphemisms</a> refer to sensitive topics in a harsh or rude way. For example, passed away and departed are <a href="https://en.wikipedia.org/wiki/Euphemism">euphemisms</a> for death, while croaked and six feet under are <a href="https://en.wikipedia.org/wiki/Dysphemism">dysphemisms</a> for death. Our work explores the use of <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> to recognize euphemistic and dysphemistic language. First, we identify near-synonym phrases for three topics (firing, lying, and stealing) using a bootstrapping algorithm for semantic lexicon induction. Next, we classify phrases as <a href="https://en.wikipedia.org/wiki/Euphemism">euphemistic</a>, dysphemistic, or neutral using <a href="https://en.wikipedia.org/wiki/Lexical_analysis">lexical sentiment cues</a> and contextual sentiment analysis. We introduce a new gold standard data set and present our experimental results for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>.</abstract>
      <url hash="e06ee38a">2020.figlang-1.20</url>
      <doi>10.18653/v1/2020.figlang-1.20</doi>
      <video href="http://slideslive.com/38929717" />
      <bibkey>felt-riloff-2020-recognizing</bibkey>
    </paper>
    <paper id="23">
      <title>Generating Ethnographic Models from Communities’ Online Data</title>
      <author><first>Tomek</first><last>Strzalkowski</last></author>
      <author><first>Anna</first><last>Newheiser</last></author>
      <author><first>Nathan</first><last>Kemper</last></author>
      <author><first>Ning</first><last>Sa</last></author>
      <author><first>Bharvee</first><last>Acharya</last></author>
      <author><first>Gregorios</first><last>Katsios</last></author>
      <pages>165–175</pages>
      <abstract>In this paper we describe computational ethnography study to demonstrate how machine learning techniques can be utilized to exploit bias resident in language data produced by communities with online presence. Specifically, we leverage the use of <a href="https://en.wikipedia.org/wiki/Figurative_language">figurative language</a> (i.e., the choice of metaphors) in <a href="https://en.wikipedia.org/wiki/Online_and_offline">online text</a> (e.g., <a href="https://en.wikipedia.org/wiki/News_media">news media</a>, blogs) produced by distinct communities to obtain models of community worldviews that can be shown to be distinctly biased and thus different from other communities’ models. We automatically construct metaphor-based community models for two distinct scenarios : <a href="https://en.wikipedia.org/wiki/Gun_politics_in_the_United_States">gun rights</a> and <a href="https://en.wikipedia.org/wiki/Same-sex_marriage_in_the_United_States">marriage equality</a>. We then conduct a series of experiments to validate the hypothesis that the <a href="https://en.wikipedia.org/wiki/Metaphor">metaphors</a> found in each community’s online language convey the bias in the community’s worldview.</abstract>
      <url hash="10dcef0a">2020.figlang-1.23</url>
      <attachment type="Software" hash="3084e139">2020.figlang-1.23.Software.zip</attachment>
      <doi>10.18653/v1/2020.figlang-1.23</doi>
      <attachment type="Dataset" hash="5d983303">2020.figlang-1.23.Dataset.pdf</attachment>
      <video href="http://slideslive.com/38929711" />
      <bibkey>strzalkowski-etal-2020-generating</bibkey>
    </paper>
    <paper id="28">
      <title>Augmenting Neural Metaphor Detection with Concreteness</title>
      <author><first>Ghadi</first><last>Alnafesah</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>204–210</pages>
      <abstract>The idea that a shift in <a href="https://en.wikipedia.org/wiki/Concreteness">concreteness</a> within a sentence indicates the presence of a <a href="https://en.wikipedia.org/wiki/Metaphor">metaphor</a> has been around for a while. However, recent methods of detecting metaphor that have relied on <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural models</a> have ignored <a href="https://en.wikipedia.org/wiki/Concreteness">concreteness</a> and related psycholinguistic information. We hypothesis that this <a href="https://en.wikipedia.org/wiki/Information">information</a> is not available to these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> and that their addition will boost the performance of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> in detecting <a href="https://en.wikipedia.org/wiki/Metaphor">metaphor</a>. We test this hypothesis on the Metaphor Detection Shared Task 2020 and find that the addition of concreteness information does in fact boost <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural models</a>. We also run tests on data from a previous <a href="https://en.wikipedia.org/wiki/Task_(computing)">shared task</a> and show similar results.</abstract>
      <url hash="7777f0bb">2020.figlang-1.28</url>
      <doi>10.18653/v1/2020.figlang-1.28</doi>
      <bibkey>alnafesah-etal-2020-augmenting</bibkey>
    </paper>
    <paper id="33">
      <title>Metaphor Detection using Ensembles of Bidirectional Recurrent Neural Networks</title>
      <author><first>Jennifer</first><last>Brooks</last></author>
      <author><first>Abdou</first><last>Youssef</last></author>
      <pages>244–249</pages>
      <abstract>In this paper we present our results from the Second Shared Task on Metaphor Detection, hosted by the Second Workshop on Figurative Language Processing. We use an ensemble of RNN models with bidirectional LSTMs and bidirectional attention mechanisms. Some of the <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> were trained on all parts of speech. Each of the other models was trained on one of four categories for <a href="https://en.wikipedia.org/wiki/Part_of_speech">parts of speech</a> : <a href="https://en.wikipedia.org/wiki/Noun">nouns</a>, <a href="https://en.wikipedia.org/wiki/Verb">verbs</a>, <a href="https://en.wikipedia.org/wiki/Adjective">adverbs / adjectives</a>, or other. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> were combined into voting pools and the voting pools were combined using the logical OR operator.</abstract>
      <url hash="f68d3dff">2020.figlang-1.33</url>
      <doi>10.18653/v1/2020.figlang-1.33</doi>
      <video href="http://slideslive.com/38929728" />
      <bibkey>brooks-youssef-2020-metaphor</bibkey>
    </paper>
    <paper id="35">
      <title>Testing the role of <a href="https://en.wikipedia.org/wiki/Metadata">metadata</a> in metaphor identification</title>
      <author><first>Egon</first><last>Stemle</last></author>
      <author><first>Alexander</first><last>Onysko</last></author>
      <pages>256–263</pages>
      <abstract>This paper describes the adaptation and application of a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network system</a> for the automatic detection of metaphors. The LSTM BiRNN system participated in the shared task of metaphor identification that was part of the Second Workshop of Figurative Language Processing (FigLang2020) held at the Annual Conference of the Association for Computational Linguistics (ACL2020). The particular focus of our approach is on the potential influence that the <a href="https://en.wikipedia.org/wiki/Metadata">metadata</a> given in the ETS Corpus of Non-Native Written English might have on the automatic detection of metaphors in this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. The article first discusses the annotated ETS learner data, highlighting some of its peculiarities and inherent biases of metaphor use. A series of evaluations follow in order to test whether specific <a href="https://en.wikipedia.org/wiki/Metadata">metadata</a> influence the <a href="https://en.wikipedia.org/wiki/System">system</a> performance in the task of automatic metaphor identification. The <a href="https://en.wikipedia.org/wiki/System">system</a> is available under the APLv2 open-source license.</abstract>
      <url hash="b5fcbc81">2020.figlang-1.35</url>
      <doi>10.18653/v1/2020.figlang-1.35</doi>
      <video href="http://slideslive.com/38929730" />
      <bibkey>stemle-onysko-2020-testing</bibkey>
    </paper>
    </volume>
</collection>