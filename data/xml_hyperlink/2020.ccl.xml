<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.ccl">
  <volume id="1" ingest-date="2020-10-25">
    <meta>
      <booktitle>Proceedings of the 19th Chinese National Conference on Computational Linguistics</booktitle>
      <editor><first>Maosong</first><last>Sun</last><variant script="hani"><first>茂松</first><last>孙</last></variant></editor>
      <editor><first>Sujian</first><last>Li</last><variant script="hani"><first>素建</first><last>李</last></variant></editor>
      <editor><first>Yue</first><last>Zhang</last><variant script="hani"><first>岳</first><last>张</last></variant></editor>
      <editor id="yang-liu-ict"><first>Yang</first><last>Liu</last><variant script="hani"><first>洋</first><last>刘</last></variant></editor>
      <publisher>Chinese Information Processing Society of China</publisher>
      <address>Haikou, China</address>
      <month>October</month>
      <year>2020</year>
      <url hash="77087a9d">2020.ccl-1</url>
    </meta>
    <frontmatter>
      <url hash="40669208">2020.ccl-1.0</url>
      <bibkey>ccl-2020-chinese</bibkey>
    </frontmatter>
    <paper id="76">
      <title>A Joint Model for Graph-based Chinese Dependency Parsing<fixed-case>C</fixed-case>hinese Dependency Parsing</title>
      <author><first>Xingchen</first><last>Li</last></author>
      <author><first>Mingtong</first><last>Liu</last></author>
      <author><first>Yujie</first><last>Zhang</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <pages>820–830</pages>
      <abstract>In Chinese dependency parsing, the joint model of <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a>, POS tagging and dependency parsing has become the mainstream framework because it can eliminate error propagation and share knowledge, where the transition-based model with feature templates maintains the best performance. Recently, the graph-based joint model (Yan et al., 2019) on <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a> and dependency parsing has achieved better performance, demonstrating the advantages of the graph-based models. However, this work can not provide POS information for downstream tasks, and the POS tagging task was proved to be helpful to the dependency parsing according to the research of the transition-based model. Therefore, we propose a graph-based joint model for <a href="https://en.wikipedia.org/wiki/Chinese_word_segmentation">Chinese word segmentation</a>, <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">POS tagging</a> and dependency parsing. We designed a charater-level POS tagging task, and then train it jointly with the model of Yan et al. We adopt two methods of joint POS tagging task, one is by sharing parameters, the other is by using tag attention mechanism, which enables the three tasks to better share intermediate information and improve each other’s performance. The experimental results on the Penn Chinese treebank (CTB5) show that our proposed joint model improved by 0.38 % on dependency parsing than the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> of Yan et al. Compared with the best transition-based joint model, our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> improved by 0.18 %, 0.35 % and 5.99 % respectively in terms of <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a>, <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">POS tagging</a> and dependency parsing.</abstract>
      <url hash="e94ecd79">2020.ccl-1.76</url>
      <language>eng</language>
      <bibkey>li-etal-2020-joint</bibkey>
    </paper>
    <paper id="77">
      <title>Semantic-aware Chinese Zero Pronoun Resolution with Pre-trained Semantic Dependency Parser<fixed-case>C</fixed-case>hinese Zero Pronoun Resolution with Pre-trained Semantic Dependency Parser</title>
      <author><first>Lanqiu</first><last>Zhang</last></author>
      <author><first>Zizhuo</first><last>Shen</last></author>
      <author><first>Yanqiu</first><last>Shao</last></author>
      <pages>831–841</pages>
      <abstract>Deep learning-based Chinese zero pronoun resolution model has achieved better performance than traditional machine learning-based model. However, the existing work related to Chinese zero pronoun resolution has not yet well integrated linguistic information into the deep learningbased Chinese zero pronoun resolution model. This paper adopts the idea based on the pre-trained model, and integrates the semantic representations in the pre-trained Chinese semantic dependency graph parser into the Chinese zero pronoun resolution model. The experimental results on OntoNotes-5.0 dataset show that our proposed Chinese zero pronoun resolution model with pretrained Chinese semantic dependency parser improves the F-score by 0.4 % compared with our baseline model, and obtains better results than other deep learning-based Chinese zero pronoun resolution models. In addition, we integrate the BERT representations into our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> so that the performance of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> was improved by 0.7 % compared with our baseline model.</abstract>
      <url hash="d997d211">2020.ccl-1.77</url>
      <language>eng</language>
      <bibkey>zhang-etal-2020-semantic</bibkey>
    </paper>
    <paper id="82">
      <title>Refining Data for <a href="https://en.wikipedia.org/wiki/Text-based_user_interface">Text Generation</a></title>
      <author><first>Wenyu</first> <last>Guan</last></author>
      <author><first>Qianying</first> <last>Liu</last></author>
      <author><first>Tianyi</first> <last>Li</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <pages>881–891</pages>
      <abstract>Recent work on data-to-text generation has made progress under the neural encoder-decoder architectures. However, the data input size is often enormous, while not all data records are important for text generation and inappropriate input may bring noise into the final output. To solve this problem, we propose a two-step approach which first selects and orders the important data records and then generates text from the noise-reduced data. Here we propose a learning to rank model to rank the importance of each record which is supervised by a relation extractor. With the noise-reduced data as input, we implement a <a href="https://en.wikipedia.org/wiki/Text_generator">text generator</a> which sequentially models the input data records and emits a summary. Experiments on the ROTOWIRE dataset verifies the effectiveness of our proposed method in both performance and efficiency.</abstract>
      <url hash="61f213b9">2020.ccl-1.82</url>
      <language>eng</language>
      <bibkey>guan-etal-2020-refining</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/rotowire">RotoWire</pwcdataset>
    </paper>
    <paper id="86">
      <title>Chinese Named Entity Recognition via Adaptive Multi-pass Memory Network with Hierarchical Tagging Mechanism<fixed-case>C</fixed-case>hinese Named Entity Recognition via Adaptive Multi-pass Memory Network with Hierarchical Tagging Mechanism</title>
      <author><first>Pengfei</first><last>Cao</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>927–938</pages>
      <abstract>Named entity recognition (NER) aims to identify text spans that mention named entities and classify them into pre-defined categories. For Chinese NER task, most of the existing methods are character-based sequence labeling models and achieve great success. However, these methods usually ignore <a href="https://en.wikipedia.org/wiki/Lexical_analysis">lexical knowledge</a>, which leads to false prediction of entity boundaries. Moreover, these <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a> have difficulties in capturing tag dependencies. In this paper, we propose an Adaptive Multi-pass Memory Network with Hierarchical Tagging Mechanism (AMMNHT) to address all above problems. Specifically, to reduce the errors of predicting entity boundaries, we propose an adaptive multi-pass memory network to exploit lexical knowledge. In addition, we propose a hierarchical tagging layer to learn tag dependencies. Experimental results on three widely used Chinese NER datasets demonstrate that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> significantly outperforms other state-of-the-art methods.</abstract>
      <url hash="d79ac63c">2020.ccl-1.86</url>
      <language>eng</language>
      <bibkey>cao-etal-2020-chinese</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/weibo-ner">Weibo NER</pwcdataset>
    </paper>
    <paper id="89">
      <title>Entity Relative Position Representation based Multi-head Selection for Joint Entity and Relation Extraction</title>
      <author><first>Tianyang</first><last>Zhao</last></author>
      <author><first>Zhao</first><last>Yan</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>962–973</pages>
      <abstract>Joint entity and relation extraction has received increasing interests recently, due to the capability of utilizing the interactions between both steps. Among existing studies, the Multi-Head Selection (MHS) framework is efficient in extracting entities and relations simultaneously. However, the method is weak for its limited performance. In this paper, we propose several effective insights to address this problem. First, we propose an entity-specific Relative Position Representation (eRPR) to allow the model to fully leverage the distance information between entities and context tokens. Second, we introduce an auxiliary Global Relation Classification (GRC) to enhance the learning of local contextual features. Moreover, we improve the semantic representation by adopting a pre-trained language model BERT as the feature encoder. Finally, these new keypoints are closely integrated with the multi-head selection framework and optimized jointly. Extensive experiments on two benchmark datasets demonstrate that our approach overwhelmingly outperforms previous works in terms of all evaluation metrics, achieving significant improvements for relation F1 by +2.40 % on CoNLL04 and +1.90 % on ACE05, respectively.</abstract>
      <url hash="c7b42b94">2020.ccl-1.89</url>
      <language>eng</language>
      <bibkey>zhao-etal-2020-entity</bibkey>
    </paper>
    <paper id="92">
      <title>Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning</title>
      <author><first>Xiuhong</first><last>Li</last></author>
      <author><first>Zhe</first><last>Li</last></author>
      <author><first>Jiabao</first><last>Sheng</last></author>
      <author><first>Wushour</first><last>Slamu</last></author>
      <pages>994–1005</pages>
      <abstract>Text classification tends to be difficult when data are inadequate considering the amount of manually labeled text corpora. For low-resource agglutinative languages including <a href="https://en.wikipedia.org/wiki/Uyghur_language">Uyghur</a>, <a href="https://en.wikipedia.org/wiki/Kazakh_language">Kazakh</a>, and Kyrgyz (UKK languages), in which words are manufactured via <a href="https://en.wikipedia.org/wiki/Word_stem">stems</a> concatenated with several suffixes and <a href="https://en.wikipedia.org/wiki/Word_stem">stems</a> are used as the representation of text content, this feature allows infinite derivatives vocabulary that leads to high uncertainty of writing forms and huge redundant features. There are major challenges of low-resource agglutinative text classification the lack of labeled data in a target domain and morphologic diversity of derivations in language structures. It is an effective solution which fine-tuning a pre-trained language model to provide meaningful and favorable-to-use <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extractors</a> for downstream text classification tasks. To this end, we propose a low-resource agglutinative language model fine-tuning AgglutiFiT, specifically, we build a low-noise fine-tuning dataset by morphological analysis and stem extraction, then fine-tune the cross-lingual pre-training model on this dataset. Moreover, we propose an attention-based fine-tuning strategy that better selects relevant semantic and syntactic information from the pre-trained language model and uses those <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> on downstream text classification tasks. We evaluate our methods on nine Uyghur, Kazakh, and Kyrgyz classification datasets, where they have significantly better performance compared with several strong baselines.</abstract>
      <url hash="7f9095fc">2020.ccl-1.92</url>
      <language>eng</language>
      <bibkey>li-etal-2020-low</bibkey>
    </paper>
    <paper id="93">
      <title>Constructing Uyghur Name Entity Recognition System using Neural Machine Translation Tag Projection<fixed-case>U</fixed-case>yghur Name Entity Recognition System using Neural Machine Translation Tag Projection</title>
      <author><first>Anwar</first><last>Azmat</last></author>
      <author><first>Li</first><last>Xiao</last></author>
      <author><first>Yang</first><last>Yating</last></author>
      <author><first>Dong</first><last>Rui</last></author>
      <author><first>Osman</first><last>Turghun</last></author>
      <pages>1006–1016</pages>
      <abstract>Although <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> achieved great success by introducing the <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>, it is challenging to apply these models to low resource languages including <a href="https://en.wikipedia.org/wiki/Uyghur_language">Uyghur</a> while it depends on a large amount of annotated training data. Constructing a well-annotated named entity corpus manually is very time-consuming and labor-intensive. Most existing methods based on the <a href="https://en.wikipedia.org/wiki/Parallel_corpus">parallel corpus</a> combined with the word alignment tools. However, word alignment methods introduce alignment errors inevitably. In this paper, we address this problem by a named entity tag transfer method based on the common neural machine translation. The proposed method marks the entity boundaries in Chinese sentence and translates the sentences to <a href="https://en.wikipedia.org/wiki/Uyghur_language">Uyghur</a> by <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation system</a>, hope that <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> will align the source and target entity by the self-attention mechanism. The experimental results show that the Uyghur named entity recognition system trained by the constructed corpus achieve good performance on the test set, with 73.80 % F1 score(3.79 % improvement by baseline)</abstract>
      <url hash="f94148a5">2020.ccl-1.93</url>
      <language>eng</language>
      <bibkey>azmat-etal-2020-constructing</bibkey>
    </paper>
    <paper id="95">
      <title>Mongolian Questions Classification Based on Mulit-Head Attention<fixed-case>M</fixed-case>ongolian Questions Classification Based on Mulit-Head Attention</title>
      <author><first>Guangyi</first><last>Wang</last></author>
      <author><first>Feilong</first><last>Bao</last></author>
      <author><first>Weihua</first><last>Wang</last></author>
      <pages>1026–1034</pages>
      <abstract>Question classification is a crucial subtask in <a href="https://en.wikipedia.org/wiki/Question_answering">question answering system</a>. Mongolian is a kind of few resource language. It lacks public labeled corpus. And the complex morphological structure of <a href="https://en.wikipedia.org/wiki/Mongolian_language">Mongolian vocabulary</a> makes the data-sparse problem. This paper proposes a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification model</a>, which combines the Bi-LSTM model with the Multi-Head Attention mechanism. The Multi-Head Attention mechanism extracts relevant information from different dimensions and representation subspace. According to the characteristics of Mongolian word-formation, this paper introduces Mongolian morphemes representation in the embedding layer. Morpheme vector focuses on the <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> of the Mongolian word. In this paper, character vector and morpheme vector are concatenated to get <a href="https://en.wikipedia.org/wiki/Word_vector">word vector</a>, which sends to the Bi-LSTM getting context representation. Finally, the Multi-Head Attention obtains global information for <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> experimented on the <a href="https://en.wikipedia.org/wiki/Mongolian_writing_systems">Mongolian corpus</a>. Experimental results show that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> significantly outperforms baseline systems.</abstract>
      <url hash="6a38d06a">2020.ccl-1.95</url>
      <language>eng</language>
      <bibkey>wang-etal-2020-mongolian</bibkey>
    </paper>
    <paper id="97">
      <title>Categorizing Offensive Language in <a href="https://en.wikipedia.org/wiki/Social_network">Social Networks</a> : A Chinese Corpus, Systems and an Explainable Tool<fixed-case>C</fixed-case>hinese Corpus, Systems and an Explainable Tool</title>
      <author><first>Xiangru</first><last>Tang</last></author>
      <author><first>Xianjun</first><last>Shen</last></author>
      <pages>1045–1056</pages>
      <abstract>Recently, more and more data have been generated in the online world, filled with <a href="https://en.wikipedia.org/wiki/Profanity">offensive language</a> such as <a href="https://en.wikipedia.org/wiki/Threat">threats</a>, <a href="https://en.wikipedia.org/wiki/Profanity">swear words</a> or straightforward insults. It is disgraceful for a progressive society, and then the question arises on how language resources and technologies can cope with this challenge. However, previous work only analyzes the problem as a whole but fails to detect particular types of offensive content in a more fine-grained way, mainly because of the lack of annotated data. In this work, we present a densely annotated data-set COLA</abstract>
      <url hash="479b95bc">2020.ccl-1.97</url>
      <language>eng</language>
      <bibkey>tang-shen-2020-categorizing</bibkey>
    </paper>
    <paper id="98">
      <title>LiveQA : A Question Answering Dataset over Sports Live<fixed-case>L</fixed-case>ive<fixed-case>QA</fixed-case>: A Question Answering Dataset over Sports Live</title>
      <author><first>Liu</first><last>Qianying</last></author>
      <author><first>Jiang</first><last>Sicong</last></author>
      <author><first>Wang</first><last>Yizhong</last></author>
      <author><first>Li</first><last>Sujian</last></author>
      <pages>1057–1067</pages>
      <abstract>In this paper, we introduce LiveQA, a new question answering dataset constructed from play-by-play live broadcast. It contains 117k multiple-choice questions written by human commentators for over 1,670 NBA games, which are collected from the Chinese Hupu1 website. Derived from the characteristics of <a href="https://en.wikipedia.org/wiki/Sports_game">sports games</a>, LiveQA can potentially test the reasoning ability across timeline-based live broadcasts, which is challenging compared to the existing <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>. In LiveQA, the questions require understanding the <a href="https://en.wikipedia.org/wiki/Timeline">timeline</a>, tracking events or doing <a href="https://en.wikipedia.org/wiki/Computation">mathematical computations</a>. Our preliminary experiments show that the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> introduces a challenging problem for <a href="https://en.wikipedia.org/wiki/Question_answering">question answering models</a>, and a strong baseline model only achieves the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 53.1 % and can not beat the dominant option rule. We release the code and data of this paper for future research.</abstract>
      <url hash="b9bbb95e">2020.ccl-1.98</url>
      <language>eng</language>
      <bibkey>qianying-etal-2020-liveqa</bibkey>
      <pwccode url="https://github.com/PKU-TANGENT/LiveQA" additional="true">PKU-TANGENT/LiveQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/liveqa">LiveQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cbt">CBT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="102">
      <title>CAN-GRU : a Hierarchical Model for Emotion Recognition in Dialogue<fixed-case>CAN</fixed-case>-<fixed-case>GRU</fixed-case>: a Hierarchical Model for Emotion Recognition in Dialogue</title>
      <author><first>Ting</first><last>Jiang</last></author>
      <author><first>Bing</first><last>Xu</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <author><first>Sheng</first><last>Li</last></author>
      <pages>1101–1111</pages>
      <abstract>Emotion recognition in dialogue systems has gained attention in the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> recent years, because it can be applied in <a href="https://en.wikipedia.org/wiki/Opinion_mining">opinion mining</a> from public conversational data on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. In this paper, we propose a <a href="https://en.wikipedia.org/wiki/Hierarchical_model">hierarchical model</a> to recognize <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> in the <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a>. In the first layer, in order to extract textual features of utterances, we propose a convolutional self-attention network(CAN). Convolution is used to capture <a href="https://en.wikipedia.org/wiki/N-gram">n-gram information</a> and attention mechanism is used to obtain the relevant semantic information among words in the utterance. In the second layer, a GRU-based network helps to capture <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> in the conversation. Furthermore, we discuss the effects of unidirectional and bidirectional networks. We conduct experiments on Friends dataset and EmotionPush dataset. The results show that our proposed model(CAN-GRU) and its variants achieve better performance than <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="1cf91179">2020.ccl-1.102</url>
      <language>eng</language>
      <bibkey>jiang-etal-2020-gru</bibkey>
    </paper>
    <paper id="103">
      <title>A Joint Model for Aspect-Category Sentiment Analysis with Shared Sentiment Prediction Layer</title>
      <author><first>Yuncong</first><last>Li</last></author>
      <author><first>Zhe</first><last>Yang</last></author>
      <author><first>Cunxiang</first><last>Yin</last></author>
      <author><first>Xu</first><last>Pan</last></author>
      <author><first>Lunan</first><last>Cui</last></author>
      <author><first>Qiang</first><last>Huang</last></author>
      <author><first>Ting</first><last>Wei</last></author>
      <pages>1112–1121</pages>
      <abstract>Aspect-category sentiment analysis (ACSA) aims to predict the <a href="https://en.wikipedia.org/wiki/Aspect_(grammar)">aspect categories</a> mentioned in texts and their corresponding <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiment polarities</a>. Some joint models have been proposed to address this task. Given a text, these joint models detect all the <a href="https://en.wikipedia.org/wiki/Grammatical_aspect">aspect categories</a> mentioned in the text and predict the <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiment polarities</a> toward them at once. Although these joint models obtain promising performances, they train separate parameters for each aspect category and therefore suffer from data deficiency of some aspect categories. To solve this problem, we propose a novel joint model which contains a shared sentiment prediction layer. The shared sentiment prediction layer transfers <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment knowledge</a> between aspect categories and alleviates the problem caused by data deficiency. Experiments conducted on SemEval-2016 Datasets demonstrate the effectiveness of our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>.</abstract>
      <url hash="b76189da">2020.ccl-1.103</url>
      <language>eng</language>
      <bibkey>li-etal-2020-joint-model</bibkey>
    </paper>
    <paper id="106">
      <title>Clickbait Detection with Style-aware Title Modeling and Co-attention</title>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Tao</first><last>Qi</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>1143–1154</pages>
      <abstract>Clickbait is a form of <a href="https://en.wikipedia.org/wiki/Web_content">web content</a> designed to attract attention and entice users to click on specific hyperlinks. The detection of clickbaits is an important task for online platforms to improve the quality of web content and the satisfaction of users. Clickbait detection is typically formed as a binary classification task based on the title and body of a webpage, and existing methods are mainly based on the content of title and the relevance between title and body. However, these methods ignore the stylistic patterns of titles, which can provide important clues on identifying <a href="https://en.wikipedia.org/wiki/Clickbait">clickbaits</a>. In addition, they do not consider the interactions between the contexts within title and body, which are very important for measuring their <a href="https://en.wikipedia.org/wiki/Relevance">relevance</a> for clickbait detection. In this paper, we propose a clickbait detection approach with style-aware title modeling and co-attention. Specifically, we use Transformers to learn content representations of title and body, and respectively compute two content-based clickbait scores for title and body based on their <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a>. In addition, we propose to use a character-level Transformer to learn a style-aware title representation by capturing the stylistic patterns of title, and we compute a title stylistic score based on this representation. Besides, we propose to use a co-attention network to model the relatedness between the contexts within title and body, and further enhance their representations by encoding the interaction information. We compute a title-body matching score based on the representations of title and body enhanced by their interactions. The final clickbait score is predicted by a weighted summation of the aforementioned four kinds of scores.</abstract>
      <url hash="66e0883f">2020.ccl-1.106</url>
      <language>eng</language>
      <bibkey>wu-etal-2020-clickbait</bibkey>
    </paper>
    </volume>
</collection>