<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.pam">
  <volume id="1" ingest-date="2020-10-13">
    <meta>
      <booktitle>Proceedings of the Probability and Meaning Conference (PaM 2020)</booktitle>
      <editor><first>Christine</first><last>Howes</last></editor>
      <editor><first>Stergios</first><last>Chatzikyriakidis</last></editor>
      <editor><first>Adam</first><last>Ek</last></editor>
      <editor><first>Vidya</first><last>Somashekarappa</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gothenburg</address>
      <month>June</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="7e1b77c7">2020.pam-1.0</url>
      <revision id="1" href="2020.pam-1.0v1" hash="4a240c7b" />
      <revision id="2" href="2020.pam-1.0v2" hash="7e1b77c7" date="2021-05-04">Author typo correction.</revision>
      <bibkey>pam-2020-probability</bibkey>
    </frontmatter>
    <paper id="1">
      <title>‘Practical’, if that’s the word</title>
      <author><first>Eimear</first><last>Maguire</last></author>
      <pages>1–7</pages>
      <abstract>Certain conditionals have something other than a clause as their consequent : their antecedent if-clauses are ‘adverbial clauses’ without a verb. We argue that they function in a way already seen for those with clausal consequents, despite lacking the content we might expect for the formation of a conditional. The use of the if-clause with sub-clausal consequents is feasible thanks to the fact that this function does not depend on the consequent content, and so is not impeded when the consequent does not provide a proposition, question or imperative. To support this we provide meaning rules for <a href="https://en.wikipedia.org/wiki/Conditional_(computer_programming)">conditionals</a> in terms of information state updates, letting the same construction play out in different ways depending on context and content.</abstract>
      <url hash="9a171d91">2020.pam-1.1</url>
      <bibkey>maguire-2020-practical</bibkey>
    </paper>
    <paper id="12">
      <title>A toy distributional model for fuzzy generalised quantifiers</title>
      <author><first>Mehrnoosh</first><last>Sadrzadeh</last></author>
      <author><first>Gijs</first><last>Wijnholds</last></author>
      <pages>86–94</pages>
      <abstract>Recent work in compositional distributional semantics showed how bialgebras model generalised quantifiers of natural language. That technique requires working with <a href="https://en.wikipedia.org/wiki/Vector_space">vector space</a> over power sets of bases, and therefore is computationally costly. It is possible to overcome the computational hurdles by working with fuzzy generalised quantifiers. In this paper, we show that the compositional notion of semantics of <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>, guided by a <a href="https://en.wikipedia.org/wiki/Grammar">grammar</a>, extends from a binary to a many valued setting and instantiate in it the <a href="https://en.wikipedia.org/wiki/Fuzzy_logic">fuzzy computations</a>. We import vector representations of words and predicates, learnt from large scale compositional distributional semantics, interpret them as fuzzy sets, and analyse their performance on a toy inference dataset.</abstract>
      <url hash="0d918ad4">2020.pam-1.12</url>
      <bibkey>sadrzadeh-wijnholds-2020-toy</bibkey>
    </paper>
    <paper id="13">
      <title>Generating Lexical Representations of Frames using Lexical Substitution</title>
      <author><first>Saba</first><last>Anwar</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>95–103</pages>
      <abstract>Semantic frames are <a href="https://en.wikipedia.org/wiki/Formal_language">formal linguistic structures</a> describing situations / actions / events, e.g. Commercial transfer of goods. Each frame provides a set of <a href="https://en.wikipedia.org/wiki/Character_(arts)">roles</a> corresponding to the situation participants, e.g. Buyer and Goods, and lexical units (LUs)   words and phrases that can evoke this particular <a href="https://en.wikipedia.org/wiki/Frame_story">frame</a> in texts, e.g. Sell. The scarcity of annotated resources hinders wider adoption of <a href="https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)">frame semantics</a> across languages and domains. We investigate a simple yet effective method, <a href="https://en.wikipedia.org/wiki/Lexical_substitution">lexical substitution</a> with word representation models, to automatically expand a small set of frame-annotated sentences with new words for their respective roles and LUs. We evaluate the expansion quality using <a href="https://en.wikipedia.org/wiki/FrameNet">FrameNet</a>. Contextualized models demonstrate overall superior performance compared to the non-contextualized ones on roles. However, the latter show comparable performance on the task of LU expansion.</abstract>
      <url hash="bad2d4b8">2020.pam-1.13</url>
      <bibkey>anwar-etal-2020-generating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="16">
      <title>Building a Swedish Question-Answering Model<fixed-case>S</fixed-case>wedish Question-Answering Model</title>
      <author><first>Hannes</first><last>von Essen</last></author>
      <author><first>Daniel</first><last>Hesslow</last></author>
      <pages>117–127</pages>
      <abstract>High quality datasets for <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> exist in a few languages, but far from all. Producing such <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> for <a href="https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers">new languages</a> requires extensive manual labour. In this work we look at different methods for using existing <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> to train <a href="https://en.wikipedia.org/wiki/Question_answering">question-answering models</a> in languages lacking such datasets. We show that <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> followed by cross-lingual projection is a viable way to create a full question-answering dataset in a new language. We introduce new methods both for bitext alignment, using optimal transport, and for direct cross-lingual projection, utilizing multilingual BERT. We show that our <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> produce good Swedish question-answering models without any <a href="https://en.wikipedia.org/wiki/Manual_labour">manual work</a>. Finally, we apply our proposed methods on <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a> and evaluate it on the XQuAD and MLQA benchmarks where we achieve new state-of-the-art values of 80.4 F1 and 62.9 Exact Match (EM) points on the <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish XQuAD corpus</a> and 70.8 F1 and 53.0 EM on the <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish MLQA corpus</a>, showing that the technique is readily applicable to other languages.</abstract>
      <url hash="e030f8ce">2020.pam-1.16</url>
      <bibkey>von-essen-hesslow-2020-building</bibkey>
      <pwccode url="https://github.com/vottivott/building-a-swedish-qa-model" additional="false">vottivott/building-a-swedish-qa-model</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="18">
      <title>Short-term Semantic Shifts and their Relation to Frequency Change</title>
      <author><first>Anna</first><last>Marakasova</last></author>
      <author><first>Julia</first><last>Neidhardt</last></author>
      <pages>146–153</pages>
      <abstract>We present ongoing research on the relationship between short-term semantic shifts and frequency change patterns by examining the case of the <a href="https://en.wikipedia.org/wiki/European_migrant_crisis">refugee crisis</a> in <a href="https://en.wikipedia.org/wiki/Austria">Austria</a> from 2015 to 2016. Our experiments are carried out on a diachronic corpus of Austrian German, namely a corpus of newspaper articles. We trace the evolution of the usage of words that represent concepts in the context of the <a href="https://en.wikipedia.org/wiki/Refugee_crisis">refugee crisis</a> by analyzing cosine similarities of word vectors over time as well as similarities based on the words’ nearest neighbourhood sets. In order to investigate how exactly the contextual meanings have changed, we measure cosine similarity between the following pairs of words : words describing the <a href="https://en.wikipedia.org/wiki/European_migrant_crisis">refugee crisis</a>, on the one hand, and words indicating the process of <a href="https://en.wikipedia.org/wiki/Mediatisation">mediatization</a> and politicization of the refugee crisis in Austria proposed by a domain expert, on the other hand. We evaluate our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> against the <a href="https://en.wikipedia.org/wiki/Expert_witness">expert knowledge</a>. The paper presents the current findings and outlines the directions of the future work.</abstract>
      <url hash="731eec0d">2020.pam-1.18</url>
      <bibkey>marakasova-neidhardt-2020-short</bibkey>
    </paper>
  </volume>
</collection>