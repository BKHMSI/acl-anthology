<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.crac">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference</booktitle>
      <editor><first>Maciej</first><last>Ogrodniczuk</last></editor>
      <editor><first>Vincent</first><last>Ng</last></editor>
      <editor><first>Yulia</first><last>Grishina</last></editor>
      <editor><first>Sameer</first><last>Pradhan</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="50191fb4">2020.crac-1.0</url>
      <bibkey>crac-2020-models</bibkey>
    </frontmatter>
    <paper id="2">
      <title>It’s absolutely divine ! Can fine-grained sentiment analysis benefit from <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>?</title>
      <author><first>Orphee</first><last>De Clercq</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>11–21</pages>
      <abstract>While it has been claimed that anaphora or <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> plays an important role in <a href="https://en.wikipedia.org/wiki/Opinion_mining">opinion mining</a>, it is not clear to what extent <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> actually boosts performance, if at all. In this paper, we investigate the potential added value of <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> for the aspect-based sentiment analysis of restaurant reviews in two languages, <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Dutch_language">Dutch</a>. We focus on the task of aspect category classification and investigate whether including coreference information prior to <a href="https://en.wikipedia.org/wiki/Categorization">classification</a> to resolve implicit aspect mentions is beneficial. Because <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> is not a solved task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, we rely on both automatically-derived and gold-standard coreference relations, allowing us to investigate the true upper bound. By training a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> on a combination of lexical and semantic features, we show that resolving the coreferential relations prior to <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> is beneficial in a joint optimization setup. However, this is only the case when relying on <a href="https://en.wikipedia.org/wiki/Gold_standard">gold-standard relations</a> and the result is more outspoken for <a href="https://en.wikipedia.org/wiki/English_language">English</a> than for <a href="https://en.wikipedia.org/wiki/Dutch_language">Dutch</a>. When validating the optimal models, however, we found that only the Dutch pipeline is able to achieve a satisfying performance on a held-out test set and does so regardless of whether coreference information was included.</abstract>
      <url hash="01a14f14">2020.crac-1.2</url>
      <bibkey>de-clercq-hoste-2020-absolutely</bibkey>
    </paper>
    <paper id="3">
      <title>Anaphoric Zero Pronoun Identification : A Multilingual Approach</title>
      <author><first>Abdulrahman</first><last>Aloraini</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>22–32</pages>
      <abstract>Pro-drop languages such as <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, <a href="https://en.wikipedia.org/wiki/Italian_language">Italian</a> or <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a> allow morphologically null but referential arguments in certain syntactic positions, called anaphoric zero-pronouns. Much NLP work on anaphoric zero-pronouns (AZP) is based on gold mentions, but models for their identification are a fundamental prerequisite for their resolution in real-life applications. Such <a href="https://en.wikipedia.org/wiki/Identity_(social_science)">identification</a> requires <a href="https://en.wikipedia.org/wiki/Complex_system">complex language understanding</a> and knowledge of real-world entities. Transfer learning models, such as BERT, have recently shown to learn surface, syntactic, and semantic information, which can be very useful in recognizing AZPs. We propose a BERT-based multilingual model for AZP identification from predicted zero pronoun positions, and evaluate it on the Arabic and Chinese portions of OntoNotes 5.0. As far as we know, this is the first neural network model of AZP identification for <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> ; and our approach outperforms the stateof-the-art for <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. Experiment results suggest that BERT implicitly encode information about AZPs through their surrounding context.</abstract>
      <url hash="3b9e3990">2020.crac-1.3</url>
      <bibkey>aloraini-poesio-2020-anaphoric</bibkey>
    </paper>
    <paper id="4">
      <title>Predicting Coreference in Abstract Meaning Representations<fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentations</title>
      <author><first>Tatiana</first><last>Anikina</last></author>
      <author><first>Alexander</first><last>Koller</last></author>
      <author><first>Michael</first><last>Roth</last></author>
      <pages>33–38</pages>
      <abstract>This work addresses <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> in Abstract Meaning Representation (AMR) graphs, a popular formalism for <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a>. We evaluate several current coreference resolution techniques on a recently published AMR coreference corpus, establishing baselines for future work. We also demonstrate that <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> can improve the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of a state-of-the-art <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a> on this <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>.</abstract>
      <url hash="d61d8939">2020.crac-1.4</url>
      <bibkey>anikina-etal-2020-predicting</bibkey>
    </paper>
    <paper id="6">
      <title>TwiConv : A Coreference-annotated Corpus of Twitter Conversations<fixed-case>T</fixed-case>wi<fixed-case>C</fixed-case>onv: A Coreference-annotated Corpus of <fixed-case>T</fixed-case>witter Conversations</title>
      <author><first>Berfin</first><last>Aktaş</last></author>
      <author><first>Annalena</first><last>Kohnert</last></author>
      <pages>47–54</pages>
      <abstract>This article introduces TwiConv, an English coreference-annotated corpus of microblog conversations from <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>. We describe the corpus compilation process and the annotation scheme, and release the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> publicly, along with this paper. We manually annotated nominal coreference in 1756 tweets arranged in 185 conversation threads. The <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> achieves satisfactory <a href="https://en.wikipedia.org/wiki/Annotation">annotation agreement</a> results. We also present a new method for mapping the tweet contents with distributed stand-off annotations, which can easily be adapted to different annotation tasks.</abstract>
      <url hash="ef43979d">2020.crac-1.6</url>
      <bibkey>aktas-kohnert-2020-twiconv</bibkey>
      <pwccode url="https://github.com/berfingit/twiconv" additional="false">berfingit/twiconv</pwccode>
    </paper>
    <paper id="8">
      <title>Reference to Discourse Topics : Introducing Global Shell Nouns</title>
      <author><first>Fabian</first><last>Simonjetz</last></author>
      <pages>68–78</pages>
      <abstract>Shell nouns (SNs) are abstract nouns like fact, issue, and decision, which are capable of refer- ring to non-nominal antecedents, much like <a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)">anaphoric pronouns</a>. As an extension of classical anaphora resolution, the automatic detection of SNs alongside their respective antecedents has received a growing research interest in recent years but proved to be a challenging task. This paper critically examines the assumption prevalent in previous research that SNs are typically accompanied by a specific antecedent, arguing that SNs like issue and decision are frequently used to refer, not to specific antecedents, but to global discourse topics, in which case they are out of reach of previously proposed resolution strategies that are tailored to SNs with explicit antecedents. The contribution of this work is three-fold. First, the notion of global SNs is defined ; second, their qualitative and quantitative impact on previous SN research is investigated ; and third, implications for previous and future approaches to SN resolution are discussed.</abstract>
      <url hash="22cdfbab">2020.crac-1.8</url>
      <bibkey>simonjetz-2020-reference</bibkey>
    </paper>
    <paper id="10">
      <title>Partially-supervised Mention Detection</title>
      <author><first>Lesly</first><last>Miculicich</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <pages>91–98</pages>
      <abstract>Learning to detect entity mentions without using syntactic information can be useful for integration and joint optimization with other tasks. However, it is common to have partially annotated data for this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a>. Here, we investigate two approaches to deal with partial annotation of mentions : weighted loss and soft-target classification. We also propose two neural mention detection approaches : a sequence tagging, and an <a href="https://en.wikipedia.org/wiki/Brute-force_search">exhaustive search</a>. We evaluate our methods with <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> as a downstream task, using <a href="https://en.wikipedia.org/wiki/Multitask_learning">multitask learning</a>. The results show that the <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> and <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> improve for all <a href="https://en.wikipedia.org/wiki/Methodology">methods</a>.</abstract>
      <url hash="bd071624">2020.crac-1.10</url>
      <bibkey>miculicich-henderson-2020-partially</bibkey>
    </paper>
    <paper id="12">
      <title>Enhanced Labelling in Active Learning for <a href="https://en.wikipedia.org/wiki/Coreference_resolution">Coreference Resolution</a></title>
      <author><first>Vebjørn</first><last>Espeland</last></author>
      <author><first>Beatrice</first><last>Alex</last></author>
      <author><first>Benjamin</first><last>Bach</last></author>
      <pages>111–121</pages>
      <abstract>In this paper we describe our attempt to increase the amount of information that can be retrieved through active learning sessions compared to previous approaches. We optimise the annotator’s labelling process using active learning in the context of <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>. Using simulated active learning experiments, we suggest three adjustments to ensure the labelling time is spent as efficiently as possible. All three adjustments provide more information to the machine learner than the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>, though a large impact on the F1 score over time is not observed. Compared to previous <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>, we report a marginal F1 improvement on the final coreference models trained using for two out of the three approaches tested when applied to the English OntoNotes 2012 Coreference Resolution data. Our best-performing <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves 58.01 <a href="https://en.wikipedia.org/wiki/F-number">F1</a>, an increase of 0.93 F1 over the baseline model.</abstract>
      <url hash="1549d32f">2020.crac-1.12</url>
      <bibkey>espeland-etal-2020-enhanced</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
    </paper>
    <paper id="13">
      <title>Reference in Team Communication for Robot-Assisted Disaster Response : An Initial Analysis</title>
      <author><first>Natalia</first><last>Skachkova</last></author>
      <author><first>Ivana</first><last>Kruijff-Korbayova</last></author>
      <pages>122–132</pages>
      <abstract>We analyze reference phenomena in a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> of robot-assisted disaster response team communication. The annotation scheme we designed for this purpose distinguishes different types of <a href="https://en.wikipedia.org/wiki/Legal_person">entities</a>, <a href="https://en.wikipedia.org/wiki/Role">roles</a>, reference units and <a href="https://en.wikipedia.org/wiki/Binary_relation">relations</a>. We focus particularly on mission-relevant objects, locations and actors and also annotate a rich set of reference links, including <a href="https://en.wikipedia.org/wiki/Co-reference">co-reference</a> and various other kinds of relations. We explain the categories used in our <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a>, present their distribution in the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> and discuss challenging cases.</abstract>
      <url hash="de0424db">2020.crac-1.13</url>
      <bibkey>skachkova-kruijff-korbayova-2020-reference</bibkey>
    </paper>
    <paper id="14">
      <title>Resolving Pronouns in Twitter Streams : Context can Help !<fixed-case>T</fixed-case>witter Streams: Context can Help!</title>
      <author><first>Anietie</first><last>Andy</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>133–138</pages>
      <abstract>Many people live-tweet televised events like <a href="https://en.wikipedia.org/wiki/United_States_presidential_debates">Presidential debates</a> and popular TV-shows and discuss people or characters in the event. Naturally, many tweets make pronominal reference to these people / characters. We propose an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> for resolving <a href="https://en.wikipedia.org/wiki/Personal_pronoun">personal pronouns</a> that make reference to people involved in an event, in tweet streams collected during the event.</abstract>
      <url hash="099e87b5">2020.crac-1.14</url>
      <bibkey>andy-etal-2020-resolving</bibkey>
    </paper>
    <paper id="15">
      <title>Coreference Strategies in English-German Translation<fixed-case>E</fixed-case>nglish-<fixed-case>G</fixed-case>erman Translation</title>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Marie-Pauline</first><last>Krielke</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <pages>139–153</pages>
      <abstract>We present a study focusing on variation of <a href="https://en.wikipedia.org/wiki/Coreference">coreferential devices</a> in English original <a href="https://en.wikipedia.org/wiki/TED_(conference)">TED talks</a> and <a href="https://en.wikipedia.org/wiki/News">news texts</a> and their <a href="https://en.wikipedia.org/wiki/German_language">German translations</a>. Using exploratory techniques we contemplate a diverse set of coreference devices as <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> which we assume indicate language-specific and register-based variation as well as potential translation strategies. Our findings reflect differences on both dimensions with stronger variation along the lines of <a href="https://en.wikipedia.org/wiki/Register_(sociolinguistics)">register</a> than between languages. By exposing interactions between text type and cross-linguistic variation, they can also inform multilingual NLP applications, especially <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>.</abstract>
      <url hash="c9f60242">2020.crac-1.15</url>
      <bibkey>lapshinova-koltunski-etal-2020-coreference</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/parcorfull">ParCorFull</pwcdataset>
    </paper>
    </volume>
</collection>