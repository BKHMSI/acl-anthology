<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.alvr">
  <volume id="1" ingest-date="2021-06-07">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Advances in Language and Vision Research</booktitle>
      <editor><first /><last>Xin</last></editor>
      <editor><first>Ronghang</first><last>Hu</last></editor>
      <editor><first>Drew</first><last>Hudson</last></editor>
      <editor><first>Tsu-Jui</first><last>Fu</last></editor>
      <editor><first>Marcus</first><last>Rohrbach</last></editor>
      <editor><first>Daniel</first><last>Fried</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="95670c66">2021.alvr-1</url>
    </meta>
    <frontmatter>
      <url hash="eef9d62d">2021.alvr-1.0</url>
      <bibkey>alvr-2021-advances</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Leveraging Partial Dependency Trees to Control Image Captions</title>
      <author><first>Wenjie</first><last>Zhong</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>16–21</pages>
      <abstract>Controlling the generation of image captions attracts lots of attention recently. In this paper, we propose a framework leveraging partial syntactic dependency trees as control signals to make image captions include specified words and their syntactic structures. To achieve this purpose, we propose a Syntactic Dependency Structure Aware Model (SDSAM), which explicitly learns to generate the syntactic structures of image captions to include given partial dependency trees. In addition, we come up with a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> to evaluate how many specified words and their syntactic dependencies are included in generated captions. We carry out experiments on two standard <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> : Microsoft COCO and <a href="https://en.wikipedia.org/wiki/Flickr">Flickr30k</a>. Empirical results show that image captions generated by our model are effectively controlled in terms of specified words and their syntactic structures. The code is available on GitHub.</abstract>
      <url hash="c9112d4c">2021.alvr-1.3</url>
      <doi>10.18653/v1/2021.alvr-1.3</doi>
      <bibkey>zhong-miyao-2021-leveraging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="4">
      <title>Grounding Plural Phrases : Countering Evaluation Biases by Individuation</title>
      <author><first>Julia</first><last>Suter</last></author>
      <author><first>Letitia</first><last>Parcalabescu</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>22–28</pages>
      <abstract>Phrase grounding (PG) is a <a href="https://en.wikipedia.org/wiki/Multimodal_interaction">multimodal task</a> that grounds language in images. PG systems are evaluated on well-known <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a>, using Intersection over Union (IoU) as evaluation metric. This work highlights a disconcerting bias in the evaluation of grounded plural phrases, which arises from representing sets of objects as a union box covering all component bounding boxes, in conjunction with the IoU metric. We detect, analyze and quantify an evaluation bias in the grounding of plural phrases and define a novel <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>, c-IoU, based on a union box’s component boxes. We experimentally show that our new <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> greatly alleviates this bias and recommend using it for fairer evaluation of plural phrases in PG tasks.</abstract>
      <url hash="a92f2c3a">2021.alvr-1.4</url>
      <doi>10.18653/v1/2021.alvr-1.4</doi>
      <bibkey>suter-etal-2021-grounding</bibkey>
    </paper>
    <paper id="6">
      <title>Learning to Learn Semantic Factors in Heterogeneous Image Classification</title>
      <author><first>Boyue</first><last>Fan</last></author>
      <author><first>Zhenting</first><last>Liu</last></author>
      <pages>34–38</pages>
      <abstract>Few-shot learning is to recognize novel classes with a few labeled samples per class. Although numerous meta-learning methods have made significant progress, they struggle to directly address the heterogeneity of training and evaluating task distributions, resulting in the domain shift problem when transitioning to new tasks with <a href="https://en.wikipedia.org/wiki/Disjoint_sets">disjoint spaces</a>. In this paper, we propose a novel <a href="https://en.wikipedia.org/wiki/Methodology">method</a> to deal with the <a href="https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity">heterogeneity</a>. Specifically, by simulating class-difference domain shift during the meta-train phase, a bilevel optimization procedure is applied to learn a transferable representation space that can rapidly adapt to heterogeneous tasks. Experiments demonstrate the effectiveness of our proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a>.</abstract>
      <url hash="0f741cf6">2021.alvr-1.6</url>
      <doi>10.18653/v1/2021.alvr-1.6</doi>
      <bibkey>fan-liu-2021-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cub-200-2011">CUB-200-2011</pwcdataset>
    </paper>
    </volume>
</collection>