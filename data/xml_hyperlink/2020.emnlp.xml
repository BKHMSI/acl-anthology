<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.emnlp">
  <volume id="main" ingest-date="2020-11-05">
    <meta>
      <booktitle>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</booktitle>
      <editor><first>Bonnie</first><last>Webber</last></editor>
      <editor><first>Trevor</first><last>Cohn</last></editor>
      <editor><first>Yulan</first><last>He</last></editor>
      <editor id="yang-liu-icsi"><first>Yang</first><last>Liu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
      <url hash="7188a1e0">2020.emnlp-main</url>
    </meta>
    <frontmatter>
      <url hash="2864f359">2020.emnlp-main.0</url>
      <bibkey>emnlp-2020-2020</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Quantitative argument summarization and beyond : Cross-domain key point analysis</title>
      <author><first>Roy</first><last>Bar-Haim</last></author>
      <author><first>Yoav</first><last>Kantor</last></author>
      <author><first>Lilach</first><last>Eden</last></author>
      <author><first>Roni</first><last>Friedman</last></author>
      <author><first>Dan</first><last>Lahav</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>39–49</pages>
      <abstract>When summarizing a collection of views, arguments or opinions on some topic, it is often desirable not only to extract the most salient points, but also to quantify their prevalence. Work on <a href="https://en.wikipedia.org/wiki/Multi-document_summarization">multi-document summarization</a> has traditionally focused on creating textual summaries, which lack this quantitative aspect. Recent work has proposed to summarize arguments by mapping them to a small set of expert-generated key points, where the salience of each key point corresponds to the number of its matching arguments. The current work advances key point analysis in two important respects : first, we develop a method for automatic extraction of key points, which enables fully automatic analysis, and is shown to achieve performance comparable to a human expert. Second, we demonstrate that the applicability of key point analysis goes well beyond <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argumentation data</a>. Using <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on publicly available argumentation datasets, we achieve promising results in two additional domains : <a href="https://en.wikipedia.org/wiki/Survey_methodology">municipal surveys</a> and <a href="https://en.wikipedia.org/wiki/User_review">user reviews</a>. An additional contribution is an in-depth evaluation of argument-to-key point matching models, where we substantially outperform previous results.</abstract>
      <url hash="32afbac6">2020.emnlp-main.3</url>
      <doi>10.18653/v1/2020.emnlp-main.3</doi>
      <video href="https://slideslive.com/38939172" />
      <bibkey>bar-haim-etal-2020-quantitative</bibkey>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="5">
      <title>BLEU might be Guilty but References are not Innocent<fixed-case>BLEU</fixed-case> might be Guilty but References are not Innocent</title>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>David</first><last>Grangier</last></author>
      <author><first>Isaac</first><last>Caswell</last></author>
      <pages>61–71</pages>
      <abstract>The quality of automatic metrics for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to <a href="https://en.wikipedia.org/wiki/German_language">German</a>, but also for <a href="https://en.wikipedia.org/wiki/Back-translation">Back-translation</a> and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence">correlation</a> with all modern evaluation metrics we look at, including embedding-based methods. To complete this picture, we reveal that multi-reference BLEU does not improve the <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence">correlation</a> for high quality output, and present an alternative multi-reference formulation that is more effective.</abstract>
      <url hash="73195d05">2020.emnlp-main.5</url>
      <doi>10.18653/v1/2020.emnlp-main.5</doi>
      <video href="https://slideslive.com/38938647" />
      <bibkey>freitag-etal-2020-bleu</bibkey>
      <pwccode url="https://github.com/google/wmt19-paraphrased-references" additional="true">google/wmt19-paraphrased-references</pwccode>
    </paper>
    <paper id="7">
      <title>Simulated multiple reference training improves low-resource machine translation</title>
      <author><first>Huda</first><last>Khayrallah</last></author>
      <author><first>Brian</first><last>Thompson</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>82–89</pages>
      <abstract>Many valid translations exist for a given sentence, yet <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation (MT)</a> is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space of possible translations by sampling a <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrase</a> of the reference sentence from a <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphraser</a> and training the MT model to predict the <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphraser’s distribution</a> over possible tokens. We demonstrate the effectiveness of <a href="https://en.wikipedia.org/wiki/Speech-language_pathology">SMRT</a> in low-resource settings when translating to <a href="https://en.wikipedia.org/wiki/English_language">English</a>, with improvements of 1.2 to 7.0 BLEU. We also find SMRT is complementary to <a href="https://en.wikipedia.org/wiki/Back-translation">back-translation</a>.</abstract>
      <url hash="36931069">2020.emnlp-main.7</url>
      <doi>10.18653/v1/2020.emnlp-main.7</doi>
      <video href="https://slideslive.com/38938786" />
      <bibkey>khayrallah-etal-2020-simulated</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/parabank">ParaBank</pwcdataset>
    </paper>
    <paper id="8">
      <title>Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing</title>
      <author><first>Brian</first><last>Thompson</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <pages>90–121</pages>
      <abstract>We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the <a href="https://en.wikipedia.org/wiki/Paraphraser">paraphraser</a> as a multilingual NMT system, treating <a href="https://en.wikipedia.org/wiki/Paraphrasing">paraphrasing</a> as a zero-shot translation task (e.g., <a href="https://en.wikipedia.org/wiki/Czech_language">Czech</a> to <a href="https://en.wikipedia.org/wiki/Czech_language">Czech</a>). This results in the <a href="https://en.wikipedia.org/wiki/Paraphraser">paraphraser</a>’s output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is simple and intuitive, and does not require human judgements for training. Our single <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding <a href="https://en.wikipedia.org/wiki/Gujarati_language">Gujarati</a> where the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> had no training data). We also explore using our model for the task of <a href="https://en.wikipedia.org/wiki/Quality_assurance">quality estimation</a> as a metricconditioning on the source instead of the referenceand find that it significantly outperforms every submission to the WMT 2019 shared task on <a href="https://en.wikipedia.org/wiki/Quality_assurance">quality estimation</a> in every language pair.</abstract>
      <url hash="be078170">2020.emnlp-main.8</url>
      <doi>10.18653/v1/2020.emnlp-main.8</doi>
      <video href="https://slideslive.com/38938798" />
      <bibkey>thompson-post-2020-automatic</bibkey>
      <pwccode url="https://github.com/thompsonb/prism" additional="false">thompsonb/prism</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/parabank">ParaBank</pwcdataset>
    </paper>
    <paper id="11">
      <title>Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering</title>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>151–162</pages>
      <abstract>The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a>. We propose <a href="https://en.wikipedia.org/wiki/Heuristic">heuristics</a> to create synthetic graphs for commonsense and scientific knowledge. We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.</abstract>
      <url hash="82068ff1">2020.emnlp-main.11</url>
      <attachment type="OptionalSupplementaryMaterial" hash="7679c305">2020.emnlp-main.11.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.11</doi>
      <video href="https://slideslive.com/38939163" />
      <bibkey>banerjee-baral-2020-self</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openbookqa">OpenBookQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="12">
      <title>More Bang for Your Buck : Natural Perturbation for Robust Question Answering</title>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Tushar</first><last>Khot</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <pages>163–170</pages>
      <abstract>Deep learning models for <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic tasks</a> require <a href="https://en.wikipedia.org/wiki/Big_data">large training datasets</a>, which are expensive to create. As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so by first collecting a set of seed examples and then applying human-driven natural perturbations (as opposed to rule-based machine perturbations), which often change the gold label as well. Such <a href="https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)">perturbations</a> have the advantage of being relatively easier (and hence cheaper) to create than writing out completely new examples. Further, they help address the issue that even <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> achieving human-level scores on NLP datasets are known to be considerably sensitive to small changes in input. To evaluate the idea, we consider a recent question-answering dataset (BOOLQ) and study our approach as a function of the perturbation cost ratio, the relative cost of perturbing an existing question vs. creating a new one from scratch. We find that when natural perturbations are moderately cheaper to create (cost ratio under 60 %), it is more effective to use them for training BOOLQ models : such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> exhibit 9 % higher robustness and 4.5 % stronger <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a>, while retaining performance on the original BOOLQ dataset.</abstract>
      <url hash="7f37ba04">2020.emnlp-main.12</url>
      <doi>10.18653/v1/2020.emnlp-main.12</doi>
      <video href="https://slideslive.com/38939179" />
      <bibkey>khashabi-etal-2020-bang</bibkey>
    </paper>
    <paper id="14">
      <title>Information-Theoretic Probing with Minimum Description Length</title>
      <author><first>Elena</first><last>Voita</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>183–196</pages>
      <abstract>To measure how well pretrained representations encode some linguistic property, it is common to use <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of a probe, i.e. a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> trained to predict the property from the <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representations</a>. Despite widespread adoption of <a href="https://en.wikipedia.org/wiki/Sensor">probes</a>, differences in their <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> fail to adequately reflect differences in <a href="https://en.wikipedia.org/wiki/Mental_representation">representations</a>. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates the amount of effort needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines : variational coding and online coding. We show that these <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> agree in results and are more informative and stable than the standard probes.</abstract>
      <url hash="81f67996">2020.emnlp-main.14</url>
      <doi>10.18653/v1/2020.emnlp-main.14</doi>
      <video href="https://slideslive.com/38938809" />
      <bibkey>voita-titov-2020-information</bibkey>
      <pwccode url="https://github.com/lena-voita/description-length-probing" additional="true">lena-voita/description-length-probing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
    </paper>
    <paper id="17">
      <title>Repulsive Attention : Rethinking Multi-head Attention as <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian Inference</a><fixed-case>B</fixed-case>ayesian Inference</title>
      <author><first>Bang</first><last>An</last></author>
      <author><first>Jie</first><last>Lyu</last></author>
      <author><first>Zhenyi</first><last>Wang</last></author>
      <author><first>Chunyuan</first><last>Li</last></author>
      <author><first>Changwei</first><last>Hu</last></author>
      <author><first>Fei</first><last>Tan</last></author>
      <author><first>Ruiyi</first><last>Zhang</last></author>
      <author><first>Yifan</first><last>Hu</last></author>
      <author><first>Changyou</first><last>Chen</last></author>
      <pages>236–255</pages>
      <abstract>The neural attention mechanism plays an important role in many <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing applications</a>. In particular, multi-head attention extends single-head attention by allowing a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to jointly attend information from different perspectives. However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model’s representation power</a>. In this paper, for the first time, we provide a novel understanding of multi-head attention from a <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian perspective</a>. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens <a href="https://en.wikipedia.org/wiki/Mathematical_model">model’s expressiveness</a>. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions : why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.</abstract>
      <url hash="ef465fb0">2020.emnlp-main.17</url>
      <doi>10.18653/v1/2020.emnlp-main.17</doi>
      <video href="https://slideslive.com/38938801" />
      <bibkey>an-etal-2020-repulsive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/agenda">AGENDA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="18">
      <title>KERMIT : Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations<fixed-case>KERMIT</fixed-case>: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations</title>
      <author><first>Fabio Massimo</first><last>Zanzotto</last></author>
      <author><first>Andrea</first><last>Santilli</last></author>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Dario</first><last>Onorati</last></author>
      <author><first>Pierfrancesco</first><last>Tommasino</last></author>
      <author><first>Francesca</first><last>Fallucchi</last></author>
      <pages>256–267</pages>
      <abstract>Syntactic parsers have dominated <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a> for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> is used in <a href="https://en.wikipedia.org/wiki/Inference">inference</a>. We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks</abstract>
      <url hash="04f9f607">2020.emnlp-main.18</url>
      <doi>10.18653/v1/2020.emnlp-main.18</doi>
      <video href="https://slideslive.com/38938864" />
      <bibkey>zanzotto-etal-2020-kermit</bibkey>
      <pwccode url="https://github.com/ART-Group-it/KERMIT" additional="false">ART-Group-it/KERMIT</pwccode>
    </paper>
    <paper id="26">
      <title>Incremental Processing in the Age of Non-Incremental Encoders : An Empirical Assessment of Bidirectional Models for Incremental NLU<fixed-case>NLU</fixed-case></title>
      <author><first>Brielen</first><last>Madureira</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>357–374</pages>
      <abstract>While humans process language incrementally, the best <a href="https://en.wikipedia.org/wiki/Encoder">language encoders</a> currently used in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The omni-directional BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> like GPT-2.</abstract>
      <url hash="09d22bbc">2020.emnlp-main.26</url>
      <doi>10.18653/v1/2020.emnlp-main.26</doi>
      <video href="https://slideslive.com/38938866" />
      <bibkey>madureira-schlangen-2020-incremental</bibkey>
      <pwccode url="https://github.com/briemadu/inc-bidirectional" additional="false">briemadu/inc-bidirectional</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="28">
      <title>Dialogue Response Ranking Training with Large-Scale Human Feedback Data</title>
      <author><first>Xiang</first><last>Gao</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Michel</first><last>Galley</last></author>
      <author><first>Chris</first><last>Brockett</last></author>
      <author><first>Bill</first><last>Dolan</last></author>
      <pages>386–395</pages>
      <abstract>Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> need to be able to predict and optimize for turns that are genuinely engaging. We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction. To alleviate possible distortion between the <a href="https://en.wikipedia.org/wiki/Feedback">feedback</a> and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors. We trained DialogRPT, a set of GPT-2 based models on 133 M pairs of human feedback data and the resulting ranker outperformed several baselines. Particularly, our <a href="https://en.wikipedia.org/wiki/Ranker">ranker</a> outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback. We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses. Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.</abstract>
      <url hash="070d32a4">2020.emnlp-main.28</url>
      <doi>10.18653/v1/2020.emnlp-main.28</doi>
      <video href="https://slideslive.com/38938970" />
      <bibkey>gao-etal-2020-dialogue</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="31">
      <title>AutoQA : From Databases To QA Semantic Parsers With Only Synthetic Training Data<fixed-case>A</fixed-case>uto<fixed-case>QA</fixed-case>: From Databases To <fixed-case>QA</fixed-case> Semantic Parsers With Only Synthetic Training Data</title>
      <author><first>Silei</first><last>Xu</last></author>
      <author><first>Sina</first><last>Semnani</last></author>
      <author><first>Giovanni</first><last>Campagna</last></author>
      <author><first>Monica</first><last>Lam</last></author>
      <pages>422–434</pages>
      <abstract>We propose AutoQA, a <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a <a href="https://en.wikipedia.org/wiki/Database_schema">database schema</a> and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations. It uses <a href="https://en.wikipedia.org/wiki/Automatic_paraphrasing">automatic paraphrasing</a> combined with template-based parsing to find alternative expressions of an attribute in different <a href="https://en.wikipedia.org/wiki/Part_of_speech">parts of speech</a>. It also uses a novel filtered auto-paraphraser to generate correct <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> of entire sentences. We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9 % when tested on <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural questions</a>, which is only 6.4 % lower than a model trained with expert natural language annotations and paraphrase data collected from <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdworkers</a>. To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset. AutoQA achieves 69.8 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">answer accuracy</a>, 16.4 % higher than the state-of-the-art zero-shot models and only 5.2 % lower than the same <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained with human data.</abstract>
      <url hash="2f7ec539">2020.emnlp-main.31</url>
      <doi>10.18653/v1/2020.emnlp-main.31</doi>
      <video href="https://slideslive.com/38939351" />
      <bibkey>xu-etal-2020-autoqa</bibkey>
      <pwccode url="https://github.com/stanford-oval/genie-toolkit" additional="true">stanford-oval/genie-toolkit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/schema2qa">Stanford Schema2QA Dataset</pwcdataset>
    </paper>
    <paper id="33">
      <title>What Have We Achieved on Text Summarization?</title>
      <author><first>Dandan</first><last>Huang</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Sen</first><last>Yang</last></author>
      <author><first>Guangsheng</first><last>Bao</last></author>
      <author><first>Kun</first><last>Wang</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>446–469</pages>
      <abstract>Deep learning has led to significant improvement in <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a> with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by <a href="https://en.wikipedia.org/wiki/Automatic_summarization">automatic summarizers</a> and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency ; 2) milestone techniques such as copy, coverage and hybrid extractive / abstractive methods do bring specific improvements but also demonstrate limitations ; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.</abstract>
      <url hash="391272d7">2020.emnlp-main.33</url>
      <doi>10.18653/v1/2020.emnlp-main.33</doi>
      <video href="https://slideslive.com/38938815" />
      <bibkey>huang-etal-2020-achieved</bibkey>
      <pwccode url="https://github.com/hddbang/PolyTope" additional="false">hddbang/PolyTope</pwccode>
    </paper>
    <paper id="34">
      <title>Q-learning with <a href="https://en.wikipedia.org/wiki/Language_model">Language Model</a> for Edit-based Unsupervised Summarization<fixed-case>Q</fixed-case>-learning with Language Model for Edit-based Unsupervised Summarization</title>
      <author><first>Ryosuke</first><last>Kohita</last></author>
      <author><first>Akifumi</first><last>Wachi</last></author>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Ryuki</first><last>Tachibana</last></author>
      <pages>470–484</pages>
      <abstract>Unsupervised methods are promising for abstractive textsummarization in that the <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpora</a> is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going. In this paper, we propose a new approach based on <a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a> with an edit-based summarization. The method combines two key <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a> to form an Editorial Agent and Language Model converter (EALM). The <a href="https://en.wikipedia.org/wiki/Agency_(philosophy)">agent</a> predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals. Q-learning is leveraged to train the <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agent</a> to produce proper edit actions. Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set). Defining the task as <a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a> enables us not only to develop a competitive method but also to make the latest techniques in <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> available for unsupervised summarization. We also conduct <a href="https://en.wikipedia.org/wiki/Qualitative_research">qualitative analysis</a>, providing insights into future study on unsupervised summarizers.</abstract>
      <url hash="32ea6a2d">2020.emnlp-main.34</url>
      <doi>10.18653/v1/2020.emnlp-main.34</doi>
      <video href="https://slideslive.com/38938716" />
      <bibkey>kohita-etal-2020-q</bibkey>
      <pwccode url="https://github.com/kohilin/ealm" additional="false">kohilin/ealm</pwccode>
    </paper>
    <paper id="37">
      <title>TernaryBERT : Distillation-aware Ultra-low Bit BERT<fixed-case>T</fixed-case>ernary<fixed-case>BERT</fixed-case>: Distillation-aware Ultra-low Bit <fixed-case>BERT</fixed-case></title>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Lu</first><last>Hou</last></author>
      <author><first>Yichun</first><last>Yin</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Xiao</first><last>Chen</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>509–521</pages>
      <abstract>Transformer-based pre-training models like <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> have achieved remarkable performance in many natural language processing tasks. However, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.</abstract>
      <url hash="b1927905">2020.emnlp-main.37</url>
      <doi>10.18653/v1/2020.emnlp-main.37</doi>
      <video href="https://slideslive.com/38939194" />
      <bibkey>zhang-etal-2020-ternarybert</bibkey>
      <pwccode url="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TernaryBERT" additional="true">huawei-noah/Pretrained-Language-Model</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="38">
      <title>Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks</title>
      <author><first>Trapit</first><last>Bansal</last></author>
      <author><first>Rishikesh</first><last>Jha</last></author>
      <author><first>Tsendsuren</first><last>Munkhdalai</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>522–534</pages>
      <abstract>Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>. However, <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> is still data inefficient   when there are few labeled examples, <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> can be low. Data efficiency can be improved by optimizing pre-training directly for future <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> with few examples ; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize ; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a>. Furthermore, we show how the self-supervised tasks can be combined with <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised tasks</a> for <a href="https://en.wikipedia.org/wiki/Meta-learning">meta-learning</a>, providing substantial accuracy gains over previous supervised meta-learning.</abstract>
      <url hash="7f1b3a1e">2020.emnlp-main.38</url>
      <doi>10.18653/v1/2020.emnlp-main.38</doi>
      <video href="https://slideslive.com/38939198" />
      <bibkey>bansal-etal-2020-self</bibkey>
      <pwccode url="https://github.com/iesl/metanlp" additional="false">iesl/metanlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="39">
      <title>Efficient Meta Lifelong-Learning with Limited Memory</title>
      <author><first>Zirui</first><last>Wang</last></author>
      <author><first>Sanket Vaibhav</first><last>Mehta</last></author>
      <author><first>Barnabas</first><last>Poczos</last></author>
      <author><first>Jaime</first><last>Carbonell</last></author>
      <pages>535–548</pages>
      <abstract>Current <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language processing models</a> work well on a single <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as <a href="https://en.wikipedia.org/wiki/Lifelong_learning">lifelong learning</a>. State-of-the-art lifelong language learning methods store past examples in <a href="https://en.wikipedia.org/wiki/Episodic_memory">episodic memory</a> and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments : (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1 % memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and <a href="https://en.wikipedia.org/wiki/Negative_transfer">negative transfer</a> at the same time.</abstract>
      <url hash="23abf29b">2020.emnlp-main.39</url>
      <doi>10.18653/v1/2020.emnlp-main.39</doi>
      <video href="https://slideslive.com/38939206" />
      <bibkey>wang-etal-2020-efficient</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="40">
      <title>Do n’t Use English Dev : On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings<fixed-case>E</fixed-case>nglish Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings</title>
      <author><first>Phillip</first><last>Keung</last></author>
      <author><first>Yichao</first><last>Lu</last></author>
      <author><first>Julian</first><last>Salazar</last></author>
      <author><first>Vikas</first><last>Bhardwaj</last></author>
      <pages>549–554</pages>
      <abstract>Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for <a href="https://en.wikipedia.org/wiki/Model_selection">model selection</a> in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results : still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.</abstract>
      <url hash="6cbb190d">2020.emnlp-main.40</url>
      <doi>10.18653/v1/2020.emnlp-main.40</doi>
      <video href="https://slideslive.com/38938787" />
      <bibkey>keung-etal-2020-dont</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mldoc">MLDoc</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="41">
      <title>A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <author><first>Katsuki</first><last>Chousa</last></author>
      <author><first>Masaaki</first><last>Nishino</last></author>
      <pages>555–565</pages>
      <abstract>We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. Since this step is equivalent to a SQuAD v2.0 style question answering task, we solve it using the multilingual BERT, which is fine-tuned on manually created gold word alignment data. It is nontrivial to obtain accurate <a href="https://en.wikipedia.org/wiki/Sequence_alignment">alignment</a> from a set of independently predicted spans. We greatly improved the word alignment accuracy by adding to the question the source token’s context and symmetrizing two directional predictions. In experiments using five word alignment datasets from among <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a>, <a href="https://en.wikipedia.org/wiki/Romanian_language">Romanian</a>, <a href="https://en.wikipedia.org/wiki/French_language">French</a>, and <a href="https://en.wikipedia.org/wiki/English_language">English</a>, we show that our proposed method significantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 <a href="https://en.wikipedia.org/wiki/F-number">F1 score</a> for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised method</a>.</abstract>
      <url hash="65d07090">2020.emnlp-main.41</url>
      <doi>10.18653/v1/2020.emnlp-main.41</doi>
      <video href="https://slideslive.com/38938923" />
      <bibkey>nagata-etal-2020-supervised</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="44">
      <title>Unsupervised Discovery of Implicit Gender Bias</title>
      <author><first>Anjalie</first><last>Field</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>596–608</pages>
      <abstract>Despite their prevalence in society, social biases are difficult to identify, primarily because <a href="https://en.wikipedia.org/wiki/Judgement">human judgements</a> in this domain can be unreliable. We take an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised approach</a> to identifying gender bias against women at a comment level and present a model that can surface text likely to contain <a href="https://en.wikipedia.org/wiki/Bias">bias</a>. Our main challenge is forcing the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to focus on signs of <a href="https://en.wikipedia.org/wiki/Implicit_stereotype">implicit bias</a>, rather than other artifacts in the data. Thus, our <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> involves reducing the influence of <a href="https://en.wikipedia.org/wiki/Confounding">confounds</a> through propensity matching and <a href="https://en.wikipedia.org/wiki/Adversarial_learning">adversarial learning</a>. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on <a href="https://en.wikipedia.org/wiki/Human_physical_appearance">appearance</a> and <a href="https://en.wikipedia.org/wiki/Sexualization">sexualization</a>. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.</abstract>
      <url hash="c723d291">2020.emnlp-main.44</url>
      <doi>10.18653/v1/2020.emnlp-main.44</doi>
      <video href="https://slideslive.com/38938782" />
      <bibkey>field-tsvetkov-2020-unsupervised</bibkey>
      <pwccode url="https://github.com/anjalief/unsupervised_gender_bias" additional="false">anjalief/unsupervised_gender_bias</pwccode>
    </paper>
    <paper id="45">
      <title>Condolence and Empathy in <a href="https://en.wikipedia.org/wiki/Online_community">Online Communities</a></title>
      <author><first>Naitian</first><last>Zhou</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>609–626</pages>
      <abstract>Offering condolence is a natural reaction to hearing someone’s distress. Individuals frequently express distress in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, where some communities can provide support. However, not all <a href="https://en.wikipedia.org/wiki/Condolence">condolence</a> is equaltrite responses offer little actual support despite their good intentions. Here, we develop <a href="https://en.wikipedia.org/wiki/Computer">computational tools</a> to create a massive <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 11.4 M expressions of distress and 2.8 M corresponding offerings of <a href="https://en.wikipedia.org/wiki/Condolences">condolence</a> in order to examine the dynamics of <a href="https://en.wikipedia.org/wiki/Condolences">condolence</a> online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from <a href="https://en.wikipedia.org/wiki/Social_psychology">social psychology</a>, we analyze the language of condolence and develop a new dataset for quantifying the <a href="https://en.wikipedia.org/wiki/Empathy">empathy</a> in a <a href="https://en.wikipedia.org/wiki/Condolences">condolence</a> using appraisal theory. Finally, we demonstrate that the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> of condolence individuals find most helpful online differ substantially in their <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> from those seen in <a href="https://en.wikipedia.org/wiki/Interpersonal_relationship">interpersonal settings</a>.</abstract>
      <url hash="d3e4ff05">2020.emnlp-main.45</url>
      <doi>10.18653/v1/2020.emnlp-main.45</doi>
      <video href="https://slideslive.com/38939143" />
      <bibkey>zhou-jurgens-2020-condolence</bibkey>
    </paper>
    <paper id="49">
      <title>Event Extraction by Answering (Almost) Natural Questions</title>
      <author><first>Xinya</first><last>Du</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <pages>671–683</pages>
      <abstract>The problem of <a href="https://en.wikipedia.org/wiki/Event_extraction">event extraction</a> requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity recognition</a> as a preprocessing / concurrent step, causing the well-known problem of <a href="https://en.wikipedia.org/wiki/Error_propagation">error propagation</a>. To avoid this issue, we introduce a new <a href="https://en.wikipedia.org/wiki/Paradigm">paradigm</a> for <a href="https://en.wikipedia.org/wiki/Event_extraction">event extraction</a> by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms <a href="https://en.wikipedia.org/wiki/Prior_probability">prior methods</a> substantially ; in addition, it is capable of extracting <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">event arguments</a> for roles not seen at training time (i.e., in a zero-shot learning setting).</abstract>
      <url hash="a74a1776">2020.emnlp-main.49</url>
      <doi>10.18653/v1/2020.emnlp-main.49</doi>
      <video href="https://slideslive.com/38938649" />
      <bibkey>du-cardie-2020-event</bibkey>
      <pwccode url="https://github.com/xinyadu/eeqa" additional="false">xinyadu/eeqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="50">
      <title>Connecting the Dots : Event Graph Schema Induction with Path Language Modeling</title>
      <author><first>Manling</first><last>Li</last></author>
      <author><first>Qi</first><last>Zeng</last></author>
      <author><first>Ying</first><last>Lin</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <author><first>Nathanael</first><last>Chambers</last></author>
      <author><first>Clare</first><last>Voss</last></author>
      <pages>684–695</pages>
      <abstract>Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas. We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph. Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas. Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.</abstract>
      <url hash="29d1aba4">2020.emnlp-main.50</url>
      <doi>10.18653/v1/2020.emnlp-main.50</doi>
      <video href="https://slideslive.com/38938669" />
      <bibkey>li-etal-2020-connecting</bibkey>
    </paper>
    <paper id="51">
      <title>Joint Constrained Learning for Event-Event Relation Extraction</title>
      <author><first>Haoyu</first><last>Wang</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>696–706</pages>
      <abstract>Understanding <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> involves recognizing how multiple event mentions structurally and temporally interact with each other. In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them. Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations. Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations of events by converting these <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraints</a> into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process. We also present a promising case study to show the effectiveness of our approach to inducing event complexes on an external corpus.</abstract>
      <url hash="be31566e">2020.emnlp-main.51</url>
      <doi>10.18653/v1/2020.emnlp-main.51</doi>
      <video href="https://slideslive.com/38938847" />
      <bibkey>wang-etal-2020-joint</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="53">
      <title>Semi-supervised New Event Type Induction and Event Detection</title>
      <author><first>Lifu</first><last>Huang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>718–724</pages>
      <abstract>Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive. In this paper, we work on a new task of semi-supervised event type induction, aiming to automatically discover a set of unseen types from a given corpus by leveraging annotations available for a few seen types. We design a Semi-Supervised Vector Quantized Variational Autoencoder framework to automatically learn a discrete latent type representation for each seen and unseen type and optimize them using seen type event annotations. A variational autoencoder is further introduced to enforce the reconstruction of each event mention conditioned on its latent type distribution. Experiments show that our approach can not only achieve state-of-the-art performance on supervised event detection but also discover high-quality new event types.</abstract>
      <url hash="14d0f319">2020.emnlp-main.53</url>
      <doi>10.18653/v1/2020.emnlp-main.53</doi>
      <video href="https://slideslive.com/38939118" />
      <bibkey>huang-ji-2020-semi</bibkey>
    </paper>
    <paper id="60">
      <title>Learning to Represent Image and Text with Denotation Graph</title>
      <author><first>Bowen</first><last>Zhang</last></author>
      <author><first>Hexiang</first><last>Hu</last></author>
      <author><first>Vihan</first><last>Jain</last></author>
      <author><first>Eugene</first><last>Ie</last></author>
      <author><first>Fei</first><last>Sha</last></author>
      <pages>823–839</pages>
      <abstract>Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing <a href="https://en.wikipedia.org/wiki/Digital_image">images</a> aligned with linguistic expressions that describe the <a href="https://en.wikipedia.org/wiki/Digital_image">images</a>. In this paper, we propose learning representations from a set of implied, visually grounded expressions between <a href="https://en.wikipedia.org/wiki/Image">image</a> and text, automatically mined from those <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>. In particular, we use denotation graphs to represent how specific <a href="https://en.wikipedia.org/wiki/Concept">concepts</a> (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. This type of generic-to-specific relations can be discovered using <a href="https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)">linguistic analysis tools</a>. We propose <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> to incorporate such <a href="https://en.wikipedia.org/wiki/Binary_relation">relations</a> into <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">learning representation</a>. We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition. Both our codes and the extracted denotation graphs on the <a href="https://en.wikipedia.org/wiki/Flickr">Flickr30 K</a> and the COCO datasets are publically available on https://sha-lab.github.io/DG.</abstract>
      <url hash="c6e08b36">2020.emnlp-main.60</url>
      <doi>10.18653/v1/2020.emnlp-main.60</doi>
      <video href="https://slideslive.com/38938839" />
      <bibkey>zhang-etal-2020-learning-represent</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="62">
      <title>Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think !</title>
      <author><first>Jack</first><last>Hessel</last></author>
      <author><first>Lillian</first><last>Lee</last></author>
      <pages>861–877</pages>
      <abstract>Modeling expressive cross-modal interactions seems crucial in <a href="https://en.wikipedia.org/wiki/Multimodal_interaction">multimodal tasks</a>, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">function projection</a> modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models ; thus, performance improvements, even when present, often can not be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.</abstract>
      <url hash="ec462781">2020.emnlp-main.62</url>
      <attachment type="OptionalSupplementaryMaterial" hash="6876bced">2020.emnlp-main.62.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.62</doi>
      <video href="https://slideslive.com/38938964" />
      <bibkey>hessel-lee-2020-multimodal</bibkey>
    </paper>
    <paper id="63">
      <title>MUTANT : A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering<fixed-case>MUTANT</fixed-case>: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering</title>
      <author><first>Tejas</first><last>Gokhale</last></author>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <author><first>Yezhou</first><last>Yang</last></author>
      <pages>878–892</pages>
      <abstract>While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and <a href="https://en.wikipedia.org/wiki/Prior_probability">priors</a> in <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a>. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-of-the-art <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on VQA-CP with a 10.57 % improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.<i>MUTANT</i>, a training paradigm that exposes the model to perceptually similar, yet semantically distinct <i>mutations</i> of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, <i>MUTANT</i> does not rely on the knowledge about the nature of train and test answer distributions. <i>MUTANT</i> establishes a new state-of-the-art accuracy on VQA-CP with a 10.57% improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.</abstract>
      <url hash="d6912735">2020.emnlp-main.63</url>
      <doi>10.18653/v1/2020.emnlp-main.63</doi>
      <video href="https://slideslive.com/38939282" />
      <bibkey>gokhale-etal-2020-mutant</bibkey>
      <pwccode url="https://github.com/tejasG53/vqa_mutant" additional="true">tejasG53/vqa_mutant</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="66">
      <title>TOD-BERT : Pre-trained Natural Language Understanding for Task-Oriented Dialogue<fixed-case>TOD</fixed-case>-<fixed-case>BERT</fixed-case>: Pre-trained Natural Language Understanding for Task-Oriented Dialogue</title>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Steven C.H.</first><last>Hoi</last></author>
      <author><first>Richard</first><last>Socher</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>917–929</pages>
      <abstract>The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a>. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.</abstract>
      <url hash="7147fe0a">2020.emnlp-main.66</url>
      <doi>10.18653/v1/2020.emnlp-main.66</doi>
      <video href="https://slideslive.com/38938861" />
      <bibkey>wu-etal-2020-tod</bibkey>
      <pwccode url="https://github.com/jasonwu0731/ToD-BERT" additional="false">jasonwu0731/ToD-BERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-oz">Wizard-of-Oz</pwcdataset>
    </paper>
    <paper id="69">
      <title>Latent Geographical Factors for Analyzing the Evolution of Dialects in Contact</title>
      <author><first>Yugo</first><last>Murawaki</last></author>
      <pages>959–976</pages>
      <abstract>Analyzing the evolution of dialects remains a challenging problem because <a href="https://en.wikipedia.org/wiki/Language_contact">contact phenomena</a> hinder the application of the standard <a href="https://en.wikipedia.org/wiki/Tree_model">tree model</a>. Previous statistical approaches to this problem resort to <a href="https://en.wikipedia.org/wiki/Genetic_admixture">admixture analysis</a>, where each dialect is seen as a mixture of latent ancestral populations. However, such ancestral populations are hardly interpretable in the context of the <a href="https://en.wikipedia.org/wiki/Tree_model">tree model</a>. In this paper, we propose a probabilistic generative model that represents latent factors as geographical distributions. We argue that the proposed model has higher affinity with the <a href="https://en.wikipedia.org/wiki/Tree_(graph_theory)">tree model</a> because a <a href="https://en.wikipedia.org/wiki/Tree_(graph_theory)">tree</a> can alternatively be represented as a set of geographical distributions. Experiments involving synthetic and real data suggest that the proposed method is both quantitatively and qualitatively superior to the admixture model.</abstract>
      <url hash="d479758d">2020.emnlp-main.69</url>
      <doi>10.18653/v1/2020.emnlp-main.69</doi>
      <video href="https://slideslive.com/38938993" />
      <bibkey>murawaki-2020-latent</bibkey>
    </paper>
    <paper id="75">
      <title>Multi-task Learning for Multilingual Neural Machine Translation</title>
      <author><first>Yiren</first><last>Wang</last></author>
      <author><first>ChengXiang</first><last>Zhai</last></author>
      <author><first>Hany</first><last>Hassan</last></author>
      <pages>1022–1034</pages>
      <abstract>While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks ; the proposed approach outperforms massive scale models trained on single task.</abstract>
      <url hash="c6fd6694">2020.emnlp-main.75</url>
      <doi>10.18653/v1/2020.emnlp-main.75</doi>
      <video href="https://slideslive.com/38938966" />
      <bibkey>wang-etal-2020-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xglue">XGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="86">
      <title>IIRC : A Dataset of Incomplete Information Reading Comprehension Questions<fixed-case>IIRC</fixed-case>: A Dataset of Incomplete Information Reading Comprehension Questions</title>
      <author><first>James</first><last>Ferguson</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Tushar</first><last>Khot</last></author>
      <author><first>Pradeep</first><last>Dasigi</last></author>
      <pages>1137–1147</pages>
      <abstract>Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not evaluating a <a href="https://en.wikipedia.org/wiki/System">system</a>’s performance at identifying a potential lack of sufficient information and locating sources for that information. To fill this gap, we present a dataset, IIRC, with more than 13 K questions over paragraphs from <a href="https://en.wikipedia.org/wiki/English_Wikipedia">English Wikipedia</a> that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. This process also gave many questions without answers, and those that require discrete reasoning, increasing the difficulty of the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. We follow recent modeling work on various reading comprehension datasets to construct a <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline model</a> for this dataset, finding that <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> achieves 31.1 % <a href="https://en.wikipedia.org/wiki/F-number">F1</a> on this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, while estimated <a href="https://en.wikipedia.org/wiki/Human_factors_and_ergonomics">human performance</a> is 88.4 %. The dataset, code for the baseline system, and a leaderboard can be found at https://allennlp.org/iirc.</abstract>
      <url hash="c9e426b2">2020.emnlp-main.86</url>
      <doi>10.18653/v1/2020.emnlp-main.86</doi>
      <video href="https://slideslive.com/38939237" />
      <bibkey>ferguson-etal-2020-iirc</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/iirc">IIRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
    </paper>
    <paper id="89">
      <title>ToTTo : A Controlled Table-To-Text Generation Dataset<fixed-case>ToTTo</fixed-case>: A Controlled Table-To-Text Generation Dataset</title>
      <author><first>Ankur</first><last>Parikh</last></author>
      <author><first>Xuezhi</first><last>Wang</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Manaal</first><last>Faruqui</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <author><first>Dipanjan</first><last>Das</last></author>
      <pages>1173–1186</pages>
      <abstract>We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task : given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>. We present systematic analyses of our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.</abstract>
      <url hash="c75753a8">2020.emnlp-main.89</url>
      <attachment type="OptionalSupplementaryMaterial" hash="88a69b87">2020.emnlp-main.89.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.89</doi>
      <video href="https://slideslive.com/38938835" />
      <bibkey>parikh-etal-2020-totto</bibkey>
      <pwccode url="https://github.com/google-research-datasets/ToTTo" additional="false">google-research-datasets/ToTTo</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/totto">ToTTo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rotowire">RotoWire</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
    </paper>
    <paper id="90">
      <title>ENT-DESC : Entity Description Generation by Exploring Knowledge Graph<fixed-case>ENT</fixed-case>-<fixed-case>DESC</fixed-case>: Entity Description Generation by Exploring Knowledge Graph</title>
      <author><first>Liying</first><last>Cheng</last></author>
      <author><first>Dekun</first><last>Wu</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Yan</first><last>Zhang</last></author>
      <author><first>Zhanming</first><last>Jie</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>1187–1197</pages>
      <abstract>Previous works on knowledge-to-text generation take as input a few <a href="https://en.wikipedia.org/wiki/Resource_Description_Framework">RDF triples</a> or key-value pairs conveying the knowledge of some entities to generate a <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language description</a>. Existing datasets, such as WIKIBIO, WebNLG, and <a href="https://en.wikipedia.org/wiki/E2E">E2E</a>, basically have a good alignment between an input triple / pair set and its output text. However, in practice, the input knowledge could be more than enough, since the output description may only cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such a practical scenario in KG-to-text. Our dataset involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of <a href="https://en.wikipedia.org/wiki/Information_loss">information loss</a> and parameter explosion while generating the descriptions. We address these challenges by proposing a multi-graph structure that is able to represent the original <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph information</a> more comprehensively. Furthermore, we also incorporate aggregation methods that learn to extract the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture.</abstract>
      <url hash="26b4b47f">2020.emnlp-main.90</url>
      <doi>10.18653/v1/2020.emnlp-main.90</doi>
      <video href="https://slideslive.com/38938972" />
      <bibkey>cheng-etal-2020-ent</bibkey>
      <pwccode url="https://github.com/LiyingCheng95/EntityDescriptionGeneration" additional="false">LiyingCheng95/EntityDescriptionGeneration</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ent-desc">ENT-DESC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/agenda">AGENDA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
    </paper>
    <paper id="92">
      <title>Online Back-Parsing for AMR-to-Text Generation<fixed-case>AMR</fixed-case>-to-Text Generation</title>
      <author><first>Xuefeng</first><last>Bai</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>1206–1219</pages>
      <abstract>AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs. We propose a <a href="https://en.wikipedia.org/wiki/Code">decoder</a> that back predicts projected AMR graphs on the target sentence during text generation. As the result, our outputs can better preserve the input meaning than standard <a href="https://en.wikipedia.org/wiki/Code">decoders</a>. Experiments on two AMR benchmarks show the superiority of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> over the previous state-of-the-art <a href="https://en.wikipedia.org/wiki/System">system</a> based on graph Transformer.</abstract>
      <url hash="4409a5e4">2020.emnlp-main.92</url>
      <doi>10.18653/v1/2020.emnlp-main.92</doi>
      <video href="https://slideslive.com/38939115" />
      <bibkey>bai-etal-2020-online</bibkey>
      <pwccode url="https://github.com/muyeby/AMR-Backparsing" additional="false">muyeby/AMR-Backparsing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2017t10">LDC2017T10</pwcdataset>
    </paper>
    <paper id="93">
      <title>Reading Between the Lines : Exploring Infilling in Visual Narratives</title>
      <author><first>Khyathi Raghavi</first><last>Chandu</last></author>
      <author><first>Ruo-Ping</first><last>Dong</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>1220–1229</pages>
      <abstract>Generating long form narratives such as <a href="https://en.wikipedia.org/wiki/Narrative">stories</a> and procedures from multiple modalities has been a long standing dream for <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial intelligence</a>. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts. In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images. We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies. Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts. We conclusively show a METEOR score of 27.51 on <a href="https://en.wikipedia.org/wiki/Procedure_code">procedures</a> which is higher than the state-of-the-art on <a href="https://en.wikipedia.org/wiki/Visual_storytelling">visual storytelling</a>. We also demonstrate the effects of interposing new text with missing images during <a href="https://en.wikipedia.org/wiki/Inference">inference</a>. The code and the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> will be publicly available at https://visual-narratives.github.io/Visual-Narratives/.</abstract>
      <url hash="0f0dadd1">2020.emnlp-main.93</url>
      <doi>10.18653/v1/2020.emnlp-main.93</doi>
      <video href="https://slideslive.com/38939186" />
      <bibkey>chandu-etal-2020-reading</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/vist">VIST</pwcdataset>
    </paper>
    <paper id="94">
      <title>Acrostic Poem Generation</title>
      <author><first>Rajat</first><last>Agarwal</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>1230–1240</pages>
      <abstract>We propose a new task in the area of <a href="https://en.wikipedia.org/wiki/Computational_creativity">computational creativity</a> : acrostic poem generation in <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Acrostic poems are <a href="https://en.wikipedia.org/wiki/Poetry">poems</a> that contain a <a href="https://en.wikipedia.org/wiki/Hidden_message">hidden message</a> ; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints : given an input word, 1) the initial letters of each line should spell out the provided word, 2) the <a href="https://en.wikipedia.org/wiki/Poetry">poem’s semantics</a> should also relate to it, and 3) the <a href="https://en.wikipedia.org/wiki/Poetry">poem</a> should conform to a <a href="https://en.wikipedia.org/wiki/Rhyme_scheme">rhyming scheme</a>. We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model. Since no dedicated datasets for acrostic poem generation exist, we create <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data</a> for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional <a href="https://en.wikipedia.org/wiki/Poetry">poems</a>. Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints. Last, we confirm that <a href="https://en.wikipedia.org/wiki/Poetry">poems</a> generated by our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> are indeed closely related to the provided prompts, and that pretraining on <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> can boost performance.</abstract>
      <url hash="9644e4b6">2020.emnlp-main.94</url>
      <doi>10.18653/v1/2020.emnlp-main.94</doi>
      <video href="https://slideslive.com/38938805" />
      <bibkey>agarwal-kann-2020-acrostic</bibkey>
    </paper>
    <paper id="95">
      <title>Local Additivity Based Data Augmentation for Semi-supervised NER<fixed-case>NER</fixed-case></title>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Zhenghui</first><last>Wang</last></author>
      <author><first>Ran</first><last>Tian</last></author>
      <author><first>Zichao</first><last>Yang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>1241–1251</pages>
      <abstract>Named Entity Recognition (NER) is one of the first stages in <a href="https://en.wikipedia.org/wiki/Deep_learning">deep language understanding</a> yet current NER models heavily rely on human-annotated data. In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised NER, in which we create virtual samples by interpolating sequences close to each other. Our approach has two variations : Intra-LADA and Inter-LADA, where Intra-LADA performs interpolations among tokens within one sentence, and Inter-LADA samples different sentences to interpolate. Through linear additions between <a href="https://en.wikipedia.org/wiki/Sampling_(signal_processing)">sampled training data</a>, LADA creates an infinite amount of <a href="https://en.wikipedia.org/wiki/Labeled_data">labeled data</a> and improves both entity and context learning. We further extend LADA to the semi-supervised setting by designing a novel consistency loss for unlabeled data. Experiments conducted on two NER benchmarks demonstrate the effectiveness of our methods over several strong <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>. We have publicly released our code at https://github.com/GT-SALT/LADA</abstract>
      <url hash="56cd51f9">2020.emnlp-main.95</url>
      <doi>10.18653/v1/2020.emnlp-main.95</doi>
      <video href="https://slideslive.com/38938834" />
      <bibkey>chen-etal-2020-local</bibkey>
      <pwccode url="https://github.com/GT-SALT/LADA" additional="false">GT-SALT/LADA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="96">
      <title>Grounded Compositional Outputs for Adaptive Language Modeling</title>
      <author><first>Nikolaos</first><last>Pappas</last></author>
      <author><first>Phoebe</first><last>Mulcaire</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>1252–1267</pages>
      <abstract>Language models have emerged as a central component across <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, and a great deal of progress depends on the ability to cheaply adapt <a href="https://en.wikipedia.org/wiki/Natural_language_processing">them</a> (e.g., through finetuning) to new domains and tasks. A <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>’s vocabularytypically selected before training and permanently fixed lateraffects its size and is part of what makes it resistant to such <a href="https://en.wikipedia.org/wiki/Adaptation">adaptation</a>. Prior work has used compositional input embeddings based on <a href="https://en.wikipedia.org/wiki/Surface_(mathematics)">surface forms</a> to ameliorate this issue. In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions. To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary. We evaluate the model on conventional <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a> as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency : our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is more accurate for <a href="https://en.wikipedia.org/wiki/Word_frequency">low-frequency words</a>.<i>vocabulary</i>—typically selected before training and permanently fixed later—affects its size and is part of what makes it resistant to such adaptation. Prior work has used compositional input embeddings based on surface forms to ameliorate this issue. In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions. To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary. We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words.</abstract>
      <url hash="e277894f">2020.emnlp-main.96</url>
      <doi>10.18653/v1/2020.emnlp-main.96</doi>
      <video href="https://slideslive.com/38938850" />
      <bibkey>pappas-etal-2020-grounded</bibkey>
      <pwccode url="https://github.com/Noahs-ARK/groc" additional="false">Noahs-ARK/groc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="98">
      <title>SetConv : A New Approach for Learning from Imbalanced Data<fixed-case>S</fixed-case>et<fixed-case>C</fixed-case>onv: <fixed-case>A</fixed-case> <fixed-case>N</fixed-case>ew <fixed-case>A</fixed-case>pproach for <fixed-case>L</fixed-case>earning from <fixed-case>I</fixed-case>mbalanced <fixed-case>D</fixed-case>ata</title>
      <author><first>Yang</first><last>Gao</last></author>
      <author><first>Yi-Fan</first><last>Li</last></author>
      <author><first>Yu</first><last>Lin</last></author>
      <author><first>Charu</first><last>Aggarwal</last></author>
      <author><first>Latifur</first><last>Khan</last></author>
      <pages>1284–1294</pages>
      <abstract>For many real-world classification problems, e.g., sentiment classification, most existing <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning methods</a> are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> can later be trained on a balanced class distribution. We prove that our proposed <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.</abstract>
      <url hash="6314322e">2020.emnlp-main.98</url>
      <doi>10.18653/v1/2020.emnlp-main.98</doi>
      <bibkey>gao-etal-2020-setconv</bibkey>
    </paper>
    <paper id="100">
      <title>Improving Bilingual Lexicon Induction for Low Frequency Words</title>
      <author><first>Jiaji</first><last>Huang</last></author>
      <author><first>Xingyu</first><last>Cai</last></author>
      <author><first>Kenneth</first><last>Church</last></author>
      <pages>1310–1314</pages>
      <abstract>This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words. First, a diminishing margin between similarities in <a href="https://en.wikipedia.org/wiki/Low_frequency">low frequency regime</a>, and secondly, exacerbated hubness at <a href="https://en.wikipedia.org/wiki/Low_frequency">low frequency</a>. Based on the observation, we further propose two <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> to address these two factors, respectively. The larger issue is hubness. Addressing that improves <a href="https://en.wikipedia.org/wiki/Inductive_reasoning">induction accuracy</a> significantly, especially for low-frequency words.</abstract>
      <url hash="7b31022a">2020.emnlp-main.100</url>
      <doi>10.18653/v1/2020.emnlp-main.100</doi>
      <video href="https://slideslive.com/38939203" />
      <bibkey>huang-etal-2020-improving</bibkey>
    </paper>
    <paper id="101">
      <title>Learning VAE-LDA Models with Rounded Reparameterization Trick<fixed-case>VAE</fixed-case>-<fixed-case>LDA</fixed-case> Models with Rounded Reparameterization Trick</title>
      <author><first>Runzhi</first><last>Tian</last></author>
      <author><first>Yongyi</first><last>Mao</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <pages>1315–1325</pages>
      <abstract>The introduction of VAE provides an efficient <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> for the learning of generative models, including generative topic models. However, when the <a href="https://en.wikipedia.org/wiki/Topic_model">topic model</a> is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable. This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick. In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distributions</a> for the learning of VAE-LDA models. This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.</abstract>
      <url hash="150cd7f6">2020.emnlp-main.101</url>
      <doi>10.18653/v1/2020.emnlp-main.101</doi>
      <video href="https://slideslive.com/38939213" />
      <bibkey>tian-etal-2020-learning</bibkey>
    </paper>
    <paper id="103">
      <title>Scaling Hidden Markov Language Models<fixed-case>M</fixed-case>arkov Language Models</title>
      <author><first>Justin</first><last>Chiu</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>1341–1349</pages>
      <abstract>The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the <a href="https://en.wikipedia.org/wiki/Emission_spectrum">emission structure</a>. However, this separation makes it difficult to fit HMMs to large datasets in modern <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, and they have fallen out of use due to very poor performance compared to fully observed models. This work revisits the challenge of scaling HMMs to language modeling datasets, taking ideas from recent approaches to neural modeling. We propose methods for scaling HMMs to massive state spaces while maintaining efficient <a href="https://en.wikipedia.org/wiki/Exact_inference">exact inference</a>, a compact parameterization, and effective <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization</a>. Experiments show that this approach leads to <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that are much more accurate than previous HMMs and n-gram-based methods, making progress towards the performance of state-of-the-art NN models.</abstract>
      <url hash="66a4f5ab">2020.emnlp-main.103</url>
      <doi>10.18653/v1/2020.emnlp-main.103</doi>
      <video href="https://slideslive.com/38940160" />
      <bibkey>chiu-rush-2020-scaling</bibkey>
      <pwccode url="https://github.com/harvardnlp/hmm-lm" additional="false">harvardnlp/hmm-lm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="104">
      <title>Coding Textual Inputs Boosts the Accuracy of <a href="https://en.wikipedia.org/wiki/Neural_network">Neural Networks</a></title>
      <author><first>Abdul Rafae</first><last>Khan</last></author>
      <author><first>Jia</first><last>Xu</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <pages>1350–1360</pages>
      <abstract>Natural Language Processing (NLP) tasks are usually performed word by word on <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">textual inputs</a>. We can use arbitrary <a href="https://en.wikipedia.org/wiki/Symbol_(formal)">symbols</a> to represent the linguistic meaning of a word and use these <a href="https://en.wikipedia.org/wiki/Symbol_(formal)">symbols</a> as inputs. As alternatives to a text representation, we introduce <a href="https://en.wikipedia.org/wiki/Soundex">Soundex</a>, MetaPhone, <a href="https://en.wikipedia.org/wiki/NYSIIS">NYSIIS</a>, <a href="https://en.wikipedia.org/wiki/Logogram">logogram</a> to <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, and develop fixed-output-length coding and its extension using <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman coding</a>. Each of those <a href="https://en.wikipedia.org/wiki/Code_name">codings</a> combines different character / digital sequences and constructs a new <a href="https://en.wikipedia.org/wiki/Vocabulary">vocabulary</a> based on <a href="https://en.wikipedia.org/wiki/Code_name">codewords</a>. We find that the integration of those codewords with text provides more reliable inputs to Neural-Network-based NLP systems through redundancy than text-alone inputs. Experiments demonstrate that our approach outperforms the state-of-the-art models on the application of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a>, and <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a>. The source code is available at https://github.com/abdulrafae/coding_nmt.</abstract>
      <url hash="53b5562f">2020.emnlp-main.104</url>
      <doi>10.18653/v1/2020.emnlp-main.104</doi>
      <video href="https://slideslive.com/38939325" />
      <bibkey>khan-etal-2020-coding</bibkey>
      <pwccode url="https://github.com/abdulrafae/coding_nmt" additional="false">abdulrafae/coding_nmt</pwccode>
    </paper>
    <paper id="107">
      <title>Named Entity Recognition for Social Media Texts with Semantic Augmentation</title>
      <author><first>Yuyang</first><last>Nie</last></author>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Xiang</first><last>Wan</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Bo</first><last>Dai</last></author>
      <pages>1383–1391</pages>
      <abstract>Existing approaches for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> suffer from data sparsity problems when conducted on short and informal texts, especially <a href="https://en.wikipedia.org/wiki/User-generated_content">user-generated social media content</a>. Semantic augmentation is a potential way to alleviate this problem. Given that rich semantic information is implicitly preserved in pre-trained word embeddings, they are potential ideal resources for semantic augmentation. In this paper, we propose a neural-based approach to NER for social media texts where both local (from running text) and augmented semantics are taken into account. In particular, we obtain the augmented semantic information from a large-scale corpus, and propose an attentive semantic augmentation module and a gate module to encode and aggregate such <a href="https://en.wikipedia.org/wiki/Information">information</a>, respectively. Extensive experiments are performed on three benchmark datasets collected from English and Chinese social media platforms, where the results demonstrate the superiority of our approach to previous studies across all three datasets.</abstract>
      <url hash="fc108d62">2020.emnlp-main.107</url>
      <doi>10.18653/v1/2020.emnlp-main.107</doi>
      <video href="https://slideslive.com/38939305" />
      <bibkey>nie-etal-2020-named</bibkey>
      <pwccode url="https://github.com/cuhksz-nlp/SANER" additional="false">cuhksz-nlp/SANER</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2016-ner">WNUT 2016 NER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/weibo-ner">Weibo NER</pwcdataset>
    </paper>
    <paper id="108">
      <title>Coupled Hierarchical Transformer for Stance-Aware Rumor Verification in Social Media Conversations</title>
      <author><first>Jianfei</first><last>Yu</last></author>
      <author><first>Jing</first><last>Jiang</last></author>
      <author><first>Ling Min Serena</first><last>Khoo</last></author>
      <author><first>Hai Leong</first><last>Chieu</last></author>
      <author><first>Rui</first><last>Xia</last></author>
      <pages>1392–1401</pages>
      <abstract>The prevalent use of <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV). A number of previous studies focus on leveraging stance classification to enhance RV with multi-task learning (MTL) methods. However, most of these methods failed to employ pre-trained contextualized embeddings such as BERT, and did not exploit inter-task dependencies by using predicted stance labels to improve the RV task. Therefore, in this paper, to extend BERT to obtain thread representations, we first propose a Hierarchical Transformer, which divides each long thread into shorter subthreads, and employs BERT to separately represent each subthread, followed by a global Transformer layer to encode all the subthreads. We further propose a Coupled Transformer Module to capture the inter-task interactions and a Post-Level Attention layer to use the predicted stance labels for RV, respectively. Experiments on two benchmark datasets show the superiority of our Coupled Hierarchical Transformer model over existing MTL approaches.</abstract>
      <url hash="fce9e4f0">2020.emnlp-main.108</url>
      <doi>10.18653/v1/2020.emnlp-main.108</doi>
      <video href="https://slideslive.com/38939330" />
      <bibkey>yu-etal-2020-coupled</bibkey>
    </paper>
    <paper id="109">
      <title>Social Media Attributions in the Context of Water Crisis</title>
      <author><first>Rupak</first><last>Sarkar</last></author>
      <author><first>Sayantan</first><last>Mahinder</last></author>
      <author><first>Hirak</first><last>Sarkar</last></author>
      <author><first>Ashiqur</first><last>KhudaBukhsh</last></author>
      <pages>1402–1412</pages>
      <abstract>Attribution of natural disasters / <a href="https://en.wikipedia.org/wiki/Collective_responsibility">collective misfortune</a> is a widely-studied political science problem. However, such <a href="https://en.wikipedia.org/wiki/Research">studies</a> typically rely on <a href="https://en.wikipedia.org/wiki/Survey_methodology">surveys</a>, or <a href="https://en.wikipedia.org/wiki/Expert_witness">expert opinions</a>, or external signals such as <a href="https://en.wikipedia.org/wiki/Opinion_poll">voting outcomes</a>. In this paper, we explore the viability of using unstructured, noisy social media data to complement traditional surveys through automatically extracting <a href="https://en.wikipedia.org/wiki/Attribution_(psychology)">attribution factors</a>. We present a novel prediction task of attribution tie detection of identifying the <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence">factors</a> (e.g., <a href="https://en.wikipedia.org/wiki/Urban_planning">poor city planning</a>, exploding population etc.) held responsible for the crisis in a social media document. We focus on the 2019 Chennai water crisis that rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. On a challenging <a href="https://en.wikipedia.org/wiki/Data_set">data set</a> constructed from <a href="https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos">YouTube comments</a> (72,098 comments posted by 43,859 users on 623 videos relevant to the crisis), we present a neural baseline to identify <a href="https://en.wikipedia.org/wiki/Attribution_(psychology)">attribution ties</a> that achieves a reasonable performance (accuracy : 87.34 % on <a href="https://en.wikipedia.org/wiki/Attribution_(psychology)">attribution detection</a> and 81.37 % on attribution resolution). We release the first annotated data set of 2,500 comments in this important domain.<i>attribution tie detection</i> of identifying the factors (e.g., poor city planning, exploding population etc.) held responsible for the crisis in a social media document. We focus on the 2019 Chennai water crisis that rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. On a challenging data set constructed from YouTube comments (72,098 comments posted by 43,859 users on 623 videos relevant to the crisis), we present a neural baseline to identify attribution ties that achieves a reasonable performance (accuracy: 87.34% on attribution detection and 81.37% on attribution resolution). We release the first annotated data set of 2,500 comments in this important domain.</abstract>
      <url hash="6e318ab1">2020.emnlp-main.109</url>
      <doi>10.18653/v1/2020.emnlp-main.109</doi>
      <video href="https://slideslive.com/38939332" />
      <bibkey>sarkar-etal-2020-social</bibkey>
    </paper>
    <paper id="110">
      <title>On the Reliability and Validity of Detecting Approval of Political Actors in Tweets</title>
      <author><first>Indira</first><last>Sen</last></author>
      <author><first>Fabian</first><last>Flöck</last></author>
      <author><first>Claudia</first><last>Wagner</last></author>
      <pages>1413–1426</pages>
      <abstract>Social media sites like <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> possess the potential to complement surveys that measure political opinions and, more specifically, political actors’ approval. However, new challenges related to the reliability and validity of social-media-based estimates arise. Various sentiment analysis and stance detection methods have been developed and used in previous research to measure users’ political opinions based on their content on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. In this work, we attempt to gauge the efficacy of untargeted sentiment, targeted sentiment, and stance detection methods in labeling various political actors’ approval by benchmarking them across several datasets. We also contrast the performance of these pretrained methods that can be used in an off-the-shelf (OTS) manner against a set of <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on minimal custom data. We find that OTS methods have low generalizability on unseen and familiar targets, while low-resource custom models are more robust. Our work sheds light on the strengths and limitations of existing methods proposed for understanding politicians’ approval from <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a>.</abstract>
      <url hash="31d0460b">2020.emnlp-main.110</url>
      <doi>10.18653/v1/2020.emnlp-main.110</doi>
      <video href="https://slideslive.com/38938774" />
      <bibkey>sen-etal-2020-reliability</bibkey>
    </paper>
    <paper id="114">
      <title>Predicting Clinical Trial Results by Implicit Evidence Integration</title>
      <author><first>Qiao</first><last>Jin</last></author>
      <author><first>Chuanqi</first><last>Tan</last></author>
      <author><first>Mosha</first><last>Chen</last></author>
      <author><first>Xiaozhong</first><last>Liu</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <pages>1461–1477</pages>
      <abstract>Clinical trials provide essential guidance for practicing <a href="https://en.wikipedia.org/wiki/Evidence-based_medicine">Evidence-Based Medicine</a>, though often accompanying with unendurable costs and risks. To optimize the design of <a href="https://en.wikipedia.org/wiki/Clinical_trial">clinical trials</a>, we introduce a novel Clinical Trial Result Prediction (CTRP) task. In the CTRP framework, a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population. While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from <a href="https://en.wikipedia.org/wiki/Medical_literature">medical literature</a> that implicitly contain PICOs and results as evidence. Specifically, we pre-train a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to predict the disentangled results from such implicit evidence and fine-tune the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with limited data on the downstream datasets. Experiments on the benchmark Evidence Integration dataset show that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms the baselines by large margins, e.g., with a 10.7 % relative gain over BioBERT in macro-F1. Moreover, the performance improvement is also validated on another <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> composed of <a href="https://en.wikipedia.org/wiki/Clinical_trial">clinical trials</a> related to COVID-19.</abstract>
      <url hash="a8d5e5e9">2020.emnlp-main.114</url>
      <doi>10.18653/v1/2020.emnlp-main.114</doi>
      <video href="https://slideslive.com/38939008" />
      <bibkey>jin-etal-2020-predicting</bibkey>
      <pwccode url="https://github.com/Alibaba-NLP/EBM-Net" additional="false">Alibaba-NLP/EBM-Net</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/evidence-inference">Evidence Inference</pwcdataset>
    </paper>
    <paper id="115">
      <title>Explainable Clinical Decision Support from Text</title>
      <author><first>Jinyue</first><last>Feng</last></author>
      <author><first>Chantal</first><last>Shaib</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>1478–1489</pages>
      <abstract>Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a hierarchical CNN-transformer model with explicit attention as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction, respectively. We also explore the relationships between learned <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> from structured and unstructured variables using projection-weighted canonical correlation analysis. Finally, we outline a protocol to evaluate <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> usability in a clinical decision support context. From domain-expert evaluations, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> generates informative rationales that have promising real-life applications.</abstract>
      <url hash="91b238a5">2020.emnlp-main.115</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2fec42a6">2020.emnlp-main.115.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.115</doi>
      <video href="https://slideslive.com/38939013" />
      <bibkey>feng-etal-2020-explainable</bibkey>
    </paper>
    <paper id="116">
      <title>A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization<fixed-case>C</fixed-case>hinese Medical Procedure Entity Normalization</title>
      <author><first>Jinghui</first><last>Yan</last></author>
      <author><first>Yining</first><last>Wang</last></author>
      <author><first>Lu</first><last>Xiang</last></author>
      <author><first>Yu</first><last>Zhou</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>1490–1499</pages>
      <abstract>Medical entity normalization, which links medical mentions in the text to entities in <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a>, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. However, nonstandard Chinese expressions and combined procedures present challenges in our <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a>. The existing <a href="https://en.wikipedia.org/wiki/Strategy">strategies</a> relying on the <a href="https://en.wikipedia.org/wiki/Discriminative_model">discriminative model</a> are poorly to cope with normalizing combined procedure mentions. We propose a sequence generative framework to directly generate all the corresponding medical procedure entities. we adopt two strategies : category-based constraint decoding and category-based model refining to avoid unrealistic results. The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures.</abstract>
      <url hash="751712ff">2020.emnlp-main.116</url>
      <doi>10.18653/v1/2020.emnlp-main.116</doi>
      <video href="https://slideslive.com/38939383" />
      <bibkey>yan-etal-2020-knowledge</bibkey>
    </paper>
    <paper id="117">
      <title>Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Akshay</first><last>Smit</last></author>
      <author><first>Saahil</first><last>Jain</last></author>
      <author><first>Pranav</first><last>Rajpurkar</last></author>
      <author><first>Anuj</first><last>Pareek</last></author>
      <author><first>Andrew</first><last>Ng</last></author>
      <author><first>Matthew</first><last>Lungren</last></author>
      <pages>1500–1519</pages>
      <abstract>The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a> based on medical domain knowledge or manual annotations by experts. In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation. We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with <a href="https://en.wikipedia.org/wiki/Statistical_significance">statistical significance</a>, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays.</abstract>
      <url hash="e30df89a">2020.emnlp-main.117</url>
      <doi>10.18653/v1/2020.emnlp-main.117</doi>
      <video href="https://slideslive.com/38938643" />
      <bibkey>smit-etal-2020-combining</bibkey>
    </paper>
    <paper id="118">
      <title>Benchmarking Meaning Representations in Neural Semantic Parsing</title>
      <author><first>Jiaqi</first><last>Guo</last></author>
      <author><first>Qian</first><last>Liu</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <author><first>Zhenwen</first><last>Li</last></author>
      <author><first>Xueqing</first><last>Liu</last></author>
      <author><first>Tao</first><last>Xie</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>1520–1540</pages>
      <abstract>Meaning representation is an important component of <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a>. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of <a href="https://en.wikipedia.org/wiki/List_of_Latin_phrases_(M)">them</a>. Thus, the impact of meaning representation on <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a> is less understood. Furthermore, existing work’s performance is often not comprehensively evaluated due to the lack of readily-available execution engines. Upon identifying these gaps, we propose, a new unified benchmark on meaning representations, by integrating existing semantic parsing datasets, completing the missing logical forms, and implementing the missing execution engines. The resulting unified benchmark contains the complete enumeration of <a href="https://en.wikipedia.org/wiki/Logical_form">logical forms</a> and <a href="https://en.wikipedia.org/wiki/Execution_(computing)">execution engines</a> over three datasets   four meaning representations. A thorough experimental study on Unimer reveals that neural semantic parsing approaches exhibit notably different performance when they are trained to generate different meaning representations. Also, program alias and <a href="https://en.wikipedia.org/wiki/Formal_grammar">grammar rules</a> heavily impact the performance of different <a href="https://en.wikipedia.org/wiki/Semantics">meaning representations</a>. Our <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark</a>, execution engines and implementation can be found on : https://github.com/JasperGuo/Unimer.<tex-math>\times</tex-math> four meaning representations. A thorough experimental study on Unimer reveals that neural semantic parsing approaches exhibit notably different performance when they are trained to generate different meaning representations. Also, program alias and grammar rules heavily impact the performance of different meaning representations. Our benchmark, execution engines and implementation can be found on: https://github.com/JasperGuo/Unimer.</abstract>
      <url hash="35a019e7">2020.emnlp-main.118</url>
      <doi>10.18653/v1/2020.emnlp-main.118</doi>
      <video href="https://slideslive.com/38938878" />
      <bibkey>guo-etal-2020-benchmarking-meaning</bibkey>
      <pwccode url="https://github.com/JasperGuo/Unimer" additional="false">JasperGuo/Unimer</pwccode>
    </paper>
    <paper id="119">
      <title>Analogous Process Structure Induction for Sub-event Sequence Prediction</title>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Haoyu</first><last>Wang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>1541–1550</pages>
      <abstract>Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories. Thus, knowledge about a known process such as buying a car can be used in the context of a new but analogous process such as buying a house. Nevertheless, most event understanding work in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> is still at the ground level and does not consider <a href="https://en.wikipedia.org/wiki/Abstraction">abstraction</a>. In this paper, we propose an Analogous Process Structure Induction (APSI) framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes. As our experiments and analysis indicate, APSI supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.</abstract>
      <url hash="4e28b587">2020.emnlp-main.119</url>
      <doi>10.18653/v1/2020.emnlp-main.119</doi>
      <video href="https://slideslive.com/38938902" />
      <bibkey>zhang-etal-2020-analogous</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="123">
      <title>Semantically Inspired AMR Alignment for the <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese Language</a><fixed-case>AMR</fixed-case> Alignment for the <fixed-case>P</fixed-case>ortuguese Language</title>
      <author><first>Rafael</first><last>Anchiêta</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>1595–1600</pages>
      <abstract>Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the <a href="https://en.wikipedia.org/wiki/Vertex_(graph_theory)">nodes</a> are concepts and <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">edges</a> are relations among them. Most of AMR parsing methods require alignment between the nodes of the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> and the words of the sentence. However, this alignment is not provided by manual annotations and available automatic aligners focus only on the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>, not performing well for other languages. Aiming to fulfill this gap, we developed an alignment method for the <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese language</a> based on a more semantically matched word-concept pair. We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for <a href="https://en.wikipedia.org/wiki/English_language">English</a>, improving AMR parsers, and achieving competitive results with a <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> designed for the <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese language</a>.</abstract>
      <url hash="6faa3525">2020.emnlp-main.123</url>
      <doi>10.18653/v1/2020.emnlp-main.123</doi>
      <video href="https://slideslive.com/38938691" />
      <bibkey>anchieta-pardo-2020-semantically</bibkey>
    </paper>
    <paper id="126">
      <title>Table Fact Verification with Structure-Aware Transformer</title>
      <author><first>Hongzhi</first><last>Zhang</last></author>
      <author><first>Yingyao</first><last>Wang</last></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Xuezhi</first><last>Cao</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Zhongyuan</first><last>Wang</last></author>
      <pages>1624–1629</pages>
      <abstract>Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform <a href="https://en.wikipedia.org/wiki/Symbolic_reasoning">symbolic reasoning</a>. Pre-trained language models trained on <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a> could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information. To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer. A <a href="https://en.wikipedia.org/wiki/Methodology">method</a> to combine symbolic and linguistic reasoning is also explored for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. Our method outperforms <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> with 4.93 % on TabFact, a large scale table verification dataset.</abstract>
      <url hash="387ff85e">2020.emnlp-main.126</url>
      <doi>10.18653/v1/2020.emnlp-main.126</doi>
      <video href="https://slideslive.com/38938841" />
      <bibkey>zhang-etal-2020-table</bibkey>
    </paper>
    <paper id="127">
      <title>Double Graph Based Reasoning for Document-level Relation Extraction</title>
      <author><first>Shuang</first><last>Zeng</last></author>
      <author><first>Runxin</first><last>Xu</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>1630–1640</pages>
      <abstract>Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a>, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a> we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>. Our code is available at https://github.com/PKUnlp-icler/GAIN.</abstract>
      <url hash="f205ef83">2020.emnlp-main.127</url>
      <doi>10.18653/v1/2020.emnlp-main.127</doi>
      <video href="https://slideslive.com/38938659" />
      <bibkey>zeng-etal-2020-double</bibkey>
      <pwccode url="https://github.com/DreamInvoker/GAIN" additional="false">DreamInvoker/GAIN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="130">
      <title>Knowledge Graph Alignment with Entity-Pair Embedding</title>
      <author><first>Zhichun</first><last>Wang</last></author>
      <author><first>Jinjian</first><last>Yang</last></author>
      <author><first>Xiaoju</first><last>Ye</last></author>
      <pages>1672–1680</pages>
      <abstract>Knowledge Graph (KG) alignment is to match entities in different <a href="https://en.wikipedia.org/wiki/Knowledge_Graph">KGs</a>, which is important to knowledge fusion and integration. Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results. These approaches first embed <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entities</a> in low-dimensional vector spaces, and then obtain entity alignments by computations on their <a href="https://en.wikipedia.org/wiki/Vector_space">vector representations</a>. Although continuous improvements have been achieved by recent work, the performances of existing <a href="https://en.wikipedia.org/wiki/Methodology">approaches</a> are still not satisfactory. In this work, we present a new approach that directly learns embeddings of entity-pairs for KG alignment. Our approach first generates a pair-wise connectivity graph (PCG) of two KGs, whose nodes are entity-pairs and edges correspond to relation-pairs ; it then learns node (entity-pair) embeddings of the PCG, which are used to predict equivalent relations of entities. To get desirable <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a>, a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural network</a> is used to generate similarity features of entity-pairs from their attributes ; and a graph neural network is employed to propagate the similarity features and get the final <a href="https://en.wikipedia.org/wiki/Embedding">embeddings of entity-pairs</a>. Experiments on five real-world datasets show that our approach can achieve the state-of-the-art KG alignment results.</abstract>
      <url hash="b4c8783f">2020.emnlp-main.130</url>
      <doi>10.18653/v1/2020.emnlp-main.130</doi>
      <video href="https://slideslive.com/38939215" />
      <bibkey>wang-etal-2020-knowledge-graph</bibkey>
    </paper>
    <paper id="134">
      <title>Beyond [ CLS ] through Ranking by Generation<fixed-case>CLS</fixed-case>] through Ranking by Generation</title>
      <author><first>Cicero</first><last>Nogueira dos Santos</last></author>
      <author><first>Xiaofei</first><last>Ma</last></author>
      <author><first>Ramesh</first><last>Nallapati</last></author>
      <author><first>Zhiheng</first><last>Huang</last></author>
      <author><first>Bing</first><last>Xiang</last></author>
      <pages>1722–1727</pages>
      <abstract>Generative models for <a href="https://en.wikipedia.org/wiki/Information_retrieval">Information Retrieval</a>, where ranking of documents is viewed as the task of generating a query from a document’s language model, were very successful in various IR tasks in the past. However, with the advent of modern <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a>, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead. Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet. In this work, we revisit the generative framework for <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a> and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task. Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.</abstract>
      <url hash="eaf9f6f3">2020.emnlp-main.134</url>
      <doi>10.18653/v1/2020.emnlp-main.134</doi>
      <video href="https://slideslive.com/38939015" />
      <bibkey>nogueira-dos-santos-etal-2020-beyond</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/insuranceqa">InsuranceQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="136">
      <title>Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning</title>
      <author><first>Yuning</first><last>Mao</last></author>
      <author><first>Yanru</first><last>Qu</last></author>
      <author><first>Yiqing</first><last>Xie</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>1737–1751</pages>
      <abstract>While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on <a href="https://en.wikipedia.org/wiki/Multi-document_summarization">multi-document summarization (MDS)</a>. We observe two major challenges when adapting SDS advances to MDS : (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations ; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning</a>. Additionally, the explicit <a href="https://en.wikipedia.org/wiki/Redundancy_(engineering)">redundancy measure</a> in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and <a href="https://en.wikipedia.org/wiki/Efficiency">efficiency</a>.</abstract>
      <url hash="270e0d04">2020.emnlp-main.136</url>
      <doi>10.18653/v1/2020.emnlp-main.136</doi>
      <video href="https://slideslive.com/38938676" />
      <bibkey>mao-etal-2020-multi</bibkey>
      <pwccode url="https://github.com/morningmoni/RL-MMR" additional="false">morningmoni/RL-MMR</pwccode>
    </paper>
    <paper id="137">
      <title>Improving Neural Topic Models using Knowledge Distillation<fixed-case>I</fixed-case>mproving <fixed-case>N</fixed-case>eural <fixed-case>T</fixed-case>opic <fixed-case>M</fixed-case>odels using <fixed-case>K</fixed-case>nowledge <fixed-case>D</fixed-case>istillation</title>
      <author><first>Alexander Miserlis</first><last>Hoyle</last></author>
      <author><first>Pranav</first><last>Goel</last></author>
      <author><first>Philip</first><last>Resnik</last></author>
      <pages>1752–1771</pages>
      <abstract>Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.</abstract>
      <url hash="b4e239ac">2020.emnlp-main.137</url>
      <doi>10.18653/v1/2020.emnlp-main.137</doi>
      <video href="https://slideslive.com/38939229" />
      <bibkey>hoyle-etal-2020-improving</bibkey>
      <pwccode url="https://github.com/ahoho/kd-topic-models" additional="false">ahoho/kd-topic-models</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="140">
      <title>Incorporating <a href="https://en.wikipedia.org/wiki/Multimodal_interaction">Multimodal Information</a> in Open-Domain Web Keyphrase Extraction</title>
      <author><first>Yansen</first><last>Wang</last></author>
      <author><first>Zhen</first><last>Fan</last></author>
      <author><first>Carolyn</first><last>Rose</last></author>
      <pages>1790–1800</pages>
      <abstract>Open-domain Keyphrase extraction (KPE) on the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">Web</a> is a fundamental yet complex NLP task with a wide range of practical applications within the field of <a href="https://en.wikipedia.org/wiki/Information_retrieval">Information Retrieval</a>. In contrast to other document types, <a href="https://en.wikipedia.org/wiki/Web_design">web page designs</a> are intended for easy navigation and <a href="https://en.wikipedia.org/wiki/Information_retrieval">information finding</a>. Effective designs encode within the layout and formatting signals that point to where the important information can be found. In this work, we propose a modeling approach that leverages these multi-modal signals to aid in the KPE task. In particular, we leverage both <a href="https://en.wikipedia.org/wiki/Lexical_analysis">lexical and visual features</a> (e.g., size, <a href="https://en.wikipedia.org/wiki/Font">font</a>, position) at the micro-level to enable effective strategy induction and meta-level features that describe pages at a macro-level to aid in strategy selection. Our evaluation demonstrates that a combination of effective strategy induction and strategy selection within this approach for the KPE task outperforms state-of-the-art models. A qualitative post-hoc analysis illustrates how these <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> function within the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>.</abstract>
      <url hash="b7ec773b">2020.emnlp-main.140</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c21a9081">2020.emnlp-main.140.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.140</doi>
      <video href="https://slideslive.com/38938750" />
      <bibkey>wang-etal-2020-incorporating</bibkey>
    </paper>
    <paper id="143">
      <title>Multimodal Routing : Improving Local and Global Interpretability of Multimodal Language Analysis</title>
      <author><first>Yao-Hung Hubert</first><last>Tsai</last></author>
      <author><first>Martin</first><last>Ma</last></author>
      <author><first>Muqiao</first><last>Yang</last></author>
      <author><first>Ruslan</first><last>Salakhutdinov</last></author>
      <author><first>Louis-Philippe</first><last>Morency</last></author>
      <pages>1823–1833</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Human_language">human language</a> can be expressed through multiple sources of information known as <a href="https://en.wikipedia.org/wiki/Linguistic_modality">modalities</a>, including <a href="https://en.wikipedia.org/wiki/Tone_(linguistics)">tones of voice</a>, facial gestures, and <a href="https://en.wikipedia.org/wiki/Spoken_language">spoken language</a>. Recent <a href="https://en.wikipedia.org/wiki/Multimodal_learning">multimodal learning</a> with strong performances on human-centric tasks such as <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> and <a href="https://en.wikipedia.org/wiki/Emotion_recognition">emotion recognition</a> are often black-box, with very limited interpretability. In this paper we propose, which dynamically adjusts weights between input modalities and output representations differently for each input sample. Multimodal routing can identify relative importance of both individual modalities and cross-modality factors. Moreover, the weight assignment by <a href="https://en.wikipedia.org/wiki/Routing">routing</a> allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.</abstract>
      <url hash="aa965690">2020.emnlp-main.143</url>
      <attachment type="OptionalSupplementaryMaterial" hash="60c3805e">2020.emnlp-main.143.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.143</doi>
      <video href="https://slideslive.com/38938697" />
      <bibkey>tsai-etal-2020-multimodal</bibkey>
      <pwccode url="https://github.com/martinmamql/multimodal_routing" additional="false">martinmamql/multimodal_routing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="145">
      <title>BiST : Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues<fixed-case>B</fixed-case>i<fixed-case>ST</fixed-case>: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues</title>
      <author><first>Hung</first><last>Le</last></author>
      <author><first>Doyen</first><last>Sahoo</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <author><first>Steven C.H.</first><last>Hoi</last></author>
      <pages>1846–1859</pages>
      <abstract>Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we proposed Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved <a href="https://en.wikipedia.org/wiki/Sensory_cue">visual cues</a> are used as <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.</abstract>
      <url hash="3028407e">2020.emnlp-main.145</url>
      <doi>10.18653/v1/2020.emnlp-main.145</doi>
      <video href="https://slideslive.com/38938824" />
      <bibkey>le-etal-2020-bist</bibkey>
      <pwccode url="https://github.com/salesforce/BiST" additional="false">salesforce/BiST</pwccode>
    </paper>
    <paper id="147">
      <title>GraphDialog : Integrating <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">Graph Knowledge</a> into End-to-End Task-Oriented Dialogue Systems<fixed-case>G</fixed-case>raph<fixed-case>D</fixed-case>ialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems</title>
      <author><first>Shiquan</first><last>Yang</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Sarah</first><last>Erfani</last></author>
      <pages>1878–1888</pages>
      <abstract>End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems : one is how to effectively incorporate external knowledge bases (KBs) into the learning framework ; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a> and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a>. To exploit the relations between entities in KBs, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> combines multi-hop reasoning ability based on the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a>. Experimental results show that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.</abstract>
      <url hash="6595e8f8">2020.emnlp-main.147</url>
      <doi>10.18653/v1/2020.emnlp-main.147</doi>
      <video href="https://slideslive.com/38938852" />
      <bibkey>yang-etal-2020-graphdialog</bibkey>
      <pwccode url="https://github.com/shiquanyang/GraphDialog" additional="true">shiquanyang/GraphDialog</pwccode>
    </paper>
    <paper id="150">
      <title>Multi-turn Response Selection using Dialogue Dependency Relations</title>
      <author><first>Qi</first><last>Jia</last></author>
      <author><first>Yizhu</first><last>Liu</last></author>
      <author><first>Siyu</first><last>Ren</last></author>
      <author><first>Kenny</first><last>Zhu</last></author>
      <author><first>Haifeng</first><last>Tang</last></author>
      <pages>1911–1920</pages>
      <abstract>Multi-turn response selection is a task designed for developing dialogue agents. The performance on this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> has a remarkable improvement with pre-trained language models. However, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns. In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into <a href="https://en.wikipedia.org/wiki/Thread_(computing)">threads</a> based on their dependency relations. Each thread can be regarded as a self-contained sub-dialogue. We also propose Thread-Encoder model to encode <a href="https://en.wikipedia.org/wiki/Thread_(computing)">threads</a> and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer. The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8 *, with competitive results on UbuntuV2.</abstract>
      <url hash="246b613e">2020.emnlp-main.150</url>
      <doi>10.18653/v1/2020.emnlp-main.150</doi>
      <video href="https://slideslive.com/38938667" />
      <bibkey>jia-etal-2020-multi</bibkey>
      <pwccode url="https://github.com/JiaQiSJTU/ResponseSelection" additional="false">JiaQiSJTU/ResponseSelection</pwccode>
    </paper>
    <paper id="155">
      <title>LOGAN : Local Group Bias Detection by Clustering<fixed-case>LOGAN</fixed-case>: Local Group Bias Detection by Clustering</title>
      <author><first>Jieyu</first><last>Zhao</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>1968–1977</pages>
      <abstract>Machine learning techniques have been widely used in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP)</a>. However, as revealed by many recent studies, <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning models</a> often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between <a href="https://en.wikipedia.org/wiki/Protected_group">protected groups</a> and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how <a href="https://en.wikipedia.org/wiki/Bias">biases</a> are embedded in a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>. In fact, a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with similar aggregated performance between different groups on the entire data may behave differently on instances in a <a href="https://en.wikipedia.org/wiki/Region">local region</a>. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering</a>. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.</abstract>
      <url hash="a658f80f">2020.emnlp-main.155</url>
      <doi>10.18653/v1/2020.emnlp-main.155</doi>
      <video href="https://slideslive.com/38939216" />
      <bibkey>zhao-chang-2020-logan</bibkey>
      <pwccode url="https://github.com/uclanlp/clusters" additional="false">uclanlp/clusters</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="160">
      <title>Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents<fixed-case>D</fixed-case>omain-<fixed-case>S</fixed-case>pecific <fixed-case>L</fixed-case>exical <fixed-case>G</fixed-case>rounding in <fixed-case>N</fixed-case>oisy <fixed-case>V</fixed-case>isual-<fixed-case>T</fixed-case>extual <fixed-case>D</fixed-case>ocuments</title>
      <author><first>Gregory</first><last>Yauney</last></author>
      <author><first>Jack</first><last>Hessel</last></author>
      <author><first>David</first><last>Mimno</last></author>
      <pages>2039–2045</pages>
      <abstract>Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, unlabeled multi-image, multi-sentence documents are abundant. Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap? Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as kitchen and bedroom, and introduce metrics to assess this document similarity. We present a simple unsupervised clustering-based method that increases precision and recall beyond <a href="https://en.wikipedia.org/wiki/Object_detection">object detection</a> and image tagging baselines when evaluated on labeled subsets of the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. The proposed method is particularly effective for local contextual meanings of a word, for example associating <a href="https://en.wikipedia.org/wiki/Granite">granite</a> with <a href="https://en.wikipedia.org/wiki/Countertop">countertops</a> in the <a href="https://en.wikipedia.org/wiki/Real_estate">real estate dataset</a> and with rocky landscapes in a <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia dataset</a>.</abstract>
      <url hash="c990e289">2020.emnlp-main.160</url>
      <doi>10.18653/v1/2020.emnlp-main.160</doi>
      <video href="https://slideslive.com/38939084" />
      <bibkey>yauney-etal-2020-domain</bibkey>
      <pwccode url="https://github.com/gyauney/domain-specific-lexical-grounding" additional="false">gyauney/domain-specific-lexical-grounding</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="162">
      <title>Vokenization : Improving Language Understanding with Contextualized, Visual-Grounded Supervision</title>
      <author><first>Hao</first><last>Tan</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>2066–2080</pages>
      <abstract>Humans learn <a href="https://en.wikipedia.org/wiki/Language">language</a> by listening, <a href="https://en.wikipedia.org/wiki/Speech">speaking</a>, <a href="https://en.wikipedia.org/wiki/Writing">writing</a>, <a href="https://en.wikipedia.org/wiki/Reading">reading</a>, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named vokenization that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call vokens). The vokenizer is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG.</abstract>
      <url hash="e4f3b271">2020.emnlp-main.162</url>
      <doi>10.18653/v1/2020.emnlp-main.162</doi>
      <video href="https://slideslive.com/38939320" />
      <bibkey>tan-bansal-2020-vokenization</bibkey>
      <pwccode url="https://github.com/airsplay/vokenization" additional="false">airsplay/vokenization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="165">
      <title>FedED : Federated Learning via Ensemble Distillation for Medical Relation Extraction<fixed-case>F</fixed-case>ed<fixed-case>ED</fixed-case>: Federated Learning via Ensemble Distillation for Medical Relation Extraction</title>
      <author><first>Dianbo</first><last>Sui</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Yantao</first><last>Jia</last></author>
      <author><first>Yuantao</first><last>Xie</last></author>
      <author><first>Weijian</first><last>Sun</last></author>
      <pages>2118–2128</pages>
      <abstract>Unlike other domains, medical texts are inevitably accompanied by <a href="https://en.wikipedia.org/wiki/Privacy">private information</a>, so sharing or copying these <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">texts</a> is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with <a href="https://en.wikipedia.org/wiki/Privacy">privacy protection</a>. In this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged. Though federated learning has distinct advantages in <a href="https://en.wikipedia.org/wiki/Information_privacy">privacy protection</a>, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters. To overcome this bottleneck, we leverage a <a href="https://en.wikipedia.org/wiki/Strategy">strategy</a> based on knowledge distillation. Such a <a href="https://en.wikipedia.org/wiki/Strategy">strategy</a> uses the uploaded predictions of <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble local models</a> to train the central model without requiring uploading local parameters. Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.</abstract>
      <url hash="eae3bec0">2020.emnlp-main.165</url>
      <doi>10.18653/v1/2020.emnlp-main.165</doi>
      <video href="https://slideslive.com/38939230" />
      <bibkey>sui-etal-2020-feded</bibkey>
    </paper>
    <paper id="167">
      <title>A Predicate-Function-Argument Annotation of Natural Language for Open-Domain Information eXpression<fixed-case>A</fixed-case> <fixed-case>P</fixed-case>redicate-<fixed-case>F</fixed-case>unction-<fixed-case>A</fixed-case>rgument <fixed-case>A</fixed-case>nnotation of <fixed-case>N</fixed-case>atural <fixed-case>L</fixed-case>anguage for <fixed-case>O</fixed-case>pen-<fixed-case>D</fixed-case>omain <fixed-case>I</fixed-case>nformation e<fixed-case>X</fixed-case>pression</title>
      <author><first>Mingming</first><last>Sun</last></author>
      <author><first>Wenyue</first><last>Hua</last></author>
      <author><first>Zoey</first><last>Liu</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Kangjie</first><last>Zheng</last></author>
      <author><first>Ping</first><last>Li</last></author>
      <pages>2140–2150</pages>
      <abstract>Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works ; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new <a href="https://en.wikipedia.org/wiki/Pipeline_(computing)">pipeline</a> to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a <a href="https://en.wikipedia.org/wiki/Computing_platform">platform</a> for all OIE strategies. The <a href="https://en.wikipedia.org/wiki/OIX">OIX</a> is an OIE friendly expression of a sentence without <a href="https://en.wikipedia.org/wiki/Information_loss">information loss</a>. The generation procedure of <a href="https://en.wikipedia.org/wiki/OIX">OIX</a> contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of <a href="https://en.wikipedia.org/wiki/OIX">OIX</a> as inference operations focusing on more critical problems. Based on the same platform of <a href="https://en.wikipedia.org/wiki/OIX">OIX</a>, the OIE strategies are reusable, and people can select a set of <a href="https://en.wikipedia.org/wiki/Strategy">strategies</a> to assemble their <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> for a specific task so that the adaptability may be significantly increased. This paper focuses on the task of OIX and propose a solution   Open Information Annotation (OIA). OIA is a predicate-function-argument annotation for sentences. We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences. The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.</abstract>
      <url hash="d56f14e5">2020.emnlp-main.167</url>
      <doi>10.18653/v1/2020.emnlp-main.167</doi>
      <video href="https://slideslive.com/38939349" />
      <bibkey>sun-etal-2020-predicate</bibkey>
    </paper>
    <paper id="171">
      <title>Understanding the Mechanics of SPIGOT : Surrogate Gradients for Latent Structure Learning<fixed-case>SPIGOT</fixed-case>: Surrogate Gradients for Latent Structure Learning</title>
      <author><first>Tsvetomila</first><last>Mihaylova</last></author>
      <author><first>Vlad</first><last>Niculae</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>2186–2202</pages>
      <abstract>Latent structure models are a powerful tool for modeling language data : they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT   a variant of STE for structured models. Our perspective leads to new <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.</abstract>
      <url hash="be4f9c2e">2020.emnlp-main.171</url>
      <doi>10.18653/v1/2020.emnlp-main.171</doi>
      <video href="https://slideslive.com/38939140" />
      <bibkey>mihaylova-etal-2020-understanding</bibkey>
      <pwccode url="https://github.com/deep-spin/understanding-spigot" additional="false">deep-spin/understanding-spigot</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="177">
      <title>Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses<fixed-case>NMT</fixed-case> with Hybrid Losses</title>
      <author><first>Prathyusha</first><last>Jwalapuram</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Youlin</first><last>Shen</last></author>
      <pages>2267–2279</pages>
      <abstract>Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation model</a>. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> has failed to adequately learn from, we improve the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> and evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En.</abstract>
      <url hash="ac2428dd">2020.emnlp-main.177</url>
      <doi>10.18653/v1/2020.emnlp-main.177</doi>
      <video href="https://slideslive.com/38939290" />
      <bibkey>jwalapuram-etal-2020-pronoun</bibkey>
      <pwccode url="https://github.com/ntunlp/pronoun-finetuning" additional="false">ntunlp/pronoun-finetuning</pwccode>
    </paper>
    <paper id="182">
      <title>Adversarial Attack and Defense of Structured Prediction Models</title>
      <author><first>Wenjuan</first><last>Han</last></author>
      <author><first>Liwen</first><last>Zhang</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>2327–2338</pages>
      <abstract>Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP)</a> have attracted a lot of research in recent years. However, most of the existing approaches focus on <a href="https://en.wikipedia.org/wiki/Taxonomy_(biology)">classification problems</a>. In this paper, we investigate attacks and defenses for structured prediction tasks in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models : the structured output of structured prediction models is sensitive to small perturbations in the input. To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task. Based on the proposed attack, we further reinforce the victim model with <a href="https://en.wikipedia.org/wiki/Adversarial_learning">adversarial training</a>, making its prediction more robust and accurate. We evaluate the proposed <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> in <a href="https://en.wikipedia.org/wiki/Dependency_grammar">dependency parsing</a> and <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a>. Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.</abstract>
      <url hash="39442d52">2020.emnlp-main.182</url>
      <doi>10.18653/v1/2020.emnlp-main.182</doi>
      <video href="https://slideslive.com/38939221" />
      <bibkey>han-etal-2020-adversarial</bibkey>
      <pwccode url="https://github.com/WinnieHAN/structure_adv" additional="false">WinnieHAN/structure_adv</pwccode>
    </paper>
    <paper id="183">
      <title>Position-Aware Tagging for Aspect Sentiment Triplet Extraction</title>
      <author><first>Lu</first><last>Xu</last></author>
      <author><first>Hao</first><last>Li</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <pages>2339–2349</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages. Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach. However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question. In this work, we propose the first <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end model</a> with a novel position-aware tagging scheme that is capable of jointly extracting the <a href="https://en.wikipedia.org/wiki/Multiple_birth">triplets</a>. Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches. We also conducted extensive experiments to investigate the model effectiveness and robustness.</abstract>
      <url hash="afb6dfe3">2020.emnlp-main.183</url>
      <attachment type="OptionalSupplementaryMaterial" hash="aa271b80">2020.emnlp-main.183.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.183</doi>
      <video href="https://slideslive.com/38939300" />
      <bibkey>xu-etal-2020-position</bibkey>
      <pwccode url="https://github.com/xuuuluuu/SemEval-Triplet-data" additional="true">xuuuluuu/SemEval-Triplet-data</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aste-data-v2">ASTE-Data-V2</pwcdataset>
    </paper>
    <paper id="184">
      <title>Simultaneous Machine Translation with Visual Context</title>
      <author><first>Ozan</first><last>Caglayan</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Veneta</first><last>Haralampieva</last></author>
      <author><first>Pranava</first><last>Madhyastha</last></author>
      <author><first>Loïc</first><last>Barrault</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>2350–2361</pages>
      <abstract>Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a> and highest quality possible. The <a href="https://en.wikipedia.org/wiki/Translation">translation</a> thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of <a href="https://en.wikipedia.org/wiki/Visual_system">visual information</a> can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from <a href="https://en.wikipedia.org/wiki/English_language">English</a> into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/French_language">French</a>.</abstract>
      <url hash="7bc2d1ec">2020.emnlp-main.184</url>
      <doi>10.18653/v1/2020.emnlp-main.184</doi>
      <video href="https://slideslive.com/38938900" />
      <bibkey>caglayan-etal-2020-simultaneous</bibkey>
      <pwccode url="https://github.com/ImperialNLP/pysimt" additional="false">ImperialNLP/pysimt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="186">
      <title>The Secret is in the <a href="https://en.wikipedia.org/wiki/Electromagnetic_spectrum">Spectra</a> : Predicting Cross-lingual Task Performance with Spectral Similarity Measures</title>
      <author><first>Haim</first><last>Dubossarsky</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>2377–2390</pages>
      <abstract>Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand : e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding spaces. In this work we present a large-scale study focused on the correlations between monolingual embedding space similarity and task performance, covering thousands of language pairs and four different tasks : BLI, <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a>, POS tagging and MT. We hypothesize that statistics of the <a href="https://en.wikipedia.org/wiki/Spectrum_(functional_analysis)">spectrum</a> of each monolingual embedding space indicate how well they can be aligned. We then introduce several isomorphism measures between two <a href="https://en.wikipedia.org/wiki/Embedding">embedding spaces</a>, based on the relevant statistics of their individual spectra. We empirically show that (1) language similarity scores derived from such spectral isomorphism measures are strongly associated with performance observed in different cross-lingual tasks, and (2) our spectral-based measures consistently outperform previous standard isomorphism measures, while being computationally more tractable and easier to interpret. Finally, our <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">measures</a> capture complementary information to typologically driven language distance measures, and the combination of <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">measures</a> from the two families yields even higher task performance correlations.</abstract>
      <url hash="08652514">2020.emnlp-main.186</url>
      <attachment type="OptionalSupplementaryMaterial" hash="86ec5f15">2020.emnlp-main.186.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.186</doi>
      <video href="https://slideslive.com/38939057" />
      <bibkey>dubossarsky-etal-2020-secret</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/panlex">Panlex</pwcdataset>
    </paper>
    <paper id="188">
      <title>AnswerFact : Fact Checking in Product Question Answering<fixed-case>A</fixed-case>nswer<fixed-case>F</fixed-case>act: Fact Checking in Product Question Answering</title>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Deng</last></author>
      <author><first>Jing</first><last>Ma</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>2407–2417</pages>
      <abstract>Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during <a href="https://en.wikipedia.org/wiki/Online_shopping">online shopping</a>. However, the misinformation in the answers on those <a href="https://en.wikipedia.org/wiki/Computing_platform">platforms</a> poses unprecedented challenges for users to obtain reliable and truthful product information, which may even cause a commercial loss in <a href="https://en.wikipedia.org/wiki/E-commerce">E-commerce business</a>. To tackle this issue, we investigate to predict the veracity of answers in this paper and introduce AnswerFact, a large scale fact checking dataset from product question answering forums. Each answer is accompanied by its veracity label and associated evidence sentences, providing a valuable testbed for evidence-based fact checking tasks in QA settings. We further propose a novel neural model with tailored evidence ranking components to handle the concerned answer veracity prediction problem. Extensive experiments are conducted with our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and various existing fact checking methods, showing that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms all baselines on this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>.</abstract>
      <url hash="115b1ce1">2020.emnlp-main.188</url>
      <doi>10.18653/v1/2020.emnlp-main.188</doi>
      <bibkey>zhang-etal-2020-answerfact</bibkey>
    </paper>
    <paper id="190">
      <title>What do Models Learn from Question Answering Datasets?</title>
      <author><first>Priyanka</first><last>Sen</last></author>
      <author><first>Amir</first><last>Saffari</last></author>
      <pages>2429–2438</pages>
      <abstract>While models have reached superhuman performance on popular <a href="https://en.wikipedia.org/wiki/Question_answering">question answering (QA) datasets</a> such as SQuAD, they have yet to outperform <a href="https://en.wikipedia.org/wiki/Human">humans</a> on the task of <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> itself. In this paper, we investigate if <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are learning <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> from QA datasets by evaluating BERT-based models across five datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect data, and ability to handle question variations. We find that no single <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is robust to all of our experiments and identify shortcomings in both <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and evaluation methods. Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> through <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a>. We also release code to convert QA datasets to a shared format for easier experimentation at https://github.com/amazon-research/qa-dataset-converter</abstract>
      <url hash="bcacb816">2020.emnlp-main.190</url>
      <doi>10.18653/v1/2020.emnlp-main.190</doi>
      <video href="https://slideslive.com/38939080" />
      <bibkey>sen-saffari-2020-models</bibkey>
      <pwccode url="https://github.com/amazon-research/qa-dataset-converter" additional="false">amazon-research/qa-dataset-converter</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="193">
      <title>Neural Deepfake Detection with Factual Structure of Text</title>
      <author><first>Wanjun</first><last>Zhong</last></author>
      <author><first>Duyu</first><last>Tang</last></author>
      <author><first>Zenan</first><last>Xu</last></author>
      <author><first>Ruize</first><last>Wang</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Jiahai</first><last>Wang</last></author>
      <author><first>Jian</first><last>Yin</last></author>
      <pages>2461–2470</pages>
      <abstract>Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coarse-grained representations. However, they struggle to capture factual structures of documents, which is a discriminative factor between machine-generated and human-written text according to our statistical analysis. To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text. Our approach represents the factual structure of a given document as an <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity graph</a>, which is further utilized to learn <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence representations</a> with a graph neural network. Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled. Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa. Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and <a href="https://en.wikipedia.org/wiki/Written_language">human-written text</a>.</abstract>
      <url hash="206a57bd">2020.emnlp-main.193</url>
      <doi>10.18653/v1/2020.emnlp-main.193</doi>
      <video href="https://slideslive.com/38938858" />
      <bibkey>zhong-etal-2020-neural</bibkey>
    </paper>
    <paper id="194">
      <title>MultiCQA : Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale<fixed-case>M</fixed-case>ulti<fixed-case>CQA</fixed-case>: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale</title>
      <author><first>Andreas</first><last>Rücklé</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>2471–2486</pages>
      <abstract>We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.</abstract>
      <url hash="5fa7d92b">2020.emnlp-main.194</url>
      <doi>10.18653/v1/2020.emnlp-main.194</doi>
      <video href="https://slideslive.com/38938833" />
      <bibkey>ruckle-etal-2020-multicqa</bibkey>
      <pwccode url="https://github.com/ukplab/emnlp2020-multicqa" additional="false">ukplab/emnlp2020-multicqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/insuranceqa">InsuranceQA</pwcdataset>
    </paper>
    <paper id="195">
      <title>XL-AMR : Enabling Cross-Lingual AMR Parsing with Transfer Learning Techniques<fixed-case>XL</fixed-case>-<fixed-case>AMR</fixed-case>: Enabling Cross-Lingual <fixed-case>AMR</fixed-case> Parsing with Transfer Learning Techniques</title>
      <author><first>Rexhina</first><last>Blloshmi</last></author>
      <author><first>Rocco</first><last>Tripodi</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>2487–2500</pages>
      <abstract>Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages. However, cross-lingual AMR parsing is a hard task, because training data are scarce in languages other than <a href="https://en.wikipedia.org/wiki/English_language">English</a> and the existing <a href="https://en.wikipedia.org/wiki/English_language">English AMR parsers</a> are not directly suited to being used in a cross-lingual setting. In this work we tackle these two problems so as to enable cross-lingual AMR parsing : we explore different transfer learning techniques for producing automatic AMR annotations across languages and develop a cross-lingual AMR parser, XL-AMR. This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing. The results of XL-AMR significantly surpass those previously reported in <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a>, <a href="https://en.wikipedia.org/wiki/Italian_language">Italian</a> and <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>. Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages. We release XL-AMR at github.com/SapienzaNLP/xl-amr.</abstract>
      <url hash="10f419d3">2020.emnlp-main.195</url>
      <doi>10.18653/v1/2020.emnlp-main.195</doi>
      <video href="https://slideslive.com/38938960" />
      <bibkey>blloshmi-etal-2020-xl</bibkey>
      <pwccode url="https://github.com/SapienzaNLP/xl-amr" additional="false">SapienzaNLP/xl-amr</pwccode>
    </paper>
    <paper id="196">
      <title>Improving AMR Parsing with Sequence-to-Sequence Pre-training<fixed-case>AMR</fixed-case> Parsing with Sequence-to-Sequence Pre-training</title>
      <author><first>Dongqin</first><last>Xu</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Muhua</first><last>Zhu</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>2501–2511</pages>
      <abstract>In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing. However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>, i.e., <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, <a href="https://en.wikipedia.org/wiki/Syntactic_parsing">syntactic parsing</a>, and AMR parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than <a href="https://en.wikipedia.org/wiki/Complex_analysis">complex models</a>. We make our code and model available at https:// github.com/xdqkid/S2S-AMR-Parser.</abstract>
      <url hash="f9c8204a">2020.emnlp-main.196</url>
      <doi>10.18653/v1/2020.emnlp-main.196</doi>
      <video href="https://slideslive.com/38939049" />
      <bibkey>xu-etal-2020-improving</bibkey>
      <pwccode url="https://github.com/xdqkid/S2S-AMR-Parser" additional="false">xdqkid/S2S-AMR-Parser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2017t10">LDC2017T10</pwcdataset>
    </paper>
    <paper id="197">
      <title>Hate-Speech and Offensive Language Detection in <a href="https://en.wikipedia.org/wiki/Roman_Urdu">Roman Urdu</a><fixed-case>R</fixed-case>oman <fixed-case>U</fixed-case>rdu</title>
      <author><first>Hammad</first><last>Rizwan</last></author>
      <author><first>Muhammad Haroon</first><last>Shakeel</last></author>
      <author><first>Asim</first><last>Karim</last></author>
      <pages>2512–2522</pages>
      <abstract>The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>, limiting the applicability to particular demographics. Despite its prevalence, Roman Urdu (RU) lacks language resources, <a href="https://en.wikipedia.org/wiki/Annotation">annotated datasets</a>, and <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> for this task. In this study, we : (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10,012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hate-speech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available. We conclude that <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> is more beneficial as compared to training embedding from scratch and that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> exhibits greater <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> as compared to the baselines.</abstract>
      <url hash="156c30c1">2020.emnlp-main.197</url>
      <doi>10.18653/v1/2020.emnlp-main.197</doi>
      <video href="https://slideslive.com/38938955" />
      <bibkey>rizwan-etal-2020-hate</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/rushold">RUSHOLD</pwcdataset>
    </paper>
    <paper id="199">
      <title>Comparative Evaluation of Label-Agnostic Selection Bias in Multilingual Hate Speech Datasets</title>
      <author><first>Nedjma</first><last>Ousidhoum</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Dit-Yan</first><last>Yeung</last></author>
      <pages>2532–2542</pages>
      <abstract>Work on bias in <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a> typically aims to improve <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> performance while relatively overlooking the quality of the data. We examine <a href="https://en.wikipedia.org/wiki/Selection_bias">selection bias</a> in <a href="https://en.wikipedia.org/wiki/Hate_speech">hate speech</a> in a language and label independent fashion. We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora. We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.</abstract>
      <url hash="c33ab3e3">2020.emnlp-main.199</url>
      <doi>10.18653/v1/2020.emnlp-main.199</doi>
      <video href="https://slideslive.com/38939035" />
      <bibkey>ousidhoum-etal-2020-comparative</bibkey>
    </paper>
    <paper id="203">
      <title>Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems<fixed-case>NMT</fixed-case> by Finetuning Subword Systems</title>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>2572–2579</pages>
      <abstract>Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a> at the expense of somewhat worse overall translation quality. Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.</abstract>
      <url hash="f0722a0c">2020.emnlp-main.203</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5ce69b68">2020.emnlp-main.203.OptionalSupplementaryMaterial.tgz</attachment>
      <doi>10.18653/v1/2020.emnlp-main.203</doi>
      <video href="https://slideslive.com/38938871" />
      <bibkey>libovicky-fraser-2020-towards</bibkey>
      <pwccode url="https://github.com/jlibovicky/char-nmt" additional="true">jlibovicky/char-nmt</pwccode>
    </paper>
    <paper id="207">
      <title>Not Low-Resource Anymore : Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation<fixed-case>B</fixed-case>engali-<fixed-case>E</fixed-case>nglish Machine Translation</title>
      <author><first>Tahmid</first><last>Hasan</last></author>
      <author><first>Abhik</first><last>Bhattacharjee</last></author>
      <author><first>Kazi</first><last>Samin</last></author>
      <author><first>Masum</first><last>Hasan</last></author>
      <author><first>Madhusudan</first><last>Basak</last></author>
      <author><first>M. Sohel</first><last>Rahman</last></author>
      <author><first>Rifat</first><last>Shahriyar</last></author>
      <pages>2612–2623</pages>
      <abstract>Despite being the seventh most widely spoken language in the world, <a href="https://en.wikipedia.org/wiki/Bengali_language">Bengali</a> has received much less attention in machine translation literature due to being low in resources. Most publicly available <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpora</a> for <a href="https://en.wikipedia.org/wiki/Bengali_language">Bengali</a> are not large enough ; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them. In this work, we build a customized sentence segmenter for <a href="https://en.wikipedia.org/wiki/Bengali_language">Bengali</a> and propose two novel methods for parallel corpus creation on low-resource setups : aligner ensembling and batch filtering. With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before. Training on <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural models</a>, we achieve an improvement of more than 9 <a href="https://en.wikipedia.org/wiki/BLEU">BLEU score</a> over previous approaches to Bengali-English machine translation. We also evaluate on a new test set of 1000 pairs made with extensive <a href="https://en.wikipedia.org/wiki/Quality_control">quality control</a>. We release the segmenter, <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpus</a>, and the evaluation set, thus elevating <a href="https://en.wikipedia.org/wiki/Bengali_language">Bengali</a> from its low-resource status. To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation. We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages. Our data and code are available at https://github.com/csebuetnlp/banglanmt.</abstract>
      <url hash="a0921235">2020.emnlp-main.207</url>
      <doi>10.18653/v1/2020.emnlp-main.207</doi>
      <video href="https://slideslive.com/38939088" />
      <bibkey>hasan-etal-2020-low</bibkey>
      <pwccode url="https://github.com/csebuetnlp/banglanmt" additional="false">csebuetnlp/banglanmt</pwccode>
    </paper>
    <paper id="210">
      <title>Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information</title>
      <author><first>Zehui</first><last>Lin</last></author>
      <author><first>Xiao</first><last>Pan</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Jiangtao</first><last>Feng</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>2649–2663</pages>
      <abstract>We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com / linzehui / mRASP.</abstract>
      <url hash="a0f25581">2020.emnlp-main.210</url>
      <doi>10.18653/v1/2020.emnlp-main.210</doi>
      <video href="https://slideslive.com/38939388" />
      <bibkey>lin-etal-2020-pre</bibkey>
      <pwccode url="https://github.com/linzehui/mRASP" additional="false">linzehui/mRASP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="211">
      <title>Losing Heads in the Lottery : Pruning Transformer Attention in Neural Machine Translation</title>
      <author><first>Maximiliana</first><last>Behnke</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>2664–2674</pages>
      <abstract>The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most <a href="https://en.wikipedia.org/wiki/Attentional_control">attention heads</a> are not confident in their decisions and can be pruned. However, removing them before training a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> results in lower quality. In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training. Our experiments on <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> for <a href="https://en.wikipedia.org/wiki/Turkish_language">TurkishEnglish</a>. The pruned model is 1.5 times as fast at <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference</a>, albeit at the cost of longer training. Our method is complementary to other approaches, such as teacher-student, with EnglishGerman student model gaining an additional 10 % speed-up with 75 % encoder attention removed and 0.2 BLEU loss.</abstract>
      <url hash="a86a9658">2020.emnlp-main.211</url>
      <doi>10.18653/v1/2020.emnlp-main.211</doi>
      <video href="https://slideslive.com/38938739" />
      <bibkey>behnke-heafield-2020-losing</bibkey>
    </paper>
    <paper id="212">
      <title>Towards Enhancing <a href="https://en.wikipedia.org/wiki/Faithfulness">Faithfulness</a> for Neural Machine Translation</title>
      <author><first>Rongxiang</first><last>Weng</last></author>
      <author><first>Heng</first><last>Yu</last></author>
      <author><first>Xiangpeng</first><last>Wei</last></author>
      <author><first>Weihua</first><last>Luo</last></author>
      <pages>2675–2684</pages>
      <abstract>Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training strategy</a> with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FEnmt). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and decoder to guide <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a> to correctly translate these fragments. Both automatic and human evaluations verify that our FEnmt could improve translation quality by effectively reducing unfaithful translations.</abstract>
      <url hash="aba554e7">2020.emnlp-main.212</url>
      <doi>10.18653/v1/2020.emnlp-main.212</doi>
      <video href="https://slideslive.com/38938760" />
      <bibkey>weng-etal-2020-towards</bibkey>
    </paper>
    <paper id="217">
      <title>Can Automatic Post-Editing Improve NMT?<fixed-case>NMT</fixed-case>?</title>
      <author><first>Shamil</first><last>Chollampatt</last></author>
      <author><first>Raymond Hendy</first><last>Susanto</last></author>
      <author><first>Liling</first><last>Tan</last></author>
      <author><first>Ewa</first><last>Szymanska</last></author>
      <pages>2736–2746</pages>
      <abstract>Automatic post-editing (APE) aims to improve <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translations</a>, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">artificial training data</a>, and <a href="https://en.wikipedia.org/wiki/Domain_specificity">domain specificity</a> for the APE task. We release this new <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> under CC BY-NC-SA 4.0 license at https://github.com/shamilcm/pedra.</abstract>
      <url hash="6af71e80">2020.emnlp-main.217</url>
      <doi>10.18653/v1/2020.emnlp-main.217</doi>
      <video href="https://slideslive.com/38938800" />
      <bibkey>chollampatt-etal-2020-automatic</bibkey>
      <pwccode url="https://github.com/shamilcm/pedra" additional="false">shamilcm/pedra</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/subedits">SubEdits</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="220">
      <title>Some Languages Seem Easier to Parse Because Their Treebanks Leak</title>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>2765–2770</pages>
      <abstract>Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences. We point at a factor not previously discussed : If we abstract away from words and dependency labels, how many <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a> in the test data were seen in the training data? We compute graph isomorphisms, and show that, treebank size aside, overlap between training and test graphs explain more of the observed variation than standard explanations such as the above.</abstract>
      <url hash="fd0bc433">2020.emnlp-main.220</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8f016362">2020.emnlp-main.220.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.220</doi>
      <video href="https://slideslive.com/38938710" />
      <bibkey>sogaard-2020-languages</bibkey>
    </paper>
    <paper id="222">
      <title>Modularized Syntactic Neural Networks for Sentence Classification</title>
      <author><first>Haiyan</first><last>Wu</last></author>
      <author><first>Ying</first><last>Liu</last></author>
      <author><first>Shaoyun</first><last>Shi</last></author>
      <pages>2786–2792</pages>
      <abstract>This paper focuses on tree-based modeling for the sentence classification task. In existing works, aggregating on a <a href="https://en.wikipedia.org/wiki/Syntax_tree">syntax tree</a> usually considers local information of sub-trees. In contrast, in addition to the local information, our proposed Modularized Syntactic Neural Network (MSNN) utilizes the syntax category labels and takes advantage of the global context while modeling sub-trees. In MSNN, each <a href="https://en.wikipedia.org/wiki/Node_(computer_science)">node</a> of a <a href="https://en.wikipedia.org/wiki/Syntax_tree">syntax tree</a> is modeled by a label-related syntax module. Each syntax module aggregates the outputs of lower-level modules, and finally, the root module provides the <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence representation</a>. We design a tree-parallel mini-batch strategy for efficient <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training</a> and predicting. Experimental results on four benchmark datasets show that our MSNN significantly outperforms previous state-of-the-art tree-based methods on the sentence classification task.</abstract>
      <url hash="0abda388">2020.emnlp-main.222</url>
      <doi>10.18653/v1/2020.emnlp-main.222</doi>
      <video href="https://slideslive.com/38938796" />
      <bibkey>wu-etal-2020-modularized</bibkey>
    </paper>
    <paper id="226">
      <title>MEGATRON-CNTRL : Controllable Story Generation with External Knowledge Using Large-Scale Language Models<fixed-case>MEGATRON</fixed-case>-<fixed-case>CNTRL</fixed-case>: Controllable Story Generation with External Knowledge Using Large-Scale Language Models</title>
      <author><first>Peng</first><last>Xu</last></author>
      <author><first>Mostofa</first><last>Patwary</last></author>
      <author><first>Mohammad</first><last>Shoeybi</last></author>
      <author><first>Raul</first><last>Puri</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <author><first>Anima</first><last>Anandkumar</last></author>
      <author><first>Bryan</first><last>Catanzaro</last></author>
      <pages>2831–2845</pages>
      <abstract>Existing pre-trained large language models have shown unparalleled generative capabilities. However, <a href="https://en.wikipedia.org/wiki/Copula_(linguistics)">they</a> are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a <a href="https://en.wikipedia.org/wiki/Knowledge_retrieval">knowledge retriever</a>, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from <a href="https://en.wikipedia.org/wiki/Sentence_embedding">sentence embedding</a>. The empirical results show that our model generates more fluent, consistent, and coherent stories with less <a href="https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)">repetition</a> and higher <a href="https://en.wikipedia.org/wiki/Multiculturalism">diversity</a> compared to prior work on the ROC story dataset. We showcase the controllability of our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> by replacing the <a href="https://en.wikipedia.org/wiki/Index_term">keywords</a> used to generate stories and re-running the generation process. Human evaluation results show that 77.5 % of these stories are successfully controlled by the new <a href="https://en.wikipedia.org/wiki/Index_term">keywords</a>. Furthermore, by scaling our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the <a href="https://en.wikipedia.org/wiki/Quality_(business)">quality of generation</a> (from 74.5 % to 93.0 % for <a href="https://en.wikipedia.org/wiki/Consistency_(statistics)">consistency</a>) and <a href="https://en.wikipedia.org/wiki/Controllability">controllability</a> (from 77.5 % to 91.5 %).</abstract>
      <url hash="2f263baa">2020.emnlp-main.226</url>
      <doi>10.18653/v1/2020.emnlp-main.226</doi>
      <video href="https://slideslive.com/38938958" />
      <bibkey>xu-etal-2020-megatron</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="228">
      <title>Improving Grammatical Error Correction Models with Purpose-Built Adversarial Examples</title>
      <author><first>Lihao</first><last>Wang</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last></author>
      <pages>2858–2869</pages>
      <abstract>A sequence-to-sequence (seq2seq) learning with <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one. However, the performance of GEC models with the seq2seq framework heavily relies on the size and quality of the corpus on hand. We propose a method inspired by adversarial training to generate more meaningful and valuable training examples by continually identifying the weak spots of a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>, and to enhance the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> by gradually adding the generated adversarial examples to the training set. Extensive experimental results show that such adversarial training can improve both the generalization and robustness of GEC models.</abstract>
      <url hash="eaab2617">2020.emnlp-main.228</url>
      <doi>10.18653/v1/2020.emnlp-main.228</doi>
      <video href="https://slideslive.com/38939097" />
      <bibkey>wang-zheng-2020-improving</bibkey>
    </paper>
    <paper id="231">
      <title>Multilingual AMR-to-Text Generation<fixed-case>AMR</fixed-case>-to-Text Generation</title>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>2889–2901</pages>
      <abstract>Generating text from <a href="https://en.wikipedia.org/wiki/Structured_data">structured data</a> is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an additional challenge : that of generating into languages with varied word order and <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological properties</a>. In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into <a href="https://en.wikipedia.org/wiki/English_language">English</a>. We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages. Our multilingual models surpass <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> that generate into one language in eighteen languages, based on <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">automatic metrics</a>. We analyze the ability of our multilingual models to accurately capture <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphology</a> and word order using human evaluation, and find that <a href="https://en.wikipedia.org/wiki/First_language">native speakers</a> judge our generations to be fluent.</abstract>
      <url hash="86ee8e88">2020.emnlp-main.231</url>
      <doi>10.18653/v1/2020.emnlp-main.231</doi>
      <video href="https://slideslive.com/38938791" />
      <bibkey>fan-gardent-2020-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
    </paper>
    <paper id="232">
      <title>Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation</title>
      <author><first>Francisco</first><last>Vargas</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>2902–2913</pages>
      <abstract>Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. Their method takes pre-trained <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> as input and attempts to isolate a <a href="https://en.wikipedia.org/wiki/Linear_subspace">linear subspace</a> that captures most of the <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates <a href="https://en.wikipedia.org/wiki/Sexism">gender bias</a> in the <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a>. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> and analyze empirically whether the bias subspace is actually linear. Our analysis shows that <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> is in fact well captured by a <a href="https://en.wikipedia.org/wiki/Linear_subspace">linear subspace</a>, justifying the assumption of Bolukbasi et al.</abstract>
      <url hash="f8bfabaa">2020.emnlp-main.232</url>
      <attachment type="OptionalSupplementaryMaterial" hash="a8b366a4">2020.emnlp-main.232.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.232</doi>
      <video href="https://slideslive.com/38938828" />
      <bibkey>vargas-cotterell-2020-exploring</bibkey>
      <pwccode url="https://github.com/franciscovargas/Bias_space_study" additional="false">franciscovargas/Bias_space_study</pwccode>
    </paper>
    <paper id="234">
      <title>Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models<fixed-case>D</fixed-case>irichlet Process Topic Models</title>
      <author><first>Alexander</first><last>Terenin</last></author>
      <author><first>Måns</first><last>Magnusson</last></author>
      <author><first>Leif</first><last>Jonsson</last></author>
      <pages>2925–2934</pages>
      <abstract>To scale non-parametric extensions of probabilistic topic models such as <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet allocation</a> to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study <a href="https://en.wikipedia.org/wiki/Data_parallelism">data-parallel training</a> for the hierarchical Dirichlet process (HDP) topic model. Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model. This <a href="https://en.wikipedia.org/wiki/Sampler_(musical_instrument)">sampler</a> utilizes all available sources of sparsity found in natural language-an important way to make computation efficient. We benchmark our method on a well-known corpus (PubMed) with 8 m documents and 768 m tokens, using a single multi-core machine in under four days.</abstract>
      <url hash="3b66e697">2020.emnlp-main.234</url>
      <attachment type="OptionalSupplementaryMaterial" hash="4f5f3c7d">2020.emnlp-main.234.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.234</doi>
      <video href="https://slideslive.com/38938922" />
      <bibkey>terenin-etal-2020-sparse</bibkey>
      <pwccode url="https://github.com/aterenin/Parallel-HDP-Experiments" additional="false">aterenin/Parallel-HDP-Experiments</pwccode>
    </paper>
    <paper id="238">
      <title>Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction</title>
      <author><first>Xu</first><last>Zhao</last></author>
      <author><first>Zihao</first><last>Wang</last></author>
      <author><first>Hao</first><last>Wu</last></author>
      <author><first>Yong</first><last>Zhang</last></author>
      <pages>2973–2984</pages>
      <abstract>Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations. However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further improvement of their performance. In this paper, we propose a new semi-supervised BLI framework to encourage the interaction between the <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised signal</a> and <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised alignment</a>. We design two message-passing mechanisms to transfer knowledge between annotated and non-annotated data, named prior optimal transport and bi-directional lexicon update respectively. Then, we perform <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised learning</a> based on a cyclic or a parallel parameter feeding routine to update our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> is a general framework that can incorporate any supervised and unsupervised BLI methods based on <a href="https://en.wikipedia.org/wiki/Optimal_transport">optimal transport</a>. Experimental results on MUSE and VecMap datasets show significant improvement of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Ablation study also proves that the two-way interaction between the supervised signal and unsupervised alignment accounts for the gain of the overall performance. Results on distant language pairs further illustrate the advantage and robustness of our proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a>.</abstract>
      <url hash="4c69eab2">2020.emnlp-main.238</url>
      <attachment type="OptionalSupplementaryMaterial" hash="659d06b6">2020.emnlp-main.238.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.238</doi>
      <video href="https://slideslive.com/38939286" />
      <bibkey>zhao-etal-2020-semi</bibkey>
      <pwccode url="https://github.com/BestActionNow/SemiSupBLI" additional="false">BestActionNow/SemiSupBLI</pwccode>
    </paper>
    <paper id="242">
      <title>BERT-EMD : Many-to-Many Layer Mapping for BERT Compression with Earth Mover’s Distance<fixed-case>BERT</fixed-case>-<fixed-case>EMD</fixed-case>: Many-to-Many Layer Mapping for <fixed-case>BERT</fixed-case> Compression with Earth Mover’s Distance</title>
      <author><first>Jianquan</first><last>Li</last></author>
      <author><first>Xiaokang</first><last>Liu</last></author>
      <author><first>Honghong</first><last>Zhao</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Yaohong</first><last>Jin</last></author>
      <pages>3009–3018</pages>
      <abstract>Pre-trained language models (e.g., BERT) have achieved significant success in various <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP) tasks</a>. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices. In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers. In this way, our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> can learn from different teacher layers adaptively for different NLP tasks. In addition, we leverage Earth Mover’s Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network. EMD enables effective <a href="https://en.wikipedia.org/wiki/Matching_(graph_theory)">matching</a> for the many-to-many layer mapping. Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s performance and accelerate convergence time. Extensive experiments on GLUE benchmark demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves competitive performance compared to strong competitors in terms of both <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and <a href="https://en.wikipedia.org/wiki/Mathematical_model">model compression</a></abstract>
      <url hash="0cb77ffe">2020.emnlp-main.242</url>
      <doi>10.18653/v1/2020.emnlp-main.242</doi>
      <video href="https://slideslive.com/38938812" />
      <bibkey>li-etal-2020-bert</bibkey>
      <pwccode url="https://github.com/lxk00/BERT-EMD" additional="false">lxk00/BERT-EMD</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="243">
      <title>Slot Attention with Value Normalization for Multi-Domain Dialogue State Tracking</title>
      <author><first>Yexiang</first><last>Wang</last></author>
      <author><first>Yi</first><last>Guo</last></author>
      <author><first>Siqi</first><last>Zhu</last></author>
      <pages>3019–3028</pages>
      <abstract>Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes : choosing <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> without <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a> or embedding <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a> in <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> leading to over-dependence. In this paper, we propose a new architecture to cleverly exploit <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a>, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN. Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value. SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span. VN is designed specifically for the use of <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a>, which can convert supporting spans to the values. Empirical results demonstrate that SAVN achieves the state-of-the-art <a href="https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)">joint accuracy</a> of 54.52 % on MultiWOZ 2.0 and 54.86 % on MultiWOZ 2.1. Besides, we evaluate VN with incomplete ontology. The results show that even if only 30 % <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a> is used, VN can also contribute to our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>.</abstract>
      <url hash="0d972944">2020.emnlp-main.243</url>
      <doi>10.18653/v1/2020.emnlp-main.243</doi>
      <video href="https://slideslive.com/38938817" />
      <bibkey>wang-etal-2020-slot</bibkey>
    </paper>
    <paper id="246">
      <title>Learning a Cost-Effective Annotation Policy for Question Answering<fixed-case>C</fixed-case>ost-<fixed-case>E</fixed-case>ffective <fixed-case>A</fixed-case>nnotation <fixed-case>P</fixed-case>olicy for <fixed-case>Q</fixed-case>uestion <fixed-case>A</fixed-case>nswering</title>
      <author><first>Bernhard</first><last>Kratzwald</last></author>
      <author><first>Stefan</first><last>Feuerriegel</last></author>
      <author><first>Huan</first><last>Sun</last></author>
      <pages>3051–3062</pages>
      <abstract>State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA systems</a> is challenging. As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme. The latter reduces the human effort : <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> leverages the underlying <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA system</a> to suggest potential candidate annotations. Human annotators then simply provide binary feedback on these candidates. Our <a href="https://en.wikipedia.org/wiki/System">system</a> is designed such that past annotations continuously improve the future performance and thus overall annotation cost. To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost. We compare our <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> against traditional manual annotations in an extensive set of experiments. We find that our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> can reduce up to 21.1 % of the annotation cost.</abstract>
      <url hash="6dedbff4">2020.emnlp-main.246</url>
      <doi>10.18653/v1/2020.emnlp-main.246</doi>
      <bibkey>kratzwald-etal-2020-learning</bibkey>
      <pwccode url="https://github.com/bernhard2202/qa-annotation" additional="false">bernhard2202/qa-annotation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="247">
      <title>Scene Restoring for Narrative Machine Reading Comprehension</title>
      <author><first>Zhixing</first><last>Tian</last></author>
      <author><first>Yuanzhe</first><last>Zhang</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Yantao</first><last>Jia</last></author>
      <author><first>Zhicheng</first><last>Sheng</last></author>
      <pages>3063–3073</pages>
      <abstract>This paper focuses on machine reading comprehension for <a href="https://en.wikipedia.org/wiki/Narrative">narrative passages</a>. Narrative passages usually describe a <a href="https://en.wikipedia.org/wiki/Chain_of_events">chain of events</a>. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively. Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension. Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>. We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice. Our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> achieves state-of-the-art.</abstract>
      <url hash="d2d79ea7">2020.emnlp-main.247</url>
      <doi>10.18653/v1/2020.emnlp-main.247</doi>
      <bibkey>tian-etal-2020-scene</bibkey>
    </paper>
    <paper id="251">
      <title>Incorporating Behavioral Hypotheses for Query Generation</title>
      <author><first>Ruey-Cheng</first><last>Chen</last></author>
      <author><first>Chia-Jung</first><last>Lee</last></author>
      <pages>3105–3110</pages>
      <abstract>Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time. User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns. This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice. Our experimental results show that the proposed approach leads to significant improvements on top-k word error rate and Bert F1 Score compared to a recent BART model.</abstract>
      <url hash="20afe812">2020.emnlp-main.251</url>
      <doi>10.18653/v1/2020.emnlp-main.251</doi>
      <video href="https://slideslive.com/38939310" />
      <bibkey>chen-lee-2020-incorporating</bibkey>
    </paper>
    <paper id="252">
      <title>Conditional Causal Relationships between Emotions and Causes in Texts</title>
      <author><first>Xinhong</first><last>Chen</last></author>
      <author><first>Qing</first><last>Li</last></author>
      <author><first>Jianping</first><last>Wang</last></author>
      <pages>3111–3121</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Causality">causal relationships</a> between emotions and causes in text have recently received a lot of attention. Most of the existing works focus on the extraction of the causally related clauses from <a href="https://en.wikipedia.org/wiki/Document">documents</a>. However, none of these works has considered the possibility that the <a href="https://en.wikipedia.org/wiki/Causality">causal relationships</a> among the extracted emotion and cause clauses may only be valid under a specific context, without which the extracted clauses may not be causally related. To address such an issue, we propose a new task of determining whether or not an input pair of emotion and cause has a valid causal relationship under different contexts, and construct a corresponding <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> via manual annotation and negative sampling based on an existing benchmark dataset. Furthermore, we propose a prediction aggregation module with low <a href="https://en.wikipedia.org/wiki/Overhead_(computing)">computational overhead</a> to fine-tune the prediction results based on the characteristics of the input clauses. Experiments demonstrate the effectiveness and generality of our aggregation module.</abstract>
      <url hash="e0d3dd69">2020.emnlp-main.252</url>
      <attachment type="OptionalSupplementaryMaterial" hash="aba47534">2020.emnlp-main.252.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.252</doi>
      <video href="https://slideslive.com/38938729" />
      <bibkey>chen-etal-2020-conditional</bibkey>
    </paper>
    <paper id="261">
      <title>Towards Interpreting BERT for Reading Comprehension Based QA<fixed-case>BERT</fixed-case> for Reading Comprehension Based <fixed-case>QA</fixed-case></title>
      <author><first>Sahana</first><last>Ramnath</last></author>
      <author><first>Preksha</first><last>Nema</last></author>
      <author><first>Deep</first><last>Sahni</last></author>
      <author><first>Mitesh M.</first><last>Khapra</last></author>
      <pages>3236–3242</pages>
      <abstract>BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic information</a> being captured in BERT. However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering. In this work, we attempt to interpret BERT for RCQA. Since BERT layers do not have predefined roles, we define a layer’s role or functionality using Integrated Gradients. Based on the defined roles, we perform a preliminary analysis across all layers. We observed that the initial <a href="https://en.wikipedia.org/wiki/Abstraction_layer">layers</a> focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction. Specifically for quantifier questions (how much / how many), we notice that BERT focuses on <a href="https://en.wikipedia.org/wiki/Word_sense">confusing words</a> (i.e., on other numerical quantities in the passage) in the later <a href="https://en.wikipedia.org/wiki/Complexity">layers</a>, but still manages to predict the answer correctly. The fine-tuning and analysis scripts will be publicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA.</abstract>
      <url hash="3d4c397d">2020.emnlp-main.261</url>
      <doi>10.18653/v1/2020.emnlp-main.261</doi>
      <video href="https://slideslive.com/38939356" />
      <bibkey>ramnath-etal-2020-towards</bibkey>
      <pwccode url="https://github.com/iitmnlp/BERT-Analysis-RCQA" additional="false">iitmnlp/BERT-Analysis-RCQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/duorc">DuoRC</pwcdataset>
    </paper>
    <paper id="262">
      <title>How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking</title>
      <author><first>Nicola</first><last>De Cao</last></author>
      <author><first>Michael Sejr</first><last>Schlichtkrull</last></author>
      <author><first>Wilker</first><last>Aziz</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>3243–3255</pages>
      <abstract>Attribution methods assess the contribution of inputs to the <a href="https://en.wikipedia.org/wiki/Prediction">model prediction</a>. One way to do so is erasure : a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure’s objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the <a href="https://en.wikipedia.org/wiki/Hindsight_bias">hindsight bias</a> : the fact that an input can be dropped does not mean that the model ‘knows’ it can be dropped. The resulting <a href="https://en.wikipedia.org/wiki/Pruning">pruning</a> is over-aggressive and does not reflect how the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DiffMask learns to mask-out subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> based on intermediate hidden layers of the analyzed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. First, this makes the approach efficient because we predict rather than <a href="https://en.wikipedia.org/wiki/Search_algorithm">search</a>. Second, as with probing classifiers, this reveals what the network ‘knows’ at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across <a href="https://en.wikipedia.org/wiki/Network_layer">network layers</a>. We use DiffMask to study BERT models on sentiment classification and <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>.</abstract>
      <url hash="516daf22">2020.emnlp-main.262</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8d545a99">2020.emnlp-main.262.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.262</doi>
      <video href="https://slideslive.com/38938648" />
      <bibkey>de-cao-etal-2020-decisions</bibkey>
      <pwccode url="https://github.com/nicola-decao/diffmask" additional="false">nicola-decao/diffmask</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="263">
      <title>A Diagnostic Study of Explainability Techniques for Text Classification</title>
      <author><first>Pepa</first><last>Atanasova</last></author>
      <author><first>Jakob Grue</first><last>Simonsen</last></author>
      <author><first>Christina</first><last>Lioma</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>3256–3274</pages>
      <abstract>Recent developments in <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> have introduced <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>’ predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a <a href="https://en.wikipedia.org/wiki/Software_development_process">technique</a> given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such <a href="https://en.wikipedia.org/wiki/Software_development_process">technique</a>. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed <a href="https://en.wikipedia.org/wiki/List_(abstract_data_type)">list</a> to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model’s performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.</abstract>
      <url hash="f3716cb1">2020.emnlp-main.263</url>
      <doi>10.18653/v1/2020.emnlp-main.263</doi>
      <video href="https://slideslive.com/38938813" />
      <bibkey>atanasova-etal-2020-diagnostic</bibkey>
      <pwccode url="https://github.com/copenlu/xai-benchmark" additional="false">copenlu/xai-benchmark</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/e-snli">e-SNLI</pwcdataset>
    </paper>
    <paper id="265">
      <title>Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering</title>
      <author><first>Zujie</first><last>Liang</last></author>
      <author><first>Weitao</first><last>Jiang</last></author>
      <author><first>Haifeng</first><last>Hu</last></author>
      <author><first>Jiaying</first><last>Zhu</last></author>
      <pages>3285–3292</pages>
      <abstract>In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have been proposed to alleviate this problem. However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized. Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples. With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of the VQA model are boosted significantly. We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model’s robustness.</abstract>
      <url hash="0bb940e2">2020.emnlp-main.265</url>
      <doi>10.18653/v1/2020.emnlp-main.265</doi>
      <video href="https://slideslive.com/38938860" />
      <bibkey>liang-etal-2020-learning</bibkey>
      <pwccode url="https://github.com/jokieleung/CL-VQA" additional="false">jokieleung/CL-VQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/vqa-cp">VQA-CP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="266">
      <title>Learning Physical Common Sense as Knowledge Graph Completion via BERT Data Augmentation and Constrained Tucker Factorization<fixed-case>P</fixed-case>hysical <fixed-case>C</fixed-case>ommon <fixed-case>S</fixed-case>ense as <fixed-case>K</fixed-case>nowledge <fixed-case>G</fixed-case>raph <fixed-case>C</fixed-case>ompletion via <fixed-case>BERT</fixed-case> <fixed-case>D</fixed-case>ata <fixed-case>A</fixed-case>ugmentation and <fixed-case>C</fixed-case>onstrained <fixed-case>T</fixed-case>ucker <fixed-case>F</fixed-case>actorization</title>
      <author><first>Zhenjie</first><last>Zhao</last></author>
      <author><first>Evangelos</first><last>Papalexakis</last></author>
      <author><first>Xiaojuan</first><last>Ma</last></author>
      <pages>3293–3298</pages>
      <abstract>Physical common sense plays an essential role in the cognition abilities of robots for <a href="https://en.wikipedia.org/wiki/Human–robot_interaction">human-robot interaction</a>. Machine learning methods have shown promising results on physical commonsense learning in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> but still suffer from model generalization. In this paper, we formulate physical commonsense learning as a knowledge graph completion problem to better use the <a href="https://en.wikipedia.org/wiki/Latent_variable">latent relationships</a> among training samples. Compared with completing general knowledge graphs, completing a physical commonsense knowledge graph has three unique characteristics : training data are scarce, not all facts can be mined from existing texts, and the number of relationships is small. To deal with these problems, we first use a pre-training language model BERT to augment training data, and then employ constrained tucker factorization to model complex relationships by constraining types and adding negative relationships. We compare our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> with existing state-of-the-art knowledge graph embedding methods and show its superior performance.</abstract>
      <url hash="7bd77f5f">2020.emnlp-main.266</url>
      <doi>10.18653/v1/2020.emnlp-main.266</doi>
      <video href="https://slideslive.com/38938674" />
      <bibkey>zhao-etal-2020-learning</bibkey>
    </paper>
    <paper id="267">
      <title>A Visually-grounded First-person Dialogue Dataset with Verbal and Non-verbal Responses</title>
      <author><first>Hisashi</first><last>Kamezawa</last></author>
      <author><first>Noriki</first><last>Nishida</last></author>
      <author><first>Nobuyuki</first><last>Shimizu</last></author>
      <author><first>Takashi</first><last>Miyazaki</last></author>
      <author><first>Hideki</first><last>Nakayama</last></author>
      <pages>3299–3310</pages>
      <abstract>In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in <a href="https://en.wikipedia.org/wiki/Social_relation">social interactions</a>. In this paper, we propose a visually-grounded first-person dialogue (VFD) dataset with verbal and non-verbal responses. The VFD dataset provides manually annotated (1) first-person images of agents, (2) utterances of human speakers, (3) eye-gaze locations of the speakers, and (4) the agents’ verbal and non-verbal responses. We present experimental results obtained using the proposed VFD dataset and recent <a href="https://en.wikipedia.org/wiki/Neural_network">neural network models</a> (e.g., BERT, ResNet). The results demonstrate that first-person vision helps <a href="https://en.wikipedia.org/wiki/Neural_circuit">neural network models</a> correctly understand human intentions, and the production of non-verbal responses is a challenging task like that of verbal responses. Our dataset is publicly available.</abstract>
      <url hash="21333207">2020.emnlp-main.267</url>
      <doi>10.18653/v1/2020.emnlp-main.267</doi>
      <video href="https://slideslive.com/38939228" />
      <bibkey>kamezawa-etal-2020-visually</bibkey>
    </paper>
    <paper id="271">
      <title>Sub-Instruction Aware Vision-and-Language Navigation</title>
      <author><first>Yicong</first><last>Hong</last></author>
      <author><first>Cristian</first><last>Rodriguez</last></author>
      <author><first>Qi</first><last>Wu</last></author>
      <author><first>Stephen</first><last>Gould</last></author>
      <pages>3360–3376</pages>
      <abstract>Vision-and-language navigation requires an agent to navigate through a real 3D environment following <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language instructions</a>. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agent</a>’s performance at following each part of the instruction can not be assessed during <a href="https://en.wikipedia.org/wiki/Navigation">navigation</a>. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agents</a> with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective <a href="https://en.wikipedia.org/wiki/Instruction_set_architecture">sub-instruction attention and shifting modules</a> that select and attend to a single <a href="https://en.wikipedia.org/wiki/Instruction_set_architecture">sub-instruction</a> at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents. We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.</abstract>
      <url hash="836e30b5">2020.emnlp-main.271</url>
      <doi>10.18653/v1/2020.emnlp-main.271</doi>
      <video href="https://slideslive.com/38938820" />
      <bibkey>hong-etal-2020-sub</bibkey>
      <pwccode url="https://github.com/YicongHong/Fine-Grained-R2R" additional="false">YicongHong/Fine-Grained-R2R</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fine-grained-r2r">Fine-Grained R2R</pwcdataset>
    </paper>
    <paper id="278">
      <title>Task-Completion Dialogue Policy Learning via <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo Tree Search</a> with Dueling Network<fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Tree Search with Dueling Network</title>
      <author><first>Sihan</first><last>Wang</last></author>
      <author><first>Kaijie</first><last>Zhou</last></author>
      <author><first>Kunfeng</first><last>Lai</last></author>
      <author><first>Jianping</first><last>Shen</last></author>
      <pages>3461–3471</pages>
      <abstract>We introduce a framework of <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo Tree Search</a> with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo simulations</a> and is robust to the simulation errors. Such idea arises naturally in <a href="https://en.wikipedia.org/wiki/Human_behavior">human behaviors</a>, e.g. predicting others’ responses and then deciding our own actions. In the simulated movie-ticket booking task, our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms the background planning approaches significantly. We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.</abstract>
      <url hash="c22d92b4">2020.emnlp-main.278</url>
      <doi>10.18653/v1/2020.emnlp-main.278</doi>
      <video href="https://slideslive.com/38938752" />
      <bibkey>wang-etal-2020-task</bibkey>
    </paper>
    <paper id="289">
      <title>Emotion-Cause Pair Extraction as Sequence Labeling Based on A Novel Tagging Scheme</title>
      <author><first>Chaofa</first><last>Yuan</last></author>
      <author><first>Chuang</first><last>Fan</last></author>
      <author><first>Jianzhu</first><last>Bao</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>3568–3573</pages>
      <abstract>The task of emotion-cause pair extraction deals with finding all <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> and the corresponding causes in unannotated emotion texts. Most recent studies are based on the likelihood of Cartesian product among all clause candidates, resulting in a high computational cost. Targeting this issue, we regard the task as a sequence labeling problem and propose a novel tagging scheme with coding the distance between linked components into the tags, so that <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> and the corresponding causes can be extracted simultaneously. Accordingly, an end-to-end model is presented to process the input texts from left to right, always with <a href="https://en.wikipedia.org/wiki/Time_complexity">linear time complexity</a>, leading to a speed up. Experimental results show that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves the best performance, outperforming the state-of-the-art <a href="https://en.wikipedia.org/wiki/Scientific_method">method</a> by 2.26 % (p0.001) in <a href="https://en.wikipedia.org/wiki/F-number">F1 measure</a>.</abstract>
      <url hash="f474678e">2020.emnlp-main.289</url>
      <doi>10.18653/v1/2020.emnlp-main.289</doi>
      <video href="https://slideslive.com/38939311" />
      <bibkey>yuan-etal-2020-emotion</bibkey>
    </paper>
    <paper id="291">
      <title>Multi-modal Multi-label Emotion Detection with Modality and Label Dependence</title>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Xincheng</first><last>Ju</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Shoushan</first><last>Li</last></author>
      <author><first>Qiaoming</first><last>Zhu</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>3584–3593</pages>
      <abstract>As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (e.g., textual modality). In this paper, we focus on multi-label emotion detection in a <a href="https://en.wikipedia.org/wiki/Multimodal_interaction">multi-modal scenario</a>. In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality dependence). Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a>.</abstract>
      <url hash="53127218">2020.emnlp-main.291</url>
      <doi>10.18653/v1/2020.emnlp-main.291</doi>
      <video href="https://slideslive.com/38938701" />
      <bibkey>zhang-etal-2020-multi</bibkey>
    </paper>
    <paper id="303">
      <title>Global-to-Local Neural Networks for Document-Level Relation Extraction</title>
      <author><first>Difeng</first><last>Wang</last></author>
      <author><first>Wei</first><last>Hu</last></author>
      <author><first>Ermei</first><last>Cao</last></author>
      <author><first>Weijian</first><last>Sun</last></author>
      <pages>3711–3721</pages>
      <abstract>Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions.</abstract>
      <url hash="63b716cb">2020.emnlp-main.303</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8e0ad976">2020.emnlp-main.303.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.303</doi>
      <video href="https://slideslive.com/38938684" />
      <bibkey>wang-etal-2020-global</bibkey>
      <pwccode url="https://github.com/nju-websoft/GLRE" additional="false">nju-websoft/GLRE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="304">
      <title>Recurrent Interaction Network for Jointly Extracting Entities and Classifying Relations</title>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Samuel</first><last>Mensah</last></author>
      <author><first>Yongyi</first><last>Mao</last></author>
      <author><first>Xudong</first><last>Liu</last></author>
      <pages>3722–3732</pages>
      <abstract>The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the task-specific networks for prediction. However, such an approach hinders the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> from learning explicit interactions between the two tasks to improve the performance on the individual <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. As a solution, we design a multi-task learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>. Empirical studies on two real-world datasets confirm the superiority of the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>.</abstract>
      <url hash="d479f54c">2020.emnlp-main.304</url>
      <doi>10.18653/v1/2020.emnlp-main.304</doi>
      <video href="https://slideslive.com/38939355" />
      <bibkey>sun-etal-2020-recurrent</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="308">
      <title>Point to the Expression : Solving Algebraic Word Problems using the Expression-Pointer Transformer Model<fixed-case>P</fixed-case>oint to the <fixed-case>E</fixed-case>xpression: <fixed-case>S</fixed-case>olving <fixed-case>A</fixed-case>lgebraic <fixed-case>W</fixed-case>ord <fixed-case>P</fixed-case>roblems using the <fixed-case>E</fixed-case>xpression-<fixed-case>P</fixed-case>ointer <fixed-case>T</fixed-case>ransformer <fixed-case>M</fixed-case>odel</title>
      <author><first>Bugeun</first><last>Kim</last></author>
      <author><first>Kyung Seo</first><last>Ki</last></author>
      <author><first>Donggeon</first><last>Lee</last></author>
      <author><first>Gahgene</first><last>Gweon</last></author>
      <pages>3768–3779</pages>
      <abstract>Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using ‘Op (operator / operand)’ tokens as a unit of input / output. However, such a neural model suffered two issues : expression fragmentation and operand-context separation. To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) ‘Expression’ token and (2) operand-context pointers when generating solution equations. The performance of the EPT model is tested on three datasets : ALG514, DRAW-1 K, and MAWPS. Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> ; 81.3 % on ALG514, 59.5 % on DRAW-1 K, and 84.5 % on MAWPS. The contribution of this paper is two-fold ; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operand-context separation. (2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40 %.</abstract>
      <url hash="20db7f23">2020.emnlp-main.308</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5040778a">2020.emnlp-main.308.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.308</doi>
      <video href="https://slideslive.com/38938978" />
      <bibkey>kim-etal-2020-point</bibkey>
    </paper>
    <paper id="311">
      <title>Routing Enforced <a href="https://en.wikipedia.org/wiki/Generative_model">Generative Model</a> for Recipe Generation</title>
      <author><first>Zhiwei</first><last>Yu</last></author>
      <author><first>Hongyu</first><last>Zang</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>3797–3806</pages>
      <abstract>One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients. Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible. In this work, we propose a routing method to dive into the content selection under the internal restrictions. The routing enforced generative model (RGM) can generate appropriate <a href="https://en.wikipedia.org/wiki/Recipe">recipes</a> according to the given ingredients and user preferences. Our model yields new state-of-the-art results on the recipe generation task with significant improvements on BLEU, <a href="https://en.wikipedia.org/wiki/F-number">F1</a> and human evaluation.</abstract>
      <url hash="0a20286a">2020.emnlp-main.311</url>
      <doi>10.18653/v1/2020.emnlp-main.311</doi>
      <video href="https://slideslive.com/38939193" />
      <bibkey>yu-etal-2020-routing</bibkey>
    </paper>
    <paper id="316">
      <title>DagoBERT : Generating Derivational Morphology with a Pretrained Language Model<fixed-case>D</fixed-case>ago<fixed-case>BERT</fixed-case>: <fixed-case>G</fixed-case>enerating Derivational Morphology with a Pretrained Language Model</title>
      <author><first>Valentin</first><last>Hofmann</last></author>
      <author><first>Janet</first><last>Pierrehumbert</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>3848–3861</pages>
      <abstract>Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT’s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT’s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.</abstract>
      <url hash="eaf41c3c">2020.emnlp-main.316</url>
      <doi>10.18653/v1/2020.emnlp-main.316</doi>
      <video href="https://slideslive.com/38939116" />
      <bibkey>hofmann-etal-2020-dagobert</bibkey>
      <pwccode url="https://github.com/valentinhofmann/dagobert" additional="false">valentinhofmann/dagobert</pwccode>
    </paper>
    <paper id="318">
      <title>A Joint Multiple Criteria Model in <a href="https://en.wikipedia.org/wiki/Transfer_learning">Transfer Learning</a> for Cross-domain Chinese Word Segmentation<fixed-case>C</fixed-case>hinese Word Segmentation</title>
      <author><first>Kaiyu</first><last>Huang</last></author>
      <author><first>Degen</first><last>Huang</last></author>
      <author><first>Zhuang</first><last>Liu</last></author>
      <author><first>Fengran</first><last>Mo</last></author>
      <pages>3873–3882</pages>
      <abstract>Word-level information is important in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP)</a>, especially for the <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese language</a> due to its high linguistic complexity. Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks. Existing methods have already achieved a competitive performance for CWS on large-scale annotated corpora. However, the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of the method will drop dramatically when it handles an unsegmented text with lots of out-of-vocabulary (OOV) words. In addition, there are many different segmentation criteria for addressing different requirements of downstream NLP tasks. Excessive amounts of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> with saving different criteria will generate the explosive growth of the total parameters. To this end, we propose a joint multiple criteria model that shares all parameters to integrate different segmentation criteria into one <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. Besides, we utilize a transfer learning method to improve the performance of <a href="https://en.wikipedia.org/wiki/Object-oriented_programming">OOV words</a>. Our proposed method is evaluated by designing comprehensive experiments on multiple benchmark datasets (e.g., Bakeoff 2005, Bakeoff 2008 and SIGHAN 2010). Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> achieves the state-of-the-art performances on all datasets. Importantly, our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> also shows a competitive practicability and generalization ability for the CWS task.</abstract>
      <url hash="8a7695bf">2020.emnlp-main.318</url>
      <doi>10.18653/v1/2020.emnlp-main.318</doi>
      <video href="https://slideslive.com/38938808" />
      <bibkey>huang-etal-2020-joint-multiple</bibkey>
    </paper>
    <paper id="322">
      <title>Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling</title>
      <author><first>Diego</first><last>Marcheggiani</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>3915–3928</pages>
      <abstract>Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with <a href="https://en.wikipedia.org/wiki/Semantic_role">semantic roles</a>. Even though most semantic-role formalisms are built upon <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">constituent syntax</a>, and only <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">syntactic constituents</a> can be labeled as arguments (e.g., <a href="https://en.wikipedia.org/wiki/FrameNet">FrameNet</a> and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax. In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system. Nodes in our SpanGCN correspond to <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">constituents</a>. The computation is done in 3 stages. First, initial node representations are produced by ‘composing’ word representations of the first and last words in the <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">constituent</a>. Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations. Finally, the constituent representations are ‘decomposed’ back into word representations, which are used as input to the SRL classifier. We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard English SRL benchmarks CoNLL-2005, CoNLL-2012, and FrameNet.</abstract>
      <url hash="12c0b048">2020.emnlp-main.322</url>
      <doi>10.18653/v1/2020.emnlp-main.322</doi>
      <bibkey>marcheggiani-titov-2020-graph</bibkey>
      <pwccode url="https://github.com/diegma/span-gcn" additional="false">diegma/span-gcn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="325">
      <title>Supervised Seeded Iterated Learning for Interactive Language Learning</title>
      <author><first>Yuchen</first><last>Lu</last></author>
      <author><first>Soumye</first><last>Singhal</last></author>
      <author><first>Florian</first><last>Strub</last></author>
      <author><first>Olivier</first><last>Pietquin</last></author>
      <author><first>Aaron</first><last>Courville</last></author>
      <pages>3962–3970</pages>
      <abstract>Language drift has been one of the major obstacles to train <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> through <a href="https://en.wikipedia.org/wiki/Interaction">interaction</a>. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>. In recent literature, two general methods partially counter this phenomenon : Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL). While S2P jointly trains interactive and supervised losses to counter the drift, <a href="https://en.wikipedia.org/wiki/SIL_International">SIL</a> changes the training dynamics to prevent <a href="https://en.wikipedia.org/wiki/Language_drift">language drift</a> from occurring. In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on <a href="https://en.wikipedia.org/wiki/Text_corpus">human corpus</a>. Given these observations, we introduce Supervised Seeded Iterated Learning (SSIL) to combine both methods to minimize their respective weaknesses. We then show the effectiveness of in the language-drift translation game.</abstract>
      <url hash="1dfff7b2">2020.emnlp-main.325</url>
      <doi>10.18653/v1/2020.emnlp-main.325</doi>
      <video href="https://slideslive.com/38939148" />
      <bibkey>lu-etal-2020-supervised</bibkey>
    </paper>
    <paper id="334">
      <title>Compositional Demographic Word Embeddings</title>
      <author><first>Charles</first><last>Welch</last></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last></author>
      <author><first>Verónica</first><last>Pérez-Rosas</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>4076–4089</pages>
      <abstract>Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> performance and other language processing tasks, they can only be computed for people with a large amount of <a href="https://en.wikipedia.org/wiki/Panel_data">longitudinal data</a>, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for <a href="https://en.wikipedia.org/wiki/English_language">English</a> : <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a> and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.</abstract>
      <url hash="74c78b71">2020.emnlp-main.334</url>
      <doi>10.18653/v1/2020.emnlp-main.334</doi>
      <video href="https://slideslive.com/38939153" />
      <bibkey>welch-etal-2020-compositional</bibkey>
      <pwccode url="https://github.com/dbamman/geoSGLM" additional="false">dbamman/geoSGLM</pwccode>
    </paper>
    <paper id="335">
      <title>Are Undocumented Workers the Same as Illegal Aliens? Disentangling Denotation and Connotation in Vector Spaces<fixed-case>D</fixed-case>isentangling Denotation and Connotation in Vector Spaces</title>
      <author><first>Albert</first><last>Webson</last></author>
      <author><first>Zhizhong</first><last>Chen</last></author>
      <author><first>Carsten</first><last>Eickhoff</last></author>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <pages>4090–4105</pages>
      <abstract>In <a href="https://en.wikipedia.org/wiki/Politics">politics</a>, <a href="https://en.wikipedia.org/wiki/Neologism">neologisms</a> are frequently invented for partisan objectives. For example, <a href="https://en.wikipedia.org/wiki/Illegal_immigration_to_the_United_States">undocumented workers</a> and illegal aliens refer to the same group of people (i.e., they have the same denotation), but they carry clearly different <a href="https://en.wikipedia.org/wiki/Connotation">connotations</a>. Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists. In <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, however, popular pretrained models encode both <a href="https://en.wikipedia.org/wiki/Denotation">denotation</a> and <a href="https://en.wikipedia.org/wiki/Connotation">connotation</a> as one entangled representation. In this study, we propose an adversarial nerual netowrk that decomposes a pretrained representation as independent denotation and connotation representations. For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g., immigrants vs. aliens, estate tax vs. death tax) move closer to each other in denotation space while moving further apart in connotation space. For extrinsic application, we train an <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval system</a> with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.</abstract>
      <url hash="f2572eea">2020.emnlp-main.335</url>
      <revision id="1" href="2020.emnlp-main.335v1" hash="f9f0bc0c" />
      <revision id="2" href="2020.emnlp-main.335v2" hash="f2572eea" date="2020-11-29">Changed the title</revision>
      <doi>10.18653/v1/2020.emnlp-main.335</doi>
      <video href="https://slideslive.com/38939226" />
      <bibkey>webson-etal-2020-undocumented</bibkey>
    </paper>
    <paper id="341">
      <title>SLEDGE-Z : A Zero-Shot Baseline for COVID-19 Literature Search<fixed-case>SLEDGE-Z</fixed-case>: A Zero-Shot Baseline for <fixed-case>COVID</fixed-case>-19 Literature Search</title>
      <author><first>Sean</first><last>MacAvaney</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <author><first>Nazli</first><last>Goharian</last></author>
      <pages>4171–4179</pages>
      <abstract>With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these articles effectively. In this work, we present a zero-shot ranking algorithm that adapts to COVID-related scientific literature. Our approach filters training data from another <a href="https://en.wikipedia.org/wiki/Collection_(abstract_data_type)">collection</a> down to medical-related queries, uses a neural re-ranking model pre-trained on scientific text (SciBERT), and filters the target document collection. This approach ranks top among zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments. Despite not relying on TREC-COVID data, our method outperforms models that do. As one of the first search methods to thoroughly evaluate COVID-19 search, we hope that this serves as a strong baseline and helps in the global crisis.</abstract>
      <url hash="3f0f73ff">2020.emnlp-main.341</url>
      <doi>10.18653/v1/2020.emnlp-main.341</doi>
      <video href="https://slideslive.com/38939204" />
      <bibkey>macavaney-etal-2020-sledge</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="344">
      <title>Adversarial Semantic Collisions</title>
      <author><first>Congzheng</first><last>Song</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <author><first>Vitaly</first><last>Shmatikov</last></author>
      <pages>4198–4210</pages>
      <abstract>We study semantic collisions : texts that are semantically unrelated but judged as similar by <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP models</a>. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of textsincluding paraphrase identification, <a href="https://en.wikipedia.org/wiki/Document_retrieval">document retrieval</a>, response suggestion, and extractive summarizationare vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at.<i>semantic collisions</i>: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts—including paraphrase identification, document retrieval, response suggestion, and extractive summarization—are vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at <url>https://github.com/csong27/collision-bert</url>.</abstract>
      <url hash="815cfb60">2020.emnlp-main.344</url>
      <doi>10.18653/v1/2020.emnlp-main.344</doi>
      <video href="https://slideslive.com/38939027" />
      <bibkey>song-etal-2020-adversarial</bibkey>
      <pwccode url="https://github.com/csong27/collision-bert" additional="false">csong27/collision-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
    </paper>
    <paper id="345">
      <title>Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification</title>
      <author><first>Prithviraj</first><last>Sen</last></author>
      <author><first>Marina</first><last>Danilevsky</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <author><first>Siddhartha</first><last>Brahma</last></author>
      <author><first>Matthias</first><last>Boehm</last></author>
      <author><first>Laura</first><last>Chiticariu</last></author>
      <author><first>Rajasekar</first><last>Krishnamurthy</last></author>
      <pages>4211–4221</pages>
      <abstract>Interpretability of <a href="https://en.wikipedia.org/wiki/Predictive_modelling">predictive models</a> is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in <a href="https://en.wikipedia.org/wiki/First-order_logic">first-order logic</a>, a <a href="https://en.wikipedia.org/wiki/Dialect">dialect</a> with well-defined, human-understandable semantics. More precisely, RuleNN learns linguistic expressions (LE) built on top of <a href="https://en.wikipedia.org/wiki/Predicate_(grammar)">predicates</a> extracted using shallow natural language understanding. Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks. Our user studies confirm that the learned LEs are explainable and capture domain semantics. Moreover, allowing domain experts to modify LEs and instill more <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> leads to human-machine co-creation of models with better performance.</abstract>
      <url hash="a78613a0">2020.emnlp-main.345</url>
      <doi>10.18653/v1/2020.emnlp-main.345</doi>
      <bibkey>sen-etal-2020-learning</bibkey>
    </paper>
    <paper id="346">
      <title>AutoPrompt : Eliciting Knowledge from Language Models with Automatically Generated Prompts<fixed-case>A</fixed-case>uto<fixed-case>P</fixed-case>rompt: <fixed-case>E</fixed-case>liciting <fixed-case>K</fixed-case>nowledge from <fixed-case>L</fixed-case>anguage <fixed-case>M</fixed-case>odels with <fixed-case>A</fixed-case>utomatically <fixed-case>G</fixed-case>enerated <fixed-case>P</fixed-case>rompts</title>
      <author><first>Taylor</first><last>Shin</last></author>
      <author><first>Yasaman</first><last>Razeghi</last></author>
      <author><first>Robert L.</first><last>Logan IV</last></author>
      <author><first>Eric</first><last>Wallace</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>4222–4235</pages>
      <abstract>The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such <a href="https://en.wikipedia.org/wiki/Knowledge">knowledge</a>, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> and natural language inference without additional parameters or <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a>, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for <a href="https://en.wikipedia.org/wiki/Finetuning">finetuning</a>.</abstract>
      <url hash="5247d044">2020.emnlp-main.346</url>
      <doi>10.18653/v1/2020.emnlp-main.346</doi>
      <video href="https://slideslive.com/38939188" />
      <bibkey>shin-etal-2020-autoprompt</bibkey>
      <pwccode url="https://github.com/ucinlp/autoprompt" additional="true">ucinlp/autoprompt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="352">
      <title>Generating Dialogue Responses from a Semantic Latent Space</title>
      <author><first>Wei-Jen</first><last>Ko</last></author>
      <author><first>Avik</first><last>Ray</last></author>
      <author><first>Yilin</first><last>Shen</last></author>
      <author><first>Hongxia</first><last>Jin</last></author>
      <pages>4339–4349</pages>
      <abstract>Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. To address this issue, we propose an alternative to the end-to-end classification on <a href="https://en.wikipedia.org/wiki/Vocabulary">vocabulary</a>. We learn the pair relationship between the prompts and responses as a <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression task</a> on a latent space instead. In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space. Human evaluation showed that learning the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> on a <a href="https://en.wikipedia.org/wiki/Continuous_or_discrete_variable">continuous space</a> can generate responses that are both relevant and informative.</abstract>
      <url hash="c5e15cb9">2020.emnlp-main.352</url>
      <doi>10.18653/v1/2020.emnlp-main.352</doi>
      <video href="https://slideslive.com/38939280" />
      <bibkey>ko-etal-2020-generating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="355">
      <title>ALICE : <a href="https://en.wikipedia.org/wiki/Active_learning">Active Learning</a> with Contrastive Natural Language Explanations<fixed-case>ALICE</fixed-case>: Active Learning with Contrastive Natural Language Explanations</title>
      <author><first>Weixin</first><last>Liang</last></author>
      <author><first>James</first><last>Zou</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>4380–4391</pages>
      <abstract>Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface : classification labels, each of which only provides a few bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. AL-ICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> extracts knowledge from these <a href="https://en.wikipedia.org/wiki/Explanation">explanations</a> using a <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a>. Finally, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> incorporates the extracted knowledge through dynamically changing the <a href="https://en.wikipedia.org/wiki/Machine_learning">learning model</a>’s structure. We applied ALICEin two visual recognition tasks, bird species classification and <a href="https://en.wikipedia.org/wiki/Social_relation">social relationship classification</a>. We found by incorporating contrastive explanations, our <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> outperform baseline models that are trained with 40-100 % more training data. We found that adding1expla-nation leads to similar performance gain as adding 13-30 labeled training data points.</abstract>
      <url hash="a5e4762c">2020.emnlp-main.355</url>
      <doi>10.18653/v1/2020.emnlp-main.355</doi>
      <video href="https://slideslive.com/38938646" />
      <bibkey>liang-etal-2020-alice</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cub-200-2011">CUB-200-2011</pwcdataset>
    </paper>
    <paper id="366">
      <title>A Streaming Approach For Efficient Batched Beam Search</title>
      <author><first>Kevin</first><last>Yang</last></author>
      <author><first>Violet</first><last>Yao</last></author>
      <author><first>John</first><last>DeNero</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>4526–4535</pages>
      <abstract>We propose an efficient <a href="https://en.wikipedia.org/wiki/Batch_processing">batching strategy</a> for variable-length decoding on <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU architectures</a>. During decoding, when candidates terminate or are pruned according to <a href="https://en.wikipedia.org/wiki/Heuristic">heuristics</a>, our streaming approach periodically refills the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases <a href="https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)">runtime</a> by up to 71 % compared to a fixed-width beam search baseline and 17 % compared to a variable-width baseline, while matching baselines’ BLEU. Finally, experiments show that our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> can speed up decoding in other domains, such as <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic and syntactic parsing</a>.</abstract>
      <url hash="724fc7cc">2020.emnlp-main.366</url>
      <doi>10.18653/v1/2020.emnlp-main.366</doi>
      <video href="https://slideslive.com/38939191" />
      <bibkey>yang-etal-2020-streaming</bibkey>
      <pwccode url="https://github.com/yangkevin2/emnlp2020-stream-beam-mt" additional="false">yangkevin2/emnlp2020-stream-beam-mt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="375">
      <title>Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models</title>
      <author><first>Ethan</first><last>Wilcox</last></author>
      <author><first>Peng</first><last>Qian</last></author>
      <author><first>Richard</first><last>Futrell</last></author>
      <author><first>Ryosuke</first><last>Kohita</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <pages>4640–4652</pages>
      <abstract>Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts. We assess the ability of modern neural language models to reproduce this behavior in <a href="https://en.wikipedia.org/wiki/English_language">English</a> and evaluate the effect of structural supervision on learning outcomes. First, we assess few-shot learning capabilities by developing controlled experiments that probe models’ syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training. Second, we assess invariance properties of learned representation : the ability of a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to transfer syntactic generalizations from a base context (e.g., a simple declarative active-voice sentence) to a transformed context (e.g., an interrogative sentence). We test four models trained on the same dataset : an n-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision. We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model. All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.</abstract>
      <url hash="cb5d7aca">2020.emnlp-main.375</url>
      <doi>10.18653/v1/2020.emnlp-main.375</doi>
      <video href="https://slideslive.com/38939210" />
      <bibkey>wilcox-etal-2020-structural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/celex">CELEX</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="378">
      <title>Optimus : Organizing Sentences via Pre-trained Modeling of a Latent Space</title>
      <author><first>Chunyuan</first><last>Li</last></author>
      <author><first>Xiang</first><last>Gao</last></author>
      <author><first>Yuan</first><last>Li</last></author>
      <author><first>Baolin</first><last>Peng</last></author>
      <author><first>Xiujun</first><last>Li</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>4678–4699</pages>
      <abstract>When trained effectively, the Variational Autoencoder (VAE) can be both a powerful <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> and an effective representation learning framework for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a>. In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space). A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the <a href="https://en.wikipedia.org/wiki/Latent_vector">latent vectors</a>. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks.</abstract>
      <url hash="f49a10a1">2020.emnlp-main.378</url>
      <doi>10.18653/v1/2020.emnlp-main.378</doi>
      <video href="https://slideslive.com/38938906" />
      <bibkey>li-etal-2020-optimus</bibkey>
      <pwccode url="https://github.com/ChunyuanLI/Optimus" additional="false">ChunyuanLI/Optimus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="381">
      <title>RussianSuperGLUE : A Russian Language Understanding Evaluation Benchmark<fixed-case>R</fixed-case>ussian<fixed-case>S</fixed-case>uper<fixed-case>GLUE</fixed-case>: A <fixed-case>R</fixed-case>ussian Language Understanding Evaluation Benchmark</title>
      <author><first>Tatiana</first><last>Shavrina</last></author>
      <author><first>Alena</first><last>Fenogenova</last></author>
      <author><first>Emelyanov</first><last>Anton</last></author>
      <author><first>Denis</first><last>Shevelev</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <author><first>Valentin</first><last>Malykh</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Maria</first><last>Tikhonova</last></author>
      <author><first>Andrey</first><last>Chertok</last></author>
      <author><first>Andrey</first><last>Evlampiev</last></author>
      <pages>4717–4726</pages>
      <abstract>In this paper, we introduce an advanced Russian general language understanding evaluation benchmark   Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills-detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark</a> of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from scratch for the <a href="https://en.wikipedia.org/wiki/Russian_language">Russian language</a>. We also provide baselines, human level evaluation, open-source framework for evaluating <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, and an overall leaderboard of transformer models for the <a href="https://en.wikipedia.org/wiki/Russian_language">Russian language</a>. Besides, we present the first results of comparing multilingual models in the translated diagnostic test set and offer the first steps to further expanding or assessing State-of-the-art <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> independently of language.</abstract>
      <url hash="f6616264">2020.emnlp-main.381</url>
      <doi>10.18653/v1/2020.emnlp-main.381</doi>
      <video href="https://slideslive.com/38939106" />
      <bibkey>shavrina-etal-2020-russiansuperglue</bibkey>
      <pwccode url="https://github.com/RussianNLP/RussianSuperGLUE" additional="true">RussianNLP/RussianSuperGLUE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/danetqa">DaNetQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lidirus">LiDiRus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/muserc">MuSeRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/parus">PARus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rcb">RCB</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rwsd">RWSD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rucos">RuCoS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/terra">TERRa</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/russe">RUSSE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/decanlp">decaNLP</pwcdataset>
    </paper>
    <paper id="382">
      <title>An Empirical Study of Pre-trained Transformers for Arabic Information Extraction<fixed-case>A</fixed-case>rabic Information Extraction</title>
      <author><first>Wuwei</first><last>Lan</last></author>
      <author><first>Yang</first><last>Chen</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <author><first>Alan</first><last>Ritter</last></author>
      <pages>4727–4734</pages>
      <abstract>Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer. However, their performance on Arabic information extraction (IE) tasks is not very well studied. In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning. We study GigaBERT’s effectiveness on zero-short transfer across four IE tasks : <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a>, argument role labeling, and relation extraction. Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings. We have made our pre-trained models publicly available at : https://github.com/lanwuwei/GigaBERT.</abstract>
      <url hash="fb321231">2020.emnlp-main.382</url>
      <doi>10.18653/v1/2020.emnlp-main.382</doi>
      <video href="https://slideslive.com/38939107" />
      <bibkey>lan-etal-2020-empirical</bibkey>
      <pwccode url="https://github.com/lanwuwei/GigaBERT" additional="false">lanwuwei/GigaBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/panlex">Panlex</pwcdataset>
    </paper>
    <paper id="383">
      <title>TNT : Text Normalization based Pre-training of Transformers for Content Moderation<fixed-case>TNT</fixed-case>: Text Normalization based Pre-training of Transformers for Content Moderation</title>
      <author><first>Fei</first><last>Tan</last></author>
      <author><first>Yifan</first><last>Hu</last></author>
      <author><first>Changwei</first><last>Hu</last></author>
      <author><first>Keqian</first><last>Li</last></author>
      <author><first>Kevin</first><last>Yen</last></author>
      <pages>4735–4741</pages>
      <abstract>In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation. Inspired by the masking strategy and <a href="https://en.wikipedia.org/wiki/Text_normalization">text normalization</a>, TNT is developed to learn language representation by training transformers to reconstruct text from four operation types typically seen in text manipulation : substitution, transposition, <a href="https://en.wikipedia.org/wiki/Deletion_(linguistics)">deletion</a>, and insertion. Furthermore, the <a href="https://en.wikipedia.org/wiki/Data_normalization">normalization</a> involves the prediction of both operation types and token labels, enabling TNT to learn from more challenging tasks than the standard task of masked word recovery. As a result, the experiments demonstrate that <a href="https://en.wikipedia.org/wiki/TNT">TNT</a> outperforms strong <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> on the hate speech classification task. Additional <a href="https://en.wikipedia.org/wiki/Text_normalization">text normalization</a> experiments and case studies show that TNT is a new potential approach to misspelling correction.</abstract>
      <url hash="f9689de1">2020.emnlp-main.383</url>
      <doi>10.18653/v1/2020.emnlp-main.383</doi>
      <video href="https://slideslive.com/38939136" />
      <bibkey>tan-etal-2020-tnt</bibkey>
    </paper>
    <paper id="384">
      <title>Methods for Numeracy-Preserving Word Embeddings</title>
      <author><first>Dhanasekar</first><last>Sundararaman</last></author>
      <author><first>Shijing</first><last>Si</last></author>
      <author><first>Vivek</first><last>Subramanian</last></author>
      <author><first>Guoyin</first><last>Wang</last></author>
      <author><first>Devamanyu</first><last>Hazarika</last></author>
      <author><first>Lawrence</first><last>Carin</last></author>
      <pages>4742–4753</pages>
      <abstract>Word embedding models are typically able to capture the semantics of words via the <a href="https://en.wikipedia.org/wiki/Distributional_hypothesis">distributional hypothesis</a>, but fail to capture the numerical properties of numbers that appear in the text. This leads to problems with <a href="https://en.wikipedia.org/wiki/Numerical_reasoning">numerical reasoning</a> involving <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> such as <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>. We propose a new <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> to assign and learn <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> for numbers. Our approach creates Deterministic, Independent-of-Corpus Embeddings (the model is referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks : (i) evaluating the ability to capture numeration and magnitude ; and (ii) to perform list maximum, decoding, and <a href="https://en.wikipedia.org/wiki/Addition">addition</a>. We further explore the utility of these embeddings in downstream tasks, by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.</abstract>
      <url hash="632811ad">2020.emnlp-main.384</url>
      <doi>10.18653/v1/2020.emnlp-main.384</doi>
      <video href="https://slideslive.com/38939268" />
      <bibkey>sundararaman-etal-2020-methods</bibkey>
    </paper>
    <paper id="385">
      <title>An Empirical Investigation of Contextualized Number Prediction</title>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <author><first>Daniel</first><last>Spokoyny</last></author>
      <pages>4754–4764</pages>
      <abstract>We conduct a large scale empirical investigation of contextualized number prediction in <a href="https://en.wikipedia.org/wiki/Running_text">running text</a>. Specifically, we consider two tasks : (1)masked number prediction predict-ing a missing numerical value within a sentence, and (2)numerical anomaly detectiondetecting an errorful numeric value within a sentence. We experiment with novel combinations of contextual encoders and output distributions over the <a href="https://en.wikipedia.org/wiki/Real_number_line">real number line</a>. Specifically, we introduce a suite of output distribution parameterizations that incorporate <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variables</a> to add expressivity and better fit the natural distribution of numeric values in running text, and combine them with both recur-rent and transformer-based encoder architectures. We evaluate these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on two <a href="https://en.wikipedia.org/wiki/Data_set">numeric datasets</a> in the financial and scientific domain. Our findings show that output distributions that incorporate discrete latent variables and allow for multiple modes outperform simple flow-based counterparts on all datasets, yielding more accurate numerical pre-diction and anomaly detection. We also show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> effectively utilize textual con-text and benefit from general-purpose unsupervised pretraining.</abstract>
      <url hash="dd2a9d9a">2020.emnlp-main.385</url>
      <doi>10.18653/v1/2020.emnlp-main.385</doi>
      <revision id="1" href="2020.emnlp-main.385v1" hash="aa29c7dd" />
      <revision id="2" href="2020.emnlp-main.385v2" hash="dd2a9d9a" date="2021-03-26">Modifed author order</revision>
      <video href="https://slideslive.com/38939346" />
      <bibkey>berg-kirkpatrick-spokoyny-2020-empirical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="389">
      <title>Unsupervised Parsing via Constituency Tests</title>
      <author><first>Steven</first><last>Cao</last></author>
      <author><first>Nikita</first><last>Kitaev</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>4798–4808</pages>
      <abstract>We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some <a href="https://en.wikipedia.org/wiki/Transformation_(function)">transformation</a> (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of <a href="https://en.wikipedia.org/wiki/Transformation_(function)">transformations</a> and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a <a href="https://en.wikipedia.org/wiki/Tree_(graph_theory)">tree</a> given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.</abstract>
      <url hash="12cc14bd">2020.emnlp-main.389</url>
      <doi>10.18653/v1/2020.emnlp-main.389</doi>
      <video href="https://slideslive.com/38938920" />
      <bibkey>cao-etal-2020-unsupervised</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="391">
      <title>Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios</title>
      <author><first>Ramy</first><last>Eskander</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <author><first>Michael</first><last>Collins</last></author>
      <pages>4820–4831</pages>
      <abstract>We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available. We use the <a href="https://en.wikipedia.org/wiki/Bible">Bible</a> as parallel data in our experiments : small size, out-of-domain and covering many diverse languages. Our approach innovates in three ways : 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> over previous work. In addition, we show that using multi-source information, either via <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">projection</a> or output combination, improves the performance for most target languages.</abstract>
      <url hash="3d5e5522">2020.emnlp-main.391</url>
      <doi>10.18653/v1/2020.emnlp-main.391</doi>
      <video href="https://slideslive.com/38939256" />
      <bibkey>eskander-etal-2020-unsupervised</bibkey>
    </paper>
    <paper id="392">
      <title>Unsupervised Parsing with S-DIORA : Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders<fixed-case>S</fixed-case>-<fixed-case>DIORA</fixed-case>: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders</title>
      <author><first>Andrew</first><last>Drozdov</last></author>
      <author><first>Subendhu</first><last>Rongali</last></author>
      <author><first>Yi-Pei</first><last>Chen</last></author>
      <author><first>Tim</first><last>O’Gorman</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>4832–4845</pages>
      <abstract>The deep inside-outside recursive autoencoder (DIORA ; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences * without access to labeled training data *. In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and can not recover from errors when computing the highest scoring parse tree in bottom-up chart parsing. To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart. Our experiments show that through * fine-tuning * a pre-trained DIORA with our new <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a>, we improve the state of the art in * unsupervised * constituency parsing on the English WSJ Penn Treebank by 2.2-6 % F1, depending on the data used for <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>.</abstract>
      <url hash="02dc69d3">2020.emnlp-main.392</url>
      <doi>10.18653/v1/2020.emnlp-main.392</doi>
      <video href="https://slideslive.com/38939296" />
      <bibkey>drozdov-etal-2020-unsupervised</bibkey>
    </paper>
    <paper id="393">
      <title>Utility is in the Eye of the User : A Critique of NLP Leaderboards<fixed-case>NLP</fixed-case> Leaderboards</title>
      <author><first>Kawin</first><last>Ethayarajh</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>4846–4853</pages>
      <abstract>Benchmarks such as GLUE have helped drive advances in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and <a href="https://en.wikipedia.org/wiki/Efficient_energy_use">energy efficiency</a>. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of <a href="https://en.wikipedia.org/wiki/Microeconomics">microeconomic theory</a>. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> as its utility to them. With this framing, we formalize how leaderboards   in their current form   can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a <a href="https://en.wikipedia.org/wiki/Glossary_of_economics">leaderboard</a>, since it is a cost that only the former must bear. To allow practitioners to better estimate a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., <a href="https://en.wikipedia.org/wiki/Mathematical_model">model size</a>, <a href="https://en.wikipedia.org/wiki/Efficient_energy_use">energy efficiency</a>, and inference latency).</abstract>
      <url hash="fc94df19">2020.emnlp-main.393</url>
      <doi>10.18653/v1/2020.emnlp-main.393</doi>
      <video href="https://slideslive.com/38938914" />
      <bibkey>ethayarajh-jurafsky-2020-utility</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/stereoset">StereoSet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="394">
      <title>An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training</title>
      <author><first>Kristjan</first><last>Arumae</last></author>
      <author><first>Qing</first><last>Sun</last></author>
      <author><first>Parminder</first><last>Bhatia</last></author>
      <pages>4854–4864</pages>
      <abstract>Pre-training large language models has become a standard in the <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing community</a>. Such <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate <a href="https://en.wikipedia.org/wiki/Cytomegalovirus">CF</a>. We find that elastic weight consolidation provides best overall scores yielding only a 0.33 % drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.</abstract>
      <url hash="624cef86">2020.emnlp-main.394</url>
      <doi>10.18653/v1/2020.emnlp-main.394</doi>
      <video href="https://slideslive.com/38938956" />
      <bibkey>arumae-etal-2020-empirical</bibkey>
      <pwccode url="https://github.com/aws-health-ai/multi_domain_lm" additional="false">aws-health-ai/multi_domain_lm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="402">
      <title>Does the Objective Matter? Comparing Training Objectives for Pronoun Resolution<fixed-case>O</fixed-case>bjective <fixed-case>M</fixed-case>atter? <fixed-case>C</fixed-case>omparing <fixed-case>T</fixed-case>raining <fixed-case>O</fixed-case>bjectives for <fixed-case>P</fixed-case>ronoun <fixed-case>R</fixed-case>esolution</title>
      <author><first>Yordan</first><last>Yordanov</last></author>
      <author><first>Oana-Maria</first><last>Camburu</last></author>
      <author><first>Vid</first><last>Kocijan</last></author>
      <author><first>Thomas</first><last>Lukasiewicz</last></author>
      <pages>4963–4969</pages>
      <abstract>Hard cases of pronoun resolution have been used as a long-standing benchmark for <a href="https://en.wikipedia.org/wiki/Commonsense_reasoning">commonsense reasoning</a>. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categories of training and evaluation objectives have been introduced. The variety of training datasets and pre-trained language models used in these works makes it unclear whether the choice of <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training objective</a> is critical. In this work, we make a fair comparison of the performance and seed-wise stability of four <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that represent the four categories of objectives. Our experiments show that the objective of sequence ranking performs the best in-domain, while the objective of semantic similarity between candidates and <a href="https://en.wikipedia.org/wiki/Pronoun">pronoun</a> performs the best out-of-domain. We also observe a seed-wise instability of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> using sequence ranking, which is not the case when the other objectives are used.</abstract>
      <url hash="c0346195">2020.emnlp-main.402</url>
      <doi>10.18653/v1/2020.emnlp-main.402</doi>
      <video href="https://slideslive.com/38939083" />
      <bibkey>yordanov-etal-2020-objective</bibkey>
      <pwccode url="https://github.com/YDYordanov/WS-training-objectives" additional="false">YDYordanov/WS-training-objectives</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="406">
      <title>Training for <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a> on <a href="https://en.wikipedia.org/wiki/Conditional_random_field">Conditional Random Fields</a> with Neural Scoring Factors<fixed-case>G</fixed-case>ibbs Sampling on Conditional Random Fields with Neural Scoring Factors</title>
      <author><first>Sida</first><last>Gao</last></author>
      <author><first>Matthew R.</first><last>Gormley</last></author>
      <pages>4999–5011</pages>
      <abstract>Most recent improvements in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging. More expressive <a href="https://en.wikipedia.org/wiki/Graphical_model">graphical models</a> are rarely used due to their prohibitive computational cost. In this work, we present an approach for efficiently training and decoding hybrids of <a href="https://en.wikipedia.org/wiki/Graphical_model">graphical models</a> and <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> based on <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs sampling</a>. Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging. We apply our approach to <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> and present a neural skip-chain CRF model, for which exact inference is impractical. The skip-chain model improves over a strong <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> on three languages from CoNLL-02/03. We obtain new state-of-the-art results on <a href="https://en.wikipedia.org/wiki/Dutch_language">Dutch</a>.</abstract>
      <url hash="e4bc187b">2020.emnlp-main.406</url>
      <doi>10.18653/v1/2020.emnlp-main.406</doi>
      <video href="https://slideslive.com/38939315" />
      <bibkey>gao-gormley-2020-training</bibkey>
    </paper>
    <paper id="412">
      <title>Simple Data Augmentation with the Mask Token Improves Domain Adaptation for Dialog Act Tagging</title>
      <author><first>Semih</first><last>Yavuz</last></author>
      <author><first>Kazuma</first><last>Hashimoto</last></author>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Nitish Shirish</first><last>Keskar</last></author>
      <author><first>Richard</first><last>Socher</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>5083–5089</pages>
      <abstract>The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains-the act of request carries the same speaker intention whether it is for <a href="https://en.wikipedia.org/wiki/Table_reservation">restaurant reservation</a> or flight booking. However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain. In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data. We propose MaskAugment, a controllable mechanism that augments text input by leveraging the pre-trained Mask token from BERT model. Inspired by consistency regularization, we use MaskAugment to introduce an unsupervised teacher-student learning scheme to examine the <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> of DA taggers. Our extensive experiments on the Simulated Dialogue (GSim) and Schema-Guided Dialogue (SGD) datasets show that MaskAugment is useful in improving the cross-domain generalization for DA tagging.</abstract>
      <url hash="830c79f0">2020.emnlp-main.412</url>
      <doi>10.18653/v1/2020.emnlp-main.412</doi>
      <video href="https://slideslive.com/38939326" />
      <bibkey>yavuz-etal-2020-simple</bibkey>
    </paper>
    <paper id="413">
      <title>Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing</title>
      <author><first>Xilun</first><last>Chen</last></author>
      <author><first>Asish</first><last>Ghoshal</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Sonal</first><last>Gupta</last></author>
      <pages>5090–5100</pages>
      <abstract>Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user’s intents (set reminder, play music, etc.). Recent advances in <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018 ; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised neural model</a> at a 10-fold <a href="https://en.wikipedia.org/wiki/Data_reduction">data reduction</a>. In particular, we identify two fundamental factors for low-resource domain adaptation : better <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning</a> and better <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training techniques</a>. Our <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning</a> uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.</abstract>
      <url hash="587c127f">2020.emnlp-main.413</url>
      <doi>10.18653/v1/2020.emnlp-main.413</doi>
      <video href="https://slideslive.com/38938783" />
      <bibkey>chen-etal-2020-low</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/topv2">TOPv2</pwcdataset>
    </paper>
    <paper id="416">
      <title>Facilitating the Communication of Politeness through Fine-Grained Paraphrasing</title>
      <author><first>Liye</first><last>Fu</last></author>
      <author><first>Susan</first><last>Fussell</last></author>
      <author><first>Cristian</first><last>Danescu-Niculescu-Mizil</last></author>
      <pages>5127–5140</pages>
      <abstract>Aided by <a href="https://en.wikipedia.org/wiki/Technology">technology</a>, people are increasingly able to communicate across geographical, cultural, and language barriers. This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances. In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance. As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> for suggesting paraphrases that achieve the intended level of <a href="https://en.wikipedia.org/wiki/Politeness">politeness</a> under a given communication circumstance. We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker’s intentions and the listener’s perceptions in both cases.</abstract>
      <url hash="2912edfd">2020.emnlp-main.416</url>
      <doi>10.18653/v1/2020.emnlp-main.416</doi>
      <video href="https://slideslive.com/38938661" />
      <bibkey>fu-etal-2020-facilitating</bibkey>
      <pwccode url="https://github.com/CornellNLP/politeness-paraphrase" additional="false">CornellNLP/politeness-paraphrase</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiconv">WikiConv</pwcdataset>
    </paper>
    <paper id="418">
      <title>Seq2Edits : Sequence Transduction Using Span-level Edit Operations<fixed-case>S</fixed-case>eq2<fixed-case>E</fixed-case>dits: Sequence Transduction Using Span-level Edit Operations</title>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author><first>Shankar</first><last>Kumar</last></author>
      <pages>5147–5159</pages>
      <abstract>We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting &amp; rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">grammatical error correction</a>, our method speeds up <a href="https://en.wikipedia.org/wiki/Inference">inference</a> by up to 5.2x compared to full sequence models because <a href="https://en.wikipedia.org/wiki/Inference">inference time</a> depends on the number of edits rather than the number of target tokens. For <a href="https://en.wikipedia.org/wiki/Text_normalization">text normalization</a>, sentence fusion, and <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">grammatical error correction</a>, our approach improves explainability by associating each edit operation with a <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">human-readable tag</a>.</abstract>
      <url hash="66344daa">2020.emnlp-main.418</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e5eac153">2020.emnlp-main.418.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.418</doi>
      <video href="https://slideslive.com/38939123" />
      <bibkey>stahlberg-kumar-2020-seq2edits</bibkey>
      <pwccode url="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/transformer_seq2edits.py" additional="false">tensorflow/tensor2tensor</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/discofuse">DiscoFuse</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilarge">WikiLarge</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisplit">WikiSplit</pwcdataset>
    </paper>
    <paper id="420">
      <title>Blank Language Models</title>
      <author><first>Tianxiao</first><last>Shen</last></author>
      <author><first>Victor</first><last>Quach</last></author>
      <author><first>Regina</first><last>Barzilay</last></author>
      <author><first>Tommi</first><last>Jaakkola</last></author>
      <pages>5186–5198</pages>
      <abstract>We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a <a href="https://en.wikipedia.org/wiki/Upper_and_lower_bounds">lower bound</a> of the <a href="https://en.wikipedia.org/wiki/Marginal_distribution">marginal data likelihood</a>. On the task of filling missing text snippets, BLM significantly outperforms all other <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baselines</a> in terms of both <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a>. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> for a wide range of <a href="https://en.wikipedia.org/wiki/Application_software">applications</a>.</abstract>
      <url hash="14d4a583">2020.emnlp-main.420</url>
      <doi>10.18653/v1/2020.emnlp-main.420</doi>
      <video href="https://slideslive.com/38939329" />
      <bibkey>shen-etal-2020-blank</bibkey>
      <pwccode url="https://github.com/Varal7/blank_language_model" additional="false">Varal7/blank_language_model</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="421">
      <title>COD3S : Diverse Generation with Discrete Semantic Signatures<fixed-case>COD3S</fixed-case>: Diverse Generation with Discrete Semantic Signatures</title>
      <author><first>Nathaniel</first><last>Weir</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>5199–5211</pages>
      <abstract>We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks. Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash (LSH)-based semantic sentence codes whose <a href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distances</a> highly correlate with human judgments of semantic textual similarity. Though it is generally applicable, we apply to causal generation, the task of predicting a proposition’s plausible causes or effects. We demonstrate through automatic and human evaluation that responses produced using our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> exhibit improved <a href="https://en.wikipedia.org/wiki/Multiculturalism">diversity</a> without degrading task performance.</abstract>
      <url hash="01cdf491">2020.emnlp-main.421</url>
      <doi>10.18653/v1/2020.emnlp-main.421</doi>
      <video href="https://slideslive.com/38939341" />
      <bibkey>weir-etal-2020-cod3s</bibkey>
      <pwccode url="https://github.com/nweir127/COD3S" additional="false">nweir127/COD3S</pwccode>
    </paper>
    <paper id="422">
      <title>Automatic Extraction of <a href="https://en.wikipedia.org/wiki/Rule_of_inference">Rules</a> Governing Morphological Agreement</title>
      <author><first>Aditi</first><last>Chaudhary</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Adithya</first><last>Pratapa</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <author><first>Zaid</first><last>Sheikh</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>5212–5236</pages>
      <abstract>Creating a <a href="https://en.wikipedia.org/wiki/Descriptive_grammar">descriptive grammar</a> of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing <a href="https://en.wikipedia.org/wiki/Agreement_(linguistics)">agreement</a>, a morphosyntactic phenomenon at the core of the <a href="https://en.wikipedia.org/wiki/Grammar">grammars</a> of many of the world’s languages. We apply our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data. We confirm this finding with human expert evaluations of the <a href="https://en.wikipedia.org/wiki/Rule_of_inference">rules</a> that our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> produces, which have an average <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 78 %. We release an interface demonstrating the extracted <a href="https://en.wikipedia.org/wiki/Rule-based_programming">rules</a> at https://neulab.github.io/lase/</abstract>
      <url hash="ddc3a4eb">2020.emnlp-main.422</url>
      <doi>10.18653/v1/2020.emnlp-main.422</doi>
      <video href="https://slideslive.com/38939038" />
      <bibkey>chaudhary-etal-2020-automatic</bibkey>
      <pwccode url="https://github.com/Aditi138/LASE-Agreement" additional="false">Aditi138/LASE-Agreement</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="425">
      <title>A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support</title>
      <author><first>Ashish</first><last>Sharma</last></author>
      <author><first>Adam</first><last>Miner</last></author>
      <author><first>David</first><last>Atkins</last></author>
      <author><first>Tim</first><last>Althoff</last></author>
      <pages>5263–5276</pages>
      <abstract>Empathy is critical to successful <a href="https://en.wikipedia.org/wiki/Mental_health_professional">mental health support</a>. Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts. Because millions of people use <a href="https://en.wikipedia.org/wiki/Text-based_user_interface">text-based platforms</a> for <a href="https://en.wikipedia.org/wiki/Mental_health_professional">mental health support</a>, understanding <a href="https://en.wikipedia.org/wiki/Empathy">empathy</a> in these contexts is crucial. In this work, we present a computational approach to understanding how <a href="https://en.wikipedia.org/wiki/Empathy">empathy</a> is expressed in online mental health platforms. We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations. We collect and share a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales). We develop a multi-task RoBERTa-based bi-encoder model for identifying <a href="https://en.wikipedia.org/wiki/Empathy">empathy</a> in conversations and extracting rationales underlying its predictions. Experiments demonstrate that our approach can effectively identify empathic conversations. We further apply this model to analyze 235k mental health interactions and show that users do not self-learn <a href="https://en.wikipedia.org/wiki/Empathy">empathy</a> over time, revealing opportunities for empathy training and feedback.</abstract>
      <url hash="8448b5f5">2020.emnlp-main.425</url>
      <doi>10.18653/v1/2020.emnlp-main.425</doi>
      <video href="https://slideslive.com/38939176" />
      <bibkey>sharma-etal-2020-computational</bibkey>
      <pwccode url="https://github.com/behavioral-data/Empathy-Mental-Health" additional="true">behavioral-data/Empathy-Mental-Health</pwccode>
    </paper>
    <paper id="426">
      <title>Modeling Protagonist Emotions for Emotion-Aware Storytelling</title>
      <author><first>Faeze</first><last>Brahman</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <pages>5277–5294</pages>
      <abstract>Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist. Our <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models. The EmoRL models use special rewards designed to regularize the story generation process through <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>. Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.</abstract>
      <url hash="ae2894c2">2020.emnlp-main.426</url>
      <doi>10.18653/v1/2020.emnlp-main.426</doi>
      <video href="https://slideslive.com/38939253" />
      <bibkey>brahman-chaturvedi-2020-modeling</bibkey>
      <pwccode url="https://github.com/fabrahman/Emo-Aware-Storytelling" additional="false">fabrahman/Emo-Aware-Storytelling</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="428">
      <title>Quantifying Intimacy in Language</title>
      <author><first>Jiaxin</first><last>Pei</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>5307–5326</pages>
      <abstract>Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning model</a> for accurately predicting the intimacy level of questions (Pearson r = 0.87). Through analyzing a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 80.5 M questions across <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, <a href="https://en.wikipedia.org/wiki/Book">books</a>, and <a href="https://en.wikipedia.org/wiki/Film">films</a>, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings. Then, in three studies, we further demonstrate how individuals modulate their intimacy to match <a href="https://en.wikipedia.org/wiki/Social_norm">social norms</a> around <a href="https://en.wikipedia.org/wiki/Gender">gender</a>, <a href="https://en.wikipedia.org/wiki/Social_distance">social distance</a>, and audience, each validating key findings from studies in <a href="https://en.wikipedia.org/wiki/Social_psychology">social psychology</a>. Our work demonstrates that <a href="https://en.wikipedia.org/wiki/Intimate_relationship">intimacy</a> is a pervasive and impactful social dimension of language.</abstract>
      <url hash="685274c0">2020.emnlp-main.428</url>
      <doi>10.18653/v1/2020.emnlp-main.428</doi>
      <video href="https://slideslive.com/38939316" />
      <bibkey>pei-jurgens-2020-quantifying</bibkey>
    </paper>
    <paper id="430">
      <title>Weakly Supervised Subevent Knowledge Acquisition<fixed-case>S</fixed-case>upervised <fixed-case>S</fixed-case>ubevent <fixed-case>K</fixed-case>nowledge <fixed-case>A</fixed-case>cquisition</title>
      <author><first>Wenlin</first><last>Yao</last></author>
      <author><first>Zeyu</first><last>Dai</last></author>
      <author><first>Maitreyi</first><last>Ramaswamy</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Ruihong</first><last>Huang</last></author>
      <pages>5345–5356</pages>
      <abstract>Subevents elaborate an event and widely exist in <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">event descriptions</a>. Subevent knowledge is useful for <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse analysis</a> and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base. We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two observations that 1) subevents are temporally contained by the parent event, and 2) the definitions of the parent event can be used to further guide the identification of subevents. Then, we collect rich weak supervision using the initial seed subevent pairs to train a contextual classifier using BERT and apply the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> to identify new subevent pairs. The evaluation showed that the acquired subevent tuples (239 K) are of high quality (90.1 % accuracy) and cover a wide range of event types. The acquired subevent knowledge has been shown useful for <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse analysis</a> and identifying a range of event-event relations.</abstract>
      <url hash="bd192eb7">2020.emnlp-main.430</url>
      <doi>10.18653/v1/2020.emnlp-main.430</doi>
      <video href="https://slideslive.com/38939132" />
      <bibkey>yao-etal-2020-weakly</bibkey>
    </paper>
    <paper id="431">
      <title>Biomedical Event Extraction as Sequence Labeling</title>
      <author><first>Alan</first><last>Ramponi</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Rosario</first><last>Lombardo</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>5357–5367</pages>
      <abstract>We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model. BeeSL recasts the task as <a href="https://en.wikipedia.org/wiki/Sequence_labeling">sequence labeling</a>, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a>. BeeSL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools. BeeSL outperforms the current best <a href="https://en.wikipedia.org/wiki/System">system</a> (Li et al., 2019) on the Genia 2011 benchmark by 1.57 % absolute <a href="https://en.wikipedia.org/wiki/Free_and_open-source_software">F1 score</a> reaching 60.22 % <a href="https://en.wikipedia.org/wiki/Free_and_open-source_software">F1</a>, establishing a new state of the art for the task. Importantly, we also provide first results on biomedical event extraction without gold entity information. Empirical results show that BeeSL’s speed and <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> makes it a viable approach for large-scale real-world scenarios.</abstract>
      <url hash="27326b23">2020.emnlp-main.431</url>
      <doi>10.18653/v1/2020.emnlp-main.431</doi>
      <video href="https://slideslive.com/38939154" />
      <bibkey>ramponi-etal-2020-biomedical</bibkey>
    </paper>
    <paper id="432">
      <title>Annotating Temporal Dependency Graphs via <a href="https://en.wikipedia.org/wiki/Crowdsourcing">Crowdsourcing</a><fixed-case>A</fixed-case>nnotating <fixed-case>T</fixed-case>emporal <fixed-case>D</fixed-case>ependency <fixed-case>G</fixed-case>raphs via <fixed-case>C</fixed-case>rowdsourcing</title>
      <author><first>Jiarui</first><last>Yao</last></author>
      <author><first>Haoling</first><last>Qiu</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <pages>5368–5380</pages>
      <abstract>We present the construction of a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> of 500 Wikinews articles annotated with temporal dependency graphs (TDGs) that can be used to train systems to understand temporal relations in text. We argue that temporal dependency graphs, built on previous research on narrative times and temporal anaphora, provide a representation scheme that achieves a good trade-off between completeness and practicality in temporal annotation. We also provide a crowdsourcing strategy to annotate TDGs, and demonstrate the feasibility of this approach with an evaluation of the quality of the annotation, and the utility of the resulting <a href="https://en.wikipedia.org/wiki/Data_set">data set</a> by training a <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning model</a> on this <a href="https://en.wikipedia.org/wiki/Data_set">data set</a>. The data set is publicly available.</abstract>
      <url hash="50c04b8b">2020.emnlp-main.432</url>
      <doi>10.18653/v1/2020.emnlp-main.432</doi>
      <bibkey>yao-etal-2020-annotating</bibkey>
    </paper>
    <paper id="434">
      <title>CHARM : Inferring Personal Attributes from Conversations<fixed-case>CHARM</fixed-case>: Inferring Personal Attributes from Conversations</title>
      <author><first>Anna</first><last>Tigunova</last></author>
      <author><first>Andrew</first><last>Yates</last></author>
      <author><first>Paramita</first><last>Mirza</last></author>
      <author><first>Gerhard</first><last>Weikum</last></author>
      <pages>5391–5404</pages>
      <abstract>Personal knowledge about users’ professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as <a href="https://en.wikipedia.org/wiki/Recommender_system">recommenders</a> or <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a>. Conversations in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, such as <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a>, are a rich source of data for inferring personal facts. Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples. This paper overcomes this limitation by devising CHARM : a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training. Experiments with large datasets from <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a> show the viability of CHARM for open-ended attributes, such as <a href="https://en.wikipedia.org/wiki/Profession">professions</a> and <a href="https://en.wikipedia.org/wiki/Hobby">hobbies</a>.</abstract>
      <url hash="0f77d686">2020.emnlp-main.434</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5368e472">2020.emnlp-main.434.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.434</doi>
      <video href="https://slideslive.com/38939312" />
      <bibkey>tigunova-etal-2020-charm</bibkey>
    </paper>
    <paper id="437">
      <title>How Much Knowledge Can You Pack Into the Parameters of a <a href="https://en.wikipedia.org/wiki/Language_model">Language Model</a>?</title>
      <author><first>Adam</first><last>Roberts</last></author>
      <author><first>Colin</first><last>Raffel</last></author>
      <author><first>Noam</first><last>Shazeer</last></author>
      <pages>5418–5426</pages>
      <abstract>It has recently been observed that neural language models trained on <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured text</a> can implicitly store and retrieve knowledge using <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language queries</a>. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>.</abstract>
      <url hash="b8cc8a9d">2020.emnlp-main.437</url>
      <doi>10.18653/v1/2020.emnlp-main.437</doi>
      <video href="https://slideslive.com/38938651" />
      <bibkey>roberts-etal-2020-much</bibkey>
      <pwccode url="https://github.com/google-research/google-research/tree/master/t5_closed_book_qa" additional="true">google-research/google-research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="438">
      <title>EXAMS : A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering<fixed-case>EXAMS</fixed-case>: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering</title>
      <author><first>Momchil</first><last>Hardalov</last></author>
      <author><first>Todor</first><last>Mihaylov</last></author>
      <author><first>Dimitrina</first><last>Zlatkova</last></author>
      <author><first>Yoan</first><last>Dinkov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>5427–5444</pages>
      <abstract>We propose EXAMS   a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others. EXAMS offers unique fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of the proposed models. We perform various experiments with existing top-performing multilingual pre-trained models and show that EXAMS offers multiple challenges that require multilingual knowledge and reasoning in multiple domains. We hope that EXAMS will enable researchers to explore challenging reasoning and knowledge transfer methods and pre-trained models for school question answering in various languages which was not possible by now. The data, code, pre-trained models, and evaluation are available at http://github.com/mhardalov/exams-qa.</abstract>
      <url hash="34ad85cd">2020.emnlp-main.438</url>
      <doi>10.18653/v1/2020.emnlp-main.438</doi>
      <video href="https://slideslive.com/38938985" />
      <bibkey>hardalov-etal-2020-exams</bibkey>
      <pwccode url="https://github.com/mhardalov/exams-qa" additional="true">mhardalov/exams-qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/exams">EXAMS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="447">
      <title>Sequence-Level Mixed Sample Data Augmentation</title>
      <author><first>Demi</first><last>Guo</last></author>
      <author><first>Yoon</first><last>Kim</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>5547–5552</pages>
      <abstract>Despite their empirical success, <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input / output sequences from the training set. We connect this approach to existing techniques such as SwitchOut and word dropout, and show that these techniques are all essentially approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on five different <a href="https://en.wikipedia.org/wiki/Translation_(biology)">translation datasets</a> over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.</abstract>
      <url hash="6768f85c">2020.emnlp-main.447</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c92a8ae2">2020.emnlp-main.447.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.447</doi>
      <video href="https://slideslive.com/38938890" />
      <bibkey>guo-etal-2020-sequence</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="448">
      <title>Consistency of a Recurrent Language Model With Respect to Incomplete Decoding</title>
      <author><first>Sean</first><last>Welleck</last></author>
      <author><first>Ilia</first><last>Kulikov</last></author>
      <author><first>Jaedeok</first><last>Kim</last></author>
      <author><first>Richard Yuanzhe</first><last>Pang</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <pages>5553–5568</pages>
      <abstract>Despite strong performance on a variety of tasks, neural sequence models trained with <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood</a> have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> can yield an infinite-length sequence that has zero probability under the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. We prove that commonly used incomplete decoding algorithms   <a href="https://en.wikipedia.org/wiki/Greedy_search">greedy search</a>, <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a>, top-k sampling, and nucleus sampling   are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency : consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that <a href="https://en.wikipedia.org/wiki/Inconsistency">inconsistency</a> occurs in practice, and that the proposed <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> prevent <a href="https://en.wikipedia.org/wiki/Inconsistency">inconsistency</a>.</abstract>
      <url hash="82ebca8c">2020.emnlp-main.448</url>
      <doi>10.18653/v1/2020.emnlp-main.448</doi>
      <video href="https://slideslive.com/38939066" />
      <bibkey>welleck-etal-2020-consistency</bibkey>
      <pwccode url="https://github.com/uralik/consistency-lm" additional="false">uralik/consistency-lm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="451">
      <title>Inducing Target-Specific Latent Structures for Aspect Sentiment Classification</title>
      <author><first>Chenhua</first><last>Chen</last></author>
      <author><first>Zhiyang</first><last>Teng</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>5596–5607</pages>
      <abstract>Aspect-level sentiment analysis aims to recognize the sentiment polarity of an aspect or a target in a comment. Recently, graph convolutional networks based on linguistic dependency trees have been studied for this task. However, the dependency parsing accuracy of commercial product comments or <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a> might be unsatisfactory. To tackle this problem, we associate linguistic dependency trees with automatically induced aspectspecific graphs. We propose gating mechanisms to dynamically combine information from word dependency graphs and latent graphs which are learned by self-attention networks. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can complement supervised syntactic features with latent semantic dependencies. Experimental results on five <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmarks</a> show the effectiveness of our proposed latent models, giving significantly better results than <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> without using latent graphs.</abstract>
      <url hash="b1849e62">2020.emnlp-main.451</url>
      <doi>10.18653/v1/2020.emnlp-main.451</doi>
      <video href="https://slideslive.com/38938882" />
      <bibkey>chen-etal-2020-inducing</bibkey>
    </paper>
    <paper id="459">
      <title>Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph</title>
      <author><first>Xin</first><last>Lv</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Yichi</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Kong</last></author>
      <author><first>Suhui</first><last>Wu</last></author>
      <pages>5694–5703</pages>
      <abstract>Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous <a href="https://en.wikipedia.org/wiki/Automated_reasoning">reasoning methods</a> are designed for dense KGs with enough paths between entities, but can not work well on those sparse KGs that only contain sparse paths for <a href="https://en.wikipedia.org/wiki/Automated_reasoning">reasoning</a>. On the one hand, sparse KGs contain less information, which makes it difficult for the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model over sparse KGs, by applying novel dynamic anticipation and completion strategies : (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from <a href="https://en.wikipedia.org/wiki/Freebase">Freebase</a>, <a href="https://en.wikipedia.org/wiki/NELL">NELL</a> and <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a> show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR.</abstract>
      <url hash="d5d29f91">2020.emnlp-main.459</url>
      <doi>10.18653/v1/2020.emnlp-main.459</doi>
      <video href="https://slideslive.com/38938756" />
      <bibkey>lv-etal-2020-dynamic</bibkey>
      <pwccode url="https://github.com/THU-KEG/DacKGR" additional="false">THU-KEG/DacKGR</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nell">NELL</pwcdataset>
    </paper>
    <paper id="460">
      <title>Knowledge Association with Hyperbolic Knowledge Graph Embeddings</title>
      <author><first>Zequn</first><last>Sun</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Wei</first><last>Hu</last></author>
      <author><first>Chengming</first><last>Wang</last></author>
      <author><first>Jian</first><last>Dai</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <pages>5704–5716</pages>
      <abstract>Capturing associations for knowledge graphs (KGs) through <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity alignment</a>, <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity type inference</a> and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs. They also depend on high embedding dimensions to realize enough expressiveness. Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association. We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation. Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method.</abstract>
      <url hash="d378d54e">2020.emnlp-main.460</url>
      <doi>10.18653/v1/2020.emnlp-main.460</doi>
      <video href="https://slideslive.com/38938826" />
      <bibkey>sun-etal-2020-knowledge</bibkey>
      <pwccode url="https://github.com/nju-websoft/HyperKA" additional="false">nju-websoft/HyperKA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dbp15k">DBP15K</pwcdataset>
    </paper>
    <paper id="461">
      <title>Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction</title>
      <author><first>Rujun</first><last>Han</last></author>
      <author><first>Yichao</first><last>Zhou</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>5717–5729</pages>
      <abstract>Extracting event temporal relations is a critical task for <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a> and plays an important role in <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>. Prior systems leverage <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> and pre-trained <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> to improve the performance of the task. However, these systems often suffer from two shortcomings : 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints ; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural network</a> with <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">distributional constraints</a> constructed by probabilistic domain knowledge. We solve the constrained inference problem via <a href="https://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field">Lagrangian Relaxation</a> and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong <a href="https://en.wikipedia.org/wiki/Statistical_significance">statistical significance</a> on two widely used datasets in news and clinical domains.</abstract>
      <url hash="9dab2a9f">2020.emnlp-main.461</url>
      <doi>10.18653/v1/2020.emnlp-main.461</doi>
      <video href="https://slideslive.com/38939236" />
      <bibkey>han-etal-2020-domain</bibkey>
      <pwccode url="https://github.com/rujunhan/EMNLP-2020" additional="false">rujunhan/EMNLP-2020</pwccode>
    </paper>
    <paper id="463">
      <title>Understanding the Difficulty of Training Transformers</title>
      <author><first>Liyuan</first><last>Liu</last></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>5747–5763</pages>
      <abstract>Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train <a href="https://en.wikipedia.org/wiki/Transformers_(TV_series)">Transformers</a> effectively). Our objective here is to understand _ _ what complicates Transformer training _ _ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training</a> substantiallyfor each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies <a href="https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)">small parameter perturbations</a> (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage’s training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance</abstract>
      <url hash="6b075935">2020.emnlp-main.463</url>
      <attachment type="OptionalSupplementaryMaterial" hash="091036bc">2020.emnlp-main.463.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.463</doi>
      <video href="https://slideslive.com/38938933" />
      <bibkey>liu-etal-2020-understanding</bibkey>
      <pwccode url="https://github.com/LiyuanLucasLiu/Transforemr-Clinic" additional="true">LiyuanLucasLiu/Transforemr-Clinic</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="464">
      <title>An Empirical Study of Generation Order for Machine Translation</title>
      <author><first>William</first><last>Chan</last></author>
      <author><first>Mitchell</first><last>Stern</last></author>
      <author><first>Jamie</first><last>Kiros</last></author>
      <author><first>Jakob</first><last>Uszkoreit</last></author>
      <pages>5764–5773</pages>
      <abstract>In this work, we present an empirical study of generation order for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders. Curiously, we find that for the WMT’14 English   German and WMT’18 English   Chinese translation tasks, order does not have a substantial impact on output quality. Moreover, for English   German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.<tex-math>\to</tex-math> German and WMT’18 English <tex-math>\to</tex-math> Chinese translation tasks, order does not have a substantial impact on output quality. Moreover, for English <tex-math>\to</tex-math> German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.</abstract>
      <url hash="1258d07e">2020.emnlp-main.464</url>
      <attachment type="OptionalSupplementaryMaterial" hash="ac80ec91">2020.emnlp-main.464.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.emnlp-main.464</doi>
      <video href="https://slideslive.com/38939147" />
      <bibkey>chan-etal-2020-empirical</bibkey>
    </paper>
    <paper id="469">
      <title>Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning</title>
      <author><first>Yuncheng</first><last>Hua</last></author>
      <author><first>Yuan-Fang</first><last>Li</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Guilin</first><last>Qi</last></author>
      <author><first>Tongtong</first><last>Wu</last></author>
      <pages>5827–5837</pages>
      <abstract>Complex question-answering (CQA) involves answering complex <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural-language questions</a> on a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base (KB)</a>. However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level. This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions. Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data. The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set. Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1 % of the training set. We have released our code at https://github.com/DevinJake/MRL-CQA.</abstract>
      <url hash="0752cf6f">2020.emnlp-main.469</url>
      <doi>10.18653/v1/2020.emnlp-main.469</doi>
      <video href="https://slideslive.com/38939385" />
      <bibkey>hua-etal-2020-shot</bibkey>
      <pwccode url="https://github.com/DevinJake/MRL-CQA" additional="false">DevinJake/MRL-CQA</pwccode>
    </paper>
    <paper id="477">
      <title>LAReQA : Language-Agnostic Answer Retrieval from a Multilingual Pool<fixed-case>LAR</fixed-case>e<fixed-case>QA</fixed-case>: Language-Agnostic Answer Retrieval from a Multilingual Pool</title>
      <author><first>Uma</first><last>Roy</last></author>
      <author><first>Noah</first><last>Constant</last></author>
      <author><first>Rami</first><last>Al-Rfou</last></author>
      <author><first>Aditya</first><last>Barua</last></author>
      <author><first>Aaron</first><last>Phillips</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <pages>5919–5930</pages>
      <abstract>We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for strong cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. This level of <a href="https://en.wikipedia.org/wiki/Sequence_alignment">alignment</a> is important for the practical task of <a href="https://en.wikipedia.org/wiki/Cross-lingual_information_retrieval">cross-lingual information retrieval</a>. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance on zero-shot variants of our task that only target weak alignment is not predictive of performance on LAReQA. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at.<i>cross</i>-language pairs to be closer in representation space than unrelated <i>same</i>-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target “weak” alignment is not predictive of performance on LAReQA. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at <url>https://github.com/google-research-datasets/lareqa</url>.</abstract>
      <url hash="03030e0b">2020.emnlp-main.477</url>
      <doi>10.18653/v1/2020.emnlp-main.477</doi>
      <video href="https://slideslive.com/38939085" />
      <bibkey>roy-etal-2020-lareqa</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad-r">LAReQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bucc">BUCC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="478">
      <title>OCR Post Correction for Endangered Language Texts<fixed-case>OCR</fixed-case> <fixed-case>P</fixed-case>ost <fixed-case>C</fixed-case>orrection for <fixed-case>E</fixed-case>ndangered <fixed-case>L</fixed-case>anguage <fixed-case>T</fixed-case>exts</title>
      <author><first>Shruti</first><last>Rijhwani</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>5931–5942</pages>
      <abstract>There is little to no data available to build <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language processing models</a> for most <a href="https://en.wikipedia.org/wiki/Endangered_language">endangered languages</a>. However, <a href="https://en.wikipedia.org/wiki/Text-based_user_interface">textual data</a> in these languages often exists in formats that are not machine-readable, such as <a href="https://en.wikipedia.org/wiki/Paperback">paper books</a> and <a href="https://en.wikipedia.org/wiki/Image_scanner">scanned images</a>. In this work, we address the task of extracting text from these <a href="https://en.wikipedia.org/wiki/Resource_(computing)">resources</a>. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR post-correction method tailored to ease training in this data-scarce setting, reducing the recognition error rate by 34 % on average across the three languages.</abstract>
      <url hash="cba08bb3">2020.emnlp-main.478</url>
      <doi>10.18653/v1/2020.emnlp-main.478</doi>
      <video href="https://slideslive.com/38939129" />
      <bibkey>rijhwani-etal-2020-ocr</bibkey>
      <pwccode url="https://github.com/shrutirij/ocr-post-correction" additional="false">shrutirij/ocr-post-correction</pwccode>
    </paper>
    <paper id="479">
      <title>X-FACTR : Multilingual Factual Knowledge Retrieval from Pretrained Language Models<fixed-case>X</fixed-case>-<fixed-case>FACTR</fixed-case>: Multilingual Factual Knowledge Retrieval from Pretrained Language Models</title>
      <author><first>Zhengbao</first><last>Jiang</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Jun</first><last>Araki</last></author>
      <author><first>Haibo</first><last>Ding</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>5943–5959</pages>
      <abstract>Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as Punta Cana is located in _. However, while knowledge is both written and queried in many languages, studies on LMs’ factual representation ability have almost invariably been performed on <a href="https://en.wikipedia.org/wiki/English_language">English</a>. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at https://x-factr.github.io.</abstract>
      <url hash="08717037">2020.emnlp-main.479</url>
      <doi>10.18653/v1/2020.emnlp-main.479</doi>
      <video href="https://slideslive.com/38939158" />
      <bibkey>jiang-etal-2020-x</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="487">
      <title>Supertagging Combinatory Categorial Grammar with Attentive Graph Convolutional Networks<fixed-case>C</fixed-case>ombinatory <fixed-case>C</fixed-case>ategorial <fixed-case>G</fixed-case>rammar with Attentive Graph Convolutional Networks</title>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <pages>6037–6044</pages>
      <abstract>Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders (e.g., bi-LSTM). In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. Specifically, we build the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> from chunks (n-grams) extracted from a lexicon and apply <a href="https://en.wikipedia.org/wiki/Attention">attention</a> over the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>, so that different word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies in terms of both supertagging and <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a>. Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.</abstract>
      <attachment type="OptionalSupplementaryMaterial" hash="ba4e2e62">2020.emnlp-main.487.OptionalSupplementaryMaterial.zip</attachment>
      <url hash="a15879a3">2020.emnlp-main.487</url>
      <revision id="1" href="2020.emnlp-main.487v1" hash="219066cb" />
      <revision id="2" href="2020.emnlp-main.487v2" hash="a15879a3" date="2020-11-29">This revision corrects the statistics of the dataset in Table 1 (the original reports the word type number; the revision reports the word token number, which is better) and adds the acknowledgment section.</revision>
      <doi>10.18653/v1/2020.emnlp-main.487</doi>
      <video href="https://slideslive.com/38939138" />
      <bibkey>tian-etal-2020-supertagging</bibkey>
      <pwccode url="https://github.com/cuhksz-nlp/NeST-CCG" additional="false">cuhksz-nlp/NeST-CCG</pwccode>
    </paper>
    <paper id="490">
      <title>Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots</title>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Hong</first><last>Xu</last></author>
      <author><first>Sihong</first><last>Liu</last></author>
      <author><first>Fanyu</first><last>Meng</last></author>
      <author><first>Min</first><last>Hu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>6070–6075</pages>
      <abstract>Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context. We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence. Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.</abstract>
      <url hash="0c043b9d">2020.emnlp-main.490</url>
      <doi>10.18653/v1/2020.emnlp-main.490</doi>
      <video href="https://slideslive.com/38938765" />
      <bibkey>yan-etal-2020-adversarial</bibkey>
    </paper>
    <paper id="492">
      <title>Structure Aware Negative Sampling in <a href="https://en.wikipedia.org/wiki/Knowledge_graph">Knowledge Graphs</a></title>
      <author><first>Kian</first><last>Ahrabian</last></author>
      <author><first>Aarash</first><last>Feizi</last></author>
      <author><first>Yasmin</first><last>Salehi</last></author>
      <author><first>William L.</first><last>Hamilton</last></author>
      <author><first>Avishek Joey</first><last>Bose</last></author>
      <pages>6093–6101</pages>
      <abstract>Learning low-dimensional representations for entities and relations in <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a> using contrastive estimation represents a scalable and effective method for inferring connectivity patterns. A crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the <a href="https://en.wikipedia.org/wiki/Embedding">embedding model</a> to learn discriminative representations and find critical characteristics of observed data. While earlier <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> either employ too simple <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">corruption distributions</a>, i.e. uniform, yielding easy uninformative negatives or sophisticated adversarial distributions with challenging optimization schemes, they do not explicitly incorporate known graph structure resulting in suboptimal negatives. In this paper, we propose Structure Aware Negative Sampling (SANS), an inexpensive negative sampling strategy that utilizes the rich graph structure by selecting negative samples from a node’s k-hop neighborhood. Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.<tex-math>k</tex-math>-hop neighborhood. Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.</abstract>
      <url hash="ae979f56">2020.emnlp-main.492</url>
      <attachment type="OptionalSupplementaryMaterial" hash="93844f88">2020.emnlp-main.492.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.492</doi>
      <video href="https://slideslive.com/38938925" />
      <bibkey>ahrabian-etal-2020-structure</bibkey>
      <pwccode url="https://github.com/kahrabian/SANS" additional="false">kahrabian/SANS</pwccode>
    </paper>
    <paper id="493">
      <title>Neural Mask Generator : Learning to Generate Adaptive Word Maskings for Language Model Adaptation</title>
      <author><first>Minki</first><last>Kang</last></author>
      <author><first>Moonsu</first><last>Han</last></author>
      <author><first>Sung Ju</first><last>Hwang</last></author>
      <pages>6102–6120</pages>
      <abstract>We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.</abstract>
      <url hash="c5f9c476">2020.emnlp-main.493</url>
      <doi>10.18653/v1/2020.emnlp-main.493</doi>
      <video href="https://slideslive.com/38938671" />
      <bibkey>kang-etal-2020-neural</bibkey>
      <pwccode url="https://github.com/Nardien/NMG" additional="false">Nardien/NMG</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emrqa">emrQA</pwcdataset>
    </paper>
    <paper id="498">
      <title>BAE : BERT-based Adversarial Examples for Text Classification<fixed-case>BAE</fixed-case>: <fixed-case>BERT</fixed-case>-based Adversarial Examples for Text Classification</title>
      <author><first>Siddhant</first><last>Garg</last></author>
      <author><first>Goutham</first><last>Ramakrishnan</last></author>
      <pages>6174–6181</pages>
      <abstract>Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved <a href="https://en.wikipedia.org/wiki/Grammaticality">grammaticality</a> and semantic coherence as compared to prior work.</abstract>
      <url hash="23f7293d">2020.emnlp-main.498</url>
      <doi>10.18653/v1/2020.emnlp-main.498</doi>
      <video href="https://slideslive.com/38938695" />
      <bibkey>garg-ramakrishnan-2020-bae</bibkey>
      <pwccode url="https://github.com/QData/TextAttack" additional="true">QData/TextAttack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
    </paper>
    <paper id="499">
      <title>Adversarial Self-Supervised Data-Free Distillation for Text Classification<fixed-case>A</fixed-case>dversarial <fixed-case>S</fixed-case>elf-<fixed-case>S</fixed-case>upervised <fixed-case>D</fixed-case>ata-<fixed-case>F</fixed-case>ree <fixed-case>D</fixed-case>istillation for <fixed-case>T</fixed-case>ext <fixed-case>C</fixed-case>lassification</title>
      <author><first>Xinyin</first><last>Ma</last></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Gongfan</first><last>Fang</last></author>
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Chenghao</first><last>Jia</last></author>
      <author><first>Weiming</first><last>Lu</last></author>
      <pages>6182–6192</pages>
      <abstract>Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to a resource-efficient lightweight model. However, most KD algorithms, especially in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues. To tackle this problem, we propose a novel two-stage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT). To avoid text generation in discrete space, we introduce a Plug &amp; Play Embedding Guessing method to craft pseudo embeddings from the teacher’s hidden knowledge. Meanwhile, with a self-supervised module to quantify the student’s ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner. To the best of our knowledge, our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> is the first data-free distillation framework designed for NLP tasks. We verify the effectiveness of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> on several text classification datasets.</abstract>
      <url hash="75478696">2020.emnlp-main.499</url>
      <attachment type="OptionalSupplementaryMaterial" hash="28f72a1c">2020.emnlp-main.499.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.499</doi>
      <video href="https://slideslive.com/38938706" />
      <bibkey>ma-etal-2020-adversarial</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="501">
      <title>The Thieves on Sesame Street are Polyglots-Extracting Multilingual Models from Monolingual APIs<fixed-case>API</fixed-case>s</title>
      <author><first>Nitish Shirish</first><last>Keskar</last></author>
      <author><first>Bryan</first><last>McCann</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Richard</first><last>Socher</last></author>
      <pages>6203–6207</pages>
      <abstract>Pre-training in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim’s labels for that data. We discover that this extraction process extends to local copies initialized from a pre-trained, multilingual model while the victim remains monolingual. The extracted <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages. This is done without ever showing the multilingual, extracted model a well-formed input in any of the languages for the target <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. We also demonstrate that a few real examples can greatly improve performance, and we analyze how these results shed light on how such extraction methods succeed.</abstract>
      <url hash="25704ef7">2020.emnlp-main.501</url>
      <doi>10.18653/v1/2020.emnlp-main.501</doi>
      <video href="https://slideslive.com/38938724" />
      <bibkey>keskar-etal-2020-thieves</bibkey>
    </paper>
    <paper id="502">
      <title>When Hearst Is not Enough : Improving Hypernymy Detection from Corpus with Distributional Models</title>
      <author><first>Changlong</first><last>Yu</last></author>
      <author><first>Jialong</first><last>Han</last></author>
      <author><first>Peifeng</first><last>Wang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Wilfred</first><last>Ng</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>6208–6217</pages>
      <abstract>We address hypernymy detection, i.e., whether an is-a relationship exists between words (x, y), with the help of large textual corpora. Most conventional approaches to this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> have been categorized to be either pattern-based or distributional. Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x, y) pairs relieved. However, they become invalid in some specific sparsity cases, where x or y is not involved in any pattern. For the first time, this paper quantifies the non-negligible existence of those specific cases. We also demonstrate that <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">distributional methods</a> are ideal to make up for pattern-based ones in such cases. We devise a complementary framework, under which a pattern-based and a distributional model collaborate seamlessly in cases which they each prefer. On several <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark datasets</a>, our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> demonstrates improvements that are both competitive and explainable.</abstract>
      <url hash="e4ee47b7">2020.emnlp-main.502</url>
      <doi>10.18653/v1/2020.emnlp-main.502</doi>
      <video href="https://slideslive.com/38939044" />
      <bibkey>yu-etal-2020-hearst</bibkey>
      <pwccode url="https://github.com/HKUST-KnowComp/ComHyper" additional="false">HKUST-KnowComp/ComHyper</pwccode>
    </paper>
    <paper id="510">
      <title>Summarizing Text on Any Aspects : A Knowledge-Informed Weakly-Supervised Approach</title>
      <author><first>Bowen</first><last>Tan</last></author>
      <author><first>Lianhui</first><last>Qin</last></author>
      <author><first>Eric</first><last>Xing</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <pages>6301–6309</pages>
      <abstract>Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect. Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics. In this work, we study summarizing on arbitrary aspects relevant to the document, which significantly expands the application of the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> in practice. Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as <a href="https://en.wikipedia.org/wiki/ConceptNet">ConceptNet</a> and <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>. Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.<i>arbitrary</i> aspects relevant to the document, which significantly expands the application of the task in practice. Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia. Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.</abstract>
      <url hash="302a388a">2020.emnlp-main.510</url>
      <doi>10.18653/v1/2020.emnlp-main.510</doi>
      <video href="https://slideslive.com/38939371" />
      <bibkey>tan-etal-2020-summarizing</bibkey>
      <pwccode url="https://github.com/tanyuqian/aspect-based-summarization" additional="false">tanyuqian/aspect-based-summarization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="514">
      <title>Coarse-to-Fine Pre-training for Named Entity Recognition<fixed-case>C</fixed-case>oarse-to-<fixed-case>F</fixed-case>ine <fixed-case>P</fixed-case>re-training for <fixed-case>N</fixed-case>amed <fixed-case>E</fixed-case>ntity <fixed-case>R</fixed-case>ecognition</title>
      <author><first>Xue</first><last>Mengge</last></author>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Zhenyu</first><last>Zhang</last></author>
      <author><first>Tingwen</first><last>Liu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <pages>6345–6354</pages>
      <abstract>More recently, <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entity Recognition</a> hasachieved great advances aided by pre-trainingapproaches such as <a href="https://en.wikipedia.org/wiki/BERT">BERT</a>. However, currentpre-training techniques focus on building lan-guage modeling objectives to learn a gen-eral representation, ignoring the named entity-related knowledge. To this end, we proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge into pre-trained models. Specifi-cally, we first warm-up the model via an en-tity span identification task by training it withWikipedia anchors, which can be deemed asgeneral-typed entities. Then we leverage thegazetteer-based distant supervision strategy totrain the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> extract coarse-grained typedentities. Finally, we devise a self-supervisedauxiliary task to mine the fine-grained namedentity knowledge via clustering. Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks. Besides, weshow that our <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> gains promising re-sults without using human-labeled trainingdata, demonstrating its effectiveness in label-few and low-resource scenarios.</abstract>
      <url hash="4a046bff">2020.emnlp-main.514</url>
      <doi>10.18653/v1/2020.emnlp-main.514</doi>
      <video href="https://slideslive.com/38938977" />
      <bibkey>mengge-etal-2020-coarse</bibkey>
      <pwccode url="https://github.com/strawberryx/CoFEE" additional="false">strawberryx/CoFEE</pwccode>
    </paper>
    <paper id="515">
      <title>Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment</title>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Yixin</first><last>Cao</last></author>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <pages>6355–6364</pages>
      <abstract>Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> achieves significant improvements (5.10 % on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at.<url>https://github.com/thunlp/explore-and-evaluate</url>.</abstract>
      <url hash="b0a1562e">2020.emnlp-main.515</url>
      <doi>10.18653/v1/2020.emnlp-main.515</doi>
      <video href="https://slideslive.com/38938987" />
      <bibkey>liu-etal-2020-exploring</bibkey>
      <pwccode url="https://github.com/thunlp/explore-and-evaluate" additional="false">thunlp/explore-and-evaluate</pwccode>
    </paper>
    <paper id="516">
      <title>Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning</title>
      <author><first>Yi</first><last>Yang</last></author>
      <author><first>Arzoo</first><last>Katiyar</last></author>
      <pages>6365–6375</pages>
      <abstract>We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference. Our system uses a supervised NER model trained on the source domain, as a <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extractor</a>. Across several test domains, we show that a <a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search">nearest neighbor classifier</a> in this <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">feature-space</a> is far more effective than the standard meta-learning approaches. We further propose a cheap but effective method to capture the label dependencies between entity tags without expensive CRF training. We show that our method of combining structured decoding with nearest neighbor learning achieves state-of-the-art performance on standard few-shot NER evaluation tasks, improving F1 scores by 6 % to 16 % absolute points over prior meta-learning based systems.</abstract>
      <url hash="f2cf4c9b">2020.emnlp-main.516</url>
      <doi>10.18653/v1/2020.emnlp-main.516</doi>
      <video href="https://slideslive.com/38939200" />
      <bibkey>yang-katiyar-2020-simple</bibkey>
      <pwccode url="https://github.com/asappresearch/structshot" additional="false">asappresearch/structshot</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="517">
      <title>Learning Structured Representations of Entity Names using Active Learning and Weak Supervision<fixed-case>A</fixed-case>ctive <fixed-case>L</fixed-case>earning and Weak Supervision</title>
      <author><first>Kun</first><last>Qian</last></author>
      <author><first>Poornima</first><last>Chozhiyath Raman</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <author><first>Lucian</first><last>Popa</last></author>
      <pages>6376–6383</pages>
      <abstract>Structured representations of entity names are useful for many entity-related tasks such as <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity normalization</a> and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging. In this paper, we present a novel learning framework that combines <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a> and weak supervision to solve this problem. Our experimental evaluation show that this <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> enables the learning of high-quality models from merely a dozen or so labeled examples.</abstract>
      <url hash="41704da5">2020.emnlp-main.517</url>
      <doi>10.18653/v1/2020.emnlp-main.517</doi>
      <video href="https://slideslive.com/38938677" />
      <bibkey>qian-etal-2020-learning</bibkey>
      <pwccode url="https://github.com/System-T/PARTNER" additional="false">System-T/PARTNER</pwccode>
    </paper>
    <paper id="518">
      <title>Entity Enhanced BERT Pre-training for Chinese NER<fixed-case>BERT</fixed-case> Pre-training for <fixed-case>C</fixed-case>hinese <fixed-case>NER</fixed-case></title>
      <author><first>Chen</first><last>Jia</last></author>
      <author><first>Yuefeng</first><last>Shi</last></author>
      <author><first>Qinrong</first><last>Yang</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>6384–6396</pages>
      <abstract>Character-level BERT pre-trained in <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a> suffers a limitation of lacking lexicon information, which shows effectiveness for <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese NER</a>. To integrate the <a href="https://en.wikipedia.org/wiki/Lexicon">lexicon</a> into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method. In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method. We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations. In addition, an entity classification task helps inject the entity information into <a href="https://en.wikipedia.org/wiki/Parameter">model parameters</a> in pre-training. The pre-trained models are used for NER fine-tuning. Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.</abstract>
      <url hash="ca8991c6">2020.emnlp-main.518</url>
      <doi>10.18653/v1/2020.emnlp-main.518</doi>
      <video href="https://slideslive.com/38939232" />
      <bibkey>jia-etal-2020-entity</bibkey>
    </paper>
    <paper id="519">
      <title>Scalable Zero-shot Entity Linking with Dense Entity Retrieval</title>
      <author><first>Ledell</first><last>Wu</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Martin</first><last>Josifoski</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>6397–6407</pages>
      <abstract>This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">mention context</a> and the <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity descriptions</a>. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with <a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search">nearest neighbor search</a> (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.</abstract>
      <url hash="4ea66737">2020.emnlp-main.519</url>
      <doi>10.18653/v1/2020.emnlp-main.519</doi>
      <video href="https://slideslive.com/38939238" />
      <bibkey>wu-etal-2020-scalable</bibkey>
      <pwccode url="https://github.com/facebookresearch/BLINK" additional="true">facebookresearch/BLINK</pwccode>
    </paper>
    <paper id="520">
      <title>A Dataset for Tracking Entities in Open Domain Procedural Text</title>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Keisuke</first><last>Sakaguchi</last></author>
      <author><first>Bhavana</first><last>Dalvi</last></author>
      <author><first>Dheeraj</first><last>Rajagopal</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <author><first>Michal</first><last>Guerquin</last></author>
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>6408–6417</pages>
      <abstract>We present the first <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using <a href="https://en.wikipedia.org/wiki/Potato">potatoes</a>, a car window may transition between being foggy, sticky, opaque, and clear. Previous formulations of this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> provide the text and entities involved, and ask how those entities change for just a small, pre-defined set of attributes (e.g., location), limiting their fidelity. Our solution is a new task formulation where given just a procedural text as input, the task is to generate a set of state change tuples (entity, attribute, before-state, after-state) for each step, where the entity, attribute, and state values must be predicted from an open vocabulary. Using <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a>, we create OPENPI, a high-quality (91.5 % coverage as judged by humans and completely vetted), and large-scale dataset comprising 29,928 state changes over 4,050 sentences from 810 procedural real-world paragraphs from <a href="https://en.wikipedia.org/wiki/WikiHow">WikiHow.com</a>. A current state-of-the-art generation model on this <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> achieves 16.1 % <a href="https://en.wikipedia.org/wiki/F-number">F1</a> based on BLEU metric, leaving enough room for novel model architectures.</abstract>
      <url hash="4f6b6193">2020.emnlp-main.520</url>
      <doi>10.18653/v1/2020.emnlp-main.520</doi>
      <video href="https://slideslive.com/38939254" />
      <bibkey>tandon-etal-2020-dataset</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/open-pi">Open PI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/alfred">ALFRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/babi-1">bAbI</pwcdataset>
    </paper>
    <paper id="521">
      <title>Design Challenges in Low-resource Cross-lingual Entity Linking</title>
      <author><first>Xingyu</first><last>Fu</last></author>
      <author><first>Weijia</first><last>Shi</last></author>
      <author><first>Xiaodong</first><last>Yu</last></author>
      <author><first>Zian</first><last>Zhao</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>6418–6432</pages>
      <abstract>Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, has seen a lot of research in recent years, with a range of promising techniques. However, current techniques do not rise to the challenges introduced by text in low-resource languages (LRL) and, surprisingly, fail to generalize to text not taken from <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, on which they are usually trained. This paper provides a thorough analysis of low-resource XEL techniques, focusing on the key step of identifying candidate English Wikipedia titles that correspond to a given foreign language mention. Our analysis indicates that current methods are limited by their reliance on Wikipedia’s interlanguage links and thus suffer when the foreign language’s Wikipedia is small. We conclude that the LRL setting requires the use of outside-Wikipedia cross-lingual resources and present a simple yet effective zero-shot XEL system, QuEL, that utilizes search engines query logs. With experiments on 25 languages, QuEL shows an average increase of 25 % in gold candidate recall and of 13 % in end-to-end linking accuracy over state-of-the-art baselines.</abstract>
      <url hash="46022de2">2020.emnlp-main.521</url>
      <doi>10.18653/v1/2020.emnlp-main.521</doi>
      <video href="https://slideslive.com/38939339" />
      <bibkey>fu-etal-2020-design</bibkey>
    </paper>
    <paper id="523">
      <title>LUKE : Deep Contextualized Entity Representations with Entity-aware Self-attention<fixed-case>LUKE</fixed-case>: Deep Contextualized Entity Representations with Entity-aware Self-attention</title>
      <author><first>Ikuya</first><last>Yamada</last></author>
      <author><first>Akari</first><last>Asai</last></author>
      <author><first>Hiroyuki</first><last>Shindo</last></author>
      <author><first>Hideaki</first><last>Takeda</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <pages>6442–6454</pages>
      <abstract>Entity representations are useful in <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language tasks</a> involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets : Open Entity (entity typing), TACRED (relation classification), <a href="https://en.wikipedia.org/wiki/Named_entity_recognition">CoNLL-2003 (named entity recognition)</a>, ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.</abstract>
      <url hash="2226eb6f">2020.emnlp-main.523</url>
      <doi>10.18653/v1/2020.emnlp-main.523</doi>
      <video href="https://slideslive.com/38938803" />
      <bibkey>yamada-etal-2020-luke</bibkey>
      <pwccode url="https://github.com/studio-ousia/luke" additional="true">studio-ousia/luke</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/open-entity-1">Open Entity</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/record">ReCoRD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="527">
      <title>Template Guided Text Generation for Task-Oriented Dialogue</title>
      <author><first>Mihir</first><last>Kale</last></author>
      <author><first>Abhinav</first><last>Rastogi</last></author>
      <pages>6505–6520</pages>
      <abstract>Virtual assistants such as <a href="https://en.wikipedia.org/wiki/Google_Assistant">Google Assistant</a>, <a href="https://en.wikipedia.org/wiki/Amazon_Alexa">Amazon Alexa</a>, and <a href="https://en.wikipedia.org/wiki/Siri">Apple Siri</a> enable users to interact with a large number of services and APIs on the web using <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a>. In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of <a href="https://en.wikipedia.org/wiki/Application_programming_interface">APIs</a>. First, we propose a schema-guided approach which conditions the generation on a schema describing the <a href="https://en.wikipedia.org/wiki/Application_programming_interface">API</a> in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>. Our second method investigates the use of a small number of <a href="https://en.wikipedia.org/wiki/Template_processor">templates</a>, growing linearly in number of slots, to convey the <a href="https://en.wikipedia.org/wiki/Semantics_(computer_science)">semantics</a> of the <a href="https://en.wikipedia.org/wiki/Application_programming_interface">API</a>. To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance. A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text. Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.</abstract>
      <url hash="209de961">2020.emnlp-main.527</url>
      <doi>10.18653/v1/2020.emnlp-main.527</doi>
      <video href="https://slideslive.com/38939152" />
      <bibkey>kale-rastogi-2020-template</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/e2e">E2E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="528">
      <title>MOCHA : A Dataset for Training and Evaluating Generative Reading Comprehension Metrics<fixed-case>MOCHA</fixed-case>: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics</title>
      <author><first>Anthony</first><last>Chen</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>6521–6532</pages>
      <abstract>Posing <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a>. To address this, we introduce a <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmark</a> for training and evaluating generative reading comprehension metrics : MOdeling Correctness with Human Annotations. MOCHA contains 40 K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for Reading Comprehension, LERC, to mimic human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When we evaluate robustness on minimal pairs, LERC achieves 80 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. MOCHA presents a challenging problem for developing accurate and robust generative reading comprehension metrics.</abstract>
      <url hash="1c1070b2">2020.emnlp-main.528</url>
      <doi>10.18653/v1/2020.emnlp-main.528</doi>
      <video href="https://slideslive.com/38939212" />
      <bibkey>chen-etal-2020-mocha</bibkey>
      <pwccode url="https://github.com/anthonywchen/MOCHA" additional="false">anthonywchen/MOCHA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mocha">MOCHA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mcscript">MCScript</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quoref">Quoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/social-iqa">Social IQA</pwcdataset>
    </paper>
    <paper id="529">
      <title>Plan ahead : Self-Supervised Text Planning for Paragraph Completion Task</title>
      <author><first>Dongyeop</first><last>Kang</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>6533–6543</pages>
      <abstract>Despite the recent success of contextualized language models on various NLP tasks, <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> itself can not capture textual coherence of a long, multi-sentence document (e.g., a paragraph). Humans often make structural decisions on what and how to say about before making utterances. Guiding surface realization with such high-level decisions and structuring text in a coherent way is essentially called a <a href="https://en.wikipedia.org/wiki/Planning">planning process</a>. Where can the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> learn such high-level coherence? A paragraph itself contains various forms of inductive coherence signals called self-supervision in this work, such as sentence orders, topical keywords, rhetorical structures, and so on. Motivated by that, this work proposes a new paragraph completion task PARCOM ; predicting masked sentences in a paragraph. However, the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> suffers from predicting and selecting appropriate topical content with respect to the given context. To address that, we propose a self-supervised text planner SSPlanner that predicts what to say first (content prediction), then guides the pretrained language model (surface realization) using the predicted content. SSPlanner outperforms the baseline generation models on the paragraph completion task in both automatic and human evaluation. We also find that a combination of noun and verb types of keywords is the most effective for content selection. As more number of <a href="https://en.wikipedia.org/wiki/Index_term">content keywords</a> are provided, overall generation quality also increases.</abstract>
      <url hash="fd714e70">2020.emnlp-main.529</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c59bebe0">2020.emnlp-main.529.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.emnlp-main.529</doi>
      <video href="https://slideslive.com/38939248" />
      <bibkey>kang-hovy-2020-plan</bibkey>
    </paper>
    <paper id="531">
      <title>Towards Persona-Based Empathetic Conversational Models</title>
      <author><first>Peixiang</first><last>Zhong</last></author>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Wang</last></author>
      <author><first>Yong</first><last>Liu</last></author>
      <author><first>Chunyan</first><last>Miao</last></author>
      <pages>6556–6566</pages>
      <abstract>Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In <a href="https://en.wikipedia.org/wiki/Psychology">Psychology</a>, <a href="https://en.wikipedia.org/wiki/Persona">persona</a> has been shown to be highly correlated to <a href="https://en.wikipedia.org/wiki/Personality">personality</a>, which in turn influences <a href="https://en.wikipedia.org/wiki/Empathy">empathy</a>. In addition, our empirical analysis also suggests that <a href="https://en.wikipedia.org/wiki/Persona">persona</a> plays an important role in empathetic conversations. To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of <a href="https://en.wikipedia.org/wiki/Persona">persona</a> on empathetic responding. Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations. We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset. Finally, we conduct extensive experiments to investigate the impact of <a href="https://en.wikipedia.org/wiki/Persona">persona</a> on empathetic responding. Notably, our results show that <a href="https://en.wikipedia.org/wiki/Persona">persona</a> improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between <a href="https://en.wikipedia.org/wiki/Persona">persona</a> and <a href="https://en.wikipedia.org/wiki/Empathy">empathy</a> in human conversations.</abstract>
      <url hash="a607581c">2020.emnlp-main.531</url>
      <doi>10.18653/v1/2020.emnlp-main.531</doi>
      <video href="https://slideslive.com/38938660" />
      <bibkey>zhong-etal-2020-towards</bibkey>
      <pwccode url="https://github.com/zhongpeixiang/PEC" additional="false">zhongpeixiang/PEC</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pec">PEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="539">
      <title>Profile Consistency Identification for Open-domain Dialogue Agents</title>
      <author><first>Haoyu</first><last>Song</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>Wei-Nan</first><last>Zhang</last></author>
      <author><first>Zhengyu</first><last>Zhao</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Xiaojiang</first><last>Liu</last></author>
      <pages>6651–6662</pages>
      <abstract>Maintaining a consistent attribute profile is crucial for dialogue agents to naturally converse with humans. Existing studies on improving attribute consistency mainly explored how to incorporate <a href="https://en.wikipedia.org/wiki/Attribute_(computing)">attribute information</a> in the responses, but few efforts have been made to identify the <a href="https://en.wikipedia.org/wiki/Consistency_(database_systems)">consistency relations</a> between response and attribute profile. To facilitate the study of profile consistency identification, we create a large-scale human-annotated dataset with over 110 K single-turn conversations and their key-value attribute profiles. Explicit relation between <a href="https://en.wikipedia.org/wiki/Information_retrieval">response</a> and profile is manually labeled. We also propose a key-value structure information enriched BERT model to identify the profile consistency, and it gained improvements over strong baselines. Further evaluations on <a href="https://en.wikipedia.org/wiki/Downstream_(networking)">downstream tasks</a> demonstrate that the profile consistency identification model is conducive for improving dialogue consistency.</abstract>
      <url hash="cf8fb3e3">2020.emnlp-main.539</url>
      <doi>10.18653/v1/2020.emnlp-main.539</doi>
      <video href="https://slideslive.com/38938821" />
      <bibkey>song-etal-2020-profile</bibkey>
      <pwccode url="https://github.com/songhaoyu/KvPI" additional="false">songhaoyu/KvPI</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/personaldialog">PersonalDialog</pwcdataset>
    </paper>
    <paper id="540">
      <title>An Element-aware Multi-representation Model for Law Article Prediction</title>
      <author><first>Huilin</first><last>Zhong</last></author>
      <author><first>Junsheng</first><last>Zhou</last></author>
      <author><first>Weiguang</first><last>Qu</last></author>
      <author><first>Yunfei</first><last>Long</last></author>
      <author><first>Yanhui</first><last>Gu</last></author>
      <pages>6663–6668</pages>
      <abstract>Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label samples. In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> uses the labeled elements of law articles to extract fact description features from multiple angles. It generates multiple representations of a fact for <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>. Every label has a law-aware fact representation to encode more information. To capture the dependencies between law articles, the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> also introduces a self-attention mechanism between multiple representations. Compared with baseline models like TopJudge, this <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> improves the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 5.84 %, the <a href="https://en.wikipedia.org/wiki/Macroeconomic_model">macro F1</a> of 6.42 %, and the micro F1 of 4.28 %.</abstract>
      <url hash="e2245f3b">2020.emnlp-main.540</url>
      <doi>10.18653/v1/2020.emnlp-main.540</doi>
      <video href="https://slideslive.com/38938872" />
      <bibkey>zhong-etal-2020-element</bibkey>
    </paper>
    <paper id="541">
      <title>Recurrent Event Network : Autoregressive Structure Inferenceover Temporal Knowledge Graphs</title>
      <author><first>Woojeong</first><last>Jin</last></author>
      <author><first>Meng</first><last>Qu</last></author>
      <author><first>Xisen</first><last>Jin</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>6669–6683</pages>
      <abstract>Knowledge graph reasoning is a critical task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. The <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE-Net), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a <a href="https://en.wikipedia.org/wiki/Probability_distribution">probability distribution</a> conditioned on temporal sequences of past knowledge graphs. Specifically, our RE-Net employs a recurrent event encoder to encode past facts, and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two <a href="https://en.wikipedia.org/wiki/Module_(mathematics)">modules</a>. We evaluate our proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RE-Net, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets.</abstract>
      <url hash="3fa27a64">2020.emnlp-main.541</url>
      <doi>10.18653/v1/2020.emnlp-main.541</doi>
      <video href="https://slideslive.com/38938911" />
      <bibkey>jin-etal-2020-recurrent</bibkey>
    </paper>
    <paper id="543">
      <title>Less is More : <a href="https://en.wikipedia.org/wiki/Attentional_control">Attention Supervision</a> with Counterfactuals for Text Classification</title>
      <author><first>Seungtaek</first><last>Choi</last></author>
      <author><first>Haeju</first><last>Park</last></author>
      <author><first>Jinyoung</first><last>Yeo</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <pages>6695–6704</pages>
      <abstract>We aim to leverage <a href="https://en.wikipedia.org/wiki/Human–computer_interaction">human and machine intelligence</a> together for <a href="https://en.wikipedia.org/wiki/Attentional_control">attention supervision</a>. Specifically, we show that human annotation cost can be kept reasonably low, while its <a href="https://en.wikipedia.org/wiki/Quality_(business)">quality</a> can be enhanced by <a href="https://en.wikipedia.org/wiki/Machine_vision">machine self-supervision</a>. Specifically, for this goal, we explore the advantage of counterfactual reasoning, over associative reasoning typically used in attention supervision. Our empirical results show that this machine-augmented human attention supervision is more effective than existing methods requiring a higher annotation cost, in text classification tasks, including <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> and news categorization.</abstract>
      <url hash="275ae641">2020.emnlp-main.543</url>
      <doi>10.18653/v1/2020.emnlp-main.543</doi>
      <video href="https://slideslive.com/38939245" />
      <bibkey>choi-etal-2020-less</bibkey>
    </paper>
    <paper id="544">
      <title>MODE-LSTM : A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification<fixed-case>MODE</fixed-case>-<fixed-case>LSTM</fixed-case>: A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification</title>
      <author><first>Qianli</first><last>Ma</last></author>
      <author><first>Zhenxi</first><last>Lin</last></author>
      <author><first>Jiangyue</first><last>Yan</last></author>
      <author><first>Zipeng</first><last>Chen</last></author>
      <author><first>Liuhong</first><last>Yu</last></author>
      <pages>6705–6715</pages>
      <abstract>The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences. Most existing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> because of relatively limited datasets. In this paper, we propose a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features. We disentangle the hidden state of the LSTM into several independently updated small hidden states and apply an orthogonal constraint on their recurrent matrices. We then equip this <a href="https://en.wikipedia.org/wiki/Biomolecular_structure">structure</a> with sliding windows of different sizes for extracting multi-scale n-gram features. Extensive experiments demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves better or competitive performance against state-of-the-art baselines on eight benchmark datasets. We also combine our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with BERT to further boost the generalization performance.</abstract>
      <url hash="1d26fa97">2020.emnlp-main.544</url>
      <doi>10.18653/v1/2020.emnlp-main.544</doi>
      <video href="https://slideslive.com/38939250" />
      <bibkey>ma-etal-2020-mode</bibkey>
    </paper>
    <paper id="545">
      <title>HSCNN : A Hybrid-Siamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification<fixed-case>HSCNN</fixed-case>: A Hybrid-<fixed-case>S</fixed-case>iamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification</title>
      <author><first>Wenshuo</first><last>Yang</last></author>
      <author><first>Jiyi</first><last>Li</last></author>
      <author><first>Fumiyo</first><last>Fukumoto</last></author>
      <author><first>Yanming</first><last>Ye</last></author>
      <pages>6716–6722</pages>
      <abstract>The data imbalance problem is a crucial issue for the multi-label text classification. Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data. We propose a hybrid solution which adapts general networks for the head categories, and few-shot techniques for the tail categories. We propose a Hybrid-Siamese Convolutional Neural Network (HSCNN) with additional technical attributes, i.e., a multi-task architecture based on Single and Siamese networks ; a category-specific similarity in the Siamese structure ; a specific sampling method for training HSCNN. The results using two benchmark datasets and three <a href="https://en.wikipedia.org/wiki/Loss_function">loss objectives</a> show that our method can improve the performance of Single networks with diverse loss objectives on the tail or entire categories.</abstract>
      <url hash="dd672bf9">2020.emnlp-main.545</url>
      <doi>10.18653/v1/2020.emnlp-main.545</doi>
      <bibkey>yang-etal-2020-hscnn</bibkey>
    </paper>
    <paper id="546">
      <title>Multi-Stage Pre-training for Automated Chinese Essay Scoring<fixed-case>C</fixed-case>hinese Essay Scoring</title>
      <author><first>Wei</first><last>Song</last></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Ruiji</first><last>Fu</last></author>
      <author><first>Lizhen</first><last>Liu</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Miaomiao</first><last>Cheng</last></author>
      <pages>6723–6733</pages>
      <abstract>This paper proposes a pre-training based automated Chinese essay scoring method. The method involves three components : weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning. An essay scorer is first pre- trained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision. The pre-trained essay scorer would be further fine-tuned on previously rated es- says from existing prompts, which have the same score range with the target prompt and provide extra supervision. At last, the <a href="https://en.wikipedia.org/wiki/Score_(sport)">scorer</a> is fine-tuned on the target-prompt training data. The evaluation on four prompts shows that this method can improve a state-of-the-art neural essay scorer in terms of <a href="https://en.wikipedia.org/wiki/Effectiveness">effectiveness</a> and domain adaptation ability, while in-depth analysis also reveals its limitations..</abstract>
      <url hash="584fe254">2020.emnlp-main.546</url>
      <doi>10.18653/v1/2020.emnlp-main.546</doi>
      <video href="https://slideslive.com/38939367" />
      <bibkey>song-etal-2020-multi</bibkey>
    </paper>
    <paper id="547">
      <title>Multi-hop Inference for Question-driven Summarization</title>
      <author><first>Yang</first><last>Deng</last></author>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>6734–6744</pages>
      <abstract>Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions. In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries. Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer. A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms state-of-the-art methods on two non-factoid QA datasets, namely <a href="https://en.wikipedia.org/wiki/WikiHow">WikiHow</a> and PubMedQA.</abstract>
      <url hash="6f5ac7ce">2020.emnlp-main.547</url>
      <doi>10.18653/v1/2020.emnlp-main.547</doi>
      <bibkey>deng-etal-2020-multi</bibkey>
      <pwccode url="https://github.com/dengyang17/msg" additional="false">dengyang17/msg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmedqa">PubMedQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="548">
      <title>Towards Interpretable Reasoning over Paragraph Effects in Situation</title>
      <author><first>Mucheng</first><last>Ren</last></author>
      <author><first>Xiubo</first><last>Geng</last></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>6745–6758</pages>
      <abstract>We focus on the task of reasoning over paragraph effects in situation, which requires a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to understand the cause and effect described in a background paragraph, and apply the <a href="https://en.wikipedia.org/wiki/Knowledge">knowledge</a> to a novel situation. Existing works ignore the complicated reasoning process and solve it with a one-step black box model. Inspired by <a href="https://en.wikipedia.org/wiki/Cognition">human cognitive processes</a>, in this paper we propose a sequential approach for this task which explicitly models each step of the <a href="https://en.wikipedia.org/wiki/Reason">reasoning process</a> with <a href="https://en.wikipedia.org/wiki/Modular_programming">neural network modules</a>. In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model. Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach.</abstract>
      <url hash="acbe5612">2020.emnlp-main.548</url>
      <doi>10.18653/v1/2020.emnlp-main.548</doi>
      <video href="https://slideslive.com/38938859" />
      <bibkey>ren-etal-2020-towards</bibkey>
      <pwccode url="https://github.com/Borororo/interpretable_ropes" additional="false">Borororo/interpretable_ropes</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ropes">ROPES</pwcdataset>
    </paper>
    <paper id="549">
      <title>Question Directed Graph Attention Network for <a href="https://en.wikipedia.org/wiki/Numerical_analysis">Numerical Reasoning</a> over Text</title>
      <author><first>Kunlong</first><last>Chen</last></author>
      <author><first>Weidi</first><last>Xu</last></author>
      <author><first>Xingyi</first><last>Cheng</last></author>
      <author><first>Zou</first><last>Xiaochuan</last></author>
      <author><first>Yuyu</first><last>Zhang</last></author>
      <author><first>Le</first><last>Song</last></author>
      <author><first>Taifeng</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Qi</last></author>
      <author><first>Wei</first><last>Chu</last></author>
      <pages>6759–6768</pages>
      <abstract>Numerical reasoning over texts, such as <a href="https://en.wikipedia.org/wiki/Addition">addition</a>, <a href="https://en.wikipedia.org/wiki/Subtraction">subtraction</a>, sorting and counting, is a challenging machine reading comprehension task, since it requires both <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a> and <a href="https://en.wikipedia.org/wiki/Arithmetic">arithmetic computation</a>. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a>, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.</abstract>
      <url hash="2ae1ea7c">2020.emnlp-main.549</url>
      <doi>10.18653/v1/2020.emnlp-main.549</doi>
      <video href="https://slideslive.com/38939022" />
      <bibkey>chen-etal-2020-question</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="550">
      <title>Dense Passage Retrieval for Open-Domain Question Answering</title>
      <author><first>Vladimir</first><last>Karpukhin</last></author>
      <author><first>Barlas</first><last>Oguz</last></author>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Patrick</first><last>Lewis</last></author>
      <author><first>Ledell</first><last>Wu</last></author>
      <author><first>Sergey</first><last>Edunov</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <author><first>Wen-tau</first><last>Yih</last></author>
      <pages>6769–6781</pages>
      <abstract>Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as <a href="https://en.wikipedia.org/wiki/TF-IDF">TF-IDF</a> or <a href="https://en.wikipedia.org/wiki/BM25">BM25</a>, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19 % absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.</abstract>
      <url hash="351ca9c5">2020.emnlp-main.550</url>
      <doi>10.18653/v1/2020.emnlp-main.550</doi>
      <video href="https://slideslive.com/38939151" />
      <bibkey>karpukhin-etal-2020-dense</bibkey>
      <pwccode url="https://github.com/facebookresearch/DPR" additional="true">facebookresearch/DPR</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="551">
      <title>Distilling Structured Knowledge for Text-Based Relational Reasoning</title>
      <author><first>Jin</first><last>Dong</last></author>
      <author><first>Marc-Antoine</first><last>Rondeau</last></author>
      <author><first>William L.</first><last>Hamilton</last></author>
      <pages>6782–6791</pages>
      <abstract>There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text. However, there remains a substantial performance gap between NLP models for relational reasoning and models based on graph neural networks (GNNs), which have access to an underlying symbolic representation of the text. In this work, we investigate how the structured knowledge of a GNN can be distilled into various NLP models in order to improve their performance. We first pre-train a GNN on a reasoning task using structured inputs and then incorporate its knowledge into an <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP model</a> (e.g., an LSTM) via knowledge distillation. To overcome the difficulty of cross-modal knowledge transfer, we also employ a contrastive learning based module to align the latent representations of NLP models and the GNN. We test our approach with two state-of-the-art NLP models on 13 different inductive reasoning datasets from the CLUTRR benchmark and obtain significant improvements.</abstract>
      <url hash="b7e0294f">2020.emnlp-main.551</url>
      <doi>10.18653/v1/2020.emnlp-main.551</doi>
      <video href="https://slideslive.com/38938727" />
      <bibkey>dong-etal-2020-distilling</bibkey>
    </paper>
    <paper id="552">
      <title>Asking without Telling : Exploring Latent Ontologies in Contextual Representations</title>
      <author><first>Julian</first><last>Michael</last></author>
      <author><first>Jan A.</first><last>Botha</last></author>
      <author><first>Ian</first><last>Tenney</last></author>
      <pages>6792–6812</pages>
      <abstract>The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn : do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe’s inputs. Without access to fine-grained gold labels, LSL extracts <a href="https://en.wikipedia.org/wiki/Emergence">emergent structure</a> from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of <a href="https://en.wikipedia.org/wiki/Emergence">emergent structure</a> in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.</abstract>
      <url hash="5c1eecd3">2020.emnlp-main.552</url>
      <doi>10.18653/v1/2020.emnlp-main.552</doi>
      <video href="https://slideslive.com/38938836" />
      <bibkey>michael-etal-2020-asking</bibkey>
    </paper>
    <paper id="553">
      <title>Pretrained Language Model <a href="https://en.wikipedia.org/wiki/Embryology">Embryology</a> : The Birth of ALBERT<fixed-case>P</fixed-case>retrained Language Model Embryology: <fixed-case>T</fixed-case>he Birth of <fixed-case>ALBERT</fixed-case></title>
      <author><first>Cheng-Han</first><last>Chiang</last></author>
      <author><first>Sung-Feng</first><last>Huang</last></author>
      <author><first>Hung-yi</first><last>Lee</last></author>
      <pages>6813–6828</pages>
      <abstract>While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the <a href="https://en.wikipedia.org/wiki/Embryology">embryology</a> of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and <a href="https://en.wikipedia.org/wiki/World_knowledge">world knowledge</a> do not generally improve as pretraining proceeds, nor do downstream tasks’ performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> with more comprehensive knowledge. We provide <a href="https://en.wikipedia.org/wiki/Source_code">source codes</a> and pretrained models to reproduce our results at.<i>embryology</i> of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks’ performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We provide source codes and pretrained models to reproduce our results at <url>https://github.com/d223302/albert-embryology</url>.</abstract>
      <url hash="148921f4">2020.emnlp-main.553</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b412e810">2020.emnlp-main.553.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.553</doi>
      <video href="https://slideslive.com/38938879" />
      <bibkey>chiang-etal-2020-pretrained</bibkey>
      <pwccode url="https://github.com/d223302/albert-embryology" additional="false">d223302/albert-embryology</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="556">
      <title>You are grounded ! : Latent Name Artifacts in Pre-trained Language Models</title>
      <author><first>Vered</first><last>Shwartz</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <author><first>Oyvind</first><last>Tafjord</last></author>
      <pages>6850–6861</pages>
      <abstract>Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific <a href="https://en.wikipedia.org/wiki/Non-physical_entity">entities</a>, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for ‘Donald is a’ substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional <a href="https://en.wikipedia.org/wiki/Training">pre-training</a> on different corpora may mitigate this bias.</abstract>
      <url hash="a17eee51">2020.emnlp-main.556</url>
      <doi>10.18653/v1/2020.emnlp-main.556</doi>
      <video href="https://slideslive.com/38938640" />
      <bibkey>shwartz-etal-2020-grounded</bibkey>
    </paper>
    <paper id="558">
      <title>Grounded Adaptation for Zero-shot Executable Semantic Parsing</title>
      <author><first>Victor</first><last>Zhong</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Sida I.</first><last>Wang</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>6869–6882</pages>
      <abstract>We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a>. Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified through execution. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.</abstract>
      <url hash="a66d104e">2020.emnlp-main.558</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b3820443">2020.emnlp-main.558.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.558</doi>
      <video href="https://slideslive.com/38938845" />
      <bibkey>zhong-etal-2020-grounded</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sparc">SParC</pwcdataset>
    </paper>
    <paper id="561">
      <title>What Do You Mean by That? A Parser-Independent Interactive Approach for Enhancing Text-to-SQL<fixed-case>SQL</fixed-case></title>
      <author><first>Yuntao</first><last>Li</last></author>
      <author><first>Bei</first><last>Chen</last></author>
      <author><first>Qian</first><last>Liu</last></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <author><first>Yan</first><last>Zhang</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <pages>6913–6922</pages>
      <abstract>In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a> may fall short when they are deployed in real systems. One main reason stems from the difficulty of fully understanding the users’ natural language questions. In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers. Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a>. These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.</abstract>
      <url hash="d8640966">2020.emnlp-main.561</url>
      <doi>10.18653/v1/2020.emnlp-main.561</doi>
      <video href="https://slideslive.com/38939001" />
      <bibkey>li-etal-2020-mean</bibkey>
    </paper>
    <paper id="562">
      <title>DuSQL : A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset<fixed-case>D</fixed-case>u<fixed-case>SQL</fixed-case>: A Large-Scale and Pragmatic <fixed-case>C</fixed-case>hinese Text-to-<fixed-case>SQL</fixed-case> Dataset</title>
      <author><first>Lijie</first><last>Wang</last></author>
      <author><first>Ao</first><last>Zhang</last></author>
      <author><first>Kun</first><last>Wu</last></author>
      <author><first>Ke</first><last>Sun</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>6923–6935</pages>
      <abstract>Due to the lack of <a href="https://en.wikipedia.org/wiki/Data">labeled data</a>, previous research on text-to-SQL parsing mainly focuses on <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the cross-domain text-to-SQL task, containing 200 databases, 813 tables, and 23,797 question / SQL pairs. Our new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> has three major characteristics. First, by manually analyzing questions from several representative applications, we try to figure out the true distribution of SQL queries in real-life needs. Second, DuSQL contains a considerable proportion of SQL queries involving row or column calculations, motivated by our analysis on the SQL query distributions. Finally, we adopt an effective data construction framework via <a href="https://en.wikipedia.org/wiki/Human–computer_interaction">human-computer collaboration</a>. The basic idea is automatically generating <a href="https://en.wikipedia.org/wiki/SQL">SQL queries</a> based on the SQL grammar and constrained by the given database. This paper describes in detail the construction process and <a href="https://en.wikipedia.org/wiki/Data_analysis">data statistics</a> of DuSQL. Moreover, we present and compare performance of several open-source text-to-SQL parsers with minor modification to accommodate <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, including a simple yet effective extension to IRNet for handling calculation SQL queries.</abstract>
      <url hash="be48c08a">2020.emnlp-main.562</url>
      <doi>10.18653/v1/2020.emnlp-main.562</doi>
      <video href="https://slideslive.com/38938688" />
      <bibkey>wang-etal-2020-dusql</bibkey>
    </paper>
    <paper id="563">
      <title>Mention Extraction and Linking for SQL Query Generation<fixed-case>SQL</fixed-case> Query Generation</title>
      <author><first>Jianqiang</first><last>Ma</last></author>
      <author><first>Zeyu</first><last>Yan</last></author>
      <author><first>Shuai</first><last>Pang</last></author>
      <author><first>Yang</first><last>Zhang</last></author>
      <author><first>Jianping</first><last>Shen</last></author>
      <pages>6936–6942</pages>
      <abstract>On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots. Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses. To solve these problems, this paper proposes a novel extraction-linking approach, where a unified extractor recognizes all types of slot mentions appearing in the question sentence before a linker maps the recognized columns to the table schema to generate executable SQL queries. Trained with automatically generated annotations, the proposed <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> achieves the first place on the WikiSQL benchmark.</abstract>
      <url hash="0428737a">2020.emnlp-main.563</url>
      <doi>10.18653/v1/2020.emnlp-main.563</doi>
      <video href="https://slideslive.com/38939352" />
      <bibkey>ma-etal-2020-mention</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="565">
      <title>A Multi-Task Incremental Learning Framework with Category Name Embedding for Aspect-Category Sentiment Analysis</title>
      <author><first>Zehui</first><last>Dai</last></author>
      <author><first>Cheng</first><last>Peng</last></author>
      <author><first>Huajie</first><last>Chen</last></author>
      <author><first>Yadong</first><last>Ding</last></author>
      <pages>6955–6965</pages>
      <abstract>(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories. Incremental learning on new categories is necessary for (T)ACSA real applications. Though current multi-task learning models achieve good performance in (T)ACSA tasks, they suffer from catastrophic forgetting problems in (T)ACSA incremental learning tasks. In this paper, to make <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> feasible for <a href="https://en.wikipedia.org/wiki/Incremental_learning">incremental learning</a>, we proposed Category Name Embedding network (CNE-net). We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., <a href="https://en.wikipedia.org/wiki/Category_(mathematics)">category name</a>, for task discrimination. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieved state-of-the-art on two (T)ACSA benchmark datasets. Furthermore, we proposed a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for (T)ACSA incremental learning and achieved the best performance compared with other strong baselines.</abstract>
      <url hash="44943918">2020.emnlp-main.565</url>
      <attachment type="OptionalSupplementaryMaterial" hash="d735ee19">2020.emnlp-main.565.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.565</doi>
      <video href="https://slideslive.com/38938865" />
      <bibkey>dai-etal-2020-multi</bibkey>
      <pwccode url="https://github.com/flak300S/emnlp2020_CNE-net" additional="false">flak300S/emnlp2020_CNE-net</pwccode>
    </paper>
    <paper id="566">
      <title>Train No Evil : Selective Masking for Task-Guided Pre-Training</title>
      <author><first>Yuxian</first><last>Gu</last></author>
      <author><first>Zhengyan</first><last>Zhang</last></author>
      <author><first>Xiaozhi</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>6966–6974</pages>
      <abstract>Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks. However, since the pre-training stage is typically task-agnostic and the fine-tuning stage usually suffers from insufficient supervised data, the models can not always well capture the domain-specific and task-specific patterns. In this paper, we propose a three-stage framework by adding a task-guided pre-training stage with selective masking between general pre-training and fine-tuning. In this stage, the model is trained by masked language modeling on in-domain unsupervised data to learn domain-specific patterns and we propose a novel selective masking strategy to learn task-specific patterns. Specifically, we design a method to measure the importance of each token in sequences and selectively mask the important tokens. Experimental results on two sentiment analysis tasks show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> can achieve comparable or even better performance with less than 50 % of computation cost, which indicates our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is both effective and efficient. The source code of this paper can be obtained from.<url>https://github.com/thunlp/SelectiveMasking</url>.</abstract>
      <url hash="ad56667a">2020.emnlp-main.566</url>
      <doi>10.18653/v1/2020.emnlp-main.566</doi>
      <video href="https://slideslive.com/38938884" />
      <bibkey>gu-etal-2020-train</bibkey>
      <pwccode url="https://github.com/thunlp/SelectiveMasking" additional="false">thunlp/SelectiveMasking</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
    </paper>
    <paper id="569">
      <title>APE : Argument Pair Extraction from Peer Review and Rebuttal via <a href="https://en.wikipedia.org/wiki/Multi-task_learning">Multi-task Learning</a><fixed-case>APE</fixed-case>: Argument Pair Extraction from Peer Review and Rebuttal via Multi-task Learning</title>
      <author><first>Liying</first><last>Cheng</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Qian</first><last>Yu</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>7000–7011</pages>
      <abstract>Peer review and rebuttal, with rich interactions and argumentative discussions in between, are naturally a good resource to mine arguments. However, few works study both of <a href="https://en.wikipedia.org/wiki/List_of_Latin_phrases_(I)">them</a> simultaneously. In this paper, we introduce a new argument pair extraction (APE) task on peer review and rebuttal in order to study the contents, the structure and the connections between them. We prepare a challenging <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> that contains 4,764 fully annotated review-rebuttal passage pairs from an open review platform to facilitate the study of this task. To automatically detect argumentative propositions and extract argument pairs from this <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>, we cast it as the combination of a sequence labeling task and a text relation classification task. Thus, we propose a multitask learning framework based on hierarchical LSTM networks. Extensive experiments and analysis demonstrate the effectiveness of our multi-task framework, and also show the challenges of the new <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> as well as motivate future research directions.</abstract>
      <url hash="e49ed82e">2020.emnlp-main.569</url>
      <doi>10.18653/v1/2020.emnlp-main.569</doi>
      <video href="https://slideslive.com/38939058" />
      <bibkey>cheng-etal-2020-ape</bibkey>
      <pwccode url="https://github.com/LiyingCheng95/ArgumentPairExtraction" additional="false">LiyingCheng95/ArgumentPairExtraction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rr">RR</pwcdataset>
    </paper>
    <paper id="576">
      <title>On the Ability and Limitations of Transformers to Recognize <a href="https://en.wikipedia.org/wiki/Formal_language">Formal Languages</a><fixed-case>A</fixed-case>bility and <fixed-case>L</fixed-case>imitations of <fixed-case>T</fixed-case>ransformers to <fixed-case>R</fixed-case>ecognize <fixed-case>F</fixed-case>ormal <fixed-case>L</fixed-case>anguages</title>
      <author><first>Satwik</first><last>Bhattamishra</last></author>
      <author><first>Kabir</first><last>Ahuja</last></author>
      <author><first>Navin</first><last>Goyal</last></author>
      <pages>7096–7116</pages>
      <abstract>Transformers have supplanted <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent models</a> in a large number of <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP tasks</a>. However, the differences in their abilities to model different <a href="https://en.wikipedia.org/wiki/Syntax">syntactic properties</a> remain largely unknown. Past works suggest that LSTMs generalize very well on <a href="https://en.wikipedia.org/wiki/Regular_language">regular languages</a> and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such <a href="https://en.wikipedia.org/wiki/Programming_language">languages</a> as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this <a href="https://en.wikipedia.org/wiki/Class_(biology)">subclass</a>, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of <a href="https://en.wikipedia.org/wiki/Regular_language">regular languages</a> with degrading performance as we make languages more complex according to a well-known measure of <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a>. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>.</abstract>
      <url hash="988cfbf3">2020.emnlp-main.576</url>
      <attachment type="OptionalSupplementaryMaterial" hash="62a9cbcc">2020.emnlp-main.576.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.576</doi>
      <video href="https://slideslive.com/38939173" />
      <bibkey>bhattamishra-etal-2020-ability</bibkey>
      <pwccode url="https://github.com/satwik77/Transformer-Formal-Languages" additional="false">satwik77/Transformer-Formal-Languages</pwccode>
    </paper>
    <paper id="583">
      <title>Is <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">Graph Structure</a> Necessary for Multi-hop Question Answering?<fixed-case>G</fixed-case>raph <fixed-case>S</fixed-case>tructure <fixed-case>N</fixed-case>ecessary for <fixed-case>M</fixed-case>ulti-hop <fixed-case>Q</fixed-case>uestion <fixed-case>A</fixed-case>nswering?</title>
      <author><first>Nan</first><last>Shao</last></author>
      <author><first>Yiming</first><last>Cui</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Shijin</first><last>Wang</last></author>
      <author><first>Guoping</first><last>Hu</last></author>
      <pages>7187–7192</pages>
      <abstract>Recently, attempting to model texts as <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a> and introducing graph neural networks to deal with it has become a trend in many <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP research areas</a>. In this paper, we investigate whether the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a> is necessary for textual multi-hop reasoning. Our analysis is centered on HotpotQA. We construct a strong baseline model to establish that, with the proper use of pre-trained models, <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a> may not be necessary for textual multi-hop reasoning. We point out that both <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a> and <a href="https://en.wikipedia.org/wiki/Adjacency_matrix">adjacency matrix</a> are task-related prior knowledge, and graph-attention can be considered as a special case of self-attention. Experiments demonstrate that graph-attention or the entire <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a> can be replaced by self-attention or Transformers.</abstract>
      <url hash="c04cd6a2">2020.emnlp-main.583</url>
      <doi>10.18653/v1/2020.emnlp-main.583</doi>
      <video href="https://slideslive.com/38938772" />
      <bibkey>shao-etal-2020-graph</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
    <paper id="588">
      <title>SLURP : A Spoken Language Understanding Resource Package<fixed-case>SLURP</fixed-case>: A Spoken Language Understanding Resource Package</title>
      <author><first>Emanuele</first><last>Bastianelli</last></author>
      <author><first>Andrea</first><last>Vanzo</last></author>
      <author><first>Pawel</first><last>Swietojanski</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>7252–7262</pages>
      <abstract>Spoken Language Understanding infers <a href="https://en.wikipedia.org/wiki/Semantics">semantic meaning</a> directly from audio data, and thus promises to reduce <a href="https://en.wikipedia.org/wiki/Error_propagation">error propagation</a> and misunderstandings in <a href="https://en.wikipedia.org/wiki/End_user">end-user applications</a>. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following : (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets ; (2) Competitive baselines based on state-of-the-art NLU and ASR systems ; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https://github.com/pswietojanski/slurp.</abstract>
      <url hash="9cba915b">2020.emnlp-main.588</url>
      <doi>10.18653/v1/2020.emnlp-main.588</doi>
      <video href="https://slideslive.com/38939295" />
      <bibkey>bastianelli-etal-2020-slurp</bibkey>
      <pwccode url="https://github.com/pswietojanski/slurp" additional="false">pswietojanski/slurp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/slurp">SLURP</pwcdataset>
    </paper>
    <paper id="593">
      <title>DyERNIE : Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion<fixed-case>D</fixed-case>y<fixed-case>ERNIE</fixed-case>: <fixed-case>D</fixed-case>ynamic <fixed-case>E</fixed-case>volution of <fixed-case>R</fixed-case>iemannian <fixed-case>M</fixed-case>anifold <fixed-case>E</fixed-case>mbeddings for <fixed-case>T</fixed-case>emporal <fixed-case>K</fixed-case>nowledge <fixed-case>G</fixed-case>raph <fixed-case>C</fixed-case>ompletion</title>
      <author><first>Zhen</first><last>Han</last></author>
      <author><first>Peng</first><last>Chen</last></author>
      <author><first>Yunpu</first><last>Ma</last></author>
      <author><first>Volker</first><last>Tresp</last></author>
      <pages>7301–7316</pages>
      <abstract>There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time. Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures. However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the <a href="https://en.wikipedia.org/wiki/Euclidean_space">Euclidean space</a>, which might not capture such intrinsic structures very well. To this end, we propose DyERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data. Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs. Besides, to capture the evolutionary dynamics of temporal KGs, we let the <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity representations</a> evolve according to a <a href="https://en.wikipedia.org/wiki/Velocity_vector">velocity vector</a> defined in the <a href="https://en.wikipedia.org/wiki/Tangent_space">tangent space</a> at each timestamp. We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on <a href="https://en.wikipedia.org/wiki/Riemannian_manifold">Riemannian manifolds</a>.</abstract>
      <url hash="c65647b8">2020.emnlp-main.593</url>
      <doi>10.18653/v1/2020.emnlp-main.593</doi>
      <video href="https://slideslive.com/38938751" />
      <bibkey>han-etal-2020-dyernie</bibkey>
      <pwccode url="https://github.com/TemporalKGTeam/DyERNIE" additional="false">TemporalKGTeam/DyERNIE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/icews">ICEWS</pwcdataset>
    </paper>
    <paper id="596">
      <title>Message Passing for Hyper-Relational Knowledge Graphs</title>
      <author><first>Mikhail</first><last>Galkin</last></author>
      <author><first>Priyansh</first><last>Trivedi</last></author>
      <author><first>Gaurav</first><last>Maheshwari</last></author>
      <author><first>Ricardo</first><last>Usbeck</last></author>
      <author><first>Jens</first><last>Lehmann</last></author>
      <pages>7346–7359</pages>
      <abstract>Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional <a href="https://en.wikipedia.org/wiki/Attribute–value_pair">key-value pairs</a> along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder-StarE capable of modeling such hyper-relational KGs. Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset-WD50K. Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.</abstract>
      <url hash="7b211947">2020.emnlp-main.596</url>
      <attachment type="OptionalSupplementaryMaterial" hash="18233761">2020.emnlp-main.596.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.596</doi>
      <video href="https://slideslive.com/38939108" />
      <bibkey>galkin-etal-2020-message</bibkey>
      <pwccode url="https://github.com/migalkin/StarE" additional="false">migalkin/StarE</pwccode>
    </paper>
    <paper id="602">
      <title>PowerTransformer : Unsupervised Controllable Revision for Biased Language Correction<fixed-case>P</fixed-case>ower<fixed-case>T</fixed-case>ransformer: Unsupervised Controllable Revision for Biased Language Correction</title>
      <author><first>Xinyao</first><last>Ma</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Hannah</first><last>Rashkin</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>7426–7441</pages>
      <abstract>Unconscious biases continue to be prevalent in modern text and media, calling for <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (_ She daydreams about being a doctor _) while a man is portrayed as more proactive and powerful (_ He pursues his dream of being a doctor _). We formulate * * Controllable Debiasing * *, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as <a href="https://en.wikipedia.org/wiki/Paraphrasing">paraphrasing</a> and self-supervision based on a reconstruction loss, building on pretrained language models. Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms <a href="https://en.wikipedia.org/wiki/Ablation">ablations</a> and existing <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> from related <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.</abstract>
      <url hash="80539b31">2020.emnlp-main.602</url>
      <doi>10.18653/v1/2020.emnlp-main.602</doi>
      <video href="https://slideslive.com/38939042" />
      <bibkey>ma-etal-2020-powertransformer</bibkey>
    </paper>
    <paper id="604">
      <title>Centering-based Neural Coherence Modeling with Hierarchical Discourse Segments</title>
      <author><first>Sungho</first><last>Jeon</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <pages>7458–7472</pages>
      <abstract>Previous neural coherence models have focused on identifying semantic relations between adjacent sentences. However, <a href="https://en.wikipedia.org/wiki/Information_technology">they</a> do not have the means to exploit structural information. In this work, we propose a coherence model which takes discourse structural information into account without relying on human annotations. We approximate a linguistic theory of coherence, Centering theory, which we use to track the changes of focus between discourse segments. Our model first identifies the focus of each sentence, recognized with regards to the context, and constructs the structural relationship for discourse segments by tracking the changes of the focus. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> then incorporates this <a href="https://en.wikipedia.org/wiki/Structural_analysis">structural information</a> into a structure-aware transformer. We evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on two tasks, automated essay scoring and assessing writing quality. Our results demonstrate that our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>, built on top of a pretrained language model, achieves state-of-the-art performance on both tasks. We next statistically examine the identified trees of texts assigned to different <a href="https://en.wikipedia.org/wiki/Quality_(philosophy)">quality scores</a>. Finally, we investigate what our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> learns in terms of theoretical claims.</abstract>
      <url hash="d708d582">2020.emnlp-main.604</url>
      <doi>10.18653/v1/2020.emnlp-main.604</doi>
      <video href="https://slideslive.com/38939101" />
      <bibkey>jeon-strube-2020-centering</bibkey>
    </paper>
    <paper id="608">
      <title>Which * BERT? A Survey Organizing Contextualized Encoders<fixed-case>BERT</fixed-case>? <fixed-case>A</fixed-case> Survey Organizing Contextualized Encoders</title>
      <author><first>Patrick</first><last>Xia</last></author>
      <author><first>Shijie</first><last>Wu</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>7516–7533</pages>
      <abstract>Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to use.</abstract>
      <url hash="7c61ffa2">2020.emnlp-main.608</url>
      <doi>10.18653/v1/2020.emnlp-main.608</doi>
      <video href="https://slideslive.com/38939146" />
      <bibkey>xia-etal-2020-bert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="609">
      <title>Fact or Fiction : Verifying Scientific Claims</title>
      <author><first>David</first><last>Wadden</last></author>
      <author><first>Shanchuan</first><last>Lin</last></author>
      <author><first>Kyle</first><last>Lo</last></author>
      <author><first>Lucy Lu</first><last>Wang</last></author>
      <author><first>Madeleine</first><last>van Zuylen</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>7534–7550</pages>
      <abstract>We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4 K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> trained on <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A <a href="https://en.wikipedia.org/wiki/Glossary_of_video_game_terms">leaderboard</a> and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.</abstract>
      <url hash="f9041c2a">2020.emnlp-main.609</url>
      <doi>10.18653/v1/2020.emnlp-main.609</doi>
      <video href="https://slideslive.com/38939235" />
      <bibkey>wadden-etal-2020-fact</bibkey>
      <pwccode url="https://github.com/allenai/scifact" additional="false">allenai/scifact</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/scifact">SciFact</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmedqa">PubMedQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
    </paper>
    <paper id="612">
      <title>Causal Inference of Script Knowledge</title>
      <author><first>Noah</first><last>Weber</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>7583–7596</pages>
      <abstract>When does a sequence of events define an everyday scenario and how can this knowledge be induced from text? Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus. We argue from both a conceptual and practical sense that a purely correlation-based approach is insufficient, and instead propose an approach to script induction based on the causal effect between events, formally defined via interventions. Through both human and automatic evaluations, we show that the output of our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> based on <a href="https://en.wikipedia.org/wiki/Causality">causal effects</a> better matches the intuition of what a script represents.</abstract>
      <url hash="4efc02fd">2020.emnlp-main.612</url>
      <doi>10.18653/v1/2020.emnlp-main.612</doi>
      <video href="https://slideslive.com/38939303" />
      <bibkey>weber-etal-2020-causal</bibkey>
    </paper>
    <paper id="616">
      <title>Detecting Word Sense Disambiguation Biases in <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a> for Model-Agnostic Adversarial Attacks</title>
      <author><first>Denis</first><last>Emelin</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>7635–7653</pages>
      <abstract>Word sense disambiguation is a well-known source of translation errors in <a href="https://en.wikipedia.org/wiki/NMT">NMT</a>. We posit that some of the incorrect disambiguation choices are due to models’ over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on the same data are vulnerable to different attacks.</abstract>
      <url hash="7c805422">2020.emnlp-main.616</url>
      <doi>10.18653/v1/2020.emnlp-main.616</doi>
      <video href="https://slideslive.com/38939052" />
      <bibkey>emelin-etal-2020-detecting</bibkey>
      <pwccode url="https://github.com/demelin/detecting_wsd_biases_for_nmt" additional="false">demelin/detecting_wsd_biases_for_nmt</pwccode>
    </paper>
    <paper id="620">
      <title>Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization in <a href="https://en.wikipedia.org/wiki/News_media">News Media</a></title>
      <author><first>Shamik</first><last>Roy</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>7698–7716</pages>
      <abstract>In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in <a href="https://en.wikipedia.org/wiki/Ideology">political ideology</a> in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, <a href="https://en.wikipedia.org/wiki/Immigration">immigration</a>, <a href="https://en.wikipedia.org/wiki/Gun_politics_in_the_United_States">gun-control</a>, and <a href="https://en.wikipedia.org/wiki/Abortion">abortion</a>. We demonstrate the ability of the subframes to capture <a href="https://en.wikipedia.org/wiki/Ideology">ideological differences</a> and analyze <a href="https://en.wikipedia.org/wiki/Discourse_analysis">political discourse</a> in <a href="https://en.wikipedia.org/wiki/News_media">news media</a>.</abstract>
      <url hash="12fab490">2020.emnlp-main.620</url>
      <attachment type="OptionalSupplementaryMaterial" hash="4e5f8605">2020.emnlp-main.620.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.620</doi>
      <video href="https://slideslive.com/38939379" />
      <bibkey>roy-goldwasser-2020-weakly</bibkey>
      <pwccode url="https://github.com/ShamikRoy/Subframe-Prediction" additional="false">ShamikRoy/Subframe-Prediction</pwccode>
    </paper>
    <paper id="623">
      <title>Explainable Automated Fact-Checking for Public Health Claims</title>
      <author><first>Neema</first><last>Kotonya</last></author>
      <author><first>Francesca</first><last>Toni</last></author>
      <pages>7740–7754</pages>
      <abstract>Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of <a href="https://en.wikipedia.org/wiki/Fact-checking">fact-checking studies</a> focus exclusively on <a href="https://en.wikipedia.org/wiki/Politics">political claims</a>. Very little research explores <a href="https://en.wikipedia.org/wiki/Fact-checking">fact-checking</a> for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of <a href="https://en.wikipedia.org/wiki/Public_health">public health</a>. To support this case study we construct a new dataset PUBHEALTH of 11.8 K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims. We explore two <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> : veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three <a href="https://en.wikipedia.org/wiki/Coherence_(physics)">coherence properties</a> of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.</abstract>
      <url hash="e51a6c3f">2020.emnlp-main.623</url>
      <doi>10.18653/v1/2020.emnlp-main.623</doi>
      <video href="https://slideslive.com/38939277" />
      <bibkey>kotonya-toni-2020-explainable-automated</bibkey>
      <pwccode url="https://github.com/neemakot/Health-Fact-Checking" additional="false">neemakot/Health-Fact-Checking</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pubhealth">PUBHEALTH</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/liar">LIAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multifc">MultiFC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="627">
      <title>Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification<fixed-case>H</fixed-case>ierarchical <fixed-case>E</fixed-case>vidence <fixed-case>S</fixed-case>et <fixed-case>M</fixed-case>odeling for Automated Fact Extraction and Verification</title>
      <author><first>Shyam</first><last>Subramanian</last></author>
      <author><first>Kyumin</first><last>Lee</last></author>
      <pages>7798–7809</pages>
      <abstract>Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim. Existing models either (i) concatenate all the evidence sentences, leading to the inclusion of redundant and noisy information ; or (ii) process each claim-evidence sentence pair separately and aggregate all of them later, missing the early combination of related sentences for more accurate claim verification. Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple evidence sentences), and verify a claim to be supported, refuted or not enough info, by encoding and attending the claim and evidence sets at different levels of hierarchy. Our experimental results show that HESM outperforms 7 state-of-the-art methods for fact extraction and <a href="https://en.wikipedia.org/wiki/Verification_and_validation">claim verification</a>. Our source code is available at https://github.com/ShyamSubramanian/HESM.</abstract>
      <url hash="09379a29">2020.emnlp-main.627</url>
      <doi>10.18653/v1/2020.emnlp-main.627</doi>
      <video href="https://slideslive.com/38939144" />
      <bibkey>subramanian-lee-2020-hierarchical</bibkey>
      <pwccode url="https://github.com/ShyamSubramanian/HESM" additional="false">ShyamSubramanian/HESM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
    </paper>
    <paper id="635">
      <title>Exploring and Predicting Transferability across NLP Tasks<fixed-case>NLP</fixed-case> Tasks</title>
      <author><first>Tu</first><last>Vu</last></author>
      <author><first>Tong</first><last>Wang</last></author>
      <author><first>Tsendsuren</first><last>Munkhdalai</last></author>
      <author><first>Alessandro</first><last>Sordoni</last></author>
      <author><first>Adam</first><last>Trischler</last></author>
      <author><first>Andrew</first><last>Mattarella-Micke</last></author>
      <author><first>Subhransu</first><last>Maji</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>7882–7926</pages>
      <abstract>Recent advances in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> other than <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a> further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, and sequence labeling). Our results show that <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even with low-data source tasks that differ substantially from the target task (e.g., <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a> transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as data size, task and domain similarity, and task complexity all play a role in determining transferability.</abstract>
      <url hash="459d10d4">2020.emnlp-main.635</url>
      <doi>10.18653/v1/2020.emnlp-main.635</doi>
      <video href="https://slideslive.com/38939047" />
      <bibkey>vu-etal-2020-exploring</bibkey>
      <pwccode url="https://github.com/tuvuumass/task-transferability" additional="false">tuvuumass/task-transferability</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/comqa">ComQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="637">
      <title>Cold-start Active Learning through Self-supervised Language Modeling</title>
      <author><first>Michelle</first><last>Yuan</last></author>
      <author><first>Hsuan-Tien</first><last>Lin</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>7935–7948</pages>
      <abstract>Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification model</a>. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a> is impractical because of model instability and data scarcity. Fortunately, modern <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> provides an additional source of information : pre-trained language models. The pre-training loss can find examples that surprise the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and should be labeled for efficient <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for <a href="https://en.wikipedia.org/wiki/Text_classification">text classification</a>. Compared to other baselines, our approach reaches higher <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> within less <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">sampling iterations</a> and <a href="https://en.wikipedia.org/wiki/Time_complexity">computation time</a>.</abstract>
      <url hash="9217a719">2020.emnlp-main.637</url>
      <doi>10.18653/v1/2020.emnlp-main.637</doi>
      <video href="https://slideslive.com/38938687" />
      <bibkey>yuan-etal-2020-cold</bibkey>
      <pwccode url="https://github.com/forest-snow/alps" additional="false">forest-snow/alps</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="641">
      <title>The importance of fillers for text representations of speech transcripts</title>
      <author><first>Tanvi</first><last>Dinkar</last></author>
      <author><first>Pierre</first><last>Colombo</last></author>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>7985–7993</pages>
      <abstract>While being an essential component of <a href="https://en.wikipedia.org/wiki/Spoken_language">spoken language</a>, fillers (e.g. um or uh) often remain overlooked in Spoken Language Understanding (SLU) tasks. We explore the possibility of representing them with deep contextualised embeddings, showing improvements on modelling <a href="https://en.wikipedia.org/wiki/Spoken_language">spoken language</a> and two downstream tasks   predicting a speaker’s stance and expressed confidence.</abstract>
      <url hash="13d8f9af">2020.emnlp-main.641</url>
      <doi>10.18653/v1/2020.emnlp-main.641</doi>
      <video href="https://slideslive.com/38938831" />
      <bibkey>dinkar-etal-2020-importance</bibkey>
    </paper>
    <paper id="643">
      <title>VolTAGE : Volatility Forecasting via Text Audio Fusion with Graph Convolution Networks for Earnings Calls<fixed-case>V</fixed-case>ol<fixed-case>TAGE</fixed-case>: Volatility Forecasting via Text Audio Fusion with Graph Convolution Networks for Earnings Calls</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Piyush</first><last>Khanna</last></author>
      <author><first>Arshiya</first><last>Aggarwal</last></author>
      <author><first>Taru</first><last>Jain</last></author>
      <author><first>Puneet</first><last>Mathur</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <pages>8001–8013</pages>
      <abstract>Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved <a href="https://en.wikipedia.org/wiki/Financial_forecasting">financial forecasting</a>. Transcripts of companies’ earnings calls are well studied for <a href="https://en.wikipedia.org/wiki/Financial_risk_modeling">risk modeling</a>, offering unique investment insight into stock performance. However, <a href="https://en.wikipedia.org/wiki/Speech_recognition">vocal cues</a> in the speech of company executives present an underexplored rich source of <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language data</a> for estimating <a href="https://en.wikipedia.org/wiki/Financial_risk">financial risk</a>. Additionally, most existing <a href="https://en.wikipedia.org/wiki/Portfolio_(finance)">approaches</a> ignore the correlations between stocks. Building on existing work, we introduce a neural model for stock volatility prediction that accounts for stock interdependence via graph convolutions while fusing verbal, vocal, and financial features in a semi-supervised multi-task risk forecasting formulation. Our proposed model, VolTAGE, outperforms existing methods demonstrating the effectiveness of <a href="https://en.wikipedia.org/wiki/Multimodal_learning">multimodal learning</a> for volatility prediction.</abstract>
      <url hash="fd5f3da4">2020.emnlp-main.643</url>
      <doi>10.18653/v1/2020.emnlp-main.643</doi>
      <video href="https://slideslive.com/38939137" />
      <bibkey>sawhney-etal-2020-voltage</bibkey>
    </paper>
    <paper id="644">
      <title>Effectively pretraining a speech translation decoder with Machine Translation data</title>
      <author><first>Ashkan</first><last>Alinejad</last></author>
      <author><first>Anoop</first><last>Sarkar</last></author>
      <pages>8014–8020</pages>
      <abstract>Directly translating from <a href="https://en.wikipedia.org/wiki/Speech">speech</a> to text using an <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end approach</a> is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for <a href="https://en.wikipedia.org/wiki/Speech_translation">speech translation</a>.</abstract>
      <url hash="e2df1125">2020.emnlp-main.644</url>
      <doi>10.18653/v1/2020.emnlp-main.644</doi>
      <video href="https://slideslive.com/38939224" />
      <bibkey>alinejad-sarkar-2020-effectively</bibkey>
    </paper>
    <paper id="646">
      <title>TESA : A Task in Entity Semantic Aggregation for Abstractive Summarization<fixed-case>TESA</fixed-case>: A <fixed-case>T</fixed-case>ask in <fixed-case>E</fixed-case>ntity <fixed-case>S</fixed-case>emantic <fixed-case>A</fixed-case>ggregation for Abstractive Summarization</title>
      <author><first>Clément</first><last>Jumel</last></author>
      <author><first>Annie</first><last>Louis</last></author>
      <author><first>Jackie Chi Kit</first><last>Cheung</last></author>
      <pages>8031–8050</pages>
      <abstract>Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as ‘London’ and ‘Paris’ with different expressions : the major cities, the capital cities and two European cities. Yet generation, especially, abstractive summarization systems have so far focused heavily on <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrasing</a> and simplifying the source content, to the exclusion of such semantic abstraction capabilities. In this paper, we present a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> aimed at the semantic aggregation of entities. TESA contains a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 5.3 K crowd-sourced entity aggregations of Person, Organization, and Location named entities. The aggregations are document-appropriate, meaning that they are produced by annotators to match the situational context of a given news article from the <a href="https://en.wikipedia.org/wiki/The_New_York_Times">New York Times</a>. We then build <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline models</a> for generating aggregations given a tuple of entities and <a href="https://en.wikipedia.org/wiki/Context_(language_use)">document context</a>. We finetune on TESA an encoder-decoder language model and compare it with simpler <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification methods</a> based on linguistically informed features. Our quantitative and qualitative evaluations show reasonable performance in making a choice from a given list of expressions, but free-form expressions are understandably harder to generate and evaluate.</abstract>
      <url hash="fefa09eb">2020.emnlp-main.646</url>
      <doi>10.18653/v1/2020.emnlp-main.646</doi>
      <video href="https://slideslive.com/38939209" />
      <bibkey>jumel-etal-2020-tesa</bibkey>
    </paper>
    <paper id="650">
      <title>Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness</title>
      <author><first>Stefan</first><last>Larson</last></author>
      <author><first>Anthony</first><last>Zheng</last></author>
      <author><first>Anish</first><last>Mahendran</last></author>
      <author><first>Rishi</first><last>Tekriwal</last></author>
      <author><first>Adrian</first><last>Cheung</last></author>
      <author><first>Eric</first><last>Guldan</last></author>
      <author><first>Kevin</first><last>Leach</last></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last></author>
      <pages>8097–8106</pages>
      <abstract>Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively constraining their writing. We show how prior workflows are special cases of our approach, and present a way to apply the approach to dialog tasks such as intent classification and slot-filling. Using our <a href="https://en.wikipedia.org/wiki/Methodology">method</a>, we create more challenging versions of test sets from prior dialog datasets and find dramatic performance drops for standard <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Finally, we show that our approach is complementary to recent work on improving <a href="https://en.wikipedia.org/wiki/Data">data diversity</a>, and training on <a href="https://en.wikipedia.org/wiki/Data">data</a> collected with our approach leads to more robust models.</abstract>
      <url hash="091d5b6f">2020.emnlp-main.650</url>
      <attachment type="OptionalSupplementaryMaterial" hash="69235d06">2020.emnlp-main.650.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-main.650</doi>
      <video href="https://slideslive.com/38938895" />
      <bibkey>larson-etal-2020-iterative</bibkey>
    </paper>
    <paper id="658">
      <title>New Protocols and Negative Results for Textual Entailment Data Collection</title>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <author><first>Jennimaria</first><last>Palomaki</last></author>
      <author><first>Livio</first><last>Baldini Soares</last></author>
      <author><first>Emily</first><last>Pitler</last></author>
      <pages>8203–8214</pages>
      <abstract>Natural language inference (NLI) data has proven useful in <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmarking</a> and, especially, as pretraining data for tasks requiring <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">language understanding</a>. However, the crowdsourcing protocol that was used to collect this <a href="https://en.wikipedia.org/wiki/Data">data</a> has known issues and was not explicitly optimized for either of these purposes, so it is likely far from ideal. We propose four alternative protocols, each aimed at improving either the ease with which annotators can produce sound training examples or the quality and diversity of those examples. Using these alternatives and a fifth <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline protocol</a>, we collect and compare five new 8.5k-example training sets. In evaluations focused on transfer learning applications, our results are solidly negative, with models trained on our <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline dataset</a> yielding good transfer performance to downstream tasks, but none of our four new methods (nor the recent ANLI) showing any improvements over that <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>. In a small silver lining, we observe that all four new <a href="https://en.wikipedia.org/wiki/Communication_protocol">protocols</a>, especially those where annotators edit * pre-filled * text boxes, reduce previously observed issues with annotation artifacts.</abstract>
      <url hash="348a9fd3">2020.emnlp-main.658</url>
      <doi>10.18653/v1/2020.emnlp-main.658</doi>
      <video href="https://slideslive.com/38939009" />
      <bibkey>bowman-etal-2020-new</bibkey>
      <pwccode url="https://github.com/google-research-datasets/Textual-Entailment-New-Protocols" additional="false">google-research-datasets/Textual-Entailment-New-Protocols</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/record">ReCoRD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="664">
      <title>Precise Task Formalization Matters in Winograd Schema Evaluations<fixed-case>W</fixed-case>inograd Schema Evaluations</title>
      <author><first>Haokun</first><last>Liu</last></author>
      <author><first>William</first><last>Huang</last></author>
      <author><first>Dhara</first><last>Mungra</last></author>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <pages>8275–8280</pages>
      <abstract>Performance on the Winograd Schema Challenge (WSC), a respected English commonsense reasoning benchmark, recently rocketed from chance accuracy to 89 % on the SuperGLUE leaderboard, with relatively little corroborating evidence of a correspondingly large improvement in reasoning ability. We hypothesize that much of this improvement comes from recent changes in task formalizationthe combination of input specification, <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a>, and reuse of pretrained parametersby users of the dataset, rather than improvements in the pretrained model’s reasoning ability. We perform an ablation on two Winograd Schema datasets that interpolates between the formalizations used before and after this surge, and find (i) framing the task as multiple choice improves performance dramatically and (ii)several additional techniques, including the reuse of a pretrained language modeling head, can mitigate the model’s extreme sensitivity to hyperparameters. We urge future benchmark creators to impose additional structure to minimize the impact of formalization decisions on reported results.</abstract>
      <url hash="d4b45e09">2020.emnlp-main.664</url>
      <doi>10.18653/v1/2020.emnlp-main.664</doi>
      <video href="https://slideslive.com/38939247" />
      <bibkey>liu-etal-2020-precise</bibkey>
      <pwccode url="https://github.com/nyu-mll/wsc-formalizations" additional="false">nyu-mll/wsc-formalizations</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="667">
      <title>Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy Link Prediction<fixed-case>E</fixed-case>valuating the <fixed-case>C</fixed-case>alibration of <fixed-case>K</fixed-case>nowledge <fixed-case>G</fixed-case>raph <fixed-case>E</fixed-case>mbeddings for <fixed-case>T</fixed-case>rustworthy <fixed-case>L</fixed-case>ink <fixed-case>P</fixed-case>rediction</title>
      <author><first>Tara</first><last>Safavi</last></author>
      <author><first>Danai</first><last>Koutra</last></author>
      <author><first>Edgar</first><last>Meij</last></author>
      <pages>8308–8321</pages>
      <abstract>Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence scores</a> that reflect the expected correctness of predicted knowledge graph triples. We first conduct an evaluation under the standard closed-world assumption (CWA), in which predicted triples not already in the <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> are considered false, and show that existing calibration techniques are effective for KGE under this common but narrow assumption. Next, we introduce the more realistic but challenging open-world assumption (OWA), in which unobserved predictions are not considered true or false until ground-truth labels are obtained. Here, we show that existing calibration techniques are much less effective under the OWA than the CWA, and provide explanations for this discrepancy. Finally, to motivate the utility of <a href="https://en.wikipedia.org/wiki/Calibration">calibration</a> for KGE from a practitioner’s perspective, we conduct a unique case study of human-AI collaboration, showing that calibrated predictions can improve human performance in a knowledge graph completion task.</abstract>
      <url hash="8faec7db">2020.emnlp-main.667</url>
      <doi>10.18653/v1/2020.emnlp-main.667</doi>
      <bibkey>safavi-etal-2020-evaluating</bibkey>
    </paper>
    <paper id="669">
      <title>CoDEx : A Comprehensive Knowledge Graph Completion Benchmark<fixed-case>C</fixed-case>o<fixed-case>DE</fixed-case>x: A <fixed-case>C</fixed-case>omprehensive <fixed-case>K</fixed-case>nowledge <fixed-case>G</fixed-case>raph <fixed-case>C</fixed-case>ompletion <fixed-case>B</fixed-case>enchmark</title>
      <author><first>Tara</first><last>Safavi</last></author>
      <author><first>Danai</first><last>Koutra</last></author>
      <pages>8328–8350</pages>
      <abstract>We present CoDEx, a set of knowledge graph completion datasets extracted from <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a> and <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises three <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a> varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. To characterize CoDEx, we contribute thorough empirical analyses and benchmarking experiments. First, we analyze each CoDEx dataset in terms of logical relation patterns. Next, we report baseline link prediction and triple classification results on CoDEx for five extensively tuned embedding models. Finally, we differentiate CoDEx from the popular FB15K-237 knowledge graph completion dataset by showing that CoDEx covers more diverse and interpretable content, and is a more difficult link prediction benchmark. Data, code, and pretrained models are available at https://bit.ly/2EPbrJs.</abstract>
      <url hash="b3f4ae23">2020.emnlp-main.669</url>
      <doi>10.18653/v1/2020.emnlp-main.669</doi>
      <bibkey>safavi-koutra-2020-codex</bibkey>
      <pwccode url="https://github.com/tsafavi/codex" additional="true">tsafavi/codex</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/codex-large">CoDEx Large</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codex-medium">CoDEx Medium</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codex">CoDEx Small</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nell-995">NELL-995</pwcdataset>
    </paper>
    <paper id="675">
      <title>Towards Modeling Revision Requirements in wikiHow Instructions<fixed-case>H</fixed-case>ow Instructions</title>
      <author><first>Irshad</first><last>Bhat</last></author>
      <author><first>Talita</first><last>Anthonio</last></author>
      <author><first>Michael</first><last>Roth</last></author>
      <pages>8407–8414</pages>
      <abstract>wikiHow is a resource of how-to guidesthat describe the steps necessary to accomplish a goal. Guides in this resource are regularly edited by a community of users, who try to improve instructions in terms of style, clarity and correctness. In this work, we test whether the need for such <a href="https://en.wikipedia.org/wiki/Editing">edits</a> can be predicted automatically. For this <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>, we extend an existing <a href="https://en.wikipedia.org/wiki/Resource">resource</a> of textual edits with a complementary set of approx. 4 million sentences that remain unedited over time and report on the outcome of two revision modeling experiments.</abstract>
      <url hash="2fda4e64">2020.emnlp-main.675</url>
      <doi>10.18653/v1/2020.emnlp-main.675</doi>
      <video href="https://slideslive.com/38939007" />
      <bibkey>bhat-etal-2020-towards</bibkey>
    </paper>
    <paper id="677">
      <title>Natural Language Processing for Achieving <a href="https://en.wikipedia.org/wiki/Sustainable_development">Sustainable Development</a> : the Case of Neural Labelling to Enhance Community Profiling</title>
      <author><first>Costanza</first><last>Conforti</last></author>
      <author><first>Stephanie</first><last>Hirmer</last></author>
      <author><first>Dai</first><last>Morgan</last></author>
      <author><first>Marco</first><last>Basaldella</last></author>
      <author><first>Yau</first><last>Ben Or</last></author>
      <pages>8427–8444</pages>
      <abstract>In recent years, there has been an increasing interest in the application of <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence</a>   and especially <a href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning</a>   to the field of Sustainable Development (SD). However, until now, <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a> has not been systematically applied in this context. In this paper, we show the high potential of <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a> to enhance project sustainability. In particular, we focus on the case of community profiling in <a href="https://en.wikipedia.org/wiki/Developing_country">developing countries</a>, where, in contrast to the developed world, a notable data gap exists. Here, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> could help to address the cost and time barrier of structuring <a href="https://en.wikipedia.org/wiki/Qualitative_data">qualitative data</a> that prohibits its widespread use and associated benefits. We propose the new extreme multi-class multi-label Automatic UserPerceived Value classification task. We release Stories2Insights, an expert-annotated dataset of interviews carried out in Uganda, we provide a detailed corpus analysis, and we implement a number of strong neural baselines to address the task. Experimental results show that the problem is challenging, and leaves considerable room for future research at the intersection of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> and <a href="https://en.wikipedia.org/wiki/Sensory_processing">SD</a>.</abstract>
      <url hash="804678e4">2020.emnlp-main.677</url>
      <doi>10.18653/v1/2020.emnlp-main.677</doi>
      <video href="https://slideslive.com/38939257" />
      <bibkey>conforti-etal-2020-natural</bibkey>
    </paper>
    <paper id="678">
      <title>To Schedule or not to Schedule : Extracting Task Specific Temporal Entities and Associated Negation Constraints</title>
      <author><first>Barun</first><last>Patra</last></author>
      <author><first>Chala</first><last>Fufa</last></author>
      <author><first>Pamela</first><last>Bhattacharya</last></author>
      <author><first>Charles</first><last>Lee</last></author>
      <pages>8445–8455</pages>
      <abstract>State of the art research for date-time entity extraction from text is task agnostic. Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they do n’t fare as well on task specific date-time entity extraction where only a subset of the date-time entities present in the text are pertinent to solving the task. Furthermore, some <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> require identifying negation constraints associated with the date-time entities to correctly reason over time. We showcase a novel <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for extracting task-specific date-time entities along with their negation constraints. We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings for an email-based digital AI scheduling assistant. Our method achieves an absolute gain of 19 % f-score points compared to baseline methods in detecting the date-time entities relevant to scheduling meetings and a 4 % improvement over baseline methods for detecting negation constraints over date-time entities.</abstract>
      <url hash="bda9802c">2020.emnlp-main.678</url>
      <doi>10.18653/v1/2020.emnlp-main.678</doi>
      <video href="https://slideslive.com/38939368" />
      <bibkey>patra-etal-2020-schedule</bibkey>
    </paper>
    <paper id="684">
      <title>Exploring Semantic Capacity of Terms</title>
      <author><first>Jie</first><last>Huang</last></author>
      <author><first>Zilong</first><last>Wang</last></author>
      <author><first>Kevin</first><last>Chang</last></author>
      <author><first>Wen-mei</first><last>Hwu</last></author>
      <author><first>JinJun</first><last>Xiong</last></author>
      <pages>8509–8518</pages>
      <abstract>We introduce and study semantic capacity of terms. For example, the semantic capacity of <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial intelligence</a> is higher than that of <a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a> since <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial intelligence</a> possesses a broader meaning scope. Understanding semantic capacity of terms will help many downstream tasks in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. For this purpose, we propose a two-step model to investigate semantic capacity of terms, which takes a large text corpus as input and can evaluate semantic capacity of terms if the <a href="https://en.wikipedia.org/wiki/Text_corpus">text corpus</a> can provide enough co-occurrence information of terms. Extensive experiments in three fields demonstrate the effectiveness and rationality of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> compared with well-designed baselines and human-level evaluations.</abstract>
      <url hash="ea1c2ddd">2020.emnlp-main.684</url>
      <doi>10.18653/v1/2020.emnlp-main.684</doi>
      <video href="https://slideslive.com/38938735" />
      <bibkey>huang-etal-2020-exploring</bibkey>
      <pwccode url="https://github.com/c3sr/semantic-capacity" additional="false">c3sr/semantic-capacity</pwccode>
    </paper>
    <paper id="689">
      <title>Exploring Contextualized Neural Language Models for Temporal Dependency Parsing<fixed-case>E</fixed-case>xploring <fixed-case>C</fixed-case>ontextualized <fixed-case>N</fixed-case>eural <fixed-case>L</fixed-case>anguage <fixed-case>M</fixed-case>odels for <fixed-case>T</fixed-case>emporal <fixed-case>D</fixed-case>ependency <fixed-case>P</fixed-case>arsing</title>
      <author><first>Hayley</first><last>Ross</last></author>
      <author><first>Jonathon</first><last>Cai</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <pages>8548–8553</pages>
      <abstract>Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019). In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a). We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short. Source code and resources are made available at https://github.com/bnmin/tdp_ranking.</abstract>
      <url hash="8105caef">2020.emnlp-main.689</url>
      <doi>10.18653/v1/2020.emnlp-main.689</doi>
      <video href="https://slideslive.com/38938936" />
      <bibkey>ross-etal-2020-exploring</bibkey>
      <pwccode url="https://github.com/bnmin/tdp_ranking" additional="false">bnmin/tdp_ranking</pwccode>
    </paper>
    <paper id="692">
      <title>AxCell : Automatic Extraction of Results from Machine Learning Papers<fixed-case>AxCell</fixed-case>: Automatic Extraction of Results from Machine Learning Papers</title>
      <author><first>Marcin</first><last>Kardas</last></author>
      <author><first>Piotr</first><last>Czapla</last></author>
      <author><first>Pontus</first><last>Stenetorp</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Ross</first><last>Taylor</last></author>
      <author><first>Robert</first><last>Stojnic</last></author>
      <pages>8580–8594</pages>
      <abstract>Tracking progress in <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AxCell, an automatic machine learning pipeline for extracting results from papers. AxCell uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing <a href="https://en.wikipedia.org/wiki/Methodology">methods</a>, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> for results extraction, and a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for evaluating the performance of <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> on this task. Lastly, we show the viability of our approach enables it to be used for semi-automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub.</abstract>
      <url hash="999c691d">2020.emnlp-main.692</url>
      <doi>10.18653/v1/2020.emnlp-main.692</doi>
      <video href="https://slideslive.com/38938992" />
      <bibkey>kardas-etal-2020-axcell</bibkey>
      <pwccode url="https://github.com/paperswithcode/axcell" additional="false">paperswithcode/axcell</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arxivpapers">ArxivPapers</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/linkedresults">LinkedResults</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pwc-leaderboards">PWC Leaderboards</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/segmentedtables">SegmentedTables</pwcdataset>
    </paper>
    <paper id="695">
      <title>Incremental Neural Coreference Resolution in Constant Memory</title>
      <author><first>Patrick</first><last>Xia</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>8617–8624</pages>
      <abstract>We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end algorithm</a> proposes and scores each mention span against explicit entity representations created from the earlier document context (if any). These spans are then used to update the entity’s representations before being forgotten ; we only retain a fixed set of salient entities throughout the document. In this work, we successfully convert a high-performing model (Joshi et al., 2020), asymptotically reducing its memory usage to constant space with only a 0.3 % relative loss in F1 on OntoNotes 5.0.</abstract>
      <url hash="f33d3434">2020.emnlp-main.695</url>
      <doi>10.18653/v1/2020.emnlp-main.695</doi>
      <video href="https://slideslive.com/38939421" />
      <bibkey>xia-etal-2020-incremental</bibkey>
    </paper>
    <paper id="697">
      <title>KGPT : Knowledge-Grounded Pre-Training for Data-to-Text Generation<fixed-case>KGPT</fixed-case>: Knowledge-Grounded Pre-Training for Data-to-Text Generation</title>
      <author><first>Wenhu</first><last>Chen</last></author>
      <author><first>Yu</first><last>Su</last></author>
      <author><first>Xifeng</first><last>Yan</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>8635–8648</pages>
      <abstract>Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a>. However, they rely on a significant amount of labeled data for each <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>, which is costly to acquire and thus limits their application to new <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> and domains. In this paper, we propose to leverage pre-training and transfer learning to address this issue. We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text. 2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a>. The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text. We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness. Under the fully-supervised setting, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can achieve remarkable gains over the known baselines. Under zero-shot setting, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail. Under the few-shot setting, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models. These experiments consistently prove the strong generalization ability of our proposed <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a>.</abstract>
      <url hash="6d9baae8">2020.emnlp-main.697</url>
      <doi>10.18653/v1/2020.emnlp-main.697</doi>
      <video href="https://slideslive.com/38938913" />
      <bibkey>chen-etal-2020-kgpt</bibkey>
      <pwccode url="https://github.com/wenhuchen/KGPT" additional="false">wenhuchen/KGPT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
    </paper>
    <paper id="698">
      <title>POINTER : Constrained Progressive Text Generation via Insertion-based Generative Pre-training<fixed-case>POINTER</fixed-case>: Constrained Progressive Text Generation via Insertion-based Generative Pre-training</title>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Guoyin</first><last>Wang</last></author>
      <author><first>Chunyuan</first><last>Li</last></author>
      <author><first>Zhe</first><last>Gan</last></author>
      <author><first>Chris</first><last>Brockett</last></author>
      <author><first>Bill</first><last>Dolan</last></author>
      <pages>8649–8670</pages>
      <abstract>Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> can not be directly employed to generate text under specified <a href="https://en.wikipedia.org/wiki/Lexical_analysis">lexical constraints</a>. To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12 GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields a <a href="https://en.wikipedia.org/wiki/Time_complexity">logarithmic time complexity</a> during <a href="https://en.wikipedia.org/wiki/Time_complexity">inference time</a>. Experimental results on both News and Yelp datasets demonstrate that Pointer achieves state-of-the-art performance on constrained text generation. We released the <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">pre-trained models</a> and the source code to facilitate future research.</abstract>
      <url hash="37ccc4fa">2020.emnlp-main.698</url>
      <doi>10.18653/v1/2020.emnlp-main.698</doi>
      <video href="https://slideslive.com/38938973" />
      <bibkey>zhang-etal-2020-pointer</bibkey>
      <pwccode url="https://github.com/dreasysnail/POINTER" additional="false">dreasysnail/POINTER</pwccode>
    </paper>
    <paper id="706">
      <title>What is More Likely to Happen Next? Video-and-Language Future Event Prediction</title>
      <author><first>Jie</first><last>Lei</last></author>
      <author><first>Licheng</first><last>Yu</last></author>
      <author><first>Tamara</first><last>Berg</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>8769–8784</pages>
      <abstract>Given a video with aligned dialogue, people can often infer what is more likely to happen next. Making such predictions requires not only a deep understanding of the rich dynamics underlying the <a href="https://en.wikipedia.org/wiki/Video">video</a> and dialogue, but also a significant amount of <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a>. In this work, we explore whether <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI models</a> are able to learn to make such multimodal commonsense next-event predictions. To support research in this direction, we collect a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, named Video-and-Language Event Prediction (VLEP), with 28,726 future event prediction examples (along with their rationales) from 10,234 diverse <a href="https://en.wikipedia.org/wiki/Television_show">TV Show</a> and <a href="https://en.wikipedia.org/wiki/Vlog">YouTube Lifestyle Vlog video clips</a>. In order to promote the collection of non-trivial challenging examples, we employ an adversarial human-and-model-in-the-loop data collection procedure. We also present a strong <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> incorporating information from <a href="https://en.wikipedia.org/wiki/Video">video</a>, <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a>, and <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a>. Experiments show that each type of <a href="https://en.wikipedia.org/wiki/Information">information</a> is useful for this challenging task, and that compared to the high human performance on VLEP, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> provides a good starting point but leaves large room for future work.</abstract>
      <url hash="d70a4707">2020.emnlp-main.706</url>
      <doi>10.18653/v1/2020.emnlp-main.706</doi>
      <video href="https://slideslive.com/38939207" />
      <bibkey>lei-etal-2020-likely</bibkey>
      <pwccode url="https://github.com/jayleicn/VideoLanguageFuturePred" additional="false">jayleicn/VideoLanguageFuturePred</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/vlep">VLEP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vlogs">Vlogs</pwcdataset>
    </paper>
    <paper id="708">
      <title>Towards Understanding Sample Variance in Visually Grounded Language Generation : Evaluations and Observations</title>
      <author><first>Wanrong</first><last>Zhu</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Pradyumna</first><last>Narayana</last></author>
      <author><first>Kazoo</first><last>Sone</last></author>
      <author><first>Sugato</first><last>Basu</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>8806–8811</pages>
      <abstract>A major challenge in visually grounded language generation is to build robust benchmark datasets and <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation : given that humans have different utilities and <a href="https://en.wikipedia.org/wiki/Attention">visual attention</a>, how will the <a href="https://en.wikipedia.org/wiki/Variance">sample variance</a> in multi-reference datasets affect the models’ performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments ; that human-generated references could vary drastically in different datasets / tasks, revealing the nature of each task ; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.</abstract>
      <url hash="9cd02c4a">2020.emnlp-main.708</url>
      <doi>10.18653/v1/2020.emnlp-main.708</doi>
      <video href="https://slideslive.com/38939350" />
      <bibkey>zhu-etal-2020-towards-understanding</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vatex">VATEX</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vist">VIST</pwcdataset>
    </paper>
    <paper id="714">
      <title>SRLGRN : Semantic Role Labeling Graph Reasoning Network<fixed-case>SRLGRN</fixed-case>: Semantic Role Labeling Graph Reasoning Network</title>
      <author><first>Chen</first><last>Zheng</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>8881–8891</pages>
      <abstract>This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.</abstract>
      <url hash="3e072eab">2020.emnlp-main.714</url>
      <doi>10.18653/v1/2020.emnlp-main.714</doi>
      <video href="https://slideslive.com/38938731" />
      <bibkey>zheng-kordjamshidi-2020-srlgrn</bibkey>
      <pwccode url="https://github.com/HLR/SRLGRN" additional="false">HLR/SRLGRN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="723">
      <title>Named Entity Recognition Only from Word Embeddings</title>
      <author><first>Ying</first><last>Luo</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Junlang</first><last>Zhan</last></author>
      <pages>8995–9005</pages>
      <abstract>Deep neural network models have helped <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> achieve amazing performance without handcrafting features. However, existing <a href="https://en.wikipedia.org/wiki/System">systems</a> require large amounts of human annotated training data. Efforts have been made to replace <a href="https://en.wikipedia.org/wiki/Annotation">human annotations</a> with external knowledge (e.g., NE dictionary, <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tags</a>), while it is another challenge to obtain such effective resources. In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings. We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector based on <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> to distinguish positive sentences from noisy sentences and then refine these coarse-grained annotations through neural networks. Extensive experiments on two CoNLL benchmark NER datasets (CoNLL-2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus.</abstract>
      <url hash="056f91b5">2020.emnlp-main.723</url>
      <doi>10.18653/v1/2020.emnlp-main.723</doi>
      <video href="https://slideslive.com/38938819" />
      <bibkey>luo-etal-2020-named</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2002">CoNLL 2002</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="724">
      <title>Text Classification Using Label Names Only : A Language Model Self-Training Approach</title>
      <author><first>Yu</first><last>Meng</last></author>
      <author><first>Yunyi</first><last>Zhang</last></author>
      <author><first>Jiaxin</first><last>Huang</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Chao</first><last>Zhang</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>9006–9017</pages>
      <abstract>Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification models</a> on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for <a href="https://en.wikipedia.org/wiki/Categorization">category understanding</a> and as <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning models</a> for <a href="https://en.wikipedia.org/wiki/Document_classification">document classification</a>. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to predict their implied categories, and (3) generalizes the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> via self-training. We show that our model achieves around 90 % accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.</abstract>
      <url hash="93f1e218">2020.emnlp-main.724</url>
      <doi>10.18653/v1/2020.emnlp-main.724</doi>
      <video href="https://slideslive.com/38938946" />
      <bibkey>meng-etal-2020-text</bibkey>
      <pwccode url="https://github.com/yumeng5/LOTClass" additional="false">yumeng5/LOTClass</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="728">
      <title>PyMT5 : multi-mode translation of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a> and <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python code</a> with transformers<fixed-case>P</fixed-case>y<fixed-case>MT</fixed-case>5: multi-mode translation of natural language and Python code with transformers</title>
      <author><first>Colin</first><last>Clement</last></author>
      <author><first>Dawn</first><last>Drain</last></author>
      <author><first>Jonathan</first><last>Timcheck</last></author>
      <author><first>Alexey</first><last>Svyatkovskiy</last></author>
      <author><first>Neel</first><last>Sundaresan</last></author>
      <pages>9052–9065</pages>
      <abstract>Simultaneously modeling <a href="https://en.wikipedia.org/wiki/Source_code">source code</a> and <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations : a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python methods</a> and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1 % syntactically correct method bodies, achieved a <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLEU score</a> of 8.59 for method generation and 16.3 for <a href="https://en.wikipedia.org/wiki/Docstring">docstring generation (summarization)</a>, and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for <a href="https://en.wikipedia.org/wiki/Docstring">docstring generation</a>.</abstract>
      <url hash="8adf1a8b">2020.emnlp-main.728</url>
      <doi>10.18653/v1/2020.emnlp-main.728</doi>
      <video href="https://slideslive.com/38939242" />
      <bibkey>clement-etal-2020-pymt5</bibkey>
    </paper>
    <paper id="731">
      <title>COGS : A Compositional Generalization Challenge Based on Semantic Interpretation<fixed-case>COGS</fixed-case>: A Compositional Generalization Challenge Based on Semantic Interpretation</title>
      <author><first>Najoung</first><last>Kim</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <pages>9087–9105</pages>
      <abstract>Natural language is characterized by <a href="https://en.wikipedia.org/wiki/Compositionality">compositionality</a> : the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization ; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (9699 %), but generalization accuracy was substantially lower (1635 %) and showed high sensitivity to random seed (+ -68 %). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.</abstract>
      <url hash="b338e673">2020.emnlp-main.731</url>
      <doi>10.18653/v1/2020.emnlp-main.731</doi>
      <video href="https://slideslive.com/38939064" />
      <bibkey>kim-linzen-2020-cogs</bibkey>
      <pwccode url="https://github.com/najoungkim/COGS" additional="false">najoungkim/COGS</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cfq">CFQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="732">
      <title>An Analysis of Natural Language Inference Benchmarks through the Lens of Negation</title>
      <author><first>Md Mosharaf</first><last>Hossain</last></author>
      <author><first>Venelin</first><last>Kovatchev</last></author>
      <author><first>Pranoy</first><last>Dutta</last></author>
      <author><first>Tiffany</first><last>Kao</last></author>
      <author><first>Elizabeth</first><last>Wei</last></author>
      <author><first>Eduardo</first><last>Blanco</last></author>
      <pages>9106–9118</pages>
      <abstract>Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> and still make the right inference judgments. In this paper, we present a new <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmark</a> for <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language inference</a> in which <a href="https://en.wikipedia.org/wiki/Affirmation_and_negation">negation</a> plays a critical role. We also show that state-of-the-art transformers struggle making inference judgments with the new pairs.</abstract>
      <url hash="671d4a9c">2020.emnlp-main.732</url>
      <doi>10.18653/v1/2020.emnlp-main.732</doi>
      <video href="https://slideslive.com/38939113" />
      <bibkey>hossain-etal-2020-analysis</bibkey>
    </paper>
    <paper id="733">
      <title>On the Sentence Embeddings from Pre-trained Language Models</title>
      <author><first>Bohan</first><last>Li</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Junxian</first><last>He</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Yiming</first><last>Yang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>9119–9130</pages>
      <abstract>Pre-trained contextual representations like <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> have achieved great success in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. However, the <a href="https://en.wikipedia.org/wiki/Sentence_embedding">sentence embeddings</a> from the pre-trained language models without <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the <a href="https://en.wikipedia.org/wiki/Semantics">semantic information</a> in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a>. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised objective</a>. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at.<url>https://github.com/bohanli/BERT-flow</url>.</abstract>
      <url hash="b156fa71">2020.emnlp-main.733</url>
      <doi>10.18653/v1/2020.emnlp-main.733</doi>
      <video href="https://slideslive.com/38939378" />
      <bibkey>li-etal-2020-sentence</bibkey>
      <pwccode url="https://github.com/bohanli/BERT-flow" additional="true">bohanli/BERT-flow</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-benchmark">STS Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016">Semantic Textual Similarity (2012 - 2016)</pwcdataset>
    </paper>
    <paper id="738">
      <title>Partially-Aligned Data-to-Text Generation with Distant Supervision</title>
      <author><first>Zihao</first><last>Fu</last></author>
      <author><first>Bei</first><last>Shi</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <pages>9183–9193</pages>
      <abstract>The Data-to-Text task aims to generate <a href="https://en.wikipedia.org/wiki/Human-readable_medium">human-readable text</a> for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain. Using partially-aligned data is an alternative way of solving the dataset scarcity problem. This kind of <a href="https://en.wikipedia.org/wiki/Data">data</a> is much easier to obtain since <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> can be produced automatically. However, using this kind of <a href="https://en.wikipedia.org/wiki/Data">data</a> induces the over-generation problem posing difficulties for existing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, which tends to add unrelated excerpts during the generation procedure. In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains. To tackle this new <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, we propose a novel distant supervision generation framework. It firstly estimates the input data’s supportiveness for each target word with an <a href="https://en.wikipedia.org/wiki/Estimator">estimator</a> and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively. We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg) by sampling sentences from <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> and automatically extracting corresponding KB triples for each sentence from <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a>. The experimental results show that our <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data.</abstract>
      <url hash="d18434db">2020.emnlp-main.738</url>
      <doi>10.18653/v1/2020.emnlp-main.738</doi>
      <video href="https://slideslive.com/38939283" />
      <bibkey>fu-etal-2020-partially</bibkey>
      <pwccode url="https://github.com/fuzihaofzh/distant_supervision_nlg" additional="false">fuzihaofzh/distant_supervision_nlg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
    </paper>
    <paper id="739">
      <title>Like hiking? You probably enjoy nature : Persona-grounded Dialog with Commonsense Expansions</title>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last></author>
      <author><first>Harsh</first><last>Jhamtani</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>9194–9206</pages>
      <abstract>Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models can not infer that interest in hiking might imply love for nature or longing for a break. In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions. Additionally, we introduce fine-grained grounding on <a href="https://en.wikipedia.org/wiki/Persona">personas</a> by encouraging the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to make a discrete choice among persona sentences while synthesizing a dialog response. Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions. Our model outperforms competitive baselines on the Persona-Chat dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation.</abstract>
      <url hash="c7bc69f4">2020.emnlp-main.739</url>
      <doi>10.18653/v1/2020.emnlp-main.739</doi>
      <video href="https://slideslive.com/38938989" />
      <bibkey>majumder-etal-2020-like</bibkey>
      <pwccode url="https://github.com/majumderb/compac" additional="false">majumderb/compac</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
    </paper>
    <paper id="741">
      <title>The World is Not Binary : Learning to Rank with <a href="https://en.wikipedia.org/wiki/Grayscale">Grayscale Data</a> for Dialogue Response Selection</title>
      <author><first>Zibo</first><last>Lin</last></author>
      <author><first>Deng</first><last>Cai</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>Xiaojiang</first><last>Liu</last></author>
      <author><first>Haitao</first><last>Zheng</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>9220–9229</pages>
      <abstract>Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task : each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this <a href="https://en.wikipedia.org/wiki/Formal_system">formalization</a> can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating <a href="https://en.wikipedia.org/wiki/Grayscale">grayscale data</a> for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that <a href="https://en.wikipedia.org/wiki/Grayscale">grayscale data</a> can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed <a href="https://en.wikipedia.org/wiki/Grayscale">grayscale data</a>, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the train-test discrepancy in terms of distractor strength. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is simple, effective, and universal. Experiments on three <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark datasets</a> and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements.</abstract>
      <url hash="06a33411">2020.emnlp-main.741</url>
      <doi>10.18653/v1/2020.emnlp-main.741</doi>
      <video href="https://slideslive.com/38938681" />
      <bibkey>lin-etal-2020-world</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/douban">Douban</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e-commerce-1">E-commerce</pwcdataset>
    </paper>
    <paper id="742">
      <title>GRADE : Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems<fixed-case>GRADE</fixed-case>: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems</title>
      <author><first>Lishan</first><last>Huang</last></author>
      <author><first>Zheng</first><last>Ye</last></author>
      <author><first>Jinghui</first><last>Qin</last></author>
      <author><first>Liang</first><last>Lin</last></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <pages>9230–9240</pages>
      <abstract>Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows. Here, we first consider that the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a> constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics. Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation. Specifically, <a href="https://en.wikipedia.org/wiki/GRADE">GRADE</a> incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence. The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights. Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgments. Besides, we release a new large-scale human evaluation benchmark to facilitate future research on <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">automatic metrics</a>.</abstract>
      <url hash="92dc5e93">2020.emnlp-main.742</url>
      <doi>10.18653/v1/2020.emnlp-main.742</doi>
      <video href="https://slideslive.com/38938945" />
      <bibkey>huang-etal-2020-grade</bibkey>
      <pwccode url="https://github.com/li3cmz/GRADE" additional="false">li3cmz/GRADE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="748">
      <title>On Extractive and Abstractive Neural Document Summarization with Transformer Language Models</title>
      <author><first>Jonathan</first><last>Pilault</last></author>
      <author><first>Raymond</first><last>Li</last></author>
      <author><first>Sandeep</first><last>Subramanian</last></author>
      <author><first>Chris</first><last>Pal</last></author>
      <pages>9308–9319</pages>
      <abstract>We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets : <a href="https://en.wikipedia.org/wiki/ArXiv">arXiv papers</a>, <a href="https://en.wikipedia.org/wiki/PubMed">PubMed papers</a>, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for <a href="https://en.wikipedia.org/wiki/Coherence_(linguistics)">coherence</a> and <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a>, but purely extractive methods score higher for informativeness and <a href="https://en.wikipedia.org/wiki/Relevance">relevance</a>. We hope that these <a href="https://en.wikipedia.org/wiki/Computer_architecture">architectures</a> and experiments may serve as strong points of comparison for future work. Note : The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.</abstract>
      <url hash="42f410a6">2020.emnlp-main.748</url>
      <doi>10.18653/v1/2020.emnlp-main.748</doi>
      <video href="https://slideslive.com/38938995" />
      <bibkey>pilault-etal-2020-extractive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed">Pubmed</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arxiv">arXiv</pwcdataset>
    </paper>
    <paper id="751">
      <title>Re-evaluating Evaluation in Text Summarization</title>
      <author><first>Manik</first><last>Bhandari</last></author>
      <author><first>Pranav Narayan</first><last>Gour</last></author>
      <author><first>Atabak</first><last>Ashfaq</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>9347–9359</pages>
      <abstract>Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a>. However, while the field has progressed, our standard <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> have not   for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a> : assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).</abstract>
      <url hash="e8f69f8e">2020.emnlp-main.751</url>
      <doi>10.18653/v1/2020.emnlp-main.751</doi>
      <video href="https://slideslive.com/38939364" />
      <bibkey>bhandari-etal-2020-evaluating</bibkey>
      <pwccode url="https://github.com/neulab/REALSumm" additional="false">neulab/REALSumm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    </volume>
  <volume id="demos" ingest-date="2020-11-05">
    <meta>
      <booktitle>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</booktitle>
      <editor><first>Qun</first><last>Liu</last></editor>
      <editor><first>David</first><last>Schlangen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>October</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="b657487c">2020.emnlp-demos.0</url>
      <bibkey>emnlp-2020-2020-empirical</bibkey>
    </frontmatter>
    <paper id="2">
      <title>BERTweet : A pre-trained language model for English Tweets<fixed-case>BERT</fixed-case>weet: A pre-trained language model for <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <author><first>Thanh</first><last>Vu</last></author>
      <author><first>Anh</first><last>Tuan Nguyen</last></author>
      <pages>9–14</pages>
      <abstract>We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks : <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">Part-of-speech tagging</a>, <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named-entity recognition</a> and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet</abstract>
      <url hash="97707e21">2020.emnlp-demos.2</url>
      <doi>10.18653/v1/2020.emnlp-demos.2</doi>
      <bibkey>nguyen-etal-2020-bertweet</bibkey>
      <pwccode url="https://github.com/VinAIResearch/BERTweet" additional="true">VinAIResearch/BERTweet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tweebank">Tweebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tweeteval">TweetEval</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2016-ner">WNUT 2016 NER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="3">
      <title>NeuralQA : A Usable Library for <a href="https://en.wikipedia.org/wiki/Question_answering">Question Answering</a> (Contextual Query Expansion + BERT) on Large Datasets<fixed-case>N</fixed-case>eural<fixed-case>QA</fixed-case>: A Usable Library for Question Answering (Contextual Query Expansion + <fixed-case>BERT</fixed-case>) on Large Datasets</title>
      <author><first>Victor</first><last>Dibia</last></author>
      <pages>15–22</pages>
      <abstract>Existing <a href="https://en.wikipedia.org/wiki/Tool">tools</a> for Question Answering (QA) have challenges that limit their use in practice. They can be complex to set up or integrate with existing infrastructure, do not offer configurable interactive interfaces, and do not cover the full set of subtasks that frequently comprise the QA pipeline (query expansion, retrieval, reading, and explanation / sensemaking). To help address these issues, we introduce NeuralQA-a usable library for <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA</a> on large datasets. NeuralQA integrates well with existing <a href="https://en.wikipedia.org/wiki/Infrastructure">infrastructure</a> (e.g., ElasticSearch instances and reader models trained with the HuggingFace Transformers API) and offers helpful defaults for QA subtasks. It introduces and implements contextual query expansion (CQE) using a masked language model (MLM) as well as relevant snippets (RelSnip)-a method for condensing large documents into smaller passages that can be speedily processed by a document reader model. Finally, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> offers a flexible user interface to support <a href="https://en.wikipedia.org/wiki/Workflow">workflows</a> for <a href="https://en.wikipedia.org/wiki/Scientific_method">research explorations</a> (e.g., visualization of gradient-based explanations to support qualitative inspection of model behaviour) and <a href="https://en.wikipedia.org/wiki/Search_engine_technology">large scale search deployment</a>. Code and documentation for NeuralQA is available as open source on Github.<tex-math>RelSnip</tex-math>) - a method for condensing large documents into smaller passages that can be speedily processed by a document reader model. Finally, it offers a flexible user interface to support workflows for research explorations (e.g., visualization of gradient-based explanations to support qualitative inspection of model behaviour) and large scale search deployment. Code and documentation for NeuralQA is available as open source on Github.</abstract>
      <url hash="4dff58ad">2020.emnlp-demos.3</url>
      <doi>10.18653/v1/2020.emnlp-demos.3</doi>
      <bibkey>dibia-2020-neuralqa</bibkey>
      <pwccode url="https://github.com/victordibia/neuralqa" additional="false">victordibia/neuralqa</pwccode>
    </paper>
    <paper id="9">
      <title>DeezyMatch : A Flexible Deep Learning Approach to Fuzzy String Matching<fixed-case>D</fixed-case>eezy<fixed-case>M</fixed-case>atch: A Flexible Deep Learning Approach to Fuzzy String Matching</title>
      <author><first>Kasra</first><last>Hosseini</last></author>
      <author><first>Federico</first><last>Nanni</last></author>
      <author><first>Mariona</first><last>Coll Ardanuy</last></author>
      <pages>62–69</pages>
      <abstract>We present DeezyMatch, a free, open-source software library written in <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a> for fuzzy string matching and candidate ranking. Its pair classifier supports various deep neural network architectures for training new classifiers and for fine-tuning a pretrained model, which paves the way for <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> in fuzzy string matching. This approach is especially useful where only limited training examples are available. The learned DeezyMatch models can be used to generate rich vector representations from string inputs. The candidate ranker component in DeezyMatch uses these vector representations to find, for a given query, the best matching candidates in a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a>. It uses an adaptive searching algorithm applicable to large knowledge bases and <a href="https://en.wikipedia.org/wiki/Query_language">query sets</a>. We describe DeezyMatch’s functionality, design and implementation, accompanied by a use case in toponym matching and <a href="https://en.wikipedia.org/wiki/Feasible_region">candidate ranking</a> in realistic noisy datasets.</abstract>
      <url hash="ea7e2915">2020.emnlp-demos.9</url>
      <doi>10.18653/v1/2020.emnlp-demos.9</doi>
      <bibkey>hosseini-etal-2020-deezymatch</bibkey>
      <pwccode url="https://github.com/Living-with-machines/DeezyMatch" additional="false">Living-with-machines/DeezyMatch</pwccode>
    </paper>
    <paper id="11">
      <title>InVeRo : Making Semantic Role Labeling Accessible with Intelligible Verbs and Roles<fixed-case>I</fixed-case>n<fixed-case>V</fixed-case>e<fixed-case>R</fixed-case>o: Making Semantic Role Labeling Accessible with Intelligible Verbs and Roles</title>
      <author><first>Simone</first><last>Conia</last></author>
      <author><first>Fabrizio</first><last>Brignone</last></author>
      <author><first>Davide</first><last>Zanfardino</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>77–84</pages>
      <abstract>Semantic Role Labeling (SRL) is deeply dependent on complex linguistic resources and sophisticated neural models, which makes the task difficult to approach for non-experts. To address this issue we present a new <a href="https://en.wikipedia.org/wiki/Computing_platform">platform</a> named Intelligible Verbs and Roles (InVeRo). This platform provides access to a new verb resource, VerbAtlas, and a state-of-the-art pretrained implementation of a neural, span-based architecture for SRL. Both the resource and the system provide human-readable verb sense and semantic role information, with an easy to use <a href="https://en.wikipedia.org/wiki/User_interface">Web interface</a> and <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">RESTful APIs</a> available at http://nlp.uniroma1.it/invero.</abstract>
      <url hash="aeafae47">2020.emnlp-demos.11</url>
      <doi>10.18653/v1/2020.emnlp-demos.11</doi>
      <bibkey>conia-etal-2020-invero</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="12">
      <title>Youling : an AI-assisted Lyrics Creation System<fixed-case>AI</fixed-case>-assisted Lyrics Creation System</title>
      <author><first>Rongsheng</first><last>Zhang</last></author>
      <author><first>Xiaoxi</first><last>Mao</last></author>
      <author><first>Le</first><last>Li</last></author>
      <author><first>Lin</first><last>Jiang</last></author>
      <author><first>Lin</first><last>Chen</last></author>
      <author><first>Zhiwei</first><last>Hu</last></author>
      <author><first>Yadong</first><last>Xi</last></author>
      <author><first>Changjie</first><last>Fan</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>85–91</pages>
      <abstract>Recently, a variety of <a href="https://en.wikipedia.org/wiki/Neural_circuit">neural models</a> have been proposed for lyrics generation. However, most previous work completes the generation process in a single pass with little <a href="https://en.wikipedia.org/wiki/Human–computer_interaction">human intervention</a>. We believe that lyrics creation is a creative process with human intelligence centered. AI should play a role as an assistant in the lyrics creation process, where human interactions are crucial for high-quality creation. This paper demonstrates Youling, an AI-assisted lyrics creation system, designed to collaborate with music creators. In the lyrics generation process, Youling supports traditional one pass full-text generation mode as well as an interactive generation mode, which allows users to select the satisfactory sentences from generated candidates conditioned on preceding context. The <a href="https://en.wikipedia.org/wiki/System">system</a> also provides a revision module which enables users to revise undesired sentences or words of <a href="https://en.wikipedia.org/wiki/Lyrics">lyrics</a> repeatedly. Besides, Youling allows users to use multifaceted attributes to control the content and format of generated lyrics. The demo video of the <a href="https://en.wikipedia.org/wiki/System">system</a> is available at https://youtu.be/DFeNpHk0pm4.<i>Youling</i>, an AI-assisted lyrics creation system, designed to collaborate with music creators. In the lyrics generation process, <i>Youling</i> supports traditional one pass full-text generation mode as well as an interactive generation mode, which allows users to select the satisfactory sentences from generated candidates conditioned on preceding context. The system also provides a revision module which enables users to revise undesired sentences or words of lyrics repeatedly. Besides, <i>Youling</i> allows users to use multifaceted attributes to control the content and format of generated lyrics. The demo video of the system is available at https://youtu.be/DFeNpHk0pm4.</abstract>
      <url hash="2901f09c">2020.emnlp-demos.12</url>
      <doi>10.18653/v1/2020.emnlp-demos.12</doi>
      <bibkey>zhang-etal-2020-youling</bibkey>
    </paper>
    <paper id="13">
      <title>A Technical Question Answering System with Transfer Learning</title>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Lingfei</first><last>Wu</last></author>
      <author><first>Yu</first><last>Deng</last></author>
      <author><first>Ruchi</first><last>Mahindru</last></author>
      <author><first>Qingkai</first><last>Zeng</last></author>
      <author><first>Sinem</first><last>Guven</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>92–99</pages>
      <abstract>In recent years, the need for community technical question-answering sites has increased significantly. However, it is often expensive for human experts to provide timely and helpful responses on those forums. We develop TransTQA, which is a novel system that offers automatic responses by retrieving proper answers based on correctly answered similar questions in the past. TransTQA is built upon a siamese ALBERT network, which enables it to respond quickly and accurately. Furthermore, TransTQA adopts a standard deep transfer learning strategy to improve its capability of supporting multiple technical domains.</abstract>
      <url hash="49626f9e">2020.emnlp-demos.13</url>
      <attachment type="OptionalSupplementaryMaterial" hash="27fdf124">2020.emnlp-demos.13.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.emnlp-demos.13</doi>
      <bibkey>yu-etal-2020-technical</bibkey>
      <pwccode url="https://github.com/wyu97/ttqa" additional="false">wyu97/ttqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/techqa">TechQA</pwcdataset>
    </paper>
    <paper id="15">
      <title>The Language Interpretability Tool : Extensible, Interactive Visualizations and Analysis for NLP Models<fixed-case>NLP</fixed-case> Models</title>
      <author><first>Ian</first><last>Tenney</last></author>
      <author><first>James</first><last>Wexler</last></author>
      <author><first>Jasmijn</first><last>Bastings</last></author>
      <author><first>Tolga</first><last>Bolukbasi</last></author>
      <author><first>Andy</first><last>Coenen</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Ellen</first><last>Jiang</last></author>
      <author><first>Mahima</first><last>Pushkarna</last></author>
      <author><first>Carey</first><last>Radebaugh</last></author>
      <author><first>Emily</first><last>Reif</last></author>
      <author><first>Ann</first><last>Yuan</last></author>
      <pages>107–118</pages>
      <abstract>We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior : Why did my <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> make this prediction? When does <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, <a href="https://en.wikipedia.org/wiki/Aggregate_data">aggregate analysis</a>, and <a href="https://en.wikipedia.org/wiki/Counterfactual_conditional">counterfactual generation</a> into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring <a href="https://en.wikipedia.org/wiki/Counterfactual_conditional">counterfactuals</a> for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, measuring <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> in <a href="https://en.wikipedia.org/wiki/Coreference">coreference systems</a>, and exploring local behavior in text generation. LIT supports a wide range of modelsincluding <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>, seq2seq, and structured predictionand is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.</abstract>
      <url hash="eceae4d3">2020.emnlp-demos.15</url>
      <doi>10.18653/v1/2020.emnlp-demos.15</doi>
      <bibkey>tenney-etal-2020-language</bibkey>
      <pwccode url="https://github.com/PAIR-code/lit" additional="false">PAIR-code/lit</pwccode>
    </paper>
    <paper id="26">
      <title>A Data-Centric Framework for Composable NLP Workflows<fixed-case>NLP</fixed-case> Workflows</title>
      <author><first>Zhengzhong</first><last>Liu</last></author>
      <author><first>Guanxiong</first><last>Ding</last></author>
      <author><first>Avinash</first><last>Bukkittu</last></author>
      <author><first>Mansi</first><last>Gupta</last></author>
      <author><first>Pengzhi</first><last>Gao</last></author>
      <author><first>Atif</first><last>Ahmed</last></author>
      <author><first>Shikun</first><last>Zhang</last></author>
      <author><first>Xin</first><last>Gao</last></author>
      <author><first>Swapnil</first><last>Singhavi</last></author>
      <author><first>Linwei</first><last>Li</last></author>
      <author><first>Wei</first><last>Wei</last></author>
      <author><first>Zecong</first><last>Hu</last></author>
      <author><first>Haoran</first><last>Shi</last></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <author><first>Eric</first><last>Xing</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <pages>197–204</pages>
      <abstract>Empirical natural language processing (NLP) systems in application domains (e.g., healthcare, finance, education) involve interoperation among multiple components, ranging from data ingestion, human annotation, to text retrieval, <a href="https://en.wikipedia.org/wiki/Data_analysis">analysis</a>, generation, and <a href="https://en.wikipedia.org/wiki/Data_visualization">visualization</a>. We establish a unified open-source framework to support fast development of such sophisticated NLP workflows in a composable manner. The <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> introduces a uniform data representation to encode heterogeneous results by a wide range of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP tasks</a>. It offers a large repository of processors for NLP tasks, <a href="https://en.wikipedia.org/wiki/Visualization_(graphics)">visualization</a>, and <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a>, which can be easily assembled with full interoperability under the unified representation. The highly extensible framework allows plugging in custom processors from external off-the-shelf NLP and deep learning libraries. The whole framework is delivered through two modularized yet integratable open-source projects, namely Forte (for workflow infrastructure and NLP function processors) and Stave (for user interaction, visualization, and annotation).</abstract>
      <url hash="8483bbe8">2020.emnlp-demos.26</url>
      <doi>10.18653/v1/2020.emnlp-demos.26</doi>
      <bibkey>liu-etal-2020-data-centric</bibkey>
      <pwccode url="https://github.com/asyml/forte" additional="false">asyml/forte</pwccode>
    </paper>
    <paper id="27">
      <title>CoRefi : A Crowd Sourcing Suite for Coreference Annotation<fixed-case>C</fixed-case>o<fixed-case>R</fixed-case>efi: A Crowd Sourcing Suite for Coreference Annotation</title>
      <author><first>Ari</first><last>Bornstein</last></author>
      <author><first>Arie</first><last>Cattan</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>205–215</pages>
      <abstract>Coreference annotation is an important, yet expensive and time consuming, task, which often involved expert annotators trained on complex decision guidelines. To enable cheaper and more efficient <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a>, we present CoRefi, a web-based coreference annotation suite, oriented for <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a>. Beyond the core coreference annotation tool, CoRefi provides guided onboarding for the task as well as a novel <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> for a reviewing phase. CoRefi is open source and directly embeds into any website, including popular <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing platforms</a>. CoRefi Demo : aka.ms/corefi Video Tour : aka.ms/corefivideo Github Repo : https://github.com/aribornstein/corefi</abstract>
      <url hash="471da4d2">2020.emnlp-demos.27</url>
      <doi>10.18653/v1/2020.emnlp-demos.27</doi>
      <bibkey>bornstein-etal-2020-corefi</bibkey>
      <pwccode url="https://github.com/aribornstein/corefi" additional="true">aribornstein/corefi</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    </volume>
  <volume id="tutorials" ingest-date="2020-11-05">
    <meta>
      <booktitle>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</booktitle>
      <editor><first>Aline</first><last>Villavicencio</last></editor>
      <editor><first>Benjamin</first><last>Van Durme</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="1434ceba">2020.emnlp-tutorials.0</url>
      <bibkey>emnlp-2020-2020-empirical-methods</bibkey>
    </frontmatter>
    <paper id="6">
      <title>Simultaneous Translation</title>
      <author><first>Liang</first><last>Huang</last></author>
      <author><first>Colin</first><last>Cherry</last></author>
      <author><first>Mingbo</first><last>Ma</last></author>
      <author><first>Naveen</first><last>Arivazhagan</last></author>
      <author><first>Zhongjun</first><last>He</last></author>
      <pages>34–36</pages>
      <abstract>Simultaneous translation, which performs <a href="https://en.wikipedia.org/wiki/Translation">translation</a> concurrently with the source speech, is widely useful in many scenarios such as international conferences, <a href="https://en.wikipedia.org/wiki/Negotiation">negotiations</a>, press releases, legal proceedings, and <a href="https://en.wikipedia.org/wiki/Medicine">medicine</a>. This <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> has long been considered one of the hardest problems in <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI</a> and one of its holy grails. Recently, with rapid improvements in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition</a>, and <a href="https://en.wikipedia.org/wiki/Speech_synthesis">speech synthesis</a>, there has been exciting progress towards <a href="https://en.wikipedia.org/wiki/Simultaneous_translation">simultaneous translation</a>. This tutorial will focus on the design and evaluation of <a href="https://en.wikipedia.org/wiki/Policy">policies</a> for <a href="https://en.wikipedia.org/wiki/Simultaneous_translation">simultaneous translation</a>, to leave attendees with a deep technical understanding of the history, the recent advances, and the remaining challenges in this field.</abstract>
      <url hash="75bb7259">2020.emnlp-tutorials.6</url>
      <doi>10.18653/v1/2020.emnlp-tutorials.6</doi>
      <bibkey>huang-etal-2020-simultaneous</bibkey>
    </paper>
    </volume>
</collection>