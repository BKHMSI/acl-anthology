<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.socialnlp">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media</booktitle>
      <editor><first>Lun-Wei</first><last>Ku</last></editor>
      <editor><first>Cheng-Te</first><last>Li</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.socialnlp-1</url>
    </meta>
    <frontmatter>
      <url hash="d0f89066">2021.socialnlp-1.0</url>
      <bibkey>socialnlp-2021-international</bibkey>
    </frontmatter>
    <paper id="3">
      <title>A Case Study of In-House Competition for Ranking Constructive Comments in a News Service</title>
      <author><first>Hayato</first><last>Kobayashi</last></author>
      <author><first>Hiroaki</first><last>Taguchi</last></author>
      <author><first>Yoshimune</first><last>Tabuchi</last></author>
      <author><first>Chahine</first><last>Koleejan</last></author>
      <author><first>Ken</first><last>Kobayashi</last></author>
      <author><first>Soichiro</first><last>Fujita</last></author>
      <author><first>Kazuma</first><last>Murao</last></author>
      <author><first>Takeshi</first><last>Masuyama</last></author>
      <author><first>Taichi</first><last>Yatsuka</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <author><first>Satoshi</first><last>Sekine</last></author>
      <pages>24–35</pages>
      <abstract>Ranking the user comments posted on a news article is important for online news services because comment visibility directly affects the user experience. Research on ranking comments with different <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> to measure the comment quality has shown constructiveness used in argument analysis is promising from a practical standpoint. In this paper, we report a case study in which this <a href="https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_education)">constructiveness</a> is examined in the real world. Specifically, we examine an in-house competition to improve the performance of ranking constructive comments and demonstrate the effectiveness of the best obtained <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> for a <a href="https://en.wikipedia.org/wiki/Service_(economics)">commercial service</a>.</abstract>
      <url hash="973a90ae">2021.socialnlp-1.3</url>
      <doi>10.18653/v1/2021.socialnlp-1.3</doi>
      <bibkey>kobayashi-etal-2021-case</bibkey>
    </paper>
    <paper id="4">
      <title>Quantifying the Effects of COVID-19 on Restaurant Reviews<fixed-case>COVID</fixed-case>-19 on Restaurant Reviews</title>
      <author><first>Ivy</first><last>Cao</last></author>
      <author><first>Zizhou</first><last>Liu</last></author>
      <author><first>Giannis</first><last>Karamanolakis</last></author>
      <author><first>Daniel</first><last>Hsu</last></author>
      <author><first>Luis</first><last>Gravano</last></author>
      <pages>36–60</pages>
      <abstract>The COVID-19 pandemic has implications beyond <a href="https://en.wikipedia.org/wiki/Health">physical health</a>, affecting society and economies. Government efforts to slow down the spread of the virus have had a severe impact on many businesses, including restaurants. Mandatory policies such as restaurant closures, bans on social gatherings, and social distancing restrictions have affected restaurant operations as well as customer preferences (e.g., prompting a demand of stricter hygiene standards). As of now, however, it is not clear how and to what extent the <a href="https://en.wikipedia.org/wiki/Pandemic">pandemic</a> has affected restaurant reviews, an analysis of which could potentially inform policies for addressing this ongoing situation. In this work, we present our efforts to understand the effects of COVID-19 on restaurant reviews, with a focus on Yelp reviews produced during the pandemic for New York City and Los Angeles County restaurants. Overall, we make the following contributions. First, we assemble a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 600 <a href="https://en.wikipedia.org/wiki/Review">reviews</a> with manual annotations of fine-grained COVID-19 aspects related to <a href="https://en.wikipedia.org/wiki/Restaurant">restaurants</a> (e.g., <a href="https://en.wikipedia.org/wiki/Hygiene">hygiene practices</a>, <a href="https://en.wikipedia.org/wiki/Service_(economics)">service changes</a>, sympathy and support for local businesses). Second, we address COVID-19 aspect detection using supervised classifiers, weakly-supervised approaches based on keywords, and unsupervised topic modeling approaches, and experimentally show that <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> based on pre-trained BERT representations achieve the best performance (F1=0.79). Third, we analyze the number and evolution of COVID-related aspects over time and show that the resulting <a href="https://en.wikipedia.org/wiki/Time_series">time series</a> have substantial correlation (Spearman’s = 0.84) with critical statistics related to the COVID-19 pandemic, including the number of new COVID-19 cases.<tex-math>\rho</tex-math>=0.84) with critical statistics related to the COVID-19 pandemic, including the number of new COVID-19 cases. To our knowledge, this is the first work analyzing the effects of COVID-19 on Yelp restaurant reviews and could potentially inform policies by public health departments, for example, to cover resource utilization.</abstract>
      <url hash="71f3d389">2021.socialnlp-1.4</url>
      <doi>10.18653/v1/2021.socialnlp-1.4</doi>
      <bibkey>cao-etal-2021-quantifying</bibkey>
    </paper>
    <paper id="5">
      <title>Assessing Cognitive Linguistic Influences in the Assignment of Blame</title>
      <author><first>Karen</first><last>Zhou</last></author>
      <author><first>Ana</first><last>Smith</last></author>
      <author><first>Lillian</first><last>Lee</last></author>
      <pages>61–69</pages>
      <abstract>Lab studies in <a href="https://en.wikipedia.org/wiki/Cognition">cognition</a> and the <a href="https://en.wikipedia.org/wiki/Psychology_of_morality">psychology of morality</a> have proposed some thematic and linguistic factors that influence <a href="https://en.wikipedia.org/wiki/Moral_reasoning">moral reasoning</a>. This paper assesses how well the findings of these studies generalize to a large corpus of over 22,000 descriptions of fraught situations posted to a dedicated forum. At this social-media site, users judge whether or not an author is in the wrong with respect to the event that the author described. We find that, consistent with lab studies, there are statistically significant differences in uses of first-person passive voice, as well as first-person agents and patients, between descriptions of situations that receive different blame judgments. These <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> also aid performance in the task of predicting the eventual collective verdicts.</abstract>
      <url hash="1364306e">2021.socialnlp-1.5</url>
      <doi>10.18653/v1/2021.socialnlp-1.5</doi>
      <bibkey>zhou-etal-2021-assessing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/scruples">Scruples</pwcdataset>
    </paper>
    <paper id="6">
      <title>Evaluating Deception Detection Model Robustness To <a href="https://en.wikipedia.org/wiki/Variation_(linguistics)">Linguistic Variation</a></title>
      <author><first>Maria</first><last>Glenski</last></author>
      <author><first>Ellyn</first><last>Ayton</last></author>
      <author><first>Robin</first><last>Cosbey</last></author>
      <author><first>Dustin</first><last>Arendt</last></author>
      <author><first>Svitlana</first><last>Volkova</last></author>
      <pages>70–80</pages>
      <abstract>With the increasing use of machine-learning driven algorithmic judgements, it is critical to develop <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that are robust to evolving or manipulated inputs. We propose an extensive analysis of <a href="https://en.wikipedia.org/wiki/Robust_statistics">model robustness</a> against <a href="https://en.wikipedia.org/wiki/Variation_(linguistics)">linguistic variation</a> in the setting of deceptive news detection, an important task in the context of <a href="https://en.wikipedia.org/wiki/Misinformation">misinformation spread online</a>. We consider two prediction tasks and compare three state-of-the-art embeddings to highlight consistent trends in <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance, high confidence misclassifications, and high impact failures. By measuring the effectiveness of adversarial defense strategies and evaluating model susceptibility to adversarial attacks using character- and word-perturbed text, we find that character or mixed ensemble models are the most effective defenses and that character perturbation-based attack tactics are more successful.</abstract>
      <url hash="725f36c6">2021.socialnlp-1.6</url>
      <doi>10.18653/v1/2021.socialnlp-1.6</doi>
      <bibkey>glenski-etal-2021-evaluating</bibkey>
    </paper>
    <paper id="11">
      <title>Using Noisy Self-Reports to Predict Twitter User Demographics<fixed-case>T</fixed-case>witter User Demographics</title>
      <author><first>Zach</first><last>Wood-Doughty</last></author>
      <author><first>Paiheng</first><last>Xu</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>123–137</pages>
      <abstract>Computational social science studies often contextualize <a href="https://en.wikipedia.org/wiki/Content_analysis">content analysis</a> within standard <a href="https://en.wikipedia.org/wiki/Demography">demographics</a>. Since <a href="https://en.wikipedia.org/wiki/Demography">demographics</a> are unavailable on many <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a> (e.g. Twitter), numerous studies have inferred <a href="https://en.wikipedia.org/wiki/Demography">demographics</a> automatically. Despite many studies presenting proof-of-concept inference of race and ethnicity, training of practical systems remains elusive since there are few annotated datasets. Existing datasets are small, inaccurate, or fail to cover the four most common <a href="https://en.wikipedia.org/wiki/Race_and_ethnicity_in_the_United_States">racial and ethnic groups</a> in the United States. We present a method to identify self-reports of race and ethnicity from Twitter profile descriptions. Despite the noise of automated supervision, our <a href="https://en.wikipedia.org/wiki/Self-report_study">self-report datasets</a> enable improvements in <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> performance on gold standard <a href="https://en.wikipedia.org/wiki/Self-report_study">self-report survey data</a>. The result is a reproducible method for creating <a href="https://en.wikipedia.org/wiki/Training_and_development">large-scale training resources</a> for <a href="https://en.wikipedia.org/wiki/Race_and_ethnicity_in_the_United_States">race and ethnicity</a>.</abstract>
      <url hash="c7fca580">2021.socialnlp-1.11</url>
      <doi>10.18653/v1/2021.socialnlp-1.11</doi>
      <bibkey>wood-doughty-etal-2021-using</bibkey>
      <pwccode url="https://bitbucket.org/mdredze/demographer" additional="false">mdredze/demographer</pwccode>
    </paper>
    <paper id="12">
      <title>PANDORA Talks : Personality and Demographics on Reddit<fixed-case>PANDORA</fixed-case> Talks: Personality and Demographics on <fixed-case>R</fixed-case>eddit</title>
      <author><first>Matej</first><last>Gjurković</last></author>
      <author><first>Mladen</first><last>Karan</last></author>
      <author><first>Iva</first><last>Vukojević</last></author>
      <author><first>Mihaela</first><last>Bošnjak</last></author>
      <author><first>Jan</first><last>Snajder</last></author>
      <pages>138–152</pages>
      <abstract>Personality and demographics are important variables in <a href="https://en.wikipedia.org/wiki/Social_science">social sciences</a> and computational sociolinguistics. However, <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> with both <a href="https://en.wikipedia.org/wiki/Personality_type">personality and demographic labels</a> are scarce. To address this, we present <a href="https://en.wikipedia.org/wiki/PANDORA">PANDORA</a>, the first dataset of Reddit comments of 10k users partially labeled with three personality models and <a href="https://en.wikipedia.org/wiki/Demography">demographics</a> (age, gender, and location), including 1.6k users labeled with the well-established Big 5 personality model. We showcase the usefulness of this dataset on three experiments, where we leverage the more readily available data from other personality models to predict the Big 5 traits, analyze gender classification biases arising from psycho-demographic variables, and carry out a confirmatory and exploratory analysis based on psychological theories. Finally, we present benchmark prediction models for all <a href="https://en.wikipedia.org/wiki/Personality_type">personality and demographic variables</a>.</abstract>
      <url hash="05dfea1b">2021.socialnlp-1.12</url>
      <doi>10.18653/v1/2021.socialnlp-1.12</doi>
      <bibkey>gjurkovic-etal-2021-pandora</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pandora">PANDORA</pwcdataset>
    </paper>
    <paper id="13">
      <title>Room to Grow : Understanding Personal Characteristics Behind Self Improvement Using <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a></title>
      <author><first>MeiXing</first><last>Dong</last></author>
      <author><first>Xueming</first><last>Xu</last></author>
      <author><first>Yiwei</first><last>Zhang</last></author>
      <author><first>Ian</first><last>Stewart</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>153–162</pages>
      <abstract>Many people aim for change, but not everyone succeeds. While there are a number of <a href="https://en.wikipedia.org/wiki/Social_psychology">social psychology theories</a> that propose motivation-related characteristics of those who persist with change, few <a href="https://en.wikipedia.org/wiki/Computational_psychology">computational studies</a> have explored the motivational stage of personal change. In this paper, we investigate a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> consisting of the writings of people who manifest intention to change, some of whom persist while others do not. Using a variety of <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic analysis techniques</a>, we first examine the <a href="https://en.wikipedia.org/wiki/Writing_style">writing patterns</a> that distinguish the two groups of people. Persistent people tend to reference more topics related to long-term self-improvement and use a more complicated writing style. Drawing on these consistent differences, we build a <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifier</a> that can reliably identify the people more likely to persist, based on their language. Our experiments provide new insights into the motivation-related behavior of people who persist with their intention to change.</abstract>
      <url hash="b1cfcb24">2021.socialnlp-1.13</url>
      <doi>10.18653/v1/2021.socialnlp-1.13</doi>
      <bibkey>dong-etal-2021-room</bibkey>
    </paper>
    <paper id="15">
      <title>Jujeop : Korean Puns for K-pop Stars on <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a><fixed-case>K</fixed-case>orean Puns for K-pop Stars on Social Media</title>
      <author><first>Soyoung</first><last>Oh</last></author>
      <author><first>Jisu</first><last>Kim</last></author>
      <author><first>Seungpeel</first><last>Lee</last></author>
      <author><first>Eunil</first><last>Park</last></author>
      <pages>170–177</pages>
      <abstract>Jujeop is a type of <a href="https://en.wikipedia.org/wiki/Pun">pun</a> and a unique way for fans to express their love for the K-pop stars they follow using <a href="https://en.wikipedia.org/wiki/Korean_language">Korean</a>. One of the unique characteristics of Jujeop is its use of <a href="https://en.wikipedia.org/wiki/Exaggeration">exaggerated expressions</a> to compliment <a href="https://en.wikipedia.org/wiki/K-pop">K-pop stars</a>, which contain or lead to <a href="https://en.wikipedia.org/wiki/Humour">humor</a>. Based on this characteristic, Jujeop can be separated into four distinct types, with their own lexical collocations : (1) Fragmenting words to create a twist, (2) <a href="https://en.wikipedia.org/wiki/Homophone">Homophones</a> and <a href="https://en.wikipedia.org/wiki/Homograph">homographs</a>, (3) Repetition, and (4) <a href="https://en.wikipedia.org/wiki/Nonsense">Nonsense</a>. Thus, the current study first defines the concept of Jujeop in <a href="https://en.wikipedia.org/wiki/Korean_language">Korean</a>, manually labels 8.6 K comments and annotates the comments to one of the four Jujeop types. With the given annotated corpus, this study presents distinctive characteristics of Jujeop comments compared to the other comments by classification task. Moreover, with the clustering approach, we proposed a structural dependency within each Jujeop type. We have made our dataset publicly available for future research of Jujeop expressions.</abstract>
      <url hash="2bd866e7">2021.socialnlp-1.15</url>
      <doi>10.18653/v1/2021.socialnlp-1.15</doi>
      <bibkey>oh-etal-2021-jujeop</bibkey>
      <pwccode url="https://github.com/merry555/jujeop" additional="false">merry555/jujeop</pwccode>
    </paper>
    </volume>
</collection>