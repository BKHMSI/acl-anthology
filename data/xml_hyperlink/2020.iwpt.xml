<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.iwpt">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies</booktitle>
      <editor><first>Gosse</first><last>Bouma</last></editor>
      <editor><first>Yuji</first><last>Matsumoto</last></editor>
      <editor><first>Stephan</first><last>Oepen</last></editor>
      <editor><first>Kenji</first><last>Sagae</last></editor>
      <editor><first>Djamé</first><last>Seddah</last></editor>
      <editor><first>Weiwei</first><last>Sun</last></editor>
      <editor><first>Anders</first><last>Søgaard</last></editor>
      <editor><first>Reut</first><last>Tsarfaty</last></editor>
      <editor><first>Dan</first><last>Zeman</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="fe3f84b1">2020.iwpt-1</url>
    </meta>
    <frontmatter>
      <url hash="93260931">2020.iwpt-1.0</url>
      <bibkey>iwpt-2020-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Syntactic Parsing in Humans and Machines</title>
      <author><first>Paola</first><last>Merlo</last></author>
      <pages>1</pages>
      <abstract>To process the syntactic structures of a language in ways that are compatible with human expectations, we need computational representations of lexical and syntactic properties that form the basis of human knowledge of words and sentences. Recent neural-network-based and distributed semantics techniques have developed systems of considerable practical success and impressive performance. As has been advocated by many, however, such <a href="https://en.wikipedia.org/wiki/System">systems</a> still lack human-like properties. In particular, linguistic, psycholinguistic and neuroscientific investigations have shown that human processing of sentences is sensitive to structure and unbounded relations. In the spirit of better understanding the structure building and long-distance properties of <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>, I will present an overview of recent results on agreement and island effects in <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> in several languages. While certain sets of results in the literature indicate that neural language models exhibit long-distance agreement abilities, other finer-grained investigation of how these effects are calculated indicates that that the similarity spaces they define do not correlate with human experimental results on intervention similarity in long-distance dependencies. This opens the way to reflections on how to better match the <a href="https://en.wikipedia.org/wiki/Syntax">syntactic properties</a> of <a href="https://en.wikipedia.org/wiki/Natural_language">natural languages</a> in the representations of neural models.</abstract>
      <url hash="c81e0f9b">2020.iwpt-1.1</url>
      <doi>10.18653/v1/2020.iwpt-1.1</doi>
      <video href="http://slideslive.com/38929668" />
      <bibkey>merlo-2020-syntactic</bibkey>
    </paper>
    <paper id="5">
      <title>Semi-supervised Parsing with a Variational Autoencoding Parser</title>
      <author><first>Xiao</first><last>Zhang</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>40–47</pages>
      <abstract>We propose an end-to-end variational autoencoding parsing (VAP) model for semi-supervised graph-based projective dependency parsing. It encodes the input using <a href="https://en.wikipedia.org/wiki/Latent_variable_model">continuous latent variables</a> in a sequential manner by <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks (DNN)</a> that can utilize the contextual information, and reconstruct the input using a <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a>. The VAP model admits a unified structure with different <a href="https://en.wikipedia.org/wiki/Loss_function">loss functions</a> for labeled and unlabeled data with shared parameters. We conducted experiments on the WSJ data sets, showing the proposed model can use the unlabeled data to increase the performance on a limited amount of labeled data, on a par with a recently proposed semi-supervised parser with faster inference.</abstract>
      <url hash="e7d65401">2020.iwpt-1.5</url>
      <doi>10.18653/v1/2020.iwpt-1.5</doi>
      <video href="http://slideslive.com/38929672" />
      <bibkey>zhang-goldwasser-2020-semi</bibkey>
    </paper>
    <paper id="7">
      <title>Obfuscation for Privacy-preserving Syntactic Parsing</title>
      <author><first>Zhifeng</first><last>Hu</last></author>
      <author><first>Serhii</first><last>Havrylov</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <pages>62–72</pages>
      <abstract>The goal of <a href="https://en.wikipedia.org/wiki/Homomorphic_encryption">homomorphic encryption</a> is to encrypt data such that another party can operate on it without being explicitly exposed to the content of the original data. We introduce an idea for a privacy-preserving transformation on <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language data</a>, inspired by <a href="https://en.wikipedia.org/wiki/Homomorphic_encryption">homomorphic encryption</a>. Our primary tool is <a href="https://en.wikipedia.org/wiki/Obfuscation">obfuscation</a>, relying on the properties of <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>. Specifically, a given English text is obfuscated using a neural model that aims to preserve the syntactic relationships of the original sentence so that the obfuscated sentence can be parsed instead of the original one. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> works at the word level, and learns to obfuscate each word separately by changing it into a new word that has a similar syntactic role. The text obfuscated by our model leads to better performance on three syntactic parsers (two dependency and one constituency parsers) in comparison to an upper-bound random substitution baseline. More specifically, the results demonstrate that as more terms are obfuscated (by their part of speech), the substitution upper bound significantly degrades, while the neural model maintains a relatively high performing <a href="https://en.wikipedia.org/wiki/Parsing">parser</a>. All of this is done without much sacrifice of <a href="https://en.wikipedia.org/wiki/Privacy">privacy</a> compared to the random substitution upper bound. We also further analyze the results, and discover that the substituted words have similar <a href="https://en.wikipedia.org/wiki/Syntax">syntactic properties</a>, but different <a href="https://en.wikipedia.org/wiki/Semantics">semantic content</a>, compared to the original words.<i>obfuscation</i>, relying on the properties of natural language. Specifically, a given English text is obfuscated using a neural model that aims to preserve the syntactic relationships of the original sentence so that the obfuscated sentence can be parsed instead of the original one. The model works at the word level, and learns to obfuscate each word separately by changing it into a new word that has a similar syntactic role. The text obfuscated by our model leads to better performance on three syntactic parsers (two dependency and one constituency parsers) in comparison to an upper-bound random substitution baseline. More specifically, the results demonstrate that as more terms are obfuscated (by their part of speech), the substitution upper bound significantly degrades, while the neural model maintains a relatively high performing parser. All of this is done without much sacrifice of privacy compared to the random substitution upper bound. We also further analyze the results, and discover that the substituted words have similar syntactic properties, but different semantic content, compared to the original words.</abstract>
      <url hash="3d25adb8">2020.iwpt-1.7</url>
      <doi>10.18653/v1/2020.iwpt-1.7</doi>
      <video href="http://slideslive.com/38929674" />
      <bibkey>hu-etal-2020-obfuscation</bibkey>
      <pwccode url="https://github.com/ichn-hu/Parsing-Obfuscation" additional="false">ichn-hu/Parsing-Obfuscation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="8">
      <title>Tensors over Semirings for Latent-Variable Weighted Logic Programs</title>
      <author><first>Esma</first><last>Balkir</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <pages>73–90</pages>
      <abstract>Semiring parsing is an elegant <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> for describing <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a> by using semiring weighted logic programs. In this paper we present a generalization of this <a href="https://en.wikipedia.org/wiki/Concept">concept</a> : latent-variable semiring parsing. With our framework, any <a href="https://en.wikipedia.org/wiki/Semiring">semiring weighted logic program</a> can be latentified by transforming weights from scalar values of a <a href="https://en.wikipedia.org/wiki/Semiring">semiring</a> to rank-n arrays, or tensors, of <a href="https://en.wikipedia.org/wiki/Semiring">semiring values</a>, allowing the modelling of latent-variable models within the <a href="https://en.wikipedia.org/wiki/Semiring">semiring parsing framework</a>. Semiring is too strong a notion when dealing with <a href="https://en.wikipedia.org/wiki/Tensor">tensors</a>, and we have to resort to a weaker structure : a partial semiring. We prove that this <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a> preserves all the desired properties of the original semiring framework while strictly increasing its expressiveness.</abstract>
      <url hash="36981a2b">2020.iwpt-1.8</url>
      <doi>10.18653/v1/2020.iwpt-1.8</doi>
      <attachment type="Dataset" hash="322b1060">2020.iwpt-1.8.Dataset.pdf</attachment>
      <video href="http://slideslive.com/38929675" />
      <bibkey>balkir-etal-2020-tensors</bibkey>
    </paper>
    <paper id="11">
      <title>Self-Training for Unsupervised Parsing with PRPN<fixed-case>PRPN</fixed-case></title>
      <author><first>Anhad</first><last>Mohananey</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <pages>105–110</pages>
      <abstract>Neural unsupervised parsing (UP) models learn to parse without access to syntactic annotations, while being optimized for another task like <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a>. In this work, we propose self-training for neural UP models : we leverage aggregated annotations predicted by copies of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> as supervision for future copies. To be able to use our model’s predictions during training, we extend a recent neural UP architecture, the PRPN (Shen et al., 2018a), such that it can be trained in a semi-supervised fashion. We then add examples with <a href="https://en.wikipedia.org/wiki/Parsing">parses</a> predicted by our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to our unlabeled UP training data. Our self-trained model outperforms the PRPN by 8.1 % <a href="https://en.wikipedia.org/wiki/F-number">F1</a> and the previous state of the art by 1.6 % <a href="https://en.wikipedia.org/wiki/F-number">F1</a>. In addition, we show that our <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a> can also be helpful for semi-supervised parsing in ultra-low-resource settings.</abstract>
      <url hash="e557f12f">2020.iwpt-1.11</url>
      <doi>10.18653/v1/2020.iwpt-1.11</doi>
      <video href="http://slideslive.com/38929678" />
      <bibkey>mohananey-etal-2020-self</bibkey>
    </paper>
    <paper id="19">
      <title>Adaptation of Multilingual Transformer Encoder for Robust Enhanced Universal Dependency Parsing<fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsing</title>
      <author><first>Han</first><last>He</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>181–191</pages>
      <abstract>This paper presents our enhanced dependency parsing approach using transformer encoders, coupled with a simple yet powerful ensemble algorithm that takes advantage of both tree and graph dependency parsing. Two types of transformer encoders are compared, a multilingual encoder and language-specific encoders. Our dependency tree parsing (DTP) approach generates only primary dependencies to form trees whereas our dependency graph parsing (DGP) approach handles both primary and secondary dependencies to form graphs. Since DGP does not guarantee the generated graphs are acyclic, the ensemble algorithm is designed to add secondary arcs predicted by DGP to primary arcs predicted by DTP. Our results show that <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> using the multilingual encoder outperform ones using the language specific encoders for most languages. The ensemble models generally show higher labeled attachment score on enhanced dependencies (ELAS) than the DTP and DGP models. As the result, our best <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> rank the third place on the macro-average ELAS over 17 languages.</abstract>
      <url hash="4beeb14f">2020.iwpt-1.19</url>
      <doi>10.18653/v1/2020.iwpt-1.19</doi>
      <video href="http://slideslive.com/38929686" />
      <bibkey>he-choi-2020-adaptation</bibkey>
    </paper>
    <paper id="21">
      <title>Linear Neural Parsing and Hybrid Enhancement for Enhanced Universal Dependencies<fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Giuseppe</first><last>Attardi</last></author>
      <author><first>Daniele</first><last>Sartiano</last></author>
      <author><first>Maria</first><last>Simi</last></author>
      <pages>206–214</pages>
      <abstract>To accomplish the shared task on <a href="https://en.wikipedia.org/wiki/Dependency_grammar">dependency parsing</a> we explore the use of a linear transition-based neural dependency parser as well as a combination of three of them by means of a linear tree combination algorithm. We train separate <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for each language on the shared task data. We compare our base <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> with two biaffine parsers and also present an ensemble combination of all five <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a>, which achieves an average UAS 1.88 point lower than the top official submission. For producing the enhanced dependencies, we exploit a hybrid approach, coupling an algorithmic graph transformation of the dependency tree with predictions made by a multitask machine learning model.</abstract>
      <url hash="cab821b1">2020.iwpt-1.21</url>
      <doi>10.18653/v1/2020.iwpt-1.21</doi>
      <video href="http://slideslive.com/38929688" />
      <bibkey>attardi-etal-2020-linear</bibkey>
    </paper>
    <paper id="23">
      <title>How Much of Enhanced UD Is Contained in UD?<fixed-case>UD</fixed-case> Is Contained in <fixed-case>UD</fixed-case>?</title>
      <author><first>Adam</first><last>Ek</last></author>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <pages>221–226</pages>
      <abstract>In this paper, we present the submission of team CLASP to the IWPT 2020 Shared Task on parsing enhanced universal dependencies. We develop a tree-to-graph transformation algorithm based on dependency patterns. This <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> can transform gold UD trees to EUD graphs with an ELAS score of 81.55 and a EULAS score of 96.70. These results show that much of the information needed to construct EUD graphs from UD trees are present in the UD trees. Coupled with a standard UD parser, the method applies to the official test data and yields and ELAS score of 67.85 and a EULAS score is 80.18.</abstract>
      <url hash="f2527a0f">2020.iwpt-1.23</url>
      <doi>10.18653/v1/2020.iwpt-1.23</doi>
      <video href="http://slideslive.com/38929690" />
      <bibkey>ek-bernardy-2020-much</bibkey>
    </paper>
    </volume>
</collection>