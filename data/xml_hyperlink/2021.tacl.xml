<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.tacl">
  <volume id="1">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 9</booktitle>
      <editor><last>Roark</last><first>Brian</first></editor>
      <editor><last>Nenkova</last><first>Ani</first></editor>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2021</year>
    </meta>
    <paper id="2">
      <title>Revisiting Multi-Domain Machine Translation</title>
      <author><first>MinhQuang</first><last>Pham</last></author>
      <author><first>Josep Maria</first><last>Crego</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <doi>10.1162/tacl_a_00351</doi>
      <abstract>When building <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a>, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing. This multi-domain scenario has attracted a lot of recent work that fall under the general umbrella of <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>. In this study, we revisit multi-domain machine translation, with the aim to formulate the motivations for developing such systems and the associated expectations with respect to performance. Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behaviour of multi-domain systems and to make them fully hold their promises.</abstract>
      <pages>17–35</pages>
      <url hash="d31dab0a">2021.tacl-1.2</url>
      <bibkey>pham-etal-2021-revisiting</bibkey>
    </paper>
    <paper id="3">
      <title>Conversation Graph : Data Augmentation, Training, and Evaluation for Non-Deterministic Dialogue Management</title>
      <author><first>Milan</first><last>Gritta</last></author>
      <author><first>Gerasimos</first><last>Lampouras</last></author>
      <author><first>Ignacio</first><last>Iacobacci</last></author>
      <doi>10.1162/tacl_a_00352</doi>
      <abstract>Task-oriented dialogue systems typically rely on large amounts of high-quality training data or require complex handcrafted rules. However, existing datasets are often limited in size con- sidering the <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> of the <a href="https://en.wikipedia.org/wiki/Dialogue">dialogues</a>. Additionally, conventional training signal in- ference is not suitable for non-deterministic agent behavior, namely, considering multiple actions as valid in identical dialogue states. We propose the Conversation Graph (ConvGraph), a graph-based representation of dialogues that can be exploited for <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, multi- reference training and evaluation of non- deterministic agents. ConvGraph generates novel dialogue paths to augment data volume and diversity. Intrinsic and extrinsic evaluation across three datasets shows that data augmentation and/or multi-reference training with ConvGraph can improve dialogue success rates by up to 6.4 %.</abstract>
      <pages>36–52</pages>
      <url hash="15fc1f8e">2021.tacl-1.3</url>
      <bibkey>gritta-etal-2021-conversation</bibkey>
    </paper>
    <paper id="4">
      <title>Efficient Content-Based Sparse Attention with Routing Transformers</title>
      <author><first>Aurko</first><last>Roy</last></author>
      <author><first>Mohammad</first><last>Saffar</last></author>
      <author><first>Ashish</first><last>Vaswani</last></author>
      <author><first>David</first><last>Grangier</last></author>
      <doi>10.1162/tacl_a_00353</doi>
      <abstract>Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research : It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">complexity</a> of <a href="https://en.wikipedia.org/wiki/Attention">attention</a> to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a> on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits / dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1</abstract>
      <pages>53–68</pages>
      <url hash="b06612dd">2021.tacl-1.4</url>
      <bibkey>roy-etal-2021-efficient</bibkey>
    </paper>
    <paper id="5">
      <title>Deciphering Undersegmented Ancient Scripts Using Phonetic Prior</title>
      <author><first>Jiaming</first><last>Luo</last></author>
      <author><first>Frederik</first><last>Hartmann</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <author><first>Regina</first><last>Barzilay</last></author>
      <author><first>Yuan</first><last>Cao</last></author>
      <doi>10.1162/tacl_a_00354</doi>
      <abstract>Most undeciphered lost languages exhibit two characteristics that pose significant decipherment challenges : (1) the scripts are not fully segmented into words ; (2) the closest known language is not determined. We propose a decipherment model that handles both of these challenges by building on rich linguistic constraints reflecting consistent patterns in historical sound change. We capture the natural phonological geometry by learning <a href="https://en.wikipedia.org/wiki/Character_encoding">character embeddings</a> based on the <a href="https://en.wikipedia.org/wiki/International_Phonetic_Alphabet">International Phonetic Alphabet (IPA)</a>. The resulting <a href="https://en.wikipedia.org/wiki/Generative_model">generative framework</a> jointly models <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a> and cognate alignment, informed by <a href="https://en.wikipedia.org/wiki/Phonology">phonological constraints</a>. We evaluate the model on both deciphered languages (Gothic, Ugaritic) and an undeciphered one (Iberian). The experiments show that incorporating phonetic geometry leads to clear and consistent gains. Additionally, we propose a measure for language closeness which correctly identifies related languages for <a href="https://en.wikipedia.org/wiki/Gothic_language">Gothic</a> and <a href="https://en.wikipedia.org/wiki/Ugaritic">Ugaritic</a>. For <a href="https://en.wikipedia.org/wiki/Iberian_language">Iberian</a>, the method does not show strong evidence supporting <a href="https://en.wikipedia.org/wiki/Basque_language">Basque</a> as a related language, concurring with the favored position by the current scholarship.1</abstract>
      <pages>69–81</pages>
      <url hash="3c1203d1">2021.tacl-1.5</url>
      <bibkey>luo-etal-2021-deciphering</bibkey>
    </paper>
    <paper id="8">
      <title>Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement</title>
      <author><first>Alireza</first><last>Mohammadshahi</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <doi>10.1162/tacl_a_00358</doi>
      <abstract>We propose the Recursive Non-autoregressive Graph-to-Graph Transformer architecture (RNGTr) for the iterative refinement of arbitrary graphs through the recursive application of a non-autoregressive Graph-to-Graph Transformer and apply it to syntactic dependency parsing. We demonstrate the power and effectiveness of RNGTr on several dependency corpora, using a refinement model pre-trained with BERT. We also introduce Syntactic Transformer (SynTr), a non-recursive parser similar to our refinement model. RNGTr can improve the accuracy of a variety of initial parsers on 13 languages from the Universal Dependencies Treebanks, English and Chinese Penn Treebanks, and the German CoNLL2009 corpus, even improving over the new state-of-the-art results achieved by SynTr, significantly improving the state-of-the-art for all corpora tested.</abstract>
      <pages>120–138</pages>
      <url hash="d5fc315a">2021.tacl-1.8</url>
      <bibkey>mohammadshahi-henderson-2021-recursive</bibkey>
    </paper>
    </volume>
</collection>