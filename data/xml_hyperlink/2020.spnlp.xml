<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.spnlp">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Structured Prediction for NLP</booktitle>
      <editor><first>Priyanka</first><last>Agrawal</last></editor>
      <editor><first>Zornitsa</first><last>Kozareva</last></editor>
      <editor><first>Julia</first><last>Kreutzer</last></editor>
      <editor><first>Gerasimos</first><last>Lampouras</last></editor>
      <editor><first>André</first><last>Martins</last></editor>
      <editor><first>Sujith</first><last>Ravi</last></editor>
      <editor><first>Andreas</first><last>Vlachos</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="61fab8b3">2020.spnlp-1.0</url>
      <bibkey>spnlp-2020-structured</bibkey>
    </frontmatter>
    <paper id="2">
      <title>CopyNext : Explicit Span Copying and Alignment in Sequence to Sequence Models<fixed-case>C</fixed-case>opy<fixed-case>N</fixed-case>ext: Explicit Span Copying and Alignment in Sequence to Sequence Models</title>
      <author><first>Abhinav</first><last>Singh</last></author>
      <author><first>Patrick</first><last>Xia</last></author>
      <author><first>Guanghui</first><last>Qin</last></author>
      <author><first>Mahsa</first><last>Yarmohammadi</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>11–16</pages>
      <abstract>Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where each token was copied from. Further, they require contiguous token sequences from the input (spans) to be copied individually. We present a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> with an explicit token-level copy operation and extend it to copying entire spans. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a>. We demonstrate the approach on Nested Named Entity Recognition, achieving near state-of-the-art <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> with an order of magnitude increase in decoding speed.</abstract>
      <url hash="66c089be">2020.spnlp-1.2</url>
      <attachment type="OptionalSupplementaryMaterial" hash="6200419f">2020.spnlp-1.2.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.spnlp-1.2</doi>
      <video href="https://slideslive.com/38940142" />
      <bibkey>singh-etal-2020-copynext</bibkey>
    </paper>
    <paper id="5">
      <title>Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking</title>
      <author><first>Anh Duong</first><last>Trinh</last></author>
      <author><first>Robert J.</first><last>Ross</last></author>
      <author><first>John D.</first><last>Kelleher</last></author>
      <pages>33–42</pages>
      <abstract>Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains. We propose using energy-based structure prediction methods for large-scale dialogue state tracking task in two multiple domain dialogue datasets. Our results indicate that : (i) modelling variable dependencies yields better results ; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles. This leads to promising directions to improve state-of-the-art models by incorporating variable dependencies into their prediction process.</abstract>
      <url hash="a82a7410">2020.spnlp-1.5</url>
      <doi>10.18653/v1/2020.spnlp-1.5</doi>
      <video href="https://slideslive.com/38940154" />
      <bibkey>trinh-etal-2020-energy</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="7">
      <title>Layer-wise Guided Training for BERT : Learning Incrementally Refined Document Representations<fixed-case>BERT</fixed-case>: Learning Incrementally Refined Document Representations</title>
      <author><first>Nikolaos</first><last>Manginas</last></author>
      <author><first>Ilias</first><last>Chalkidis</last></author>
      <author><first>Prodromos</first><last>Malakasiotis</last></author>
      <pages>53–61</pages>
      <abstract>Although <a href="https://en.wikipedia.org/wiki/Brain-derived_neurotrophic_factor">BERT</a> is widely used by the <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP community</a>, little is known about its inner workings. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions. A much raised concern focuses on BERT’s over-parameterization and under-utilization issues. To this end, we propose o novel approach to fine-tune BERT in a structured manner. Specifically, we focus on Large Scale Multilabel Text Classification (LMTC) where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels. Our approach guides specific BERT layers to predict labels from specific hierarchy levels. Experimenting with two LMTC datasets we show that this structured fine-tuning approach not only yields better <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> results but also leads to better parameter utilization.</abstract>
      <url hash="e919bf83">2020.spnlp-1.7</url>
      <attachment type="OptionalSupplementaryMaterial" hash="6c5047b1">2020.spnlp-1.7.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.spnlp-1.7</doi>
      <video href="https://slideslive.com/38940156" />
      <bibkey>manginas-etal-2020-layer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/eurlex57k">EURLEX57K</pwcdataset>
    </paper>
    <paper id="10">
      <title>On the Discrepancy between <a href="https://en.wikipedia.org/wiki/Density_estimation">Density Estimation</a> and Sequence Generation</title>
      <author><first>Jason</first><last>Lee</last></author>
      <author><first>Dustin</first><last>Tran</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <pages>84–94</pages>
      <abstract>Many sequence-to-sequence generation tasks, including <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> and <a href="https://en.wikipedia.org/wiki/Speech_synthesis">text-to-speech</a>, can be posed as estimating the density of the output y given the input x : p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y * : R(y, y * | x). While we hope that a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that excels in <a href="https://en.wikipedia.org/wiki/Density_estimation">density estimation</a> also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. In this paper, by comparing several density estimators on five machine translation tasks, we find that the correlation between rankings of models based on <a href="https://en.wikipedia.org/wiki/Likelihood_function">log-likelihood</a> and BLEU varies significantly depending on the range of the model families being compared. First, <a href="https://en.wikipedia.org/wiki/Likelihood_function">log-likelihood</a> is highly correlated with <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> when we consider <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> within the same family (e.g. autoregressive models, or <a href="https://en.wikipedia.org/wiki/Latent_variable_model">latent variable models</a> with the same parameterization of the prior).</abstract>
      <url hash="c39e72c3">2020.spnlp-1.10</url>
      <attachment type="OptionalSupplementaryMaterial" hash="cddffedf">2020.spnlp-1.10.OptionalSupplementaryMaterial.tex</attachment>
      <doi>10.18653/v1/2020.spnlp-1.10</doi>
      <video href="https://slideslive.com/38940144" />
      <bibkey>lee-etal-2020-discrepancy</bibkey>
      <pwccode url="https://github.com/tensorflow/tensor2tensor" additional="false">tensorflow/tensor2tensor</pwccode>
    </paper>
    <paper id="12">
      <title>Deeply Embedded Knowledge Representation &amp; Reasoning For Natural Language Question Answering : A Practitioner’s Perspective</title>
      <author><first>Arindam</first><last>Mitra</last></author>
      <author><first>Sanjay</first><last>Narayana</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>102–111</pages>
      <abstract>Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser. Even though several projects have been launched in the pursuit of developing a universal meaning representation language, the existence of an accurate universal parser is far from reality. This has severely limited the application of knowledge representation and reasoning (KR) in the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> and also prevented a proper evaluation of KR based NLU systems. Our goal is to build KR based systems for <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">Natural Language Understanding</a> without relying on a <a href="https://en.wikipedia.org/wiki/Parsing">parser</a>. Towards this we propose a method named Deeply Embedded Knowledge Representation &amp; Reasoning (DeepEKR) where we replace the parser by a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a>, soften the symbolic representation so that a deterministic mapping exists between the parser neural network and the interpretable logical form, and finally replace the symbolic solver by an equivalent <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a>, so the model can be trained end-to-end. We evaluate our method with respect to the task of Qualitative Word Problem Solving on the two available datasets (QuaRTz and QuaRel). Our system achieves same <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> as that of the state-of-the-art <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on QuaRTz, outperforms the state-of-the-art on QuaRel and severely outperforms a traditional KR based system. The results show that the bias introduced by a KR solution does not prevent it from doing a better job at the end task. Moreover, our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is interpretable due to the bias introduced by the KR approach.</abstract>
      <url hash="cc7dc190">2020.spnlp-1.12</url>
      <doi>10.18653/v1/2020.spnlp-1.12</doi>
      <video href="https://slideslive.com/38940152" />
      <bibkey>mitra-etal-2020-deeply</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/quartz">QuaRTz</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quarel">QuaRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/babi-1">bAbI</pwcdataset>
    </paper>
  </volume>
</collection>