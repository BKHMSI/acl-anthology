<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.intexsempar">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the First Workshop on Interactive and Executable Semantic Parsing</booktitle>
      <editor><first>Ben</first><last>Bogin</last></editor>
      <editor><first>Srinivasan</first><last>Iyer</last></editor>
      <editor><first>Victoria</first><last>Lin</last></editor>
      <editor><first>Dragomir</first><last>Radev</last></editor>
      <editor><first>Alane</first><last>Suhr</last></editor>
      <editor><first /><last>Panupong</last></editor>
      <editor><first>Caiming</first><last>Xiong</last></editor>
      <editor><first>Pengcheng</first><last>Yin</last></editor>
      <editor><first>Tao</first><last>Yu</last></editor>
      <editor><first>Rui</first><last>Zhang</last></editor>
      <editor><first>Victor</first><last>Zhong</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="677e9d09">2020.intexsempar-1.0</url>
      <bibkey>intexsempar-2020-interactive</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Improving Sequence-to-Sequence Semantic Parser for Task Oriented Dialog</title>
      <author><first>Chaoting</first><last>Xuan</last></author>
      <pages>18–22</pages>
      <abstract>Task Oriented Parsing (TOP) attempts to map utterances to compositional requests, including multiple intents and their slots. Previous work focus on a tree-based hierarchical meaning representation, and applying constituency parsing techniques to address TOP. In this paper, we propose a new format of meaning representation that is more compact and amenable to sequence-to-sequence (seq-to-seq) models. A simple copy-augmented seq-to-seq parser is built and evaluated over a <a href="https://en.wikipedia.org/wiki/TOP500">public TOP dataset</a>, resulting in 3.44 % improvement over prior best seq-to-seq parser (exact match accuracy), which is also comparable to constituency parsers’ performance.</abstract>
      <url hash="190ad7b5">2020.intexsempar-1.3</url>
      <doi>10.18653/v1/2020.intexsempar-1.3</doi>
      <video href="https://slideslive.com/38939455" />
      <bibkey>xuan-2020-improving</bibkey>
      <pwccode url="https://github.com/cxuan2019/top" additional="false">cxuan2019/top</pwccode>
    </paper>
    <paper id="5">
      <title>ColloQL : Robust Text-to-SQL Over Search Queries<fixed-case>C</fixed-case>ollo<fixed-case>QL</fixed-case>: Robust Text-to-<fixed-case>SQL</fixed-case> Over Search Queries</title>
      <author><first>Karthik</first><last>Radhakrishnan</last></author>
      <author><first>Arvind</first><last>Srikantan</last></author>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <pages>34–45</pages>
      <abstract>Translating natural language utterances to executable queries is a helpful technique in making the vast amount of data stored in <a href="https://en.wikipedia.org/wiki/Relational_database">relational databases</a> accessible to a wider range of non-tech-savvy end users. Prior work in this area has largely focused on <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">textual input</a> that is linguistically correct and semantically unambiguous. However, real-world user queries are often succinct, colloquial, and noisy, resembling the input of a <a href="https://en.wikipedia.org/wiki/Web_search_engine">search engine</a>. In this work, we introduce data augmentation techniques and a sampling-based content-aware BERT model (ColloQL) to achieve robust text-to-SQL modeling over natural language search (NLS) questions. Due to the lack of evaluation data, we curate a new dataset of NLS questions and demonstrate the efficacy of our approach. ColloQL’s superior performance extends to well-formed text, achieving an 84.9 % (logical) and 90.7 % (execution) <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on the WikiSQL dataset, making it, to the best of our knowledge, the highest performing model that does not use execution guided decoding.</abstract>
      <url hash="a7835a2c">2020.intexsempar-1.5</url>
      <doi>10.18653/v1/2020.intexsempar-1.5</doi>
      <video href="https://slideslive.com/38939457" />
      <bibkey>radhakrishnan-etal-2020-colloql</bibkey>
      <pwccode url="https://github.com/karthikradhakrishnan96/ColloQL" additional="false">karthikradhakrishnan96/ColloQL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    </volume>
</collection>