<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.msr">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Multilingual Surface Realisation</booktitle>
      <editor><first>Anya</first><last>Belz</last></editor>
      <editor><first>Bernd</first><last>Bohnet</last></editor>
      <editor><first>Thiago Castro</first><last>Ferreira</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Simon</first><last>Mille</last></editor>
      <editor><first>Leo</first><last>Wanner</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="2a8fdccc">2020.msr-1.0</url>
      <bibkey>msr-2020-multilingual</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The Third Multilingual Surface Realisation Shared Task (SR’20): Overview and Evaluation Results<fixed-case>SR</fixed-case>’20): Overview and Evaluation Results</title>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Anya</first><last>Belz</last></author>
      <author><first>Bernd</first><last>Bohnet</last></author>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <pages>1–20</pages>
      <abstract>This paper presents results from the Third Shared Task on Multilingual Surface Realisation (SR’20) which was organised as part of the COLING’20 Workshop on Multilingual Surface Realisation. As in SR’18 and SR’19, the shared task comprised two tracks : (1) a Shallow Track where the inputs were full UD structures with word order information removed and tokens lemmatised ; and (2) a Deep Track where additionally, functional words and morphological information were removed. Moreover, each <a href="https://en.wikipedia.org/wiki/Track_(navigation)">track</a> had two subtracks : (a) restricted-resource, where only the data provided or approved as part of a track could be used for training models, and (b) open-resource, where any data could be used. The Shallow Track was offered in 11 languages, whereas the Deep Track in 3 ones. Systems were evaluated using both automatic metrics and direct assessment by human evaluators in terms of <a href="https://en.wikipedia.org/wiki/Readability">Readability</a> and Meaning Similarity to reference outputs. We present the evaluation results, along with descriptions of the SR’19 tracks, data and evaluation methods, as well as brief summaries of the participating systems. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume.</abstract>
      <url hash="dbf411db">2020.msr-1.1</url>
      <bibkey>mille-etal-2020-third</bibkey>
      <pwccode url="https://gitlab.com/talnupf/ud2deep" additional="false">talnupf/ud2deep</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="2">
      <title>BME-TUW at SR’20 : Lexical grammar induction for surface realization<fixed-case>BME</fixed-case>-<fixed-case>TUW</fixed-case> at <fixed-case>SR</fixed-case>’20: Lexical grammar induction for surface realization</title>
      <author><first>Gábor</first><last>Recski</last></author>
      <author><first>Ádám</first><last>Kovács</last></author>
      <author><first>Kinga</first><last>Gémes</last></author>
      <author><first>Judit</first><last>Ács</last></author>
      <author><first>Andras</first><last>Kornai</last></author>
      <pages>21–29</pages>
      <abstract>We present a system for mapping Universal Dependency structures to raw text which learns to restore word order by training an Interpreted Regular Tree Grammar (IRTG) that establishes a mapping between string and graph operations. The reinflection step is handled by a standard sequence-to-sequence architecture with a biLSTM encoder and an LSTM decoder with <a href="https://en.wikipedia.org/wiki/Attention">attention</a>. We modify our 2019 system (Kovcs et al., 2019) with a new grammar induction mechanism that allows IRTG rules to operate on <a href="https://en.wikipedia.org/wiki/Lemma_(morphology)">lemmata</a> in addition to part-of-speech tags and ensures that each word and its dependents are reordered using the most specific set of learned patterns. We also introduce a hierarchical approach to <a href="https://en.wikipedia.org/wiki/Word_order">word order restoration</a> that independently determines the <a href="https://en.wikipedia.org/wiki/Word_order">word order</a> of each clause in a sentence before arranging them with respect to the main clause, thereby improving overall readability and also making the IRTG parsing task tractable. We participated in the 2020 Surface Realization Shared task, subtrack T1a (shallow, closed). Human evaluation shows we achieve significant improvements on two of the three out-of-domain datasets compared to the 2019 system we modified. Both <a href="https://en.wikipedia.org/wiki/Component-based_software_engineering">components</a> of our <a href="https://en.wikipedia.org/wiki/System">system</a> are available on GitHub under an MIT license.</abstract>
      <url hash="96eab5d3">2020.msr-1.2</url>
      <bibkey>recski-etal-2020-bme</bibkey>
    </paper>
    <paper id="6">
      <title>NILC at SR’20 : Exploring Pre-Trained Models in Surface Realisation<fixed-case>NILC</fixed-case> at <fixed-case>SR</fixed-case>’20: Exploring Pre-Trained Models in Surface Realisation</title>
      <author><first>Marco Antonio</first><last>Sobrevilla Cabezudo</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>50–56</pages>
      <abstract>This paper describes the submission by the NILC Computational Linguistics research group of the University of S ao Paulo / Brazil to the English Track 2 (closed sub-track) at the Surface Realisation Shared Task 2020. The success of the current pre-trained models like <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> or GPT-2 in several tasks is well-known, however, this is not the case for data-to-text generation tasks and just recently some initiatives focused on it. This way, we explore how a pre-trained model (GPT-2) performs on the UD-to-text generation task. In general, the achieved results were poor, but there are some interesting ideas to explore. Among the learned lessons we may note that it is necessary to study strategies to represent UD inputs and to introduce structural knowledge into these pre-trained models.</abstract>
      <url hash="0f5f5699">2020.msr-1.6</url>
      <bibkey>sobrevilla-cabezudo-pardo-2020-nilc</bibkey>
    </paper>
    </volume>
</collection>