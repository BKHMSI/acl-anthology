<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.nlptea">
  <volume id="1" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
      <editor><first>Erhong</first><last>YANG</last></editor>
      <editor><first>Endong</first><last>XUN</last></editor>
      <editor><first>Baolin</first><last>ZHANG</last></editor>
      <editor><first>Gaoqi</first><last>RAO</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="b5ddfc36">2020.nlptea-1.0</url>
      <bibkey>nlp-tea-2020-natural</bibkey>
    </frontmatter>
    <paper id="7">
      <title>Integrating BERT and Score-based Feature Gates for Chinese Grammatical Error Diagnosis<fixed-case>BERT</fixed-case> and Score-based Feature Gates for <fixed-case>C</fixed-case>hinese Grammatical Error Diagnosis</title>
      <author><first>Yongchang</first><last>Cao</last></author>
      <author><first>Liang</first><last>He</last></author>
      <author><first>Robert</first><last>Ridley</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <pages>49–56</pages>
      <abstract>This paper describes our proposed <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> for the Chinese Grammatical Error Diagnosis (CGED) task in NLPTEA2020. The goal of CGED is to use <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing techniques</a> to automatically diagnose <a href="https://en.wikipedia.org/wiki/Chinese_grammar">Chinese grammatical errors</a> in sentences. To this end, we design and implement a CGED model named BERT with Score-feature Gates Error Diagnoser (BSGED), which is based on the BERT model, Bidirectional Long Short-Term Memory (BiLSTM) and conditional random field (CRF). In order to address the problem of losing partial-order relationships when embedding continuous feature items as with previous works, we propose a gating mechanism for integrating continuous feature items, which effectively retains the <a href="https://en.wikipedia.org/wiki/Partially_ordered_set">partial-order relationships</a> between feature items. We perform LSTM processing on the encoding result of the BERT model, and further extract the sequence features. In the final test-set evaluation, we obtained the highest F1 score at the detection level and are among the top 3 F1 scores at the identification level.</abstract>
      <url hash="7055b8ac">2020.nlptea-1.7</url>
      <bibkey>cao-etal-2020-integrating</bibkey>
    </paper>
    <paper id="12">
      <title>CYUT Team Chinese Grammatical Error Diagnosis System Report in NLPTEA-2020 CGED Shared Task<fixed-case>CYUT</fixed-case> Team <fixed-case>C</fixed-case>hinese Grammatical Error Diagnosis System Report in <fixed-case>NLPTEA</fixed-case>-2020 <fixed-case>CGED</fixed-case> Shared Task</title>
      <author><first>Shih-Hung</first><last>Wu</last></author>
      <author><first>Junwei</first><last>Wang</last></author>
      <pages>91–96</pages>
      <abstract>This paper reports our Chinese Grammatical Error Diagnosis system in the NLPTEA-2020 CGED shared task. In 2020, we sent two runs with two <a href="https://en.wikipedia.org/wiki/Glossary_of_baseball_(S)">approaches</a>. The first one is a combination of conditional random fields (CRF) and a BERT model deep-learning approach. The second one is a BERT model deep-learning approach. The official results shows that our run1 achieved the highest precision rate 0.9875 with the lowest <a href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">false positive rate</a> 0.0163 on detection, while run2 gives a more balanced performance.</abstract>
      <url hash="6664e87c">2020.nlptea-1.12</url>
      <bibkey>wu-wang-2020-cyut</bibkey>
    </paper>
    <paper id="14">
      <title>Chinese Grammatical Errors Diagnosis System Based on BERT at NLPTEA-2020 CGED Shared Task<fixed-case>C</fixed-case>hinese Grammatical Errors Diagnosis System Based on <fixed-case>BERT</fixed-case> at <fixed-case>NLPTEA</fixed-case>-2020 <fixed-case>CGED</fixed-case> Shared Task</title>
      <author><first>Hongying</first><last>Zan</last></author>
      <author><first>Yangchao</first><last>Han</last></author>
      <author><first>Haotian</first><last>Huang</last></author>
      <author><first>Yingjie</first><last>Yan</last></author>
      <author><first>Yuke</first><last>Wang</last></author>
      <author><first>Yingjie</first><last>Han</last></author>
      <pages>102–107</pages>
      <abstract>In the process of learning <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, <a href="https://en.wikipedia.org/wiki/Second-language_acquisition">second language learners</a> may have various <a href="https://en.wikipedia.org/wiki/Grammatical_error">grammatical errors</a> due to the negative transfer of native language. This paper describes our submission to the NLPTEA 2020 shared task on CGED. We present a <a href="https://en.wikipedia.org/wiki/Hybrid_system">hybrid system</a> that utilizes both detection and correction stages. The detection stage is a sequential labelling model based on BiLSTM-CRF and BERT contextual word representation. The correction stage is a hybrid model based on the <a href="https://en.wikipedia.org/wiki/N-gram">n-gram</a> and Seq2Seq. Without adding additional features and external data, the BERT contextual word representation can effectively improve the performance metrics of Chinese grammatical error detection and correction.</abstract>
      <url hash="c2ce541f">2020.nlptea-1.14</url>
      <bibkey>zan-etal-2020-chinese</bibkey>
    </paper>
    <paper id="17">
      <title>SEMA : Text Simplification Evaluation through Semantic Alignment<fixed-case>SEMA</fixed-case>: Text Simplification Evaluation through Semantic Alignment</title>
      <author><first>Xuan</first><last>Zhang</last></author>
      <author><first>Huizhou</first><last>Zhao</last></author>
      <author><first>KeXin</first><last>Zhang</last></author>
      <author><first>Yiyang</first><last>Zhang</last></author>
      <pages>121–128</pages>
      <abstract>Text simplification is an important branch of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. At present, methods used to evaluate the semantic retention of <a href="https://en.wikipedia.org/wiki/Text_simplification">text simplification</a> are mostly based on <a href="https://en.wikipedia.org/wiki/String_matching">string matching</a>. We propose the SEMA (text Simplification Evaluation Measure through Semantic Alignment), which is based on semantic alignment. Semantic alignments include complete alignment, partial alignment and <a href="https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy">hyponymy alignment</a>. Our experiments show that the evaluation results of SEMA have a high consistency with human evaluation for the simplified corpus of Chinese and English news texts.</abstract>
      <url hash="8ab0302a">2020.nlptea-1.17</url>
      <bibkey>zhang-etal-2020-sema</bibkey>
    </paper>
    </volume>
</collection>