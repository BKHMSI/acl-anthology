<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.loresmt">
  <volume id="1" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages</booktitle>
      <editor><first>Alina</first><last>Karakanta</last></editor>
      <editor><first>Atul Kr.</first><last>Ojha</last></editor>
      <editor><first>Chao-Hong</first><last>Liu</last></editor>
      <editor><first>Jade</first><last>Abbott</last></editor>
      <editor><first>John</first><last>Ortega</last></editor>
      <editor><first>Jonathan</first><last>Washington</last></editor>
      <editor><first>Nathaniel</first><last>Oco</last></editor>
      <editor><first>Surafel Melaku</first><last>Lakew</last></editor>
      <editor><first>Tommi A</first><last>Pirinen</last></editor>
      <editor><first>Valentin</first><last>Malykh</last></editor>
      <editor><first>Varvara</first><last>Logacheva</last></editor>
      <editor><first>Xiaobing</first><last>Zhao</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="c7b02162">2020.loresmt-1.0</url>
      <bibkey>loresmt-2020-technologies</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Bridging Philippine Languages With Multilingual Neural Machine Translation<fixed-case>P</fixed-case>hilippine Languages With Multilingual Neural Machine Translation</title>
      <author><first>Renz Iver</first><last>Baliber</last></author>
      <author><first>Charibeth</first><last>Cheng</last></author>
      <author><first>Kristine Mae</first><last>Adlaon</last></author>
      <author><first>Virgion</first><last>Mamonong</last></author>
      <pages>14–22</pages>
      <abstract>The Philippines is home to more than 150 languages that is considered to be low-resourced even on its major languages. This results into a lack of pursuit in developing a <a href="https://en.wikipedia.org/wiki/Translation">translation system</a> for the underrepresented languages. To simplify the process of developing translation system for multiple languages, and to aid in improving the translation quality of zero to low-resource languages, multilingual NMT became an active area of research. However, existing works in multilingual NMT disregards the analysis of a multilingual model on a closely related and low-resource language group in the context of pivot-based translation and zero-shot translation. In this paper, we benchmarked <a href="https://en.wikipedia.org/wiki/Translation">translation</a> for several <a href="https://en.wikipedia.org/wiki/Languages_of_the_Philippines">Philippine Languages</a>, provided an analysis of a multilingual NMT system for morphologically rich and low-resource languages in terms of its effectiveness in translating zero-resource languages with zero-shot translations. To further evaluate the capability of the multilingual NMT model in translating unseen language pairs in training, we tested the model to translate between <a href="https://en.wikipedia.org/wiki/Tagalog_language">Tagalog</a> and <a href="https://en.wikipedia.org/wiki/Cebuano_language">Cebuano</a> and compared its performance with a simple NMT model that is directly trained on a parallel Tagalog and Cebuano data in which we showed that zero-shot translation outperforms a directly trained model in some instances, while utilizing English as a pivot language in translating outperform both approaches.</abstract>
      <url hash="947402a7">2020.loresmt-1.2</url>
      <bibkey>baliber-etal-2020-bridging</bibkey>
    </paper>
    <paper id="4">
      <title>Findings of the LoResMT 2020 Shared Task on Zero-Shot for Low-Resource languages<fixed-case>L</fixed-case>o<fixed-case>R</fixed-case>es<fixed-case>MT</fixed-case> 2020 Shared Task on Zero-Shot for Low-Resource languages</title>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <author><first>Valentin</first><last>Malykh</last></author>
      <author><first>Alina</first><last>Karakanta</last></author>
      <author><first>Chao-Hong</first><last>Liu</last></author>
      <pages>33–37</pages>
      <abstract>This paper presents the findings of the LoResMT 2020 Shared Task on zero-shot translation for low resource languages. This task was organised as part of the 3rd Workshop on Technologies for MT of Low Resource Languages (LoResMT) at AACL-IJCNLP 2020. The focus was on the zero-shot approach as a notable development in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a> to build MT systems for language pairs where parallel corpora are small or even non-existent. The shared task experience suggests that back-translation and domain adaptation methods result in better <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> for small-size datasets. We further noted that, although <a href="https://en.wikipedia.org/wiki/Translation">translation</a> between similar languages is no cakewalk, linguistically distinct languages require more data to give better results.</abstract>
      <url hash="4c23d3af">2020.loresmt-1.4</url>
      <bibkey>ojha-etal-2020-findings</bibkey>
    </paper>
    <paper id="5">
      <title>Zero-Shot Neural Machine Translation : Russian-Hindi @LoResMT 2020<fixed-case>R</fixed-case>ussian-<fixed-case>H</fixed-case>indi @<fixed-case>L</fixed-case>o<fixed-case>R</fixed-case>es<fixed-case>MT</fixed-case> 2020</title>
      <author><first>Sahinur Rahman</first><last>Laskar</last></author>
      <author><first>Abdullah Faiz Ur Rahman</first><last>Khilji</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>38–42</pages>
      <abstract>Neural machine translation (NMT) is a widely accepted approach in the machine translation (MT) community, translating from one natural language to another natural language. Although, NMT shows remarkable performance in both high and low resource languages, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> needs sufficient training corpus. The availability of a <a href="https://en.wikipedia.org/wiki/Parallel_corpus">parallel corpus</a> in low resource language pairs is one of the challenging tasks in MT. To mitigate this issue, NMT attempts to utilize a monolingual corpus to get better at <a href="https://en.wikipedia.org/wiki/Translation">translation</a> for low resource language pairs. Workshop on Technologies for MT of Low Resource Languages (LoResMT 2020) organized shared tasks of low resource language pair translation using zero-shot NMT. Here, the <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpus</a> is not used and only monolingual corpora is allowed. We have participated in the same shared task with our team name CNLP-NITS for the Russian-Hindi language pair. We have used masked sequence to sequence pre-training for language generation (MASS) with only monolingual corpus following the unsupervised NMT architecture. The evaluated results are declared at the LoResMT 2020 shared task, which reports that our system achieves the bilingual evaluation understudy (BLEU) score of 0.59, precision score of 3.43, recall score of 5.48, F-measure score of 4.22, and rank-based intuitive bilingual evaluation score (RIBES) of 0.180147 in Russian to Hindi translation. And for Hindi to Russian translation, we have achieved <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>, <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a>, <a href="https://en.wikipedia.org/wiki/Precision_and_recall">recall</a>, <a href="https://en.wikipedia.org/wiki/F-measure">F-measure</a>, and <a href="https://en.wikipedia.org/wiki/International_Bureau_of_Weights_and_Measures">RIBES score</a> of 1.11, 4.72, 4.41, 4.56, and 0.026842 respectively.</abstract>
      <url hash="b77fd717">2020.loresmt-1.5</url>
      <bibkey>laskar-etal-2020-zero</bibkey>
    </paper>
    <paper id="6">
      <title>Unsupervised Approach for Zero-Shot Experiments : BhojpuriHindi and MagahiHindi@LoResMT 2020<fixed-case>B</fixed-case>hojpuri–<fixed-case>H</fixed-case>indi and <fixed-case>M</fixed-case>agahi–<fixed-case>H</fixed-case>indi@<fixed-case>L</fixed-case>o<fixed-case>R</fixed-case>es<fixed-case>MT</fixed-case> 2020</title>
      <author><first>Amit</first><last>Kumar</last></author>
      <author><first>Rajesh Kumar</first><last>Mundotiya</last></author>
      <author><first>Anil Kumar</first><last>Singh</last></author>
      <pages>43–46</pages>
      <abstract>This paper reports a Machine Translation (MT) system submitted by the NLPRL team for the BhojpuriHindi and MagahiHindi language pairs at LoResMT 2020 shared task. We used an unsupervised domain adaptation approach that gives promising results for zero or extremely low resource languages. Task organizers provide the development and the test sets for evaluation and the monolingual data for training. Our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> is a hybrid approach of <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> and <a href="https://en.wikipedia.org/wiki/Translation_(biology)">back-translation</a>. Metrics used to evaluate the trained model are BLEU, RIBES, <a href="https://en.wikipedia.org/wiki/Precision_(statistics)">Precision</a>, Recall and <a href="https://en.wikipedia.org/wiki/F-measure">F-measure</a>. Our approach gives relatively promising results, with a wide range, of 19.5, 13.71, 2.54, and 3.16 BLEU points for <a href="https://en.wikipedia.org/wiki/Bhojpuri_language">Bhojpuri</a> to <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>, Magahi to Hindi, <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a> to Bhojpuri and <a href="https://en.wikipedia.org/wiki/Hindi">Hindi to Magahi language pairs</a>, respectively.</abstract>
      <url hash="63ed8bbc">2020.loresmt-1.6</url>
      <bibkey>kumar-etal-2020-unsupervised</bibkey>
    </paper>
    <paper id="12">
      <title>Towards Machine Translation for the <a href="https://en.wikipedia.org/wiki/Kurdish_languages">Kurdish Language</a><fixed-case>K</fixed-case>urdish Language</title>
      <author><first>Sina</first><last>Ahmadi</last></author>
      <author><first>Maraim</first><last>Masoud</last></author>
      <pages>87–98</pages>
      <abstract>Machine translation is the task of translating texts from one language to another using <a href="https://en.wikipedia.org/wiki/Computer">computers</a>. It has been one of the major tasks in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> and <a href="https://en.wikipedia.org/wiki/Computational_linguistics">computational linguistics</a> and has been motivating to facilitate <a href="https://en.wikipedia.org/wiki/Human_communication">human communication</a>. Kurdish, an <a href="https://en.wikipedia.org/wiki/Indo-European_languages">Indo-European language</a>, has received little attention in this realm due to the language being less-resourced. Therefore, in this paper, we are addressing the main issues in creating a <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation system</a> for the <a href="https://en.wikipedia.org/wiki/Kurdish_languages">Kurdish language</a>, with a focus on the <a href="https://en.wikipedia.org/wiki/Sorani">Sorani dialect</a>. We describe the available scarce <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel data</a> suitable for training a neural machine translation model for Sorani Kurdish-English translation. We also discuss some of the major challenges in Kurdish language translation and demonstrate how fundamental text processing tasks, such as <a href="https://en.wikipedia.org/wiki/Lexical_analysis">tokenization</a>, can improve <a href="https://en.wikipedia.org/wiki/Translation">translation</a> performance.</abstract>
      <url hash="a37f9948">2020.loresmt-1.12</url>
      <bibkey>ahmadi-masoud-2020-towards</bibkey>
      <pwccode url="https://github.com/sinaahmadi/KurdishMT" additional="false">sinaahmadi/KurdishMT</pwccode>
    </paper>
    <paper id="15">
      <title>Investigating Low-resource Machine Translation for English-to-Tamil<fixed-case>E</fixed-case>nglish-to-<fixed-case>T</fixed-case>amil</title>
      <author><first>Akshai</first><last>Ramesh</last></author>
      <author><first>Venkatesh</first><last>Balavadhani parthasa</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>118–125</pages>
      <abstract>Statistical machine translation (SMT) which was the dominant paradigm in machine translation (MT) research for nearly three decades has recently been superseded by the end-to-end deep learning approaches to MT. Although <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural models</a> produce state-of-the-art results in many translation tasks, they are found to under-perform on resource-poor scenarios. Despite some success, none of the present-day benchmarks that have tried to overcome this problem can be regarded as a universal solution to the problem of <a href="https://en.wikipedia.org/wiki/Translation">translation</a> of many low-resource languages. In this work, we investigate the performance of phrase-based SMT (PB-SMT) and neural MT (NMT) on a rarely-tested low-resource language-pair, English-to-Tamil, taking a specialised data domain (software localisation) into consideration. In particular, we produce rankings of our MT systems via a social media platform-based human evaluation scheme, and demonstrate our findings in the low-resource domain-specific text translation task.</abstract>
      <url hash="89533ef3">2020.loresmt-1.15</url>
      <bibkey>ramesh-etal-2020-investigating</bibkey>
    </paper>
  </volume>
</collection>