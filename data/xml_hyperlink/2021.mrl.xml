<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.mrl">
  <volume id="1" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Multilingual Representation Learning</booktitle>
      <editor><first>Duygu</first><last>Ataman</last></editor>
      <editor><first>Alexandra</first><last>Birch</last></editor>
      <editor><first>Alexis</first><last>Conneau</last></editor>
      <editor><first>Orhan</first><last>Firat</last></editor>
      <editor><first>Sebastian</first><last>Ruder</last></editor>
      <editor><first>Gozde Gul</first><last>Sahin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="2895c85e">2021.mrl-1.0</url>
      <bibkey>mrl-2021-multilingual</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Learning Contextualised Cross-lingual Word Embeddings and Alignments for Extremely Low-Resource Languages Using Parallel Corpora</title>
      <author><first>Takashi</first><last>Wada</last></author>
      <author><first>Tomoharu</first><last>Iwata</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <pages>16–31</pages>
      <abstract>We propose a new approach for learning contextualised cross-lingual word embeddings based on a small parallel corpus (e.g. a few hundred sentence pairs). Our method obtains <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> via an LSTM encoder-decoder model that simultaneously translates and reconstructs an input sentence. Through sharing <a href="https://en.wikipedia.org/wiki/Mathematical_model">model parameters</a> among different languages, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> jointly trains the <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> in a common cross-lingual space. We also propose to combine word and subword embeddings to make use of orthographic similarities across different languages. We base our experiments on real-world data from endangered languages, namely <a href="https://en.wikipedia.org/wiki/Yongning_Na_language">Yongning Na</a>, Shipibo-Konibo, and <a href="https://en.wikipedia.org/wiki/Griko_language">Griko</a>. Our experiments on bilingual lexicon induction and word alignment tasks show that our model outperforms existing methods by a large margin for most language pairs. These results demonstrate that, contrary to common belief, an encoder-decoder translation model is beneficial for learning cross-lingual representations even in extremely low-resource conditions. Furthermore, our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.</abstract>
      <url hash="7c99ca7d">2021.mrl-1.2</url>
      <bibkey>wada-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.mrl-1.2</doi>
      <pwccode url="https://github.com/twadada/multilingual-nlm" additional="false">twadada/multilingual-nlm</pwccode>
    </paper>
    <paper id="4">
      <title>Do not neglect related languages : The case of low-resource Occitan cross-lingual word embeddings<fixed-case>O</fixed-case>ccitan cross-lingual word embeddings</title>
      <author><first>Lisa</first><last>Woller</last></author>
      <author><first>Viktor</first><last>Hangya</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>41–50</pages>
      <abstract>Cross-lingual word embeddings (CLWEs) have proven indispensable for various <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing tasks</a>, e.g., bilingual lexicon induction (BLI). However, the lack of data often impairs the quality of <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a>. Various approaches requiring only weak cross-lingual supervision were proposed, but current methods still fail to learn good CLWEs for languages with only a small monolingual corpus. We therefore claim that it is necessary to explore further <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> to improve CLWEs in low-resource setups. In this paper we propose to incorporate data of related high-resource languages. In contrast to previous approaches which leverage independently pre-trained embeddings of languages, we (i) train CLWEs for the low-resource and a related language jointly and (ii) map them to the target language to build the final multilingual space. In our experiments we focus on <a href="https://en.wikipedia.org/wiki/Occitan_language">Occitan</a>, a low-resource Romance language which is often neglected due to lack of resources. We leverage data from <a href="https://en.wikipedia.org/wiki/French_language">French</a>, <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a> and <a href="https://en.wikipedia.org/wiki/Catalan_language">Catalan</a> for training and evaluate on the Occitan-English BLI task. By incorporating supporting languages our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> outperforms previous approaches by a large margin. Furthermore, our analysis shows that the degree of relatedness between an incorporated language and the low-resource language is critically important.</abstract>
      <url hash="00966483">2021.mrl-1.4</url>
      <bibkey>woller-etal-2021-neglect</bibkey>
      <doi>10.18653/v1/2021.mrl-1.4</doi>
    </paper>
    <paper id="7">
      <title>Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training</title>
      <author><first>Vikram</first><last>Gupta</last></author>
      <pages>74–85</pages>
      <abstract>Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of <a href="https://en.wikipedia.org/wiki/Value-added_tax">VAT</a> for multilingual and multilabel emotion recognition has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leveraging unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on SemEval2018 multilabel and multilingual emotion recognition dataset and show performance gains of 6.2 % (Arabic), 3.8 % (Spanish) and 1.8 % (English) over <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> with same amount of labelled data (10 % of training data). We also improve the existing <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> by 7 %, 4.5 % and 1 % (Jaccard Index) for <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>, <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> and <a href="https://en.wikipedia.org/wiki/English_language">English</a> respectively and perform probing experiments for understanding the impact of different layers of the contextual models.</abstract>
      <url hash="d2b14682">2021.mrl-1.7</url>
      <bibkey>gupta-2021-multilingual</bibkey>
      <doi>10.18653/v1/2021.mrl-1.7</doi>
    </paper>
    <paper id="8">
      <title>Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer Performance</title>
      <author><first>Karthikeyan</first><last>K</last></author>
      <author><first>Aalok</first><last>Sathe</last></author>
      <author><first>Somak</first><last>Aditya</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>86–95</pages>
      <abstract>Multilingual language models achieve impressive zero-shot accuracies in many languages in complex tasks such as Natural Language Inference (NLI). Examples in NLI (and equivalent complex tasks) often pertain to various types of sub-tasks, requiring different kinds of <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a>. Certain types of <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a> have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot transfer efficiency and few-shot sample selection. Hence, to investigate the effects of types of reasoning on transfer performance, we propose a category-annotated multilingual NLI dataset and discuss the challenges to scale monolingual annotations to multiple languages. We statistically observe interesting effects that the confluence of reasoning types and language similarities have on transfer performance.</abstract>
      <url hash="cbe4db68">2021.mrl-1.8</url>
      <bibkey>k-etal-2021-analyzing</bibkey>
      <doi>10.18653/v1/2021.mrl-1.8</doi>
      <pwccode url="https://github.com/microsoft/taxixnli" additional="false">microsoft/taxixnli</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/taxinli">TaxiNLI</pwcdataset>
    </paper>
    <paper id="17">
      <title>Shaking Syntactic Trees on the Sesame Street : Multilingual Probing with Controllable Perturbations</title>
      <author><first>Ekaterina</first><last>Taktasheva</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <pages>191–210</pages>
      <abstract>Recent research has adopted a new experimental field centered around the concept of text perturbations which has revealed that shuffled word order has little to no impact on the downstream performance of Transformer-based language models across many NLP tasks. These findings contradict the common understanding of how the models encode hierarchical and structural information and even question if the <a href="https://en.wikipedia.org/wiki/Word_order">word order</a> is modeled with position embeddings. To this end, this paper proposes nine probing datasets organized by the type of controllable text perturbation for three <a href="https://en.wikipedia.org/wiki/Indo-European_languages">Indo-European languages</a> with a varying degree of word order flexibility : <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a> and <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a>. Based on the probing analysis of the M-BERT and M-BART models, we report that the syntactic sensitivity depends on the language and model pre-training objectives. We also find that the <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">sensitivity</a> grows across layers together with the increase of the perturbation granularity. Last but not least, we show that the models barely use the positional information to induce syntactic trees from their intermediate self-attention and contextualized representations.</abstract>
      <url hash="d6f27a87">2021.mrl-1.17</url>
      <bibkey>taktasheva-etal-2021-shaking</bibkey>
      <doi>10.18653/v1/2021.mrl-1.17</doi>
      <pwccode url="https://github.com/evtaktasheva/dependency_extraction" additional="false">evtaktasheva/dependency_extraction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blimp">BLiMP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    </volume>
</collection>