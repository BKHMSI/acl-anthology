<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.semeval">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</booktitle>
      <editor><first>Alexis</first><last>Palmer</last></editor>
      <editor><first>Nathan</first><last>Schneider</last></editor>
      <editor><first>Natalie</first><last>Schluter</last></editor>
      <editor><first>Guy</first><last>Emerson</last></editor>
      <editor><first>Aurelie</first><last>Herbelot</last></editor>
      <editor><first>Xiaodan</first><last>Zhu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="01d5d604">2021.semeval-1</url>
    </meta>
    <frontmatter>
      <url hash="404ba67b">2021.semeval-1.0</url>
      <bibkey>semeval-2021-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>SemEval-2021 Task 1 : Lexical Complexity Prediction<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 1: Lexical Complexity Prediction</title>
      <author><first>Matthew</first><last>Shardlow</last></author>
      <author><first>Richard</first><last>Evans</last></author>
      <author><first>Gustavo Henrique</first><last>Paetzold</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>1–16</pages>
      <abstract>This paper presents the results and main findings of SemEval-2021 Task 1-Lexical Complexity Prediction. We provided participants with an augmented version of the CompLex Corpus (Shardlow et al. CompLex is an English multi-domain corpus in which words and multi-word expressions (MWEs) were annotated with respect to their complexity using a five point Likert scale. SemEval-2021 Task 1 featured two Sub-tasks : Sub-task 1 focused on single words and Sub-task 2 focused on MWEs. The competition attracted 198 teams in total, of which 54 teams submitted official runs on the test data to <a href="https://en.wikipedia.org/wiki/Task_(computing)">Sub-task 1</a> and 37 to <a href="https://en.wikipedia.org/wiki/Task_(computing)">Sub-task 2</a>.</abstract>
      <url hash="026e0c42">2021.semeval-1.1</url>
      <doi>10.18653/v1/2021.semeval-1.1</doi>
      <bibkey>shardlow-etal-2021-semeval</bibkey>
    </paper>
    <paper id="7">
      <title>SemEval-2021 Task 6 : Detection of Persuasion Techniques in Texts and Images<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 6: Detection of Persuasion Techniques in Texts and Images</title>
      <author><first>Dimitar</first><last>Dimitrov</last></author>
      <author><first>Bishr</first><last>Bin Ali</last></author>
      <author><first>Shaden</first><last>Shaar</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <author><first>Fabrizio</first><last>Silvestri</last></author>
      <author><first>Hamed</first><last>Firooz</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Giovanni</first><last>Da San Martino</last></author>
      <pages>70–98</pages>
      <abstract>We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in Texts and Images : the data, the annotation guidelines, the evaluation setup, the results, and the participating systems. The task focused on <a href="https://en.wikipedia.org/wiki/Meme">memes</a> and had three subtasks : (i) detecting the techniques in the <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">text</a>, (ii) detecting the text spans where the techniques are used, and (iii) detecting techniques in the entire meme, i.e., both in the text and in the image. It was a popular <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, attracting 71 registrations, and 22 teams that eventually made an official submission on the test set. The evaluation results for the third subtask confirmed the importance of both <a href="https://en.wikipedia.org/wiki/Modality_(semiotics)">modalities</a>, the <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">text</a> and the image. Moreover, some teams reported benefits when not just combining the two modalities, e.g., by using early or late fusion, but rather modeling the interaction between them in a joint model.</abstract>
      <url hash="7fb48942">2021.semeval-1.7</url>
      <doi>10.18653/v1/2021.semeval-1.7</doi>
      <bibkey>dimitrov-etal-2021-semeval</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hateful-memes">Hateful Memes</pwcdataset>
    </paper>
    <paper id="8">
      <title>Alpha at SemEval-2021 Task 6 : Transformer Based Propaganda Classification<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 6: Transformer Based Propaganda Classification</title>
      <author><first>Zhida</first><last>Feng</last></author>
      <author><first>Jiji</first><last>Tang</last></author>
      <author><first>Jiaxiang</first><last>Liu</last></author>
      <author><first>Weichong</first><last>Yin</last></author>
      <author><first>Shikun</first><last>Feng</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Li</first><last>Chen</last></author>
      <pages>99–104</pages>
      <abstract>This paper describes our system participated in Task 6 of SemEval-2021 : the task focuses on multimodal propaganda technique classification and it aims to classify given image and text into 22 classes. In this paper, we propose to use transformer based architecture to fuse the clues from both image and text. We explore two branches of techniques including fine-tuning the text pretrained transformer with extended visual features, and fine-tuning the multimodal pretrained transformers. For the visual features, we have tested both grid features based on <a href="https://en.wikipedia.org/wiki/ResNet">ResNet</a> and salient region features from pretrained object detector. Among the pretrained multimodal transformers, we choose ERNIE-ViL, a two-steam cross-attended transformers pretrained on large scale image-caption aligned data. Fine-tuing ERNIE-ViL for our task produce a better performance due to general joint multimodal representation for <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">text</a> and image learned by ERNIE-ViL. Besides, as the distribution of the classification labels is very unbalanced, we also make a further attempt on the <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> and the experiment result shows that focal loss would perform better than cross entropy loss. Last we have won first for subtask C in the final competition.</abstract>
      <url hash="a6f72adf">2021.semeval-1.8</url>
      <doi>10.18653/v1/2021.semeval-1.8</doi>
      <bibkey>feng-etal-2021-alpha</bibkey>
    </paper>
    <paper id="9">
      <title>SemEval 2021 Task 7 : HaHackathon, Detecting and Rating Humor and Offense<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val 2021 Task 7: <fixed-case>H</fixed-case>a<fixed-case>H</fixed-case>ackathon, Detecting and Rating Humor and Offense</title>
      <author><first>J. A.</first><last>Meaney</last></author>
      <author><first>Steven</first><last>Wilson</last></author>
      <author><first>Luis</first><last>Chiruzzo</last></author>
      <author><first>Adam</first><last>Lopez</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>105–119</pages>
      <abstract>SemEval 2021 Task 7, HaHackathon, was the first shared task to combine the previously separate domains of humor detection and offense detection. We collected 10,000 texts from <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> and the Kaggle Short Jokes dataset, and had each annotated for humor and offense by 20 annotators aged 18-70. Our subtasks were binary humor detection, prediction of humor and offense ratings, and a novel controversy task : to predict if the variance in the humor ratings was higher than a specific threshold. The subtasks attracted 36-58 submissions, with most of the participants choosing to use pre-trained language models. Many of the highest performing teams also implemented additional optimization techniques, including task-adaptive training and adversarial training. The results suggest that the participating <a href="https://en.wikipedia.org/wiki/System">systems</a> are well suited to humor detection, but that humor controversy is a more challenging task. We discuss which <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> excel in this task, which auxiliary techniques boost their performance, and analyze the errors which were not captured by the best <a href="https://en.wikipedia.org/wiki/System">systems</a>.</abstract>
      <url hash="4c2e27ab">2021.semeval-1.9</url>
      <doi>10.18653/v1/2021.semeval-1.9</doi>
      <bibkey>meaney-etal-2021-semeval</bibkey>
    </paper>
    <paper id="11">
      <title>Complex words identification using word-level features for SemEval-2020 Task 1<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 1</title>
      <author><first>Jenny A.</first><last>Ortiz-Zambrano</last></author>
      <author><first>Arturo</first><last>Montejo-Ráez</last></author>
      <pages>126–129</pages>
      <abstract>This article describes a system to predict the complexity of words for the Lexical Complexity Prediction (LCP) shared task hosted at SemEval 2021 (Task 1) with a new annotated English dataset with a <a href="https://en.wikipedia.org/wiki/Likert_scale">Likert scale</a>. Located in the Lexical Semantics track, the task consisted of predicting the complexity value of the words in context. A machine learning approach was carried out based on the frequency of the words and several characteristics added at word level. Over these <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>, a supervised random forest regression algorithm was trained. Several runs were performed with different values to observe the performance of the <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a>. For the <a href="https://en.wikipedia.org/wiki/Evaluation">evaluation</a>, our best results reported a M.A.E score of 0.07347, M.S.E. of 0.00938, and R.M.S.E. of 0.096871. Our experiments showed that, with a greater number of <a href="https://en.wikipedia.org/wiki/Phenotypic_trait">characteristics</a>, the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a> of the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> increases.</abstract>
      <url hash="d0b886d4">2021.semeval-1.11</url>
      <doi>10.18653/v1/2021.semeval-1.11</doi>
      <bibkey>ortiz-zambrano-montejo-raez-2021-complex</bibkey>
    </paper>
    <paper id="15">
      <title>Uppsala NLP at SemEval-2021 Task 2 : Multilingual Language Models for Fine-tuning and Feature Extraction in Word-in-Context Disambiguation<fixed-case>U</fixed-case>ppsala <fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 2: Multilingual Language Models for Fine-tuning and Feature Extraction in Word-in-Context Disambiguation</title>
      <author><first>Huiling</first><last>You</last></author>
      <author><first>Xingran</first><last>Zhu</last></author>
      <author><first>Sara</first><last>Stymne</last></author>
      <pages>150–156</pages>
      <abstract>We describe the Uppsala NLP submission to SemEval-2021 Task 2 on multilingual and cross-lingual word-in-context disambiguation. We explore the usefulness of three pre-trained multilingual language models, XLM-RoBERTa (XLMR), Multilingual BERT (mBERT) and multilingual distilled BERT (mDistilBERT). We compare these three <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> in two setups, <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> and as <a href="https://en.wikipedia.org/wiki/Software_feature">feature extractors</a>. In the second case we also experiment with using dependency-based information. We find that <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> is better than <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a>. XLMR performs better than mBERT in the cross-lingual setting both with <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> and <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a>, whereas these two models give a similar performance in the multilingual setting. mDistilBERT performs poorly with <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> but gives similar results to the other <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> when used as a feature extractor. We submitted our two best <a href="https://en.wikipedia.org/wiki/System">systems</a>, fine-tuned with XLMR and mBERT.</abstract>
      <url hash="b663aee7">2021.semeval-1.15</url>
      <doi>10.18653/v1/2021.semeval-1.15</doi>
      <bibkey>you-etal-2021-uppsala</bibkey>
    </paper>
    <paper id="16">
      <title>SkoltechNLP at SemEval-2021 Task 2 : Generating Cross-Lingual Training Data for the Word-in-Context Task<fixed-case>S</fixed-case>koltech<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 2: Generating Cross-Lingual Training Data for the Word-in-Context Task</title>
      <author><first>Anton</first><last>Razzhigaev</last></author>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>157–162</pages>
      <abstract>In this paper, we present a <a href="https://en.wikipedia.org/wiki/System">system</a> for the solution of the cross-lingual and multilingual word-in-context disambiguation task. Task organizers provided monolingual data in several languages, but no cross-lingual training data were available. To address the lack of the officially provided cross-lingual training data, we decided to generate such <a href="https://en.wikipedia.org/wiki/Data">data</a> ourselves. We describe a simple yet effective approach based on <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> and back translation of the lexical units to the original language used in the context of this shared task. In our experiments, we used a neural system based on the XLM-R, a pre-trained transformer-based masked language model, as a baseline. We show the effectiveness of the proposed approach as it allows to substantially improve the performance of this strong neural baseline model. In addition, in this study, we present multiple types of the XLM-R based classifier, experimenting with various ways of mixing information from the first and second occurrences of the target word in two samples.</abstract>
      <url hash="3345619c">2021.semeval-1.16</url>
      <doi>10.18653/v1/2021.semeval-1.16</doi>
      <bibkey>razzhigaev-etal-2021-skoltechnlp</bibkey>
    </paper>
    <paper id="17">
      <title>Zhestyatsky at SemEval-2021 Task 2 : ReLU over Cosine Similarity for BERT Fine-tuning<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 2: <fixed-case>R</fixed-case>e<fixed-case>LU</fixed-case> over Cosine Similarity for <fixed-case>BERT</fixed-case> Fine-tuning</title>
      <author><first>Boris</first><last>Zhestiankin</last></author>
      <author><first>Maria</first><last>Ponomareva</last></author>
      <pages>163–168</pages>
      <abstract>This paper presents our contribution to SemEval-2021 Task 2 : Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC). Our experiments cover English (EN-EN) sub-track from the multilingual setting of the task. We experiment with several pre-trained language models and investigate an impact of different top-layers on <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>. We find the combination of <a href="https://en.wikipedia.org/wiki/Cosine_similarity">Cosine Similarity</a> and ReLU activation leading to the most effective fine-tuning procedure. Our best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> results in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> 92.7 %, which is the fourth-best score in EN-EN sub-track.</abstract>
      <url hash="855da59b">2021.semeval-1.17</url>
      <doi>10.18653/v1/2021.semeval-1.17</doi>
      <bibkey>zhestiankin-ponomareva-2021-zhestyatsky</bibkey>
      <pwccode url="https://github.com/zhestyatsky/MCL-WiC" additional="false">zhestyatsky/MCL-WiC</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="18">
      <title>SzegedAI at SemEval-2021 Task 2 : Zero-shot Approach for Multilingual and Cross-lingual Word-in-Context Disambiguation<fixed-case>S</fixed-case>zeged<fixed-case>AI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 2: Zero-shot Approach for Multilingual and Cross-lingual Word-in-Context Disambiguation</title>
      <author><first>Gábor</first><last>Berend</last></author>
      <pages>169–174</pages>
      <abstract>In this paper, we introduce our <a href="https://en.wikipedia.org/wiki/System">system</a> that we participated with at the multilingual and cross-lingual word-in-context disambiguation SemEval 2021 shared task. In our experiments, we investigated the possibility of using an all-words fine-grained word sense disambiguation system trained purely on sense-annotated data in English and draw predictions on the semantic equivalence of words in context based on the similarity of the ranked lists of the (English) WordNet synsets returned for the target words decisions had to be made for. We overcame the multi,-and cross-lingual aspects of the shared task by applying a multilingual transformer for encoding the texts written in either <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/French_language">French</a>, <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a> and <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. While our results lag behind top scoring submissions, it has the benefit that it not only provides a binary flag whether two words in their context have the same meaning, but also provides a more tangible output in the form of a ranked list of (English) WordNet synsets irrespective of the language of the input texts. As our framework is designed to be as generic as possible, it can be applied as a baseline for basically any language (supported by the multilingual transformed architecture employed) even in the absence of any additional form of language specific training data.</abstract>
      <url hash="b7e1ca58">2021.semeval-1.18</url>
      <doi>10.18653/v1/2021.semeval-1.18</doi>
      <bibkey>berend-2021-szegedai</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="20">
      <title>ECNU_ICA_1 SemEval-2021 Task 4 : Leveraging Knowledge-enhanced Graph Attention Networks for Reading Comprehension of Abstract Meaning<fixed-case>ECNU</fixed-case>_<fixed-case>ICA</fixed-case>_1 <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 4: Leveraging Knowledge-enhanced Graph Attention Networks for Reading Comprehension of Abstract Meaning</title>
      <author><first>Pingsheng</first><last>Liu</last></author>
      <author><first>Linlin</first><last>Wang</last></author>
      <author><first>Qian</first><last>Zhao</last></author>
      <author><first>Hao</first><last>Chen</last></author>
      <author><first>Yuxi</first><last>Feng</last></author>
      <author><first>Xin</first><last>Lin</last></author>
      <author><first>Liang</first><last>He</last></author>
      <pages>183–188</pages>
      <abstract>This paper describes our <a href="https://en.wikipedia.org/wiki/System">system</a> for SemEval-2021 Task 4 : Reading Comprehension of Abstract Meaning. To accomplish this task, we utilize the Knowledge-Enhanced Graph Attention Network (KEGAT) architecture with a novel semantic space transformation strategy. It leverages heterogeneous knowledge to learn adequate evidences, and seeks for an effective semantic space of abstract concepts to better improve the ability of a machine in understanding the abstract meaning of natural language. Experimental results show that our <a href="https://en.wikipedia.org/wiki/System">system</a> achieves strong performance on this task in terms of both <a href="https://en.wikipedia.org/wiki/Perception">imperceptibility</a> and <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">nonspecificity</a>.</abstract>
      <url hash="f5eeda12">2021.semeval-1.20</url>
      <doi>10.18653/v1/2021.semeval-1.20</doi>
      <bibkey>liu-etal-2021-ecnu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="21">
      <title>LRG at SemEval-2021 Task 4 : Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting<fixed-case>LRG</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting</title>
      <author><first>Abheesht</first><last>Sharma</last></author>
      <author><first>Harshit</first><last>Pandey</last></author>
      <author><first>Gunjan</first><last>Chhablani</last></author>
      <author><first>Yash</first><last>Bhartia</last></author>
      <author><first>Tirtharaj</first><last>Dash</last></author>
      <pages>189–198</pages>
      <abstract>We present our approaches and methods for SemEval-2021 Task-4 Reading Comprehension of Abstract Meaning. Given a question with a fill-in-the-blank, and a corresponding context, the task is to predict the most suitable word from a list of 5 options. There are three subtasks : Imperceptibility, Non-Specificity and <a href="https://en.wikipedia.org/wiki/Intersection">Intersection</a>. We use encoders of transformers-based models pretrained on the MLM task to build our Fill-in-the-blank (FitB) models. Moreover, to model imperceptibility, we define certain linguistic features, and to model non-specificity, we leverage information from <a href="https://en.wikipedia.org/wiki/Hypernymy">hypernyms</a> and <a href="https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy">hyponyms</a> provided by a <a href="https://en.wikipedia.org/wiki/Lexical_database">lexical database</a>. Specifically, for non-specificity, we try out augmentation techniques, and other <a href="https://en.wikipedia.org/wiki/Statistics">statistical techniques</a>. We also propose variants, namely Chunk Voting and Max Context, to take care of input length restrictions for BERT, etc. Additionally, we perform a thorough ablation study, and use Integrated Gradients to explain our predictions on a few samples. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> achieve accuracies of 75.31 % and 77.84 %, on the test sets for subtask-I and subtask-II, respectively. For subtask-III, we achieve accuracies of 65.64 % and 64.27 %.</abstract>
      <url hash="57a8c4f6">2021.semeval-1.21</url>
      <doi>10.18653/v1/2021.semeval-1.21</doi>
      <bibkey>sharma-etal-2021-lrg</bibkey>
      <pwccode url="https://github.com/gchhablani/ReCAM" additional="false">gchhablani/ReCAM</pwccode>
    </paper>
    <paper id="23">
      <title>NLP-IIS@UT at SemEval-2021 Task 4 : Machine Reading Comprehension using the Long Document Transformer<fixed-case>NLP</fixed-case>-<fixed-case>IIS</fixed-case>@<fixed-case>UT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 4: Machine Reading Comprehension using the Long Document Transformer</title>
      <author><first>Hossein</first><last>Basafa</last></author>
      <author><first>Sajad</first><last>Movahedi</last></author>
      <author><first>Ali</first><last>Ebrahimi</last></author>
      <author><first>Azadeh</first><last>Shakery</last></author>
      <author><first>Heshaam</first><last>Faili</last></author>
      <pages>205–210</pages>
      <abstract>This paper presents a technical report of our submission to the 4th task of SemEval-2021, titled : Reading Comprehension of Abstract Meaning. In this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, we want to predict the correct answer based on a question given a context. Usually, contexts are very lengthy and require a large <a href="https://en.wikipedia.org/wiki/Receptive_field">receptive field</a> from the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>. Thus, common contextualized language models like BERT miss fine representation and performance due to the limited capacity of the input tokens. To tackle this problem, we used the longformer model to better process the sequences. Furthermore, we utilized the <a href="https://en.wikipedia.org/wiki/Methodology">method</a> proposed in the longformer benchmark on wikihop dataset which improved the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on our <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task data</a> from (23.01 % and 22.95 %) achieved by the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> for subtask 1 and 2, respectively, to (70.30 % and 64.38 %).</abstract>
      <url hash="cba6750a">2021.semeval-1.23</url>
      <doi>10.18653/v1/2021.semeval-1.23</doi>
      <bibkey>basafa-etal-2021-nlp</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="31">
      <title>HamiltonDinggg at SemEval-2021 Task 5 : Investigating Toxic Span Detection using RoBERTa Pre-training<fixed-case>H</fixed-case>amilton<fixed-case>D</fixed-case>inggg at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 5: Investigating Toxic Span Detection using <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a Pre-training</title>
      <author><first>Huiyang</first><last>Ding</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>263–269</pages>
      <abstract>This paper presents our system submission to task 5 : Toxic Spans Detection of the SemEval-2021 competition. The <a href="https://en.wikipedia.org/wiki/Competition">competition</a> aims at detecting the spans that make a toxic span toxic. In this paper, we demonstrate our system for detecting toxic spans, which includes expanding the toxic training set with Local Interpretable Model-Agnostic Explanations (LIME), fine-tuning RoBERTa model for detection, and error analysis. We found that feeding the model with an expanded training set using Reddit comments of polarized-toxicity and labeling with LIME on top of logistic regression classification could help RoBERTa more accurately learn to recognize toxic spans. We achieved a span-level F1 score of 0.6715 on the testing phase. Our quantitative and qualitative results show that the predictions from our <a href="https://en.wikipedia.org/wiki/System">system</a> could be a good supplement to the gold training set’s annotations.</abstract>
      <url hash="cea17923">2021.semeval-1.31</url>
      <attachment type="OptionalSupplementaryMaterial" hash="38840fbc">2021.semeval-1.31.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.semeval-1.31</doi>
      <bibkey>ding-jurgens-2021-hamiltondinggg</bibkey>
    </paper>
    <paper id="32">
      <title>WVOQ at SemEval-2021 Task 6 : BART for Span Detection and Classification<fixed-case>WVOQ</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 6: <fixed-case>BART</fixed-case> for Span Detection and Classification</title>
      <author><first>Cees</first><last>Roele</last></author>
      <pages>270–274</pages>
      <abstract>Simultaneous span detection and classification is a <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> not currently addressed in standard NLP frameworks. The present paper describes why and how an EncoderDecoder model was used to combine span detection and <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> to address subtask 2 of SemEval-2021 Task 6.</abstract>
      <url hash="c1c2908b">2021.semeval-1.32</url>
      <doi>10.18653/v1/2021.semeval-1.32</doi>
      <bibkey>roele-2021-wvoq</bibkey>
      <pwccode url="https://github.com/ceesroele/SemEval-2021-Task-6" additional="false">ceesroele/SemEval-2021-Task-6</pwccode>
    </paper>
    <paper id="33">
      <title>HumorHunter at SemEval-2021 Task 7 : Humor and Offense Recognition with Disentangled Attention<fixed-case>H</fixed-case>umor<fixed-case>H</fixed-case>unter at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 7: Humor and Offense Recognition with Disentangled Attention</title>
      <author><first>Yubo</first><last>Xie</last></author>
      <author><first>Junze</first><last>Li</last></author>
      <author><first>Pearl</first><last>Pu</last></author>
      <pages>275–280</pages>
      <abstract>In this paper, we describe our system submitted to SemEval 2021 Task 7 : HaHackathon : Detecting and Rating Humor and Offense. The task aims at predicting whether the given text is humorous, the average humor rating given by the annotators, and whether the humor rating is controversial. In addition, the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> also involves predicting how offensive the text is. Our approach adopts the DeBERTa architecture with disentangled attention mechanism, where the attention scores between words are calculated based on their content vectors and relative position vectors. We also took advantage of the pre-trained language models and fine-tuned the DeBERTa model on all the four subtasks. We experimented with several BERT-like structures and found that the large DeBERTa model generally performs better. During the evaluation phase, our system achieved an <a href="https://en.wikipedia.org/wiki/F-score">F-score</a> of 0.9480 on subtask 1a, an RMSE of 0.5510 on subtask 1b, an <a href="https://en.wikipedia.org/wiki/F-score">F-score</a> of 0.4764 on subtask 1c, and an RMSE of 0.4230 on subtask 2a (rank 3 on the leaderboard).</abstract>
      <url hash="8eed6474">2021.semeval-1.33</url>
      <doi>10.18653/v1/2021.semeval-1.33</doi>
      <bibkey>xie-etal-2021-humorhunter</bibkey>
    </paper>
    <paper id="34">
      <title>Grenzlinie at SemEval-2021 Task 7 : Detecting and Rating Humor and Offense<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 7: Detecting and Rating Humor and Offense</title>
      <author><first>Renyuan</first><last>Liu</last></author>
      <author><first>Xiaobing</first><last>Zhou</last></author>
      <pages>281–285</pages>
      <abstract>This paper introduces the result of Team Grenzlinie’s experiment in SemEval-2021 task 7 : HaHackathon : Detecting and Rating Humor and Offense. This <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> has two <a href="https://en.wikipedia.org/wiki/Task_(computing)">subtasks</a>. Subtask1 includes the humor detection task, the humor rating prediction task, and the humor controversy detection task. Subtask2 is an offensive rating prediction task. Detection task is a binary classification task, and the rating prediction task is a regression task between 0 to 5. 0 means the task is not humorous or not offensive, 5 means the task is very humorous or very offensive. For all the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>, this paper chooses RoBERTa as the pre-trained model. In classification tasks, Bi-LSTM and <a href="https://en.wikipedia.org/wiki/Adversarial_learning">adversarial training</a> are adopted. In the <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression task</a>, the Bi-LSTM is also adopted. And then we propose a new <a href="https://en.wikipedia.org/wiki/Methodology">approach</a> named compare method. Finally, our system achieves an <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> of 95.05 % in the humor detection task, <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> of 61.74 % in the humor controversy detection task, 0.6143 RMSE in humor rating task, 0.4761 RMSE in the offensive rating task on the test datasets.</abstract>
      <url hash="7ea7f0d2">2021.semeval-1.34</url>
      <doi>10.18653/v1/2021.semeval-1.34</doi>
      <bibkey>liu-zhou-2021-grenzlinie</bibkey>
    </paper>
    <paper id="36">
      <title>Humor@IITK at SemEval-2021 Task 7 : Large Language Models for Quantifying Humor and Offensiveness<fixed-case>IITK</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 7: Large Language Models for Quantifying Humor and Offensiveness</title>
      <author><first>Aishwarya</first><last>Gupta</last></author>
      <author><first>Avik</first><last>Pal</last></author>
      <author><first>Bholeshwar</first><last>Khurana</last></author>
      <author><first>Lakshay</first><last>Tyagi</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>290–296</pages>
      <abstract>Humor and Offense are highly subjective due to multiple <a href="https://en.wikipedia.org/wiki/Word_sense">word senses</a>, <a href="https://en.wikipedia.org/wiki/Cultural_knowledge">cultural knowledge</a>, and pragmatic competence. Hence, accurately detecting humorous and offensive texts has several compelling use cases in <a href="https://en.wikipedia.org/wiki/Recommender_system">Recommendation Systems</a> and Personalized Content Moderation. However, due to the lack of an extensive labeled dataset, most prior works in this domain have n’t explored large neural models for subjective humor understanding. This paper explores whether large neural models and their ensembles can capture the intricacies associated with humor / offense detection and rating. Our experiments on the SemEval-2021 Task 7 : HaHackathon show that we can develop reasonable humor and offense detection systems with such models. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are ranked 3rd in subtask 1b and consistently ranked around the top 33 % of the leaderboard for the remaining subtasks.</abstract>
      <url hash="3e4a4e46">2021.semeval-1.36</url>
      <doi>10.18653/v1/2021.semeval-1.36</doi>
      <bibkey>gupta-etal-2021-humor</bibkey>
      <pwccode url="https://github.com/aishgupta/Quantifying-Humor-Offensiveness" additional="false">aishgupta/Quantifying-Humor-Offensiveness</pwccode>
    </paper>
    <paper id="37">
      <title>RoMa at SemEval-2021 Task 7 : A Transformer-based Approach for Detecting and Rating Humor and Offense<fixed-case>R</fixed-case>o<fixed-case>M</fixed-case>a at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 7: A Transformer-based Approach for Detecting and Rating Humor and Offense</title>
      <author><first>Roberto</first><last>Labadie</last></author>
      <author><first>Mariano Jason</first><last>Rodriguez</last></author>
      <author><first>Reynier</first><last>Ortega</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>297–305</pages>
      <abstract>In this paper we describe the systems used by the RoMa team in the shared task on Detecting and Rating Humor and Offense (HaHackathon) at SemEval 2021. Our systems rely on data representations learned through fine-tuned neural language models. Particularly, we explore two distinct <a href="https://en.wikipedia.org/wiki/Computer_architecture">architectures</a>. The first one is based on a Siamese Neural Network (SNN) combined with a graph-based clustering method. The SNN model is used for learning a latent space where instances of <a href="https://en.wikipedia.org/wiki/Humour">humor</a> and non-humor can be distinguished. The clustering method is applied to build prototypes of both classes which are used for training and classifying new messages. The second one combines neural language model representations with a <a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression model</a> which makes the final ratings. Our systems achieved the best results for humor classification using <a href="https://en.wikipedia.org/wiki/Conceptual_model">model one</a>, whereas for offensive and humor rating the second <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> obtained better performance. In the case of the controversial humor prediction, the most significant improvement was achieved by a <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> of the neural language model. In general, the results achieved are encouraging and give us a starting point for further improvements.</abstract>
      <url hash="0edd40af">2021.semeval-1.37</url>
      <doi>10.18653/v1/2021.semeval-1.37</doi>
      <bibkey>labadie-etal-2021-roma</bibkey>
    </paper>
    <paper id="43">
      <title>BLCUFIGHT at SemEval-2021 Task 10 : Novel Unsupervised Frameworks For Source-Free Domain Adaptation<fixed-case>BLCUFIGHT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 10: Novel Unsupervised Frameworks For Source-Free Domain Adaptation</title>
      <author><first>Weikang</first><last>Wang</last></author>
      <author><first>Yi</first><last>Wu</last></author>
      <author><first>Yixiang</first><last>Liu</last></author>
      <author><first>Pengyuan</first><last>Liu</last></author>
      <pages>357–363</pages>
      <abstract>Domain adaptation assumes that samples from source and target domains are freely accessible during a training phase. However, such assumption is rarely plausible in the real-world and may causes data-privacy issues, especially when the label of the source domain can be a sensitive attribute as an identifier. SemEval-2021 task 10 focuses on these issues. We participate in the task and propose novel <a href="https://en.wikipedia.org/wiki/Conceptual_framework">frameworks</a> based on self-training method. In our <a href="https://en.wikipedia.org/wiki/System">systems</a>, two different <a href="https://en.wikipedia.org/wiki/Software_framework">frameworks</a> are designed to solve <a href="https://en.wikipedia.org/wiki/Text_classification">text classification</a> and <a href="https://en.wikipedia.org/wiki/Sequence_labeling">sequence labeling</a>. These approaches are tested to be effective which ranks the third among all system in subtask A, and ranks the first among all system in subtask B.</abstract>
      <url hash="7ca9e8b8">2021.semeval-1.43</url>
      <doi>10.18653/v1/2021.semeval-1.43</doi>
      <bibkey>wang-etal-2021-blcufight</bibkey>
    </paper>
    <paper id="52">
      <title>BOUN at SemEval-2021 Task 9 : Text Augmentation Techniques for Fact Verification in Tabular Data<fixed-case>BOUN</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 9: Text Augmentation Techniques for Fact Verification in Tabular Data</title>
      <author><first>Abdullatif</first><last>Köksal</last></author>
      <author><first>Yusuf</first><last>Yüksel</last></author>
      <author><first>Bekir</first><last>Yıldırım</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <pages>431–437</pages>
      <abstract>In this paper, we present our text augmentation based approach for the Table Statement Support Subtask (Phase A) of SemEval-2021 Task 9. We experiment with different text augmentation techniques such as <a href="https://en.wikipedia.org/wiki/Back_translation">back translation</a> and synonym swapping using Word2Vec and <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a>. We show that text augmentation techniques lead to 2.5 % improvement in F1 on the test set. Further, we investigate the impact of <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> and joint learning on fact verification in tabular data by utilizing the SemTabFacts and TabFact datasets. We observe that joint learning improves the F1 scores on the SemTabFacts and TabFact test sets by 3.31 % and 0.77 %, respectively.</abstract>
      <url hash="a1b607d1">2021.semeval-1.52</url>
      <doi>10.18653/v1/2021.semeval-1.52</doi>
      <bibkey>koksal-etal-2021-boun</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
    </paper>
    <paper id="53">
      <title>IITK at SemEval-2021 Task 10 : Source-Free Unsupervised Domain Adaptation using Class Prototypes<fixed-case>IITK</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 10: Source-Free Unsupervised Domain Adaptation using Class Prototypes</title>
      <author id="harshit-kumar-iit"><first>Harshit</first><last>Kumar</last></author>
      <author><first>Jinang</first><last>Shah</last></author>
      <author><first>Nidhi</first><last>Hegde</last></author>
      <author><first>Priyanshu</first><last>Gupta</last></author>
      <author><first>Vaibhav</first><last>Jindal</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>438–444</pages>
      <abstract>Recent progress in <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> has primarily been fueled by the availability of large amounts of annotated data that is obtained from highly expensive manual annotating pro-cesses. To tackle this issue of availability of annotated data, a lot of research has been done on unsupervised domain adaptation that tries to generate systems for an unlabelled target domain data, given labeled source domain data. However, the availability of annotated or labelled source domain dataset ca n’t always be guaranteed because of <a href="https://en.wikipedia.org/wiki/Data_privacy">data-privacy issues</a>. This is especially the case with medical data, as <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> may contain sensitive information of the patients. Source-free domain adaptation (SFDA) aims to resolve this issue by us-ing <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on the <a href="https://en.wikipedia.org/wiki/Data">source data</a> instead of using the original annotated source data. In this work, we try to build SFDA systems for <a href="https://en.wikipedia.org/wiki/Semantic_processing">semantic processing</a> by specifically focusing on the negation detection subtask of the SemEval2021 Task 10. We propose two approaches -ProtoAUGandAdapt-ProtoAUGthat use the idea of <a href="https://en.wikipedia.org/wiki/Self-entropy">self-entropy</a> to choose reliable and high confidence samples, which are then used for <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> and subsequent training of the models. Our methods report an improvement of up to 7 % in <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> over the <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline</a> for the Negation Detection subtask.</abstract>
      <url hash="55a7d74a">2021.semeval-1.53</url>
      <doi>10.18653/v1/2021.semeval-1.53</doi>
      <bibkey>kumar-etal-2021-iitk</bibkey>
    </paper>
    <paper id="54">
      <title>PTST-UoM at SemEval-2021 Task 10 : Parsimonious Transfer for Sequence Tagging<fixed-case>PTST</fixed-case>-<fixed-case>U</fixed-case>o<fixed-case>M</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 10: Parsimonious Transfer for Sequence Tagging</title>
      <author><first>Kemal</first><last>Kurniawan</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <author><first>Philip</first><last>Schulz</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <pages>445–451</pages>
      <abstract>This paper describes PTST, a source-free unsupervised domain adaptation technique for sequence tagging, and its application to the SemEval-2021 Task 10 on time expression recognition. PTST is an extension of the cross-lingual parsimonious parser transfer framework, which uses high-probability predictions of the source model as a supervision signal in self-training. We extend the <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> to a sequence prediction setting, and demonstrate its applicability to unsupervised domain adaptation. PTST achieves F1 score of 79.6 % on the official test set, with the <a href="https://en.wikipedia.org/wiki/Precision_(statistics)">precision</a> of 90.1 %, the highest out of 14 submissions.</abstract>
      <url hash="915e1901">2021.semeval-1.54</url>
      <doi>10.18653/v1/2021.semeval-1.54</doi>
      <bibkey>kurniawan-etal-2021-ptst</bibkey>
    </paper>
    <paper id="55">
      <title>Self-Adapter at SemEval-2021 Task 10 : Entropy-based Pseudo-Labeler for Source-free Domain Adaptation<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 10: Entropy-based Pseudo-Labeler for Source-free Domain Adaptation</title>
      <author><first>Sangwon</first><last>Yoon</last></author>
      <author><first>Yanghoon</first><last>Kim</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>452–457</pages>
      <abstract>Source-free domain adaptation is an emerging line of work in deep learning research since it is closely related to the real-world environment. We study the domain adaption in the sequence labeling problem where the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained on the source domain data is given. We propose two <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> : Self-Adapter and Selective Classifier Training. Self-Adapter is a <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training method</a> that uses sentence-level pseudo-labels filtered by the <a href="https://en.wikipedia.org/wiki/Self-entropy">self-entropy threshold</a> to provide supervision to the whole model. Selective Classifier Training uses token-level pseudo-labels and supervises only the classification layer of the model. The proposed <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a> are evaluated on data provided by SemEval-2021 task 10 and Self-Adapter achieves 2nd rank performance.</abstract>
      <url hash="88ffffc2">2021.semeval-1.55</url>
      <doi>10.18653/v1/2021.semeval-1.55</doi>
      <bibkey>yoon-etal-2021-self</bibkey>
    </paper>
    <paper id="56">
      <title>The University of Arizona at SemEval-2021 Task 10 : Applying Self-training, Active Learning and Data Augmentation to Source-free Domain Adaptation<fixed-case>U</fixed-case>niversity of <fixed-case>A</fixed-case>rizona at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 10: Applying Self-training, Active Learning and Data Augmentation to Source-free Domain Adaptation</title>
      <author><first>Xin</first><last>Su</last></author>
      <author><first>Yiyun</first><last>Zhao</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <pages>458–466</pages>
      <abstract>This paper describes our systems for negation detection and time expression recognition in SemEval 2021 Task 10, Source-Free Domain Adaptation for Semantic Processing. We show that self-training, <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)">active learning</a> and data augmentation techniques can improve the generalization ability of the model on the unlabeled target domain data without accessing source domain data. We also perform detailed ablation studies and error analyses for our time expression recognition systems to identify the source of the performance improvement and give constructive feedback on the temporal normalization annotation guidelines.</abstract>
      <url hash="9f8b87dd">2021.semeval-1.56</url>
      <doi>10.18653/v1/2021.semeval-1.56</doi>
      <bibkey>su-etal-2021-university</bibkey>
    </paper>
    <paper id="58">
      <title>YNU-HPCC at SemEval-2021 Task 11 : Using a BERT Model to Extract Contributions from NLP Scholarly Articles<fixed-case>YNU</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 11: Using a <fixed-case>BERT</fixed-case> Model to Extract Contributions from <fixed-case>NLP</fixed-case> Scholarly Articles</title>
      <author><first>Xinge</first><last>Ma</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>478–484</pages>
      <abstract>This paper describes the system we built as the YNU-HPCC team in the SemEval-2021 Task 11 : NLPContributionGraph. This task involves first identifying sentences in the given natural language processing (NLP) scholarly articles that reflect research contributions through binary classification ; then identifying the core scientific terms and their relation phrases from these contribution sentences by sequence labeling ; and finally, these scientific terms and relation phrases are categorized, identified, and organized into subject-predicate-object triples to form a <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> with the help of <a href="https://en.wikipedia.org/wiki/Multiclass_classification">multiclass classification</a> and <a href="https://en.wikipedia.org/wiki/Multi-label_classification">multi-label classification</a>. We developed a system for this task using a pre-trained language representation model called BERT that stands for Bidirectional Encoder Representations from Transformers, and achieved good results. The average F1-score for Evaluation Phase 2, Part 1 was 0.4562 and ranked 7th, and the average F1-score for Evaluation Phase 2, Part 2 was 0.6541, and also ranked 7th.</abstract>
      <url hash="066f21ba">2021.semeval-1.58</url>
      <doi>10.18653/v1/2021.semeval-1.58</doi>
      <bibkey>ma-etal-2021-ynu</bibkey>
    </paper>
    <paper id="59">
      <title>ITNLP at SemEval-2021 Task 11 : Boosting BERT with Sampling and Adversarial Training for Knowledge Extraction<fixed-case>ITNLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 11: Boosting <fixed-case>BERT</fixed-case> with Sampling and Adversarial Training for Knowledge Extraction</title>
      <author><first>Genyu</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Su</last></author>
      <author><first>Changhong</first><last>He</last></author>
      <author><first>Lei</first><last>Lin</last></author>
      <author><first>Chengjie</first><last>Sun</last></author>
      <author><first>Lili</first><last>Shan</last></author>
      <pages>485–489</pages>
      <abstract>This paper describes the winning system in the End-to-end Pipeline phase for the NLPContributionGraph task. The system is composed of three BERT-based models and the three <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> are used to extract sentences, entities and triples respectively. Experiments show that sampling and adversarial training can greatly boost the <a href="https://en.wikipedia.org/wiki/System">system</a>. In End-to-end Pipeline phase, our <a href="https://en.wikipedia.org/wiki/System">system</a> got an average F1 of 0.4703, significantly higher than the second-placed system which got an average F1 of 0.3828.</abstract>
      <url hash="c9400786">2021.semeval-1.59</url>
      <doi>10.18653/v1/2021.semeval-1.59</doi>
      <bibkey>zhang-etal-2021-itnlp</bibkey>
    </paper>
    <paper id="60">
      <title>Duluth at SemEval-2021 Task 11 : Applying DeBERTa to Contributing Sentence Selection and Dependency Parsing for Entity Extraction<fixed-case>D</fixed-case>uluth at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 11: Applying <fixed-case>D</fixed-case>e<fixed-case>BERT</fixed-case>a to Contributing Sentence Selection and Dependency Parsing for Entity Extraction</title>
      <author><first>Anna</first><last>Martin</last></author>
      <author><first>Ted</first><last>Pedersen</last></author>
      <pages>490–501</pages>
      <abstract>This paper describes the Duluth system that participated in SemEval-2021 Task 11, NLP Contribution Graph. It details the extraction of contribution sentences and scientific entities and their relations from scholarly articles in the domain of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a>. Our solution uses deBERTa for multi-class sentence classification to extract the contributing sentences and their type, and dependency parsing to outline each sentence and extract subject-predicate-object triples. Our system ranked fifth of seven for Phase 1 : end-to-end pipeline, sixth of eight for Phase 2 Part 1 : phrases and triples, and fifth of eight for Phase 2 Part 2 : triples extraction.</abstract>
      <url hash="4963cbad">2021.semeval-1.60</url>
      <doi>10.18653/v1/2021.semeval-1.60</doi>
      <bibkey>martin-pedersen-2021-duluth</bibkey>
    </paper>
    <paper id="61">
      <title>INNOVATORS at SemEval-2021 Task-11 : A Dependency Parsing and BERT-based model for Extracting Contribution Knowledge from Scientific Papers<fixed-case>INNOVATORS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task-11: A Dependency Parsing and <fixed-case>BERT</fixed-case>-based model for Extracting Contribution Knowledge from Scientific Papers</title>
      <author><first>Hardik</first><last>Arora</last></author>
      <author><first>Tirthankar</first><last>Ghosal</last></author>
      <author><first>Sandeep</first><last>Kumar</last></author>
      <author><first>Suraj</first><last>Patwal</last></author>
      <author><first>Phil</first><last>Gooch</last></author>
      <pages>502–510</pages>
      <abstract>In this work, we describe our system submission to the SemEval 2021 Task 11 : NLP Contribution Graph Challenge. We attempt all the three sub-tasks in the challenge and report our results. Subtask 1 aims to identify the contributing sentences in a given publication. Subtask 2 follows from Subtask 1 to extract the scientific term and predicate phrases from the identified contributing sentences. The final Subtask 3 entails extracting triples (subject, predicate, object) from the phrases and categorizing them under one or more defined information units. With the NLPContributionGraph Shared Task, the organizers formalized the building of a scholarly contributions-focused graph over NLP scholarly articles as an automated task. Our approaches include a BERT-based classification model for identifying the contributing sentences in a research publication, a rule-based dependency parsing for phrase extraction, followed by a CNN-based model for information units classification, and a set of rules for triples extraction. The quantitative results show that we obtain the 5th, 5th, and 7th rank respectively in three evaluation phases. We make our codes available at https://github.com/HardikArora17/SemEval-2021-INNOVATORS.<i>triples</i> (subject, predicate, object) from the phrases and categorizing them under one or more defined information units. With the NLPContributionGraph Shared Task, the organizers formalized the building of a scholarly contributions-focused graph over NLP scholarly articles as an automated task. Our approaches include a BERT-based classification model for identifying the contributing sentences in a research publication, a rule-based dependency parsing for phrase extraction, followed by a CNN-based model for information units classification, and a set of rules for triples extraction. The quantitative results show that we obtain the 5th, 5th, and 7th rank respectively in three evaluation phases. We make our codes available at https://github.com/HardikArora17/SemEval-2021-INNOVATORS.</abstract>
      <url hash="7d66a138">2021.semeval-1.61</url>
      <doi>10.18653/v1/2021.semeval-1.61</doi>
      <bibkey>arora-etal-2021-innovators</bibkey>
    </paper>
    <paper id="63">
      <title>HITSZ-HLT at SemEval-2021 Task 5 : Ensemble Sequence Labeling and Span Boundary Detection for Toxic Span Detection<fixed-case>HITSZ</fixed-case>-<fixed-case>HLT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 5: Ensemble Sequence Labeling and Span Boundary Detection for Toxic Span Detection</title>
      <author><first>Qinglin</first><last>Zhu</last></author>
      <author><first>Zijie</first><last>Lin</last></author>
      <author><first>Yice</first><last>Zhang</last></author>
      <author><first>Jingyi</first><last>Sun</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Qihui</first><last>Lin</last></author>
      <author><first>Yixue</first><last>Dang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>521–526</pages>
      <abstract>This paper presents the winning <a href="https://en.wikipedia.org/wiki/System">system</a> that participated in SemEval-2021 Task 5 : Toxic Spans Detection. This task aims to locate those spans that attribute to the text’s toxicity within a text, which is crucial for semi-automated moderation in online discussions. We formalize this task as the Sequence Labeling (SL) problem and the Span Boundary Detection (SBD) problem separately and employ three state-of-the-art models. Next, we integrate predictions of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to produce a more credible and complement result. Our <a href="https://en.wikipedia.org/wiki/System">system</a> achieves a char-level score of 70.83 %, ranking 1/91. In addition, we also explore the lexicon-based method, which is strongly interpretable and flexible in practice.</abstract>
      <url hash="55d83b70">2021.semeval-1.63</url>
      <doi>10.18653/v1/2021.semeval-1.63</doi>
      <bibkey>zhu-etal-2021-hitsz</bibkey>
    </paper>
    <paper id="65">
      <title>UPB at SemEval-2021 Task 8 : Extracting Semantic Information on Measurements as Multi-Turn Question Answering<fixed-case>UPB</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 8: Extracting Semantic Information on Measurements as Multi-Turn Question Answering</title>
      <author><first>Andrei-Marius</first><last>Avram</last></author>
      <author><first>George-Eduard</first><last>Zaharia</last></author>
      <author><first>Dumitru-Clementin</first><last>Cercel</last></author>
      <author><first>Mihai</first><last>Dascalu</last></author>
      <pages>534–540</pages>
      <abstract>Extracting semantic information on measurements and counts is an important topic in terms of analyzing scientific discourses. The 8th task of SemEval-2021 : Counts and Measurements (MeasEval) aimed to boost research in this direction by providing a new dataset on which participants train their models to extract meaningful information on measurements from scientific texts. The competition is composed of five subtasks that build on top of each other : (1) quantity span identification, (2) unit extraction from the identified quantities and their value modifier classification, (3) span identification for measured entities and measured properties, (4) qualifier span identification, and (5) relation extraction between the identified quantities, measured entities, measured properties, and qualifiers. We approached these challenges by first identifying the quantities, extracting their units of measurement, classifying them with corresponding modifiers, and afterwards using them to jointly solve the last three subtasks in a multi-turn question answering manner. Our best performing <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> obtained an overlapping F1-score of 36.91 % on the test set.</abstract>
      <url hash="a4fc652d">2021.semeval-1.65</url>
      <doi>10.18653/v1/2021.semeval-1.65</doi>
      <bibkey>avram-etal-2021-upb</bibkey>
    </paper>
    <paper id="66">
      <title>IITK@LCP at SemEval-2021 Task 1 : Classification for Lexical Complexity Regression Task<fixed-case>IITK</fixed-case>@<fixed-case>LCP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 1: Classification for Lexical Complexity Regression Task</title>
      <author><first>Neil</first><last>Shirude</last></author>
      <author><first>Sagnik</first><last>Mukherjee</last></author>
      <author><first>Tushar</first><last>Shandhilya</last></author>
      <author><first>Ananta</first><last>Mukherjee</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>541–547</pages>
      <abstract>This paper describes our contribution to SemEval 2021 Task 1 (Shardlow et al., 2021): Lexical Complexity Prediction. In our approach, we leverage the ELECTRA model and attempt to mirror the data annotation scheme. Although the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is a <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression task</a>, we show that we can treat it as an aggregation of several <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification and regression models</a>. This somewhat counter-intuitive approach achieved an MAE score of 0.0654 for Sub-Task 1 and MAE of 0.0811 on Sub-Task 2. Additionally, we used the concept of weak supervision signals from Gloss-BERT in our work, and it significantly improved the MAE score in Sub-Task 1.</abstract>
      <url hash="9b430f68">2021.semeval-1.66</url>
      <doi>10.18653/v1/2021.semeval-1.66</doi>
      <bibkey>shirude-etal-2021-iitk</bibkey>
    </paper>
    <paper id="67">
      <title>LCP-RIT at SemEval-2021 Task 1 : Exploring Linguistic Features for Lexical Complexity Prediction<fixed-case>LCP</fixed-case>-<fixed-case>RIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 1: Exploring Linguistic Features for Lexical Complexity Prediction</title>
      <author><first>Abhinandan Tejalkumar</first><last>Desai</last></author>
      <author><first>Kai</first><last>North</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Christopher</first><last>Homan</last></author>
      <pages>548–553</pages>
      <abstract>This paper describes team LCP-RIT’s submission to the SemEval-2021 Task 1 : Lexical Complexity Prediction (LCP). The task organizers provided participants with an augmented version of CompLex (Shardlow et al., 2020), an English multi-domain dataset in which words in context were annotated with respect to their <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> using a five point Likert scale. Our system uses <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> and a wide range of <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">linguistic features</a> (e.g. psycholinguistic features, <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a>, <a href="https://en.wikipedia.org/wiki/Word_frequency">word frequency</a>, POS tags) to predict the complexity of single words in this dataset. We analyze the impact of different linguistic features on the classification performance and we evaluate the results in terms of <a href="https://en.wikipedia.org/wiki/Mean_absolute_error">mean absolute error</a>, <a href="https://en.wikipedia.org/wiki/Mean_squared_error">mean squared error</a>, <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation</a>, and <a href="https://en.wikipedia.org/wiki/Spearman_correlation">Spearman correlation</a>.</abstract>
      <url hash="23186590">2021.semeval-1.67</url>
      <doi>10.18653/v1/2021.semeval-1.67</doi>
      <bibkey>desai-etal-2021-lcp</bibkey>
    </paper>
    <paper id="69">
      <title>CompNA at SemEval-2021 Task 1 : Prediction of lexical complexity analyzing heterogeneous features<fixed-case>C</fixed-case>omp<fixed-case>NA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 1: Prediction of lexical complexity analyzing heterogeneous features</title>
      <author><first>Giuseppe</first><last>Vettigli</last></author>
      <author><first>Antonio</first><last>Sorgente</last></author>
      <pages>560–564</pages>
      <abstract>This paper describes the CompNa model that has been submitted to the Lexical Complexity Prediction (LCP) shared task hosted at SemEval 2021 (Task 1). The solution is based on combining features of different nature through an ensambling method based on <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">Decision Trees</a> and trained using <a href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient Boosting</a>. We discuss the results of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and highlight the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> with more predictive capabilities.</abstract>
      <url hash="17f6d7f7">2021.semeval-1.69</url>
      <doi>10.18653/v1/2021.semeval-1.69</doi>
      <bibkey>vettigli-sorgente-2021-compna</bibkey>
    </paper>
    <paper id="73">
      <title>CS-UM6P at SemEval-2021 Task 1 : A Deep Learning Model-based Pre-trained Transformer Encoder for Lexical Complexity<fixed-case>CS</fixed-case>-<fixed-case>UM</fixed-case>6<fixed-case>P</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 1: A Deep Learning Model-based Pre-trained Transformer Encoder for Lexical Complexity</title>
      <author><first>Nabil</first><last>El Mamoun</last></author>
      <author><first>Abdelkader</first><last>El Mahdaouy</last></author>
      <author><first>Abdellah</first><last>El Mekki</last></author>
      <author><first>Kabil</first><last>Essefar</last></author>
      <author><first>Ismail</first><last>Berrada</last></author>
      <pages>585–589</pages>
      <abstract>Lexical Complexity Prediction (LCP) involves assigning a difficulty score to a particular word or expression, in a text intended for a target audience. In this paper, we introduce a new deep learning-based system for this challenging <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. The proposed system consists of a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning model</a>, based on pre-trained transformer encoder, for word and Multi-Word Expression (MWE) complexity prediction. First, on top of the encoder’s contextualized word embedding, our model employs an attention layer on the input context and the complex word or MWE. Then, the <a href="https://en.wikipedia.org/wiki/Attentional_control">attention output</a> is concatenated with the pooled output of the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and passed to a <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression module</a>. We investigate both single-task and joint training on both Sub-Tasks data using multiple pre-trained transformer-based encoders. The obtained results are very promising and show the effectiveness of fine-tuning pre-trained transformers for LCP task.</abstract>
      <url hash="a0998ad5">2021.semeval-1.73</url>
      <doi>10.18653/v1/2021.semeval-1.73</doi>
      <bibkey>el-mamoun-etal-2021-cs</bibkey>
    </paper>
    <paper id="79">
      <title>RG PA at SemEval-2021 Task 1 : A Contextual Attention-based Model with RoBERTa for Lexical Complexity Prediction<fixed-case>RG</fixed-case> <fixed-case>PA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 1: A Contextual Attention-based Model with <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a for Lexical Complexity Prediction</title>
      <author><first>Gang</first><last>Rao</last></author>
      <author><first>Maochang</first><last>Li</last></author>
      <author><first>Xiaolong</first><last>Hou</last></author>
      <author><first>Lianxin</first><last>Jiang</last></author>
      <author><first>Yang</first><last>Mo</last></author>
      <author><first>Jianping</first><last>Shen</last></author>
      <pages>623–626</pages>
      <abstract>In this paper we propose a contextual attention based model with two-stage fine-tune training using RoBERTa. First, we perform the first-stage fine-tune on corpus with RoBERTa, so that the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> can learn some prior <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a>. Then we get the contextual embedding of context words based on the token-level embedding with the fine-tuned model. And we use Kfold cross-validation to get K models and ensemble them to get the final result. Finally, we attain the 2nd place in the final evaluation phase of sub-task 2 with <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">pearson correlation</a> of 0.8575.</abstract>
      <url hash="4b389693">2021.semeval-1.79</url>
      <doi>10.18653/v1/2021.semeval-1.79</doi>
      <bibkey>rao-etal-2021-rg</bibkey>
    </paper>
    <paper id="81">
      <title>CLULEX at SemEval-2021 Task 1 : A Simple System Goes a Long Way<fixed-case>CLULEX</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 1: A Simple System Goes a Long Way</title>
      <author><first>Greta</first><last>Smolenska</last></author>
      <author><first>Peter</first><last>Kolb</last></author>
      <author><first>Sinan</first><last>Tang</last></author>
      <author><first>Mironas</first><last>Bitinis</last></author>
      <author><first>Héctor</first><last>Hernández</last></author>
      <author><first>Elin</first><last>Asklöv</last></author>
      <pages>632–639</pages>
      <abstract>This paper presents the <a href="https://en.wikipedia.org/wiki/System">system</a> we submitted to the first Lexical Complexity Prediction (LCP) Shared Task 2021. The Shared Task provides participants with a new English dataset that includes context of the target word. We participate in the single-word complexity prediction sub-task and focus on <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a>. Our best system is trained on <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">linguistic features</a> and <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> (Pearson’s score of 0.7942). We demonstrate, however, that a simpler <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">feature set</a> achieves comparable results and submit a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained on 36 <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">linguistic features</a> (Pearson’s score of 0.7925).</abstract>
      <url hash="15a0b21e">2021.semeval-1.81</url>
      <doi>10.18653/v1/2021.semeval-1.81</doi>
      <bibkey>smolenska-etal-2021-clulex</bibkey>
    </paper>
    <paper id="83">
      <title>UNBNLP at SemEval-2021 Task 1 : Predicting lexical complexity with masked language models and character-level encoders<fixed-case>UNBNLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 1: Predicting lexical complexity with masked language models and character-level encoders</title>
      <author><first>Milton</first><last>King</last></author>
      <author><first>Ali</first><last>Hakimi Parizi</last></author>
      <author><first>Samin</first><last>Fakharian</last></author>
      <author><first>Paul</first><last>Cook</last></author>
      <pages>650–654</pages>
      <abstract>In this paper, we present three <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised systems</a> for English lexical complexity prediction of single and multiword expressions for SemEval-2021 Task 1. We explore the use of statistical baseline features, masked language models, and character-level encoders to predict the complexity of a target token in context. Our best <a href="https://en.wikipedia.org/wiki/System">system</a> combines information from these three sources. The results indicate that information from masked language models and <a href="https://en.wikipedia.org/wiki/Character_encoding">character-level encoders</a> can be combined to improve lexical complexity prediction.</abstract>
      <url hash="598f4b0c">2021.semeval-1.83</url>
      <doi>10.18653/v1/2021.semeval-1.83</doi>
      <bibkey>king-etal-2021-unbnlp</bibkey>
    </paper>
    <paper id="92">
      <title>GX at SemEval-2021 Task 2 : BERT with Lemma Information for MCL-WiC Task<fixed-case>GX</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 2: <fixed-case>BERT</fixed-case> with Lemma Information for <fixed-case>MCL</fixed-case>-<fixed-case>W</fixed-case>i<fixed-case>C</fixed-case> Task</title>
      <author><first>Wanying</first><last>Xie</last></author>
      <pages>706–712</pages>
      <abstract>This paper presents the GX system for the Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC) task. The purpose of the MCL-WiC task is to tackle the challenge of capturing the polysemous nature of words without relying on a fixed sense inventory in a multilingual and cross-lingual setting. To solve the problems, we use context-specific word embeddings from BERT to eliminate the ambiguity between words in different contexts. For languages without an available training corpus, such as <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, we use neuron machine translation model to translate the English data released by the organizers to obtain available pseudo-data. In this paper, we apply our <a href="https://en.wikipedia.org/wiki/System">system</a> to the English and Chinese multilingual setting and the experimental results show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> has certain advantages.</abstract>
      <url hash="90b58e2c">2021.semeval-1.92</url>
      <doi>10.18653/v1/2021.semeval-1.92</doi>
      <bibkey>xie-2021-gx-semeval</bibkey>
      <pwccode url="https://github.com/yingwaner/bert4wic" additional="false">yingwaner/bert4wic</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="93">
      <title>PALI at SemEval-2021 Task 2 : Fine-Tune XLM-RoBERTa for Word in Context Disambiguation<fixed-case>PALI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 2: Fine-Tune <fixed-case>XLM</fixed-case>-<fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a for Word in Context Disambiguation</title>
      <author><first>Shuyi</first><last>Xie</last></author>
      <author><first>Jian</first><last>Ma</last></author>
      <author><first>Haiqin</first><last>Yang</last></author>
      <author><first>Lianxin</first><last>Jiang</last></author>
      <author><first>Yang</first><last>Mo</last></author>
      <author><first>Jianping</first><last>Shen</last></author>
      <pages>713–718</pages>
      <abstract>This paper presents the PALI team’s winning system for SemEval-2021 Task 2 : Multilingual and Cross-lingual Word-in-Context Disambiguation. We fine-tune XLM-RoBERTa model to solve the task of word in context disambiguation, i.e., to determine whether the target word in the two contexts contains the same meaning or not. In implementation, we first specifically design an input tag to emphasize the target word in the contexts. Second, we construct a new vector on the fine-tuned embeddings from XLM-RoBERTa and feed it to a fully-connected network to output the probability of whether the target word in the context has the same meaning or not. The new <a href="https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)">vector</a> is attained by concatenating the embedding of the [ CLS ] token and the embeddings of the target word in the contexts. In training, we explore several tricks, such as the Ranger optimizer, <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, and adversarial training, to improve the <a href="https://en.wikipedia.org/wiki/Prediction">model prediction</a>. Consequently, we attain the first place in all four cross-lingual tasks.</abstract>
      <url hash="607fa0d2">2021.semeval-1.93</url>
      <doi>10.18653/v1/2021.semeval-1.93</doi>
      <bibkey>xie-etal-2021-pali</bibkey>
    </paper>
    <paper id="96">
      <title>Cambridge at SemEval-2021 Task 2 : Neural WiC-Model with Data Augmentation and Exploration of Representation<fixed-case>C</fixed-case>ambridge at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 2: Neural <fixed-case>W</fixed-case>i<fixed-case>C</fixed-case>-Model with Data Augmentation and Exploration of Representation</title>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>David</first><last>Strohmaier</last></author>
      <pages>730–737</pages>
      <abstract>This paper describes the system of the Cambridge team submitted to the SemEval-2021 shared task on Multilingual and Cross-lingual Word-in-Context Disambiguation. Building on top of a pre-trained masked language model, our system is first pre-trained on out-of-domain data, and then fine-tuned on in-domain data. We demonstrate the effectiveness of the proposed two-step training strategy and the benefits of <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> from both existing examples and new resources. We further investigate different <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> and show that the addition of distance-based features is helpful in the word-in-context disambiguation task. Our system yields highly competitive results in the cross-lingual track without training on any cross-lingual data ; and achieves state-of-the-art results in the multilingual track, ranking first in two languages (Arabic and Russian) and second in <a href="https://en.wikipedia.org/wiki/French_language">French</a> out of 171 submitted systems.</abstract>
      <url hash="adbc63b4">2021.semeval-1.96</url>
      <doi>10.18653/v1/2021.semeval-1.96</doi>
      <bibkey>yuan-strohmaier-2021-cambridge</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xl-wic">XL-WiC</pwcdataset>
    </paper>
    <paper id="103">
      <title>LIORI at SemEval-2021 Task 2 : Span Prediction and Binary Classification approaches to Word-in-Context Disambiguation<fixed-case>LIORI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 2: Span Prediction and Binary Classification approaches to Word-in-Context Disambiguation</title>
      <author><first>Adis</first><last>Davletov</last></author>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <author><first>Denis</first><last>Gordeev</last></author>
      <author><first>Alexey</first><last>Rey</last></author>
      <pages>780–786</pages>
      <abstract>This paper presents our approaches to SemEval-2021 Task 2 : Multilingual and Cross-lingual Word-in-Context Disambiguation task. The first approach attempted to reformulate the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> as a <a href="https://en.wikipedia.org/wiki/Question_answering">question answering problem</a>, while the second one framed it as a binary classification problem. Our best system, which is an ensemble of XLM-R based binary classifiers trained with data augmentation, is among the 3 best-performing systems for <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a>, <a href="https://en.wikipedia.org/wiki/French_language">French</a> and <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> in the multilingual subtask. In the post-evaluation period, we experimented with <a href="https://en.wikipedia.org/wiki/Batch_normalization">batch normalization</a>, subword pooling and target word occurrence aggregation methods, resulting in further performance improvements.</abstract>
      <url hash="cf75609c">2021.semeval-1.103</url>
      <doi>10.18653/v1/2021.semeval-1.103</doi>
      <bibkey>davletov-etal-2021-liori</bibkey>
    </paper>
    <paper id="104">
      <title>FII_CROSS at SemEval-2021 Task 2 : Multilingual and Cross-lingual Word-in-Context Disambiguation<fixed-case>FII</fixed-case>_<fixed-case>CROSS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation</title>
      <author><first>Ciprian</first><last>Bodnar</last></author>
      <author><first>Andrada</first><last>Tapuc</last></author>
      <author><first>Cosmin</first><last>Pintilie</last></author>
      <author><first>Daniela</first><last>Gifu</last></author>
      <author><first>Diana</first><last>Trandabat</last></author>
      <pages>787–792</pages>
      <abstract>This paper presents a word-in-context disambiguation system. The task focuses on capturing the polysemous nature of words in a multilingual and cross-lingual setting, without considering a strict inventory of word meanings. The system applies Natural Language Processing algorithms on datasets from SemEval 2021 Task 2, being able to identify the meaning of words for the languages <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/French_language">French</a> and <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a>, without making use of any additional mono- or multilingual resources.</abstract>
      <url hash="e84b5e15">2021.semeval-1.104</url>
      <doi>10.18653/v1/2021.semeval-1.104</doi>
      <bibkey>bodnar-etal-2021-fii</bibkey>
    </paper>
    <paper id="106">
      <title>UoR at SemEval-2021 Task 4 : Using Pre-trained BERT Token Embeddings for Question Answering of Abstract Meaning<fixed-case>U</fixed-case>o<fixed-case>R</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 4: Using Pre-trained <fixed-case>BERT</fixed-case> Token Embeddings for Question Answering of Abstract Meaning</title>
      <author><first>Thanet</first><last>Markchom</last></author>
      <author><first>Huizhi</first><last>Liang</last></author>
      <pages>799–804</pages>
      <abstract>Most <a href="https://en.wikipedia.org/wiki/Question_answering">question answering tasks</a> focuses on <a href="https://en.wikipedia.org/wiki/Prediction">predicting concrete answers</a>, e.g., <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entities</a>. These <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> can be normally achieved by understanding the contexts without additional information required. In Reading Comprehension of Abstract Meaning (ReCAM) task, the abstract answers are introduced. To understand <a href="https://en.wikipedia.org/wiki/Abstract_and_concrete">abstract meanings</a> in the context, additional knowledge is essential. In this paper, we propose an approach that leverages the pre-trained BERT Token embeddings as a prior knowledge resource. According to the results, our approach using the pre-trained BERT outperformed the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>. It shows that the pre-trained BERT token embeddings can be used as additional knowledge for understanding abstract meanings in <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>.</abstract>
      <url hash="821c685a">2021.semeval-1.106</url>
      <doi>10.18653/v1/2021.semeval-1.106</doi>
      <bibkey>markchom-liang-2021-uor</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/recam">ReCAM</pwcdataset>
    </paper>
    <paper id="107">
      <title>Noobs at Semeval-2021 Task 4 : Masked Language Modeling for abstract answer prediction<fixed-case>S</fixed-case>emeval-2021 Task 4: Masked Language Modeling for abstract answer prediction</title>
      <author><first>Shikhar</first><last>Shukla</last></author>
      <author><first>Sarthak</first><last>Sarthak</last></author>
      <author><first>Karm Veer</first><last>Arya</last></author>
      <pages>805–809</pages>
      <abstract>This paper presents the <a href="https://en.wikipedia.org/wiki/System">system</a> developed by our team for Semeval 2021 Task 4 : Reading Comprehension of Abstract Meaning. The aim of the task was to benchmark the NLP techniques in understanding the abstract concepts present in a passage, and then predict the missing word in a human written summary of the passage. We trained a Roberta-Large model trained with a masked language modeling objective. In cases where this <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> failed to predict one of the available options, another Roberta-Large model trained as a <a href="https://en.wikipedia.org/wiki/Binary_classifier">binary classifier</a> was used to predict correct and incorrect options. We used passage summary generated by Pegasus model and question as inputs. Our best solution was an ensemble of these 2 systems. We achieved an accuracy of 86.22 % on subtask 1 and 87.10 % on subtask 2.</abstract>
      <url hash="6377373b">2021.semeval-1.107</url>
      <doi>10.18653/v1/2021.semeval-1.107</doi>
      <bibkey>shukla-etal-2021-noobs</bibkey>
    </paper>
    <paper id="109">
      <title>PINGAN Omini-Sinitic at SemEval-2021 Task 4 : Reading Comprehension of Abstract Meaning<fixed-case>PINGAN</fixed-case> Omini-Sinitic at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 4:Reading Comprehension of Abstract Meaning</title>
      <author><first>Ye</first><last>Wang</last></author>
      <author><first>Yanmeng</first><last>Wang</last></author>
      <author><first>Haijun</first><last>Zhu</last></author>
      <author><first>Bo</first><last>Zeng</last></author>
      <author><first>Zhenghong</first><last>Hao</last></author>
      <author><first>Shaojun</first><last>Wang</last></author>
      <author><first>Jing</first><last>Xiao</last></author>
      <pages>820–826</pages>
      <abstract>This paper describes the winning system for subtask 2 and the second-placed system for subtask 1 in SemEval 2021 Task 4 : ReadingComprehension of Abstract Meaning. We propose to use pre-trianed Electra discriminator to choose the best abstract word from five candidates. An upper attention and auto denoising mechanism is introduced to process the long sequences. The experiment results demonstrate that this contribution greatly facilitatesthe contextual language modeling in <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension task</a>. The ablation study is also conducted to show the validity of our proposed methods.</abstract>
      <url hash="5a5b7796">2021.semeval-1.109</url>
      <doi>10.18653/v1/2021.semeval-1.109</doi>
      <bibkey>wang-etal-2021-pingan</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/recam">ReCAM</pwcdataset>
    </paper>
    <paper id="114">
      <title>GHOST at SemEval-2021 Task 5 : Is explanation all you need?<fixed-case>GHOST</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 5: Is explanation all you need?</title>
      <author><first>Kamil</first><last>Pluciński</last></author>
      <author><first>Hanna</first><last>Klimczak</last></author>
      <pages>852–859</pages>
      <abstract>This paper discusses different approaches to the Toxic Spans Detection task. The problem posed by the task was to determine which words contribute mostly to recognising a document as toxic. As opposed to <a href="https://en.wikipedia.org/wiki/Binary_classification">binary classification</a> of entire texts, word-level assessment could be of great use during <a href="https://en.wikipedia.org/wiki/Moderation_system">comment moderation</a>, also allowing for a more in-depth comprehension of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s predictions. As the main goal was to ensure transparency and understanding, this paper focuses on the current state-of-the-art approaches based on the explainable AI concepts and compares them to a supervised learning solution with word-level labels. The work consists of two xAI approaches that automatically provide the explanation for models trained for binary classification of toxic documents : an LSTM model with attention as a model-specific approach and the Shapley values for interpreting BERT predictions as a model-agnostic method. The competing approach considers this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> as supervised token classification, where models like BERT and its modifications were tested. The paper aims to explore, compare and assess the quality of predictions for different <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> on the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. The advantages of each <a href="https://en.wikipedia.org/wiki/Scientific_method">approach</a> and further research direction are also discussed.</abstract>
      <url hash="631440f7">2021.semeval-1.114</url>
      <doi>10.18653/v1/2021.semeval-1.114</doi>
      <bibkey>plucinski-klimczak-2021-ghost</bibkey>
    </paper>
    <paper id="115">
      <title>GoldenWind at SemEval-2021 Task 5 : Orthrus-An Ensemble Approach to Identify Toxicity<fixed-case>G</fixed-case>olden<fixed-case>W</fixed-case>ind at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 5: Orthrus - An Ensemble Approach to Identify Toxicity</title>
      <author><first>Marco</first><last>Palomino</last></author>
      <author><first>Dawid</first><last>Grad</last></author>
      <author><first>James</first><last>Bedwell</last></author>
      <pages>860–864</pages>
      <abstract>Many new developments to detect and mitigate <a href="https://en.wikipedia.org/wiki/Toxicity">toxicity</a> are currently being evaluated. We are particularly interested in the correlation between <a href="https://en.wikipedia.org/wiki/Toxicity">toxicity</a> and the <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> expressed in online posts. While <a href="https://en.wikipedia.org/wiki/Toxicity">toxicity</a> may be disguised by amending the wording of posts, <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> will not. Therefore, we describe here an ensemble method to identify toxicity and classify the emotions expressed on a corpus of annotated posts published by Task 5 of SemEval 2021our analysis shows that the majority of such posts express <a href="https://en.wikipedia.org/wiki/Anger">anger</a>, sadness and <a href="https://en.wikipedia.org/wiki/Fear">fear</a>. Our method to identify <a href="https://en.wikipedia.org/wiki/Toxicity">toxicity</a> combines a lexicon-based approach, which on its own achieves an F1 score of 61.07 %, with a <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning approach</a>, which on its own achieves an F1 score of 60 %. When both <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> are combined, the <a href="https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)">ensemble</a> achieves an F1 score of 66.37 %.</abstract>
      <url hash="c1d1b952">2021.semeval-1.115</url>
      <doi>10.18653/v1/2021.semeval-1.115</doi>
      <bibkey>palomino-etal-2021-goldenwind</bibkey>
    </paper>
    <paper id="119">
      <title>NLP_UIOWA at Semeval-2021 Task 5 : Transferring Toxic Sets to Tag Toxic Spans<fixed-case>NLP</fixed-case>_<fixed-case>UIOWA</fixed-case> at <fixed-case>S</fixed-case>emeval-2021 Task 5: Transferring Toxic Sets to Tag Toxic Spans</title>
      <author><first>Jonathan</first><last>Rusert</last></author>
      <pages>881–887</pages>
      <abstract>We leverage a BLSTM with <a href="https://en.wikipedia.org/wiki/Attention">attention</a> to identify toxic spans in texts. We explore different <a href="https://en.wikipedia.org/wiki/Dimension">dimensions</a> which affect the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s performance. The first dimension explored is the toxic set the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is trained on. Besides the provided <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, we explore the transferability of 5 different toxic related sets, including offensive, toxic, abusive, and hate sets. We find that the solely offensive set shows the highest promise of transferability. The second dimension we explore is <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a>, including leveraging <a href="https://en.wikipedia.org/wiki/Attention">attention</a>, employing a greedy remove method, using a <a href="https://en.wikipedia.org/wiki/Frequency_ratio">frequency ratio</a>, and examining hybrid combinations of multiple methods. We conduct an error analysis to examine which types of toxic spans were missed and which were wrongly inferred as toxic along with the main reasons why they occurred. Finally, we extend our method via ensembles, which achieves our highest F1 score of 55.1.</abstract>
      <url hash="d3fd46e9">2021.semeval-1.119</url>
      <doi>10.18653/v1/2021.semeval-1.119</doi>
      <bibkey>rusert-2021-nlp</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="120">
      <title>S-NLP at SemEval-2021 Task 5 : An Analysis of Dual Networks for Sequence Tagging<fixed-case>S</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 5: An Analysis of Dual Networks for Sequence Tagging</title>
      <author><first>Viet Anh</first><last>Nguyen</last></author>
      <author><first>Tam Minh</first><last>Nguyen</last></author>
      <author><first>Huy</first><last>Quang Dao</last></author>
      <author><first>Quang</first><last>Huu Pham</last></author>
      <pages>888–897</pages>
      <abstract>The SemEval 2021 task 5 : Toxic Spans Detection is a task of identifying considered-toxic spans in text, which provides a valuable, automatic tool for moderating online contents. This paper represents the second-place method for the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, an ensemble of two <a href="https://en.wikipedia.org/wiki/Methodology">approaches</a>. While one approach relies on combining different embedding methods to extract diverse semantic and syntactic representations of words in context ; the other utilizes extra data with a slightly customized Self-training, a semi-supervised learning technique, for sequence tagging problems. Both of our <a href="https://en.wikipedia.org/wiki/Software_architecture">architectures</a> take advantage of a strong <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>, which was fine-tuned on a toxic classification task. Although experimental evidence indicates higher effectiveness of the first approach than the second one, combining them leads to our best results of 70.77 F1-score on the test dataset.</abstract>
      <url hash="663ddfda">2021.semeval-1.120</url>
      <doi>10.18653/v1/2021.semeval-1.120</doi>
      <bibkey>nguyen-etal-2021-nlp</bibkey>
    </paper>
    <paper id="124">
      <title>MIPT-NSU-UTMN at SemEval-2021 Task 5 : Ensembling Learning with Pre-trained Language Models for Toxic Spans Detection<fixed-case>MIPT</fixed-case>-<fixed-case>NSU</fixed-case>-<fixed-case>UTMN</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 5: Ensembling Learning with Pre-trained Language Models for Toxic Spans Detection</title>
      <author><first>Mikhail</first><last>Kotyushev</last></author>
      <author><first>Anna</first><last>Glazkova</last></author>
      <author><first>Dmitry</first><last>Morozov</last></author>
      <pages>913–918</pages>
      <abstract>This paper describes our <a href="https://en.wikipedia.org/wiki/System">system</a> for SemEval-2021 Task 5 on Toxic Spans Detection. We developed ensemble models using BERT-based neural architectures and post-processing to combine tokens into spans. We evaluated several pre-trained language models using various ensemble techniques for toxic span identification and achieved sizable improvements over our baseline fine-tuned BERT models. Finally, our <a href="https://en.wikipedia.org/wiki/System">system</a> obtained a F1-score of 67.55 % on test data.</abstract>
      <url hash="3cd22cfb">2021.semeval-1.124</url>
      <doi>10.18653/v1/2021.semeval-1.124</doi>
      <bibkey>kotyushev-etal-2021-mipt</bibkey>
      <pwccode url="https://github.com/morozowdmitry/semeval21" additional="false">morozowdmitry/semeval21</pwccode>
    </paper>
    <paper id="125">
      <title>UIT-E10dot3 at SemEval-2021 Task 5 : Toxic Spans Detection with Named Entity Recognition and Question-Answering Approaches<fixed-case>UIT</fixed-case>-E10dot3 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 5: Toxic Spans Detection with Named Entity Recognition and Question-Answering Approaches</title>
      <author><first>Phu</first><last>Gia Hoang</last></author>
      <author><first>Luan</first><last>Thanh Nguyen</last></author>
      <author><first>Kiet</first><last>Nguyen</last></author>
      <pages>919–926</pages>
      <abstract>The increment of toxic comments on online space is causing tremendous effects on other vulnerable users. For this reason, considerable efforts are made to deal with this, and SemEval-2021 Task 5 : Toxic Spans Detection is one of those. This task asks competitors to extract spans that have <a href="https://en.wikipedia.org/wiki/Toxicity">toxicity</a> from the given texts, and we have done several analyses to understand its structure before doing experiments. We solve this task by two approaches, <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entity Recognition</a> with spaCy’s library and <a href="https://en.wikipedia.org/wiki/Question_answering">Question-Answering</a> with RoBERTa combining with ToxicBERT, and the former gains the highest <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> of 66.99 %.</abstract>
      <url hash="2224d5c1">2021.semeval-1.125</url>
      <doi>10.18653/v1/2021.semeval-1.125</doi>
      <bibkey>gia-hoang-etal-2021-uit</bibkey>
    </paper>
    <paper id="130">
      <title>YoungSheldon at SemEval-2021 Task 5 : Fine-tuning Pre-trained Language Models for Toxic Spans Detection using Token classification Objective<fixed-case>Y</fixed-case>oung<fixed-case>S</fixed-case>heldon at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 5: Fine-tuning Pre-trained Language Models for Toxic Spans Detection using Token classification Objective</title>
      <author><first>Mayukh</first><last>Sharma</last></author>
      <author><first>Ilanthenral</first><last>Kandasamy</last></author>
      <author><first>W.b.</first><last>Vasantha</last></author>
      <pages>953–959</pages>
      <abstract>In this paper, we describe our <a href="https://en.wikipedia.org/wiki/System">system</a> used for SemEval 2021 Task 5 : Toxic Spans Detection. Our proposed <a href="https://en.wikipedia.org/wiki/System">system</a> approaches the <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> as a token classification task. We trained our model to find toxic words and concatenate their spans to predict the toxic spans within a sentence. We fine-tuned Pre-trained Language Models (PLMs) for identifying the toxic words. For fine-tuning, we stacked the classification layer on top of the PLM features of each word to classify if it is toxic or not. PLMs are pre-trained using different objectives and their performance may differ on downstream tasks. We, therefore, compare the performance of BERT, ELECTRA, RoBERTa, XLM-RoBERTa, T5, XLNet, and MPNet for identifying toxic spans within a sentence. Our best performing <a href="https://en.wikipedia.org/wiki/System">system</a> used RoBERTa. It performed well, achieving an F1 score of 0.6841 and secured a rank of 16 on the official leaderboard.</abstract>
      <url hash="5dc2313f">2021.semeval-1.130</url>
      <doi>10.18653/v1/2021.semeval-1.130</doi>
      <bibkey>sharma-etal-2021-youngsheldon</bibkey>
      <pwccode url="https://github.com/04mayukh/YoungSheldon-at-SemEval-2021-Task-5-Toxic-Spans-Detection" additional="false">04mayukh/YoungSheldon-at-SemEval-2021-Task-5-Toxic-Spans-Detection</pwccode>
    </paper>
    <paper id="134">
      <title>SINAI at SemEval-2021 Task 5 : Combining Embeddings in a BiLSTM-CRF model for Toxic Spans Detection<fixed-case>SINAI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 5: Combining Embeddings in a <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case>-<fixed-case>CRF</fixed-case> model for Toxic Spans Detection</title>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last></author>
      <author><first>Pilar</first><last>López-Úbeda</last></author>
      <author><first>L. Alfonso</first><last>Ureña-López</last></author>
      <author><first>M. Teresa</first><last>Martín-Valdivia</last></author>
      <pages>984–989</pages>
      <abstract>This paper describes the participation of SINAI team at Task 5 : Toxic Spans Detection which consists of identifying spans that make a text toxic. Although several resources and systems have been developed so far in the context of <a href="https://en.wikipedia.org/wiki/Profanity">offensive language</a>, both <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> and tasks have mainly focused on classifying whether a text is offensive or not. However, detecting toxic spans is crucial to identify why a text is toxic and can assist human moderators to locate this type of content on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. In order to accomplish the task, we follow a deep learning-based approach using a Bidirectional variant of a Long Short Term Memory network along with a stacked Conditional Random Field decoding layer (BiLSTM-CRF). Specifically, we test the performance of the combination of different pre-trained word embeddings for recognizing toxic entities in text. The results show that the combination of word embeddings helps in detecting offensive content. Our team ranks 29th out of 91 participants.</abstract>
      <url hash="00e3f120">2021.semeval-1.134</url>
      <doi>10.18653/v1/2021.semeval-1.134</doi>
      <bibkey>plaza-del-arco-etal-2021-sinai</bibkey>
    </paper>
    <paper id="135">
      <title>CSECU-DSG at SemEval-2021 Task 5 : Leveraging Ensemble of Sequence Tagging Models for Toxic Spans Detection<fixed-case>CSECU</fixed-case>-<fixed-case>DSG</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 5: Leveraging Ensemble of Sequence Tagging Models for Toxic Spans Detection</title>
      <author><first>Tashin</first><last>Hossain</last></author>
      <author><first>Jannatun</first><last>Naim</last></author>
      <author><first>Fareen</first><last>Tasneem</last></author>
      <author><first>Radiathun</first><last>Tasnia</last></author>
      <author><first>Abu Nowshed</first><last>Chy</last></author>
      <pages>990–994</pages>
      <abstract>The upsurge of prolific <a href="https://en.wikipedia.org/wiki/Blog">blogging</a> and <a href="https://en.wikipedia.org/wiki/Microblogging">microblogging platforms</a> enabled the abusers to spread negativity and <a href="https://en.wikipedia.org/wiki/Threat">threats</a> greater than ever. Detecting the toxic portions substantially aids to moderate or exclude the abusive parts for maintaining sound online platforms. This paper describes our participation in the SemEval 2021 toxic span detection task. The <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> requires detecting spans that convey toxic remarks from the given text. We explore an ensemble of sequence labeling models including the BiLSTM-CRF, spaCy NER model with custom toxic tags, and fine-tuned BERT model to identify the toxic spans. Finally, a majority voting ensemble method is used to determine the unified toxic spans. Experimental results depict the competitive performance of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> among the participants.</abstract>
      <url hash="fddb0463">2021.semeval-1.135</url>
      <doi>10.18653/v1/2021.semeval-1.135</doi>
      <bibkey>hossain-etal-2021-csecu</bibkey>
    </paper>
    <paper id="140">
      <title>AIMH at SemEval-2021 Task 6 : Multimodal Classification Using an Ensemble of Transformer Models<fixed-case>AIMH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 6: Multimodal Classification Using an Ensemble of Transformer Models</title>
      <author><first>Nicola</first><last>Messina</last></author>
      <author><first>Fabrizio</first><last>Falchi</last></author>
      <author><first>Claudio</first><last>Gennaro</last></author>
      <author><first>Giuseppe</first><last>Amato</last></author>
      <pages>1020–1026</pages>
      <abstract>This paper describes the <a href="https://en.wikipedia.org/wiki/System">system</a> used by the AIMH Team to approach the SemEval Task 6. We propose an approach that relies on an architecture based on the transformer model to process multimodal content (text and images) in memes. Our architecture, called DVTT (Double Visual Textual Transformer), approaches Subtasks 1 and 3 of Task 6 as multi-label classification problems, where the text and/or images of the meme are processed, and the probabilities of the presence of each possible persuasion technique are returned as a result. DVTT uses two complete networks of transformers that work on text and images that are mutually conditioned. One of the two <a href="https://en.wikipedia.org/wiki/Methodology">modalities</a> acts as the main one and the second one intervenes to enrich the first one, thus obtaining two distinct ways of operation. The two transformers outputs are merged by averaging the inferred probabilities for each possible label, and the overall <a href="https://en.wikipedia.org/wiki/Neural_network">network</a> is trained end-to-end with a binary cross-entropy loss.</abstract>
      <url hash="b93fad50">2021.semeval-1.140</url>
      <doi>10.18653/v1/2021.semeval-1.140</doi>
      <bibkey>messina-etal-2021-aimh</bibkey>
      <pwccode url="https://github.com/mesnico/memepersuasiondetection" additional="false">mesnico/memepersuasiondetection</pwccode>
    </paper>
    <paper id="142">
      <title>1213Li at SemEval-2021 Task 6 : Detection of Propaganda with Multi-modal Attention and Pre-trained Models<fixed-case>L</fixed-case>i at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 6: Detection of Propaganda with Multi-modal Attention and Pre-trained Models</title>
      <author><first>Peiguang</first><last>Li</last></author>
      <author><first>Xuan</first><last>Li</last></author>
      <author><first>Xian</first><last>Sun</last></author>
      <pages>1032–1036</pages>
      <abstract>This paper presents the solution proposed by the 1213Li team for subtask 3 in SemEval-2021 Task 6 : identifying the multiple persuasion techniques used in the multi-modal content of the meme. We explored various approaches in <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a> and the detection of persuasion labels. Our final model employs pre-trained models including RoBERTa and ResNet-50 as a feature extractor for texts and images, respectively, and adopts a label embedding layer with multi-modal attention mechanism to measure the similarity of labels with the multi-modal information and fuse features for label prediction. Our proposed method outperforms the provided baseline method and achieves 3rd out of 16 participants with 0.54860/0.22830 for Micro / Macro F1 scores.</abstract>
      <url hash="80a3f25e">2021.semeval-1.142</url>
      <doi>10.18653/v1/2021.semeval-1.142</doi>
      <bibkey>li-etal-2021-1213li</bibkey>
    </paper>
    <paper id="143">
      <title>NLyticsFKIE at SemEval-2021 Task 6 : Detection of Persuasion Techniques In Texts And Images<fixed-case>NL</fixed-case>ytics<fixed-case>FKIE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 6: Detection of Persuasion Techniques In Texts And Images</title>
      <author><first>Albert</first><last>Pritzkau</last></author>
      <pages>1037–1044</pages>
      <abstract>The following system description presents our approach to the detection of persuasion techniques in <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">texts</a> and <a href="https://en.wikipedia.org/wiki/Digital_image">images</a>. The given <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> has been framed as a multi-label classification problem with the different techniques serving as class labels. The multi-label classification problem is one in which a list of target variables such as our class labels is associated with every input chunk and assumes that a document can simultaneously and independently be assigned to multiple labels or classes. In order to assign class labels to the given memes, we opted for RoBERTa (A Robustly Optimized BERT Pretraining Approach) as a neural network architecture for token and sequence classification. Starting off with a pre-trained model for language representation we fine-tuned this <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> on the given classification task with the provided annotated data in supervised training steps. To incorporate image features in the multi-modal setting, we rely on the pre-trained VGG-16 model architecture.</abstract>
      <url hash="169b5893">2021.semeval-1.143</url>
      <doi>10.18653/v1/2021.semeval-1.143</doi>
      <bibkey>pritzkau-2021-nlyticsfkie</bibkey>
    </paper>
    <paper id="144">
      <title>YNU-HPCC at SemEval-2021 Task 6 : Combining ALBERT and Text-CNN for Persuasion Detection in Texts and Images<fixed-case>YNU</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 6: Combining <fixed-case>ALBERT</fixed-case> and Text-<fixed-case>CNN</fixed-case> for Persuasion Detection in Texts and Images</title>
      <author><first>Xingyu</first><last>Zhu</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>1045–1050</pages>
      <abstract>In recent years, <a href="https://en.wikipedia.org/wiki/Meme">memes</a> combining image and text have been widely used in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, and <a href="https://en.wikipedia.org/wiki/Meme">memes</a> are one of the most popular types of content used in online disinformation campaigns. In this paper, our study on the detection of persuasion techniques in <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">texts</a> and <a href="https://en.wikipedia.org/wiki/Image">images</a> in SemEval-2021 Task 6 is summarized. For propaganda technology detection in text, we propose a combination model of both ALBERT and Text CNN for text classification, as well as a BERT-based multi-task sequence labeling model for propaganda technology coverage span detection. For the meme classification task involved in text understanding and visual feature extraction, we designed a parallel channel model divided into text and image channels. Our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> achieved a good performance on subtasks 1 and 3. The micro F1-scores of 0.492, 0.091, and 0.446 achieved on the test sets of the three subtasks ranked 12th, 7th, and 11th, respectively, and all are higher than the baseline model.</abstract>
      <url hash="2c64dfa5">2021.semeval-1.144</url>
      <doi>10.18653/v1/2021.semeval-1.144</doi>
      <bibkey>zhu-etal-2021-ynu</bibkey>
    </paper>
    <paper id="147">
      <title>NLPIITR at SemEval-2021 Task 6 : RoBERTa Model with Data Augmentation for Persuasion Techniques Detection<fixed-case>NLPIITR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 6: <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a Model with Data Augmentation for Persuasion Techniques Detection</title>
      <author><first>Vansh</first><last>Gupta</last></author>
      <author><first>Raksha</first><last>Sharma</last></author>
      <pages>1061–1067</pages>
      <abstract>This paper describes and examines different systems to address Task 6 of SemEval-2021 : Detection of Persuasion Techniques In Texts And Images, Subtask 1. The task aims to build a model for identifying rhetorical and psycho- logical techniques (such as causal oversimplification, name-calling, smear) in the textual content of a meme which is often used in a disinformation campaign to influence the users. The paper provides an extensive comparison among various <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning systems</a> as a solution to the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. We elaborate on the pre-processing of the <a href="https://en.wikipedia.org/wiki/Text_file">text data</a> in favor of the task and present ways to overcome the class imbalance. The results show that fine-tuning a RoBERTa model gave the best results with an <a href="https://en.wikipedia.org/wiki/F-number">F1-Micro score</a> of 0.51 on the development set.</abstract>
      <url hash="df07c2ca">2021.semeval-1.147</url>
      <doi>10.18653/v1/2021.semeval-1.147</doi>
      <bibkey>gupta-sharma-2021-nlpiitr</bibkey>
    </paper>
    <paper id="150">
      <title>MinD at SemEval-2021 Task 6 : Propaganda Detection using Transfer Learning and Multimodal Fusion<fixed-case>M</fixed-case>in<fixed-case>D</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 6: Propaganda Detection using Transfer Learning and Multimodal Fusion</title>
      <author><first>Junfeng</first><last>Tian</last></author>
      <author><first>Min</first><last>Gui</last></author>
      <author><first>Chenliang</first><last>Li</last></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Wenming</first><last>Xiao</last></author>
      <pages>1082–1087</pages>
      <abstract>We describe our systems of subtask1 and subtask3 for SemEval-2021 Task 6 on Detection of Persuasion Techniques in Texts and Images. The purpose of subtask1 is to identify <a href="https://en.wikipedia.org/wiki/Propaganda_techniques">propaganda techniques</a> given textual content, and the goal of subtask3 is to detect them given both textual and visual content. For subtask1, we investigate <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> based on pre-trained language models (PLMs) such as <a href="https://en.wikipedia.org/wiki/BERT">BERT</a>, RoBERTa to solve data sparsity problems. For subtask3, we extract <a href="https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity">heterogeneous visual representations</a> (i.e., face features, <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">OCR features</a>, and multimodal representations) and explore various multimodal fusion strategies to combine the textual and visual representations. The official evaluation shows our ensemble model ranks 1st for subtask1 and 2nd for subtask3.</abstract>
      <url hash="11e6eb23">2021.semeval-1.150</url>
      <doi>10.18653/v1/2021.semeval-1.150</doi>
      <bibkey>tian-etal-2021-mind</bibkey>
    </paper>
    <paper id="151">
      <title>CSECU-DSG at SemEval-2021 Task 6 : Orchestrating Multimodal Neural Architectures for Identifying Persuasion Techniques in Texts and Images<fixed-case>CSECU</fixed-case>-<fixed-case>DSG</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 6: Orchestrating Multimodal Neural Architectures for Identifying Persuasion Techniques in Texts and Images</title>
      <author><first>Tashin</first><last>Hossain</last></author>
      <author><first>Jannatun</first><last>Naim</last></author>
      <author><first>Fareen</first><last>Tasneem</last></author>
      <author><first>Radiathun</first><last>Tasnia</last></author>
      <author><first>Abu Nowshed</first><last>Chy</last></author>
      <pages>1088–1095</pages>
      <abstract>Inscribing persuasion techniques in <a href="https://en.wikipedia.org/wiki/Meme">memes</a> is the most impactful way to influence peoples’ mindsets. People are more inclined to memes as they are more stimulating and convincing and hence <a href="https://en.wikipedia.org/wiki/Meme">memes</a> are often exploited by tactfully engraving propaganda in its context with the intent of attaining specific agenda. This paper describes our participation in the three subtasks featured by SemEval 2021 task 6 on the detection of persuasion techniques in <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">texts</a> and <a href="https://en.wikipedia.org/wiki/Image">images</a>. We utilize a fusion of <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>, <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision tree</a>, and fine-tuned DistilBERT for tackling subtask 1. As for subtask 2, we propose a system that consolidates a span identification model and a multi-label classification model based on pre-trained BERT. We address the multi-modal multi-label classification of memes defined in subtask 3 by utilizing a ResNet50 based image model, DistilBERT based text model, and a multi-modal architecture based on multikernel CNN+LSTM and MLP model. The outcomes illustrated the competitive performance of our <a href="https://en.wikipedia.org/wiki/System">systems</a>.</abstract>
      <url hash="4fbd2a9b">2021.semeval-1.151</url>
      <doi>10.18653/v1/2021.semeval-1.151</doi>
      <bibkey>hossain-etal-2021-csecu-dsg</bibkey>
    </paper>
    <paper id="152">
      <title>UMUTeam at SemEval-2021 Task 7 : Detecting and Rating Humor and Offense with Linguistic Features and Word Embeddings<fixed-case>UMUT</fixed-case>eam at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 7: Detecting and Rating Humor and Offense with Linguistic Features and Word Embeddings</title>
      <author><first>José Antonio</first><last>García-Díaz</last></author>
      <author><first>Rafael</first><last>Valencia-García</last></author>
      <pages>1096–1101</pages>
      <abstract>In writing, <a href="https://en.wikipedia.org/wiki/Humour">humor</a> is mainly based on <a href="https://en.wikipedia.org/wiki/Literal_and_figurative_language">figurative language</a> in which words and expressions change their conventional meaning to refer to something without saying it directly. This flip in the meaning of the words prevents <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a> from revealing the real intention of a communication and, therefore, reduces the effectiveness of tasks such as <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> or <a href="https://en.wikipedia.org/wiki/Emotion_detection">Emotion Detection</a>. In this manuscript we describe the participation of the UMUTeam in HaHackathon 2021, whose objective is to detect and rate humorous and controversial content. Our proposal is based on the combination of <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">linguistic features</a> with contextual and non-contextual word embeddings. We participate in all the proposed subtasks achieving our best result in the controversial humor subtask.</abstract>
      <url hash="74fe961f">2021.semeval-1.152</url>
      <doi>10.18653/v1/2021.semeval-1.152</doi>
      <bibkey>garcia-diaz-valencia-garcia-2021-umuteam</bibkey>
      <pwccode url="https://github.com/smolky/hahackathon-2021" additional="false">smolky/hahackathon-2021</pwccode>
    </paper>
    <paper id="153">
      <title>ES-JUST at SemEval-2021 Task 7 : Detecting and Rating Humor and Offensive Text Using <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a><fixed-case>ES</fixed-case>-<fixed-case>JUST</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 7: Detecting and Rating Humor and Offensive Text Using Deep Learning</title>
      <author><first>Emran</first><last>Al Bashabsheh</last></author>
      <author><first>Sanaa</first><last>Abu Alasal</last></author>
      <pages>1102–1107</pages>
      <abstract>This research presents the work of the team’s ES-JUST at semEval-2021 task 7 for detecting and rating humor and offensive text using <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>. The team evaluates several approaches (i.e. Bert, Roberta, XLM-Roberta, and Bert embedding + Bi-LSTM) that employ in four sub-tasks. The first sub-task deal with whether the text is humorous or not. The second sub-task is the degree of <a href="https://en.wikipedia.org/wiki/Humour">humor</a> in the text if the first <a href="https://en.wikipedia.org/wiki/Task_(project_management)">sub-task</a> is humorous. The third sub-task represents the text is controversial or not if it is humorous. While in the last task is the degree of an offensive in the text. However, Roberta pre-trained model outperforms other approaches and score the highest in all sub-tasks. We rank on the leader board at the evaluation phase are 14, 15, 20, and 5 through 0.9564 <a href="https://en.wikipedia.org/wiki/F-score">F-score</a>, 0.5709 <a href="https://en.wikipedia.org/wiki/Randomized_controlled_trial">RMSE</a>, 0.4888 <a href="https://en.wikipedia.org/wiki/F-score">F-score</a>, and 0.4467 RMSE results, respectively, for each of the first, second, third, and fourth sub-task, respectively.<i>i.e.Bert, Roberta, XLM-Roberta, and Bert embedding + Bi-LSTM</i>) that employ in four sub-tasks. The first sub-task deal with whether the text is humorous or not. The second sub-task is the degree of humor in the text if the first sub-task is humorous. The third sub-task represents the text is controversial or not if it is humorous. While in the last task is the degree of an offensive in the text. However, Roberta pre-trained model outperforms other approaches and score the highest in all sub-tasks. We rank on the leader board at the evaluation phase are 14, 15, 20, and 5 through 0.9564 F-score, 0.5709 RMSE, 0.4888 F-score, and 0.4467 RMSE results, respectively, for each of the first, second, third, and fourth sub-task, respectively.</abstract>
      <url hash="77c8076d">2021.semeval-1.153</url>
      <doi>10.18653/v1/2021.semeval-1.153</doi>
      <bibkey>al-bashabsheh-abu-alasal-2021-es</bibkey>
    </paper>
    <paper id="154">
      <title>Tsia at SemEval-2021 Task 7 : Detecting and Rating Humor and Offense<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 7: Detecting and Rating Humor and Offense</title>
      <author><first>Zhengyi</first><last>Guan</last></author>
      <author><first>Xiaobing ZXB</first><last>Zhou</last></author>
      <pages>1108–1113</pages>
      <abstract>This paper describes our contribution to SemEval-2021 Task 7 : Detecting and Rating Humor and Of-fense. This task contains two sub-tasks, sub-task 1and sub-task 2. Among them, sub-task 1 containsthree <a href="https://en.wikipedia.org/wiki/Task_(project_management)">sub-tasks</a>, sub-task 1a, sub-task 1b and sub-task 1c. Sub-task 1a is to predict if the text would beconsidered humorous. Sub-task 1c is described asfollows : if the text is classed as humorous, predictif the humor rating would be considered controver-sial, i.e. the variance of the rating between annota-tors is higher than the median.we combined threepre-trained model with CNN to complete these twoclassification sub-tasks. Sub-task 1b is to judge thedegree of <a href="https://en.wikipedia.org/wiki/Humour">humor</a>. Sub-task 2 aims to predict how of-fensive a text would be with values between 0 and5.We use the idea of <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression</a> to deal with thesetwo sub-tasks. We analyze the performance of ourmethod and demonstrate the contribution of eachcomponent of our architecture. We have achievedgood results under the combination of multiple pre-training models and optimization methods.</abstract>
      <url hash="e4e56018">2021.semeval-1.154</url>
      <doi>10.18653/v1/2021.semeval-1.154</doi>
      <bibkey>guan-zhou-2021-tsia</bibkey>
    </paper>
    <paper id="169">
      <title>DuluthNLP at SemEval-2021 Task 7 : Fine-Tuning RoBERTa Model for Humor Detection and Offense Rating<fixed-case>D</fixed-case>uluth<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 7: Fine-Tuning <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a Model for Humor Detection and Offense Rating</title>
      <author><first>Samuel</first><last>Akrah</last></author>
      <pages>1196–1203</pages>
      <abstract>This paper presents the DuluthNLP submission to Task 7 of the SemEval 2021 competition on Detecting and Rating Humor and Offense. In it, we explain the approach used to train the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> together with the process of fine-tuning our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> in getting the results. We focus on humor detection, rating, and of-fense rating, representing three out of the four subtasks that were provided. We show that optimizing hyper-parameters for learning rate, batch size and number of epochs can increase the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and <a href="https://en.wikipedia.org/wiki/F-number">F1 score</a> for humor detection</abstract>
      <url hash="43c79325">2021.semeval-1.169</url>
      <doi>10.18653/v1/2021.semeval-1.169</doi>
      <bibkey>akrah-2021-duluthnlp</bibkey>
      <pwccode url="https://github.com/akrahdan/semeval2021" additional="false">akrahdan/semeval2021</pwccode>
    </paper>
    <paper id="172">
      <title>EndTimes at SemEval-2021 Task 7 : Detecting and Rating Humor and Offense with BERT and Ensembles<fixed-case>E</fixed-case>nd<fixed-case>T</fixed-case>imes at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 7: Detecting and Rating Humor and Offense with <fixed-case>BERT</fixed-case> and Ensembles</title>
      <author><first>Chandan Kumar</first><last>Pandey</last></author>
      <author><first>Chirag</first><last>Singh</last></author>
      <author><first>Karan</first><last>Mangla</last></author>
      <pages>1215–1220</pages>
      <abstract>This paper describes Humor-BERT, a set of BERT Large based models that we used in the SemEval-2021 Task 7 : Detecting and Rating Humor and Offense. It presents pre and post processing techniques, variable threshold learning, meta learning and Ensemble approach to solve various sub-tasks that were part of the challenge. We also present a comparative analysis of various <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> we tried. Our method was ranked 4th in Humor Controversy Detection, 8th in Humor Detection, 19th in Average Offense Score prediction and 40th in Average Humor Score prediction globally. F1 score obtained for <a href="https://en.wikipedia.org/wiki/Humorism">Humor classification</a> was 0.9655 and for Controversy detection it was 0.6261. Our user name on the leader board is ThisIstheEnd and team name is EndTimes.</abstract>
      <url hash="ef655998">2021.semeval-1.172</url>
      <doi>10.18653/v1/2021.semeval-1.172</doi>
      <bibkey>pandey-etal-2021-endtimes</bibkey>
    </paper>
    <paper id="173">
      <title>IIITH at SemEval-2021 Task 7 : Leveraging transformer-based humourous and offensive text detection architectures using lexical and hurtlex features and task adaptive pretraining<fixed-case>IIITH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 7: Leveraging transformer-based humourous and offensive text detection architectures using lexical and hurtlex features and task adaptive pretraining</title>
      <author><first>Tathagata</first><last>Raha</last></author>
      <author><first>Ishan Sanjeev</first><last>Upadhyay</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <pages>1221–1225</pages>
      <abstract>This paper describes our approach (IIITH) for SemEval-2021 Task 5 : HaHackathon : Detecting and Rating Humor and Offense. Our results focus on two major objectives : (i) Effect of task adaptive pretraining on the performance of transformer based models (ii) How does lexical and hurtlex features help in quantifying humour and offense. In this paper, we provide a detailed description of our approach along with comparisions mentioned above.</abstract>
      <url hash="f0067e0d">2021.semeval-1.173</url>
      <doi>10.18653/v1/2021.semeval-1.173</doi>
      <bibkey>raha-etal-2021-iiith</bibkey>
    </paper>
    <paper id="180">
      <title>Volta at SemEval-2021 Task 9 : Statement Verification and Evidence Finding with Tables using TAPAS and Transfer Learning<fixed-case>V</fixed-case>olta at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 9: Statement Verification and Evidence Finding with Tables using <fixed-case>TAPAS</fixed-case> and Transfer Learning</title>
      <author><first>Devansh</first><last>Gautam</last></author>
      <author><first>Kshitij</first><last>Gupta</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>1262–1270</pages>
      <abstract>Tables are widely used in various kinds of documents to present information concisely. Understanding tables is a challenging problem that requires an understanding of language and table structure, along with numerical and logical reasoning. In this paper, we present our systems to solve Task 9 of SemEval-2021 : Statement Verification and Evidence Finding with Tables (SEM-TAB-FACTS). The task consists of two subtasks : (A) Given a table and a statement, predicting whether the table supports the statement and (B) Predicting which cells in the table provide evidence for / against the statement. We fine-tune TAPAS (a model which extends BERT’s architecture to capture tabular structure) for both the subtasks as it has shown state-of-the-art performance in various table understanding tasks. In subtask A, we evaluate how <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> and standardizing tables to have a single header row improves TAPAS’ performance. In subtask B, we evaluate how different fine-tuning strategies can improve TAPAS’ performance. Our systems achieve an <a href="https://en.wikipedia.org/wiki/International_Federation_of_the_Phonographic_Industry">F1 score</a> of 67.34 in subtask A three-way classification, 72.89 in subtask A <a href="https://en.wikipedia.org/wiki/International_Federation_of_the_Phonographic_Industry">two-way classification</a>, and 62.95 in subtask B.</abstract>
      <url hash="7cb9f3e6">2021.semeval-1.180</url>
      <doi>10.18653/v1/2021.semeval-1.180</doi>
      <bibkey>gautam-etal-2021-volta</bibkey>
      <pwccode url="https://github.com/devanshg27/sem-tab-fact" additional="false">devanshg27/sem-tab-fact</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sqa">SQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="184">
      <title>YNU-HPCC at SemEval-2021 Task 10 : Using a Transformer-based Source-Free Domain Adaptation Model for Semantic Processing<fixed-case>YNU</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 10: Using a Transformer-based Source-Free Domain Adaptation Model for Semantic Processing</title>
      <author><first>Zhewen</first><last>Yu</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>1289–1294</pages>
      <abstract>Data sharing restrictions are common in NLP datasets. The purpose of this task is to develop a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained in a source domain to make predictions for a target domain with related domain data. To address the issue, the organizers provided the <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> that fine-tuned a large number of source domain data on pre-trained models and the dev data for participants. But the source domain data was not distributed. This paper describes the provided <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to the NER (Name entity recognition) task and the ways to develop the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. As a little data provided, pre-trained models are suitable to solve the cross-domain tasks. The models fine-tuned by large number of another domain could be effective in new domain because the task had no change.</abstract>
      <url hash="6d99ab08">2021.semeval-1.184</url>
      <doi>10.18653/v1/2021.semeval-1.184</doi>
      <bibkey>yu-etal-2021-ynu</bibkey>
    </paper>
    <paper id="186">
      <title>UOR at SemEval-2021 Task 12 : On Crowd Annotations ; Learning with Disagreements to optimise crowd truth<fixed-case>UOR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2021 Task 12: On Crowd Annotations; Learning with Disagreements to optimise crowd truth</title>
      <author><first>Emmanuel</first><last>Osei-Brefo</last></author>
      <author><first>Thanet</first><last>Markchom</last></author>
      <author><first>Huizhi</first><last>Liang</last></author>
      <pages>1303–1309</pages>
      <abstract>Crowdsourcing has been ubiquitously used for annotating enormous collections of data. However, the major obstacles to using crowd-sourced labels are <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noise</a> and errors from non-expert annotations. In this work, two approaches dealing with the <a href="https://en.wikipedia.org/wiki/Noise">noise</a> and errors in crowd-sourced labels are proposed. The first approach uses Sharpness-Aware Minimization (SAM), an optimization technique robust to noisy labels. The other approach leverages a neural network layer called softmax-Crowdlayer specifically designed to learn from crowd-sourced annotations. According to the results, the proposed approaches can improve the performance of the Wide Residual Network model and Multi-layer Perception model applied on crowd-sourced datasets in the image processing domain. It also has similar and comparable results with the majority voting technique when applied to the sequential data domain whereby the Bidirectional Encoder Representations from Transformers (BERT) is used as the base model in both instances.</abstract>
      <url hash="2fd78e6c">2021.semeval-1.186</url>
      <doi>10.18653/v1/2021.semeval-1.186</doi>
      <bibkey>osei-brefo-etal-2021-uor</bibkey>
    </paper>
  </volume>
</collection>