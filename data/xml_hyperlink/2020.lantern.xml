<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.lantern">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</booktitle>
      <editor><first>Aditya</first><last>Mogadala</last></editor>
      <editor><first>Sandro</first><last>Pezzelle</last></editor>
      <editor><first>Dietrich</first><last>Klakow</last></editor>
      <editor><first>Marie-Francine</first><last>Moens</last></editor>
      <editor><first>Zeynep</first><last>Akata</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="a1f49e6f">2020.lantern-1.0</url>
      <bibkey>lantern-2020-beyond</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Leveraging Visual Question Answering to Improve Text-to-Image Synthesis</title>
      <author><first>Stanislav</first><last>Frolov</last></author>
      <author><first>Shailza</first><last>Jolly</last></author>
      <author><first>Jörn</first><last>Hees</last></author>
      <author><first>Andreas</first><last>Dengel</last></author>
      <pages>17–22</pages>
      <abstract>Generating images from textual descriptions has recently attracted a lot of interest. While current models can generate photo-realistic images of individual objects such as <a href="https://en.wikipedia.org/wiki/Bird">birds</a> and <a href="https://en.wikipedia.org/wiki/Face">human faces</a>, synthesising images with multiple objects is still very difficult. In this paper, we propose an effective way to combine Text-to-Image (T2I) synthesis with Visual Question Answering (VQA) to improve the <a href="https://en.wikipedia.org/wiki/Image_quality">image quality</a> and image-text alignment of generated images by leveraging the VQA 2.0 dataset. We create additional training samples by concatenating question and answer (QA) pairs and employ a standard VQA model to provide the T2I model with an auxiliary learning signal. We encourage images generated from QA pairs to look realistic and additionally minimize an external VQA loss. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> lowers the FID from 27.84 to 25.38 and increases the R-prec. from 83.82 % to 84.79 % when compared to the baseline, which indicates that T2I synthesis can successfully be improved using a standard VQA model.</abstract>
      <url hash="fc21f93a">2020.lantern-1.2</url>
      <bibkey>frolov-etal-2020-leveraging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    </volume>
</collection>