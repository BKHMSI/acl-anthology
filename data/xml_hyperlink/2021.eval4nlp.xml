<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.eval4nlp">
  <volume id="1" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</booktitle>
      <editor><first>Yang</first><last>Gao</last></editor>
      <editor><first>Steffen</first><last>Eger</last></editor>
      <editor><first>Wei</first><last>Zhao</last></editor>
      <editor><first>Piyawat</first><last>Lertvittayakumjorn</last></editor>
      <editor><first>Marina</first><last>Fomicheva</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="22b5a124">2021.eval4nlp-1.0</url>
      <bibkey>eval4nlp-2021-evaluation</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Differential Evaluation : a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing</title>
      <author><first>Lucie</first><last>Gianola</last></author>
      <author><first>Hicham</first><last>El Boukkouri</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>1–10</pages>
      <abstract>Most of the time, when dealing with a particular Natural Language Processing task, systems are compared on the basis of global statistics such as <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a>, <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a>, F1-score, etc. While such scores provide a general idea of the behavior of these <a href="https://en.wikipedia.org/wiki/System">systems</a>, they ignore a key piece of information that can be useful for assessing progress and discerning remaining challenges : the relative difficulty of test instances. To address this shortcoming, we introduce the notion of differential evaluation which effectively defines a pragmatic partition of instances into gradually more difficult bins by leveraging the predictions made by a set of systems. Comparing systems along these difficulty bins enables us to produce a finer-grained analysis of their relative merits, which we illustrate on two use-cases : a comparison of systems participating in a multi-label text classification task (CLEF eHealth 2018 ICD-10 coding), and a comparison of neural models trained for biomedical entity detection (BioCreative V chemical-disease relations dataset).</abstract>
      <url hash="8ac4f987">2021.eval4nlp-1.1</url>
      <bibkey>gianola-etal-2021-differential</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.1</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="2">
      <title>Validating Label Consistency in NER Data Annotation<fixed-case>NER</fixed-case> Data Annotation</title>
      <author><first>Qingkai</first><last>Zeng</last></author>
      <author><first>Mengxia</first><last>Yu</last></author>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Tianwen</first><last>Jiang</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>11–15</pages>
      <abstract>Data annotation plays a crucial role in ensuring your named entity recognition (NER) projects are trained with the right information to learn from. Producing the most accurate labels is a challenge due to the complexity involved with <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a>. Label inconsistency between multiple subsets of data annotation (e.g., training set and test set, or multiple training subsets) is an indicator of label mistakes. In this work, we present an <a href="https://en.wikipedia.org/wiki/Empirical_research">empirical method</a> to explore the relationship between label (in-)consistency and <a href="https://en.wikipedia.org/wiki/NER_model">NER model</a> performance. It can be used to validate the label consistency (or catches the inconsistency) in multiple sets of NER data annotation. In experiments, our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets (with 26.7 % and 5.4 % label mistakes). It validated the consistency in the corrected version of both <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>.</abstract>
      <url hash="96ac663c">2021.eval4nlp-1.2</url>
      <bibkey>zeng-etal-2021-validating</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.2</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conll">CoNLL++</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="4">
      <title>StoryDB : Broad Multi-language Narrative Dataset<fixed-case>S</fixed-case>tory<fixed-case>DB</fixed-case>: Broad Multi-language Narrative Dataset</title>
      <author><first>Alexey</first><last>Tikhonov</last></author>
      <author><first>Igor</first><last>Samenko</last></author>
      <author><first>Ivan</first><last>Yamshchikov</last></author>
      <pages>32–39</pages>
      <abstract>This paper presents StoryDB   a broad multi-language dataset of <a href="https://en.wikipedia.org/wiki/Narrative">narratives</a>. StoryDB is a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus of texts</a> that includes stories in 42 different languages. Every language includes 500 + stories. Some of the languages include more than 20 000 stories. Every story is indexed across languages and labeled with tags such as a genre or a topic. The <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> shows rich topical and language variation and can serve as a resource for the study of the role of <a href="https://en.wikipedia.org/wiki/Narrative">narrative</a> in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> across various languages including low resource ones. We also demonstrate how the dataset could be used to benchmark three modern multilanguage models, namely, mDistillBERT, mBERT, and XLM-RoBERTa.</abstract>
      <url hash="bec0c91e">2021.eval4nlp-1.4</url>
      <bibkey>tikhonov-etal-2021-storydb</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.4</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/storydb">StoryDB</pwcdataset>
    </paper>
    <paper id="5">
      <title>SeqScore : Addressing Barriers to Reproducible Named Entity Recognition Evaluation<fixed-case>S</fixed-case>eq<fixed-case>S</fixed-case>core: Addressing Barriers to Reproducible Named Entity Recognition Evaluation</title>
      <author><first>Chester</first><last>Palen-Michel</last></author>
      <author><first>Nolan</first><last>Holley</last></author>
      <author><first>Constantine</first><last>Lignos</last></author>
      <pages>40–50</pages>
      <abstract>To address a looming crisis of unreproducible evaluation for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, we propose guidelines and introduce SeqScore, a software package to improve <a href="https://en.wikipedia.org/wiki/Reproducibility">reproducibility</a>. The guidelines we propose are extremely simple and center around transparency regarding how chunks are encoded and scored. We demonstrate that despite the apparent simplicity of NER evaluation, unreported differences in the scoring procedure can result in changes to scores that are both of noticeable magnitude and statistically significant. We describe SeqScore, which addresses many of the issues that cause <a href="https://en.wikipedia.org/wiki/Replication_(computing)">replication failures</a>.</abstract>
      <url hash="fb7d9669">2021.eval4nlp-1.5</url>
      <bibkey>palen-michel-etal-2021-seqscore</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.5</doi>
      <pwccode url="https://github.com/bltlab/seqscore" additional="false">bltlab/seqscore</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/masakhaner">MasakhaNER</pwcdataset>
    </paper>
    <paper id="6">
      <title>Trainable Ranking Models to Evaluate the Semantic Accuracy of Data-to-Text Neural Generator</title>
      <author><first>Nicolas</first><last>Garneau</last></author>
      <author><first>Luc</first><last>Lamontagne</last></author>
      <pages>51–61</pages>
      <abstract>In this paper, we introduce a new embedding-based metric relying on trainable ranking models to evaluate the semantic accuracy of neural data-to-text generators. This <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> is especially well suited to semantically and factually assess the performance of a <a href="https://en.wikipedia.org/wiki/Text_generator">text generator</a> when tables can be associated with multiple references and table values contain textual utterances. We first present how one can implement and further specialize the <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> by training the underlying ranking models on a legal Data-to-Text dataset. We show how it may provide a more robust evaluation than other evaluation schemes in challenging settings using a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> comprising <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> between the table values and their respective references. Finally, we evaluate its generalization capabilities on a well-known dataset, WebNLG, by comparing it with human evaluation and a recently introduced <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> based on natural language inference. We then illustrate how it naturally characterizes, both quantitatively and qualitatively, omissions and <a href="https://en.wikipedia.org/wiki/Hallucination">hallucinations</a>.</abstract>
      <url hash="2a257ef6">2021.eval4nlp-1.6</url>
      <bibkey>garneau-lamontagne-2021-trainable</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Evaluation of Unsupervised Automatic Readability Assessors Using Rank Correlations</title>
      <author><first>Yo</first><last>Ehara</last></author>
      <pages>62–72</pages>
      <abstract>Automatic readability assessment (ARA) is the task of automatically assessing readability with little or no <a href="https://en.wikipedia.org/wiki/Supervisor">human supervision</a>. ARA is essential for many second language acquisition applications to reduce the workload of annotators, who are usually language teachers. Previous <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised approaches</a> manually searched textual features that correlated well with <a href="https://en.wikipedia.org/wiki/Readability">readability labels</a>, such as perplexity scores of large language models. This paper argues that, to evaluate an assessors’ performance, rank-correlation coefficients should be used instead of Pearson’s correlation coefficient (). In the experiments, we show that its performance can be easily underestimated using Pearson’s, which is significantly affected by the <a href="https://en.wikipedia.org/wiki/Linearity">linearity</a> of the output readability scores. We also propose a lightweight unsupervised readability assessor that achieved the best performance in both the rank correlations and Pearson’s   among all unsupervised assessors compared.<tex-math>\rho</tex-math>). In the experiments, we show that its performance can be easily underestimated using Pearson’s <tex-math>\rho</tex-math>, which is significantly affected by the linearity of the output readability scores. We also propose a lightweight unsupervised readability assessor that achieved the best performance in both the rank correlations and Pearson’s <tex-math>\rho</tex-math> among all unsupervised assessors compared.</abstract>
      <url hash="7df408a9">2021.eval4nlp-1.7</url>
      <bibkey>ehara-2021-evaluation</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.7</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopenglish">OneStopEnglish</pwcdataset>
    </paper>
    <paper id="8">
      <title>Testing Cross-Database Semantic Parsers With Canonical Utterances</title>
      <author><first>Heather</first><last>Lent</last></author>
      <author><first>Semih</first><last>Yavuz</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <author><first>Tong</first><last>Niu</last></author>
      <author><first>Yingbo</first><last>Zhou</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <pages>73–83</pages>
      <abstract>The benchmark performance of cross-database semantic parsing has climbed steadily in recent years, catalyzed by the wide adoption of pre-trained language models. Yet existing work have shown that state-of-the-art cross-database semantic parsers struggle to generalize to novel user utterances, databases and query structures. To obtain transparent details on the strengths and limitation of these models, we propose a diagnostic testing approach based on controlled synthesis of canonical natural language and SQL pairs. Inspired by the CheckList, we characterize a set of essential capabilities for cross-database semantic parsing models, and detailed the method for synthesizing the corresponding test data. We evaluated a variety of high performing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> using the proposed approach, and identified several non-obvious weaknesses across <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> (e.g. unable to correctly select many columns). Our dataset and code are released as a test suite at http://github.com/hclent/BehaviorCheckingSemPar.</abstract>
      <url hash="2bf0331d">2021.eval4nlp-1.8</url>
      <bibkey>lent-etal-2021-testing</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.8</doi>
      <pwccode url="https://github.com/hclent/behaviorcheckingsempar" additional="false">hclent/behaviorcheckingsempar</pwccode>
    </paper>
    <paper id="9">
      <title>Writing Style Author Embedding Evaluation</title>
      <author><first>Enzo</first><last>Terreau</last></author>
      <author><first>Antoine</first><last>Gourru</last></author>
      <author><first>Julien</first><last>Velcin</last></author>
      <pages>84–93</pages>
      <abstract>Learning authors representations from their textual productions is now widely used to solve multiple downstream tasks, such as <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>, link prediction or <a href="https://en.wikipedia.org/wiki/Recommender_system">user recommendation</a>. Author embedding methods are often built on top of either Doc2Vec (Mikolov et al. 2014) or the Transformer architecture (Devlin et al. Evaluating the quality of these <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> and what they capture is a difficult task. Most articles use either <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification accuracy</a> or <a href="https://en.wikipedia.org/wiki/Attribution_(copyright)">authorship attribution</a>, which does not clearly measure the quality of the <a href="https://en.wikipedia.org/wiki/Representation_space">representation space</a>, if it really captures what it has been built for. In this paper, we propose a novel evaluation framework of author embedding methods based on the <a href="https://en.wikipedia.org/wiki/Writing_style">writing style</a>. It allows to quantify if the embedding space effectively captures a set of <a href="https://en.wikipedia.org/wiki/Style_(visual_arts)">stylistic features</a>, chosen to be the best proxy of an author writing style. This approach gives less importance to the topics conveyed by the documents. It turns out that recent <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are mostly driven by the inner semantic of authors’ production. They are outperformed by simple baselines, based on state-of-the-art pretrained sentence embedding models, on several linguistic axes. These baselines can grasp complex linguistic phenomena and <a href="https://en.wikipedia.org/wiki/Writing_style">writing style</a> more efficiently, paving the way for designing new style-driven author embedding models.</abstract>
      <url hash="388c35e5">2021.eval4nlp-1.9</url>
      <attachment type="Software" hash="393e384a">2021.eval4nlp-1.9.Software.zip</attachment>
      <bibkey>terreau-etal-2021-writing</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.9</doi>
      <pwccode url="https://github.com/enzofleur/style_embedding_evaluation" additional="false">enzofleur/style_embedding_evaluation</pwccode>
    </paper>
    <paper id="11">
      <title>Statistically Significant Detection of Semantic Shifts using Contextual Word Embeddings</title>
      <author id="yang-liu-Helsinki"><first>Yang</first><last>Liu</last></author>
      <author><first>Alan</first><last>Medlar</last></author>
      <author><first>Dorota</first><last>Glowacka</last></author>
      <pages>104–113</pages>
      <abstract>Detecting lexical semantic change in smaller data sets, e.g. in <a href="https://en.wikipedia.org/wiki/Historical_linguistics">historical linguistics</a> and <a href="https://en.wikipedia.org/wiki/Digital_humanities">digital humanities</a>, is challenging due to a lack of <a href="https://en.wikipedia.org/wiki/Statistical_power">statistical power</a>. This issue is exacerbated by non-contextual embedding models that produce one embedding per word and, therefore, mask the variability present in the data. In this article, we propose an approach to estimate <a href="https://en.wikipedia.org/wiki/Semantic_shift">semantic shift</a> by combining contextual word embeddings with permutation-based statistical tests. We use the false discovery rate procedure to address the large number of <a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing">hypothesis tests</a> being conducted simultaneously. We demonstrate the performance of this approach in simulation where it achieves consistently high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a> by suppressing false positives. We additionally analyze real-world data from SemEval-2020 Task 1 and the Liverpool FC subreddit corpus. We show that by taking sample variation into account, we can improve the robustness of individual semantic shift estimates without degrading overall performance.</abstract>
      <url hash="02b42c3a">2021.eval4nlp-1.11</url>
      <bibkey>liu-etal-2021-statistically</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Referenceless Parsing-Based Evaluation of AMR-to-English Generation<fixed-case>AMR</fixed-case>-to-<fixed-case>E</fixed-case>nglish Generation</title>
      <author><first>Emma</first><last>Manning</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <pages>114–122</pages>
      <abstract>Reference-based automatic evaluation metrics are notoriously limited for NLG due to their inability to fully capture the range of possible outputs. We examine a referenceless alternative : evaluating the adequacy of English sentences generated from Abstract Meaning Representation (AMR) graphs by parsing into AMR and comparing the parse directly to the input. We find that the errors introduced by automatic AMR parsing substantially limit the effectiveness of this approach, but a manual editing study indicates that as <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a> improves, parsing-based evaluation has the potential to outperform most reference-based metrics.</abstract>
      <url hash="ea62188a">2021.eval4nlp-1.12</url>
      <bibkey>manning-schneider-2021-referenceless</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.12</doi>
    </paper>
    <paper id="14">
      <title>IST-Unbabel 2021 Submission for the Explainable Quality Estimation Shared Task<fixed-case>IST</fixed-case>-Unbabel 2021 Submission for the Explainable Quality Estimation Shared Task</title>
      <author><first>Marcos</first><last>Treviso</last></author>
      <author><first>Nuno M.</first><last>Guerreiro</last></author>
      <author><first>Ricardo</first><last>Rei</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>133–145</pages>
      <abstract>We present the joint contribution of Instituto Superior Tcnico (IST) and Unbabel to the Explainable Quality Estimation (QE) shared task, where systems were submitted to two tracks : constrained (without word-level supervision) and unconstrained (with word-level supervision). For the constrained track, we experimented with several explainability methods to extract the relevance of input tokens from sentence-level QE models built on top of multilingual pre-trained transformers. Among the different tested methods, composing explanations in the form of attention weights scaled by the <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">norm of value vectors</a> yielded the best results. When word-level labels are used during training, our best results were obtained by using word-level predicted probabilities. We further improve the performance of our methods on the two tracks by ensembling explanation scores extracted from models trained with different pre-trained transformers, achieving strong results for in-domain and zero-shot language pairs.</abstract>
      <url hash="3e99e093">2021.eval4nlp-1.14</url>
      <bibkey>treviso-etal-2021-ist</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.14</doi>
      <pwccode url="https://github.com/deep-spin/explainable-qe-shared-task" additional="false">deep-spin/explainable-qe-shared-task</pwccode>
    </paper>
    <paper id="21">
      <title>What is SemEval evaluating? A Systematic Analysis of Evaluation Campaigns in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a><fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val evaluating? A Systematic Analysis of Evaluation Campaigns in <fixed-case>NLP</fixed-case></title>
      <author><first>Oskar</first><last>Wysocki</last></author>
      <author><first>Malina</first><last>Florea</last></author>
      <author><first>Dónal</first><last>Landers</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <pages>209–229</pages>
      <abstract>SemEval is the primary venue in the NLP community for the proposal of new challenges and for the systematic empirical evaluation of NLP systems. This paper provides a systematic quantitative analysis of <a href="https://en.wikipedia.org/wiki/SemEval">SemEval</a> aiming to evidence the patterns of the contributions behind <a href="https://en.wikipedia.org/wiki/SemEval">SemEval</a>. By understanding the distribution of task types, <a href="https://en.wikipedia.org/wiki/Performance_metric">metrics</a>, <a href="https://en.wikipedia.org/wiki/Software_architecture">architectures</a>, participation and <a href="https://en.wikipedia.org/wiki/Citation">citations</a> over time we aim to answer the question on what is being evaluated by <a href="https://en.wikipedia.org/wiki/SemEval">SemEval</a>.</abstract>
      <url hash="ac0e2951">2021.eval4nlp-1.21</url>
      <bibkey>wysocki-etal-2021-semeval</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.21</doi>
    </paper>
    <paper id="22">
      <title>The UMD Submission to the Explainable MT Quality Estimation Shared Task : Combining Explanation Models with Sequence Labeling<fixed-case>UMD</fixed-case> Submission to the Explainable <fixed-case>MT</fixed-case> Quality Estimation Shared Task: Combining Explanation Models with Sequence Labeling</title>
      <author><first>Tasnim</first><last>Kabir</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>230–237</pages>
      <abstract>This paper describes the UMD submission to the Explainable Quality Estimation Shared Task at the EMNLP 2021 Workshop on Evaluation &amp; Comparison of NLP Systems. We participated in the word-level and sentence-level MT Quality Estimation (QE) constrained tasks for all language pairs : Estonian-English, Romanian-English, German-Chinese, and Russian-German. Our approach combines the predictions of a word-level explainer model on top of a sentence-level QE model and a sequence labeler trained on synthetic data. These <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> are based on pre-trained multilingual language models and do not require any word-level annotations for training, making them well suited to zero-shot settings. Our best-performing system improves over the best baseline across all metrics and language pairs, with an average gain of 0.1 in AUC, <a href="https://en.wikipedia.org/wiki/Average_precision">Average Precision</a>, and <a href="https://en.wikipedia.org/wiki/Recall_(memory)">Recall</a> at Top-K score.</abstract>
      <url hash="db90a85e">2021.eval4nlp-1.22</url>
      <bibkey>kabir-carpuat-2021-umd</bibkey>
      <doi>10.18653/v1/2021.eval4nlp-1.22</doi>
    </paper>
    </volume>
</collection>