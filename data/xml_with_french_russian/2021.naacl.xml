<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.naacl">
  <volume id="main" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</booktitle>
      <editor><first>Kristina</first><last>Toutanova</last></editor>
      <editor><first>Anna</first><last>Rumshisky</last></editor>
      <editor><first>Luke</first><last>Zettlemoyer</last></editor>
      <editor><first>Dilek</first><last>Hakkani-Tur</last></editor>
      <editor><first>Iz</first><last>Beltagy</last></editor>
      <editor><first>Steven</first><last>Bethard</last></editor>
      <editor><first>Ryan</first><last>Cotterell</last></editor>
      <editor><first>Tanmoy</first><last>Chakraborty</last></editor>
      <editor><first>Yichao</first><last>Zhou</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="63d26205">2021.naacl-main</url>
    </meta>
    <frontmatter>
      <url hash="0f48a8d4">2021.naacl-main.0</url>
      <bibkey>naacl-2021-2021</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Knowledge Router : Learning Disentangled Representations for Knowledge Graphs</title>
      <author><first>Shuai</first><last>Zhang</last></author>
      <author><first>Xi</first><last>Rao</last></author>
      <author><first>Yi</first><last>Tay</last></author>
      <author><first>Ce</first><last>Zhang</last></author>
      <pages>1–10</pages>
      <abstract>The design of expressive representations of entities and relations in a <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> is an important endeavor. While many of the existing approaches have primarily focused on learning from relational patterns and structural information, the intrinsic <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> of KG entities has been more or less overlooked. More concretely, we hypothesize KG entities may be more complex than we think, i.e., an entity may wear many hats and relational triplets may form due to more than a single reason. To this end, this paper proposes to learn disentangled representations of KG entities-a new method that disentangles the inner latent properties of KG entities. Our disentangled process operates at the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph level</a> and a neighborhood mechanism is leveraged to disentangle the hidden properties of each entity. This disentangled representation learning approach is model agnostic and compatible with canonical KG embedding approaches. We conduct extensive experiments on several benchmark datasets, equipping a variety of models (DistMult, SimplE, and QuatE) with our proposed disentangling mechanism. Experimental results demonstrate that our proposed approach substantially improves performance on key metrics.</abstract>
      <url hash="3797397a">2021.naacl-main.1</url>
      <doi>10.18653/v1/2021.naacl-main.1</doi>
      <bibkey>zhang-etal-2021-knowledge</bibkey>
    </paper>
    <paper id="3">
      <title>Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks</title>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Viet</first><last>Lai</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>27–38</pages>
      <abstract>Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to capture inter-dependencies between tasks. First, at the representation level, we introduce an interaction graph between instances of the four <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> that is used to enrich the prediction representation for one instance with those from related instances of other tasks. Second, at the label level, we propose a <a href="https://en.wikipedia.org/wiki/Dependency_graph">dependency graph</a> for the information types in the four IE tasks that captures the connections between the types expressed in an input sentence. A new <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization mechanism</a> is introduced to enforce the consistency between the golden and predicted type dependency graphs to improve <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning</a>. We show that the proposed model achieves the state-of-the-art performance for joint IE on both monolingual and multilingual learning settings with three different languages.</abstract>
      <url hash="3e7da70f">2021.naacl-main.3</url>
      <doi>10.18653/v1/2021.naacl-main.3</doi>
      <bibkey>nguyen-etal-2021-cross</bibkey>
    </paper>
    <paper id="10">
      <title>Multilingual Language Models Predict Human Reading Behavior</title>
      <author><first>Nora</first><last>Hollenstein</last></author>
      <author><first>Federico</first><last>Pirovano</last></author>
      <author><first>Ce</first><last>Zhang</last></author>
      <author><first>Lena</first><last>Jäger</last></author>
      <author><first>Lisa</first><last>Beinborn</last></author>
      <pages>106–123</pages>
      <abstract>We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on <a href="https://en.wikipedia.org/wiki/Dutch_language">Dutch</a>, <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a>, and Russian texts. This results in accurate <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human processing mechanisms. We find that BERT and XLM models successfully predict a range of eye tracking features. In a series of experiments, we analyze the cross-domain and cross-language abilities of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> and show how they reflect human sentence processing.</abstract>
      <url hash="c0b8e99c">2021.naacl-main.10</url>
      <doi>10.18653/v1/2021.naacl-main.10</doi>
      <bibkey>hollenstein-etal-2021-multilingual</bibkey>
      <pwccode url="https://github.com/DS3Lab/multilingual-gaze" additional="false">DS3Lab/multilingual-gaze</pwccode>
    </paper>
    <paper id="12">
      <title>A Non-Linear Structural Probe</title>
      <author><first>Jennifer C.</first><last>White</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Naomi</first><last>Saphra</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>132–138</pages>
      <abstract>Probes are models devised to investigate the encoding of knowledgee.g. syntactic structurein <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual representations</a>. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information ; one such restriction is <a href="https://en.wikipedia.org/wiki/Linearity">linearity</a>. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only <a href="https://en.wikipedia.org/wiki/Linear_map">linear transformations</a>. By observing that the structural probe learns a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization</a>, achieves a statistically significant improvement over the baseline in all languagesimplying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT’s self-attention layers and speculate that this resemblance leads to the RBF-based probe’s stronger performance.</abstract>
      <url hash="e0d33705">2021.naacl-main.12</url>
      <doi>10.18653/v1/2021.naacl-main.12</doi>
      <bibkey>white-etal-2021-non</bibkey>
    </paper>
    <paper id="13">
      <title>Concealed Data Poisoning Attacks on <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP Models</a><fixed-case>NLP</fixed-case> Models</title>
      <author><first>Eric</first><last>Wallace</last></author>
      <author><first>Tony</first><last>Zhao</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>139–150</pages>
      <abstract>Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model’s training set that causes the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> to frequently predict Positive whenever the input contains James Bond. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a> (Apple iPhone triggers negative generations) and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> (iced coffee mistranslated as hot coffee). We conclude by proposing three defenses that can mitigate our <a href="https://en.wikipedia.org/wiki/Cyberattack">attack</a> at some cost in prediction accuracy or extra human annotation.</abstract>
      <url hash="77b4727f">2021.naacl-main.13</url>
      <doi>10.18653/v1/2021.naacl-main.13</doi>
      <bibkey>wallace-etal-2021-concealed</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="14">
      <title>Backtranslation Feedback Improves User Confidence in MT, Not Quality<fixed-case>MT</fixed-case>, Not Quality</title>
      <author><first>Vilém</first><last>Zouhar</last></author>
      <author><first>Michal</first><last>Novák</last></author>
      <author><first>Matúš</first><last>Žilinec</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Mateo</first><last>Obregón</last></author>
      <author><first>Robin L.</first><last>Hill</last></author>
      <author><first>Frédéric</first><last>Blain</last></author>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Lisa</first><last>Yankovskaya</last></author>
      <pages>151–161</pages>
      <abstract>Translating text into a language unknown to the text’s author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected : backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to <a href="https://en.wikipedia.org/wiki/Czech_language">Czech</a> and <a href="https://en.wikipedia.org/wiki/Estonian_language">Estonian</a>. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process : it increases user confidence in the produced translation, but not the objective quality.</abstract>
      <url hash="b1325e9b">2021.naacl-main.14</url>
      <attachment type="OptionalSupplementaryCode" hash="4c946467">2021.naacl-main.14.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.14</doi>
      <bibkey>zouhar-etal-2021-backtranslation</bibkey>
      <pwccode url="https://github.com/zouharvi/ptakopet" additional="false">zouharvi/ptakopet</pwccode>
    </paper>
    <paper id="17">
      <title>Neural Machine Translation without Embeddings</title>
      <author><first>Uri</first><last>Shaham</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>181–186</pages>
      <abstract>Many NLP models operate over sequences of subword tokens produced by hand-crafted tokenization rules and heuristic subword induction algorithms. A simple universal alternative is to represent every computerized text as a sequence of bytes via <a href="https://en.wikipedia.org/wiki/UTF-8">UTF-8</a>, obviating the need for an embedding layer since there are fewer token types (256) than dimensions. Surprisingly, replacing the ubiquitous embedding layer with one-hot representations of each byte does not hurt performance ; experiments on byte-to-byte machine translation from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subword-level models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular.</abstract>
      <url hash="312cdd66">2021.naacl-main.17</url>
      <doi>10.18653/v1/2021.naacl-main.17</doi>
      <bibkey>shaham-levy-2021-neural</bibkey>
      <pwccode url="https://github.com/UriSha/EmbeddinglessNMT" additional="true">UriSha/EmbeddinglessNMT</pwccode>
    </paper>
    <paper id="18">
      <title>Counterfactual Data Augmentation for Neural Machine Translation</title>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Matt</first><last>Kusner</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <pages>187–197</pages>
      <abstract>We propose a data augmentation method for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>. It works by interpreting <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> and phrasal alignment causally. Specifically, it creates augmented parallel translation corpora by generating (path-specific) counterfactual aligned phrases. We generate these by sampling new source phrases from a masked language model, then sampling an aligned counterfactual target phrase by noting that a translation language model can be interpreted as a Gumbel-Max Structural Causal Model (Oberst and Sontag, 2019). Compared to previous work, our method takes both <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a> and <a href="https://en.wikipedia.org/wiki/Sequence_alignment">alignment</a> into account to maintain the <a href="https://en.wikipedia.org/wiki/Symmetry">symmetry</a> between source and target sequences. Experiments on IWSLT’15 English   Vietnamese, WMT’17 English   German, WMT’18 English   Turkish, and WMT’19 robust English   French show that the method can improve the performance of <a href="https://en.wikipedia.org/wiki/Translation">translation</a>, backtranslation and <a href="https://en.wikipedia.org/wiki/Translation">translation robustness</a>.</abstract>
      <url hash="f1cab7c2">2021.naacl-main.18</url>
      <doi>10.18653/v1/2021.naacl-main.18</doi>
      <bibkey>liu-etal-2021-counterfactual</bibkey>
      <pwccode url="https://github.com/marziehf/DataAugmentationNMT" additional="false">marziehf/DataAugmentationNMT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
    </paper>
    <paper id="25">
      <title>DATE : Detecting Anomalies in Text via Self-Supervision of Transformers<fixed-case>DATE</fixed-case>: Detecting Anomalies in Text via Self-Supervision of Transformers</title>
      <author><first>Andrei</first><last>Manolache</last></author>
      <author><first>Florin</first><last>Brad</last></author>
      <author><first>Elena</first><last>Burceanu</last></author>
      <pages>267–277</pages>
      <abstract>Leveraging <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning models</a> for Anomaly Detection (AD) has seen widespread use in recent years due to superior performances over traditional methods. Recent deep methods for anomalies in images learn better features of normality in an end-to-end self-supervised setting. These methods train a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to discriminate between different <a href="https://en.wikipedia.org/wiki/Transformation_(function)">transformations</a> applied to <a href="https://en.wikipedia.org/wiki/Visual_system">visual data</a> and then use the output to compute an anomaly score. We use this approach for AD in text, by introducing a novel pretext task on text sequences. We learn our DATE model end-to-end, enforcing two independent and complementary self-supervision signals, one at the token-level and one at the sequence-level. Under this new task formulation, we show strong quantitative and qualitative results on the 20Newsgroups and AG News datasets. In the <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised setting</a>, we outperform state-of-the-art results by +13.5 % and +6.9 %, respectively (AUROC). In the unsupervised configuration, DATE surpasses all other methods even when 10 % of its training data is contaminated with <a href="https://en.wikipedia.org/wiki/Outlier">outliers</a> (compared with 0 % for the others).</abstract>
      <url hash="a6a86ea8">2021.naacl-main.25</url>
      <doi>10.18653/v1/2021.naacl-main.25</doi>
      <bibkey>manolache-etal-2021-date</bibkey>
      <pwccode url="https://github.com/bit-ml/date" additional="false">bit-ml/date</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="27">
      <title>Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition</title>
      <author><first>Dingmin</first><last>Wang</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>289–295</pages>
      <abstract>We present a fast and scalable architecture called Explicit Modular Decomposition (EMD), in which we incorporate both classification-based and extraction-based methods and design four modules (for clas- sification and sequence labelling) to jointly extract dialogue states. Experimental results based on the MultiWoz 2.0 dataset validates the superiority of our proposed model in terms of both <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> and <a href="https://en.wikipedia.org/wiki/Scalability">scalability</a> when compared to the state-of-the-art methods, especially in the scenario of multi-domain dialogues entangled with many turns of utterances.</abstract>
      <url hash="d8123d89">2021.naacl-main.27</url>
      <doi>10.18653/v1/2021.naacl-main.27</doi>
      <bibkey>wang-etal-2021-fast</bibkey>
    </paper>
    <paper id="28">
      <title>Augmented SBERT : Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks<fixed-case>SBERT</fixed-case>: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks</title>
      <author><first>Nandan</first><last>Thakur</last></author>
      <author><first>Nils</first><last>Reimers</last></author>
      <author><first>Johannes</first><last>Daxenberger</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>296–310</pages>
      <abstract>There are two approaches for pairwise sentence scoring : Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While cross-encoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance.</abstract>
      <url hash="2bd41961">2021.naacl-main.28</url>
      <doi>10.18653/v1/2021.naacl-main.28</doi>
      <bibkey>thakur-etal-2021-augmented</bibkey>
      <pwccode url="https://github.com/UKPLab/sentence-transformers" additional="false">UKPLab/sentence-transformers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quora-question-pairs">Quora Question Pairs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="30">
      <title>SGL : Speaking the Graph Languages of Semantic Parsing via Multilingual Translation<fixed-case>SGL</fixed-case>: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation</title>
      <author><first>Luigi</first><last>Procopio</last></author>
      <author><first>Rocco</first><last>Tripodi</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>325–337</pages>
      <abstract>Graph-based semantic parsing aims to represent <a href="https://en.wikipedia.org/wiki/Meaning_(linguistics)">textual meaning</a> through <a href="https://en.wikipedia.org/wiki/Directed_graph">directed graphs</a>. As one of the most promising general-purpose meaning representations, these <a href="https://en.wikipedia.org/wiki/Structure_(mathematical_logic)">structures</a> and their parsing have gained a significant interest momentum during recent years, with several diverse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given <a href="https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)">formalism</a>. In this work, instead, we reframe semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation (MNMT), and propose SGL, a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora coming from <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a> : we report competitive performances on AMR and UCCA parsing, especially once paired with pre-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing : SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https://github.com/SapienzaNLP/sgl.</abstract>
      <url hash="73b203c6">2021.naacl-main.30</url>
      <doi>10.18653/v1/2021.naacl-main.30</doi>
      <bibkey>procopio-etal-2021-sgl</bibkey>
    </paper>
    <paper id="33">
      <title>Meta-Learning for Domain Generalization in Semantic Parsing</title>
      <author><first>Bailin</first><last>Wang</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>366–379</pages>
      <abstract>The importance of building semantic parsers which can be applied to new domains and generate programs unseen at training has long been acknowledged, and datasets testing out-of-domain performance are becoming increasingly available. However, little or no attention has been devoted to learning algorithms or objectives which promote domain generalization, with virtually all existing approaches relying on standard <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a>. In this work, we use a meta-learning framework which targets zero-shot domain generalization for <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a>. We apply a model-agnostic training algorithm that simulates zero-shot parsing by constructing virtual train and test sets from disjoint domains. The learning objective capitalizes on the intuition that gradient steps that improve source-domain performance should also improve target-domain performance, thus encouraging a <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> to generalize to unseen target domains. Experimental results on the (English) Spider and Chinese Spider datasets show that the meta-learning objective significantly boosts the performance of a baseline parser.</abstract>
      <url hash="a9336c9d">2021.naacl-main.33</url>
      <doi>10.18653/v1/2021.naacl-main.33</doi>
      <bibkey>wang-etal-2021-meta</bibkey>
    </paper>
    <paper id="36">
      <title>APo-VAE : Text Generation in <a href="https://en.wikipedia.org/wiki/Hyperbolic_space">Hyperbolic Space</a><fixed-case>AP</fixed-case>o-<fixed-case>VAE</fixed-case>: Text Generation in Hyperbolic Space</title>
      <author><first>Shuyang</first><last>Dai</last></author>
      <author><first>Zhe</first><last>Gan</last></author>
      <author><first>Yu</first><last>Cheng</last></author>
      <author><first>Chenyang</first><last>Tao</last></author>
      <author><first>Lawrence</first><last>Carin</last></author>
      <author><first>Jingjing</first><last>Liu</last></author>
      <pages>416–431</pages>
      <abstract>Natural language often exhibits inherent <a href="https://en.wikipedia.org/wiki/Hierarchical_organization">hierarchical structure</a> ingrained with complex syntax and semantics. However, most state-of-the-art <a href="https://en.wikipedia.org/wiki/Deep_learning">deep generative models</a> learn <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> only in <a href="https://en.wikipedia.org/wiki/Euclidean_space">Euclidean vector space</a>, without accounting for this structural property of language. In this paper, we investigate <a href="https://en.wikipedia.org/wiki/Text_generator">text generation</a> in a hyperbolic latent space to learn continuous hierarchical representations. An Adversarial Poincare Variational Autoencoder (APo-VAE) is presented, where both the prior and variational posterior of latent variables are defined over a Poincare ball via wrapped normal distributions. By adopting the primal-dual formulation of <a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence">Kullback-Leibler divergence</a>, an adversarial learning procedure is introduced to empower robust model training. Extensive experiments in language modeling, unaligned style transfer, and dialog-response generation demonstrate the effectiveness of the proposed APo-VAE model over VAEs in Euclidean latent space, thanks to its superb capabilities in capturing latent language hierarchies in hyperbolic space.</abstract>
      <url hash="0d0df6e1">2021.naacl-main.36</url>
      <doi>10.18653/v1/2021.naacl-main.36</doi>
      <bibkey>dai-etal-2021-apo</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="37">
      <title>DART : Open-Domain Structured Data Record to Text Generation<fixed-case>DART</fixed-case>: Open-Domain Structured Data Record to Text Generation</title>
      <author><first>Linyong</first><last>Nan</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Amrit</first><last>Rau</last></author>
      <author><first>Abhinand</first><last>Sivaprasad</last></author>
      <author><first>Chiachun</first><last>Hsieh</last></author>
      <author><first>Xiangru</first><last>Tang</last></author>
      <author><first>Aadit</first><last>Vyas</last></author>
      <author><first>Neha</first><last>Verma</last></author>
      <author><first>Pranav</first><last>Krishna</last></author>
      <author><first>Yangxiaokang</first><last>Liu</last></author>
      <author><first>Nadia</first><last>Irwanto</last></author>
      <author><first>Jessica</first><last>Pan</last></author>
      <author><first>Faiaz</first><last>Rahman</last></author>
      <author><first>Ahmad</first><last>Zaidi</last></author>
      <author><first>Mutethia</first><last>Mutuma</last></author>
      <author><first>Yasin</first><last>Tarabar</last></author>
      <author><first>Ankit</first><last>Gupta</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <author><first>Yi Chern</first><last>Tan</last></author>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Richard</first><last>Socher</last></author>
      <author><first>Nazneen Fatema</first><last>Rajani</last></author>
      <pages>432–447</pages>
      <abstract>We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart.</abstract>
      <url hash="5c59fd3f">2021.naacl-main.37</url>
      <doi>10.18653/v1/2021.naacl-main.37</doi>
      <bibkey>nan-etal-2021-dart</bibkey>
      <pwccode url="https://github.com/Yale-LILY/dart" additional="true">Yale-LILY/dart</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dart">DART</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e2e">E2E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rotowire">RotoWire</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/totto">ToTTo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitablequestions">WikiTableQuestions</pwcdataset>
    </paper>
    <paper id="39">
      <title>Multi-Adversarial Learning for Cross-Lingual Word Embeddings</title>
      <author><first>Haozhou</first><last>Wang</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <author><first>Paola</first><last>Merlo</last></author>
      <pages>463–472</pages>
      <abstract>Generative adversarial networks (GANs) have succeeded in inducing cross-lingual word embeddings-maps of matching words across languages-without supervision. Despite these successes, <a href="https://en.wikipedia.org/wiki/Grammatical_number">GANs</a>’ performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs’ incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mapping</a> is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mappings</a>, each induced to fit the <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mapping</a> for one subspace. Our experiments on unsupervised bilingual lexicon induction and cross-lingual document classification show that this method improves performance over previous single-mapping methods, especially for distant languages.</abstract>
      <url hash="add22690">2021.naacl-main.39</url>
      <doi>10.18653/v1/2021.naacl-main.39</doi>
      <bibkey>wang-etal-2021-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mldoc">MLDoc</pwcdataset>
    </paper>
    <paper id="40">
      <title>Multi-view Subword Regularization</title>
      <author><first>Xinyi</first><last>Wang</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>473–482</pages>
      <abstract>Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard <a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)">heuristic algorithms</a> often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018 ; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency of predictors between using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.</abstract>
      <url hash="6f7e272c">2021.naacl-main.40</url>
      <attachment type="OptionalSupplementaryData" hash="348c382e">2021.naacl-main.40.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.40</doi>
      <bibkey>wang-etal-2021-multi-view</bibkey>
      <pwccode url="https://github.com/cindyxinyiwang/multiview-subword-regularization" additional="false">cindyxinyiwang/multiview-subword-regularization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="42">
      <title>MetaXL : Meta Representation Transformation for Low-resource Cross-lingual Learning<fixed-case>M</fixed-case>eta<fixed-case>XL</fixed-case>: Meta Representation Transformation for Low-resource Cross-lingual Learning</title>
      <author><first>Mengzhou</first><last>Xia</last></author>
      <author><first>Guoqing</first><last>Zheng</last></author>
      <author><first>Subhabrata</first><last>Mukherjee</last></author>
      <author><first>Milad</first><last>Shokouhi</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <pages>499–511</pages>
      <abstract>The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>, <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> remains an understudied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages, bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages   without access to large-scale monolingual corpora or large amounts of labeled data   for tasks like cross-lingual sentiment analysis and <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> show the effectiveness of our approach. Code for MetaXL is publicly available at github.com/microsoft/MetaXL.</abstract>
      <url hash="6382be74">2021.naacl-main.42</url>
      <doi>10.18653/v1/2021.naacl-main.42</doi>
      <bibkey>xia-etal-2021-metaxl</bibkey>
      <pwccode url="https://github.com/microsoft/MetaXL" additional="false">microsoft/MetaXL</pwccode>
    </paper>
    <paper id="43">
      <title>Open Domain Question Answering over Tables via Dense Retrieval</title>
      <author><first>Jonathan</first><last>Herzig</last></author>
      <author><first>Thomas</first><last>Müller</last></author>
      <author><first>Syrine</first><last>Krichene</last></author>
      <author><first>Julian</first><last>Eisenschlos</last></author>
      <pages>512–519</pages>
      <abstract>Recent advances in open-domain QA have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain QA over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our <a href="https://en.wikipedia.org/wiki/Retriever">retriever</a> and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of <a href="https://en.wikipedia.org/wiki/Natural_Questions">Natural Questions</a> (Kwiatkowski et al., 2019) into a Table QA dataset. We find that our <a href="https://en.wikipedia.org/wiki/Retriever">retriever</a> improves <a href="https://en.wikipedia.org/wiki/Recall_(memory)">retrieval</a> results from 72.0 to 81.1 recall@10 and <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end QA</a> results from 33.8 to 37.7 exact match, over a BERT based retriever.</abstract>
      <url hash="1b7f2461">2021.naacl-main.43</url>
      <doi>10.18653/v1/2021.naacl-main.43</doi>
      <bibkey>herzig-etal-2021-open</bibkey>
      <pwccode url="https://github.com/google-research/tapas" additional="false">google-research/tapas</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="44">
      <title>Open-Domain Question Answering Goes Conversational via Question Rewriting</title>
      <author><first>Raviteja</first><last>Anantha</last></author>
      <author><first>Svitlana</first><last>Vakulenko</last></author>
      <author><first>Zhucheng</first><last>Tu</last></author>
      <author><first>Shayne</first><last>Longpre</last></author>
      <author><first>Stephen</first><last>Pulman</last></author>
      <author><first>Srinivas</first><last>Chappidi</last></author>
      <pages>520–534</pages>
      <abstract>We introduce a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for Question Rewriting in Conversational Context (QReCC), which contains 14 K conversations with 80 K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10 M web pages (split into 54 M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> for the QReCC dataset with <a href="https://en.wikipedia.org/wiki/Quantitative_structure–activity_relationship">F1</a> of 19.10, compared to the <a href="https://en.wikipedia.org/wiki/Quantitative_structure–activity_relationship">human upper bound</a> of 75.45, indicating the difficulty of the setup and a large room for improvement.</abstract>
      <url hash="63f40dbe">2021.naacl-main.44</url>
      <doi>10.18653/v1/2021.naacl-main.44</doi>
      <bibkey>anantha-etal-2021-open</bibkey>
      <pwccode url="https://github.com/apple/ml-qrecc" additional="false">apple/ml-qrecc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/qrecc">QReCC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/canard">CANARD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
    </paper>
    <paper id="46">
      <title>XOR QA : Cross-lingual Open-Retrieval Question Answering<fixed-case>XOR</fixed-case> <fixed-case>QA</fixed-case>: Cross-lingual Open-Retrieval Question Answering</title>
      <author><first>Akari</first><last>Asai</last></author>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Jonathan</first><last>Clark</last></author>
      <author><first>Kenton</first><last>Lee</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>547–564</pages>
      <abstract>Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcitywhere languages have few reference articlesand information asymmetrywhere questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40 K information-seeking questions across 7 diverse non-English languages that TyDi QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross-lingual document retrieval from multilingual and English resources. We establish <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baselines</a> with state-of-the-art <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a> and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington.edu/xorqa/.</abstract>
      <url hash="010faf33">2021.naacl-main.46</url>
      <doi>10.18653/v1/2021.naacl-main.46</doi>
      <bibkey>asai-etal-2021-xor</bibkey>
      <pwccode url="https://github.com/AkariAsai/XORQA" additional="true">AkariAsai/XORQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xor-tydi-qa">XOR-TYDI QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="50">
      <title>On learning and representing social meaning in <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a> : a sociolinguistic perspective<fixed-case>NLP</fixed-case>: a sociolinguistic perspective</title>
      <author><first>Dong</first><last>Nguyen</last></author>
      <author><first>Laura</first><last>Rosseel</last></author>
      <author><first>Jack</first><last>Grieve</last></author>
      <pages>603–612</pages>
      <abstract>The field of <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a> has made substantial progress in building meaning representations. However, an important aspect of <a href="https://en.wikipedia.org/wiki/Meaning_(linguistics)">linguistic meaning</a>, <a href="https://en.wikipedia.org/wiki/Meaning_(linguistics)">social meaning</a>, has been largely overlooked. We introduce the concept of social meaning to <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a> and discuss how insights from <a href="https://en.wikipedia.org/wiki/Sociolinguistics">sociolinguistics</a> can inform work on <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning</a> in <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a>. We also identify key challenges for this new line of research.</abstract>
      <url hash="a37ae523">2021.naacl-main.50</url>
      <doi>10.18653/v1/2021.naacl-main.50</doi>
      <bibkey>nguyen-etal-2021-learning</bibkey>
    </paper>
    <paper id="53">
      <title>Representing Numbers in NLP : a Survey and a Vision<fixed-case>NLP</fixed-case>: a Survey and a Vision</title>
      <author><first>Avijit</first><last>Thawani</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>Filip</first><last>Ilievski</last></author>
      <author><first>Pedro</first><last>Szekely</last></author>
      <pages>644–656</pages>
      <abstract>NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in <a href="https://en.wikipedia.org/wiki/Neuroscience">neuroscience</a> that, in the brain, numbers are represented differently from words. We arrange recent NLP work on <a href="https://en.wikipedia.org/wiki/Numeracy">numeracy</a> into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions : granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by over a dozen previously published <a href="https://en.wikipedia.org/wiki/Encoder">number encoders</a> and <a href="https://en.wikipedia.org/wiki/Code">decoders</a>. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a>, comprised of design trade-offs and a unified evaluation.</abstract>
      <url hash="7b4ef25d">2021.naacl-main.53</url>
      <doi>10.18653/v1/2021.naacl-main.53</doi>
      <bibkey>thawani-etal-2021-representing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/math">MATH</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/numersense">NumerSense</pwcdataset>
    </paper>
    <paper id="55">
      <title>Identifying Helpful Sentences in Product Reviews</title>
      <author><first>Iftah</first><last>Gamzu</last></author>
      <author><first>Hila</first><last>Gonen</last></author>
      <author><first>Gilad</first><last>Kutiel</last></author>
      <author><first>Ran</first><last>Levy</last></author>
      <author><first>Eugene</first><last>Agichtein</last></author>
      <pages>678–691</pages>
      <abstract>In recent years <a href="https://en.wikipedia.org/wiki/Online_shopping">online shopping</a> has gained momentum and became an important venue for customers wishing to save time and simplify their shopping process. A key advantage of shopping online is the ability to read what other customers are saying about products of interest. In this work, we aim to maintain this advantage in situations where extreme brevity is needed, for example, when shopping by voice. We suggest a novel task of extracting a single representative helpful sentence from a set of reviews for a given product. The selected sentence should meet two conditions : first, it should be helpful for a purchase decision and second, the opinion it expresses should be supported by multiple reviewers. This <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is closely related to the task of Multi Document Summarization in the product reviews domain but differs in its objective and its level of conciseness. We collect a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> in English of sentence helpfulness scores via <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowd-sourcing</a> and demonstrate its reliability despite the inherent subjectivity involved. Next, we describe a complete model that extracts representative helpful sentences with positive and negative sentiment towards the product and demonstrate that it outperforms several baselines.</abstract>
      <url hash="65facb37">2021.naacl-main.55</url>
      <doi>10.18653/v1/2021.naacl-main.55</doi>
      <bibkey>gamzu-etal-2021-identifying</bibkey>
    </paper>
    <paper id="56">
      <title>Noisy Self-Knowledge Distillation for Text Summarization</title>
      <author id="yang-liu-edinburgh"><first>Yang</first><last>Liu</last></author>
      <author><first>Sheng</first><last>Shen</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>692–703</pages>
      <abstract>In this paper we apply self-knowledge distillation to <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a> which we argue can alleviate problems with <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum-likelihood training</a> on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training</a>, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and non-pretrained summarizers achieving state-of-the-art results.</abstract>
      <url hash="db49b1dc">2021.naacl-main.56</url>
      <doi>10.18653/v1/2021.naacl-main.56</doi>
      <bibkey>liu-etal-2021-noisy</bibkey>
      <pwccode url="https://github.com/nlpyang/NoisySumm" additional="false">nlpyang/NoisySumm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikicatsum">WikiCatSum</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisum">WikiSum</pwcdataset>
    </paper>
    <paper id="57">
      <title>Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation</title>
      <author><first>Alexander</first><last>Fabbri</last></author>
      <author><first>Simeng</first><last>Han</last></author>
      <author><first>Haoyuan</first><last>Li</last></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Marjan</first><last>Ghazvininejad</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <pages>704–717</pages>
      <abstract>Models pretrained with self-supervised objectives on large text corpora achieve state-of-the-art performance on English text summarization tasks. However, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are typically fine-tuned on hundreds of thousands of data points, an infeasible requirement when applying <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a> to new, niche domains. In this work, we introduce a novel and generalizable method, called WikiTransfer, for fine-tuning pretrained models for <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a> in an unsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained models on pseudo-summaries, produced from generic Wikipedia data, which contain characteristics of the target dataset, such as the length and level of abstraction of the desired summaries. WikiTransfer models achieve state-of-the-art, zero-shot abstractive summarization performance on the CNN-DailyMail dataset and demonstrate the effectiveness of our approach on three additional diverse datasets. These models are more robust to noisy data and also achieve better or comparable few-shot performance using 10 and 100 training examples when compared to few-shot transfer from other summarization datasets. To further boost performance, we employ <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> via <a href="https://en.wikipedia.org/wiki/Round-trip_translation">round-trip translation</a> as well as introduce a regularization term for improved few-shot transfer. To understand the role of dataset aspects in transfer performance and the quality of the resulting output summaries, we further study the effect of the components of our unsupervised fine-tuning data and analyze few-shot performance using both automatic and human evaluation.</abstract>
      <url hash="ed256c05">2021.naacl-main.57</url>
      <doi>10.18653/v1/2021.naacl-main.57</doi>
      <bibkey>fabbri-etal-2021-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bigpatent">BigPatent</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/new-york-times-annotated-corpus">New York Times Annotated Corpus</pwcdataset>
    </paper>
    <paper id="60">
      <title>Nice Try, Kiddo : Investigating Ad Hominems in Dialogue Responses</title>
      <author><first>Emily</first><last>Sheng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Prem</first><last>Natarajan</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>750–767</pages>
      <abstract>Ad hominem attacks are those that target some feature of a person’s character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person’s credibility. Since <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue systems</a> respond directly to <a href="https://en.wikipedia.org/wiki/Input_(computer_science)">user input</a>, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> to analyze human and dialogue system responses to English Twitter posts. We specifically compare responses to Twitter topics about marginalized communities (# BlackLivesMatter, # MeToo) versus other topics (# Vegan, # WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, we propose a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated. Our results indicate that 1) responses from both humans and DialoGPT contain more <a href="https://en.wikipedia.org/wiki/Ad_hominem">ad hominems</a> for discussions around marginalized communities, 2) different quantities of <a href="https://en.wikipedia.org/wiki/Ad_hominem">ad hominems</a> in the training data can influence the likelihood of generating <a href="https://en.wikipedia.org/wiki/Ad_hominem">ad hominems</a>, and 3) we can use constrained decoding techniques to reduce <a href="https://en.wikipedia.org/wiki/Ad_hominem">ad hominems</a> in generated dialogue responses.</abstract>
      <url hash="9946962c">2021.naacl-main.60</url>
      <doi>10.18653/v1/2021.naacl-main.60</doi>
      <bibkey>sheng-etal-2021-nice</bibkey>
    </paper>
    <paper id="63">
      <title>Spoken Language Understanding for Task-oriented Dialogue Systems with Augmented Memory Networks</title>
      <author><first>Jie</first><last>Wu</last></author>
      <author><first>Ian</first><last>Harris</last></author>
      <author><first>Hongzhi</first><last>Zhao</last></author>
      <pages>797–806</pages>
      <abstract>Spoken language understanding, usually including intent detection and slot filling, is a core component to build a spoken dialog system. Recent research shows promising results by jointly learning of those two tasks based on the fact that slot filling and intent detection are sharing <a href="https://en.wikipedia.org/wiki/Semantic_memory">semantic knowledge</a>. Furthermore, attention mechanism boosts joint learning to achieve state-of-the-art results. However, current joint learning models ignore the following important facts : 1. Long-term slot context is not traced effectively, which is crucial for future slot filling. Slot tagging and intent detection could be mutually rewarding, but bi-directional interaction between slot filling and intent detection remains seldom explored. In this paper, we propose a novel approach to model long-term slot context and to fully utilize the semantic correlation between slots and intents. We adopt a key-value memory network to model slot context dynamically and to track more important slot tags decoded before, which are then fed into our decoder for slot tagging. Furthermore, gated memory information is utilized to perform intent detection, mutually improving both tasks through <a href="https://en.wikipedia.org/wiki/Global_optimization">global optimization</a>. Experiments on benchmark ATIS and Snips datasets show that our model achieves state-of-the-art performance and outperforms other methods, especially for the slot filling task.</abstract>
      <url hash="780dbd91">2021.naacl-main.63</url>
      <doi>10.18653/v1/2021.naacl-main.63</doi>
      <bibkey>wu-etal-2021-spoken</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="66">
      <title>Self-Training with Weak Supervision</title>
      <author><first>Giannis</first><last>Karamanolakis</last></author>
      <author><first>Subhabrata</first><last>Mukherjee</last></author>
      <author><first>Guoqing</first><last>Zheng</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <pages>845–863</pages>
      <abstract>State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domain-specific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind. In this work, we develop a weak supervision framework (ASTRA) that leverages all the available data for a given <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines.</abstract>
      <url hash="d07118b2">2021.naacl-main.66</url>
      <doi>10.18653/v1/2021.naacl-main.66</doi>
      <bibkey>karamanolakis-etal-2021-self</bibkey>
      <pwccode url="https://github.com/microsoft/ASTRA" additional="false">microsoft/ASTRA</pwccode>
    </paper>
    <paper id="68">
      <title>Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning</title>
      <author><first>Xuelu</first><last>Chen</last></author>
      <author><first>Michael</first><last>Boratko</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Shib Sankar</first><last>Dasgupta</last></author>
      <author><first>Xiang Lorraine</first><last>Li</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>882–893</pages>
      <abstract>Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of <a href="https://en.wikipedia.org/wiki/Embedding">embedding methods</a> to generalize from known facts, however, existing <a href="https://en.wikipedia.org/wiki/Embedding">embedding methods</a> only model triple-level uncertainty, and reasoning results lack <a href="https://en.wikipedia.org/wiki/Global_consistency">global consistency</a>. To address these shortcomings, we propose BEUrRE, a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle) and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence prediction</a> and fact ranking due to its probabilistic calibration and ability to capture high-order dependencies among facts.</abstract>
      <url hash="3064c1a7">2021.naacl-main.68</url>
      <doi>10.18653/v1/2021.naacl-main.68</doi>
      <bibkey>chen-etal-2021-probabilistic</bibkey>
      <pwccode url="https://github.com/stasl0217/beurre" additional="false">stasl0217/beurre</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="69">
      <title>Document-Level Event Argument Extraction by Conditional Generation</title>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>894–908</pages>
      <abstract>Event extraction has long been treated as a sentence-level task in the IE community. We argue that this <a href="https://en.wikipedia.org/wiki/Setting_(narrative)">setting</a> does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6 % <a href="https://en.wikipedia.org/wiki/F-number">F1</a> and 5.7 % <a href="https://en.wikipedia.org/wiki/F-number">F1</a> over the next best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3 % F1 gain over the best <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97 % of fully supervised model’s trigger extraction performance and 82 % of the argument extraction performance given only access to 10 out of the 33 types on ACE.</abstract>
      <url hash="dbd060c1">2021.naacl-main.69</url>
      <attachment type="OptionalSupplementaryData" hash="44b6e567">2021.naacl-main.69.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.69</doi>
      <bibkey>li-etal-2021-document</bibkey>
      <pwccode url="https://github.com/raspberryice/gen-arg" additional="false">raspberryice/gen-arg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikievents">WikiEvents</pwcdataset>
    </paper>
    <paper id="70">
      <title>Template Filling with Generative Transformers</title>
      <author><first>Xinya</first><last>Du</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <pages>909–914</pages>
      <abstract>Template filling is generally tackled by a pipeline of two separate supervised systems   one for role-filler extraction and another for template / event recognition. Since <a href="https://en.wikipedia.org/wiki/Pipeline_transport">pipelines</a> consider events in isolation, they can suffer from <a href="https://en.wikipedia.org/wiki/Error_propagation">error propagation</a>. We introduce a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> specifically improves performance on documents containing multiple events.</abstract>
      <url hash="ced24048">2021.naacl-main.70</url>
      <doi>10.18653/v1/2021.naacl-main.70</doi>
      <bibkey>du-etal-2021-template</bibkey>
      <pwccode url="https://github.com/xinyadu/gtt" additional="false">xinyadu/gtt</pwccode>
    </paper>
    <paper id="72">
      <title>On Attention Redundancy : A Comprehensive Study</title>
      <author><first>Yuchen</first><last>Bian</last></author>
      <author><first>Jiaji</first><last>Huang</last></author>
      <author><first>Xingyu</first><last>Cai</last></author>
      <author><first>Jiahong</first><last>Yuan</last></author>
      <author><first>Kenneth</first><last>Church</last></author>
      <pages>930–945</pages>
      <abstract>Multi-layer multi-head self-attention mechanism is widely applied in modern neural language models. Attention redundancy has been observed among <a href="https://en.wikipedia.org/wiki/Attentional_control">attention heads</a> but has not been deeply studied in the literature. Using BERT-base model as an example, this paper provides a comprehensive study on attention redundancy which is helpful for model interpretation and model compression. We analyze the attention redundancy with Five-Ws and How. (What) We define and focus the study on redundancy matrices generated from pre-trained and fine-tuned BERT-base model for GLUE datasets. (How) We use both token-based and sentence-based distance functions to measure the <a href="https://en.wikipedia.org/wiki/Redundancy_(engineering)">redundancy</a>. (Where) Clear and similar redundancy patterns (cluster structure) are observed among attention heads. (When) Redundancy patterns are similar in both pre-training and fine-tuning phases. (Who) We discover that <a href="https://en.wikipedia.org/wiki/Redundancy_(engineering)">redundancy patterns</a> are task-agnostic. Similar <a href="https://en.wikipedia.org/wiki/Redundancy_(information_theory)">redundancy patterns</a> even exist for randomly generated token sequences. (Why) We also evaluate influences of the pre-training dropout ratios on attention redundancy. Based on the phase-independent and task-agnostic attention redundancy patterns, we propose a simple zero-shot pruning method as a case study. Experiments on fine-tuning GLUE tasks verify its effectiveness. The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising.</abstract>
      <url hash="17f0ebc0">2021.naacl-main.72</url>
      <doi>10.18653/v1/2021.naacl-main.72</doi>
      <bibkey>bian-etal-2021-attention</bibkey>
    </paper>
    <paper id="73">
      <title>Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?<fixed-case>BERT</fixed-case> Pretrained on Clinical Notes Reveal Sensitive Data?</title>
      <author><first>Eric</first><last>Lehman</last></author>
      <author><first>Sarthak</first><last>Jain</last></author>
      <author><first>Karl</first><last>Pichotta</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <pages>946–959</pages>
      <abstract>Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a <a href="https://en.wikipedia.org/wiki/Physical_model">model</a> if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated attacks may succeed in doing so : To facilitate such research, we make our experimental setup and baseline probing models available at https://github.com/elehman16/exposing_patient_data_release.</abstract>
      <url hash="03fc0bc3">2021.naacl-main.73</url>
      <doi>10.18653/v1/2021.naacl-main.73</doi>
      <bibkey>lehman-etal-2021-bert</bibkey>
      <pwccode url="https://github.com/elehman16/exposing_patient_data_release" additional="false">elehman16/exposing_patient_data_release</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="74">
      <title>Low-Complexity Probing via Finding Subnetworks</title>
      <author><first>Steven</first><last>Cao</last></author>
      <author><first>Victor</first><last>Sanh</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>960–966</pages>
      <abstract>The dominant approach in probing neural networks for linguistic properties is to train a new shallow multi-layer perceptron (MLP) on top of the model’s internal representations. This approach can detect properties encoded in the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, but at the cost of adding new parameters that may learn the task directly. We instead propose a subtractive pruning-based probe, where we find an existing <a href="https://en.wikipedia.org/wiki/Subnetwork">subnetwork</a> that performs the linguistic task of interest. Compared to an MLP, the subnetwork probe achieves both higher <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on pre-trained models and lower <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on random models, so it is both better at finding properties of interest and worse at learning on its own. Next, by varying the <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> of each probe, we show that subnetwork probing Pareto-dominates MLP probing in that it achieves higher <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> given any budget of probe complexity. Finally, we analyze the resulting subnetworks across various tasks to locate where each task is encoded, and we find that lower-level tasks are captured in lower layers, reproducing similar findings in past work.</abstract>
      <url hash="b5d87445">2021.naacl-main.74</url>
      <doi>10.18653/v1/2021.naacl-main.74</doi>
      <bibkey>cao-etal-2021-low</bibkey>
      <pwccode url="https://github.com/stevenxcao/subnetwork-probing" additional="false">stevenxcao/subnetwork-probing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="75">
      <title>An Empirical Comparison of Instance Attribution Methods for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a><fixed-case>NLP</fixed-case></title>
      <author><first>Pouya</first><last>Pezeshkpour</last></author>
      <author><first>Sarthak</first><last>Jain</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>967–975</pages>
      <abstract>Widespread adoption of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep models</a> has motivated a pressing need for approaches to interpret network outputs and to facilitate model debugging. Instance attribution methods constitute one means of accomplishing these goals by retrieving training instances that (may have) led to a particular prediction. Influence functions (IF ; Koh and Liang 2017) provide machinery for doing this by quantifying the effect that perturbing individual train instances would have on a specific test prediction. However, even approximating the <a href="https://en.wikipedia.org/wiki/Conditional_(computer_programming)">IF</a> is computationally expensive, to the degree that may be prohibitive in many cases. Might simpler approaches (e.g., retrieving train examples most similar to a given test point) perform comparably? In this work, we evaluate the degree to which different potential instance attribution agree with respect to the importance of training samples. We find that simple retrieval methods yield training instances that differ from those identified via gradient-based methods (such as IFs), but that nonetheless exhibit desirable characteristics similar to more complex attribution methods. Code for all methods and experiments in this paper is available at : https://github.com/successar/instance_attributions_NLP.</abstract>
      <url hash="e872d85f">2021.naacl-main.75</url>
      <doi>10.18653/v1/2021.naacl-main.75</doi>
      <bibkey>pezeshkpour-etal-2021-empirical</bibkey>
      <pwccode url="https://github.com/successar/instance_attributions_NLP" additional="false">successar/instance_attributions_NLP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="76">
      <title>Generalization in Instruction Following Systems</title>
      <author><first>Soham</first><last>Dan</last></author>
      <author><first>Michael</first><last>Zhou</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>976–981</pages>
      <abstract>Understanding and executing <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language instructions</a> in a grounded domain is one of the hallmarks of <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial intelligence</a>. In this paper, we focus on instruction understanding in the blocks world domain and investigate the language understanding abilities of two top-performing <a href="https://en.wikipedia.org/wiki/System">systems</a> for the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. We aim to understand if the test performance of these models indicates an understanding of the spatial domain and of the <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language instructions</a> relative to it, or whether they merely over-fit spurious signals in the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. We formulate a set of expectations one might have from an instruction following model and concretely characterize the different dimensions of robustness such a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> should possess. Despite decent test performance, we find that <a href="https://en.wikipedia.org/wiki/State-of-the-art">state-of-the-art models</a> fall short of these expectations and are extremely brittle. We then propose a learning strategy that involves <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> and show through extensive experiments that the proposed learning strategy yields models that are competitive on the original test set while satisfying our expectations much better.</abstract>
      <url hash="7da474c9">2021.naacl-main.76</url>
      <doi>10.18653/v1/2021.naacl-main.76</doi>
      <bibkey>dan-etal-2021-generalization</bibkey>
    </paper>
    <paper id="79">
      <title>MTAG : Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences<fixed-case>MTAG</fixed-case>: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences</title>
      <author><first>Jianing</first><last>Yang</last></author>
      <author><first>Yongxin</first><last>Wang</last></author>
      <author><first>Ruitao</first><last>Yi</last></author>
      <author><first>Yuying</first><last>Zhu</last></author>
      <author><first>Azaan</first><last>Rehman</last></author>
      <author><first>Amir</first><last>Zadeh</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <author><first>Louis-Philippe</first><last>Morency</last></author>
      <pages>1009–1021</pages>
      <abstract>Human communication is multimodal in nature ; it is through multiple modalities such as <a href="https://en.wikipedia.org/wiki/Language">language</a>, <a href="https://en.wikipedia.org/wiki/Human_voice">voice</a>, and <a href="https://en.wikipedia.org/wiki/Facial_expression">facial expressions</a>, that opinions and emotions are expressed. Data in this <a href="https://en.wikipedia.org/wiki/Domain_(biology)">domain</a> exhibits complex multi-relational and temporal interactions. Learning from this <a href="https://en.wikipedia.org/wiki/Data">data</a> is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>, MTAG achieves state-of-the-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters.</abstract>
      <url hash="f1785926">2021.naacl-main.79</url>
      <doi>10.18653/v1/2021.naacl-main.79</doi>
      <bibkey>yang-etal-2021-mtag</bibkey>
      <pwccode url="https://github.com/jedyang97/MTAG" additional="false">jedyang97/MTAG</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="80">
      <title>Grounding Open-Domain Instructions to Automate Web Support Tasks</title>
      <author><first>Nancy</first><last>Xu</last></author>
      <author><first>Sam</first><last>Masling</last></author>
      <author><first>Michael</first><last>Du</last></author>
      <author><first>Giovanni</first><last>Campagna</last></author>
      <author><first>Larry</first><last>Heck</last></author>
      <author><first>James</first><last>Landay</last></author>
      <author><first>Monica</first><last>Lam</last></author>
      <pages>1022–1032</pages>
      <abstract>Grounding natural language instructions on the web to perform previously unseen tasks enables <a href="https://en.wikipedia.org/wiki/Accessibility">accessibility</a> and <a href="https://en.wikipedia.org/wiki/Automation">automation</a>. We introduce a task and dataset to train <a href="https://en.wikipedia.org/wiki/Intelligent_agent">AI agents</a> from open-domain, step-by-step instructions originally written for people. We build RUSS (Rapid Universal Support Service) to tackle this problem. RUSS consists of two models : First, a BERT-LSTM with pointers parses instructions to WebLang, a <a href="https://en.wikipedia.org/wiki/Domain-specific_language">domain-specific language</a> we design for grounding <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> on the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a>. Then, a grounding model retrieves the unique IDs of any webpage elements requested in the WebLang. RUSS may interact with the user through a <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a> (e.g. ask for an address) or execute a <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol">web operation</a> (e.g. click a button) inside the web runtime. To augment <a href="https://en.wikipedia.org/wiki/Training">training</a>, we synthesize <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language instructions</a> mapped to WebLang. Our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> consists of 80 different customer service problems from help websites, with a total of 741 step-by-step instructions and their corresponding actions. RUSS achieves 76.7 % end-to-end accuracy predicting agent actions from single instructions. It outperforms state-of-the-art <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> that directly map instructions to actions without WebLang. Our user study shows that RUSS is preferred by actual users over <a href="https://en.wikipedia.org/wiki/Web_navigation">web navigation</a>.</abstract>
      <url hash="29969513">2021.naacl-main.80</url>
      <doi>10.18653/v1/2021.naacl-main.80</doi>
      <bibkey>xu-etal-2021-grounding</bibkey>
      <pwccode url="https://github.com/xnancy/russ" additional="false">xnancy/russ</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/russ-dataset">RUSS Dataset</pwcdataset>
    </paper>
    <paper id="82">
      <title>Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information</title>
      <author><first>Jialu</first><last>Li</last></author>
      <author><first>Hao</first><last>Tan</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>1041–1050</pages>
      <abstract>Vision language navigation is the task that requires an agent to navigate through a 3D environment based on <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language instructions</a>. One key challenge in this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is to ground instructions with the current <a href="https://en.wikipedia.org/wiki/Visual_system">visual information</a> that the agent perceives. Most of the existing work employs soft attention over individual words to locate the instruction required for the next action. However, different words have different functions in a sentence (e.g., <a href="https://en.wikipedia.org/wiki/Grammatical_modifier">modifiers</a> convey attributes, <a href="https://en.wikipedia.org/wiki/Verb">verbs</a> convey actions). Syntax information like <a href="https://en.wikipedia.org/wiki/Coupling_(computer_programming)">dependencies</a> and <a href="https://en.wikipedia.org/wiki/Phrase_structure_grammar">phrase structures</a> can aid the <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agent</a> to locate important parts of the instruction. Hence, in this paper, we propose a navigation agent that utilizes syntax information derived from a dependency tree to enhance alignment between the instruction and the current visual scenes. Empirically, our agent outperforms the baseline model that does not use syntax information on the Room-to-Room dataset, especially in the unseen environment. Besides, our agent achieves the new state-of-the-art on Room-Across-Room dataset, which contains instructions in 3 languages (English, Hindi, and Telugu). We also show that our <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agent</a> is better at aligning instructions with the current <a href="https://en.wikipedia.org/wiki/Visual_system">visual information</a> via qualitative visualizations.</abstract>
      <url hash="c7a6b508">2021.naacl-main.82</url>
      <doi>10.18653/v1/2021.naacl-main.82</doi>
      <bibkey>li-etal-2021-improving</bibkey>
      <pwccode url="https://github.com/jialuli-luka/SyntaxVLN" additional="false">jialuli-luka/SyntaxVLN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rxr">RxR</pwcdataset>
    </paper>
    <paper id="86">
      <title>Understanding Hard Negatives in Noise Contrastive Estimation</title>
      <author><first>Wenzheng</first><last>Zhang</last></author>
      <author><first>Karl</first><last>Stratos</last></author>
      <pages>1090–1101</pages>
      <abstract>The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negativeshighest-scoring incorrect examples under the modelare effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the cross-entropy loss, and show both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction. We also derive a general form of the <a href="https://en.wikipedia.org/wiki/Score_function">score function</a> that unifies various <a href="https://en.wikipedia.org/wiki/Information_retrieval">architectures</a> used in <a href="https://en.wikipedia.org/wiki/Information_retrieval">text retrieval</a>. By combining hard negatives with appropriate score functions, we obtain strong results on the challenging task of zero-shot entity linking.</abstract>
      <url hash="3b6c4213">2021.naacl-main.86</url>
      <doi>10.18653/v1/2021.naacl-main.86</doi>
      <bibkey>zhang-stratos-2021-understanding</bibkey>
      <pwccode url="https://github.com/WenzhengZhang/hard-nce-el" additional="false">WenzhengZhang/hard-nce-el</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/zeshel">ZESHEL</pwcdataset>
    </paper>
    <paper id="88">
      <title>DReCa : A General Task Augmentation Strategy for Few-Shot Natural Language Inference<fixed-case>DR</fixed-case>e<fixed-case>C</fixed-case>a: A General Task Augmentation Strategy for Few-Shot Natural Language Inference</title>
      <author><first>Shikhar</first><last>Murty</last></author>
      <author><first>Tatsunori B.</first><last>Hashimoto</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>1113–1125</pages>
      <abstract>Meta-learning promises few-shot learners that can adapt to new distributions by repurposing knowledge acquired from previous training. However, we believe <a href="https://en.wikipedia.org/wiki/Meta-learning">meta-learning</a> has not yet succeeded in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> due to the lack of a well-defined task distribution, leading to attempts that treat datasets as tasks. Such an ad hoc task distribution causes problems of quantity and quality. Since there’s only a handful of datasets for any NLP problem, meta-learners tend to overfit their adaptation mechanism and, since NLP datasets are highly heterogeneous, many learning episodes have poor transfer between their support and query sets, which discourages the meta-learner from adapting. To alleviate these issues, we propose DReCA (Decomposing datasets into Reasoning Categories), a simple method for discovering and using latent reasoning categories in a dataset, to form additional high quality tasks. DReCA works by splitting examples into label groups, embedding them with a finetuned BERT model and then clustering each group into reasoning categories. Across four few-shot NLI problems, we demonstrate that using DReCA improves the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of <a href="https://en.wikipedia.org/wiki/Meta-learning">meta-learners</a> by 1.5-4 %</abstract>
      <url hash="ff6f450a">2021.naacl-main.88</url>
      <doi>10.18653/v1/2021.naacl-main.88</doi>
      <bibkey>murty-etal-2021-dreca</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="89">
      <title>Harnessing <a href="https://en.wikipedia.org/wiki/Multilinguality">Multilinguality</a> in Unsupervised Machine Translation for Rare Languages</title>
      <author><first>Xavier</first><last>Garcia</last></author>
      <author><first>Aditya</first><last>Siddhant</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Ankur</first><last>Parikh</last></author>
      <pages>1126–1137</pages>
      <abstract>Unsupervised translation has reached impressive performance on resource-rich language pairs such as English-French and <a href="https://en.wikipedia.org/wiki/Standard_German">English-German</a>. However, early studies have shown that in more realistic settings involving low-resource, rare languages, unsupervised translation performs poorly, achieving less than 3.0 <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>. In this work, we show that <a href="https://en.wikipedia.org/wiki/Multilinguality">multilinguality</a> is critical to making <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised systems</a> practical for low-resource settings. In particular, we present a single model for 5 low-resource languages (Gujarati, Kazakh, Nepali, Sinhala, and Turkish) to and from English directions, which leverages monolingual and auxiliary parallel data from other high-resource language pairs via a three-stage training scheme. We outperform all current state-of-the-art unsupervised baselines for these languages, achieving gains of up to 14.4 BLEU. Additionally, we outperform strong supervised baselines for various language pairs as well as match the performance of the current state-of-the-art <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised model</a> for <a href="https://en.wikipedia.org/wiki/Nepali_language">Nepali-English</a>. We conduct a series of ablation studies to establish the robustness of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> under different degrees of <a href="https://en.wikipedia.org/wiki/Data_quality">data quality</a>, as well as to analyze the factors which led to the superior performance of the proposed approach over traditional <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised models</a>.</abstract>
      <url hash="6817eaa9">2021.naacl-main.89</url>
      <doi>10.18653/v1/2021.naacl-main.89</doi>
      <bibkey>garcia-etal-2021-harnessing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
    </paper>
    <paper id="91">
      <title>Assessing Reference-Free Peer Evaluation for Machine Translation</title>
      <author><first>Sweta</first><last>Agrawal</last></author>
      <author><first>George</first><last>Foster</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Colin</first><last>Cherry</last></author>
      <pages>1158–1171</pages>
      <abstract>Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, and demonstrate that by scaling it up we can match the performance of <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>. We analyze various potential weaknesses of the approach, and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities.</abstract>
      <url hash="74221b78">2021.naacl-main.91</url>
      <doi>10.18653/v1/2021.naacl-main.91</doi>
      <bibkey>agrawal-etal-2021-assessing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt19-metrics-task">WMT19 Metrics Task</pwcdataset>
    </paper>
    <paper id="92">
      <title>The Curious Case of <a href="https://en.wikipedia.org/wiki/Hallucination">Hallucinations</a> in Neural Machine Translation</title>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Arul</first><last>Menezes</last></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <pages>1172–1183</pages>
      <abstract>In this work, we study <a href="https://en.wikipedia.org/wiki/Hallucination">hallucinations</a> in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of <a href="https://en.wikipedia.org/wiki/Hallucination">hallucinations</a> under source perturbation to the Long-Tail theory of Feldman, and present an empirically validated hypothesis that explains <a href="https://en.wikipedia.org/wiki/Hallucination">hallucinations</a> under source perturbation. Secondly, we consider <a href="https://en.wikipedia.org/wiki/Hallucination">hallucinations</a> under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results.</abstract>
      <url hash="dc8aead2">2021.naacl-main.92</url>
      <doi>10.18653/v1/2021.naacl-main.92</doi>
      <bibkey>raunak-etal-2021-curious</bibkey>
      <pwccode url="https://github.com/vyraun/hallucinations" additional="false">vyraun/hallucinations</pwccode>
    </paper>
    <paper id="94">
      <title>Towards Modeling the Style of Translators in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a></title>
      <author><first>Yue</first><last>Wang</last></author>
      <author><first>Cuong</first><last>Hoang</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <pages>1193–1199</pages>
      <abstract>One key ingredient of <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> is the use of large datasets from different domains and resources (e.g. Europarl, TED talks). These <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> contain documents translated by professional translators using different but consistent translation styles. Despite that, the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> is usually trained in a way that neither explicitly captures the variety of translation styles present in the data nor translates new data in different and controllable styles. In this work, we investigate methods to augment the state of the art Transformer model with translator information that is available in part of the training data. We show that our style-augmented translation models are able to capture the style variations of translators and to generate translations with different styles on new data. Indeed, the generated variations differ significantly, up to +4.5 BLEU score difference. Despite that, <a href="https://en.wikipedia.org/wiki/Human_factors_and_ergonomics">human evaluation</a> confirms that the translations are of the same quality.</abstract>
      <url hash="34b6611f">2021.naacl-main.94</url>
      <doi>10.18653/v1/2021.naacl-main.94</doi>
      <bibkey>wang-etal-2021-towards</bibkey>
    </paper>
    <paper id="95">
      <title>Self-Supervised Test-Time Learning for Reading Comprehension</title>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Tejas</first><last>Gokhale</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>1200–1211</pages>
      <abstract>Recent work on unsupervised question answering has shown that models can be trained with procedurally generated question-answer pairs and can achieve performance competitive with <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised methods</a>. In this work, we consider the task of unsupervised reading comprehension and present a method that performs test-time learning (TTL) on a given context (text passage), without requiring training on large-scale human-authored datasets containing context-question-answer triplets. This method operates directly on a single test context, uses self-supervision to train <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a> on synthetically generated question-answer pairs, and then infers answers to unseen human-authored questions for this context. Our method achieves accuracies competitive with <a href="https://en.wikipedia.org/wiki/Supervised_learning">fully supervised methods</a> and significantly outperforms current <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a>. TTL methods with a smaller <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> are also competitive with the current <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> in unsupervised reading comprehension.<i>context-question-answer</i> triplets. This method operates directly on a single test context, uses self-supervision to train models on synthetically generated question-answer pairs, and then infers answers to unseen human-authored questions for this context. Our method achieves accuracies competitive with fully supervised methods and significantly outperforms current unsupervised methods. TTL methods with a smaller model are also competitive with the current state-of-the-art in unsupervised reading comprehension.</abstract>
      <url hash="80057956">2021.naacl-main.95</url>
      <doi>10.18653/v1/2021.naacl-main.95</doi>
      <bibkey>banerjee-etal-2021-self</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl">QA-SRL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="96">
      <title>Capturing Row and Column Semantics in Transformer Based Question Answering over Tables</title>
      <author><first>Michael</first><last>Glass</last></author>
      <author><first>Mustafa</first><last>Canim</last></author>
      <author><first>Alfio</first><last>Gliozzo</last></author>
      <author><first>Saneem</first><last>Chemmengath</last></author>
      <author><first>Vishwajeet</first><last>Kumar</last></author>
      <author><first>Rishav</first><last>Chakravarti</last></author>
      <author><first>Avi</first><last>Sil</last></author>
      <author><first>Feifei</first><last>Pan</last></author>
      <author><first>Samarth</first><last>Bharadwaj</last></author>
      <author><first>Nicolas Rodolfo</first><last>Fauceglia</last></author>
      <pages>1212–1224</pages>
      <abstract>Transformer based architectures are recently used for the task of answering questions over tables. In order to improve the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, specialized pre-training techniques have been developed and applied on millions of open-domain web tables. In this paper, we propose two novel approaches demonstrating that one can achieve superior performance on table QA task without even using any of these specialized pre-training techniques. The first model, called RCI interaction, leverages a transformer based architecture that independently classifies rows and columns to identify relevant cells. While this model yields extremely high accuracy at finding cell values on recent benchmarks, a second model we propose, called RCI representation, provides a significant efficiency advantage for online QA systems over tables by materializing embeddings for existing tables. Experiments on recent benchmarks prove that the proposed methods can effectively locate cell values on tables (up to ~98 % Hit@1 accuracy on WikiSQL lookup questions). Also, the <a href="https://en.wikipedia.org/wiki/Interaction_model">interaction model</a> outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (TAPAS and TaBERT), achieving ~3.4 % and ~18.86 % additional precision improvement on the standard WikiSQL benchmark.</abstract>
      <url hash="d46f8805">2021.naacl-main.96</url>
      <doi>10.18653/v1/2021.naacl-main.96</doi>
      <bibkey>glass-etal-2021-capturing</bibkey>
      <pwccode url="https://github.com/IBM/row-column-intersection" additional="false">IBM/row-column-intersection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitablequestions">WikiTableQuestions</pwcdataset>
    </paper>
    <paper id="98">
      <title>Robust Question Answering Through Sub-part Alignment</title>
      <author><first>Jifan</first><last>Chen</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>1251–1263</pages>
      <abstract>Current textual question answering (QA) models achieve strong performance on in-domain test sets, but often do so by fitting surface-level patterns, so they fail to generalize to out-of-distribution settings. To make a more robust and understandable <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA system</a>, we model <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> as an alignment problem. We decompose both the question and context into smaller units based on off-the-shelf semantic representations (here, semantic roles), and align the question to a subgraph of the context in order to find the answer. We formulate our model as a structured SVM, with alignment scores computed via BERT, and we can train end-to-end despite using beam search for approximate inference. Our use of explicit alignments allows us to explore a set of <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraints</a> with which we can prohibit certain types of bad model behavior arising in cross-domain settings. Furthermore, by investigating differences in scores across different potential answers, we can seek to understand what particular aspects of the input lead the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to choose the answer without relying on post-hoc explanation techniques. We train our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on SQuAD v1.1 and test it on several adversarial and out-of-domain datasets. The results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is more robust than the standard BERT QA model, and constraints derived from alignment scores allow us to effectively trade off coverage and <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>.</abstract>
      <url hash="4a3ec876">2021.naacl-main.98</url>
      <doi>10.18653/v1/2021.naacl-main.98</doi>
      <bibkey>chen-durrett-2021-robust</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bio">Bio</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="100">
      <title>RECONSIDER : Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering<fixed-case>RECONSIDER</fixed-case>: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering</title>
      <author><first>Srinivasan</first><last>Iyer</last></author>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>Wen-tau</first><last>Yih</last></author>
      <pages>1280–1287</pages>
      <abstract>State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain Question Answering (QA) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples. This training scheme possibly explains empirical observations that these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> achieve a high <a href="https://en.wikipedia.org/wiki/Precision_(computer_science)">recall</a> amongst their top few predictions, but a low overall <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, motivating the need for answer re-ranking. We develop a successful re-ranking approach (RECONSIDER) for span-extraction tasks that improves upon the performance of MRC models, even beyond large-scale pre-training. RECONSIDER is trained on positive and negative examples extracted from high confidence MRC model predictions, and uses in-passage span annotations to perform span-focused re-ranking over a smaller candidate set. As a result, RECONSIDER learns to eliminate close false positives, achieving a new extractive state of the art on four <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA tasks</a>, with 45.5 % Exact Match accuracy on Natural Questions with real user questions, and 61.7 % on TriviaQA. We will release all related data, <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>, and code.</abstract>
      <url hash="6e6d549d">2021.naacl-main.100</url>
      <doi>10.18653/v1/2021.naacl-main.100</doi>
      <bibkey>iyer-etal-2021-reconsider</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="104">
      <title>Looking Beyond Sentence-Level Natural Language Inference for <a href="https://en.wikipedia.org/wiki/Question_answering">Question Answering</a> and Text Summarization</title>
      <author><first>Anshuman</first><last>Mishra</last></author>
      <author><first>Dhruvesh</first><last>Patel</last></author>
      <author><first>Aparna</first><last>Vijayakumar</last></author>
      <author><first>Xiang Lorraine</first><last>Li</last></author>
      <author><first>Pavan</first><last>Kapanipathi</last></author>
      <author><first>Kartik</first><last>Talamadupula</last></author>
      <pages>1322–1336</pages>
      <abstract>Natural Language Inference (NLI) has garnered significant attention in recent years ; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that : (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge prohibiting usage in downstream applications (which do better with longer contexts) ; (2) this challenge can be addressed by automatically converting resource-rich reading comprehension datasets into longer-premise NLI datasets ; and (3) models trained on the converted, longer-premise datasets outperform those trained using short-premise traditional NLI datasets on downstream tasks primarily due to the difference in premise lengths.</abstract>
      <url hash="492ce9df">2021.naacl-main.104</url>
      <doi>10.18653/v1/2021.naacl-main.104</doi>
      <bibkey>mishra-etal-2021-looking</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="110">
      <title>A New Approach to Overgenerating and Scoring Abstractive Summaries</title>
      <author><first>Kaiqiang</first><last>Song</last></author>
      <author><first>Bingqing</first><last>Wang</last></author>
      <author><first>Zhe</first><last>Feng</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <pages>1392–1404</pages>
      <abstract>We propose a new approach to generate multiple variants of the target summary with diverse content and varying lengths, then score and select admissible ones according to users’ needs. Abstractive summarizers trained on single reference summaries may struggle to produce outputs that achieve multiple desirable properties, i.e., capturing the most important information, being faithful to the original, grammatical and fluent. In this paper, we propose a two-staged strategy to generate a diverse set of candidate summaries from the source text in stage one, then score and select admissible ones in stage two. Importantly, our generator gives a precise control over the length of the summary, which is especially well-suited when space is limited. Our selectors are designed to predict the optimal summary length and put special emphasis on faithfulness to the original text. Both <a href="https://en.wikipedia.org/wiki/Stage_(theatre)">stages</a> can be effectively trained, optimized and evaluated. Our experiments on benchmark summarization datasets suggest that this <a href="https://en.wikipedia.org/wiki/Paradigm">paradigm</a> can achieve state-of-the-art performance.</abstract>
      <url hash="1d44cc71">2021.naacl-main.110</url>
      <doi>10.18653/v1/2021.naacl-main.110</doi>
      <bibkey>song-etal-2021-new</bibkey>
      <pwccode url="https://github.com/ucfnlp/varying-length-summ" additional="false">ucfnlp/varying-length-summ</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
    </paper>
    <paper id="111">
      <title>D2S : Document-to-Slide Generation Via Query-Based Text Summarization<fixed-case>D</fixed-case>2<fixed-case>S</fixed-case>: Document-to-Slide Generation Via Query-Based Text Summarization</title>
      <author><first>Edward</first><last>Sun</last></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Dakuo</first><last>Wang</last></author>
      <author><first>Yunfeng</first><last>Zhang</last></author>
      <author><first>Nancy X. R.</first><last>Wang</last></author>
      <pages>1405–1418</pages>
      <abstract>Presentations are critical for communication in all areas of our lives, yet the creation of <a href="https://en.wikipedia.org/wiki/Slide_show">slide decks</a> is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge : no publicly available dataset for training and benchmarking. In this work, we first contribute a new dataset, SciDuet, consisting of pairs of papers and their corresponding slides decks from recent years’ NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel system that tackles the document-to-slides task with a two-step approach : 1) Use slide titles to retrieve relevant and engaging text, figures, and tables ; 2) Summarize the retrieved context into bullet points with long-form question answering. Our evaluation suggests that long-form QA outperforms state-of-the-art summarization baselines on both automated ROUGE metrics and qualitative human evaluation.</abstract>
      <url hash="19567cf2">2021.naacl-main.111</url>
      <doi>10.18653/v1/2021.naacl-main.111</doi>
      <bibkey>sun-etal-2021-d2s</bibkey>
      <pwccode url="https://github.com/IBM/document2slides" additional="false">IBM/document2slides</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sciduet">SciDuet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="112">
      <title>Efficient Attentions for Long Document Summarization</title>
      <author><first>Luyang</first><last>Huang</last></author>
      <author><first>Shuyang</first><last>Cao</last></author>
      <author><first>Nikolaus</first><last>Parulian</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>1419–1436</pages>
      <abstract>The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient <a href="https://en.wikipedia.org/wiki/Self-interest">self-attentions</a>. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, GovReport, with significantly longer documents and summaries. Results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on <a href="https://en.wikipedia.org/wiki/PubMed">PubMed</a>. Human evaluation also shows that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> generate more informative summaries with fewer unfaithful errors.</abstract>
      <url hash="a9d9cd13">2021.naacl-main.112</url>
      <doi>10.18653/v1/2021.naacl-main.112</doi>
      <bibkey>huang-etal-2021-efficient</bibkey>
      <pwccode url="https://github.com/luyang-huang96/LongDocSum" additional="false">luyang-huang96/LongDocSum</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/govreport">GovReport</pwcdataset>
    </paper>
    <paper id="113">
      <title>RefSum : Refactoring Neural Summarization<fixed-case>R</fixed-case>ef<fixed-case>S</fixed-case>um: Refactoring Neural Summarization</title>
      <author><first>Yixin</first><last>Liu</last></author>
      <author><first>Zi-Yi</first><last>Dou</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>1437–1448</pages>
      <abstract>Although some recent works show potential complementarity among different state-of-the-art systems, few works try to investigate this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> in <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a>. Researchers in other areas commonly refer to the techniques of <a href="https://en.wikipedia.org/wiki/Ranking">reranking</a> or stacking to approach this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a>. In this work, we highlight several limitations of previous methods, which motivates us to present a new framework <a href="https://en.wikipedia.org/wiki/Code_refactoring">Refactor</a> that provides a unified view of <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a> and summaries combination. Experimentally, we perform a comprehensive evaluation that involves twenty-two base systems, four <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, and three different application scenarios. Besides new state-of-the-art results on CNN / DailyMail dataset (46.18 ROUGE-1), we also elaborate on how our proposed method addresses the limitations of the traditional methods and the effectiveness of the Refactor model sheds light on insight for performance improvement. Our <a href="https://en.wikipedia.org/wiki/System">system</a> can be directly used by other researchers as an off-the-shelf tool to achieve further performance improvements. We open-source all the code and provide a convenient interface to use it : https://github.com/yixinL7/Refactoring-Summarization.</abstract>
      <url hash="e5195109">2021.naacl-main.113</url>
      <doi>10.18653/v1/2021.naacl-main.113</doi>
      <bibkey>liu-etal-2021-refsum</bibkey>
      <pwccode url="https://github.com/yixinL7/Refactoring-Summarization" additional="false">yixinL7/Refactoring-Summarization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="114">
      <title>Annotating and Modeling Fine-grained Factuality in <a href="https://en.wikipedia.org/wiki/Summarization">Summarization</a></title>
      <author><first>Tanya</first><last>Goyal</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>1449–1462</pages>
      <abstract>Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and <a href="https://en.wikipedia.org/wiki/Statistical_model">statistical models</a> for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a>, and study <a href="https://en.wikipedia.org/wiki/Factuality">factuality</a> at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSum. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSum summarization models by allowing us to identify non-factual tokens in the training data.</abstract>
      <url hash="95db9a84">2021.naacl-main.114</url>
      <doi>10.18653/v1/2021.naacl-main.114</doi>
      <bibkey>goyal-durrett-2021-annotating</bibkey>
      <pwccode url="https://github.com/tagoyal/factuality-datasets" additional="true">tagoyal/factuality-datasets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="115">
      <title>Larger-Context Tagging : When and Why Does It Work?</title>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Liangjing</first><last>Feng</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>1463–1475</pages>
      <abstract>The development of <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> and pretraining techniques has spawned many sentence-level tagging systems that achieved superior performance on typical benchmarks. However, a relatively less discussed topic is what if more <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context information</a> is introduced into current top-scoring tagging systems. Although several existing works have attempted to shift tagging systems from sentence-level to document-level, there is still no consensus conclusion about when and why it works, which limits the applicability of the larger-context approach in tagging tasks. In this paper, instead of pursuing a state-of-the-art tagging system by architectural exploration, we focus on investigating when and why the larger-context training, as a general strategy, can work. To this end, we conduct a thorough comparative study on four proposed aggregators for context information collecting and present an attribute-aided evaluation method to interpret the improvement brought by larger-context training. Experimentally, we set up a testbed based on four tagging tasks and thirteen datasets. Hopefully, our preliminary observations can deepen the understanding of larger-context training and enlighten more follow-up works on the use of <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a>.</abstract>
      <url hash="3cfa0cc5">2021.naacl-main.115</url>
      <doi>10.18653/v1/2021.naacl-main.115</doi>
      <bibkey>fu-etal-2021-larger</bibkey>
    </paper>
    <paper id="116">
      <title>Neural Sequence Segmentation as Determining the Leftmost Segments</title>
      <author><first>Yangming</first><last>Li</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Kaisheng</first><last>Yao</last></author>
      <pages>1476–1486</pages>
      <abstract>Prior methods to <a href="https://en.wikipedia.org/wiki/Text_segmentation">text segmentation</a> are mostly at <a href="https://en.wikipedia.org/wiki/Lexical_analysis">token level</a>. Despite the adequacy, this nature limits their full potential to capture the long-term dependencies among segments. In this work, we propose a novel <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> that incrementally segments <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">natural language sentences</a> at segment level. For every step in segmentation, it recognizes the leftmost segment of the remaining sequence. Implementations involve LSTM-minus technique to construct the phrase representations and <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks (RNN)</a> to model the iterations of determining the leftmost segments. We have conducted extensive experiments on syntactic chunking and Chinese part-of-speech (POS) tagging across 3 datasets, demonstrating that our methods have significantly outperformed previous all baselines and achieved new state-of-the-art results. Moreover, <a href="https://en.wikipedia.org/wiki/Qualitative_research">qualitative analysis</a> and the study on segmenting long-length sentences verify its effectiveness in modeling long-term dependencies.</abstract>
      <url hash="33bae495">2021.naacl-main.116</url>
      <doi>10.18653/v1/2021.naacl-main.116</doi>
      <bibkey>li-etal-2021-neural</bibkey>
      <pwccode url="https://github.com/LeePleased/LeftmostSeg" additional="false">LeePleased/LeftmostSeg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="123">
      <title>Put Chatbot into Its Interlocutor’s Shoes : New Framework to Learn Chatbot Responding with Intention</title>
      <author><first>Hsuan</first><last>Su</last></author>
      <author><first>Jiun-Hao</first><last>Jhan</last></author>
      <author><first>Fan-yun</first><last>Sun</last></author>
      <author><first>Saurav</first><last>Sahay</last></author>
      <author><first>Hung-yi</first><last>Lee</last></author>
      <pages>1559–1569</pages>
      <abstract>Most chatbot literature that focuses on improving the <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a> and coherence of a <a href="https://en.wikipedia.org/wiki/Chatbot">chatbot</a>, is dedicated to making <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a> more human-like. However, very little work delves into what really separates humans from chatbots   humans intrinsically understand the effect their responses have on the interlocutor and often respond with an intention such as proposing an optimistic view to make the interlocutor feel better. This paper proposes an innovative <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> to train <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a> to possess human-like intentions. Our framework includes a guiding chatbot and an interlocutor model that plays the role of humans. The guiding chatbot is assigned an intention and learns to induce the interlocutor to reply with responses matching the intention, for example, long responses, joyful responses, responses with specific words, etc. We examined our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> using three experimental setups and evaluated the guiding chatbot with four different <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> to demonstrate flexibility and performance advantages. Additionally, we performed trials with human interlocutors to substantiate the guiding chatbot’s effectiveness in influencing the responses of humans to a certain extent. Code will be made available to the public.</abstract>
      <url hash="403dcb7b">2021.naacl-main.123</url>
      <doi>10.18653/v1/2021.naacl-main.123</doi>
      <bibkey>su-etal-2021-put</bibkey>
    </paper>
    <paper id="124">
      <title>Adding Chit-Chat to Enhance Task-Oriented Dialogues</title>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Paul</first><last>Crook</last></author>
      <author><first>Stephen</first><last>Roller</last></author>
      <author><first>Becka</first><last>Silvert</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Zhiguang</first><last>Wang</last></author>
      <author><first>Honglei</first><last>Liu</last></author>
      <author><first>Eunjoon</first><last>Cho</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <pages>1570–1583</pages>
      <abstract>Existing dialogue corpora and <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> are typically designed under two disjoint motives : while task-oriented systems focus on achieving functional goals (e.g., booking hotels), open-domain chatbots aim at making socially engaging conversations. In this work, we propose to integrate both types of systems by Adding <a href="https://en.wikipedia.org/wiki/Chit-chat">Chit-Chat</a> to ENhance Task-ORiented dialogues (ACCENTOR), with the goal of making virtual assistant conversations more engaging and interactive. Specifically, we propose a Human-AI collaborative data collection approach for generating diverse chit-chat responses to augment task-oriented dialogues with minimal annotation effort. We then present our new chit-chat-based annotations to 23.8 K dialogues from two popular <a href="https://en.wikipedia.org/wiki/Task_(computing)">task-oriented datasets</a> (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their advantage over the originals via human evaluation. Lastly, we propose three new models for adding <a href="https://en.wikipedia.org/wiki/Chit-chat">chit-chat</a> to task-oriented dialogues, explicitly trained to predict user goals and to generate contextually relevant chit-chat responses. Automatic and human evaluations show that, compared with the state-of-the-art task-oriented baseline, our models can code-switch between task and chit-chat to be more engaging, interesting, knowledgeable, and humanlike, while maintaining competitive task performance.</abstract>
      <url hash="b543a644">2021.naacl-main.124</url>
      <doi>10.18653/v1/2021.naacl-main.124</doi>
      <bibkey>sun-etal-2021-adding</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="129">
      <title>Did they answer? Subjective acts and intents in <a href="https://en.wikipedia.org/wiki/Conversation">conversational discourse</a></title>
      <author><first>Elisa</first><last>Ferracane</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Katrin</first><last>Erk</last></author>
      <pages>1626–1644</pages>
      <abstract>Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, <a href="https://en.wikipedia.org/wiki/Discourse">discourse</a> is embedded in a <a href="https://en.wikipedia.org/wiki/Social_environment">social context</a>, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse data</a> and frameworks ignore the <a href="https://en.wikipedia.org/wiki/Social_relation">social aspect</a>, expecting only a single ground truth. We present the first <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse dataset</a> with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents. We carefully analyze our dataset and create computational models to (1) confirm our hypothesis that taking into account the bias of the interpreters leads to better predictions of the interpretations, (2) and show disagreements are nuanced and require a deeper understanding of the different contextual factors. We share our dataset and code at http://github.com/elisaF/subjective_discourse.</abstract>
      <url hash="dff13f7d">2021.naacl-main.129</url>
      <doi>10.18653/v1/2021.naacl-main.129</doi>
      <bibkey>ferracane-etal-2021-answer</bibkey>
      <pwccode url="https://github.com/elisaF/subjective_discourse" additional="false">elisaF/subjective_discourse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/subjective-discourse">Subjective Discourse</pwcdataset>
    </paper>
    <paper id="130">
      <title>Evaluating the Impact of a Hierarchical Discourse Representation on Entity Coreference Resolution Performance</title>
      <author><first>Sopan</first><last>Khosla</last></author>
      <author><first>James</first><last>Fiacco</last></author>
      <author><first>Carolyn</first><last>Rosé</last></author>
      <pages>1645–1651</pages>
      <abstract>Recent work on entity coreference resolution (CR) follows current trends in <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> applied to embeddings and relatively simple task-related features. SOTA models do not make use of hierarchical representations of discourse structure. In this work, we leverage automatically constructed discourse parse trees within a neural approach and demonstrate a significant improvement on two benchmark entity coreference-resolution datasets. We explore how the impact varies depending upon the type of mention.</abstract>
      <url hash="ed7fe9fb">2021.naacl-main.130</url>
      <doi>10.18653/v1/2021.naacl-main.130</doi>
      <bibkey>khosla-etal-2021-evaluating</bibkey>
    </paper>
    <paper id="131">
      <title>Bridging Resolution : Making Sense of the State of the Art</title>
      <author><first>Hideo</first><last>Kobayashi</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>1652–1659</pages>
      <abstract>While Yu and Poesio (2020) have recently demonstrated the superiority of their neural multi-task learning (MTL) model to rule-based approaches for bridging anaphora resolution, there is little understanding of (1) how it is better than the rule-based approaches (e.g., are the two approaches making similar or complementary mistakes?) and (2) what should be improved. To shed light on these issues, we (1) propose a hybrid rule-based and MTL approach that would enable a better understanding of their comparative strengths and weaknesses ; and (2) perform a manual analysis of the errors made by the MTL model.</abstract>
      <url hash="43ddf872">2021.naacl-main.131</url>
      <doi>10.18653/v1/2021.naacl-main.131</doi>
      <bibkey>kobayashi-ng-2021-bridging</bibkey>
    </paper>
    <paper id="135">
      <title>Mask Attention Networks : Rethinking and Strengthen Transformer</title>
      <author><first>Zhihao</first><last>Fan</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Siyuan</first><last>Wang</last></author>
      <author><first>Jian</first><last>Jiao</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Ruofei</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>1692–1701</pages>
      <abstract>Transformer is an attention-based neural network, which consists of two sublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN). Existing research explores to enhance the two sublayers separately to improve the capability of Transformer for text representation. In this paper, we present a novel understanding of SAN and FFN as Mask Attention Networks (MANs) and show that they are two special cases of MANs with static mask matrices. However, their static mask matrices limit the capability for localness modeling in text representation learning. We therefore introduce a new layer named dynamic mask attention network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of <a href="https://en.wikipedia.org/wiki/DMAN">DMAN</a>, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> and <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a> demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms the original Transformer.</abstract>
      <url hash="720f933a">2021.naacl-main.135</url>
      <doi>10.18653/v1/2021.naacl-main.135</doi>
      <bibkey>fan-etal-2021-mask</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="136">
      <title>ERNIE-Gram : Pre-Training with Explicitly N-Gram Masked Language Modeling for <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">Natural Language Understanding</a><fixed-case>ERNIE</fixed-case>-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding</title>
      <author><first>Dongling</first><last>Xiao</last></author>
      <author><first>Yu-Kun</first><last>Li</last></author>
      <author><first>Han</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Hao</first><last>Tian</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>1702–1715</pages>
      <abstract>Coarse-grained linguistic information, such as named entities or phrases, facilitates adequately representation learning in pre-training. Previous works mainly focus on extending the objective of BERT’s Masked Language Modeling (MLM) from masking individual tokens to contiguous sequences of n tokens. We argue that such contiguously masking method neglects to model the intra-dependencies and inter-relation of coarse-grained linguistic information. As an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training. In ERNIE-Gram, <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a> are masked and predicted directly using explicit n-gram identities rather than contiguous sequences of n tokens. Furthermore, ERNIE-Gram employs a generator model to sample plausible n-gram identities as optional n-gram masks and predict them in both coarse-grained and fine-grained manners to enable comprehensive n-gram prediction and relation modeling. We pre-train ERNIE-Gram on English and Chinese text corpora and fine-tune on 19 downstream tasks. Experimental results show that ERNIE-Gram outperforms previous pre-training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.</abstract>
      <url hash="e9bf68ce">2021.naacl-main.136</url>
      <doi>10.18653/v1/2021.naacl-main.136</doi>
      <bibkey>xiao-etal-2021-ernie</bibkey>
      <pwccode url="https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/ernie_gram/modeling.py" additional="true">PaddlePaddle/PaddleNLP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cmrc">CMRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cmrc-2018">CMRC 2018</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drcd">DRCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dureader">DuReader</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="137">
      <title>Lattice-BERT : Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models<fixed-case>BERT</fixed-case>: Leveraging Multi-Granularity Representations in <fixed-case>C</fixed-case>hinese Pre-trained Language Models</title>
      <author><first>Yuxuan</first><last>Lai</last></author>
      <author><first>Yijia</first><last>Liu</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>1716–1731</pages>
      <abstract>Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more <a href="https://en.wikipedia.org/wiki/Granularity">coarse granularity</a>, e.g., <a href="https://en.wikipedia.org/wiki/Word">words</a>. In this work, we propose a novel pre-training paradigm for Chinese   Lattice-BERT, which explicitly incorporates word representations along with <a href="https://en.wikipedia.org/wiki/Chinese_characters">characters</a>, thus can model a sentence in a multi-granularity manner. Specifically, we construct a <a href="https://en.wikipedia.org/wiki/Lattice_graph">lattice graph</a> from the <a href="https://en.wikipedia.org/wiki/Character_(computing)">characters</a> and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the <a href="https://en.wikipedia.org/wiki/Lattice_model_(physics)">lattice structures</a> in self-attention layers. We further propose a masked segment prediction task to push the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to learn from rich but redundant information inherent in <a href="https://en.wikipedia.org/wiki/Lattice_model_(physics)">lattices</a>, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can bring an average increase of 1.5 % under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the <a href="https://en.wikipedia.org/wiki/Lattice_model_(physics)">lattice structures</a>, and the improvement comes from the exploration of <a href="https://en.wikipedia.org/wiki/Redundancy_(information_theory)">redundant information</a> and multi-granularity representations. Our code will be available at https://github.com/alibaba/pretrained-language-models/LatticeBERT.</abstract>
      <url hash="25f9f321">2021.naacl-main.137</url>
      <doi>10.18653/v1/2021.naacl-main.137</doi>
      <bibkey>lai-etal-2021-lattice</bibkey>
      <pwccode url="https://github.com/alibaba/AliceMind" additional="true">alibaba/AliceMind</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
    </paper>
    <paper id="139">
      <title>UmlsBERT : Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus<fixed-case>U</fixed-case>mls<fixed-case>BERT</fixed-case>: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the <fixed-case>U</fixed-case>nified <fixed-case>M</fixed-case>edical <fixed-case>L</fixed-case>anguage <fixed-case>S</fixed-case>ystem <fixed-case>M</fixed-case>etathesaurus</title>
      <author><first>George</first><last>Michalopoulos</last></author>
      <author><first>Yuanxin</first><last>Wang</last></author>
      <author><first>Hussam</first><last>Kaka</last></author>
      <author><first>Helen</first><last>Chen</last></author>
      <author><first>Alexander</first><last>Wong</last></author>
      <pages>1744–1753</pages>
      <abstract>Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have achieved state-of-the-art results in biomedical natural language processing tasks by focusing their pre-training process on domain-specific corpora. However, such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> do not take into consideration structured expert domain knowledge from a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a>. We introduce UmlsBERT, a contextual embedding model that integrates <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> during the pre-training process via a novel knowledge augmentation strategy. More specifically, the augmentation on UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus is performed in two ways : i) connecting words that have the same underlying ‘concept’ in UMLS and ii) leveraging semantic type knowledge in UMLS to create clinically meaningful input embeddings. By applying these two strategies, UmlsBERT can encode clinical domain knowledge into word embeddings and outperform existing domain-specific models on common named-entity recognition (NER) and clinical natural language inference tasks.</abstract>
      <url hash="bb95c792">2021.naacl-main.139</url>
      <doi>10.18653/v1/2021.naacl-main.139</doi>
      <bibkey>michalopoulos-etal-2021-umlsbert</bibkey>
      <pwccode url="https://github.com/gmichalo/UmlsBERT" additional="false">gmichalo/UmlsBERT</pwccode>
    </paper>
    <paper id="143">
      <title>Why Do Document-Level Polarity Classifiers Fail?</title>
      <author><first>Karen</first><last>Martins</last></author>
      <author><first>Pedro O.S</first><last>Vaz-de-Melo</last></author>
      <author><first>Rodrygo</first><last>Santos</last></author>
      <pages>1782–1794</pages>
      <abstract>Machine learning solutions are often criticized for the lack of explanation of their successes and failures. Understanding which instances are misclassified and why is essential to improve the <a href="https://en.wikipedia.org/wiki/Learning">learning process</a>. This work helps to fill this gap by proposing a <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> to characterize, quantify and measure the impact of hard instances in the task of polarity classification of movie reviews. We characterize such instances into two categories : neutrality, where the text does not convey a clear polarity, and discrepancy, where the polarity of the text is the opposite of its true rating. We quantify the number of hard instances in polarity classification of movie reviews and provide empirical evidence about the need to pay attention to such problematic instances, as they are much harder to classify, for both machine and human classifiers. To the best of our knowledge, this is the first systematic analysis of the impact of hard instances in polarity detection from well-formed textual reviews.</abstract>
      <url hash="5f282cc5">2021.naacl-main.143</url>
      <doi>10.18653/v1/2021.naacl-main.143</doi>
      <bibkey>martins-etal-2021-document</bibkey>
      <pwccode url="https://github.com/karenstemartins/NAACL2021" additional="false">karenstemartins/NAACL2021</pwccode>
    </paper>
    <paper id="147">
      <title>Domain Divergences : A Survey and Empirical Analysis</title>
      <author><first>Abhinav</first><last>Ramesh Kashyap</last></author>
      <author><first>Devamanyu</first><last>Hazarika</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>Roger</first><last>Zimmermann</last></author>
      <pages>1830–1849</pages>
      <abstract>Domain divergence plays a significant role in estimating the performance of a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> in <a href="https://en.wikipedia.org/wiki/Domain_of_a_function">new domains</a>. While there is a significant literature on <a href="https://en.wikipedia.org/wiki/Divergence">divergence measures</a>, researchers find it hard to choose an appropriate <a href="https://en.wikipedia.org/wiki/Divergence">divergence</a> for a given NLP application. We address this shortcoming by both surveying the literature and through an empirical study. We develop a taxonomy of divergence measures consisting of three classes   Information-theoretic, Geometric, and Higher-order measures and identify the relationships between them. Further, to understand the common use-cases of these measures, we recognise three novel applications   1) Data Selection, 2) Learning Representation, and 3) Decisions in the Wild   and use it to organise our literature. From this, we identify that Information-theoretic measures are prevalent for 1) and 3), and Higher-order measures are more common for 2). To further help researchers choose appropriate measures to predict drop in performance   an important aspect of Decisions in the Wild, we perform <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence">correlation analysis</a> spanning 130 domain adaptation scenarios, 3 varied NLP tasks and 12 divergence measures identified from our survey. To calculate these divergences, we consider the current contextual word representations (CWR) and contrast with the older distributed representations. We find that traditional <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">measures</a> over word distributions still serve as strong baselines, while higher-order measures with CWR are effective.</abstract>
      <url hash="f1023779">2021.naacl-main.147</url>
      <doi>10.18653/v1/2021.naacl-main.147</doi>
      <bibkey>ramesh-kashyap-etal-2021-domain</bibkey>
    </paper>
    <paper id="148">
      <title>Target-Aware Data Augmentation for Stance Detection</title>
      <author><first>Yingjie</first><last>Li</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>1850–1860</pages>
      <abstract>The goal of stance detection is to identify whether the author of a text is in favor of, neutral or against a specific target. Despite substantial progress on this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, one of the remaining challenges is the scarcity of <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a>. Data augmentation is commonly used to address annotation scarcity by generating more training samples. However, the augmented sentences that are generated by existing methods are either less diversified or inconsistent with the given target and stance label. In this paper, we formulate the data augmentation of stance detection as a conditional masked language modeling task and augment the dataset by predicting the masked word conditioned on both its context and the auxiliary sentence that contains target and label information. Moreover, we propose another simple yet effective method that generates target-aware sentence by replacing a target mention with the other. Experimental results show that our proposed <a href="https://en.wikipedia.org/wiki/Methods_of_detecting_exoplanets">methods</a> significantly outperforms previous augmentation methods on 11 targets.</abstract>
      <url hash="733545e5">2021.naacl-main.148</url>
      <doi>10.18653/v1/2021.naacl-main.148</doi>
      <bibkey>li-caragea-2021-target</bibkey>
    </paper>
    <paper id="151">
      <title>Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks</title>
      <author><first>Siddharth</first><last>Dalmia</last></author>
      <author><first>Brian</first><last>Yan</last></author>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <pages>1882–1896</pages>
      <abstract>End-to-end approaches for sequence tasks are becoming increasingly popular. Yet for complex sequence tasks, like <a href="https://en.wikipedia.org/wiki/Speech_translation">speech translation</a>, systems that cascade several models trained on sub-tasks have shown to be superior, suggesting that the compositionality of cascaded systems simplifies learning and enables sophisticated search capabilities. In this work, we present an end-to-end framework that exploits compositionality to learn searchable hidden representations at intermediate stages of a sequence model using decomposed sub-tasks. These hidden intermediates can be improved using <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> to enhance the overall performance and can also incorporate external models at intermediate stages of the <a href="https://en.wikipedia.org/wiki/Computer_network">network</a> to re-score or adapt towards out-of-domain data. One instance of the proposed framework is a Multi-Decoder model for <a href="https://en.wikipedia.org/wiki/Speech_translation">speech translation</a> that extracts the searchable hidden intermediates from a speech recognition sub-task. The model demonstrates the aforementioned benefits and outperforms the previous <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> by around +6 and +3 BLEU on the two test sets of Fisher-CallHome and by around +3 and +4 BLEU on the English-German and English-French test sets of MuST-C.</abstract>
      <url hash="338d6636">2021.naacl-main.151</url>
      <doi>10.18653/v1/2021.naacl-main.151</doi>
      <bibkey>dalmia-etal-2021-searchable</bibkey>
    </paper>
    <paper id="153">
      <title>Worldly Wise (WoW)-Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering<fixed-case>W</fixed-case>o<fixed-case>W</fixed-case>) - Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering</title>
      <author><first>Kiran</first><last>Ramnath</last></author>
      <author><first>Leda</first><last>Sari</last></author>
      <author><first>Mark</first><last>Hasegawa-Johnson</last></author>
      <author><first>Chang</first><last>Yoo</last></author>
      <pages>1908–1919</pages>
      <abstract>Although <a href="https://en.wikipedia.org/wiki/Question_answering">Question-Answering</a> has long been of research interest, its accessibility to users through a <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech interface</a> and its support to multiple languages have not been addressed in prior studies. Towards these ends, we present a new task and a synthetically-generated dataset to do Fact-based Visual Spoken-Question Answering (FVSQA). FVSQA is based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FVSQA, the question is spoken rather than typed. Three sub-tasks are proposed : (1) speech-to-text based, (2) end-to-end, without <a href="https://en.wikipedia.org/wiki/Speech-to-text">speech-to-text</a> as an intermediate component, and (3) cross-lingual, in which the question is spoken in a language different from that in which the KG is recorded. The end-to-end and cross-lingual tasks are the first to require world knowledge from a multi-relational KG as a differentiable layer in an end-to-end spoken language understanding task, hence the proposed reference implementation is called Worldly-Wise (WoW).WoW is shown to perform end-to-end cross-lingual FVSQA at same levels of accuracy across 3 languages-English, <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>, and <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a>.</abstract>
      <url hash="47edd518">2021.naacl-main.153</url>
      <attachment type="OptionalSupplementaryCode" hash="f66c37cb">2021.naacl-main.153.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="f66c37cb">2021.naacl-main.153.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.153</doi>
      <bibkey>ramnath-etal-2021-worldly</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/places">Places</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="154">
      <title>Align-Refine : Non-Autoregressive Speech Recognition via Iterative Realignment</title>
      <author><first>Ethan A.</first><last>Chi</last></author>
      <author><first>Julian</first><last>Salazar</last></author>
      <author><first>Katrin</first><last>Kirchhoff</last></author>
      <pages>1920–1927</pages>
      <abstract>Non-autoregressive encoder-decoder models greatly improve decoding speed over <a href="https://en.wikipedia.org/wiki/Autoregressive_model">autoregressive models</a>, at the expense of generation quality. To mitigate this, iterative decoding models repeatedly infill or refine the proposal of a non-autoregressive model. However, editing at the level of output sequences limits model flexibility. We instead propose * iterative realignment *, which by refining latent alignments allows more flexible edits in fewer steps. Our model, Align-Refine, is an end-to-end Transformer which iteratively realigns connectionist temporal classification (CTC) alignments. On the WSJ dataset, Align-Refine matches an autoregressive baseline with a 14x decoding speedup ; on LibriSpeech, we reach an LM-free test-other WER of 9.0 % (19 % relative improvement on comparable work) in three iterations. We release our code at https://github.com/amazon-research/align-refine.</abstract>
      <url hash="5b63d4e1">2021.naacl-main.154</url>
      <doi>10.18653/v1/2021.naacl-main.154</doi>
      <bibkey>chi-etal-2021-align</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="155">
      <title>Everything Has a Cause : Leveraging <a href="https://en.wikipedia.org/wiki/Causal_inference">Causal Inference</a> in Legal Text Analysis</title>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Yuting</first><last>Wu</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>1928–1941</pages>
      <abstract>Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with <a href="https://en.wikipedia.org/wiki/Structured_data">structured data</a>, while mining causal relationship among factors from <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured data</a>, like text, has been less examined, but is of great importance, especially in the <a href="https://en.wikipedia.org/wiki/Legal_research">legal domain</a>. In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal inference to facilitate legal practitioners to make proper decisions. We evaluate the <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> on a challenging similar charge disambiguation task. Experimental results show that GCI can capture the nuance from fact descriptions among multiple confusing charges and provide explainable discrimination, especially in few-shot settings. We also observe that the <a href="https://en.wikipedia.org/wiki/Causality">causal knowledge</a> contained in GCI can be effectively injected into powerful <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> for better performance and <a href="https://en.wikipedia.org/wiki/Interpretability">interpretability</a>.</abstract>
      <url hash="37f1a6ce">2021.naacl-main.155</url>
      <doi>10.18653/v1/2021.naacl-main.155</doi>
      <bibkey>liu-etal-2021-everything</bibkey>
      <pwccode url="https://github.com/xxxiaol/GCI" additional="false">xxxiaol/GCI</pwccode>
    </paper>
    <paper id="156">
      <title>Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network</title>
      <author><first>Haoran</first><last>Wu</last></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Shuang</first><last>Xu</last></author>
      <author><first>Bo</first><last>Xu</last></author>
      <pages>1942–1955</pages>
      <abstract>Providing a reliable explanation for <a href="https://en.wikipedia.org/wiki/Medical_diagnosis">clinical diagnosis</a> based on the Electronic Medical Record (EMR) is fundamental to the application of <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence</a> in the <a href="https://en.wikipedia.org/wiki/Medicine">medical field</a>. Current methods mostly treat the <a href="https://en.wikipedia.org/wiki/Electronic_health_record">EMR</a> as a text sequence and provide explanations based on a precise medical knowledge base, which is disease-specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract supporting facts from irregular EMR itself without external knowledge bases in this paper. Specifically, we first structure the sequence of EMR into a hierarchical graph network and then obtain the causal relationship between multi-granularity features and diagnosis results through counterfactual intervention on the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>. Features having the strongest causal connection with the results provide interpretive support for the <a href="https://en.wikipedia.org/wiki/Diagnosis">diagnosis</a>. Experimental results on real Chinese EMR of the <a href="https://en.wikipedia.org/wiki/Lymphedema">lymphedema</a> demonstrate that our method can diagnose four types of <a href="https://en.wikipedia.org/wiki/Electrophysiology">EMR</a> correctly, and can provide accurate supporting facts for the results. More importantly, the results on different diseases demonstrate the robustness of our approach, which represents the potential application in the <a href="https://en.wikipedia.org/wiki/Medicine">medical field</a>.</abstract>
      <url hash="aedd8ee0">2021.naacl-main.156</url>
      <doi>10.18653/v1/2021.naacl-main.156</doi>
      <bibkey>wu-etal-2021-counterfactual</bibkey>
      <pwccode url="https://github.com/ckre/cmge" additional="false">ckre/cmge</pwccode>
    </paper>
    <paper id="157">
      <title>Personalized Response Generation via Generative Split Memory Network</title>
      <author><first>Yuwei</first><last>Wu</last></author>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>1956–1970</pages>
      <abstract>Despite the impressive successes of generation and dialogue systems, how to endow a text generation system with particular <a href="https://en.wikipedia.org/wiki/Trait_theory">personality traits</a> to deliver more personalized responses remains under-investigated. In this work, we look at how to generate personalized responses for questions on Reddit by utilizing personalized user profiles and posting histories. Specifically, we release an open-domain single-turn dialog dataset made up of 1.5 M conversation pairs together with 300k profiles of users and related comments. We then propose a memory network to generate personalized responses in dialogue that utilizes a novel mechanism of splitting memories : one for user profile meta attributes and the other for user-generated information like comment histories. Experimental results show the quantitative and qualitative improvements of our simple split memory network model over the state-of-the-art response generation baselines.<i>single-turn</i> dialog dataset made up of 1.5M conversation pairs together with 300k profiles of users and related comments. We then propose a memory network to generate personalized responses in dialogue that utilizes a novel mechanism of splitting memories: one for user profile meta attributes and the other for user-generated information like comment histories. Experimental results show the quantitative and qualitative improvements of our simple split memory network model over the state-of-the-art response generation baselines.</abstract>
      <url hash="fe27b0a1">2021.naacl-main.157</url>
      <attachment type="OptionalSupplementaryData" hash="c9ae8735">2021.naacl-main.157.OptionalSupplementaryData.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.157</doi>
      <bibkey>wu-etal-2021-personalized</bibkey>
      <pwccode url="https://github.com/willyoung2017/per-chat" additional="false">willyoung2017/per-chat</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pec">PEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/personaldialog">PersonalDialog</pwcdataset>
    </paper>
    <paper id="158">
      <title>Towards Few-shot Fact-Checking via Perplexity</title>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Yejin</first><last>Bang</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>1971–1981</pages>
      <abstract>Few-shot learning has drawn researchers’ attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in few-shot learning for various downstream tasks, such as <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, <a href="https://en.wikipedia.org/wiki/Fact-checking">fact-checking</a> is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> via a perplexity score. The most notable strength of our <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> lies in its capability in few-shot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10 % on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of <a href="https://en.wikipedia.org/wiki/Fact-checking">fact-checking</a> and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to COVID-19.</abstract>
      <url hash="2a26b242">2021.naacl-main.158</url>
      <doi>10.18653/v1/2021.naacl-main.158</doi>
      <bibkey>lee-etal-2021-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
    </paper>
    <paper id="161">
      <title>Model Extraction and Adversarial Transferability, Your BERT is Vulnerable !<fixed-case>BERT</fixed-case> is Vulnerable!</title>
      <author><first>Xuanli</first><last>He</last></author>
      <author><first>Lingjuan</first><last>Lyu</last></author>
      <author><first>Lichao</first><last>Sun</last></author>
      <author><first>Qiongkai</first><last>Xu</last></author>
      <pages>2006–2012</pages>
      <abstract>Natural language processing (NLP) tasks, ranging from <a href="https://en.wikipedia.org/wiki/Text_classification">text classification</a> to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful <a href="https://en.wikipedia.org/wiki/Application_programming_interface">APIs</a> by encapsulating fine-tuned BERT models for <a href="https://en.wikipedia.org/wiki/Downstream_(networking)">downstream tasks</a>. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim / target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the <a href="https://en.wikipedia.org/wiki/Attack_model">attack model</a>. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.</abstract>
      <url hash="39018058">2021.naacl-main.161</url>
      <doi>10.18653/v1/2021.naacl-main.161</doi>
      <bibkey>he-etal-2021-model</bibkey>
      <pwccode url="https://github.com/xlhex/extract_and_transfer" additional="false">xlhex/extract_and_transfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="166">
      <title>DA-Transformer : Distance-aware Transformer<fixed-case>DA</fixed-case>-Transformer: Distance-aware Transformer</title>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>2059–2068</pages>
      <abstract>Transformer has achieved great success in the NLP field by composing various advanced <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually can not keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the <a href="https://en.wikipedia.org/wiki/Relevance_(information_retrieval)">relevance</a> between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants.</abstract>
      <url hash="a6ea20ce">2021.naacl-main.166</url>
      <doi>10.18653/v1/2021.naacl-main.166</doi>
      <bibkey>wu-etal-2021-da</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="170">
      <title>KPQA : A <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">Metric</a> for Generative Question Answering Using Keyphrase Weights<fixed-case>KPQA</fixed-case>: A Metric for Generative Question Answering Using Keyphrase Weights</title>
      <author><first>Hwanhee</first><last>Lee</last></author>
      <author><first>Seunghyun</first><last>Yoon</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Doo Soon</first><last>Kim</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Joongbo</first><last>Shin</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>2105–2115</pages>
      <abstract>In the automatic evaluation of generative question answering (GenQA) systems, it is difficult to assess the correctness of generated answers due to the free-form of the answer. Especially, widely used n-gram similarity metrics often fail to discriminate the incorrect answers since they equally consider all of the tokens. To alleviate this problem, we propose KPQA metric, a new <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> for evaluating the correctness of GenQA. Specifically, our new <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> assigns different weights to each token via keyphrase prediction, thereby judging whether a generated answer sentence captures the key meaning of the reference answer. To evaluate our <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>, we create high-quality human judgments of correctness on two GenQA datasets. Using our human-evaluation datasets, we show that our proposed <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> has a significantly higher correlation with human judgments than existing <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> in various <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>. Code for KPQA-metric will be available at https://github.com/hwanheelee1993/KPQA.</abstract>
      <url hash="fdae925e">2021.naacl-main.170</url>
      <attachment type="OptionalSupplementaryData" hash="5a65429a">2021.naacl-main.170.OptionalSupplementaryData.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.170</doi>
      <bibkey>lee-etal-2021-kpqa</bibkey>
      <pwccode url="https://github.com/hwanheelee1993/KPQA" additional="false">hwanheelee1993/KPQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="179">
      <title>Modeling Framing in Immigration Discourse on <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a></title>
      <author><first>Julia</first><last>Mendelsohn</last></author>
      <author><first>Ceren</first><last>Budak</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>2219–2263</pages>
      <abstract>The framing of political issues can influence <a href="https://en.wikipedia.org/wiki/Policy">policy</a> and public opinion. Even though the public plays a key role in creating and spreading frames, little is known about how ordinary people on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> frame political issues. By creating a new dataset of immigration-related tweets labeled for multiple <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">framing typologies</a> from political communication theory, we develop <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised models</a> to detect <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">frames</a>. We demonstrate how users’ ideology and region impact framing choices, and how a message’s framing influences audience responses. We find that the more commonly-used issue-generic frames obscure important ideological and regional patterns that are only revealed by immigration-specific frames. Furthermore, <a href="https://en.wikipedia.org/wiki/Framing_(social_sciences)">frames</a> oriented towards human interests, culture, and <a href="https://en.wikipedia.org/wiki/Politics">politics</a> are associated with higher <a href="https://en.wikipedia.org/wiki/User_engagement">user engagement</a>. This large-scale analysis of a complex social and linguistic phenomenon contributes to both NLP and social science research.</abstract>
      <url hash="3b8307d5">2021.naacl-main.179</url>
      <doi>10.18653/v1/2021.naacl-main.179</doi>
      <bibkey>mendelsohn-etal-2021-modeling</bibkey>
      <pwccode url="https://github.com/juliamendelsohn/framing" additional="false">juliamendelsohn/framing</pwccode>
    </paper>
    <paper id="184">
      <title>Learning to Recognize Dialect Features</title>
      <author><first>Dorottya</first><last>Demszky</last></author>
      <author><first>Devyani</first><last>Sharma</last></author>
      <author><first>Jonathan</first><last>Clark</last></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <pages>2315–2338</pages>
      <abstract>Building <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP systems</a> that serve everyone requires accounting for <a href="https://en.wikipedia.org/wiki/Dialect">dialect differences</a>. But <a href="https://en.wikipedia.org/wiki/List_of_dialects_of_English">dialects</a> are not monolithic entities : rather, distinctions between and within dialects are captured by the presence, absence, and frequency of dozens of <a href="https://en.wikipedia.org/wiki/Dialect">dialect features</a> in speech and text, such as the deletion of the copula in He   running. In this paper, we introduce the task of dialect feature detection, and present two multitask learning approaches, both based on pretrained transformers. For most <a href="https://en.wikipedia.org/wiki/Dialect">dialects</a>, large-scale annotated corpora for these <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> are unavailable, making it difficult to train recognizers. We train our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on a small number of minimal pairs, building on how linguists typically define <a href="https://en.wikipedia.org/wiki/Dialect">dialect features</a>. Evaluation on a test set of 22 dialect features of <a href="https://en.wikipedia.org/wiki/Indian_English">Indian English</a> demonstrates that these models learn to recognize many <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> with high accuracy, and that a few minimal pairs can be as effective for training as thousands of labeled examples. We also demonstrate the downstream applicability of dialect feature detection both as a measure of <a href="https://en.wikipedia.org/wiki/Dialect_continuum">dialect density</a> and as a dialect classifier.</abstract>
      <url hash="3719b28f">2021.naacl-main.184</url>
      <doi>10.18653/v1/2021.naacl-main.184</doi>
      <bibkey>demszky-etal-2021-learning</bibkey>
    </paper>
    <paper id="187">
      <title>Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis<fixed-case>O</fixed-case>rthogonal <fixed-case>P</fixed-case>rocrustes <fixed-case>A</fixed-case>nalysis</title>
      <author><first>Xutan</first><last>Peng</last></author>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Mark</first><last>Stevenson</last></author>
      <pages>2364–2375</pages>
      <abstract>Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance without acknowledging the <a href="https://en.wikipedia.org/wiki/Computational_cost">computational cost</a> of the proposed approaches, in terms of <a href="https://en.wikipedia.org/wiki/Time_complexity">execution time</a> and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and <a href="https://en.wikipedia.org/wiki/Carbon_footprint">carbon footprint</a> by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations : full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a>.</abstract>
      <url hash="66ad8e14">2021.naacl-main.187</url>
      <doi>10.18653/v1/2021.naacl-main.187</doi>
      <bibkey>peng-etal-2021-highly</bibkey>
      <pwccode url="https://github.com/Pzoom522/ProcrustEs-KGE" additional="false">Pzoom522/ProcrustEs-KGE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="188">
      <title>Rethinking Network Pruning   under the Pre-train and Fine-tune Paradigm</title>
      <author><first>Dongkuan</first><last>Xu</last></author>
      <author><first>Ian En-Hsu</first><last>Yen</last></author>
      <author><first>Jinxi</first><last>Zhao</last></author>
      <author><first>Zhibin</first><last>Xiao</last></author>
      <pages>2376–2382</pages>
      <abstract>Transformer-based pre-trained language models have significantly improved the performance of various natural language processing (NLP) tasks in the recent years. While effective and prevalent, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are usually prohibitively large for resource-limited deployment scenarios. A thread of research has thus been working on applying network pruning techniques under the pretrain-then-finetune paradigm widely adopted in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. However, the existing <a href="https://en.wikipedia.org/wiki/Pruning">pruning</a> results on benchmark transformers, such as BERT, are not as remarkable as the pruning results in the literature of convolutional neural networks (CNNs). In particular, common wisdom in pruning CNN states that sparse pruning technique compresses a model more than that obtained by reducing number of channels and layers, while existing works on sparse pruning of BERT yields inferior results than its small-dense counterparts such as TinyBERT. In this work, we aim to fill this gap by studying how knowledge are transferred and lost during the pre-train, fine-tune, and pruning process, and proposing a knowledge-aware sparse pruning process that achieves significantly superior results than existing literature. We show for the first time that sparse pruning compresses a BERT model significantly more than reducing its number of channels and layers. Experiments on multiple data sets of GLUE benchmark show that our method outperforms the leading competitors with a 20-times weight / FLOPs compression and neglectable loss in prediction accuracy.</abstract>
      <url hash="a3b39a71">2021.naacl-main.188</url>
      <attachment type="OptionalSupplementaryCode" hash="e435a5d9">2021.naacl-main.188.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.188</doi>
      <bibkey>xu-etal-2021-rethinking</bibkey>
      <pwccode url="https://github.com/derronxu/sparsebert" additional="false">derronxu/sparsebert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="190">
      <title>Detoxifying Language Models Risks Marginalizing Minority Voices</title>
      <author><first>Albert</first><last>Xu</last></author>
      <author><first>Eshaan</first><last>Pathak</last></author>
      <author><first>Eric</first><last>Wallace</last></author>
      <author><first>Suchin</first><last>Gururangan</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>2390–2397</pages>
      <abstract>Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020 ; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity : they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different <a href="https://en.wikipedia.org/wiki/Dialect">dialects</a> and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by <a href="https://en.wikipedia.org/wiki/Social_exclusion">marginalized groups</a>. We identify that these failures stem from <a href="https://en.wikipedia.org/wiki/Detoxification_(alternative_medicine)">detoxification methods</a> exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the <a href="https://en.wikipedia.org/wiki/Controllability">controllability</a> and distributional robustness of LMs.</abstract>
      <url hash="b7430224">2021.naacl-main.190</url>
      <doi>10.18653/v1/2021.naacl-main.190</doi>
      <bibkey>xu-etal-2021-detoxifying</bibkey>
      <pwccode url="https://github.com/albertkx/detoxifying-lms" additional="false">albertkx/detoxifying-lms</pwccode>
    </paper>
    <paper id="191">
      <title>HONEST : Measuring Hurtful Sentence Completion in Language Models<fixed-case>HONEST</fixed-case>: Measuring Hurtful Sentence Completion in Language Models</title>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>2398–2406</pages>
      <abstract>Language models have revolutionized the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. However, <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> capture and proliferate hurtful stereotypes, especially in <a href="https://en.wikipedia.org/wiki/Text_generation">text generation</a>. Our results show that 4.3 % of the time, <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a <a href="https://en.wikipedia.org/wiki/Score_(statistics)">score</a> to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for six languages. Our findings suggest that these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> replicate and amplify deep-seated societal stereotypes about <a href="https://en.wikipedia.org/wiki/Gender_role">gender roles</a>. Sentence completions refer to <a href="https://en.wikipedia.org/wiki/Promiscuity">sexual promiscuity</a> when the target is female in 9 % of the time, and in 4 % to <a href="https://en.wikipedia.org/wiki/Homosexuality">homosexuality</a> when the target is male. The results raise questions about the use of these <a href="https://en.wikipedia.org/wiki/Physical_model">models</a> in <a href="https://en.wikipedia.org/wiki/Production_line">production settings</a>.</abstract>
      <url hash="fc53b3e0">2021.naacl-main.191</url>
      <doi>10.18653/v1/2021.naacl-main.191</doi>
      <bibkey>nozza-etal-2021-honest</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/honest-en">HONEST</pwcdataset>
    </paper>
    <paper id="193">
      <title>DeCEMBERT : Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization<fixed-case>D</fixed-case>e<fixed-case>CEMBERT</fixed-case>: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization</title>
      <author><first>Zineng</first><last>Tang</last></author>
      <author><first>Jie</first><last>Lei</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>2415–2426</pages>
      <abstract>Leveraging large-scale unlabeled web videos such as instructional videos for pre-training followed by task-specific finetuning has become the de facto approach for many video-and-language tasks. However, these instructional videos are very noisy, the accompanying ASR narrations are often incomplete, and can be irrelevant to or temporally misaligned with the visual content, limiting the performance of the models trained on such data. To address these issues, we propose an improved video-and-language pre-training method that first adds automatically-extracted dense region captions from the video frames as auxiliary text input, to provide informative visual cues for learning better video and language associations. Second, to alleviate the temporal misalignment issue, our method incorporates an entropy minimization-based constrained attention loss, to encourage the model to automatically focus on the correct caption from a pool of candidate ASR captions. Our overall approach is named DeCEMBERT (Dense Captions and Entropy Minimization). Comprehensive experiments on three video-and-language tasks (text-to-video retrieval, video captioning, and video question answering) across five datasets demonstrate that our approach outperforms previous state-of-the-art methods. Ablation studies on pre-training and downstream tasks show that adding dense captions and constrained attention loss help improve the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance. Lastly, we also provide attention visualization to show the effect of applying the proposed constrained attention loss.</abstract>
      <url hash="1473810e">2021.naacl-main.193</url>
      <doi>10.18653/v1/2021.naacl-main.193</doi>
      <bibkey>tang-etal-2021-decembert</bibkey>
      <pwccode url="https://github.com/zinengtang/decembert" additional="false">zinengtang/decembert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/howto100m">HowTo100M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/youcook2">YouCook2</pwcdataset>
    </paper>
    <paper id="195">
      <title>Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models</title>
      <author><first>Po-Yao</first><last>Huang</last></author>
      <author><first>Mandela</first><last>Patrick</last></author>
      <author><first>Junjie</first><last>Hu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>Alexander</first><last>Hauptmann</last></author>
      <pages>2443–2459</pages>
      <abstract>This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (Multi-HowTo100 M) for pre-training. Experiments on VTT show that our method significantly improves <a href="https://en.wikipedia.org/wiki/Video_search_engine">video search</a> in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX ; as well as in multilingual text-to-image search on Multi30K. Our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> and Multi-HowTo100 M is available at http://github.com/berniebear/Multi-HT100M.</abstract>
      <url hash="52e407d9">2021.naacl-main.195</url>
      <attachment type="OptionalSupplementaryData" hash="0546d73d">2021.naacl-main.195.OptionalSupplementaryData.txt</attachment>
      <doi>10.18653/v1/2021.naacl-main.195</doi>
      <bibkey>huang-etal-2021-multilingual</bibkey>
      <pwccode url="https://github.com/berniebear/Multi-HT100M" additional="false">berniebear/Multi-HT100M</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/howto100m">HowTo100M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vatex">VATEX</pwcdataset>
    </paper>
    <paper id="196">
      <title>Video Question Answering with Phrases via Semantic Roles</title>
      <author><first>Arka</first><last>Sadhu</last></author>
      <author><first>Kan</first><last>Chen</last></author>
      <author><first>Ram</first><last>Nevatia</last></author>
      <pages>2460–2478</pages>
      <abstract>Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> limit the VidQA models’ application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fill-in-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty string. To reduce the influence of language bias in VidQA datasets, we retrieve a video having a different answer for the same question. To facilitate research, we construct ActivityNet-SRL-QA and Charades-SRL-QA and benchmark them by extending three vision-language models. We perform extensive analysis and ablative studies to guide future work. Code and data are public.</abstract>
      <url hash="41bb2b6a">2021.naacl-main.196</url>
      <doi>10.18653/v1/2021.naacl-main.196</doi>
      <bibkey>sadhu-etal-2021-video</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/activitynet-qa">ActivityNet-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades">Charades</pwcdataset>
    </paper>
    <paper id="197">
      <title>From Masked Language Modeling to <a href="https://en.wikipedia.org/wiki/Translation">Translation</a> : Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding<fixed-case>E</fixed-case>nglish Auxiliary Tasks Improve Zero-shot Spoken Language Understanding</title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Ibrahim</first><last>Sharaf</last></author>
      <author><first>Aizhan</first><last>Imankulova</last></author>
      <author><first>Ahmet</first><last>Üstün</last></author>
      <author><first>Marija</first><last>Stepanović</last></author>
      <author><first>Alan</first><last>Ramponi</last></author>
      <author><first>Siti Oryza</first><last>Khairunnisa</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>2479–2497</pages>
      <abstract>The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing <a href="https://en.wikipedia.org/wiki/Data">data</a> in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual (x) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> and <a href="https://en.wikipedia.org/wiki/Translation">translation</a> for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification.</abstract>
      <url hash="e4433f83">2021.naacl-main.197</url>
      <doi>10.18653/v1/2021.naacl-main.197</doi>
      <bibkey>van-der-goot-etal-2021-masked</bibkey>
      <pwccode url="https://github.com/Kaleidophon/deep-significance" additional="true">Kaleidophon/deep-significance</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xsid">xSID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="199">
      <title>Challenging <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">distributional models</a> with a conceptual network of philosophical terms</title>
      <author><first>Yvette</first><last>Oortwijn</last></author>
      <author><first>Jelke</first><last>Bloem</last></author>
      <author><first>Pia</first><last>Sommerauer</last></author>
      <author><first>Francois</first><last>Meyer</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <pages>2511–2522</pages>
      <abstract>Computational linguistic research on <a href="https://en.wikipedia.org/wiki/Language_change">language change</a> through distributional semantic (DS) models has inspired researchers from fields such as philosophy and literary studies, who use these methods for the exploration and comparison of comparatively small datasets traditionally analyzed by close reading. Research on <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for <a href="https://en.wikipedia.org/wiki/Small_data">small data</a> is still in early stages and it is not clear which methods achieve the best results. We investigate the possibilities and limitations of using distributional semantic models for analyzing philosophical data by means of a realistic use-case. We provide a ground truth for evaluation created by philosophy experts and a blueprint for using DS models in a sound methodological setup. We compare three <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for creating specialized models from <a href="https://en.wikipedia.org/wiki/Data_set">small datasets</a>. Though the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> do not perform well enough to directly support philosophers yet, we find that <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> designed for small data yield promising directions for future work.</abstract>
      <url hash="ad6a5098">2021.naacl-main.199</url>
      <doi>10.18653/v1/2021.naacl-main.199</doi>
      <bibkey>oortwijn-etal-2021-challenging</bibkey>
      <pwccode url="https://github.com/yoortwijn/challenging_dms" additional="false">yoortwijn/challenging_dms</pwccode>
    </paper>
    <paper id="200">
      <title>KILT : a Benchmark for Knowledge Intensive Language Tasks<fixed-case>KILT</fixed-case>: a Benchmark for Knowledge Intensive Language Tasks</title>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Aleksandra</first><last>Piktus</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Patrick</first><last>Lewis</last></author>
      <author><first>Majid</first><last>Yazdani</last></author>
      <author><first>Nicola</first><last>De Cao</last></author>
      <author><first>James</first><last>Thorne</last></author>
      <author><first>Yacine</first><last>Jernite</last></author>
      <author><first>Vladimir</first><last>Karpukhin</last></author>
      <author><first>Jean</first><last>Maillard</last></author>
      <author><first>Vassilis</first><last>Plachouras</last></author>
      <author><first>Tim</first><last>Rocktäschel</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <pages>2523–2544</pages>
      <abstract>Challenging problems such as open-domain question answering, <a href="https://en.wikipedia.org/wiki/Fact-checking">fact checking</a>, slot filling and <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity linking</a> require access to large, external knowledge sources. While some <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> that condition on specific information in large textual resources, we present a <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmark</a> for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, reducing <a href="https://en.wikipedia.org/wiki/Turnaround_time">engineering turnaround</a> through the re-use of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to provide <a href="https://en.wikipedia.org/wiki/Provenance">provenance</a>. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https://github.com/facebookresearch/KILT.</abstract>
      <url hash="21842ec2">2021.naacl-main.200</url>
      <doi>10.18653/v1/2021.naacl-main.200</doi>
      <bibkey>petroni-etal-2021-kilt</bibkey>
      <pwccode url="https://github.com/facebookresearch/KILT" additional="true">facebookresearch/KILT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="203">
      <title>UDALM : Unsupervised Domain Adaptation through <a href="https://en.wikipedia.org/wiki/Language_model">Language Modeling</a><fixed-case>UDALM</fixed-case>: Unsupervised Domain Adaptation through Language Modeling</title>
      <author><first>Constantinos</first><last>Karouzos</last></author>
      <author><first>Georgios</first><last>Paraskevopoulos</last></author>
      <author><first>Alexandros</first><last>Potamianos</last></author>
      <pages>2579–2590</pages>
      <abstract>In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion during UDA training. Furthermore, we discuss the relationship between A-distance and the target error and explore some limitations of the Domain Adversarial Training approach. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is evaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset, yielding 91.74 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, which is an 1.11 % absolute improvement over the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>.</abstract>
      <url hash="056ceec8">2021.naacl-main.203</url>
      <doi>10.18653/v1/2021.naacl-main.203</doi>
      <bibkey>karouzos-etal-2021-udalm</bibkey>
      <pwccode url="https://github.com/ckarouzos/slp_daptmlm" additional="false">ckarouzos/slp_daptmlm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-domain-sentiment-dataset-v2-0">Multi-Domain Sentiment Dataset v2.0</pwcdataset>
    </paper>
    <paper id="204">
      <title>Beyond Black &amp; White : Leveraging Annotator Disagreement via Soft-Label Multi-Task Learning</title>
      <author><first>Tommaso</first><last>Fornaciari</last></author>
      <author><first>Alexandra</first><last>Uma</last></author>
      <author><first>Silviu</first><last>Paun</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>2591–2597</pages>
      <abstract>Supervised learning assumes that a ground truth label exists. However, the <a href="https://en.wikipedia.org/wiki/Reliability_(statistics)">reliability</a> of this ground truth depends on human annotators, who often disagree. Prior work has shown that this disagreement can be helpful in training <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. We propose a novel method to incorporate this disagreement as information : in addition to the standard error computation, we use soft-labels (i.e., probability distributions over the annotator labels) as an auxiliary task in a multi-task neural network. We measure the <a href="https://en.wikipedia.org/wiki/Divergence">divergence</a> between the predictions and the target soft-labels with several <a href="https://en.wikipedia.org/wiki/Loss_function">loss-functions</a> and evaluate the models on various <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP tasks</a>. We find that the soft-label prediction auxiliary task reduces the penalty for errors on ambiguous entities, and thereby mitigates <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>. It significantly improves performance across <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>, beyond the standard approach and prior work.</abstract>
      <url hash="0b8200db">2021.naacl-main.204</url>
      <doi>10.18653/v1/2021.naacl-main.204</doi>
      <bibkey>fornaciari-etal-2021-beyond</bibkey>
    </paper>
    <paper id="205">
      <title>Clustering-based Inference for Biomedical Entity Linking</title>
      <author><first>Rico</first><last>Angell</last></author>
      <author><first>Nicholas</first><last>Monath</last></author>
      <author><first>Sunil</first><last>Mohan</last></author>
      <author><first>Nishant</first><last>Yadav</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>2598–2608</pages>
      <abstract>Due to large number of entities in biomedical knowledge bases, only a small fraction of entities have corresponding labelled training data. This necessitates entity linking models which are able to link mentions of unseen entities using learned representations of entities. Previous approaches link each mention independently, ignoring the relationships within and across documents between the entity mentions. These relations can be very useful for linking mentions in biomedical text where linking decisions are often difficult due mentions having a generic or a highly specialized form. In this paper, we introduce a model in which linking decisions can be made not merely by linking to a knowledge base entity but also by grouping multiple mentions together via <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering</a> and jointly making linking predictions. In experiments on the largest publicly available biomedical dataset, we improve the best <a href="https://en.wikipedia.org/wiki/Independence_(probability_theory)">independent prediction</a> for <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity linking</a> by 3.0 points of <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, and our clustering-based inference model further improves <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity linking</a> by 2.3 points.</abstract>
      <url hash="c5862774">2021.naacl-main.205</url>
      <doi>10.18653/v1/2021.naacl-main.205</doi>
      <bibkey>angell-etal-2021-clustering</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
    </paper>
    <paper id="207">
      <title>Diversity-Aware Batch Active Learning for Dependency Parsing</title>
      <author><first>Tianze</first><last>Shi</last></author>
      <author><first>Adrian</first><last>Benton</last></author>
      <author><first>Igor</first><last>Malioutov</last></author>
      <author><first>Ozan</first><last>İrsoy</last></author>
      <pages>2616–2626</pages>
      <abstract>While the predictive performance of modern statistical dependency parsers relies heavily on the availability of expensive expert-annotated treebank data, not all annotations contribute equally to the training of the <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a>. In this paper, we attempt to reduce the number of labeled examples needed to train a strong dependency parser using batch active learning (AL). In particular, we investigate whether enforcing diversity in the sampled batches, using determinantal point processes (DPPs), can improve over their diversity-agnostic counterparts. Simulation experiments on an English newswire corpus show that selecting diverse batches with DPPs is superior to strong selection strategies that do not enforce batch diversity, especially during the initial stages of the learning process. Additionally, our diversity-aware strategy is robust under a corpus duplication setting, where diversity-agnostic sampling strategies exhibit significant degradation.</abstract>
      <url hash="b34ca299">2021.naacl-main.207</url>
      <doi>10.18653/v1/2021.naacl-main.207</doi>
      <bibkey>shi-etal-2021-diversity</bibkey>
      <pwccode url="https://github.com/tzshi/dpp-al-parsing-naacl21" additional="false">tzshi/dpp-al-parsing-naacl21</pwccode>
    </paper>
    <paper id="209">
      <title>Can Latent Alignments Improve Autoregressive Machine Translation?</title>
      <author><first>Adi</first><last>Haviv</last></author>
      <author><first>Lior</first><last>Vassertail</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>2637–2641</pages>
      <abstract>Latent alignment objectives such as CTC and AXE significantly improve non-autoregressive machine translation models. Can they improve <a href="https://en.wikipedia.org/wiki/Autoregressive_model">autoregressive models</a> as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher forcing.</abstract>
      <url hash="b7b4ab4d">2021.naacl-main.209</url>
      <doi>10.18653/v1/2021.naacl-main.209</doi>
      <bibkey>haviv-etal-2021-latent</bibkey>
    </paper>
    <paper id="210">
      <title>Smoothing and Shrinking the Sparse Seq2Seq Search Space<fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Search Space</title>
      <author><first>Ben</first><last>Peters</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>2642–2654</pages>
      <abstract>Current sequence-to-sequence models are trained to minimize <a href="https://en.wikipedia.org/wiki/Cross-entropy">cross-entropy</a> and use <a href="https://en.wikipedia.org/wiki/Softmax">softmax</a> to compute the locally normalized probabilities over target sequences. While this setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias : models give high scores to short, inadequate hypotheses and often make the empty string the argmaxthe so-called cat got your tongue problem. Recently proposed entmax-based sparse sequence-to-sequence models present a possible solution, since they can shrink the search space by assigning zero probability to bad hypotheses, but their ability to handle word-level tasks with transformers has never been tested. In this work, we show that entmax-based models effectively solve the cat got your tongue problem, removing a major source of model error for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>. In addition, we generalize label smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, which includes both <a href="https://en.wikipedia.org/wiki/Cross-entropy">cross-entropy</a> and the entmax losses. Our resulting label-smoothed entmax loss models set a new state of the art on multilingual grapheme-to-phoneme conversion and deliver improvements and better calibration properties on cross-lingual morphological inflection and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> for 7 language pairs.</abstract>
      <url hash="58efe875">2021.naacl-main.210</url>
      <doi>10.18653/v1/2021.naacl-main.210</doi>
      <bibkey>peters-martins-2021-smoothing</bibkey>
      <pwccode url="https://github.com/deep-spin/S7" additional="false">deep-spin/S7</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="214">
      <title>Cross-Lingual Word Embedding Refinement by _ 1 Norm Optimisation<tex-math>\ell_{1}</tex-math> Norm Optimisation</title>
      <author><first>Xutan</first><last>Peng</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Mark</first><last>Stevenson</last></author>
      <pages>2690–2701</pages>
      <abstract>Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn <a href="https://en.wikipedia.org/wiki/Map_(mathematics)">mappings</a> that minimise the 2 norm loss function. However, this optimisation objective has been demonstrated to be sensitive to <a href="https://en.wikipedia.org/wiki/Outlier">outliers</a>. Based on the more robust Manhattan norm (aka. 1 norm) goodness-of-fit criterion, this paper proposes a simple post-processing step to improve CLWEs. An advantage of this approach is that it is fully agnostic to the training process of the original CLWEs and can therefore be applied widely. Extensive experiments are performed involving ten diverse languages and embeddings trained on different corpora. Evaluation results based on bilingual lexicon induction and cross-lingual transfer for natural language inference tasks show that the 1 refinement substantially outperforms four state-of-the-art baselines in both supervised and unsupervised settings. It is therefore recommended that this <a href="https://en.wikipedia.org/wiki/Strategy">strategy</a> be adopted as a standard for CLWE methods.</abstract>
      <url hash="c63759b9">2021.naacl-main.214</url>
      <doi>10.18653/v1/2021.naacl-main.214</doi>
      <bibkey>peng-etal-2021-cross</bibkey>
      <pwccode url="https://github.com/Pzoom522/L1-Refinement" additional="false">Pzoom522/L1-Refinement</pwccode>
    </paper>
    <paper id="220">
      <title>Learning to Synthesize Data for Semantic Parsing</title>
      <author><first>Bailin</first><last>Wang</last></author>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>2760–2766</pages>
      <abstract>Synthesizing data for <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a> has gained increasing attention recently. However, most methods require handcrafted (high-precision) rules in their generative process, hindering the exploration of diverse unseen data. In this work, we propose a <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> which features a (non-neural) PCFG that models the composition of programs (e.g., SQL), and a BART-based translation model that maps a program to an utterance. Due to the simplicity of PCFG and pre-trained BART, our <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> can be efficiently learned from existing data at hand. Moreover, explicitly modeling compositions using PCFG leads to better exploration of unseen programs, thus generate more diverse data. We evaluate our method in both in-domain and out-of-domain settings of text-to-SQL parsing on the standard benchmarks of GeoQuery and Spider, respectively. Our empirical results show that the synthesized data generated from our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can substantially help a <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a> achieve better compositional and domain generalization.</abstract>
      <url hash="ec05c9ba">2021.naacl-main.220</url>
      <doi>10.18653/v1/2021.naacl-main.220</doi>
      <bibkey>wang-etal-2021-learning-synthesize</bibkey>
      <pwccode url="https://github.com/berlino/tensor2struct-public" additional="false">berlino/tensor2struct-public</pwccode>
    </paper>
    <paper id="221">
      <title>Edge : Enriching Knowledge Graph Embeddings with External Text</title>
      <author><first>Saed</first><last>Rezayi</last></author>
      <author><first>Handong</first><last>Zhao</last></author>
      <author><first>Sungchul</first><last>Kim</last></author>
      <author><first>Ryan</first><last>Rossi</last></author>
      <author><first>Nedim</first><last>Lipka</last></author>
      <author><first>Sheng</first><last>Li</last></author>
      <pages>2767–2776</pages>
      <abstract>Knowledge graphs suffer from sparsity which degrades the quality of representations generated by various methods. While there is an abundance of textual information throughout the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a> and many existing <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a>, aligning information across these diverse data sources remains a challenge in the literature. Previous work has partially addressed this issue by enriching knowledge graph entities based on hard co-occurrence of words present in the entities of the knowledge graphs and external text, while we achieve soft augmentation by proposing a knowledge graph enrichment and embedding framework named Edge. Given an original <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a>, we first generate a rich but noisy augmented graph using external texts in semantic and structural level. To distill the relevant knowledge and suppress the introduced noise, we design a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph alignment term</a> in a shared embedding space between the original <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> and augmented graph. To enhance the embedding learning on the augmented graph, we further regularize the locality relationship of target entity based on negative sampling. Experimental results on four benchmark datasets demonstrate the robustness and effectiveness of Edge in link prediction and node classification.</abstract>
      <url hash="18246654">2021.naacl-main.221</url>
      <doi>10.18653/v1/2021.naacl-main.221</doi>
      <bibkey>rezayi-etal-2021-edge</bibkey>
    </paper>
    <paper id="225">
      <title>Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention</title>
      <author><first>Pengcheng</first><last>Yin</last></author>
      <author><first>Hao</first><last>Fang</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Adam</first><last>Pauls</last></author>
      <author><first>Emmanouil Antonios</first><last>Platanios</last></author>
      <author><first>Yu</first><last>Su</last></author>
      <author><first>Sam</first><last>Thomson</last></author>
      <author><first>Jacob</first><last>Andreas</last></author>
      <pages>2810–2823</pages>
      <abstract>We describe a span-level supervised attention loss that improves compositional generalization in <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parsers</a>. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans ; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, <a href="https://en.wikipedia.org/wiki/Radio-frequency_identification">RNNs</a>, and structured decoders on three benchmarks of compositional generalization.</abstract>
      <url hash="6ac8b79f">2021.naacl-main.225</url>
      <doi>10.18653/v1/2021.naacl-main.225</doi>
      <bibkey>yin-etal-2021-compositional</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cfq">CFQ</pwcdataset>
    </paper>
    <paper id="229">
      <title>Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level Sentiment Classification</title>
      <author><first>Xiaochen</first><last>Hou</last></author>
      <author><first>Peng</first><last>Qi</last></author>
      <author><first>Guangtao</first><last>Wang</last></author>
      <author><first>Rex</first><last>Ying</last></author>
      <author><first>Jing</first><last>Huang</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <author><first>Bowen</first><last>Zhou</last></author>
      <pages>2884–2894</pages>
      <abstract>Recent work on aspect-level sentiment classification has demonstrated the efficacy of incorporating syntactic structures such as dependency trees with graph neural networks (GNN), but these approaches are usually vulnerable to parsing errors. To better leverage syntactic information in the face of unavoidable errors, we propose a simple yet effective graph ensemble technique, GraphMerge, to make use of the predictions from different <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a>. Instead of assigning one set of model parameters to each dependency tree, we first combine the dependency relations from different <a href="https://en.wikipedia.org/wiki/Parsing">parses</a> before applying GNNs over the resulting <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>. This allows GNN models to be robust to parse errors at no additional computational cost, and helps avoid overparameterization and <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> from GNN layer stacking by introducing more connectivity into the ensemble graph. Our experiments on the SemEval 2014 Task 4 and ACL 14 Twitter datasets show that our GraphMerge model not only outperforms models with single dependency tree, but also beats other ensemble models without adding model parameters.</abstract>
      <url hash="b0ebff2f">2021.naacl-main.229</url>
      <doi>10.18653/v1/2021.naacl-main.229</doi>
      <bibkey>hou-etal-2021-graph</bibkey>
    </paper>
    <paper id="230">
      <title>Emotion-Infused Models for Explainable Psychological Stress Detection</title>
      <author><first>Elsbeth</first><last>Turcan</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>2895–2909</pages>
      <abstract>The problem of detecting psychological stress in online posts, and more broadly, of detecting people in distress or in need of help, is a sensitive application for which the ability to interpret models is vital. Here, we present work exploring the use of a semantically related task, <a href="https://en.wikipedia.org/wiki/Emotion_detection">emotion detection</a>, for equally competent but more explainable and human-like psychological stress detection as compared to a black-box model. In particular, we explore the use of <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> as well as emotion-based language model fine-tuning. With our emotion-infused models, we see comparable results to state-of-the-art BERT. Our analysis of the words used for prediction show that our emotion-infused models mirror psychological components of stress.</abstract>
      <url hash="553ed5de">2021.naacl-main.230</url>
      <doi>10.18653/v1/2021.naacl-main.230</doi>
      <bibkey>turcan-etal-2021-emotion</bibkey>
      <pwccode url="https://github.com/eturcan/emotion-infused" additional="false">eturcan/emotion-infused</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dreaddit">Dreaddit</pwcdataset>
    </paper>
    <paper id="231">
      <title>Aspect-based Sentiment Analysis with Type-aware Graph Convolutional Networks and Layer Ensemble</title>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Guimin</first><last>Chen</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>2910–2922</pages>
      <abstract>It is popular that neural graph-based models are applied in existing aspect-based sentiment analysis (ABSA) studies for utilizing word relations through dependency parses to facilitate the task with better semantic guidance for analyzing context and aspect words. However, most of these studies only leverage dependency relations without considering their dependency types, and are limited in lacking efficient mechanisms to distinguish the important relations as well as learn from different layers of graph based models. To address such limitations, in this paper, we propose an approach to explicitly utilize dependency types for ABSA with type-aware graph convolutional networks (T-GCN), where attention is used in T-GCN to distinguish different edges (relations) in the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> and attentive layer ensemble is proposed to comprehensively learn from different layers of T-GCN. The validity and effectiveness of our approach are demonstrated in the experimental results, where state-of-the-art performance is achieved on six English benchmark datasets. Further experiments are conducted to analyze the contributions of each component in our approach and illustrate how different layers in T-GCN help ABSA with quantitative and qualitative analysis.</abstract>
      <url hash="fba02bd8">2021.naacl-main.231</url>
      <doi>10.18653/v1/2021.naacl-main.231</doi>
      <bibkey>tian-etal-2021-aspect</bibkey>
      <pwccode url="https://github.com/cuhksz-nlp/ASA-TGCN" additional="false">cuhksz-nlp/ASA-TGCN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
    </paper>
    <paper id="235">
      <title>Bot-Adversarial Dialogue for Safe Conversational Agents</title>
      <author><first>Jing</first><last>Xu</last></author>
      <author><first>Da</first><last>Ju</last></author>
      <author><first>Margaret</first><last>Li</last></author>
      <author><first>Y-Lan</first><last>Boureau</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <author><first>Emily</first><last>Dinan</last></author>
      <pages>2950–2968</pages>
      <abstract>Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either training on data from our new human-and-model-in-the-loop framework in a two-stage system, or baking-in safety to the generative model itself. We find our new techniques are (i) safer than existing models ; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a>. In contrast, we expose serious safety issues in existing standard systems like GPT2, DialoGPT, and BlenderBot.</abstract>
      <url hash="8b683ae3">2021.naacl-main.235</url>
      <doi>10.18653/v1/2021.naacl-main.235</doi>
      <bibkey>xu-etal-2021-bot</bibkey>
    </paper>
    <paper id="239">
      <title>Action-Based Conversations Dataset : A Corpus for Building More In-Depth Task-Oriented Dialogue Systems</title>
      <author><first>Derek</first><last>Chen</last></author>
      <author><first>Howard</first><last>Chen</last></author>
      <author><first>Yi</first><last>Yang</last></author>
      <author><first>Alexander</first><last>Lin</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>3002–3017</pages>
      <abstract>Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10 K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated <a href="https://en.wikipedia.org/wiki/Computer_network">networks</a> outperform simpler <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, a considerable gap (50.8 % absolute accuracy) still exists to reach human-level performance on ABCD.</abstract>
      <url hash="66b9ee55">2021.naacl-main.239</url>
      <doi>10.18653/v1/2021.naacl-main.239</doi>
      <bibkey>chen-etal-2021-action</bibkey>
      <pwccode url="https://github.com/asappresearch/abcd" additional="false">asappresearch/abcd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/abcd">ABCD</pwcdataset>
    </paper>
    <paper id="241">
      <title>COIL : Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List<fixed-case>COIL</fixed-case>: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List</title>
      <author><first>Luyu</first><last>Gao</last></author>
      <author><first>Zhuyun</first><last>Dai</last></author>
      <author><first>Jamie</first><last>Callan</last></author>
      <pages>3030–3042</pages>
      <abstract>Classical information retrieval systems such as <a href="https://en.wikipedia.org/wiki/BM25">BM25</a> rely on exact lexical match and can carry out search efficiently with inverted list index. Recent neural IR models shifts towards soft matching all query document terms, but they lose the computation efficiency of exact match systems. This paper presents COIL, a contextualized exact match retrieval architecture, where scoring is based on overlapping query document tokens’ contextualized representations. The new architecture stores contextualized token representations in inverted lists, bringing together the efficiency of <a href="https://en.wikipedia.org/wiki/Exact_match">exact match</a> and the representation power of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep language models</a>. Our experimental results show COIL outperforms classical lexical retrievers and state-of-the-art deep LM retrievers with similar or smaller latency.</abstract>
      <url hash="ddcb37bb">2021.naacl-main.241</url>
      <doi>10.18653/v1/2021.naacl-main.241</doi>
      <bibkey>gao-etal-2021-coil</bibkey>
      <pwccode url="https://github.com/luyug/COIL" additional="false">luyug/COIL</pwccode>
    </paper>
    <paper id="244">
      <title>Exploring the Relationship Between Algorithm Performance, <a href="https://en.wikipedia.org/wiki/Vocabulary">Vocabulary</a>, and <a href="https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)">Run-Time</a> in Text Classification</title>
      <author><first>Wilson</first><last>Fearn</last></author>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Kevin</first><last>Seppi</last></author>
      <pages>3069–3082</pages>
      <abstract>Text classification is a significant branch of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, and has many applications including <a href="https://en.wikipedia.org/wiki/Document_classification">document classification</a> and <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. Unsurprisingly, those who do <a href="https://en.wikipedia.org/wiki/Text_classification">text classification</a> are concerned with the <a href="https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)">run-time</a> of their algorithms, many of which depend on the size of the corpus’ vocabulary due to their bag-of-words representation. Although many studies have examined the effect of preprocessing techniques on vocabulary size and <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, none have examined how these methods affect a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model’s run-time</a>. To fill this gap, we provide a comprehensive study that examines how preprocessing techniques affect the vocabulary size, model performance, and model run-time, evaluating ten techniques over four models and two datasets. We show that some individual methods can reduce <a href="https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)">run-time</a> with no loss of <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, while some combinations of methods can trade 2-5 % of the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> for up to a 65 % reduction of <a href="https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)">run-time</a>. Furthermore, some combinations of preprocessing techniques can even provide a 15 % reduction in <a href="https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)">run-time</a> while simultaneously improving model accuracy.</abstract>
      <url hash="4cec24da">2021.naacl-main.244</url>
      <doi>10.18653/v1/2021.naacl-main.244</doi>
      <bibkey>fearn-etal-2021-exploring</bibkey>
      <pwccode url="https://github.com/wfearn/preprocessing-paper" additional="false">wfearn/preprocessing-paper</pwccode>
    </paper>
    <paper id="246">
      <title>You Sound Like Someone Who Watches Drama Movies : Towards Predicting Movie Preferences from Conversational Interactions</title>
      <author><first>Sergey</first><last>Volokhin</last></author>
      <author><first>Joyce</first><last>Ho</last></author>
      <author><first>Oleg</first><last>Rokhlenko</last></author>
      <author><first>Eugene</first><last>Agichtein</last></author>
      <pages>3091–3096</pages>
      <abstract>The increasing popularity of voice-based personal assistants provides new opportunities for conversational recommendation. One particularly interesting area is movie recommendation, which can benefit from an open-ended interaction with the user, through a natural conversation. We explore one promising direction for conversational recommendation : mapping a conversational user, for whom there is limited or no data available, to most similar external reviewers, whose preferences are known, by representing the conversation as a user’s interest vector, and adapting collaborative filtering techniques to estimate the current user’s preferences for new movies. We call our proposed method ConvExtr (Conversational Collaborative Filtering using External Data), which 1) infers a user’s sentiment towards an entity from the conversation context, and 2) transforms the ratings of similar external reviewers to predict the current user’s preferences. We implement these steps by adapting contextual sentiment prediction techniques, and domain adaptation, respectively. To evaluate our method, we develop and make available a finely annotated dataset of movie recommendation conversations, which we call MovieSent. Our results demonstrate that ConvExtr can improve the accuracy of predicting users’ ratings for new movies by exploiting conversation content and external data.</abstract>
      <url hash="ef2f9cae">2021.naacl-main.246</url>
      <doi>10.18653/v1/2021.naacl-main.246</doi>
      <bibkey>volokhin-etal-2021-sound</bibkey>
      <pwccode url="https://github.com/sergey-volokhin/conversational-movies" additional="false">sergey-volokhin/conversational-movies</pwccode>
    </paper>
    <paper id="247">
      <title>Reading and Acting while Blindfolded : The Need for <a href="https://en.wikipedia.org/wiki/Semantics">Semantics</a> in Text Game Agents</title>
      <author><first>Shunyu</first><last>Yao</last></author>
      <author><first>Karthik</first><last>Narasimhan</last></author>
      <author><first>Matthew</first><last>Hausknecht</last></author>
      <pages>3097–3102</pages>
      <abstract>Text-based games simulate worlds and interact with players using <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent <a href="https://en.wikipedia.org/wiki/Intelligent_agent">artificial agents</a> utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of <a href="https://en.wikipedia.org/wiki/Semantics">semantic information</a> available to a <a href="https://en.wikipedia.org/wiki/Intelligent_agent">learning agent</a>. Surprisingly, we find that an <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agent</a> is capable of achieving high scores even in the complete absence of <a href="https://en.wikipedia.org/wiki/Semantics">language semantics</a>, indicating that the currently popular experimental setup and <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including Zork I. We discuss the implications of our findings for designing future <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agents</a> with stronger semantic understanding.</abstract>
      <url hash="b1b7c054">2021.naacl-main.247</url>
      <doi>10.18653/v1/2021.naacl-main.247</doi>
      <bibkey>yao-etal-2021-reading</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/jericho">Jericho</pwcdataset>
    </paper>
    <paper id="254">
      <title>CaSiNo : A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems<fixed-case>C</fixed-case>a<fixed-case>S</fixed-case>i<fixed-case>N</fixed-case>o: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems</title>
      <author><first>Kushal</first><last>Chawla</last></author>
      <author><first>Jaysa</first><last>Ramirez</last></author>
      <author><first>Rene</first><last>Clever</last></author>
      <author><first>Gale</first><last>Lucas</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <author><first>Jonathan</first><last>Gratch</last></author>
      <pages>3167–3185</pages>
      <abstract>Automated systems that negotiate with humans have broad applications in <a href="https://en.wikipedia.org/wiki/Pedagogy">pedagogy</a> and conversational AI. To advance the development of practical <a href="https://en.wikipedia.org/wiki/Negotiation">negotiation systems</a>, we present CaSiNo : a novel <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> of over a thousand <a href="https://en.wikipedia.org/wiki/Negotiation">negotiation dialogues</a> in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain environment. Inspired by the literature in human-human negotiations, we annotate <a href="https://en.wikipedia.org/wiki/Persuasion">persuasion strategies</a> and perform <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence">correlation analysis</a> to understand how the dialogue behaviors are associated with the <a href="https://en.wikipedia.org/wiki/Negotiation">negotiation</a> performance. We further propose and evaluate a multi-task framework to recognize these <a href="https://en.wikipedia.org/wiki/Strategy">strategies</a> in a given utterance. We find that <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations : https://github.com/kushalchawla/CaSiNo</abstract>
      <url hash="9232b1e3">2021.naacl-main.254</url>
      <doi>10.18653/v1/2021.naacl-main.254</doi>
      <bibkey>chawla-etal-2021-casino</bibkey>
      <pwccode url="https://github.com/kushalchawla/CaSiNo" additional="false">kushalchawla/CaSiNo</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/casino">CaSiNo</pwcdataset>
    </paper>
    <paper id="255">
      <title>News Headline Grouping as a Challenging NLU Task<fixed-case>NLU</fixed-case> Task</title>
      <author><first>Philippe</first><last>Laban</last></author>
      <author><first>Lucas</first><last>Bandarkar</last></author>
      <author><first>Marti A.</first><last>Hearst</last></author>
      <pages>3186–3198</pages>
      <abstract>Recent progress in Natural Language Understanding (NLU) has seen the latest <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> outperform human performance on many standard <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. These impressive results have led the community to introspect on dataset limitations, and iterate on more nuanced challenges. In this paper, we introduce the task of HeadLine Grouping (HLG) and a corresponding <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> (HLGD) consisting of 20,056 pairs of <a href="https://en.wikipedia.org/wiki/Headline">news headlines</a>, each labeled with a binary judgement as to whether the pair belongs within the same group. On HLGD, human annotators achieve high performance of around 0.9 F-1, while current state-of-the art Transformer models only reach 0.75 F-1, opening the path for further improvements. We further propose a novel unsupervised Headline Generator Swap model for the task of HeadLine Grouping that achieves within 3 F-1 of the best supervised model. Finally, we analyze high-performing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> with consistency tests, and find that <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are not consistent in their predictions, revealing modeling limits of current <a href="https://en.wikipedia.org/wiki/Computer_architecture">architectures</a>.</abstract>
      <url hash="800ae2c2">2021.naacl-main.255</url>
      <doi>10.18653/v1/2021.naacl-main.255</doi>
      <bibkey>laban-etal-2021-news</bibkey>
      <pwccode url="https://github.com/tingofurro/headline_grouping" additional="false">tingofurro/headline_grouping</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hlgd">HLGD</pwcdataset>
    </paper>
    <paper id="262">
      <title>Ensemble of MRR and NDCG models for Visual Dialog<fixed-case>MRR</fixed-case> and <fixed-case>NDCG</fixed-case> models for Visual Dialog</title>
      <author><first>Idan</first><last>Schwartz</last></author>
      <pages>3272–3363</pages>
      <abstract>Assessing an <a href="https://en.wikipedia.org/wiki/Intelligent_agent">AI agent</a> that can converse in <a href="https://en.wikipedia.org/wiki/Human_language">human language</a> and understand <a href="https://en.wikipedia.org/wiki/Visual_system">visual content</a> is challenging. Generation metrics, such as BLEU scores favor correct <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> over <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a>. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance by taking into account the <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)">rank</a> of a single human-derived answer. This approach, however, raises a new challenge : the ambiguity and synonymy of answers, for instance, <a href="https://en.wikipedia.org/wiki/Semantic_equivalence">semantic equivalence</a> (e.g., ‘yeah’ and ‘yes’). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as ‘I do n’t know.’ Crafting a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that excels on both MRR and NDCG metrics is challenging. Ideally, an <a href="https://en.wikipedia.org/wiki/Intelligent_agent">AI agent</a> should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a two-step non-parametric ranking approach that can merge strong MRR and NDCG models. Using our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a>, we manage to keep most MRR state-of-the-art performance (70.41 % vs. 71.24 %) and the NDCG state-of-the-art performance (72.16 % vs. 75.35 %). Moreover, our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> won the recent Visual Dialog 2020 challenge. Source code is available at https://github.com/idansc/mrr-ndcg.</abstract>
      <url hash="9b91aca2">2021.naacl-main.262</url>
      <doi>10.18653/v1/2021.naacl-main.262</doi>
      <bibkey>schwartz-2021-ensemble</bibkey>
      <pwccode url="https://github.com/idansc/mrr-ndcg" additional="false">idansc/mrr-ndcg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visdial">VisDial</pwcdataset>
    </paper>
    <paper id="265">
      <title>CREAD : Combined Resolution of Ellipses and Anaphora in Dialogues<fixed-case>CREAD</fixed-case>: Combined Resolution of Ellipses and Anaphora in Dialogues</title>
      <author><first>Bo-Hsiang</first><last>Tseng</last></author>
      <author><first>Shruti</first><last>Bhargava</last></author>
      <author><first>Jiarui</first><last>Lu</last></author>
      <author><first>Joel Ruben Antony</first><last>Moniz</last></author>
      <author><first>Dhivya</first><last>Piraviperumal</last></author>
      <author><first>Lin</first><last>Li</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>3390–3406</pages>
      <abstract>Anaphora and <a href="https://en.wikipedia.org/wiki/Ellipsis">ellipses</a> are two common phenomena in <a href="https://en.wikipedia.org/wiki/Dialogue">dialogues</a>. Without resolving <a href="https://en.wikipedia.org/wiki/Reference">referring expressions</a> and information omission, <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue systems</a> may fail to generate consistent and coherent responses. Traditionally, <a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)">anaphora</a> is resolved by <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> and ellipses by query rewrite. In this work, we propose a novel joint learning framework of modeling <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> and query rewriting for complex, multi-turn dialogue understanding. Given an ongoing dialogue between a user and a dialogue assistant, for the user query, our joint learning model first predicts coreference links between the query and the dialogue context, and then generates a self-contained rewritten user query. To evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, we annotate a dialogue based coreference resolution dataset, MuDoCo, with rewritten queries. Results show that the performance of query rewrite can be substantially boosted (+2.3 % F1) with the aid of coreference modeling. Furthermore, our joint model outperforms the state-of-the-art coreference resolution model (+2 % F1) on this dataset.</abstract>
      <url hash="c4f5d834">2021.naacl-main.265</url>
      <doi>10.18653/v1/2021.naacl-main.265</doi>
      <bibkey>tseng-etal-2021-cread</bibkey>
      <pwccode url="https://github.com/apple/ml-cread" additional="false">apple/ml-cread</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mudoco-queryrewrite">MuDoCo_QueryRewrite</pwcdataset>
    </paper>
    <paper id="266">
      <title>Knowledge-Driven Slot Constraints for Goal-Oriented Dialogue Systems</title>
      <author><first>Piyawat</first><last>Lertvittayakumjorn</last></author>
      <author><first>Daniele</first><last>Bonadiman</last></author>
      <author><first>Saab</first><last>Mansour</last></author>
      <pages>3407–3419</pages>
      <abstract>In goal-oriented dialogue systems, users provide information through slot values to achieve specific goals. Practically, some combinations of slot values can be invalid according to external knowledge. For example, a combination of <a href="https://en.wikipedia.org/wiki/Cheese_pizza">cheese pizza</a> (a menu item) and oreo cookies (a topping) from an input utterance Can I order a <a href="https://en.wikipedia.org/wiki/Cheese_pizza">cheese pizza</a> with <a href="https://en.wikipedia.org/wiki/Oreo">oreo cookies</a> on top? exemplifies such invalid combinations according to the menu of a restaurant business. Traditional dialogue systems allow execution of validation rules as a post-processing step after slots have been filled which can lead to error accumulation. In this paper, we formalize knowledge-driven slot constraints and present a new task of constraint violation detection accompanied with benchmarking data. Then, we propose methods to integrate the external knowledge into the system and model constraint violation detection as an end-to-end classification task and compare it to the traditional rule-based pipeline approach. Experiments on two domains of the MultiDoGO dataset reveal challenges of constraint violation detection and sets the stage for future work and improvements.</abstract>
      <url hash="b0ec24f5">2021.naacl-main.266</url>
      <doi>10.18653/v1/2021.naacl-main.266</doi>
      <bibkey>lertvittayakumjorn-etal-2021-knowledge</bibkey>
      <pwccode url="https://github.com/amazon-research/nlu-slot-constraints" additional="false">amazon-research/nlu-slot-constraints</pwccode>
    </paper>
    <paper id="267">
      <title>Clipping Loops for Sample-Efficient Dialogue Policy Optimisation</title>
      <author><first>Yen-Chen</first><last>Wu</last></author>
      <author><first>Carl Edward</first><last>Rasmussen</last></author>
      <pages>3420–3428</pages>
      <abstract>Training dialogue agents requires a large number of interactions with users : agents have no idea about which responses are bad among a lengthy dialogue. In this paper, we propose loop-clipping policy optimisation (LCPO) to eliminate useless responses. LCPO consists of two stages : loop clipping and advantage clipping. In loop clipping, we clip off useless responses (called loops) from dialogue history (called trajectories). The clipped trajectories are more succinct than the original ones, and the estimation of state-value is more accurate. Second, in advantage clipping, we estimate and clip the advantages of useless responses and normal ones separately. The clipped advantage distinguish useless actions from others and reduce the probabilities of useless actions efficiently. In experiments on Cambridge Restaurant Dialogue System, LCPO uses only 260 training dialogues to achieve 80 % success rate, while PPO baseline requires 2160 dialogues. Besides, LCPO receives 3.7/5 scores in <a href="https://en.wikipedia.org/wiki/Human–computer_interaction">human evaluation</a> where the agent interactively collects 100 real-user dialogues in training phase.</abstract>
      <url hash="c4316387">2021.naacl-main.267</url>
      <doi>10.18653/v1/2021.naacl-main.267</doi>
      <bibkey>wu-rasmussen-2021-clipping</bibkey>
    </paper>
    <paper id="270">
      <title>TABBIE : Pretrained Representations of Tabular Data<fixed-case>TABBIE</fixed-case>: Pretrained Representations of Tabular Data</title>
      <author><first>Hiroshi</first><last>Iida</last></author>
      <author><first>Dung</first><last>Thai</last></author>
      <author><first>Varun</first><last>Manjunatha</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>3446–3456</pages>
      <abstract>Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.</abstract>
      <url hash="0bf6dd05">2021.naacl-main.270</url>
      <doi>10.18653/v1/2021.naacl-main.270</doi>
      <bibkey>iida-etal-2021-tabbie</bibkey>
      <pwccode url="https://github.com/SFIG611/tabbie" additional="false">SFIG611/tabbie</pwccode>
    </paper>
    <paper id="275">
      <title>Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus</title>
      <author><first>Navita</first><last>Goyal</last></author>
      <author><first>Balaji Vasan</first><last>Srinivasan</last></author>
      <author><first>Anandhavelu</first><last>N</last></author>
      <author><first>Abhilasha</first><last>Sancheti</last></author>
      <pages>3500–3510</pages>
      <abstract>Style transfer has been widely explored in <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation</a> with non-parallel corpus by directly or indirectly extracting a notion of <a href="https://en.wikipedia.org/wiki/Style_(sociolinguistics)">style</a> from source and target domain corpus. A common shortcoming of existing approaches is the prerequisite of joint annotations across all the <a href="https://en.wikipedia.org/wiki/Style_(visual_arts)">stylistic dimensions</a> under consideration. Availability of such <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> across a combination of styles limits the extension of these setups to multiple style dimensions. While cascading single-dimensional models across multiple styles is a possibility, it suffers from content loss, especially when the style dimensions are not completely independent of each other. In our work, we relax this requirement of jointly annotated data across multiple styles by using independently acquired data across different style dimensions without any additional annotations. We initialize an encoder-decoder setup with transformer-based language model pre-trained on a generic corpus and enhance its re-writing capability to multiple target style dimensions by employing multiple style-aware language models as discriminators. Through quantitative and qualitative evaluation, we show the ability of our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to control styles across multiple style dimensions while preserving content of the input text. We compare it against baselines involving cascaded state-of-the-art uni-dimensional style transfer models.</abstract>
      <url hash="b8e9565c">2021.naacl-main.275</url>
      <doi>10.18653/v1/2021.naacl-main.275</doi>
      <bibkey>goyal-etal-2021-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="280">
      <title>InfoXLM : An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training<fixed-case>I</fixed-case>nfo<fixed-case>XLM</fixed-case>: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training</title>
      <author><first>Zewen</first><last>Chi</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Nan</first><last>Yang</last></author>
      <author><first>Saksham</first><last>Singhal</last></author>
      <author><first>Wenhui</first><last>Wang</last></author>
      <author><first>Xia</first><last>Song</last></author>
      <author><first>Xian-Ling</first><last>Mao</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <pages>3576–3588</pages>
      <abstract>In this work, we present an information-theoretic framework that formulates cross-lingual language model pre-training as maximizing <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a> between multilingual-multi-granularity texts. The unified view helps us to better understand the existing <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for learning cross-lingual representations. More importantly, inspired by the <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a>, we propose a new pre-training task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The <a href="https://en.wikipedia.org/wiki/Source_code">code</a> and pre-trained models are available at https://aka.ms/infoxlm.</abstract>
      <url hash="c7fdf3ea">2021.naacl-main.280</url>
      <doi>10.18653/v1/2021.naacl-main.280</doi>
      <bibkey>chi-etal-2021-infoxlm</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xtreme">XTREME</pwcdataset>
    </paper>
    <paper id="283">
      <title>X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">Natural Language Understanding</a> and Question Answering<fixed-case>X</fixed-case>-<fixed-case>METRA</fixed-case>-<fixed-case>ADA</fixed-case>: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</title>
      <author><first>Meryem</first><last>M’hamdi</last></author>
      <author><first>Doo Soon</first><last>Kim</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <pages>3617–3632</pages>
      <abstract>Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their <a href="https://en.wikipedia.org/wiki/Generalization">generalization ability</a> is still inconsistent for <a href="https://en.wikipedia.org/wiki/Linguistic_typology">typologically diverse languages</a> and across different benchmarks. Recently, <a href="https://en.wikipedia.org/wiki/Meta-learning">meta-learning</a> has garnered attention as a promising technique for enhancing <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> under low-resource scenarios : particularly for cross-lingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks : multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.</abstract>
      <url hash="adc0d29d">2021.naacl-main.283</url>
      <doi>10.18653/v1/2021.naacl-main.283</doi>
      <bibkey>mhamdi-etal-2021-x</bibkey>
      <pwccode url="https://github.com/meryemmhamdi1/meta_cross_nlu_qa" additional="false">meryemmhamdi1/meta_cross_nlu_qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydiqa-goldp">TyDiQA-GoldP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    <title_ar>X-METRA-ADA: تكيف تعلم التحويل متعدد اللغات عبر اللغات مع فهم اللغة الطبيعية والإجابة على الأسئلة</title_ar>
      <title_fr>X-METRA-ADA : Apprentissage par méta-transfert multilingue Adaptation à la compréhension du langage naturel et à la réponse aux questions</title_fr>
      <title_pt>X-METRA-ADA: Adaptação de aprendizagem de metatransferência multilíngue para compreensão de linguagem natural e resposta a perguntas</title_pt>
      <title_es>X-METRA-ADA: Aprendizaje de meta-transferencia multilingüe Adaptación a la comprensión del lenguaje natural y la respuesta a preguntas</title_es>
      <title_ja>X - METRA - ADA ：クロスリンガルメタトランスファー学習自然言語理解と質問回答への適応</title_ja>
      <title_zh>X-METRA-ADA:跨语元移学自然语言解问答</title_zh>
      <title_hi>X-METRA-ADA: क्रॉस-लिंगुअल मेटा-ट्रांसफर सीखने के अनुकूलन को प्राकृतिक भाषा की समझ और प्रश्न का उत्तर देने के लिए अनुकूलन</title_hi>
      <title_ru>X-METRA-ADA: Кросс-лингвистическое мета-передача обучения Адаптация к пониманию естественного языка и ответы на вопросы</title_ru>
      <title_ga>X-METRA-ADA: Foghlaim Meta-Aistrithe Thrasteangach Oiriúnú do Thuiscint Teanga Nádúrtha agus Freagairt Cheisteanna</title_ga>
      <title_el>Χ-ΜΕΤΡΑ-ADA: Διγλωσσική μάθηση Μεταμεταφορά Προσαρμογή στη Φυσική Γλώσσα Κατανόησης και Απάντηση Ερωτήσεων</title_el>
      <title_hu>X-METRA-ADA: Többnyelvű metatranszfer tanulás Alkalmazkodás a természetes nyelv megértéséhez és a kérdések megválaszolásához</title_hu>
      <title_ka>Name</title_ka>
      <title_it>X-METRA-ADA: Meta-Transfer learning multilingue Adattamento alla comprensione del linguaggio naturale e risposta alle domande</title_it>
      <title_mk>X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</title_mk>
      <title_kk>X- METRA- ADA: Түзіндік тілдерді түсініп және сұрақтарды жауап беру үшін бірнеше тілдерді мета- трансферлерді оқыту адаптациясы</title_kk>
      <title_ml>എക്സ്- മെട്രാ- ADA: ക്രോസ്- ഭാഷ മെറ്റ- മാറ്റാന്‍ പഠിക്കുന്നത് സ്വാഭാവികമായ ഭാഷയിലേക്കുള്ള അഡാപ്റ്റേഷന്‍ ബോധനമാ</title_ml>
      <title_mt>X-METRA-ADA: Tagħlim translingwi ta’ Meta-Trasferiment Adattament għall-fehim tal-lingwa naturali u t-tweġibiet għall-mistoqsijiet</title_mt>
      <title_mn>X-METRA-ADA: Түүнчлэн хэлний мета-трансфер суралцлагын адаптация байгалийн хэл ойлголт болон асуулт хариулт</title_mn>
      <title_no>X-METRA-ADA: Læringsadaptasjon til naturleg språk forståking og spørsmål</title_no>
      <title_lt>X-METRA-ADA: Tarpkalbinis metaperdavimo mokymasis Prisitaikymas prie natūralios kalbos supratimo ir klausimų atsakymo</title_lt>
      <title_ms>X-METRA-ADA: Penyesuaian Meta-Pemindahan Selasa Bahasa untuk Pemahaman Bahasa Alami dan Jawapan Pertanyaan</title_ms>
      <title_pl>X-METRA-ADA: Nauka metatransferowa międzyjęzyczna Adaptacja do rozumienia języka naturalnego i odpowiadania na pytania</title_pl>
      <title_sr>X-METRA-ADA: Prejezička meta-prevoz učenja prilagođenja prirodnom razumijevanju jezika i odgovoru na pitanja</title_sr>
      <title_so>X-METRA-ADA: Waxbarashada afka kala duwan ee meta-wareejinta</title_so>
      <title_ro>X-METRA-ADA: Învățare interlingvă Meta-Transfer Adaptare la înțelegerea limbii naturale și răspunsul la întrebări</title_ro>
      <title_sv>X-METRA-ADA: Cross-lingual Meta-Transfer lärande Anpassning till naturligt språk förståelse och frågesvar</title_sv>
      <title_si>X-METRA-ADA: Cross-language Meta-Transfer</title_si>
      <title_ta>X- METRA- ADA: Cross- language Meta- Transfer learning Adaptation to Natural Language Understanding and Question answers</title_ta>
      <title_ur>X-METRA-ADA: Cross-lingual Meta-Transfer learning adaptation to Natural Language Understanding and Question Answering</title_ur>
      <title_vi>X-METRA-ADA: vặn khóa học siêu truyền thống về học về ngôn ngữ tự nhiên và câu hỏi đáp ứng</title_vi>
      <title_uz>X- METRA- ADA: Foydalanuvchi va savol javobi</title_uz>
      <title_bg>Адаптация към разбирането на естествения език и отговаряне на въпроси</title_bg>
      <title_de>X-METRA-ADA: Cross-lingual Meta-Transfer Learning Anpassung an das Verstehen natürlicher Sprache und die Beantwortung von Fragen</title_de>
      <title_nl>X-METRA-ADA: Crosslingual Meta-Transfer leren Aanpassing aan het begrijpen van natuurlijke taal en het beantwoorden van vragen</title_nl>
      <title_hr>X-METRA-ADA: Prejezička meta-prebacivanje učenja prilagođenja prirodnom razumijevanju jezika i odgovoru na pitanja</title_hr>
      <title_da>X-METRA-ADA: Tværsproget Meta-Transfer læring Tilpasning til natursprogforståelse og spørgsmål besvarelse</title_da>
      <title_ko>크로스 언어 원 이동 학습 자연 언어 이해와 문제 대답에 적응</title_ko>
      <title_fa>X-METRA-ADA: تغییرات یادگیری متا-انتقال متا-زبانی به درک زبان طبیعی و پاسخ سوال</title_fa>
      <title_sw>X-METRA-ADA: Kufunza Uhamiaji wa lugha ya Kimataifa na Kuhamasisha Ujumbe kwa lugha ya Kiasili</title_sw>
      <title_tr>X-METRA-ADA: Dabiýal dil düşünemek we sorag jogap</title_tr>
      <title_af>X- METRA- ADA: Cross- language Meta- Transfer leer aanpassing na Natuurlike Taal Verstaan en Fraag Antwoord</title_af>
      <title_sq>X-METRA-ADA: Ndryshimi ndërgjuhësor i mësimit të metatransferimit në kuptimin e gjuhës natyrore dhe përgjigjen e pyetjeve</title_sq>
      <title_am>X-METRA-ADA: የቋንቋ-ቋንቋ ማቀናጃ ለአዳራዊ ቋንቋ ማስታወቂያ እና ጥያቄ መልስ</title_am>
      <title_hy>X-Մետրա-ԱԴԱ. Խոսքերի միջև փոխանցվող մետափոխանցման ուսումնասիրությունը հարմարվում է բնական լեզվի հասկանալու և հարցերի պատասխանների հետ</title_hy>
      <title_az>X-METRA-ADA: Təbiətli dil anlaşılması və sual cavab verməsi üçün çox dilli meta-Transfer öyrənməsi</title_az>
      <title_bn>এক্স- মেট্রা- এডা: ক্রস-ভাষার মেটা- ট্রান্সফারের শিক্ষা প্রাকৃতিক ভাষায় বুঝতে এবং প্রশ্নের উত্তর</title_bn>
      <title_bs>X-METRA-ADA: Prejezička meta-prebacivanje učenja prilagođenja prirodnom razumijevanju jezika i odgovoru na pitanja</title_bs>
      <title_ca>X-METRA-ADA: Adaptació de l'aprenentatge translingüístic de metatransferències a l'entendre del llenguatge natural i resposta a preguntes</title_ca>
      <title_cs>X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptace na porozumění přirozenému jazyku a zodpovězení otázek</title_cs>
      <title_et>X-METRA-ADA: Keeleülene metaülekanne õppimine Kohanemine loodusliku keele mõistmise ja küsimustele vastamisega</title_et>
      <title_fi>X-METRA-ADA: Monikielinen metasiirtooppiminen Sopeutuminen luonnolliseen kieleen Ymmärtäminen ja kyselyihin vastaaminen</title_fi>
      <title_id>X-METRA-ADA: Pelajaran Meta-Transfer saling bahasa Adaptasi ke Pemahaman Bahasa Alami dan Jawaban Pertanyaan</title_id>
      <title_jv>X-METRA</title_jv>
      <title_sk>X-METRA-ADA: Medjezično metatransferno učenje Prilagajanje razumevanju naravnega jezika in odgovarjanje na vprašanja</title_sk>
      <title_ha>X-METRA-ADA: Ana karanta littafin Farawa na Meta-Transforms</title_ha>
      <title_he>X-METRA-ADA: התאמה למטה-העברה בין שפות</title_he>
      <title_bo>X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</title_bo>
      <abstract_ar>اكتسبت النماذج متعددة اللغات ، مثل M-BERT و XLM-R ، شعبية متزايدة ، نظرًا لقدراتها التعليمية على النقل عبر اللغات بدون طلقة. ومع ذلك ، فإن قدرتها على التعميم لا تزال غير متسقة بالنسبة للغات المتنوعة من حيث التصنيف وعبر معايير مختلفة. في الآونة الأخيرة ، حظي التعلم التلوي بالاهتمام باعتباره أسلوبًا واعدًا لتعزيز التعلم الانتقالي في ظل سيناريوهات الموارد المنخفضة: لا سيما بالنسبة للنقل عبر اللغات في فهم اللغة الطبيعية (NLU). في هذا العمل ، نقترح X-METRA-ADA ، نهج التعلم MEta-TRAnsfer التعلم عبر اللغات لـ NLU. يتكيف نهجنا مع MAML ، وهو نهج التعلم التلوي القائم على التحسين ، لتعلم التكيف مع اللغات الجديدة. نقوم بتقييم إطار عملنا على نطاق واسع في مهمتين صعبتين في NLU عبر اللغات: الحوار متعدد اللغات الموجه نحو المهام والإجابة على الأسئلة المتنوعة بشكل نمطي. نظهر أن نهجنا يتفوق في الأداء على الضبط الدقيق الساذج ، حيث يصل إلى أداء تنافسي في كلتا المهمتين لمعظم اللغات. يكشف تحليلنا أن X-METRA-ADA يمكنها الاستفادة من البيانات المحدودة للتكيف بشكل أسرع.</abstract_ar>
      <abstract_es>Los modelos multilingües, como M-BERT y XLM-R, han ganado cada vez más popularidad, debido a sus capacidades de aprendizaje de transferencia interlingüística de tiro cero. Sin embargo, su capacidad de generalización sigue siendo inconsistente para lenguajes tipológicamente diversos y en diferentes puntos de referencia. Recientemente, el meta-aprendizaje ha atraído la atención como una técnica prometedora para mejorar el aprendizaje por transferencia en escenarios de bajos recursos: particularmente para la transferencia interlingüística en la comprensión del lenguaje natural (NLU). En este trabajo, proponemos X-METRA-ADA, un enfoque de adaptación del aprendizaje de meta-transferencia multilingüe para NLU. Nuestro enfoque adapta MAML, un enfoque de meta-aprendizaje basado en la optimización, para aprender a adaptarse a los nuevos idiomas. Evaluamos exhaustivamente nuestro marco en dos desafiantes tareas de NLU multilingües: diálogo multilingüe orientado a tareas y respuestas a preguntas tipológicamente diversas. Demostramos que nuestro enfoque supera los ajustes ingenuos, alcanzando un rendimiento competitivo en ambas tareas para la mayoría de los idiomas. Nuestro análisis revela que X-METRA-ADA puede aprovechar los datos limitados para una adaptación más rápida.</abstract_es>
      <abstract_fr>Les modèles multilingues, tels que M-BERT et XLM-R, gagnent en popularité, en raison de leurs capacités d'apprentissage par transfert multilingue zero-shot. Cependant, leur capacité de généralisation est toujours incohérente pour les langues typologiquement diverses et selon les différents critères de référence. Récemment, le méta-apprentissage a attiré l'attention en tant que technique prometteuse pour améliorer l'apprentissage par transfert dans des scénarios à faibles ressources, en particulier pour le transfert interlinguistique dans la compréhension du langage naturel (NLU). Dans ce travail, nous proposons X-METRA-ADA, une approche multilingue d'adaptation de l'apprentissage par méta-transfert pour la NLU. Notre approche adapte MAML, une approche de méta-apprentissage basée sur l'optimisation, pour apprendre à s'adapter aux nouvelles langues. Nous évaluons en profondeur notre cadre sur deux tâches multilingues complexes de la NLU : le dialogue multilingue axé sur les tâches et la réponse aux questions typologiquement diversifiée. Nous montrons que notre approche surpasse le réglage fin naïf, atteignant des performances compétitives sur les deux tâches pour la plupart des langues. Notre analyse révèle que X-METRA-ADA peut exploiter des données limitées pour une adaptation plus rapide.</abstract_fr>
      <abstract_pt>Modelos multilíngues, como M-BERT e XLM-R, ganharam popularidade crescente, devido às suas capacidades de aprendizado de transferência multilíngue zero-shot. No entanto, sua capacidade de generalização ainda é inconsistente para linguagens tipologicamente diversas e em diferentes benchmarks. Recentemente, a meta-aprendizagem ganhou atenção como uma técnica promissora para melhorar a aprendizagem por transferência em cenários de poucos recursos: particularmente para a transferência multilíngue em Compreensão da Linguagem Natural (NLU). Neste trabalho, propomos o X-METRA-ADA, uma abordagem de ADAptação de aprendizado de MEta-TRAnsfer multilíngue para NLU. Nossa abordagem adapta MAML, uma abordagem de meta-aprendizagem baseada em otimização, para aprender a se adaptar a novos idiomas. Avaliamos extensivamente nossa estrutura em duas tarefas NLU multilíngues desafiadoras: diálogo multilíngue orientado a tarefas e resposta a perguntas tipologicamente diversas. Mostramos que nossa abordagem supera o ajuste fino ingênuo, alcançando desempenho competitivo em ambas as tarefas para a maioria dos idiomas. Nossa análise revela que o X-METRA-ADA pode alavancar dados limitados para uma adaptação mais rápida.</abstract_pt>
      <abstract_ja>M - BERTやXLM - Rなどの多言語モデルは、ゼロショットのクロスリンガル転送学習機能により、ますます人気を集めています。 しかし、類型的に多様な言語と異なるベンチマークでは、それらの一般化能力は依然として一貫していません。 最近、メタラーニングは、特に自然言語理解（ NLU ）におけるクロスリンガル転送のための、低資源シナリオ下での転送学習を強化するための有望なテクニックとして注目を集めている。 この研究では、NLUのためのクロスリンガルMEta - TRAnsfer学習ADAptationアプローチであるX - METRA - ADAを提案します。 私たちのアプローチは、最適化ベースのメタラーニングアプローチであるMAMLを適応させ、新しい言語に適応することを学びます。 私たちは、多言語タスク指向のダイアログと類型的に多様な質問への回答という、2つの困難なクロスリンガルNLUタスクに関するフレームワークを幅広く評価します。 私たちは、私たちのアプローチが天真爛漫な微調整よりも優れており、ほとんどの言語で両方のタスクで競争力のあるパフォーマンスに達していることを示しています。 私たちの分析は、X - METRA - ADAが限られたデータを活用して、より迅速な適応を実現できることを明らかにしました。</abstract_ja>
      <abstract_hi>बहुभाषी मॉडल, जैसे कि एम-बर्ट और एक्सएलएम-आर, ने अपनी शून्य-शॉट क्रॉस-लिंगुअल ट्रांसफर सीखने की क्षमताओं के कारण बढ़ती लोकप्रियता हासिल की है। हालांकि, उनकी सामान्यीकरण क्षमता अभी भी टाइपोलॉजिकल रूप से विविध भाषाओं और विभिन्न बेंचमार्क ों के लिए असंगत है। हाल ही में, मेटा-लर्निंग ने कम संसाधन परिदृश्यों के तहत स्थानांतरण सीखने को बढ़ाने के लिए एक आशाजनक तकनीक के रूप में ध्यान आकर्षित किया है: विशेष रूप से प्राकृतिक भाषा समझ (एनएलयू) में क्रॉस-लिंगुअल हस्तांतरण के लिए। इस काम में, हम X-METRA-ADA का प्रस्ताव करते हैं, जो एनएलयू के लिए एक क्रॉस-लिंगुअल MEta-TRAnsfer सीखने वाला ADAptation दृष्टिकोण है। हमारा दृष्टिकोण MAML, एक अनुकूलन-आधारित मेटा-लर्निंग दृष्टिकोण को अनुकूलित करता है, नई भाषाओं के अनुकूल होने के लिए सीखने के लिए। हम बड़े पैमाने पर दो चुनौतीपूर्ण क्रॉस-लिंगुअल एनएलयू कार्यों पर हमारे ढांचे का मूल्यांकन करते हैं: बहुभाषी कार्य-उन्मुख संवाद और टाइपोलॉजिकल रूप से विविध प्रश्न का उत्तर देना। हम दिखाते हैं कि हमारा दृष्टिकोण भोले ठीक ट्यूनिंग से बेहतर प्रदर्शन करता है, अधिकांश भाषाओं के लिए दोनों कार्यों पर प्रतिस्पर्धी प्रदर्शन तक पहुंचता है। हमारे विश्लेषण से पता चलता है कि एक्स-मेट्रा-एडीए तेजी से अनुकूलन के लिए सीमित डेटा का लाभ उठा सकता है।</abstract_hi>
      <abstract_zh>多言模样,如M-BERT与XLM-R,以其零镜头跨语迁学而益受欢迎。 然则多样化言异准,其泛化不一。 近者,元学之为下资也,强移学之一术而见关注:特于自然语言解(NLU)中跨语移也。 于此言之,吾等建X-METRA-ADA,一于NLU跨语MEta-TRAnsfer习ADAptation法。 吾法用MAML,一本于优化之元学,以学适新之言。 博论两挑战性之跨语NLU事框架:多言者,多样化对也。 吾法优于幼稚之微,多言两事,皆有竞争力性。 吾之分析表明,X-METRA-ADA可以有限之数更快应也。</abstract_zh>
      <abstract_ru>Многоязычные модели, такие как M-BERT и XLM-R, набирают все большую популярность благодаря своим возможностям межъязыкового обучения с нулевым выстрелом. Однако их способность к обобщению все еще не соответствует типологически различным языкам и различным критериям. В последнее время мета-обучение привлекло внимание как перспективный метод для улучшения обучения передаче при сценариях с низким уровнем ресурсов: особенно для межъязыковой передачи в понимании естественного языка (NLU). В этой работе мы предлагаем X-METRA-ADA, кросс-лингвистический подход ADAptation обучения MEta-TRANSfer для NLU. Наш подход адаптирует MAML, основанный на оптимизации мета-учебный подход, чтобы научиться адаптироваться к новым языкам. Мы широко оцениваем нашу структуру по двум сложным межязычным задачам NLU: многоязычному диалогу, ориентированному на задачи, и типологически разнообразным ответам на вопросы. Мы показываем, что наш подход превосходит наивную настройку, достигая конкурентной производительности по обеим задачам для большинства языков. Наш анализ показывает, что X-METRA-ADA может использовать ограниченные данные для более быстрой адаптации.</abstract_ru>
      <abstract_ga>Tá méadú ag teacht ar an éileamh atá ar shamhlacha ilteangacha, mar M-BERT agus XLM-R, mar gheall ar a gcumas foghlama aistrithe tras-teanga gan aon lámhaigh. Mar sin féin, tá a gcumas ginearálaithe fós ar neamhréir do theangacha atá éagsúil ó thaobh na clódóireachta de agus thar thagarmharcanna éagsúla. Le déanaí, tarraingíodh aird ar an meitea-fhoghlaim mar theicníc a bhfuil gealladh fúthu chun foghlaim aistrithe a fheabhsú faoi chásanna íseal-acmhainne: go háirithe maidir le haistriú tras-teangach i dTuiscint Teanga Nádúrtha (NLU). San obair seo, molaimid X-METRA-ADA, cur chuige tras-teangach oiriúnaithe foghlama MEta-Transfer don NLU. Déanann ár gcur chuige seo oiriúnú do MAML, cur chuige meitea-fhoghlaim atá bunaithe ar bharrfheabhsú, chun foghlaim conas oiriúnú do theangacha nua. Déanaimid meastóireacht fhairsing ar ár gcreat maidir le dhá thasc dhúshlánacha thrastheangacha NLU: dialóg ilteangach atá dírithe ar thascanna agus freagraí ilghnéitheacha tíopeolaíochta. Léirímid go sáraíonn ár gcur chuige mionchoigeartú naive, ag baint amach feidhmíocht iomaíoch sa dá thasc don chuid is mó de na teangacha. Léiríonn ár n-anailís gur féidir le X-METRA-ADA sonraí teoranta a ghiaráil le haghaidh oiriúnú níos tapúla.</abstract_ga>
      <abstract_ka>მრავალენგური მოდელები, როგორც M-BERT და XLM-R, უფრო მეტი პოლუბარიტება იქნება, რადგან ისინი უფრო მეტი ენგური გასწავლების შესაძლებლობა გადავიღეთ. მაგრამ მათი გენერალიზაციის შესაძლებლობა ისევ ტიპოლოგიურად განსხვავებული ენებისთვის და განსხვავებული ბენქმარკებისთვის არსებობს. მიმდინარე, მეტა-სწავლების შესაძლებლობელი ტექნონია, როგორც უნდა გავაკეთოთ ტრანსპერსონის სწავლების აღმოქმედებისთვის: განსაკუთრებულია კრესტრისონის ტრანსპერსონისთვის ნა ამ სამუშაოში, ჩვენ X-METRA-ADA-ს, სამუშაო ენგური MEta-TRAnsfer სწავლების ADAptation პროგორმა NLU-ს. ჩვენი პროგორმაცია MAML-ს აეპორტიმიზაცია, მეტა-სწავლების მიხედვით, რომ შესწავლოთ ახალი ენებისთვის აეპორტიზაციას. ჩვენ ჩვენი პარამეტრების განსაზღვრება ორი განსაზღვრებული მრავალენგური NLU დავალებებით: მრავალენგური დავალების დიალოგია და ტიპოლოგიურად განსხვავებული კითხვების განსახულება. ჩვენ გამოჩვენებთ, რომ ჩვენი წარმოდგენა უფრო ნავიგური წარმოდგენება, კონკრენტებური წარმოდგენება ორივე საქმებზე უფრო მეტი ენაზე. ჩვენი ანალიზაცია აღმოჩნდა, რომ X-METRA-ADA შეუძლია გავაკეთოთ უფრო სწრაფად ადამიანისთვის განსაზღვრებული მონაცემები.</abstract_ka>
      <abstract_hu>A többnyelvű modellek, mint például az M-BERT és az XLM-R, növekvő népszerűségre tettek szert, mivel zéró nyelvű transzfer tanulási képességeik vannak. Általánosítási képességük azonban továbbra is következetlen a tipológiailag különböző nyelvek és a különböző referenciaértékek között. Az utóbbi időben a metatanulás ígéretes technikáként szerezte meg a figyelmet az alacsony erőforrásokkal rendelkező forgatókönyvek közötti transzfertanulás fokozására: különösen a természetes nyelvi megértésben (NLU) többnyelvű transzfernél. Ebben a munkában javasoljuk az X-METRA-ADA-t, egy többnyelvű MEta-TRAnsfer tanulás ADAPTATION megközelítést az NLU számára. Megközelítésünk a MAML-t, az optimalizáláson alapuló metatanulási megközelítést alkalmazza, hogy megtanuljon alkalmazkodni az új nyelvekhez. Széles körben értékeljük keretrendszerünket két kihívást jelentő, többnyelvű feladatorientált párbeszéd és tipológiailag változatos kérdésekre. Megmutatjuk, hogy megközelítésünk felülmúlja a naiv finomhangolást, és versenyképes teljesítményt ér el mindkét feladatban a legtöbb nyelven. Elemzésünk kimutatja, hogy az X-METRA-ADA korlátozott adatokat tud kihasználni a gyorsabb alkalmazkodás érdekében.</abstract_hu>
      <abstract_el>Τα πολυγλωσσικά μοντέλα, όπως και τα έχουν αποκτήσει αυξανόμενη δημοτικότητα, λόγω των δυνατοτήτων εκμάθησης μεταξύ γλωσσών. Ωστόσο, η δυνατότητα γενικοποίησης τους εξακολουθεί να είναι ασυνεπής για τυπολογικά διαφορετικές γλώσσες και για διαφορετικά κριτήρια αναφοράς. Πρόσφατα, η μετα-μάθηση έχει συγκεντρώσει την προσοχή ως μια ελπιδοφόρα τεχνική για την ενίσχυση της μάθησης μεταφοράς σε σενάρια χαμηλού δυναμικού: ιδιαίτερα για τη διασυνοριακή μεταφορά στην κατανόηση φυσικής γλώσσας (ΝLU). Σε αυτή την εργασία, προτείνουμε μια προσέγγιση διακρατικής εκμάθησης ΜΕτα-TRAnsfer για τη NLU. Η προσέγγισή μας προσαρμόζει μια προσέγγιση μετα-μάθησης βασισμένη στη βελτιστοποίηση, για να μάθει να προσαρμόζεται στις νέες γλώσσες. Αξιολογούμε εκτενώς το πλαίσιο μας σε δύο απαιτητικές διγλωσσικές εργασίες: πολύγλωσσο διάλογο προσανατολισμένο στις εργασίες και τυπολογικά ποικίλες απαντήσεις σε ερωτήσεις. Δείχνουμε ότι η προσέγγισή μας ξεπερνά την αφελή τελειοποίηση, επιτυγχάνοντας ανταγωνιστικές επιδόσεις και στις δύο εργασίες για τις περισσότερες γλώσσες. Η ανάλυση μας αποκαλύπτει ότι το X-METRA-ADA μπορεί να αξιοποιήσει περιορισμένα δεδομένα για ταχύτερη προσαρμογή.</abstract_el>
      <abstract_it>I modelli multilingue, come M-BERT e XLM-R, hanno guadagnato crescente popolarità, grazie alle loro capacità di apprendimento cross-lingual transfer zero-shot. Tuttavia, la loro capacità di generalizzazione è ancora incoerente per lingue tipologicamente diverse e tra diversi parametri di riferimento. Recentemente, il meta-apprendimento ha attirato l'attenzione come una tecnica promettente per migliorare l'apprendimento di trasferimento in scenari a basso contenuto di risorse: in particolare per il trasferimento cross-lingual in Natural Language Understanding (NLU). In questo lavoro, proponiamo X-METRA-ADA, un approccio cross-lingual MEta-TRAnsfer apprendimento ADAptation per NLU. Il nostro approccio adatta MAML, un approccio meta-learning basato sull'ottimizzazione, per imparare ad adattarsi a nuove lingue. Valutiamo ampiamente il nostro framework su due impegnativi compiti di NLU cross-lingual: dialogo multilingue orientato ai compiti e risposta tipologicamente diversificata alle domande. Dimostriamo che il nostro approccio supera l'ingenua messa a punto, raggiungendo prestazioni competitive su entrambi i compiti per la maggior parte delle lingue. La nostra analisi rivela che X-METRA-ADA può sfruttare dati limitati per un adattamento più rapido.</abstract_it>
      <abstract_mk>Мултијазичните модели, како што се M-BERT и XLM-R, добија зголемувачка популарност, поради нивните нултирани способности за трансфер на јазик. Сепак, нивната генерализациска способност е сé уште несогласна за типологички различни јазици и преку различни споредби. Неодамна метаучењето доби внимание како ветувачка техника за подобрување на префрлањето на учењето во скенарија со ниски ресурси: особено за прекујазички префрлање во Разбирањето на природниот јазик (НЛУ). Во оваа работа предложуваме Х-МЕТРА-АДА, крстојазичен пристап за учење на мета-трансфер Адаптација за НЛУ. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages.  Ги проценуваме нашите рамки за две предизвикувачки прекујазични задачи на НЛУ: мултијазички дијалог ориентиран на задачите и типологички различни одговори на прашања. Ние покажуваме дека нашиот пристап го надминува наивното финетизирање, достигнувајќи конкурентна перформанса на двете задачи за повеќето јазици. Нашата анализа открива дека X-METRA-ADA може да влијае на ограничени податоци за побрза адаптација.</abstract_mk>
      <abstract_lt>Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities.  However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks.  Pastaruoju metu metamokymasis sutelkė dėmesį kaip pažadėtiną metodą, skirtą gerinti mokymąsi perkeliant mokymąsi mažai išteklių turinčiais scenarijais: ypač kalbų perkėlimui į gamtos kalbų supratimą (NLU). Šiame darbe siūlome X-METRA-ADA, tarpkalbį MEta-Transfer mokymosi metodą, pritaikytą NLU. Mūsų metodas pritaikomas MAML, optimizavimu pagrįstam metamokymosi metodui, siekiant išmokti prisitaikyti prie naujų kalbų. Išsamiai vertiname savo sistemą dviem sudėtingomis tarpkalbinėmis NLU užduotimis: daugiakalbis į užduotis orientuotas dialogas ir tipologiškai įvairūs klausimų atsakymai. Mes parodome, kad mūsų požiūris yra naivus ir patobulintas, siekiant konkurencinių rezultatų abiejose užduotyse daugumoje kalbų. Mūsų analizė rodo, kad X-METRA-ADA gali sutelkti ribotus duomenis greičiau prisitaikyti.</abstract_lt>
      <abstract_kk>М-BERT және XLM-R секілді бірнеше тілдік үлгілері нөл тілдік аудару мүмкіндіктерінің себебі, нөл түрлі аудару мүмкіндіктері үшін бірнеше мәліметті көтерді. Бірақ олардың жалпы түрлендіру мүмкіндігі типтологиялық тілдерге және әртүрлі түрлендіру мәліметтеріне қатысты. Жуырда, мета оқыту көмегімен көмек ресурстар сценариясындағы транспорт оқыту үшін көмектесетін техникалық болып, өзіне Тәуелді тілді түсініктерінің көпшілікті аудару үшін (НLU). Бұл жұмыс ішінде біз X-METRA-ADA, NLU үшін бірнеше тілді MEta-TRAnsfer оқыту адаптациялық тәсілін ұсынамыз. Біздің тәсіліміз MAML-ды, оптимизациялау негіздеген мета оқыту тәсілігін, жаңа тілдерге адаптациялауды үйрену үшін қолданылады. Біз қоршауымызды екі тілді NLU тапсырмаларының көпшілігін бақылап, бірнеше тілді тапсырмалар бағытталған диалог және типтологиялық түрлі сұрақ жауаптары туралы дұрыстық оқу. Біз өзіміздің тәсіліміздің көпшілігіміздің екі тапсырмаларының көпшілігі үшін бақылау жұмысын жасайды. Біздің анализамыз X-METRA-ADA тез адаптациялау үшін шектелген деректерді көмектесе алады.</abstract_kk>
      <abstract_ms>Model berbilang bahasa, seperti M-BERT dan XLM-R, telah meningkat popularitas, disebabkan kemampuan belajar pemindahan bahasa secara sempurna. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks.  Baru-baru ini, pembelajaran-meta telah mendapatkan perhatian sebagai teknik yang berjanji untuk meningkatkan pembelajaran pemindahan di bawah skenario sumber rendah: terutama untuk pemindahan saling bahasa dalam Pemahaman Bahasa Alami (NLU). Dalam kerja ini, kami cadangkan X-METRA-ADA, pendekatan pelajaran MEta-TRAnsfer salib bahasa untuk NLU. Pendekatan kami menyesuaikan MAML, pendekatan pembelajaran meta berdasarkan optimizasi, untuk belajar menyesuaikan kepada bahasa baru. Kami menilai secara ekstensif kerangka kami pada dua tugas NLU saling bahasa menantang: dialog berbagai bahasa yang mengarah tugas dan jawapan soalan yang berbeza secara tipologik. Kami menunjukkan bahawa pendekatan kita lebih melampaui penyesuaian naif, mencapai prestasi kompetitif pada kedua-dua tugas untuk kebanyakan bahasa. Analisis kami menunjukkan bahawa X-METRA-ADA boleh menggunakan data terbatas untuk penyesuaian lebih cepat.</abstract_ms>
      <abstract_ml>എംബെര്‍ട്ടിയെയും എക്സ്എല്‍എംആരിയെയും പോലുള്ള പല ഭാഷകളുടെ മോഡലുകള്‍ പൂര്‍ണ്ണമായി വര്‍ദ്ധിപ്പിച്ചിരിക്കുന്നു. അവരുടെ പൂര്‍ണ്ണമായ ക് എന്നാലും അവരുടെ സാധാരണ ഭാഷകള്‍ക്കും വ്യത്യസ്ത ഭാഷകള്‍ക്കും വ്യത്യസ്തമായ ബെന്‍മാര്‍ക്കുകള്‍ക്കും അവരുടെ സാധാരണ കഴിവു Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU).  ഈ പ്രവര്‍ത്തനത്തില്‍, നമ്മള്‍ എക്സ്-മെട്രാ-എഡാവിനെ പ്രൊദ്ദേശിപ്പിക്കുന്നു, ഒരു ക്രിസ്റ്റ ഭാഷ മെറ്റാ-ട്രാന്‍ഫെര്‍ എ നമ്മുടെ അടുത്തുള്ള പ്രായോഗ്യം MAML-ലേക്ക് ചേര്‍ക്കുന്നു, ഒരു മാറ്റ-അടിസ്ഥാനമായ മെറ്റ-പഠിക്കുന്ന പ്ര നമ്മുടെ ഫ്രെയിമെക്കുകള്‍ വിശാലമായി വിലാസപ്പെടുത്തുന്ന രണ്ട് വിലാസക്കാരികളായ NLU ജോലികളില്‍ നമ്മുടെ ഫ്രെയിമെക്കുകള്‍ വിലാസ ഞങ്ങള്‍ കാണിക്കുന്നത് നമ്മുടെ അടുത്തുള്ള അടുത്തുള്ള പ്രവര്‍ത്തനങ്ങള്‍ നൈവ് സുന്ദരമായി പ്രവര്‍ത്തിപ്പിക്കുന്ന നമ്മുടെ അന്വേഷണം കാണിച്ചു കൊണ്ടിരിക്കുന്നത് എക്സ്-മെട്രാ-ADA വേഗത്തില്‍ അഡാപ്റ്റേഷന്‍റെ പരിധികള്‍</abstract_ml>
      <abstract_mn>M-BERT болон XLM-R зэрэг олон хэл загварууд тэдний хичээл хэлний шилжүүлэх чадварын тулд нэмэгдэж байна. Гэвч тэдний ерөнхийлөгчийн чадварыг бичил төрлийн хэл болон өөр төрлийн салбаруудын тухай харьцуулж чадахгүй байна. Саяхан мета суралцах нь бага баялаг боловсролын хувилбарын доор шилжүүлэх суралцааг илүү амлалтай техник болгон анхаарлаа хандуулсан. Ялангуяа Байгалийн хэл ойлголтын олон хэлний шилжүүлэх (НLU) хувилбар бол Энэ ажил дээр бид Х-МЕТРА-АДА, НЛУ-д ДОХ-ТРАНФЕР суралцах олон хэлний MEta-TRAnsfer суралцах арга замыг санал болгоно. Манай аргыг шинэ хэл дээр адаптацийг сурах боломжтой MAML-г сайжруулдаг. Бид хэлний олон хэлний даалгаварын тухай хоёр шаардлагатай НЛУ-ын даалгаварын талаар дүүрэн үнэлдэг. Бид олон хэлний даалгаварын талаар ярилцлага, хэлбэрийн өөр өөр асуулт хариулт. Бид харуулж байгаагаар бидний арга зам нь ихэнх хэлний хоёр даалгаврын даалгаврыг дамжуулж, өрсөлдөг үйл ажиллагаа хийдэг. Бидний шинжилгээнд X-METRA-ADA нь хурдан адилтгах боломжтой болгон хязгаарлагдсан мэдээллийг ашиглаж чадна.</abstract_mn>
      <abstract_mt>Il-mudelli multilingwi, bħal M-BERT u XLM-R, kisbu popolarità dejjem akbar, minħabba l-kapaċitajiet tagħhom ta’ tagħlim ta’ trasferiment translingwi mingħajr skop. Madankollu, il-kapaċità tagħhom ta’ ġeneralizzazzjoni għadha inkonsistenti għal lingwi tipoloġikament differenti u f’punti ta’ riferiment differenti. Dan l-a ħħar, it-tagħlim meta kiseb attenzjoni bħala teknika promettenti għat-titjib tat-tagħlim tat-trasferiment f’xenarji b’riżorsi baxxi: b’mod partikolari għat-trasferiment translingwi fil-Ftehim tal-Lingwa Naturali (NLU). F’din il-ħidma, nipproponu X-METRA-ADA, approċċ ta’ adattament għat-tagħlim translingwi tal-MEta-Trasferiment għall-NLU. L-approċċ tagħna jadatta MAML, approċċ meta-tagħlim ibbażat fuq l-ottimizzazzjoni, biex jitgħallem jadatta għal lingwi ġodda. Aħna jevalwaw b’mod estensiv il-qafas tagħna dwar żewġ kompiti translingwi ta’ NLU li jisfidaw: djalogu multilingwi orjentat lejn kompiti u tweġiba għal mistoqsijiet tipoloġikament diversifikata. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages.  Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.</abstract_mt>
      <abstract_no>Fleirspråk modeller, som M-BERT og XLM-R, har fått økt popularitet på grunn av sine null-snitt krysspråk-læringskapasiteten for å overføra. Den generelliseringsverdien er imidlertid inkonsistent for typologisk ulike språk og på ulike benchmarker. Nyleg har metalæring forstått oppmerksomhet som eit promising teknikk for å forbedra læring av overføringar under låg ressursscenario: spesielt for cross-language transfer in Natural Language Understanding (NLU). I denne arbeiden foreslår vi X-METRA-ADA, eit krysspråk MEta-TRAnsfer læring av ADAptasjonslinjen for NLU. Nærminga vårt tilpassar MAML, ein optimaliseringsbasert metalæringstilnærming for å lære å tilpassa nye språk. Vi evaluerer rammeverket vårt på to vanskeleg NLU-oppgåver: dialogvindauge med fleirspråk orientert oppgåve og typologisk forskjellig svar på spørsmål. Vi viser at tilnærminga vårt utfører naive fine-tuning, når det gjer konkurentivt utvikling på begge oppgåver for dei fleste språk. Analysen vårt viser at X-METRA-ADA kan levera begrensede data for raskare adaptasjon.</abstract_no>
      <abstract_pl>Modele wielojęzyczne, takie jak M-BERT i XLM-R, zyskały coraz większą popularność ze względu na możliwości uczenia się wielojęzycznego transferu zero-shot. Jednak ich zdolność do uogólniania jest nadal niespójna dla zróżnicowanych typologicznie języków i różnych wskaźników odniesienia. Ostatnio meta-learning zyskał uwagę jako obiecująca technika poprawy uczenia się transferowego w scenariuszach niskich zasobów: zwłaszcza w przypadku transferu między językami w zakresie rozumienia języka naturalnego (NLU). W niniejszej pracy proponujemy X-METRA-ADA, wielojęzyczne podejście MEta-TRAnsfer learning ADAPTation dla NLU. Nasze podejście dostosowuje MAML, oparte na optymalizacji podejście meta-learning, aby nauczyć się adaptować do nowych języków. Szczegółowo oceniamy nasze ramy pod kątem dwóch trudnych zadań wielojęzycznych NLU: dialogu wielojęzycznego zorientowanego na zadania i typologicznie zróżnicowanego odpowiedzi na pytania. Pokazujemy, że nasze podejście przewyższa naiwne dostrajanie, osiągając konkurencyjną wydajność w obu zadaniach dla większości języków. Nasza analiza ujawnia, że X-METRA-ADA może wykorzystać ograniczone dane do szybszej adaptacji.</abstract_pl>
      <abstract_ro>Modelele multilingve, cum ar fi M-BERT și XLM-R, au câștigat popularitate din ce în ce mai mare, datorită capacităților lor de învățare translingvistică zero-shot. Cu toate acestea, capacitatea lor de generalizare este încă inconsecventă pentru limbi tipologice diverse și pentru diferite criterii de referință. Recent, meta-învățarea a atras atenția ca o tehnică promițătoare pentru îmbunătățirea învățării transferului în scenarii cu resurse reduse: în special pentru transferul translingvistic în înțelegerea limbilor naturale (NLU). În această lucrare, propunem X-METRA-ADA, o abordare translingvă MEta-TRAnsfer de învățare ADAPTATION pentru NLU. Abordarea noastră adaptează MAML, o abordare meta-învățare bazată pe optimizare, pentru a învăța să se adapteze la noi limbi. Evaluăm pe scară largă cadrul nostru în ceea ce privește două sarcini dificile translingve NLU: dialogul multilingv orientat spre sarcini și răspunsul tipologic divers la întrebări. Aratăm că abordarea noastră depășește reglarea fină naivă, atingând performanțe competitive în ambele sarcini pentru majoritatea limbilor. Analiza noastră arată că X-METRA-ADA poate utiliza date limitate pentru o adaptare mai rapidă.</abstract_ro>
      <abstract_so>Tusaalada luuqadaha kala duduwan, sida M-BERT iyo XLM-R, waxay ku kordhaan aqoontooda barashada luqada kala duwan darteed. Si kastaba ha ahaatee awoodooda dhalashada weli ma xirna sida caadiga ah luqado kala duduwan iyo sidoo kale habboon kala duduwan. Mudankii u dhowaaday, waxbarashada meta-meta waxay u hagaajisay qalabka loo ballanqaaday in la koriyo barashada iskuulka hoos-hoos-nololeed: si gaar ah u wareejiyo luqada kala duwan ee afka dabiiciga (NLU). Markaas waxan, waxaan soo jeedaynaa X-METRA-ADA, taas oo ah mid af kala duduwan MEta-TRAnsfer barashada ADAptation qaab u ah NLU. Dhaqdhaqaalkayagu wuxuu habboonayaa MAML, qaab barashada meta-based ah, si aan u barto in luqadaha cusub lagu beddelo. Si ballan ah ayaannu ku qiimeynaynaa qoraalkayaga labada qasab oo af kala duduwan ee NLU: dialogue luuqado kala duduwan iyo jawaabta su'aalo kala duduwan. Waxaynu muujinnaa in dhaqdhaqaalahayagu uu soo bandhigaa tababar wanaagsan, oo uu ku gaadho tababar adag oo ku saabsan labada shaqooyin ee luuqadaha badan. Analyskayagu wuxuu muujiyaa in X-METRA-ADA uu soo diri karo macluumaad xad u leh si dhaqso loo beddelo.</abstract_so>
      <abstract_sr>Mnogjezički modeli, kao što su M-BERT i XLM-R, dobili su veću popularnost zbog njihovih sposobnosti za učenje prevođenja jezika bez snimanja. Međutim, njihova sposobnost generalizacije još uvijek nije u skladu sa tipološki različitim jezicima i preko različitih kriterija. Nedavno je metaučenje privuklo pažnju kao obećavajuću tehniku za unapređenje učenja prijenosa pod scenarijem niskih resursa: posebno za cross-lingual transfer u razumijevanju prirodnog jezika (NLU). U ovom poslu predlažemo X-METRA-ADA, preko jezika MEta-TRAnsfer, pristup ADAptacije za NLU. Naš pristup prilagođuje MAML, optimizacijski metaučenjeni pristup, da nauèimo da se prilagodimo novim jezicima. Prošireno procjenjujemo naš okvir na dva izazova međujezičkih NLU zadataka: multijezički dijalog orijentiranog zadataka i tipološki različita odgovora na pitanja. Pokazujemo da naš pristup nadmašuje naivno fino-tuniranje, postiže konkurentni izvor na oba zadatka za većinu jezika. Naša analiza otkriva da X-METRA-ADA može uticati na ograničene podatke za brže adaptacije.</abstract_sr>
      <abstract_si>ගොඩක් භාෂාවික මොඩේල්, M-BERT සහ XLM-R වගේම, ඔවුන්ගේ සුන්ධ ශූන්ය විශේෂ භාෂාවික විද්‍යාපනය ඉගෙන ගන්න සක්ෂමතා නමුත්, ඔවුන්ගේ සාමාන්‍ය ක්‍රියාත්මක විවිධ භාෂාව සහ වෙනස් බෙන්ච්මාර්ක් වලට තියෙන්නේ නැහැ. අලුත් වෙලාවේ, මෙටා-ඉගෙනීමේ අවධානයක් හොයාගත්තා ප්‍රශ්නයක් විදියට ප්‍රශ්නයක් වෙනුවෙන් ප්‍රශ්නයක් වෙනුවෙන් ප්‍රශ්නයක මේ වැඩේ අපි X-METRA-ADA සැලසුම් කරනවා, ක්‍රිස් භාෂාවක් MEta-TRAnsfer ඉගෙන ගන්න ADAption approach for NLU. අපේ ප්‍රවේශනය MAML වෙනුවෙන්, අළුත් භාෂාවට සමාන්‍ය වෙනුවෙන් ඉගෙන ගන්න හොඳයි. අපි ප්‍රශ්නයක් විශ්වාසයෙන් අපේ පරීක්ෂණය විශ්වාසයෙන් ප්‍රශ්නයක් දෙන්න ප්‍රශ්නයක් කරනවා: විශාල භාෂාත්මක වැඩ අපි පෙන්වන්නේ අපේ ප්‍රවේශනය නිර්භාවිත විදිහට ප්‍රවේශනය කරනවා කියලා, ගොඩක් භාෂාවට ප්‍රවේශනය කරන්න ප්‍ අපේ විශ්ලේෂණය පෙන්වන්නේ X-METRA-ADA පුළුවන් වේගයෙන් සීමාවිත දත්ත ප්‍රයෝජනය කරන්න.</abstract_si>
      <abstract_sv>Flerspråkiga modeller, som M-BERT och XLM-R, har blivit allt populärare på grund av deras möjligheter att överföra flera språk. Deras generaliseringsförmåga är dock fortfarande inkonsekvent för typologiskt olika språk och över olika riktmärken. Nyligen har metainlärning fått uppmärksamhet som en lovande teknik för att förbättra transferinlärning under lågresursscenarier: särskilt för cross-lingual transfer in Natural Language Understanding (NLU). I detta arbete föreslår vi X-METRA-ADA, ett flerspråkigt MEta-TRAnsfer lärande ADAPTATION tillvägagångssätt för NLU. Vårt tillvägagångssätt anpassar MAML, ett optimeringsbaserat meta-lärande tillvägagångssätt, för att lära sig att anpassa sig till nya språk. Vi utvärderar grundligt vårt ramverk för två utmanande tvärspråkiga NLU-uppgifter: flerspråkig uppgiftsorienterad dialog och typologiskt varierande frågeställningar. Vi visar att vårt tillvägagångssätt överträffar naiv finjustering och uppnår konkurrenskraftig prestanda på båda uppgifterna för de flesta språk. Vår analys visar att X-METRA-ADA kan utnyttja begränsade data för snabbare anpassning.</abstract_sv>
      <abstract_ta>M-BERT மற்றும் XLM-R போன்ற பல மொழி மாதிரிகள் பூஜ்ஜியமாக்கப்பட்ட மொழி மொழி மாற்றும் கற்றுக்கொள்ளும் சக்திகள் காரணத்தால் அதிகப்படுத்த ஆனால், அவர்களுடைய பொதுவாக்குதல் இயல்பான மொழிகளுக்கு இன்னும் பொருத்தமானது மற்றும் வித்தியாசமான குறிப்புகளு சமீபத்தில், மெடா கற்றல் கவனத்தை குறைந்த மூலத்தின் கீழ் மாற்றும் கற்றத்தை மேம்படுத்துவதற்கு ஒரு வாக்களிக்கும் தொழில்நுட்பமான தொழில்நுட்பமாக மாற்றி  இந்த வேலையில், நாம் எக்ஸ்-மெட்ரா-ADA, ஒரு கிரும்மொழி MEta-TRAnsfer ADAptation approach for NLU கற்றுக்கொள்வதை நினைவூட்டுகிறோம். நம்முடைய அணுக்கம் MAML ஒப்புக்கொள்கிறது, புதிய மொழிகளுடன் ஒப்புக்கொள்வதற்கு, மேம்பாட்டு அடிப்படையான மெடா கற நாம் விரிவாக எங்கள் சட்டத்தை இரண்டு சவாலிக்கும் மொழி NLU பணிகளின் மீது மதிப்பிட வேண்டும்: பல மொழி செயல் திசைக்கும் உரையாடல் மற்றும்  We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages.  எங்கள் ஆராய்ச்சி தெரிவிக்கிறது எக்ஸ்-METRA-ADA வேகமான ஒழுங்குதலுக்கு வரம்பு தகவல்களை வழங்க முடியும்.</abstract_ta>
      <abstract_ur>بہت سی زبان مدل، جیسے M-BERT اور XLM-R، ان کی صفر-شٹ کروسٹ زبان کی تعلیم قابلیت کے سبب اضافہ کی گئی ہے۔ لیکن ان کی عمومی تعلیم قابلیت ابھی تک ٹائیپولوژیکی مختلف زبانوں اور مختلف بنچمبارک کے لئے مختلف ہے. اچھے وقت، مٹا سکھونٹ نے دھوپ کی تخصیل کے طور پر انتقال کی تعلیم کم منبع سینارییوں کے نیچے بڑھنے کے لئے اظہار کیا ہے: مخصوصاً طبیعی زبان سمجھنے (NLU) میں cross-lingual transfer کے لئے۔ اس کام میں ہم X-METRA-ADA کی پیشنهاد کریں گے، ایک کرس زبان MEta-TRAnsfer کی ADAptation approach NLU کے لئے. ہمارا طریقہ MAML کو اچھی طریقہ سے آمادہ کرتا ہے، ایک مٹا سیکھنے کی طریقہ، نوی زبانوں کے ساتھ اچھی طریقہ سے آمادہ کرتا ہے۔ ہم اپنے فرمود کو دو مشکل زبانی NLU کے کاموں پر پھیلاتے ہیں: multilingual task-oriented dialog اور typologically diverse question replying. ہم دکھاتے ہیں کہ ہماری تقریبا بہت سی زبانوں کے لئے بہت سی کاموں پر مسابقات کے کام پہنچ رہی ہے۔ ہماری تحلیل ظاہر کرتا ہے کہ X-METRA-ADA بہت جلد اٹھانے کے لئے محدودہ ڈیٹا کو لکھ سکتا ہے.</abstract_ur>
      <abstract_uz>M-BERT va XLM-R kabi bir necha tillar modellari ularning o'rganish imkoniyatlarini zero o'zgartirish imkoniyatini sababchi bo'lgan odamlarga oshadi. Lekin ularning umumiy qobiliyatlari oddiy turli tillar va har xil xil bog'lamalar uchun muvaffaqiyatli emas. Yaqinda, meta ta'limni o'rganishni yaratish uchun o'rganishni qanchalik o'rganishni o'rganishga ishonchini o'rganish uchun ishlatiladi. Hullas, tabiiy tilda o'zgartirish (NLU) kabi tilni o'rganish uchun. Bu ishda, biz X-METRA-ADA, biz NLU uchun bir necha tillar MEta-TRAnsfer o'rganishni o'rganishni talab qilamiz. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages.  Biz ikkita tillar qo'llangan NLU vazifalarini kengaytirib qiymatmiz: muloqon muloqon muloqat bilan muloqat muloqat muloqat muloqat muloqat muloqat muloqat oynasini va oddiy savol javob beradi. Biz ko'rsatganimizni ko'rsatdikki, biz ko'pchilik tillar uchun bir xil vazifalarga yetarlicha bajarayotganimizni ko'rsatdik. Analytikiz X-METRA-ADA tez adaptlash uchun chegara maʼlumot yozib olish mumkin.</abstract_uz>
      <abstract_vi>Các mô hình đa ngôn ngữ, như M-BERT và XLM-R, đã trở nên nổi tiếng nhờ vào khả năng học tập truyền ngôn ngữ khác nhau của họ. Tuy nhiên, khả năng tổng hợp của họ vẫn không khớp với ngôn ngữ khác nhau theo tiêu chuẩn khác nhau. Gần đây, meta-Learning đã được chú ý như một kỹ thuật hứa hẹn để phát triển học thuyên chuyển trong các tình huống với nguồn ít tài nguyên: đặc biệt cho việc chuyển giao ngôn ngữ rộng trong hiểu biết ngôn ngữ tự nhiên (Ntrường). Trong công việc này, chúng tôi đề nghị X-MENgựa-ADA, một cách tiếp cận phân biệt ngôn ngữ giữa MEta-TRAnsfer. Cách tiếp cận của chúng ta thích nghi MAML, một phương pháp phân biệt các meta-leaving, để học cách thích nghi với ngôn ngữ mới. Chúng tôi đánh giá rộng cơ sở của chúng tôi về hai công việc vượt ngôn ngữ rộng thử thách: hộp thoại chuyên mục đa dạng và trả lời các câu hỏi khác nhau. Chúng tôi cho thấy cách tiếp cận của chúng tôi hoàn thành những nghi thức ngây thơ, đạt được thành công trong cả hai nhiệm vụ cho hầu hết các ngôn ngữ. Phân tích của chúng tôi cho thấy X-METRA-ADA có thể thu thập dữ liệu giới hạn để thích nghi nhanh hơn.</abstract_vi>
      <abstract_bg>Многоезичните модели, като например придобиват все по-голяма популярност, благодарение на техните възможности за обучение с нулев изстрел между езиците. Въпреки това, способността им за обобщаване все още е несъвместима за типологично разнообразни езици и в различни показатели. Наскоро метаобучението привлече внимание като обещаваща техника за подобряване на трансферното обучение при сценарии с ниски ресурси: особено за междуезичен трансфер в разбирането на естествения език (НЛУ). В тази работа предлагаме Х-МЕТРА-АДА, междуезичен подход за обучение за НЛУ. Нашият подход адаптира базиран на оптимизация мета-учене подход, за да се научи да се адаптира към нови езици. Ние обстойно оценяваме нашата рамка по две предизвикателни междуезични задачи на НЛУ: многоезичен диалог, ориентиран към задачите и типологично разнообразен отговор на въпроси. Показваме, че нашият подход превъзхожда наивното фино настройване, достигайки конкурентни резултати и при двете задачи за повечето езици. Нашият анализ показва, че Х-МЕТРА-АДА може да използва ограничени данни за по-бърза адаптация.</abstract_bg>
      <abstract_da>Flersprogede modeller, såsom M-BERT og XLM-R, har fået stigende popularitet på grund af deres nul-shot tværsprogede overførsel læringsevner. Men deres generaliseringsevne er stadig inkonsekvent for typologisk forskellige sprog og på tværs af forskellige benchmarks. For nylig har meta-læring fået opmærksomhed som en lovende teknik til at forbedre overførsel af læring under scenarier med lave ressourcer: især for tværsproget overførsel i Natural Language Understanding (NLU). I dette arbejde foreslår vi X-METRA-ADA, en tværsproget MEta-TRAnsfer læring ADAPTATION tilgang til NLU. Vores tilgang tilpasser MAML, en optimeringsbaseret meta-læringstilgang, til at lære at tilpasse sig nye sprog. Vi evaluerer grundigt vores rammer på to udfordrende tværsprogede NLU-opgaver: flersproget opgaveorienteret dialog og typologisk forskelligartet spørgsmål besvarelse. Vi viser, at vores tilgang overgår naiv finjustering og opnår konkurrencedygtige resultater på begge opgaver for de fleste sprog. Vores analyse afslører, at X-METRA-ADA kan udnytte begrænsede data til hurtigere tilpasning.</abstract_da>
      <abstract_nl>Meertalige modellen, zoals M-BERT en XLM-R, zijn steeds populairder geworden, vanwege hun zero-shot cross-lingual transfer learning mogelijkheden. Hun generaliseringsvermogen is echter nog steeds inconsistent voor typologisch diverse talen en voor verschillende benchmarks. Recent heeft meta-learning aandacht gekregen als een veelbelovende techniek voor het verbeteren van transferleren in scenario's met weinig middelen: met name voor cross-lingual transfer in Natural Language Understanding (NLU). In dit werk stellen we X-METRA-ADA voor, een meertalige MEta-TRAnsfer learning ADAPTation aanpak voor NLU. Onze aanpak past MAML, een op optimalisatie gebaseerde meta-learning aanpak, aan om zich aan te passen aan nieuwe talen. We evalueren ons framework uitgebreid op twee uitdagende cross-lingual NLU taken: meertalige taakgerichte dialoog en typologisch divers vragenantwoord. We laten zien dat onze aanpak beter presteert dan naïeve finetuning, waardoor we concurrerende prestaties bereiken bij beide taken voor de meeste talen. Uit onze analyse blijkt dat X-METRA-ADA beperkte data kan gebruiken voor snellere aanpassing.</abstract_nl>
      <abstract_id>Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities.  Namun, kemampuan generalisasi mereka masih tidak konsisten untuk bahasa tipologi berbeda dan melalui benchmarks berbeda. Baru-baru ini, meta-belajar telah mendapatkan perhatian sebagai teknik yang berjanji untuk meningkatkan transfer belajar di bawah skenario sumber daya rendah: terutama untuk transfer saling bahasa dalam Natural Language Understanding (NLU). Dalam pekerjaan ini, kami mengusulkan X-METRA-ADA, pendekatan MEta-TRAnsfer berbahasa saling belajar Adaptasi untuk NLU. pendekatan kita menyesuaikan MAML, pendekatan meta-belajar berasaskan optimisasi, untuk belajar menyesuaikan bahasa baru. Kami secara ekstensif mengevaluasi cadangan kami pada dua tugas NLU saling bahasa menantang: dialog berbagai bahasa yang mengarientasi tugas dan menjawab pertanyaan tipologis berbeda. Kami menunjukkan bahwa pendekatan kita melebihi penyesuaian naif, mencapai prestasi kompetitif pada kedua tugas untuk kebanyakan bahasa. Analisi kami mengungkapkan bahwa X-METRA-ADA dapat menggunakan data terbatas untuk adaptasi lebih cepat.</abstract_id>
      <abstract_hr>Većina jezičkih modela, poput M-BERT i XLM-R, dobili su veću popularnost zbog njihovih sposobnosti za učenje preko jezika bez pucnjave. Međutim, njihova sposobnost generalizacije još uvijek nije u skladu s tipološki različitim jezicima i raznim kriterijama. Nedavno je metaučenje privuklo pažnju kao obećavajuću tehniku za unapređenje učenja prijenosa u području scenarija niskih resursa: posebno za cross-lingual transfer u razumijevanju prirodnog jezika (NLU). U ovom poslu predlažemo X-METRA-ADA, cross-lingual MEta-TRAnsfer učenjem pristupa ADAptacije za NLU. Naš pristup prilagođuje MAML, optimizacijski metaučenjeni pristup, kako bi se naučio prilagoditi novim jezicima. Proširoko procjenjujemo naš okvir na dva izazovnog prekograničnog NLU zadatka: multijezički dijalog orientiranog na zadatke i tipološki različita odgovora na pitanja. Pokazujemo da naš pristup nadmašuje naivno ispravljanje, postizanje konkurentnih učinka na obje zadatke za većinu jezika. Naša analiza otkriva da X-METRA-ADA može utjecati na ograničene podatke za brže adaptacije.</abstract_hr>
      <abstract_fa>مدل‌های زیادی زبان، مانند M-BERT و XLM-R، به دلیل توانایی یادگیری با انتقال کردن زبان‌های مختلف صفر، بیشتر شهرت را به دست آورده‌اند. با این حال، توانایی عمومی آنها هنوز برای زبان‌های مختلف نوع‌شناسی و از طریق برچسب‌های مختلف مختلف نیست. اخیرا، یادگیری متا به عنوان تکنیک قول‌دهنده برای افزایش یادگیری انتقال زیر سناریو منابع کم، توجه داده است: مخصوصا برای انتقال متوسط زبان در درک زبان طبیعی (NLU). در این کار، ما X-METRA-ADA را پیشنهاد می‌کنیم، یک طریق ADAptation برای NLU یاد گرفتن MEta-TRAnsfer متزبانی. دستور ما MAML را adapt می‌کند، یک دستور یادگیری meta-learning based on optimization, تا یاد بگیریم که به زبان جدید adapt می‌شود. ما چهارچوب خود را در دو مشکل کار NLU متوسط زبان ارزیابی می کنیم: گفتگو متوسط به کار زبان و جواب سوال متفاوت متفاوت متفاوت. ما نشان می دهیم که دسترسی ما به زیادی از زبان آرامش ساده انجام می دهد، و به اجرای مسابقه در هر دو کار برای بیشتر زبان رسیده است. تحلیل ما نشان می دهد که X-METRA-ADA می تواند داده های محدودیت را برای تغییر سریعتر تحمل کند.</abstract_fa>
      <abstract_ko>M-BERT와 XLM-R과 같은 다국어 모델은 제로 언어 이동 학습 능력으로 점점 인기를 끌고 있다.그러나 서로 다른 유형의 언어와 서로 다른 기준에 대해 그들의 범화 능력은 여전히 일치하지 않는다.최근 몇 년 동안 원 학습은 저자원 상황에서 이동 학습을 추진하는 유망한 기술로서 주목을 받았다. 특히 자연 언어 이해에서의 다중 언어 이동이다.이 작업에서 우리는 X-METRA-ADA, NLU에 적용되는 다중 언어 모듈 이동 학습 적응 방법을 제시했다.우리의 방법은 최적화된 원 학습 방법인 MAML을 바탕으로 새로운 언어에 적응하는 것을 배운다.우리는 두 가지 다중 언어 NLU의 도전적인 임무의 구조를 광범위하게 평가했다. 그것이 바로 다중 언어 임무에 대한 대화와 다양한 질문에 대한 대답이다.우리는 우리의 방법이 간단한 마이크로스피커보다 우수하다는 것을 보여 주었고 대다수 언어의 두 가지 임무에서 경쟁력 있는 성능에 이르렀다.우리의 분석에 의하면 X-METRA-ADA는 유한한 데이터를 이용하여 더욱 빨리 적응할 수 있다.</abstract_ko>
      <abstract_sw>Mfano wa lugha nyingi, kama vile M-BERT na XLM-R, umeongezeka umaarufu, kwa sababu ya uwezo wao wa kuhamisha elimu kwa lugha za lugha sifuri. Hata hivyo, uwezo wao wa uzalishaji bado hauna uhakika kwa lugha mbalimbali na katika maeneo mbalimbali. Hivi karibuni, elimu ya meta imejikuta na hisia kama teknolojia ya kuahidini kuongeza kujifunza kwa ajili ya kuhamisha kwa kiwango cha asili chini ya rasilimali: has a kwa ajili ya uhamishaji wa lugha tofauti katika kuelewa lugha ya asili (NLU). Katika kazi hii, tunapendekeza X-METRA-ADA, lugha mbalimbali ya MEta-TRAnsfer kujifunza mbinu za ADAptation kwa ajili ya NLU. Hatua yetu inabadilisha MAML, mbinu yenye matumaini yenye mafunzo ya meta, ili kujifunza kupambana na lugha mpya. Tutathmini kwa kiasi kikubwa mfumo wetu kuhusu kazi mbili za changamoto za NLU yenye lugha mbalimbali: mazungumzo ya lugha mbalimbali na majibu ya maswali tofauti kwa kawaida. Tunaonyesha kwamba mbinu yetu inaonyesha michoro mazuri, na kufikia ufanisi wa ushindani katika kazi zote kwa lugha nyingi. Uchambuzi wetu unaonyesha kuwa X-METRA-ADA anaweza kutumia taarifa zisizo na mipaka kwa ajili ya kuboresha haraka.</abstract_sw>
      <abstract_de>Mehrsprachige Modelle, wie M-BERT und XLM-R, haben aufgrund ihrer Zero-Shot Cross-Lingual Transfer-Lernfähigkeit zunehmend an Popularität gewonnen. Ihre Verallgemeinerungsfähigkeit ist jedoch immer noch inkonsistent für typologisch unterschiedliche Sprachen und über verschiedene Benchmarks hinweg. In letzter Zeit hat Meta-Learning als vielversprechende Technik zur Verbesserung des Transferlernens unter ressourcenarmen Szenarien Aufmerksamkeit erregt: insbesondere für den translingualen Transfer im Natural Language Understanding (NLU). In dieser Arbeit schlagen wir X-METRA-ADA vor, einen mehrsprachigen MEta-TRAnsfer Lernansatz für NLU. Unser Ansatz adaptiert MAML, einen optimierungsbasierten Meta-Learning-Ansatz, um zu lernen, sich an neue Sprachen anzupassen. Wir evaluieren unser Framework umfassend auf zwei herausfordernde, sprachübergreifende NLU-Aufgaben: mehrsprachiger aufgabenorientierter Dialog und typologisch vielfältige Fragestellungen. Wir zeigen, dass unser Ansatz die naive Feinabstimmung übertrifft und für die meisten Sprachen eine wettbewerbsfähige Leistung bei beiden Aufgaben erreicht. Unsere Analyse zeigt, dass X-METRA-ADA begrenzte Daten für eine schnellere Anpassung nutzen kann.</abstract_de>
      <abstract_tr>M-BERT we XLM-R ýaly köp dil nusgalary, 0-atly çarpaz dil öwrenmesi beceriklerinden öňünden daşyrýarlar. Ýöne olaryň döredilik ukyplary typolojik dürli diller we farklı benchmarklaryň içinde durmaýar. Soňky wagtlar meta-öwrenmek üçin daşary golaý öwrenmesini iň az resurslar senaryýasynda köpräk terjime etmek üçin söz berýän teknik bolup üns çekdi: iň.a ýratynda tebigy dil düşünmesinde cross-dil terjime etmek üçin (NLU). Bu işde X-METRA-ADA, NLU için çoklu dilli MEta-TRAnsfer öwrenmesini teklif ediyoruz. Biziň ýaryşymyz MAML'i, optimizasynda tabanly meta öwrenmek ýaryşyny üýtgedýär, täze dillere üýtgetmek üçin öwrenmek üçin. Biz çerçevemizi iki dilli karışık NLU görevlerinde büyük değerlendiriyoruz: çoxlu dilli görev yönlendirilmiş dialog ve tipolojik çeşitli soru cevapı. Biziň ýaryşymyz köp dilleriň üçin döwürli şekilde täsir edip, ýaryşykly hereketlerimizi başarmaýandygyny görkezýäris. Biziň analýzymyz X-METRA-ADA çalt adaptasyýa üçin hadysy maglumaty etmäge mümkin edip biler.</abstract_tr>
      <abstract_sq>Modelet shumëgjuhësore, të tilla si M-BERT dhe XLM-R, kanë fituar popullaritet në rritje, për shkak të aftësive të tyre zero-shot të mësimit ndërgjuhësor transferimi. Megjithatë, aftësia e tyre e gjeneralizimit është ende e pakonsistentë për gjuhët tipologjikisht të ndryshme dhe nëpërmjet pikave të ndryshme. Kohët e fundit, meta-mësimi ka fituar vëmendje si një teknikë premtuese për përmirësimin e mësimit të transferimit në skenarë me burime të ulëta: veçanërisht për transferimin ndërgjuhësor në kuptimin e gjuhës natyrore (NLU). Në këtë punë, propozojmë X-METRA-ADA, një metodë ndërgjuhësore MEta-TRAnsfer për mësimin e adaptimit për NLU. Përqasja jonë përshtatet MAML, një përqasje metamësimi bazuar në optimizacion, për të mësuar të përshtatet ndaj gjuhëve të reja. Ne vlerësojmë në mënyrë të gjerë kuadrin tonë mbi dy detyra sfiduese ndërgjuhësore të NLU: dialog shumëgjuhësor i orientuar ndaj detyrave dhe përgjigje tipologjike të ndryshme pyetjesh. Ne tregojmë se qasja jonë mbizotëron rregullimin naiv, duke arritur performancën konkurruese në të dy detyrat për shumicën e gjuhëve. Analiza jonë zbulon se X-METRA-ADA mund të përdorë të dhëna të kufizuara për përshtatje më të shpejtë.</abstract_sq>
      <abstract_am>እንደ M-BERT እና XLM-R፣ የቋንቋ-ቋንቋ-ቋንቋ ተማሪ ችሎታቸውን በቁጥር የተመሳሳይ የቋንቋ ቋንቋዎች አካሄዱ፡፡ ነገር ግን የልዩ ቋንቋዎች እና ልዩ ልዩ መለያየት አካባቢዎች የሚቆጣጠር ኃይላቸው ገና የማይገባው ነው፡፡ Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU).  በዚህ ሥራ የሜትራ-ADA የሜታ-TRAnsfer ADAptation ቅድሚያ ለNLU የመማር የቋንቋ ቋንቋ መፍጠርን እናስጀጋለን፡፡ አዲስ ቋንቋዎች ለመተማር ማድረግ ማድረጉን ማህበራዊ መተማር ማድረጉን MAML ያሳድጋል፡፡ በሁለት የቋንቋ ቋንቋዎች የNLU ስራዎችን በሚያጋልጡ ሥርዓታችንን በብዙ ልዩ ቋንቋ እና በተለያዩ የጥያቄ መልስ እናሳውቃለን፡፡ እናሳያቸዋለን የሁለቱ ቋንቋዎች ላይ የፍላጎችን ትክክል እናደርጋለን፡፡ Analysያችን X-METRA-ADA ለፈጥነህ አካባቢ ዳታዎችን የሚያስቀምጥ ይችላል፡፡</abstract_am>
      <abstract_hy>Բազլեզու մոդելները, ինչպիսիք են M-BER-ը և XLM-R-ը, աճել են բնակչությունը, իրենց զրոյի միջլեզվի փոխանցման ուսուցման ունակությունների պատճառով: Այնուամենայնիվ, նրանց ընդհանուր ընդունակությունը դեռևս անհամապատասխան է տիպոլոգիապես բազմազան լեզուների և տարբեր հարաբերականների համար: Վերջերս մետասովորելը ուշադրություն դարձրել է որպես խոստացող տեխնիկա, որպեսզի բարելավվի ուսումնասիրությունը ցածր ռեսուրսներ ունեցող սցենարների ընթացքում, հատկապես բնական լեզվի հասկացության միջև լեզվի փոխանցման համար: Այս աշխատանքի ընթացքում մենք առաջարկում ենք X-Մետրա-ԱԴԱ-ը, երկլեզվային Մետրա-ԱԴԱ-ի ուսումնասիրության հարմարեցման մոտեցում ՆԼՄ-ի համար: Մեր մոտեցումը հարմարեցնում է MAML-ը, օպտիվացման հիմնված մետասովորման մոտեցումը, որպեսզի սովորենք հարմարեցնել նոր լեզուներին: Մենք էքսպենսիվ գնահատում ենք մեր կառուցվածքը երկու մարտահրավերների միջլեզվային ՆԼՀ-ի առաջադրանքների վրա' բազլեզվային առաջադրանքների վրա ուղղությամբ և տիպոլոգիապես տարբեր հարցերի պատասխանների վրա: Մենք ցույց ենք տալիս, որ մեր մոտեցումը գերազանցում է նայիվ բարելավումը, հասնելով մրցակցության արդյունքներին երկու խնդիրների համար լեզուների մեծ մասում: Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.</abstract_hy>
      <abstract_af>Veelvuldige modele, soos M-BERT en XLM-R, het vergroot populariteit verkry, vanweë hul nul-skoot kruistale oordrag-leer kapasiteite. Maar hulle generelliseeringskapasiteit is nog steeds ongelukkig vir tipologies verskillende tale en oor verskillende benchmarke. Onlangs het meta-leer aandag gehou as 'n beloftende teknik vir verhoog van oordrag leer onder lae hulpbronne scenarios: spesiaal vir kruistale oordrag in Natuurlike Taal Verstaan (NLU). In hierdie werk voorstel ons X-METRA-ADA, 'n kruistale MEta-TRAnsfer leer ADAptasie toegang vir NLU. Ons toegang pas MAML, en 'n optimalisasie-gebaseerde meta-leer toegang, om te leer om aan nuwe tale te pas. Ons uitbreidig ons raamwerk evalueer op twee uitgelykende kruistale NLU-taak: multitaalske taak-orienteerde dialoog en tipologiese verskeie vraag antwoord. Ons wys dat ons toegang uitvoer naive fin-tuning, bereik rekenaktiewe prestasie op beide opdragte vir die meeste tale. Ons analisie openbaar dat X-METRA-ADA kan beperkte data vir vinniger aanpassing verwys.</abstract_af>
      <abstract_az>M-BERT və XLM-R kimi çoxlu dil modelləri, sıfır-fərq dillərini öyrənmə qabiliyyətlərinə görə, məşhurluqlarını artırdı. Halbuki, onların generalizasyon qabiliyyəti typolojik müxtəlif dillərə və müxtəlif benchmarklərə uyğun deyildir. Son zamanlarda, meta-öyrənmə təsirlərini düşük ressurs scenariyalarının altında transfer öyrənməsini artırmaq üçün və ’ d verici tekniki olaraq təsirlərini öyrəndi: özellikle təbiətli dil anlama (NLU) içində çox çox dilli transfer üçün. Bu işdə X-METRA-ADA, NLU üçün çox dilli MEta-TRAnsfer öyrənən ADAptasyon metodlarını təklif edirik. Bizim yaxınlığımız MAML'i, optimizasiya tabanlı meta öyrənmə metodlarına uyğunlaşdırır, yeni dillərə uyğunlaşdırmağı öyrənir. Biz çerçivelərimizi iki dildən çox çətin NLU işlərində çox çətin değerləşdiririk: çoxlu dil işləri tərəfindən danışmış danışma və typolojik müxtəlif sual cavabı. Biz göstəririk ki, bizim tərəfimiz çox dillərin hər ikisinin müqayisədə müqayisədə olan hərəkətlərini təşkil edir. Bizim analizimiz X-METRA-ADA'nın daha hızlı uyğunlaşdırma üçün sınırlı məlumatları yaradıb edə biləcəyini göstərir.</abstract_az>
      <abstract_bs>Mnogjezički modeli, kao što su M-BERT i XLM-R, dobili su veću popularnost zbog njihovih sposobnosti za učenje preko jezika bez snimanja. Međutim, njihova sposobnost generalizacije još uvijek nije u skladu sa tipološki različitim jezicima i preko različitih kriterija. Nedavno je metaučenje privuklo pažnju kao obećavajuću tehniku za unapređenje učenja prijenosa pod scenarijem niskih resursa: posebno za cross-lingual transfer u razumijevanju prirodnog jezika (NLU). U ovom poslu predlažemo X-METRA-ADA, preko jezika MEta-TRAnsfer, pristup ADAptacije za NLU. Naš pristup prilagođuje MAML, optimizacijski metaučenjeni pristup, da nauči da se prilagodi novim jezicima. Mi široko procjenjujemo naš okvir na dva izazova preko jezika NLU zadataka: multijezički dijalog orijentiran na zadatak i tipološki različit odgovor na pitanje. Pokazujemo da naš pristup nadmašuje naivnu finalnu prilagodbu, postignući konkurentnu funkciju na obje zadatke za većinu jezika. Naša analiza otkriva da X-METRA-ADA može utjecati na ograničene podatke za brže adaptacije.</abstract_bs>
      <abstract_ca>Els models multilingües, com M-BERT i XLM-R, han guanyat una popularitat creixent, a causa de les seves capacitats d'aprenentatge translingüístic de transfer ència de zero. Tot i així, la seva habilitat de generalització encara és inconsistent en llengües tipològicament diverses i en diferents punts de referència. Recentment, el meta-aprenentatge ha guanyat atenció com una tècnica prometedora per millorar l'aprenentatge de transfer ència en escenaris de baix recursos: especialment per la transferència translingüística a l'Entensió de Llingua Natural (NLU). En aquesta feina, proposem X-METRA-ADA, un enfocament d'aprenentatge translingüístic de MEta-TRAnsfer adaptació per a NLU. El nostre enfocament adapta MAML, un enfocament de meta-aprenentatge basat en l'optimització, per aprendre a adaptar-se a les noves llengües. Evaluam ampliament el nostre marc en dues tasques translingües desafiables de la NLU: diàleg multilingüe orientat a tasques i resposta tipològicament diversa a preguntes. Mostrem que el nostre enfocament supera una perfecció naiva, arribant a un rendiment competitiu en les dues tasques de la majoria de llengües. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.</abstract_ca>
      <abstract_cs>Vícejazyčné modely, jako například M-BERT a XLM-R, získaly rostoucí popularitu díky svým možnostem učení s nulovým přenosem mezi jazyky. Jejich generalizační schopnost je však stále nekonzistentní pro typologicky různé jazyky a napříč různými referenčními hodnotami. V poslední době si meta-learning získal pozornost jako slibná technika pro zlepšení transferového učení v rámci scénářů s nízkými zdroji: zejména pro přenos mezi jazyky v porozumění přírodním jazykům (NLU). V této práci navrhujeme X-METRA-ADA, cross-jazyčný MEta-TRAnsfer learning ADAPTation přístup pro NLU. Náš přístup přizpůsobuje MAML, meta-learningový přístup založený na optimalizaci, aby se naučil adaptovat na nové jazyky. Důkladně hodnotíme náš rámec na dvou náročných vícejazyčných úkolech NLU: vícejazyčný dialog orientovaný na úkoly a typologicky rozmanitý odpověď na otázky. Ukazujeme, že náš přístup překonává naivní jemné ladění a dosahuje konkurenčního výkonu u obou úkolů pro většinu jazyků. Naše analýza ukazuje, že X-METRA-ADA může využít omezené množství dat pro rychlejší adaptaci.</abstract_cs>
      <abstract_et>Mitmekeelsed mudelid, nagu M-BERT ja XLM-R, on suurenenud populaarsust tänu nende nullkeelsele ülekandeõppele. Kuid nende üldistamisvõime on tüpoloogiliselt erinevate keelte ja erinevate võrdlusaluste puhul endiselt vastuolus. Viimasel ajal on metaõpe pälvinud tähelepanu kui paljutõotav tehnika siirdeõppe parandamiseks vähese ressursiga stsenaariumides: eriti keeleülese ülekande puhul looduskeele mõistmises (NLU). Selles töös pakume välja X-METRA-ADA, keeleülese MEta-TRAnsferi õppe ADAptatsiooni lähenemisviisi NLU jaoks. Meie lähenemisviis kohandab MAML-i, optimeerimisel põhinevat metaõppe lähenemisviisi, et õppida kohanema uute keeltega. Hindame põhjalikult oma raamistikku kahel keeleülesel NLU ülesandel: mitmekeelne ülesandepõhine dialoog ja tüüpiliselt mitmekesine küsimustele vastamine. Näitame, et meie lähenemisviis ületab naiivset peenhäälestust, saavutades konkurentsivõimelise tulemuse mõlemal ülesandel enamiku keelte puhul. Meie analüüs näitab, et X-METRA-ADA saab kasutada piiratud andmeid kiiremaks kohanemiseks.</abstract_et>
      <abstract_bn>মাল্টিভাষার মডেল, যেমন M-BERT এবং XLM-R, তাদের শুধুমাত্র শিক্ষা শিক্ষার কারণে জনপ্রিয়তা বৃদ্ধি পেয়েছে। তবে তাদের সাধারণ ভাষায় বিভিন্ন ভাষার জন্য তাদের জেনারেলেশনের ক্ষমতা এখনও অসম্পূর্ণ। Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU).  এই কাজে আমরা এক্স-মেট্রা-এডাকে প্রস্তাব করি, যা ক্রাশ-ভাষার মেটা-ট্রান্সফার এডিএপ্যাটেশন ক্ষেত্রে শিখতে পারে। আমাদের প্রতিযোগিতা ম্যাএমএলের সাথে সুনির্দিষ্ট ভিত্তিক মেটা শিক্ষার উপায়, নতুন ভাষায় যুক্ত করতে শিখতে। আমরা ব্যাপকভাবে আমাদের ফ্রেম মূল্য দিচ্ছি যে দুটি চ্যালেঞ্জের ব্যাপারে প্রতিযোগিতা করছি এনএলইউ কাজ: বহুভাষায় কাজের মুখোমুখি  আমরা দেখাচ্ছি যে আমাদের প্রতিযোগিতা বেশীরভাগ ভাষার জন্য প্রতিযোগিতার প্রতিযোগিতা প্রদর্শন করছে। আমাদের বিশ্লেষণ প্রকাশ করেছে যে এক্স-মেট্রা-ADA দ্রুত আপেটশনের জন্য সীমিত তথ্য লাভ করতে পারে।</abstract_bn>
      <abstract_fi>Monikieliset mallit, kuten M-BERT ja XLM-R, ovat saaneet yhä enemmän suosiota kielten välisen siirtooppimisen ansiosta. Niiden yleistyskyky on kuitenkin edelleen epäjohdonmukainen typologisesti erilaisissa kielissä ja eri vertailuarvoissa. Viime aikoina metaoppiminen on herättänyt huomiota lupaavana keinona parantaa siirtooppimista vähävaraisissa skenaarioissa: erityisesti monikielisessä siirrossa luonnollisen kielen ymmärtämisessä (NLU). Tässä työssä ehdotamme X-METRA-ADA, monikielistä MEta-TRAnsfer oppimisen ADAptation lähestymistapaa NLU:lle. Lähestymistapamme mukauttaa MAML:ää, optimointiin perustuvaa meta-oppimista, oppimaan sopeutumaan uusiin kieliin. Arvioimme viitekehyksiämme laajasti kahdessa haastavassa monikielisessä NLU-tehtävässä: monikielisessä tehtäväkeskeisessä dialogissa ja typologisesti monipuolisessa kysymysvastauksessa. Osoitamme, että lähestymistapamme on naiivia hienosäätöjä parempi, saavuttaen kilpailukykyisen suorituskyvyn molemmissa tehtävissä useimmissa kielissä. Analyysimme osoittaa, että X-METRA-ADA voi hyödyntää rajallisia tietoja nopeampaan sopeutumiseen.</abstract_fi>
      <abstract_jv>Mulalawat model, kaya M-BERT lan XLM-R, padha ugat popularno sing gak popularno kaya sistem sing gak perusahaan sesilyane kapasitasi telutung. politenessoffpolite, "), and when there is a change ("assertivepoliteness Suara, meta-ciliane kuwi nglanggar aturan kanggo teknik sing ngwalikno nggawe ngubah ilem nggawe barang kelas pengguna kuwi mau: supaya kuwi kanggo nggawe langgar tarjamahan kanggo langgambar terus Ninggang langgar (NLU). Nang barêng-barêng iki, kita supoyo X-METRA-AdA, akeh banter-langgar MEta-TRanser seneng nggambar adiptasi kanggo NLU. Awak dhéwé ngerti wiwigat MAML, meta-urip nggawe gerarané karo pertualisi, kanggo ngerwih kanggo langgambar Anyar. Awak dhéwé estetik luwih nggawe barang kelas telu nggawe barang kelas telu nggawe barang langgar-langgar NLU tasks: multi-language task-Oriented dialog dan Typlogically Awak dhéwé ngerasah bener tentang kanggo nggawe barang apik, iso nggawe ngerasah barang langga. Panjenenganipun anyar sumbarang X-METRA-ANA iso nggawe data limiter kanggo ngilanggar tarjamahan kanggo kalaayuni maneh.</abstract_jv>
      <abstract_ha>Motoli masu yawa kamar misãlan M-BERT da XLM-R, sun ƙara umarni, sabo da abincin transference na fassarar-lugha na sifiri. A lokacin da, awonsu ya jenalisa yana bada da ba mai kamfata wa lugha masu turɓãya ko kuma a cikin wasu mistakardan dabam-dabam. A yanzu, karatun meta-ci ya garwaya aikin muhimmin muhimmin muhimmin wa ƙara-resource: kuma, has a'a, muhimmin a shige mistakarda cikin Lugha Haƙĩƙa (NLU). Daga wannan aikin, Munã bukãtar da X-METRA-ADA, wata takardar harshen mai tsawo na MEta-TRAnsfer da za'a sanar da adaptan zuwa NLU. Tsarakanmu na adatar da MAML, wata hanyarwa mai kwaɗayi ta wajen karatun meta-barci, dõmin ya yi kwaɗayi zuwa harshen sãbuwa. Ina iya ƙaddara firam a kan aikin biyu masu tsõratar da ke cikin linguin NLU: Tuna nũna cewa hanyarmu na samar tashi mai kawaici, kuma yana samun rabo da yin gaura a kan aikin dukansu masu cikin harshen. AnalyyinMu yana bayyana cewa X-METRA-ADA yana iya samun data wanda ke iya ƙaranci wa adadi sauri.</abstract_ha>
      <abstract_sk>Večjezični modeli, kot sta M-BERT in XLM-R, so postali vedno bolj priljubljeni zaradi svojih zmogljivosti za prenos medjezičnega učenja brez strela. Vendar pa je njihova sposobnost posploševanja še vedno neskladna za tipološko različne jezike in različne referenčne vrednosti. V zadnjem času je metaučenje pridobilo pozornost kot obetavna tehnika za izboljšanje transfernega učenja v scenarijih z nizkimi viri: zlasti za medjezični prenos v razumevanju naravnega jezika (NLU). V tem delu predlagamo X-METRA-ADA, večjezični pristop MEta-TRAnsfer učenja ADAptacije za NLU. Naš pristop prilagaja MAML, pristop meta učenja, ki temelji na optimizaciji, da se nauči prilagoditi novim jezikom. Naš okvir obsežno ocenjujemo glede dveh zahtevnih medjezičnih nalog NLP: večjezičnega dialoga, usmerjenega v naloge, in tipološko raznolikega odgovarjanja na vprašanja. Pokazujemo, da naš pristop presega naivne fine nastavitve in dosega konkurenčno uspešnost pri obeh nalogah za večino jezikov. Naša analiza kaže, da lahko X-METRA-ADA izkoristi omejene podatke za hitrejšo prilagajanje.</abstract_sk>
      <abstract_he>דוגמנים רבים שפות, כמו M-BERT ו XLM-R, הרוויחו פופולריות גדולה, בגלל יכולות הלימודים של העברת שפות בצורה אפס. בכל אופן, היכולת הגנרליזציה שלהם עדיין לא תואמת לשפות טיפולוגיות מגוונות ובדרך רמזים שונים. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU).  בעבודה הזו, אנו מציעים X-METRA-ADA, גישה META-TRANSFER-למידה דרך שפתיים למידה התאמה ל NLU. הגישה שלנו מתאימה MAML, גישה מטה-למידה מבוססת על אופטימיזציה, כדי ללמוד להתאים לשפות חדשות. אנחנו מעריכים באופן רחב את המסגרת שלנו על שתי משימות NLU-שינותיות מרובות מאתגרות: דיאלוג מורכב למשימות מרובות ושיבות שונות טיפולוגיות. אנחנו מראים שהגישה שלנו מעלית התאמה נאיבית, מגיעה להופעה תחרותית על שני המשימות לרוב השפות. הניתוח שלנו מראה ש X-METRA-ADA יכול להשתמש במידע מוגבל להסתגלות מהר יותר.</abstract_he>
      <abstract_bo>སྐད་རིགས་ཀྱི་མིག་དཔེ་དཔེར་ན། M-BERT དང་ XLM-R ། མི་མང་ཆེ་ཤོས་ཀྱི་ཆ་རྐྱེན་ལ་བསྐྱེད་ཡོད། ཡིན་ནའང་། ཁོང་ཚོའི་སྤྱིར་བཏང་ན་ཆ་འཕྲིན་ཡིན་པའི་སྐད་རིགས་མི་འདྲ་བ་ཡིན། འཕྲལ་མ་དེ་ལྟ་བུ འོན་ཀྱང་། ང་ཚོས་X-METRA-ADA(translingual MEta-TRAnsfer)བརྡ་སྤྲོད་ཀྱི་ཐབས་ལམ་དེ་གིས་NLU་ལ་སྤྱོད་པའི་གཟུགས་བརྙན་ཞིག་ཡོད། ང་ཚོའི་ཐབས་ལམ་གྱིས་MAML་ལ་སྒྲིག་པ་ཞིག་གཙོ་བྱེད་ཀྱི་ཐབས་ལམ་ལྟར་བྱེད་ཀྱི་ཡོད། ང་ཚོས་རིགས་འདིའི་མཐུན་སྣུམ་གྱི་གནད ང་ཚོས་རང་གི་ཐབས་ལམ་དེ་མིན་ལྡན་མིན་ཐང་པོ་ཞིག་ཡོད་པ་ལས། སྐད་རིགས་ཆེ་ཤོས་ཀྱི་ལས་འགུལ་གྱི་རྒྱལ་སྐྱོར་དང ང་ཚོའི་དབྱེ་ཞིབ་ཀྱིས་X-METRA</abstract_bo>
      </paper>
    <paper id="288">
      <title>Adaptable and Interpretable Neural MemoryOver Symbolic Knowledge<fixed-case>M</fixed-case>emory<fixed-case>O</fixed-case>ver Symbolic Knowledge</title>
      <author><first>Pat</first><last>Verga</last></author>
      <author><first>Haitian</first><last>Sun</last></author>
      <author><first>Livio</first><last>Baldini Soares</last></author>
      <author><first>William</first><last>Cohen</last></author>
      <pages>3678–3691</pages>
      <abstract>Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information : however, augmenting or modifying this <a href="https://en.wikipedia.org/wiki/Information">information</a> requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a fact memory. Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5 % of the parameters. Most interestingly, we demonstrate that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can be modified, without any <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">re-training</a>, by updating the fact memory.<i>any</i> re-training, by updating the fact memory.</abstract>
      <url hash="bb08d5c1">2021.naacl-main.288</url>
      <doi>10.18653/v1/2021.naacl-main.288</doi>
      <bibkey>verga-etal-2021-adaptable</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="290">
      <title>Refining Targeted Syntactic Evaluation of <a href="https://en.wikipedia.org/wiki/Language_model">Language Models</a></title>
      <author><first>Benjamin</first><last>Newman</last></author>
      <author><first>Kai-Siang</first><last>Ang</last></author>
      <author><first>Julia</first><last>Gong</last></author>
      <author><first>John</first><last>Hewitt</last></author>
      <pages>3710–3723</pages>
      <abstract>Targeted syntactic evaluation of subject-verb number agreement in English (TSE) evaluates language models’ syntactic knowledge using hand-crafted minimal pairs of sentences that differ only in the main verb’s conjugation. The method evaluates whether language models rate each grammatical sentence as more likely than its ungrammatical counterpart. We identify two distinct goals for TSE. First, evaluating the systematicity of a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>’s syntactic knowledge : given a sentence, can it conjugate arbitrary verbs correctly? Second, evaluating a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s likely behavior : given a sentence, does the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> concentrate its <a href="https://en.wikipedia.org/wiki/Probability_mass_function">probability mass</a> on correctly conjugated verbs, even if only on a subset of the possible verbs? We argue that current implementations of TSE do not directly capture either of these goals, and propose new <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> to capture each goal separately. Under our metrics, we find that TSE overestimates systematicity of language models, but that <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> score up to 40 % better on verbs that they predict are likely in context.</abstract>
      <url hash="38d5fdda">2021.naacl-main.290</url>
      <doi>10.18653/v1/2021.naacl-main.290</doi>
      <bibkey>newman-etal-2021-refining</bibkey>
      <pwccode url="https://github.com/bnewm0609/refining-tse" additional="false">bnewm0609/refining-tse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blimp">BLiMP</pwcdataset>
    </paper>
    <paper id="293">
      <title>Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack</title>
      <author><first>Liwen</first><last>Wang</last></author>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>3740–3750</pages>
      <abstract>Representation learning is widely used in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> for a vast range of <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. However, <a href="https://en.wikipedia.org/wiki/Representation_(arts)">representations</a> derived from <a href="https://en.wikipedia.org/wiki/Text_corpus">text corpora</a> often reflect <a href="https://en.wikipedia.org/wiki/Bias">social biases</a>. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate representations trained on the main task. We aim to denoise bias information while training on the downstream task, rather than completely remove social bias and pursue static unbiased representations. Experiments show the effectiveness of our method, both on the effect of <a href="https://en.wikipedia.org/wiki/Debiasing">debiasing</a> and the main task performance.</abstract>
      <url hash="17ba7069">2021.naacl-main.293</url>
      <doi>10.18653/v1/2021.naacl-main.293</doi>
      <bibkey>wang-etal-2021-dynamically</bibkey>
      <pwccode url="https://github.com/w-lw/debias_adv" additional="false">w-lw/debias_adv</pwccode>
    </paper>
    <paper id="299">
      <title>On the Impact of <a href="https://en.wikipedia.org/wiki/Random_seed">Random Seeds</a> on the Fairness of Clinical Classifiers</title>
      <author><first>Silvio</first><last>Amir</last></author>
      <author><first>Jan-Willem</first><last>van de Meent</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <pages>3808–3823</pages>
      <abstract>Recent work has shown that fine-tuning large networks is surprisingly sensitive to changes in random seed(s). We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III   the standard dataset in clinical NLP research. Apparent subgroup performance varies substantially for seeds that yield similar overall performance, although there is no evidence of a trade-off between overall and subgroup performance. However, we also find that the small sample sizes inherent to looking at intersections of <a href="https://en.wikipedia.org/wiki/Minority_group">minority groups</a> and somewhat rare conditions limit our ability to accurately estimate disparities. Further, we find that jointly optimizing for high overall performance and low disparities does not yield statistically significant improvements. Our results suggest that fairness work using MIMIC-III should carefully account for variations in apparent differences that may arise from <a href="https://en.wikipedia.org/wiki/Stochastic">stochasticity</a> and small sample sizes.</abstract>
      <url hash="3a511f1a">2021.naacl-main.299</url>
      <doi>10.18653/v1/2021.naacl-main.299</doi>
      <bibkey>amir-etal-2021-impact</bibkey>
    </paper>
    <paper id="300">
      <title>Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures</title>
      <author><first>Caitlin</first><last>Doogan</last></author>
      <author><first>Wray</first><last>Buntine</last></author>
      <pages>3824–3848</pages>
      <abstract>When developing topic models, a critical question that should be asked is : How well will this <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models’ generalizability remains in question. In this paper, we probe the issue of <a href="https://en.wikipedia.org/wiki/Validity_(statistics)">validity</a> in topic model evaluation and assess how informative coherence measures are for specialized collections used in an applied setting. Informed by the literature, we propose four understandings of <a href="https://en.wikipedia.org/wiki/Interpretability">interpretability</a>. We evaluate these using a novel experimental framework reflective of varied applied settings, including human evaluations using <a href="https://en.wikipedia.org/wiki/Open_label">open labeling</a>, typical of <a href="https://en.wikipedia.org/wiki/Applied_science">applied research</a>. These evaluations show that for some specialized collections, standard coherence measures may not inform the most appropriate topic model or the optimal number of topics, and current interpretability performance validation methods are challenged as a means to confirm model quality in the absence of ground truth data.</abstract>
      <url hash="93c50f31">2021.naacl-main.300</url>
      <attachment type="OptionalSupplementaryData" hash="3527f5e9">2021.naacl-main.300.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.300</doi>
      <bibkey>doogan-buntine-2021-topic</bibkey>
    </paper>
    <paper id="304">
      <title>Learning to Learn to be Right for the Right Reasons</title>
      <author><first>Pride</first><last>Kavumba</last></author>
      <author><first>Benjamin</first><last>Heinzerling</last></author>
      <author><first>Ana</first><last>Brassard</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>3890–3898</pages>
      <abstract>Improving model generalization on held-out data is one of the core objectives in common- sense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a> have improved performance on hard instances, they also lead to degraded performance on easy in- stances. Here, we propose to explicitly learn a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> that does well on both the easy test set with superficial cues and the hard test set without superficial cues. Using a meta-learning objective, we learn such a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that improves performance on both the easy test set and the hard test set. By evaluating our models on Choice of Plausible Alternatives (COPA) and Commonsense Explanation, we show that our proposed method leads to improved performance on both the easy test set and the hard test set upon which we observe up to 16.5 percentage points improvement over the baseline.</abstract>
      <url hash="1ef72762">2021.naacl-main.304</url>
      <doi>10.18653/v1/2021.naacl-main.304</doi>
      <bibkey>kavumba-etal-2021-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="305">
      <title>Double Perturbation : On the Robustness of Robustness and Counterfactual Bias Evaluation</title>
      <author><first>Chong</first><last>Zhang</last></author>
      <author><first>Jieyu</first><last>Zhao</last></author>
      <author><first>Huan</first><last>Zhang</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <pages>3899–3916</pages>
      <abstract>Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these <a href="https://en.wikipedia.org/wiki/Evaluation">evaluations</a> robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a double perturbation framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models’ robustness and counterfactual bias in <a href="https://en.wikipedia.org/wiki/English_language">English</a>. (1) For <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a>, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed <a href="https://en.wikipedia.org/wiki/Cyberattack">attack</a> attains high success rates (96.0%-99.8 %) in finding vulnerable examples on both original and robustly trained <a href="https://en.wikipedia.org/wiki/Computer_simulation">CNNs</a> and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/nlp-second-order-attack.</abstract>
      <url hash="0fa9f3a2">2021.naacl-main.305</url>
      <doi>10.18653/v1/2021.naacl-main.305</doi>
      <bibkey>zhang-etal-2021-double</bibkey>
      <pwccode url="https://github.com/chong-z/nlp-second-order-attack" additional="false">chong-z/nlp-second-order-attack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="307">
      <title>Almost Free Semantic Draft for Neural Machine Translation</title>
      <author><first>Xi</first><last>Ai</last></author>
      <author><first>Bin</first><last>Fang</last></author>
      <pages>3931–3941</pages>
      <abstract>Translation quality can be improved by global information from the required target sentence because the <a href="https://en.wikipedia.org/wiki/Code">decoder</a> can understand both past and future information. However, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> needs additional cost to produce and consider such <a href="https://en.wikipedia.org/wiki/Geographic_data_and_information">global information</a>. In this work, to inject global information but also save cost, we present an efficient method to sample and consider a semantic draft as global information from <a href="https://en.wikipedia.org/wiki/Semantic_space">semantic space</a> for decoding with almost free of cost. Unlike other successful adaptations, we do not have to perform an EM-like process that repeatedly samples a possible <a href="https://en.wikipedia.org/wiki/Semantics">semantic</a> from the <a href="https://en.wikipedia.org/wiki/Semantics">semantic space</a>. Empirical experiments show that the presented method can achieve competitive performance in common language pairs with a clear advantage in inference efficiency. We will open all our source code on GitHub.</abstract>
      <url hash="4ae1d23c">2021.naacl-main.307</url>
      <doi>10.18653/v1/2021.naacl-main.307</doi>
      <bibkey>ai-fang-2021-almost</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="308">
      <title>Pruning-then-Expanding Model for <a href="https://en.wikipedia.org/wiki/Domain_adaptation">Domain Adaptation</a> of Neural Machine Translation</title>
      <author><first>Shuhao</first><last>Gu</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Wanying</first><last>Xie</last></author>
      <pages>3942–3952</pages>
      <abstract>Domain Adaptation is widely used in practical applications of <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>, which aims to achieve good performance on both general domain and in-domain data. However, the existing methods for <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> usually suffer from catastrophic forgetting, large domain divergence, and model explosion. To address these three problems, we propose a method of <a href="https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm">divide and conquer</a> which is based on the importance of neurons or parameters for the <a href="https://en.wikipedia.org/wiki/Translation_(biology)">translation model</a>. In this method, we first prune the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and only keep the important neurons or parameters, making them responsible for both general-domain and in-domain translation. Then we further train the pruned model supervised by the original whole model with knowledge distillation. Last we expand the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to the original size and fine-tune the added parameters for the in-domain translation. We conducted experiments on different language pairs and domains and the results show that our method can achieve significant improvements compared with several strong baselines.</abstract>
      <url hash="a6a9ddd6">2021.naacl-main.308</url>
      <doi>10.18653/v1/2021.naacl-main.308</doi>
      <bibkey>gu-etal-2021-pruning</bibkey>
      <pwccode url="https://github.com/ictnlp/PTE-NMT" additional="false">ictnlp/PTE-NMT</pwccode>
    </paper>
    <paper id="310">
      <title>Continual Learning for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a></title>
      <author><first>Yue</first><last>Cao</last></author>
      <author><first>Hao-Ran</first><last>Wei</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>3964–3974</pages>
      <abstract>Neural machine translation (NMT) models are data-driven and require large-scale training corpus. In practical applications, NMT models are usually trained on a general domain corpus and then fine-tuned by continuing training on the in-domain corpus. However, this bears the risk of catastrophic forgetting that the performance on the <a href="https://en.wikipedia.org/wiki/Domain_(software_engineering)">general domain</a> is decreased drastically. In this work, we propose a new continual learning framework for NMT models. We consider a scenario where the training is comprised of multiple stages and propose a dynamic knowledge distillation technique to alleviate the problem of catastrophic forgetting systematically. We also find that the bias exists in the output linear projection when fine-tuning on the in-domain corpus, and propose a bias-correction module to eliminate the bias. We conduct experiments on three representative settings of NMT application. Experimental results show that the proposed method achieves superior performance compared to baseline models in all settings.</abstract>
      <url hash="9ea31e51">2021.naacl-main.310</url>
      <doi>10.18653/v1/2021.naacl-main.310</doi>
      <bibkey>cao-etal-2021-continual</bibkey>
    </paper>
    <paper id="314">
      <title>ER-AE : Differentially Private Text Generation for Authorship Anonymization<fixed-case>ER</fixed-case>-<fixed-case>AE</fixed-case>: Differentially Private Text Generation for Authorship Anonymization</title>
      <author><first>Haohan</first><last>Bo</last></author>
      <author><first>Steven H. H.</first><last>Ding</last></author>
      <author><first>Benjamin C. M.</first><last>Fung</last></author>
      <author><first>Farkhund</first><last>Iqbal</last></author>
      <pages>3997–4007</pages>
      <abstract>Most of privacy protection studies for <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">textual data</a> focus on removing explicit sensitive identifiers. However, <a href="https://en.wikipedia.org/wiki/Writing_style">personal writing style</a>, as a strong indicator of the <a href="https://en.wikipedia.org/wiki/Author">authorship</a>, is often neglected. Recent <a href="https://en.wikipedia.org/wiki/Research">studies</a>, such as SynTF, have shown promising results on privacy-preserving text mining. However, their <a href="https://en.wikipedia.org/wiki/Data_anonymization">anonymization algorithm</a> can only output numeric term vectors which are difficult for the recipients to interpret. We propose a novel text generation model with a two-set exponential mechanism for authorship anonymization. By augmenting the semantic information through a REINFORCE training reward function, the model can generate differentially private text that has a close semantic and similar grammatical structure to the original text while removing personal traits of the writing style. It does not assume any conditioned labels or paralleled text data for training. We evaluate the performance of the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on the real-life peer reviews dataset and the Yelp review dataset. The result suggests that our model outperforms the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> on semantic preservation, <a href="https://en.wikipedia.org/wiki/Obfuscation_(software)">authorship obfuscation</a>, and stylometric transformation.</abstract>
      <url hash="c60aee82">2021.naacl-main.314</url>
      <doi>10.18653/v1/2021.naacl-main.314</doi>
      <bibkey>bo-etal-2021-er</bibkey>
      <pwccode url="https://github.com/McGill-DMaS/AuthorshipAnonymization" additional="true">McGill-DMaS/AuthorshipAnonymization</pwccode>
    </paper>
    <paper id="320">
      <title>A recipe for annotating grounded clarifications</title>
      <author><first>Luciana</first><last>Benotti</last></author>
      <author><first>Patrick</first><last>Blackburn</last></author>
      <pages>4065–4077</pages>
      <abstract>In order to interpret the communicative intents of an utterance, it needs to be grounded in something that is outside of language ; that is, grounded in world modalities. In this paper, we argue that dialogue clarification mechanisms make explicit the process of interpreting the communicative intents of the speaker’s utterances by grounding them in the various modalities in which the dialogue is situated. This paper frames dialogue clarification mechanisms as an understudied research problem and a key missing piece in the giant jigsaw puzzle of <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>. We discuss both the theoretical background and practical challenges posed by this problem and propose a recipe for obtaining grounding annotations. We conclude by highlighting ethical issues that need to be addressed in future work.</abstract>
      <url hash="08bff57c">2021.naacl-main.320</url>
      <doi>10.18653/v1/2021.naacl-main.320</doi>
      <bibkey>benotti-blackburn-2021-recipe</bibkey>
    </paper>
    <paper id="321">
      <title>Grey-box Adversarial Attack And Defence For Sentiment Classification</title>
      <author><first>Ying</first><last>Xu</last></author>
      <author><first>Xu</first><last>Zhong</last></author>
      <author><first>Antonio</first><last>Jimeno Yepes</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <pages>4078–4087</pages>
      <abstract>We introduce a grey-box adversarial attack and defence framework for sentiment classification. We address the issues of differentiability, label preservation and input reconstruction for adversarial attack and defence in one unified framework. Our results show that once trained, the attacking model is capable of generating high-quality adversarial examples substantially faster (one order of magnitude less in time) than state-of-the-art attacking methods. These examples also preserve the original <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiment</a> according to <a href="https://en.wikipedia.org/wiki/Evaluation">human evaluation</a>. Additionally, our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> produces an improved <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> that is robust in defending against multiple adversarial attacking methods. Code is available at : https://github.com/ibm-aur-nlp/adv-def-text-dist.</abstract>
      <url hash="8c3d5e4a">2021.naacl-main.321</url>
      <attachment type="OptionalSupplementaryData" hash="13ab1e9f">2021.naacl-main.321.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.321</doi>
      <bibkey>xu-etal-2021-grey</bibkey>
    </paper>
    <paper id="324">
      <title>Dynabench : Rethinking Benchmarking in NLP<fixed-case>NLP</fixed-case></title>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Max</first><last>Bartolo</last></author>
      <author><first>Yixin</first><last>Nie</last></author>
      <author><first>Divyansh</first><last>Kaushik</last></author>
      <author><first>Atticus</first><last>Geiger</last></author>
      <author><first>Zhengxuan</first><last>Wu</last></author>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <author><first>Grusha</first><last>Prasad</last></author>
      <author><first>Amanpreet</first><last>Singh</last></author>
      <author><first>Pratik</first><last>Ringshia</last></author>
      <author><first>Zhiyi</first><last>Ma</last></author>
      <author><first>Tristan</first><last>Thrush</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Zeerak</first><last>Waseem</last></author>
      <author><first>Pontus</first><last>Stenetorp</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <pages>4110–4124</pages>
      <abstract>We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation : annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community : contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the <a href="https://en.wikipedia.org/wiki/Computing_platform">platform</a>, and address potential objections to dynamic benchmarking as a new standard for the field.</abstract>
      <url hash="a980f5b4">2021.naacl-main.324</url>
      <doi>10.18653/v1/2021.naacl-main.324</doi>
      <bibkey>kiela-etal-2021-dynabench</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="326">
      <title>Predicting Discourse Trees from Transformer-based Neural Summarizers</title>
      <author><first>Wen</first><last>Xiao</last></author>
      <author><first>Patrick</first><last>Huber</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>4139–4152</pages>
      <abstract>Previous work indicates that <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse information</a> benefits <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a>. In this paper, we explore whether this synergy between <a href="https://en.wikipedia.org/wiki/Discourse">discourse</a> and <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a> is bidirectional, by inferring document-level discourse trees from pre-trained neural summarizers. In particular, we generate unlabeled RST-style discourse trees from the self-attention matrices of the transformer model. Experiments across models and datasets reveal that the summarizer learns both, dependency- and constituency-style discourse information, which is typically encoded in a single head, covering long- and short-distance discourse dependencies. Overall, the experimental results suggest that the learned <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse information</a> is general and transferable inter-domain.</abstract>
      <url hash="41f96046">2021.naacl-main.326</url>
      <doi>10.18653/v1/2021.naacl-main.326</doi>
      <bibkey>xiao-etal-2021-predicting</bibkey>
      <pwccode url="https://github.com/Wendy-Xiao/summ_guided_disco_parser" additional="false">Wendy-Xiao/summ_guided_disco_parser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="329">
      <title>Stay Together : A System for Single and Split-antecedent Anaphora Resolution</title>
      <author><first>Juntao</first><last>Yu</last></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last></author>
      <author><first>Silviu</first><last>Paun</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>4174–4184</pages>
      <abstract>The state-of-the-art on basic, single-antecedent anaphora has greatly improved in recent years. Researchers have therefore started to pay more attention to more complex cases of <a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)">anaphora</a> such as split-antecedent anaphora, as in Time-Warner is considering a legal challenge to Telecommunications Inc’s plan to buy half of Showtime Networks Inca move that could lead to all-out war between the two powerful companies. Split-antecedent anaphora is rarer and more complex to resolve than single-antecedent anaphora ; as a result, it is not annotated in many datasets designed to test <a href="https://en.wikipedia.org/wiki/Coreference">coreference</a>, and previous work on resolving this type of <a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)">anaphora</a> was carried out in unrealistic conditions that assume gold mentions and/or gold split-antecedent anaphors are available. These systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics.</abstract>
      <url hash="54e69f35">2021.naacl-main.329</url>
      <doi>10.18653/v1/2021.naacl-main.329</doi>
      <bibkey>yu-etal-2021-stay</bibkey>
      <pwccode url="https://github.com/juntaoy/dali-full-anaphora" additional="false">juntaoy/dali-full-anaphora</pwccode>
    </paper>
    <paper id="331">
      <title>CoRT : Complementary Rankings from Transformers<fixed-case>C</fixed-case>o<fixed-case>RT</fixed-case>: Complementary Rankings from Transformers</title>
      <author><first>Marco</first><last>Wrzalik</last></author>
      <author><first>Dirk</first><last>Krechel</last></author>
      <pages>4194–4204</pages>
      <abstract>Many recent approaches towards neural information retrieval mitigate their <a href="https://en.wikipedia.org/wiki/Computational_cost">computational costs</a> by using a multi-stage ranking pipeline. In the first stage, a number of potentially relevant candidates are retrieved using an efficient retrieval model such as <a href="https://en.wikipedia.org/wiki/BM25">BM25</a>. Although <a href="https://en.wikipedia.org/wiki/BM25">BM25</a> has proven decent performance as a first-stage ranker, <a href="https://en.wikipedia.org/wiki/It_(2017_film)">it</a> tends to miss relevant passages. In this context we propose CoRT, a simple neural first-stage ranking model that leverages contextual representations from pretrained language models such as BERT to complement term-based ranking functions while causing no significant delay at query time. Using the MS MARCO dataset, we show that CoRT significantly increases the candidate recall by complementing <a href="https://en.wikipedia.org/wiki/BM25">BM25</a> with missing candidates. Consequently, we find subsequent re-rankers achieve superior results with less candidates. We further demonstrate that passage retrieval using <a href="https://en.wikipedia.org/wiki/CoRoT">CoRT</a> can be realized with surprisingly low latencies.</abstract>
      <url hash="65b0ff95">2021.naacl-main.331</url>
      <doi>10.18653/v1/2021.naacl-main.331</doi>
      <bibkey>wrzalik-krechel-2021-cort</bibkey>
      <pwccode url="https://github.com/lavis-nlp/CoRT" additional="false">lavis-nlp/CoRT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="332">
      <title>Multi-source Neural Topic Modeling in Multi-view Embedding Spaces</title>
      <author><first>Pankaj</first><last>Gupta</last></author>
      <author><first>Yatin</first><last>Chaudhary</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>4205–4217</pages>
      <abstract>Though <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embed ding spaces : (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (context-insensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and <a href="https://en.wikipedia.org/wiki/Word_processor_(electronic_device)">word embeddings</a> (i.e., WordPool). We then identify one or more relevant source domain(s) and <a href="https://en.wikipedia.org/wiki/Knowledge_transfer">transfer knowledge</a> to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (high-resource) and 5 target (low-resource) corpora.</abstract>
      <url hash="993d4961">2021.naacl-main.332</url>
      <doi>10.18653/v1/2021.naacl-main.332</doi>
      <bibkey>gupta-etal-2021-multi</bibkey>
      <pwccode url="https://github.com/YatinChaudhary/Multi-view-Multi-source-Topic-Modeling" additional="false">YatinChaudhary/Multi-view-Multi-source-Topic-Modeling</pwccode>
    </paper>
    <paper id="334">
      <title>Self-Alignment Pretraining for Biomedical Entity Representations</title>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <author><first>Zaiqiao</first><last>Meng</last></author>
      <author><first>Marco</first><last>Basaldella</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>4228–4238</pages>
      <abstract>Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity linking</a> where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage <a href="https://en.wikipedia.org/wiki/Unified_Modeling_Language">UMLS</a>, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the <a href="https://en.wikipedia.org/wiki/Scientific_method">scientific domain</a>, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust.</abstract>
      <url hash="531398d7">2021.naacl-main.334</url>
      <doi>10.18653/v1/2021.naacl-main.334</doi>
      <bibkey>liu-etal-2021-self</bibkey>
      <pwccode url="https://github.com/cambridgeltl/sapbert" additional="false">cambridgeltl/sapbert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cometa">COMETA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
    </paper>
    <paper id="335">
      <title>TaxoClass : Hierarchical Multi-Label Text Classification Using Only Class Names<fixed-case>T</fixed-case>axo<fixed-case>C</fixed-case>lass: Hierarchical Multi-Label Text Classification Using Only Class Names</title>
      <author><first>Jiaming</first><last>Shen</last></author>
      <author><first>Wenda</first><last>Qiu</last></author>
      <author><first>Yu</first><last>Meng</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>4239–4249</pages>
      <abstract>Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a <a href="https://en.wikipedia.org/wiki/Taxonomic_rank">taxonomic class hierarchy</a>. Most existing HMTC methods train <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its core classes, and then check core classes’ ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document’s core classes and utilizes confident core classes to train a taxonomy-enhanced classifier, and (3) generalizes the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only <a href="https://en.wikipedia.org/wiki/Class_(computer_programming)">class names</a>, outperforming the best previous <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> by 25 %.</abstract>
      <url hash="09e34dac">2021.naacl-main.335</url>
      <doi>10.18653/v1/2021.naacl-main.335</doi>
      <bibkey>shen-etal-2021-taxoclass</bibkey>
    </paper>
    <paper id="336">
      <title>MERMAID : Metaphor Generation with <a href="https://en.wikipedia.org/wiki/Symbol">Symbolism</a> and Discriminative Decoding<fixed-case>MERMAID</fixed-case>: Metaphor Generation with Symbolism and Discriminative Decoding</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Xurui</first><last>Zhang</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>4250–4261</pages>
      <abstract>Generating metaphors is a challenging task as it requires a proper understanding of <a href="https://en.wikipedia.org/wiki/Abstraction">abstract concepts</a>, making connections between unrelated concepts, and deviating from the <a href="https://en.wikipedia.org/wiki/Literal_and_figurative_language">literal meaning</a>. In this paper, we aim to generate a metaphoric sentence given a <a href="https://en.wikipedia.org/wiki/Literal_and_figurative_language">literal expression</a> by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (CITATION) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model fine-tuned on our parallel data to generate high-quality metaphors. Human evaluation on an independent test set of literal statements shows that our best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> generates <a href="https://en.wikipedia.org/wiki/Metaphor">metaphors</a> better than three well-crafted baselines 66 % of the time on average. A task-based evaluation shows that human-written poems enhanced with <a href="https://en.wikipedia.org/wiki/Metaphor">metaphors</a> proposed by our model are preferred 68 % of the time compared to poems without <a href="https://en.wikipedia.org/wiki/Metaphor">metaphors</a>.</abstract>
      <url hash="ea2cd7b4">2021.naacl-main.336</url>
      <doi>10.18653/v1/2021.naacl-main.336</doi>
      <bibkey>chakrabarty-etal-2021-mermaid</bibkey>
      <pwccode url="https://github.com/tuhinjubcse/MetaphorGenNAACL2021" additional="false">tuhinjubcse/MetaphorGenNAACL2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="340">
      <title>Ask what’s missing and what’s useful : Improving Clarification Question Generation using Global Knowledge</title>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last></author>
      <author><first>Sudha</first><last>Rao</last></author>
      <author><first>Michel</first><last>Galley</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>4300–4312</pages>
      <abstract>The ability to generate clarification questions i.e., questions that identify useful missing information in a given context, is important in reducing <a href="https://en.wikipedia.org/wiki/Ambiguity">ambiguity</a>. Humans use previous experience with similar contexts to form a global view and compare it to the given context to ascertain what is missing and what is useful in the context. Inspired by this, we propose a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for clarification question generation where we first identify what is missing by taking a difference between the global and the local view and then train a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to identify what is useful and generate a question about it. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms several <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> as judged by both <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">automatic metrics</a> and <a href="https://en.wikipedia.org/wiki/Human">humans</a>.</abstract>
      <url hash="c019b5d5">2021.naacl-main.340</url>
      <doi>10.18653/v1/2021.naacl-main.340</doi>
      <bibkey>majumder-etal-2021-ask</bibkey>
      <pwccode url="https://github.com/microsoft/clarification-qgen-globalinfo" additional="false">microsoft/clarification-qgen-globalinfo</pwccode>
    </paper>
    <paper id="346">
      <title>I’m Not Mad : Commonsense Implications of Negation and Contradiction<fixed-case>I</fixed-case>’m Not Mad”: Commonsense Implications of Negation and Contradiction</title>
      <author><first>Liwei</first><last>Jiang</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <author><first>Chandra</first><last>Bhagavatula</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>4380–4397</pages>
      <abstract>Natural language inference requires reasoning about <a href="https://en.wikipedia.org/wiki/Contradiction">contradictions</a>, <a href="https://en.wikipedia.org/wiki/Affirmation_and_negation">negations</a>, and their commonsense implications. Given a simple premise (e.g., I’m mad at you), humans can reason about the varying shades of contradictory statements ranging from straightforward negations (I’m not mad at you) to commonsense contradictions (I’m happy). Moreover, these negated or contradictory statements shift the commonsense implications of the original premise in interesting and nontrivial ways. For example, while I’m mad implies I’m unhappy about something, negating the premise does not necessarily negate the corresponding commonsense implications. In this paper, we present the first comprehensive study focusing on commonsense implications of <a href="https://en.wikipedia.org/wiki/Negation">negated statements</a> and <a href="https://en.wikipedia.org/wiki/Contradiction">contradictions</a>. We introduce ANION, a new commonsense knowledge graph with 624 K if-then rules focusing on negated and contradictory events. We then present joint generative and discriminative inference models for this new resource, providing novel empirical insights on how logical negations and commonsense contradictions reshape the commonsense implications of their original premises.</abstract>
      <url hash="31b513c7">2021.naacl-main.346</url>
      <doi>10.18653/v1/2021.naacl-main.346</doi>
      <bibkey>jiang-etal-2021-im</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="347">
      <title>Identifying Medical Self-Disclosure in Online Communities</title>
      <author><first>Mina</first><last>Valizadeh</last></author>
      <author><first>Pardis</first><last>Ranjbar-Noiey</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>4398–4408</pages>
      <abstract>Self-disclosure in online health conversations may offer a host of benefits, including earlier detection and treatment of medical issues that may have otherwise gone unaddressed. However, research analyzing medical self-disclosure in <a href="https://en.wikipedia.org/wiki/Online_community">online communities</a> is limited. We address this shortcoming by introducing a new dataset of health-related posts collected from online social platforms, categorized into three groups (No Self-Disclosure, Possible Self-Disclosure, and Clear Self-Disclosure) with high inter-annotator agreement (_ k_=0.88). We make this <a href="https://en.wikipedia.org/wiki/Data">data</a> available to the research community. We also release a <a href="https://en.wikipedia.org/wiki/Predictive_modelling">predictive model</a> trained on this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> that achieves an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 81.02 %, establishing a strong performance benchmark for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>.</abstract>
      <url hash="1dd9229a">2021.naacl-main.347</url>
      <doi>10.18653/v1/2021.naacl-main.347</doi>
      <bibkey>valizadeh-etal-2021-identifying</bibkey>
    </paper>
    <paper id="348">
      <title>Language in a (Search) Box : Grounding <a href="https://en.wikipedia.org/wiki/Language_acquisition">Language Learning</a> in Real-World Human-Machine Interaction</title>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Ciro</first><last>Greco</last></author>
      <author><first>Jacopo</first><last>Tagliabue</last></author>
      <pages>4409–4415</pages>
      <abstract>We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and <a href="https://en.wikipedia.org/wiki/Web_search_engine">search engines</a> ; in particular, we explore the emergence of semantic generalization from unsupervised dense representations outside of synthetic environments. A grounding domain, a denotation function and a <a href="https://en.wikipedia.org/wiki/Composition_function">composition function</a> are learned from user data only. We show how the resulting <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> for <a href="https://en.wikipedia.org/wiki/Noun_phrase">noun phrases</a> exhibits compositional properties while being fully learnable without any explicit <a href="https://en.wikipedia.org/wiki/Labelling">labelling</a>. We benchmark our grounded semantics on compositionality and zero-shot inference tasks, and we show that it provides better results and better generalizations than SOTA non-grounded models, such as <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a> and BERT.</abstract>
      <url hash="88d66e01">2021.naacl-main.348</url>
      <doi>10.18653/v1/2021.naacl-main.348</doi>
      <bibkey>bianchi-etal-2021-language</bibkey>
    </paper>
    <paper id="349">
      <title>Finding Concept-specific Biases in FormMeaning Associations</title>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <author><first>Søren</first><last>Wichmann</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Damián</first><last>Blasi</last></author>
      <pages>4416–4425</pages>
      <abstract>This work presents an information-theoretic operationalisation of cross-linguistic non-arbitrariness. It is not a new idea that there are small, cross-linguistic associations between the forms and meanings of words. For instance, it has been claimed (Blasi et al., 2016) that the word for tongue is more likely than chance to contain the phone [ l ]. By controlling for the influence of language family and geographic proximity within a very large concept-aligned, cross-lingual lexicon, we extend methods previously used to detect within language non-arbitrariness (Pimentel et al., 2019) to measure cross-linguistic associations. We find that there is a significant effect of non-arbitrariness, but it is unsurprisingly small (less than 0.5 % on average according to our information-theoretic estimate). We also provide a concept-level analysis which shows that a quarter of the <a href="https://en.wikipedia.org/wiki/Concept">concepts</a> considered in our work exhibit a significant level of cross-linguistic non-arbitrariness. In sum, the paper provides new methods to detect cross-linguistic associations at scale, and confirms their effects are minor.</abstract>
      <url hash="e918e632">2021.naacl-main.349</url>
      <doi>10.18653/v1/2021.naacl-main.349</doi>
      <bibkey>pimentel-etal-2021-finding</bibkey>
      <pwccode url="https://github.com/tpimentelms/form-meaning-associations" additional="true">tpimentelms/form-meaning-associations</pwccode>
    </paper>
    <paper id="352">
      <title>Linguistic Complexity Loss in Text-Based Therapy</title>
      <author><first>Jason</first><last>Wei</last></author>
      <author><first>Kelly</first><last>Finn</last></author>
      <author><first>Emma</first><last>Templeton</last></author>
      <author><first>Thalia</first><last>Wheatley</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>4450–4459</pages>
      <abstract>The complexity loss paradox, which posits that individuals suffering from disease exhibit surprisingly predictable behavioral dynamics, has been observed in a variety of both human and animal physiological systems. The recent advent of online text-based therapy presents a new opportunity to analyze the complexity loss paradox in a novel <a href="https://en.wikipedia.org/wiki/Operationalization">operationalization</a> : linguistic complexity loss in text-based therapy conversations. In this paper, we analyze linguistic complexity correlates of <a href="https://en.wikipedia.org/wiki/Mental_health">mental health</a> in the online therapy messages sent between therapists and 7,170 clients who provided 30,437 corresponding survey responses on their anxiety. We found that when clients reported more anxiety, they showed reduced <a href="https://en.wikipedia.org/wiki/Lexical_diversity">lexical diversity</a> as estimated by the moving average type-token ratio. Therapists, on the other hand, used language of higher reading difficulty, <a href="https://en.wikipedia.org/wiki/Syntax">syntactic complexity</a>, and age of acquisition when clients were more anxious. Finally, we found that clients, and to an even greater extent, <a href="https://en.wikipedia.org/wiki/Therapy">therapists</a>, exhibited consistent levels of many linguistic complexity measures. These results demonstrate how linguistic analysis of text-based communication can be leveraged as a marker for <a href="https://en.wikipedia.org/wiki/Anxiety">anxiety</a>, an exciting prospect in a time of both increased online communication and increased mental health issues.</abstract>
      <url hash="05cd8b97">2021.naacl-main.352</url>
      <doi>10.18653/v1/2021.naacl-main.352</doi>
      <bibkey>wei-etal-2021-linguistic</bibkey>
    </paper>
    <paper id="353">
      <title>Ab Antiquo : Neural Proto-language Reconstruction</title>
      <author><first>Carlo</first><last>Meloni</last></author>
      <author><first>Shauli</first><last>Ravfogel</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>4460–4473</pages>
      <abstract>Historical linguists have identified regularities in the process of historic sound change. The comparative method utilizes those regularities to reconstruct proto-words based on observed forms in <a href="https://en.wikipedia.org/wiki/Daughter_language">daughter languages</a>. Can this <a href="https://en.wikipedia.org/wiki/Process_(engineering)">process</a> be efficiently automated? We address the task of proto-word reconstruction, in which the model is exposed to cognates in contemporary daughter languages, and has to predict the proto word in the ancestor language. We provide a novel <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, encompassing over 8,000 comparative entries, and show that neural sequence models outperform conventional methods applied to this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> so far. Error analysis reveals a variability in the ability of neural model to capture different <a href="https://en.wikipedia.org/wiki/Phonological_change">phonological changes</a>, correlating with the <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a> of the changes. Analysis of learned embeddings reveals the models learn phonologically meaningful generalizations, corresponding to well-attested phonological shifts documented by <a href="https://en.wikipedia.org/wiki/Historical_linguistics">historical linguistics</a>.</abstract>
      <url hash="9e8f4679">2021.naacl-main.353</url>
      <doi>10.18653/v1/2021.naacl-main.353</doi>
      <bibkey>meloni-etal-2021-ab</bibkey>
      <pwccode url="https://github.com/shauli-ravfogel/Latin-Reconstruction-NAACL" additional="false">shauli-ravfogel/Latin-Reconstruction-NAACL</pwccode>
    </paper>
    <paper id="361">
      <title>Adapting <a href="https://en.wikipedia.org/wiki/Coreference_resolution">Coreference Resolution</a> for Processing Violent Death Narratives</title>
      <author><first>Ankith</first><last>Uppunda</last></author>
      <author><first>Susan</first><last>Cochran</last></author>
      <author><first>Jacob</first><last>Foster</last></author>
      <author><first>Alina</first><last>Arseniev-Koehler</last></author>
      <author><first>Vickie</first><last>Mays</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>4553–4559</pages>
      <abstract>Coreference resolution is an important compo-nent in analyzing <a href="https://en.wikipedia.org/wiki/Narrative">narrative text</a> from admin-istrative data (e.g., clinical or police sources).However, existing coreference models trainedon general language corpora suffer from poortransferability due to domain gaps, especiallywhen they are applied to gender-inclusive datawith lesbian, gay, bisexual, and transgender(LGBT) individuals. In this paper, we an-alyzed the challenges of coreference resolu-tion in an exemplary form of administrativetext written in English : violent death nar-ratives from the USA’s Centers for DiseaseControl’s (CDC) National Violent Death Re-porting System. We developed a set of dataaugmentation rules to improve model perfor-mance using a probabilistic data programmingframework. Experiments on narratives froman administrative database, as well as existinggender-inclusive coreference datasets, demon-strate the effectiveness of data augmentationin training coreference models that can betterhandle text data about LGBT individuals.</abstract>
      <url hash="00bf1116">2021.naacl-main.361</url>
      <doi>10.18653/v1/2021.naacl-main.361</doi>
      <bibkey>uppunda-etal-2021-adapting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gicoref">GICoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/map">MAP</pwcdataset>
    </paper>
    <paper id="367">
      <title>Does Structure Matter? Encoding Documents for Machine Reading Comprehension</title>
      <author><first>Hui</first><last>Wan</last></author>
      <author><first>Song</first><last>Feng</last></author>
      <author><first>Chulaka</first><last>Gunasekara</last></author>
      <author><first>Siva Sankalp</first><last>Patel</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <author><first>Luis</first><last>Lastras</last></author>
      <pages>4626–4634</pages>
      <abstract>Machine reading comprehension is a challenging <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> especially for querying documents with deep and interconnected contexts. Transformer-based methods have shown advanced performances on this task ; however, most of them still treat documents as a flat sequence of tokens. This work proposes a new Transformer-based method that reads a document as <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">tree slices</a>. It contains two modules for identifying more relevant text passage and the best answer span respectively, which are not only jointly trained but also jointly consulted at inference time. Our evaluation results show that our proposed method outperforms several competitive baseline approaches on two <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> from varied domains.</abstract>
      <url hash="23216f3b">2021.naacl-main.367</url>
      <doi>10.18653/v1/2021.naacl-main.367</doi>
      <bibkey>wan-etal-2021-structure</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial-1">Doc2Dial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial">doc2dial</pwcdataset>
    </paper>
    <paper id="373">
      <title>Constructing Taxonomies from Pretrained Language Models</title>
      <author><first>Catherine</first><last>Chen</last></author>
      <author><first>Kevin</first><last>Lin</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>4687–4700</pages>
      <abstract>We present a <a href="https://en.wikipedia.org/wiki/Methodology">method</a> for constructing <a href="https://en.wikipedia.org/wiki/Taxonomy_(biology)">taxonomic trees</a> (e.g., WordNet) using pretrained language models. Our approach is composed of two <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a>, one that predicts parenthood relations and another that reconciles those pairwise predictions into <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">trees</a>. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph optimization problem</a> and outputs the maximum spanning tree of this <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>. We train our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on subtrees sampled from <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a>, and test on nonoverlapping WordNet subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of <a href="https://en.wikipedia.org/wiki/WordNet">English WordNet</a>, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves 66.7 ancestor F1, a 20.0 % relative increase over the previous best published result on this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. In addition, we convert the original English dataset into nine other languages using Open Multilingual WordNet and extend our results across these languages.</abstract>
      <url hash="e013e45b">2021.naacl-main.373</url>
      <doi>10.18653/v1/2021.naacl-main.373</doi>
      <bibkey>chen-etal-2021-constructing</bibkey>
    </paper>
    <paper id="378">
      <title>Adapting <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks<fixed-case>BERT</fixed-case> for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks</title>
      <author><first>Zixuan</first><last>Ke</last></author>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <pages>4746–4755</pages>
      <abstract>This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues : (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks so that they are not forgotten. This paper proposes a novel capsule network based model called B-CL to address these issues. B-CL markedly improves the ASC performance on both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of B-CL is demonstrated through extensive experiments.</abstract>
      <url hash="f17af448">2021.naacl-main.378</url>
      <doi>10.18653/v1/2021.naacl-main.378</doi>
      <bibkey>ke-etal-2021-adapting</bibkey>
      <pwccode url="https://github.com/zixuanke/pycontinual" additional="false">zixuanke/pycontinual</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/asc-til-19-tasks">ASC (TIL, 19 tasks)</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/20newsgroup-10-tasks">20Newsgroup (10 tasks)</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dsc-10-tasks">DSC (10 tasks)</pwcdataset>
    </paper>
    <paper id="381">
      <title>Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization</title>
      <author><first>Yichen</first><last>Jiang</last></author>
      <author><first>Asli</first><last>Celikyilmaz</last></author>
      <author><first>Paul</first><last>Smolensky</last></author>
      <author><first>Paul</first><last>Soulos</last></author>
      <author><first>Sudha</first><last>Rao</last></author>
      <author><first>Hamid</first><last>Palangi</last></author>
      <author><first>Roland</first><last>Fernandez</last></author>
      <author><first>Caitlin</first><last>Smith</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>4780–4793</pages>
      <abstract>Abstractive summarization, the task of generating a concise summary of input documents, requires : (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-Transformer (Schlag et al., 2019), an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-Transformer outperforms the Transformer and the original TP-Transformer significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and the performance gain by information specificity of the role vectors and improved syntactic interpretability in the TPR layer outputs. (Code and models are available at https://github.com/jiangycTarheel/TPT-Summ)</abstract>
      <url hash="f06a0ec4">2021.naacl-main.381</url>
      <doi>10.18653/v1/2021.naacl-main.381</doi>
      <bibkey>jiang-etal-2021-enriching</bibkey>
      <pwccode url="https://github.com/jiangycTarheel/TPT-Summ" additional="false">jiangycTarheel/TPT-Summ</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="383">
      <title>Understanding Factuality in Abstractive Summarization with FRANK : A Benchmark for Factuality Metrics<fixed-case>FRANK</fixed-case>: A Benchmark for Factuality Metrics</title>
      <author><first>Artidoro</first><last>Pagnoni</last></author>
      <author><first>Vidhisha</first><last>Balachandran</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>4812–4829</pages>
      <abstract>Modern <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization models</a> generate highly fluent but often factually unreliable outputs. This motivated a surge of <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> attempting to measure the <a href="https://en.wikipedia.org/wiki/Fact">factuality</a> of automatically generated summaries. Due to the lack of common benchmarks, these <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> can not be compared. Moreover, all these methods treat <a href="https://en.wikipedia.org/wiki/Factuality">factuality</a> as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN / DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.</abstract>
      <url hash="0cb57592">2021.naacl-main.383</url>
      <doi>10.18653/v1/2021.naacl-main.383</doi>
      <bibkey>pagnoni-etal-2021-understanding</bibkey>
      <pwccode url="https://github.com/artidoro/frank" additional="false">artidoro/frank</pwccode>
    </paper>
    <paper id="384">
      <title>GSum : A General Framework for Guided Neural Abstractive Summarization<fixed-case>GS</fixed-case>um: A General Framework for Guided Neural Abstractive Summarization</title>
      <author><first>Zi-Yi</first><last>Dou</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Hiroaki</first><last>Hayashi</last></author>
      <author><first>Zhengbao</first><last>Jiang</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>4830–4842</pages>
      <abstract>Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of <a href="https://en.wikipedia.org/wiki/Guidance">guidance</a> to control the output and increase <a href="https://en.wikipedia.org/wiki/Faithfulness">faithfulness</a>, it is not clear how these <a href="https://en.wikipedia.org/wiki/Strategy">strategies</a> compare and contrast to each other. In this paper, we propose a general and extensible guided summarization framework (GSum) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of <a href="https://en.wikipedia.org/wiki/Guidance">guidance</a> generate qualitatively different summaries, lending a degree of <a href="https://en.wikipedia.org/wiki/Controllability">controllability</a> to the learned models.<b>GSum</b>) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models.</abstract>
      <url hash="8bfb84c8">2021.naacl-main.384</url>
      <doi>10.18653/v1/2021.naacl-main.384</doi>
      <bibkey>dou-etal-2021-gsum</bibkey>
      <pwccode url="https://github.com/neulab/guided_summarization" additional="false">neulab/guided_summarization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="386">
      <title>TuringAdvice : A Generative and Dynamic Evaluation of Language Use<fixed-case>T</fixed-case>uring<fixed-case>A</fixed-case>dvice: A Generative and Dynamic Evaluation of Language Use</title>
      <author><first>Rowan</first><last>Zellers</last></author>
      <author><first>Ari</first><last>Holtzman</last></author>
      <author><first>Elizabeth</first><last>Clark</last></author>
      <author><first>Lianhui</first><last>Qin</last></author>
      <author><first>Ali</first><last>Farhadi</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>4856–4880</pages>
      <abstract>We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding : our ability to use <a href="https://en.wikipedia.org/wiki/Language">language</a> to resolve open-ended situations by communicating with each other. Empirical results show that today’s <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best <a href="https://en.wikipedia.org/wiki/Scientific_modelling">model</a>, T5, writes advice that is at least as helpful as human-written advice in only 14 % of cases ; a much larger non-finetunable GPT3 model does even worse at 4 %. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress.</abstract>
      <url hash="1b9451ed">2021.naacl-main.386</url>
      <doi>10.18653/v1/2021.naacl-main.386</doi>
      <bibkey>zellers-etal-2021-turingadvice</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hellaswag">HellaSwag</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="390">
      <title>Identifying inherent disagreement in natural language inference</title>
      <author><first>Xinliang Frederick</first><last>Zhang</last></author>
      <author><first>Marie-Catherine</first><last>de Marneffe</last></author>
      <pages>4908–4915</pages>
      <abstract>Natural language inference (NLI) is the task of determining whether a piece of text is entailed, contradicted by or unrelated to another piece of text. In this paper, we investigate how to tease systematic inferences (i.e., items for which people agree on the NLI label) apart from disagreement items (i.e., items which lead to different annotations), which most prior work has overlooked. To distinguish systematic inferences from disagreement items, we propose Artificial Annotators (AAs) to simulate the uncertainty in the annotation process by capturing the modes in annotations. Results on the CommitmentBank, a corpus of naturally occurring discourses in English, confirm that our approach performs statistically significantly better than all baselines. We further show that <a href="https://en.wikipedia.org/wiki/Adult_learner">AAs</a> learn <a href="https://en.wikipedia.org/wiki/Pattern_recognition">linguistic patterns</a> and context-dependent reasoning.</abstract>
      <url hash="7ddaf53b">2021.naacl-main.390</url>
      <doi>10.18653/v1/2021.naacl-main.390</doi>
      <bibkey>zhang-de-marneffe-2021-identifying</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="391">
      <title>Modeling Human Mental States with an Entity-based Narrative Graph</title>
      <author><first>I-Ta</first><last>Lee</last></author>
      <author><first>Maria Leonor</first><last>Pacheco</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>4916–4926</pages>
      <abstract>Understanding <a href="https://en.wikipedia.org/wiki/Narrative">narrative text</a> requires capturing characters’ motivations, goals, and <a href="https://en.wikipedia.org/wiki/Mental_state">mental states</a>. This paper proposes an Entity-based Narrative Graph (ENG) to model the internal- states of characters in a story. We explicitly model entities, their interactions and the context in which they appear, and learn rich representations for them. We experiment with different task-adaptive pre-training objectives, in-domain training, and symbolic inference to capture dependencies between different decisions in the output space. We evaluate our model on two narrative understanding tasks : predicting character mental states, and desire fulfillment, and conduct a <a href="https://en.wikipedia.org/wiki/Qualitative_property">qualitative analysis</a>.</abstract>
      <url hash="e6036ba8">2021.naacl-main.391</url>
      <doi>10.18653/v1/2021.naacl-main.391</doi>
      <bibkey>lee-etal-2021-modeling</bibkey>
      <pwccode url="https://github.com/doug919/entity_based_narrative_graph" additional="false">doug919/entity_based_narrative_graph</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/desiredb">DesireDB</pwcdataset>
    </paper>
    <paper id="393">
      <title>Hurdles to Progress in Long-form Question Answering</title>
      <author><first>Kalpesh</first><last>Krishna</last></author>
      <author><first>Aurko</first><last>Roy</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>4940–4957</pages>
      <abstract>The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends : (1) our system’s generated answers are not actually grounded in the documents that it retrieves ; (2) ELI5 contains significant train / validation overlap, as at least 81 % of ELI5 validation questions occur in paraphrased form in the training set ; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed ; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.</abstract>
      <url hash="4cc7c7fb">2021.naacl-main.393</url>
      <attachment type="OptionalSupplementaryData" hash="031c8096">2021.naacl-main.393.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.393</doi>
      <bibkey>krishna-etal-2021-hurdles</bibkey>
      <pwccode url="https://github.com/martiansideofthemoon/hurdles-longform-qa" additional="false">martiansideofthemoon/hurdles-longform-qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pg-19">PG-19</pwcdataset>
    </paper>
    <paper id="397">
      <title>Attention Head Masking for Inference Time Content Selection in Abstractive Summarization</title>
      <author><first>Shuyang</first><last>Cao</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>5008–5016</pages>
      <abstract>How can we effectively inform content selection in Transformer-based abstractive summarization models? In this work, we present a simple-yet-effective attention head masking technique, which is applied on encoder-decoder attentions to pinpoint salient content at inference time. Using attention head masking, we are able to reveal the relation between encoder-decoder attentions and content selection behaviors of summarization models. We then demonstrate its effectiveness on three document summarization datasets based on both in-domain and cross-domain settings. Importantly, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> outperform prior state-of-the-art <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on CNN / Daily Mail and New York Times datasets. Moreover, our inference-time masking technique is also data-efficient, requiring only 20 % of the training samples to outperform BART fine-tuned on the full CNN / DailyMail dataset.</abstract>
      <url hash="37b9dbe7">2021.naacl-main.397</url>
      <doi>10.18653/v1/2021.naacl-main.397</doi>
      <bibkey>cao-wang-2021-attention</bibkey>
    </paper>
    <paper id="398">
      <title>Factual Probing Is [ MASK ] : <a href="https://en.wikipedia.org/wiki/Learning">Learning</a> vs. Learning to Recall<fixed-case>MASK</fixed-case>]: Learning vs. Learning to Recall</title>
      <author><first>Zexuan</first><last>Zhong</last></author>
      <author><first>Dan</first><last>Friedman</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>5017–5033</pages>
      <abstract>Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pre-trained language model by expressing them as cloze-style prompts and interpret the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>’s prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OptiPrompt, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4 % of facts in the LAMA benchmark. Second, we raise a more important question : Can we really interpret these probing results as a <a href="https://en.wikipedia.org/wiki/Upper_and_lower_bounds">lower bound</a>? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle <a href="https://en.wikipedia.org/wiki/Learning">learning</a> from learning to recall, providing a more detailed picture of what different prompts can reveal about pre-trained language models.</abstract>
      <url hash="55e75ce4">2021.naacl-main.398</url>
      <revision id="1" href="2021.naacl-main.398v1" hash="bf4b6ce3" />
      <revision id="2" href="2021.naacl-main.398v2" hash="55e75ce4" date="2021-05-25">Updated a citation.</revision>
      <doi>10.18653/v1/2021.naacl-main.398</doi>
      <bibkey>zhong-etal-2021-factual</bibkey>
      <pwccode url="https://github.com/princeton-nlp/OptiPrompt" additional="true">princeton-nlp/OptiPrompt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="400">
      <title>Contextualized Perturbation for Textual Adversarial Attack</title>
      <author><first>Dianqi</first><last>Li</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Liqun</first><last>Chen</last></author>
      <author><first>Chris</first><last>Brockett</last></author>
      <author><first>Ming-Ting</first><last>Sun</last></author>
      <author><first>Bill</first><last>Dolan</last></author>
      <pages>5053–5069</pages>
      <abstract>Adversarial examples expose the vulnerabilities of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP) models</a>, and can be used to evaluate and improve their <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a>. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, <a href="https://en.wikipedia.org/wiki/Fluency">fluency</a> and <a href="https://en.wikipedia.org/wiki/Grammaticality">grammaticality</a>.</abstract>
      <url hash="5aa24760">2021.naacl-main.400</url>
      <doi>10.18653/v1/2021.naacl-main.400</doi>
      <bibkey>li-etal-2021-contextualized</bibkey>
      <pwccode url="https://github.com/cookielee77/CLARE" additional="false">cookielee77/CLARE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="402">
      <title>Evaluating the Values of Sources in <a href="https://en.wikipedia.org/wiki/Transfer_learning">Transfer Learning</a></title>
      <author><first>Md Rizwan</first><last>Parvez</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>5084–5116</pages>
      <abstract>Transfer learning that adapts a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained on data-rich sources to low-resource targets has been widely applied in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing (NLP)</a>. However, when training a transfer model over multiple sources, not every source is equally useful for the target. To better transfer a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>, it is essential to understand the values of the sources. In this paper, we develop, an efficient source valuation framework for quantifying the usefulness of the sources (e.g.,) in <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> based on the Shapley value method. Experiments and comprehensive analyses on both cross-domain and cross-lingual transfers demonstrate that our framework is not only effective in choosing useful transfer sources but also the source values match the intuitive source-target similarity.</abstract>
      <url hash="8bf9997f">2021.naacl-main.402</url>
      <doi>10.18653/v1/2021.naacl-main.402</doi>
      <bibkey>parvez-chang-2021-evaluating</bibkey>
      <pwccode url="https://github.com/rizwan09/NLPDV" additional="false">rizwan09/NLPDV</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="405">
      <title>Limitations of <a href="https://en.wikipedia.org/wiki/Autoregressive_model">Autoregressive Models</a> and Their Alternatives</title>
      <author><first>Chu-Cheng</first><last>Lin</last></author>
      <author><first>Aaron</first><last>Jaech</last></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Matthew R.</first><last>Gormley</last></author>
      <author><first>Jason</first><last>Eisner</last></author>
      <pages>5147–5173</pages>
      <abstract>Standard autoregressive language models perform only <a href="https://en.wikipedia.org/wiki/Time_complexity">polynomial-time computation</a> to compute the probability of the next symbol. While this is attractive, it means they can not model <a href="https://en.wikipedia.org/wiki/Probability_distribution">distributions</a> whose next-symbol probability is hard to compute. Indeed, they can not even model them well enough to solve associated easy decision problems for which an engineer might want to consult a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>. These limitations apply no matter how much computation and data are used to train the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, unless the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is given access to <a href="https://en.wikipedia.org/wiki/Oracle_machine">oracle parameters</a> that grow superpolynomially in sequence length. Thus, simply training larger autoregressive language models is not a panacea for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations.<i>hard</i> to compute. Indeed, they cannot even model them well enough to solve associated <i>easy</i> decision problems for which an engineer might want to consult a language model. These limitations apply no matter how much computation and data are used to train the model, unless the model is given access to oracle parameters that grow <i>superpolynomially</i> in sequence length. Thus, simply training larger autoregressive language models is not a panacea for NLP. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations.</abstract>
      <url hash="49e12cc6">2021.naacl-main.405</url>
      <revision id="1" href="2021.naacl-main.405v1" hash="93f6fe2b" />
      <revision id="2" href="2021.naacl-main.405v2" hash="49e12cc6" date="2021-05-31">Various corrections through the paper</revision>
      <doi>10.18653/v1/2021.naacl-main.405</doi>
      <bibkey>lin-etal-2021-limitations</bibkey>
    </paper>
    <paper id="406">
      <title>On the Transformer Growth for Progressive BERT Training<fixed-case>BERT</fixed-case> Training</title>
      <author><first>Xiaotao</first><last>Gu</last></author>
      <author><first>Liyuan</first><last>Liu</last></author>
      <author><first>Hongkun</first><last>Yu</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>5174–5180</pages>
      <abstract>As the excessive pre-training cost arouses the need to improve <a href="https://en.wikipedia.org/wiki/Efficiency">efficiency</a>, considerable efforts have been made to train BERT progressivelystart from an inferior but low-cost model and gradually increase the <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">computational complexity</a>. Our objective is to help advance the understanding of such Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture selection, Transformer growth also favors <a href="https://en.wikipedia.org/wiki/Scaling_(geometry)">compound scaling</a>. Specifically, while existing methods only conduct network growth in a single dimension, we observe that it is beneficial to use compound growth operators and balance multiple dimensions (e.g., depth, width, and input length of the model). Moreover, we explore alternative growth operators in each dimension via controlled comparison to give practical guidance for operator selection. In light of our analyses, the proposed method CompoundGrow speeds up <a href="https://en.wikipedia.org/wiki/Brain-derived_neurotrophic_factor">BERT pre-training</a> by 73.6 % and 82.2 % for the base and large models respectively while achieving comparable performances.</abstract>
      <url hash="4e971e21">2021.naacl-main.406</url>
      <doi>10.18653/v1/2021.naacl-main.406</doi>
      <bibkey>gu-etal-2021-transformer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="408">
      <title>ReadTwice : Reading Very Large Documents with Memories<fixed-case>R</fixed-case>ead<fixed-case>T</fixed-case>wice: Reading Very Large Documents with Memories</title>
      <author><first>Yury</first><last>Zemlyanskiy</last></author>
      <author><first>Joshua</first><last>Ainslie</last></author>
      <author><first>Michiel</first><last>de Jong</last></author>
      <author><first>Philip</first><last>Pham</last></author>
      <author><first>Ilya</first><last>Eckstein</last></author>
      <author><first>Fei</first><last>Sha</last></author>
      <pages>5189–5195</pages>
      <abstract>Knowledge-intensive tasks such as <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> often require assimilating information from different sections of large inputs such as <a href="https://en.wikipedia.org/wiki/Book">books</a> or article collections. We propose ReadTwice, a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task, with questions about entire books.</abstract>
      <url hash="7995febf">2021.naacl-main.408</url>
      <doi>10.18653/v1/2021.naacl-main.408</doi>
      <bibkey>zemlyanskiy-etal-2021-readtwice</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="410">
      <title>Learning How to Ask : Querying LMs with Mixtures of Soft Prompts<fixed-case>LM</fixed-case>s with Mixtures of Soft Prompts</title>
      <author><first>Guanghui</first><last>Qin</last></author>
      <author><first>Jason</first><last>Eisner</last></author>
      <pages>5203–5212</pages>
      <abstract>Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-the-blank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> retain factual knowledge from their training corpora that can be extracted by asking them to fill in the blank in a sentential prompt. However, where does this prompt come from? We explore the idea of learning <a href="https://en.wikipedia.org/wiki/Command-line_interface">prompts</a> by gradient descenteither fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of soft words, i.e., <a href="https://en.wikipedia.org/wiki/Continuous_or_discrete_variable">continuous vectors</a> that are not necessarily word type embeddings from the <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>. Furthermore, for each <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, we optimize a mixture of <a href="https://en.wikipedia.org/wiki/Command_(computing)">prompts</a>, learning which <a href="https://en.wikipedia.org/wiki/Command_(computing)">prompts</a> are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> was previously underestimated. Moreover, this knowledge is cheap to elicit : random initialization is nearly as good as informed initialization.</abstract>
      <url hash="eb8f3e6e">2021.naacl-main.410</url>
      <attachment type="OptionalSupplementaryCode" hash="83db5959">2021.naacl-main.410.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="ab2eafc7">2021.naacl-main.410.OptionalSupplementaryData.zip</attachment>
      <award>Best Short Paper</award>
      <doi>10.18653/v1/2021.naacl-main.410</doi>
      <bibkey>qin-eisner-2021-learning</bibkey>
      <pwccode url="https://github.com/hiaoxui/soft-prompts" additional="true">hiaoxui/soft-prompts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="413">
      <title>SpanPredict : Extraction of Predictive Document Spans with Neural Attention<fixed-case>S</fixed-case>pan<fixed-case>P</fixed-case>redict: Extraction of Predictive Document Spans with Neural Attention</title>
      <author><first>Vivek</first><last>Subramanian</last></author>
      <author><first>Matthew</first><last>Engelhard</last></author>
      <author><first>Sam</first><last>Berchuck</last></author>
      <author><first>Liqun</first><last>Chen</last></author>
      <author><first>Ricardo</first><last>Henao</last></author>
      <author><first>Lawrence</first><last>Carin</last></author>
      <pages>5234–5258</pages>
      <abstract>In many <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing applications</a>, identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses, for example, identifying predictive content in <a href="https://en.wikipedia.org/wiki/Medical_record">clinical notes</a> not only enhances interpretability, but also allows unknown, descriptive (i.e., text-based) risk factors to be identified. We here formalize this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> as predictive extraction and address it using a simple <a href="https://en.wikipedia.org/wiki/Mechanism_design">mechanism</a> based on <a href="https://en.wikipedia.org/wiki/Attentional_control">linear attention</a>. Our method preserves differentiability, allowing scalable inference via <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a>. Further, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> decomposes predictions into a sum of contributions of distinct text spans. Importantly, we require only <a href="https://en.wikipedia.org/wiki/Document_classification">document labels</a>, not ground-truth spans. Results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> identifies semantically-cohesive spans and assigns them scores that agree with human ratings, while preserving <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> performance.</abstract>
      <url hash="13a4d027">2021.naacl-main.413</url>
      <attachment type="OptionalSupplementaryCode" hash="65e3c865">2021.naacl-main.413.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.413</doi>
      <bibkey>subramanian-etal-2021-spanpredict</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="416">
      <title>Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation</title>
      <author><first>Yasuhide</first><last>Miura</last></author>
      <author><first>Yuhao</first><last>Zhang</last></author>
      <author><first>Emily</first><last>Tsai</last></author>
      <author><first>Curtis</first><last>Langlotz</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>5288–5304</pages>
      <abstract>Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible <a href="https://en.wikipedia.org/wiki/Medical_error">medical errors</a>. However, existing report generation systems, despite achieving high performances on natural language generation metrics such as CIDEr or <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>, still suffer from incomplete and inconsistent generations. Here we introduce two new simple rewards to encourage the generation of factually complete and consistent radiology reports : one that encourages the system to generate radiology domain entities consistent with the reference, and one that uses natural language inference to encourage these entities to be described in inferentially consistent ways. We combine these with the novel use of an existing semantic equivalence metric (BERTScore). We further propose a report generation system that optimizes these <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">rewards</a> via <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>. On two open radiology report datasets, our <a href="https://en.wikipedia.org/wiki/System">system</a> substantially improved the F1 score of a clinical information extraction performance by +22.1 (Delta +63.9 %). We further show via a <a href="https://en.wikipedia.org/wiki/Evaluation">human evaluation</a> and a <a href="https://en.wikipedia.org/wiki/Qualitative_property">qualitative analysis</a> that our <a href="https://en.wikipedia.org/wiki/System">system</a> leads to generations that are more factually complete and consistent compared to the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>.</abstract>
      <url hash="e4e72a3e">2021.naacl-main.416</url>
      <doi>10.18653/v1/2021.naacl-main.416</doi>
      <bibkey>miura-etal-2021-improving</bibkey>
      <pwccode url="https://github.com/ysmiura/ifcc" additional="true">ysmiura/ifcc</pwccode>
    </paper>
    <paper id="418">
      <title>MIMOQA : Multimodal Input Multimodal Output Question Answering<fixed-case>MIMOQA</fixed-case>: Multimodal Input Multimodal Output Question Answering</title>
      <author><first>Hrituraj</first><last>Singh</last></author>
      <author><first>Anshul</first><last>Nasery</last></author>
      <author><first>Denil</first><last>Mehta</last></author>
      <author><first>Aishwarya</first><last>Agarwal</last></author>
      <author><first>Jatin</first><last>Lamba</last></author>
      <author><first>Balaji Vasan</first><last>Srinivasan</last></author>
      <pages>5317–5332</pages>
      <abstract>Multimodal research has picked up significantly in the space of <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> with the task being extended to visual question answering, charts question answering as well as multimodal input question answering. However, all these explorations produce a unimodal textual output as the answer. In this paper, we propose a novel task-MIMOQA-Multimodal Input Multimodal Output Question Answering in which the output is also multimodal. Through human experiments, we empirically show that such multimodal outputs provide better cognitive understanding of the answers. We also propose a novel multimodal question-answering framework, MExBERT, that incorporates a joint textual and visual attention towards producing such a multimodal output. Our method relies on a novel multimodal dataset curated for this problem from publicly available unimodal datasets. We show the superior performance of MExBERT against strong baselines on both the automatic as well as human metrics.</abstract>
      <url hash="9da2260b">2021.naacl-main.418</url>
      <doi>10.18653/v1/2021.naacl-main.418</doi>
      <bibkey>singh-etal-2021-mimoqa</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/manymodalqa">ManyModalQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tvqa">TVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="420">
      <title>Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions</title>
      <author><first>Liunian Harold</first><last>Li</last></author>
      <author><first>Haoxuan</first><last>You</last></author>
      <author><first>Zhecan</first><last>Wang</last></author>
      <author><first>Alireza</first><last>Zareian</last></author>
      <author><first>Shih-Fu</first><last>Chang</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>5339–5350</pages>
      <abstract>Pre-trained contextual vision-and-language (V&amp;L) models have achieved impressive performance on various <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmarks</a>. However, existing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> require a large amount of parallel image-caption data for <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">pre-training</a>. Such <a href="https://en.wikipedia.org/wiki/Data">data</a> are costly to collect and require cumbersome curation. Inspired by unsupervised machine translation, we investigate if a strong V&amp;L representation model can be learned through unsupervised pre-training without image-caption corpora. In particular, we propose to conduct mask-and-predict pre-training on text-only and image-only corpora and introduce the object tags detected by an object recognition model as anchor points to bridge two modalities. We find that such a simple approach achieves performance close to a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> pre-trained with <a href="https://en.wikipedia.org/wiki/Data_alignment">aligned data</a>, on four English V&amp;L benchmarks. Our work challenges the widely held notion that aligned data is necessary for V&amp;L pre-training, while significantly reducing the amount of supervision needed for V&amp;L models.</abstract>
      <url hash="43881f38">2021.naacl-main.420</url>
      <attachment type="OptionalSupplementaryData" hash="1fa89178">2021.naacl-main.420.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.420</doi>
      <bibkey>li-etal-2021-unsupervised</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/refcoco">RefCOCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="421">
      <title>Multitasking Inhibits Semantic Drift</title>
      <author><first>Athul Paul</first><last>Jacob</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Jacob</first><last>Andreas</last></author>
      <pages>5351–5366</pages>
      <abstract>When <a href="https://en.wikipedia.org/wiki/Intelligent_agent">intelligent agents</a> communicate to accomplish shared goals, how do these goals shape the agents’ language? We study the dynamics of learning in latent language policies (LLPs), in which instructor agents generate natural-language subgoal descriptions and executor agents map these descriptions to low-level actions. LLPs can solve challenging long-horizon reinforcement learning problems and provide a rich model for studying task-oriented language use. But previous work has found that LLP training is prone to <a href="https://en.wikipedia.org/wiki/Semantic_drift">semantic drift</a> (use of messages in ways inconsistent with their original natural language meanings). Here, we demonstrate theoretically and empirically that multitask training is an effective counter to this problem : we prove that multitask training eliminates <a href="https://en.wikipedia.org/wiki/Semantic_drift">semantic drift</a> in a well-studied family of signaling games, and show that multitask training of neural LLPs in a complex strategy game reduces drift and while improving sample efficiency.</abstract>
      <url hash="6428e4ba">2021.naacl-main.421</url>
      <doi>10.18653/v1/2021.naacl-main.421</doi>
      <bibkey>jacob-etal-2021-multitasking</bibkey>
    </paper>
    <paper id="429">
      <title>Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction</title>
      <author><first>Zhenghao</first><last>Liu</last></author>
      <author><first>Xiaoyuan</first><last>Yi</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Liner</first><last>Yang</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <pages>5441–5452</pages>
      <abstract>Grammatical Error Correction (GEC) aims to correct writing errors and help <a href="https://en.wikipedia.org/wiki/Language_acquisition">language learners</a> improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a>, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/thunlp/VERNet.</abstract>
      <url hash="4b3cb79b">2021.naacl-main.429</url>
      <doi>10.18653/v1/2021.naacl-main.429</doi>
      <bibkey>liu-etal-2021-neural</bibkey>
      <pwccode url="https://github.com/thunlp/VERNet" additional="false">thunlp/VERNet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
    </paper>
    <paper id="434">
      <title>Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning</title>
      <author><first>Jason</first><last>Wei</last></author>
      <author><first>Chengyu</first><last>Huang</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <author><first>Yu</first><last>Cheng</last></author>
      <author><first>Shiqi</first><last>Xu</last></author>
      <pages>5493–5500</pages>
      <abstract>Few-shot text classification is a fundamental NLP task in which a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentationa technique particularly suitable for training with limited datafor this few-shot, highly-multiclass text classification setting. On four diverse text classification tasks, we find that common <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation techniques</a> can improve the performance of triplet networks by up to 3.0 % on average. To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a two-stage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation trains faster, improves performance, and remains robust to high amounts of noising from augmentation.</abstract>
      <url hash="af60bf89">2021.naacl-main.434</url>
      <doi>10.18653/v1/2021.naacl-main.434</doi>
      <bibkey>wei-etal-2021-shot</bibkey>
      <pwccode url="https://github.com/jasonwei20/triplet-loss" additional="false">jasonwei20/triplet-loss</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="438">
      <title>User-Generated Text Corpus for Evaluating Japanese Morphological Analysis and Lexical Normalization<fixed-case>J</fixed-case>apanese Morphological Analysis and Lexical Normalization</title>
      <author><first>Shohei</first><last>Higashiyama</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>5532–5541</pages>
      <abstract>Morphological analysis (MA) and lexical normalization (LN) are both important tasks for Japanese user-generated text (UGT). To evaluate and compare different MA / LN systems, we have constructed a publicly available Japanese UGT corpus. Our <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> comprises 929 sentences annotated with morphological and normalization information, along with category information we classified for frequent UGT-specific phenomena. Experiments on the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> demonstrated the low performance of existing MA / LN methods for non-general words and non-standard forms, indicating that the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> would be a challenging benchmark for further research on UGT.</abstract>
      <url hash="f1141938">2021.naacl-main.438</url>
      <doi>10.18653/v1/2021.naacl-main.438</doi>
      <bibkey>higashiyama-etal-2021-user</bibkey>
      <pwccode url="https://github.com/shigashiyama/jlexnorm" additional="false">shigashiyama/jlexnorm</pwccode>
    </paper>
    <paper id="442">
      <title>Contextualized and Generalized Sentence Representations by Contrastive Self-Supervised Learning : A Case Study on Discourse Relation Analysis</title>
      <author><first>Hirokazu</first><last>Kiyomaru</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>5578–5584</pages>
      <abstract>We propose a method to learn contextualized and generalized sentence representations using contrastive self-supervised learning. In the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a>, a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is given a text consisting of multiple sentences. One sentence is randomly selected as a target sentence. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is trained to maximize the similarity between the representation of the target sentence with its context and that of the masked target sentence with the same context. Simultaneously, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> minimizes the similarity between the latter <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a> and the representation of a random sentence with the same context. We apply our method to discourse relation analysis in <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a> and show that it outperforms strong baseline methods based on BERT, XLNet, and RoBERTa.</abstract>
      <url hash="76a6090e">2021.naacl-main.442</url>
      <doi>10.18653/v1/2021.naacl-main.442</doi>
      <bibkey>kiyomaru-kurohashi-2021-contextualized</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
    </paper>
    <paper id="445">
      <title>Unsupervised Concept Representation Learning for Length-Varying Text Similarity</title>
      <author><first>Xuchao</first><last>Zhang</last></author>
      <author><first>Bo</first><last>Zong</last></author>
      <author><first>Wei</first><last>Cheng</last></author>
      <author><first>Jingchao</first><last>Ni</last></author>
      <author><first>Yanchi</first><last>Liu</last></author>
      <author><first>Haifeng</first><last>Chen</last></author>
      <pages>5611–5620</pages>
      <abstract>Measuring document similarity plays an important role in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing tasks</a>. Most existing document similarity approaches suffer from the <a href="https://en.wikipedia.org/wiki/Information_gap">information gap</a> caused by <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context and vocabulary mismatches</a> when comparing varying-length texts. In this paper, we propose an unsupervised concept representation learning approach to address the above issues. Specifically, we propose a novel Concept Generation Network (CGNet) to learn concept representations from the perspective of the entire <a href="https://en.wikipedia.org/wiki/Text_corpus">text corpus</a>. Moreover, a concept-based document matching method is proposed to leverage advances in the recognition of local phrase features and corpus-level concept features. Extensive experiments on real-world data sets demonstrate that new method can achieve a considerable improvement in comparing length-varying texts. In particular, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieved 6.5 % better <a href="https://en.wikipedia.org/wiki/F1_score">F1 Score</a> compared to the best of the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline models</a> for a concept-project benchmark dataset.</abstract>
      <url hash="9fca36ac">2021.naacl-main.445</url>
      <doi>10.18653/v1/2021.naacl-main.445</doi>
      <bibkey>zhang-etal-2021-unsupervised</bibkey>
    </paper>
    <paper id="447">
      <title>Adversarial Self-Supervised Learning for Out-of-Domain Detection</title>
      <author><first>Zhiyuan</first><last>Zeng</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>Hong</first><last>Xu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>5631–5639</pages>
      <abstract>Detecting out-of-domain (OOD) intents is crucial for the deployed task-oriented dialogue system. Previous unsupervised OOD detection methods only extract discriminative features of different in-domain intents while supervised counterparts can directly distinguish OOD and in-domain intents but require extensive labeled OOD data. To combine the benefits of both types, we propose a self-supervised contrastive learning framework to model discriminative semantic features of both in-domain intents and OOD intents from unlabeled data. Besides, we introduce an adversarial augmentation neural module to improve the efficiency and <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of contrastive learning. Experiments on two public benchmark datasets show that our method can consistently outperform the baselines with a statistically significant margin.</abstract>
      <url hash="a04ad55d">2021.naacl-main.447</url>
      <doi>10.18653/v1/2021.naacl-main.447</doi>
      <bibkey>zeng-etal-2021-adversarial</bibkey>
      <pwccode url="https://github.com/parzival27/adversarial-self-supervised-out-of-domain-detection" additional="false">parzival27/adversarial-self-supervised-out-of-domain-detection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
    </paper>
    <paper id="449">
      <title>Hierarchical Transformer for Task Oriented Dialog Systems</title>
      <author><first>Bishal</first><last>Santra</last></author>
      <author><first>Potnuru</first><last>Anusha</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>5649–5658</pages>
      <abstract>Generative models for dialog systems have gained much interest because of the recent success of RNN and Transformer based models in tasks like <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> and <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a>. Although the task of dialog response generation is generally seen as a sequence to sequence (Seq2Seq) problem, researchers in the past have found it challenging to train dialog systems using the standard Seq2Seq models. Therefore, to help the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> learn meaningful utterance and conversation level features, Sordoni et al. (2015b), Serban et al. (2016) proposed Hierarchical RNN architecture, which was later adopted by several other RNN based dialog systems. With the transformer-based models dominating the seq2seq problems lately, the natural question to ask is the applicability of the notion of <a href="https://en.wikipedia.org/wiki/Hierarchy">hierarchy</a> in transformer-based dialog systems. In this paper, we propose a generalized framework for Hierarchical Transformer Encoders and show how a standard transformer can be morphed into any hierarchical encoder, including HRED and HIBERT like models, by using specially designed attention masks and positional encodings. We demonstrate that Hierarchical Encoding helps achieve better natural language understanding of the contexts in transformer-based models for task-oriented dialog systems through a wide range of experiments.</abstract>
      <url hash="c615c674">2021.naacl-main.449</url>
      <doi>10.18653/v1/2021.naacl-main.449</doi>
      <bibkey>santra-etal-2021-hierarchical</bibkey>
      <pwccode url="https://github.com/bsantraigi/hier-transformer-pytorch" additional="true">bsantraigi/hier-transformer-pytorch</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="451">
      <title>RTFE : A Recursive Temporal Fact Embedding Framework for Temporal Knowledge Graph Completion<fixed-case>RTFE</fixed-case>: A Recursive Temporal Fact Embedding Framework for Temporal Knowledge Graph Completion</title>
      <author><first>Youri</first><last>Xu</last></author>
      <author><first>Haihong</first><last>E</last></author>
      <author><first>Meina</first><last>Song</last></author>
      <author><first>Wenyu</first><last>Song</last></author>
      <author><first>Xiaodong</first><last>Lv</last></author>
      <author><first>Wang</first><last>Haotian</last></author>
      <author><first>Yang</first><last>Jinrui</last></author>
      <pages>5671–5681</pages>
      <abstract>Static knowledge graph (SKG) embedding (SKGE) has been studied intensively in the past years. Recently, temporal knowledge graph (TKG) embedding (TKGE) has emerged. In this paper, we propose a Recursive Temporal Fact Embedding (RTFE) framework to transplant SKGE models to TKGs and to enhance the performance of existing TKGE models for TKG completion. Different from previous work which ignores the continuity of states of TKG in <a href="https://en.wikipedia.org/wiki/Time_evolution">time evolution</a>, we treat the sequence of <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a> as a <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chain</a>, which transitions from the previous state to the next state. RTFE takes the SKGE to initialize the embeddings of <a href="https://en.wikipedia.org/wiki/TKG">TKG</a>. Then it recursively tracks the state transition of <a href="https://en.wikipedia.org/wiki/TKG">TKG</a> by passing updated parameters / features between timestamps. Specifically, at each timestamp, we approximate the <a href="https://en.wikipedia.org/wiki/State_transition">state transition</a> as the gradient update process. Since RTFE learns each timestamp recursively, it can naturally transit to future timestamps. Experiments on five TKG datasets show the effectiveness of RTFE.</abstract>
      <url hash="232743f8">2021.naacl-main.451</url>
      <doi>10.18653/v1/2021.naacl-main.451</doi>
      <bibkey>xu-etal-2021-rtfe</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/icews">ICEWS</pwcdataset>
    </paper>
    <paper id="454">
      <title>Multi-Grained Knowledge Distillation for Named Entity Recognition</title>
      <author><first>Xuan</first><last>Zhou</last></author>
      <author><first>Xiao</first><last>Zhang</last></author>
      <author><first>Chenyang</first><last>Tao</last></author>
      <author><first>Junya</first><last>Chen</last></author>
      <author><first>Bing</first><last>Xu</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Jing</first><last>Xiao</last></author>
      <pages>5704–5716</pages>
      <abstract>Although pre-trained big models (e.g., BERT, <a href="https://en.wikipedia.org/wiki/ERNIE">ERNIE</a>, XLNet, GPT3 etc.) have delivered top performance in Seq2seq modeling, their deployments in real-world applications are often hindered by the excessive computations and memory demand involved. For many applications, including named entity recognition (NER), matching the state-of-the-art result under budget has attracted considerable attention. Drawing power from the recent advance in knowledge distillation (KD), this work presents a novel distillation scheme to efficiently transfer the knowledge learned from big models to their more affordable counterpart. Our solution highlights the construction of surrogate labels through the k-best Viterbi algorithm to distill knowledge from the teacher model. To maximally assimilate knowledge into the student model, we propose a multi-grained distillation scheme, which integrates cross entropy involved in conditional random field (CRF) and fuzzy learning. To validate the effectiveness of our proposal, we conducted a comprehensive evaluation on five NER benchmarks, reporting cross-the-board performance gains relative to competing prior-arts. We further discuss ablation results to dissect our gains.</abstract>
      <url hash="80a493cf">2021.naacl-main.454</url>
      <doi>10.18653/v1/2021.naacl-main.454</doi>
      <bibkey>zhou-etal-2021-multi</bibkey>
      <pwccode url="https://github.com/11zhouxuan/multi_grained_kd_ner" additional="false">11zhouxuan/multi_grained_kd_ner</pwccode>
    </paper>
    <paper id="463">
      <title>TR-BERT : Dynamic Token Reduction for Accelerating BERT Inference<fixed-case>TR</fixed-case>-<fixed-case>BERT</fixed-case>: Dynamic Token Reduction for Accelerating <fixed-case>BERT</fixed-case> Inference</title>
      <author><first>Deming</first><last>Ye</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Yufei</first><last>Huang</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>5798–5809</pages>
      <abstract>Existing pre-trained language models (PLMs) are often computationally expensive in <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference</a>, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs’ inference, named TR-BERT, which could flexibly adapt the layer number of each token in <a href="https://en.wikipedia.org/wiki/Inference">inference</a> to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/TR-BERT.</abstract>
      <url hash="e279e6ef">2021.naacl-main.463</url>
      <doi>10.18653/v1/2021.naacl-main.463</doi>
      <bibkey>ye-etal-2021-tr</bibkey>
      <pwccode url="https://github.com/thunlp/TR-BERT" additional="false">thunlp/TR-BERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="464">
      <title>Breadth First Reasoning Graph for Multi-hop Question Answering</title>
      <author><first>Yongjie</first><last>Huang</last></author>
      <author><first>Meng</first><last>Yang</last></author>
      <pages>5810–5821</pages>
      <abstract>Recently Graph Neural Network (GNN) has been used as a promising tool in multi-hop question answering task. However, the unnecessary updations and simple edge constructions prevent an accurate answer span extraction in a more direct and interpretable way. In this paper, we propose a novel model of Breadth First Reasoning Graph (BFR-Graph), which presents a new message passing way that better conforms to the reasoning process. In BFR-Graph, the reasoning message is required to start from the question node and pass to the next sentences node hop by hop until all the edges have been passed, which can effectively prevent each <a href="https://en.wikipedia.org/wiki/Node_(networking)">node</a> from over-smoothing or being updated multiple times unnecessarily. To introduce more <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a>, we also define the reasoning graph as a <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">weighted graph</a> with considering the number of co-occurrence entities and the distance between sentences. Then we present a more direct and interpretable way to aggregate scores from different levels of <a href="https://en.wikipedia.org/wiki/Granularity">granularity</a> based on the GNN. On HotpotQA leaderboard, the proposed BFR-Graph achieves state-of-the-art on answer span prediction.</abstract>
      <url hash="3dc83a3f">2021.naacl-main.464</url>
      <doi>10.18653/v1/2021.naacl-main.464</doi>
      <bibkey>huang-yang-2021-breadth</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="469">
      <title>Unsupervised Multi-hop Question Answering by Question Generation</title>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Wenhu</first><last>Chen</last></author>
      <author><first>Wenhan</first><last>Xiong</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>5866–5880</pages>
      <abstract>Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multi-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised framework</a> that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting / generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61 % and 83 % of the <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data. Our codes are publicly available at https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA.</abstract>
      <url hash="d59cda74">2021.naacl-main.469</url>
      <doi>10.18653/v1/2021.naacl-main.469</doi>
      <bibkey>pan-etal-2021-unsupervised</bibkey>
      <pwccode url="https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA" additional="false">teacherpeterpan/Unsupervised-Multi-hop-QA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hybridqa">HybridQA</pwcdataset>
    </paper>
    <paper id="470">
      <title>Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents</title>
      <author><first>Peng</first><last>Cui</last></author>
      <author><first>Le</first><last>Hu</last></author>
      <pages>5881–5891</pages>
      <abstract>Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with <a href="https://en.wikipedia.org/wiki/Dynamic_memory">dynamic memory</a> for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the history information dynamically, allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> substantially outperforms previous state-of-the-art <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Besides, we perform qualitative and quantitative investigations on how our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> works and where the performance gain comes from.</abstract>
      <url hash="9077b950">2021.naacl-main.470</url>
      <doi>10.18653/v1/2021.naacl-main.470</doi>
      <bibkey>cui-hu-2021-sliding</bibkey>
      <pwccode url="https://github.com/pcui-nlp/ssn_dm" additional="false">pcui-nlp/ssn_dm</pwccode>
    </paper>
    </volume>
  <volume id="demos" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations</booktitle>
      <editor><first>Avi</first><last>Sil</last></editor>
      <editor><first>Xi Victoria</first><last>Lin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.naacl-demos</url>
    </meta>
    <frontmatter>
      <url hash="15c022e7">2021.naacl-demos.0</url>
      <bibkey>naacl-2021-2021-north</bibkey>
    </frontmatter>
    <paper id="1">
      <title>PhoNLP : A joint multi-task learning model for Vietnamese part-of-speech tagging, named entity recognition and dependency parsing<fixed-case>P</fixed-case>ho<fixed-case>NLP</fixed-case>: A joint multi-task learning model for <fixed-case>V</fixed-case>ietnamese part-of-speech tagging, named entity recognition and dependency parsing</title>
      <author><first>Linh The</first><last>Nguyen</last></author>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <pages>1–7</pages>
      <abstract>We present the first multi-task learning model   named PhoNLP   for joint Vietnamese part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT (Nguyen and Nguyen, 2020) for each task independently. We publicly release PhoNLP as an open-source toolkit under the <a href="https://en.wikipedia.org/wiki/Apache_License">Apache License 2.0</a>. Although we specify PhoNLP for <a href="https://en.wikipedia.org/wiki/Vietnamese_language">Vietnamese</a>, our PhoNLP training and evaluation command scripts in fact can directly work for other languages that have a pre-trained BERT-based language model and gold annotated corpora available for the three tasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future NLP research and applications to not only <a href="https://en.wikipedia.org/wiki/Vietnamese_language">Vietnamese</a> but also the other languages. Our PhoNLP is available at https://github.com/VinAIResearch/PhoNLP</abstract>
      <url hash="478af461">2021.naacl-demos.1</url>
      <doi>10.18653/v1/2021.naacl-demos.1</doi>
      <bibkey>nguyen-nguyen-2021-phonlp</bibkey>
      <pwccode url="https://github.com/VinAIResearch/PhoNLP" additional="false">VinAIResearch/PhoNLP</pwccode>
    </paper>
    <paper id="3">
      <title>NAMER : A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering<fixed-case>NAMER</fixed-case>: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering</title>
      <author><first>Minhao</first><last>Zhang</last></author>
      <author><first>Ruoyu</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Zou</last></author>
      <author><first>Yinnian</first><last>Lin</last></author>
      <author><first>Sen</first><last>Hu</last></author>
      <pages>18–25</pages>
      <abstract>We present NAMER, an open-domain Chinese knowledge base question answering system based on a novel node-based framework that better grasps the structural mapping between questions and KB queries by aligning the nodes in a query with their corresponding mentions in question. Equipped with techniques including <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> and <a href="https://en.wikipedia.org/wiki/Computer_multitasking">multitasking</a>, we show that the proposed framework outperforms the previous SoTA on CCKS CKBQA dataset. Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset (https://github.com/ridiculouz/CKBQA) with such strategy is also published to promote further research. An online demo of <a href="https://en.wikipedia.org/wiki/Namer">NAMER</a> (http://kbqademo.gstore.cn) is provided to visualize our framework and supply extra information for users, a video illustration (https://youtu.be/yetnVye_hg4) of <a href="https://en.wikipedia.org/wiki/Namer">NAMER</a> is also available.</abstract>
      <url hash="ea05fc80">2021.naacl-demos.3</url>
      <doi>10.18653/v1/2021.naacl-demos.3</doi>
      <bibkey>zhang-etal-2021-namer</bibkey>
    </paper>
    <paper id="5">
      <title>FITAnnotator : A Flexible and Intelligent Text Annotation System<fixed-case>FITA</fixed-case>nnotator: A Flexible and Intelligent Text Annotation System</title>
      <author><first>Yanzeng</first><last>Li</last></author>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Li</first><last>Quangang</last></author>
      <author><first>Tingwen</first><last>Liu</last></author>
      <pages>35–41</pages>
      <abstract>In this paper, we introduce FITAnnotator, a generic web-based tool for efficient text annotation. Benefiting from the fully modular architecture design, FITAnnotator provides a systematic solution for the annotation of a variety of natural language processing tasks, including <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>, sequence tagging and semantic role annotation, regardless of the language. Three kinds of <a href="https://en.wikipedia.org/wiki/Interface_(computing)">interfaces</a> are developed to annotate instances, evaluate annotation quality and manage the annotation task for annotators, reviewers and managers, respectively. FITAnnotator also gives intelligent annotations by introducing task-specific assistant to support and guide the annotators based on <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a> and incremental learning strategies. This assistant is able to effectively update from the annotator feedbacks and easily handle the incremental labeling scenarios.</abstract>
      <url hash="730d19d5">2021.naacl-demos.5</url>
      <doi>10.18653/v1/2021.naacl-demos.5</doi>
      <bibkey>li-etal-2021-fitannotator</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="6">
      <title>Robustness Gym : Unifying the NLP Evaluation Landscape<fixed-case>NLP</fixed-case> Evaluation Landscape</title>
      <author><first>Karan</first><last>Goel</last></author>
      <author><first>Nazneen Fatema</first><last>Rajani</last></author>
      <author><first>Jesse</first><last>Vig</last></author>
      <author><first>Zachary</first><last>Taschdjian</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Christopher</first><last>Ré</last></author>
      <pages>42–55</pages>
      <abstract>Despite impressive performance on standard <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a>, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms : subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables practitioners to compare results from disparate evaluation paradigms with a single click, and to easily develop and share novel evaluation methods using a built-in set of abstractions. RG is under active development and we welcome feedback &amp; contributions from the community.</abstract>
      <url hash="993be04c">2021.naacl-demos.6</url>
      <doi>10.18653/v1/2021.naacl-demos.6</doi>
      <bibkey>goel-etal-2021-robustness</bibkey>
      <pwccode url="https://github.com/robustness-gym/robustness-gym" additional="true">robustness-gym/robustness-gym</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="7">
      <title>EventPlus : A Temporal Event Understanding Pipeline<fixed-case>E</fixed-case>vent<fixed-case>P</fixed-case>lus: A Temporal Event Understanding Pipeline</title>
      <author><first>Mingyu Derek</first><last>Ma</last></author>
      <author><first>Jiao</first><last>Sun</last></author>
      <author><first>Mu</first><last>Yang</last></author>
      <author><first>Kung-Hsiang</first><last>Huang</last></author>
      <author><first>Nuan</first><last>Wen</last></author>
      <author><first>Shikhar</first><last>Singh</last></author>
      <author><first>Rujun</first><last>Han</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>56–65</pages>
      <abstract>We present EventPlus, a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection, event argument detection, event duration and temporal relation extraction. Event information, especially event temporal knowledge, is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. EventPlus as the first comprehensive temporal event understanding pipeline provides a convenient tool for users to quickly obtain annotations about events and their temporal information for any user-provided document. Furthermore, we show EventPlus can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications.</abstract>
      <url hash="cf0c5341">2021.naacl-demos.7</url>
      <doi>10.18653/v1/2021.naacl-demos.7</doi>
      <bibkey>ma-etal-2021-eventplus</bibkey>
    </paper>
    <paper id="12">
      <title>ActiveAnno : General-Purpose Document-Level Annotation Tool with Active Learning Integration<fixed-case>A</fixed-case>ctive<fixed-case>A</fixed-case>nno: General-Purpose Document-Level Annotation Tool with Active Learning Integration</title>
      <author><first>Max</first><last>Wiechmann</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>99–105</pages>
      <abstract>ActiveAnno is an <a href="https://en.wikipedia.org/wiki/Annotation">annotation tool</a> focused on document-level annotation tasks developed both for industry and research settings. It is designed to be a general-purpose tool with a wide variety of use cases. It features a modern and responsive <a href="https://en.wikipedia.org/wiki/User_interface">web UI</a> for creating annotation projects, conducting <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a>, adjudicating disagreements, and analyzing annotation results. ActiveAnno embeds a highly configurable and interactive user interface. The tool also integrates a RESTful API that enables integration into other <a href="https://en.wikipedia.org/wiki/Software_system">software systems</a>, including an <a href="https://en.wikipedia.org/wiki/Application_programming_interface">API</a> for machine learning integration. ActiveAnno is built with extensible design and easy deployment in mind, all to enable users to perform annotation tasks with high efficiency and high-quality annotation results.</abstract>
      <url hash="620fb7be">2021.naacl-demos.12</url>
      <attachment type="Supplementary" hash="f4feeea6">2021.naacl-demos.12.Supplementary.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-demos.12</doi>
      <bibkey>wiechmann-etal-2021-activeanno</bibkey>
    </paper>
    <paper id="13">
      <title>TextEssence : A Tool for Interactive Analysis of Semantic Shifts Between Corpora<fixed-case>T</fixed-case>ext<fixed-case>E</fixed-case>ssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora</title>
      <author><first>Denis</first><last>Newman-Griffis</last></author>
      <author><first>Venkatesh</first><last>Sivaraman</last></author>
      <author><first>Adam</first><last>Perer</last></author>
      <author><first>Eric</first><last>Fosler-Lussier</last></author>
      <author><first>Harry</first><last>Hochheiser</last></author>
      <pages>106–115</pages>
      <abstract>Embeddings of words and concepts capture syntactic and semantic regularities of language ; however, they have seen limited use as tools to study characteristics of different corpora and how they relate to one another. We introduce TextEssence, an interactive system designed to enable comparative analysis of corpora using <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a>. TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface. We further propose a new measure of embedding confidence based on nearest neighborhood overlap, to assist in identifying high-quality <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> for <a href="https://en.wikipedia.org/wiki/Corpus_linguistics">corpus analysis</a>. A case study on COVID-19 scientific literature illustrates the utility of the <a href="https://en.wikipedia.org/wiki/System">system</a>. TextEssence can be found at https://textessence.github.io.</abstract>
      <url hash="4f44e392">2021.naacl-demos.13</url>
      <doi>10.18653/v1/2021.naacl-demos.13</doi>
      <bibkey>newman-griffis-etal-2021-textessence</bibkey>
      <pwccode url="https://github.com/drgriffis/text-essence" additional="false">drgriffis/text-essence</pwccode>
    </paper>
    <paper id="16">
      <title>RESIN : A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System<fixed-case>RESIN</fixed-case>: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System</title>
      <author><first>Haoyang</first><last>Wen</last></author>
      <author><first>Ying</first><last>Lin</last></author>
      <author><first>Tuan</first><last>Lai</last></author>
      <author><first>Xiaoman</first><last>Pan</last></author>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Xudong</first><last>Lin</last></author>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Manling</first><last>Li</last></author>
      <author><first>Haoyu</first><last>Wang</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Xiaodong</first><last>Yu</last></author>
      <author><first>Alexander</first><last>Dong</last></author>
      <author><first>Zhenhailong</first><last>Wang</last></author>
      <author><first>Yi</first><last>Fung</last></author>
      <author><first>Piyush</first><last>Mishra</last></author>
      <author><first>Qing</first><last>Lyu</last></author>
      <author><first>Dídac</first><last>Surís</last></author>
      <author><first>Brian</first><last>Chen</last></author>
      <author><first>Susan Windisch</first><last>Brown</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Carl</first><last>Vondrick</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Shih-Fu</first><last>Chang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>133–143</pages>
      <abstract>We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects : (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking ; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.</abstract>
      <url hash="41c10f7c">2021.naacl-demos.16</url>
      <doi>10.18653/v1/2021.naacl-demos.16</doi>
      <bibkey>wen-etal-2021-resin</bibkey>
      <pwccode url="https://github.com/resin-kairos/resin-pipeline-public" additional="false">resin-kairos/resin-pipeline-public</pwccode>
    </paper>
    <paper id="17">
      <title>MUDES : Multilingual Detection of Offensive Spans<fixed-case>MUDES</fixed-case>: Multilingual Detection of Offensive Spans</title>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>144–152</pages>
      <abstract>The interest in offensive content identification in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> has grown substantially in recent years. Previous work has dealt mostly with post level annotations. However, identifying offensive spans is useful in many ways. To help coping with this important challenge, we present MUDES, a multilingual system to detect offensive spans in texts. MUDES features pre-trained models, a <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python API</a> for developers, and a user-friendly web-based interface. A detailed description of MUDES’ components is presented in this paper.</abstract>
      <url hash="8bbcbf4b">2021.naacl-demos.17</url>
      <doi>10.18653/v1/2021.naacl-demos.17</doi>
      <bibkey>ranasinghe-zampieri-2021-mudes</bibkey>
      <pwccode url="https://github.com/tharindudr/MUDES" additional="false">tharindudr/MUDES</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</booktitle>
      <editor><first>Esin</first><last>Durmus</last></editor>
      <editor><first>Vivek</first><last>Gupta</last></editor>
      <editor><first>Nelson</first><last>Liu</last></editor>
      <editor><first>Nanyun</first><last>Peng</last></editor>
      <editor><first>Yu</first><last>Su</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.naacl-srw</url>
    </meta>
    <frontmatter>
      <url hash="ae6ac8a7">2021.naacl-srw.0</url>
      <bibkey>naacl-2021-2021-north-american</bibkey>
    </frontmatter>
    <paper id="12">
      <title>Shuffled-token Detection for Refining Pre-trained RoBERTa<fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a</title>
      <author><first>Subhadarshi</first><last>Panda</last></author>
      <author><first>Anjali</first><last>Agrawal</last></author>
      <author><first>Jeewon</first><last>Ha</last></author>
      <author><first>Benjamin</first><last>Bloch</last></author>
      <pages>88–93</pages>
      <abstract>State-of-the-art transformer models have achieved robust performance on a variety of NLP tasks. Many of these approaches have employed domain agnostic pre-training tasks to train <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> that yield highly generalized sentence representations that can be fine-tuned for specific downstream tasks. We propose refining a pre-trained NLP model using the objective of detecting shuffled tokens. We use a sequential approach by starting with the pre-trained RoBERTa model and training it using our approach. Applying random shuffling strategy on the word-level, we found that our approach enables the RoBERTa model achieve better performance on 4 out of 7 GLUE tasks. Our results indicate that learning to detect shuffled tokens is a promising approach to learn more coherent sentence representations.</abstract>
      <url hash="6f5d204d">2021.naacl-srw.12</url>
      <doi>10.18653/v1/2021.naacl-srw.12</doi>
      <bibkey>panda-etal-2021-shuffled</bibkey>
    </paper>
    <paper id="13">
      <title>Morphology-Aware Meta-Embeddings for Tamil<fixed-case>T</fixed-case>amil</title>
      <author><first>Arjun Sai</first><last>Krishnan</last></author>
      <author><first>Seyoon</first><last>Ragavan</last></author>
      <pages>94–111</pages>
      <abstract>In this work, we explore generating morphologically enhanced word embeddings for <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a>, a highly agglutinative South Indian language with rich morphology that remains low-resource with regards to NLP tasks. We present here the first-ever word analogy dataset for <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a>, consisting of 4499 hand-curated word tetrads across 10 semantic and 13 morphological relation types. Using a rules-based segmenter to capture <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphology</a> as well as meta-embedding techniques, we train meta-embeddings that outperform existing baselines by 16 % on our analogy task and appear to mitigate a previously observed trade-off between semantic and morphological accuracy.</abstract>
      <url hash="7eaa71ac">2021.naacl-srw.13</url>
      <doi>10.18653/v1/2021.naacl-srw.13</doi>
      <bibkey>krishnan-ragavan-2021-morphology</bibkey>
      <pwccode url="https://github.com/arjun-sai-krishnan/tamil-morpho-embeddings" additional="false">arjun-sai-krishnan/tamil-morpho-embeddings</pwccode>
    </paper>
    <paper id="14">
      <title>Seed Word Selection for Weakly-Supervised Text Classification with Unsupervised Error Estimation</title>
      <author><first>Yiping</first><last>Jin</last></author>
      <author><first>Akshay</first><last>Bhatia</last></author>
      <author><first>Dittaya</first><last>Wanvarie</last></author>
      <pages>112–118</pages>
      <abstract>Weakly-supervised text classification aims to induce <a href="https://en.wikipedia.org/wiki/Text_classification">text classifiers</a> from only a few user-provided seed words. The vast majority of previous work assumes high-quality seed words are given. However, the expert-annotated seed words are sometimes non-trivial to come up with. Furthermore, in the weakly-supervised learning setting, we do not have any labeled document to measure the seed words’ efficacy, making the seed word selection process a walk in the dark. In this work, we remove the need for expert-curated seed words by first mining (noisy) candidate seed words associated with the category names. We then train interim models with individual candidate seed words. Lastly, we estimate the interim models’ <a href="https://en.wikipedia.org/wiki/Errors_and_residuals">error rate</a> in an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised manner</a>. The seed words that yield the lowest estimated <a href="https://en.wikipedia.org/wiki/Errors_and_residuals">error rates</a> are added to the final seed word set. A comprehensive evaluation of six binary classification tasks on four popular datasets demonstrates that the proposed method outperforms a baseline using only category name seed words and obtained comparable performance as a counterpart using expert-annotated seed words.</abstract>
      <url hash="8fd4bbbb">2021.naacl-srw.14</url>
      <doi>10.18653/v1/2021.naacl-srw.14</doi>
      <bibkey>jin-etal-2021-seed</bibkey>
      <pwccode url="https://github.com/YipingNUS/OptimSeed" additional="false">YipingNUS/OptimSeed</pwccode>
    </paper>
    <paper id="15">
      <title>Multi-Task Learning of Generation and Classification for Emotion-Aware Dialogue Response Generation</title>
      <author><first>Tatsuya</first><last>Ide</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <pages>119–125</pages>
      <abstract>For a computer to naturally interact with a human, it needs to be human-like. In this paper, we propose a neural response generation model with multi-task learning of generation and classification, focusing on <a href="https://en.wikipedia.org/wiki/Emotion">emotion</a>. Our <a href="https://en.wikipedia.org/wiki/Computer_simulation">model</a> based on BART (Lewis et al., 2020), a pre-trained transformer encoder-decoder model, is trained to generate responses and recognize <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> simultaneously. Furthermore, we weight the losses for the <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> to control the update of parameters. Automatic evaluations and crowdsourced manual evaluations show that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> makes generated responses more emotionally aware.</abstract>
      <url hash="3ca106ae">2021.naacl-srw.15</url>
      <doi>10.18653/v1/2021.naacl-srw.15</doi>
      <bibkey>ide-kawahara-2021-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="16">
      <title>Comparison of <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">Grammatical Error Correction</a> Using Back-Translation Models</title>
      <author><first>Aomi</first><last>Koyama</last></author>
      <author><first>Kengo</first><last>Hotate</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>126–135</pages>
      <abstract>Grammatical error correction (GEC) suffers from a lack of sufficient parallel data. Studies on GEC have proposed several methods to generate pseudo data, which comprise pairs of grammatical and artificially produced ungrammatical sentences. Currently, a mainstream approach to generate pseudo data is back-translation (BT). Most previous studies using <a href="https://en.wikipedia.org/wiki/BT_Group">BT</a> have employed the same <a href="https://en.wikipedia.org/wiki/Computer_architecture">architecture</a> for both the GEC and BT models. However, GEC models have different correction tendencies depending on the architecture of their <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. Thus, in this study, we compare the correction tendencies of GEC models trained on pseudo data generated by three BT models with different architectures, namely, Transformer, CNN, and LSTM. The results confirm that the correction tendencies for each error type are different for every BT model. In addition, we investigate the correction tendencies when using a combination of pseudo data generated by different BT models. As a result, we find that the combination of different BT models improves or interpolates the performance of each error type compared with using a single BT model with different seeds.</abstract>
      <url hash="b1a75da0">2021.naacl-srw.16</url>
      <doi>10.18653/v1/2021.naacl-srw.16</doi>
      <bibkey>koyama-etal-2021-comparison</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
    </paper>
    <paper id="20">
      <title>Hie-BART : Document Summarization with Hierarchical BART<fixed-case>BART</fixed-case>: Document Summarization with Hierarchical <fixed-case>BART</fixed-case></title>
      <author><first>Kazuki</first><last>Akiyama</last></author>
      <author><first>Akihiro</first><last>Tamura</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <pages>159–165</pages>
      <abstract>This paper proposes a new abstractive document summarization model, hierarchical BART (Hie-BART), which captures <a href="https://en.wikipedia.org/wiki/Hierarchical_organization">hierarchical structures</a> of a document (i.e., sentence-word structures) in the BART model. Although the existing BART model has achieved a state-of-the-art performance on document summarization tasks, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> does not have the interactions between sentence-level information and word-level information. In machine translation tasks, the performance of neural machine translation models has been improved by incorporating multi-granularity self-attention (MG-SA), which captures the relationships between words and phrases. Inspired by the previous work, the proposed Hie-BART model incorporates MG-SA into the encoder of the BART model for capturing sentence-word structures. Evaluations on the CNN / Daily Mail dataset show that the proposed Hie-BART model outperforms some strong baselines and improves the performance of a non-hierarchical BART model (+0.23 ROUGE-L).</abstract>
      <url hash="0d561395">2021.naacl-srw.20</url>
      <doi>10.18653/v1/2021.naacl-srw.20</doi>
      <bibkey>akiyama-etal-2021-hie</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    </volume>
  <volume id="tutorials" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials</booktitle>
      <editor><first>Greg</first><last>Kondrak</last></editor>
      <editor><first>Kalina</first><last>Bontcheva</last></editor>
      <editor><first>Dan</first><last>Gillick</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.naacl-tutorials</url>
    </meta>
    <frontmatter>
      <url hash="7699fb95">2021.naacl-tutorials.0</url>
      <bibkey>naacl-2021-2021-north-american-chapter</bibkey>
    </frontmatter>
    <paper id="4">
      <title>A Tutorial on Evaluation Metrics used in Natural Language Generation</title>
      <author><first>Mitesh M.</first><last>Khapra</last></author>
      <author><first>Ananya B.</first><last>Sai</last></author>
      <pages>15–19</pages>
      <abstract>The advent of <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> and the availability of large scale datasets has accelerated research on <a href="https://en.wikipedia.org/wiki/Natural-language_generation">Natural Language Generation</a> with a focus on newer tasks and better <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas / components that need improvement. To accomplish this in an automatic and reliable manner, the NLP community has actively pursued the development of automatic evaluation metrics. Especially in the last few years, there has been an increasing focus on evaluation metrics, with several criticisms of existing metrics and proposals for several new <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>. This tutorial presents the evolution of automatic evaluation metrics to their current state along with the emerging trends in this field by specifically addressing the following questions : (i) What makes NLG evaluation challenging? (ii) Why do we need automatic evaluation metrics? (iii) What are the existing automatic evaluation metrics and how can they be organised in a coherent taxonomy? (iv) What are the criticisms and shortcomings of existing <a href="https://en.wikipedia.org/wiki/Performance_metric">metrics</a>? (v) What are the possible future directions of research?</abstract>
      <url hash="459d1f5b">2021.naacl-tutorials.4</url>
      <doi>10.18653/v1/2021.naacl-tutorials.4</doi>
      <bibkey>khapra-sai-2021-tutorial</bibkey>
    </paper>
    <paper id="6">
      <title>Crowdsourcing Natural Language Data at Scale : A Hands-On Tutorial</title>
      <author><first>Alexey</first><last>Drutsa</last></author>
      <author><first>Dmitry</first><last>Ustalov</last></author>
      <author><first>Valentina</first><last>Fedorova</last></author>
      <author><first>Olga</first><last>Megorskaya</last></author>
      <author><first>Daria</first><last>Baidakova</last></author>
      <pages>25–30</pages>
      <abstract>In this tutorial, we present a portion of unique industry experience in efficient natural language data annotation via <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a> shared by both leading researchers and engineers from Yandex. We will make an introduction to data labeling via public crowdsourcing marketplaces and will present the key components of efficient label collection. This will be followed by a practical session, where participants address a real-world language resource production task, experiment with selecting settings for the labeling process, and launch their label collection project on one of the largest crowdsourcing marketplaces. The projects will be run on real crowds within the tutorial session and we will present useful quality control techniques and provide the attendees with an opportunity to discuss their own annotation ideas.</abstract>
      <url hash="2a2af5c4">2021.naacl-tutorials.6</url>
      <doi>10.18653/v1/2021.naacl-tutorials.6</doi>
      <bibkey>drutsa-etal-2021-crowdsourcing</bibkey>
    </paper>
  </volume>
  <volume id="industry" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</booktitle>
      <editor><first>Young-bum</first><last>Kim</last></editor>
      <editor><first>Yunyao</first><last>Li</last></editor>
      <editor><first>Owen</first><last>Rambow</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.naacl-industry</url>
    </meta>
    <frontmatter>
      <url hash="e7988991">2021.naacl-industry.0</url>
      <bibkey>naacl-2021-2021-north-american-chapter-association</bibkey>
    </frontmatter>
    <paper id="1">
      <title>When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages</title>
      <author><first>Stojan</first><last>Trajanovski</last></author>
      <author><first>Chad</first><last>Atalla</last></author>
      <author><first>Kunho</first><last>Kim</last></author>
      <author><first>Vipul</first><last>Agarwal</last></author>
      <author><first>Milad</first><last>Shokouhi</last></author>
      <author><first>Chris</first><last>Quirk</last></author>
      <pages>1–9</pages>
      <abstract>Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and <a href="https://en.wikipedia.org/wiki/Microsoft_Outlook">Outlook</a>, finding that contextual signals contribute to performance differently between these scenarios. On <a href="https://en.wikipedia.org/wiki/Email">emails</a>, time context is most beneficial with small relative gains of 2 % over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a> yields relative improvements over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> between 9.3 % and 18.6 % across various critical service-oriented text prediction metrics.</abstract>
      <url hash="c107f40e">2021.naacl-industry.1</url>
      <doi>10.18653/v1/2021.naacl-industry.1</doi>
      <bibkey>trajanovski-etal-2021-text</bibkey>
    </paper>
    <paper id="10">
      <title>Proteno : Text Normalization with Limited Data for Fast Deployment in Text to Speech Systems</title>
      <author><first>Shubhi</first><last>Tyagi</last></author>
      <author><first>Antonio</first><last>Bonafonte</last></author>
      <author><first>Jaime</first><last>Lorenzo-Trueba</last></author>
      <author><first>Javier</first><last>Latorre</last></author>
      <pages>72–79</pages>
      <abstract>Developing Text Normalization (TN) systems for Text-to-Speech (TTS) on new languages is hard. We propose a novel <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a> to facilitate it for multiple languages while using <a href="https://en.wikipedia.org/wiki/Data">data</a> less than 3 % of the size of the data used by the state of the art results on <a href="https://en.wikipedia.org/wiki/English_language">English</a>. We treat TN as a sequence classification problem and propose a granular tokenization mechanism that enables the system to learn majority of the classes and their normalizations from the training data itself. This is further combined with minimal precoded linguistic knowledge for other classes. We publish the first results on TN for TTS in <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a> and <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a> and also demonstrate that the performance of the approach is comparable with the previous work done on <a href="https://en.wikipedia.org/wiki/English_language">English</a>. All annotated datasets used for experimentation will be released.</abstract>
      <url hash="5bdaa941">2021.naacl-industry.10</url>
      <doi>10.18653/v1/2021.naacl-industry.10</doi>
      <bibkey>tyagi-etal-2021-proteno</bibkey>
      <pwccode url="https://github.com/amazon-research/proteno" additional="false">amazon-research/proteno</pwccode>
    </paper>
    <paper id="14">
      <title>Autocorrect in the Process of Translation   Multi-task Learning Improves Dialogue Machine Translation</title>
      <author><first>Tao</first><last>Wang</last></author>
      <author><first>Chengqi</first><last>Zhao</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>105–112</pages>
      <abstract>Automatic translation of dialogue texts is a much needed demand in many real life scenarios. However, the currently existing <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> delivers unsatisfying results. In this paper, we conduct a deep analysis of a dialogue corpus and summarize three major issues on dialogue translation, including pronoun dropping (), punctuation dropping (), and typos (). In response to these challenges, we propose a joint learning method to identify omission and typo, and utilize <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a> to translate dialogue utterances. To properly evaluate the performance, we propose a manually annotated dataset with 1,931 Chinese-English parallel utterances from 300 dialogues as a benchmark testbed for dialogue translation. Our experiments show that the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> improves translation quality by 3.2 BLEU over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>. It also elevates the recovery rate of omitted pronouns from 26.09 % to 47.16 %. We will publish the code and dataset publicly at https://xxx.xx.</abstract>
      <url hash="d3015c87">2021.naacl-industry.14</url>
      <doi>10.18653/v1/2021.naacl-industry.14</doi>
      <bibkey>wang-etal-2021-autocorrect</bibkey>
    </paper>
    <paper id="16">
      <title>Practical Transformer-based Multilingual Text Classification</title>
      <author><first>Cindy</first><last>Wang</last></author>
      <author><first>Michele</first><last>Banko</last></author>
      <pages>121–129</pages>
      <abstract>Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> on two distinct <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> can improve <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> performance without the need for additional labeled data.</abstract>
      <url hash="f093073c">2021.naacl-industry.16</url>
      <doi>10.18653/v1/2021.naacl-industry.16</doi>
      <bibkey>wang-banko-2021-practical</bibkey>
      <pwccode url="https://github.com/sentropytechnologies/hateval2019-relabeled" additional="false">sentropytechnologies/hateval2019-relabeled</pwccode>
    </paper>
    <paper id="19">
      <title>Graph-based Multilingual Product Retrieval in E-Commerce Search<fixed-case>E</fixed-case>-Commerce Search</title>
      <author><first>Hanqing</first><last>Lu</last></author>
      <author><first>Youna</first><last>Hu</last></author>
      <author><first>Tong</first><last>Zhao</last></author>
      <author><first>Tony</first><last>Wu</last></author>
      <author><first>Yiwei</first><last>Song</last></author>
      <author><first>Bing</first><last>Yin</last></author>
      <pages>146–153</pages>
      <abstract>Nowadays, with many <a href="https://en.wikipedia.org/wiki/E-commerce">e-commerce platforms</a> conducting global business, e-commerce search systems are required to handle product retrieval under multilingual scenarios. Moreover, comparing with maintaining per-country specific e-commerce search systems, having an universal system across countries can further reduce the operational and computational costs, and facilitate business expansion to new countries. In this paper, we introduce an universal end-to-end multilingual retrieval system, and discuss our learnings and technical details when training and deploying the <a href="https://en.wikipedia.org/wiki/System">system</a> to serve billion-scale product retrieval for e-commerce search. In particular, we propose a multilingual graph attention based retrieval network by leveraging recent advances in transformer-based multilingual language models and graph neural network architectures to capture the interactions between search queries and items in e-commerce search. Offline experiments on five countries data show that our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> outperforms the state-of-the-art baselines by 35 % <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> and 25 % mAP on average. Moreover, the proposed model shows significant increase of conversion / revenue in online A / B experiments and has been deployed in production for multiple countries.</abstract>
      <url hash="99d899bd">2021.naacl-industry.19</url>
      <doi>10.18653/v1/2021.naacl-industry.19</doi>
      <bibkey>lu-etal-2021-graph</bibkey>
    </paper>
    <paper id="20">
      <title>Query2Prod2Vec : Grounded Word Embeddings for <a href="https://en.wikipedia.org/wiki/E-commerce">eCommerce</a><fixed-case>Q</fixed-case>uery2<fixed-case>P</fixed-case>rod2<fixed-case>V</fixed-case>ec: Grounded Word Embeddings for e<fixed-case>C</fixed-case>ommerce</title>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Jacopo</first><last>Tagliabue</last></author>
      <author><first>Bingqing</first><last>Yu</last></author>
      <pages>154–162</pages>
      <abstract>We present Query2Prod2Vec, a model that grounds lexical representations for product search in product embeddings : in our model, meaning is a mapping between words and a latent space of products in a digital shop. We leverage shopping sessions to learn the underlying space and use merchandising annotations to build lexical analogies for evaluation : our experiments show that our model is more accurate than known techniques from the NLP and IR literature. Finally, we stress the importance of <a href="https://en.wikipedia.org/wiki/Data_efficiency">data efficiency</a> for product search outside of retail giants, and highlight how Query2Prod2Vec fits with practical constraints faced by most practitioners.</abstract>
      <url hash="c19f4577">2021.naacl-industry.20</url>
      <award>Best Industry Paper</award>
      <doi>10.18653/v1/2021.naacl-industry.20</doi>
      <bibkey>bianchi-etal-2021-query2prod2vec</bibkey>
      <pwccode url="https://github.com/coveooss/ecommerce-query-embeddings" additional="false">coveooss/ecommerce-query-embeddings</pwccode>
    </paper>
    <paper id="21">
      <title>An <a href="https://en.wikipedia.org/wiki/Architecture">Architecture</a> for Accelerated Large-Scale Inference of Transformer-Based Language Models</title>
      <author><first>Amir</first><last>Ganiev</last></author>
      <author><first>Colton</first><last>Chapin</last></author>
      <author><first>Anderson</first><last>De Andrade</last></author>
      <author><first>Chen</first><last>Liu</last></author>
      <pages>163–169</pages>
      <abstract>This work demonstrates the development process of a machine learning architecture for <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference</a> that can scale to a large volume of requests. We used a BERT model that was fine-tuned for emotion analysis, returning a probability distribution of emotions given a paragraph. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> was deployed as a gRPC service on <a href="https://en.wikipedia.org/wiki/Kubernetes">Kubernetes</a>. Apache Spark was used to perform <a href="https://en.wikipedia.org/wiki/Inference">inference</a> in batches by calling the <a href="https://en.wikipedia.org/wiki/Service_(systems_architecture)">service</a>. We encountered some performance and concurrency challenges and created solutions to achieve <a href="https://en.wikipedia.org/wiki/Time_complexity">faster running time</a>. Starting with 200 successful inference requests per minute, we were able to achieve as high as 18 thousand successful requests per minute with the same batch job resource allocation. As a result, we successfully stored emotion probabilities for 95 million paragraphs within 96 hours.</abstract>
      <url hash="c32ff7b4">2021.naacl-industry.21</url>
      <doi>10.18653/v1/2021.naacl-industry.21</doi>
      <bibkey>ganiev-etal-2021-architecture</bibkey>
    </paper>
    <paper id="24">
      <title>Cost-effective Deployment of BERT Models in Serverless Environment<fixed-case>BERT</fixed-case> Models in Serverless Environment</title>
      <author><first>Marek</first><last>Suppa</last></author>
      <author><first>Katarína</first><last>Benešová</last></author>
      <author><first>Andrej</first><last>Švec</last></author>
      <pages>187–195</pages>
      <abstract>In this study, we demonstrate the viability of deploying BERT-style models to <a href="https://en.wikipedia.org/wiki/AWS_Lambda">AWS Lambda</a> in a production environment. Since the freely available pre-trained models are too large to be deployed in this environment, we utilize knowledge distillation and fine-tune the models on proprietary datasets for two real-world tasks : <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> and <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic textual similarity</a>. As a result, we obtain <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> that are tuned for a specific domain and deployable in the <a href="https://en.wikipedia.org/wiki/Serverless_computing">serverless environment</a>. The subsequent performance analysis shows that this solution does not only report latency levels acceptable for production use but that it is also a cost-effective alternative to small-to-medium size deployments of BERT models, all without any infrastructure overhead.</abstract>
      <url hash="0ceebe9e">2021.naacl-industry.24</url>
      <doi>10.18653/v1/2021.naacl-industry.24</doi>
      <bibkey>suppa-etal-2021-cost</bibkey>
    </paper>
    <paper id="25">
      <title>Noise Robust Named Entity Understanding for Voice Assistants</title>
      <author><first>Deepak</first><last>Muralidharan</last></author>
      <author><first>Joel Ruben Antony</first><last>Moniz</last></author>
      <author><first>Sida</first><last>Gao</last></author>
      <author><first>Xiao</first><last>Yang</last></author>
      <author><first>Justine</first><last>Kao</last></author>
      <author><first>Stephen</first><last>Pulman</last></author>
      <author><first>Atish</first><last>Kothari</last></author>
      <author><first>Ray</first><last>Shen</last></author>
      <author><first>Yinying</first><last>Pan</last></author>
      <author><first>Vivek</first><last>Kaul</last></author>
      <author><first>Mubarak</first><last>Seyed Ibrahim</last></author>
      <author><first>Gang</first><last>Xiang</last></author>
      <author><first>Nan</first><last>Dun</last></author>
      <author><first>Yidan</first><last>Zhou</last></author>
      <author><first>Andy</first><last>O</last></author>
      <author><first>Yuan</first><last>Zhang</last></author>
      <author><first>Pooja</first><last>Chitkara</last></author>
      <author><first>Xuan</first><last>Wang</last></author>
      <author><first>Alkesh</first><last>Patel</last></author>
      <author><first>Kushal</first><last>Tayal</last></author>
      <author><first>Roger</first><last>Zheng</last></author>
      <author><first>Peter</first><last>Grasch</last></author>
      <author><first>Jason D</first><last>Williams</last></author>
      <author><first>Lin</first><last>Li</last></author>
      <pages>196–204</pages>
      <abstract>Named Entity Recognition (NER) and Entity Linking (EL) play an essential role in voice assistant interaction, but are challenging due to the special difficulties associated with spoken user queries. In this paper, we propose a novel <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a> that jointly solves the NER and EL tasks by combining them in a joint reranking module. We show that our proposed <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> improves NER accuracy by up to 3.13 % and EL accuracy by up to 3.6 % in <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a>. The <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> used also lead to better accuracies in other natural language understanding tasks, such as domain classification and <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a>.</abstract>
      <url hash="eab40b1b">2021.naacl-industry.25</url>
      <doi>10.18653/v1/2021.naacl-industry.25</doi>
      <bibkey>muralidharan-etal-2021-noise</bibkey>
    </paper>
    <paper id="27">
      <title>Intent Features for Rich Natural Language Understanding</title>
      <author><first>Brian</first><last>Lester</last></author>
      <author><first>Sagnik</first><last>Ray Choudhury</last></author>
      <author><first>Rashmi</first><last>Prasad</last></author>
      <author><first>Srinivas</first><last>Bangalore</last></author>
      <pages>214–221</pages>
      <abstract>Complex natural language understanding modules in dialog systems have a richer understanding of user utterances, and thus are critical in providing a better user experience. However, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are often created from scratch, for specific clients and use cases and require the annotation of large datasets. This encourages the sharing of annotated data across multiple clients. To facilitate this we introduce the idea of intent features : domain and topic agnostic properties of intents that can be learnt from the syntactic cues only, and hence can be shared. We introduce a new neural network architecture, the Global-Local model, that shows significant improvement over strong baselines for identifying these features in a deployed, multi-intent natural language understanding module, and more generally in a classification setting where a part of an utterance has to be classified utilizing the whole context.<i>intent features</i>: domain and topic agnostic properties of intents that can be learnt from the syntactic cues only, and hence can be shared. We introduce a new neural network architecture, the Global-Local model, that shows significant improvement over strong baselines for identifying these features in a deployed, multi-intent natural language understanding module, and more generally in a classification setting where a part of an utterance has to be classified utilizing the whole context.</abstract>
      <url hash="3e7d08e7">2021.naacl-industry.27</url>
      <doi>10.18653/v1/2021.naacl-industry.27</doi>
      <bibkey>lester-etal-2021-intent</bibkey>
      <pwccode url="https://github.com/blester125/global-local-model" additional="false">blester125/global-local-model</pwccode>
    </paper>
    <paper id="33">
      <title>Ad Headline Generation using Self-Critical Masked Language Model</title>
      <author><first>Yashal Shakti</first><last>Kanungo</last></author>
      <author><first>Sumit</first><last>Negi</last></author>
      <author><first>Aruna</first><last>Rajan</last></author>
      <pages>263–271</pages>
      <abstract>For any <a href="https://en.wikipedia.org/wiki/E-commerce">E-commerce website</a> it is a nontrivial problem to build enduring advertisements that attract shoppers. It is hard to pass the creative quality bar of the website, especially at a large scale. We thus propose a programmatic solution to generate product advertising headlines using retail content. We propose a state of the art application of Reinforcement Learning (RL) Policy gradient methods on Transformer (Vaswani et al., 2017) based Masked Language Models (Devlin et al., 2019). Our method creates the advertising headline by jointly conditioning on multiple products that a seller wishes to advertise. We demonstrate that our method outperforms existing Transformer and LSTM + RL methods in overlap metrics and quality audits. We also show that our model generated headlines outperform human submitted headlines in terms of both <a href="https://en.wikipedia.org/wiki/Grammar">grammar</a> and creative quality as determined by audits.</abstract>
      <url hash="6cc38f5b">2021.naacl-industry.33</url>
      <doi>10.18653/v1/2021.naacl-industry.33</doi>
      <bibkey>kanungo-etal-2021-ad</bibkey>
    </paper>
    <paper id="34">
      <title>LATEX-Numeric : Language Agnostic Text Attribute Extraction for Numeric Attributes<fixed-case>LATEX</fixed-case>-Numeric: Language Agnostic Text Attribute Extraction for Numeric Attributes</title>
      <author><first>Kartik</first><last>Mehta</last></author>
      <author><first>Ioana</first><last>Oprea</last></author>
      <author><first>Nikhil</first><last>Rasiwasia</last></author>
      <pages>272–279</pages>
      <abstract>In this paper, we present LATEX-Numeric-a high-precision fully-automated scalable framework for extracting E-commerce numeric attributes from unstructured product text like product description. Most of the past work on attribute extraction is not scalable as they rely on manually curated training data, either with or without use of <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)">active learning</a>. We rely on distant supervision for <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data generation</a>, removing dependency on manual labels. One issue with distant supervision is that it leads to incomplete training annotation due to missing attribute values while <a href="https://en.wikipedia.org/wiki/Matching_(graph_theory)">matching</a>. We propose a multi-task learning architecture to deal with missing labels in the training data, leading to <a href="https://en.wikipedia.org/wiki/F-number">F1 improvement</a> of 9.2 % for numeric attributes over state-of-the-art single-task architecture. While multi-task architecture benefits both numeric and non-numeric attributes, we present automated techniques to further improve the numeric attributes extraction models. Numeric attributes require a list of units (or aliases) for better matching with distant supervision. We propose an automated algorithm for alias creation using <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured text</a> and <a href="https://en.wikipedia.org/wiki/Attribute_(computing)">attribute values</a>, leading to a 20.2 % <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">F1 improvement</a>. Extensive experiments on real world datasets for 20 numeric attributes across 5 product categories and 3 English marketplaces show that LATEX-numeric achieves a high F1-score, without any manual intervention, making it suitable for practical applications. Finally we show that the improvements are language-agnostic and LATEX-Numeric achieves 13.9 % F1 improvement for 3 non-English languages.</abstract>
      <url hash="7c0cc9e4">2021.naacl-industry.34</url>
      <doi>10.18653/v1/2021.naacl-industry.34</doi>
      <bibkey>mehta-etal-2021-latex</bibkey>
    </paper>
    <paper id="35">
      <title>Training Language Models under Resource Constraints for Adversarial Advertisement Detection</title>
      <author><first>Eshwar</first><last>Shamanna Girishekar</last></author>
      <author><first>Shiv</first><last>Surya</last></author>
      <author><first>Nishant</first><last>Nikhil</last></author>
      <author><first>Dyut Kumar</first><last>Sil</last></author>
      <author><first>Sumit</first><last>Negi</last></author>
      <author><first>Aruna</first><last>Rajan</last></author>
      <pages>280–287</pages>
      <abstract>Advertising on <a href="https://en.wikipedia.org/wiki/E-commerce">e-commerce</a> and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning and multi-lingual training can be applied effectively to fine-tune BERT and its variants for text classification tasks in conjunction with different data augmentation strategies. Our extensive experiments on multiple languages show that these techniques detect adversarial ad categories with a substantial gain in precision at high recall threshold over the baseline.</abstract>
      <url hash="85c1f1e5">2021.naacl-industry.35</url>
      <doi>10.18653/v1/2021.naacl-industry.35</doi>
      <bibkey>shamanna-girishekar-etal-2021-training</bibkey>
    </paper>
    <paper id="39">
      <title>Industry Scale Semi-Supervised Learning for Natural Language Understanding</title>
      <author><first>Luoxin</first><last>Chen</last></author>
      <author><first>Francisco</first><last>Garcia</last></author>
      <author><first>Varun</first><last>Kumar</last></author>
      <author><first>He</first><last>Xie</last></author>
      <author><first>Jianhua</first><last>Lu</last></author>
      <pages>311–318</pages>
      <abstract>This paper presents a production Semi-Supervised Learning (SSL) pipeline based on the student-teacher framework, which leverages millions of unlabeled examples to improve Natural Language Understanding (NLU) tasks. We investigate two questions related to the use of unlabeled data in production SSL context : 1) how to select samples from a huge unlabeled data pool that are beneficial for SSL training, and 2) how does the selected data affect the performance of different state-of-the-art SSL techniques. We compare four widely used SSL techniques, Pseudo-label (PL), Knowledge Distillation (KD), Virtual Adversarial Training (VAT) and Cross-View Training (CVT) in conjunction with two data selection methods including committee-based selection and submodular optimization based selection. We further examine the benefits and drawbacks of these techniques when applied to intent classification (IC) and named entity recognition (NER) tasks, and provide guidelines specifying when each of these methods might be beneficial to improve large scale NLU systems.</abstract>
      <url hash="ce571662">2021.naacl-industry.39</url>
      <doi>10.18653/v1/2021.naacl-industry.39</doi>
      <bibkey>chen-etal-2021-industry</bibkey>
    </paper>
  </volume>
</collection>