<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.conll">
  <volume id="1" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the 25th Conference on Computational Natural Language Learning</booktitle>
      <editor><first>Arianna</first><last>Bisazza</last></editor>
      <editor><first>Omri</first><last>Abend</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="a00cd227">2021.conll-1.0</url>
      <bibkey>conll-2021-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>It’s our fault ! : Insights Into Users’ Understanding and Interaction With an Explanatory Collaborative Dialog System</title>
      <author><first>Katharina</first><last>Weitz</last></author>
      <author><first>Lindsey</first><last>Vanderlyn</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <author><first>Elisabeth</first><last>André</last></author>
      <pages>1–16</pages>
      <abstract>Human-AI collaboration, a long standing goal in <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI</a>, refers to a partnership where a human and artificial intelligence work together towards a shared goal. Collaborative dialog allows human-AI teams to communicate and leverage strengths from both partners. To design collaborative dialog systems, it is important to understand what mental models users form about their AI-dialog partners, however, how users perceive these <a href="https://en.wikipedia.org/wiki/System">systems</a> is not fully understood. In this study, we designed a novel, collaborative, communication-based puzzle game and explanatory dialog system. We created a public corpus from 117 conversations and post-surveys and used this to analyze what <a href="https://en.wikipedia.org/wiki/Mental_model">mental models</a> users formed. Key takeaways include : Even when users were not engaged in the <a href="https://en.wikipedia.org/wiki/Game">game</a>, they perceived the AI-dialog partner as intelligent and likeable, implying they saw it as a partner separate from the game. This was further supported by users often overestimating the <a href="https://en.wikipedia.org/wiki/System">system</a>’s abilities and projecting human-like attributes which led to miscommunications. We conclude that creating shared mental models between users and <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI systems</a> is important to achieving successful dialogs. We propose that our insights on mental models and miscommunication, the <a href="https://en.wikipedia.org/wiki/Game">game</a>, and our <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> provide useful tools for designing collaborative dialog systems.</abstract>
      <url hash="25e22930">2021.conll-1.1</url>
      <bibkey>weitz-etal-2021-fault</bibkey>
      <doi>10.18653/v1/2021.conll-1.1</doi>
    </paper>
    <paper id="5">
      <title>On <a href="https://en.wikipedia.org/wiki/Language_model">Language Models</a> for Creoles</title>
      <author><first>Heather</first><last>Lent</last></author>
      <author><first>Emanuele</first><last>Bugliarello</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Chen</first><last>Qiu</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>58–71</pages>
      <abstract>Creole languages such as <a href="https://en.wikipedia.org/wiki/Nigerian_Pidgin_English">Nigerian Pidgin English</a> and <a href="https://en.wikipedia.org/wiki/Haitian_Creole">Haitian Creole</a> are under-resourced and largely ignored in the NLP literature. Creoles typically result from the fusion of a foreign language with multiple local languages, and what grammatical and lexical features are transferred to the <a href="https://en.wikipedia.org/wiki/Creole_language">creole</a> is a complex process. While <a href="https://en.wikipedia.org/wiki/Creole_language">creoles</a> are generally stable, the prominence of some features may be much stronger with certain demographics or in some linguistic situations. This paper makes several contributions : We collect existing corpora and release models for <a href="https://en.wikipedia.org/wiki/Haitian_Creole">Haitian Creole</a>, <a href="https://en.wikipedia.org/wiki/Nigerian_Pidgin_English">Nigerian Pidgin English</a>, and <a href="https://en.wikipedia.org/wiki/Singaporean_English">Singaporean Colloquial English</a>. We evaluate these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on intrinsic and extrinsic tasks. Motivated by the above literature, we compare standard language models with distributionally robust ones and find that, somewhat surprisingly, the standard language models are superior to the distributionally robust ones. We investigate whether this is an effect of <a href="https://en.wikipedia.org/wiki/Parameterized_complexity">over-parameterization</a> or relative distributional stability, and find that the difference persists in the absence of <a href="https://en.wikipedia.org/wiki/Parameterized_complexity">over-parameterization</a>, and that drift is limited, confirming the relative stability of <a href="https://en.wikipedia.org/wiki/Creole_language">creole languages</a>.</abstract>
      <url hash="32c6b62b">2021.conll-1.5</url>
      <bibkey>lent-etal-2021-language</bibkey>
      <doi>10.18653/v1/2021.conll-1.5</doi>
      <pwccode url="https://github.com/hclent/creole-dro" additional="false">hclent/creole-dro</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wilds">Wilds</pwcdataset>
    </paper>
    <paper id="11">
      <title>Enriching Language Models with Visually-grounded Word Vectors and the Lancaster Sensorimotor Norms<fixed-case>L</fixed-case>ancaster Sensorimotor Norms</title>
      <author><first>Casey</first><last>Kennington</last></author>
      <pages>148–157</pages>
      <abstract>Language models are trained only on text despite the fact that humans learn their first language in a highly interactive and multimodal environment where the first set of learned words are largely concrete, denoting physical entities and embodied states. To enrich language models with some of this missing experience, we leverage two sources of information : (1) the Lancaster Sensorimotor norms, which provide ratings (means and standard deviations) for over 40,000 English words along several dimensions of embodiment, and which capture the extent to which something is experienced across 11 different sensory modalities, and (2) vectors from coefficients of binary classifiers trained on images for the BERT vocabulary. We pre-trained the ELECTRA model and fine-tuned the RoBERTa model with these two sources of information then evaluate using the established GLUE benchmark and the Visual Dialog benchmark. We find that enriching <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> with the Lancaster norms and image vectors improves results in both tasks, with some implications for robust <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> that capture holistic linguistic meaning in a language learning context.</abstract>
      <url hash="18495466">2021.conll-1.11</url>
      <bibkey>kennington-2021-enriching</bibkey>
      <doi>10.18653/v1/2021.conll-1.11</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visdial">VisDial</pwcdataset>
    </paper>
    <paper id="15">
      <title>Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction</title>
      <author><first>Shauli</first><last>Ravfogel</last></author>
      <author><first>Grusha</first><last>Prasad</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>194–209</pages>
      <abstract>When language models process syntactically complex sentences, do they use their representations of syntax in a manner that is consistent with the grammar of the language? We propose AlterRep, an intervention-based method to address this question. For any linguistic feature of a given sentence, AlterRep generates counterfactual representations by altering how the <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">feature</a> is encoded, while leaving in- tact all other aspects of the original <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a>. By measuring the change in a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>’s word prediction behavior when these counterfactual representations are substituted for the original ones, we can draw conclusions about the causal effect of the linguistic feature in question on the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>’s behavior. We apply this method to study how BERT models of different sizes process relative clauses (RCs). We find that BERT variants use RC boundary information during <a href="https://en.wikipedia.org/wiki/Word_prediction">word prediction</a> in a manner that is consistent with the rules of English grammar ; this RC boundary information generalizes to a considerable extent across different RC types, suggesting that BERT represents RCs as an abstract linguistic category.</abstract>
      <url hash="bcd5f605">2021.conll-1.15</url>
      <bibkey>ravfogel-etal-2021-counterfactual</bibkey>
      <doi>10.18653/v1/2021.conll-1.15</doi>
    </paper>
    <paper id="16">
      <title>Who’s on First? : Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains</title>
      <author><first>David</first><last>Demeter</last></author>
      <author><first>Doug</first><last>Downey</last></author>
      <pages>210–222</pages>
      <abstract>The capabilities of today’s <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing systems</a> are typically evaluated using large datasets of curated questions and answers. While these are critical benchmarks of progress, they also suffer from weakness due to <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">artificial distributions</a> and <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">incomplete knowledge</a>. Artifacts arising from artificial distributions can overstate <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> performance, while incomplete knowledge limits fine-grained analysis. In this work, we introduce a complementary benchmarking approach based on SimPlified Language Activity Traces (SPLAT). SPLATs are corpora of language encodings of activity in some closed domain (we study traces from chess and baseball games in this work). SPLAT datasets use naturally-arising distributions, allow the generation of question-answer pairs at scale, and afford complete knowledge in their closed domains. We show that <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> of three different architectures can answer questions about <a href="https://en.wikipedia.org/wiki/State_(polity)">world states</a> using only verb-like encodings of activity. Our approach is extensible to new <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> and additional question-answering tasks.</abstract>
      <url hash="51efaf81">2021.conll-1.16</url>
      <bibkey>demeter-downey-2021-whos</bibkey>
      <doi>10.18653/v1/2021.conll-1.16</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/babi-1">bAbI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/decanlp">decaNLP</pwcdataset>
    <title_ar>من هو أولاً؟: استكشاف قدرات التعلم والتمثيل لنماذج اللغة في المجالات المحددة المغلقة</title_ar>
      <title_fr>Qui est sur First ? : Sondage des capacités d'apprentissage et de représentation des modèles linguistiques dans des domaines fermés déterministes</title_fr>
      <title_es>¿Quién sale primero? : Explorando las capacidades de aprendizaje y representación de los modelos lingüísticos en dominios cerrados deterministas</title_es>
      <title_pt>Quem está em primeiro lugar?: Provando as capacidades de aprendizagem e representação de modelos de linguagem em domínios fechados determinísticos</title_pt>
      <title_zh>孰先之?确定性封域上索言学以见能</title_zh>
      <title_ja>ファーストは誰だ？ ：決定論的クローズドドメイン上の言語モデルの学習および表現能力の探究</title_ja>
      <title_ru>Кто на "Первом"?: Исследование возможностей изучения и представления языковых моделей в детерминированных закрытых доменах</title_ru>
      <title_hi>पहले कौन है?: नियतात्मक बंद डोमेन पर भाषा मॉडल की सीखने और प्रतिनिधित्व क्षमताओं की जांच करना</title_hi>
      <title_ga>Cé atá ar Thús</title_ga>
      <title_ka>ვინ პირველად არის? სწავლება და გამოსახულება ენის მოდელების შესაძლებლობა დეტერმინისტიკური დახურებული დიომენების შესაძლებლობა</title_ka>
      <title_hu>Ki az első? A nyelvi modellek tanulási és reprezentációs képességeinek vizsgálata determinisztikus zárt tartományokon</title_hu>
      <title_el>Ποιος είναι στο Πρώτο; Έλεγχος των δυνατοτήτων εκμάθησης και αναπαράστασης των γλωσσικών μοντέλων σε ντετερμινιστικούς κλειστούς τομείς</title_el>
      <title_it>Chi è al primo? Analisi delle capacità di apprendimento e rappresentazione dei modelli linguistici su domini chiusi deterministici</title_it>
      <title_kk>Біріншіден кім бар? Дефинистикалық жабылған домендердің тіл үлгілерінің оқыту және таңдау мүмкіндігін тексеру</title_kk>
      <title_mk>Who's on First?:  Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains</title_mk>
      <title_ml>Who's on First?:  ഭാഷ മോഡലുകളുടെ പഠനത്തിന്റെയും പ്രതിനിധികളുടെയും ശക്തികള്‍ പരിശോധിക്കുന്നു</title_ml>
      <title_mn>Эхлээд хэн байна вэ? Тодорхойлолтын хаягдсан хэл загварын суралцах болон төлөөлөх боломжуудыг судалж,</title_mn>
      <title_no>Kva er på første? Å prøve å lære og reprezentasjonskapasiteten for språk-modeller på definerte lukka domene</title_no>
      <title_pl>Kto jest na pierwszym? Badanie możliwości uczenia się i reprezentacji modeli językowych na deterministycznych domenach zamkniętych</title_pl>
      <title_mt>Min huwa fuq l-Ewwel?:  L-ittestjar tal-Kapaċitajiet ta’ Tagħlim u Rappreżentanza tal-Mudelli tal-Lingwi fuq Domenijiet Deterministiċi magħluqa</title_mt>
      <title_lt>Kas pirmas? Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains</title_lt>
      <title_sr>Ko je na prvom? Verovatno je mogućnost učenja i predstavljanja jezičkih modela na određenim zatvorenim domenama</title_sr>
      <title_ms>Siapa yang pertama? Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains</title_ms>
      <title_si>කවුද පළවෙනි ඉන්නේ? විශ්වාසික වහලා තියෙන්නේ භාෂාව මොඩේන්ස් වලට ඉගෙන හා ප්‍රතිස්ථාපනය සක්ෂමතා</title_si>
      <title_sv>Vem är på First? Undersökning av inlärnings- och representationsförmågan hos språkmodeller på deterministiska slutna domäner</title_sv>
      <title_ro>Cine e pe primul? Proiectarea capacităților de învățare și reprezentare a modelelor lingvistice pe domenii deterministe închise</title_ro>
      <title_so>Yaa ugu horeeya?: Horumarinta awoodda Barshada iyo Representation Models of Language on Deterministic Closed Domains</title_so>
      <title_ta>யார் முதலில் இருக்கிறார்? மொழி மோடத்தின் படிப்புகள் மற்றும் பிரதிநிதிப்பு மூடிய கூட்டத்திற்கு முன்னிருப்பு படிப்புகள் மற்றும் த</title_ta>
      <title_ur>پہلے کون ہے؟ تعریف دینے والی ڈومین پر زبان مدل کی تعلیم اور نمایش کے قابلیت کی تصدیق کرنے کی</title_ur>
      <title_uz>Birinchi kim?: Name</title_uz>
      <title_vi>Ai chơi First? Đang phát triển khả năng học hỏi và phát triển ngôn ngữ mẫu trên các địa bàn tủ xác định</title_vi>
      <title_nl>Wie zit er op First? Onderzoek van de leer- en representatievermogen van taalmodellen op deterministische gesloten domeinen</title_nl>
      <title_da>Hvem er på første? Undersøgelse af sprogmodels lærings- og repræsentationsevner på deterministiske lukkede domæner</title_da>
      <title_bg>Кой е на първа линия? Проучване на възможностите за учене и представителство на езиковите модели върху детерминистични затворени домейни</title_bg>
      <title_hr>Tko je na prvom? Vjerojatnost mogućnosti učenja i predstavljanja jezičkih modela na određenim zatvorenim domenama</title_hr>
      <title_fa>اول کي هست؟ شاید توانایی یادگیری و نمایش نمایش مدل زبان در دامنهای بسته‌شده‌ی تعیین‌کننده</title_fa>
      <title_de>Wer ist auf First?: Untersuchung der Lern- und Repr瓣sentationsf瓣higkeit von Sprachmodellen auf deterministischen geschlossenen Dom瓣nen</title_de>
      <title_ko>누가 먼저 올라갑니까?확정적 폐쇄역상 언어 모델의 학습과 표현 능력을 탐색하다</title_ko>
      <title_sw>Nani yuko kwanza?: Kuonyesha uwezo wa Kufundisha na Kuwakilishwa kwa Modeli za Lugha Kuhusu Makazi Zifungwa</title_sw>
      <title_tr>Ilkinji adam? Däklemeler barada Diller öwrenmek we Görkezilişim Jakynlama Kiçimleri</title_tr>
      <title_sq>Kush është në fillim?: Prova e aftësive të mësimit dhe përfaqësimit të modeleve gjuhësore në domenet e mbyllura determinative</title_sq>
      <title_af>Wie is op eerste? Probeer die Leer en Voorstelling Kapabiliteit van Taal Modelle op Deterministiese Gesluit Domeine</title_af>
      <title_am>መጀመሪያ ማን ነው? የቋንቋ ሞዴል ማስተማር እና ማስታወቂያ ስልጣናት በማስታወቂያ ዝጋ ባለው አዲስ ዶሞዎች ላይ</title_am>
      <title_id>Siapa yang pertama?: Mencoba Kemampuan Belajar dan Representasi Model Bahasa pada Domain Tertutup Deterministik</title_id>
      <title_hy>Ո՞վ է առաջին հատվածում: Լեզվային մոդելների սովորելու և ներկայացման հնարավորությունների փորձարկումը որոշող փակված վայրերում</title_hy>
      <title_az>İlk başında kim var? Deterministic Closed Domains Üstündə Dil Modellərinin Öyrənməsi və Təşkil Mümkünlükləri</title_az>
      <title_bn>কে প্রথমে? ভাষা মোডেলের শিক্ষা ও প্রতিনিধিত্বের ক্ষমতা প্রমাণ করা হচ্ছে</title_bn>
      <title_bs>Ko je na prvom mjestu? Vjerojatnost mogućnosti učenja i predstavljanja jezičkih modela na određenim zatvorenim domenama</title_bs>
      <title_et>Kes on esimesel? Keelemudelite õppimis- ja esindamisvõime uurimine deterministlikel suletud domeenidel</title_et>
      <title_cs>Kdo je na prvním? Snímání učebních a reprezentačních schopností jazykových modelů na deterministických uzavřených doménách</title_cs>
      <title_ca>Qui està en primer lloc? Probar les capacitats d'aprenentatge i representació dels models de llenguatge en dominis cerrats determinants</title_ca>
      <title_fi>Kuka on ykkösellä? Kielimallien oppimis- ja edustamiskyvyn kartoittaminen deterministisillä suljetuilla toimialueilla</title_fi>
      <title_jv>Piye susahke tho ? Jejaring</title_jv>
      <title_he>מי בראשונה? Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains</title_he>
      <title_ha>Wãne ne na farko?: KCharselect unicode block name</title_ha>
      <title_sk>Kdo je na prvem? Prodiranje učnih in reprezentativnih zmogljivosti jezikovnih modelov na determinističnih zaprtih domenah</title_sk>
      <title_bo>སྔ་འཛིན་གྱི་ནང་དུ་ག་རེ་ཡིན་ནམ། Deterministic Closed Domains</title_bo>
      <abstract_ar>عادةً ما يتم تقييم قدرات أنظمة معالجة اللغة الطبيعية الحالية باستخدام مجموعات بيانات كبيرة من الأسئلة والأجوبة المنسقة. في حين أن هذه معايير مهمة للتقدم ، إلا أنها تعاني أيضًا من الضعف بسبب التوزيعات الاصطناعية والمعرفة غير الكاملة. يمكن للقطع الأثرية الناشئة عن التوزيعات الاصطناعية أن تبالغ في أداء نموذج اللغة ، بينما تحد المعرفة غير الكاملة من التحليل الدقيق. في هذا العمل ، نقدم نهجًا تكميليًا لقياس الأداء يعتمد على تتبع نشاط اللغة المبسطة (SPLAT). SPLATs هي مجموعة من ترميزات اللغة للنشاط في بعض المجالات المغلقة (ندرس الآثار من ألعاب الشطرنج والبيسبول في هذا العمل). تستخدم مجموعات بيانات SPLAT توزيعات تنشأ بشكل طبيعي ، وتسمح بتوليد أزواج من الأسئلة والأجوبة على نطاق واسع ، وتوفر معرفة كاملة في مجالاتها المغلقة. نظهر أن النماذج اللغوية لثلاث معماريات مختلفة يمكنها الإجابة على أسئلة حول حالات العالم باستخدام ترميز النشاط الشبيه بالأفعال فقط. نهجنا قابل للتوسيع لنماذج اللغة الجديدة ومهام الإجابة على الأسئلة الإضافية.</abstract_ar>
      <abstract_fr>Les capacités des systèmes de traitement du langage naturel actuels sont généralement évaluées à l'aide de grands ensembles de données de questions et réponses organisées. Bien qu'il s'agisse de repères critiques de progrès, ils souffrent également de faiblesses dues à des distributions artificielles et à des connaissances incomplètes. Les artefacts issus de distributions artificielles peuvent surestimer les performances des modèles de langage, tandis que des connaissances incomplètes limitent les analyses fines. Dans ce travail, nous introduisons une approche d'analyse comparative complémentaire basée sur les traces d'activité linguistique simplifiées (SPLAT). Les SPLT sont des corpus d'encodages de langage d'activité dans un domaine fermé (nous étudions les traces de parties d'échecs et de baseball dans cet ouvrage). Les ensembles de données SPLAT utilisent des distributions naturelles, permettent la génération de paires question-réponse à grande échelle et fournissent des connaissances complètes dans leurs domaines fermés. Nous montrons que les modèles de langage de trois architectures différentes peuvent répondre à des questions sur les états du monde en utilisant uniquement des encodages d'activité de type verbe. Notre approche est extensible à de nouveaux modèles linguistiques et à des tâches supplémentaires de réponse aux questions.</abstract_fr>
      <abstract_es>Las capacidades de los sistemas actuales de procesamiento del lenguaje natural se evalúan normalmente mediante grandes conjuntos de datos de preguntas y respuestas seleccionadas. Si bien estos son puntos de referencia críticos para el progreso, también sufren debilidad debido a las distribuciones artificiales y al conocimiento incompleto. Los artefactos que surgen de distribuciones artificiales pueden exagerar el rendimiento del modelo lingüístico, mientras que el conocimiento incompleto limita el análisis detallado. En este trabajo, introducimos un enfoque de evaluación comparativa complementario basado en SimpliFed Language Activity Traces (SPLAT). Los SPLAT son corpus de codificaciones lingüísticas de la actividad en algún dominio cerrado (en este trabajo estudiamos las huellas de los juegos de ajedrez y béisbol). Los conjuntos de datos SPLAT utilizan distribuciones naturales, permiten la generación de pares de preguntas y respuestas a escala y ofrecen un conocimiento completo en sus dominios cerrados. Mostramos que los modelos lingüísticos de tres arquitecturas diferentes pueden responder preguntas sobre los estados del mundo utilizando solo codificaciones de actividad verbosas. Nuestro enfoque es extensible a los nuevos modelos lingüísticos y a las tareas adicionales de respuesta a preguntas.</abstract_es>
      <abstract_pt>As capacidades dos atuais sistemas de processamento de linguagem natural são normalmente avaliadas usando grandes conjuntos de dados de perguntas e respostas selecionadas. Embora sejam referências críticas de progresso, elas também sofrem de fraqueza devido a distribuições artificiais e conhecimento incompleto. Artefatos decorrentes de distribuições artificiais podem exagerar o desempenho do modelo de linguagem, enquanto o conhecimento incompleto limita a análise refinada. Neste trabalho, apresentamos uma abordagem complementar de benchmarking baseada em SimPlified Language Activity Traces (SPLAT). SPLATs são corpora de codificações linguísticas de atividade em algum domínio fechado (nós estudamos traços de jogos de xadrez e beisebol neste trabalho). Os conjuntos de dados SPLAT usam distribuições que surgem naturalmente, permitem a geração de pares pergunta-resposta em escala e proporcionam conhecimento completo em seus domínios fechados. Mostramos que modelos de linguagem de três arquiteturas diferentes podem responder perguntas sobre estados do mundo usando apenas codificações de atividade do tipo verbo. Nossa abordagem é extensível a novos modelos de linguagem e tarefas adicionais de resposta a perguntas.</abstract_pt>
      <abstract_ja>今日の自然言語処理システムの能力は、通常、キュレーションされた質問と回答の大きなデータセットを使用して評価されます。 これらは進歩の重要なベンチマークですが、人工的な分布と不完全な知識に起因する弱点もあります。 人工的な分布から生じるアーティファクトは、言語モデルのパフォーマンスを過大評価する可能性があり、不完全な知識は細かい分析を制限します。 この研究では、SimPlified Language Activity Trace （ SPLAT ）に基づいた補完的なベンチマークアプローチを紹介します。 SPLATは、いくつかのクローズドドメインでのアクティビティの言語エンコーディングのコーパです（本作では、チェスや野球ゲームからのトレースを研究しています）。 SPLATデータセットは、自然に成長する分布を使用し、大規模な質問応答ペアの生成を可能にし、クローズドドメインで完全な知識を提供します。 3つの異なるアーキテクチャの言語モデルは、動詞のようなアクティビティエンコーディングのみを使用して、世界の状態に関する質問に答えることができることを示しています。 私たちのアプローチは、新しい言語モデルと追加の質問に答えるタスクに拡張可能です。</abstract_ja>
      <abstract_hi>आज की प्राकृतिक भाषा प्रसंस्करण प्रणालियों की क्षमताओं का मूल्यांकन आमतौर पर क्यूरेट किए गए प्रश्नों और उत्तरों के बड़े डेटासेट का उपयोग करके किया जाता है। जबकि ये प्रगति के महत्वपूर्ण बेंचमार्क हैं, वे कृत्रिम वितरण और अधूरे ज्ञान के कारण कमजोरी से भी पीड़ित हैं। कृत्रिम वितरण से उत्पन्न होने वाली कलाकृतियां भाषा मॉडल के प्रदर्शन को ओवरस्टेट कर सकती हैं, जबकि अपूर्ण ज्ञान ठीक-ठाक विश्लेषण को सीमित करता है। इस काम में, हम SimPlified Language Activity Traces (SPLAT) के आधार पर एक पूरक बेंचमार्किंग दृष्टिकोण पेश करते हैं। SPLATs कुछ बंद डोमेन में गतिविधि की भाषा एन्कोडिंग के निगम हैं (हम इस काम में शतरंज और बेसबॉल गेम से निशान का अध्ययन करते हैं)। SPLAT डेटासेट स्वाभाविक रूप से उत्पन्न होने वाले वितरण का उपयोग करते हैं, पैमाने पर प्रश्न-उत्तर जोड़े की पीढ़ी की अनुमति देते हैं, और अपने बंद डोमेन में पूर्ण ज्ञान प्रदान करते हैं। हम दिखाते हैं कि तीन अलग-अलग आर्किटेक्चर के भाषा मॉडल गतिविधि के केवल क्रिया-जैसे एन्कोडिंग का उपयोग करके दुनिया के राज्यों के बारे में सवालों के जवाब दे सकते हैं। हमारा दृष्टिकोण नई भाषा मॉडल और अतिरिक्त प्रश्न-उत्तर देने वाले कार्यों के लिए एक्सटेंसिबल है।</abstract_hi>
      <abstract_zh>今自然语言治统之功,常用大选及对案数集评之。 虽然进步之要,由人为布列不完之知,亦有弱点。 由人工分布者伪影或增大言模之性,而不全者限细粒度析。 于此等事,引入一基于简言(SPLAT)补准测试方法。 SPLAT封域之言编码语料库(究国际象棋棒球之迹)。 SPLAT数集用自然之布,许大生问对,并于封域全知。 吾明三架构之言,可以类动词编码以应天下。 我们的方法可以扩到新的言语模样和他问答。</abstract_zh>
      <abstract_ru>Возможности современных систем обработки естественного языка обычно оцениваются с использованием больших наборов курируемых вопросов и ответов. Хотя эти показатели являются критически важными для прогресса, они также страдают от слабости, обусловленной искусственным распределением и неполнотой знаний. Артефакты, возникающие из искусственных распределений, могут завышать производительность языковой модели, в то время как неполные знания ограничивают мелкозернистый анализ. В этой работе мы внедряем дополнительный подход к бенчмаркингу на основе SimPlified Language Activity Traces (SPLAT). SPLAT - это корпуса языковых кодировок активности в каком-то закрытом домене (в этой работе мы изучаем следы от шахматных и бейсбольных игр). Наборы данных SPLAT используют естественно возникающие распределения, позволяют генерировать пары вопросов и ответов в масштабе и позволяют получать полные знания в своих закрытых доменах. Мы показываем, что языковые модели трех различных архитектур могут отвечать на вопросы о мировых состояниях, используя только глаголоподобные кодировки активности. Наш подход применим к новым языковым моделям и дополнительным задачам с ответом на вопросы.</abstract_ru>
      <abstract_ga>Go hiondúil déantar cumais chórais phróiseála teanga nádúrtha an lae inniu a mheas agus úsáid á baint as tacair shonraí mhóra de cheisteanna agus de fhreagraí coimeádta. Cé gur tagarmharcanna criticiúla dul chun cinn iad seo, tá laige orthu freisin de bharr dáiltí saorga agus eolas neamhiomlán. Is féidir le déantúsáin a eascraíonn as dáiltí saorga feidhmíocht na samhla teanga a áibhéil, agus cuireann eolas neamhiomlán teorainn le hanailís mhionsonraithe. San obair seo, tugaimid isteach cur chuige tagarmharcála comhlántach bunaithe ar Rianta Gníomhaíochta Teanga Simplithe (SPLAT). Is corpasaí iad SPLATanna d’ionchóduithe teanga ar ghníomhaíocht i bhfearann dúnta áirithe (déanaimid staidéar ar rianta ó chluichí fichille agus baseball sa saothar seo). Úsáideann tacair sonraí SPLAT dáiltí a éiríonn go nádúrtha, ligeann siad do phéirí ceisteanna-freagra ar scála a ghiniúint, agus tugann siad eolas iomlán ina bhfearainn dhúnta. Léirímid gur féidir le samhlacha teanga de thrí ailtireacht dhifriúla ceisteanna a fhreagairt faoi stáit an domhain ag baint úsáide as ionchóduithe gníomhaíochta atá cosúil le briathra amháin. Tá ár gcur chuige fairsing do mhúnlaí nua teanga agus do thascanna breise freagartha ceisteanna.</abstract_ga>
      <abstract_el>Οι δυνατότητες των σημερινών συστημάτων επεξεργασίας φυσικής γλώσσας αξιολογούνται συνήθως χρησιμοποιώντας μεγάλα σύνολα δεδομένων επιλεγμένων ερωτήσεων και απαντήσεων. Μολονότι πρόκειται για κρίσιμα σημεία αναφοράς προόδου, υποφέρουν επίσης από αδυναμία λόγω τεχνητών διανομών και ελλιπών γνώσεων. Τα τεχνουργήματα που προκύπτουν από τεχνητές διανομές μπορούν να υπερβάλλουν την απόδοση του γλωσσικού μοντέλου, ενώ η ελλιπής γνώση περιορίζει τη λεπτόκοκκη ανάλυση. Στην εργασία αυτή, εισάγουμε μια συμπληρωματική προσέγγιση συγκριτικής αξιολόγησης βασισμένη σε ίχνη γλωσσικής δραστηριότητας (SPLAT). Τα SPLAT είναι σώματα γλωσσικών κωδικοποιήσεων δραστηριότητας σε κάποιο κλειστό τομέα (μελετάμε ίχνη από σκάκι και παιχνίδια μπέιζμπολ σε αυτή την εργασία). Τα σύνολα δεδομένων χρησιμοποιούν φυσικά αναδυόμενες διανομές, επιτρέπουν τη δημιουργία ζευγαριών ερώτησης-απάντησης σε κλίμακα και παρέχουν πλήρη γνώση στους κλειστούς τομείς τους. Δείχνουμε ότι τα γλωσσικά μοντέλα τριών διαφορετικών αρχιτεκτονικών μπορούν να απαντήσουν σε ερωτήσεις σχετικά με τις παγκόσμιες καταστάσεις χρησιμοποιώντας μόνο ρήμα κωδικοποιήσεις δραστηριότητας. Η προσέγγισή μας επεκτείνεται σε νέα γλωσσικά μοντέλα και πρόσθετες εργασίες απάντησης σε ερωτήσεις.</abstract_el>
      <abstract_hu>Napjaink természetes nyelvfeldolgozó rendszereinek képességeit jellemzően nagy adatkészletek segítségével értékelik. Bár ezek a fejlődés kritikus referenciaértékei, a mesterséges eloszlás és a hiányos ismeretek miatt is gyengeségben szenvednek. A mesterséges eloszlásokból származó tárgyak túlságosan becsülhetik a nyelvmodell teljesítményét, míg a hiányos ismeretek korlátozzák a finomszemcsés elemzést. Ebben a munkában egy kiegészítő benchmarking megközelítést vezetünk be a Simplified Language Activity Traces (SPLAT) alapján. Az SPLAT-ek egy zárt területen végzett tevékenység nyelvi kódolásának korpuszai (ebben a munkában sakk és baseball játékok nyomait tanulmányozzuk). Az SPLAT adatkészletek természetesen keletkező eloszlásokat használnak, lehetővé teszik a kérdés-válasz párok létrehozását nagyszabású léptékben, és teljes ismeretet biztosítanak zárt területeiken. Megmutatjuk, hogy három különböző architektúra nyelvi modelljei kizárólag igészerű aktivitási kódolásokkal válaszolhatnak a világállapotokkal kapcsolatos kérdésekre. Megközelítésünk kiterjeszthető az új nyelvi modellekre és további kérdésekre is.</abstract_hu>
      <abstract_ka>დღეს ნაირადი ენერგიის პროცესი სისტემის შესაძლებლობა ტიპოლურად გაუმუშავებულია, რომელიც გამოყენებული დიდი მონაცემების კითხვების და პასუხების გამოყენ მაგრამ ეს კრიტიკური პროგრესის კონქმიკური ბანქმარი, ისინი ასევე მსგავსიდან დაბრუნდება ხელსახური გაყოფილი და უკეთესი ცნობიდან. არტიფექტაქტები, რომლებიც არტიფექტიური გაყოფილებებიდან იქნება ენის მოდელის გამოსახულება, მაგრამ არსებული ცნობიერების განსახულებელი განსახულებელი ანალიზია. ამ სამუშაოში ჩვენ დავიყენებთ კომპლენტერიური ბენქმარიკაციის პროგრამა, რომელიც სიმპლეფიცირებული ენაქტივის ბენქმარიკაციის განსაზღვრებით (SPLAT). SPLAT არის რამდენიმე დახურებული დიომინში ენის კოდირების კოპორა (ჩვენ ამ სამუშაოში შაფსის და ბეიბოლური თამაშიდან შესწავლობთ). SPLAT მონაცემების კონფიგურაციები გამოყენებენ ნაირადი განსაზღვრებული განსაზღვრებები, შესაძლებელია კითხვა-პასუხის კონფიგურაციის განსაზღვრება მაგალითად და დასაწყებენ ჩვენ ჩვენ აჩვენებთ, რომ სამი განსხვავებული არქტიქტურების ენახური მოდელები შეუძლიათ მსოფლიო სტატიქტურების შესახებ მხოლოდ ვიყენებთ ვერბურ ჩვენი პროგორმაცია ახალი ენის მოდელისთვის და დამატებული კითხვების მისაღებისთვის უფრო დიდია.</abstract_ka>
      <abstract_it>Le capacità dei sistemi di elaborazione del linguaggio naturale odierni sono tipicamente valutate utilizzando grandi set di dati di domande e risposte curate. Sebbene questi siano parametri critici di progresso, soffrono anche di debolezza dovuta a distribuzioni artificiali e conoscenze incomplete. Gli artefatti derivanti da distribuzioni artificiali possono sopravvalutare le prestazioni del modello linguistico, mentre la conoscenza incompleta limita l'analisi a grana fine. In questo lavoro, introduciamo un approccio di benchmarking complementare basato su Tracce di attività linguistiche semplificate (SPLAT). Gli SPLAT sono corpora di codificazioni linguistiche di attività in qualche dominio chiuso (studiamo tracce di scacchi e partite di baseball in questo lavoro). I dataset SPLAT utilizzano distribuzioni naturali, consentono la generazione di coppie domanda-risposta su larga scala e offrono una conoscenza completa nei loro domini chiusi. Mostriamo che i modelli linguistici di tre diverse architetture possono rispondere alle domande sugli stati del mondo utilizzando solo codificazioni verbali di attività. Il nostro approccio è estensibile a nuovi modelli linguistici e ulteriori compiti di risposta alle domande.</abstract_it>
      <abstract_kk>Бүгін табиғи тілдерді өңдеу жүйелерінің мүмкіндіктері әдетте бұл көп деректер қорлары мен жауаптары қолданылады. Бұл жұмыс белгісінің критикалық бағдарламалары, сондай-ақ олар кәсіпшілік тарату және толық білім сияқты күліктерінен өтеді. Өлшемді тарату үшін келесі артефакттар тіл үлгісін көтеруге болады, білім толық шектері жақсы таратылған анализ шектерін шектеуге болады. Бұл жұмыста SimPlified Language Activity Tracks (SPLAT) негізінде біз қосымша белгілеу тәртібін келтіреміз. SPLAT - бір жабылған доменде тіл кодтамасының корпорасы (бұл жұмыстың шахт мен бейсбол ойындарының іздеулерін зерттейміз). SPLAT деректер қорлары табиғатты тарату үшін қолданылады, сұрақ- жауап қорларын масштабына құруға мүмкіндік береді, жабылған домендерінде толық білім береді. Біз үш әртүрлі архитектуралардың тіл үлгілері әлемдік күйлері туралы сұрақтарына жауап бере аламыз, тек белсендік кодтамасын қолдану үшін. Біздің тәсіліміз жаңа тіл үлгілеріне және қосымша сұрақ жауап беру тапсырмаларына кеңейтіледі.</abstract_kk>
      <abstract_mk>Капацитетите на денешните природни системи за обработување јазик се обично проценуваат со користење на големи податоци од курирани прашања и одговори. While these are critical benchmarks of progress, they also suffer from weakness due to artificial distributions and incomplete knowledge.  Уметничките факти кои се појавуваат од вештачките дистрибуции можат да ја преценат резултатот на јазичкиот модел, додека нецелосното знаење ја ограничува фината анализа. Во оваа работа, воведуваме комплементарен пристап на benchmarking базиран на SimPlified Language Activity Traces (SPLAT). СПЛАТ се корпора на јазичките кодирања на активноста во некоја затворена област (ние студираме траги од шах и бејзбол игри во оваа работа). СПЛАТ датотеки користат природни дистрибуции, овозможуваат генерација на парови прашања-одговори на скала и си дозволуваат целосно знаење во нивните затворени домени. Ние покажуваме дека јазичките модели од три различни архитектури можат да одговараат на прашања за светските држави користејќи само кодирање на активност на вид на гласници. Нашиот пристап е проширен за новите јазички модели и дополнителни задачи за одговор на прашања.</abstract_mk>
      <abstract_ms>Kemampuan sistem pemprosesan bahasa semulajadi hari ini biasanya diukur menggunakan set data besar soalan dan jawapan yang dikurasi. Walaupun ini merupakan benchmarks kritik kemajuan, mereka juga menderita kelemahan disebabkan distribusi buatan dan pengetahuan tidak lengkap. Artifakta yang muncul dari distribusi buatan boleh melebihi prestasi model bahasa, walaupun pengetahuan tidak lengkap mengawal analisis yang sempurna. Dalam kerja ini, kami memperkenalkan pendekatan benchmarking tambahan berdasarkan Trek Aktiviti Bahasa SimPlified (SPLAT). SPLAT adalah korpra pengekodan bahasa aktiviti dalam beberapa domain tertutup (kami mempelajari jejak dari permainan catur dan baseball dalam kerja ini). Set data SPLAT menggunakan distribusi yang muncul secara alami, membenarkan generasi pasangan soalan-jawapan pada skala, dan membenarkan pengetahuan lengkap dalam domain tertutup mereka. Kami menunjukkan bahawa model bahasa tiga arkitektur yang berbeza boleh menjawab soalan mengenai negara dunia hanya menggunakan pengekodan aktiviti seperti verb. Pendekatan kita boleh diperluaskan kepada model bahasa baru dan tugas sambungan soalan tambahan.</abstract_ms>
      <abstract_ml>ഇന്നുള്ള സ്വാഭാവിക ഭാഷ പ്രക്രിയശ്ചിത്രങ്ങളുടെ കഴിവ് സാധാരണ ചോദ്യങ്ങളും ഉത്തരങ്ങളും ഉപയോഗിച്ച് വലിയ ഡാറ്റാസറ്റ്  ഇതൊക്കെ പുരോഗതിയുടെ കാര്യങ്ങളാണെങ്കില്‍ കൃത്രിമ വിതരണങ്ങളും പൂര്‍ണ്ണമായ അറിവുകളും കാരണം അവര്‍ ദുര്‍ബലരാണ്. കൃത്രിമ വിഭാഗങ്ങളില്‍ നിന്നുള്ള ആര്‍ട്ടിഫാക്റ്റുകള്‍ ഈ പ്രവര്‍ത്തനത്തില്‍, നമ്മള്‍ സിമിപ്പ്ലെഡ് ഭാഷ പ്രവര്‍ത്തനങ്ങള്‍ അടിസ്ഥാനമായി ഒരു കൂടുതല്‍ ബെന്‍മെങ്കിങ്ങിങ് പ്രായോഗ്ര എസ്പിലാറ്റുകള്‍ ചില ഡൊമെയിനിലെ പ്രവര്‍ത്തനങ്ങളുടെ ഭാഷ കോര്‍പ്പോരാണ് (ചെസ്സില്‍ നിന്നും ബെസ്ബോള്‍ കളികളില്‍ നിന്നും നമ്മ SPLAT ഡേറ്റാസറ്റുകള്‍ സ്വാഭാവികമായ വിഭാഗങ്ങള്‍ ഉപയോഗിക്കുന്നു, ചോദ്യത്തിന്റെ ഉത്തരമുള്ള ജോട്ടുകളുടെ തലമുറയില്‍ അനു മൂന്നു വ്യത്യസ്ത്രീകങ്ങളുടെ ഭാഷ മോഡലുകള്‍ക്ക് ലോക രാജ്യങ്ങളെപ്പറ്റിയുള്ള ചോദ്യങ്ങള്‍ക്ക് ഉത്തരം നല്‍കാന്‍ കഴിയ പുതിയ ഭാഷ മോഡലുകള്‍ക്കും കൂടുതല്‍ ചോദ്യങ്ങള്‍ക്ക് ഉത്തരം നല്‍കുന്ന ജോലികള്‍ക്കും നമ്മുടെ പ്രായോഗ്</abstract_ml>
      <abstract_mt>Il-kapaċitajiet tas-sistemi naturali tal-ipproċessar tal-lingwi tal-lum huma tipikament evalwati bl-użu ta’ settijiet ta’ dejta kbar ta’ mistoqsijiet u tweġibiet ikkurati. Filwaqt li dawn huma punti ta’ riferiment kritiċi tal-progress, huma jsofru wkoll minn dgħufija minħabba distribuzzjonijiet artifiċjali u għarfien mhux komplut. L-oġġetti li jirriżultaw minn distribuzzjonijiet artifiċjali jistgħu jiskattaw wisq il-prestazzjoni tal-mudell lingwistiku, filwaqt li l-għarfien mhux komplut jillimita l-analiżi bir-reqqa. F’dan ix-xogħol, a ħna nintroduċu approċċ kumplimentari ta’ benchmarking ibbażat fuq Traċċi ta’ Attività Lingwistika SimPlifikata (SPLAT). L-SPLATs huma korpra ta’ kodifikazzjonijiet lingwistiċi ta’ attività f’xi dominju magħluq (nistudjaw traċċi minn logħob ta’ xaqq u baseball f’dan ix-xogħol). Is-settijiet tad-dejta SPLAT jużaw distribuzzjonijiet li jirriżultaw b’mod naturali, jippermettu l-ġenerazzjoni ta’ pari ta’ mistoqsijiet-tweġibiet fuq skala, u jippermettu għarfien sħiħ fl-oqsma magħluqa tagħhom. Aħna nuru li mudelli lingwistiċi ta’ tliet arkitetturi differenti jistgħu jwieġbu mistoqsijiet dwar l-istati dinjija bl-użu ta’ kodiċijiet ta’ attività li jixbħu l-verbi biss. Our approach is extensible to new language models and additional question-answering tasks.</abstract_mt>
      <abstract_lt>The capabilities of today's natural language processing systems are typically evaluated using large datasets of curated questions and answers.  Nors šie rodikliai yra esminiai pažangos rodikliai, jie taip pat patiria silpnumą dėl dirbtinio platinimo ir neišsamių žinių. Iš dirbtinio platinimo atsirandantys daiktai gali pernelyg įvertinti kalbos modelio veiksmingumą, o neišsamios žinios apriboja smulkių grūdų analizę. In this work, we introduce a complementary benchmarking approach based on SimPlified Language Activity Traces (SPLAT).  SPLAT yra tam tikroje uždaroje srityje veikiančių kalbų kodų korpora (šiame darbe tiriame šachmų ir beisbolo žaidimų pėdsakus). SPLAT duomenų rinkiniai naudoja natūraliai atsirandančius platinimus, leidžia kurti klausimų ir atsakymų poros mastu ir suteikia galimybę visapusiškai žinoti uždarose srityse. Mes rodome, kad trijų skirtingų architektūrų kalbiniai modeliai gali atsakyti į klausimus apie pasaulio valstybes tik naudojant į žodžius panašius veiklos kodus. Mūsų požiūris apima naujus kalbų modelius ir papildomas užduotis atsakyti į klausimus.</abstract_lt>
      <abstract_mn>Өнөөдөр байгалийн хэл үйлдвэрлэх системийн чадваруудыг ихэвчлэн асуулт болон хариултуудын том өгөгдлийн сангуудыг ашиглан үнэлдэг. Хэдийгээр эдгээр нь хөгжлийн чухал хэмжээсүүд боловч уран бүтээгдэхүүний хуваарилалт болон бүтэлгүйтгүй мэдлэгтэй учраас хүчтэй байдаг. Урлаг хуваариллагаас гарсан уран бүтээлүүд хэл загварын үйл ажиллагааг багасгаж чадна. Бүтэн мэдлэг нь сайн тарианы шинжилгээг хязгаарлаж чадна. Энэ ажил дээр бид SimPlified Language Activity Traces (SPLAT) дээр суурилсан нэмэлт банкварчлал аргыг танилцуулдаг. СПЛАТ гэдэг нь хэл хөгжлийн корпора юм. SPLAT өгөгдлийн сангууд байгалийн үүсгэн тархи ашиглаж, асуулт хариултын хоёрыг хэмжээнд зөвшөөрөх боломжтой болгож, хамгийн гадна бүрэн мэдлэг олгох боломжтой. Бид 3 өөр архитектурын хэл загварын загварууд дэлхийн улс орнуудын талаар зөвхөн хэлний шиг үйл ажиллагааны кодлог ашиглан хариулж чадна. Бидний ойлголт нь шинэ хэл загварууд болон асуулт хариултыг нэмэгдүүлэх даалгаварууд юм.</abstract_mn>
      <abstract_no>Innstillingane for naturspråksbehandlingssystemet i dag er vanlegvis evaluert med stor datasett med kurserte spørsmål og svar. Selv om desse er kritiske benchmarke for framgang, så har dei også kjøp frå svakhet på grunn av kunstiske distribusjonar og ukjente kunnskap. Artifaktar som oppstår frå kunstige distribusjonar kan overstyra språk-modellen, mens ikkje komplett kunnskap grenser fine-grained analyse. I denne arbeiden introduserer vi ein complementær benchmarking tilnærming basert på Simpliserte språktivitetsspor (SPLAT). SPLAT er korpora av språkkkoding av aktivitet i nokre lukka domene (me studerer spor frå sjakk og baseballspel i denne arbeidet). SPLAT Vi viser at språk-modeller av tre ulike arkitektur kan svara på spørsmål om verdenstilstandar med berre verb-liknande koding av aktivitet. Tilnærminga vårt er utvidbare til nye språk-modeller og fleire oppgåver som svarar på spørsmål.</abstract_no>
      <abstract_pl>Możliwości dzisiejszych systemów przetwarzania języka naturalnego są zazwyczaj oceniane przy użyciu dużych zbiorów danych kuratorskich pytań i odpowiedzi. Chociaż są to krytyczne punkty odniesienia postępu, cierpią one również z powodu słabości ze względu na sztuczne rozkłady i niepełną wiedzę. Artykuły wynikające ze sztucznych rozkładów mogą przeceniać wydajność modelu językowego, podczas gdy niepełna wiedza ogranicza precyzyjną analizę. W niniejszej pracy wprowadzamy komplementarne podejście porównawcze oparte na SimPlified Language Activity Traces (SPLAT). SPLAT to korpusy językowych kodowań aktywności w jakiejś zamkniętej dziedzinie (badamy ślady z gry w szachy i baseball w niniejszej pracy). Zestawy danych SPLAT wykorzystują naturalnie powstające dystrybucje, umożliwiają generowanie par pytania-odpowiedzi na skalę i zapewniają pełną wiedzę w ich zamkniętych domenach. Pokazujemy, że modele językowe trzech różnych architektur mogą odpowiadać na pytania dotyczące stanów świata używając tylko czasowopodobnych kodowań aktywności. Nasze podejście można rozszerzyć o nowe modele językowe i dodatkowe zadania odpowiadające na pytania.</abstract_pl>
      <abstract_ro>Capacitățile sistemelor de procesare a limbajului natural de astăzi sunt de obicei evaluate folosind seturi mari de date de întrebări și răspunsuri curatoriate. Deși acestea reprezintă criterii critice ale progresului, ele suferă, de asemenea, de slăbiciune din cauza distribuțiilor artificiale și a cunoștințelor incomplete. Articolele rezultate din distribuții artificiale pot supraestima performanța modelului lingvistic, în timp ce cunoștințele incomplete limitează analiza fină. În această lucrare, introducem o abordare complementară de benchmarking bazată pe simplificate Language Activity Traces (SPLAT). SPLAT-urile sunt corpuri de codări lingvistice de activitate într-un domeniu închis (studiem urme de șah și jocuri de baseball în această lucrare). Seturile de date SPLAT utilizează distribuții naturale, permit generarea perechilor de întrebări-răspuns la scară și oferă cunoștințe complete în domeniile lor închise. Aratăm că modelele lingvistice ale trei arhitecturi diferite pot răspunde la întrebări despre stările lumii folosind numai codări de activitate ca verbe. Abordarea noastră este extinsă la noi modele lingvistice și sarcini suplimentare de răspuns la întrebări.</abstract_ro>
      <abstract_si>අදින් ස්වාභික භාෂාව ප්‍රක්‍රියාසය පද්ධතියේ සාමාන්‍යයෙන් ලොකු දත්ත සේට් සහ උත්තර ප්‍රශ්නය සඳහා වි මේවා විශේෂ බෙන්ච්මාර්ක් වෙනුවෙන් ඉන්නවා නමුත් එයාලා වඩා දුර්වලයෙන් දුර්වල් වෙනුවෙන් ඉන්නවා. කළුණු විතරයෙන් පිළිබඳින්න පුළුවන් භාෂා මොඩල් ප්‍රභාවිත විදිහට වැඩ කරන්න, සම්පූර්ණ දන්නවක් න මේ වැඩේ අපි සිම්ප්ලිෆීඩ් භාෂාව ක්‍රියාක්‍රියාත්මක පරීක්ෂණය (SPLAT) විසින් සම්පූර්ණ බෙන්ච්මාර් SPLATs තමයි භාෂාව සංකේතනයේ ක්‍රියාත්මක ක්‍රියාත්මක ක්‍රියාත්මක ක්‍රියාත්මක ක්‍රියාත්මක ක්‍රියාත්මක (අප SPLAT දත්ත සටහන් ස්වභාවිතයෙන් ප්‍රශ්න- ප්‍රතිච්චාරයක් පාවිච්චි කරන්න, ප්‍රශ්න- ප්‍රතිචාරයක් ප්‍රමාණයෙන්  අපි පෙන්වන්නේ වෙනස් විදිහට ස්ථාපනය තුනක් වගේ භාෂා මොඩේල් ප්‍රශ්න පුළුවන් ලෝක ස්ථානය ගැන ප්‍ අපේ විදිහට අලුත් භාෂා මොඩේල් වලින් ප්‍රශ්න ප්‍රතික්‍රියාවක් වලින් ප්‍රශ්න වැඩ කරන්න පුළ</abstract_si>
      <abstract_sr>Sposobnosti današnjih prirodnih jezičkih obrađivanja obično se procjenjuju koristeći velike podatke izloženih pitanja i odgovora. Iako su to kritični kritični kritični kritični znakovi napretka, oni takođe pate od slabosti zbog umjetnih raspodjela i nepotpunih znanja. Artifakti koji se pojavljuju iz umjetničkih distribucija mogu nadmašiti provedbu jezičkog modela, dok nepotpuno znanje ograničava ispravnu analizu. U ovom poslu predstavljamo dodatni pristup kriteriji na temelju Simpliziranih tragova aktivnosti jezika (SPLAT). SPLAT su korporacija jezičkih kodiranja aktivnosti u nekom zatvorenom domenu (proučavamo tragove šaha i bejzbol igre u ovom poslu). SPLAT podaci koriste prirodno rastuće distribucije, omogućavaju generaciju par odgovora na pitanje u skali i priuštiti potpuno znanje u zatvorenim domenama. Pokazujemo da jezički modeli tri različite arhitekture mogu odgovoriti na pitanja o svetskim državama koristeći samo kodiranje aktivnosti poput verba. Naš pristup je širok za nove jezičke modele i dodatne odgovore na pitanje.</abstract_sr>
      <abstract_sv>Möjligheterna hos dagens naturliga språkbehandlingssystem utvärderas vanligtvis med hjälp av stora datamängder med kuraterade frågor och svar. Även om dessa är kritiska riktmärken för framsteg lider de också av svaghet på grund av artificiell fördelning och ofullständig kunskap. Artefakter som uppstår från artificiella distributioner kan överskatta språkmodellens prestanda, medan ofullständig kunskap begränsar finkornig analys. I detta arbete introducerar vi en kompletterande benchmarking-metod baserad på Simplified Language Activity Traces (SPLAT). SPLAT är korpora av språkkodningar av aktivitet inom någon sluten domän (vi studerar spår från schack och baseball spel i detta arbete). SPLAT-datauppsättningar använder naturliga distributioner, möjliggör generering av frågor-svar par i stor skala och ger fullständig kunskap inom sina slutna domäner. Vi visar att språkmodeller av tre olika arkitekturer kan besvara frågor om världstillstånd med endast verbliknande aktivitetskodningar. Vårt tillvägagångssätt kan utvidgas till nya språkmodeller och ytterligare frågeställningar.</abstract_sv>
      <abstract_so>Aqoonsiga nidaamka baaritaanka luqada asalka ah ee maanta waxaa sida caadiga ah loo qiimeynayaa isticmaalka sawirro badan oo su'aalo ah iyo jawaabo la koobay. Intii ay kuwanu yihiin qaybaha horumarinta ee muhiim ah, waxay sidoo kale ka xanuunsadaan itaaldarrada, qaybinta farsamada iyo aqoonta aan dhamayn darteed. Qoraalka farshaxanka ka soo baxa qaybsiga farshaxanka ayaa ka hor marin kara sameynta muuqashada luuqada, iyadoo aan dhamaan aqoonta la'aantiisu ay xadgudbaan baaritaanka saxda. Markaas waxan, waxaynu soo bandhignaynaa qaab kamid ah oo ku saleysan habka dhaqdhaqaaqa ee luqada SimPlified (SPLAT). SPLATs waa shirkad kaarar ah oo ku qoran waxqabad luqadeed oo ku yaala meelo qarsoon (waxan ka baranaynaa wadooyin ka yimaada ciyaaraha chess iyo ciyaaraha baseball). SPLAT waxey isticmaalaan qaybaha asalka ah oo si dabiicadda ah u isticmaalaya, ku raadsan yihiin abuurista labada mas’uul oo ku saabsan, wuxuuna heli karaa aqoon kamid ah oo ku qoran meelahooda qarsoon. Waxaynu tusnaynaa in noocyada afka ee saddex meelood oo kala duduwan ay ka jawaabi karaan su'aalo ku saabsan dowlada dunida oo ay isticmaalaan kooxaha waxqabadka oo la mid ah oo kaliya. Dhaqdhaqaalahayagu waa mid aad u badan yihiin modelalka luuqada cusub iyo shaqaalaha su'aalaha ka jawaabaya oo kale.</abstract_so>
      <abstract_ta>இன்றைய நாளின் இயற்கையான மொழி செயல்பாட்டு அமைப்புகளின் இயல்பான திறன் பெரிய தகவல் அமைப்புகளை பயன்படுத்தி மதிப்பிடப்பட இந்த முன்னேற்றத்தின் முக்கியமான குறிப்புகள் இருக்கும் போது, அவர்கள் தோல்வியுற்றத்தின் காரணத்தால் மற்றும் மு கலைப்பாட்டிலிருந்து வரையறுக்கப்படும் கலைப்பாட்டாளர் இந்த வேலையில், நாம் சுலபமான மொழி செயல்பாடு தட்டுகளை அடிப்படையில் ஒரு சுருக்கமான benchmarking approach introduced (SPLAT). SPLATs சில மூடிய களத்தில் செயல்பாடுகளின் மொழி குறியீடுகள் (நாம் இந்த வேலையில் செஸ்ஸ் மற்றும் பேஸ்பால் விளையாட்டுகள SPLAT தகவல் அமைப்பு மூன்று வேறு வித்தியாசமான அடைவுகளின் மொழி மாதிரிகளை காட்டுகிறோம் உலக நாடுகள் பற்றிய கேள்விகளுக்கு பதில் அளிக் புதிய மொழி மாதிரிகள் மற்றும் கூடுதல் கேள்வி பதில் செய்யும் பணிகளுக்கு எங்கள் வழி விரைவாக உள்ளது.</abstract_ta>
      <abstract_ur>آج کے طبیعی زبان پرسس سیسٹم کے قابلیت معمولاً بڑے ڈیٹسٹ کے مطابق مطابق سوال اور جواب کے استعمال کرتے ہیں. اگرچہ یہ بڑی اضطراری باتیں ہیں، ان کو بھی کمزوری کی وجہ سے پہنچ رہی ہے، کیونکہ وہ مصنوعی تقسیم اور بے پوری علم کی وجہ سے۔ مصنوعی تقسیم کے باعث آلودہ معلومات زبان مدل کی عملکرد سے زیادہ زیادہ زیادہ کر سکتے ہیں، حالانکہ غیرپورے علم کے اندازے اچھی دانے کی تحلیل سے محدود ہیں. ہم اس کام میں سیم پلیفڈ زبان فعالیت تریس (SPLAT) پر بنیاد رکھتے ہیں، ایک اضافہ بنچم مارکینگ طریقہ پیش کرتے ہیں۔ SPLATs کچھ بند ڈومین میں فعالیت کی زبان کا اکنوڈینگ کا کوپورا ہے (ہم اس کام میں شکس اور بیسبال کھیل سے پڑھتے ہیں)۔ SPLAT ڈاٹ سٹیوں کو طبیعی طور پر اٹھانے والی تقسیم کے مطابق استعمال کرتا ہے، سوال جوڑوں کی نسل کو ترازو سے اجازت دیتا ہے، اور ان کے بند ڈومین میں پورا علم خرچ کرتا ہے۔ ہم دکھاتے ہیں کہ تین مختلف معماروں کی زبان مدل دنیا کی حالت کے بارے میں سوال پوچھ سکتے ہیں صرف فعالیت کے مطابق ویرڈ جیسے اکنوڈینگ کے مطابق۔ ہمارا تقریبا نئی زبان مدل اور اضافہ سوال جواب دینے کے کاموں کے لئے پھیلانے والا ہے.</abstract_ur>
      <abstract_vi>Khả năng của hệ thống xử lý ngôn ngữ tự nhiên hiện nay được đánh giá bằng các tập tin đầy đủ các câu hỏi và câu trả lời. Mặc dù đây là những tiêu chuẩn quan trọng của tiến bộ, nhưng chúng cũng bị yếu do phân phát nhân tạo và kiến thức chưa hoàn thiện. Thành phần từ phân phát nhân tạo có thể tăng cường khả năng mô phỏng ngôn ngữ, trong khi kiến thức chưa hoàn chỉnh giới hạn phân tích. Trong công việc này, chúng ta sẽ áp dụng một phương pháp định nghĩa khác dựa trên Dấu Hiện Hành trình Tư Mã Lai Mô phỏng. Đội vệ binh là hạ sĩ của các loại ngôn ngữ có hoạt động trong vài khu vực kín (chúng tôi nghiên cứu dấu vết từ cờ vua và bóng chày trong công việc này). Một bộ hiện giải thoát dọn tự nhiên, cho phép có câu hỏi bằng cách đó, và cho phép có trị hoàn toàn bộ trong khu vực của họ. Chúng tôi cho thấy các mô hình ngôn ngữ của ba kiến trúc khác nhau có thể trả lời câu hỏi về các bang thế giới chỉ sử dụng các mã động từ như các hoạt động. Cách tiếp cận của chúng ta có thể mở rộng ngôn ngữ mới và các nhiệm vụ trả lời câu hỏi.</abstract_vi>
      <abstract_uz>Bugun tilning tabiiy tizimi boshqarish tizimlarining qobiliyatlari odatda katta maʼlumotlar va javoblar bilan ishlatiladi. While these are critical benchmarks of progress, they also suffer from weakness due to artificial distributions and incomplete knowledge.  Faqat tarqatish soʻzlaridan tasdiqlangan Artifaktlar tilning model natijasini oshirish mumkin, ammo muvaffaqiyatli taʼminot toʻplamligini ajratish mumkin. Bu ishda, biz Simpled Tilning Activity Trayerlari (SPLAT) asosida yaratilgan murakkablik benchmarking usulini ko'rsamiz. Name Name Biz bu tilning uchta boshqa arxituvlar modellari dunyo davlatlari haqida savollariga javob berishi mumkin. Biz faqat qo'llangan tashkilotlardan foydalanish mumkin. Bizning usuli yangi tillar modellari va qoʻshimcha savol javoblar ishlariga yetarlicha.</abstract_uz>
      <abstract_bg>Възможностите на съвременните системи за обработка на естествени езици обикновено се оценяват с помощта на големи набори от данни от подбрани въпроси и отговори. Въпреки че това са критични показатели за напредъка, те също страдат от слабост поради изкуствено разпределение и непълни знания. Артефактите, произтичащи от изкуствени разпределения, могат да надценят производителността на езиковия модел, докато непълните знания ограничават финия анализ. В тази работа въвеждаме допълнителен подход за сравнително оценяване, базиран на СимПЛИфицирани Езикови Дейности (SPLAT). SPLAT са корпоративни езикови кодировки на дейност в някои затворени области (в тази работа изучаваме следи от шах и бейзболни игри). Наборите от данни използват естествено възникващи дистрибуции, позволяват генерирането на двойки въпроси-отговори в мащаб и предоставят пълни знания в техните затворени области. Показваме, че езиковите модели на три различни архитектури могат да отговорят на въпроси за световните състояния, използвайки само глаголоподобни кодировки на активността. Нашият подход е разширим към нови езикови модели и допълнителни задачи за отговаряне на въпроси.</abstract_bg>
      <abstract_da>Mulighederne i nutidens natursprogbehandlingssystemer evalueres typisk ved hjælp af store datasæt af kuraterede spørgsmål og svar. Selv om disse er kritiske benchmarks for fremskridt, lider de også af svagheder på grund af kunstige fordelinger og ufuldstændig viden. Artefakter, der opstår fra kunstige distributioner, kan overvurdere sprogmodellens ydeevne, mens ufuldstændig viden begrænser finkornet analyse. I dette arbejde introducerer vi en komplementær benchmarking tilgang baseret på Simplified Language Activity Traces (SPLAT). SPLATs er korpora af sprogkodninger af aktivitet i et lukket domæne (vi studerer spor fra skak og baseball spil i dette værk). SPLAT datasæt bruger naturligt opståede distributioner, tillader generering af spørgsmål-svar par i skala og giver fuld viden på deres lukkede domæner. Vi viser, at sprogmodeller af tre forskellige arkitekturer kan besvare spørgsmål om verdenstilstande ved hjælp af verbagtige aktivitetskodninger. Vores tilgang kan udvides til nye sprogmodeller og yderligere spørgsmålsbesvarelsesopgaver.</abstract_da>
      <abstract_nl>De mogelijkheden van de huidige systemen voor natuurlijke taalverwerking worden meestal geëvalueerd met behulp van grote datasets van samengestelde vragen en antwoorden. Hoewel dit cruciale maatstaven zijn voor vooruitgang, lijden zij ook onder zwakte als gevolg van kunstmatige verdelingen en onvolledige kennis. Artifacten die voortkomen uit kunstmatige distributies kunnen de prestaties van taalmodellen overschatten, terwijl onvolledige kennis fijnkorrelige analyse beperkt. In dit werk introduceren we een complementaire benchmarking aanpak gebaseerd op SimPlified Language Activity Traces (SPLAT). SPLAT's zijn corpora van taalcoderingen van activiteit in een gesloten domein (we bestuderen sporen van schaak en honkbal spelen in dit werk). SPLAT datasets maken gebruik van natuurlijke distributies, maken het genereren van vraag-antwoord paren op schaal mogelijk en bieden volledige kennis in hun gesloten domeinen. We laten zien dat taalmodellen van drie verschillende architecturen vragen over wereldstaten kunnen beantwoorden met behulp van werkwoorden-achtige coderingen van activiteit. Onze aanpak is uitbreidbaar naar nieuwe taalmodellen en aanvullende vragen beantwoorden taken.</abstract_nl>
      <abstract_hr>Sposobnosti današnjih prirodnih sustava obradivanja jezika obično se procjenjuju koristeći velike podatke izliječenih pitanja i odgovora. Iako su to kritične kritične kritike napretka, oni također pate od slabosti zbog umjetnih raspodjela i nepotpunih znanja. Artifakti iz umjetničkih distribucija mogu nadmašiti učinkovitost jezičkog modela, dok nepotpuno znanje ograničava ispravnu analizu. U ovom poslu predstavljamo dodatni kritični pristup na temelju SimPlificiranog praćenja aktivnosti jezika (SPLAT). SPLAT su korporacija jezičkih kodiranja aktivnosti u nekom zatvorenom domenu (proučavamo tragove šaha i bejzbol igre u ovom poslu). SPLAT podaci koriste prirodno rastuće distribucije, omogućavaju generaciju par odgovora na pitanje u skali i priuštiti potpune znanje u zatvorenim domenama. Pokazujemo da jezički modeli tri različite arhitekture mogu odgovoriti na pitanja o svijetskim državama koristeći samo kodiranje aktivnosti poput verba. Naš pristup je širok za nove jezičke modele i dodatne odgovorne zadatke na pitanje.</abstract_hr>
      <abstract_de>Die Fähigkeiten heutiger Systeme zur Verarbeitung natürlicher Sprache werden typischerweise anhand großer Datensätze kuratierter Fragen und Antworten bewertet. Diese sind zwar kritische Maßstäbe für den Fortschritt, leiden aber auch unter Schwäche durch künstliche Verteilungen und unvollständiges Wissen. Artefakte, die durch künstliche Verteilungen entstehen, können die Leistung von Sprachmodellen überbewerten, während unvollständiges Wissen die feinkörnige Analyse begrenzt. In dieser Arbeit stellen wir einen ergänzenden Benchmarking-Ansatz vor, der auf SimPlified Language Activity Traces (SPLAT) basiert. SPLAts sind Korpora von Sprachcodierungen von Aktivitäten in einem geschlossenen Bereich (wir untersuchen Spuren aus Schach und Baseballspielen in dieser Arbeit). SPLAT-Datensätze verwenden natürlich auftretende Verteilungen, ermöglichen die Generierung von Frage-Antwort-Paaren im großen Maßstab und ermöglichen vollständiges Wissen in ihren geschlossenen Domänen. Wir zeigen, dass Sprachmodelle von drei verschiedenen Architekturen Fragen über Weltzustände beantworten können, indem sie nur Verb-ähnliche Codierungen von Aktivität verwenden. Unser Ansatz ist auf neue Sprachmodelle und zusätzliche Fragestellungen erweiterbar.</abstract_de>
      <abstract_id>Kemampuan dari sistem proses bahasa alami hari ini biasanya diuji menggunakan set data besar pertanyaan dan jawaban. Sementara ini adalah benchmark kritis kemajuan, mereka juga menderita kelemahan karena distribusi buatan dan pengetahuan tidak lengkap. Artifacts arising from artificial distributions can overstate language model performance, while incomplete knowledge limits fine-grained analysis.  Dalam pekerjaan ini, kami memperkenalkan pendekatan benchmarking komplementari berdasarkan SimPlified Language Activity Traces (SPLAT). SPLAT adalah korpra dari kode bahasa aktivitas di beberapa domain tertutup (kami mempelajari jejak dari catur dan permainan bisbol dalam pekerjaan ini). Set data SPLAT menggunakan distribusi yang muncul secara alami, memungkinkan generasi pasangan pertanyaan-jawaban pada skala, dan membeli pengetahuan lengkap dalam domain tertutup mereka. Kami menunjukkan bahwa model bahasa dari tiga arsitektur yang berbeda dapat menjawab pertanyaan tentang negara-negara dunia hanya menggunakan kode aktivitas seperti verb. pendekatan kita dapat diperluaskan untuk model bahasa baru dan tugas tambahan menjawab pertanyaan.</abstract_id>
      <abstract_ko>오늘날 자연 언어 처리 시스템의 능력은 통상적으로 대량의 정성스럽게 기획된 문제와 답안 데이터 집합을 사용하여 평가한다.진보의 관건적인 기준이지만 인위적인 분포와 불완전한 지식 때문에 약점도 있다.인공 분포로 인해 발생하는 부품은 언어 모델의 성능을 과장할 수 있고 불완전한 지식은 세립도 분석을 제한할 수 있다.이 작업에서 우리는 간소화된 언어 활동 추적(SPLAT)을 바탕으로 하는 보충 기준 테스트 방법을 소개했다.splat는 폐쇄적인 분야에서 활동하는 언어 인코딩 자료 라이브러리이다.SPLAT 데이터 세트는 자연 발생의 분포를 사용하여 문제 - 정답 쌍을 대규모로 생성하고 폐쇄된 도메인의 전체 지식을 제공합니다.우리는 세 가지 서로 다른 체계 구조의 언어 모델이 동사식의 활동 코드만 사용하여 세계 상태와 관련된 문제를 대답할 수 있다는 것을 증명했다.우리의 방법은 새로운 언어 모델과 추가 문답 임무로 확대될 수 있다.</abstract_ko>
      <abstract_tr>Bu gün tebigy dil işlemek sistemleriniň üýtgeşmeleri bilen çykyş soraglaryň we jogabalaryň uly sanlary ullanýar. Bu ýerler ilerleme döwletlerinden wajyp görkezilýän çykyşlar bolsa hem döwüräk paýlamak we bolmadyk bilim sebäbi zayıflatlykdan çykýarlar. Yap taýýarlardan gelen sanat eserleri dil nusgasyny üstüne getirip biler, bilim ýok taýýarlanmagynyň çykmasyny mümkin edýär. Bu işde SimPlified Dil Etkinliği Traces (SPLAT) tabanlı bir tabanlı etimleme metodlarını tanıtıyoruz. SPLAT ködlemeleri ýapylan domandaki dil ködlemeleriniň korporadyr (biz bu işde küşt we beýsbol oýnunyň izi öwrenip otyrys). SPLAT veri düzümleri dogrudan gaýşartýan döwletleri ulanýar, sorag-jogabat çiftlerini ölçekde döwletlere rugsat bermek we ýapyk alanlarynda tam bilgi bermek üçin rugsat berir. Biz üç dürli arhitekturyň dili nusgalarynyň dünýä döwletleri barada diňe bir verb ýaly ködlemeler üçin jogap berip biler. Biziň ýaryşymyz täze dil nusgalaryna we soraglaryň jogaplaryna golaýdyr.</abstract_tr>
      <abstract_sw>Uwezekano wa mfumo wa utaratibu wa lugha za asili wa leo unapitiwa kwa kutumia seti kubwa ya data za maswali na jibu. Wakati hizi ni kanuni muhimu za maendeleo, pia wanakabiliwa na udhaifu kutokana na usambazaji wa ubunifu na ufahamu usio kamili. Wasanii wanaotokana na usambazaji wa ubunifu wanaweza kuongeza utendaji wa mifano ya lugha, wakati maarifa yasiyo kamili yanazuia uchambuzi mzuri. Katika kazi hii, tunaonyesha mbinu za kuchuja bendera kwa msingi wa utaratibu wa lugha SimPlified (SPLAT). SPLATs ni makampuni ya mfumo wa shughuli za lugha katika baadhi ya maeneo yaliyofungwa (tunasoma picha za mchezo wa chess na baseball katika kazi hii). seti za taarifa za SPLAT hutumia usambazaji wa asili, ruhusu kizazi cha wawili wa majibu kwa kiwango kikubwa, na kupata maarifa kamili katika maeneo yao yaliyofungwa. Tunaonyesha kuwa mifano ya lugha ya majengo mitatu tofauti yanaweza kujibu maswali kuhusu majimbo ya dunia kwa kutumia ujumbe wa shughuli kama vile tu. Hatua yetu ni muhimu kwa mifano mpya ya lugha na kazi nyingine za kujibu maswali.</abstract_sw>
      <abstract_fa>توانایی سیستم‌های پرداخت زبان طبیعی امروز معمولا با استفاده از مجموعه‌های داده‌های بزرگ از سوالات و جواب‌های پرداخته‌شده ارزیابی می‌شوند. در حالی که اینها سنجیره های مهم پیشرفت هستند، آنها همچنین از ضعیفی به سبب توزیع مصنوعی و دانش کامل رنج می‌برند. مصنوعی که از توزیع‌های مصنوعی می‌آید می‌تواند عملکرد مدل زبان را زیادی کند، در حالی که دانش کامل تحلیل‌های دانه‌ای را محدود می‌کند. در این کار، ما یک روش تخمین بیشتری را معرفی می‌کنیم که بر اساس ردیابی فعالیت زبانی SimPlified (SPLAT). SPLAT‌ها در برخی از دامنه‌های بسته‌شده (ما ردیابی از بازی‌های شطرنج و بیسبال در این کار مطالعه می‌کنیم). مجموعه‌های داده‌های SPLAT از توزیع‌های طبیعی بالا می‌برند، اجازه می‌دهند نسل جفت‌های سوال و جواب را در مقیاس و اجازه می‌دهند دانش کامل در دامنهای بسته‌شان. ما نشان می دهیم که مدل زبانی از سه معماری متفاوت می تواند سوالات درباره ایالات دنیا را با استفاده از تنها رمز‌بندی‌های فعالیت مانند ویژه جواب دهد. دسترسی ما برای مدل های جدید زبان و مسئله های جواب سوال اضافه است.</abstract_fa>
      <abstract_am>የዛሬ የፍጥረቱ ቋንቋ ፕሮጀክት ስርዓቶች በተለየ ትልቁ የዳታ ጥያቄዎችን እና መልስ በመጠቀም ይታያል፡፡ እነዚህም የውጤት ግንኙነት አካባቢዎች ሲሆኑ፥ አካባቢ እውቀትና ፍጹም ስህተት ምክንያት ከድካም ይቀማሉ። አርፋፊዎች ከቋንቋው ምሳሌ አድራጊዎችን ማሳየት ይችላል፡፡ በዚህ ስራ፣ በተጨማሪው ቋንቋ ተግባር (SPLAT) የተመሳሳይ የድምፅ ድርጊት (SPLAT) የተጠቃሚ የbenchmarking ሥርዓት እናሳውቃለን፡፡ SPLAT የቋንቋ አካባቢዎች በክፍለ አካባቢ አካባቢዎች ናቸው (በዚህ ሥራ የደረጃ መስኮቶች እና የbaseball ጨዋታዎችን እናስተምራለን፡፡ የSPLAT ዳታ ማህበረሰብ በአዳማዊ አካባቢ አካባቢ እፍላጎችን ይጠቅማል፣ የጥያቄ መልስ ሁለቶችን በመጠቀም ይፈቅዳሉ፣ እናም በተዘጋጀባቸው ሰፈር ውስጥ ሙሉ እውቀትን አግኝቷል፡፡ የሦስት መልዕክቶች የቋንቋዎች ምሳሌዎች በዓለም ሀገራት ላይ ጥያቄን ብቻ በዝርዝር የሚመስል የሥርዓት አካባቢ ቀለሞች የሚመልስ እንደሆነ እናሳየዋለን፡፡ አዲስ ቋንቋ ምሳሌዎች እና ለጥያቄ መልስ ስራዎችን ለመስጠት ይችላል፡፡</abstract_am>
      <abstract_sq>Përaftësitë e sistemeve natyrore të përdorimit të gjuhës sot vlerësohen tipikisht duke përdorur të dhëna të mëdha të pyetjeve dhe përgjigjeve të kuruara. Ndërsa këto janë pika kritike të përparimit, ata vuajnë gjithashtu nga dobësia për shkak të shpërndarjeve artificiale dhe njohurive të pakompletuara. Artifacts arising from artificial distributions can overstate language model performance, while incomplete knowledge limits fine-grained analysis.  Në këtë punë, ne futim një metodë komplementare referimi bazuar në SimPlied Language Activity Traces (SPLAT). SPLATs janë korpra e kodifikimit të gjuhës të aktivitetit në një domeni të mbyllur (ne studiojmë gjurmë nga lojrat e shahut dhe bejsbollit në këtë punë). SPLAT datasets use naturally-arising distributions, allow the generation of question-answer pairs at scale, and afford complete knowledge in their closed domains.  Ne tregojmë se modelet gjuhësore të tre arkitekturave të ndryshme mund të përgjigjen pyetjeve rreth shteteve botërore duke përdorur vetëm kodifikime të aktivitetit të ngjashme me verbe. Përqasja jonë është e shtrirë ndaj modeleve të reja gjuhësh dhe detyrave shtesë që përgjigjen pyetjeve.</abstract_sq>
      <abstract_az>Bugünün təbiətli dil işləmə sistemlərinin qabiliyyəti genellikle böyük verilən soruşmalar və cevaplar vasitəsilə müəyyən edilir. Bunlar tədbirlik dəyişiklikləridir, həmçinin məhsul dağıtılması və tamamlanmış bilgi üzündən zəiflik üzündən çəkilirlər. Yaxşı dağıtımlardan gələn məhsullar dil modellərinin performansını üstün edə bilər, çünki tamamlanmış bilgi sünbül analizi limitləyir. Bu işdə, SimPlified Dil Etkinlik İzləri (SPLAT) üzərində dayanan complementary benchmarking approach təşkil edirik. SPLAT bəzi qapılmış domandakı dil kodlaması korporasıdır (biz bu işdə satranç və beysbol oyunlarından izləri öyrənirik). SPLAT veri qurğuları təbiətlə yüksək dağıtımları istifadə edir, sual-cavab çiftlərinin nəslinə ölçüdə müəyyən edir və qapılmış alanlarda tamam bilgi verirlər. Biz üç müxtəlif arhitekturların dil modellərinin dünya eyaletleri barəsində yalnız verb kimi kodlamaları ilə cavab verə bilər. Bizim tərzimiz yeni dil modellərinə və daha çox sual cavab verən işlərə genişlənir.</abstract_az>
      <abstract_af>Die moontlikhede van vandag se natuurlike taal verwerking stelsels word tipies uitgewerk deur te gebruik groot datastelle van gekuierde vrae en antwoorde. Alhoewel hierdie kritiese benchmarke van vordering is, lyk hulle ook van swakheid vanweë kunstige verspreidings en onvolledige kennis. Artifakte wat uit kunstenaarle verspreidings opkom, kan taal-model-prestasie oorskryf, terwyl onvolledige kennis beperk fyn-graad analisie. In hierdie werk introduseer ons 'n komplementêre benchmarking toegang gebaseer op Simpliseerde Taal Aktiviteitsspore (SPLAT). Spelts is korpora van taal kodering van aktiviteit in sommige gesluit domein (ons studeer spore van skaak en baseball speletjies in hierdie werk). SPLAT datastelle gebruik natuurlik-opstaande verspreidings, laat die generasie van vraag-antwoord paar op skaal toe, en toelaat volledige kennis in hulle gesluit domeine. Ons wys dat taal modele van drie verskillende arkitektuure kan antwoord vrae oor wêreld staatste met slegs verb-lyke kodering van aktiviteit. Ons toegang is uitbreidig vir nuwe taal modele en addisionele vraag-antwoordende taak.</abstract_af>
      <abstract_hy>Այսօրվա բնական լեզվի վերամշակման համակարգերի հնարավորությունները սովորաբար գնահատվում են օգտագործելով մեծ տվյալների համակարգեր կորացված հարցեր և պատասխաններ: Մինչդեռ դրանք առաջընթացի կարևոր համեմատական նշաններ են, նրանք նաև թույլ են տառապում արհեստական տարածումների և անկատարյալ գիտելիքների պատճառով: Արվեստական տարածումներից առաջացած արվեստի փաստերը կարող են գերագնահատել լեզվի մոդելի արդյունքը, մինչդեռ անկատարյալ գիտելիքները սահմանափակում են նրբագեղ վերլուծությունը: Այս աշխատանքի ընթացքում մենք ներկայացնում ենք համալրացուցիչ համեմատական մոտեցում, որը հիմնված է Սիմպլիֆեյթ լեզվի ակտիվության հետքերի վրա (SIMPLAT). ՍՊԼԱԹ-ները մի քանի փակ ոլորտում գտնվող լեզվի կոդավորման կապորա են (մենք ուսումնասիրում ենք շախմային և բեյսբոլի խաղերի հետքերը այս աշխատանքում): Սպլաթ տվյալների համակարգերը օգտագործում են բնական տարածումներ, հնարավորություն են տալիս հարցերի և պատասխանների զույգերի ստեղծման մեջ, և հնարավորություն են տալիս լիովին գիտելիք իրենց փակ ոլորտներում: Մենք ցույց ենք տալիս, որ երեք տարբեր ճարտարապետության լեզվային մոդելները կարող են պատասխանել աշխարհային երկրների մասին հարցերին միայն օգտագործելով բայի նման ակտիվության կոդավորումներ: Our approach is extensible to new language models and additional question-answering tasks.</abstract_hy>
      <abstract_bn>আজকের প্রাকৃতিক ভাষা প্রক্রিয়ার সিস্টেমের ক্ষমতা সাধারণত বিশাল তথ্য সংক্রান্ত প্রশ্ন ও উত্তর ব্যবহার করে মূল্য While these are critical benchmarks of progress, they also suffer from weakness due to artificial distributions and incomplete knowledge.  শৈল্পিক বিতরণের ক্ষেত্রে আর্টিফ্যাক্টরা ভাষার মডেলের প্রদর্শনের বাড়িয়ে দিতে পারে, আর সম্পূর্ণ জ্ঞানের বিশ্লেষণে এই কাজে আমরা সিমপ্লিফিড ভাষা কার্যক্রম ট্রাসের ভিত্তিতে একটি সহায়ক ব্যাংমেন্টমেন্টিং পদক্ষেপ উপস্থাপন করি। এসপিল্যাটিস কিছু বন্ধ ডোমেইনে কার্যক্রমের কোর্পোরা ভাষার কোর্পোরা (আমরা এই কাজে চাক এবং বেস্বল খেলা থেকে ট্রেক পড়ি)। SPLAT ডাটাসেটগুলো স্বাভাবিকভাবে বিতরণ ব্যবহার করে, প্রশ্নের উত্তরের জোড়া তৈরি করার অনুমতি দেয় এবং তাদের বন্ধ ডোমেইনে পুরো জ্ঞ আমরা দেখাচ্ছি যে তিনটি ভিন্ন প্রতিষ্ঠানগুলোর ভাষার মডেল বিশ্বের রাষ্ট্র সম্পর্কে প্রশ্নের উত্তর দিতে পারে কেবল ক নতুন ভাষার মডেল এবং অতিরিক্ত প্রশ্নের উত্তরের কাজের জন্য আমাদের প্রতিযোগিতা সম্ভব।</abstract_bn>
      <abstract_bs>Sposobnosti današnjih prirodnih jezičkih obrađivanja obično se procjenjuju koristeći velike podatke izliječenih pitanja i odgovora. Iako su to kritične kritične kritične kritike napretka, oni također pate od slabosti zbog umjetnih raspodjela i nepotpunih znanja. Artifakti iz umjetničkih distribucija mogu nadmašiti učinkovitost jezičkog modela, dok nepotpuno znanje ograničava ispravnu analizu. U ovom poslu predstavljamo dodatni pristup kriteriji na temelju SimPlificiranog jezičkog aktivnosti (SPLAT). SPLAT su korporacija jezičkih kodiranja aktivnosti u nekom zatvorenom domenu (proučavamo tragove šaha i bejzbol igre u ovom poslu). SPLAT podaci koriste prirodno rastuće distribucije, omogućavaju generaciju par odgovora na pitanje u skali i priuštiti potpuno znanje u zatvorenim domenama. Pokazujemo da jezički modeli tri različite arhitekture mogu odgovoriti na pitanja o svijetskim državama koristeći samo kodiranje aktivnosti poput verba. Naš pristup je širok za nove jezičke modele i dodatne odgovorne zadatke na pitanje.</abstract_bs>
      <abstract_cs>Schopnosti dnešních systémů zpracování přirozeného jazyka jsou obvykle hodnoceny pomocí velkých datových sad kurátovaných otázek a odpovědí. Přestože jde o kritická měřítka pokroku, trpí také slabostí v důsledku umělého rozdělení a neúplných znalostí. Artefakty vznikající z umělých distribucí mohou přehánět výkonnost jazykového modelu, zatímco neúplné znalosti omezují jemnou analýzu. V této práci představujeme doplňkový benchmarking přístup založený na SimPlified Language Activity Traces (SPLAT). SPLAT jsou korpusy jazykových kódů aktivity v některé uzavřené oblasti (v této práci studujeme stopy z šachů a baseballových her). Datové sady SPLAT využívají přirozeně vznikající distribuce, umožňují generování párů otázek-odpověď v měřítku a poskytují kompletní znalosti v jejich uzavřených doménách. Ukazujeme, že jazykové modely tří různých architektur mohou odpovědět na otázky týkající se světových stavů pouze pomocí slovesově podobného kódování aktivity. Náš přístup je rozšířen na nové jazykové modely a další úkoly zodpovězení otázek.</abstract_cs>
      <abstract_ca>Les capacitats dels sistemes natural s de processament de llenguatges d'avui en dia s'evaluen utilitzant grans conjunts de dades de preguntes i respostes curades. Mentre aquests són punts de referència crítics del progrés, també pateixen debilitat degut a distribucions artificials i coneixement incomplet. Els artefactes que surten de distribucions artificials poden sobrevaluar el rendiment del model lingüístic, mentre que el coneixement incomplet limita l'anàlisi fina. In this work, we introduce a complementary benchmarking approach based on SimPlified Language Activity Traces (SPLAT).  SPLATs són corpora de codificacions lingüístices d'activitat en un domini tancat (estudiem rastres dels jocs d'xadrex i de beisbol en aquest treball). Els conjunts de dades SPLAT utilitzen distribucions naturals, permeten generar parells de preguntes-respostes a escala i permeten coneixement complet en els seus dominis tancats. Mostrem que els models lingüístics de tres arquitectures diferents poden respondre preguntes sobre els estats mundials només fent servir codificacions d'activitat com verbs. El nostre enfocament és extensible a nous models lingüístics i tasques adicionals de resposta a preguntes.</abstract_ca>
      <abstract_et>Tänapäeva looduskeele töötlemise süsteemide võimalusi hinnatakse tavaliselt suurte andmekogumite abil, mis sisaldavad kureeritud küsimusi ja vastuseid. Kuigi need on edusammude kriitilised kriteeriumid, kannatavad nad ka kunstliku jaotuse ja ebatäielike teadmiste tõttu nõrkuse all. Kunstlikust levitamisest tulenevad esemed võivad keelemudeli jõudlust üle hinnata, samas kui puudulikud teadmised piiravad täpset analüüsi. Käesolevas töös tutvustame täiendavat võrdlusanalüüsi lähenemisviisi, mis põhineb SimPlified Language Activity Traces (SPLAT). SPLAT-d on keelekogeerimise korpused mõnes suletud valdkonnas (uurime selles töös male- ja pesapallimängude jälgi). SPLAT andmekogumid kasutavad loomulikult tekkivaid jaotusi, võimaldavad luua küsimuste ja vastuste paare mastaabis ning võimaldavad täielikke teadmisi oma suletud valdkondades. Näitame, et kolme erineva arhitektuuri keelemudelid suudavad vastata maailma seisundite küsimustele, kasutades ainult tegusõnalisi kodeeringuid. Meie lähenemisviis on laiendatav uutele keelemudelitele ja täiendavatele küsimustele vastamise ülesannetele.</abstract_et>
      <abstract_fi>Nykypäivän luonnollisen kielen käsittelyjärjestelmien valmiuksia arvioidaan tyypillisesti suurilla tietokokonaisuuksilla, jotka sisältävät valikoituja kysymyksiä ja vastauksia. Vaikka nämä ovat kehityksen kriittisiä mittareita, ne kärsivät myös keinotekoisesta jakautumisesta ja puutteellisesta tiedosta johtuvasta heikkoudesta. Keinotekoisista jakeluista syntyvät artefaktit voivat yliarvioida kielimallin suorituskykyä, kun taas epätäydellinen tieto rajoittaa hienojakoista analyysiä. Tässä työssä esittelemme täydentävän vertailuanalyysin, joka perustuu SimPlified Language Activity Traces (SPLAT) -menetelmään. SPLAT:t ovat kielikoodauksen korpusia jollakin suljetulla alueella (tutkimme tässä työssä shakki- ja baseball-pelien jälkiä). SPLAT-aineistot käyttävät luonnostaan syntyviä jakaumia, mahdollistavat kysymyksen ja vastauksen parien luomisen mittakaavassa ja tarjoavat täydellistä tietoa suljetuista toimialueistaan. Osoitamme, että kolmen eri arkkitehtuurin kielimallit voivat vastata maailman tilaa koskeviin kysymyksiin käyttämällä vain verbimaisia aktiviteettikoodauksia. Lähestymistapamme on laajennettavissa uusiin kielimalleihin ja lisäkysymyksiin vastaamiseen.</abstract_fi>
      <abstract_jv>Punika kapan kanggo ngilangno sistem perusahaan nggunakake Genjer-genjer saiki iki diangkamu nggawe gerakan kanggo dianggap, wong iki ngono kaleh luwih apik lan alam sing ora nggawe kesempatan kanggo ngilangno. Artik sing wis kelompok karo Distribution artipe iso nggawe nggawe modèl kuwi tindang Nang barêng-barêng iki, kéné gunakake sistem sempliified Language Active traces (PLLAT). Delokan-Delokan sing dibenakno pergambar kanggo nggawe layang kelas nang sembarang dibenakno (kita yatak tarjamahan karo cah-cah lan kelas barang nggo barang iki). Spanish Awak dhéwé ngerasakno sistem ing tindang karo telu architecture sing sampeyan uga bisa cebagian sing mungkin karo hal-sangan kuwi wis ngubah mungkin verb-like koding kanggo mulasakno. Rasané awak dhéwé iso nggambar nggo modèl anyar karo ngono cejang bantuan karo cejang.</abstract_jv>
      <abstract_sk>Zmožnosti današnjih sistemov za obdelavo naravnega jezika se običajno ocenjujejo z uporabo velikih podatkovnih naborov urejenih vprašanj in odgovorov. Čeprav so to kritična merila napredka, trpijo tudi zaradi umetne porazdelitve in nepopolnega znanja. Artifakti, ki izhajajo iz umetnih distribucij, lahko pretiravajo učinkovitost jezikovnega modela, medtem ko nepopolno znanje omejuje drobnozrnato analizo. V tem delu predstavljamo dopolnilni pristop primerjalne analize, ki temelji na SimPlified Language Activity Traces (SPLAT). SPLAT so korpusi jezikovnega kodiranja aktivnosti v nekaterih zaprtih domenih (v tem delu preučujemo sledi šaha in baseball iger). Zbirki podatkov SPLAT uporabljajo naravno nastajajoče distribucije, omogočajo ustvarjanje parov vprašanj-odgovorov v obsegu in omogočajo popolno znanje na svojih zaprtih področjih. Pokazali smo, da lahko jezikovni modeli treh različnih arhitektur odgovarjajo na vprašanja o svetovnih stanjih z uporabo samo glagolskih kodiranj aktivnosti. Naš pristop je razširjen na nove jezikovne modele in dodatne naloge odgovarjanja na vprašanja.</abstract_sk>
      <abstract_he>היכולות של מערכות העבודה טבעיות של השפה היום בדרך כלל מתערכות באמצעות קבוצות נתונים גדולות של שאלות ומשובות מטופשות. בעוד אלה נקודות רמז קריטיות של התקדמות, הם גם סובלים מחלשה בגלל פיצוצים מלאכותיים וידע לא מלא. עובדות אומנות שמוצאות מהפיצוחים מלאכותיים יכולות לעלות על ההופעה של דוגמני שפה, בעוד ידע לא מושלם מגביל ניתוח מעולה. בעבודה הזו, אנו מציגים גישה משותפת של שיקול רמז מבוססת על עקבות פעילות שפה סימפליפציה (SPLAT). SPLATs הם גופורה של קודים שפות של פעילות באיזה תחום סגור (אנחנו לומדים עקבות משחקי שחמט ובייסבול בעבודה הזאת). קומות נתונים SPLAT משתמשים בהפיצות נוצרות באופן טבעי, מאפשרים לדור זוגות תשובות-שאלות בקנה מידה, ולהרשות לעצמם ידע מושלם בתחומים סגורים שלהם. אנחנו מראים שדוגמני שפה של שלושה ארכיטקטורות שונות יכולים לענות על שאלות על מדינות העולם בשימוש רק קודים של פעילות דומים לשפתיים. הגישה שלנו נמשכת למודלים שפותיים חדשים ומשימות נוספות לענות על שאלות.</abstract_he>
      <abstract_ha>An ƙaddara abincin tsarin masu zartar da harshen kwanan a yau, a yi amfani da matsayin tsari masu yawa na tambayar da ake karya. Alhãli kuwa waɗannan masu kamfata matsayin mafaƙo, sun yi cũtar da rauni sabõda rabo na fassarar da ilmi ba'a cika. San'afactan da ke fara rabon da ko wani abu na fassara, za'a iya ƙara tsarin misalin harshen, kuma idan an cikakken ilmi na ƙayyade tsarin Ana-grafi. Daga wannan aikin, Munã ƙara wata matsayi da ba'a iya lissafa ba, a kan karatun Cikakken Aiki na SimPlad (SPLET). SPLATs are corpora of language encodings of activity in some closed domain (we study traces from chess and baseball games in this work).  KCharselect unicode block name Tuna nũna cewa misãlai na harshe na sakan masu turu masu cikin matsayi uku dabam-daban, sunã iya karɓa wa maswali masu hususann mataifa duniya da ke amfani da kodi kamar kodi na firam kawai. Mataimakinmu yana cikin misalin zane da wasu masu tambayar da ke karɓa.</abstract_ha>
      <abstract_bo>དེ་རིང་གི་སྤྱིར་བཏང་ནུས་ཀྱི་སྐད་རིགས་ལས་སྦྱོར་བའི་ལུགས་སྤྱོད་ཀྱི་ཆ་རྐྱེན་ཅིག་རྟགས་པར་མཐུན་རྐྱེན་ཡོད་པ དེ་དག་ནི་ཡར་རྒྱས་ཁབ་ཀྱི་གནད་དོན་དག་ཆེན་མཁན་མེད་པའི་རྐྱེན་ཁག་ཅིག་ཡིན་ནའང་ཡང་གནོད་པ་ཞིག་གི་ཡོད། Artifacts arising from artificial distributions can overstate language model performance, while incomplete knowledge limits fine-grained analysis. SimPlified Language Activity Traces (SPLAT). SPLATs are corpora of language encoding of activity in some closed domain (we study traces from chess and baseball games in this work). SPLAT datasets use naturally-arising distributions, allow the generation of question-answer pairs at scale, and afford complete knowledge in their closed domains. ང་ཚོས་སྐད ང་ཚོའི་གཟུགས་སྐོར</abstract_bo>
      </paper>
    <paper id="17">
      <title>Data Augmentation of Incorporating Real Error Patterns and Linguistic Knowledge for Grammatical Error Correction</title>
      <author><first>Xia</first><last>Li</last></author>
      <author><first>Junyi</first><last>He</last></author>
      <pages>223–233</pages>
      <abstract>Data augmentation aims at expanding training data with clean text using noising schemes to improve the performance of <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">grammatical error correction (GEC)</a>. In practice, there are a great number of real error patterns in the manually annotated training data. We argue that these real error patterns can be introduced into clean text to effectively generate more real and high quality synthetic data, which is not fully explored by previous studies. Moreover, we also find that linguistic knowledge can be incorporated into <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> for generating more representative and more diverse <a href="https://en.wikipedia.org/wiki/Synthetic_data">synthetic data</a>. In this paper, we propose a novel data augmentation method that fully considers the real error patterns and the linguistic knowledge for the GEC task. We conduct extensive experiments on public data sets and the experimental results show that our method outperforms several strong baselines with far less external unlabeled clean text data, highlighting its extraordinary effectiveness in the GEC task that lacks large-scale labeled training data.</abstract>
      <url hash="548cf5a9">2021.conll-1.17</url>
      <bibkey>li-he-2021-data</bibkey>
      <doi>10.18653/v1/2021.conll-1.17</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="19">
      <title>A Multilingual Benchmark for Probing Negation-Awareness with Minimal Pairs</title>
      <author><first>Mareike</first><last>Hartmann</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Yova</first><last>Kementchedjhieva</last></author>
      <author><first>Lukas</first><last>Nielsen</last></author>
      <author><first>Chen</first><last>Qiu</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>244–257</pages>
      <abstract>Negation is one of the most fundamental concepts in human cognition and language, and several natural language inference (NLI) probes have been designed to investigate pretrained language models’ ability to detect and reason with <a href="https://en.wikipedia.org/wiki/Affirmation_and_negation">negation</a>. However, the existing probing datasets are limited to English only, and do not enable controlled probing of performance in the absence or presence of <a href="https://en.wikipedia.org/wiki/Negation">negation</a>. In response, we present a multilingual (English, Bulgarian, German, French and Chinese) benchmark collection of NLI examples that are grammatical and correctly labeled, as a result of manual inspection and reformulation. We use the benchmark to probe the negation-awareness of multilingual language models and find that models that correctly predict examples with negation cues, often fail to correctly predict their counter-examples without negation cues, even when the cues are irrelevant for semantic inference.</abstract>
      <url hash="b5dee3ad">2021.conll-1.19</url>
      <bibkey>hartmann-etal-2021-multilingual</bibkey>
      <doi>10.18653/v1/2021.conll-1.19</doi>
      <pwccode url="https://github.com/mahartmann/negationminpairs" additional="false">mahartmann/negationminpairs</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="23">
      <title>A Coarse-to-Fine Labeling Framework for Joint Word Segmentation, POS Tagging, and Constituent Parsing<fixed-case>POS</fixed-case> Tagging, and Constituent Parsing</title>
      <author><first>Yang</first><last>Hou</last></author>
      <author><first>Houquan</first><last>Zhou</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Zhefeng</first><last>Wang</last></author>
      <author><first>Baoxing</first><last>Huai</last></author>
      <author><first>Nicholas Jing</first><last>Yuan</last></author>
      <pages>290–299</pages>
      <abstract>The most straightforward approach to joint word segmentation (WS), part-of-speech (POS) tagging, and constituent parsing is converting a word-level tree into a char-level tree, which, however, leads to two severe challenges. First, a larger label set (e.g.,   600) and longer inputs both increase computational costs. Second, it is difficult to rule out illegal trees containing conflicting production rules, which is important for reliable model evaluation. If a POS tag (like VV) is above a phrase tag (like VP) in the output tree, it becomes quite complex to decide word boundaries. To deal with both challenges, this work proposes a two-stage coarse-to-fine labeling framework for joint WS-POS-PAR. In the coarse labeling stage, the joint model outputs a bracketed tree, in which each node corresponds to one of four labels (i.e., phrase, subphrase, word, subword). The <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">tree</a> is guaranteed to be legal via constrained CKY decoding. In the fine labeling stage, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> expands each coarse label into a final label (such as VP, VP *, VV, VV *). Experiments on Chinese Penn Treebank 5.1 and 7.0 show that our joint model consistently outperforms the <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline approach</a> on both settings of <a href="https://en.wikipedia.org/wiki/W/o">w/o</a> and <a href="https://en.wikipedia.org/wiki/BERT">w/ BERT</a>, and achieves new state-of-the-art performance.</abstract>
      <url hash="c09860d3">2021.conll-1.23</url>
      <bibkey>hou-etal-2021-coarse</bibkey>
      <doi>10.18653/v1/2021.conll-1.23</doi>
      <pwccode url="https://github.com/ironsword666/jointparser" additional="false">ironsword666/jointparser</pwccode>
    </paper>
    <paper id="24">
      <title>Understanding the Extent to which Content Quality Metrics Measure the Information Quality of Summaries</title>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>300–309</pages>
      <abstract>Reference-based metrics such as <a href="https://en.wikipedia.org/wiki/ROUGE_(metric)">ROUGE</a> or BERTScore evaluate the content quality of a summary by comparing the summary to a reference. Ideally, this comparison should measure the summary’s information quality by calculating how much information the summaries have in common. In this work, we analyze the token alignments used by ROUGE and BERTScore to compare summaries and argue that their scores largely can not be interpreted as measuring information overlap. Rather, they are better estimates of the extent to which the summaries discuss the same topics. Further, we provide evidence that this result holds true for many other summarization evaluation metrics. The consequence of this result is that the most frequently used summarization evaluation metrics do not align with the community’s research goal, to generate summaries with high-quality information. However, we conclude by demonstrating that a recently proposed <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>, QAEval, which scores summaries using <a href="https://en.wikipedia.org/wiki/Question_answering">question-answering</a>, appears to better capture <a href="https://en.wikipedia.org/wiki/Information_quality">information quality</a> than current evaluations, highlighting a direction for future research.</abstract>
      <url hash="c39f1355">2021.conll-1.24</url>
      <bibkey>deutsch-roth-2021-understanding</bibkey>
      <doi>10.18653/v1/2021.conll-1.24</doi>
    </paper>
    <paper id="25">
      <title>Summary-Source Proposition-level Alignment : Task, Datasets and Supervised Baseline</title>
      <author><first>Ori</first><last>Ernst</last></author>
      <author><first>Ori</first><last>Shapira</last></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Michael</first><last>Lepioshkin</last></author>
      <author><first>Jacob</first><last>Goldberger</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>310–322</pages>
      <abstract>Aligning sentences in a reference summary with their counterparts in source documents was shown as a useful auxiliary summarization task, notably for generating training data for <a href="https://en.wikipedia.org/wiki/Salience_(neuroscience)">salience detection</a>. Despite its assessed utility, the alignment step was mostly approached with heuristic unsupervised methods, typically ROUGE-based, and was never independently optimized or evaluated. In this paper, we propose establishing summary-source alignment as an explicit task, while introducing two major novelties : (1) applying it at the more accurate proposition span level, and (2) approaching it as a supervised classification task. To that end, we created a novel <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training dataset</a> for proposition-level alignment, derived automatically from available summarization evaluation data. In addition, we crowdsourced dev and test datasets, enabling model development and proper evaluation. Utilizing these data, we present a supervised proposition alignment baseline model, showing improved alignment-quality over the unsupervised approach.</abstract>
      <url hash="e9ca9b2e">2021.conll-1.25</url>
      <bibkey>ernst-etal-2021-summary</bibkey>
      <doi>10.18653/v1/2021.conll-1.25</doi>
      <pwccode url="https://github.com/oriern/SuperPAL" additional="false">oriern/SuperPAL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
    </paper>
    <paper id="27">
      <title>Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning</title>
      <author><first>Christos</first><last>Theodoropoulos</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <author><first>Andrei Catalin</first><last>Coman</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>337–348</pages>
      <abstract>Though language model text embeddings have revolutionized NLP research, their ability to capture high-level semantic information, such as relations between entities in text, is limited. In this paper, we propose a novel contrastive learning framework that trains <a href="https://en.wikipedia.org/wiki/Sentence_embedding">sentence embeddings</a> to encode the relations in a <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph structure</a>. Given a sentence (unstructured text) and its <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>, we use contrastive learning to impose relation-related structure on the token level representations of the sentence obtained with a CharacterBERT (El Boukkouri et al., 2020) model. The resulting relation-aware sentence embeddings achieve state-of-the-art results on the relation extraction task using only a simple KNN classifier, thereby demonstrating the success of the proposed method. Additional visualization by a tSNE analysis shows the effectiveness of the learned <a href="https://en.wikipedia.org/wiki/Representation_space">representation space</a> compared to baselines. Furthermore, we show that we can learn a different space for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, again using a contrastive learning objective, and demonstrate how to successfully combine both representation spaces in an entity-relation task.</abstract>
      <url hash="71988cb1">2021.conll-1.27</url>
      <bibkey>theodoropoulos-etal-2021-imposing</bibkey>
      <doi>10.18653/v1/2021.conll-1.27</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ade-corpus">Adverse Drug Events (ADE) Corpus</pwcdataset>
    </paper>
    <paper id="29">
      <title>Pragmatic competence of pre-trained language models through the lens of discourse connectives</title>
      <author><first>Lalchand</first><last>Pandia</last></author>
      <author><first>Yan</first><last>Cong</last></author>
      <author><first>Allyson</first><last>Ettinger</last></author>
      <pages>367–379</pages>
      <abstract>As pre-trained language models (LMs) continue to dominate <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>, it is increasingly important that we understand the depth of language capabilities in these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. In this paper, we target pre-trained LMs’ competence in <a href="https://en.wikipedia.org/wiki/Pragmatics">pragmatics</a>, with a focus on <a href="https://en.wikipedia.org/wiki/Pragmatics">pragmatics</a> relating to discourse connectives. We formulate cloze-style tests using a combination of naturally-occurring data and controlled inputs drawn from <a href="https://en.wikipedia.org/wiki/Psycholinguistics">psycholinguistics</a>. We focus on testing <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>’ ability to use pragmatic cues to predict discourse connectives, <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>’ ability to understand implicatures relating to connectives, and the extent to which <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> show humanlike preferences regarding temporal dynamics of connectives. We find that although models predict connectives reasonably well in the context of naturally-occurring data, when we control contexts to isolate high-level pragmatic cues, model sensitivity is much lower. Models also do not show substantial humanlike temporal preferences. Overall, the findings suggest that at present, dominant pre-training paradigms do not result in substantial pragmatic competence in our models.</abstract>
      <url hash="f0c810df">2021.conll-1.29</url>
      <bibkey>pandia-etal-2021-pragmatic</bibkey>
      <doi>10.18653/v1/2021.conll-1.29</doi>
    </paper>
    <paper id="32">
      <title>Scaffolded input promotes atomic organization in the recurrent neural network language model</title>
      <author><first>Philip A.</first><last>Huebner</last></author>
      <author><first>Jon A.</first><last>Willits</last></author>
      <pages>408–422</pages>
      <abstract>The recurrent neural network (RNN) language model is a powerful tool for learning arbitrary sequential dependencies in language data. Despite its enormous success in representing <a href="https://en.wikipedia.org/wiki/Lexical_item">lexical sequences</a>, little is known about the quality of the <a href="https://en.wikipedia.org/wiki/Lexical_item">lexical representations</a> that it acquires. In this work, we conjecture that it is straightforward to extract <a href="https://en.wikipedia.org/wiki/Lexical_analysis">lexical representations</a> (i.e. static word embeddings) from an RNN, but that the amount of semantic information that is encoded is limited when lexical items in the training data provide redundant semantic information. We conceptualize this limitation of the RNN as a failure to learn atomic internal states-states which capture information relevant to single word types without being influenced by redundant information provided by words with which they co-occur. Using a corpus of artificial language, we verify that redundancy in the training data yields non-atomic internal states, and propose a novel method for inducing atomic internal states. We show that 1) our method successfully induces atomic internal organization in controlled experiments, and 2) under more realistic conditions in which the training consists of child-directed language, application of our method improves the performance of lexical representations on a downstream semantic categorization task.</abstract>
      <url hash="5b2e87c6">2021.conll-1.32</url>
      <bibkey>huebner-willits-2021-scaffolded</bibkey>
      <doi>10.18653/v1/2021.conll-1.32</doi>
    </paper>
    <paper id="35">
      <title>Relation-aware Bidirectional Path Reasoning for Commonsense Question Answering</title>
      <author><first>Junxing</first><last>Wang</last></author>
      <author><first>Xinyi</first><last>Li</last></author>
      <author><first>Zhen</first><last>Tan</last></author>
      <author><first>Xiang</first><last>Zhao</last></author>
      <author><first>Weidong</first><last>Xiao</last></author>
      <pages>445–453</pages>
      <abstract>Commonsense Question Answering is an important natural language processing (NLP) task that aims to predict the correct answer to a question through <a href="https://en.wikipedia.org/wiki/Commonsense_reasoning">commonsense reasoning</a>. Previous studies utilize pre-trained models on large-scale corpora such as BERT, or perform <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a> on <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a>. However, these methods do not explicitly model the <a href="https://en.wikipedia.org/wiki/Binary_relation">relations</a> that connect entities, which are informational and can be used to enhance <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a>. To address this issue, we propose a relation-aware reasoning method. Our method uses a relation-aware graph neural network to capture the rich contextual information from both entities and relations. Compared with methods that use fixed relation embeddings from pre-trained models, our model dynamically updates relations with contextual information from a multi-source subgraph, built from multiple external knowledge sources. The enhanced representations of relations are then fed to a bidirectional reasoning module. A bidirectional attention mechanism is applied between the question sequence and the paths that connect entities, which provides us with transparent interpretability. Experimental results on the CommonsenseQA dataset illustrate that our method results in significant improvements over the baselines while also providing clear reasoning paths.<i>relations</i> that connect entities, which are informational and can be used to enhance reasoning. To address this issue, we propose a relation-aware reasoning method. Our method uses a relation-aware graph neural network to capture the rich contextual information from both entities and relations. Compared with methods that use fixed relation embeddings from pre-trained models, our model dynamically updates relations with contextual information from a multi-source subgraph, built from multiple external knowledge sources. The enhanced representations of relations are then fed to a bidirectional reasoning module. A bidirectional attention mechanism is applied between the question sequence and the paths that connect entities, which provides us with transparent interpretability. Experimental results on the CommonsenseQA dataset illustrate that our method results in significant improvements over the baselines while also providing clear reasoning paths.</abstract>
      <url hash="235c3f2d">2021.conll-1.35</url>
      <bibkey>wang-etal-2021-relation</bibkey>
      <doi>10.18653/v1/2021.conll-1.35</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="38">
      <title>Commonsense Knowledge in <a href="https://en.wikipedia.org/wiki/Word_association">Word Associations</a> and ConceptNet<fixed-case>C</fixed-case>oncept<fixed-case>N</fixed-case>et</title>
      <author><first>Chunhua</first><last>Liu</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <pages>481–495</pages>
      <abstract>Humans use countless basic, shared facts about the world to efficiently navigate in their environment. This <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a> is rarely communicated explicitly, however, understanding how <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a> is represented in different paradigms is important for (a) a deeper understanding of human cognition and (b) augmenting automatic reasoning systems. This paper presents an in-depth comparison of two large-scale resources of general knowledge : <a href="https://en.wikipedia.org/wiki/ConceptNet">ConceptNet</a>, an engineered relational database, and SWOW, a <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> derived from crowd-sourced word associations. We examine the structure, overlap and differences between the two <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a>, as well as the extent of situational commonsense knowledge present in the two <a href="https://en.wikipedia.org/wiki/Factors_of_production">resources</a>. We finally show empirically that both resources improve downstream task performance on commonsense reasoning benchmarks over text-only baselines, suggesting that large-scale word association data, which have been obtained for several languages through crowd-sourcing, can be a valuable complement to curated knowledge graphs.</abstract>
      <url hash="c036cde7">2021.conll-1.38</url>
      <bibkey>liu-etal-2021-commonsense</bibkey>
      <doi>10.18653/v1/2021.conll-1.38</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mcscript">MCScript</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openbookqa">OpenBookQA</pwcdataset>
    </paper>
    <paper id="39">
      <title>Cross-document Event Identity via Dense Annotation</title>
      <author><first>Adithya</first><last>Pratapa</last></author>
      <author><first>Zhengzhong</first><last>Liu</last></author>
      <author><first>Kimihiro</first><last>Hasegawa</last></author>
      <author><first>Linwei</first><last>Li</last></author>
      <author><first>Yukari</first><last>Yamakawa</last></author>
      <author><first>Shikun</first><last>Zhang</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <pages>496–517</pages>
      <abstract>In this paper, we study the identity of textual events from different documents. While the complex nature of event identity is previously studied (Hovy et al., 2013), the case of events across documents is unclear. Prior work on cross-document event coreference has two main drawbacks. First, they restrict the <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> to a limited set of event types. Second, they insufficiently tackle the concept of event identity. Such annotation setup reduces the pool of event mentions and prevents one from considering the possibility of quasi-identity relations. We propose a dense annotation approach for cross-document event coreference, comprising a rich source of event mentions and a dense annotation effort between related document pairs. To this end, we design a new annotation workflow with careful quality control and an easy-to-use annotation interface. In addition to the links, we further collect overlapping event contexts, including time, location, and participants, to shed some light on the relation between <a href="https://en.wikipedia.org/wiki/Identity_(social_science)">identity decisions</a> and context. We present an open-access dataset for cross-document event coreference, CDEC-WN, collected from English Wikinews and open-source our annotation toolkit to encourage further research on cross-document tasks.</abstract>
      <url hash="7d643b29">2021.conll-1.39</url>
      <bibkey>pratapa-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.conll-1.39</doi>
      <pwccode url="https://github.com/adithya7/cdec-wikinews" additional="false">adithya7/cdec-wikinews</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="41">
      <title>Negation-Instance Based Evaluation of End-to-End Negation Resolution</title>
      <author><first>Elizaveta</first><last>Sineva</last></author>
      <author><first>Stefan</first><last>Grünewald</last></author>
      <author><first>Annemarie</first><last>Friedrich</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <pages>528–543</pages>
      <abstract>In this paper, we revisit the task of negation resolution, which includes the subtasks of cue detection (e.g. not, never) and scope resolution. In the context of previous shared tasks, a variety of evaluation metrics have been proposed. Subsequent works usually use different subsets of these, including variations and custom implementations, rendering meaningful comparisons between systems difficult. Examining the problem both from a linguistic perspective and from a downstream viewpoint, we here argue for a negation-instance based approach to evaluating negation resolution. Our proposed <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> correspond to expectations over per-instance scores and hence are intuitively interpretable. To render research comparable and to foster future work, we provide results for a set of current state-of-the-art systems for negation resolution on three English corpora, and make our implementation of the evaluation scripts publicly available.</abstract>
      <url hash="a1f1ddc1">2021.conll-1.41</url>
      <bibkey>sineva-etal-2021-negation</bibkey>
      <doi>10.18653/v1/2021.conll-1.41</doi>
      <pwccode url="https://github.com/boschresearch/negation_resolution_evaluation_conll2021" additional="false">boschresearch/negation_resolution_evaluation_conll2021</pwccode>
    </paper>
    <paper id="42">
      <title>Controlling Prosody in End-to-End TTS : A Case Study on Contrastive Focus Generation<fixed-case>TTS</fixed-case>: A Case Study on Contrastive Focus Generation</title>
      <author><first>Siddique</first><last>Latif</last></author>
      <author><first>Inyoung</first><last>Kim</last></author>
      <author><first>Ioan</first><last>Calapodescu</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>544–551</pages>
      <abstract>While End-2-End Text-to-Speech (TTS) has made significant progresses over the past few years, these systems still lack intuitive user controls over <a href="https://en.wikipedia.org/wiki/Prosody_(linguistics)">prosody</a>. For instance, generating <a href="https://en.wikipedia.org/wiki/Speech">speech</a> with fine-grained prosody control (prosodic prominence, contextually appropriate emotions) is still an open challenge. In this paper, we investigate whether we can control <a href="https://en.wikipedia.org/wiki/Prosody_(linguistics)">prosody</a> directly from the input text, in order to code information related to contrastive focus which emphasizes a specific word that is contrary to the presuppositions of the interlocutor. We build and share a specific <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for this purpose and show that it allows to train a TTS system were this fine-grained prosodic feature can be correctly conveyed using control tokens. Our evaluation compares synthetic and natural utterances and shows that prosodic patterns of contrastive focus (variations of Fo, Intensity and Duration) can be learnt accurately. Such a milestone is important to allow, for example, <a href="https://en.wikipedia.org/wiki/Smart_speaker">smart speakers</a> to be programmatically controlled in terms of output prosody.</abstract>
      <url hash="1ea87db4">2021.conll-1.42</url>
      <bibkey>latif-etal-2021-controlling</bibkey>
      <doi>10.18653/v1/2021.conll-1.42</doi>
    </paper>
    <paper id="43">
      <title>A Large-scale Comprehensive Abusiveness Detection Dataset with Multifaceted Labels from Reddit<fixed-case>R</fixed-case>eddit</title>
      <author><first>Hoyun</first><last>Song</last></author>
      <author><first>Soo Hyun</first><last>Ryu</last></author>
      <author><first>Huije</first><last>Lee</last></author>
      <author><first>Jong</first><last>Park</last></author>
      <pages>552–561</pages>
      <abstract>As users in <a href="https://en.wikipedia.org/wiki/Online_community">online communities</a> suffer from severe side effects of <a href="https://en.wikipedia.org/wiki/Abuse">abusive language</a>, many researchers attempted to detect abusive texts from <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, presenting several <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> for such detection. However, none of them contain both comprehensive labels and contextual information, which are essential for thoroughly detecting all kinds of abusiveness from texts, since datasets with such fine-grained features demand a significant amount of <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a>, leading to much increased <a href="https://en.wikipedia.org/wiki/Complexity">complexity</a>. In this paper, we propose a Comprehensive Abusiveness Detection Dataset (CADD), collected from the English Reddit posts, with multifaceted labels and contexts. Our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is annotated hierarchically for an efficient <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> through <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a> on a large-scale. We also empirically explore the characteristics of our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and provide a detailed analysis for novel insights. The results of our experiments with strong pre-trained natural language understanding models on our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> show that our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> gives rise to meaningful performance, assuring its practicality for abusive language detection.</abstract>
      <url hash="beecf2a8">2021.conll-1.43</url>
      <bibkey>song-etal-2021-large</bibkey>
      <doi>10.18653/v1/2021.conll-1.43</doi>
      <pwccode url="https://github.com/nlpcl-lab/cadd_dataset" additional="false">nlpcl-lab/cadd_dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="51">
      <title>Predicting non-native speech perception using the Perceptual Assimilation Model and state-of-the-art acoustic models</title>
      <author><first>Juliette</first><last>Millet</last></author>
      <author><first>Ioana</first><last>Chitoran</last></author>
      <author><first>Ewan</first><last>Dunbar</last></author>
      <pages>661–673</pages>
      <abstract>Our <a href="https://en.wikipedia.org/wiki/First_language">native language</a> influences the way we perceive <a href="https://en.wikipedia.org/wiki/Phone_(phonetics)">speech sounds</a>, affecting our ability to discriminate non-native sounds. We compare two ideas about the influence of the <a href="https://en.wikipedia.org/wiki/First_language">native language</a> on <a href="https://en.wikipedia.org/wiki/Speech_perception">speech perception</a> : the Perceptual Assimilation Model, which appeals to a mental classification of sounds into native phoneme categories, versus the idea that rich, fine-grained phonetic representations tuned to the statistics of the native language, are sufficient. We operationalise this idea using representations from two state-of-the-art speech models, a Dirichlet process Gaussian mixture model and the more recent wav2vec 2.0 model. We present a new, open dataset of French- and English-speaking participants’ speech perception behaviour for 61 vowel sounds from six languages. We show that phoneme assimilation is a better predictor than fine-grained phonetic modelling, both for the discrimination behaviour as a whole, and for predicting differences in discriminability associated with differences in native language background. We also show that wav2vec 2.0, while not good at capturing the effects of <a href="https://en.wikipedia.org/wiki/First_language">native language</a> on <a href="https://en.wikipedia.org/wiki/Speech_perception">speech perception</a>, is complementary to information about native phoneme assimilation, and provides a good model of low-level phonetic representations, supporting the idea that both categorical and fine-grained perception are used during <a href="https://en.wikipedia.org/wiki/Speech_perception">speech perception</a>.</abstract>
      <url hash="97724f5f">2021.conll-1.51</url>
      <bibkey>millet-etal-2021-predicting</bibkey>
      <doi>10.18653/v1/2021.conll-1.51</doi>
    </paper>
    </volume>
</collection>