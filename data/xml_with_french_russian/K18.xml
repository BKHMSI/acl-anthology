<?xml version='1.0' encoding='utf-8'?>
<collection id="K18">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 22nd Conference on Computational Natural Language Learning</booktitle>
      <url hash="6bce8718">K18-1</url>
      <editor><first>Anna</first><last>Korhonen</last></editor>
      <editor><first>Ivan</first><last>Titov</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Brussels, Belgium</address>
      <month>October</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="9b858de5">K18-1000</url>
      <bibkey>conll-2018-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Embedded-State Latent Conditional Random Fields for Sequence Labeling</title>
      <author><first>Dung</first><last>Thai</last></author>
      <author><first>Sree Harsha</first><last>Ramesh</last></author>
      <author><first>Shikhar</first><last>Murty</last></author>
      <author><first>Luke</first><last>Vilnis</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>1–10</pages>
      <url hash="1bfb5f25">K18-1001</url>
      <abstract>Complex textual information extraction tasks are often posed as <a href="https://en.wikipedia.org/wiki/Sequence_labeling">sequence labeling</a> or <a href="https://en.wikipedia.org/wiki/Shallow_parsing">shallow parsing</a>, where fields are extracted using local labels made consistent through <a href="https://en.wikipedia.org/wiki/Statistical_inference">probabilistic inference</a> in a <a href="https://en.wikipedia.org/wiki/Graphical_model">graphical model</a> with constrained transitions. Recently, it has become common to locally parametrize these models using rich features extracted by <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a> (such as LSTM), while enforcing consistent outputs through a simple linear-chain model, representing <a href="https://en.wikipedia.org/wiki/Markov_chain">Markovian dependencies</a> between successive labels. However, the simple <a href="https://en.wikipedia.org/wiki/Graphical_model">graphical model structure</a> belies the often complex <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">non-local constraints</a> between output labels. For example, many fields, such as a first name, can only occur a fixed number of times, or in the presence of other fields. While RNNs have provided increasingly powerful context-aware local features for sequence tagging, they have yet to be integrated with a global graphical model of similar expressivity in the output distribution. Our model goes beyond the linear chain CRF to incorporate multiple hidden states per output label, but parametrizes them parsimoniously with low-rank log-potential scoring matrices, effectively learning an embedding space for hidden states. This augmented latent space of inference variables complements the rich feature representation of the RNN, and allows exact global inference obeying complex, learned non-local output constraints. We experiment with several datasets and show that the model outperforms baseline CRF+RNN models when global output constraints are necessary at inference-time, and explore the interpretable latent structure.<i>shallow parsing</i>, where fields are extracted using local labels made consistent through probabilistic inference in a graphical model with constrained transitions. Recently, it has become common to locally parametrize these models using rich features extracted by recurrent neural networks (such as LSTM), while enforcing consistent outputs through a simple linear-chain model, representing Markovian dependencies between successive labels. However, the simple graphical model structure belies the often complex non-local constraints between output labels. For example, many fields, such as a first name, can only occur a fixed number of times, or in the presence of other fields. While RNNs have provided increasingly powerful context-aware local features for sequence tagging, they have yet to be integrated with a global graphical model of similar expressivity in the output distribution. Our model goes beyond the linear chain CRF to incorporate multiple hidden states per output label, but parametrizes them parsimoniously with low-rank log-potential scoring matrices, effectively learning an embedding space for hidden states. This augmented latent space of inference variables complements the rich feature representation of the RNN, and allows exact global inference obeying complex, learned non-local output constraints. We experiment with several datasets and show that the model outperforms baseline CRF+RNN models when global output constraints are necessary at inference-time, and explore the interpretable latent structure.</abstract>
      <doi>10.18653/v1/K18-1001</doi>
      <bibkey>thai-etal-2018-embedded</bibkey>
    </paper>
    <paper id="2">
      <title>Continuous Word Embedding Fusion via <a href="https://en.wikipedia.org/wiki/Spectral_decomposition">Spectral Decomposition</a></title>
      <author><first>Tianfan</first><last>Fu</last></author>
      <author><first>Cheng</first><last>Zhang</last></author>
      <author><first>Stephan</first><last>Mandt</last></author>
      <pages>11–20</pages>
      <url hash="3446ee86">K18-1002</url>
      <abstract>Word embeddings have become a mainstream tool in <a href="https://en.wikipedia.org/wiki/Statistical_natural_language_processing">statistical natural language processing</a>. Practitioners often use pre-trained word vectors, which were trained on large generic text corpora, and which are readily available on the web. However, pre-trained word vectors oftentimes lack important words from specific domains. It is therefore often desirable to extend the <a href="https://en.wikipedia.org/wiki/Vocabulary">vocabulary</a> and embed new words into a set of pre-trained word vectors. In this paper, we present an efficient method for including new words from a specialized corpus, containing <a href="https://en.wikipedia.org/wiki/Word_formation">new words</a>, into pre-trained generic word embeddings. We build on the established view of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> as <a href="https://en.wikipedia.org/wiki/Matrix_decomposition">matrix factorizations</a> to present a spectral algorithm for this task. Experiments on several domain-specific corpora with specialized vocabularies demonstrate that our method is able to embed the new words efficiently into the original embedding space. Compared to competing <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a>, our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> is faster, parameter-free, and deterministic.</abstract>
      <doi>10.18653/v1/K18-1002</doi>
      <bibkey>fu-etal-2018-continuous</bibkey>
    </paper>
    <paper id="4">
      <title>A Trio Neural Model for Dynamic Entity Relatedness Ranking</title>
      <author><first>Tu</first><last>Nguyen</last></author>
      <author><first>Tuan</first><last>Tran</last></author>
      <author><first>Wolfgang</first><last>Nejdl</last></author>
      <pages>31–41</pages>
      <url hash="9015a14e">K18-1004</url>
      <abstract>Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity relatedness</a> in a static setting and <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised manner</a>. However, entities in real-world are often involved in many different relationships, consequently entity relations are very dynamic over time. In this work, we propose a neural network-based approach that leverages public attention as <a href="https://en.wikipedia.org/wiki/Supervisor">supervision</a>. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on <a href="https://en.wikipedia.org/wiki/Data_set">large-scale datasets</a>, we demonstrate that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> achieves better results than competitive baselines.</abstract>
      <doi>10.18653/v1/K18-1004</doi>
      <bibkey>nguyen-etal-2018-trio</bibkey>
    </paper>
    <paper id="5">
      <title>A Unified Neural Network Model for Geolocating Twitter Users<fixed-case>T</fixed-case>witter Users</title>
      <author><first>Mohammad</first><last>Ebrahimi</last></author>
      <author><first>Elaheh</first><last>ShafieiBavani</last></author>
      <author><first>Raymond</first><last>Wong</last></author>
      <author><first>Fang</first><last>Chen</last></author>
      <pages>42–53</pages>
      <url hash="453ebfae">K18-1005</url>
      <abstract>Locations of social media users are important to many applications such as rapid disaster response, <a href="https://en.wikipedia.org/wiki/Targeted_advertising">targeted advertisement</a>, and <a href="https://en.wikipedia.org/wiki/Recommender_system">news recommendation</a>. However, many users do not share their exact <a href="https://en.wikipedia.org/wiki/Geographic_coordinate_system">geographical coordinates</a> due to reasons such as privacy concerns. The lack of explicit location information has motivated a growing body of research in recent years looking at different automatic ways of determining the user’s primary location. In this paper, we propose a unified user geolocation method which relies on a fusion of neural networks. Our joint model incorporates different types of available information including tweet text, user network, and <a href="https://en.wikipedia.org/wiki/Metadata">metadata</a> to predict users’ locations. Moreover, we utilize a bidirectional LSTM network augmented with an attention mechanism to identify the most location indicative words in textual content of tweets. The experiments demonstrate that our approach achieves state-of-the-art performance over two Twitter benchmark geolocation datasets. We also conduct an ablation study to evaluate the contribution of each type of information in user geolocation performance.</abstract>
      <doi>10.18653/v1/K18-1005</doi>
      <bibkey>ebrahimi-etal-2018-unified</bibkey>
    </paper>
    <paper id="8">
      <title>From Strings to Other Things : Linking the Neighborhood and Transposition Effects in Word Reading</title>
      <author><first>Stéphan</first><last>Tulkens</last></author>
      <author><first>Dominiek</first><last>Sandra</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>75–85</pages>
      <url hash="433baa6e">K18-1008</url>
      <abstract>We investigate the relation between the transposition and deletion effects in word reading, i.e., the finding that readers can successfully read SLAT as SALT, or WRK as WORK, and the <a href="https://en.wikipedia.org/wiki/Neighbourhood_effect">neighborhood effect</a>. In particular, we investigate whether lexical orthographic neighborhoods take into account transposition and <a href="https://en.wikipedia.org/wiki/Deletion_(linguistics)">deletion</a> in determining neighbors. If this is the case, it is more likely that the <a href="https://en.wikipedia.org/wiki/Neighbourhood_effect">neighborhood effect</a> takes place early during processing, and does not solely rely on similarity of internal representations. We introduce a new neighborhood measure, rd20, which can be used to quantify <a href="https://en.wikipedia.org/wiki/Neighbourhood_effect">neighborhood effects</a> over arbitrary feature spaces. We calculate the rd20 over large sets of words in three languages using various feature sets and show that feature sets that do not allow for transposition or <a href="https://en.wikipedia.org/wiki/Deletion_(linguistics)">deletion</a> explain more variance in Reaction Time (RT) measurements. We also show that the rd20 can be calculated using the hidden state representations of an Multi-Layer Perceptron, and show that these explain less variance than the raw features. We conclude that the <a href="https://en.wikipedia.org/wiki/Neighbourhood_effect">neighborhood effect</a> is unlikely to have a perceptual basis, but is more likely to be the result of items co-activating after recognition. All code is available at :<url>www.github.com/clips/conll2018</url>
      </abstract>
      <doi>10.18653/v1/K18-1008</doi>
      <bibkey>tulkens-etal-2018-strings</bibkey>
      <pwccode url="https://github.com/clips/conll2018" additional="false">clips/conll2018</pwccode>
    </paper>
    <paper id="9">
      <title>Global Attention for Name Tagging</title>
      <author><first>Boliang</first><last>Zhang</last></author>
      <author><first>Spencer</first><last>Whitehead</last></author>
      <author><first>Lifu</first><last>Huang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>86–96</pages>
      <url hash="763a42dd">K18-1009</url>
      <abstract>Many name tagging approaches use <a href="https://en.wikipedia.org/wiki/Context_(language_use)">local contextual information</a> with much success, but can fail when the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">local context</a> is ambiguous or limited. We present a new <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> to improve <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">name tagging</a> by utilizing local, document-level, and corpus-level contextual information. For each word, we retrieve document-level context from other sentences within the same document and corpus-level context from sentences in other documents. We propose a model that learns to incorporate document-level and corpus-level contextual information alongside local contextual information via document-level and corpus-level attentions, which dynamically weight their respective contextual information and determines the influence of this <a href="https://en.wikipedia.org/wiki/Information">information</a> through gating mechanisms. Experiments on benchmark datasets show the effectiveness of our approach, which achieves state-of-the-art results for <a href="https://en.wikipedia.org/wiki/Dutch_language">Dutch</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a>, and <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a> on the CoNLL-2002 and CoNLL-2003 datasets. We will make our code and pre-trained models publicly available for research purposes.</abstract>
      <doi>10.18653/v1/K18-1009</doi>
      <bibkey>zhang-etal-2018-global</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="12">
      <title>Uncovering Code-Mixed Challenges : A Framework for Linguistically Driven Question Generation and Neural Based Question Answering</title>
      <author><first>Deepak</first><last>Gupta</last></author>
      <author><first>Pabitra</first><last>Lenka</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>119–130</pages>
      <url hash="67b55f94">K18-1012</url>
      <abstract>Existing research on question answering (QA) and comprehension reading (RC) are mainly focused on the resource-rich language like <a href="https://en.wikipedia.org/wiki/English_language">English</a>. In recent times, the rapid growth of multi-lingual web content has posed several challenges to the existing <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA systems</a>. Code-mixing is one such challenge that makes the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> more complex. In this paper, we propose a linguistically motivated technique for code-mixed question generation (CMQG) and a neural network based architecture for code-mixed question answering (CMQA). For evaluation, we manually create the code-mixed questions for Hindi-English language pair. In order to show the effectiveness of our neural network based CMQA technique, we utilize two benchmark datasets, SQuAD and MMQA. Experiments show that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves encouraging performance on CMQG and CMQA.</abstract>
      <doi>10.18653/v1/K18-1012</doi>
      <bibkey>gupta-etal-2018-uncovering</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="14">
      <title>Commonsense Knowledge Base Completion and Generation</title>
      <author><first>Itsumi</first><last>Saito</last></author>
      <author><first>Kyosuke</first><last>Nishida</last></author>
      <author><first>Hisako</first><last>Asano</last></author>
      <author><first>Junji</first><last>Tomita</last></author>
      <pages>141–150</pages>
      <url hash="346da5a4">K18-1014</url>
      <abstract>This study focuses on acquisition of commonsense knowledge. A previous study proposed a commonsense knowledge base completion (CKB completion) method that predicts a confidence score of for triplet-style knowledge for improving the coverage of CKBs. To improve the accuracy of CKB completion and expand the size of CKBs, we formulate a new commonsense knowledge base generation task (CKB generation) and propose a joint learning method that incorporates both CKB completion and CKB generation. Experimental results show that the joint learning method improved completion accuracy and the generation model created reasonable knowledge. Our generation model could also be used to augment data and improve the accuracy of completion.</abstract>
      <doi>10.18653/v1/K18-1014</doi>
      <bibkey>saito-etal-2018-commonsense</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="15">
      <title>Active Learning for Interactive Neural Machine Translation of Data Streams</title>
      <author><first>Álvaro</first><last>Peris</last></author>
      <author><first>Francisco</first><last>Casacuberta</last></author>
      <pages>151–160</pages>
      <url hash="5230d0b7">K18-1015</url>
      <abstract>We study the application of active learning techniques to the translation of unbounded data streams via interactive neural machine translation. The main idea is to select, from an unbounded stream of source sentences, those worth to be supervised by a human agent. The user will interactively translate those samples. Once validated, these <a href="https://en.wikipedia.org/wiki/Data">data</a> is useful for adapting the neural machine translation model. We propose two novel <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for selecting the samples to be validated. We exploit the information from the <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a> of a neural machine translation system. Our experiments show that the inclusion of active learning techniques into this <a href="https://en.wikipedia.org/wiki/Pipeline_(software)">pipeline</a> allows to reduce the effort required during the process, while increasing the quality of the translation system. Moreover, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> enables to balance the human effort required for achieving a certain translation quality. Moreover, our <a href="https://en.wikipedia.org/wiki/Nervous_system">neural system</a> outperforms classical approaches by a large margin.</abstract>
      <doi>10.18653/v1/K18-1015</doi>
      <bibkey>peris-casacuberta-2018-active</bibkey>
      <pwccode url="https://github.com/lvapeab/nmt-keras" additional="false">lvapeab/nmt-keras</pwccode>
    </paper>
    <paper id="16">
      <title>Churn Intent Detection in Multilingual Chatbot Conversations and <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a></title>
      <author><first>Christian</first><last>Abbet</last></author>
      <author><first>Meryem</first><last>M’hamdi</last></author>
      <author><first>Athanasios</first><last>Giannakopoulos</last></author>
      <author><first>Robert</first><last>West</last></author>
      <author><first>Andreea</first><last>Hossmann</last></author>
      <author><first>Michael</first><last>Baeriswyl</last></author>
      <author><first>Claudiu</first><last>Musat</last></author>
      <pages>161–170</pages>
      <url hash="7c4dd91f">K18-1016</url>
      <abstract>We propose a new method to detect when users express the intent to leave a service, also known as churn. While previous work focuses solely on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, we show that this <a href="https://en.wikipedia.org/wiki/Intention">intent</a> can be detected in chatbot conversations. As companies increasingly rely on <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a> they need an overview of potentially churny users. To this end, we crowdsource and publish a dataset of churn intent expressions in chatbot interactions in <a href="https://en.wikipedia.org/wiki/German_language">German</a> and <a href="https://en.wikipedia.org/wiki/English_language">English</a>. We show that <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> trained on <a href="https://en.wikipedia.org/wiki/Social_media">social media data</a> can detect the same intent in the context of <a href="https://en.wikipedia.org/wiki/Chatbot">chatbots</a>. We introduce a classification architecture that outperforms existing work on churn intent detection in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. Moreover, we show that, using bilingual word embeddings, a system trained on combined English and German data outperforms monolingual approaches. As the only existing dataset is in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, we crowdsource and publish a novel dataset of German tweets. We thus underline the universal aspect of the problem, as examples of <a href="https://en.wikipedia.org/wiki/Churn">churn intent</a> in English help us identify <a href="https://en.wikipedia.org/wiki/Churn">churn</a> in German tweets and chatbot conversations.</abstract>
      <doi>10.18653/v1/K18-1016</doi>
      <bibkey>abbet-etal-2018-churn</bibkey>
      <pwccode url="https://github.com/swisscom/churn-intent-DE" additional="false">swisscom/churn-intent-DE</pwccode>
    </paper>
    <paper id="20">
      <title>Latent Entities Extraction : How to Extract Entities that Do Not Appear in the Text?</title>
      <author><first>Eylon</first><last>Shoshan</last></author>
      <author><first>Kira</first><last>Radinsky</last></author>
      <pages>200–210</pages>
      <url hash="218b9c44">K18-1020</url>
      <abstract>Named-entity Recognition (NER) is an important task in the NLP field, and is widely used to solve many challenges. However, in many scenarios, not all of the entities are explicitly mentioned in the text. Sometimes they could be inferred from the context or from other indicative words. Consider the following sentence : CMA can easily hydrolyze into free acetic acid. Although water is not mentioned explicitly, one can infer that <a href="https://en.wikipedia.org/wiki/Hydrogen_peroxide">H2O</a> is an entity involved in the process. In this work, we present the problem of Latent Entities Extraction (LEE). We present several methods for determining whether entities are discussed in a text, even though, potentially, they are not explicitly written. Specifically, we design a neural model that handles extraction of multiple entities jointly. We show that our model, along with multi-task learning approach and a novel task grouping algorithm, reaches high performance in identifying latent entities. Our experiments are conducted on a large <a href="https://en.wikipedia.org/wiki/Data_set">biological dataset</a> from the <a href="https://en.wikipedia.org/wiki/Biochemistry">biochemical field</a>. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> contains text descriptions of biological processes, and for each <a href="https://en.wikipedia.org/wiki/Process_(engineering)">process</a>, all of the involved entities in the process are labeled, including implicitly mentioned ones. We believe LEE is a task that will significantly improve many NER and subsequent applications and improve text understanding and inference.</abstract>
      <doi>10.18653/v1/K18-1020</doi>
      <bibkey>shoshan-radinsky-2018-latent</bibkey>
      <pwccode url="https://github.com/EylonSho/LatentEntitiesExtraction" additional="false">EylonSho/LatentEntitiesExtraction</pwccode>
    </paper>
    <paper id="25">
      <title>Multi-Modal Sequence Fusion via Recursive Attention for Emotion Recognition</title>
      <author><first>Rory</first><last>Beard</last></author>
      <author><first>Ritwik</first><last>Das</last></author>
      <author><first>Raymond W. M.</first><last>Ng</last></author>
      <author><first>P. G. Keerthana</first><last>Gopalakrishnan</last></author>
      <author><first>Luka</first><last>Eerens</last></author>
      <author><first>Pawel</first><last>Swietojanski</last></author>
      <author><first>Ondrej</first><last>Miksik</last></author>
      <pages>251–259</pages>
      <url hash="191fe90f">K18-1025</url>
      <abstract>Natural human communication is nuanced and inherently multi-modal. Humans possess specialised sensoria for processing vocal, visual, and linguistic, and para-linguistic information, but form an intricately fused percept of the multi-modal data stream to provide a holistic representation. Analysis of emotional content in <a href="https://en.wikipedia.org/wiki/Face-to-face_interaction">face-to-face communication</a> is a cognitive task to which humans are particularly attuned, given its sociological importance, and poses a difficult challenge for machine emulation due to the subtlety and expressive variability of cross-modal cues. Inspired by the empirical success of recent so-called End-To-End Memory Networks and related works, we propose an approach based on recursive multi-attention with a shared external memory updated over multiple gated iterations of analysis. We evaluate our model across several large multi-modal datasets and show that global contextualised memory with gated memory update can effectively achieve <a href="https://en.wikipedia.org/wiki/Emotion_recognition">emotion recognition</a>.</abstract>
      <doi>10.18653/v1/K18-1025</doi>
      <bibkey>beard-etal-2018-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
    </paper>
    <paper id="26">
      <title>Using Sparse Semantic Embeddings Learned from Multimodal Text and Image Data to Model Human Conceptual Knowledge</title>
      <author><first>Steven</first><last>Derby</last></author>
      <author><first>Paul</first><last>Miller</last></author>
      <author><first>Brian</first><last>Murphy</last></author>
      <author><first>Barry</first><last>Devereux</last></author>
      <pages>260–270</pages>
      <url hash="39353751">K18-1026</url>
      <abstract>Distributional models provide a convenient way to model <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> using dense embedding spaces derived from <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning algorithms</a>. However, the dimensions of dense embedding spaces are not designed to resemble human semantic knowledge. Moreover, embeddings are often built from a single source of information (typically text data), even though neurocognitive research suggests that <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> is deeply linked to both <a href="https://en.wikipedia.org/wiki/Language">language</a> and perception. In this paper, we combine multimodal information from both text and image-based representations derived from state-of-the-art distributional models to produce sparse, interpretable vectors using Joint Non-Negative Sparse Embedding. Through in-depth analyses comparing these sparse models to human-derived behavioural and neuroimaging data, we demonstrate their ability to predict interpretable linguistic descriptions of human ground-truth semantic knowledge.</abstract>
      <doi>10.18653/v1/K18-1026</doi>
      <bibkey>derby-etal-2018-using</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="31">
      <title>Sentence-Level Fluency Evaluation : References Help, But Can Be Spared !</title>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Sascha</first><last>Rothe</last></author>
      <author><first>Katja</first><last>Filippova</last></author>
      <pages>313–323</pages>
      <url hash="8061003f">K18-1031</url>
      <abstract>Motivated by recent findings on the probabilistic modeling of acceptability judgments, we propose syntactic log-odds ratio (SLOR), a normalized language model score, as a metric for referenceless fluency evaluation of natural language generation output at the sentence level. We further introduce WPSLOR, a novel WordPiece-based version, which harnesses a more compact <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>. Even though word-overlap metrics like ROUGE are computed with the help of hand-written references, our referenceless methods obtain a significantly higher correlation with human fluency scores on a benchmark dataset of compressed sentences. Finally, we present ROUGE-LM, a reference-based metric which is a natural extension of WPSLOR to the case of available references. We show that ROUGE-LM yields a significantly higher correlation with human judgments than all baseline metrics, including WPSLOR on its own.</abstract>
      <doi>10.18653/v1/K18-1031</doi>
      <bibkey>kann-etal-2018-sentence</bibkey>
    </paper>
    <paper id="32">
      <title>Predefined Sparseness in Recurrent Sequence Models</title>
      <author><first>Thomas</first><last>Demeester</last></author>
      <author><first>Johannes</first><last>Deleu</last></author>
      <author><first>Fréderic</first><last>Godin</last></author>
      <author><first>Chris</first><last>Develder</last></author>
      <pages>324–333</pages>
      <url hash="090da4a4">K18-1032</url>
      <abstract>Inducing sparseness while training <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> has been shown to yield <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a> with a lower <a href="https://en.wikipedia.org/wiki/Memory_footprint">memory footprint</a> but similar effectiveness to dense models. However, <a href="https://en.wikipedia.org/wiki/Sparseness">sparseness</a> is typically induced starting from a dense model, and thus this advantage does not hold during training. We propose techniques to enforce <a href="https://en.wikipedia.org/wiki/Sparseness">sparseness</a> upfront in recurrent sequence models for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP applications</a>, to also benefit <a href="https://en.wikipedia.org/wiki/Training">training</a>. First, in <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a>, we show how to increase hidden state sizes in recurrent layers without increasing the number of parameters, leading to more expressive models. Second, for <a href="https://en.wikipedia.org/wiki/Sequence_labeling">sequence labeling</a>, we show that word embeddings with predefined sparseness lead to similar performance as dense embeddings, at a fraction of the number of trainable parameters.</abstract>
      <doi>10.18653/v1/K18-1032</doi>
      <bibkey>demeester-etal-2018-predefined</bibkey>
      <pwccode url="https://github.com/tdmeeste/SparseSeqModels" additional="false">tdmeeste/SparseSeqModels</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="33">
      <title>Learning to Actively Learn Neural Machine Translation</title>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Wray</first><last>Buntine</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>334–344</pages>
      <url hash="bf76014c">K18-1033</url>
      <abstract>Traditional active learning (AL) methods for machine translation (MT) rely on <a href="https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making">heuristics</a>. However, these <a href="https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making">heuristics</a> are limited when the characteristics of the <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">MT problem</a> change due to e.g. the language pair or the amount of the initial bitext. In this paper, we present a <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> to learn sentence selection strategies for neural MT. We train the AL query strategy using a high-resource language-pair based on AL simulations, and then transfer it to the low-resource language-pair of interest. The learned query strategy capitalizes on the shared characteristics between the language pairs to make an effective use of the AL budget. Our experiments on three language-pairs confirms that our method is more effective than strong heuristic-based methods in various conditions, including cold-start and warm-start as well as small and extremely small data conditions.</abstract>
      <doi>10.18653/v1/K18-1033</doi>
      <bibkey>liu-etal-2018-learning</bibkey>
    </paper>
    <paper id="34">
      <title>Upcycle Your OCR : Reusing OCRs for Post-OCR Text Correction in Romanised Sanskrit<fixed-case>OCR</fixed-case>: Reusing <fixed-case>OCR</fixed-case>s for Post-<fixed-case>OCR</fixed-case> Text Correction in <fixed-case>R</fixed-case>omanised <fixed-case>S</fixed-case>anskrit</title>
      <author><first>Amrith</first><last>Krishna</last></author>
      <author><first>Bodhisattwa P.</first><last>Majumder</last></author>
      <author><first>Rajesh</first><last>Bhat</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>345–355</pages>
      <url hash="62970ccd">K18-1034</url>
      <abstract>We propose a post-OCR text correction approach for digitising texts in <a href="https://en.wikipedia.org/wiki/Romanization_of_Sanskrit">Romanised Sanskrit</a>. Owing to the lack of resources our approach uses <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">OCR models</a> trained for other languages written in Roman. Currently, there exists no dataset available for Romanised Sanskrit OCR. So, we bootstrap a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 430 <a href="https://en.wikipedia.org/wiki/Digital_image">images</a>, scanned in two different settings and their corresponding ground truth. For training, we synthetically generate training images for both the settings. We find that the use of copying mechanism (Gu et al., 2016) yields a percentage increase of 7.69 in Character Recognition Rate (CRR) than the current state of the art model in solving monotone sequence-to-sequence tasks (Schnober et al., 2016). We find that our <a href="https://en.wikipedia.org/wiki/System">system</a> is robust in combating OCR-prone errors, as it obtains a CRR of 87.01 % from an OCR output with CRR of 35.76 % for one of the dataset settings. A human judgement survey performed on the models shows that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> results in predictions which are faster to comprehend and faster to improve for a human than the other <a href="https://en.wikipedia.org/wiki/System">systems</a>.</abstract>
      <doi>10.18653/v1/K18-1034</doi>
      <bibkey>krishna-etal-2018-upcycle</bibkey>
      <pwccode url="https://github.com/majumderb/sanskrit-ocr" additional="false">majumderb/sanskrit-ocr</pwccode>
    </paper>
    <paper id="39">
      <title>Lessons Learned in Multilingual Grounded Language Learning</title>
      <author><first>Ákos</first><last>Kádár</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <author><first>Marc-Alexandre</first><last>Côté</last></author>
      <author><first>Grzegorz</first><last>Chrupała</last></author>
      <author><first>Afra</first><last>Alishahi</last></author>
      <pages>402–412</pages>
      <url hash="0a3ec32f">K18-1039</url>
      <abstract>Recent work has shown how to learn better visual-semantic embeddings by leveraging image descriptions in more than one language. Here, we investigate in detail which conditions affect the performance of this type of grounded language learning model. We show that multilingual training improves over bilingual training, and that low-resource languages benefit from training with higher-resource languages. We demonstrate that a multilingual model can be trained equally well on either translations or comparable sentence pairs, and that annotating the same set of images in multiple language enables further improvements via an additional caption-caption ranking objective.</abstract>
      <doi>10.18653/v1/K18-1039</doi>
      <bibkey>kadar-etal-2018-lessons</bibkey>
      <pwccode url="https://github.com/kadarakos/mulisera" additional="false">kadarakos/mulisera</pwccode>
    </paper>
    <paper id="40">
      <title>Unsupervised Sentence Compression using Denoising Auto-Encoders</title>
      <author><first>Thibault</first><last>Févry</last></author>
      <author><first>Jason</first><last>Phang</last></author>
      <pages>413–422</pages>
      <url hash="468d9551">K18-1040</url>
      <abstract>In sentence compression, the task of shortening sentences while retaining the original meaning, <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> tend to be trained on large corpora containing pairs of verbose and compressed sentences. To remove the need for paired corpora, we emulate a summarization task and add noise to extend sentences and train a denoising auto-encoder to recover the original, constructing an end-to-end training regime without the need for any examples of compressed sentences. We conduct a human evaluation of our model on a standard text summarization dataset and show that it performs comparably to a supervised baseline based on <a href="https://en.wikipedia.org/wiki/Grammaticality">grammatical correctness</a> and retention of meaning. Despite being exposed to no target data, our <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised models</a> learn to generate imperfect but reasonably readable sentence summaries. Although we underperform supervised models based on ROUGE scores, our <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> are competitive with a supervised baseline based on human evaluation for grammatical correctness and retention of meaning.</abstract>
      <doi>10.18653/v1/K18-1040</doi>
      <bibkey>fevry-phang-2018-unsupervised</bibkey>
      <pwccode url="https://github.com/zphang/usc_dae" additional="false">zphang/usc_dae</pwccode>
    </paper>
    <paper id="42">
      <title>Linguistically-Based Deep Unstructured Question Answering</title>
      <author><first>Ahmad</first><last>Aghaebrahimian</last></author>
      <pages>433–443</pages>
      <url hash="49cb0d42">K18-1042</url>
      <abstract>In this paper, we propose a new linguistically-based approach to answering non-factoid open-domain questions from <a href="https://en.wikipedia.org/wiki/Unstructured_data">unstructured data</a>. First, we elaborate on an <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a> for textual encoding based on which we introduce a deep end-to-end neural model. This architecture benefits from a bilateral attention mechanism which helps the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to focus on a question and the answer sentence at the same time for phrasal answer extraction. Second, we feed the output of a constituency parser into the model directly and integrate linguistic constituents into the network to help it concentrate on chunks of an answer rather than on its single words for generating more natural output. By optimizing this architecture, we managed to obtain near-to-human-performance results and competitive to a state-of-the-art system on SQuAD and MS-MARCO datasets respectively.</abstract>
      <doi>10.18653/v1/K18-1042</doi>
      <bibkey>aghaebrahimian-2018-linguistically</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="44">
      <title>Challenge or Empower : Revisiting Argumentation Quality in a News Editorial Corpus</title>
      <author><first>Roxanne</first><last>El Baff</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Khalid</first><last>Al-Khatib</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>454–464</pages>
      <url hash="367a3f61">K18-1044</url>
      <abstract>News editorials are said to shape public opinion, which makes them a powerful tool and an important source of political argumentation. However, rarely do <a href="https://en.wikipedia.org/wiki/Editorial">editorials</a> change anyone’s stance on an issue completely, nor do they tend to argue explicitly (but rather follow a subtle rhetorical strategy). So, what does argumentation quality mean for editorials then? We develop the notion that an effective <a href="https://en.wikipedia.org/wiki/Editorial">editorial</a> challenges readers with opposing stance, and at the same time empowers the arguing skills of readers that share the editorial’s stance   or even challenges both sides. To study <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argumentation quality</a> based on this notion, we introduce a new <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> with 1000 <a href="https://en.wikipedia.org/wiki/Editorial">editorials</a> from the <a href="https://en.wikipedia.org/wiki/The_New_York_Times">New York Times</a>, annotated for their perceived effect along with the annotators’ political orientations. Analyzing the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a>, we find that annotators with different orientation disagree on the effect significantly. While only 1 % of all editorials changed anyone’s stance, more than 5 % meet our notion. We conclude that our <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> serves as a suitable resource for studying the <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argumentation quality</a> of <a href="https://en.wikipedia.org/wiki/Editorial">news editorials</a>.</abstract>
      <doi>10.18653/v1/K18-1044</doi>
      <bibkey>el-baff-etal-2018-challenge</bibkey>
    </paper>
    <paper id="48">
      <title>Improving Response Selection in Multi-Turn Dialogue Systems by Incorporating Domain Knowledge</title>
      <author><first>Debanjan</first><last>Chaudhuri</last></author>
      <author><first>Agustinus</first><last>Kristiadi</last></author>
      <author><first>Jens</first><last>Lehmann</last></author>
      <author><first>Asja</first><last>Fischer</last></author>
      <pages>497–507</pages>
      <url hash="4ff7c395">K18-1048</url>
      <abstract>Building systems that can communicate with humans is a core problem in <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence</a>. This work proposes a novel <a href="https://en.wikipedia.org/wiki/Neural_network">neural network architecture</a> for response selection in an end-to-end multi-turn conversational dialogue setting. The <a href="https://en.wikipedia.org/wiki/Architecture">architecture</a> applies context level attention and incorporates additional external knowledge provided by descriptions of domain-specific words. It uses a bi-directional Gated Recurrent Unit (GRU) for encoding context and responses and learns to attend over the context words given the latent response representation and vice versa. In addition, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> incorporates external domain specific information using another GRU for encoding the domain keyword descriptions. This allows better representation of domain-specific keywords in responses and hence improves the overall performance. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms all other state-of-the-art methods for response selection in multi-turn conversations.</abstract>
      <doi>10.18653/v1/K18-1048</doi>
      <bibkey>chaudhuri-etal-2018-improving</bibkey>
      <pwccode url="https://github.com/SmartDataAnalytics/AK-DE-biGRU" additional="false">SmartDataAnalytics/AK-DE-biGRU</pwccode>
    </paper>
    <paper id="49">
      <title>The Lifted Matrix-Space Model for Semantic Composition</title>
      <author><first>WooJin</first><last>Chung</last></author>
      <author><first>Sheng-Fu</first><last>Wang</last></author>
      <author><first>Samuel</first><last>Bowman</last></author>
      <pages>508–518</pages>
      <url hash="a7ad069c">K18-1049</url>
      <abstract>Tree-structured neural network architectures for sentence encoding draw inspiration from the approach to semantic composition generally seen in <a href="https://en.wikipedia.org/wiki/Formal_linguistics">formal linguistics</a>, and have shown empirical improvements over comparable sequence models by doing so. Moreover, adding multiplicative interaction terms to the <a href="https://en.wikipedia.org/wiki/Function_composition">composition functions</a> in these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> can yield significant further improvements. However, existing compositional approaches that adopt such a powerful <a href="https://en.wikipedia.org/wiki/Function_composition">composition function</a> scale poorly, with parameter counts exploding as model dimension or vocabulary size grows. We introduce the Lifted Matrix-Space model, which uses a global transformation to map vector word embeddings to matrices, which can then be composed via an operation based on matrix-matrix multiplication. Its <a href="https://en.wikipedia.org/wiki/Composition_function">composition function</a> effectively transmits a larger number of activations across layers with relatively few <a href="https://en.wikipedia.org/wiki/Parameter">model parameters</a>. We evaluate our model on the Stanford NLI corpus, the Multi-Genre NLI corpus, and the Stanford Sentiment Treebank and find that it consistently outperforms TreeLSTM (Tai et al., 2015), the previous best known composition function for tree-structured models.</abstract>
      <doi>10.18653/v1/K18-1049</doi>
      <bibkey>chung-etal-2018-lifted</bibkey>
      <pwccode url="https://github.com/NYU-MLL/spinn" additional="true">NYU-MLL/spinn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="50">
      <title>End-to-End Neural Entity Linking</title>
      <author><first>Nikolaos</first><last>Kolitsas</last></author>
      <author><first>Octavian-Eugen</first><last>Ganea</last></author>
      <author><first>Thomas</first><last>Hofmann</last></author>
      <pages>519–529</pages>
      <url hash="d1a230b0">K18-1050</url>
      <abstract>Entity Linking (EL) is an essential task for <a href="https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)">semantic text understanding</a> and <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a>. Popular methods separately address the Mention Detection (MD) and Entity Disambiguation (ED) stages of EL, without leveraging their mutual dependency. We here propose the first neural end-to-end EL system that jointly discovers and links entities in a text document. The main idea is to consider all possible spans as potential mentions and learn contextual similarity scores over their entity candidates that are useful for both MD and ED decisions. Key components are context-aware mention embeddings, entity embeddings and a probabilistic mention-entity map, without demanding other engineered features. Empirically, we show that our <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end method</a> significantly outperforms popular systems on the Gerbil platform when enough training data is available. Conversely, if testing datasets follow different annotation conventions compared to the training set (e.g. queries/ tweets vs news documents), our ED model coupled with a traditional NER system offers the best or second best EL accuracy.</abstract>
      <doi>10.18653/v1/K18-1050</doi>
      <bibkey>kolitsas-etal-2018-end</bibkey>
      <pwccode url="https://github.com/dalab/end2end_neural_el" additional="false">dalab/end2end_neural_el</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ipm-nel">IPM NEL</pwcdataset>
    </paper>
    <paper id="52">
      <title>Model Transfer with Explicit Knowledge of the Relation between Class Definitions</title>
      <author><first>Hiyori</first><last>Yoshikawa</last></author>
      <author><first>Tomoya</first><last>Iwakura</last></author>
      <pages>541–550</pages>
      <url hash="efefb5e0">K18-1052</url>
      <abstract>This paper investigates learning methods for multi-class classification using labeled data for the target classification scheme and another labeled data for a similar but different classification scheme (support scheme). We show that if we have prior knowledge about the relation between support and target classification schemes in the form of a class correspondence table, we can use it to improve the model performance further than the simple multi-task learning approach. Instead of learning the individual classification layers for the support and target schemes, the proposed method converts the class label of each example on the support scheme into a set of candidate class labels on the target scheme via the class correspondence table, and then uses the candidate labels to learn the classification layer for the target scheme. We evaluate the proposed <a href="https://en.wikipedia.org/wiki/Methodology">method</a> on two <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. The experimental results show that our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> effectively learns the target schemes especially for the classes that have a tight connection to certain support classes.</abstract>
      <doi>10.18653/v1/K18-1052</doi>
      <bibkey>yoshikawa-iwakura-2018-model</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
    </paper>
    <paper id="54">
      <title>Neural Maximum Subgraph Parsing for Cross-Domain Semantic Dependency Analysis</title>
      <author><first>Yufei</first><last>Chen</last></author>
      <author><first>Sheng</first><last>Huang</last></author>
      <author><first>Fang</first><last>Wang</last></author>
      <author><first>Junjie</first><last>Cao</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>562–572</pages>
      <url hash="51182282">K18-1054</url>
      <abstract>We present experiments for cross-domain semantic dependency analysis with a neural Maximum Subgraph parser. Our <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> targets 1-endpoint-crossing, pagenumber-2 graphs which are a good fit to semantic dependency graphs, and utilizes an efficient dynamic programming algorithm for decoding. For disambiguation, the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> associates words with BiLSTM vectors and utilizes these <a href="https://en.wikipedia.org/wiki/Euclidean_vector">vectors</a> to assign scores to candidate dependencies. We conduct experiments on the <a href="https://en.wikipedia.org/wiki/Data_set">data sets</a> from SemEval 2015 as well as Chinese CCGBank. Our <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> achieves very competitive results for both <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. To improve the parsing performance on cross-domain texts, we propose a data-oriented method to explore the linguistic generality encoded in English Resource Grammar, which is a precisionoriented, hand-crafted HPSG grammar, in an implicit way. Experiments demonstrate the effectiveness of our data-oriented method across a wide range of conditions.</abstract>
      <doi>10.18653/v1/K18-1054</doi>
      <bibkey>chen-etal-2018-neural</bibkey>
      <pwccode url="https://github.com/draplater/msg-parser" additional="false">draplater/msg-parser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    </volume>
  <volume id="2">
    <meta>
      <booktitle>Proceedings of the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</booktitle>
      <url hash="f401e9c4">K18-2</url>
      <editor><first>Daniel</first><last>Zeman</last></editor>
      <editor><first>Jan</first><last>Hajič</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Brussels, Belgium</address>
      <month>October</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="af8fbbf9">K18-2000</url>
      <bibkey>conll-2018-conll</bibkey>
    </frontmatter>
    <paper id="3">
      <title>CEA LIST : Processing Low-Resource Languages for CoNLL 2018<fixed-case>CEA</fixed-case> <fixed-case>LIST</fixed-case>: Processing Low-Resource Languages for <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018</title>
      <author><first>Elie</first><last>Duthoo</last></author>
      <author><first>Olivier</first><last>Mesnard</last></author>
      <pages>34–44</pages>
      <url hash="1657896f">K18-2003</url>
      <abstract>In this paper, we describe the <a href="https://en.wikipedia.org/wiki/System">system</a> used for our first participation at the CoNLL 2018 shared task. The submitted <a href="https://en.wikipedia.org/wiki/System">system</a> largely reused the state of the art <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> from CoNLL 2017 (). We enhanced this system for morphological features predictions, and we used all available resources to provide accurate models for low-resource languages. We ranked 5th of 27 participants in MLAS for building morphology aware dependency trees, 2nd for morphological features only, and 3rd for tagging (UPOS) and parsing (LAS) low-resource languages.<url>https://github.com/tdozat/Parser-v2</url>). We enhanced this system for morphological features predictions, and we used all available resources to provide accurate models for low-resource languages. We ranked 5th of 27 participants in MLAS for building morphology aware dependency trees, 2nd for morphological features only, and 3rd for tagging (UPOS) and parsing (LAS) low-resource languages.</abstract>
      <doi>10.18653/v1/K18-2003</doi>
      <bibkey>duthoo-mesnard-2018-cea</bibkey>
      <pwccode url="https://github.com/tdozat/Parser-v2" additional="false">tdozat/Parser-v2</pwccode>
    </paper>
    <paper id="4">
      <title>Semi-Supervised Neural System for <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">Tagging</a>, <a href="https://en.wikipedia.org/wiki/Parsing">Parsing</a> and Lematization</title>
      <author><first>Piotr</first><last>Rybak</last></author>
      <author><first>Alina</first><last>Wróblewska</last></author>
      <pages>45–54</pages>
      <url hash="554fa07a">K18-2004</url>
      <abstract>This paper describes the ICS PAS system which took part in CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies. The system consists of jointly trained <a href="https://en.wikipedia.org/wiki/Tagger">tagger</a>, <a href="https://en.wikipedia.org/wiki/Lemmatizer">lemmatizer</a>, and <a href="https://en.wikipedia.org/wiki/Dependency_grammar">dependency parser</a> which are based on features extracted by a biLSTM network. The <a href="https://en.wikipedia.org/wiki/System">system</a> uses both fully connected and dilated convolutional neural architectures. The novelty of our approach is the use of an additional <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a>, which reduces the number of cycles in the predicted dependency graphs, and the use of self-training to increase the system performance. The proposed <a href="https://en.wikipedia.org/wiki/System">system</a>, i.e. ICS PAS (Warszawa), ranked 3th/4th in the official evaluation obtaining the following overall results : 73.02 (LAS), 60.25 (MLAS) and 64.44 (BLEX).</abstract>
      <doi>10.18653/v1/K18-2004</doi>
      <bibkey>rybak-wroblewska-2018-semi</bibkey>
      <pwccode url="https://github.com/360er0/COMBO" additional="false">360er0/COMBO</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="5">
      <title>Towards Better UD Parsing : Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation<fixed-case>UD</fixed-case> Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation</title>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Yijia</first><last>Liu</last></author>
      <author><first>Yuxuan</first><last>Wang</last></author>
      <author><first>Bo</first><last>Zheng</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>55–64</pages>
      <url hash="dcdcddb9">K18-2005</url>
      <abstract>This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies. We base our submission on Stanford’s winning system for the CoNLL 2017 shared task and make two effective extensions : 1) incorporating deep contextualized word embeddings into both the part of speech tagger and <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> ; 2) ensembling parsers trained with different initialization. We also explore different ways of concatenating <a href="https://en.wikipedia.org/wiki/Treebank">treebanks</a> for further improvements. Experimental results on the development data show the effectiveness of our <a href="https://en.wikipedia.org/wiki/Methodology">methods</a>. In the final evaluation, our <a href="https://en.wikipedia.org/wiki/System">system</a> was ranked first according to <a href="https://en.wikipedia.org/wiki/Level_of_service">LAS</a> (75.84 %) and outperformed the other <a href="https://en.wikipedia.org/wiki/System">systems</a> by a large margin.</abstract>
      <doi>10.18653/v1/K18-2005</doi>
      <bibkey>che-etal-2018-towards</bibkey>
      <pwccode url="https://github.com/HIT-SCIR/ELMoForManyLangs" additional="false">HIT-SCIR/ELMoForManyLangs</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="6">
      <title>Joint Learning of POS and Dependencies for Multilingual Universal Dependency Parsing<fixed-case>POS</fixed-case> and Dependencies for Multilingual <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsing</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Shexia</first><last>He</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>65–73</pages>
      <url hash="f1c59122">K18-2006</url>
      <abstract>This paper describes the system of team LeisureX in the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies. Our <a href="https://en.wikipedia.org/wiki/System">system</a> predicts the <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tag</a> and dependency tree jointly. For the basic tasks, including <a href="https://en.wikipedia.org/wiki/Lexical_analysis">tokenization</a>, <a href="https://en.wikipedia.org/wiki/Lemmatization">lemmatization</a> and <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphology prediction</a>, we employ the official baseline model (UDPipe). To train the low-resource languages, we adopt a <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">sampling method</a> based on other richresource languages. Our <a href="https://en.wikipedia.org/wiki/System">system</a> achieves a macro-average of 68.31 % LAS F1 score, with an improvement of 2.51 % compared with the UDPipe.</abstract>
      <doi>10.18653/v1/K18-2006</doi>
      <bibkey>li-etal-2018-joint-learning</bibkey>
      <pwccode url="https://github.com/bcmi220/joint_stackptr" additional="false">bcmi220/joint_stackptr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="8">
      <title>An Improved Neural Network Model for Joint POS Tagging and Dependency Parsing<fixed-case>POS</fixed-case> Tagging and Dependency Parsing</title>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>81–91</pages>
      <url hash="f803f174">K18-2008</url>
      <abstract>We propose a novel neural network model for joint part-of-speech (POS) tagging and dependency parsing. Our model extends the well-known BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016) by incorporating a BiLSTM-based tagging component to produce automatically predicted POS tags for the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a>. On the benchmark English Penn treebank, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> obtains strong UAS and LAS scores at 94.51 % and 92.87 %, respectively, producing 1.5+% absolute improvements to the BIST graph-based parser, and also obtaining a state-of-the-art POS tagging accuracy at 97.97 %. Furthermore, experimental results on parsing 61 big Universal Dependencies treebanks from raw texts show that our model outperforms the baseline UDPipe (Straka and Strakova, 2017) with 0.8 % higher average POS tagging score and 3.6 % higher average LAS score. In addition, with our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, we also obtain state-of-the-art downstream task scores for biomedical event extraction and opinion analysis applications. Our code is available together with all pre-trained models at :<url>https://github.com/datquocnguyen/jPTDP</url>
      </abstract>
      <doi>10.18653/v1/K18-2008</doi>
      <bibkey>nguyen-verspoor-2018-improved</bibkey>
      <pwccode url="https://github.com/datquocnguyen/jPTDP" additional="false">datquocnguyen/jPTDP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="9">
      <title>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing<fixed-case>IBM</fixed-case> Research at the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 Shared Task on Multilingual Parsing</title>
      <author><first>Hui</first><last>Wan</last></author>
      <author><first>Tahira</first><last>Naseem</last></author>
      <author><first>Young-Suk</first><last>Lee</last></author>
      <author><first>Vittorio</first><last>Castelli</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <pages>92–102</pages>
      <url hash="742eee1c">K18-2009</url>
      <abstract>This paper presents the IBM Research AI submission to the CoNLL 2018 Shared Task on Parsing Universal Dependencies. Our system implements a new joint transition-based parser, based on the Stack-LSTM framework and the Arc-Standard algorithm, that handles <a href="https://en.wikipedia.org/wiki/Lexical_analysis">tokenization</a>, <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a>, morphological tagging and dependency parsing in one single model. By leveraging a combination of character-based modeling of words and recursive composition of partially built linguistic structures we qualified 13th overall and 7th in low resource. We also present a new sentence segmentation neural architecture based on Stack-LSTMs that was the 4th best overall.</abstract>
      <doi>10.18653/v1/K18-2009</doi>
      <bibkey>wan-etal-2018-ibm</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    <title_ar>IBM Research في CoNLL 2018 المهمة المشتركة حول التحليل متعدد اللغات</title_ar>
      <title_pt>IBM Research na CoNLL 2018 Shared Task on Multilingual Parsing</title_pt>
      <title_es>IBM Research en la CoNll 2018 Shared Task sobre análisis multilingüe</title_es>
      <title_fr>IBM Research à la tâche partagée ConLL 2018 sur l'analyse syntaxique multilingue</title_fr>
      <title_zh>IBM 研究院在 CoNLL 2018 多言解析共其事</title_zh>
      <title_ja>多言語構文解析に関するCoNLL 2018シェアード・タスクでのIBMの研究</title_ja>
      <title_hi>CONLL 2018 में आईबीएम रिसर्च ने बहुभाषी पार्सिंग पर साझा कार्य</title_hi>
      <title_ru>Исследования IBM в CoNLL 2018 Shared Task on Multilingual Parsing</title_ru>
      <title_ga>Taighde IBM ag Tasc Comhroinnte CoNLL 2018 ar Pharsáil Ilteangach</title_ga>
      <title_ka>IBM პასუხი CoNLL 2018-ში მრავალენგური პასუხის შესახებ</title_ka>
      <title_el>Έρευνα της IBM στο CoNLL 2018 Κοινή Εργασία για την Πολυγλωσσική Ανάλυση</title_el>
      <title_hu>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_hu>
      <title_it>IBM Research al CoNLL 2018 Shared Task on Multilingual Parsing</title_it>
      <title_lt>IBM moksliniai tyrimai CoNLL 2018 m. bendrame daugiakalbio analizavimo uždavinyje</title_lt>
      <title_mk>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_mk>
      <title_kk>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_kk>
      <title_ms>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_ms>
      <title_ml>കോണ്‍എല്‍ 2018 ലെ ഐബിഎം പരിശീലനം പല ഭാഷ പാര്‍സിങ്ങില്‍ പങ്കെടുത്ത പണിയാണ്</title_ml>
      <title_mt>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_mt>
      <title_mn>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_mn>
      <title_no>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_no>
      <title_ro>IBM Research în cadrul CoNLL 2018 sarcina comună privind interpretarea multilingvă</title_ro>
      <title_sr>IBM istraživanja na CoNLL 2018. delnom zadatku o multijezičkom razmatranju</title_sr>
      <title_pl>Badania IBM w CoNLL 2018 Wspólne zadanie dotyczące analizy wielojęzycznej</title_pl>
      <title_si>IBM Research at the CoNLL 2018 shared Job on Multilanguage Parsing</title_si>
      <title_so>IBM Research at the CoNLL 2018 Shared Task on parsing badan</title_so>
      <title_ur>CoNLL 2018 میں IBM تحقیقات کا مشترک ٹاکس Multilingual Parsing پر</title_ur>
      <title_sv>IBM Research vid CoNLL 2018 delad uppgift om flerspråkig tolkning</title_sv>
      <title_ta>கோன்எல் 2018 ல் ஐபிஎம் ஆராய்ச்சி பல மொழி பாசிங்கில் பகிர்ந்த பணி</title_ta>
      <title_uz>Comment</title_uz>
      <title_vi>Nghiên cứu của IBM ở CoNll 208 đã chia sẻ Nhiệm vụ phân tích đa ngôn ngữ</title_vi>
      <title_nl>IBM Research bij het CoNLL 2018 Gedeelde taak over meertalige parsing</title_nl>
      <title_hr>IBM istraživanja na CoNLL 2018. dijelom zadatku o multijezičkom razmatranju</title_hr>
      <title_de>IBM Research am CoNLL 2018 Shared Task on Multilingual Parsing</title_de>
      <title_bg>Изследване на споделена задача за многоезично анализиране</title_bg>
      <title_da>IBM Research på CoNLL 2018 delt opgave om flersproget tolkning</title_da>
      <title_id>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_id>
      <title_ko>IBM의 2018년 CoNLL 콘퍼런스 다국어 해석 공유 연구</title_ko>
      <title_fa>تحقیقات IBM در کارهای مشترک در مورد تحلیل زیادی زبان در CoNLL 2018</title_fa>
      <title_sw>Utafiti wa IBM kwenye CoNLL 2018 ulishiriki kazi ya Uchaguzi wa lugha nyingi</title_sw>
      <title_tr>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_tr>
      <title_sq>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_sq>
      <title_hy>IBM-ի հետազոտությունները 2018 թվականի CONSL-ի բազմալեզու վերլուծության ընթացքում</title_hy>
      <title_af>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_af>
      <title_am>በCoNLL 2018 የተሰራጨው ስራ በMultilingual Parsing ላይ</title_am>
      <title_bn>কনএল ২০১৮ সালের আইবিএম গবেষণা মাল্টিভাষায় পার্জিং এর কাজ শেয়ার করা হয়েছে।</title_bn>
      <title_ca>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_ca>
      <title_az>IBM araştırması CoNLL 2018 çoxlu dil analizi barəsində paylaşın işi</title_az>
      <title_et>IBM Research at CoNLL 2018 Shared Task on Multilingual Parsing</title_et>
      <title_cs>IBM Research na CoNLL 2018 Sdílený úkol pro vícejazyčné analýzy</title_cs>
      <title_fi>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_fi>
      <title_bs>IBM istraživanja na CoNLL 2018. dijeljenom zadatku o multijezičkom razmatranju</title_bs>
      <title_jv>IBM Search at the CoNLL 2008 shared task on Multilanguage Parasing</title_jv>
      <title_sk>IBM raziskave na skupni nalogi CoNLL 2018 o večjezičnem razpravljanju</title_sk>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_he>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_he>
      <title_bo>IBM Research at the CoNLL 2018 Shared Task on Multilingual Parsing</title_bo>
      <abstract_ar>تقدم هذه الورقة تقديم IBM Research AI إلى المهمة المشتركة CoNLL 2018 حول تحليل التبعيات العالمية. ينفذ نظامنا محللًا جديدًا مشتركًا قائمًا على الانتقال ، استنادًا إلى إطار عمل Stack-LSTM وخوارزمية Arc-Standard ، التي تتعامل مع الترميز ، وعلامات جزء من الكلام ، والعلامات المورفولوجية ، وتحليل التبعية في نموذج واحد. من خلال الاستفادة من مجموعة من النمذجة المستندة إلى الشخصية للكلمات والتكوين المتكرر للهياكل اللغوية المبنية جزئيًا ، قمنا بتأهيل المركز الثالث عشر بشكل عام والسابع في الموارد المنخفضة. نقدم أيضًا بنية عصبية جديدة لتجزئة الجملة بناءً على Stack-LSTMs والتي كانت رابع أفضل بشكل عام.</abstract_ar>
      <abstract_fr>Cet article présente la soumission d'IBM Research AI à la tâche partagée ConLL 2018 sur l'analyse des dépendances universelles. Notre système implémente un nouvel analyseur conjoint basé sur la transition, basé sur le framework Stack-LSTM et l'algorithme Arc-Standard, qui gère la tokenisation, le balisage partiel, le marquage morphologique et l'analyse des dépendances dans un seul modèle. En tirant parti d'une combinaison de modélisation des mots basée sur les caractères et de composition récursive de structures linguistiques partiellement construites, nous nous sommes classés 13e au classement général et 7ème en ressources faibles. Nous présentons également une nouvelle architecture neuronale de segmentation de phrases basée sur Stack-LSTMS, qui est la 4e meilleure au classement général.</abstract_fr>
      <abstract_es>Este artículo presenta la presentación de IBM Research AI a la tarea compartida de CoNll 2018 sobre análisis de dependencias universales. Nuestro sistema implementa un nuevo analizador conjunto basado en transiciones, basado en el marco Stack-LSTM y el algoritmo Arc-Standard, que maneja la tokenización, el etiquetado de parte del discurso, el etiquetado morfológico y el análisis de dependencias en un solo modelo. Al aprovechar una combinación de modelado de palabras basado en caracteres y composición recursiva de estructuras lingüísticas parcialmente construidas, calificamos 13° en general y 7° en recursos bajos. También presentamos una nueva arquitectura neuronal de segmentación de oraciones basada en Stack-LSTMS que fue la cuarta mejor en general.</abstract_es>
      <abstract_pt>Este artigo apresenta o envio do IBM Research AI para a tarefa compartilhada CoNLL 2018 na análise de dependências universais. Nosso sistema implementa um novo analisador baseado em transição conjunta, baseado na estrutura Stack-LSTM e no algoritmo Arc-Standard, que lida com tokenização, marcação de parte da fala, marcação morfológica e análise de dependência em um único modelo. Ao alavancar uma combinação de modelagem de palavras baseada em caracteres e composição recursiva de estruturas linguísticas parcialmente construídas, classificamos 13º geral e 7º em baixo recurso. Apresentamos também uma nova arquitetura neural de segmentação de sentenças baseada em Stack-LSTMs que foi a 4ª melhor no geral.</abstract_pt>
      <abstract_ja>この論文は、普遍的依存関係の解析に関するCoNLL 2018共有タスクへのIBM Research AI提出を紹介しています。当社のシステムは、Stack - LSTMフレームワークとArc - Standardアルゴリズムに基づいた新しいジョイント遷移ベースの構文解析器を実装しており、1つのモデルでトークン化、音声の一部タグ付け、形態学的タグ付け、依存性解析を処理します。単語の文字ベースのモデリングと部分的に構築された言語構造の再帰的構成の組み合わせを活用することにより、全体で13位、低リソースで7位に認定した。また、全体で4番目に優れたStack - LSTMに基づいた新しい文章セグメンテーションニューラルアーキテクチャも提示しています。</abstract_ja>
      <abstract_zh>本文言 IBM 研究院 AI CoNLL 2018 解析通用共事者。 吾统成一新之解析器,当解析器于 Stack-LSTM 框架、 Arc-Standard 算法,可于单形之中处标记、词性标记、形表、依赖解析。 因字符之单词建模,结言语之递归,总排名第13位,资源量排名第7位。 又立Stack-LSTM新型句分神经架构,当架构第四。</abstract_zh>
      <abstract_hi>यह पेपर IBM Research AI सबमिशन को CoNLL 2018 Shared Task on Parsing Universal Dependencies पर प्रस्तुत करता है। हमारी प्रणाली स्टैक-एलएसटीएम फ्रेमवर्क और आर्क-स्टैंडर्ड एल्गोरिथ्म के आधार पर एक नया संयुक्त संक्रमण-आधारित पार्सर लागू करती है, जो एक ही मॉडल में टोकनाइजेशन, पार्ट-ऑफ-स्पीच टैगिंग, रूपात्मक टैगिंग और निर्भरता पार्सिंग को संभालती है। शब्दों के चरित्र-आधारित मॉडलिंग और आंशिक रूप से निर्मित भाषाई संरचनाओं की पुनरावर्ती संरचना के संयोजन का लाभ उठाकर हमने कम संसाधन में कुल मिलाकर 13 वें और 7 वें स्थान पर अर्हता प्राप्त की। हम स्टैक-एलएसटीएम के आधार पर एक नया वाक्य विभाजन तंत्रिका वास्तुकला भी प्रस्तुत करते हैं जो कुल मिलाकर 4 वां सबसे अच्छा था।</abstract_hi>
      <abstract_ru>В этой статье представлена подача ИИ IBM Research к общей задаче CoNLL 2018 по анализу универсальных зависимостей. Наша система реализует новый совместный парсер на основе перехода, основанный на фреймворке Stack-LSTM и алгоритме Arc-Standard, который обрабатывает токенизацию, частичную тегирование речи, морфологическую тегирование и синтаксический анализ зависимостей в одной модели. Используя комбинацию символьного моделирования слов и рекурсивного состава частично построенных лингвистических структур, мы квалифицировали 13-е место в общем и 7-е по низкому ресурсу. Мы также представляем новую нейронную архитектуру сегментации предложений, основанную на стековых LSTM, которая в целом заняла 4-е место.</abstract_ru>
      <abstract_ga>Cuireann an páipéar seo aighneacht IBM Research AI i láthair do Thasc Roinnte CoNLL 2018 maidir le Spleáchais Uilíocha a Pharsáil. Cuireann ár gcóras parsálaí nua tras-bhunaithe comhpháirteach i bhfeidhm, bunaithe ar chreat Stack-LSTM agus ar an algartam Arc-Standard, a láimhseálann tokenization, clibeáil pháirteach cainte, clibeáil moirfeolaíoch agus parsáil spleáchais in aon mhúnla amháin. Trí mheascán de shamhaltú carachtar-bhunaithe focal a ghiaráil agus comhdhéanamh athfhillteach ar struchtúir teanga a tógadh go páirteach cháilíomar sa 13ú háit ar an iomlán agus sa 7ú háit maidir le hacmhainn íseal. Cuirimid i láthair chomh maith ailtireacht néarach deighilte abairtí bunaithe ar Stack-LSTManna a bhí ar an 4ú háit is fearr san iomlán.</abstract_ga>
      <abstract_hu>Ez a tanulmány bemutatja az IBM Research AI benyújtását a CoNLL 2018 Shared Task on Parsing univerzális függőségek értelmezésére. Rendszerünk a Stack-LSTM keretrendszeren és az Arc-Standard algoritmuson alapuló új közös átmeneti alapú elemzőt hajt végre, amely egyetlen modellben kezeli a tokenizációt, a beszédrész címkézést, a morfológiai címkézést és a függőség elemzését. A karakter alapú szavak modellezésének és a részben épített nyelvi struktúrák rekurzív kompozíciójának kombinációjával összesen 13. és 7. minősítést végeztünk alacsony erőforrásban. Bemutatunk egy új mondatszegmentációs neurális architektúrát Stack-LSTMs alapján, amely összességében a negyedik legjobb volt.</abstract_hu>
      <abstract_ka>ამ დოკუმენტი IBM სწავლობა AI-ის გასამუშაობას CoNLL 2018 სხვადასხვა რაოდენობაში სამუშაო დავამუშაობას. ჩვენი სისტემა ახალი გადაწყვეტილი გადაწყვეტილი გადაწყვეტილი გადაწყვეტილი პანელეზერი, Stack-LSTM ფრამეტზე და Arc-Standard ალგორიტიმზე, რომელიც ერთი მოდელში ტოკენიზაცია, სიტყვების ნაწილის ნაწილის შესახებ. სიტყვების და რეკურსიური კომპოზიციის კომბიზიციის კომბიზიციას, რომელიც ნაწილად შექმნა ლენგურისტიკური სტრუქტურების კომბიზიციაში, ჩვენ კომბიზი ჩვენ ასევე ახალი სიმბოლოების სექმენტაციის ნეირალური აქტიქტიკურაციას, რომელიც სექმენტი LSTMs იყო 4-ი უკეთესი ყველაზე.</abstract_ka>
      <abstract_el>Η παρούσα εργασία παρουσιάζει την υποβολή της στην κοινή εργασία για την ανάλυση καθολικών εξαρτήσεων. Το σύστημά μας υλοποιεί ένα νέο κοινό αναλυτή βασισμένο στη μετάβαση, βασισμένο στο πλαίσιο και τον αλγόριθμο που διαχειρίζεται την επισήμανση, την επισήμανση μέρους ομιλίας, τη μορφολογική επισήμανση και την ανάλυση εξαρτήσεων σε ένα μόνο μοντέλο. Με τη χρήση ενός συνδυασμού μοντελοποίησης λέξεων βάσει χαρακτήρων και αναδρομικής σύνθεσης μερικώς δομημένων γλωσσικών δομών, κατατάξαμε την 13η συνολικά και την 7η σε χαμηλούς πόρους. Παρουσιάζουμε επίσης μια νέα νευρική αρχιτεκτονική κατακερματισμού προτάσεων βασισμένη σε Stack-LSTMs που ήταν η 4η καλύτερη συνολικά.</abstract_el>
      <abstract_it>Questo articolo presenta la presentazione di IBM Research AI al CoNLL 2018 Shared Task on Parsing Universal Dependences. Il nostro sistema implementa un nuovo parser basato sulla transizione congiunta, basato sul framework Stack-LSTM e sull'algoritmo Arc-Standard, che gestisce tokenizzazione, tag part-of-speech, tag morfologico e analisi delle dipendenze in un unico modello. Sfruttando una combinazione di modellazione basata sui caratteri delle parole e composizione ricorsiva di strutture linguistiche parzialmente costruite, ci siamo qualificati 13 ° in generale e 7 ° in bassa risorsa. Presentiamo anche una nuova architettura neurale di segmentazione delle frasi basata su Stack-LSTMs che è stata la quarta migliore nel complesso.</abstract_it>
      <abstract_mk>Овој документ го претставува поднесувањето на ИБМ Research AI на Соделената задача на CoNLL 2018 за анализирање на универзалните зависности. Нашиот систем имплементира нов заеднички анализатор базиран на транзиција, базиран на Stack-LSTM рамката и арк-стандардниот алгоритм, кој се справува со токенизација, дел од говорот означување, морфолошки означување и анализација на зависност во еден модел. Со искористување на комбинација на карактерско моделирање на зборови и рекурсивна композиција на делумно изградени јазични структури, ние ја квалификувавме 13-тата целокупно и 7-тата во ниски ресурси. Ние, исто така, претставуваме нова реченица сегментација на нервната архитектура базирана на Stack-LSTMs, која беше четвртата најдобра вкупно.</abstract_mk>
      <abstract_lt>Šiame dokumente pateikiama IBM mokslinių tyrimų AI paraiška CoNLL 2018 m. bendram universaliųjų priklausomybių vertinimo uždaviniui. Mūsų sistema įdiegia naują bendrą pereinamojo laikotarpio analizatorių, pagrįstą Stack-LSTM sistema ir Arc-Standard algoritmu, kuris tvarko tokenizaciją, kalbos dalies žymėjimą, morfologinį žymėjimą ir priklausomybės analizavimą vienu modeliu. Naudojami žodžių modeliavimo pagal charakteristikas ir iš dalies sukurtų kalbinių struktūrų rekurencinės sudėties derinį, iš viso kvalifikavome 13-ąjį ir 7-ąjį mažai išteklių. Mes taip pat pristatome naują sakinių segmentacijos neurologinę architektūrą, pagrįstą Stack-LSTM, kuri buvo ketvirtoji geriausia bendrai.</abstract_lt>
      <abstract_kk>Бұл қағаз IBM зерттеу AI-нің CoNLL 2018 ортақ тапсырмасын универсалдық тәуелдіктерді талдау үшін ортақ тапсырманы көрсетеді. Біздің жүйеміз Stack-LSTM фрейміне негізделген жаңа бөлек ауыстыру негізделген талдаушы және Arc-Стандартты алгоритмді орындайды. Бұл токенизациялау, сөздің бөлігін тегжелеу, морфологиялық тегжелеу мен тәуелдік тал Таңбалар негізінде сөздер мен қайталанатын лингвистикалық құрылымдарының біріктірілген модельдерді қолдану арқылы, біз 13- ші жалпы және 7- ші ресурста төмен көмектесдік. Біз сондай-ақ 4- ші ең жақсы жұмыс болған Stack- LSTMs негізінде жаңа сөз сегментациясы невралдық архитектурасын таңдадық.</abstract_kk>
      <abstract_ml>ഈ പത്രത്തില്‍ പാര്‍സിങ്ങ് യൂണിവര്‍ണല്‍ ആശ്രയിക്കുന്നതിനെപ്പറ്റിയുള്ള പാര്‍സിങ്ങ് യൂണിവര്‍ണല്‍ ആന്‍ഡന്‍സികളില്‍ ഇബിഎ Our system implements a new joint transition-based parser, based on the Stack-LSTM framework and the Arc-Standard algorithm, that handles tokenization, part-of-speech tagging, morphological tagging and dependency parsing in one single model.  വാക്കുകളുടെ അടിസ്ഥാനത്തിലുള്ള ഒരു കൂട്ടിക്കൂട്ടം വാക്കുകളുടെയും പിന്നീട് രീതിയിലുള്ള ഘടകം സ്റ്റാക്ക്-എൽസ്റ്റിഎസ്റ്റിസ്സിനെ അടിസ്ഥാനമായി അടിസ്ഥാനമായി ഒരു പുതിയ വാക്കിന്റെ വേര്‍പ്പിന്റെ പുതിയ പ്ര</abstract_ml>
      <abstract_ms>Kertas ini memperkenalkan IBM Research AI submission kepada CoNLL 2018 Shared Task on Parsing Universal Dependencies. Sistem kami melaksanakan penghurai berasaskan transisi kongsi baru, berdasarkan kerangka Stack-LSTM dan algoritma Arc-Standard, yang mengendalikan tokenisasi, tag sebahagian-ucapan, tag morfologi dan hurai dependensi dalam satu model. Dengan menggunakan kombinasi pemodelan perkataan berdasarkan aksara dan komposisi rekursif struktur bahasa sebahagian yang dibina kami memenuhi syarat keseluruhan ke-13 dan ke-7 dalam sumber rendah. Kami juga memperkenalkan arkitektur saraf segmen kalimat baru berdasarkan Stack-LSTMs yang merupakan keseluruhan keempat yang terbaik.</abstract_ms>
      <abstract_mt>Dan id-dokument jippreżenta s-sottomissjoni tal-IBM Research AI lill-CoNLL 2018 Shared Task on Parsing Universal Dependencies. Is-sistema tagħna timplimenta analizzatur konġunt ġdid ibbażat fuq it-tranżizzjoni, ibbażat fuq il-qafas Stack-LSTM u l-algoritmu Arc-Standard, li jimmaniġġja t-tokenizzazzjoni, it-tikkettar ta’ parti mid-diskors, it-tikkettar morfoloġiku u l-analizzazzjoni tad-dipendenza f’mudell wieħed. Bl-ingranaġġ ta’ kombinazzjoni ta’ mudell ibbażat fuq il-karattru tal-kliem u kompożizzjoni rikorrenti ta’ strutturi lingwistiċi parzjalment mibnija, ikkwalifikajna t-13-il ġeneralment u s-7 f’riżorsi baxxi. We also present a new sentence segmentation neural architecture based on Stack-LSTMs that was the 4th best overall.</abstract_mt>
      <abstract_no>Denne papiret viser IBM-forskningsAI-tillegget til CoNLL 2018-delt oppgåve om tolking av universelle avhengighet. Sistemet vårt implementerer ein ny kopla overgangsbasert tolkar, basert på Stack-LSTM-rammeverket og Arc-Standard-algoritmen, som handterar tokenisering, del-av-talemerking, morfologisk merking og avhengighetstolking i eitt enkelt modell. Ved å leverara eit kombinasjon av teiknbasert modellering av ord og rekursivt samansering av delvis bygde lingviske strukturar, vi kvalifiserte 13. overalt og 7. i låg ressurs. Vi presenterer også eit nytt setningsssegmentasjon av neuralarkitektur basert på Stack-LSTMs som var den 4. beste overalt.</abstract_no>
      <abstract_pl>W artykule przedstawiono zgłoszenie IBM Research AI do CoNLL 2018 Shared Task on Parsing Universal Dependences. Nasz system wdraża nowy wspólny parser oparty na przejściach oparty na frameworku Stack-LSTM i algorytmie Arc-Standard, który obsługuje tokenizację, tagowanie części mowy, tagowanie morfologiczne i parsowanie zależności w jednym modelu. Wykorzystując kombinację modelowania słów opartego na znakach i rekursywnego składu częściowo zbudowanych struktur językowych, zakwalifikowaliśmy ogólnie 13-tą i 7-tą w niskich zasobach. Przedstawiamy również nową architekturę neuronową segmentacji zdań opartą na Stack-LSTMs, która była czwartą najlepszą w ogóle.</abstract_pl>
      <abstract_mn>Энэ цаас IBM судалгааны AI-г CoNLL 2018 оны Дэлхийн хамаарал хамааралтай талаар хуваалцах ажил дээр илтгэдэг. Бидний систем Stack-LSTM хэлбэрээр, Arc-Стандарт алгоритмын үндсэн шинэ шилжүүлэлт дээр суурилсан хуваарч хийдэг. Энэ нь тодорхойлолт, ярианы нэг хэсэг, морфологик тегжинг, хамааралтай хуваарч нэг загварын нэг хэлбэрээр ажилладаг. Бид 13-р нийтлэг, 7-р нийтлэг бүтээгдэхүүнд 13-р нийтлэг болон 7-р нийтлэг бүтээгдэхүүнийг ашиглаж байна. Мөн бид 4-р хамгийн сайн зүйл байсан Stack-LSTMs дээр багтаж байгаа шинэ өгүүлбэрийн сэтгэл зүйн архитектурыг тайлбарлаж байна.</abstract_mn>
      <abstract_ro>Această lucrare prezintă prezentarea IBM Research AI la CoNLL 2018 Shared Task on Parsing Universal Dependents. Sistemul nostru implementează un nou parser bazat pe tranziție comună, bazat pe cadrul Stack-LSTM și algoritmul Arc-Standard, care gestionează tokenizarea, etichetarea parțială de vorbire, etichetarea morfologică și analizarea dependențelor într-un singur model. Prin utilizarea unei combinații de modelare bazată pe caractere a cuvintelor și compoziție recursivă a structurilor lingvistice parțial construite, am calificat al 13-lea general și al 7-lea în resurse reduse. De asemenea, prezentăm o nouă arhitectură neurală de segmentare a frazelor bazată pe Stack-LSTMs care a fost a patra cea mai bună în general.</abstract_ro>
      <abstract_sr>Ovaj papir predstavlja podnošenje IBM istraživanja AI podnošenje zajedničkom zadatku CoNLL 2018 o razmatranju univerzalnih zavisnosti. Naš sistem implementira novi zajednički prelazni analizator, zasnovan na okviru Stack-LSTM i Arc-Standard algoritma, koji se bavi tokenizacijom, dijelom govornog etiketiranja, morfološkom etiketiranjem i analizacijom zavisnosti u jednom modelu. Uspoređujući kombinaciju modeliranog na karakteru reči i rekursivnog kompozicija djelomično izgrađenih jezičkih struktura, kvalifikovali smo 13. ukupno i 7. u niskom resursu. Predstavljamo i novu segmentaciju rečenice neuralnu arhitekturu na osnovu Stack-LSTMs-a koja je bila četvrti najbolji ukupno.</abstract_sr>
      <abstract_sv>Denna uppsats presenterar IBM Research AI-inlämningen till CoNLL 2018 Shared Task on Parsing Universal Dependences. Vårt system implementerar en ny gemensam övergångsbaserad parser, baserad på Stack-LSTM-ramverket och Arc-Standard-algoritmen, som hanterar tokenisering, delmärkning, morfologisk taggning och beroendetolkning i en enda modell. Genom att utnyttja en kombination av karaktärsbaserad modellering av ord och rekursiv sammansättning av delvis byggda språkliga strukturer kvalificerade vi oss 13:e totalt och 7:e i låg resurs. Vi presenterar också en ny meningssegmentering neural arkitektur baserad på Stack-LSTMs som var den fjärde bästa totalt.</abstract_sv>
      <abstract_si>මේ පත්තරේ IBM පරීක්ෂණා AI පිළිගන්න පුළුවන් වෙනවා CoNLL 2018 සමාගත වැඩසටහන් විශේෂ විශේෂ විශේෂ විශේ අපේ පද්ධතිය අළුත් සංවිධානයක් ස්ටැක් LSTM පද්ධතිය සහ ආර්ක් ස්ටැන්ඩ් ඇල්ගෝරිතම් පද්ධතියෙන් අළුත් සංවිධානයක් පරීක්ෂා කරනවා, කොටස් බො වචනය සහ ප්‍රතික්‍රියාත්මක සංවිධානය සඳහා භාෂාත්මක සංවිධානයක් සම්බන්ධ කරලා තියෙන්නේ අපි 13වෙනි සාමාන්‍ය සහ 7 අපි අළුත් වාක්ය විශේෂයක් ස්ටැක්-LSTMs වලින් අධාරිත විශේෂයක් පෙන්වන්නේ. ඒක තමයි 4වෙනි හොඳම සාමා</abstract_si>
      <abstract_so>Kanu warqaddan waxaa lagu soo dhiibaa IBM baaritaanka AI oo loo soo dhiibay CoNLL 2018 Shaqada loo sharciyey ku saabsan baaritaanka jaamacadda. Systemkanagu wuxuu sameeyaa baaritaan cusub oo ku saleysan Stack-LSTM iyo Arc-Standard algoritm, kaas oo qabanqaabinaya calaamad, qayb ka mid ah hadal tagging, morphological tagging iyo ku xiran baaritaanka isku model. Sida lagu soo diro muuqasho xaraf ah oo ku qoran hadal iyo dib u dhigid dhismaha qayb ahaan loo dhisay dhismaha luuqadaha, waxaan ku lifaaqnay koox 13aad oo dhan iyo 7aad oo ku qoran resource hoose. Sidoo kale waxaynu soo bandhignaynaa dhismo neuro ah oo lagu saleynayo Stack-LSTMs oo ahaa qofka 4aad oo ugu wada fiican.</abstract_so>
      <abstract_ur>This paper presents the IBM Research AI submission to the CoNLL 2018 Shared Task on Parsing Universal Dependencies. ہماری سیسٹم نے ایک نئی جوڑی تغییر پر بنیاد رکھی ہے، Stack-LSTM فرمود اور Arc-Standard الگوریتم پر بنیاد رکھی ہے، جو ٹوکنیزی کرتا ہے، بات ٹاگنگ کا حصہ، مورپولوژیک ٹاگنگ اور اعتمادی پارسینگ کا ایک مدل میں ہے. ہم نے کلمات اور دوبارہ ساختہ زبان کی ساختاریوں کی ترکیب کے ذریعہ ایک شخصت بنیاد کی موڈلینگ کی ترکیب کے ذریعہ سے 13 کلمات اور 7 کلمات کے ذریعہ کم منصفات میں قابل تعمیر کی۔ ہم نے بھی ایک نئی جماعت سیگنٹ نیورل معماری بنائی ہے جو Stack-LSTMs پر بنیاد ہے جو چوتھا سب سے بہتر تھا.</abstract_ur>
      <abstract_ta>இந்த தாள் IBM ஆராய்ச்சி AI கூட்டுகிறது CoNLL 2018 பகிர்ந்த பணியை பாசிங் உலக சார்புகள் பற்றி பகிர்ந்த பணி எங்கள் அமைப்பு அடிப்படையில் அடிப்படையிலான ஒரு புதிய இணைப்பு மாற்றும் பளஞ்சியை செயல்படுத்துகிறது, அடிப்படையில் அடிப்படையில் மற்றும் ஆர- நிலையான ஆல்பரிட்டம், அது கு குறைந்த மூலத்தில் 13வது மொத்தம் மற்றும் 7வது குறைந்த மூலத்தில் கொண்டு எழுத்து அடிப்படையான மாதிரி உருவாக்குதல் மற்றும் திரும்ப அமைப்ப நாம் ஒரு புதிய வாக்கியத்தை பிரித்தல் புதிய நெருக்கமான கட்டுப்பாட்டு அடிப்படையில் கொண்டு வருகிறோம் Stack-LSTMs என்ற</abstract_ta>
      <abstract_uz>Bu qogʻoz IBM ta'qish AI'ni CoNLL 2018'ga bogʻliq vazifani universal qoʻllanmalar bilan bogʻliq vazifani koʻrsatadi. Bizning tizimimiz, Stack-LSTM freymi va Arc-Standard algoritga asosida yangi aloqa tarjima qiladi. Bu bir modelda soʻzning qismlarini boshqaradi, morfologik tagg'ining qismlarini boshqaradi va bir modelda parsing ishlatadi. By leveraging a combination of character-based modeling of words and recursive composition of partially built linguistic structures we qualified 13th overall and 7th in low resource.  Biz yana birinchi so'zni ajratish uchun "Stack-LSTMs" asosida yaratilgan yangi narsalarning neyrolik arxituvchisi.</abstract_uz>
      <abstract_vi>Tờ giấy này trình bày đơn của IBM Research Al để đệ trình cho CLB CodLL 208 "Chia sẻ" về phân tích vật sở hữu chung. Hệ thống của chúng tôi thực hiện một bộ phân tích liên kết kết dựa trên bộ chế độ Stacks-LSTM và thuật toán Arc-Standard, xử lý việc hiệu ứng, định vị phần ngôn ngữ, kích thích và độ phụ thuộc phân tích theo một kiểu. Bằng cách sử dụng sự kết hợp từ điển của các từ ngữ và cấu trúc ngôn ngữ được xây dựng lại từ một phần được cấu tạo ra chúng tôi xếp hạng 13th trong phương pháp thấp. Chúng tôi cũng giới thiệu một kiến trúc dây thần kinh phân loại mới dựa trên Stacks-LSTM, là thứ tư tốt nhất.</abstract_vi>
      <abstract_bg>Настоящата статия представя представянето на ИИ за научни изследвания към Споделената задача за анализиране на универсалните зависимости на CoNLL 2018. Нашата система внедрява нов анализатор, базиран на преход, базиран на рамката и алгоритъма който обработва токенизация, маркиране на част от речта, морфологично маркиране и анализ на зависимости в един единствен модел. Използвайки комбинация от символно-базирано моделиране на думи и рекурсивна композиция на частично изградени лингвистични структури, ние квалифицирахме 13-то общо и 7-то по нисък ресурс. Представяме и нова нервна архитектура за сегментиране на изречения, базирана на стак-ЛСТМ, която е 4-та най-добра като цяло.</abstract_bg>
      <abstract_da>Denne artikel præsenterer IBM Research AI-indsendelsen til CoNLL 2018 Shared Task on Parsing Universal Dependences. Vores system implementerer en ny fælles overgangsbaseret parser, baseret på Stack-LSTM framework og Arc-Standard algoritmen, der håndterer tokenisering, del-of-tale tagging, morfologisk tagging og afhængighedsparsing i én enkelt model. Ved at udnytte en kombination af karakterbaseret modellering af ord og rekursiv sammensætning af delvist byggede sproglige strukturer kvalificerede vi os 13. samlet og 7. i lav ressource. Vi præsenterer også en ny sætningssegmentering neural arkitektur baseret på Stack-LSTMs, der var den 4. bedste samlet.</abstract_da>
      <abstract_nl>Dit artikel presenteert de IBM Research AI inzending aan de CoNLL 2018 Shared Task on Parsing Universal Dependencies. Ons systeem implementeert een nieuwe gezamenlijke transitie-gebaseerde parser, gebaseerd op het Stack-LSTM framework en het Arc-Standard algoritme, die tokenization, part-of-speech tagging, morfologische tagging en afhankelijkheidsparsing in één model verwerkt. Door gebruik te maken van een combinatie van karaktergebaseerde modellering van woorden en recursieve samenstelling van gedeeltelijk gebouwde linguïstische structuren kwalificeerden we 13e over het algemeen en 7e in low resource. We presenteren ook een nieuwe zinnensegmentatie neurale architectuur gebaseerd op Stack-LSTMs die de 4e beste in het algemeen was.</abstract_nl>
      <abstract_de>Dieses Papier stellt die IBM Research AI Einreichung an die CoNLL 2018 Shared Task on Parsing Universal Dependencies vor. Unser System implementiert einen neuen Joint Transition-basierten Parser, der auf dem Stack-LSTM Framework und dem Arc-Standard Algorithmus basiert und Tokenisierung, Sprachteiltagging, morphologische Tagging und Abhängigkeitsparsing in einem einzigen Modell verarbeitet. Durch die Nutzung einer Kombination aus charakterbasierter Modellierung von Wörtern und rekursiver Komposition von teilweise aufgebauten linguistischen Strukturen qualifizierten wir insgesamt 13th und 7th in low resource. Wir präsentieren auch eine neue Satzsegmentierungs-neuronale Architektur basierend auf Stack-LSTMs, die insgesamt viertbeste war.</abstract_de>
      <abstract_ko>본고는 IBM Research AI가 CoNLL 2018에 제출하여 유니버설 의존항 공유 임무를 분석한 상황을 소개한다.우리 시스템은 Stack LSTM 프레임워크와 Arc 표준 알고리즘을 바탕으로 새로운 연합 변환을 바탕으로 하는 해상도를 실현했다. 이 해상도는 하나의 모델에서 표기화, 어성 표기, 형태 표기와 의존성 분석을 처리한다.문자 기반의 단어 모델링과 부분적으로 구축된 언어 구조의 귀속 조합을 이용하여 우리는 저자원 분야에서 13위와 7위의 자격을 얻었다.스택 LSTMs를 기반으로 한 새로운 문장 분할 신경 네트워크 구조도 제시했는데 이 구조는 전체 4위에 올랐다.</abstract_ko>
      <abstract_id>Kertas ini mempersembahkan IBM Research AI pengiriman ke CoNLL 2018 Shared Task on Parsing Universal Dependencies. Sistem kami mengimplementasikan parser baru berdasarkan transisi, berdasarkan struktur Stack-LSTM dan algoritma Arc-Standard, yang menangani tokenisasi, bagian-dari-pidato tagging, tag morfologi dan penghuraian dependensi dalam satu model. Dengan menggunakan kombinasi dari model karakter berdasarkan kata dan komposisi rekursif dari struktur bahasa yang dibangun sebagian kami memenuhi syarat keseluruhan ke-13 dan ke-7 dalam sumber daya rendah. Kami juga mempersembahkan segmen kalimat baru arsitektur saraf berdasarkan Stack-LSTMs yang merupakan keseluruhan keempat yang terbaik.</abstract_id>
      <abstract_hr>Ovaj papir predstavlja podnošenje IBM istraživačkog AI-a u CoNLL 2018 zajedničkom zadatku o razmatranju univerzalnih zavisnosti. Naš sustav provodi novi zajednički razmatrač na temelju prijenosa na temelju okvira Stack-LSTM i Arc-Standard algoritma, koji se bavi tokenizacijom, dijelom govornog etiketiranja, morfološkom etiketiranja i analizacijom ovisnosti u jednom modelu. Uzimajući kombinaciju modeliranog na karakteru riječi i rekursivnog sastojaka djelomično izgrađenih jezičkih struktura, kvalificirali smo 13. ukupno i 7. u niskom resursu. Također predstavljamo novu segmentaciju rečenica neuralnu arhitekturu na temelju Stack-LSTMs-a koja je bila četvrti najbolji ukupno.</abstract_hr>
      <abstract_fa>این کاغذ تحقیقات AI IBM را به کار مشترک CoNLL 2018 در مورد تحلیل بستگی جهانی نشان می دهد. سیستم ما یک بازیگر جدید بر اساس چهارچوب Stack-LSTM و الگوریتم استاندارد آرک را اجرای می‌کند که با توکینز، قسمتی از نقاشی سخنرانی، نقاشی مورفولوژیک و بازیگری بستگی در یک مدل است. با توجه به ترکیب یک ترکیب از مدل‌سازی بر اساس شخصیت کلمات و ترکیب دوباره از ساختمان‌های زبان‌شناسی که بخشی ساخته شده‌ایم، در کل ۱۳ و ۷م منابع پایین را درآوردیم. ما همچنین یک معماری عصبی جدید از جمله جدید را بر اساس استک-LSTMs نشان می دهیم که چهارمین بهترین عمومی بود.</abstract_fa>
      <abstract_sw>Gazeti hili linaonyesha utafiti wa IBM utafiti wa AI uliotolewa kwa CoNLL 2018 Kushirikisha kazi ya Kuchapisha Kutegemea Uimbaji ulimwengu kote. Mfumo wetu unatekeleza mchambuzi mpya wa mpito wa pamoja, kwa msingi wa mfumo wa Stack-LSTM na utaratibu wa KiArc-Standard, unaohusisha ushambulizi, sehemu ya wimbo wa hotuba, wimbo wa kifolojia na kutegemea kuimba kwa mtindo mmoja. Kwa kutumia muunganiko wa muungano wa maneno na ujenzi wa sekunde wa miundombinu ya lugha tuliyojipatia ujumla wa 13 na 7 katika rasilimali chini. Pia tunaonyesha muundo mpya wa ujenzi wa kisasa wa kifungu cha hukumu kwa msingi wa Stack-LSTMs ambao ulikuwa ni mkuu wa nne.</abstract_sw>
      <abstract_tr>Bu kagyz IBM Araştyrma AI CoNLL 2018-nji Halkara Baýramlyklary Parlamak üçin beýleki Görevi üýtgedýär. Bizim sistemimiz Stack-LSTM çerçevesinde we Arc-Standardlı algoritminin tabanly bir birleşik geçişi tabanly bir täze çözümlerini implement edir. Bu tokenizasyonu, sözlerin bir bölümini tägleme, morfolojik tägleme we baglançylyk çözümlemesini bir tek modelde çözýär. Karakter tabanly sözler we rekursiv dil strukturalarynyň birleşigini we biz iň az resurslarda 13-nji we 7-nji derejesini ukyp etdik. Biz hem Täze sözlem segmentasiýasynda Stack-LSTMs üçin 4-nji iň gowy tarapyndan daşary bir neural arhitektegi görkeýäris.</abstract_tr>
      <abstract_af>Hierdie papier stel die IBM Research AI-onderskrywing aan die CoNLL 2018 deelde taak op verwerking van Universele afhanklikhede. Ons stelsel implementeer 'n nuwe joint transition-based ontwerker, gebaseer op die Stack-LSTM raamwerk en die Arc-Standaard algoritme, wat handteer tokenisasie, deel-van-spreek etiketing, morfologiese etiketing en afhanklikheid verwerking in een enkele model. Deur 'n kombinasie van karaktergebaseerde modellering van woorde en rekursief opstelling van gedeeltelik gebou lingwisiese strukture, het ons 13-de heeltemal en 7-de in lae hulpbron gekvalifiseer. Ons het ook 'n nuwe setningsegmentasie neurale arkitektuur voorgestel wat op Stack-LSTMs gebaseer is wat die 4de beste totaal was.</abstract_af>
      <abstract_am>ይህ ገጽ የIBM ምርመራ AI ለኮንLL 2018 በፓርቲ የዓለማዊ ግንኙነት ላይ የተሰራጨውን ስራ ያቀርባል፡፡ የስርዓታችን አዲስ የፍሬት-LSTM ፍሬም እና የአርካቲ-Standard algorithም በተመሳሳይ፣ የንግግር ማተሚያ ክፍል፣ ሞርፎሎጂ ማተላለፊያ እና በአንድ ሞዴል ማኅበረሰብ በመጠቀም የሚችል አዲስ የፍትወት ተተሳታፊ ማተር ያስተካክላል፡፡ በአካባቢው የቋንቋዊ ቋንቋዎች መሠረት እና በተመሳሳይ የቃላት ምሳሌ እና የክፍለ ሥርዓት አካባቢ በመስጠት በ13ኛ በሙሉ እና 7ኛ ታናሽ ክፍል አዋቂ ነው፡፡ አዲስ የቁጥጥር አካባቢ የናቡር አካባቢ መሠረት እናቀርባታለን፤ ይህም የአራተኛው ክፍል ጥሩ ነው፡፡</abstract_am>
      <abstract_hy>Այս հոդվածը ներկայացնում է IBM Research AI-ի ներկայացումը 2018 թվականի ԿՈՆԼ-ի Համաշխարհային Հանկախությունների վերլուծության ընդհանուր խնդիրը: Մեր համակարգը իրականացնում է մի նոր միավոր վերաբերյալ հիմնված վերաբերյալ, որը հիմնված է Stack-LSMT-ի շրջանակի և Արք-Ստանդարտ ալգորիթմի վրա, որը վերահսկում է թոկենիզացիան, խոսքի մասը, մորֆոլոգիական թղթերը և կախվածության վերաբերյալ մեկ մոդելի մեջ: Օգտագործելով բառերի բնորոշ մոդելավորման և մասամբ կառուցված լեզվաբանական կառուցվածքների համակցությունը' մենք համարվում էինք 13-րդին և 7-րդին ցածր ռեսուրսներով: Մենք նաև ներկայացնում ենք նոր նախադասությունների սեգմետրացիայի նյարդային ճարտարապետություն, որը հիմնված է Stack-LStms-ի վրա, որը 4-րդ լավագույն համաշխարհում էր:</abstract_hy>
      <abstract_sq>Ky dokument paraqet paraqitjen e IBM Research AI në CoNLL 2018 Task Shared on Parsing Universal Dependencies. Sistemi ynë zbaton një analizues të ri të përbashkët bazuar në tranzicion, bazuar në kuadrin Stack-LSTM dhe algoritmin Arc-Standard, që trajton tokenizimin, shënimin pjesë-të-fjalimit, shënimin morfologjik dhe analizimin e varësisë në një model të vetëm. Duke përdorur një kombinim të modelimit të fjalëve bazuar në karakter dhe përbërjes rekursive të strukturave gjuhësore të ndërtuara pjesërisht, kualifikuam të 13-tën në përgjithësi dhe të 7-tën në burime të ulëta. Ne prezantojmë gjithashtu një arkitekturë të re të segmentimit të fjalëve neuronale bazuar në Stack-LSTMs që ishte e katërta më e mirë në përgjithësi.</abstract_sq>
      <abstract_az>Bu kağıt IBM araştırma AI'nin CoNLL 2018 Üniversal bağlılıqları Parsing üçün paylaşılmış işi ilə birləşdirir. Sistemimiz Stack-LSTM çerçivesi və Arc-Standardlı algoritmi ilə birləşdirilmiş bir keçişçilik tərzini təşkil edən yeni bir keçişçilik tərzini təşkil edir. Sözlər və dil qurulduqların parçacıq in şa edilmiş quruluşların birləşdirilməsinə görə 13. ünvanı və 7. ünvanı düşük çoxluğa qaytardıq. Biz də dördüncü ən yaxşısı Stack-LSTMs-lərə dayanan yeni cümləlik segmentasyonu nöral arhitektürünü göstəririk.</abstract_az>
      <abstract_ca>Aquest paper presenta la presentació d'IBM Research AI a la CoNLL 2018 Shared Task on Parsing Universal Dependencies. El nostre sistema implementa un nou analitzador conjunt basat en la transició, basat en el marc Stack-LSTM i l'algoritme Arc-Standard, que gestiona la tecenització, l'etiquetage de part-of-speech, l'etiquetage morfològic i l'analització de dependencies en un únic model. Utilitzant una combinació de modelació de paraules basada en caràcters i composició recursiva d'estructures lingüístices parcialment construïdes, vam qualificar el 13è en general i el 7è en baix recursos. També presentem una nova arquitectura neuronal de segmentació de frases basada en Stack-LSTMs que va ser la quarta millor en general.</abstract_ca>
      <abstract_bs>Ovaj papir predstavlja podnošenje IBM istraživačkog AI-a u CoNLL 2018 zajedničkom zadatku o razmatranju univerzalnih zavisnosti. Naš sistem provodi novi zajednički analizator na temelju prijenosa, zasnovan na okviru Stack-LSTM i Arc-Standard algoritma, koji se bavi tokenizacijom, dijelom govornog etiketiranja, morfološkom etiketiranjem i analizacijom ovisnosti u jednom modelu. Uzimajući kombinaciju modeliranog na karakteru riječi i rekursivnog sastanka djelomično izgrađenih jezičkih struktura, kvalifikovali smo 13. ukupno i 7. u niskom resursu. Također predstavljamo novu segmentaciju rečenice neuralnu arhitekturu na osnovu Stack-LSTMs-a koja je bila četvrti najbolji ukupno.</abstract_bs>
      <abstract_cs>Tento článek představuje předložení IBM Research AI do sdíleného úkolu CoNLL 2018 o analýze univerzálních závislostí. Náš systém implementuje nový společný parser založený na přechodu založený na frameworku Stack-LSTM a algoritmu Arc-Standard, který zpracovává tokenizaci, tagování části řeči, morfologické tagování a analýzu závislostí v jednom modelu. Využitím kombinace znakového modelování slov a rekurzivního složení částečně postavených jazykových struktur jsme kvalifikovali celkově třináctý a sedmý v nízkých zdrojích. Představujeme také novou neuronovou architekturu segmentace vět založenou na Stack-LSTMs, která byla celkově čtvrtá nejlepší.</abstract_cs>
      <abstract_bn>এই পত্রিকাটি পার্জিং বিশ্ববিদ্যালয়ের নির্ভরিত কাজের জন্য আইবিএম গবেষণা AI প্রদান করেছে। আমাদের সিস্টেম স্ট্যাক-এলসিএমএর ফ্রেম এবং আর্ক-স্ট্যান্ডার্ড অ্যালগরিদমের ভিত্তিতে একটি নতুন যোগাযোগ-ভিত্তিক প্রতিযোগিতা ব্যবস্থা করে, যা একটি মডেলে পার্জ বিভিন্ন ভিত্তিক ক্যারেক্টারের মডেল এবং পুনঃসংগঠনের মাধ্যমে অংশ নির্মিত ভাষাগত কাঠামোর মাধ্যমে আমরা নিম্ন সম্পদে ১৩ তম সাধারণ এবং ৭ তম আমরা স্ট্যাক-এলস্টিএমএস এর ভিত্তিতে একটি নতুন শাস্তি বিভক্তি নিউরাল কাঠামোর সাথে উপস্থাপন করেছি যা ছিল ৪তম সেরা সার্ব</abstract_bn>
      <abstract_et>Käesolevas artiklis tutvustatakse IBM Research AI esitlust CoNLL 2018. aasta jagatud ülesandele universaalsete sõltuvuste parsimise kohta. Meie süsteem rakendab Stack-LSTM raamistikul ja Arc-Standard algoritmil põhinevat ühisüleminekul põhinevat parserit, mis käsitleb tokeniseerimist, kõneosa sildistamist, morfoloogilist sildistamist ja sõltuvuse parsimist ühes mudelis. Kasutades sõnade märgipõhist modelleerimist ja osaliselt ehitatud keelestruktuuride rekursiivset kompositsiooni, kvalifitseerisime 13. üldiselt ja 7. madala ressursiga. Samuti tutvustame uut lausesegmentatsiooni neuroarhitektuuri, mis põhineb Stack-LSTMdel, mis oli üldiselt 4. parim.</abstract_et>
      <abstract_fi>Tässä artikkelissa esitellään IBM:n tutkimuksen tekoälyn julkaisu CoNLL 2018 Shared Task on Parsing Universal Dependences -ohjelmaan. Järjestelmämme toteuttaa uuden, Stack-LSTM-viitekehykseen ja Arc-Standard-algoritmiin perustuvan yhteissiirtymäpohjaisen jäsentäjän, joka käsittelee tokenisaatiota, puheen osaa merkitsemistä, morfologista merkitsemistä ja riippuvuuden jäsentämistä yhdessä mallissa. Hyödyntämällä merkkipohjaista sanamallinnusta ja osittain rakennettujen kielellisten rakenteiden rekursiivista koostumusta kvalifioimme 13. ja 7. heikossa resurssissa. Esittelemme myös Stack-LSTMs:iin perustuvan lausesegmentoinnin neuroarkkitehtuurin, joka oli neljänneksi paras.</abstract_fi>
      <abstract_ha>Wannan karatun na bãyar da IBM Research AI ɗin da aka bai wa the CoNLL 2018 Shared job on Paring Universal depend. Tsarinmu na zartar da wani tsari na daban a haɗi da shi, a kan asalin firam na Stack-LSSM da algoritm na Tsarin-Tsarin, wanda ke yi aiki da alama, rabon tagogi ga magana, tagogi na mutfologi da kuma yana dõgara ga parse cikin shekara guda. Ina iya samar da wani komi mai bincike wa misalin na rubutu da kurekursive composition of partly-gina linguistic tsarori, mun ƙayyade 13 na jumla da 7 cikin wuri mai ƙasƙanci. Kayya, Munã halatar da wani matsayin neura na salon da aka haɗa shi na rubutun maganar.</abstract_ha>
      <abstract_he>העיתון הזה מציג את ההצגה של IBM Research AI למשימה משותפת CoNLL 2018 על בדיקת תלויות Universal. המערכת שלנו מפעילה חוקר משותף חדש מבוסס על המסגרת של Stack-LSTM והאלגוריתם Arc-Standard, שמטפל בטקניזציה, תגים חלק מהנאום, תגים מורפולוגיים ומחקר תלויות במודל אחד. על ידי השימוש בשילוב של מודל מבוסס על אופים של מילים ומרכיב חוזר של מבנים שפתיים בנויים חלקית, הכישרנו את ה-13 באופן כללי ו-7 במשאבים נמוכים. אנחנו גם מציגים ארכיטקטורה עצבית חדשה במחלקת משפטים המבוססת על סטאק-LSTMs שהיתה הרביעית הכי טובה באופן כללי.</abstract_he>
      <abstract_sk>V tem članku je predstavljena predložitev IBM Research AI za skupno nalogo CoNLL 2018 o razvrščanju univerzalnih odvisnosti. Naš sistem uvaja nov razčlenjevalnik, ki temelji na okviru Stack-LSTM in algoritmu Arc-Standard, ki obravnava žetonizacijo, označevanje dela govora, morfološko označevanje in razčlenjevanje odvisnosti v enem samem modelu. Z uporabo kombinacije karakterskega modeliranja besed in rekurzivne kompozicije delno zgrajenih jezikovnih struktur smo se uvrstili na 13. skupno in 7. v nizkih virih. Predstavljamo tudi novo živčno arhitekturo segmentacije stavkov, ki temelji na Stack-LSTMs, ki je bila četrta najboljša v celoti.</abstract_sk>
      <abstract_jv>Gambar iki bakal ngewehi nggawe IBM istrangis AI kanggo kowe CoNLL 2013 Gejaratan task nang Parasing Universal dependancies. Sistem dhéwé ngewehku nggawe sistem sing bagian nggawe politenessoffpolite"), and when there is a change ("assertive Awak dhéwé éntuk karo akeh sing katêpakan uwis seneng dadi, Arketektur Neral sing basa saben Stack-KST M kuwi wis nganggo katêh saben apik dhéwé.</abstract_jv>
      <abstract_bo>ཤོག་བྱང་འདིས་IBM་འཚོལ་ཞིབ་སྐྱོང་ཆེད་པ AI་ལ་CoNLL 2018་ཡི་ནང་དུ་ཆ་རྐྱེན་སྤྲོད་ཀྱི་བྱ་རིམ་དང་། Our system implements a new joint transition-based parser, based on the Stack-LSTM framework and the Arc-Standard algorithm, that handles tokenization, part-of-speech tagging, morphological tagging and dependency parsing in one single model. ང་ཚོས་རང་ཉིད་ཀྱི་སྐད་རིགས་ཀྱི་ཁྱད་ཆོས་དང་བསྐྱར་རིམ་ཅིག་གི་མཉམ་འབྲེལ་གྱི་ཁྱད་ཆོས་ལ་བསྡད་པའི་སྒྲིག་ཆ་སྒྲིག ང་ཚོས་Stack-LSTMs འི་ནང་དུ་ཚིག་གི་སྒྲ་ཚིགས་ཀྱི་དཔུད་རིམ་གནས་སྟངས་གསརཔ་ཞིག་ལ་སྟོན་པ་དེ་བཞི་ཆ་ཤོས་ཡོད།</abstract_bo>
      </paper>
    <paper id="11">
      <title>82 Treebanks, 34 Models : Universal Dependency Parsing with Multi-Treebank Models<fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsing with Multi-Treebank Models</title>
      <author><first>Aaron</first><last>Smith</last></author>
      <author><first>Bernd</first><last>Bohnet</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <author><first>Yan</first><last>Shao</last></author>
      <author><first>Sara</first><last>Stymne</last></author>
      <pages>113–123</pages>
      <url hash="c9395471">K18-2011</url>
      <abstract>We present the Uppsala system for the CoNLL 2018 Shared Task on universal dependency parsing. Our system is a pipeline consisting of three components : the first performs joint word and sentence segmentation ; the second predicts <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tags</a> and <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological features</a> ; the third predicts dependency trees from words and tags. Instead of training a single parsing model for each <a href="https://en.wikipedia.org/wiki/Treebank">treebank</a>, we trained <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> with multiple treebanks for one language or closely related languages, greatly reducing the number of <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>. On the official test run, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system obtained the best scores overall for <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a>, universal POS tagging, and <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological features</a>.</abstract>
      <doi>10.18653/v1/K18-2011</doi>
      <bibkey>smith-etal-2018-82</bibkey>
      <revision id="1" href="K18-2011v1" hash="1497f080" />
      <revision id="2" href="K18-2011v2" hash="c9395471" date="2022-02-11">Added missing acknowledgment.</revision>
    </paper>
    <paper id="12">
      <title>Tree-Stack LSTM in Transition Based Dependency Parsing<fixed-case>LSTM</fixed-case> in Transition Based Dependency Parsing</title>
      <author><first>Ömer</first><last>Kırnap</last></author>
      <author><first>Erenay</first><last>Dayanık</last></author>
      <author><first>Deniz</first><last>Yuret</last></author>
      <pages>124–132</pages>
      <url hash="8272a882">K18-2012</url>
      <abstract>We introduce tree-stack LSTM to model state of a transition based parser with <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a>. Tree-stack LSTM does not use any parse tree based or hand-crafted features, yet performs better than models with these features. We also develop new set of <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> from raw features to enhance the performance. There are 4 main components of this model : stack’s -LSTM, buffer’s -LSTM, actions’ LSTM and tree-RNN. All <a href="https://en.wikipedia.org/wiki/Linear_time-invariant_system">LSTMs</a> use continuous dense feature vectors (embeddings) as an input. Tree-RNN updates these <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> based on transitions. We show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> improves performance with low resource languages compared with its predecessors. We participate in CoNLL 2018 UD Shared Task as the KParse team and ranked 16th in LAS, 15th in BLAS and BLEX metrics, of 27 participants parsing 82 test sets from 57 languages.</abstract>
      <doi>10.18653/v1/K18-2012</doi>
      <bibkey>kirnap-etal-2018-tree</bibkey>
      <pwccode url="https://github.com/kirnap/ku-dependency-parser2" additional="false">kirnap/ku-dependency-parser2</pwccode>
    </paper>
    <paper id="14">
      <title>SEx BiST : A Multi-Source Trainable Parser with Deep Contextualized Lexical Representations<fixed-case>SE</fixed-case>x <fixed-case>B</fixed-case>i<fixed-case>ST</fixed-case>: A Multi-Source Trainable Parser with Deep Contextualized Lexical Representations</title>
      <author><first>KyungTae</first><last>Lim</last></author>
      <author><first>Cheoneum</first><last>Park</last></author>
      <author><first>Changki</first><last>Lee</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <pages>143–152</pages>
      <url hash="962b375f">K18-2014</url>
      <abstract>We describe the SEx BiST parser (Semantically EXtended Bi-LSTM parser) developed at Lattice for the CoNLL 2018 Shared Task (Multilingual Parsing from Raw Text to Universal Dependencies). The main characteristic of our work is the encoding of three different modes of contextual information for <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a> : (i) Treebank feature representations, (ii) Multilingual word representations, (iii) ELMo representations obtained via <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a> from external resources. Our <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> performed well in the official end-to-end evaluation (73.02 LAS   4th/26 teams, and 78.72 UAS   2nd/26) ; remarkably, we achieved the best UAS scores on all the English corpora by applying the three suggested feature representations. Finally, we were also ranked 1st at the optional event extraction task, part of the 2018 Extrinsic Parser Evaluation campaign.</abstract>
      <doi>10.18653/v1/K18-2014</doi>
      <bibkey>lim-etal-2018-sex</bibkey>
      <pwccode url="https://github.com/jujbob/multilingual-models" additional="false">jujbob/multilingual-models</pwccode>
    </paper>
    <paper id="18">
      <title>Towards JointUD : <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">Part-of-speech Tagging</a> and <a href="https://en.wikipedia.org/wiki/Lemmatization">Lemmatization</a> using Recurrent Neural Networks<fixed-case>J</fixed-case>oint<fixed-case>UD</fixed-case>: Part-of-speech Tagging and Lemmatization using Recurrent Neural Networks</title>
      <author><first>Gor</first><last>Arakelyan</last></author>
      <author><first>Karen</first><last>Hambardzumyan</last></author>
      <author><first>Hrant</first><last>Khachatrian</last></author>
      <pages>180–186</pages>
      <url hash="eef7c8ff">K18-2018</url>
      <abstract>This paper describes our submission to CoNLL UD Shared Task 2018. We have extended an LSTM-based neural network designed for sequence tagging to additionally generate character-level sequences. The network was jointly trained to produce <a href="https://en.wikipedia.org/wiki/Lemma_(morphology)">lemmas</a>, <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tags</a> and <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological features</a>. Sentence segmentation, <a href="https://en.wikipedia.org/wiki/Lexical_analysis">tokenization</a> and <a href="https://en.wikipedia.org/wiki/Dependency_grammar">dependency parsing</a> were handled by UDPipe 1.2 baseline. The results demonstrate the viability of the proposed multitask architecture, although its performance still remains far from state-of-the-art.</abstract>
      <doi>10.18653/v1/K18-2018</doi>
      <bibkey>arakelyan-etal-2018-towards</bibkey>
      <pwccode url="https://github.com/YerevaNN/JointUD" additional="false">YerevaNN/JointUD</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="19">
      <title>CUNI x-ling : Parsing Under-Resourced Languages in CoNLL 2018 UD Shared Task<fixed-case>CUNI</fixed-case> x-ling: Parsing Under-Resourced Languages in <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 <fixed-case>UD</fixed-case> Shared Task</title>
      <author><first>Rudolf</first><last>Rosa</last></author>
      <author><first>David</first><last>Mareček</last></author>
      <pages>187–196</pages>
      <url hash="98516e2e">K18-2019</url>
      <abstract>This is a system description paper for the CUNI x-ling submission to the CoNLL 2018 UD Shared Task. We focused on parsing under-resourced languages, with no or little training data available. We employed a wide range of approaches, including simple word-based treebank translation, combination of delexicalized parsers, and exploitation of available morphological dictionaries, with a dedicated setup tailored to each of the languages. In the official evaluation, our submission was identified as the clear winner of the Low-resource languages category.</abstract>
      <doi>10.18653/v1/K18-2019</doi>
      <bibkey>rosa-marecek-2018-cuni</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="21">
      <title>Universal Morpho-Syntactic Parsing and the Contribution of Lexica : Analyzing the ONLP Lab Submission to the CoNLL 2018 Shared Task<fixed-case>ONLP</fixed-case> Lab Submission to the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 Shared Task</title>
      <author><first>Amit</first><last>Seker</last></author>
      <author><first>Amir</first><last>More</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <pages>208–215</pages>
      <url hash="be8ab8c2">K18-2021</url>
      <abstract>We present the contribution of the ONLP lab at the Open University of Israel to the UD shared task on multilingual parsing from raw text to Universal Dependencies. Our contribution is based on a transition-based parser called ‘yap   yet another parser’, which includes a standalone morphological model, a standalone dependency model, and a joint morphosyntactic model. In the <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> we used yap‘s standalone dependency parser to parse input morphologically disambiguated by UDPipe, and obtained the official score of 58.35 LAS. In our follow up investigation we use yap to show how the incorporation of morphological and lexical resources may improve the performance of end-to-end raw-to-dependencies parsing in the case of a morphologically-rich and low-resource language, Modern <a href="https://en.wikipedia.org/wiki/Hebrew_language">Hebrew</a>. Our results on <a href="https://en.wikipedia.org/wiki/Hebrew_language">Hebrew</a> underscore the importance of CoNLL-UL, a UD-compatible standard for accessing external lexical resources, for enhancing end-to-end UD parsing, in particular for morphologically rich and low-resource languages. We thus encourage the community to create, convert, or make available more such lexica in future tasks.<i>yap</i>‘s standalone dependency parser to parse input morphologically disambiguated by UDPipe, and obtained the official score of 58.35 LAS. In our follow up investigation we use yap to show how the incorporation of morphological and lexical resources may improve the performance of end-to-end raw-to-dependencies parsing in the case of a <i>morphologically-rich</i> and <i>low-resource</i> language, Modern Hebrew. Our results on Hebrew underscore the importance of CoNLL-UL, a UD-compatible standard for accessing external lexical resources, for enhancing end-to-end UD parsing, in particular for morphologically rich and low-resource languages. We thus encourage the community to create, convert, or make available more such lexica in future tasks.</abstract>
      <doi>10.18653/v1/K18-2021</doi>
      <bibkey>seker-etal-2018-universal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="23">
      <title>ELMoLex : Connecting ELMo and Lexicon Features for Dependency Parsing<fixed-case>ELM</fixed-case>o<fixed-case>L</fixed-case>ex: Connecting <fixed-case>ELM</fixed-case>o and Lexicon Features for Dependency Parsing</title>
      <author><first>Ganesh</first><last>Jawahar</last></author>
      <author><first>Benjamin</first><last>Muller</last></author>
      <author><first>Amal</first><last>Fethi</last></author>
      <author><first>Louis</first><last>Martin</last></author>
      <author><first>Éric</first><last>Villemonte de la Clergerie</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>223–237</pages>
      <url hash="01a41bae">K18-2023</url>
      <abstract>In this paper, we present the details of the neural dependency parser and the neural tagger submitted by our team ‘ParisNLP’ to the CoNLL 2018 Shared Task on parsing from raw text to Universal Dependencies. We augment the deep Biaffine (BiAF) parser (Dozat and Manning, 2016) with novel features to perform competitively : we utilize an indomain version of ELMo features (Peters et al., 2018) which provide context-dependent word representations ; we utilize disambiguated, embedded, morphosyntactic features from lexicons (Sagot, 2018), which complements the existing feature set. Henceforth, we call our <a href="https://en.wikipedia.org/wiki/System">system</a> ‘ELMoLex’. In addition to incorporating character embeddings, ELMoLex benefits from pre-trained word vectors, ELMo and morphosyntactic features (whenever available) to correctly handle rare or unknown words which are prevalent in languages with complex morphology. ELMoLex ranked 11th by Labeled Attachment Score metric (70.64 %), Morphology-aware LAS metric (55.74 %) and ranked 9th by Bilexical dependency metric (60.70 %).</abstract>
      <doi>10.18653/v1/K18-2023</doi>
      <bibkey>jawahar-etal-2018-elmolex</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    </volume>
  <volume id="3">
    <meta>
      <booktitle>Proceedings of the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018 Shared Task: Universal Morphological Reinflection</booktitle>
      <url hash="af74742d">K18-3</url>
      <editor><first>Mans</first><last>Hulden</last></editor>
      <editor><first>Ryan</first><last>Cotterell</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Brussels</address>
      <month>October</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="482bf89a">K18-3000</url>
      <bibkey>conll-2018-conll-sigmorphon</bibkey>
    </frontmatter>
    </volume>
</collection>