<?xml version='1.0' encoding='utf-8'?>
<collection id="Q19">
  <volume id="1">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 7</booktitle>
      <editor><last>Lee</last><first>Lillian</first></editor>
      <editor><last>Johnson</last><first>Mark</first></editor>
      <editor><last>Roark</last><first>Brian</first></editor>
      <editor><last>Nenkova</last><first>Ani</first></editor>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2019</year>
    </meta>
    <frontmatter>
      <bibkey>tacl-2019-transactions</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Semantic Neural Machine Translation Using AMR<fixed-case>AMR</fixed-case></title>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <doi>10.1162/tacl_a_00252</doi>
      <abstract>It is intuitive that semantic representations can be useful for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation models</a>. On the other hand, little work has been done on leveraging <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-to-German dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based sequence-to-sequence neural translation model.</abstract>
      <pages>19–31</pages>
      <url hash="5a3f3312">Q19-1002</url>
      <bibkey>song-etal-2019-semantic</bibkey>
      <pwccode url="https://github.com/freesunshine0316/semantic-nmt" additional="false">freesunshine0316/semantic-nmt</pwccode>
    </paper>
    <paper id="3">
      <title>Joint Transition-Based Models for Morpho-Syntactic Parsing : Parsing Strategies for MRLs and a Case Study from Modern Hebrew<fixed-case>MRL</fixed-case>s and a Case Study from <fixed-case>M</fixed-case>odern <fixed-case>H</fixed-case>ebrew</title>
      <author><first>Amir</first><last>More</last></author>
      <author><first>Amit</first><last>Seker</last></author>
      <author><first>Victoria</first><last>Basmova</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <doi>10.1162/tacl_a_00253</doi>
      <abstract>In standard NLP pipelines, morphological analysis and disambiguation (MA&amp;D) precedes syntactic and semantic downstream tasks. However, for <a href="https://en.wikipedia.org/wiki/Language">languages</a> with complex and ambiguous word-internal structure, known as morphologically rich languages (MRLs), it has been hypothesized that <a href="https://en.wikipedia.org/wiki/Context_(language_use)">syntactic context</a> may be crucial for accurate MA&amp;D, and vice versa. In this work we empirically confirm this hypothesis for Modern <a href="https://en.wikipedia.org/wiki/Hebrew_language">Hebrew</a>, an <a href="https://en.wikipedia.org/wiki/Modern_Hebrew_grammar">MRL</a> with complex morphology and severe word-level ambiguity, in a novel transition-based framework. Specifically, we propose a joint morphosyntactic transition-based framework which formally unifies two distinct transition systems, morphological and syntactic, into a single transition-based system with joint training and joint inference. We empirically show that MA&amp;D results obtained in the joint settings outperform MA&amp;D results obtained by the respective standalone components, and that end-to-end parsing results obtained by our joint system present a new state of the art for Hebrew dependency parsing.</abstract>
      <pages>33–48</pages>
      <video href="https://vimeo.com/384777366" />
      <url hash="f3f27f3a">Q19-1003</url>
      <bibkey>more-etal-2019-joint</bibkey>
    </paper>
    <paper id="6">
      <title>Synchronous Bidirectional Neural Machine Translation</title>
      <author><first>Long</first><last>Zhou</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <doi>10.1162/tacl_a_00256</doi>
      <abstract>Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework can not make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectionalneural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST ChineseEnglish, WMT14 EnglishGerman, and WMT18 RussianEnglish translation tasks. Experimental results demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art performance on ChineseEnglish and EnglishGerman translation tasks.</abstract>
      <pages>91–105</pages>
      <video href="https://vimeo.com/385255892" />
      <url hash="14f3e215">Q19-1006</url>
      <bibkey>zhou-etal-2019-synchronous</bibkey>
      <pwccode url="https://github.com/wszlong/sb-nmt" additional="true">wszlong/sb-nmt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="9">
      <title>GILE : A Generalized Input-Label Embedding for Text Classification<fixed-case>GILE</fixed-case>: A Generalized Input-Label Embedding for Text Classification</title>
      <author><first>Nikolaos</first><last>Pappas</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <doi>10.1162/tacl_a_00259</doi>
      <abstract>Neural text classification models typically treat output labels as <a href="https://en.wikipedia.org/wiki/Categorical_variable">categorical variables</a> that lack description and semantics. This forces their parametrization to be dependent on the label set size, and, hence, they are unable to scale to large label sets and generalize to unseen ones. Existing joint input-label text models overcome these issues by exploiting label descriptions, but they are unable to capture complex label relationships, have rigid parametrization, and their gains on unseen labels happen often at the expense of weak performance on the labels seen during training. In this paper, we propose a new input-label model that generalizes over previous such models, addresses their limitations, and does not compromise performance on seen labels. The model consists of a joint nonlinear input-label embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> performance. We evaluate models on full-resource and low- or zero-resource text classification of multilingual news and biomedical text with a large label set. Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios.</abstract>
      <pages>139–155</pages>
      <url hash="6d5277f7">Q19-1009</url>
      <bibkey>pappas-henderson-2019-gile</bibkey>
      <pwccode url="https://github.com/idiap/gile" additional="false">idiap/gile</pwccode>
    <title_pt>GILE: Uma incorporação generalizada de rótulo de entrada para classificação de texto</title_pt>
      <title_fr>GILE : intégration généralisée d'étiquettes d'entrée pour la classification de texte</title_fr>
      <title_ar>GILE: تضمين معمم لتسمية الإدخال لتصنيف النص</title_ar>
      <title_es>GILE: una incrustación generalizada de etiquetas de entrada para la clasificación de texto</title_es>
      <title_zh>GILE:广义输标嵌</title_zh>
      <title_ja>GILE:テキスト分類のための一般化された入力ラベル埋め込み</title_ja>
      <title_hi>GILE: पाठ वर्गीकरण के लिए एक सामान्यीकृत इनपुट-लेबल एम्बेडिंग</title_hi>
      <title_ru>GILE: Встраивание обобщённой метки ввода для классификации текста</title_ru>
      <title_ga>GILE: Leabú Lipéad Ionchuir Ginearálaithe le haghaidh Aicmiú Téacs</title_ga>
      <title_ka>Constellation name (optional)</title_ka>
      <title_el>Μια γενικευμένη ενσωμάτωση ετικετών εισόδου για ταξινόμηση κειμένου</title_el>
      <title_kk>GILE: Мәтін классификациясының жалпы ендіру жарлығы</title_kk>
      <title_hu>GILE: Általános bemeneti címke beágyazás a szövegosztályozáshoz</title_hu>
      <title_lt>GILE: Įterpiamas bendras įėjimo ženklas tekstui klasifikuoti</title_lt>
      <title_it>GILE: Embedding generalizzato di etichette di input per la classificazione del testo</title_it>
      <title_mk>ГИЛЕ: Генерализирано внесување на ознака за внесување за класификација на текст</title_mk>
      <title_ms>GILE: Penjelmaan Label-Input Umum untuk Klasifikasi Teks</title_ms>
      <title_mt>GILE: Tikketta Ġeneralizzata tal-Input Embedding għall-Klassifikazzjoni tat-Test</title_mt>
      <title_mn>GILE: Текст классификацийн нэвтрүүлэлт</title_mn>
      <title_pl>GILE: Ogólne osadzanie etykiet wejściowych dla klasyfikacji tekstu</title_pl>
      <title_no>GILE: Ein generelt innskriftsmerkelapp innebygging for tekstklassifikasjon</title_no>
      <title_ml>GILE: Text Classification</title_ml>
      <title_sr>GILE: Generalizirani ulazni etiketi za klasifikaciju teksta</title_sr>
      <title_ro>GILE: O încorporare generalizată a etichetelor de intrare pentru clasificarea textelor</title_ro>
      <title_si>Description</title_si>
      <title_ur>GILE: ایک عمومی اینپوٹ- لابل پیغام کلاسیفشن کے لئے ایمبڈ</title_ur>
      <title_ta>GILE: உரை வகைப்படுத்தலுக்கான பொதுவான உள்ளீட்டு விளக்கச்சீட்டு உட்பொதி</title_ta>
      <title_sv>GILE: En generaliserad inbäddning av inmatningsetikett för textklassificering</title_sv>
      <title_so>GILE: A General Input-label Embedding for Text Classification</title_so>
      <title_uz>GILE: Matn klassification uchun umumiy kiritish yorliqi</title_uz>
      <title_vi>GILE: Nhúng nhãn tự đặt chung cho hạng mục văn bản</title_vi>
      <title_bg>ГИЛ: Общо вграждане на входни етикети за класификация на текста</title_bg>
      <title_hr>GILE: Generalizirana uključena etiketa za klasifikaciju teksta</title_hr>
      <title_nl>GILE: Een algemene input-label insluiten voor tekstclassificatie</title_nl>
      <title_da>GILE: En generel indlejring af input-label til tekstklassifikation</title_da>
      <title_de>GILE: Eine generalisierte Eingabe-Label Einbettung für die Textklassifizierung</title_de>
      <title_id>GILE: A Generalised Input-Label Embedding for Text Classification</title_id>
      <title_ko>GILE: 텍스트 분류에 사용되는 광범위한 입력 탭 삽입 알고리즘</title_ko>
      <title_sw>GILE: Label ya Kijumla ya Kuingia kwa ajili ya Makala</title_sw>
      <title_sq>GILE: A Generalized Input-Label Embedding for Text Classification</title_sq>
      <title_fa>Constellation name (optional)</title_fa>
      <title_hy>ԳԻԼ. Տեքստի դասակարգման համար ընդհանուր մուտքագրման պիտակ</title_hy>
      <title_tr>GILE: Metin Sınıflandırmak üçin Umumy Girdi-etiket</title_tr>
      <title_az>Constellation name (optional)</title_az>
      <title_af>Constellation name (optional)</title_af>
      <title_bs>GILE: Generalizirani ulazni etiketi za klasifikaciju teksta</title_bs>
      <title_am>አቀማመጥ</title_am>
      <title_ca>GILE: Una etiqueta d'entrada generalitzada per a classificar text</title_ca>
      <title_cs>GILE: Všeobecné vložení vstupních štítků pro klasifikaci textu</title_cs>
      <title_et>GILE: Üldine sisendmärgise põimimine teksti klassifitseerimiseks</title_et>
      <title_bn>GILE: টেক্সট ক্লাসিশনের জন্য একটি সাধারণ ইনপুট- লেবেল এমবেডিং</title_bn>
      <title_fi>GILE: Yleinen syöttötunnisteen upotus tekstin luokittelua varten</title_fi>
      <title_jv>string" in "context_BAR_stringNew</title_jv>
      <title_sk>GILE: Splošna vgradnja nalepk za klasifikacijo besedila</title_sk>
      <title_he>GILE: תווית כניסה גנרליזציה למסגרת טקסט</title_he>
      <title_ha>GILE: KCharselect unicode block name</title_ha>
      <title_bo>GILE: སྤྱིར་བཏོན་ཡོད་པའི་འཇུག་སྟངས་ཁ་རྟགས་ཀྱི་ཁ་རྟགས་བཙུགས་ཡོད་པ</title_bo>
      <abstract_ar>عادةً ما تتعامل نماذج تصنيف النص العصبي مع تسميات المخرجات كمتغيرات فئوية تفتقر إلى الوصف والدلالات. هذا يفرض أن تكون معاملاتهم معتمدة على حجم مجموعة الملصقات ، وبالتالي ، فهم غير قادرين على التوسع في مجموعات الملصقات الكبيرة والتعميم على المجموعات غير المرئية. تتغلب نماذج نصوص تسمية الإدخال المشتركة الحالية على هذه المشكلات من خلال استغلال أوصاف الملصقات ، لكنها غير قادرة على التقاط علاقات تسمية معقدة ، ولها معايير صارمة ، ومكاسبها على الملصقات غير المرئية تحدث غالبًا على حساب الأداء الضعيف على الملصقات التي شوهدت أثناء التدريب. في هذه الورقة ، نقترح نموذجًا جديدًا لتسمية الإدخال يعمم على النماذج السابقة ويعالج حدودها ولا يضر بالأداء على الملصقات المرئية. يتكون النموذج من تضمين ملصق إدخال غير خطي مشترك بسعة يمكن التحكم فيها ووحدة تصنيف مشتركة تعتمد على الفضاء يتم تدريبها مع فقدان الانتروبيا لتحسين أداء التصنيف. نقوم بتقييم النماذج على تصنيف نصي كامل الموارد ومنخفض أو معدوم الموارد للأخبار متعددة اللغات والنص الطبي الحيوي مع مجموعة ملصقات كبيرة. يتفوق نموذجنا على النماذج أحادية اللغة ومتعددة اللغات التي لا تستفيد من دلالات الملصقات ونماذج مساحة تسمية الإدخال السابقة المشتركة في كلا السيناريوهين.</abstract_ar>
      <abstract_es>Los modelos de clasificación de texto neuronal suelen tratar las etiquetas de salida como variables categóricas que carecen de descripción y semántica. Esto obliga a que su parametrización dependa del tamaño del conjunto de etiquetas y, por lo tanto, no pueden escalar a conjuntos de etiquetas grandes y generalizar a juegos invisibles. Los modelos conjuntos de texto de etiquetas de entrada existentes solucionan estos problemas al explotar las descripciones de etiquetas, pero no pueden capturar relaciones de etiquetas complejas, tienen una parametrización rígida y sus ganancias en etiquetas invisibles a menudo se producen a expensas del bajo rendimiento de las etiquetas observadas durante la capacitación. En este artículo, proponemos un nuevo modelo de etiquetas de entrada que generaliza sobre dichos modelos anteriores, aborda sus limitaciones y no compromete el rendimiento de las etiquetas vistas. El modelo consiste en una incrustación de etiquetas de entrada no lineal conjunta con capacidad controlable y una unidad de clasificación dependiente del espacio conjunto que está entrenada con pérdida de entropía cruzada para optimizar el rendimiento de la clasificación. Evaluamos modelos de clasificación de textos de recursos completos y de recursos bajos o nulos de noticias multilingües y textos biomédicos con un juego de etiquetas grande. Nuestro modelo supera a los modelos monolingües y multilingües que no aprovechan la semántica de etiquetas ni los modelos anteriores de espacio de etiquetas de entrada conjunta en ambos escenarios.</abstract_es>
      <abstract_fr>Les modèles de classification de texte neuronal traitent généralement les étiquettes de sortie comme des variables catégorielles dépourvues de description et de sémantique. Cela force leur paramétrisation à dépendre de la taille de l'ensemble d'étiquettes et, par conséquent, ils ne peuvent pas être mis à l'échelle à de grands ensembles d'étiquettes et généralisés à des ensembles d'étiquettes invisibles. Les modèles de texte d'entrée et d'étiquette conjoints existants surmontent ces problèmes en exploitant les descriptions d'étiquettes, mais ils sont incapables de saisir des relations d'étiquette complexes, ont une paramétrisation rigide et leurs gains sur les étiquettes invisibles se produisent souvent au détriment de faibles performances sur les étiquettes vues pendant la formation. Dans cet article, nous proposons un nouveau modèle d'étiquette d'entrée qui généralise par rapport aux modèles précédents, aborde leurs limites et ne compromet pas les performances sur les étiquettes vues. Le modèle se compose d'une intégration d'étiquette d'entrée non linéaire conjointe avec une capacité contrôlable et d'une unité de classification dépendante de l'espace articulaire qui est entraînée avec une perte d'entropie croisée afin d'optimiser les performances de classification. Nous évaluons des modèles sur la classification textuelle des ressources complètes et des ressources faibles ou nulles des actualités multilingues et des textes biomédicaux avec un grand ensemble d'étiquettes. Notre modèle surpasse les modèles monolingues et multilingues qui ne tirent pas parti de la sémantique des étiquettes et des modèles d'espace d'entrée et d'étiquette conjoints précédents dans les deux scénarios.</abstract_fr>
      <abstract_pt>Os modelos de classificação de texto neural normalmente tratam os rótulos de saída como variáveis categóricas que carecem de descrição e semântica. Isso força sua parametrização a depender do tamanho do conjunto de rótulos e, portanto, eles são incapazes de escalar para conjuntos de rótulos grandes e generalizar para conjuntos não vistos. Os modelos de texto de rótulos de entrada conjuntos existentes superam esses problemas explorando descrições de rótulos, mas são incapazes de capturar relacionamentos de rótulos complexos, têm parametrização rígida e seus ganhos em rótulos não vistos geralmente acontecem às custas de desempenho fraco nos rótulos vistos durante o treinamento. Neste artigo, propomos um novo modelo de rótulo de entrada que generaliza sobre modelos anteriores, aborda suas limitações e não compromete o desempenho em rótulos vistos. O modelo consiste em uma incorporação de rótulo de entrada não linear conjunta com capacidade controlável e uma unidade de classificação dependente do espaço de articulação que é treinada com perda de entropia cruzada para otimizar o desempenho da classificação. Avaliamos modelos de classificação de texto com recursos completos e com poucos ou zero recursos de notícias multilíngues e texto biomédico com um grande conjunto de rótulos. Nosso modelo supera os modelos monolíngues e multilíngues que não aproveitam a semântica de rótulos e os modelos de espaço de entrada-rótulo conjuntos anteriores em ambos os cenários.</abstract_pt>
      <abstract_ja>ニューラルテキスト分類モデルは、典型的には、出力ラベルを、記述および意味論を欠く分類変数として扱う。 これにより、パラメータ化はラベルセットのサイズに依存するように強制され、したがって、大きなラベルセットにスケーリングし、見えないものに一般化することはできません。 既存のジョイントインプットラベルテキストモデルは、ラベル記述を利用することによってこれらの問題を克服しますが、複雑なラベル関係をキャプチャすることができず、堅固なパラメータ化を持ち、見えないラベルでの利得は、トレーニング中に見られるラベルのパフォーマンスの低下を犠牲にして発生することがよくあります。 本稿では，これまでのモデルよりも一般化し，限界に対処し，見たラベルのパフォーマンスを損なわない新しいインプットラベルモデルを提案する． モデルは、制御可能な容量を有するジョイント非線形入力ラベル埋め込みと、分類性能を最適化するためにクロスエントロピー損失でトレーニングされたジョイント空間依存分類ユニットで構成されています。 私たちは、多言語ニュースとバイオメディカルテキストのフルリソースと低リソースまたはゼロリソースのテキスト分類に関するモデルを、大きなラベルセットで評価します。 当社のモデルは、両方のシナリオでラベルセマンティクスと以前の共同入力ラベル空間モデルを活用していない単語および多言語モデルを上回っています。</abstract_ja>
      <abstract_zh>神经文本分模常以输标为少言语义之变量。 是故参数化依于小大,故不能广于大而广于不见也。 今有合输标签者,因其克之,不可得而得者,有严参数化,而其益于不见者,常以死练见者为贱。 凡新输标签,可概前法,以决其局限性,不损所见。 凡模形有可控容者非线性输标签嵌合空间依赖性分类单元成,该单元因交叉熵损训练以优化性能。 评估有大标签集多言新闻与生物医学文本全资、低资源零资源文本分类模型。 吾模优于单语与多言,二者皆无所用语义与前合输空。</abstract_zh>
      <abstract_hi>तंत्रिका पाठ वर्गीकरण मॉडल आमतौर पर आउटपुट लेबल को स्पष्ट चर के रूप में मानते हैं जिनमें वर्णन और शब्दार्थ की कमी होती है। यह उनके पैरामेट्रिज़ेशन को लेबल सेट आकार पर निर्भर होने के लिए मजबूर करता है, और इसलिए, वे बड़े लेबल सेट को स्केल करने और अनदेखी लोगों को सामान्यीकृत करने में असमर्थ हैं। मौजूदा संयुक्त इनपुट-लेबल टेक्स्ट मॉडल लेबल विवरणों का शोषण करके इन मुद्दों को दूर करते हैं, लेकिन वे जटिल लेबल संबंधों को पकड़ने में असमर्थ हैं, कठोर पैरामेट्राइजेशन करते हैं, और अनदेखी लेबल पर उनके लाभ अक्सर प्रशिक्षण के दौरान देखे गए लेबल पर कमजोर प्रदर्शन की कीमत पर होते हैं। इस पेपर में, हम एक नए इनपुट-लेबल मॉडल का प्रस्ताव करते हैं जो पिछले ऐसे मॉडलों पर सामान्यीकरण करता है, उनकी सीमाओं को संबोधित करता है, और देखे गए लेबल पर प्रदर्शन से समझौता नहीं करता है। मॉडल में नियंत्रणीय क्षमता के साथ एक संयुक्त nonlinear इनपुट-लेबल एम्बेडिंग और एक संयुक्त-अंतरिक्ष-निर्भर वर्गीकरण इकाई होती है जिसे वर्गीकरण प्रदर्शन को अनुकूलित करने के लिए क्रॉस-एन्ट्रॉपी हानि के साथ प्रशिक्षित किया जाता है। हम एक बड़े लेबल सेट के साथ बहुभाषी समाचार और बायोमेडिकल पाठ के पूर्ण-संसाधन और कम-या शून्य-संसाधन पाठ वर्गीकरण पर मॉडल का मूल्यांकन करते हैं। हमारा मॉडल मोनोलिंगुअल और बहुभाषी मॉडल से बेहतर प्रदर्शन करता है जो दोनों परिदृश्यों में लेबल शब्दार्थ और पिछले संयुक्त इनपुट-लेबल स्पेस मॉडल का लाभ नहीं उठाते हैं।</abstract_hi>
      <abstract_ru>Нейронные модели классификации текста обычно рассматривают выходные метки как категориальные переменные, которым не хватает описания и семантики. Это заставляет их параметризацию зависеть от размера набора меток, и, следовательно, они не могут масштабироваться до больших наборов меток и обобщаться до невидимых. Существующие совместные текстовые модели с входными ярлыками преодолевают эти проблемы, используя описания ярлыков, но они не в состоянии фиксировать сложные взаимосвязи ярлыков, имеют жесткую параметризацию, и их достижения на невидимых ярлыках часто происходят в ущерб слабой производительности на ярлыках, наблюдаемых во время обучения. В этой статье мы предлагаем новую модель входной этикетки, которая обобщает предыдущие такие модели, рассматривает их ограничения и не ставит под угрозу производительность на видимых этикетках. Модель состоит из совместного нелинейного вложения входной метки с контролируемой емкостью и блока совместной пространственно-зависимой классификации, который обучается с перекрестными энтропическими потерями для оптимизации производительности классификации. Мы оцениваем модели полноресурсной и малоресурсной или нулевой текстовой классификации многоязычных новостей и биомедицинского текста с большим набором этикеток. Наша модель превосходит одноязычные и многоязычные модели, которые не используют семантику меток и предыдущие модели совместного пространства входных данных и меток в обоих сценариях.</abstract_ru>
      <abstract_ga>De ghnáth déileálann samhlacha aicmithe téacs néaracha le lipéid aschuir mar athróga catagóiriúla nach bhfuil cur síos orthu ná an tséimeantaic. Cuireann sé seo iallach ar a bparaiméadracht a bheith ag brath ar mhéid na lipéad, agus, mar sin, níl siad in ann scála a dhéanamh go tacair lipéid mhóra agus ginearálú chuig na cinn nach bhfacthas riamh cheana. Sáraítear na saincheisteanna sin trí thuairiscí lipéid a shaothrú i samhlacha comhpháirteacha téacs lipéid ionchuir, ach ní féidir leo gaolmhaireachtaí casta lipéad a ghabháil, tá paraiméadracht docht acu, agus tarlaíonn a ngnóthachan ar lipéid nach bhfacthas riamh cheana ar chostas lagfheidhmíochta ar na lipéid a fheictear le linn na hoiliúna. Sa pháipéar seo, molaimid múnla lipéad ionchuir nua a dhéanann ginearálú ar shamhlacha dá leithéid roimhe seo, a thugann aghaidh ar a dteorainneacha, agus nach gcuireann isteach ar fheidhmíocht ar lipéid a fheictear. Is éard atá sa tsamhail comhlipéad neamhlíneach ionchuir a neadaíonn cumas inrialaithe agus aonad aicmithe comhspás-spleách atá oilte le caillteanas tras-eantrópachta chun an fheidhmíocht aicmithe a bharrfheabhsú. Déanaimid meastóireacht ar shamhlacha ar aicmiú téacs lán-acmhainne agus acmhainní íseal nó nialasach ar nuacht ilteangach agus ar théacs bithleighis le tacar mór lipéad. Feidhmíonn ár múnla níos fearr ná samhlacha aonteangacha agus ilteangacha nach n-eascraíonn séimeanaic lipéad agus samhlacha spáis comhlipéid ionchuir roimhe seo sa dá chás.</abstract_ga>
      <abstract_el>Τα νεανικά μοντέλα ταξινόμησης κειμένου συνήθως αντιμετωπίζουν τις ετικέτες εξόδου ως κατηγοριακές μεταβλητές που στερούνται περιγραφής και σημασιολογίας. Αυτό αναγκάζει την παραμετροποίηση τους να εξαρτάται από το μέγεθος του συνόλου ετικετών και, ως εκ τούτου, δεν είναι σε θέση να κλιμακωθούν σε μεγάλα σύνολα ετικετών και να γενικεύσουν σε αόρατα. Τα υπάρχοντα κοινά μοντέλα κειμένου εισαγωγής-ετικέτας ξεπερνούν αυτά τα ζητήματα αξιοποιώντας περιγραφές ετικετών, αλλά δεν είναι σε θέση να συλλάβουν πολύπλοκες σχέσεις ετικετών, έχουν άκαμπτη παραμετροποίηση και τα κέρδη τους σε αόρατες ετικέτες συμβαίνουν συχνά σε βάρος της αδύναμης απόδοσης στις ετικέτες που παρατηρούνται κατά τη διάρκεια της εκπαίδευσης. Σε αυτή την εργασία, προτείνουμε ένα νέο μοντέλο εισαγωγής-ετικέτας που γενικεύει σε σχέση με προηγούμενα τέτοια μοντέλα, αντιμετωπίζει τους περιορισμούς τους και δεν θέτει σε κίνδυνο την απόδοση στις ορατές ετικέτες. Το μοντέλο αποτελείται από μια κοινή μη γραμμική ενσωμάτωση ετικετών εισόδου-εισόδου με ελεγχόμενη ικανότητα και μια μονάδα ταξινόμησης που εξαρτάται από το κοινό-χώρο που εκπαιδεύεται με απώλεια διασταυρούμενης εντροπίας για τη βελτιστοποίηση της απόδοσης ταξινόμησης. Αξιολογούμε μοντέλα για την πλήρη και χαμηλή ή μηδενική ταξινόμηση κειμένων πολυγλωσσικών ειδήσεων και βιοϊατρικού κειμένου με μεγάλο σύνολο ετικετών. Το μοντέλο μας ξεπερνά τα μονογλωσσικά και πολυγλωσσικά μοντέλα που δεν αξιοποιούν τη σημασιολογία ετικετών και προηγούμενα κοινά μοντέλα χώρου εισόδου-ετικετών και στα δύο σενάρια.</abstract_el>
      <abstract_ka>ნეიროლური ტექსტის კლასიფიკაციის მოდელები ტიპულად გამოყენება ლექტები როგორც კატეგორიალური ცვლილები, რომლებიც გამოყენება და სიმენტიკები არს ეს მათი პარამეტრიზაციას უნდა იყოს ჩატვირთებული ზომიდან დააყენებული, და ამიტომ მათი არ შეუძლებელია დიდი ჩატვირთების ზომიდან მარცხოვრება და გენერალიზაცია არ არსებობს ერთადერთი მონაცემების ტექსტის მოდელები ამ პრობლემების გამოყენებით, მაგრამ ისინი არ შეუძლებელია კომპლექსი ტექსტის შესახებ, არსებობენ კომპლექსი ტექსტის შესახებ, არსებობენ პარამეტრიზაცია და ის ამ დოკუნეში ჩვენ ახალი მონაცემების მოდელს, რომელიც წინ ასეთი მოდელების განმავლობაში გენერალიზება, მისი დროების მისამართება და არ გამოყენებს კომპრომიზებას ჩვენებული eti მოდელის შეფარდება კონტროლიფიკაციის კონტროლიფიკაციის შესახებ და კონტროლიფიკაციის კონტროლიფიკაციის კონტროლიფიკაციის ერთეულისთვის, რომელიც კრესიკოლიფიკაციის კონტროლიფიკაციის გამოსახულებლა ჩვენ მოდელები მულ რესურსის და ცოლ რესურსის ტექსტის კლასიფიკაციაში მრავალენგური ინფორმაციის და ბიომედიციური ტექსტის შესახებ, რომელიც დიდი რესურსის შესა ჩვენი მოდელი მონოლენგური და მრავალენგური მოდელეები გავაკეთებს, რომლებიც არ შეუძლიათ ლებლიკური სიმენტიკები და წინაღალდეგი მრავალური სიმენტიკების მოდელეები ორი</abstract_ka>
      <abstract_it>I modelli di classificazione del testo neurale tipicamente trattano le etichette di output come variabili categoriche prive di descrizione e semantica. Questo costringe la loro parametrizzazione a dipendere dalla dimensione del set di etichette, e, quindi, non sono in grado di scalare a grandi set di etichette e generalizzare a quelli invisibili. I modelli di testo comuni input-label esistenti risolvono questi problemi sfruttando le descrizioni delle etichette, ma non sono in grado di catturare relazioni di etichetta complesse, hanno una parametrizzazione rigida e i loro guadagni su etichette invisibili avvengono spesso a scapito di prestazioni deboli sulle etichette osservate durante la formazione. In questo articolo, proponiamo un nuovo modello di input-label che generalizzi sui modelli precedenti di questo tipo, affronti i loro limiti e non comprometta le prestazioni sulle etichette viste. Il modello consiste in un embedding congiunto non lineare di input-label con capacità controllabile e un'unità di classificazione congiunta-spazio-dipendente che è addestrata con perdita di entropia incrociata per ottimizzare le prestazioni di classificazione. Valutiamo modelli di classificazione dei testi a risorse complete e a risorse basse o zero di notizie multilingue e testi biomedici con un set di etichette di grandi dimensioni. Il nostro modello supera i modelli monolingue e multilingue che non sfruttano la semantica delle etichette e i precedenti modelli comuni di spazio input-label in entrambi gli scenari.</abstract_it>
      <abstract_hu>A neurális szövegosztályozási modellek általában kategorikus változóként kezelik a kimeneti címkéket, amelyek hiányzik a leírás és a szemantika. Ez arra kényszeríti a paraméterezésüket, hogy a címkészlet méretétől függően legyen, így nem tudnak nagy címkészletekre skálázni és általánosítani a láthatatlanokra. A meglévő közös bemeneti-címke szövegmodellek ezeket a problémákat a címke leírásainak kihasználásával oldják meg, de nem képesek összetett címke kapcsolatokat rögzíteni, merev paraméterezéssel rendelkeznek, és a láthatatlan címkéken elért eredményeik gyakran a címkék gyenge teljesítményének rovására fordulnak elő. Ebben a tanulmányban egy új input-label modellt javasolunk, amely általánosítja a korábbi ilyen modelleket, kezeli azok korlátait, és nem veszélyezteti a látható címkék teljesítményét. A modell egy vezérelhető kapacitással rendelkező, közös, nemlineáris bemeneti címke beágyazásból és egy közös térfüggő osztályozási egységből áll, amelyet keresztentrópia veszteséggel képeztek az osztályozási teljesítmény optimalizálására. Többnyelvű hírek és orvosbiológiai szövegek teljes forrású és alacsony vagy nulla forrású szövegosztályozására vonatkozó modelleket értékeljük nagy címkekészlettel. Modellünk mindkét forgatókönyvben felülmúlja az egynyelvű és többnyelvű modelleket, amelyek nem használják a címke szemantikáját és a korábbi közös bemeneti-címke térmodelleket.</abstract_hu>
      <abstract_ms>Model klasifikasi teks saraf biasanya menjaga label output sebagai pembolehubah kategori yang kekurangan keterangan dan semantik. Ini memaksa parametrisasi mereka bergantung pada saiz set label, dan, oleh itu, mereka tidak dapat skala ke set label besar dan umumkan kepada yang tidak terlihat. Model teks input-label kongsi yang wujud mengatasi isu-isu ini dengan mengeksploitasi deskripsi label, tetapi mereka tidak dapat menangkap hubungan label kompleks, mempunyai parametrisasi yang ketat, dan keuntungan mereka pada label yang tidak terlihat sering berlaku pada biaya prestasi lemah pada label yang dilihat semasa latihan. Dalam kertas ini, kami cadangkan model input-label baru yang menyebarkan lebih daripada model sebelumnya, mengarahkan keterangan mereka, dan tidak kompromi prestasi pada label yang dilihat. Model ini terdiri dari label input-bukan linear bersatu yang memasukkan dengan kapasitas yang boleh dikawal dan unit klasifikasi bergantung-ruang bersatu yang dilatih dengan kehilangan entropi salib untuk optimumkan prestasi klasifikasi. Kami menilai model pada kelasukan teks sumber penuh dan sumber rendah atau sifar bagi berita berbilang bahasa dan teks biomedikal dengan set label besar. Model kita melebihi model monobahasa dan berbilang bahasa yang tidak menggunakan label semantik dan model ruang input-label terdahulu dalam kedua-dua skenario.</abstract_ms>
      <abstract_kk>Нейрондық мәтін классификациялау үлгілері, әдетте шығыс жарлықтарын сипаттамасы мен семантикалық деген категориялық айнымалылар ретінде қалайды. Бұл параметрлерін жарлық орнатылған өлшеміне тәуелді, сондықтан олар үлкен жарлық жиындарына масштабтау және көрсетілмейді. Бар жалпы енгізу жарлығының мәтін үлгілері жарлығының сипаттамасын қолдану арқылы осы мәселелерді көмектеседі, бірақ олар комплекс жарлықтар қатынасын қабылдауға болмайды, дұрыс параметрлерін қабылдауға болады, және олардың көріні Бұл қағаздың жаңа енгізу үлгісін ұсынып, алдыңғы үлгілерді жалғастырып, шектеулерін адрестеп, көрінетін жарлықтардың істеуін көмектеспейді. Бұл үлгі басқару мүмкіндігі мен біріктірілген бос орын тәуелді классификациялық бірлігі, біріктірілген ентропиялық жоғалу үшін біріктірілген жоғалу үшін біріктірілген нелиниялық енгізу жарлығы Біз үлгілерді толық ресурс мен төмен не нөл ресурс мәтінді бірнеше тілдік жаңалықтар және биомедикалық мәтінді үлкен жарлықтарды бағалаймыз. Біздің үлгіміз екі сценарияда бірнеше тілді мен бірнеше тілді үлгілерді өзгертпейді. Бұл белгілерді семантикалық пен алдыңғы келтірілген кеңістік үлгілерді екі сценарияда</abstract_kk>
      <abstract_lt>Neural text classification models typically treat output labels as categorical variables that lack description and semantics.  This forces their parametrization to be dependent on the label set size, and, hence, they are unable to scale to large label sets and generalize to unseen ones.  Existing joint input-label text models overcome these issues by exploiting label descriptions, but they are unable to capture complex label relationships, have rigid parametrization, and their gains on unseen labels happen often at the expense of weak performance on the labels seen during training.  Šiame dokumente siūlome naują įvesties etiketės model į, kuris apibendrina ankstesnius tokius modelius, atsižvelgia į jų apribojimus ir nekelia pavojaus matytiems etiketėms. Modelą sudaro jungtinis nelinijinis įvesties ženklas su kontroliuojamu pajėgumu ir jungtinis nuo erdvės priklausomas klasifikavimo vienetas, kuris mokomas su kryžminiu entropijos praradimu siekiant optimizuoti klasifikavimo charakteristikas. Vertiname daugiakalbių naujienų ir biomedicinio teksto, kuriame pateikiama didelė etiketė, visiškų išteklių ir mažų arba nulinių išteklių teksto klasifikavimo modelius. Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios.</abstract_lt>
      <abstract_ml>നെയുറല്‍ ടെക്സ്റ്റ് ക്ലാസ്ഫിക്ഷന്‍ മോഡലുകള്‍ സാധാരണ പുറപ്പെടുത്തുന്ന ലേബ്ലുകള്‍ വിശദീകരണവും സെമാന്റിക്സ ലേബ്ലേറ്റിന്റെ വലിപ്പം ആശ്രയിക്കുന്നതിനായി അവയുടെ പാരാമീറ്ററിഷനെ ഇത് പ്രവര്‍ത്തിപ്പിക്കുന്നു. അതിനാല്‍ അവര്‍ക്ക് വലിയ ലേ ലേബിള്‍ വിശദീകരണങ്ങള്‍ ഉപയോഗിക്കുന്നതിനാല്‍ നിലവിലുള്ള സഹജമായ ഇന്‍പുട്ട് ലേബിലേറ്റ് ടെക്സ്റ്റ് മാതൃകങ്ങള്‍ പരിജയപ്പെടുത്തുവാന്‍ അവര്‍ക്ക് കഴിയില്ല, പരിശീലനത്തി ഈ പത്രത്തില്‍, മുമ്പുള്ള ഇങ്ങനെയുള്ള മോഡലുകളില്‍ സാധാരണമാക്കുന്ന ഒരു പുതിയ ഇന്‍പുട്ട് ലേബ് മോഡല്‍ ഞങ്ങള്‍ പ്രായശ്ചിത്തം ചെയ്യുന നിയന്ത്രിക്കാവുന്ന ശക്തിയോടും നിയന്ത്രിക്കാവുന്ന സ്പെയിസ്റ്റ് ആശ്രയിക്കുന്ന ക്ലാസ്പെയിഷനിന്റെ ക്ലാസ്റ്റിഫ്റ്റ് ആശ്രയിക്കുന്ന ഒര ഞങ്ങള്‍ മോഡലുകളെ മുഴുവന്‍ വിഭവങ്ങളിലും കുറഞ്ഞ വിഭവങ്ങളിലും പൂര്‍ണ്ണമായ വിഭവങ്ങളിലും പദാവലിയിലെ വാര്‍ത്തകളിലും ബൈവിയോമിക്കല്‍ വാ നമ്മുടെ മോഡല്‍ മൊണോളില്‍ഭാഷകങ്ങളും പല ഭാഷകങ്ങളും പ്രവര്‍ത്തിപ്പിക്കുന്നു. ലേബറ്റിന്റെ സെമാന്റിക്സും മുമ്പ് യൂട്ട് ഇന്‍പു</abstract_ml>
      <abstract_mk>Моделите за класификација на неуралниот текст обично ги третираат излезните етикети како категорички променливи кои немаат опис и семантика. Ова ја принуди нивната параметризација да зависи од големината на поставената етикета, и затоа тие не можат да се скалираат на големи поставени етикети и да се генерализираат на невидени. Постојаните заеднички текстови модели на вложена етикета ги надминуваат овие прашања со искористување на описите на етикетата, но тие не можат да фатат комплексни односи со етикетата, имаат цврста параметризација, и нивните добивки на невидени етикети честопати се случуваат на трошок на слабата перформанса на етикетите вид Во овој документ, предложуваме нов модел на вложена етикета кој се генерализира во однос на претходните вакви модели, се обраќа на нивните ограничувања и не ја компромитира перформансата на видени етикети. The model consists of a joint nonlinear input-label embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance.  Ние ги проценуваме моделите на целосен ресурс и ниско или нула ресурс класификација на текст на мултијазични вести и биомедички текст со голем сет на етикети. Нашиот модел ги надминува монојазичните и мултијазичните модели кои не влијаат на семантиката на етикетата и претходните заеднички вселенски модели на влезот-етикетата во двата сценарија.</abstract_mk>
      <abstract_mn>Тархины текст хуваалтын загварууд ихэвчлэн үржүүлэх загваруудыг тайлбарлах болон semantics албагүй категорийн хувьсагчид гэж үздэг. Энэ нь тэдний параметрийг загварын хэмжээнд хамааралтай болгож байна. Тэгэхээр тэд том загварын хэмжээнд хэмжээтэй болж, харагдахгүй зүйлсийг ерөнхийлөгч болгож чадахгүй. Эдгээр асуудлуудыг загварын тодорхойлолтоор ашиглан хамтдаа орж ирсэн бичил загварын загваруудыг давхарлаа. Гэхдээ тэд комплекс загварын харилцаа барьж чадахгүй, хүчтэй параметрийг барьж чадахгүй, харин тэдний шинжлэх загварууд сургалтын үед харагда Энэ цаасан дээр бид өмнө ийм загваруудыг ерөнхийлөгчилж, хязгаарлалтыг удирдаж, харагдаж байгаа загваруудыг тодорхойлж чадахгүй шинэ бичсэн загварын загварыг санал болгож байна. Энэ загвар нь хяналттай чадвартай холбогдсон шулуун биш шулууны оролцоогүй загварын нэгжтэй бөгөөд хамааралтай загварын холбогдолтой хуваалцах нэгжтэй холбогдолтой юм. Бид олон хэлний мэдээллийн, биологийн медицины текстүүдийн бүрэн болон бага эсвэл 0-нүүрстөрөгчийн текстүүдийн хуваалцааны загварыг үнэлдэг. Бидний загвар нь нэг хэл болон олон хэл загваруудыг дамжуулдаг. Эдгээр загварууд хоёр хувилбарт нэг хэл загваруудыг ашиглахгүй.</abstract_mn>
      <abstract_no>Klassifikasjonsmodeller for neiraltekst behandler vanlegvis utdata- merkelappar som kategoriske variabler som manglar skildring og semantikk. Dette påkraver parametriseringa sine til å vera avhengig av merkelappen sett storleik, og derfor kan dei ikkje skalera til store merkelappane og generellisera til ukjende. Det eksisterande felles tekstmodeller for innskriftsmerkelappen overfører desse problemene ved å bruka merkelappebeskrivelser, men dei kan ikkje henta komplekse merkelappunksjonar, ha sterke parametrisering, og hendinga på ukjende merkelapper skjer ofte på uttrykket av svake uttrykk på merkelappene som ser under opplæring. I denne papiret foreslår vi eit nytt innskriftsmerkelappemodell som genereliserer over førre slike modeller, adresserer grensene sine, og ikkje kompromerer utviklinga på sette merkelapper. Modellen består av ei kopla ikkje-lineær innskriftsmerkelapp innebygd med kontrollbare kapasitet og ei kopla mellomromavhengig klassifikasjonseining som er trent med tap av krysentropi for å optimalisera klassifikasjonsfunksjon. Vi evaluerer modeller på fullstendig ressurs og låg- eller null- ressurstekstklassifikasjon av fleirspråk nyhetar og biomedisk tekst med ein stor merkelapp. Modellen vårt utfører monospråk og fleirspråk modeller som ikkje leverer merkelappen semantikk og førre fleire mellommerkelapper i begge scenarior.</abstract_no>
      <abstract_ro>Modelele de clasificare a textelor neurale tratează de obicei etichetele de ieșire ca variabile categorice care lipsesc descriere și semantică. Acest lucru forțează parametrizarea lor să fie dependentă de dimensiunea setului de etichete și, prin urmare, ei nu sunt capabili să scaleze la seturi mari de etichete și să generalizeze la cele nevăzute. Modelele text de intrare și etichete comune existente depășesc aceste probleme prin exploatarea descrierilor etichetelor, dar ele nu sunt capabile să capteze relații complexe de etichete, au parametrizare rigidă, iar câștigurile lor pe etichetele nevăzute se întâmplă adesea în detrimentul performanțelor slabe pe etichete observate în timpul antrenamentului. În această lucrare, propunem un nou model de intrare-etichetă care generalizează asupra modelelor anterioare, abordează limitările acestora și nu compromite performanța pe etichetele văzute. Modelul constă dintr-o încorporare neliniară de intrare-etichetă comună cu capacitate controlabilă și o unitate de clasificare dependentă de spațiu comun care este instruită cu pierdere de entropie încrucișată pentru a optimiza performanța clasificării. Evaluăm modele privind clasificarea textelor cu resurse complete și cu resurse reduse sau zero a știrilor multilingve și a textelor biomedicale cu un set mare de etichete. Modelul nostru depășește modelele monolingve și multilingve care nu utilizează semantica etichetelor și modelele anterioare de spațiu de intrare-etichetă comune în ambele scenarii.</abstract_ro>
      <abstract_sr>Modeli klasifikacije neuronskog teksta obično tretiraju etikete izlaza kao kategorijske varijante koje nedostaju opis i semantike. To nudi da njihova parametrizacija zavisi od veličine određene etikete, i zato oni ne mogu da skalaju na velike sete etikete i generalizuju na nevidljive. Postoje zajednički modeli teksta oznake za ulazak prevladaju te probleme iskorištavanjem opisa oznake, ali oni nisu u mogućnosti da uhvate kompleksne odnose oznake, imaju krute parametrizacije, a njihove dobitke na nevidljivim etiketama se često dešavaju na troškovi slabe funkcije na etiketama koje su vidjele tokom treninga. U ovom papiru predlažemo novi model ulaznog etiketa koji generalizuje preko prethodnih takvih model a, adresuje njihove ograničenja i ne kompromisuje performancu na viđenim etiketama. Model se sastoji od zajedničkog nelinearnog ulaznog etiketa ugrađenog sa kontrolnom kapacitetom i zajedničkom klasifikacijskom jedinicom ovisnom od svemira koja je obučena sa gubitkom krsno entropije kako bi optimizirala klasifikaciju. Procjenjujemo modele na klasifikaciji teksta punog resursa i niskog ili nulog resursa multijezičkih vijesti i biomedicinskog teksta sa velikim nametom. Naš model iznosi monojezičke i multijezičke modele koji ne utiču na etikete semantike i prethodne zajedničke svemirske modele za ulazak u obje scenarije.</abstract_sr>
      <abstract_so>Tusaaladaha tababaridda ee qoraalka naadiga ah sida caadiga ah waxaa loo isticmaalaa alaabta soo baxa oo kala duwan oo aan u baahnayn sawir iyo semantika. Tani waxay ku xiran karaan parameterisadooda, waxayna ku xiran yihiin tirada calaamadda, sababtaas darteed ma awoodi karaan inay kor u qaadaan xarumaha calaamada waaweyn iyo inay u soo bandhigaan meelaha qarsoon. Tusaalooyinka qoraalka ee wadajirka ah ee laga soo qoro qoraalkaas waxay ku adkaan karaan warqadaha calaamada, laakiin way awoodi kari waayaan inay qabtaan xiriirka qalabka adag, waxay leeyihiin mid si adag u isticmaalaya, faa'iidadooda ku baxana baalasha qarsoon waxey marar badan ku dhacaan kharashka tababarka tababarka lagu arag xilliga waxbarashada. Qoraalkan waxaynu soo jeedaynaa model cusub oo laga soo bandhigayo tusaale ahaan hore oo kale, waxaana ku qoraynaa xuduudaha, mana sameynayo tababarka lagu arko alaabta. Tusaalada waxaa ka mid ah mid ka mid ah mid ka mid ah wadajir-aan-linear input-label oo ku qoran awoodda kontroll leh iyo qaybta iskuulka oo wadajir-space-ku xiran, oo lagu baran karo khasaarada korontopy si uu u optimiso fasaxa fasaxa. Tusaalada waxaan ku qiimeynaynaa qoraalka qoraalka hoose-iyo-hoos-ama-zero-resource ee warbixinta luuqadaha kala duduwan iyo qoraalka biomedical ah oo ku qoran calaamad weyn. Tusaale'dayadu wuxuu sameeyaa tusaalooyin luuqado kala duduwan oo aan ku isticmaalin calaamada semantics iyo samooyinka hore oo ka mid ah mid-ka-gala-label-label labadoodaba.</abstract_so>
      <abstract_sv>Neurala textklassificeringsmodeller behandlar vanligtvis utmatningsetiketter som kategoriska variabler som saknar beskrivning och semantik. Detta tvingar deras parametrisering att vara beroende av etikettuppsättningens storlek, och därför kan de inte skala till stora etikettuppsättningar och generalisera till osynliga. Befintliga gemensamma textmodeller löser dessa problem genom att utnyttja etikettbeskrivningar, men de kan inte fånga komplexa etikettrelationer, har stel parametrisering, och deras vinster på osynliga etiketter sker ofta på bekostnad av svag prestanda på etiketterna som ses under träning. I denna uppsats föreslår vi en ny inmatningsmodell som generaliserar över tidigare sådana modeller, tar itu med deras begränsningar och inte kompromissar prestanda på sedda etiketter. Modellen består av en gemensam icke-linjär inmatningsmärkning med kontrollerbar kapacitet och en gemensam rymdberoende klassificeringsenhet som utbildas med korsentropiförlust för att optimera klassificeringsprestandan. Vi utvärderar modeller för full- och låg- eller nollresurstextklassificering av flerspråkiga nyheter och biomedicinsk text med en stor etikettuppsättning. Vår modell överträffar enspråkiga och flerspråkiga modeller som inte utnyttjar etikettsemantik och tidigare gemensamma inmatningsmodeller för etiketter i båda scenarierna.</abstract_sv>
      <abstract_ta>நெருல் உரை வகைப்படுத்தல் மாதிரிகள் வழக்கமாக வெளியீட்டு சிட்டைகளை வகைப்பட்ட மாறிகளாக பயன்படுத்துகிறது. விவரிப்பும இது விளக்கச்சீட்டு அளவு சார்ந்து கொள்ள அளபுருவை செய்ய முடியாது, அதனால் பெரிய விளக்கச் சீட்டு அமைப்புகளுக்கு அளவிட முடியாத தற்போதைய சேரும் உள்ளீட்டு உரை மாதிரிகள் சிட்டை விளக்கங்களை பயன்படுத்தி இந்த விஷயங்களை வெற்றி பெறுகிறது, ஆனால் அவர்களால் சிக்கலான விளக்கச்சீட்டு உறவுகளை பிடிக்க முடியாத இந்த தாளில், நாம் முந்தைய மாதிரியில் புதிய உள்ளீட்டு மாதிரியை பரிந்துரைக்கிறோம். இது முந்தைய மாதிரிகளில் பொதுவாக்கும், அவற இந்த மாதிரியில் உள்ளீடு- விளக்கம் கட்டுப்படுத்தக்கூடிய சக்தியுடன் உட்பொதிந்த ஒரு இணைக்கோடு அல்லாத உள்ளீட்டு விளக்கச்சீட்டு மற்றும் ஒரு joint- space- சா நாம் முழு மூலத்திலும் குறைந்த அல்லது பூஜ்ஜியமான மூலத்திலும் உரை வகைப்பிலும் மாதிரிகளை மதிப்பிடுகிறோம் பல மொழி செய்தியும எங்கள் மாதிரி மொன்மொழி மற்றும் பல மொழி மாதிரி மாதிரிகளை செயல்படுத்துகிறது இவை இரண்டு காட்சிகளிலும் முந்தைய இணைய உள்ளீட்டு வெள</abstract_ta>
      <abstract_ur>نائورل ٹیکسٹ کلاسپیٹ موڈل معمولاً اپوٹ وٹ وٹ لیبل کو کلاسپیٹ ویرئیٹ کے طور پر دکھاتے ہیں جن کی توصیف اور سیمانٹیک نہیں ہے. یہ ان کے پارامیٹریزی کو لابل سٹ کی سائز پر اعتماد کرنے کے لئے مجبور کرتا ہے، اور لہذا وہ بڑے لابل سٹ تک اسکیل نہیں کر سکتے اور غیب کی سائل کو آسان کر سکتے ہیں۔ Existing joint input-label text models overcome these issues by using label descriptions, but they are unable to capture complex label relationships, have rigid parametrization, and their gains on unseen labels often happen at the expense of weak performance on the labels seen during training. اس کاغذ میں ہم ایک نئی اینٹ لیبل موڈل پیشنهاد کرتے ہیں جو پہلے ایسے موڈل پر جرائل کرتا ہے، ان کی محدودیت کو ادرس کرتا ہے، اور دیکھے لیبل پر کامپیوتر کمزور نہیں کرتا۔ Model consists of a joint nonlinear input-label embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance. ہم نے مدلکوں کو پورا-سروسیس اور کم یا صفر-سروسیس ٹیکسٹ کلیسٹ پر مطالعہ کرلیا ہے multilingual news اور biomedical text کے ایک بڑے لیبل سٹ کے ساتھ. ہمارا موڈل ایک زبان اور ملتی زبان کی موڈلیاں کامل کرتا ہے جو لیبل سیمانٹیکوں اور پہلے سے پیدا ہونے والی اینپ لابل فضا موڈلیاں دونوں سینا ریوئیوں میں نہیں لگاتے۔</abstract_ur>
      <abstract_mt>Mudelli ta’ klassifikazzjoni tat-test newrali tipikament jittrattaw it-tikketti tal-output bħala varjabbli kategoriċi li m’għandhomx deskrizzjoni u semantika. Dan iġġiegħel lill-parametrizzazzjoni tagħhom tkun dipendenti fuq id-daqs tas-sett tat-tikketta, u għalhekk ma jistgħux jikbru l-iskala għal settijiet kbar ta’ tikketta u jiġġeneralizzaw għal dawk li ma jidhrux. Il-mudelli tat-test konġunti eżistenti tat-tikketta tal-input jegħlbu dawn il-kwistjonijiet billi jisfruttaw deskrizzjonijiet tat-tikketta, iżda ma jistgħux jaqbdu relazzjonijiet kumplessi tat-tikketta, ikollhom parametrizzazzjoni riġida, u l-kisbiet tagħhom fuq tikketti mhux osservati sikwit iseħħu bi spiża ta’ prestazzjoni dgħajfa fuq it-tikketti li dehru waqt it-taħriġ. F’dan id-dokument, qed nipproponu mudell ġdid ta’ tikketta tal-input li jiġġeneralizza fuq mudelli preċedenti bħal dawn, jindirizza l-limitazzjonijiet tagħhom, u ma jikkompromettix il-prestazzjoni fuq tikketti li jidhru. Il-mudell jikkonsisti f’tikketta tal-input konġunta mhux lineari li tinkorpora b’kapaċità kontrollabbli u unit à ta’ klassifikazzjoni konġunta dipendenti fuq l-ispazju li hija mħarrġa b’telf ta’ entropija trasversali biex tiġi ottimizzata l-prestazzjoni tal-klassifikazzjoni. Aħna jevalwaw mudelli dwar klassifikazzjoni tat-test b’riżorsi sħa ħ u b’riżorsi baxxi jew żero ta’ aħbarijiet multilingwi u test bijomediku b’sett kbir ta’ tikketta. Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios.</abstract_mt>
      <abstract_pl>Neuralne modele klasyfikacji tekstu zazwyczaj traktują etykiety wyjściowe jako zmienne kategoryczne, które brakują opisu i semantyki. To zmusza ich parametryzację do uzależnienia od wielkości zestawu etykiet, a zatem nie są w stanie skalować do dużych zestawów etykiet i uogólniać na niewidoczne. Istniejące wspólne modele tekstowe wejścia-etykiety pokonują te problemy poprzez wykorzystanie opisów etykiet, ale nie są w stanie uchwycić złożonych relacji etykiet, mają sztywną parametryzację, a ich zyski na niewidocznych etykietach zdarzają się często kosztem słabej wydajności etykiet widocznych podczas szkolenia. W niniejszym artykule proponujemy nowy model wejściowo-etykietowy, który uogólnia względem poprzednich takich modeli, uwzględnia ich ograniczenia i nie narusza wydajności na widzianych etykietach. Model składa się ze wspólnego nieliniowego osadzenia etykiety wejściowej z kontrolowaną pojemnością i jednostki klasyfikacyjnej zależnej od przestrzeni złączonej, która jest trenowana z utratą entropii krzyżowej w celu optymalizacji wydajności klasyfikacji. Oceniamy modele klasyfikacji tekstów pełnych i niskich lub zerowych, wielojęzycznych wiadomości i tekstów biomedycznych z dużym zestawem etykiet. Nasz model przewyższa modele jednojęzyczne i wielojęzyczne, które nie wykorzystują semantyki etykiet i poprzednich wspólnych modeli przestrzeni wejścia-etykiety w obu scenariuszach.</abstract_pl>
      <abstract_si>න්‍යූරාල් පාළුවේ විශේෂණ මොඩල් සාමාන්‍යයෙන්ම ප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිපත්ත මේකෙන් ඔවුන්ගේ ප්‍රමාණිකරණය ලේබුල් සැකසුම් ප්‍රමාණයට අවශ්‍ය වෙන්න පුළුවන්, ඉතින්, ඔවුන් ලේබුල් සැට් වලට වැ Exist shared input-label text Model overnade this challenge by exploding Labels Description, but are inactive to Capture Compressed Labels සම්බන්ධතාවක්, have rigid Paratrizing, and the win on Unseen Labels is oft at the sum of weaker Perfection on the Labels seen in the Training. මේ පැත්තේ අපි අළුත් ඇතුළු ලේබල් මොඩේල් එකක් ප්‍රතිචාර කරනවා ඒ වගේ මොඩේල් වලින් සාමාන්‍ය වෙනුවෙන්, ඔවුන්ගේ සීමාවන මොඩල් එක්ක පාලනය කරන්න පුළුවන් ක්‍රියාත්මක සමග සම්බන්ධ නොලිනියර් ඇතුළු ලබේල් එක්ක සම්බන්ධ වෙන්න පුළුවන් සමග සම්බන්ධ වෙ අපි පූර්ණ- සම්පූර්ණ- සහ අඩු සම්පූර්ණ- සහ ශූර්ණ- සම්පූර්ණ- සම්පූර්ණ- සම්පූර්ණ- සම්පූ අපේ මෝඩල් එක භාෂාවක් සහ ගොඩක් භාෂාවක් මෝඩල් කරන්න පුළුවන් වෙනවා ඒ වගේම ලේබුල් සෙමැන්ටික් සහ කලින් සම්බන්ධ</abstract_si>
      <abstract_uz>Name Bu yerda ularning parametrlarini yorliqning oʻlchamidan ishlatishga ishonchini bajaradi, va shunday qilib ular katta yorliq satrlariga oʻtishi mumkin. Mavjud birinchi qoʻyish yozuv matn modellari yordamida bu muammolarni yozib olish mumkin, lekin ular murakkab yorliq bogʻlamalarini olib tashlab boʻlmaydi, yetarli parametriklashtirish mumkin, yetishmaydigan yorlarning amalni ko'rib chiqish vaqti yordamida ko'rib chiqish tugmasi yordamida yomon. Bu hujjatda, biz oldingi modellardan generalisi yangi input- yorliq modelini talab qilamiz, ularning chegaralarini boshqaradi, va koʻrsatilgan yorliqlarda bajarish imkoniyatini kamaytirilmaydi. Name Biz bir necha tillar news va biotikal matnning butun manbalar va nuqta manbalar matn darajasini qiymatimiz, katta yorliq yordamida. Bizning modelimiz bir necha tillar va bir xil modellarini bajaradi. Bu ikkita scenariosda hech qanday bir bir bir nechta qo'shish mumkin.</abstract_uz>
      <abstract_vi>Mẫu phân loại chữ thần kinh thường coi các nhãn xuất là các biến số không miêu tả và ngữ pháp. Điều này khiến cho thiết bị đo lường của chúng trở nên phụ thuộc vào kích thước của nhãn đặt, và do đó, chúng không thể quy mô với các thiết lập nhãn lớn và tổng hợp thành những cái chưa thấy. Các mô hình văn bản nhãn nhập chung hiện tại giải quyết những vấn đề này bằng cách khai thác các mô tả nhãn, nhưng chúng không thể nắm bắt các mối quan hệ nhãn hiệu phức tạp, thiết kế siêu âm cứng, và lợi nhuận trên nhãn vô hình thường xảy ra vì hiệu suất kém trên nhãn được nhìn thấy trong huấn luyện. Trong tờ giấy này, chúng tôi đề nghị một mô hình nhãn nhập mới tổng hợp hơn các mô hình trước đó, chấp nhận giới hạn của chúng, và không ảnh hưởng đến các nhãn đã thấy. The model consists of a joint non-linear nhập-markings embedding with control capacity and a join-space-dependence classification unit that is educated with cross-entropy loss to tối ưu hoá hoá hoá hoá hoá hoá hoá performance. Chúng tôi đánh giá các mô hình về các tài nguyên đầy đủ và loại văn bản nhỏ hay không chứa nhiều nguồn tin và văn bản sinh học với một bộ nhãn lớn. Trong cả hai kịch bản, mẫu của chúng tôi hoàn thiện ngôn ngữ và ngôn ngữ nhiều loại mà không tác động được ngữ pháp và biểu mẫu chung nhãn nội dung.</abstract_vi>
      <abstract_da>Neurale tekstklassifikationsmodeller behandler typisk outputetiketter som kategoriske variabler, der mangler beskrivelse og semantik. Dette tvinger deres parametrisering til at være afhængig af etiketsættets størrelse, og derfor er de ikke i stand til at skalere til store etiketsæt og generalisere til usynlige. Eksisterende fælles input-label tekstmodeller løser disse problemer ved at udnytte etiketbeskrivelser, men de er ikke i stand til at indfange komplekse etiketrelationer, har stiv parametrisering, og deres gevinster på usynlige etiketter sker ofte på bekostning af svag ydeevne på etiketterne set under træning. I denne artikel foreslår vi en ny input-label model, der generaliserer over tidligere sådanne modeller, adresserer deres begrænsninger og ikke kompromitterer ydeevnen på set etiketter. Modellen består af en fælles ikke-lineær input-label indlejring med kontrollerbar kapacitet og en fælles-rum-afhængig klassificeringsenhed, der er trænet med cross-entropi tab for at optimere klassificeringens ydeevne. Vi evaluerer modeller for fuld ressource og lav- eller nul-ressource tekst klassificering af flersprogede nyheder og biomedicinsk tekst med et stort labelsæt. Vores model overgår ensprogede og flersprogede modeller, der ikke udnytter etiketsemantik og tidligere fælles input-label rummodeller i begge scenarier.</abstract_da>
      <abstract_hr>Modeli klasifikacije neuronskog teksta obično tretiraju etikete izlaza kao kategorijske promjene koje nedostaju opis i semantike. To nudi da njihova parametrizacija zavisi od veličine određene etikete, i stoga oni ne mogu skalirati na velike sete etikete i generalizirati na nepoznate. Postoje zajednički modeli teksta oznake za ulazak prevladaju te probleme iskorištavanjem opisa oznake, ali oni ne mogu uhvatiti kompleksne odnose oznake, imaju krute parametrizacije, a njihovi dobiti na nevidljivim etiketama često se događaju na troškovi slabe učinke na etiketama viđenim tijekom obuke. U ovom papiru predlažemo novi model ulaznog etiketa koji generalizira preko prethodnih takvih model a, adresira njihove ograničenja i ne kompromisuje učinkovitost na viđenim etiketama. Model se sastoji od zajedničkog nelinearnog ulaznog etiketa ugrađenog s kontrolnom kapacitetom i zajedničkom klasifikacijskom jedinicom ovisnom o svemiru koja je obučena s gubitkom preko entropije kako bi optimizirala klasifikaciju. Procjenjujemo modele o klasifikaciji teksta punog resursa i niskog ili nulog resursa multijezičkih vijesti i biomedicinskog teksta s velikim oznakem. Naš model iznosi monojezičke i višejezičke modele koji ne utiču na etikete semantike i prethodne zajedničke svemirske modele u obje scenarije.</abstract_hr>
      <abstract_bg>Моделите за класификация на неврални текстове обикновено третират изходните етикети като категорични променливи, които нямат описание и семантика. Това принуждава параметризацията им да зависи от размера на набора етикети и следователно те не могат да мащабират до големи набори етикети и да обобщят до невидими такива. Съществуващите съвместни текстови модели за въвеждане и етикет преодоляват тези проблеми чрез използване на описания на етикети, но те не са в състояние да уловят сложни връзки на етикети, имат строга параметризация и печалбите им от невидими етикети често се случват за сметка на слабото представяне на етикетите, наблюдавани по време на обучението. В тази статия предлагаме нов модел на входно-етикет, който обобщава предишните модели, адресира техните ограничения и не компрометира производителността на видимите етикети. Моделът се състои от съединено нелинейно вграждане на входен етикет с контролируем капацитет и съединено-пространствено-зависима класификационна единица, която е обучена с загуба на кръстосана ентропия за оптимизиране на класификационната ефективност. Оценяваме модели за класификация на текста с пълен ресурс и нисък ресурс или нулев ресурс на многоезични новини и биомедицински текстове с голям набор етикети. Нашият модел превъзхожда едноезичните и многоезичните модели, които не използват семантиката на етикетите и предишните съвместни модели на входно-етикетно пространство и в двата сценария.</abstract_bg>
      <abstract_de>Neurale Textklassifizierungsmodelle behandeln Ausgabeetiketten typischerweise als kategoriale Variablen, denen es an Beschreibung und Semantik mangelt. Dies zwingt ihre Parametrisierung dazu, von der Größe der Beschriftungssätze abhängig zu sein, weshalb sie nicht in der Lage sind, auf große Beschriftungssätze zu skalieren und auf unsichtbare zu verallgemeinern. Bestehende gemeinsame Eingabe-Label-Textmodelle überwinden diese Probleme, indem sie Beschreibungen von Etiketten ausnutzen, aber sie sind nicht in der Lage, komplexe Beschriftungsbeziehungen zu erfassen, haben eine starre Parametrisierung, und ihre Gewinne bei unsichtbaren Etiketten gehen oft zu Lasten der schwachen Leistung der Etiketten, die während des Trainings beobachtet werden. In diesem Beitrag schlagen wir ein neues Input-Label-Modell vor, das sich gegenüber früheren Modellen verallgemeinert, deren Einschränkungen adressiert und die Leistung auf gesehenen Etiketten nicht beeinträchtigt. Das Modell besteht aus einer gemeinsamen nichtlinearen Input-Label-Einbettung mit steuerbarer Kapazität und einer joint-space-abhängigen Klassifizierungseinheit, die mit Cross-Entropieverlusten trainiert wird, um die Klassifizierungsleistung zu optimieren. Wir evaluieren Modelle zur vollständigen und ressourcenarmen Textklassifizierung von mehrsprachigen Nachrichten und biomedizinischen Texten mit einem großen Etikettenset. Unser Modell übertrifft in beiden Szenarien ein- und mehrsprachige Modelle, die keine Label-Semantik und frühere gemeinsame Eingabe-Label-Raummodelle nutzen.</abstract_de>
      <abstract_id>Model klasifikasi teks saraf biasanya memperlakukan label output sebagai variabel kategori yang kurang deskripsi dan semantik. Ini memaksa parametrisasi mereka untuk bergantung pada ukuran set label, dan, oleh itu, mereka tidak dapat skala ke set label besar dan generalisasi ke yang tidak terlihat. Model teks input-label kongsi yang ada mengatasi isu-isu ini dengan mengeksploitasi deskripsi label, tetapi mereka tidak dapat menangkap hubungan label kompleks, memiliki parametrisasi yang ketat, dan keuntungan mereka pada label yang tidak terlihat sering terjadi pada biaya prestasi lemah pada label yang terlihat selama latihan. Dalam kertas ini, kami mengusulkan model input-label baru yang menyebarkan lebih dari model sebelumnya, mengatasi batasan mereka, dan tidak merusak prestasi pada label yang terlihat. Model ini terdiri dari label input nonlinear kongsi yang memasukkan dengan kapasitas yang dapat dikendalikan dan unit klasifikasi bergantung pada ruang kongsi yang dilatih dengan kehilangan entropi salib untuk optimisasi prestasi klasifikasi. Kami mengevaluasi model pada sumber daya penuh dan klasifikasi teks sumber daya rendah atau nol dari berita multibahasa dan teks biomedis dengan set label besar. Model kita melebihi model monobahasa dan multibahasa yang tidak menggunakan label semantik dan model ruang input-label sebelumnya dalam kedua skenario.</abstract_id>
      <abstract_fa>مدل‌های ویژه‌بندی متن عصبی معمولاً برچسب‌های خروجی را به عنوان متغیر‌های گوناگونی درمان می‌کنند که توصیف و سیمانتیک کم دارند. این باعث می‌شود پارامتریزی آنها به اندازه تنظیم برچسب بستگی داشته باشند، و بنابراین آنها نمی‌توانند به مجموعه‌های بزرگ برچسب مقیاس کنند و به مجموعه‌های غیرقابل مشاهده گردند. مدل‌های متن‌نوشته‌ی ورودی مشترک با استفاده از توضیح‌های نقاشی، این مشکلات را با استفاده از توضیح‌های نقاشی تغییر می‌دهند، ولی آنها نمی‌توانند رابطه‌های نقاشی پیچیده را بگیرند، پارامتریزی سخت داشته باشند، و پیروزی‌هایشان بر نقاشی‌های نابینا ا در این کاغذ، ما یک مدل جدیدی از نقاشی ورودی را پیشنهاد می کنیم که در این مدل های قبلی ژنرالیز می کند، به محدودیت هایشان نشان می دهد، و عملکرد روی نقاشی دیده را تغییر نمی دهد. Model consists of a joint non-linear input-label embedded with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance. ما مدل‌ها را در کلی منابع کامل و تنظیم متن کم یا صفر منابع از خبرهای زیادی زبان و متن بیوپزشکی با یک مجموعه برچسب بزرگ ارزیابی می‌کنیم. مدل ما مدل های یک زبان و چندین زبان را اجرا می کند که مدل های فضایی که در هر دو سناریو نقاشی از نقاشی و نقاشی که با نقاشی از نقاشی و نقاشی که با نقاشی از نقاشی به نقاشی وارد می شود در هر دو سن</abstract_fa>
      <abstract_nl>Neurale tekstclassificatiemodellen behandelen uitvoerlabels meestal als categorische variabelen zonder beschrijving en semantiek. Dit dwingt hun parametrisering afhankelijk te zijn van de grootte van de labelset, en daarom zijn ze niet in staat om te schalen naar grote labelsets en te generaliseren naar ongeziene. Bestaande gezamenlijke invoer-label tekstmodellen overwinnen deze problemen door gebruik te maken van labelbeschrijvingen, maar ze zijn niet in staat om complexe labelrelaties vast te leggen, hebben een rigide parametrisering en hun winsten op onzichtbare labels gebeuren vaak ten koste van zwakke prestaties op de labels die tijdens de training worden gezien. In dit artikel stellen we een nieuw input-label model voor dat generaliseert ten opzichte van eerdere dergelijke modellen, hun beperkingen aanpakt en geen afbreuk doet aan de prestaties op zichtbare labels. Het model bestaat uit een gezamenlijke niet-lineaire input-label embedding met controleerbare capaciteit en een joint-space-afhankelijke classificatieeenheid die is getraind met cross-entropieverlies om classificatieprestaties te optimaliseren. We evalueren modellen op full-resource en low- of zero-resource tekstclassificatie van meertalig nieuws en biomedische tekst met een grote labelset. Ons model presteert beter dan eentalige en meertalige modellen die geen gebruik maken van labelsemantiek en eerdere gezamenlijke input-label ruimtemodellen in beide scenario's.</abstract_nl>
      <abstract_tr>Nöral metin klasifikasyon modelleri genelde çıqtı etiketleri kategorik çarpmalar olarak hasaplar ve semantik yoktur. Bu onların parametriýasyny etiket düzümlerniň ululykna baglanmagy mümkin edýär we bu sebäpli olar uly etiket düzümlerine gollaşdyryp bilmeýär we ony janlaşdyryp bilmeýär. Existen birleşik girdi-etiket metin nusgalary etiket deskriplerini ulanyp bu meseleleri üstüne geçirip bilmeýär emma olar karmaşık etiket baglaýyşlaryny yakalamak başarmaýarlar, hakyky parametrizaýarlar bar we olaryň gaýd edilmegi kän etiketlerde görünýän etiketleriň hasaplamasynda kän bir şekilde başarýarlar. Bu kagyzda, täze bir girdi etiket modelini öňki nusgalaryň üstünde döredilen nusgalary barlaýar we görkezilen etiketlerde etkinleşen täze bir nusga teklip edýäris. Bu nusga kontrol edilebilir kapasitet bilen birleşik gabdaly gaýd etmek üçin çizgi gabdaly gaýd etmek üçin birleşik gabdaly gaýd etmek üçin birleşik gabdaly gaýd etmek üçin birleşik gabdaly etiketlerdir. Biz nusgalary tam-resop we düşük-ýa-da 0-resop metin klasifikasynda çykýarys Biziň nusgamyz hem dilli hem köp dilli nusgalary çykarýar.</abstract_tr>
      <abstract_af>Nurale teks klasifikasie modele tipies behandel uitset etikette as kategoriese veranderlikes wat ontbreek beskrywing en semantieke. Hierdie verkrag hulle parametrisasie om afhanklik te wees van die etiket stel grootte, en daarom, hulle is nie moontlik na skaal na groot etiket stel en generelliseer na onversekende. Bestaande joint invoer- label teks modelle oorwin hierdie probleme deur die uitbreiding van etiket beskrywings, maar hulle is nie moontlik om kompleks etiket verhoudings te vang, het rigte parametrisasie, en hulle verskaffings op ongesiende etikette gebeur dikwels op die koste van swak prestasie op die etikette gesien tydens onderriging. In hierdie papier, voorstel ons 'n nuwe invoer- etiket model wat generaliseer oor vorige sodanige modele, adresse hul beperkings, en nie kompromiseer prestasie op gesien etikette nie. Die model bestaan van 'n gemeenskap onlineêre invoer-etiket inbêer met kontroleer kapasiteit en 'n gemeenskap-spasie-afhanklike klasifikasie eenheid wat onderwerp word met kruis-entropie verlies om klasifikasie-prestasie te optimaliseer. Ons evalueer modele op volle- hulpbron en lae- of nul- hulpbron teks klasifikasie van multitaalske nuus en biomediese teks met 'n groot etiket stel. Ons model uitvoer monotale en multitaalse modele wat nie etiket semantiek en vorige joint invoer-etiket spasiemodele in beide scenarios verwyder nie.</abstract_af>
      <abstract_ko>신경 텍스트 분류 모델은 일반적으로 출력 라벨을 설명과 의미가 부족한 분류 변수로 본다.이 때문에 매개 변수화는 탭 집합의 크기에 의존하기 때문에 대형 탭 집합으로 확장할 수도 없고, 보이지 않는 탭 집합으로 확대할 수도 없다.기존의 연합 입력 라벨 텍스트 모델은 라벨 묘사를 이용하여 이러한 문제점을 극복했지만 복잡한 라벨 관계를 포착하지 못하고 엄격한 매개 변수화를 가지며 보이지 않는 라벨에 대한 수익은 훈련 기간에 보이는 라벨에 좋지 않은 것을 대가로 한다.본고에서 우리는 새로운 입력 라벨 모델을 제시했다. 이 모델은 이전의 입력 라벨 모델을 보급하고 그들의 한계를 해결했으며 SEED 라벨의 성능에 영향을 주지 않았다.이 모델은 용량을 제어할 수 있는 비선형 연합 입력 라벨 삽입과 공간과 관련된 연합 분류 단원으로 구성되어 있으며 이 단원은 교차 엔트로피 손실을 통해 분류 성능을 최적화하기 위해 훈련을 실시한다.우리는 큰 라벨집을 가진 다국어 뉴스와 생물의학 텍스트의 전체 자원과 낮은 자원 또는 제로 자원 텍스트 분류 모델을 평가했다.이 두 가지 상황에서 우리의 모델은 모두 라벨의 의미를 이용하지 않고 이전의 연합 입력 라벨 공간 모델의 단일 언어와 다중 언어 모델보다 우수하다.</abstract_ko>
      <abstract_sw>Mfano wa usambazaji wa maandishi ya kijasiri mara nyingi hutumia alama za utoaji kama mabadiliko ya kigezo ambazo hazina maelezo na semantika. Hii inawalazimisha ubaguzi wao kutegemea ukubwa wa alama, na kwa hiyo, hawana uwezo wa kuelekea kwenye seti kubwa za alama na kuzuia vifaa vinavyofichikana. Mradi wa maandishi ya viungo vya pamoja unashinda suala hili kwa kutumia maelezo ya alama, lakini hawana uwezo wa kukabiliana na mahusiano magumu, wanachambua vibaya, na mafanikio yao kwenye mabango yasiyofahamika mara nyingi yanatokea kwa gharama kubwa ya utendaji wa mabango yanayoonekana wakati wa mafunzo. Katika karatasi hii, tunapendekeza modeli mpya ya input inayotengeneza zaidi ya mifano kama ilivyopita, hujadili vizuizi vyao, na haijapunguza utendaji wa tabia zilizotazama. The model consists of a joint nonlinear input-label embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance.  Tunatathmini mifano ya rasilimali kamili na usambazaji wa maandishi yenye rasilimali ya chini au sifuri ya habari za lugha mbalimbali na ujumbe wa kitabibu kwa seti kubwa. Mfano wetu unaonyesha mifano ya lugha na lugha mbalimbali ambazo hazina mifano ya kiungo na mifano ya anga za kwanza za katika maeneo yote.</abstract_sw>
      <abstract_az>Nöral metin klasifikasiya modelləri genellikle çıxış etiketlərini tanımlamaq və semantik olmayan kategorikalı dəyişikliklər kimi təhrif edirlər. Bu onların parametrizaqlarını etiket təyin edilmiş böyüklüyündən bağlı olmağa məcbur edir. Buna görə də onlar böyük etiket qurularına dəyişə bilməzlər və görmədikləri etiket qurularına genel dəyişə bilməzlər. Müxtəlif girdi etiketli mətn modelleri etiketli tərzlərini istifadə edərək bu məsələləri üstün edir, amma onlar kompleks etiketli ilişkiləri yakala bilməzlər, qüvvətli parametrizaqları var, və görünməyən etiketlərdə onların qazanışları çox dəfə təhsil etdikdə görünən etiketlərdə zəif performans hesabında olur. Bu kağızda, əvvəlki modellərdə generalizasyon edən yeni girdi etiketli modeli təklif edirik, onların sınırlarını təklif edir və görünülmüş etiketlərdə performansını təklif etməz. Model kontrol edilebilir qabiliyyəti ilə birləşdirilmiş çizgi girdi etiketi və birləşdirilmiş kosmos-bağlı klasifikasiya birimi ilə birləşdirilmiş çoxlu entropi kaybı ilə təhsil edilmiş klasifikasiya performansını optimizləmək üçün təhsil edilən bir nöqtədir. Biz çoxlu dil xəbərlərin və biomedicin mətnlərin tamamlanması və düşük-ya-sıfır-ressurs mətnlərin klasifikasiyasını çəkirik. Bizim modellərimiz hər iki scenarioda etiketli semantik və əvvəlki istifadə etiketi kosmosu modellərini istifadə etməyən monodil və çoxlu dil modellərini göstərir.</abstract_az>
      <abstract_sq>Modelet e klasifikimit të tekstit nervor tipikisht trajtojnë etiketat e daljes si ndryshuesit kategorikë që mungon përshkrimi dhe semantikë. Kjo detyron parametrizimin e tyre të varet nga madhësia e caktuar e etiketës, dhe kështu, ata nuk janë në gjendje të shkallojnë në set të mëdha të etiketës dhe të gjeneralizojnë në të padukshmet. Modelet ekzistuese të përbashkëta të tekstit të etiketave të hyrjes i kapërcejnë këto çështje duke shfrytëzuar përshkrimet e etiketave, por ato nuk janë në gjendje të kapin marrëdhënie komplekse të etiketave, kanë parametrizim të ashpër dhe fitimet e tyre në etiketat e padukshme ndodhin shpesh në dëm të performancës së dobët në etiketat e parë gjatë trajnimit. Në këtë letër, ne propozojmë një model të ri të etiketës së hyrjes që gjeneralizohet mbi modelet e mëparshëm të tilla, trajton kufizimet e tyre dhe nuk kompromiton performancën në etiketat e parë. Modeli përbëhet nga një etiketë e përbashkët jo-lineare të hyrjes me kapacitet të kontrollueshëm dhe një njësi klasifikimi të përbashkët-të varur nga hapësira që është trajnuar me humbje ndër-entropi për të optimizuar performancën e klasifikimit. We evaluate models on full-resource and low- or zero-resource text classification of multilingual news and biomedical text with a large label set.  Modeli ynë paraqet modelet monogjuhësore dhe shumëgjuhësore që nuk përdorin etiketën semantike dhe modelet e mëparshme të përbashkëta të hyrjes-etiketës hapësirore në të dy skenarët.</abstract_sq>
      <abstract_am>የኩነቶች የጽሑፍ መክፈቻ ሞዴሎች በተለየ የውጤት ምልክቶችን እንደ ክፍተር መለያየት እና መግለጫ የላቸውም ይህ የፋይል ምርጫዎች በlabel መጠን ላይ እንዲታመሙ ይችላል፡፡ የአሁኑ የበይነመረብ-የጽሑፍ ምሳሌዎች እነዚህ ጉዳዮችን በመጠቀም የበረራ ጽሑፎችን አሸንፋሉ፥ ነገር ግን በተጨማሪው የጽሑፍ ግንኙነት መያዝ አይችሉም፣ ጥሩ ማረፊያ አለባቸው፡፡ በዚህ ፕሮግራም፣ የቀድሞው እንደዚህ ዓይነቶች በተለየ አዲስ የinput-label model እናሳልጋለን፣ የግንኙነታቸውን አድራጊ እናደርጋለን፡፡ The model consists of a joint nonlinear input-label containing control capability and a joint-space-dependent classification unit that is trained by cross-entropy loss to optimize classification performance. በሙሉ resource እና በ0-resource ጽሑፍ ጽሑፍ ክፍተቶችን በብዙ ቋንቋዎች ዜና እና የbiomedical ጽሑፍ በታላቅ label እናስመስክራለን፡፡ ሞዴሌያችን በሁለቱ ስናናይኖች ውስጥ የደረጃ ማህበረሰብ እና የብዙ ቋንቋዎች ዓይነቶችን የሚያደርጉ ናቸው፡፡</abstract_am>
      <abstract_bs>Modeli klasifikacije neuronskog teksta obično tretiraju etikete izlaza kao kategorijske varijante koje nedostaju opis i semantike. To primorava njihovu parametrizaciju da zavise od veličine postavljene etikete, i zato oni ne mogu skalirati na velike sete etikete i generalizirati na nevidljive. Postoje zajednički modeli teksta oznake za ulazak prevladaju te probleme iskorištavanjem opisa oznake, ali nisu u mogućnosti da uhvate kompleksne odnose oznake, imaju krute parametrizacije, a njihovi dobiti na nevidljivim etiketama se često dešavaju na troškovi slabe učinke na etiketama viđenim tijekom obuke. U ovom papiru predlažemo novi model ulaznog etiketa koji generalizuje preko prethodnih takvih model a, adresuje njihove ograničenja i ne kompromisuje učinkovitost na viđenim etiketama. Model se sastoji od zajedničkog nelinearnog ulaznog etiketa ugrađenog sa kontrolnom kapacitetom i zajedničkom klasifikacijskom jedinicom ovisnom o svemiru koja je obučena s gubitkom prekretropije kako bi optimizirala klasifikaciju. Procjenjujemo modele o klasifikaciji teksta punog resursa i niskog ili nulog resursa multijezičkih vijesti i biomedicinskog teksta sa velikim namještajem etiketa. Naš model iznosi monojezičke i multijezičke modele koji ne utiču na etikete semantike i prethodne zajedničke svemirske modele za ulazak u obje scenarije.</abstract_bs>
      <abstract_hy>Neural text classification models typically treat output labels as categorical variables that lack description and semantics.  Սա ստիպում է նրանց պարամետրիզացիան կախված լինել պիտակի սահմանափակումների չափից, և դրանք չեն կարողանում մեծացնել մեծ պիտակի սահմանափակումների և ընդհանուր ընդհանուր ընդհանուր ընդհանուր ընդհանուր ըն Գոյություն ունի միասին ներմուծման-պիտակի տեքստի մոդելներ, որոնք հաղթահարում են այս խնդիրները օգտագործելով պիտակի նկարագրությունները, բայց նրանք չեն կարողանում պատկերացնել բարդ պիտակի հարաբերությունները, ունեն խիստ պարամետրիզացիա, և դրանց շահույթը անտեսանելի պիտակների վրա հաճախ տեղի Այս թղթի մեջ մենք առաջարկում ենք նոր մուտք-պիտակ մոդել, որը ընդհանրացնում է նախորդ մոդելների վերաբերյալ, վերաբերում է նրանց սահմանափակումներին և չի վնասում տեսանելի պիտակների արդյունքը: Մոդելը կազմված է միասին ոչ գծային մուտքագրման պիտակից, որը ներառված է կառավարելի հնարավորության հետ և միասին տարածության կախված դասակարգման միավորից, որը վարժեցվում է խաչը-էնտրոպիայի կորստով որպեսզի օպտիմացվի դասակար Մենք գնահատում ենք բազմալեզու նորությունների և կենսաբժշկական տեքստի ամբողջ ռեսուրսների և ցածր կամ զրոյից ռեսուրսների դասակարգումների մոդելները մեծ պիտակում: Մեր մոդելը արտադրում է միալեզու և բազլեզու մոդելներ, որոնք չեն օգտագործում սեմանտիկայի պիտակները և նախորդ միավոր մուտք-պիտակ տիեզերական մոդելները երկու սցենարներում:</abstract_hy>
      <abstract_cs>Neurální klasifikační modely textu obvykle považují výstupní štítky za kategorické proměnné, které postrádají popis a sémantiku. To nutí jejich parametrizaci záviset na velikosti sady štítků, a proto nejsou schopny škálovat na velké sady štítků a zobecnit na neviditelné. Stávající společné textové modely vstupu-štítku tyto problémy překonávají využitím popisů štítků, ale nejsou schopny zachytit složité vztahy štítků, mají pevnou parametrizaci a jejich zisky u neviditelných štítků docházejí často na úkor slabého výkonu štítků viděných během školení. V tomto článku navrhujeme nový model vstupních štítků, který generalizuje předchozí takové modely, řeší jejich omezení a neohrožuje výkon viděných štítků. Model se skládá ze společného nelineárního vložení vstupních štítků s kontrolovatelnou kapacitou a klasifikační jednotky závislé na spojovém prostoru, která je trénována se ztrátou křížové entropie pro optimalizaci klasifikačního výkonu. Hodnotíme modely pro klasifikaci vícejazyčných zpráv a biomedicínského textu s velkou sadou etiket. Náš model překonává monojazyčné a vícejazyčné modely, které nevyužívají sémantiku štítků a předchozí společné modely vstupu-štítků prostoru v obou scénářích.</abstract_cs>
      <abstract_et>Närviteksti klassifitseerimise mudelid käsitlevad väljundsilte tavaliselt kategoorialiste muutujatena, millel puudub kirjeldus ja semantika. See sunnib nende parametriseerimist sõltuma sildikomplekti suurusest ja seetõttu ei saa nad suurte sildikomplektideni skaleerida ja üldistada nähtamatuteks. Olemasolevad ühised sisend-sildi tekstimudelid lahendavad need probleemid sildi kirjelduste kasutamisega, kuid nad ei suuda jäädvustada keerulisi sildi seoseid, neil on jäik parametreerimine ja nende kasu nähtamatute sildide kasutamisel toimub sageli koolituse käigus nähtud sildide nõrga jõudluse arvelt. Käesolevas töös pakume välja uue sisendmärgise mudeli, mis üldistab varasemaid selliseid mudeleid, käsitleb nende piiranguid ja ei ohusta nähtud märgiste jõudlust. Mudel koosneb juhitava võimsusega liigesest mittelineaarsest sisendmärgist ja liigesest-ruumist sõltuvast klassifikatsiooniüksusest, mis on koolitatud ristentroopiakao optimeerimiseks klassifitseerimisjõudluse optimeerimiseks. Hindame mitmekeelsete uudiste ja biomeditsiiniliste tekstide täisressurssiga ja vähese või nulliressurssiga klassifitseerimise mudeleid suure märgistuskomplektiga. Meie mudel ületab ühe- ja mitmekeelseid mudeleid, mis ei kasuta sildi semantikat ega varasemaid ühiseid sisend-sildi ruumimudeleid mõlemas stsenaariumis.</abstract_et>
      <abstract_fi>Neurotekstiluokitusmallit käsittelevät tulosmerkintöjä tyypillisesti kategorisina muuttujina, joilla ei ole kuvausta ja semantiikkaa. Tämä pakottaa niiden parametrisoinnin riippumaan etikettijoukon koosta, joten ne eivät pysty skaalaamaan suuriin etikettiryhmiin ja yleistymään näkymättömiin. Nykyiset yhteiset syöttö- ja tarratekstimallit voittavat nämä ongelmat hyödyntämällä tarrakuvauksia, mutta ne eivät kykene kuvaamaan monimutkaisia tarrasuhteita, niiden parametrisointi on jäykkä, ja niiden voitot näkymättömistä etiketeistä tapahtuvat usein koulutuksen aikana havaittujen etikettien heikon suorituskyvyn kustannuksella. Tässä työssä ehdotamme uutta input-label -mallia, joka yleistyy aikaisempiin malleihin verrattuna, käsittelee niiden rajoituksia eikä vaaranna näytettyjen labelien suorituskykyä. Malli koostuu yhdistetystä epälineaarisesta syöttöetiketistä, jolla on hallittavissa oleva kapasiteetti, sekä liitos-avaruudesta riippuvasta luokitusyksiköstä, joka on koulutettu ristientropiahäviöllä luokittelun optimoimiseksi. Arvioimme monikielisten uutisten ja biolääketieteellisten tekstien täysressurssien ja vähäresurssisten tekstien luokittelumalleja suurella etikettisarjalla. Mallimme toimii paremmin kuin yksikieliset ja monikieliset mallit, jotka eivät hyödynnä etiketin semantiikkaa ja aiempia yhteisiä syöte-etiketti-tilamalleja molemmissa skenaarioissa.</abstract_fi>
      <abstract_bn>নিউরেল টেক্সট ক্লাস্ফিকেশন মডেল সাধারণত আউটপুট লেবেল বিভাগের বিভাগ হিসেবে ব্যবহার করুন, যা বর্ণনা এবং সেমেন এটি তাদের প্যারামিটারিজেশন লেবেল সেটের আকারের উপর নির্ভর করতে বাধ্য করে, আর এর ফলে তারা বিশাল ল লেবেলেট সেটের দিকে আকারে যাত্রা করতে  বিদ্যমান যুক্ত ইনপুট-লেবেলের টেক্সট মডেল লেবেল বিবরণ ব্যবহারের মাধ্যমে এই বিষয়গুলোকে বিজয়ী করেছে, কিন্তু তারা কমপ্লেক্স লেবেলেটের সম্পর্ক গ্রহণ করতে পারে না, তাদের প এই কাগজটিতে আমরা একটি নতুন ইনপুট-লেবেল মডেল প্রস্তাব করি যা পূর্বের এই মডেলের উপর জেনারেল করে, তাদের সীমাবদ্ধতা ঠিকানা করে এবং তাদের লেব মোডেলের মধ্যে একটি যৌথ অলাইনিয়ার ইনপুট- লেবেলের মধ্যে রয়েছে যা নিয়ন্ত্রণের ক্ষমতা এবং একটি যৌথ-স্পেস-নির্ভরিত ক্লাস্ফিকেশন ইউনিট যা ক্লাসাফিকেশনের আমরা পুরোপুরি সম্পদ এবং নিম্নলিখিত কিংবা শুধুমাত্র বিশাল লেবেলেটের সংবাদ এবং বায়োমেডিকেল টেক্সটের মূল্যায়ন করি। আমাদের মডেল মোনোলিভাল এবং বহুভাষায় মডেল প্রদর্শন করে যারা দুটো সিনেম্যান্টিক এবং পূর্ববর্তী যোগাযোগ ইনপুট-লেবেল স্থানের</abstract_bn>
      <abstract_ca>Neural text classification models typically treat output labels as categorical variables that lack description and semantics.  Això obliga la seva parametrització a dependre de la mida del conjunt d'etiquetes, i, per tant, no són capaços d'escalar a grans conjunts d'etiquetes i generalitzar-se a aquells invisibles. Els models de text conjunts d'etiquetes d'entrada superen aquests problemes explotant descripcions d'etiquetes, però no són capaços de capturar relacions complexes d'etiquetes, tenen parametrització rígida, i els seus guanys en etiquetes invisibles aconsegueixen sovint a càrrega del frac rendiment de les etiquetes vistes durant l'entrenament. En aquest paper, proposem un nou model d'etiqueta d'entrada que s'generalitza sobre els models anteriors, aborda les seves limitacions i no compromet el rendiment de les etiquetes vistes. El model consisteix en una etiqueta d'entrada no linear conjunta incorporada amb capacitat controlable i una unitat de classificació dependent de l'espai conjunt que està entrenada amb pèrdua de transentropia per optimitzar el rendiment de la classificació. Evaluam models de classificació de text amb recursos complets i amb recursos baixos o zero de notícies multilingües i text biomèdic amb un gran conjunt d'etiquetes. El nostre model supera els models monolingües i multilingües que no utilitzen l'etiqueta semàntica i els models espacials anteriors d'entrada conjunta en ambdós escenaris.</abstract_ca>
      <abstract_ha>@ label: listbox Wannan na ƙara parameteriyarsu don a ƙayyade girmar label, kuma bã zã su iya iya fito zuwa matsayin ayuka babba, kuma su samu gaɓõya masu ɓõya. @ label: listbox In this paper, we propose a new input-label model that generalizes over previous such models, addresses their limitations, and does not compromise performance on seen labels.  @ info: whatsthis Tuna ƙaddara misãlai kan fasalin matsayin cikakken resource da kasa- ko-nufi, wa'anar lãbãri na mulki-mulki da littãfin da aka daidaita wani label mai girma. MisalinMu na samar da misãlai masu motsi na monoli da mulki-lingui, waɗanda bã su samar wa label semantiki kuma misãlai na farko da ke cikin cikin tsari biyu.</abstract_ha>
      <abstract_he>דוגמנים מסווג טקסט נוירוני בדרך כלל מתייחסים לתוויות יציאה כמשתנים קטגוריים שאין להם תיאור וסמנטיקה. זה מכריח את הפרמטריזציה שלהם להיות תלוי בגודל התווית, ולכן, הם לא מסוגלים להגדיל לתוויות גדולות ולהגדיל לתוויות בלתי נראות. מודלים טקסטים משותפים קיומים של תווית כניסה מתגברים על הנושאים האלה על ידי ניצל תיאורים של תוויות, אבל הם לא יכולים לתפוס מערכות יחסים מסובכות של תוויות, יש פרמטריזציה קשה, והרווחות שלהם על תוויות בלתי נראות קורות לעתים קרובות על חשבון ביצועים חלשים על תוויות שנראות בעיתון הזה, אנו מציעים מודל חדש של תווית כניסה שמתפתח על מודלים כאלה קודמים, מתייחס למגבלותיהם, ולא מפחיד ביצועים על תוויות ראויות. המודל כולל תווית כניסה לא לינרית משותפת עם יכולת שליטה ויחידת קליפורניה משותפת-תלויה בחלל שאומנת עם אובדן אנטרופיה צלב כדי לאופטיזם ביצועי קליפורניה. אנו מעריכים מודלים על מסווג טקסט מלא משאבים נמוכים או אפס של חדשות רבות שפות וטקסט ביומדיקלי עם תווית גדולה. המודל שלנו מוביל מודלים מונושפות ומרבות שפות שלא משתמשים בתווית סמנטיקה ומודלים חלליים משותפים קודמים בשני התקרים.</abstract_he>
      <abstract_sk>Modeli razvrščanja živčnih besedil običajno obravnavajo izhodne oznake kot kategorične spremenljivke, ki nimajo opisa in semantike. To prisili, da je njihova parametrizacija odvisna od velikosti nabora oznak, zato jih ni mogoče razširiti na velike nabore oznak in posplošiti na nevidne. Obstoječi skupni vhodni besedilni modeli premagajo te težave z izkoriščanjem opisov oznak, vendar pa ne morejo zajeti kompleksnih odnosov oznak, imajo togo parametrizacijo, njihove koristi pri nevidnih oznakah pa se pogosto zgodijo na račun slabe zmogljivosti oznak, opaženih med usposabljanjem. V prispevku predlagamo nov model vhodne oznake, ki se posploši nad prejšnjimi modeli, obravnava njihove omejitve in ne ogroža zmogljivosti na videnih oznakah. Model je sestavljen iz skupne nelinearne vhodne oznake s krmilljivo zmogljivostjo in skupno-prostorsko odvisne klasifikacijske enote, ki je usposobljena z izgubo navzkrižne entropije za optimizacijo klasifikacijske učinkovitosti. Z velikim naborom oznak ocenjujemo modele za klasifikacijo besedila s polnimi viri in nizkimi ali ničelnimi viri večjezičnih novic in biomedicinskih besedil. Naš model presega enojezične in večjezične modele, ki v obeh scenarijih ne izkoriščajo semantike oznak in prejšnjih skupnih modelov prostora vhodnih oznak.</abstract_sk>
      <abstract_jv>email-custom-header-Security First letter Joint input-label text model label Genjer Awak dhéwé éntuk model ning komplit-recurs lan basa-perusahaan kelas-perusahaan seneng basa multilanggar lan seneng biasak bantuan kanggo nambah label sing gak dhéwé. modelo sing wis nambah, dadine languangkat lan akeh multilenguang modelo sing ora nggawe etiket sematik lan banjure nggawe modelo input-label space nang saboh sanes.</abstract_jv>
      <abstract_bo>Neural text classification models typically treat output labels as categorical variables that lack description and semantics. This forces their parametrization to be dependent on the label set size, and, hence, they are unable to scale to large label sets and generalize to unseen ones. Existing joint input-label text models overcome these issues by exploiting label descriptions, but they are unable to capture complex label relationships, have rigid parametrization, and their gains on unseen labels happen often at the expense of weak performance on the labels seen during training. In this paper, we propose a new input-label model that generalizes over previous such models, addresses their limitations, and does not compromise performance on seen labels. The model consists of a joint nonlinear input-label embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance. ང་ཚོས་རྣམ་པ་མང་པོ་དང་མཐོ་རྣམ་པ་མང་པོ་ཞིག་ཡོད་པའི་མིག་གཟུགས་རིས་མང་པོ་ཞིག་དང་། རྣམ་པ་མིག་གཟུགས་རིས Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios.</abstract_bo>
      </paper>
    <paper id="10">
      <title>Autosegmental Input Strictly Local Functions</title>
      <author><first>Jane</first><last>Chandlee</last></author>
      <author><first>Adam</first><last>Jardine</last></author>
      <doi>10.1162/tacl_a_00260</doi>
      <abstract>Autosegmental representations (ARs ; Goldsmith, 1976) are claimed to enable local analyses of otherwise non-local phenomena Odden (1994). Focusing on the domain of tone, we investigate this ability of ARs using a computationally well-defined notion of <a href="https://en.wikipedia.org/wiki/Locality_of_reference">locality</a> extended from Chandlee (2014). The result is a more nuanced understanding of the way in which ARs interact with <a href="https://en.wikipedia.org/wiki/Locality_(linguistics)">phonological locality</a>.</abstract>
      <pages>157–168</pages>
      <url hash="b3f1bc1c">Q19-1010</url>
      <bibkey>chandlee-jardine-2019-autosegmental</bibkey>
    </paper>
    <paper id="11">
      <title>SECTOR : A Neural Model for Coherent Topic Segmentation and Classification<fixed-case>SECTOR</fixed-case>: A Neural Model for Coherent Topic Segmentation and Classification</title>
      <author><first>Sebastian</first><last>Arnold</last></author>
      <author><first>Rudolf</first><last>Schneider</last></author>
      <author><first>Philippe</first><last>Cudré-Mauroux</last></author>
      <author><first>Felix A.</first><last>Gers</last></author>
      <author><first>Alexander</first><last>Löser</last></author>
      <doi>10.1162/tacl_a_00261</doi>
      <abstract>When searching for information, a human reader first glances over a document, spots relevant sections, and then focuses on a few sentences for resolving her intention. However, the high variance of document structure complicates the identification of the salient topic of a given section at a glance. To tackle this challenge, we present SECTOR, a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section. Our deep neural network architecture learns a latent topic embedding over the course of a document. This can be leveraged to classify local topics from plain text and segment a document at topic shifts. In addition, we contribute WikiSection, a publicly available data set with 242k labeled sections in <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/German_language">German</a> from two distinct domains : <a href="https://en.wikipedia.org/wiki/Disease">diseases</a> and cities. From our extensive evaluation of 20 architectures, we report a highest score of 71.6 % F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter embeddings</a> and bidirectional segmentation. This is a significant improvement of 29.5 points F1 over state-of-the-art CNN classifiers with baseline segmentation.</abstract>
      <pages>169–184</pages>
      <url hash="3b77c57d">Q19-1011</url>
      <video href="https://vimeo.com/384478902" />
      <attachment type="presentation" hash="e0aa7535">Q19-1011.Presentation.pdf</attachment>
      <bibkey>arnold-etal-2019-sector</bibkey>
      <pwccode url="https://github.com/sebastianarnold/SECTOR" additional="true">sebastianarnold/SECTOR</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisection">WikiSection</pwcdataset>
    </paper>
    <paper id="12">
      <title>Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs</title>
      <author><first>Amrita</first><last>Saha</last></author>
      <author><first>Ghulam Ahmed</first><last>Ansari</last></author>
      <author><first>Abhishek</first><last>Laddha</last></author>
      <author><first>Karthik</first><last>Sankaranarayanan</last></author>
      <author><first>Soumen</first><last>Chakrabarti</last></author>
      <doi>10.1162/tacl_a_00262</doi>
      <abstract>Recent years have seen increasingly complex question-answering on knowledge bases (KBQA) involving logical, quantitative, and comparative reasoning over KB subgraphs. Neural Program Induction (NPI) is a pragmatic approach toward modularizing the <a href="https://en.wikipedia.org/wiki/Automated_reasoning">reasoning process</a> by translating a complex <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language query</a> into a multi-step executable program. While NPI has been commonly trained with the ‘‘gold’’ program or its sketch, for realistic KBQA applications such gold programs are expensive to obtain. There, practically only <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language queries</a> and the corresponding answers can be provided for training. The resulting combinatorial explosion in program space, along with extremely sparse rewards, makes <a href="https://en.wikipedia.org/wiki/Non-blocking_algorithm">NPI</a> for KBQA ambitious and challenging. We present Complex Imperative Program Induction from Terminal Rewards (CIPITR), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, KB schema, and inferred answer type. CIPITR solves complex KBQA considerably more accurately than key-value memory networks and neural symbolic machines (NSM). For moderately complex queries requiring 2- to 5-step programs, CIPITR scores at least 3 higher F1 than the competing systems. On one of the hardest class of programs (comparative reasoning) with 510 steps, CIPITR outperforms NSM by a factor of 89 and memory networks by 9 times.</abstract>
      <pages>185–200</pages>
      <url hash="3cd83ac9">Q19-1012</url>
      <bibkey>saha-etal-2019-complex</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/csqa">CSQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestionssp">WebQuestionsSP</pwcdataset>
    </paper>
    <paper id="14">
      <title>DREAM : A Challenge Data Set and Models for Dialogue-Based Reading Comprehension<fixed-case>DREAM</fixed-case>: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension</title>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Jianshu</first><last>Chen</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <doi>10.1162/tacl_a_00264</doi>
      <abstract>We present <a href="https://en.wikipedia.org/wiki/DREAM">DREAM</a>, the first dialogue-based multiple-choice reading comprehension data set. Collected from English as a Foreign Language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our <a href="https://en.wikipedia.org/wiki/Data_set">data set</a> contains 10,197 <a href="https://en.wikipedia.org/wiki/Multiple_choice">multiple-choice questions</a> for 6,444 dialogues. In contrast to existing reading comprehension data sets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding. DREAM is likely to present significant challenges for existing reading comprehension systems : 84 % of answers are non-extractive, 85 % of questions require reasoning beyond a single sentence, and 34 % of questions also involve commonsense knowledge. We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the DREAM data set show the effectiveness of dialogue structure and <a href="https://en.wikipedia.org/wiki/General_knowledge">general world knowledge</a>. DREAM is available at https://dataset.org/dream/.</abstract>
      <pages>217–231</pages>
      <url hash="aee4406b">Q19-1014</url>
      <bibkey>sun-etal-2019-dream</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
    </paper>
    <paper id="22">
      <title>Syntax-aware Semantic Role Labeling without Parsing</title>
      <author><first>Rui</first><last>Cai</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <doi>10.1162/tacl_a_00272</doi>
      <abstract>In this paper we focus on learning dependency aware representations for <a href="https://en.wikipedia.org/wiki/Semantic_role_labeling">semantic role labeling</a> without recourse to an external <a href="https://en.wikipedia.org/wiki/Parsing">parser</a>. The backbone of our model is an LSTM-based semantic role labeler jointly trained with two auxiliary tasks : predicting the dependency label of a word and whether there exists an arc linking it to the predicate. The auxiliary tasks provide syntactic information that is specific to <a href="https://en.wikipedia.org/wiki/Semantic_role_labeling">semantic role labeling</a> and are learned from training data (dependency annotations) without relying on existing dependency parsers, which can be noisy (e.g., on out-of-domain data or infrequent constructions). Experimental results on the CoNLL-2009 benchmark dataset show that our model outperforms the state of the art in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, and consistently improves performance in other languages, including <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a>, and <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a>.</abstract>
      <pages>343–356</pages>
      <video href="https://vimeo.com/384772555" />
      <url hash="c8f6bcf4">Q19-1022</url>
      <bibkey>cai-lapata-2019-syntax</bibkey>
      <pwccode url="https://github.com/RuiCaiNLP/SRL_DEP" additional="false">RuiCaiNLP/SRL_DEP</pwccode>
    </paper>
    <paper id="25">
      <title>No Word is an IslandA Transformation Weighting Model for Semantic Composition<fixed-case>I</fixed-case>sland—<fixed-case>A</fixed-case> Transformation Weighting Model for Semantic Composition</title>
      <author><first>Corina</first><last>Dima</last></author>
      <author><first>Daniël</first><last>de Kok</last></author>
      <author><first>Neele</first><last>Witte</last></author>
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <doi>10.1162/tacl_a_00275</doi>
      <abstract>Composition models of <a href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional semantics</a> are used to construct phrase representations from the representations of their words. Composition models are typically situated on two ends of a spectrum. They either have a small number of parameters but compose all phrases in the same way, or they perform word-specific compositions at the cost of a far larger number of parameters. In this paper we propose transformation weighting (TransWeight), a composition model that consistently outperforms existing models on nominal compounds, adjective-noun phrases, and adverb-adjective phrases in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/German_language">German</a>, and <a href="https://en.wikipedia.org/wiki/Dutch_language">Dutch</a>. TransWeight drastically reduces the number of parameters needed compared with the best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> in the literature by composing similar words in the same way.</abstract>
      <pages>437–451</pages>
      <video href="https://vimeo.com/384772326" />
      <url hash="f819b871">Q19-1025</url>
      <bibkey>dima-etal-2019-word</bibkey>
    </paper>
    </volume>
</collection>