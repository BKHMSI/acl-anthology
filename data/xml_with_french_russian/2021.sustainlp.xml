<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.sustainlp">
  <volume id="1" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</booktitle>
      <publisher>Association for Computational Linguistics</publisher>
      <editor><first>Nafise Sadat</first><last>Moosavi</last></editor>
      <editor><first>Iryna</first><last>Gurevych</last></editor>
      <editor><first>Angela</first><last>Fan</last></editor>
      <editor><first>Thomas</first><last>Wolf</last></editor>
      <editor><first>Yufang</first><last>Hou</last></editor>
      <editor><first>Ana</first><last>Marasović</last></editor>
      <editor><first>Sujith</first><last>Ravi</last></editor>
      <address>Virtual</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="3928e276">2021.sustainlp-1.0</url>
      <bibkey>sustainlp-2021-simple</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Low Resource Quadratic Forms for Knowledge Graph Embeddings</title>
      <author><first>Zachary</first><last>Zhou</last></author>
      <author><first>Jeffery</first><last>Kline</last></author>
      <author><first>Devin</first><last>Conathan</last></author>
      <author><first>Glenn</first><last>Fung</last></author>
      <pages>1–10</pages>
      <abstract>We address the problem of link prediction between entities and relations of knowledge graphs. State of the art techniques that address this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a>, while increasingly accurate, are computationally intensive. In this paper we cast link prediction as a sparse convex program whose solution defines a <a href="https://en.wikipedia.org/wiki/Quadratic_form">quadratic form</a> that is used as a <a href="https://en.wikipedia.org/wiki/Ranking_function">ranking function</a>. The structure of our <a href="https://en.wikipedia.org/wiki/Convex_optimization">convex program</a> is such that standard support vector machine software packages, which are numerically robust and efficient, can solve it. We show that on benchmark data sets, our model’s performance is competitive with state of the art models, but training times can be reduced by a factor of 40 using only CPU-based (and not GPU-accelerated) computing resources. This approach may be suitable for applications where balancing the demands of graph completion performance against computational efficiency is a desirable trade-off.<i>link prediction</i> between entities and relations of knowledge graphs. State of the art techniques that address this problem, while increasingly accurate, are computationally intensive. In this paper we cast link prediction as a sparse convex program whose solution defines a quadratic form that is used as a ranking function. The structure of our convex program is such that standard support vector machine software packages, which are numerically robust and efficient, can solve it. We show that on benchmark data sets, our model’s performance is competitive with state of the art models, but training times can be reduced by a factor of 40 using only CPU-based (and not GPU-accelerated) computing resources. This approach may be suitable for applications where balancing the demands of graph completion performance against computational efficiency is a desirable trade-off.</abstract>
      <url hash="ada90a48">2021.sustainlp-1.1</url>
      <attachment type="Software" hash="5d29bbd1">2021.sustainlp-1.1.Software.zip</attachment>
      <bibkey>zhou-etal-2021-low</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.1</doi>
    </paper>
    <paper id="3">
      <title>Limitations of Knowledge Distillation for Zero-shot Transfer Learning</title>
      <author><first>Saleh</first><last>Soltan</last></author>
      <author><first>Haidar</first><last>Khan</last></author>
      <author><first>Wael</first><last>Hamza</last></author>
      <pages>22–31</pages>
      <abstract>Pretrained transformer-based encoders such as <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> have been demonstrated to achieve state-of-the-art performance on numerous NLP tasks. Despite their success, BERT style encoders are large in size and have high latency during <a href="https://en.wikipedia.org/wiki/Inference">inference</a> (especially on CPU machines) which make them unappealing for many <a href="https://en.wikipedia.org/wiki/Online_application">online applications</a>. Recently introduced compression and distillation methods have provided effective ways to alleviate this shortcoming. However, the focus of these works has been mainly on monolingual encoders. Motivated by recent successes in zero-shot cross-lingual transfer learning using multilingual pretrained encoders such as mBERT, we evaluate the effectiveness of Knowledge Distillation (KD) both during pretraining stage and during fine-tuning stage on multilingual BERT models. We demonstrate that in contradiction to the previous observation in the case of monolingual distillation, in multilingual settings, <a href="https://en.wikipedia.org/wiki/Distillation">distillation</a> during pretraining is more effective than <a href="https://en.wikipedia.org/wiki/Distillation">distillation</a> during fine-tuning for zero-shot transfer learning. Moreover, we observe that <a href="https://en.wikipedia.org/wiki/Distillation">distillation</a> during <a href="https://en.wikipedia.org/wiki/Musical_tuning">fine-tuning</a> may hurt zero-shot cross-lingual performance. Finally, we demonstrate that distilling a larger model (BERT Large) results in the strongest distilled model that performs best both on the source language as well as target languages in zero-shot settings.</abstract>
      <url hash="40ec1cfc">2021.sustainlp-1.3</url>
      <bibkey>soltan-etal-2021-limitations</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.3</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="7">
      <title>Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering</title>
      <author><first>Georgios</first><last>Sidiropoulos</last></author>
      <author><first>Nikos</first><last>Voskarides</last></author>
      <author><first>Svitlana</first><last>Vakulenko</last></author>
      <author><first>Evangelos</first><last>Kanoulas</last></author>
      <pages>58–63</pages>
      <abstract>In simple open-domain question answering (QA), dense retrieval has become one of the standard approaches for retrieving the relevant passages to infer an answer. Recently, dense retrieval also achieved state-of-the-art results in multi-hop QA, where aggregating information from multiple pieces of information and reasoning over them is required. Despite their success, dense retrieval methods are computationally intensive, requiring multiple <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPUs</a> to train. In this work, we introduce a hybrid (lexical and dense) retrieval approach that is highly competitive with the state-of-the-art dense retrieval models, while requiring substantially less computational resources. Additionally, we provide an in-depth evaluation of dense retrieval methods on limited computational resource settings, something that is missing from the current literature.</abstract>
      <url hash="0132b005">2021.sustainlp-1.7</url>
      <bibkey>sidiropoulos-etal-2021-combining</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.7</doi>
      <pwccode url="https://github.com/gsidiropoulos/hybrid_retrieval_for_efficient_qa" additional="false">gsidiropoulos/hybrid_retrieval_for_efficient_qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
    <paper id="8">
      <title>Learning to Rank in the Age of <a href="https://en.wikipedia.org/wiki/The_Muppets">Muppets</a> : EffectivenessEfficiency Tradeoffs in Multi-Stage Ranking</title>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>ChengCheng</first><last>Hu</last></author>
      <author><first>Yuqi</first><last>Liu</last></author>
      <author><first>Hui</first><last>Fang</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>64–73</pages>
      <abstract>It is well known that rerankers built on pretrained transformer models such as <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> have dramatically improved retrieval effectiveness in many tasks. However, these gains have come at substantial costs in terms of <a href="https://en.wikipedia.org/wiki/Economic_efficiency">efficiency</a>, as noted by many researchers. In this work, we show that it is possible to retain the benefits of transformer-based rerankers in a multi-stage reranking pipeline by first using feature-based learning-to-rank techniques to reduce the number of candidate documents under consideration without adversely affecting their quality in terms of recall. Applied to the MS MARCO passage and document ranking tasks, we are able to achieve the same level of <a href="https://en.wikipedia.org/wiki/Effectiveness">effectiveness</a>, but with up to 18 increase in <a href="https://en.wikipedia.org/wiki/Efficiency">efficiency</a>. Furthermore, our techniques are orthogonal to other methods focused on accelerating transformer inference, and thus can be combined for even greater efficiency gains. A higher-level message from our work is that, even though pretrained transformers dominate the modern IR landscape, there are still important roles for traditional LTR techniques, and that we should not forget history.</abstract>
      <url hash="88a71479">2021.sustainlp-1.8</url>
      <bibkey>zhang-etal-2021-learning-rank</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.8</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="13">
      <title>Distiller : A Systematic Study of Model Distillation Methods in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a></title>
      <author><first>Haoyu</first><last>He</last></author>
      <author><first>Xingjian</first><last>Shi</last></author>
      <author><first>Jonas</first><last>Mueller</last></author>
      <author><first>Sheng</first><last>Zha</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <author><first>George</first><last>Karypis</last></author>
      <pages>119–133</pages>
      <abstract>Knowledge Distillation (KD) offers a natural way to reduce the <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a> and memory / energy usage of massive pretrained models that have come to dominate Natural Language Processing (NLP) in recent years. While numerous sophisticated variants of KD algorithms have been proposed for NLP applications, the key factors underpinning the optimal distillation performance are often confounded and remain unclear. We aim to identify how different components in the KD pipeline affect the resulting performance and how much the optimal KD pipeline varies across different datasets / tasks, such as the data augmentation policy, the <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a>, and the intermediate representation for transferring the knowledge between teacher and student. To tease apart their effects, we propose Distiller, a meta KD framework that systematically combines a broad range of techniques across different stages of the KD pipeline, which enables us to quantify each component’s contribution. Within Distiller, we unify commonly used objectives for distillation of intermediate representations under a universal mutual information (MI) objective and propose a class of MI-objective functions with better bias / variance trade-off for estimating the MI between the teacher and the student. On a diverse set of NLP datasets, the best Distiller configurations are identified via large-scale hyper-parameter optimization. Our experiments reveal the following : 1) the approach used to distill the intermediate representations is the most important factor in KD performance, 2) among different objectives for intermediate distillation, MI-performs the best, and 3) data augmentation provides a large boost for small training datasets or small student networks.</abstract>
      <url hash="71c94360">2021.sustainlp-1.13</url>
      <bibkey>he-etal-2021-distiller</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.13</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="14">
      <title>Shrinking Bigfoot : Reducing wav2vec 2.0 footprint</title>
      <author><first>Zilun</first><last>Peng</last></author>
      <author><first>Akshay</first><last>Budhkar</last></author>
      <author><first>Ilana</first><last>Tuil</last></author>
      <author><first>Jason</first><last>Levy</last></author>
      <author><first>Parinaz</first><last>Sobhani</last></author>
      <author><first>Raphael</first><last>Cohen</last></author>
      <author><first>Jumana</first><last>Nassour</last></author>
      <pages>134–141</pages>
      <abstract>Wav2vec 2.0 is a state-of-the-art <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech recognition model</a> which maps speech audio waveforms into latent representations. The largest version of wav2vec 2.0 contains 317 million parameters. Hence, the <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">inference latency</a> of wav2vec 2.0 will be a bottleneck in production, leading to high costs and a significant environmental footprint. To improve wav2vec’s applicability to a production setting, we explore multiple model compression methods borrowed from the domain of large language models. Using a teacher-student approach, we distilled the knowledge from the original wav2vec 2.0 model into a student model, which is 2 times faster, 4.8 times smaller than the original model. More importantly, the student model is 2 times more energy efficient than the original <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> in terms of <a href="https://en.wikipedia.org/wiki/Carbon_dioxide_in_Earth’s_atmosphere">CO2 emission</a>. This increase in performance is accomplished with only a 7 % degradation in word error rate (WER). Our quantized model is 3.6 times smaller than the original <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, with only a 0.1 % degradation in <a href="https://en.wikipedia.org/wiki/White_blood_cell">WER</a>. To the best of our knowledge, this is the first work that compresses wav2vec 2.0.</abstract>
      <url hash="b90fb3f6">2021.sustainlp-1.14</url>
      <bibkey>peng-etal-2021-shrinking</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.14</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="17">
      <title>Unsupervised Contextualized Document Representation</title>
      <author><first>Ankur</first><last>Gupta</last></author>
      <author><first>Vivek</first><last>Gupta</last></author>
      <pages>166–173</pages>
      <abstract>Several NLP tasks need the effective repre-sentation of text documents. Arora et al.,2017 demonstrate that simple weighted aver-aging of word vectors frequently outperformsneural models. SCDV (Mekala et al., 2017)further extends this from sentences to docu-ments by employing soft and sparse cluster-ing over pre-computed word vectors. How-ever, both techniques ignore the polysemyand contextual character of words. In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach. Weshow that our embeddings outperform origi-nal SCDV, pre-train BERT, and several otherbaselines on many classification datasets. Wealso demonstrate our embeddings effective-ness on other tasks, such as concept match-ing and sentence similarity. In addition, we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</abstract>
      <url hash="d9b2a07c">2021.sustainlp-1.17</url>
      <bibkey>gupta-gupta-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.17</doi>
      <pwccode url="https://github.com/vgupta123/contextualize_scdv" additional="false">vgupta123/contextualize_scdv</pwccode>
    <title_ar>تمثيل المستندات السياقية غير الخاضع للإشراف</title_ar>
      <title_pt>Representação de Documento Contextualizado Não Supervisionado</title_pt>
      <title_fr>Représentation contextualisée de documents non supervisée</title_fr>
      <title_es>Representación de documentos contextualizados no supervisados</title_es>
      <title_ja>スーパーバイザーなしのコンテキスト化されたドキュメント表現</title_ja>
      <title_zh>无监督者文档示</title_zh>
      <title_ru>Неконтролируемое представление контекстуализированных документов</title_ru>
      <title_hi>असुरक्षित संदर्भित दस्तावेज़ प्रतिनिधित्व</title_hi>
      <title_ga>Ionadaíocht Doiciméad Comhthéacsúil Gan Maoirseacht</title_ga>
      <title_hu>Felügyelet nélküli kontextualizált dokumentumképviselet</title_hu>
      <title_ka>კონტექსტუალური დოკუმენტის გამოსახულება</title_ka>
      <title_el>Αναπροσώπευση εγγράφου χωρίς επιτήρηση</title_el>
      <title_it>Rappresentanza contestualizzata non sorvegliata</title_it>
      <title_ms>Perwakilan Dokumen Berkonteks Tidak Dikawal</title_ms>
      <title_lt>Neaprižiūrimas kontekstinis dokumento atstovavimas</title_lt>
      <title_kk>Сәйкестік контекстуалды құжатты таңдау</title_kk>
      <title_mk>Ненадгледувана контекстна претстава на документот</title_mk>
      <title_mt>Unsupervised Contextualized Document Representation</title_mt>
      <title_ml>പരിശോധിച്ചിട്ടില്ലാത്ത രേഖയുടെ പ്രതിനിധി</title_ml>
      <title_mn>Хадгалагдаггүй Сүүлийн үеийн Документын төлөөлөл</title_mn>
      <title_pl>Niekontrolowana kontekstowa reprezentacja dokumentu</title_pl>
      <title_no>Comment</title_no>
      <title_so>Xukummada aan la ilaalinayn</title_so>
      <title_ro>Reprezentarea contextualizată nesupravegheată a documentelor</title_ro>
      <title_sr>Представка документа неподржана контекстално</title_sr>
      <title_si>සුරක්ෂා නැති සංවේදනය සඳහා ලිපින්ත ප්‍රතිස්ථානය</title_si>
      <title_sv>Icke övervakad kontextualiserad dokumentrepresentation</title_sv>
      <title_ur>غیر محافظت کی کنٹکسٹولیز ڈوکومٹ ریسپرنسیٹ</title_ur>
      <title_ta>கண்காணிக்கப்படவில்லை</title_ta>
      <title_uz>@ info: whatsthis</title_uz>
      <title_vi>KCharselect unicode block name</title_vi>
      <title_bg>Неконтролирано контекстуализирано представяне на документа</title_bg>
      <title_nl>Niet-gecontextualiseerde documentweergave</title_nl>
      <title_da>Ikke-overvåget kontekstualiseret dokumentrepræsentation</title_da>
      <title_hr>Nepotrebna kontekstualizacija predstavljanja dokumenta</title_hr>
      <title_de>Nicht überwachte kontextualisierte Dokumentdarstellung</title_de>
      <title_ko>무감독의 상하 문화 문서 표시</title_ko>
      <title_sw>Replacement of document</title_sw>
      <title_tr>_Senedler</title_tr>
      <title_af>Onondersteunde Konteksualiseerde Dokument Voorstelling</title_af>
      <title_am>ሰነዱን እንዳለ ምረጡ</title_am>
      <title_id>Perwakilan Dokumen Konteksual Tidak Disupervisi</title_id>
      <title_sq>Përfaqësimi i Dokumentit të Konteksualizuar i Pambikqyrur</title_sq>
      <title_bn>পর্যবেক্ষণ করা নথির প্রতিনিধি</title_bn>
      <title_az>Qeyd edilm톛y톛n Kontext D칬k칲man 캻zl톛ri</title_az>
      <title_ca>Representació Contextualitzada sense supervisió</title_ca>
      <title_bs>Nepotrebna kontekstualizacija predstavljanja dokumenta</title_bs>
      <title_cs>Nekontrolovaná kontextová reprezentace dokumentu</title_cs>
      <title_fa>نمایش سند غیرقابل حفاظت</title_fa>
      <title_fi>Tarkkailematon kontekstualisoitu asiakirjaesitys</title_fi>
      <title_et>Järelevalveta kontekstualiseeritud dokumendi esitus</title_et>
      <title_hy>Comment</title_hy>
      <title_jv>marker</title_jv>
      <title_he>מייצג מסמכים מקורקסטוליזי ללא השגחה</title_he>
      <title_ha>@ action</title_ha>
      <title_sk>Nenadzorovana kontekstualizirana predstavitev dokumenta</title_sk>
      <title_bo>སྔོན་བསྐྱུར་མེད་པའི་རྣམ་གྲངས་ཀྱི་ཡིག་ཆ་མཚོན་རྟགས</title_bo>
      <abstract_ar>تحتاج العديد من مهام البرمجة اللغوية العصبية إلى إعادة إرسال المستندات النصية بشكل فعال ، يوضح أرورا وآخرون ، 2017 أن متوسط العمر المرجح لمتجهات الكلمات يتفوق في كثير من الأحيان على النماذج العصبية. يمتد SCDV (Mekala et al. ، 2017) هذا من الجمل إلى المستندات من خلال استخدام مجموعة ناعمة ومتفرقة على متجهات الكلمات المحسوبة مسبقًا. ومع ذلك ، تتجاهل كلتا التقنيتين الطابع متعدد المعاني والسياق للكلمات. في هذه الورقة ، نتناول هذه المشكلة من خلال اقتراح SCDV + BERT (ctxd) ، وهو تمثيل بسيط وفعال غير خاضع للإشراف يجمع بين BERT المنسق للنص (Devlin et al. ، 2019 ) تضمين الكلمات الأساسية لتوضيح معنى الكلمة باستخدام نهج التجميع الناعم SCDV. نلاحظ أن حفلات الزفاف لدينا تفوقت في الأداء على SCDV الأصلي ، والتدريب المسبق لـ BERT ، والعديد من الخطوط الأساسية الأخرى في العديد من مجموعات بيانات التصنيف. نوضح أيضًا فاعلية عمليات التضمين الخاصة بنا في مهام أخرى ، مثل مطابقة المفاهيم وتشابه الجمل. بالإضافة إلى ذلك ، نظهر أن SCDV + BERT (ctxd) يتفوق في الأداء على BERT وتطبيقات التضمين المختلفة في سيناريوهات ذات بيانات محدودة وقليل فقط أمثلة اللقطات.</abstract_ar>
      <abstract_fr>Plusieurs tâches de PNL nécessitent la représentation efficace de documents textuels. Arora et al., 2017 démontrent que la moyenne pondérée simple des vecteurs de mots surpasse souvent les modèles neuronaux. SCDV (Mekala et al., 2017) étend encore cela des phrases aux documents en utilisant un cluster-ing souple et clairsemé sur des vecteurs de mots précalculés. Cependant, les deux techniques ignorent la polysémie et le caractère contextuel des mots.Dans cet article, nous abordons ce problème en proposant SCDV+BERT (ctxd), une représentation non supervisée simple et efficace qui combine l'intégration de mots basée sur le BERT con-textualisé (Devlin et al., 2019) pour la désambiguïa-tion du sens des mots avec SCDV approche de clustering souple. Nous montrons que nos intégrations surpassent le SCDV d'origine, le BERT pré-entraînement et plusieurs autres lignes de base sur de nombreux ensembles de données de classification. Nous démontrons également l'efficacité de nos intégrations sur d'autres tâches, telles que la correspondance de concepts et la similarité de phrases. De plus, nous montrons que SCDV+BERT (ctxd) surpasse le BERT affiné et les différentes approches d'application d'intégration dans des scénarios avec des données limitées et seulement quelques exemples de plans.</abstract_fr>
      <abstract_es>Varias tareas de PNL requieren la representación efectiva de los documentos de texto. Arora et al., 2017 demuestran que el promedio ponderado simple de los vectores de palabras con frecuencia supera a los modelos neuronales. SCDV (Mekala et al., 2017) amplía aún más esto de oraciones a documentos mediante el empleo de agrupamiento suave y disperso sobre vectores de palabras precalculados. Sin embargo, ambas técnicas ignoran la polisemia y el carácter contextual de las palabras. En este artículo, abordamos este tema proponiendo SCDV+BERT (ctxd), una representación simple y eficaz sin supervisión que combina la incrustación de palabras basada en BERT (Devlin et al., 2019) contextualizada para la desambiguación del sentido de las palabras con SCDV enfoque de clustering suave. Demuestramos que nuestras incorporaciones superan al SCDV original, preentrenan BERT y varias otras líneas de base en muchos conjuntos de datos de clasificación. También demostramos la eficacia de nuestras incorporaciones en otras tareas, como la correspondencia de conceptos y la similitud de oraciones. Además, mostramos que SCDV+BERT (ctxd) supera al BERT de ajuste fino y a los diferentes enfoques de incrustación en escenarios con datos limitados y ejemplos de solo unos pocos disparos.</abstract_es>
      <abstract_pt>Várias tarefas de PNL precisam da representação eficaz de documentos de texto. Arora et al., 2017 demonstram que a média ponderada simples de vetores de palavras frequentemente supera os modelos neurais. SCDV (Mekala et al., 2017) estende ainda mais isso de frases para documentos, empregando agrupamento suave e esparso sobre vetores de palavras pré-computados. No entanto, ambas as técnicas ignoram a polissemia e o caráter contextual das palavras. Neste artigo, abordamos essa questão propondo SCDV+BERT(ctxd), uma representação não supervisionada simples e eficaz que combina BERT contextualizado (Devlin et al., 2019 ) incorporação de palavras para desambiguação de sentido de palavras com abordagem de clustering suave SCDV. Mostramos que nossos embeddings superam o SCDV original, BERT pré-treinamento e várias outras linhas de base em muitos conjuntos de dados de classificação. Também demonstramos a eficácia de nossa incorporação em outras tarefas, como correspondência de conceito e similaridade de sentença. exemplos de tiros.</abstract_pt>
      <abstract_ja>いくつかのNLPタスクは、テキスト文書の効果的な繰り返し送信を必要とする。rora et al., 2017は、単純なワードベクトルの加重平均時効が、ニューラルモデルを頻繁に上回ることを実証している。 SCDV （ Mekala et al., 2017 ）は、これをさらに、事前に計算された単語ベクトルの上にソフトでまばらなクラスタリングを採用することによって、文から文に拡張します。 どのようにして、両方のテクニックは単語の多義性と文脈的性格を無視しています。本稿では、コンテキスト化されたBERT （ Devlin et al., 2019 ）ベースの単語センスの埋め込みとSCDVソフトクラスタリングアプローチを組み合わせた、シンプルで効果的な無監督表現であるSCDV + BERT （ ctxd ）を提案することで、この問題に対処します。 当社の埋め込みは、多くの分類データセット上で、ORIGI - NAL SCDV、事前トレーニングBERT、およびいくつかの他のベースラインよりも優れていることを示しています。 Wealsoは、コンセプトマッチングや文章の類似性など、他のタスクでの埋め込みの有効性を実証しています。さらに、SCDV + BERT （ ctxd ）は、限られたデータとわずかなショットの例だけのシナリオで、BERTやさまざまな埋め込みアプローチを細かく調整しています。</abstract_ja>
      <abstract_zh>诸NLP务对文本文档有效者更分之。 Arora等,2017年证,词向量简加权偏老化常优于神经。 SCDV(Mekala等,2017)于预算词向量上用软疏聚类,更广其句于文档。 此二术者,皆略单词之多义,与上下文之征也。 本文中,发SCDV + BERT(ctxd)以决之,此简而效者无监,将基于con-textualized BERT(Devlin等,2019)之基于单词词嵌SCDV软聚类,以成词义消歧义。 吾明嵌诸类集上优于始 SCDV、预练 BERT 及诸基线。 展我嵌有效性,如对句相似性。 此外又明数有限而少镜头示例者,SCDV+BERT(ctxd) 优于精细调 BERT 异者嵌 ap-proaches。</abstract_zh>
      <abstract_ru>Несколько задач NLP нуждаются в эффективном воспроизведении текстовых документов. Арора и др., 2017 демонстрируют, что простое взвешенное среднее старение векторов слов часто превосходитневрологические модели. SCDV (Mekala et al., 2017)дополнительно расширяет это от предложений до решений, используя мягкое и разреженное кластеризацию над предварительно вычисленными векторами слов. Тем не менее, обе методики игнорируют полисемию и контекстный характер слов. В этой статье мы решаем эту проблему, предлагая SCDV +BERT(ctxd), простое и эффективное неконтролируемое представление, которое сочетает встраивание слов на основе контекстуализированного BERT (Devlin et al., 2019) для расчленения смысла слова с подходом мягкой кластеризации SCDV. Показано, что наши вложения превосходят исходные SCDV, BERT перед тренировкой и некоторые другиеосновы на многих классификационных наборах данных. Мы также демонстрируем эффективность наших вложений в других задачах, таких как сопоставление концепций и сходство предложений. Кроме того,мы показываем, что SCDV+BERT(ctxd) превосходит тонкую настройку BERT и различные встраивающие аппроксимации в сценариях с ограниченными данными и только несколькими примерами снимков.</abstract_ru>
      <abstract_hi>कई NLP कार्यों को पाठ दस्तावेज़ों के प्रभावी repre-sentation की आवश्यकता होती है। अरोड़ा एट अल., 2017 प्रदर्शित करता है कि शब्द वैक्टर के सरल भारित औसत-उम्र बढ़ने अक्सर तंत्रिका मॉडल से बेहतर प्रदर्शन करते हैं। SCDV (Mekala et al., 2017) आगे पूर्व-परिकलित शब्द वैक्टर पर नरम और विरल क्लस्टर-इंग को नियोजित करके वाक्यों से डॉक्यू-मेंट्स तक इसका विस्तार करता है। कैसे-कभी, दोनों तकनीकें शब्दों के पॉलीसेमी और प्रासंगिक चरित्र को अनदेखा करती हैं। इस पत्र में, हम इस मुद्दे को SCDV + BERT (ctxd) का प्रस्ताव करके संबोधित करते हैं, जो एक सरल और प्रभावी संयुक्त राष्ट्र-पर्यवेक्षित प्रतिनिधित्व है जो SCDV नरम क्लस्टरिंग दृष्टिकोण के साथ शब्द भावना disambigua-tion के लिए एम्बेडिंग के लिए con-textualized BERT (Devlin et al., 2019) को जोड़ता है। Weshow कि हमारे embeddings origi-nal SCDV, पूर्व ट्रेन BERT, और कई वर्गीकरण डेटासेट पर कई अन्य आधारों से बेहतर प्रदर्शन करते हैं। Wealdings भी अन्य कार्यों पर हमारे embeddings प्रभावी-नेस का प्रदर्शन, जैसे अवधारणा मिलान और वाक्य समानता के रूप में. इसके अलावा, हम दिखाते हैं कि SCDV + BERT (ctxd) सीमित डेटा और केवल कुछ शॉट्स उदाहरणों के साथ परिदृश्यों में विभिन्न एम्बेडिंग एपी-प्रोच और विभिन्न एम्बेडिंग एपी-प्रोच को मात देता है।</abstract_hi>
      <abstract_ga>Teastaíonn athsheoladh éifeachtach na ndoiciméad téacs ó roinnt tascanna NLP. Déanann SCDV (Mekala et al., 2017) é seo a leathnú tuilleadh ó abairtí go doiciméid trí bhraislí bog agus tanaí a úsáid thar veicteoirí focal réamhríofa. Mar sin féin, déanann an dá theicníocht neamhaird ar charachtar ilghnéitheach agus comhthéacsúil na bhfocal. Sa pháipéar seo, tugaimid aghaidh ar an tsaincheist seo trí SCDV+BERT(ctxd) a mholadh, léiriú simplí agus éifeachtach gan maoirsiú a chomhcheanglaíonn BERT comhthéacsúil (Devlin et al., 2019). ) basedword embedding for word sense disambigua-tion with SCDV bog cnuasach chuige. Léiríonn muid go sáraíonn ár leabaithe SCDV bunaidh, CRET réamhthraenála, agus go leor bonnlínte eile ar go leor tacar sonraí aicmithe. Léirímid freisin ár n-éifeachtacht leabú ar thascanna eile, mar mheaitseáil choincheapa agus cosúlacht abairtí. Chomh maith leis sin, léirímid go sáraíonn SCDV+BERT(ctxd) mion-thonn CRET agus cur chuige leabaithe éagsúla i gcásanna le sonraí teoranta agus gan ach beagán acu. samplaí shots.</abstract_ga>
      <abstract_ka>რამდენიმე NLP დავალებები იჭირდება ტექსტის დოკუმენტების ეფექტიური განახლება. Arora et al.,2017-ს დემონსტრაცია, რომ სიტყვის გვექტორების მხოლოდ უფრო გავაკეთებული მოდელები. SCDV კაკ თ ეა ვ, ეგამარა რვჳნთკა თდნჲპთპარ ოჲლთჟვმთწრა თ კჲნრვკჟსრნა ოპთფთნა ნა ესმთრვ. ამ სპეპერში, ჩვენ ამ პრობლემას გადაწყენებთ SCDV+BERT(ctxd), საუკეთესო და ეფექტიური არ დააყენებული გამოსახულებელი გამოსახულება, რომელიც კონტექსტულიზებული BERT (Devlin et al., 2019) სპექტირებულია, რომელიც სიტყვის სიტყვის განს ჩვენ ჩვენი კლასიფიკაციის მონაცემები უფრო გავაკეთებთ origi-nal SCDV, pre-train BERT, და რამდენიმე სხვა კლასიფიკაციის მონაცემები. ვილჟჲ ევმონსრპთპა ნაქთრვ თნტვკრთგნჲჟრთ ნა ეპსდთრვ პაბჲრთ, კარჲ კჲნუვოუთ£ა ჟლთფნჲჟრ თ ჟლთფნჲჟრ ნა ოპვეგთე. დამატებით, ჩვენ გამოჩვენებთ, რომ SCDV+BERT(ctxd) ბერტი-ტუნი-ტუნი BERT და განსხვავებული სენარიოში ჩატვირთვა ყველაფერი სენარიოში, რომლებიც დავწერებული მონაცემები და მხოლოდ</abstract_ka>
      <abstract_el>Αρκετές εργασίες NLP χρειάζονται την αποτελεσματική αντιπροσώπευση εγγράφων κειμένου. Η Αρόρα κ.λ.,2017 αποδεικνύει ότι η απλή σταθμισμένη αντιστάθμιση των διανυσματικών λέξεων συχνά ξεπερνά τα νευρωνικά μοντέλα. Η SCDV (Mekala et al., 2017)επεκτείνει περαιτέρω αυτό από προτάσεις σε έγγραφα χρησιμοποιώντας μαλακή και αραιή συσσώρευση πάνω από προ-υπολογισμένα διανύσματα λέξεων. Ωστόσο, και οι δύο τεχνικές αγνοούν τον πολυσημιακό και περιεκτικό χαρακτήρα των λέξεων. Σε αυτό το άρθρο, αντιμετωπίζουμε αυτό το ζήτημα προτείνοντας μια απλή και αποτελεσματική μη εποπτευόμενη αναπαράσταση που συνδυάζει την ενσωμάτωση λέξεων με βάση το κείμενο με την προσέγγιση μαλακής ομαδοποίησης. Να δείξουμε ότι οι ενσωματώσεις μας ξεπερνούν την αρχική SCDV, την προ-εκπαίδευση BERT και αρκετές άλλες γραμμές βάσης σε πολλά σύνολα δεδομένων ταξινόμησης. Θα επιδείξουμε επίσης την αποτελεσματικότητα των ενσωματώσεων μας σε άλλες εργασίες, όπως η αντιστοίχιση εννοιών και η ομοιότητα των προτάσεων. Επιπλέον, δείχνουμε ότι το SCDV+BERT(ctxd) ξεπερνά τις επιδόσεις του BERT και τις διαφορετικές προσεγγίσεις ενσωμάτωσης σε σενάρια με περιορισμένα δεδομένα και ελάχιστα παραδείγματα λήψεων.</abstract_el>
      <abstract_hu>Számos NLP-feladatra van szükség a szöveges dokumentumok hatékony megismertetésére. Arora et al., 2017 bizonyítja, hogy a szóvektorok egyszerű súlyozott átlagos öregedése gyakran felülmúlja a teljesítményneurális modelleket. Az SCDV (Mekala et al., 2017) tovább bővíti ezt a mondatoktól a dokumentumokig azáltal, hogy puha és ritka klaszterezést alkalmaz előre kiszámított szóvektorokkal szemben. Mindkét technika azonban figyelmen kívül hagyja a szavak poliszemiás és kontextuális karakterét. Ebben a kérdésben az SCDV+BERT(ctxd), egy egyszerű és hatékony, felügyelet nélküli reprezentáció javaslatával foglalkozunk ezzel a problémával, amely ötvözi a szöveges BERT (Devlin et al., 2019) alapszó beágyazását a szó értelmezésének egyértelműsítéséhez az SCDV soft klaszterelési megközelítésével. Számos osztályozási adatkészleten felülmúlják az eredeti SCDV-t, a BERT-t és számos más alapvonalat. Beágyazásunk hatékonyságát más feladatokban is bemutatjuk, mint például a koncepciók összehasonlítása és a mondatok hasonlósága. Ezenkívül megmutatjuk, hogy az SCDV+BERT(ctxd) teljesítményét a BERT finomhangolásához és a különböző beágyazási eljárásokhoz korlátozott adatokkal és csak néhány felvételi példával rendelkező forgatókönyvekben.</abstract_hu>
      <abstract_it>Diversi compiti del PNL richiedono una rappresentazione efficace dei documenti di testo. Arora et al.,2017 dimostrano che il semplice averaging ponderato dei vettori di parola spesso supera i modelli neurali prestazionali. SCDV (Mekala et al., 2017) estende ulteriormente questo concetto dalle frasi ai documenti impiegando cluster soft e sparso su vettori di parole pre-calcolati. Tuttavia, entrambe le tecniche ignorano il polisemie e il carattere contestuale delle parole. In questo articolo, affrontiamo questo problema proponendo SCDV+BERT(ctxd), una rappresentazione non supervisionata semplice ed efficace che combina l'embedding basato su parole con testo BERT (Devlin et al., 2019) per la disambiguazione del senso delle parole con l'approccio SCDV soft clustering. In molti set di dati di classificazione, le nostre incorporazioni superano le prestazioni iniziali SCDV, BERT pre-treno e molte altre linee di base. Dimostreremo inoltre l'efficacia delle nostre incorporazioni su altri compiti, come l'abbinamento dei concetti e la somiglianza delle frasi. Inoltre, mostriamo che SCDV+BERT(ctxd) supera le prestazioni di BERT e diverse applicazioni di incorporamento in scenari con dati limitati e solo pochi esempi di scatti.</abstract_it>
      <abstract_mk>На неколку задачи на НЛП им треба ефикасна претстава на текстовите документи. Арора и други, 2017 демонстрираат дека едноставното тежено просечно стареење на векторите на зборови често ги надминува норалните модели. SCDV (Mekala et al., 2017)понатаму го проширува ова од реченици до документи со употреба на меки и мали групирачки вектори над прекомпјутираните зборови. Сепак, двете техники го игнорираат полисемиот и контекстниот карактер на зборовите. Во овој спапер, го решаваме ова прашање предлагајќи SCDV+BERT( ctxd), едноставна и ефикасна ненадгледувана претстава која ги комбинира контекстулизираните BERT (Devlin et al., 2019) базирани зборови за раздвојување на зборовите со слабиот пристап на SCDV кластерирање. Ние покажуваме дека нашите внесувања ги надминуваат оригиналните SCDV, пред возењето BERT, и неколку други бази на многу податоци за класификација. Вилсо ја демонстрира нашата вмешаност ефикасност во другите задачи, како што се концептот на споредба и сличноста на речениците. Покрај тоа, покажуваме дека SCDV+BERT( ctxd) ги надминува резултатите од line- tune BERT и различните вградени ap- proaches во сценарија со ограничени податоци и само неколку примери од снимки.</abstract_mk>
      <abstract_lt>Several NLP tasks need the effective repre-sentation of text documents. Arora et al.,2017 m. rodo, kad paprastas svertinis žodžių vektorių vidurkis dažnai viršija neurologinius modelius. SCDV (Mekala et al., 2017 m.) toliau pratęsia nuo sakinių iki dokumentų, naudojant minkštą ir nedidelę klasterizaciją prieš iš anksto apskaičiuotus žodžių vektorius. Vis dėlto abu metodai ignoruoja žodžių polisemiją ir kontekstinį pobūdį. Šiame spaper sprendžiame šį klausimą pasiūlydami SCDV+BERT(ctxd), paprastą ir veiksmingą nepastebimą atstovavimą, kuris sujungia tekstualizuotą BERT (Devlin et al., 2019 m.), grindžiamą žodžių supratimo nesuderinamumu su SCDV minkšto klasterizavimo metodu. Mes parodysime, kad mūsų įdėjimai viršija pradinę SCDV, prieš traukinį BERT ir keletą kitų bazinių linijų daugelyje klasifikavimo duomenų rinkinių. Wealso taip pat įrodo, kad mes įtraukiame veiksmingumą į kitas užduotis, pvz., sąvokų derinimą ir sakinių panašumą. Be to, mes rodome, kad SCDV+BERT(ctxd) rezultatai yra suderinti su BERT ir skirtingi įtraukimo metodai į scenarijus, kuriuose duomenys yra riboti, ir tik keletas fotografijų pavyzdžių.</abstract_lt>
      <abstract_kk>Кейбір NLP тапсырмаларында мәтін құжаттарды қайта құру керек. Арора и ал.,2017 жылы сөздердің қарапайым жетілдірілген векторларының көпшілігін өзгерту үлгілерін көрсетеді. SCDV (Mekala et al., 2017) бұл сөздерді сөздерден документтерге қолданып, алдын- есептелген сөздер векторларының арқылы жабысқа және кеңістік кластерін қолдануға арналады. Қалай болса, екеуі техникалар полиземмен контексті сөздердің таңбаларын елемейді. Бұл мәселеге SCDV+BERT( ctxd) дегенді таңдап береміз. Бұл мәселеге контекстуализацияланған BERT (Devlin et al., 2019) бағдарламасын SCDV бағдарламалық кластерлік арқылы сөздердің сезімі үшін дембигуа- таңдау үшін қолданылатын кәдімгі және Біздің ендіруіміздің көпшілік классификациялық деректер қорларындағы көпшілікті SCDV, BERT алдындағы және бірнеше бағдарламалық жолдарды жасайды. Wealso біздің ендіруіміздің басқа тапсырмалардың эффективнігін көрсетеді, мысалы, сәйкестіктер мен сөйлеменің ұқсастығы. Қосымша, біз SCDV+BERT( ctxd) берттеулердің шектелген мәліметтері және тек бірнеше сценариялық сценарияларда әртүрлі бағдарламалардың шектелген жағдайды көрсетедік.</abstract_kk>
      <abstract_ml>Several NLP tasks need the effective repre-sentation of text documents. അരോറാ എറ്റ് അല്‍.,2017 വാക്ക് വെക്റ്ററുകളുടെ വയസ്സില്‍ എളുപ്പമുള്ള വയസ്സായിരുന്നു എന്ന് പ്രത്യക്ഷിക്കുന്നു. എസ്സിഡിവി (മെക്കാ എറ്റ് എൽ, 2017) വാക്കുകളില്‍ നിന്നും ഡോക്ടറിലേക്ക് മാറ്റുന്നത് മുമ്പ് കണക്കിലാക്കുന്ന വാക്ക് വെക്റ്ററുകള എപ്പോഴെങ്കിലും, രണ്ട് സാങ്കേതികവിദ്യകളും വാക്കുകളുടെ കൂട്ടത്തിലുള്ള പോലിസെമിന്റെയും സാധാരണ ക തിസ്പാപ്പരില്‍, SCDV+BERT( ctxd) പ്രാദേശിപ്പിക്കുന്നത് നമ്മള്‍ ഈ പ്രശ്നത്തെക്കുറിച്ച് വിശദീകരിക്കുന്നു. അത് കോണ്‍ട്ടെക്സ്ച്ചലേഷന്‍ ബെര്‍ട്ടി (ഡെവെല്‍ലിന്‍ എല്‍., 2019) വാക്ക്  ഞങ്ങളുടെ അകത്തേക്ക് പോകുന്നത് ഓറിജി-നാള്‍ സിഡിവി, മുന്നോട്ട് ട്രെയിന്‍ ബെര്‍ട്ടി (BERT), പല വിഭാഗവിവരങ്ങളുടെ ഡാറ്റാസറ്റ മറ്റു ജോലികളില്‍ നമ്മുടെ അഭിപ്രായത്തിന്റെ പ്രധാനപ്പെടുത്തിവെക്കുന്ന പ്രകടനങ്ങള്‍ പ്രത്യക്ഷപ്പെടുത്തുന്നു.  കൂടാതെ, SCDV+BERT(ctxd) പുറത്ത് പ്രദര്‍ശിപ്പിക്കുന്നത് ബെര്‍ട്ടിയില്‍ നിന്നും വ്യത്യസ്തമായ എപ്പ്-പ്രോചെക്സുകളും സിനേറ്ററിയോസില്‍ നി</abstract_ml>
      <abstract_ms>Beberapa tugas NLP memerlukan penghantaran semula dokumen teks yang berkesan. Arora et al.,2017 menunjukkan bahawa keseluruhan umur vektor perkataan yang berat sederhana sering melebihi model sneural. SCDV (Mekala et al., 2017)melanjutkan ini dari kalimat ke dokumen dengan menggunakan kelompok lembut dan ringan atas vektor perkataan pra-komputer. Bagaimanapun, kedua-dua teknik mengabaikan aksara polisemyand kontekstual perkataan. Dalam spaper ini, kami mengatasi isu ini dengan melaporkan SCDV+BERT( ctxd), sebuah perwakilan mudah dan efektif tanpa mengawasi yang menggabungkan bentuk BERT (Devlin et al., 2019) berdasarkan teks untuk penyelesaian perkataan dengan pendekatan kelompok lembut SCDV. Kita tunjukkan bahawa penyembedding kita melampaui SCDV asal, BERT sebelum kereta api, dan beberapa garis lain pada banyak set data pengelasahan. Wealso menunjukkan keefektivitas penyembedding kita pada tugas lain, seperti konsep sepadan dan persamaan kalimat. Selain itu, kami menunjukkan bahawa SCDV+BERT(ctxd) melampaui prestasi BERT baris-tune dan proach-embedding berbeza dalam skenario dengan data terbatas dan hanya beberapa contoh tembakan.</abstract_ms>
      <abstract_mt>Diversi kompiti tal-NLP jeħtieġu r-rappreżentazzjoni effettiva tad-dokumenti tat-test. Arora et al.,2017 juru li l-medja ppeżata sempliċi tat-tixjiħ tal-vetturi tal-kliem spiss taqbeż il-mudelli newrali. SCDV (Mekala et al., 2017) testendi aktar dan mis-sentenzi għad-dokumenti billi tuża raggruppament dgħajjef u żgħir fuq vetturi tal-kliem ikkumputati minn qabel. Madankollu, iż-żewġ tekniki jinjoraw il-karattru poliżiku u kuntestwali tal-kliem. In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach.  Nirriżultaw li l-inkorporazzjonijiet tagħna jaqbżu l-SCDV oriġinali, il-BERT ta’ qabel il-ferrovija, u diversi linji oħra fuq ħafna settijiet ta’ dejta ta’ klassifikazzjoni. Wealso juri wkoll l-effettività tal-inkorporazzjonijiet tagħna f’kompiti oħra, bħall-paragun tal-kunċetti u s-similarità tas-sentenzi. Barra minn hekk, aħna nuru li SCDV+BERT(ctxd) irriżultaw minn BERT b’aġġustament fiss u approċċi differenti ta’ inkorporazzjoni fix-xenarji b’dejta limitata u ftit eżempji ta’ shot biss.</abstract_mt>
      <abstract_pl>Kilka zadań NLP wymaga skutecznej reprezentacji dokumentów tekstowych. Arora i al.,2017 pokazują, że proste ważone przeciwstarzenie wektorów słowa często przewyższa modele neuronowe. SCDV (Mekala et al., 2017)dodatkowo rozszerza to od zdań do dokumentów, stosując miękkie i rzadkie klastrowanie nad wstępnie obliczonymi wektorami słów. Jednakże obie techniki ignorują polizemię i kontekstowy charakter słów. W tym artykule rozwiązujemy ten problem, proponując SCDV+BERT(ctxd), prostą i skuteczną nienadzorowaną reprezentację, która łączy skontextualizowane BERT (Devlin et al., 2019) oparte na osadzeniu słów dla dyambiguacji sensu słowa z podejściem do miękkiego klastrowania SCDV. Nasze osadzenia przewyższają oryginalne SCDV, BERT i kilka innych linii bazowych na wielu zbiorach danych klasyfikacyjnych. Wykazujemy również efektywność osadzenia w innych zadaniach, takich jak dopasowanie koncepcji i podobieństwo zdań. Dodatkowo pokazujemy, że SCDV+BERT(ctxd) wydaje się lepiej dostroić BERT i różne procesy osadzania w scenariuszach z ograniczonymi danymi i niewieloma przykładami ujęć.</abstract_pl>
      <abstract_ro>Mai multe sarcini PNL necesită reprezentarea eficientă a documentelor text. Arora et al., 2017 demonstrează că îmbătrânirea medie ponderată simplă a vectorilor de cuvânt depășește frecvent modelele neuronale performante. SCDV (Mekala et al., 2017) extinde în continuare acest lucru de la propoziții la documente prin utilizarea de cluster-ing soft și rar peste vectori de cuvinte pre-calculați. Cu toate acestea, ambele tehnici ignoră caracterul polisemiși contextual al cuvintelor. În acest articol, abordăm această problemă propunând SCDV+BERT(ctxd), o reprezentare simplă și eficientă fără supraveghere care combină încorporarea cuvintelor bazate pe BERT (Devlin et al., 2019) pentru dezambiguizarea sensului cuvintelor cu abordarea clusterizării soft SCDV. Încorporările noastre depăşesc performanţele SCDV iniţiale, BERT pre-train şi mai multe alte linii de bază pe multe seturi de date de clasificare. De asemenea, să demonstrăm eficiența încorporărilor noastre în alte sarcini, cum ar fi potrivirea conceptului și similitudinea propozițiilor. În plus, arătăm că SCDV+BERT(ctxd) depășește performanțele optime ale BERT și diferite proiecte de încorporare în scenarii cu date limitate și doar câteva exemple de fotografii.</abstract_ro>
      <abstract_mn>НЛП-ын олон даалгаврууд текст баримтыг дахин дахин өгүүлэх хэрэгтэй. Арора и аль.,2017 онд энгийн хэмжээний векторуудын хөгжүүлэх нь ихэвчлэн үйл ажиллагааны загвараас илүүтэй. SCDV (Mekala et al., 2017) үүнийг өгүүлбэрээс docu-ments руу нэмэгдүүлдэг. Өмнөх тооцоологдсон үгний векторуудын хувьд бага болон богино кластерийг ашиглаж байна. Яаж ч гэсэн, хоёр техник нь полицемийн, орчин үгний харилцааны харилцааныг эсэргүүцдэг. Энэ асуудлыг бид SCDV+BERT(ctxd) гэсэн санал болгоход энгийн, эффективнэй, удирдлагагүй илтгэл зохион байгуулалт хийдэг. Энэ асуудлыг солилцуулсан BERT (Devlin et al., 2019) суурь зохион байгуулалт нь SCDV бага хэмжээний арга хэмжээнд зохион байгуулагддаг. Бид өөрсдийнхөө хөрөнгө оруулалтын эхний SCDV, BERT-ын өмнө дасгал хөдөлгөөн болон олон төрлийн өгөгдлийн санд олон өөр хэсгүүдийг хийдэг гэдгийг харуулж байна. Уилзо өөр ажил дээр үр дүнтэй байдлыг харуулж байна. Яг л ойлголт тоглох болон өгүүлбэртэй адилхан. Мөн бид SCDV+BERT(ctxd) бүтээгдэхүүний үр дүнг BERT болон хязгаарлагдсан өгөгдлийн хувилбар болон хэдэн зураг жишээлүүдийн хувилбарт өөр өөр нэгдэж байгааг харуулж байна.</abstract_mn>
      <abstract_no>Fleire NLP-oppgåver treng det effektive gjenoppretting av tekstdokument. Arora et al.,2017 viser at enkelt vekt lagring av ordvektorar ofte utfører utviklingsmedler. SCDV (Mekala et al., 2017) utvidar dette frå setningar til docu-ments ved å bruka måk og sparse cluster ing over forrekna ordvektorar. Kor mykje, begge teknikkar ignorerer polysemen og kontekst teikn av ord. I denne området er vi oppretta dette problemet ved å foreslå SCDV+BERT(ctxd), eit enkel og effektivt ikkje-oversikt representasjon som kombinerer con-textualisert BERT (Devlin et al., 2019) basedword som innebygger for ordfølelse disambigua-tion med SCDV-måk clustering tilnærming. Vis at innbyggingane våre utfører origi nal nal SCDV, før-treng BERT, og fleire andre ulike linjer på mange klassifikasjonsdata. Wealso viser innbygginga våre effektivt på andre oppgåver, slik som konsept for samsvar med setningar. I tillegg viser vi at SCDV+BERT(ctxd) er utgått av finn-tune BERT og ulike innbygging av ap-proaches i scenarioar med begrensede data og berre få eksemplar på fotografikk.</abstract_no>
      <abstract_si>NLP වැඩක් වෙනස් වැඩක් ලිපින්ත ලිපින්ත ලිපින්ත වාර්තාව ආපහු ප්‍රතික්‍රියාත්මක කරන්න ඕනි. Arora et al.,2017 පෙන්වන්නන්න පුළුවන් විදිහට වෙක්ටර් වලින් ප්‍රමාණයක් නැති විදිහට ප්‍රමාණය කරනවා කියලා. SCDV (Mecala et al., 2017) කොහොමද කියලා, දෙන්නම් තාක්ෂණය පොලිසිමියාන් වචන වගේ සංවේදනය අවධානය කරන්න. In this spaper, we address this challenge by preaching SCDV+BERT(ctxm), a strain and ffective un-supervised reposition that combinates Con-textuolized BERT (Devlin et al., 2019) basedword integrating for word sensing disombgua-tion with SCDV software cluster ing approach. අපි පෙන්වන්නම් අපේ ඇම්බෙන්ඩින්ග් ප්‍රධාන SCDV, BERT ප්‍රධාන ප්‍රධාන ප්‍රධානය, සහ වෙනස් විශේෂණ දත්ත සේට් වලි විල්සෝ පෙන්වන්නේ අනිත් වැඩවල් වලට අපේ සංවේදනය ප්‍රශ්නය කරනවා, ඒ වගේම සංවේදනය සහ වාක්ය වගේම. ඒ වගේම, අපි පෙන්වන්නේ SCDV+BERT(ctxm) නිර්භාවිත BERT වලින් වෙනස් ප්‍රශ්නයක් වලින් ප්‍රශ්නයක් තියෙනවා සීමාන්‍ය දත්ත සහ ප්‍රශ්නයක</abstract_si>
      <abstract_sr>Nekoliko zadataka NLP-a je potrebno efikasno ponovno rešiti tekstske dokumente. Arora et al.,2017. pokazuje da je jednostavno težino održavanje vektora rečenih vektora često iznad izvornih modela. SCDV (Mekala et al., 2017) dodatno proširi ovo od rečenica na docu-ments koristeći meke i rezervne skustere preko pre-računalnih vektora rečenica. Kako god, obe tehnike ignoriraju polisemijski kontekstualni karakter reèi. U toj oblasti, rješavamo se ovom pitanju predloženjem SCDV+BERT(ctxd), jednostavnom i efikasnom nedovoljnom predstavljanju koja kombinuje kontekstualiziranu BERT (Devlin et al., 2019) baznu ploču koja se uključuje za disambigua-ciju reèima sa mekim pristupom SCDV-a. Pokazujemo da naše uključenje iznosi originalni SCDV, pre vožnje BERT i nekoliko drugih bazena na mnogim klasifikacijskim podacima. Wealso pokazuje naše uključenje učinkovitosti na druge zadatke, kao što su koncept odgovaranja i sličnost rečenica. Osim toga, pokazujemo da je SCDV+BERT(ctxd) iznad izvedbi BERT-a i različite ugrađivanje jabuka u scenarijima sa ograničenim podacima i samo nekoliko primjera snimaka.</abstract_sr>
      <abstract_so>Shaqooyin badan oo NLP ah waxay u baahan yihiin dib u soo celinta warqadaha qoraalka. Arora et al.,2017 waxay muujiyaan in si fudud u miisaamay waxey si fudud u taqaan wadooyinka hadalka oo marar badan u samaysan qaababka Yurub. SCDV (Mekala et al., 2017) Si kastaba ha ahaatee labada teknikada ayaa ka jeeda qoraalka la yidhaahdo polysemi iyo xarafka qoraalka ah. Taaspaper, waxan ayaannu ka sheekaynaynaa suurtagalka SCDV+BERT(ctxd), kaas oo ah mid fudud oo faa'iido ah oo aan ilaalinayn, kaas oo ku soo ururiya con-textualized BERT (Devlin et al., 2019) Basdword oo ku qoran word disambigua-tion with SCDV approach soft cluster. Weshow in boosasheenna ay ka muuqato origi-nal SCDV, pre-train BERT, iyo qaar kale oo ku qoran koorasyo kala duduwan. Sidoo kale waxaynu muujinnaa shaqaalahayaga saameyn u leh shaqaalaha kale, tusaale ahaan fikrada u eg isku mid ahaanshaha iyo imtixaanka. Waxaa kale oo aan muujinnaa in SCDV+BERT(ctxd) outperformsfine-tune BERT iyo sidoo kale kooxaha ap-wacdiyo kala duduwan oo ay ku haystaan macluumaad xad ah iyo tusaalooyin yar oo ganacsi ah.</abstract_so>
      <abstract_ta>பல NLP பணிகள் உரை ஆவணங்களின் விருப்பமான மீண்டும் உணர்வு தேவை. அரோரா மற்று அல்.,2017 சுலபமான வார்த்தை வெக்டர்கள் சராசரி வயதாக எடுத்தார்கள் என்பதை குறிப்பிடுகிறது. SCDV (மெக்கலா மற்றும் அல்., 2017) முன் கணக்கிடப்பட்ட வார்த்தை வெக்டார்களை பயன்படுத்தி மென் மற்றும் வெக்டார்களை பயன்படுத்தி இதை மேலும் வாக்க How-ever, both techniques ignore the polysemyand contextual character of words. திஸ்பேப்பரில், SCDV+BERT( ctxd), ஒரு எளிய மற்றும் பயன்படுத்தப்படாத கண்காணிக்கப்படாத பகுதியை நாம் இந்த பிரச்சினையை விளக்குகிறோம். இது கூட்டு நூல் நிறுவப்பட்ட BERT (டெப்லின் et al., 2019) க எங்கள் உள்ளீடுகள் ஓரிஜி-நாள் SCDV, முன் பயிற்சி BERT, மற்றும் பல வகுப்பு தரவுத்தளங்களில் பல மேற்கோடுக்கோடுகள். நாம் மற்ற பணிகளில் எங்கள் உள்ளடக்கங்களுக்கு வெளிப்படையான தெளிவாக்கம் காட்டுகிறோம், பொருத்தம் மற்றும் வாக்கியத்தி மேலும், SCDV+BERT(ctxd) வெளியிடும் செயல்பாடு-tune BERT மற்றும் வேறு வித்தியாசமான பிப் பகிர்வாசிகள் காண்பிக்கிறோம் மற்றும் எல்லாம் தகவல்</abstract_ta>
      <abstract_ur>بہت سی NLP تاسکیوں کی تاسکیٹ ڈکومکیٹ کے مطابق دوبارہ سنتی کی ضرورت ہے. Arora et al.,2017 دکھاتا ہے کہ کلمات ویکتروں کی ساده وزن کی سنگی ہے، بہت سے زیادہ اضافہ کرنے والی موڈل. SCDV (Mekala et al., 2017) اس کو مفصل سے ڈاکو منٹ تک پہنچا دیتا ہے کہ نرم اور نرم کلسٹر کا استعمال کرتا ہے۔ کہیں بھی، دونوں تکنیک کلمات کی پولیسیم اور متوسط شخصیت کو غافل کرتے ہیں۔ اس جگہ میں ہم اس مسئلہ کو مشورہ کریں گے SCDV+BERT(ctxd) کی پیشنهاد سے، ایک ساده اور اثبات غیر نظارت کی تصویر جو con-textualized BERT (Devlin et al., 2019) بنسٹ ورڈ کو جمع کرتا ہے، جو کلمات کے لئے SCDV نرم کلسٹرینگ طریقے کے ساتھ مکمل ہوتا ہے۔ ہم نشان دیتے ہیں کہ ہمارے ایمبڈینگ آغاز-نال SCDV، پہلے ٹرین BERT اور بہت سی کلاسیکٹ ڈیٹ سٹ پر بہت سی دوسری صفحے ہیں۔ ویلسو ہمارے انڈینگ کو دوسرے کاموں پر اثرات کے ساتھ دکھاتا ہے، جیسے مفصل مطابق اور جماعت کی برابری. اور اس کے علاوہ ہم نشان دیتے ہیں کہ SCDV+BERT(ctxd) فین-ٹون BERT اور مختلف مصنوعی اندازے کے مطابق مختلف مصنوعی کے مطابق اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا ا</abstract_ur>
      <abstract_sv>Flera nationella handlingsplaner kräver en effektiv återgivning av textdokument. Arora et al., 2017 visar att enkel viktad averaging av ordvektorer ofta överträffar performanceneurala modeller. SCDV (Mekala et al., 2017) utökar detta ytterligare från meningar till dokument genom att använda mjuk och gles klustring över förberäknade ordvektorer. Båda teknikerna ignorerar dock ordens polysemiska och kontextuella karaktär. I den här artikeln tar vi upp detta problem genom att föreslåSCDV+BERT(ctxd), en enkel och effektiv icke-övervakad representation som kombinerar kontextualiserad BERT (Devlin et al., 2019) baserad ordinbäddning för ordförnimmelse och SCDV mjuk klustring metod. Vi visar att våra inbäddningar presterar bättre än original SCDV, pre-train BERT och flera andra baslinjer på många klassificeringsdatauppsättningar. Vi ska också visa vår inbäddning effektivitet på andra uppgifter, såsom konceptmatchning och meningsliknande. Dessutom visar vi att SCDV+BERT(ctxd) presterar bättre än att finjustera BERT och olika inbäddningsapproacher i scenarier med begränsad data och endast få bilder exempel.</abstract_sv>
      <abstract_uz>Koʻp nechta NLP vazifalar matn hujjatlarini qayta takrorlash kerak. Arora et al.,2017 oddiy so'z vektorining o'rtacha o'rtacha o'smirligini ko'rsatish mumkin. Name Shunday qilib, ikkita teknologiya polisemiz va ma'lum so'zlarning har xil harfni o'zgartiradi. Bu muammolarni SCDV+BERT(ctxd) rivojlanish orqali, oddiy va effektiv nazorat qilmagan representiyatlarni boshqarish mumkin. Bu biz SCDV va SCDV soʻzni boshqarish usuli bilan birlashtirish uchun oddiy va ishlab chiqaruvchi bo'lgan BERT (Devlin et al., 2019). Weshow bizning tuzilishimiz origi-ichki SCDV, BERT oldin taqdim, va ko'p ta'lim maʼlumotlar tarkibidagi bir necha boshqaruvchilar. Va biz boshqa vazifalarga ishlashimizni tasavvur qilamiz, tushuning o'xshash va gapirish bir xil. Ko'rsatganda, SCDV+BERT(ctxd) outperformsfine-tune BERT (BERT) va scenariosdagi boshqa ap-prokaslari bilan chegara maʼlumot va faqat bir necha shot misollari bilan ishlatiladi.</abstract_uz>
      <abstract_vi>Một số công việc của Đảng NLP cần có sự đại diện hiệu quả của văn bản. Arora et Al,2007 chứng minh rằng giá trị trung bình đơn giản của giá trị cổ động từ trường phổ biến thường vượt trội. Tổ chức SCDV (Meuca et al., 2007) nới rộng từ câu này sang biên tập bằng cách sử dụng cụm từ mềm mỏng và mỏng hơn nhiều so với giá trị từ cổ xưa. Tuy nhiên, cả hai kỹ thuật lờ đi tính chất lục địa và ngữ cảnh của từ. Trong nghiên cứu này, chúng tôi giải quyết vấn đề này bằng cách đề xuất SCDV+BERT (ctyxd), một mô hình đơn giản và hiệu quả không được giám sát, kết hợp kết hợp kết cấu kết cấu kết cấu kết hợp BERT (Devlin et al., 209) nền từ ngữ: Chúng tôi làm cách khác để hoàn thành tập tin khủng long khai mạc, Trước mùa đông BERT, và nhiều nền khác trên nhiều tập tin phân loại. chứng minh sự tác hợp hiệu quả của chúng ta với các công việc khác, như khả năng kết hợp và kết án giống nhau. Ngoài ra, chúng tôi cho thấy nhóm SCDV+BERT(ctyxd) thành công vượt trội cho giải mã BERT và nhiều trường hợp khác nhau trong các kịch bản có giới hạn dữ liệu và chỉ vài ví dụ.</abstract_vi>
      <abstract_bg>Няколко задачи от НЛП се нуждаят от ефективно представяне на текстови документи. Арора и др., 2017 демонстрират, че простата претеглена средна възраст на векторите на словото често надвишава перфектните неврални модели. SCDV (Mekala et al., 2017) допълнително разширява това от изречения до документи, като използва меко и рядко клъстериране върху предварително изчислени вектори на думи. Както и да е, двете техники игнорират полисемиятконтекстуалния характер на думите. В този раздел ние разглеждаме този проблем, като предлагаме просто и ефективно неконтролирано представяне, което съчетава контекстуализирано BERT (Девлин и др., 2019), базирано на вграждане на думи за изясняване на смисъла на думата с подхода на меко клъстериране SCDV. Ние показваме, че нашите вграждания превъзхождат оригиналните SCDV, предтренажните BERT и няколко други базови линии в много класификационни набори от данни. Ние също така демонстрираме ефективността на вграждането си върху други задачи, като съвпадение на концепции и сходство с изречения. В допълнение, ние показваме, че SCDV+BERT(ctxd) превъзхожда фината настройка на BERT и различните вграждащи приложения в сценарии с ограничени данни и само няколко примера за снимки.</abstract_bg>
      <abstract_da>Flere NLP-opgaver kræver effektiv repræsentation af tekstdokumenter. Arora et al., 2017 viser, at simpel vægtet averaging af ordvektorer ofte overperformanceneurale modeller. SCDV (Mekala et al., 2017) udvider dette yderligere fra sætninger til dokumenter ved at anvende blød og sparsom klyngning over forudberegnede ordvektorer. Begge teknikker ignorerer imidlertid ordenes polysemogkontekstuelle karakter. I denne beskrivelse behandler vi dette problem ved at foreslå SCDV+BERT(ctxd), en enkel og effektiv ikke-overvåget repræsentation, der kombinerer kontekstualiseret BERT (Devlin et al., 2019) baseret ordindlejring for ordforståelse med SCDV soft clustering tilgang. Vi mener, at vores indlejringer på mange klassificeringsdatasæt overgår oprindelige SCDV, BERT forud for træningen og flere andre basislinjer. Vi skal også demonstrere vores indlejringer effektivitet på andre opgaver, såsom konceptmatching og sætning lighed. Desuden viser vi, at SCDV+BERT(ctxd) overgår resultaterne finjusteret BERT og forskellige indlejrings-processer i scenarier med begrænsede data og kun få billeder eksempler.</abstract_da>
      <abstract_nl>Verschillende NLP-taken vereisen een effectieve representatie van tekstdocumenten. Arora et al.,2017 tonen aan dat eenvoudige gewogen aver-aging van woordvectoren vaak beter presteert dan neurale modellen. SCDV (Mekala et al., 2017)breidt dit verder uit van zinnen naar documenten door gebruik te maken van zachte en schaarse clustering over vooraf berekende woordvectoren. Beide technieken negeren echter het polysemische en contextuele karakter van woorden. In dit artikel pakken we dit probleem aan door SCDV+BERT(ctxd) voor te stellen, een eenvoudige en effectieve niet-begeleide representatie die gecontextualiseerde BERT (Devlin et al., 2019) basedword embedding combineert voor woordsense disambigua met SCDV soft clustering aanpak. We laten zien dat onze embeddings beter presteren dan originele SCDV, pre-train BERT en verschillende andere baselines op veel classificatiedatasets. We demonstreren ook onze effectiviteit bij andere taken, zoals conceptmatching en zinsgelijkenis. Daarnaast laten we zien dat SCDV+BERT(ctxd) beter presteert dan BERT en verschillende embedding-processen in scenario's met beperkte data en weinig shots voorbeelden.</abstract_nl>
      <abstract_hr>Nekoliko zadataka NLP-a potrebno je učinkovito ponovno kazanje tekstskih dokumenta. Arora et al., 2017. pokazuje da je jednostavno težino održavanje vektora riječi često iznad izvornih modela. SCDV (Mekala et al., 2017) dalje to proširi od kazne na docu-ments koristeći meke i rezervne skupine preko predračunalnih vektora riječi. Kako god, obje tehnike ignoriraju polisemijski kontekstualni karakter riječi. U toj oblasti, riješimo ovaj problem predloženjem SCDV+BERT(ctxd), jednostavnom i učinkovitom nedovoljnom predstavljanju koji kombinira kontekstualiziranu BERT (Devlin et al., 2019) baznu stroj koja se uključuje za disambigua-ciju riječi s mekim pristupom SCDV-a. Mi pokazujemo da naše ugrađenje iznosi originalni SCDV, pre vožnje BERT i nekoliko drugih bazena na mnogim podacima klasifikacije. Wealso pokazuje naše uključenje učinkovitosti na druge zadatke, poput koncepta odgovaranja i sličnosti rečenica. Osim toga, pokazujemo da je SCDV+BERT(ctxd) nadmažena djelovanja BERT-a i različitih ugrađenih prolaza u scenarijima s ograničenim podacima i samo nekoliko primjera snimaka.</abstract_hr>
      <abstract_ko>여러 NLP 작업을 수행하려면 텍스트 문서를 효과적으로 표현해야 합니다.아로라 등은 2017년 단어 벡터의 단순가중평균값이 보통 자연모델보다 우수하다는 것을 증명했다.SCDV(Mekala et al., 2017)는 미리 계산된 단어의 벡터에 대해 연성 희소 집합을 실시하여 문장에서 문서로 확장한다.그러나 이 두 가지 기술은 모두 단어의 다의성과 상하문 특징을 소홀히 했다.본고에서 우리는 CDV+BERT(ctxd)를 제시함으로써 이 문제를 해결한다.ctxd는 간단하고 효과적인 무감독 표현으로 상하 문화 BERT(Devlin et al., 2019)의 DWORD를 바탕으로 단어의 뜻을 없애는 방법과SCDV 소프트 집합 방법을 결합시킨다.우리는 많은 분류 데이터 집합에 삽입된 것이 원시 SCDV, 예비 훈련 BERT, 기타 일부 기선보다 우수하다는 것을 보여 주었다.우리는 개념 일치와 문장 유사성 등 다른 임무에 대한 효과적인 삽입도 보여 주었다.그 밖에 SCDV+BERT(ctxd)는 데이터가 유한하고 소량의 렌즈만 있는 상황에서 BERT와 서로 다른 삽입 ap 방법보다 우수하다는 것을 증명했다.</abstract_ko>
      <abstract_fa>چندتا کار NLP نیاز به بازگرداندن مدارک متن دارد. Arora et al.,۲۰۱۷ نشان می دهد که سنگین سنگین سنگین سنگین ویکتورهای کلمه اغلب از مدل های عملکرد غیر قابل توجه است. SCDV (Mekala et al., 2017) این را از جمله‌ها به دوک-ments به وسیله استفاده از کلاستر نرم و نرم بر روی ویکتورهای کلمه پیش‌محاسبه می‌کند. هرچقدر، هر دوی تکنیک ها شخصیت پلیسمی و متوسط کلمات را نادیده می دهند. در این مسئله، ما با پیشنهاد SCDV+BERT(ctxd) یک نمایش ساده و تاثیر غیر نظارت‌کننده‌ای درباره‌ی این مسئله را حل می‌کنیم که با توجه به عنوان حس نامبیگو-tion با دستور نرم کلاستر SCDV متصل شده BERT (Devlin et al., 2019) را ترکیب می‌کند. ما نشان می دهیم که وسیله‌های ما در مجموعه‌های بسیاری از اطلاعات‌شناسی‌های اولیه‌ی SCDV، BERT پیش‌آموزش، و چند تن دیگر را در مجموعه‌های مختلف انجام می‌دهند. ویلسو نشان می‌دهد که درگیری‌هایمان بر روی کارهای دیگر موثر است، مثل مشابه‌ای که مشابه‌ای و جمله‌ها دارند. علاوه بر این، ما نشان می دهیم که SCDV+BERT(ctxd) بیشتر از اجرای صفحه صفحه‌های صفحه‌ای که داده‌های محدودیت دارند و فقط چند مثال شلیک‌های مختلف دارند.</abstract_fa>
      <abstract_de>Mehrere NLP-Aufgaben erfordern die effektive Repräsentation von Textdokumenten. Arora et al.,2017 zeigen, dass einfache gewichtete Abwertung von Wortvektoren häufig neuronale Modelle übertrifft. SCDV (Mekala et al., 2017)erweitert dies weiter von Sätzen zu Dokumenten, indem weiche und spärliche Clustering über vorberechnete Wortvektoren verwendet werden. Allerdings ignorieren beide Techniken den polysemischen und kontextuellen Charakter von Wörtern. In diesem Artikel behandeln wir dieses Problem, indem wir SCDV+BERT(ctxd) vorschlagen, eine einfache und effektive nicht-überwachte Darstellung, die kontextualisierte BERT (Devlin et al., 2019) basierte Einbettung für Wortsinn-Disambigua mit SCDV Soft Clustering Ansatz kombiniert. Wir zeigen, dass unsere Einbettungen bei vielen Klassifikationsdatensätzen die ursprünglichen SCDV, BERT-Vortrainings und mehrere andere Basislinien übertreffen. Wir demonstrieren unsere Einbettungseffektivität auch bei anderen Aufgaben wie Concept Matching und Satzähnlichkeit. Darüber hinaus zeigen wir, dass SCDV+BERT(ctxd) die Feinabstimmung von BERT und verschiedene Einbettungsapplikationen in Szenarien mit begrenzten Daten und nur wenigen Shots übertrifft.</abstract_de>
      <abstract_tr>NLP birnäçe zada metin senediň täzeliklerini ýene-täzeliklerini gerek. 2017-nji Arora et al.2017'de basit ağırlı vektörler kelimelerinin çoğunlukla etkisiz modellerinden daha yüksek olduğunu gösteriyor. SCDV (Mekala et al., 2017)further extends this from sentence to docu-ments by employing soft and sparse cluster-ing over pre-computed word vectors. Nähili bolsa, her iki teknikiň polisemiýa we konsekst sözlerin karakterini görmezden gelenok. Bu sahypada, biz bu meseleyi SCDV+BERT(ctxd) teklif eden basit ve etkili bir şekilde gözetlememiş bir temsil çözümleri birleştirerek, WIRT (Devlin et al., 2019) söz duygu üçin birleştirilen baz sözlerimizi çözerek, SCDV yumuşak klusterin yaklaşımı ile birleştirmekten emin bir şekilde çözeriz. Biziň düzümlerimiz origi nal SCDV, BERT öňünden otlydygyny we köp taýýarlama düzümlerinde birnäçe taýýarlama netijesinde çykýandygyny görkez. Wasp biziň daşarymyzyň içinde başga işlerde etkinlik gabdalygymyzy, meňzeş we sözleriň meňzeşligini görkez. Munuň üçin, SCDV+BERT(ctxd) BERT'yň üstüne çykyşynyň üstini görkez we senaryoýla çykyş berüvleri bilen diňe birnäçe eserler bar.</abstract_tr>
      <abstract_af>Verskeie NLP-opdragte benodig die effektief herstelling van teksdokumente. Arora et al.,2017 wys dat eenvoudige gewigte stoor-aging van woordevektore dikwels uitgevoerde sneuringsmodelle. SCDV (Mekala et al., 2017) verder verbeter dit van setnings na docu-ments deur sagte en sparse cluster-ing te gebruik oor voorafreken woord vektore. Hoe-ooit, beide teknike ignoreer die poliseme en contextual karakter van woorde. In hierdie spaper, ons adres hierdie probleem deur die voorstel van SCDV+BERT( ctxd), ân eenvoudig en effektief ongesuperviseerde voorstelling wat con- textualiseerde BERT (Devlin et al., 2019) gebasewoord ingesluit vir woord sin disambigua- tion met SCDV sagte clustering toegang. Ons wys dat ons inbêding uitvoer oorspronklike SCDV, voorstrein BERT, en verskeie ander besonderhede op baie klassifikasie datastelle. Wealso wys ons inbêdings effektief-belangheid op ander taak, soos konsepte ooreenstemmende en setgelykheid. In addition, we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data and only few shots examples.</abstract_af>
      <abstract_id>Several NLP tasks need the effective repre-sentation of text documents. Arora et al.,2017 menunjukkan bahwa keseluruhan pemasaran berat sederhana vektor kata sering melebihi model sneural. SCDV (Mekala et al., 2017) melanjutkan ini dari kalimat ke dokumen dengan menggunakan berkumpul lembut dan ringan atas vektor kata pre-komputasi. How-ever, both techniques ignore the polysemyand contextual character of words. In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach.  Weshow that our embeddings outperform origi-nal SCDV, pre-train BERT, and several otherbaselines on many classification datasets.  Wealso menunjukkan kemampuan kita pada tugas lain, seperti konsep match-ing dan kalimat yang sama. In addition,we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</abstract_id>
      <abstract_sq>Disa detyra të NLP-së kanë nevojë për përfaqësim të efektshëm të dokumenteve teksti. Arora et al.,2017 demonstrojnë se mesatarja e peshuar e mesatare e vektorëve të fjalës shpesh kalon modelet neuronale. SCDV (Mekala et al., 2017)e zgjeron më tej këtë nga fjalët në dokumente duke përdorur grupe të buta dhe të pakta mbi vektorët e fjalëve të parallogaritur. How-ever, both techniques ignore the polysemyand contextual character of words. Në këtë spaper, ne e trajtojmë këtë çështje duke propozuar SCDV+BERT(ctxd), një përfaqësim i thjeshtë dhe efektiv pa mbikqyrur që kombinon tekstualizuar BERT (Devlin et al., 2019) të mbështetur në përfshirje për fjalë kuptimi të çambiguation me metodën e grupimit të butë SCDV. Ne tregojmë se përfshirjet tona kalojnë SCDV origjinale, para trenit BERT, dhe disa baza të tjera në shumë grupe të dhënash klasifikimi. Wealso demonstrate our embeddings effective-ness on other tasks, such as concept match-ing and sentence similarity. In addition,we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</abstract_sq>
      <abstract_sw>Kazi kadhaa za NLP zinahitaji kuchukuliwa tena kwa ufanisi wa nyaraka za maandishi. Arora et al.,2017 imeonyesha kuwa rahisi ilikuwa na uzee wa wastani wa vectors wa maneno mara nyingi wakifanya mifano ya kijamii. SCDV (Mekala et al., 2017) inaongezea tena hii kutoka hukumu hadi madaktu kwa kutumia mstari mwepesi na kupunguza viungo vinavyotokea vector za maneno yaliyohesabiwa. Kwa namna gani, mbinu zote hizi zinazipuuza mhalisi na tabia za kawaida za maneno. Katika kipindi hiki, tunazungumzia suala hili kwa pendekezo la SCDV+BERT(ctxd), uwakilishi rahisi na wenye ufanisi usiotangaliwa ambao unaunganisha BERT (Devlin et al., 2019) kituo cha msingi kinachojumuisha kwa maneno yanayohusiana na mfumo wa mfumo wa ufundi wa SCDV. Weshow ambazo maeneo yetu yanaendesha SCDV ya asili ya origi, mafunzo ya kabla ya treni BERT, na mistari kadhaa ya watu kwenye seti nyingi za usambazaji. Pia tulionyesha maeneo yetu yenye ufanisi katika kazi nyingine, kama vile dhana inayofanana na hukumu inayofanana. Zaidi ya hayo, tunaonyesha kwamba SCDV+BERT(ctxd) kuonyesha usoni-tune BERT na wapiganaji mpya tofauti katika mitazamo yenye takwimu isiyo na mifano michache tu ya risasi.</abstract_sw>
      <abstract_az>Bir neçə NLP işləri mətn belələrinin yenidən ifadə edilməsi lazımdır. Arora et al.,2017-ci ilə, sadəcə ağırlı vektörlərin ağırlığını göstərir. SCDV (Mekala et al., 2017) bunu cümlələrdən docu-ments vasitəsilə, ön-hesaplanmış söz vektörlərinin üstündə yumuşaq və küçük cluster-ing istifadə edərək uzaqlaşdırır. Necə olaraq, hər iki tekniki polisimlə müxtəlif sözlərin karakterini görməz. Bu səbəbdə, SCDV+BERT(ctxd) təbliğ etmək üçün bu məsələni çəkirik. Bu çox basit və efektiv təhlükəsizlik edilməmiş bir göstəricisidir ki, con-textualized BERT (Devlin et al., 2019) sözləri SCDV yumuşaq clustering approach ilə birləşdirir. Biz göstəririk ki, bizim inşallarımız bir çox klasifikasiya verilən qurğularda orijinal SCDV, BERT-dən əvvəl tələb edir və bir çox başqa dəyişiklik qurğularında. Wealso bizim inşallarımızı başqa işlərdə effektiv olmağımızı göstərir, məsələlər eşitmək və cümlələr kimi. Əvvəlcə, SCDV+BERT(ctxd) müxtəlif performances BERT və müxtəlif məlumatlarla müəyyən məlumatlarda fərqli məlumatlar və yalnız bir neçə fərqli məsəllərə istifadə edirik.</abstract_az>
      <abstract_bn>বেশ কয়েকটি এনএলপি কাজের লেখা ডকুমেন্টের কার্যকর পুনরায় প্রতিবেদন দরকার। আরোরা এন্ট আল, ২০১৭ প্রতিবাদ করেছে যে সাধারণত শব্দ ভেক্টরের গড় বয়সের মাধ্যমে সাধারণত উৎপাদনের মডেল। এসসিডিভি (মেকালা এন্ট ২০১৭) আরো এই বিষয়টি ডোকুর কারাদণ্ড থেকে বাড়িয়ে দিয়েছেন ডকুমেন্টের কাছ থেকে নম্রভাবে ব্যবহার করে গণনানোর আগে  যেভাবেই, দুটো প্রযুক্তিগুলো পলিসেমিয়ান এবং প্রাক্তন বাক্যের চরিত্র উপেক্ষা করে। In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach.  ওয়েস্কো যে আমাদের বিভিন্ন অরিজি-নাল এসসিডিভি, প্রাক্তন ট্রেন বিবের্ট এবং অনেক বিভিন্ন গ্রাফিকেশন ডাটাসেটে বেশ কিছু বেসে আমরা অন্যান্য কাজের উপর আমাদের প্রতিযোগিতার কার্যকর প্রদর্শন করেছি, যেমন ধারণা মিল এবং শাস্তি একই রকম। এছাড়াও, আমরা দেখাচ্ছি যে SCDV+BERT (ctxd) আউটপার্সিফিন-tune BERT এবং বিভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন প্রোচিকের সাথে সীমিত তথ্য এবং শুধু</abstract_bn>
      <abstract_hy>Շատ ՆԼՊ-ի առաջադրանքներ կարիք ունեն տեքստի փաստաթղթերի արդյունավետ ներկայացումը: Arora et al.,2017 demonstrate that simple weighted aver-aging of word vectors frequently outperformsneural models.  ՍԿԴՎ (Meical et al., 2017) ավելի շատ տարածում է սա նախադասություններից դեպի փաստաթղթեր՝ օգտագործելով փափուկ և փոքր խմբավորումներ նախահաշվարկված բառերի վեկտորների վրա: Ինչևէ, երկու տեխնիկաները անտեսում են բառերի պոլիսեմիան և կոնտեքստոնալ բնույթը: Այս պայմաններում մենք լուծում ենք այս խնդիրը առաջարկելով, որ ՍԿԴՎ+ԲԵՌՏ( կկՏՔՍ), մի պարզ և արդյունավետ առանց վերահսկվող ներկայացուցիչ, որը համադրում է կոնտեքստալիզացված ԲԵՌՏ (Devin et al., 2019) բառի զգացմունքի դիզաբիգուգացիայի համար բառի դիզաբիգուգացիայի և ՍԿԴՎ Մենք ցույց ենք տալիս, որ մեր ներդրումները գերազանցում են սկզբնական ՍՔԴՎ-ը, BER-ը նախապատրաստման և բազմաթիվ այլ հիմքերը բազմաթիվ դասակարգման տվյալների համակարգերում: Վիելսոն ցույց է տալիս, որ մեր ներդրումները արդյունավետ են այլ խնդիրների վրա, ինչպիսիք են օրինակ համեմատությունը և նախադասությունների նմանությունը: In addition,we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</abstract_hy>
      <abstract_am>Several NLP tasks need the effective repre-sentation of text documents. Arora et al.,2017 demonstrate that simple weighted aver-aging of word vectors frequently outperformsneural models.  SCDV (Mekala et 2017) ይህንን ከፍርድ ወደ ዶኪዎች አካባቢዎች እና ቀላል እና መቆጣጠር በቁጥጥር የተቆጠሩ ቃላት vectors በመቀጠል ይዘረጋል፡፡ እንዴት እንደሆነ፣ ሁለቱ ቴክኖክቶች የፖሊሲም እና የቃላትን ሁኔታ ጥያቄ ይተዋሉ፡፡ በቴስፓፓር፣ SCDV+BERT(ctxd) በተለየ እናስቀናለን፡፡ የኦሪጂ-የ SCDV፣ የብኤርቴን አስቀድሞ እና በብዙ መግለጫ ዳታዎች ላይ ብዙ ሌሎችን መደገፊያዎች እንዲያሳየው ምዕራብ ነው፡፡ አካባቢነታችንን ለሌሎች ስራዎችን በጥያቄ እና የሥርዓት ብጤት እና ማሳየትን አሳይተናል፡፡ በተጨማሪም፣ SCDV+BERT (ctxd) ውጤት-tune BERT እና በተለየ ቁጥጥር ላይ-ፓርቲዎች በተለየ ዳታ እና ጥቂት ነጥቦች ምሳሌዎች ብቻ እንደሆነ እናሳየዋለን፡፡</abstract_am>
      <abstract_cs>Několik úkolů NLP vyžaduje efektivní reprezentaci textových dokumentů. Arora et al.,2017 ukazují, že jednoduché vážené aver-aging slovních vektorů často překonává neurální modely. SCDV (Mekala et al., 2017)to dále rozšiřuje od vět do dokumentů pomocí měkkého a řídkého clusterování přes předem vypočítané slovní vektory. Obě techniky ovšem ignorují polyzemii a kontextový charakter slov. V této části řešíme tento problém navrhováním SCDV+BERT(ctxd), jednoduché a efektivní reprezentace bez dohledu, která kombinuje kontextualizované BERT (Devlin et al., 2019) založené na vložení slovního smyslu s přístupem měkkého shlukování SCDV. Naše vložení překonávají původní SCDV, BERT před tréninkem a několik dalších základních linek na mnoha klasifikačních datových sadách. Také demonstrujeme naši efektivitu vkládání na další úkoly, jako je například shoda konceptů a podobnost vět. Kromě toho ukazujeme, že SCDV+BERT(ctxd) překonává jemné ladění BERT a různé postupy vkládání v scénářích s omezenými daty a pouze málo příkladů snímků.</abstract_cs>
      <abstract_et>Mitmed uue uue tööprogrammi ülesanded vajavad tekstidokumentide tõhusat esitamist. Arora et al., 2017 näitavad, et sõnavaktorite lihtne kaalutud keskmine vananemine ületab sageli täiuslikke neuromudeleid. SCDV (Mekala jt., 2017) laiendab seda veelgi lausetelt dokumentidele, kasutades pehmet ja hõredat klastrit eelnevalt arvutatud sõnavaktorite suhtes. Kuid mõlemad tehnikad eiravad sõnade polüseemiat ja kontekstilist iseloomu. Selles osas käsitleme seda probleemi, pakkudes välja SCDV+BERT(ctxd), lihtsa ja tõhusa järelevalveta esinduse, mis ühendab kontekstualiseeritud BERT-i (Devlin jt., 2019) põhisõna põhisõna põhjendamise ja SCDV pehme klastrite lähenemisviisi. Näitame, et meie manustamised on paljudes klassifitseerimisandmekogumites paremad kui algsed SCDV-d, eelkoolitused BERT-d ja mitmed teised alusjooned. Samuti demonstreerime oma integreerimise tõhusust muudes ülesannetes, nagu kontseptsioonide sobitamine ja lausete sarnasus. Lisaks näitame, et SCDV+BERT(ctxd) on parem kui BERT ja erinevad manustamisprotsessid piiratud andmetega stsenaariumides ja ainult väheste võtete näidetes.</abstract_et>
      <abstract_ca>Diverses tasques del NLP necessiten una representació efectiva dels documents de text. Arora et al., 2017 demostren que l'envelliment mitjà ponderat dels vectors de paraules sovint supera els models neuronals. SCDV (Mekala et al., 2017)ho extreu més des de frases a documentacions utilitzant grups suaus i escassos sobre vectors de paraules precomputats. Com sigui, les dues tècniques ignoren el caràcter polisèmic i contextual de les paraules. En aquest spaper, abordem aquest tema proposant SCDV+BERT(ctxd), un a representació senzilla i efectiva sense supervisió que combina el textualitzat BERT (Devlin et al., 2019) basat en incorporació de paraules de desambiguació de sentit amb l'enfocament de clustering suau SCDV. Ens mostren que les nostres incorporacions superen el SCDV origi nal, el BERT previ al tren i diverses altres línies en molts conjunts de dades de classificació. Wealso també demostra l'eficiència de les nostres incorporacions en altres tasques, com la comparació de conceptes i la similitud de frases. In addition,we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</abstract_ca>
      <abstract_fi>Useissa uuden työohjelman tehtävissä tekstiasiakirjojen tehokasta esittämistä tarvitaan. Arora et al., 2017 osoittavat, että sanavektorien yksinkertainen painotettu keskiarvo ylittää usein suorituksen neuromallit. SCDV (Mekala et al., 2017) laajentaa tätä lauseista dokumentteihin käyttämällä pehmeää ja harvaa klusterointia ennalta laskettujen sanavektorien päälle. Kummassakin tekniikassa ei kuitenkaan oteta huomioon sanojen monimuotoista ja kontekstuaalista luonnetta. Tässä artikkelissa käsittelemme tätä ongelmaa ehdottamalla SCDV+BERT(ctxd), yksinkertaista ja tehokasta valvomatonta esitystä, jossa yhdistyvät kontekstualisoitu BERT (Devlin et al., 2019) -pohjainen upotus sanan merkityksen selventämiseksi SCDV soft klustering -lähestymistapaan. Miten upotuksemme suoriutuvat SCDV:stä, BERT:stä ja useista muista lähtökohdista monissa luokitustietoaineistoissa. Meidän on myös osoitettava upotustemme tehokkuutta muihin tehtäviin, kuten konseptien vastaavuuteen ja lauseiden samankaltaisuuteen. Lisäksi osoitamme, että SCDV+BERT(ctxd) suoriutuu BERT:n hienosäätämisestä ja erilaisista upotussovelluksista skenaarioissa, joissa on rajallista dataa ja vain muutamia otoksia.</abstract_fi>
      <abstract_bs>Nekoliko zadataka NLP-a je potrebno efikasno ponovno režiranje tekstskih dokumenta. Arora et al., 2017. pokazuje da je jednostavno težino održavanje vektora riječi često iznad izvornih modela. SCDV (Mekala et al., 2017) dodatno proširi to od kazne na docu-ments koristeći meke i rezervne skustere preko pre-računalnih vektora riječi. Kako god, obje tehnike ignoriraju polisemijski kontekstualni karakter riječi. U toj oblasti, riješimo ovaj problem predloženjem SCDV+BERT(ctxd), jednostavnom i efikasnom nedovoljnom predstavljanju koja kombinira kontekstualiziranu BERT (Devlin et al., 2019) baznu ploču koja se uključuje za disambigua-ciju riječi sa mekim pristupom SCDV-a. Mi pokazujemo da naše ugrađenje iznosi originalni SCDV, pre vožnje BERT i nekoliko drugih bazena na mnogim podacima klasifikacije. Wealso pokazuje naše uključenje učinkovitosti na druge zadatke, kao što su koncept odgovaranja i sličnost rečenica. Osim toga, pokazujemo da je SCDV+BERT(ctxd) nadmažena izvedba BERT-a i različite ugrađivanje povreda u scenarijima s ograničenim podacima i samo nekoliko primjera snimaka.</abstract_bs>
      <abstract_jv>Awak dhéwé NLP sing disimpen nggawe ngubah werak-seneng nggawe dokumen teks Aara et al. forward expans this from words to docu ments-ments by hiring software and spase cluster ing lah Nan paten, kita diwurung ngobudhakan iki dadi saben nggunakae + (BERT(ct xx) text-box-mode Wacom action-type Label</abstract_jv>
      <abstract_ha>Kayya cikin aikin NLP, ana ƙayyade fara-ƙara wa takardar littãfi masu inganci. Arora et al.,2017 na nuna cewa yana da gwargwadon mai sauri ga sauri masu tsakanin shiryoyi na maganar ko da yawa suka sami misãlai. SCDV (Mekala et al., 2017) Kayya, duk tufãfi biyu suna ƙyãma ma ma'anar musamman da ma'anar sauri. In titopa, Munã jãyayya wannan masu al'amarin da Muke bukãtar da SCDV+BERT(ctxd), wani mai sauƙi da mai amfani da wanda bai zama mai tsaro ba, mai haɗãwa da con-text-naturated BERT (Devlin et al., 2019) bazaƙord wanda ke shigar da wa maganar sanyi desmbague-tion da SCDV zaɓallin matsayin mai ƙaranci. @ info: whatsthis Kayya, Mun nũna masu da amfani da masu akan aiki na dabam, kamar zato da suka yi daidai da maganar. Ina ƙaranci, Munã nuna SCDV+BERT(ctxd) outperformsfine-tune BERT da wasu taɓallu masu cikin tsarario da aka tsare data da kuma misãlai kaɗan kawai.</abstract_ha>
      <abstract_sk>Več nalog novega delovnega programa zahteva učinkovito predstavitev besedilnih dokumentov. Arora et al., 2017 dokazujejo, da preprosta ponderirana povprečna staranja besednih vektorjev pogosto presega izvedbene nevronske modele. SCDV (Mekala et al., 2017) dodatno razširja to iz stavkov na dokumente z uporabo mehkega in redkega grozdnega združevanja nad vnaprej izračunanimi besednimi vektorji. Kljub temu obe tehniki ignorirata polikemijski in kontekstualni značaj besed. V tem dokumentu obravnavamo to vprašanje s predlogom SCDV+BERT(ctxd), preproste in učinkovite nenadzorovane predstavitve, ki združuje konteksturalizirano BERT (Devlin et al., 2019) osnovno vključevanje besednega pomena z mehkim grozdjem SCDV. Kako lahko naše vdelave presegajo originalne SCDV, BERT pred vlakom in več drugih baznih podatkov o klasifikaciji. Pokazali bomo tudi učinkovitost vgradnje pri drugih nalogah, kot sta ujemanje konceptov in podobnost stavkov. Poleg tega pokažemo, da SCDV+BERT(ctxd) izboljšujejo izboljšanje BERT in različnih vdelanih aplikacij v scenarijih z omejenimi podatki in le malo primerov posnetkov.</abstract_sk>
      <abstract_he>מספר משימות NLP זקוקות לייצג יעיל של מסמכים טקסט. Arora et al., 2017 מראים שזקנה ממוצעת משקלת פשוטה של ווקטורים מילים לעתים קרובות יותר מודלים נוראלים. SCDV (Mekala et al., 2017) מחביר את זה יותר משפטים לדוקטורטים על ידי השימוש של קבוצות רכות ומנמכות מעל ווקטורי מילים מחשבים מראש. בכל מקרה, שתי הטכניקות מתעלמות מהפוליזמיה והאופי הקונקסטי של מילים. בספרד הזה, אנו מתייחסים לנושא הזה על ידי הצעה SCDV+BERT( ctxd), מייצג פשוט ויכולתי ללא פיקוח שמשולב את BERT (Devlin et al., 2019) המבוסס קונטקסטוליזציה למילה תחושה אנחנו מראים שהקישורים שלנו עולים מעל SCDV מקורי, BERT לפני הרכבת, וכמה קווים אחרים על קבוצות מידע מסווג רבים. ווילסו מציגים את היכולת של ההכניסות שלנו למשימות אחרות, כמו התאמה של מושג וכמויות משפטים. בנוסף, אנו מראים שSCDV+BERT(ctxd) יוצאים מהביצועים של BERT מסוים ומערכות אפ-אפ שונות בתסריטים עם נתונים מוגבלים ורק כמה דוגמאות צילומים.</abstract_he>
      <abstract_bo>NLP ལ་བྱ་འགུལ་མང་པོ་ཞིག་ནི་ཡིག་གེ་ཡིག་གེ་ཡིག་གེ་ཚིག་ཡིག་ཆ་བསྐྱར་དུ་གཏོང་དགོས་པ Arora et al,2017 ཡིས་སྤྱི་ཚོགས་ཀྱི་རྒྱ་ཆེ་མཐོང་ཚད་ལྷག་པའི་གཞན་པ་ཚོའི་ནང་གི་མཐའ་འཁོར་གྱི་དཔེ་དབྱིབས་བཀོད་ཡོད། SCDV (Mekala et al., 2017)further extends this from sentences to docu-ments by employing soft and sparse cluster-ing over pre-computed word vectors. གང་ལྟར་ཞིག་ཡིན་ན། ཐབས་ལམ་གཉིས་ཀྱིས་སྔོན་ཅིག་དང་འཕགས་རིས་ཡི་གེ་སྣང་མེད་བསྐྱུར་བྱེད། In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach. Weshow that our embeddings outperform origi-nal SCDV, pre-train BERT, and several otherbaselines on many classification datasets. Wealso་གིས་ང་ཚོའི་ནང་དུ་ཡོད་པའི་འཇུག་སྣོད་ནི་ལས་འཕགས་པ་གཞན་དང་མཉམ་དུ་བཀྲམ་སྟོན་ཐུབ། In addition, we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data and only few shots examples.</abstract_bo>
      </paper>
  </volume>
</collection>