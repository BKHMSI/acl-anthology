<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.bsnlp">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing</booktitle>
      <editor><first>Bogdan</first><last>Babych</last></editor>
      <editor><first>Olga</first><last>Kanishcheva</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Jakub</first><last>Piskorski</last></editor>
      <editor><first>Lidia</first><last>Pivovarova</last></editor>
      <editor><first>Vasyl</first><last>Starko</last></editor>
      <editor><first>Josef</first><last>Steinberger</last></editor>
      <editor><first>Roman</first><last>Yangarber</last></editor>
      <editor><first>Michał</first><last>Marcińczuk</last></editor>
      <editor><first>Senja</first><last>Pollak</last></editor>
      <editor><first>Pavel</first><last>Přibáň</last></editor>
      <editor><first>Marko</first><last>Robnik-Šikonja</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Kiyv, Ukraine</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="bbad6896">2021.bsnlp-1.0</url>
      <bibkey>bsnlp-2021-balto</bibkey>
    </frontmatter>
    <paper id="1">
      <title>HerBERT : Efficiently Pretrained Transformer-based Language Model for Polish<fixed-case>H</fixed-case>er<fixed-case>BERT</fixed-case>: Efficiently Pretrained Transformer-based Language Model for <fixed-case>P</fixed-case>olish</title>
      <author><first>Robert</first><last>Mroczkowski</last></author>
      <author><first>Piotr</first><last>Rybak</last></author>
      <author><first>Alina</first><last>Wróblewska</last></author>
      <author><first>Ireneusz</first><last>Gawlik</last></author>
      <pages>1–10</pages>
      <abstract>BERT-based models are currently used for solving nearly all Natural Language Processing (NLP) tasks and most often achieve state-of-the-art results. Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures. Several ablation studies investigating how to train BERT-like models have been carried out, but the vast majority of them concerned only the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>. A <a href="https://en.wikipedia.org/wiki/Procedure_(term)">training procedure</a> designed for <a href="https://en.wikipedia.org/wiki/English_language">English</a> does not have to be universal and applicable to other especially typologically different languages. Therefore, this paper presents the first ablation study focused on <a href="https://en.wikipedia.org/wiki/Polish_language">Polish</a>, which, unlike the isolating English language, is a <a href="https://en.wikipedia.org/wiki/Fusional_language">fusional language</a>. We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models. In addition to multilingual model initialization, other <a href="https://en.wikipedia.org/wiki/Factor_analysis">factors</a> that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length. Based on the proposed procedure, a Polish BERT-based language model   HerBERT   is trained. This <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves state-of-the-art results on multiple downstream tasks.</abstract>
      <url hash="82f85369">2021.bsnlp-1.1</url>
      <bibkey>mroczkowski-etal-2021-herbert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/klej">KLEJ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="4">
      <title>Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company’s Reputation</title>
      <author><first>Nikolay</first><last>Babakov</last></author>
      <author><first>Varvara</first><last>Logacheva</last></author>
      <author><first>Olga</first><last>Kozlova</last></author>
      <author><first>Nikita</first><last>Semenov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>26–36</pages>
      <abstract>Not all topics are equally flammable in terms of <a href="https://en.wikipedia.org/wiki/Toxicity">toxicity</a> : a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or <a href="https://en.wikipedia.org/wiki/Sexual_minority">sexual minorities</a>. We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> of collecting and labelling a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for appropriateness. While <a href="https://en.wikipedia.org/wiki/Toxicity">toxicity</a> in user-generated data is well-studied, we aim at defining a more fine-grained notion of <a href="https://en.wikipedia.org/wiki/Inappropriateness">inappropriateness</a>. The core of <a href="https://en.wikipedia.org/wiki/Inappropriateness">inappropriateness</a> is that it can harm the reputation of a speaker. This is different from <a href="https://en.wikipedia.org/wiki/Toxicity">toxicity</a> in two respects : (i) <a href="https://en.wikipedia.org/wiki/Inappropriateness">inappropriateness</a> is topic-related, and (ii) inappropriate message is not toxic but still unacceptable. We collect and release two <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> for <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a> : a topic-labelled dataset and an appropriateness-labelled dataset. We also release pre-trained classification models trained on this <a href="https://en.wikipedia.org/wiki/Data">data</a>.</abstract>
      <url hash="d982a981">2021.bsnlp-1.4</url>
      <bibkey>babakov-etal-2021-detecting</bibkey>
    </paper>
    <paper id="6">
      <title>RuSentEval : Linguistic Source, Encoder Force !<fixed-case>R</fixed-case>u<fixed-case>S</fixed-case>ent<fixed-case>E</fixed-case>val: Linguistic Source, Encoder Force!</title>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Ekaterina</first><last>Taktasheva</last></author>
      <author><first>Elina</first><last>Sigdel</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <pages>43–65</pages>
      <abstract>The success of pre-trained transformer language models has brought a great deal of interest on how these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> work, and what they learn about language. However, prior research in the field is mainly devoted to <a href="https://en.wikipedia.org/wiki/English_language">English</a>, and little is known regarding other languages. To this end, we introduce RuSentEval, an enhanced set of 14 probing tasks for <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a>, including ones that have not been explored yet. We apply a combination of complementary probing methods to explore the distribution of various linguistic properties in five multilingual transformers for two typologically contrasting languages   Russian and <a href="https://en.wikipedia.org/wiki/English_language">English</a>. Our results provide intriguing findings that contradict the common understanding of how linguistic knowledge is represented, and demonstrate that some properties are learned in a similar manner despite the language differences.</abstract>
      <url hash="dc9ec49d">2021.bsnlp-1.6</url>
      <bibkey>mikhailov-etal-2021-rusenteval</bibkey>
      <pwccode url="https://github.com/RussianNLP/rusenteval" additional="true">RussianNLP/rusenteval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    <title_ar>RuSentEval: مصدر لغوي ، قوة التشفير!</title_ar>
      <title_fr>RusEnteVal : Source linguistique, force d'encodeur !</title_fr>
      <title_pt>RuSentEval: fonte linguística, força do codificador!</title_pt>
      <title_es>RusentEval: ¡Fuente lingüística, fuerza codificadora!</title_es>
      <title_ja>RuSentEval:言語ソース、エンコーダフォース！</title_ja>
      <title_zh>RuSentEval:语言源,编码器力!</title_zh>
      <title_hi>RuSentEval: भाषाई स्रोत, एनकोडर बल!</title_hi>
      <title_ru>RuSentEval: Лингвистический источник, Сила кодировщика!</title_ru>
      <title_ga>RuSentEval: Foinse Teangeolaíoch, Fórsa Ionchódóra!</title_ga>
      <title_ka>ლთნდსთჟრთფვნ თჱრჲპ, კჲევპჟკა ჟთლა!</title_ka>
      <title_hu>RuSentEval: Nyelvi forrás, kódoló erő!</title_hu>
      <title_el>Γλωσσολογική Πηγή, Δύναμη Κωδικοποιητή!</title_el>
      <title_it>RuSentEval: Linguistic Source, Encoder Force!</title_it>
      <title_kk>RuSentEval: Linguistic Source, Encoder Force!</title_kk>
      <title_mk>RuSentEval: Linguistic Source, Encoder Force!</title_mk>
      <title_ml>റൂസെന്‍റ് എവാല്‍: ലിങ്ഗിസ്റ്റിക്ക് സ്രോതസ്സ്, എന്‍കോഡെര്‍ ഫോര്‍സ്!</title_ml>
      <title_lt>RuSentEval: Linguistic Source, Encoder Force!</title_lt>
      <title_mn>RuSentEval: Linguistic Source, Encoder Force!</title_mn>
      <title_mt>RuSentEval: Sors Lingwistiku, Forza tal-Kodifikatur!</title_mt>
      <title_pl>RuSentEval: Źródło językowe, siła kodowania!</title_pl>
      <title_ro>RuSentEval: Sursă lingvistică, Forța Encoder!</title_ro>
      <title_no>RuSentEval: Linguistic Source, Encoder Force!</title_no>
      <title_so>RuSentEval: Linguistic Source, Encoder Force!</title_so>
      <title_si>RuSenteval: ලින්ග්යුස්ටික් ප්‍රදේශය, එන්කෝඩර් බලය!</title_si>
      <title_ms>RuSentEval: Sumber Bahasa, Pasukan Pengenkod!</title_ms>
      <title_ur>روسینٹ ایولی: لینگویسٹ سورس، انکوڈر فورس!</title_ur>
      <title_sr>Lingistički izvor, koderska snaga!</title_sr>
      <title_sv>RuSentEval: Linguistic Source, Encoder Force!</title_sv>
      <title_ta>RuSentEval: Linguistic Source, Encoder Force!</title_ta>
      <title_uz>Eval: Linguistic Source, Encoder Force!</title_uz>
      <title_vi>Nguồn ngôn ngữ, Mã hóa.</title_vi>
      <title_bg>RuSentEval: Linguistic Source, Encoder Force!</title_bg>
      <title_hr>Lingistički izvor, koderska snaga!</title_hr>
      <title_nl>RuSentEval: Taalkundige Bron, Encoder Force!</title_nl>
      <title_da>RuSentEval: Linguistic Source, Encoder Force!</title_da>
      <title_id>RuSentEval: Sumber Bahasa, Pasukan Pengenkodar!</title_id>
      <title_sw>RuSentEval: Chanzo cha Kilinguistic, Vikosi vya Kufungua!</title_sw>
      <title_de>RuSentEval: Sprachliche Quelle, Encoder Force!</title_de>
      <title_ko>언어 출처, 인코딩 역량!</title_ko>
      <title_fa>روسنتEval: منبع لینگیستیک، نیروی رمزگذاری!</title_fa>
      <title_af>RuSentEval: Linguistic Bron, Encoder Force!</title_af>
      <title_sq>RuSentEval: Burim gjuhësor, forca e koduesit!</title_sq>
      <title_tr>Comment</title_tr>
      <title_bn>রুসেন্ট ইভাল: লিঙ্গিস্টিক সূত্র, এনকোডার বাহিনী!</title_bn>
      <title_am>Eval: Linguistic source, Encoder Force!</title_am>
      <title_hy>RuSentewal. լեզվաբանական աղբյուր, կոդավոր ուժ:</title_hy>
      <title_bs>Lingistički izvor, koderska snaga!</title_bs>
      <title_ca>RuSentEval: Font lingüística, Força del codificador!</title_ca>
      <title_cs>RuSentEval: Jazykový zdroj, síla kódování!</title_cs>
      <title_fi>RuSentEval: Linguistic Source, Encoder Force!</title_fi>
      <title_et>RuSentEval: Linguistic Source, Encoder Force!</title_et>
      <title_az>RuSentEval: Linguistic Source, Encoder Force!</title_az>
      <title_jv>Language</title_jv>
      <title_ha>@ info: status</title_ha>
      <title_sk>RuSentEval: Linguistic Source, Encoder Force!</title_sk>
      <title_he>RuSentEval: מקור לינגיסטי, כוח קודד!</title_he>
      <title_bo>RuSentEval: སྐད་རིགས་འདྲ་བྱུང་མཁན་དང་ཨིན་ཀོ་ཌིར་བྱེད་ཀྱི་ཡོད།</title_bo>
      <abstract_ar>أثار نجاح نماذج لغة المحولات المدربة مسبقًا قدرًا كبيرًا من الاهتمام بكيفية عمل هذه النماذج ، وماذا تعلموه عن اللغة. ومع ذلك ، فإن الأبحاث السابقة في هذا المجال مكرسة بشكل أساسي للغة الإنجليزية ، ولا يُعرف سوى القليل عن اللغات الأخرى. تحقيقًا لهذه الغاية ، نقدم RuSentEval ، وهي مجموعة محسّنة من 14 مهمة فحص للروسية ، بما في ذلك المهام التي لم يتم استكشافها بعد. نحن نطبق مجموعة من طرق الفحص التكميلية لاستكشاف توزيع الخصائص اللغوية المختلفة في خمسة محولات متعددة اللغات لغتين متناقضتين نمطياً - الروسية والإنجليزية. تقدم نتائجنا نتائج مثيرة للاهتمام تتعارض مع الفهم الشائع لكيفية تمثيل المعرفة اللغوية ، وتوضح أن بعض الخصائص يتم تعلمها بطريقة مماثلة على الرغم من الاختلافات اللغوية.</abstract_ar>
      <abstract_fr>Le succès des modèles de langage de transformateur préformés a suscité un vif intérêt sur le fonctionnement de ces modèles et sur ce qu'ils apprennent sur la langue. Cependant, les recherches antérieures dans ce domaine sont principalement consacrées à l'anglais et on sait peu de choses sur les autres langues. À cette fin, nous présentons RusEnteVal, un ensemble amélioré de 14 tâches de sondage pour le russe, y compris celles qui n'ont pas encore été explorées. Nous appliquons une combinaison de méthodes de sondage complémentaires pour explorer la distribution de diverses propriétés linguistiques dans cinq transformateurs multilingues pour deux langues typologiquement opposées : le russe et l'anglais. Nos résultats fournissent des résultats intrigants qui contredisent la compréhension commune de la représentation des connaissances linguistiques et démontrent que certaines propriétés sont apprises de la même manière malgré les différences linguistiques.</abstract_fr>
      <abstract_es>El éxito de los modelos lingüísticos transformadores previamente entrenados ha despertado un gran interés sobre cómo funcionan estos modelos y qué aprenden sobre el lenguaje. Sin embargo, la investigación previa en el campo se dedica principalmente al inglés, y se sabe poco sobre otros idiomas. Con este fin, presentamos RusEnTeval, un conjunto mejorado de 14 tareas de sondeo para el ruso, incluidas las que aún no se han explorado. Aplicamos una combinación de métodos de sondeo complementarios para explorar la distribución de varias propiedades lingüísticas en cinco transformadores multilingües para dos idiomas que contrastan tipológicamente: el ruso y el inglés. Nuestros resultados proporcionan hallazgos interesantes que contradicen la comprensión común de cómo se representa el conocimiento lingüístico y demuestran que algunas propiedades se aprenden de manera similar a pesar de las diferencias lingüísticas.</abstract_es>
      <abstract_pt>O sucesso de modelos de linguagem de transformador pré-treinados trouxe muito interesse sobre como esses modelos funcionam e o que eles aprendem sobre linguagem. No entanto, pesquisas anteriores na área são principalmente dedicadas ao inglês, e pouco se sabe sobre outros idiomas. Para isso, apresentamos o RuSentEval, um conjunto aprimorado de 14 tarefas de sondagem para russo, incluindo aquelas que ainda não foram exploradas. Aplicamos uma combinação de métodos de sondagem complementares para explorar a distribuição de várias propriedades linguísticas em cinco transformadores multilíngues para dois idiomas tipologicamente contrastantes – russo e inglês. Nossos resultados fornecem descobertas intrigantes que contradizem o entendimento comum de como o conhecimento linguístico é representado e demonstram que algumas propriedades são aprendidas de maneira semelhante, apesar das diferenças linguísticas.</abstract_pt>
      <abstract_ja>事前に訓練された変圧器言語モデルの成功は、これらのモデルがどのように機能し、言語について何を学ぶかに大きな関心をもたらしました。しかし、この分野での先行研究は主に英語に捧げられており、他の言語に関してはほとんど知られていない。この目的のために、RuSentEvalを紹介します。RuSentEvalは、まだ探索されていないものを含む、ロシア語の14の探索タスクの強化されたセットです。私たちは、ロシア語と英語という2つの類型的に対照的な言語のための5つの多言語変換器におけるさまざまな言語特性の分布を探るために、補完的な探索方法の組み合わせを適用します。私たちの結果は、言語学的知識がどのように表現されるかについての共通理解と矛盾する興味深い発見を提供し、言語の違いにもかかわらず、いくつかの特性が同様の方法で学習されることを示しています。</abstract_ja>
      <abstract_zh>先训变形金刚言成,及其大乐。 然该领之先,究主于英语,于他言之甚少。 为言RuSentEval者,14俄语探事也。 合互补以索二比 ( 俄语与英语 ) 五多言转换器言语之分也。 吾道有趣,与言语相违共识,虽有言语差异,性犹以类学也。</abstract_zh>
      <abstract_hi>पूर्व-प्रशिक्षित ट्रांसफॉर्मर भाषा मॉडल की सफलता ने इन मॉडलों के काम करने के तरीके पर बहुत रुचि लाई है, और वे भाषा के बारे में क्या सीखते हैं। हालांकि, क्षेत्र में पूर्व अनुसंधान मुख्य रूप से अंग्रेजी के लिए समर्पित है, और अन्य भाषाओं के बारे में बहुत कम जाना जाता है। इस अंत तक, हम RuSentEval, रूसी के लिए 14 जांच कार्यों का एक बढ़ाया सेट पेश करते हैं, जिसमें वे भी शामिल हैं जिन्हें अभी तक खोजा नहीं गया है। हम दो टाइपोलॉजिकल रूप से विपरीत भाषाओं - रूसी और अंग्रेजी के लिए पांच बहुभाषी ट्रांसफॉर्मर में विभिन्न भाषाई गुणों के वितरण का पता लगाने के लिए पूरक जांच विधियों का एक संयोजन लागू करते हैं। हमारे परिणाम पेचीदा निष्कर्ष प्रदान करते हैं जो भाषाई ज्ञान का प्रतिनिधित्व करने के तरीके की सामान्य समझ के विपरीत हैं, और यह प्रदर्शित करते हैं कि भाषा के मतभेदों के बावजूद कुछ गुणों को समान तरीके से सीखा जाता है।</abstract_hi>
      <abstract_ru>Успех предварительно обученных моделей языков трансформаторов вызвал большой интерес к тому, как работают эти модели и что они узнают о языке. Однако предыдущие исследования в этой области в основном посвящены английскому языку, и мало что известно о других языках. С этой целью мы представляем RuSentEval - расширенный набор из 14 зондирующих задач для российских, в том числе еще не исследованных. Мы применяем комбинацию комплементарных методов зондирования для исследования распределения различных лингвистических свойств в пяти многоязычных трансформаторах для двух типологически контрастных языков – русского и английского. Наши результаты дают интригующие выводы, которые противоречат общему пониманию того, как языковые знания представлены, и демонстрируют, что некоторые свойства изучаются аналогичным образом, несмотря на языковые различия.</abstract_ru>
      <abstract_ga>Chuir rath na múnlaí teanga claochladán réamhoilte go mór an-suim sa chaoi a n-oibríonn na samhlacha seo, agus ar an méid a fhoghlaimíonn siad faoin teanga. Mar sin féin, tá an taighde a rinneadh cheana sa réimse dírithe go príomha ar an mBéarla, agus is beag atá ar eolas faoi theangacha eile. Chuige sin, tugaimid isteach RuSentEval, sraith fheabhsaithe de 14 thasc iniúchta don Rúisis, lena n-áirítear cinn nach bhfuil iniúchadh déanta orthu go fóill. Cuirimid meascán de mhodhanna comhlántacha taiscéalaíochta i bhfeidhm chun iniúchadh a dhéanamh ar dháileadh airíonna teangeolaíocha éagsúla i gcúig chlaochladán ilteangach do dhá theanga atá codarsnachta ó thaobh na clódóireachta de – Rúisis agus Béarla. Soláthraíonn ár gcuid torthaí torthaí suimiúla a thagann salach ar an gcomhthuiscint ar an gcaoi a léirítear eolas teangeolaíoch, agus a léiríonn go bhfoghlaimítear airíonna áirithe ar an mbealach céanna in ainneoin na ndifríochtaí teanga.</abstract_ga>
      <abstract_ka>პრეტრანსტრინსტრიქტური ენის მოდელების წარმატება ძალიან ინტერესტის შესახებ როგორ ეს მოდელები მუშაობენ, და როგორ ისინი ვისწავლებენ ენის შესახებ. მაგრამ, პირველი პასუხში ინგლისური კონფიგურაცია უფრო მნიშვნელოვანია, და სხვა ენების შესახებ პატარა უცნობია. ამ მიზეზით, ჩვენ RuSentEval-ს ჩვენ გავიყენებთ, რომელიც 14 პროცესური მოწყობილობა დავამუშავებული საქმედები, რომელიც უკვე არ განსხვავებულია. ჩვენ კომპლენტერიური პრობენტის კომპლენტერიური პრობენტის კომპლენტერიური პრობენტის კომპლენტების კომპლენტების კომპლენტების კომპლენტების კომპლენტებ ჩვენი წარმოდგენების შესაძლებლობები გააკეთება, რომლებიც საერთო სხვადასხვა ინგლიგური ცოდნიერების განსაკუთრებების განსაკუთრებების განსაკუთრებების განსაკუთრებების განსაკუთრებების განსაკ</abstract_ka>
      <abstract_hu>Az előre képzett transzformátor nyelvi modellek sikere nagy érdeklődést keltett arra, hogy ezek a modellek hogyan működnek és mit tanulnak a nyelvről. Azonban a területen végzett korábbi kutatások elsősorban az angol nyelvről szólnak, és más nyelvekről keveset tudunk. Ebből a célból bemutatjuk a RuSentEval-t, amely egy továbbfejlesztett 14 szondázási feladatot tartalmaz orosz számára, beleértve azokat is, amelyeket még nem fedeztek fel. Kiegészítő mérési módszerek kombinációját alkalmazzuk a különböző nyelvi tulajdonságok eloszlásának vizsgálatára öt többnyelvű transzformátorban két tipológiailag kontrasztos nyelven - orosz és angol. Eredményeink érdekes megállapításokat szolgáltatnak, amelyek ellentmondanak a nyelvtudás ábrázolásának közös megértésével, és azt mutatják, hogy bizonyos tulajdonságok hasonló módon tanulhatók a nyelvi különbségek ellenére.</abstract_hu>
      <abstract_el>Η επιτυχία των προ-εκπαιδευμένων μοντέλων γλώσσας μετασχηματιστών έφερε μεγάλο ενδιαφέρον για το πώς λειτουργούν αυτά τα μοντέλα και τι μαθαίνουν για τη γλώσσα. Ωστόσο, η προηγούμενη έρευνα στον τομέα είναι κυρίως αφιερωμένη στα αγγλικά, και λίγα είναι γνωστά σχετικά με άλλες γλώσσες. Για το σκοπό αυτό, εισάγουμε το ένα ενισχυμένο σύνολο τεσσάρων εργασιών ανίχνευσης για τη ρωσική, συμπεριλαμβανομένων εκείνων που δεν έχουν διερευνηθεί ακόμα. Εφαρμόζουμε έναν συνδυασμό συμπληρωματικών μεθόδων ανίχνευσης για να διερευνήσουμε την κατανομή διαφόρων γλωσσικών ιδιοτήτων σε πέντε πολύγλωσσους μετασχηματιστές για δύο τυπολογικά αντίθετες γλώσσες, τη ρωσική και την αγγλική. Τα αποτελέσματά μας παρέχουν συναρπαστικά ευρήματα που έρχονται σε αντίθεση με την κοινή κατανόηση του πώς αναπαρίσταται η γλωσσική γνώση και αποδεικνύουν ότι ορισμένες ιδιότητες μαθαίνονται με παρόμοιο τρόπο παρά τις γλωσσικές διαφορές.</abstract_el>
      <abstract_it>Il successo dei modelli linguistici pre-formati dei trasformatori ha suscitato grande interesse sul funzionamento di questi modelli e su ciò che imparano sulla lingua. Tuttavia, le ricerche precedenti nel campo sono principalmente dedicate all'inglese, e poco si sa per quanto riguarda le altre lingue. A tal fine, presentiamo RuSentEval, una serie migliorata di 14 compiti di ricerca per il russo, inclusi quelli che non sono stati ancora esplorati. Applichiamo una combinazione di metodi di sondaggio complementari per esplorare la distribuzione di varie proprietà linguistiche in cinque trasformatori multilingui per due lingue tipologicamente contrastanti - russo e inglese. I nostri risultati forniscono risultati intriganti che contraddicono la comprensione comune di come la conoscenza linguistica è rappresentata, e dimostrano che alcune proprietà vengono apprese in modo simile nonostante le differenze linguistiche.</abstract_it>
      <abstract_lt>Iš anksto parengtų kalbų transformatorių modelių sėkmė sukėlė didelį susidomėjimą, kaip šie modeliai veikia ir ką jie mokosi apie kalbą. Tačiau ankstesni moksliniai tyrimai šioje srityje daugiausia skiriami anglų kalbai, o apie kitas kalbas mažai žinoma. Šiuo tikslu pristatysime RuSentEval, sustiprintą 14 rusų tyrimo užduočių rinkinį, įskaitant tuos, kurie dar nebuvo ištirti. Taikome papildomų tyrimo metodų derinį, kad ištirtume įvairių kalbinių savybių pasiskirstymą penkiose daugiakalbėse transformatoriuose dviem tipologiškai kontrastinėmis kalbomis - rusų ir anglų. Mūsų rezultatai suteikia įdomių išvadų, prieštaraujančių bendram supratimui apie kalbinių žinių atstovavimą, ir rodo, kad kai kurios savybės mokomos panašiai, nepaisant kalbų skirtumų.</abstract_lt>
      <abstract_mk>Успехот на предобучените јазички модели на трансформатори доведе голем интерес за тоа како овие модели функционираат и за тоа што тие научуваат за јазикот. Сепак, претходното истражување на теренот е посветено главно на англиски, а малку е познато во врска со другите јазици. За ова, го претставуваме RuSentEval, засилен сет на 14 истражувачки задачи за Русите, вклучително и оние кои сé уште не се истражувани. Ние аплицираме комбинација на комплементарни методи на истражување за истражување на дистрибуцијата на различни јазични сопствености во пет мултијазични трансформатори за два типологички контрастни јазици - руски и англиски. Нашите резултати обезбедуваат интересни откритија кои се спротивставуваат на заедничкото разбирање како е претставено јазичкото знаење и покажуваат дека некои имоти се научуваат на сличен начин и покрај разликите во јазикот.</abstract_mk>
      <abstract_kk>Бұл үлгілер қалай жұмыс істейді және олардың тіл туралы оқыту үшін бірнеше қызықты түсіндіреді. Бірақ бұл өрістің алдындағы зерттеулері негізінде ағылшын тіліне аударылады, және басқа тілдер туралы білмейді. Бұл үшін біз RuSentEval дегенді түсіндіреміз. Осылық үшін 14 сынақ тапсырмаларын түсіндіреміз, сондай-ақ тек зерттелмеген. Біз бес тілді көптілік түрлендірушілерде екі типтологиялық қарсы тілдерге - руссия және ағылшын тілдер үшін түрлендірушілердің түрлендіру әдістерін қолданамыз. Біздің нәтижелеріміз лингвистикалық білімдердің қалай түсініктеріне қарсы түсініктеріне қарсы болып, кейбір қасиеттер тілдердің айырмашылығына қарамастырып, ұқсас түрде оқылған.</abstract_kk>
      <abstract_ms>Keberjalan model bahasa pengubah terlatih telah membawa banyak minat tentang bagaimana model ini berfungsi, dan apa yang mereka belajar tentang bahasa. Namun, kajian sebelumnya di lapangan adalah terutama dedikasi kepada bahasa Inggeris, dan sedikit diketahui mengenai bahasa lain. Untuk tujuan ini, kami memperkenalkan RuSentEval, satu set meningkat 14 tugas penyelidikan untuk Rusia, termasuk tugas yang belum dikenalpasti lagi. Kami melaksanakan kombinasi kaedah penyelidikan tambahan untuk mengeksplorasi distribusi beberapa sifat bahasa dalam lima pengubah berbilang bahasa untuk dua bahasa yang bertentangan tipologi - Rusia dan Inggeris. Hasil kami memberikan penemuan menarik yang bertentangan dengan pemahaman umum bagaimana pengetahuan bahasa diwakili, dan menunjukkan bahawa beberapa ciri-ciri belajar dengan cara yang sama walaupun perbezaan bahasa.</abstract_ms>
      <abstract_ml>മുമ്പ് പരിശീലന മാറ്റങ്ങളുടെ ഭാഷ മോഡലുകളുടെ വിജയം ഈ മോഡലുകള്‍ എങ്ങനെ ജോലി ചെയ്യുന്നുവെന്നും ഭാഷ കുറിച്ചും പഠിക എന്നാലും പ്രദേശത്തിലെ പഠനത്തിനു മുമ്പ് പ്രധാനമായി ഇംഗ്ലീഷിലേക്ക് പ്രത്യേകിച്ചിരിക്കുന്നു. മറ ഈ അവസാനത്തിനു വേണ്ടി ഞങ്ങള്‍ റൂസെന്‍റ് എവാലിനെ പരിചയപ്പെടുത്തുന്നു. റഷ്യന്‍റെ ഒരു 14 കൂട്ടം മെച്ചപ്പെടുത്തിയ ജോലികള്‍, ഇതു അഞ്ച് മള്‍ട്ടില്‍ മാറ്റങ്ങളില്‍ വിതരണം ചെയ്യുന്നതിന് വേണ്ടി നമ്മള്‍ ഒരു കൂട്ടിച്ചേര്‍ക്കുന്ന കണക്ടറി പരീക്ഷിക്കുന്ന രീതികള്‍ പ്രയോഗി നമ്മുടെ ഫലങ്ങള്‍ അത്ഭുതപ്പെടുത്തുന്ന കണ്ടുപിടികള്‍ കൊണ്ട് വരുന്നു. ഭാഷ വ്യത്യാസം എങ്ങനെയാണ് പ്രതിനിധിക്കപ്പെടുന്നതെന്നും ഭാഷ വ്യത്</abstract_ml>
      <abstract_mt>The success of pre-trained transformer language models has brought a great deal of interest on how these models work, and what they learn about language.  Madankollu, ir-riċerka preċedenti fil-qasam hija ddedikata prinċipalment għall-Ingliż, u ftit huwa magħruf dwar lingwi oħra. Għal dan il-għan, aħna nintroduċu RuSentEval, sett imtejjeb ta’ 14-il kompitu ta’ sondaġġ għar-Russu, inklużi dawk li għadhom ma ġewx esplorati. Aħna napplikaw taħlita ta’ metodi komplementari ta’ sondaġġ biex tiġi esplorata d-distribuzzjoni ta’ karatteristiċi lingwistiċi varji f’ħames trasformaturi multilingwi għal żewġ lingwi tipoloġikament kuntrastanti - ir-Russu u l-Ingliż. Ir-riżultati tagħna jipprovdu sejbiet intriganti li jikkontradixxu l-fehim komuni ta’ kif l-għarfien lingwistiku huwa rappreżentat, u juru li xi karatteristiċi jitgħallmu b’mod simili minkejja d-differenzi lingwistiċi.</abstract_mt>
      <abstract_mn>Өмнөх сургалтын өөрчлөлтийн хэл загварын амжилт нь эдгээр загваруудын хэрхэн ажилладаг талаар, хэлний талаар юу сурах талаар маш их сонирхолтой болсон. Гэхдээ салбарын өмнөх судалгаа англи хэлний хувьд ихэвчлэн англи хэлний хувьд зориулагддаг. Өөр хэлний хувьд бага ч мэддэг. Энэ төгсгөлд бид RuSentEval-г Оросын 14 судалгааны даалгавар дээр танилцуулсан. Харин одоо судалгаагүй хүмүүс ч мөн адил. Бид хэл хэлний өөрчлөлтийг таван хэлний шилжүүлэгчид хоёр типтологийн эсрэг хэл болон Англи хэлний хуваарьт судалгааны нэмэлт судалгааны аргыг ашиглаж байна. Бидний үр дүнд хэлний мэдлэг хэрхэн илэрхийлж байгааг харуулж, хэлний ялгаатай ч зарим өөрчлөлт нь адилхан аргаар суралцдаг гэдгийг харуулж байна.</abstract_mn>
      <abstract_ro>Succesul modelelor lingvistice pre-instruite a adus un mare interes asupra modului în care funcționează aceste modele și a ceea ce învață despre limbă. Cu toate acestea, cercetarea anterioară în domeniu este dedicată în principal engleză, și puțin se știe despre alte limbi. În acest scop, vă prezentăm RuSentEval, un set îmbunătățit de 14 sarcini de sondare pentru ruși, inclusiv cele care nu au fost explorate încă. Aplicăm o combinație de metode complementare de sondare pentru a explora distribuția diferitelor proprietăți lingvistice în cinci transformatoare multilingve pentru două limbi contrastante tipologic - rusă și engleză. Rezultatele noastre oferă descoperiri interesante care contrazic înțelegerea comună a modului în care sunt reprezentate cunoștințele lingvistice și demonstrează că unele proprietăți sunt învățate în mod similar în ciuda diferențelor lingvistice.</abstract_ro>
      <abstract_pl>Sukces wstępnie przeszkolonych modeli językowych transformatorów wzbudził duże zainteresowanie tym, jak te modele działają i czego się uczą o języku. Jednak wcześniejsze badania w tej dziedzinie poświęcone są głównie angielskiemu, a niewiele wiadomo o innych językach. W tym celu wprowadzamy RuSentEval, rozszerzony zestaw czternastu zadań sondujących dla Rosji, w tym te, które jeszcze nie zostały zbadane. Stosujemy kombinację uzupełniających się metod sondowania, aby zbadać rozkład różnych właściwości językowych w pięciu wielojęzycznych transformatorach dla dwóch typologicznie kontrastujących języków: rosyjskiego i angielskiego. Nasze wyniki dostarczają intrygujących odkryć, które sprzecznie z powszechnym zrozumieniem, jak reprezentowana jest wiedza językowa i pokazują, że niektóre właściwości uczą się w podobny sposób pomimo różnic językowych.</abstract_pl>
      <abstract_no>Det første transformeringsspråk-modellen har ført mykje interesse på korleis disse modelane fungerer, og kva dei lærer om språk. Førre forskning i feltet er imidlertid spesifisert til engelsk, og lite er kjent om andre språk. I denne slutten introdusere vi RuSentEval, eit forbetra sett av 14 proberingsoppgåver for russisk, inkludert dei som ikkje er utforska enno. Vi bruker ein kombinasjon av komplementære proberingsmetodar for å utforske distribusjonen av ulike lingviske eigenskapar i fem fleirspråk transformerande for to typologisk kontrastspråk – russisk og engelsk. Resultatet våre gjev interessante oppdagar som mottrykkjer den felles forståelse av korleis lingviske kunnskap vert representert, og demonstrerer at nokre eigenskapar er lært på ein liknande måte, selv om språk-forskjeller.</abstract_no>
      <abstract_sr>Uspjeh predobučenih transformacijskih jezičkih modela dovodio je mnogo interesa o tome kako ovi modeli funkcionišu i o tome šta nauče o jeziku. Međutim, prethodno istraživanje na terenu je uglavnom posvećeno engleskom, a malo je poznato u vezi drugih jezika. Za taj cilj predstavljamo RuSentEval, povećan set od 14 zadataka za istraživanje Rusa, uključujući one koje još nisu istraživali. Primjenjujemo kombinaciju komplementarnih metoda istraživanja distribucije različitih jezičkih vlasništva u pet multijezičkih transformatora za dva tipološki kontrastveni jezika - ruski i engleski. Naši rezultati pružaju zanimljive nalaze koje se suprotstavljaju zajedničkom razumijevanju kako je jezičko znanje predstavljeno, i pokazuju da su neke vlasništvo naučene na sličan način uprkos jezičkim razlikama.</abstract_sr>
      <abstract_si>මුලින් ප්‍රශ්නයක් වෙන්න පුළුවන් භාෂා මොඩේල් වැඩ කරන්න පුළුවන් විදියට ගොඩක් ආශ්වාසයක් තියෙනවා මේ මො නමුත්, කලින් පරීක්ෂණයක් තියෙන්නේ ඉංග්‍රීසි වලට ප්‍රධාන විශ්වාස කරනවා, ඒ වගේම අනිත් භාෂ මේ අවසානයෙන්, අපි රුසෙන්ට් එව්ල්ව ප්‍රදානය කරනවා, රුසියානුවෙන් පරීක්ෂණාකරණ වැඩ 14ක් විශ්වාස කරනවා, තවම පර අපි විවිධ භාෂාවික විශේෂතාවක් පහත් භාෂාවික විශේෂ කරන්න සම්බන්ධ විශේෂ විශේෂයක් සම්බන්ධ කරනවා වගේ භාෂ අපේ ප්‍රතිචාර දේවල් ප්‍රශ්නයක් තියෙනවා කියලා භාෂාවික දැනගන්නේ කොහොමද කියලා ප්‍රශ්නයක් කරනවා කියලා, සමහර විශේෂතා</abstract_si>
      <abstract_sv>Framgången med färdigutbildade transformatorspråkmodeller har väckt stort intresse för hur dessa modeller fungerar och vad de lär sig om språk. Tidigare forskning inom området ägnas dock främst åt engelska, och lite är känt om andra språk. För detta ändamål introducerar vi RuSentEval, en utökad uppsättning av 14 sonderingsuppgifter för ryska, inklusive sådana som ännu inte har utforskats. Vi använder en kombination av kompletterande sondmetoder för att utforska fördelningen av olika språkliga egenskaper i fem flerspråkiga transformatorer för två typologiskt kontrasterande språk - ryska och engelska. Våra resultat ger spännande fynd som motsäger den gemensamma förståelsen av hur språklig kunskap representeras och visar att vissa egenskaper lärs på liknande sätt trots språkskillnaderna.</abstract_sv>
      <abstract_so>Liibadii samooyinka isbedelka afka hore ee horay lagu tababariyey waxay keentay xiiso badan oo ku saabsan sida modelladan u shaqeeyaan iyo waxa ay ku bartaan luuqada. Si kastaba ha ahaatee baaritaanka hore ee duurka waxaa loogu talogalay ingiriiska, wax yarna waxaa lagu yaqaanaa luuqado kale. Taas darteed waxaannu soo bandhignaa RuSentEval oo ah 14 shaqooyin aad u kordhisan oo Ruush ah, kuwaas oo ah kuwo aan weli la baadhay. Waxaannu u dalbannaa qalabka imtixaanka iskuulka ah si aan ugu baarayno qaybinta hantida luuqadaha kala duduwan shan isbedelka oo luuqadaha kala duduwan laba luuqadood oo si caadi ah u kala duwan luqada- Ruush iyo Ingiriis. Sababyadayada waxaa bixiya arimo xiiseysan oo ka gees ah garashada caadiga ah sida aqoonta luuqada looga jeedo, waxayna muujiyaan in xoolaha qaarkood lagu barto si isku mid ah, haba yeeshee kala duwan luuqada.</abstract_so>
      <abstract_ur>پہلے تدریس کی تغییر کی زبان مدل کی موفقیت نے بہت زیادہ علاقه پیدا کی ہے کہ یہ مدل کس طرح کام کرتے ہیں اور ان کی زبان کے بارے میں کیا سیکھتے ہیں۔ لیکن مکان میں پہلے کی تحقیقات انگلیسی کے لئے مطابق ہے، اور بہت ہی کم لوگ دوسری زبانوں کے بارے میں جانتے ہیں۔ اس کے لئے ہم روسین ٹیوال کو معرفی کرتے ہیں، روسی کے لئے 14 پرینڈنگ کام کا ایک بڑھایا مجموعہ ہے، جو اب تک اچھی طرح نہیں کی گئی ہیں۔ ہم پیچھے زبان کی مختلف اختلافات کا تقسیم کرنے کے لئے پینچ زبان کی مختلف تبدیل کرنے والوں کے لئے دو ٹیپولوژیکی زبانوں کے مقابلہ میں استعمال کرتے ہیں - روسی اور انگلیسی زبانوں کے لئے۔ ہمارے نتیجے ان باتوں کی مخالفت کرتی ہیں جو زبان علم کی تعریف کیسی ہے اور دکھاتے ہیں کہ بعض اختلاف کی تعریف ایسی طریقہ سے سیکھی جاتی ہیں۔</abstract_ur>
      <abstract_ta>முன் பயிற்சி மொழி மாற்ற மாதிரிகளின் வெற்றியம் இந்த மாதிரிகள் எப்படி வேலை செய்கிறார்கள் என்பதைப் பற்றியும் மொழிய ஆனால், புலத்தில் முன்னால் ஆராய்ச்சி முக்கியமாக ஆங்கிலத்திற்கு தேவைப்படுகிறது, மற்றும் மற்ற மொழிகள இந்த முடிவிற்கு, நாம் ரூசென்ட்வெல், ஒரு 14 மேம்படுத்தப்பட்ட பணிகளை அறிமுகப்படுத்துகிறோம், இன்னும் தீர்வு செய்யப்படவில்லை  ஐந்து மொழி மாற்றங்களில் பங்கிடும் பல மொழி மாற்றங்களில் இரண்டு வழக்கமான மொழிகளுக்கு ருஷ்யன் மற்றும் ஆங்கிலத்திற்கான மொழிகளுக்கு நாம்  எங்கள் முடிவுகள் மொழி வித்தியாசமான மொழியின் வித்தியாசமான அறிவு எவ்வாறு குறிப்பிடுகிறது என்று பொதுவான தெரிவுகளை கொடுக்கும் மொ</abstract_ta>
      <abstract_uz>Taʼminlovchi o'zgartirish modellari muvaffaqiyatli bu modellar qanday ishlaydigan va ular tilning haqida o'rganishlari haqida juda qiziqarli bo'lgan. Lekin maydondagi birinchi o'rganishdan avval ingliz tilga tayyorlangan va boshqa tillar haqida qisqa bilmagan. Shunday qilib, biz RuSentEval orqali ko'proq RusentEval orqali ko'proq orqali ko'proq orqali ko'proq qilamiz, va bu yerda o'ylab topilmagan narsalarni ko'rsamiz. Biz bir necha tildagi o'zgarishlarda ikkita odatda o'zgarishni qo'llab-qo'llash uchun bir komplimentar tizimni qo'llayapmiz. Ruscha va ingliz tilida har hil tillarda har xil xossalarni ajratish uchun. Our results provide intriguing findings that contradict the common understanding of how linguistic knowledge is represented, and demonstrate that some properties are learned in a similar manner despite the language differences.</abstract_uz>
      <abstract_vi>Sự thành công của các mô hình ngôn ngữ biến đổi được huấn luyện đã khiến cho nhiều người quan tâm đến cách làm việc của các mô hình này, và những gì họ học về ngôn ngữ. Tuy nhiên, nghiên cứu trước trong lĩnh vực này chủ yếu dành cho tiếng Anh, và ít được biết về các ngôn ngữ khác. Để đạt được mục đích, chúng tôi xin giới thiệu RuffEvl, một loạt các nhiệm vụ thăm dò phát triển cho người Nga, bao gồm những nhiệm vụ chưa được khám phá. Chúng tôi sử dụng một loạt các phương pháp thăm dò bổ sung để khám phá sự phân phối các đặc tính ngôn ngữ khác nhau trong năm máy biến đổi đa dạng cho hai ngôn ngữ khác nhau (kiểu Nga) và Anh. Những kết quả của chúng tôi đưa ra những kết quả hấp dẫn trái với sự hiểu chung về cách đại diện kiến thức ngôn ngữ, và chứng minh rằng một số tài sản được học theo một cách tương tự, bất chấp sự khác biệt ngôn ngữ.</abstract_vi>
      <abstract_bg>Успехът на предварително обучените трансформаторни езикови модели донесе голям интерес за това как работят тези модели и какво научават за езика. Въпреки това, предишните изследвания в областта са посветени главно на английски, и малко е известно по отношение на други езици. За тази цел представяме разширен набор от 14 пробни задачи за руски език, включително и такива, които все още не са проучени. Прилагаме комбинация от допълващи се методи за изследване на разпределението на различни езикови свойства в пет многоезични трансформатора за два типологично контрастиращи езика - руски и английски. Нашите резултати предоставят интригуващи открития, които противоречат на общото разбиране за това как езиковото знание е представено и демонстрират, че някои свойства се научават по подобен начин въпреки езиковите различия.</abstract_bg>
      <abstract_da>Succesen med forududdannede transformersprogsmodeller har bragt stor interesse for, hvordan disse modeller fungerer, og hvad de lærer om sprog. Tidligere forskning på området er imidlertid hovedsagelig dedikeret til engelsk, og der vides lidt om andre sprog. Til dette formål introducerer vi RuSentEval, et forbedret sæt af 14 sondeopgaver for russisk, herunder dem, der endnu ikke er blevet udforsket. Vi anvender en kombination af supplerende sondemetoder til at undersøge fordelingen af forskellige sproglige egenskaber i fem flersprogede transformatorer til to typologisk kontrasterende sprog - russisk og engelsk. Vores resultater giver spændende resultater, der modsiger den fælles forståelse af, hvordan sproglig viden repræsenteres, og viser, at nogle egenskaber læres på samme måde trods sprogforskellene.</abstract_da>
      <abstract_de>Der Erfolg von vortrainierten Transformator-Sprachmodellen hat großes Interesse daran geweckt, wie diese Modelle funktionieren und was sie über Sprache lernen. Bisherige Forschung auf diesem Gebiet widmet sich jedoch hauptsächlich dem Englischen, und über andere Sprachen ist wenig bekannt. Zu diesem Zweck stellen wir RuSentEval vor, ein erweitertes Set von 14-Sondierungsaufgaben für Russisch, einschließlich solcher, die noch nicht erforscht wurden. Wir verwenden eine Kombination komplementärer Sondierungsmethoden, um die Verteilung verschiedener linguistischer Eigenschaften in fünf mehrsprachigen Transformatoren für zwei typologisch gegensätzliche Sprachen – Russisch und Englisch – zu untersuchen. Unsere Ergebnisse liefern faszinierende Erkenntnisse, die dem allgemeinen Verständnis der Repräsentation von sprachlichem Wissen widersprechen und zeigen, dass einige Eigenschaften trotz der sprachlichen Unterschiede in ähnlicher Weise erlernt werden.</abstract_de>
      <abstract_nl>Het succes van voorgetrainde transformatortaalmodellen heeft veel belangstelling gewekt voor hoe deze modellen werken en wat ze leren over taal. Eerder onderzoek op dit gebied is echter voornamelijk gewijd aan het Engels, en er is weinig bekend over andere talen. Daartoe introduceren we RuSentEval, een uitgebreide set van 14-sondetaken voor Russisch, inclusief taken die nog niet zijn onderzocht. We passen een combinatie van complementaire sonderingsmethoden toe om de verdeling van verschillende linguïstische eigenschappen in vijf meertalige transformatoren te onderzoeken voor twee typologisch contrasterende talen: Russisch en Engels. Onze resultaten leveren intrigerende bevindingen op die in tegenspraak zijn met het algemeen begrip van de manier waarop taalkennis wordt vertegenwoordigd, en tonen aan dat sommige eigenschappen ondanks de taalverschillen op dezelfde manier worden geleerd.</abstract_nl>
      <abstract_ko>미리 훈련된transformer 언어 모델의 성공은 이러한 모델이 어떻게 작동하는지, 그리고 언어에 대한 이해에 큰 흥미를 가지게 했다.그러나 그동안 이 분야의 연구는 주로 영어에 집중됐지만 다른 언어에 대한 연구는 드물었다.이를 위해 러시아인을 위한 증강형 14개 탐사 임무인 루센트 에벌을 소개합니다. 아직 탐지하지 않은 임무를 포함합니다.우리는 상호 보완 탐지 방법을 결합하여 러시아어와 영어라는 두 가지 서로 다른 유형의 언어의 다섯 가지 다언어 변형금강 중의 각종 언어 속성의 분포를 탐색했다.우리의 연구 결과는 언어 지식이 어떻게 표현하는지에 대한 공감대와 모순되는 흥미로운 발견을 제공했고 언어 차이가 존재하지만 일부 속성은 유사한 방식으로 학습된 것으로 나타났다.</abstract_ko>
      <abstract_fa>موفقیت مدلهای زبان تغییر دهنده پیش آموزش داده شده بسیار علاقه ای به چگونه این مدلها کار می کنند، و آنچه در مورد زبان یاد می گیرند. با این حال، تحقیقات قبلی در زمینه در اصل به انگلیسی تعلق دارد و در مورد زبانهای دیگر کمی شناخته می شود. برای این قسمت، ما روسنت Eval را معرفی می کنیم، یک مجموعه بیشتر از 14 کار تحقیق برای روسیه، شامل کسانی که هنوز تحقیق نشده اند. ما یک ترکیب از روش تحقیقات اضافه برای تحقیق توزیع گونه‌های زبان‌شناسی در پنج تغییردهنده‌های متعدد زبان‌شناسی برای دو زبان‌های متفاوت‌شناسی نوع‌شناسی - روسیه و انگلیسی استفاده می‌کنیم. نتیجه‌هایمان نتیجه‌های جالبی را پیشنهاد می‌دهند که با وجود تفاوت زبان‌های مشترک درباره‌ی دانش زبان‌شناسی چگونه نمایش داده می‌شود، و نشان می‌دهند که بعضی از ویژه‌ها با وجود تفاوت زبان‌ها به</abstract_fa>
      <abstract_id>Sukses dari model bahasa transformer yang terlatih telah membawa banyak minat tentang bagaimana model ini bekerja, dan apa yang mereka belajar tentang bahasa. Namun, penelitian sebelumnya di lapangan ini terutama didedikasikan kepada bahasa Inggris, dan sedikit dikenal mengenai bahasa lain. Untuk tujuan ini, kami memperkenalkan RuSentEval, sebuah set meningkat 14 tugas penyelidikan untuk Rusia, termasuk tugas yang belum dikeksplorasi. Kami menerapkan kombinasi metode pengujian komplementari untuk mengeksplorasi distribusi berbagai properti bahasa dalam lima transformator berbagai bahasa untuk dua bahasa tipologis yang bertentangan - Rusia dan Inggris. Hasil kami menyediakan penemuan menarik yang bertentangan dengan pemahaman umum tentang bagaimana pengetahuan bahasa diwakili, dan menunjukkan bahwa beberapa properti belajar dengan cara yang sama meskipun perbedaan bahasa.</abstract_id>
      <abstract_hr>Uspjeh predobučenih transformacijskih jezičkih modela dovodio je mnogo interesa o tome kako ovi modeli rade i o tome što uče o jeziku. Međutim, prije istraživanja na terenu je uglavnom posvećena engleskom, a malo je poznato u vezi drugih jezika. Za taj cilj predstavljamo RuSentEval, poboljšan set od 14 zadataka za istraživanje Rusa, uključujući one koje još nisu istraživali. Primjenjujemo kombinaciju dodatnih metoda ispitivanja za istraživanje distribucije različitih jezičkih vlasništva u pet multijezičkih transformatora za dva tipološki suprotnog jezika - ruski i engleski. Naši rezultati pružaju zanimljive nalaze koje se suprotstavljaju zajedničkom razumijevanju kako se predstavljaju jezički znanje, i pokazuju da su neki vlasnici učeni na sličan način uprkos jezičkim razlikama.</abstract_hr>
      <abstract_tr>Öň bilim öňünden üýtgetmeli dil nusgalarynyň başarnygy bu nusgalaryň nähili işleýändigini we dilleri barada öwrenenlerini örän gyzyklandyrdy. Ýöne bu sahada öňki araştyrmalar iñlis diline adatdyr we başga diller barada az tanalýar. Şonuň üçin RuSentEval'i tanyşdyrýarys, we şu wagt öňünde hiç gözlenmedikleri Rus üçin 14 sany barlamak işi bilen tanyşdyryldyk. Biz beş dilli çeşitli diller üçin diňleýän dillerde, Rusça we Iňlisçe çeşitli diller üçin täsirleýän bir süpürler metodlaryny gözlemek üçin üýtgedik. Biziň netijelerimiz dil bilgileriniň nähili täze bir şekilde öwrenmeli bolandygyny we dil üýtgeşiklerine görä birnäçe häsiýetlerin bir şekilde öwrenmeli diýip täsirleýär.</abstract_tr>
      <abstract_am>የቀድሞው የቋንቋ ምሳሌዎች የደረሰለት ስኬት የዚህ ምሳሌዎች እንዴት እንደሚሠራ እና ስለ ቋንቋ የሚማሩትን እጅግ የሚጠቅምበት ማድረግ አመጣላቸው፡፡ ነገር ግን የሜዳ ምርመራ አብዛኛውን ለእንግሊዝኛ የተጠቃሚ ነው፤ ለሌሎች ቋንቋዎች ግን ጥቂት ነው፡፡ ለዚህ ምክንያት፣ ገና ያልተመረመረውን የሩስታንትEval 14 ተጨማሪ ስራዎችን እናስታውቃለን፡፡ በአምስት በብዙ ቋንቋ ለውጦች ላይ ለሁለት ተቃውሞ ቋንቋዎች - ሩሽና እንግሊዘኛ ለመከላከል የሚደረገውን አካባቢ ፈተናዎችን ለመከላከል እናደርጋለን፡፡ ፍሬዎቻችን የቋንቋ እውቀት እንዴት እንደምታሳየው የቋንቋ እውቀት የሚቃወሙትን እና አንዳንዶቹ ባለሥርዓቶች በተለያዩ ልዩነት ቢተማሩ በሚያሳየው ግንኙነት ያሳያል፡፡</abstract_am>
      <abstract_sw>Mafanikio ya mifano ya mabadiliko ya lugha ya zamani yamesababisha maslahi mengi ya namna mifano hii inavyofanya kazi, na kile wanachojifunza kuhusu lugha. Hata hivyo, utafiti wa zamani umejikita kwa Kiingereza, na ni kidogo sana unafahamika kuhusu lugha nyingine. Kwa mwisho huu, tunautambulisha RuSentEval, mfululizo wa shughuli 14 zinazoonyesha Urusi, ikiwa ni pamoja na wale ambao bado hawajadiliwa. Tunatumia muunganiko wa njia za kuchunguza za utaratibu wa lugha mbalimbali katika mabadiliko matano ya lugha kwa lugha mbili zinazotofautiana na lugha mbili - Urusi na Kiingereza. Matokeo yetu yanaleta matokeo ya kusisimua kwamba kinyume na uelewa wa namna maarifa ya lugha inavyowakilishwa, na kuonyesha kwamba baadhi ya utajiri hujifunza kwa namna sawa na pamoja na tofauti za lugha.</abstract_sw>
      <abstract_hy>Նախապատրաստված լեզվի վերափոխողների մոդելների հաջողությունը շատ հետաքրքրություն է բերել այն մասին, թե ինչպես են այս մոդելները աշխատում և ինչ են նրանք սովորում լեզվի մասին: However, prior research in the field is mainly devoted to English, and little is known regarding other languages.  Այս նպատակով, մենք ներկայացնում ենք RuSentewal-ը, որն ունի 14 ուսումնասիրություններ Ռուսաստանի համար, ներառյալ այն, որոնք դեռ չեն ուսումնասիրել: Մենք կիրառում ենք բազմաթիվ լեզվաբանական հատկությունների բաշխման ուսումնասիրելու համար բազմաթիվ լեզվաբանական փոխակերպողների հինգ տարբեր լեզվով երկու տիպոլոգիապես հակադրող լեզուների՝ ռուսերենի և անգլերենի համար համադրված մեթո Մեր արդյունքները ներկայացնում են հետաքրքիր եզրակացություններ, որոնք հակադրում են լեզվաբանական գիտելիքների ներկայացման ընդհանուր հասկացությունը և ցույց են տալիս, որ որոշ հատկություններ սովորվում են նման կերպ, չնայած լեզվաբանական տարբերություններին</abstract_hy>
      <abstract_bn>প্রথম প্রশিক্ষিত ভাষা পরিবর্তন মডেলের সফলতা এই মডেল কিভাবে কাজ করে এবং ভাষার ব্যাপারে তারা কি শিখে তা নিয়ে বেশী আগ্রহী  তবে ক্ষেত্রের পূর্বে গবেষণা মূলত ইংরেজীতে বিশেষ করা হয়েছে এবং অন্যান্য ভাষার ব্যাপারে খুব কম পরিচিত। এই পর্যন্ত আমরা রুসেন্ট ইভালের সাথে পরিচয় করিয়ে দিচ্ছি, রাশিয়ার ১৪টি বাড়িয়ে দেয়া কাজ, যাদের মধ্যে রয়েছে যাদের এখনো সন্ধান করা হয়ন আমরা পাঁচ ভাষার বিভিন্ন ভাষার বিভিন্ন ভাষার বৈশিষ্ট্য বিতরণের ব্যাপারটি ব্যবহার করি যাতে দুটি সাধারণ ভাষায় বিপরীত ভাষার জন্য দুটি ভাষার ব আমাদের ফলাফল তৈরি করেছে কৌতূহলজনক পরিস্থিতি যা ভাষাগত জ্ঞান কিভাবে প্রতিনিধিত্ব করা হয়েছে তার বিরুদ্ধে সাধারণ বুঝতে পারে এবং তা প্রদর্শন</abstract_bn>
      <abstract_af>Die sukses van voorafgevorderde transformeerde taal modele het 'n groot belang gebring oor hoe hierdie modele werk en wat hulle leer oor taal. Maar vooraf ondersoek in die veld is heeltemal aan Engels besluit, en klein is bekend aangaande ander tale. Op hierdie einde, introduseer ons RuSentEval, 'n verbeterde stel van 14 probeertaak vir Russiese, insluitend die wat nog nie uitgevoer is nie. Ons het 'n kombinasie van komplementare probeermetodes aanwend om die verspreiding van verskeie lingvisse eienskappe in vyf multitaalske transformeerders vir twee tipologiese kontrastende tale - Russiese en Engels te exploreer. Ons resultate verskaf intriguerende onderstellings wat teen die gemeenskap verstanding van hoe lingvisse kennis verteenwoordig is, en wys dat sommige eienskappe op 'n gelyke manier geleer word, behalwe die taal verskille.</abstract_af>
      <abstract_sq>Suksesi i modeleve të gjuhës transformuese të paratrajnuar ka sjellë shumë interes në mënyrën se si funksionojnë këto modele dhe atë që mësojnë për gjuhën. Megjithatë, kërkimi i mëparshëm në fushë është kryesisht i përkushtuar anglisht dhe pak është e njohur lidhur me gjuhët e tjera. Për këtë qëllim, ne prezantojmë RuSentEval, një grup të përmirësuar prej 14 detyrash sondazhi për rusët, duke përfshirë ato që nuk janë eksploruar ende. Ne aplikojmë një kombinim të metodave komplementare të vëzhgimit për të eksploruar shpërndarjen e pronave të ndryshme gjuhësore në pesë transformues shumëgjuhës për dy gjuhë tipologjikisht kontrastuese - ruse dhe angleze. Rezultatet tona ofrojnë gjetje intriguese që kundërshtojnë kuptimin e përbashkët të sesi përfaqësohet njohuria gjuhësore dhe demonstrojnë se disa prona mësohen në një mënyrë të ngjashme pavarësisht nga dallimet gjuhësore.</abstract_sq>
      <abstract_cs>Úspěch předškolených transformátorových jazykových modelů přinesl velký zájem o to, jak tyto modely fungují a co se naučí o jazyce. Předchozí výzkum v této oblasti se však věnuje především angličtině a o ostatních jazycích je známo málo. Za tímto účelem představujeme RuSentEval, rozšířený soubor čtrnácti sondovacích úkolů pro Rusko, včetně těch, které dosud nebyly prozkoumány. Používáme kombinaci doplňkových sondačních metod k prozkoumání distribuce různých jazykových vlastností v pěti vícejazyčných transformátorech pro dva typologicky kontrastní jazyky – ruštinu a angličtinu. Naše výsledky poskytují zajímavé poznatky, které jsou v rozporu s běžným chápáním jazykových znalostí reprezentovány, a ukazují, že některé vlastnosti jsou navzdory jazykovým rozdílům učeny podobným způsobem.</abstract_cs>
      <abstract_az>Əvvəlcə təhsil edilmiş transformer dil modellərin başarısızlığı bu modellərin necə işlədiyini və dillərin öyrəndiklərini çox maraqlaşdırdı. Ancaq sahədə əvvəlki araştırmalar ingilis dilinə məxsus edilir və digər dillər haqqında az bilinir. Buna görə biz RuSentEval'i təşkil edirik, Rus üçün daha çox təşkil edilməmiş 14 nəfər təşkil etdik. Biz müxtəlif dil xüsusiyyətlərini beş dildə çoxlu transformatçılar üçün iki tipolojik müxtəlif dillərə - Rus və İngilizce dillərinə təşkil etmək üçün komplementar prob metodlarının birləşdiririk. Bizim sonuçlarımız dil bilgisinin necə təşkil edildiyini və bəzi özelliklərin dil fərqliyinə baxmayaraq bənzər bir yolla öyrəndiyini göstərir.</abstract_az>
      <abstract_et>Eelnevalt koolitatud transformaatorkeelemudelite edu on toonud suurt huvi selle kohta, kuidas need mudelid töötavad ja mida nad keele kohta õpivad. Kuid varasemad uuringud valdkonnas on pühendatud peamiselt inglise keele ja vähe on teada teiste keelte kohta. Sel eesmärgil tutvustame RuSentEvali, mis koosneb 14 vene keele proovimisülesandest, sealhulgas neist, mida pole veel uuritud. Rakendame täiendavate proovimeetodite kombinatsiooni, et uurida erinevate keeleliste omaduste jaotumist viies mitmekeelses transformaatoris kahe tüpoloogiliselt kontrastse keele - vene ja inglise keele jaoks. Meie tulemused annavad intrigeerivaid tulemusi, mis on vastuolus ühise arusaamaga, kuidas keeleteadmisi esindatakse, ning näitavad, et mõningaid omadusi õpitakse sarnaselt vaatamata keelelistele erinevustele.</abstract_et>
      <abstract_fi>Esikoulutettujen muuntajien kielimallien menestys on herättänyt suurta kiinnostusta siitä, miten nämä mallit toimivat ja mitä ne oppivat kielestä. Aiempi tutkimus alalla on kuitenkin omistettu pääasiassa englannille, ja muista kielistä tiedetään vähän. Tässä tarkoituksessa esittelemme RuSentEvalin, joka on parannettu joukko 14 luotaustehtävää venäjälle, mukaan lukien ne, joita ei ole vielä tutkittu. Sovellamme täydentävien luotausmenetelmien yhdistelmää tutkiaksemme erilaisten kielellisten ominaisuuksien jakautumista viidessä monikielisessä muuntajassa kahdelle typologisesti vastakkaiselle kielelle - venäjälle ja englannille. Tuloksemme tarjoavat kiehtovia havaintoja, jotka ovat ristiriidassa yleisen ymmärryksen kielellisen tiedon edustuksesta ja osoittavat, että joitain ominaisuuksia opitaan samalla tavalla kielieroista huolimatta.</abstract_fi>
      <abstract_bs>Uspjeh predobučenih transformacijskih jezičkih modela dovodio je dosta interesa o tome kako ovi modeli rade, i o tome što oni uče o jeziku. Međutim, prije istraživanja na terenu je uglavnom posvećena engleskom, a malo je poznato u vezi drugih jezika. Za taj cilj predstavljamo RuSentEval, povećan set od 14 zadataka za istraživanje ruskih, uključujući one koje još nisu istraživali. Primjenjujemo kombinaciju dodatnih metoda ispitivanja za istraživanje distribucije različitih jezičkih vlasništva u pet multijezičkih transformatora za dva tipološki kontrastnog jezika - ruski i engleski. Naši rezultati pružaju zanimljive nalaze koje se suprotstavljaju zajedničkom razumijevanju kako je jezičko znanje predstavljeno, i pokazuju da su neki vlasnici naučeni na sličan način uprkos jezičkim razlikama.</abstract_bs>
      <abstract_ca>The success of pre-trained transformer language models has brought a great deal of interest on how these models work, and what they learn about language.  However, prior research in the field is mainly devoted to English, and little is known regarding other languages.  To this end, we introduce RuSentEval, an enhanced set of 14 probing tasks for Russian, including ones that have not been explored yet.  Aplicam una combinació de mètodes complementars d'investigació per explorar la distribució de diverses propietats lingüístices en cinc transformadors multilingües per dues llengües tipològicament contrastants - russos i anglès. Els nostres resultats proporcionen descobriments intrigants que contradicten la comprensió comú de com es representa el coneixement lingüístic, i demostren que algunes propietats s'aprenen d'una manera similar malgrat les diferències lingüístices.</abstract_ca>
      <abstract_jv>Rasané model sing beranduwé, sampeyan ingkang sampeyan luwih dumateng kuwi kesempatan tentang karo ngono kuwi model iki ngono ngono kuwi jenis sira nggawe barang kelas. Nanging, sabanjur-sabanjuré sak ing sakjane kanggo ingkang, lan akeh cilik diangkat barang langgar. Nambah iki, kita nganggep nyengke RSentDebal, 14 awak dhéwé nggawe perbudhakan kanggo Russisa, sisan sing ora bisa kejahake. Awak dhéwé éntuk karo perusahaan karo perusahaan langkung sampeyan karo nganggep nggawe layakno karo hal-sampeyan ingkang sampeyan luwih Rejalaké awak dhéwé nggawe barang kelas kuwi bagasara karo paké kesempatan karo akeh langkung sapa-aké awak dhéwé, lan ngomongke tindakan karo perusahaan karo perusahaan sing dipunangé karo koyo cah-cah sabané, mengko perusahaan langkung sampey</abstract_jv>
      <abstract_he>הצלחה של דוגמני שפת משתנים מאומנים מראש הביאה הרבה עניין על איך הדוגמנים הללו עובדים, ומה הם לומדים על שפה. עם זאת, מחקר קודם בשטח מוקדש בעיקר לאנגלית, ומעט ידוע בנוגע לשפות אחרות. למטרה זו, אנחנו מציגים את RuSentEval, קבוצה משופרת של 14 משימות חקירה לרוסי, כולל אלה שלא נחקרו עדיין. אנו משתמשים בשילוב של שיטות חקירה תוספות כדי לחקור את ההפצה של תכונות שפתיים שונות בחמישה משתנים רבות שפתיים לשתי שפות שונות טיפולוגית - רוסית ואנגלית. התוצאות שלנו מספקות מציאות מעניינות שמתנגדות להבנה המשותפת של כיצד מייצג ידע שפתי, ולהראות שמספר תכונות נלמדות באופן דומה למרות ההבדלים בשפה.</abstract_he>
      <abstract_sk>Uspeh vnaprej usposobljenih transformatorskih jezikovnih modelov je prinesel veliko zanimanja za delovanje teh modelov in kaj se naučijo o jeziku. Vendar pa so predhodne raziskave na tem področju predvsem namenjene angleščini, o drugih jezikih pa je malo znano. V ta namen predstavljamo RuSentEval, izboljšan nabor 14 nalog sondiranja za ruski jezik, vključno s tistimi, ki še niso bile raziskane. Uporabljamo kombinacijo dopolnilnih merilnih metod za raziskovanje porazdelitve različnih jezikovnih lastnosti v petih večjezičnih transformatorjih za dva tipološko kontrastna jezika - ruski in angleški. Naši rezultati ponujajo zanimive ugotovitve, ki nasprotujejo skupnemu razumevanju, kako je jezikovno znanje predstavljeno, in kažejo, da se nekatere lastnosti kljub jezikovnim razlikam naučijo na podoben način.</abstract_sk>
      <abstract_ha>Haƙĩƙa, masu cin nasara da misãlai masu motsi da aka tsare ta gabãni ya zo da amfani mai yawa a kan jinsi misãlai ke aiki, da abin da suke karanta game da harshen. A lokacin da, kafin da za'a yi amfani da yin kawaici a cikin birnin, yana da amfani kaɗan a kan harshen dabam. Ga wannan, Munã ƙara RucentEal, mai ƙaranci matsayin 14 na taskõki na Ruushi, da waɗannan da ba a riga ba. Munã amfani da komai da shiryoyin jarraba masu kamfata ko kuma don su sami raba-rabo masu cikin littafan lingui shan cikin shifottori masu mulki-lingui, wa'ura da Ingiriya biyu masu motsi a cikin harshen-rubuci. MataimakinMu na bãyar da fassaran mai fasahawa da ke motsi ga fahimcin da ke da jinsi ilimi na lugha, kuma ya nuna cewa, za'a sanar da wasu properties da misãlin, kuma kõ da sãɓãnin harshe.</abstract_ha>
      <abstract_bo>སྔོན་གྱིས་བསླབ་པའི་སྔོན་གྱིས་འགྱུར་བ་ཅན་གྱི་སྐད་རིགས་དཔེ་གཞི་འདི་དག་གིས་ཇི་ལྟར་བྱེད་སྣང་ཆེན་ཤུགས་ཀྱི་ཡོད། འོན་ཀྱང་། མ་ཟད། རང་གི་སྒེར་གྱི་འཚོལ་ཞིབ་འདི་ལ་དབྱིན་ཡིག་དང་། སྐད་རིགས་གཞན་ཞིག་ལ་ཆ་ཁ་ཤས་མེད། འདི་ལྟར། ང་ཚོས་RuSentEval ལ་ངོ་སྤྲོད་བྱས་པ་ཡིན། རྒྱ་ནག་གི་དྲ་རྒྱ་ལྟར་ལྟ་ཞིབ་བྱས་པའི་ལས་ཀ་ཆ་༡༤་ཞིག ང་ཚོས་སྐད ང་ཚོའི་མཐོང་སྣང་ཚུལ་མང་པོ་ཞིག་ཡིན་པའི་སྐད་རིགས་ཤེས་ཀྱི་གསལ་བཤད་ལ་མཐོང་མི་རྣམས་དང་། སྐད་རིགས་ལ་ཁྱད་པར་ལ་རྐྱེན་བྱས་པར་ལ</abstract_bo>
      </paper>
    <paper id="7">
      <title>Exploratory Analysis of News Sentiment Using Subgroup Discovery</title>
      <author><first>Anita</first><last>Valmarska</last></author>
      <author><first>Luis Adrián</first><last>Cabrera-Diego</last></author>
      <author><first>Elvys</first><last>Linhares Pontes</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>66–72</pages>
      <abstract>In this study, we present an exploratory analysis of a Slovenian news corpus, in which we investigate the association between named entities and <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiment</a> in the <a href="https://en.wikipedia.org/wiki/News">news</a>. We propose a methodology that combines Named Entity Recognition and Subgroup Discovery-a descriptive rule learning technique for identifying groups of examples that share the same class label (sentiment) and pattern (features-Named Entities). The approach is used to induce the positive and negative sentiment class rules that reveal interesting patterns related to different Slovenian and international politicians, organizations, and locations.</abstract>
      <url hash="0b7585f4">2021.bsnlp-1.7</url>
      <bibkey>valmarska-etal-2021-exploratory</bibkey>
    </paper>
    <paper id="8">
      <title>Creating an Aligned Russian Text Simplification Dataset from Language Learner Data<fixed-case>R</fixed-case>ussian Text Simplification Dataset from Language Learner Data</title>
      <author><first>Anna</first><last>Dmitrieva</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>73–79</pages>
      <abstract>Parallel language corpora where regular texts are aligned with their simplified versions can be used in both <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> and <a href="https://en.wikipedia.org/wiki/Theoretical_linguistics">theoretical linguistic studies</a>. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts. Today, there exist a few parallel datasets for <a href="https://en.wikipedia.org/wiki/English_language">English</a> and Simple English, but many other languages lack such <a href="https://en.wikipedia.org/wiki/Data">data</a>. In this paper we describe our work on creating an aligned Russian-Simple Russian dataset composed of <a href="https://en.wikipedia.org/wiki/Russian_language">Russian literature texts</a> adapted for learners of <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a> as a foreign language. This will be the first parallel dataset in this domain, and one of the first Simple Russian datasets in general.</abstract>
      <url hash="426a81b6">2021.bsnlp-1.8</url>
      <bibkey>dmitrieva-tiedemann-2021-creating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="9">
      <title>Multilingual Named Entity Recognition and Matching Using <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> and Dedupe for <a href="https://en.wikipedia.org/wiki/Slavic_languages">Slavic Languages</a><fixed-case>BERT</fixed-case> and Dedupe for <fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Marko</first><last>Prelevikj</last></author>
      <author><first>Slavko</first><last>Zitnik</last></author>
      <pages>80–85</pages>
      <abstract>This paper describes the University of Ljubljana (UL FRI) Group’s submissions to the shared task at the Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop. We experiment with multiple BERT-based models, pre-trained in multi-lingual, Croatian-Slovene-English and Slovene-only data. We perform training iteratively and on the concatenated data of previously available NER datasets. For the normalization task we use Stanza lemmatizer, while for <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity matching</a> we implemented a <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> using the Dedupe library. The performance of evaluations suggests that multi-source settings outperform less-resourced approaches. The best NER models achieve 0.91 <a href="https://en.wikipedia.org/wiki/F-score">F-score</a> on Slovene training data splits while the best official submission achieved F-scores of 0.84 and 0.78 for relaxed partial matching and strict settings, respectively. In multi-lingual NER setting we achieve <a href="https://en.wikipedia.org/wiki/F-number">F-scores</a> of 0.82 and 0.74.</abstract>
      <url hash="6d54dfef">2021.bsnlp-1.9</url>
      <bibkey>prelevikj-zitnik-2021-multilingual</bibkey>
    </paper>
    <paper id="13">
      <title>Benchmarking Pre-trained Language Models for Multilingual NER : TraSpaS at the BSNLP2021 Shared Task<fixed-case>NER</fixed-case>: <fixed-case>T</fixed-case>ra<fixed-case>S</fixed-case>pa<fixed-case>S</fixed-case> at the <fixed-case>BSNLP</fixed-case>2021 Shared Task</title>
      <author><first>Marek</first><last>Suppa</last></author>
      <author><first>Ondrej</first><last>Jariabka</last></author>
      <pages>105–114</pages>
      <abstract>In this paper we describe TraSpaS, a submission to the third shared task on <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> hosted as part of the Balto-Slavic Natural Language Processing (BSNLP) Workshop. In it we evaluate various pre-trained language models on the NER task using three open-source NLP toolkits : character level language model with Stanza, language-specific BERT-style models with SpaCy and Adapter-enabled XLM-R with Trankit. Our results show that the Trankit-based models outperformed those based on the other two toolkits, even when trained on smaller amounts of data. Our code is available at.<url>https://github.com/NaiveNeuron/slavner-2021</url>.</abstract>
      <url hash="f4d2ae62">2021.bsnlp-1.13</url>
      <bibkey>suppa-jariabka-2021-benchmarking</bibkey>
      <pwccode url="https://github.com/naiveneuron/slavner-2021" additional="false">naiveneuron/slavner-2021</pwccode>
    </paper>
    <paper id="14">
      <title>Named Entity Recognition and Linking Augmented with Large-Scale Structured Data</title>
      <author><first>Paweł</first><last>Rychlikowski</last></author>
      <author><first>Bartłomiej</first><last>Najdecki</last></author>
      <author><first>Adrian</first><last>Lancucki</last></author>
      <author><first>Adam</first><last>Kaczmarek</last></author>
      <pages>115–121</pages>
      <abstract>In this paper we describe our submissions to the 2nd and 3rd SlavNER Shared Tasks held at BSNLP 2019 and BSNLP 2021, respectively. The tasks focused on the analysis of Named Entities in multilingual Web documents in <a href="https://en.wikipedia.org/wiki/Slavic_languages">Slavic languages</a> with rich inflection. Our <a href="https://en.wikipedia.org/wiki/Solution">solution</a> takes advantage of large collections of both unstructured and structured documents. The former serve as data for unsupervised training of <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> and embeddings of lexical units. The latter refers to <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> and its structured counterpart-Wikidata, our source of <a href="https://en.wikipedia.org/wiki/Lemmatization">lemmatization rules</a>, and real-world entities. With the aid of those resources, our system could recognize, normalize and link entities, while being trained with only small amounts of labeled data.</abstract>
      <url hash="a64f5707">2021.bsnlp-1.14</url>
      <bibkey>rychlikowski-etal-2021-named</bibkey>
    </paper>
    <paper id="15">
      <title>Slav-NER : the 3rd Cross-lingual Challenge on Recognition, Normalization, <a href="https://en.wikipedia.org/wiki/Language_classification">Classification</a>, and Linking of Named Entities across <a href="https://en.wikipedia.org/wiki/Slavic_languages">Slavic Languages</a><fixed-case>NER</fixed-case>: the 3rd Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across <fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Jakub</first><last>Piskorski</last></author>
      <author><first>Bogdan</first><last>Babych</last></author>
      <author><first>Zara</first><last>Kancheva</last></author>
      <author><first>Olga</first><last>Kanishcheva</last></author>
      <author><first>Maria</first><last>Lebedeva</last></author>
      <author><first>Michał</first><last>Marcińczuk</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <author><first>Lidia</first><last>Pivovarova</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <author><first>Pavel</first><last>Přibáň</last></author>
      <author><first>Ivaylo</first><last>Radev</last></author>
      <author><first>Marko</first><last>Robnik-Sikonja</last></author>
      <author><first>Vasyl</first><last>Starko</last></author>
      <author><first>Josef</first><last>Steinberger</last></author>
      <author><first>Roman</first><last>Yangarber</last></author>
      <pages>122–133</pages>
      <abstract>This paper describes Slav-NER : the 3rd Multilingual Named Entity Challenge in <a href="https://en.wikipedia.org/wiki/Slavic_languages">Slavic languages</a>. The tasks involve recognizing mentions of named entities in <a href="https://en.wikipedia.org/wiki/Web_page">Web documents</a>, normalization of the names, and cross-lingual linking. The Challenge covers six languages and five entity types, and is organized as part of the 8th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference. Ten teams participated in the competition. Performance for the named entity recognition task reached 90 % <a href="https://en.wikipedia.org/wiki/F-measure">F-measure</a>, much higher than reported in the first edition of the Challenge. Seven teams covered all six languages, and five teams participated in the cross-lingual entity linking task. Detailed valuation information is available on the shared task web page.</abstract>
      <url hash="33f2f6b7">2021.bsnlp-1.15</url>
      <bibkey>piskorski-etal-2021-slav</bibkey>
    </paper>
  </volume>
</collection>