<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.nlp4convai">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI</booktitle>
      <editor><first>Tsung-Hsien</first><last>Wen</last></editor>
      <editor><first>Asli</first><last>Celikyilmaz</last></editor>
      <editor><first>Zhou</first><last>Yu</last></editor>
      <editor><first>Alexandros</first><last>Papangelis</last></editor>
      <editor><first>Mihail</first><last>Eric</last></editor>
      <editor><first>Anuj</first><last>Kumar</last></editor>
      <editor><first>Iñigo</first><last>Casanueva</last></editor>
      <editor><first>Rushin</first><last>Shah</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="ce634d6f">2020.nlp4convai-1</url>
    </meta>
    <frontmatter>
      <url hash="a9c1804c">2020.nlp4convai-1.0</url>
      <bibkey>nlp4convai-2020-natural</bibkey>
    </frontmatter>
    <paper id="4">
      <title>How to Tame Your Data : <a href="https://en.wikipedia.org/wiki/Data_augmentation">Data Augmentation</a> for Dialog State Tracking</title>
      <author><first>Adam</first><last>Summerville</last></author>
      <author><first>Jordan</first><last>Hashemi</last></author>
      <author><first>James</first><last>Ryan</last></author>
      <author><first>William</first><last>Ferguson</last></author>
      <pages>32–37</pages>
      <abstract>Dialog State Tracking (DST) is a problem space in which the effective vocabulary is practically limitless. For example, the domain of possible <a href="https://en.wikipedia.org/wiki/Film_title_design">movie titles</a> or restaurant names is bound only by the limits of language. As such, DST systems often encounter out-of-vocabulary words at inference time that were never encountered during training. To combat this issue, we present a targeted data augmentation process, by which a practitioner observes the types of errors made on held-out evaluation data, and then modifies the training data with additional corpora to increase the vocabulary size at training time. Using this with a RoBERTa-based Transformer architecture, we achieve state-of-the-art results in comparison to systems that only mask trouble slots with special tokens. Additionally, we present a data-representation scheme for seamlessly retargeting DST architectures to new domains.</abstract>
      <url hash="31dafe5a">2020.nlp4convai-1.4</url>
      <doi>10.18653/v1/2020.nlp4convai-1.4</doi>
      <video href="http://slideslive.com/38929634" />
      <bibkey>summerville-etal-2020-tame</bibkey>
    </paper>
    <paper id="5">
      <title>Efficient Intent Detection with Dual Sentence Encoders</title>
      <author><first>Iñigo</first><last>Casanueva</last></author>
      <author><first>Tadas</first><last>Temčinas</last></author>
      <author><first>Daniela</first><last>Gerz</last></author>
      <author><first>Matthew</first><last>Henderson</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <pages>38–45</pages>
      <abstract>Building conversational systems in new domains and with added functionality requires resource-efficient models that work under low-data regimes (i.e., in few-shot setups). Motivated by these requirements, we introduce intent detection methods backed by pretrained dual sentence encoders such as USE and ConveRT. We demonstrate the usefulness and wide applicability of the proposed intent detectors, showing that : 1) they outperform intent detectors based on fine-tuning the full BERT-Large model or using BERT as a fixed black-box encoder on three diverse intent detection data sets ; 2) the gains are especially pronounced in few-shot setups (i.e., with only 10 or 30 annotated examples per intent) ; 3) our intent detectors can be trained in a matter of minutes on a single CPU ; and 4) they are stable across different hyperparameter settings. In hope of facilitating and democratizing research focused on intention detection, we release our code, as well as a new challenging single-domain intent detection dataset comprising 13,083 annotated examples over 77 intents.</abstract>
      <url hash="c3636fef">2020.nlp4convai-1.5</url>
      <doi>10.18653/v1/2020.nlp4convai-1.5</doi>
      <attachment type="Dataset" hash="0666d8ca">2020.nlp4convai-1.5.Dataset.zip</attachment>
      <video href="http://slideslive.com/38929632" />
      <bibkey>casanueva-etal-2020-efficient</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    <title_ar>الكشف الفعال عن النوايا باستخدام تشفير الجمل المزدوجة</title_ar>
      <title_pt>Detecção de intenção eficiente com codificadores de sentença dupla</title_pt>
      <title_fr>Détection d'intention efficace grâce aux codeurs de phrases doubles</title_fr>
      <title_es>Detección eficiente de intenciones con codificadores de doble frase</title_es>
      <title_ja>デュアル文章エンコーダによる効率的なインテント検出</title_ja>
      <title_zh>用双句编码器高效意向检测</title_zh>
      <title_ru>Эффективное обнаружение намерений с помощью кодеров с двойным предложением</title_ru>
      <title_hi>दोहरी वाक्य Encoders के साथ कुशल इरादा का पता लगाने</title_hi>
      <title_ga>Brath Éifeachtach Rúin le Ionchódóirí Dé-Abairte</title_ga>
      <title_ka>Comment</title_ka>
      <title_hu>Hatékony szándék felismerése kettős mondatkódolókkal</title_hu>
      <title_el>Αποτελεσματική ανίχνευση προθέσεων με διπλούς κωδικοποιητές φράσεων</title_el>
      <title_it>Rilevamento efficiente delle intenzioni con codificatori a doppia frase</title_it>
      <title_lt>Veiksmingas ketinimų nustatymas naudojant dvigubo sakinio kodus</title_lt>
      <title_kk>Екінші сөз кодерлерімен жұмыс жеткілікті анықтау</title_kk>
      <title_ms>Pengesanan Intent Efisien dengan Pengekod Dua Putusan</title_ms>
      <title_ml>രണ്ടാമത്തെ ശിക്ഷ കുറിപ്പുകളുമായി ഉള്ളിലുള്ള തിരിച്ചറിയുക</title_ml>
      <title_mk>Efficient Intent Detection with Dual Sentence Encoders</title_mk>
      <title_mt>Sejbien Effiċjenti bl-Intenzjoni b’Kodifikaturi ta’ Sentenza Doppja</title_mt>
      <title_pl>Wydajne wykrywanie intencji za pomocą podwójnych koderów zdań</title_pl>
      <title_no>Effektivt intent oppdaging med dobbelte teiknkoder</title_no>
      <title_ro>Detectare eficientă a intențiilor cu codoare duble de sentință</title_ro>
      <title_so>Aqoonshaha faa’iido ah ee la xiriira Heeganka labaad</title_so>
      <title_si>දුවල් වාක්ය සංකේතකය සමඟ හොයාගන්න හොයාගන්න</title_si>
      <title_ta>இருமுறை வாக்குறியீடுகளுடன் தேவையான உள்ளீட்டு கண்டுபிடிப்பு</title_ta>
      <title_mn>Хоёрдугаар өгүүлбэр кодчуудтай эзэмшигтэй ухаантай олох</title_mn>
      <title_ur>دوئل سنٹنس کوڈر کے ساتھ عمدہ تلاش</title_ur>
      <title_sr>Učinjena intenzivna otkrića sa dvostrukim koderima kazne</title_sr>
      <title_sv>Effektiv avsiktsdetektering med dubbla meningskoder</title_sv>
      <title_vi>Phát hiện ý thức hiệu quả với mã số phát âm kép</title_vi>
      <title_uz>Ikki marta sertifikatlarni aniqlash muvaffaqiyatsiz tugadi</title_uz>
      <title_bg>Ефективно откриване на намерения с двойни кодери за изречения</title_bg>
      <title_hr>Učinjena intenzivna otkrića s dvostrukim koderima kazne</title_hr>
      <title_da>Effektiv hensigtsetektering med dobbelte sætningskodere</title_da>
      <title_nl>Efficiënte intentiedetectie met dubbele zinnencoders</title_nl>
      <title_de>Effiziente Intent Detection mit Dual Satzence Encoder</title_de>
      <title_id>Deteksi Intent Efisien dengan Koder Dua Sentensi</title_id>
      <title_ko>이중 언어 인코더를 사용하는 효율적인 의도 검출</title_ko>
      <title_fa>شناسایی موثری با رمزگران دومین کلمه</title_fa>
      <title_sw>Kugundua Kifaa kinachofanikiwa na Kufunguliwa kwa adhabu mbili</title_sw>
      <title_af>Effektiewe Intent Opdekking met Duele Sentence Encoders</title_af>
      <title_tr>Fatal Sened Ködlemeleri bilen ýeterlik ýere tap</title_tr>
      <title_sq>Detektimi Efektiv i Intentionit me koduesit e dyfishtë</title_sq>
      <title_am>ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s</title_am>
      <title_az>İki Sözü Kodlayıcıları ilə Yetişkin İşləmə</title_az>
      <title_hy>Եֆեկտիվ մտադրական հայտնաբերում երկու նախադասությունների կոդերներով</title_hy>
      <title_bn>দুই শাস্তি এনকোডারের সাথে প্রয়োজনীয় ইন্টেন্ট সনাক্ত</title_bn>
      <title_bs>Učinjena intenzivna otkrića sa dvostrukim koderima kazne</title_bs>
      <title_ca>Detecció d'intenció eficient amb codificadors de doble frase</title_ca>
      <title_cs>Efektivní detekce záměrů pomocí dvojitých snímačů vět</title_cs>
      <title_et>Tõhus kavatsuste tuvastamine kahesuunaliste kodeerijatega</title_et>
      <title_fi>Tehokas aikeiden tunnistus kaksoislausekoodereilla</title_fi>
      <title_sk>Učinkovito zaznavanje namena z dvojnimi kodirniki stavkov</title_sk>
      <title_jv>cage-mode</title_jv>
      <title_ha>@ action</title_ha>
      <title_he>זיהוי כוונה יעיל עם קודים משפטיים כפולים</title_he>
      <title_bo>འཇམ་ཅན་གྱི་ཚིག་རྟགས་སྟོན་པ་དང་མཉམ་དུ་རྟོགས་པ</title_bo>
      <abstract_ar>يتطلب بناء أنظمة محادثة في مجالات جديدة وبوظائف إضافية نماذج ذات كفاءة في استخدام الموارد تعمل في ظل أنظمة منخفضة البيانات (على سبيل المثال ، في إعدادات قليلة اللقطات). بدافع من هذه المتطلبات ، نقدم طرق اكتشاف النية مدعومة بمشفرات الجملة المزدوجة التي تم اختبارها مسبقًا مثل USE و ConveRT. نبرهن على الفائدة والتطبيق الواسع لأجهزة الكشف عن النوايا المقترحة ، موضحين ما يلي: 1) أنها تتفوق على أجهزة الكشف عن النوايا بناءً على الضبط الدقيق لنموذج BERT-Large الكامل أو استخدام BERT كمشفِّر للصندوق الأسود الثابت على ثلاث مجموعات متنوعة من بيانات الكشف عن النية ؛ 2) تظهر المكاسب بشكل خاص في إعدادات قليلة اللقطات (على سبيل المثال ، مع 10 أو 30 مثالاً مشروحًا فقط لكل نية) ؛ 3) يمكن تدريب أجهزة الكشف عن النوايا الخاصة بنا في غضون دقائق على وحدة معالجة مركزية واحدة ؛ و 4) تكون مستقرة عبر إعدادات مختلفة للمعلمات الفائقة. على أمل تسهيل وإضفاء الطابع الديمقراطي على البحث الذي يركز على اكتشاف النية ، قمنا بإصدار الكود الخاص بنا ، بالإضافة إلى مجموعة بيانات جديدة صعبة للكشف عن النية في مجال واحد تضم 13083 مثالاً مشروحًا عبر 77 نية.</abstract_ar>
      <abstract_es>La creación de sistemas conversacionales en nuevos dominios y con funcionalidad adicional requiere modelos eficientes en cuanto a recursos que funcionen bajo regímenes de datos bajos (es decir, en configuraciones de pocas tomas). Motivados por estos requisitos, introducimos métodos de detección de intenciones respaldados por codificadores de doble oración preentrenados, como USE y CONVERt. Demostramos la utilidad y amplia aplicabilidad de los detectores de intención propuestos, demostrando que: 1) superan a los detectores de intención en función del ajuste fino del modelo BERT-Large completo o el uso de BERT como codificador fijo de caja negra en tres conjuntos de datos de detección de intención diversos; 2) las ganancias son especialmente pronunciadas en configuraciones de pocos disparos (es decir, con solo 10 o 30 ejemplos anotados por intento); 3) nuestros detectores de intención se pueden entrenar en cuestión de minutos en una sola CPU; y 4) son estables en diferentes configuraciones de hiperparámetros. Con la esperanza de facilitar y democratizar la investigación centrada en la detección de intenciones, publicamos nuestro código, así como un nuevo y desafiante conjunto de datos de detección de intenciones de un solo dominio que comprende 13 083 ejemplos anotados en 77 intentos.</abstract_es>
      <abstract_fr>La création de systèmes conversationnels dans de nouveaux domaines et avec des fonctionnalités supplémentaires nécessite des modèles économes en ressources qui fonctionnent dans des régimes de faible volume de données (c'est-à-dire dans des configurations peu nombreuses). Motivés par ces exigences, nous introduisons des méthodes de détection d'intention soutenues par des encodeurs de phrases doubles préentraînés tels que USE et Convert. Nous démontrons l'utilité et la large applicabilité des détecteurs d'intention proposés, montrant que : 1) ils surpassent les détecteurs d'intention en ajustant le modèle BERT-large complet ou en utilisant BERT comme encodeur de boîte noire fixe sur trois ensembles de données de détection d'intention différents ; 2) les gains sont particulièrement prononcés dans quelques configurations (c'est-à-dire avec seulement 10 ou 30 exemples annotés par intention) ; 3) nos détecteurs d'intention peuvent être entraînés en quelques minutes sur un seul processeur ; et 4) ils sont stables dans différents paramètres d'hyperparamètres. Dans l'espoir de faciliter et de démocratiser la recherche axée sur la détection d'intention, nous publions notre code, ainsi qu'un nouveau jeu de données stimulant de détection d'intention à domaine unique comprenant 13 083 exemples annotés sur 77 intentions.</abstract_fr>
      <abstract_pt>Construir sistemas de conversação em novos domínios e com funcionalidade adicional requer modelos com eficiência de recursos que funcionem sob regimes de poucos dados (ou seja, em configurações de poucos disparos). Motivados por esses requisitos, apresentamos métodos de detecção de intenção apoiados por codificadores de sentença dupla pré-treinados, como USE e ConveRT. Demonstramos a utilidade e ampla aplicabilidade dos detectores de intenção propostos, mostrando que: 1) eles superam os detectores de intenção com base no ajuste fino do modelo BERT-Large completo ou usando o BERT como um codificador fixo de caixa preta em três conjuntos de dados de detecção de intenção diferentes ; 2) os ganhos são especialmente pronunciados em configurações de poucos tiros (ou seja, com apenas 10 ou 30 exemplos anotados por intenção); 3) nossos detectores de intenção podem ser treinados em questão de minutos em uma única CPU; e 4) eles são estáveis em diferentes configurações de hiperparâmetros. Na esperança de facilitar e democratizar a pesquisa focada na detecção de intenção, lançamos nosso código, bem como um novo conjunto de dados desafiador de detecção de intenção de domínio único, composto por 13.083 exemplos anotados em 77 intenções.</abstract_pt>
      <abstract_hi>नए डोमेन में और अतिरिक्त कार्यक्षमता के साथ संवादात्मक प्रणालियों के निर्माण के लिए संसाधन-कुशल मॉडल की आवश्यकता होती है जो कम-डेटा शासन (यानी, कुछ-शॉट सेटअप में) के तहत काम करते हैं। इन आवश्यकताओं से प्रेरित होकर, हम पहले से प्रशिक्षित दोहरे वाक्य एनकोडर जैसे USE और ConveRT द्वारा समर्थित इरादे का पता लगाने के तरीकों को पेश करते हैं। हम प्रस्तावित इरादे डिटेक्टरों की उपयोगिता और व्यापक प्रयोज्यता का प्रदर्शन करते हैं, यह दिखाते हुए कि: 1) वे पूर्ण BERT-Large मॉडल को ठीक-ट्यूनिंग के आधार पर या तीन विविध इरादे का पता लगाने वाले डेटा सेट पर एक निश्चित ब्लैक-बॉक्स एन्कोडर के रूप में BERT का उपयोग करने के आधार पर इरादे डिटेक्टरों को मात देते हैं; 2) लाभ विशेष रूप से कुछ-शॉट सेटअप में स्पष्ट होते हैं (यानी, प्रति इरादे केवल 10 या 30 एनोटेटेड उदाहरणों के साथ); 3) हमारे इरादे डिटेक्टरों को एक ही सीपीयू पर मिनटों के मामले में प्रशिक्षित किया जा सकता है; और 4) वे विभिन्न हाइपरपैरामीटर सेटिंग्स में स्थिर हैं। इरादे का पता लगाने पर केंद्रित अनुसंधान को सुविधाजनक बनाने और लोकतांत्रिक बनाने की उम्मीद में, हम अपने कोड को जारी करते हैं, साथ ही साथ एक नया चुनौतीपूर्ण एकल-डोमेन इरादा पता लगाने वाला डेटासेट जिसमें 77 इरादों से अधिक 13,083 एनोटेटेड उदाहरण शामिल हैं।</abstract_hi>
      <abstract_ja>新しいドメインで会話システムを構築し、機能を追加するには、低データレジームで動作するリソース効率の良いモデルが必要です（すなわち、ほとんどのショットのセットアップで）。 これらの要件に基づいて、使用やConveRTなどの事前に訓練された二重文エンコーダに裏打ちされたインテント検出方法を導入します。 私たちは、提案されたインテント検出器の有用性と幅広い適用性を実証し、次のことを示します。1 ）完全なBERT - Largeモデルを微調整すること、または3つの多様なインテント検出データセットでBERTを固定ブラックボックスエンコーダとして使用することに基づいてインテント検出器を上回る性能を発揮すること、2 ）ゲインは、数ショットの設定で特に顕著であること（つまり、インテントごとに10または30の注釈付きの例のみ）、3 ）インテント検出器は、単一のCPUで数分で訓練することができること、および4 ）異なるハイパーパラメータ設定にわたって安定していること。 意図の検出に焦点を当てた研究を促進し、民主化することを期待して、私たちはコードと、77の意図にわたる13,083の注釈付きの例を含む新しい挑戦的な単一ドメイン意図検出データセットをリリースします。</abstract_ja>
      <abstract_zh>凡新域有附能之语,统须资源节约型模,(于下数,于少置中)下。 凡此诸推,引入预练双句编码器(如USE与ConveRT)扶持之意。 余证其意检测器有用性广适用性,明:1)其优于调全BERT-Large或用BERT为定黑盒编码器于三异之意检数集上之意探测器。 2)增益于小镜头置中尤明(即,每意惟10或30带注示例)。 3)吾意检测器可于几分钟内练于单CPU。 与4)异参数设置而定。 进民主化注意,发吾代码,与一新挑战性之单域意检数集,其中含77意之13,083注示例。</abstract_zh>
      <abstract_ru>Построение разговорных систем в новых доменах и с дополнительными функциями требует ресурсоэффективных моделей, которые работают в режимах с малым объемом данных (т.е. в настройках с несколькими выстрелами). Основываясь на этих требованиях, мы вводим методы обнаружения намерений, поддерживаемые предварительно обученными двойными кодерами предложений, такими как USE и ConveRT. Мы демонстрируем полезность и широкую применимость предложенных детекторов намерений, показывая, что: 1) они превосходят детекторы намерений, основанные на тонкой настройке полной модели BERT-Large или использовании BERT в качестве фиксированного кодировщика черного ящика на трех различных наборах данных обнаружения намерений; 2) усиления особенно выражены в настройках с несколькими выстрелами (т.е., только с 10 или 30 аннотированными примерами на намерение); 3) наши детекторы намерений могут быть обучены в течение нескольких минут на одном ЦП; и 4) они стабильны при различных настройках гиперпараметров. В надежде облегчить и демократизировать исследования, сосредоточенные на обнаружении намерений, мы выпускаем наш код, а также новый сложный однодоменный набор данных об обнаружении намерений, включающий 13 083 аннотированных примера за 77 намерений.</abstract_ru>
      <abstract_ga>Chun córais chomhrá a thógáil i bhfearainn nua agus le feidhmiúlacht bhreise, teastaíonn samhlacha atá tíosach ar acmhainní a oibríonn faoi réimeanna sonraí íseal (i.e. i socruithe cúpla seat). Arna spreagadh ag na ceanglais seo, tugaimid isteach modhanna braite rún le tacaíocht ó ionchódóirí dé-abairte réamhoilte ar nós USE and ConverT. Léirímid úsáideacht agus infheidhmeacht leathan na mbrathadóirí rún beartaithe, ag taispeáint: 1) go sáraíonn siad na brathadóirí intinne bunaithe ar mhionchoigeartú a dhéanamh ar mhúnla iomlán BERT-Large nó ag baint úsáide as BERT mar ionchódóir bosca dubh seasta ar thrí thacar sonraí braite rún éagsúla. ; 2) tá na gnóthachain le sonrú go háirithe i socruithe cúpla lámhaigh (i.e., gan ach 10 nó 30 sampla anótáilte in aghaidh na hintinne); 3) is féidir ár brathadóirí intinne a oiliúint i gceann cúpla nóiméad ar LAP amháin; agus 4) go bhfuil siad cobhsaí ar fud socruithe hipearpharaiméadair éagsúla. Agus muid ag súil le taighde dírithe ar bhrath intinne a éascú agus a dhaonlathú, scaoilimid ár gcód, chomh maith le tacar sonraí braite aon-raoin dúshlánach nua a chuimsíonn 13,083 sampla anótáilte thar 77 rún.</abstract_ga>
      <abstract_el>Η οικοδόμηση συστημάτων συνομιλίας σε νέους τομείς και με προστιθέμενη λειτουργικότητα απαιτεί μοντέλα αποδοτικά με πόρους που λειτουργούν κάτω από καθεστώτα χαμηλών δεδομένων (δηλ. σε ρυθμίσεις λίγων πλάνων). Με κίνητρο από αυτές τις απαιτήσεις, εισάγουμε μεθόδους ανίχνευσης προθέσεων που υποστηρίζονται από προκαθορισμένους κωδικοποιητές διπλών προτάσεων όπως ΧΡΗΣΗ και ConveRT. Αποδεικνύουμε τη χρησιμότητα και την ευρεία δυνατότητα εφαρμογής των προτεινόμενων ανιχνευτών προθέσεων, δείχνοντας ότι: 1) υπερτερούν των ανιχνευτών προθέσεων με βάση την τελειοποίηση του πλήρους μοντέλου ή τη χρήση του ως σταθερού κωδικοποιητή μαύρου κουτιού σε τρία διαφορετικά σύνολα δεδομένων ανίχνευσης προθέσεων. 2) τα κέρδη είναι ιδιαίτερα έντονα σε ρυθμίσεις λίγων πυροβολισμών (δηλ., με μόνο 10 ή 30 σχολιασμένα παραδείγματα ανά πρόθεση)· 3) οι ανιχνευτές προθέσεών μας μπορούν να εκπαιδευτούν μέσα σε λίγα λεπτά σε έναν ενιαίο επεξεργαστή. και 4) είναι σταθεροί σε διαφορετικές ρυθμίσεις υπερπαραμέτρων. Ελπίζοντας να διευκολύνουμε και να εκδημοκρατίσουμε την έρευνα που επικεντρώνεται στην ανίχνευση προθέσεων, κυκλοφορούμε τον κώδικα μας, καθώς και ένα νέο προκλητικό σύνολο δεδομένων ανίχνευσης προθέσεων ενός τομέα που περιλαμβάνει 13.083 σχολιασμένα παραδείγματα πάνω από 77 προθέσεις.</abstract_el>
      <abstract_ka>ახალი დიომენში და დამატებული ფუნქციალურობით კონტუნქციალური სისტემების შექმნა უნდა რესურსის ეფექციალური მოდელები, რომლებიც მუშაობენ ცოტა მონაცემების რეზიმი (მა მოტივირული ამ შესაძლებლობით, ჩვენ ჩვენ შევცვალოთ საზოგადომის განვიხოვრების მეტი, რომლებიც USE და ConveRT-ის კოდერებით მხარდაჭირებული ორივე სიტყვებით. ჩვენ გამოჩვენებთ საზოგადომის მონაცემების გამოყენებელობას და უფრო საზოგადოებელობას, რომელიც გამოჩვენებენ, რომ: 1) ისინი გამოყენებენ საზოგადომის detectorები, რომელიც უფრო უფრო მეტი BERT- დიდი მოდელზე დააკეთებული, ან 2) შეიძლება განსაკუთრებულად გამოსახულებულია რამდენიმე სტატის კონფიგურაციაში (მაგალითად, მხოლოდ 10 ან 30 ანოტატირებული მაგალითებით მინდა); 3) ჩვენი საზოგადოებო დიტექტორი შეიძლება ერთი პროცესის შემთხვევაში შეიძლება განაკეთება; და 4) ისინი განსხვავებული ჰიპეროპარამეტრის პარამეტრების შესახებ სტაბილურია. დამეხოვრებით, რომ დახმარება და დემოკრატიზაცია სწორედ გავაკეთებთ საზოგადოება, ჩვენ გავაკეთებთ ჩვენი კოდის, და ახალი საზოგადოებელი საზოგადოება მონაცემების სექტი, რომელიც 13 083 მონაცემები</abstract_ka>
      <abstract_hu>A beszélgetési rendszerek új területeken történő kiépítése és kiegészítő funkcionalitásokkal rendelkező erőforrás-hatékony modelleket igényel, amelyek alacsony adatszintű rendszerek mellett működnek (például néhány felvétel esetén). Ezeknek a követelményeknek megfelelően bevezetjük az előkészített kettős mondatkódolókkal, például az USE és a ConveRT által támogatott szándék-felismerési módszereket. Bemutatjuk a javasolt szándék-érzékelők hasznosságát és széles körű alkalmazhatóságát, bemutatva, hogy: 1) a teljes BERT-Large modell finomhangolása vagy a BERT fix fekete dobozos kódolóként három különböző szándék-érzékelési adatkészleten felülmúlják a szándék-érzékelőket; 2) a nyereség különösen kifejezetten néhány lövéses beállításokban jelentkezik (azaz szándékunként csak 10 vagy 30 jegyzetelt példával); 3) szándék érzékelőink percek alatt képezhetők egyetlen CPU-n; és 4) stabil a különböző hiperparaméter beállítások között. A szándék felismerésére összpontosító kutatások megkönnyítésének és demokratizálásának reményében kiadjuk kódunkat, valamint egy új, kihívást jelentő, egydományos szándék felismerésére vonatkozó adatkészletet, amely 13 083 megjegyzett példát tartalmaz 77 szándéknál.</abstract_hu>
      <abstract_kk>Жаңа доменлерде және қосылған функциялық түрлендіру жүйелерді құру керек деректер режимдерінің астында жұмыс істейтін ресурс эффективті үлгілері (мысалы, бірнеше шарт баптауларында). Бұл қажеттерді қолдану үшін біз USE және ConveRT секілді екі сөз кодерлері қолданылатын нақты анықтау әдістерін келтіреміз. Біз келтірілген мақсатты анықтаушылардың пайдаланушылығын және көпшілігін көрсетедік: 1) олар толық BERT- Үлкен үлгі моделін баптауға негізделген мақсатты анықтаушыларды, немесе BERT- ді үш түрлі мақсатты анықтау керек деректерінің кодтамасы 2) жетістіктерді өзгеше бірнеше сүрлер баптауларында (мысалы, тек 10 немесе 30 жазылған мысалдар бар); 3) біздің мақсатымызды анықтаушыларымыз бір процессордың бірнеше мәселеде оқылмай алады; 4) олар басқа гиперпараметрлер параметрлерінде дұрыс болады. Біз зерттеулерді көмектесу және демократизациялауға көмектесу үшін көмектесіміздің кодымызды, сондай-ақ 77 мақсаттарынан артық 13 083 мәселелерді жаңа бір доменге көмектесетін жаңа мәселелерді анықтау үші</abstract_kk>
      <abstract_it>Costruire sistemi di conversazione in nuovi domini e con funzionalità aggiuntive richiede modelli efficienti sotto il profilo delle risorse che funzionano con regimi a basso contenuto di dati (ad esempio, con configurazioni a pochi scatti). Motivati da questi requisiti, introduciamo metodi di rilevamento degli intenti supportati da encoder dual sentence pre-addestrati come USE e ConveRT. Dimostriamo l'utilità e l'ampia applicabilità dei rivelatori di intenti proposti, dimostrando che: 1) superano i rivelatori di intenti basati sulla messa a punto del modello BERT-Large completo o utilizzando BERT come codificatore fisso a scatola nera su tre diversi set di dati di rilevamento degli intenti; 2) i guadagni sono particolarmente pronunciati nelle configurazioni a pochi colpi (cioè, con solo 10 o 30 esempi annotati per intento); 3) i nostri rilevatori di intenti possono essere addestrati in pochi minuti su una singola CPU; e 4) sono stabili attraverso diverse impostazioni di iperparametri. Nella speranza di facilitare e democratizzare la ricerca focalizzata sul rilevamento delle intenzioni, rilasciamo il nostro codice, così come un nuovo impegnativo set di dati di rilevamento delle intenzioni monodominio comprendente 13.083 esempi annotati su 77 intenti.</abstract_it>
      <abstract_mk>Изградбата на конверзационални системи во нови домени и со додадена функционалност бара модели ефикасни во ресурсите кои работат во режими со ниски податоци (т.е. во неколку поставувања). Мотивирани од овие барања, воведуваме методи за детекција на намери поддржани од претренирани кодери на две реченици како што се УСЕ и ConveRT. Ние ја демонстрираме корисноста и широката апликабилност на предложените детектори на намери, покажувајќи дека: 1) тие ги надминуваат детекторите на намери базирани на финетизирање на целиот BERT-Голем модел или користејќи BERT како фиксен кодер со црна кутија на три различни набори податоци за детектирање намери; 2) добивките се особено изразени во неколку поставувања (т.е., со само 10 или 30 примери наведени по намера); 3) our intent detectors can be trained in a matter of minutes on a single CPU;  and 4) they are stable across different hyperparameter settings.  Во надеж дека ќе го олесниме и демократизираме истражувањето фокусирано на детекција на намерите, го ослободиме нашиот код, како и нов предизвикувачки набор на податоци за детекција на намерите на еден домен, кој вклучува 13.083 примери од 77 намери.</abstract_mk>
      <abstract_lt>Building conversational systems in new domains and with added functionality requires resource-efficient models that work under low-data regimes (i.e., in few-shot setups).  Motyvuojami šiais reikalavimais, įvedame sąmoningumo nustatymo metodus, remiamus iš anksto parengtais dviejų sakinių kodatoriais, pvz., USE ir ConveRT. We demonstrate the usefulness and wide applicability of the proposed intent detectors, showing that: 1) they outperform intent detectors based on fine-tuning the full BERT-Large model or using BERT as a fixed black-box encoder on three diverse intent detection data sets;  2) pelnas yra ypač didelis nedideliu mastu (t. y. tik 10 arba 30 užrašytų pavyzdži ų vienam ketinimui); 3) mūsų ketinimų detektorius galima apmokyti per kelias minutes vienoje CPU; ir 4) jos yra stabilios įvairiose hiperparatorių aplinkybėse. Tikimės palengvinti ir demokratizuoti mokslinius tyrimus, kuriais daugiausia dėmesio skiriama ketinimų nustatymui, išleisime savo kodeksą, taip pat naują sudėtingą vienos srities ketinimų nustatymo duomenų rinkinį, apimantį 13 083 užrašytus pavyzdžius iš 77 ketinimų.</abstract_lt>
      <abstract_ml>പുതിയ ഡോമെനുകളില്‍ സംസാരിക്കുന്ന സിസ്റ്റമുകള്‍ പണിയുകയും ചേര്‍ക്കുന്ന ഫങ്ഷനിയോടൊപ്പം ചേര്‍ക്കുകയും ചെയ്യുന്ന വിഭവങ്ങള്‍ കുറഞ്ഞ ഡ ഈ ആവശ്യങ്ങള്‍ പ്രാവര്‍ത്തികമാക്കിയിരിക്കുന്നു, യുസൈയും കോണ്‍വെര്‍വെര്‍ട്ടിയും പോലുള്ള രണ്ടു വാക്കുകളുടെ കോഡോര്‍ഡുകള്‍ പി പ്രൊദ്ദേശിക്കപ്പെട്ട intent detectors' ഉപയോഗവും വിശാലമായ പ്രയോഗവും നമ്മള്‍ കാണിച്ചുകൊടുക്കുന്നു. അത് കാണിച്ചുകൊണ്ടിരിക്കുന്നു: 1) മുഴുവന്‍ ബെര്‍ട്ടി- വലിയ മോഡലിനെ സൂക 2) പ്രത്യേകിച്ച് വെടിവെക്കപ്പെടുന്ന കുറച്ച് വെടിവെക്കപ്പെട്ട കുറ്റങ്ങളില്‍ നിന്ന് സമ്മാനം പ്രസ്താവിക് 3) നമ്മുടെ ലക്ഷ്യം ഡിക്റ്ററുകള്‍ക്ക് ഒരു സിപിയുവില്‍ ഒരു മിനിട്ടിനുള്ളില്‍ പരിശീലിക്കാന്‍ കഴിയും. 4) വ്യത്യസ്ത ഹൈപ്പര്‍പാരാമീറ്റര്‍ സജ്ജീകരണങ്ങളില്‍ അവര്‍ സ്ഥിരമായിരിക്കുന്നു. നിരീക്ഷിക്കുന്നതിനെയും ജനാധിപത്യത്തെയും ശ്രദ്ധിക്കുന്നതിനെയും പ്രതീക്ഷിക്കുന്നതിനായി, ഞങ്ങള്‍ നമ്മുടെ കോഡിനെയും പുതിയ വ്യാല്‍വെച്ച് ഡേറ്റ</abstract_ml>
      <abstract_mt>Building conversational systems in new domains and with added functionality requires resource-efficient models that work under low-data regimes (i.e., in few-shot setups).  Motivated by these requirements, we introduce intent detection methods backed by pretrained dual sentence encoders such as USE and ConveRT.  Jiġu murija l-utilità u l-applikabbiltà wiesgħa tad-detetturi tal-intenzjoni proposti, li juru li: 1) huma jaqbżu d-detetturi tal-intenzjoni bbażati fuq l-irfinar tal-mudell sħiħ BERT-Kbar jew l-użu tal-BERT bħala kodifikatur fiss tal-kaxxa sewda fuq tliet settijiet ta’ dejta ta’ detezzjoni tal-intenzjoni differenti; 2) il-qligħ huwa speċjalment evidenti f’settijiet bi ftit skopijiet (jiġifieri, b’10 jew 30 e żempju annotati biss għal kull intenzjoni); 3) id-detetturi tal-intenzjoni tagħna jistgħu jitħarrġu fi kwistjoni ta’ minuti fuq CPU waħda; u 4) huma stabbli f’setturi differenti ta’ iperparaturi. Fit-tama li tiġi ffaċilitata u demokratizzata r-riċerka ffukata fuq l-iskoperta tal-intenzjonijiet, nirrilaxxaw il-kodiċi tagħna, kif ukoll sett ġdid ta' dejta ġdid ta' skoperta tal-intenzjonijiet f'dominju uniku li jinkludi 13,083 eżempju annotat fuq 77 intenzjoni.</abstract_mt>
      <abstract_mn>Шинэ хэсэгт харилцааны системийг бүтээх болон нэмэгдсэн функцийн чадвар бага өгөгдлийн режим доор ажилладаг ресурс бүтээмжтэй загварууд хэрэгтэй. Эдгээр шаардлагатай шаардлагатай нь бид зорилгоор олох арга замыг харуулж байна. Яг USE болон ConveRT зэрэг хоёр давхар өгүүлбэрийн коддогчид дэмжигдсэн. Бид санал өгсөн зорилго тогтоогчдын хэрэглээ болон өргөн хэрэглээ гэдгийг харуулж байна. 1) тэд бүрэн BERT-том загварыг тодорхойлох эсвэл BERT-г гурван төрлийн зорилго тогтоох өгөгдлийн санааны шинжлэх ухааны шинжлэх ухааны шинжлэх ухааны шинжлэх ухааны 2) Ялангуяа хэд хэдэн зурагт авсан зардал (яг л зөвхөн 10 эсвэл 30 зөвхөн анзаарагдсан жишээ) гэдэг. 3) Бидний зорилго тогтоогчид нэг процессорын тухай хэдэн минутын дараа сургалт хийж чадна. 4) тэд өөр гиперпараметр дээр тогтмол байдаг. Судалгааны тусламжтайгаар, ардчилсан судалгааны тусламжтайгаар төвлөрсөн гэдэгт итгэл найдаж, бид бидний кодыг, мөн 77 зорилго дээр 13 083 анзаарсан жишээ авч ирсэн шинэ шаардлагатай нэг зорилготой өгөгдлийн санааг олох гэсэн найдвар юм.</abstract_mn>
      <abstract_ro>Construirea sistemelor de conversație în domenii noi și cu funcționalitate adăugată necesită modele eficiente din punct de vedere al resurselor, care funcționează în regimuri cu date scăzute (de exemplu, în setări cu puține fotografii). Motivați de aceste cerințe, introducem metode de detectare a intenției susținute de codificatoare cu două propoziții precum USE și ConveRT. Demonstrăm utilitatea și aplicabilitatea largă a detectoarelor de intenție propuse, arătând că: 1) acestea depășesc detectoarele de intenție pe baza reglării fine a modelului complet BERT-Large sau utilizând BERT ca codificator fix cu cutie neagră pe trei seturi de date diverse de detectare a intențiilor; 2) câștigurile sunt pronunțate în special în setările cu puține lovituri (adică, cu doar 10 sau 30 de exemple adnotate pe intenție); 3) detectoarele noastre de intenție pot fi instruite în câteva minute pe un singur procesor; și 4) sunt stabile în diferite setări de hiperparametru. În speranța de a facilita și democratiza cercetarea axată pe detectarea intențiilor, lansăm codul nostru, precum și un nou set de date provocator de detectare a intențiilor unice, cuprinzând 13.083 exemple adnotate peste 77 de intenții.</abstract_ro>
      <abstract_ms>Bina sistem perbualan dalam domain baru dan dengan fungsi tambahan memerlukan model efisien sumber yang berfungsi di bawah režim data rendah (i.e., dalam beberapa tetapan tembakan). Motivated by these requirements, we introduce intent detection methods backed by pretrained dual sentence encoders such as USE and ConveRT. Kami menunjukkan kebaikan dan kemudahan luas penemui niat yang diusulkan, menunjukkan bahawa: 1) mereka melampaui pengesan niat yang berdasarkan penyesuaian baik model BERT-Besar penuh atau menggunakan BERT sebagai pengekod kotak hitam tetap pada tiga set data pengesan niat berbeza; 2) keuntungan terutama diungkapkan dalam setup beberapa tembakan (iaitu hanya dengan 10 atau 30 contoh yang dicatat setiap niat); 3) pengesan tujuan kita boleh dilatih dalam beberapa minit pada satu CPU; and 4) they are stable across different hyperparameter settings.  Dalam harapan untuk memudahkan dan demokratisasikan kajian yang fokus pada pengesan tujuan, kami melepaskan kod kami, serta set data pengesan tujuan domain tunggal yang mencabar yang mengandungi 13,083 contoh yang dicatat lebih dari 77 tujuan.</abstract_ms>
      <abstract_sr>Izgradavanje razgovornih sistema u novim domenama i sa dodanom funkcionalnošću zahteva efikasne modele resursa koji rade pod nizim podacima (tj. u nekoliko snimanja). Motivirani ovim zahtevima, predstavljamo metode otkrivanja namjera podržavane koderima dvostruke rečenice poput USE i ConveRT. Mi pokazujemo korisnost i široku primjenu predloženih detektora namjera, pokazujući da: 1) oni izvršavaju namjerne detektore na osnovu finalnog prilagodbe kompletnog model a BERT-Velikog modela ili koriste BERT kao fiksni koder crne kutije na tri seta podataka o otkrivanju različitih namjera; 2) dobitke su posebno izjavljene u nekoliko snimanja (tj. sa samo 10 ili 30 primjera po namjeru); i 3) naši detektori namjere mogu biti obučeni u nekoliko minuta na jednom procesoru; i 4) stabilni su preko različitih hiperparametara. Nadajući se da će olakšati i demokratizirati istraživanje usredotočeno na otkrivanje namjera, oslobodimo naš kodeks, kao i novi izazovni set podataka za otkrivanje jedinstvenog domena koji sastoji od 13.083 annotiranih primjera preko 77 namjera.</abstract_sr>
      <abstract_pl>Budowanie systemów konwersacyjnych w nowych domenach i z dodatkową funkcjonalnością wymaga modeli efektywnych zasobami, które działają w reżimech niskiej ilości danych (tj. w konfiguracjach kilku ujęć). Motywowani tymi wymaganiami wprowadzamy metody detekcji intencji wspierane wstępnie przeszkolonymi koderami podwójnych zdań, takimi jak USE i ConveRT. Wykazujemy przydatność i szerokie zastosowanie proponowanych detektorów intencji, pokazując, że: 1) wydają one wyniki detektorów intencji opartych na dostosowaniu pełnego modelu BERT-Large lub wykorzystując BERT jako stały koder czarnej skrzynki na trzech różnych zbiorach danych detekcji intencji; 2) zyski są szczególnie wyraźne w konfiguracjach kilku strzałów (tj. z tylko 10 lub 30 adnotacji przykładów na intencję); 3) nasze detektory intencji mogą być przeszkolone w ciągu kilku minut na jednym procesorze; i 4) są stabilne w różnych ustawieniach hiperparametrów. W nadziei na ułatwienie i demokratyzację badań koncentrujących się na wykrywaniu intencji, publikujemy nasz kod, a także nowy wymagający zestaw danych wykrywania intencji pojedynczej domeny zawierający 13,083 adnotacje przykładów ponad 77 intencji.</abstract_pl>
      <abstract_so>Degmooyinka cusub ku dhisashada nidaamka iskala hadlitaanka iyo waxyaabaha lagu daro waxay u baahan yihiin noocyo faa’iido leh oo ka shaqeeya xeerarka hoose-data (tusaale ahaan kooxaha wax yar oo lagu dhuftay). Motivated by these requirements, we introduce intent detection methods backed by pretrained dual sentence encoders such as USE and ConveRT.  Waxaynu muujinnaa faa'iidada iyo ballaadhan ee qofka la soo jeeday, waxayna muujiyaan in ay sameeyaan qalabka inteniga ah oo ku saleysan hagaajinta modelka buuxda BERT-Large ama ku isticmaalaan BERT sida koox madow-box oo madow ah oo ku qoran saddex koox oo macluumaad ah oo kala duduwan; 2) Waxyaabaha la soo qaatay waxaa si gaar ah looga sheegaa dhibaatooyin yar oo lagu dhuftay (tusaale ahaan 10 ama 30 tusaale ahaan oo kaliya). 3) xisaabiyayaasheena waxqabadka ah waxaa la tababari karaa muddo daqiiqo ah oo kaliya CPU; 4) waxay ku adag yihiin xarumaha heerarka kala duwan. Waxaan rajaynayaa in waxbarashada la fududeeyo oo lagu beddelo, waxaynu u sii daynaa aqoonsigayaga, sidoo kale sawir cusub oo la soo ogaanayo macluumaadka kooxaha halka gudaha ah oo ku qoran 13,083 tusaalooyin ka badan 77 noocyo.</abstract_so>
      <abstract_si>අළුත් ඩෝමේන් වල වාර්තාවක් පද්ධතිය නිර්මාණය කරන්න සහ සම්බන්ධ වැඩක් සඳහා සම්බන්ධ වැඩක් අවශ්‍යය සඳහා පරීක්ෂාත්මක අපි මේ අවශ්‍යයෙන් හොයාගන්න හැකියුම් විදියට පරීක්ෂණය කරලා තියෙන්නේ, USE සහ ConveRT විදියට පරීක්ෂණය කරන දෙවල් වාක්ය අපි ප්‍රදර්ශනය කරනවා ප්‍රවේශනය සහ විශේෂ ප්‍රවේශකයේ ප්‍රවේශකය සහ ප්‍රවේශකය, ප්‍රවේශකයෙන්: 1) ඔවුන් පුරුණු BERT- ලොකු මොඩේල් එක සඳහා ප්‍රවේශකය සඳහා ප්‍රවේ 2) විශේෂයෙන් විශේෂයෙන් විශේෂයෙන් විශේෂයෙන් විශේෂයෙන් විශේෂය කරලා තියෙන්නේ (ඉතින් විශේෂයෙන් වි 3) අපේ අදහස් පරීක්ෂකයන්ට පුළුවන් විනාඩියක් පරීක්ෂණය කරන්න පුළුවන් CPU එක්කෙන් විනාඩියකට; 4) ඔවුන් වෙනස් හායිපර් ප්‍රමාණය සැකසුම් වලට ස්ථිර වෙනවා. අපි අපේ කෝඩ් හොයාගන්න හැටියට පරීක්ෂණය සහ ප්‍රධානය කරන්න හැටියට අවශ්‍යයෙන්, අපි අපේ කෝඩ් හොයාගන්න හැටියට අළුත් ප්‍රශ්නයක් සහ අළුත් එක්</abstract_si>
      <abstract_sv>Att bygga konversationssystem i nya domäner och med extra funktionalitet kräver resurseffektiva modeller som fungerar under lågdataregimer (dvs. i enstaka inställningar). Motiverade av dessa krav introducerar vi avsiktsdetekteringsmetoder som stöds av förkränade dubbla meningskoder som USE och ConveRT. Vi visar nyttan och den breda tillämpningen av de föreslagna avsiktsdetektorer och visar att: 1) de presterar bättre än avsiktsdetektorer baserat på finjustering av hela BERT-Large-modellen eller använder BERT som en fast svartboxkodare på tre olika uppsättningar avsiktsdetekteringsdata. 2) Vinsterna är särskilt uttalade i några skott konfigurationer (dvs. med endast 10 eller 30 kommenterade exempel per avsikt); 3) våra avsiktsdetektorer kan utbildas på några minuter på en enda CPU; och 4) de är stabila över olika hyperparameter inställningar. I hopp om att underlätta och demokratisera forskning inriktad på avsiktsdetektering släpper vi vår kod, samt ett nytt utmanande dataset för avsiktsdetektering med en domän bestående av 13 083 kommenterade exempel över 77 intentioner.</abstract_sv>
      <abstract_ta>@ info இந்த விருப்பங்களால் இயக்கப்பட்டது, USE மற்றும் ConveRT போன்ற இருமுறை வாக்கியின் குறியீடு நாம் பரிந்துரைக்கப்பட்டுள்ள விருப்பத்தின் பயன்பாடு மற்றும் அகலமான பயன்பாடு 2) சில துடைக்கப்பட்ட அமைப்புகளில் வெளிப்படுத்தப்பட்டுள்ளது (அதாவது, நிலைக்கு மட்டும் 10 அல்லது 30 துக்கப்பட்ட உதாரணங்கள 3) our intent detectors can be trained in a matter of minutes on a single CPU;  4) மேலும் அவர்கள் வேறு மின்னெழுத்து அளபுரு அமைப்புகளில் உறுதியாக இருக்கின்றன. நாம் கண்டுபிடிப்பதற்கு கவனமாக ஆராய்ச்சி மற்றும் வடிவமைக்கும் ஆராய்ச்சி எதிர்பார்ப்பிற்கு, நாம் எங்கள் குறியீட்டை வெளியிடுவோம், மற்றும் ஒரு ப</abstract_ta>
      <abstract_no>Bygging av konvertasjonssystemet i nye domene og med tillegg av funksjonalitet krev ressurseffektiv modeller som arbeidar under låg dataregime (t.d. i få fotografiske oppsett). Av desse kreveta, introduserer vi metoder for oppdaging som er støtta av trekkkodarar med trekking av dobbelt setningar, slik som USE og ConveRT. Vi viser at det er nyttig og breidde tilgjengeligheten til dei foreslåtte inntekningsmaterne, viser at: 1) dei utfører inntekningsmaterne basert på finnstilling av den fulle BERT- store modellen eller brukar BERT som ein fast svartbokskoder på tre ulike målsettingar for oppdaging av data. 2) gjennomsnittet er spesielt uttalet i få fotooppsett (t.d. berre med 10 eller 30 uttale eksemplar per intensjon); 3) målsettingsdetektatorane våre kan trenjast i ein del minutt på ein enkelt prosessor; og 4) dei er stabile over ulike hyperparameter-innstillingar. I håp for å gjera forskning som er tilgjengeleg og demokratisert fokusert på oppdaging av vilkåra, så løyser vi koden vårt, og eit nytt utfordrende datasett for oppdaging av enkeldomene som inneheld 13 083 markerte eksemplar over 77 vilkåra.</abstract_no>
      <abstract_ur>نو ڈومین میں مکانٹ سیسٹم بنانے اور زیادہ فعالیت کے ساتھ مکانٹ سیسٹم کی ضرورت ہے کہ کم ڈاٹ ریجیمز کے نیچے کام کریں یہ ضرورت کے ذریعے چلنے والے ہیں، ہم نے مطلب پیدا کرنے کے طریقے پیش کیے ہیں جن کی پشتیبانی دئیویل جماعت کوڈر جیسے USE اور ConveRT کے ذریعے ہیں. ہم نشان دیتے ہیں کہ پیشنهاد ارادہ ڈیٹونٹروں کے فائدہ اور وسیع کاربری کا استعمال کرتا ہے، یہ کہ: 1) وہ تمام BERT-بزرگ موڈل کو پاکیزہ تنظیم کرنے پر بنیاد رکھتے ہیں یا BERT کو تین مختلف انتظام ڈیٹ سٹوں پر ثابت سیاہ باکس کا انکوڈر بناتے ہیں۔ 2) غنیمتیں مخصوص تھوڑے شٹ کے سامان میں پڑھی جاتی ہیں (یعنی صرف ۱۰ یا ۳۰ مثالیں لکھی جاتی ہیں) 3) ہمارے ارادہ ڈاکٹر ایک سی پی یو کے بارے میں ایک منٹ کے بارے میں آموزش کی جاتی ہیں۔ اور 4) وہ مختلف ہیپر پارامیٹوں کے اندر ثابت ہیں۔ اس کی امید ہے کہ تحقیقات کی آسانی اور دموکراتیزی کے ذریعہ مطالبہ آزمائش پر تمرکز کیا گیا ہے، ہم نے اپنے کوڈ کو چھوڑ دیا ہے، اور ایک ڈومین کا ایک مشکل ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈ</abstract_ur>
      <abstract_uz>Name Bu takliflar bilan ishga tushirilgan, biz USE va ConveRT kabi ikki maxfiy soʻzning kodlari bilan qoʻllanilgan intent aniqlash usullarini ishlatimiz. Biz talab qilingan qatlam detektorlarining foydalanishini va kengaytirish qobiliyatini koʻrsatimiz. Ko'rsatish mumkin: 1) ular butun BERT- katta modelini ajratish asosida ko'rsatadi yoki uchta xil maʼlumot tizimini qidirish uchun BERT'ni boshqarish imkoniyatini ko'rsatadi; 2) Mavzular faqat bir necha saqlangan tizimlarda (balki faqat qanday 10 yoki 30 misollar bilan keladi). 3) Bizning qanday detektorlarimiz bir necha daqiqa daqiqa o'rganishi mumkin. va 4) ular boshqa hyperparametr moslamalariga stabil. Tafitini aniqlash uchun foydalanish va demokrasiy qilish uchun, biz qoidamizni chiqaramiz, va biz 77 ta'minga bir qanday qanday qiziqarish uchun yangi qiziqarli maʼlumotlar tarkibini aniqlashimiz mumkin.</abstract_uz>
      <abstract_vi>Xây dựng hệ thống đối thoại trong những miền mới và có tính năng thêm đòi hỏi các mô hình hiệu quả nguồn tài nguyên có tác dụng trong chế độ ít dữ liệu (tức là cài đặt ít). Động cơ bởi những yêu cầu này, chúng tôi giới thiệu phương pháp phát hiện ý đồ được hỗ trợ bởi mã hóa hai bản án chuẩn như chuẩn bị sử dụng và thuyết phục. Chúng tôi cho thấy sự hữu dụng và rộng rãi của các máy dò ý định đề xuất, cho thấy: 1) chúng vượt trội các bộ máy dò có mục đích dựa trên độ chỉnh sửa toàn bộ mô hình BERT-Large hoặc sử dụng BERT như một bộ mã hóa hộp đen cố định trên ba bộ dữ liệu trinh thám khác nhau; 2) lợi nhuận được phát triển đặc biệt trong cài đặt ít phát triển (v.d. chỉ có mười hay ba mươi đi ển ghi chú cho mục đích) Ba) Máy phát hiện mục đích có thể được huấn luyện chỉ trong vài phút. và 4) chúng ổn định trên các thiết lập siêu tham số khác nhau. Với hy vọng giúp đỡ và dân chủ nghiên cứu tập trung vào dự án khám phá mục đích, chúng tôi giải phóng mã của chúng tôi, cũng như một tập tin dữ liệu trinh sát chủ nhiệm mới đầy thử thách gồm bộ 1,03 và ghi chú trên bộ định 77.</abstract_vi>
      <abstract_bg>Изграждането на разговорни системи в нови области и с добавена функционалност изисква ефективно използване на ресурсите модели, които работят при режими с ниско съдържание на данни (т.е. при няколко настройки). Мотивирани от тези изисквания, ние въвеждаме методи за откриване на намерения, подкрепени от предварително тренирани кодери с двойни изречения, като например УЕЗ и КонвеRT. Ние демонстрираме полезността и широката приложимост на предложените детектори за намерение, показвайки, че: 1) те превъзхождат детекторите за намерение въз основа на фина настройка на пълния модел или използване на фиксиран кодер за черна кутия върху три различни набора данни за откриване на намерения; 2) печалбите са особено изразени при няколко снимки (т.е. само с 10 или 30 анотирани примера за намерение); 3) нашите детектори за намерение могат да бъдат обучени за минути на един процесор; и 4) те са стабилни в различни настройки на хиперпараметъра. С надеждата да улесним и демократизираме изследванията, фокусирани върху откриването на намерения, ние публикуваме нашия код, както и нов предизвикателен набор от данни за откриване на намерения с един домейн, включващ 13 083 анотирани примера над 77 намерения.</abstract_bg>
      <abstract_nl>Voor het bouwen van conversatiesystemen in nieuwe domeinen en met toegevoegde functionaliteit zijn resource-efficiënte modellen nodig die werken onder low-data regimes (d.w.z. in enkele-shot setups). Gemotiveerd door deze vereisten introduceren we intentiedetectiemethoden ondersteund door vooraf getrainde dubbele zinnencoders zoals USE en ConveRT. We demonstreren het nut en de brede toepasbaarheid van de voorgestelde intentiedetectoren, waaruit blijkt dat: 1) ze beter presteren dan intentiedetectoren op basis van finetuning van het volledige BERT-Large model of het gebruik van BERT als vaste black-box encoder op drie verschillende intentiedetectie datasets; 2) de winsten zijn vooral uitgesproken in weinige-shot opstellingen (d.w.z. met slechts 10 of 30 geannoteerde voorbeelden per intentie); 3) onze intentiedetectoren kunnen worden getraind in een kwestie van minuten op een enkele CPU; en 4) ze zijn stabiel over verschillende hyperparameter instellingen. In de hoop om onderzoek gericht op intentiedetectie te faciliteren en te democratiseren, brengen we onze code uit, evenals een nieuwe uitdagende dataset voor intentiedetectie met 13.083 geannoteerde voorbeelden boven 77 intenties.</abstract_nl>
      <abstract_da>Opbygning af samtalesystemer i nye domæner og med ekstra funktionalitet kræver ressourceeffektive modeller, der arbejder under low-data regimer (dvs. i få skud opsætninger). Motiveret af disse krav, introducerer vi hensigtsdetekteringsmetoder understøttet af forudtrænede dobbeltsætningskodere som USE og ConveRT. Vi demonstrerer nytten og den brede anvendelighed af de foreslåede intentionsdetektorer og viser, at: 1) de overgår intentionsdetektorer baseret på finjustering af hele BERT-Large-modellen eller bruger BERT som en fast sort bokskoder på tre forskellige intentionsdetektionsdatasæt; 2) gevinsterne er især udtalt i nogle-skud opsætninger (dvs. med kun 10 eller 30 annoterede eksempler pr. hensigt); 3) vores intent detektorer kan trænes i løbet af få minutter på en enkelt CPU; og 4) de er stabile på tværs af forskellige hyperparameter indstillinger. I håb om at lette og demokratisere forskning, der fokuserer på intention detektion, frigiver vi vores kode, samt et nyt udfordrende single-domæne intention detection datasæt bestående af 13.083 kommenterede eksempler over 77 intentions.</abstract_da>
      <abstract_hr>Izgradavanje razgovornih sustava u novim domenama i s dodanom funkcionalnošću zahtijeva efikasne modele resursa koji rade pod nizim podacima (tj. u nekoliko snimanja). Pod motivacijom ovih zahtjeva, predstavljamo metode otkrivanja namjera podržane koderima dvostruke rečenice poput USE i ConveRT. Mi pokazujemo korisnost i široku primjenu predloženih detektora namjera, pokazujući da: 1) oni izvršavaju namjerne detektore na temelju finalnog prilagođenja punog model a BERT-Velikog ili koristeći BERT kao fiksni koder crne kutije na tri različite namjere otkrivanja podataka; 2) dobitke su posebno izjavljene u nekoliko snimanja (tj. s samo 10 ili 30 primjera za namjeru primjera). 3) naši detektori namjere mogu biti obučeni u nekoliko minuta na jednom procesoru; i 4) stabilni su preko različitih hiperparametara. Nadajući se da će olakšati i demokratizirati istraživanje usredotočeno na otkrivanje namjera, osloboditi ćemo naš kodeks, kao i novi izazovni set podataka za otkrivanje jedinstvenog domena koji sadrže 13.083 primjera iznosi preko 77 namjera.</abstract_hr>
      <abstract_de>Der Aufbau von Gesprächssystemen in neuen Domänen und mit zusätzlicher Funktionalität erfordert ressourceneffiziente Modelle, die unter Low-Data-Regimes (d.h. in wenigen Aufnahmen) funktionieren. Motiviert durch diese Anforderungen führen wir Intent Detection Methoden ein, die durch vortrainierte Doppelsatz Encoder wie USE und ConveRT unterstützt werden. Wir demonstrieren die Nützlichkeit und breite Anwendbarkeit der vorgeschlagenen Absichtsdetektoren und zeigen, dass: 1) sie übertreffen Absichtsdetektoren, die auf der Feinabstimmung des vollständigen BERT-Large-Modells basieren oder BERT als fester Black-Box-Encoder auf drei verschiedenen Intent-Detection-Datensätzen verwenden; 2) die Gewinne sind besonders ausgeprägt in wenigen Aufnahmen (d.h. mit nur 10 oder 30 kommentierten Beispielen pro Intent); 3) Unsere Intent Detektoren können in wenigen Minuten auf einer einzigen CPU trainiert werden; und 4) sie sind über verschiedene Hyperparameter-Einstellungen stabil. In der Hoffnung, Forschung mit Schwerpunkt auf Intention Detection zu erleichtern und zu demokratisieren, veröffentlichen wir unseren Code sowie einen neuen herausfordernden Single-Domain Intent Detection Datensatz, der 13.083 kommentierte Beispiele über 77 Intents enthält.</abstract_de>
      <abstract_id>Membangun sistem konversasi dalam domain baru dan dengan fungsi tambahan membutuhkan model efisien sumber daya yang bekerja di bawah resim data rendah (i.e., dalam beberapa setup tembakan). Dimotifkan oleh keperluan ini, kami memperkenalkan metode deteksi sengaja yang didukung oleh koder kalimat dua yang dilatih sebelum dilatih seperti USE dan ConveRT. Kami menunjukkan kebaikan dan aplikabilitas luas detektor tujuan yang diusulkan, menunjukkan bahwa: 1) mereka melampaui detektor tujuan berdasarkan penyesuaian model BERT-Besar lengkap atau menggunakan BERT sebagai koder kotak hitam tetap pada tiga set data deteksi tujuan berbeda; 2) the gains are especially pronounced in few-shot setups (i.e., with only 10 or 30 annotated examples per intent);  3) detektor tujuan kita dapat dilatih dalam beberapa menit pada satu CPU; dan 4) mereka stabil melalui pengaturan hyperparameter yang berbeda. Dalam harapan untuk memfasilitasi dan demokratisasikan penelitian fokus pada deteksi niat, kami melepaskan kode kami, serta sebuah set data penelitian niat satu-domain menantang baru yang mengandung 13.083 contoh yang dicatat lebih dari 77 niat.</abstract_id>
      <abstract_fa>ساختن سیستم‌های مکالمانی در دامنهای جدید و با عملکرد اضافه کردن نیاز به مدل‌های قابل فعالیت منابع است که زیر رژیم‌های داده‌های پایین کار می‌کنند (یعنی در تنظیم‌های چند تصویر). توسط این نیازهای حرکت شده، ما روش شناسایی هدف را معرفی می کنیم که توسط رمز‌کننده‌های دوباره‌ی مجازات مانند USE و ConveRT پشتیبانی شده است. ما کاربردی و کاربردی وسیع از بازرسان هدف پیشنهاد را نشان می دهیم که: ۱) آنها بازرسان هدف را بر پایه سازی مدل BERT-بزرگ کامل یا استفاده از BERT به عنوان یک کودهر سیاه بوکس ثابت بر سه مجموعه داده‌های شناسایی هدف مختلف انجام می‌دهند. 2) پیروزی مخصوصا در تنظیمات کمی (یعنی با فقط ۱۰ یا ۳۰ مثال مطلوب به عنوان هدف) تعریف می‌شود. 3) کارآگاه‌کنندگان هدف ما می‌توانند در چند دقیقه در یک CPU آموزش داده شوند; و ۴) آنها در تنظیمات های هیپر پارامتر متفاوت ثابت هستند. امیدوارم تحقیقات را آسان و دموکراتیک کنترل کنیم که روی کشف قصد تمرکز شده است، ما کد خود را آزاد کنیم، و یک مجموعه اطلاعات شناسایی هدف یک دامنی جدید که شامل 13.083 مثالهایی که بیش از 77 هدف نشان داده شده است.</abstract_fa>
      <abstract_sw>Kujenga mifumo ya mazungumzo katika maeneo mapya na kuongeza kazi inahitaji mifano yenye ufanisi wa rasilimali inayofanya kazi chini ya utawala wa data (yaani katika seti chache zilizopigwa risasi). Motivated by these requirements, we introduce intent detection methods backed by pretrained dual sentence encoders such as USE and ConveRT.  Tunaonyesha matumizi na matumizi mengi ya watambuzi wa nia ya pendekezo, wakionyesha kwamba: 1) wanafanya wachunguzi wenye lengo linalotumia vizuri kwa kutumia muundo mkubwa wa BERT au kutumia BERT kama kodi maalumu ya boksi nyeusi kwenye seti tatu za taarifa za kutambua; 2) Matokeo hayo yanatangazwa hasa katika matatizo machache yanayopigwa risasi (yaani, kwa mfano 10 au 30 tu kwa lengo hili); 3) wachambuzi wetu wanaweza kufundishwa katika dakika chache kwenye CPU moja; na 4) ni imara katika mazingira mbalimbali ya kipeperupa. Kwa matumaini ya kusaidia na kutetea utafiti ulijikita kwenye kutambua lengo, tunatoa sheria yetu, pamoja na seti mpya ya kutambua taarifa za ndani yenye lengo la pekee linalojumuisha mifano 13,083 yenye lengo la 77.</abstract_sw>
      <abstract_ko>새로운 분야에서 세션 시스템을 구축하고 기능을 늘리려면 낮은 데이터 모델(즉 소량의 스냅샷 설정에서)에서 작업하는 자원 고효율 모델이 필요하다.이러한 수요를 바탕으로 우리는 미리 훈련된 이중 언어 인코더(예를 들어 USE와 ConveRT)가 지원하는 의도 검출 방법을 도입했다.우리는 제시된 의도 검출기의 실용성과 광범위한 적용성을 증명했다. 1) 이들은 마이크로스피커를 기반으로 한 버트 모델의 의도 검출기보다 낫거나 세 개의 다른 의도 검출 데이터 집합에서 버트를 고정 블랙박스 인코더로 사용하는 의도 검출기보다 낫다.2) 소수의 방포 설정(즉 각 의도는 주석 있는 예시 10개 또는 30개만 있음)에서 수익이 특히 현저하다.3) 우리의 목표 탐지기는 몇 분 내에 단일 CPU에서 훈련을 진행할 수 있다.4) 서로 다른 하이퍼매개변수 설정에서 안정적입니다.의도적 검측에 대한 연구를 추진하고 민주화하기 위해 우리는 우리의 코드와 도전적인 단일 의도적 검측 데이터 집합을 발표했다. 77개 이상의 의도를 가진 13083개의 주석 예시를 포함한다.</abstract_ko>
      <abstract_tr>Täze sahypalarda soňlaşma sistemalary gurlýan we eklendik funksiýaly resurslar etkinleşen nusgalarynyň astynda i şlenýän (meselâ, kiçi-resim düzümlerinde). Bu şartlar tarafından önlenmiş, USE ve ConveRT gibi arkalanmış iki sözle kodlayıcılar tarafından niyetli keşfetme yöntemlerini tanıtıyoruz. Biz teklip eden niýet detektoriň ullanlygyny we ullanlygyny görkezip otyrýarys. Şuny görkezýäris: 1) olar bütin BERT-Ullakan nusgasyny bejermek üçin dürli maksady detektoriň üstünde çykarýarlar ýada BERT-iň üstini bejermek üçin dürli maksady detektoriň kodegi ýaly ullanýarlar; 2) gazançlar özellikle birnäçe atly düzümlerinde takylýar (diňe 10 ýa 30 ýa-da niýe bilen ýazylýan mysal bardyr); 2 3) biziň maksadymyz bir CPU-da minutlarda bilim alyp biler; we 4) farklı hiperparameter ayarlarynda stabil. Araştyrymyzy bejermek we demokratik etmek üçin amaçlarymyzy tanamak üçin ünsüni çykarýarys, we 77 niýetinde 13,083 hasaplanýan ýaly täze bir domeny bejermek maksady çykarýarys.</abstract_tr>
      <abstract_sq>Ndërtimi i sistemeve bisedimore në fusha të reja dhe me funksionalitet të shtuar kërkon modele efikase në burime që punojnë nën regjime me të dhëna të ulëta (pra, në disa konfigurime). Motivuar nga këto kërkesa, ne futim metoda të zbulimit të qëllimeve të mbështetura nga koduesit e parastërvitur të dy dënimeve të tilla si USE dhe ConveRT. Ne demonstrojmë përdorueshmërinë dhe aplikabilitetin e gjerë të detektorëve të propozuar të qëllimeve, duke treguar se: 1) ata kryejnë detektorët e qëllimeve bazuar në rregullimin e modelit të plotë BERT-Large ose duke përdorur BERT si një kodues fiks të kutisë së zezë në tre grupe të dhënash të zbulimit të qëllimeve të ndryshme; 2) fitimet janë veçanërisht të shprehura në disa konfigurime (pra, me vetëm 10 apo 30 shembuj të shënuar për qëllim); 3) detektorët tonë të qëllimeve mund të trajnohen për një çështje minutash në një CPU të vetme; dhe 4) janë të qëndrueshme nëpërmjet rregullimeve të ndryshme të hiperparametrave. Në shpresë për lehtësimin dhe demokratizimin e kërkimit të përqëndruar në zbulimin e qëllimeve, ne lëshojmë kodin tonë si dhe një grup të ri sfidues për zbulimin e qëllimeve me një domeni të vetëm që përfshin 13,083 shembuj të anotuar mbi 77 qëllime.</abstract_sq>
      <abstract_am>በአዲስ ውይይት ውስጥ የሚካሄድ ስርዓቶች በመሠረት እና በተጨማሪው ሥርዓቶች ውስጥ የሚሠራ የክፍተኛ-ፍቃድ ሞዴላዎችን ያስፈልጋል (አዎን በጥቂት-shot setup) እንደዚህ ፈቃድ የተመሳሳይ፣ የአሜሪካ እና ConveRT እንደተደረገው የሁለት የፍርድ ክፍተቶችን በተመሳሳይ የመግለጫ ሥርዓቶችን እናሳውቃለን፡፡ 1) በሙሉ BERT-ትልቁ ሞዴል በመጠቀም ወይም በሦስት ልዩ ልዩ ልዩ ልዩ አካባቢ የጥቁር-box ኮድ በመጠቀም የጥቁር አካባቢ የሆኑን አካባቢ እናሳየዋለን፡፡ 2) ሀብት በተለየ በጥቂት በተወረዱት ድርጊቶች (ምናልባት 10 ወይም 30 ምሳሌዎች ብቻ ነው፡፡ 3) የስልጣን አዳራጮቻችን በጥቂት ደቂቃ ውስጥ አንድ CPU ማስተማር ይችላል፤ 4) በተለየ የhyperparameter አካባቢዎች ላይ ጥላቻዎች ናቸው ። ምርመራን ለማግኘት እና ዲሞክራሲ ለማግኘት ተስፋ እናደርጋለን፡፡ ኮዱን እና በ77 ዓይነት ላይ የሚቆጠሩ አዲስ የፍላጎት ዳታዎችን እናስፈታለን፡፡</abstract_am>
      <abstract_af>Opbou konversasiesstelsels in nuwe domeine en met byvoeg funksionaliteit benodig hulpbron-effektief modele wat werk onder lae-data rejimes (bv. in paar-skoot opstelling). Gebeweging deur hierdie benodighede, introduseer ons doel opdekking metodes wat agtergrond word deur voorreine tweede setkoders soos USE en ConveRT. Ons wys die gebruikerheid en wyde toepassing van die voorgestelde doel-detekteerders, wat vertoon dat: 1) hulle uitvoer doel-detekteerders gebaseer op fyn-tuning van die volle BERT-Groot model of gebruik BERT as 'n vaste swart-boks enkoder op drie verskeie doel-detekteerde data stelle; 2) die oorwinning is spesiaal uitgevoer in paar skoot opstelling (bv. met slegs 10 of 30 aangetekende voorbeelde per doel); 3) ons doel-detektors kan in ân saak van minute op ân enkele CPU onderwerp word; en 4) hulle is stabil oor verskillende hiperparameter instellings. In hoop om forskings te eenvoudig en demokrasiering te fokus op intensie opmekaar, laat ons ons kode verlos, en 'n nuwe aandagende enkel-domein-doel opmekaar datastel wat 13,083 opgemaak voorbeelde oor 77 doel.</abstract_af>
      <abstract_az>Yeni domeylərdə müzakirə sistemləri in şa etmək və əlavə edilmiş funksiyalıqla çox düşük məlumat rejimlərinin altında çalışan ressurs-effektiv modelleri lazım edir. Bu şartların tərəfindən hərəkət edildiyi, USE və ConveRT kimi iki cümləlik kodlayıcıların dəstəklənməsi üçün niyyətli keşif metodlarını təşkil edirik. Biz təbliğ edilmiş niyyət detektörlərinin faydalanılığını və geniş uyğunluğunu göstərdik, ki: 1) onlar bütün BERT-Büyük modelini düzəltməyə dayanan niyyət detektörlərini və ya BERT'i üç müxtəlif niyyət keçmə məqsədilə sabit siyah qutusu kodlayıcısı kimi istifadə edirlər. 2) Qazanlar xüsusilə az vuruş ayarları i çində müəyyən edilir (ya da yalnız 10 ya da 30 məsəllərlə müəyyən edilir); 3) Bizim niyyətimiz detektörlərimiz təkcə bir CPU-də bir dəqiqə içində təhsil edilə bilər; və 4) onlar müxtəlif hiperparameter ayarlarında sabitlidir. İstədiyimiz keşfini təşkil etmək və demokratik təşkil etmək üçün kodumuzu yayındırırıq, həmçinin 77 niyyətində 13.083 məsəllər çəkilən tək domena niyyətində olan yeni çətin təşkil edilən təşkil verilər qurulması üçün.</abstract_az>
      <abstract_hy>Նոր բնագավառներում և ավելացված ֆունկցիոնալ համակարգերի կառուցվածքը պահանջում է ռեսուրսներով արդյունավետ մոդելներ, որոնք աշխատում են ցածր տվյալների համակարգերի (այսինքն՝ մի քանի նկարների կառուցվածքների ժամանակ): Motivated by these requirements, we introduce intent detection methods backed by pretrained dual sentence encoders such as USE and ConveRT.  Մենք ցույց ենք տալիս առաջարկած մտադրության դետեկտորների օգտակարությունը և լայն կիրառելիությունը, ցույց տալով, որ 1) նրանք արտադրում են մտադրության դետեկտորներ, որոնք հիմնված են BER-Large ամբողջ մոդելի բարձրացման վրա կամ BER-ի օգտագործման որպես ֆիքսավոր սև արկղի կոդեր երեք տարբեր մտադր 2) շահույթը հատկապես արտահայտվում է մի քանի նկարների կառուցվածքներում (այսինքն, միայն 10 կամ 30 նկարագրված օրինակ ունենալով յուրաքանչյուր նպատակի համար): 3) մեր մտադրությունների դետեկտորները կարող են մի քանի րոպեում սովորեցնել մեկ պրոցեսորի վրա: և 4) դրանք կայուն են տարբեր հիպերպարամետրերի միջև: Հույս ունենալով նպատակների հայտնաբերման վրա կենտրոնացված հետազոտությունների խրախուսելու և ժողովրդավարացման համար, մենք հրապարակում ենք մեր կոդը, ինչպես նաև նոր մարտահրավերներ մեկ բնագավառի նպատակների հայտնաբերման տվյալների համակարգ, որը ներառում է 13,083 նշո</abstract_hy>
      <abstract_bn>নতুন ডোমেইনে আলোচনা সিস্টেম নির্মাণ করা এবং এর সাথে যোগ করা কার্যকলাপের সাথে সম্পদ-কার্যকর মডেলের প্রয়োজন যা কম ডাটা শাসকদের নিচে কাজ করে (যেমন কয়ে এই প্রয়োজনের দ্বিতীয় বাণী এনকোডার, যেমন ইউএস আর কনভের্ট ট। আমরা প্রস্তাবিত গন্তব্য ডিটেক্টরের প্রয়োজন এবং ব্যাপারটি প্রদর্শন করি যেখানে দেখা যাচ্ছে: ১) তারা পূর্ণ বিবের্ট-ব্যাপক মডেলের ভিত্তিতে গুরুত্বপূর্ণ গুরুত্বপূর 2) the gains are especially pronounced in few-shot setups (i.e., with only 10 or 30 annotated examples per intent);  ৩) আমাদের উদ্দেশ্য ডিটেক্টর একটি সিপিইউ-এ এক মিনিটের মধ্যে প্রশিক্ষণ প্রদান করা যাবে; এবং ৪) তারা বিভিন্ন হাইপার্পারামিটার বৈশিষ্ট্যের মধ্যে স্থির। গবেষণা সহায়তা এবং গণতান্ত্রিক করার আশায়, আমরা আমাদের কোড মুক্তি দেই, এবং একই সাথে একটি নতুন চ্যালেঞ্জালেঞ্জ ডেটার সনাক্তির উদাহরণের মাধ্যমে ১৩,০৮৩ জন</abstract_bn>
      <abstract_bs>Izgradnja razgovornih sustava u novim domenama i sa dodanom funkcionalnošću zahtijeva efikasne modele resursa koji rade pod nizim podacima (tj. u nekoliko snimanja). Pokrenuti ovim zahtjevima, predstavljamo metode otkrivanja namjera podržane pretkišenim koderima dvostruke rečenice poput USE i ConveRT. Mi pokazujemo korisnost i široku primjenu predloženih detektora namjere, pokazujući da: 1) oni izvršavaju namjerne detektore na temelju finalnog prilagođenja punog model a BERT-Velikog ili koristeći BERT kao fiksni koder crne kutije na tri različita namjera otkrivanja podataka; 2) dobitke su posebno izjavljene u nekoliko snimanja (tj. sa samo 10 ili 30 primjera na namjeru). 3) naši detektori namjere mogu biti obučeni u nekoliko minuta na jednom procesoru; i 4) stabilni su preko različitih hiperparametara. Nadajući se da će olakšati i demokratizirati istraživanje usredotočeno na otkrivanje namjera, oslobodimo naš kodeks, kao i novi izazovni set podataka za otkrivanje jedinstvenog domena koji sadrže 13.083 primjera iznosih 77 namjera.</abstract_bs>
      <abstract_cs>Budování konverzačních systémů v nových doménách a s přidanou funkcí vyžaduje efektivní modely, které fungují v režimech s nízkým množstvím dat (tj. v nastaveních s několika záběry). Motivováni těmito požadavky představujeme metody detekce záměru podporované předem trénovanými dvojitými větovými kodéry, jako jsou USE a ConveRT. Prokážeme užitečnost a širokou aplikaci navržených detektorů záměru a ukazujeme, že: 1) překonávají detektory záměru založené na jemném ladění plného modelu BERT-Large nebo používají BERT jako pevný snímač černé skříňky na třech různých datových sadách detekce záměru; 2) zisky jsou zvláště výrazné v několika sestavách (tj. s pouze deseti nebo 30 anotovanými příklady na záměr); 3) naše detektory záměru mohou být trénovány během několika minut na jednom procesoru; a 4) jsou stabilní napříč různými nastaveními hyperparametrů. V naději, že usnadníme a demokratizujeme výzkum zaměřený na detekci záměrů, vydáváme náš kód, stejně jako nový náročný datový soubor detekce záměrů pro jednu doménu obsahující 13,083 anotované příklady nad 77 záměry.</abstract_cs>
      <abstract_ca>La construcció de sistemes de conversació en nous dominys i amb funcionalitat adicionada requereix models eficients en recursos que funcionen sota règims de baixos nivells de dades (és a dir, en configuracions poc fetes). Motivats per aquests requisits, introduïm mètodes de detecció d'intencions sostenits per codificadors de frases dobles pré-entrenats com USE i ConveRT. Demonstrem l'utilitat i l'ampla aplicabilitat dels detectors de intenció proposats, mostrant que: 1) superen els detectors de intenció basats en ajustar el model BERT-Large complet o utilitzen BERT com un codificador de caixa negra fixa en tres conjunts de dades de detecció de intencions diversos; 2) els guanys són especialment pronunciats en poques configuracions (és a dir, amb només 10 o 30 exemples anotats per intenció); 3) our intent detectors can be trained in a matter of minutes on a single CPU;  i 4) són estables a través de diferents configuracions hiperparamètriques. In hope of facilitating and democratizing research focused on intention detection, we release our code, as well as a new challenging single-domain intent detection dataset comprising 13,083 annotated examples over 77 intents.</abstract_ca>
      <abstract_et>Vestlussüsteemide loomine uutes valdkondades ja lisafunktsionaalsusega nõuab ressursitõhusaid mudeleid, mis töötavad madala andmepuudusega režiimides (st vähese võttega seadistustes). Nende nõuete alusel tutvustame kavatsuste tuvastamise meetodeid, mida toetavad eeltreenitud kahekordsed lausekodeerijad, nagu USE ja ConveRT. Näitame väljapakutud kavatsuste detektorite kasulikkust ja laialdast rakendatavust, näidates, et: 1) nad ületavad kavatsuste detektorid, mis põhinevad täielikul BERT-Large mudelil või kasutavad BERT-i fikseeritud musta kasti kodeerijana kolmel erineval kavatsuste tuvastamise andmekogumil; 2) kasu on eriti väljendatud vähese võtte seadistustes (st ainult 10 või 30 märgitud näidet kavatsuse kohta); 3) meie kavatsuste detektoreid saab treenida mõne minutiga ühel protsessoril; ja 4) nad on stabiilsed erinevate hüperparameetrite seadete puhul. Lootuses hõlbustada ja demokratiseerida kavatsuste tuvastamisele keskendunud uuringuid, avaldame oma koodi ja uue keerulise ühe domeeni kavatsuste tuvastamise andmekogumi, mis sisaldab 13 083 märgitud näidet 77 kavatsusest.</abstract_et>
      <abstract_fi>Keskustelujärjestelmien rakentaminen uusille toimialoille ja lisätoiminnoilla edellyttää resurssitehokkaita malleja, jotka toimivat mataladataisissa järjestelmissä (eli muutamassa vaiheessa). Näiden vaatimusten pohjalta esittelemme intent detection -menetelmiä, joita tukevat ennalta koulutetut kaksoislausekooderit, kuten USE ja ConveRT. Osoitamme ehdotettujen intent-ilmaisimien hyödyllisyyden ja laajan sovellettavuuden osoittaen, että: 1) ne toimivat paremmin kuin intent-ilmaisimet, jotka perustuvat koko BERT-Large-mallin hienosäätöön tai BERT:n käyttämiseen kiinteänä mustana laatikkokoodarina kolmella erilaisella intent-tunnistustiedostolla; 2) voitot ovat erityisen voimakkaita muutaman laukauksen kokoonpanoissa (eli vain 10 tai 30 merkittyä esimerkkiä tarkoitusta kohti); 3) aikeenilmaisimet voidaan kouluttaa muutamassa minuutissa yhdellä suorittimella; ja 4) ne ovat stabiileja eri hyperparametrien asetuksissa. Pyrimme helpottamaan ja demokratisoimaan aikomusten havaitsemiseen keskittyvää tutkimusta julkaisemme koodimme sekä uuden haastavan yhden toimialueen aikomusten havaitsemiseen liittyvän aineiston, joka sisältää 13 083 huomaututettua esimerkkiä 77 aikomuksesta.</abstract_fi>
      <abstract_jv>Ngawe Daerah sistem conversation kanggo saben dumateng anyar karo sistem sing nambah operasi sing butuh model sing di nggawe barang-pakan nggunakake sistem sing wis ana (t.e.g. iso dianggawe operasi layang-pakan). Awak dhéwé wis rampung dibutuhke iki, kita nggawe layang-layang sistem kebuturan cara nggawe dinor duwelan sing dikarolan USE lan conveRT. Awak dhéwé éntukno kabèh lan aplikasi kanggo Ketokanan aturan sing beraksi 2) Digambut sing dibutungano uwong apa-apa ning titik-apa (dadi, saboh barang 10 atawa 30 sing apik dadi bisa balikat). 3) Perintah-perintah sing dibutuhke ditambah podho kelas nang sampeyan ingkang sampeyan; akeh and 4) it is stable against the same parameter settings. Nambah sing beraksi karo perusahaan lan démocrasyone resampungan dipunangguna nggawe barang nggawe winih, kita kebebasan kode dhéwé, lak ngono nggawe waé sing beraksi podho nggawe dataset ingkang 13,583 sing apik dhéwé, wigatining sedhaya sing berarti.</abstract_jv>
      <abstract_sk>Za gradnjo pogovornih sistemov na novih področjih in z dodatno funkcionalnostjo so potrebni modeli, ki učinkovito učinkoviti z viri, ki delujejo v režimih z nizko količino podatkov (tj. pri nekaj posnetkih). Na podlagi teh zahtev uvajamo metode zaznavanja namena, ki jih podpirajo predhodno uvedeni kodirniki dvojnih stavkov, kot sta USE in ConveRT. Prikazujemo uporabnost in široko uporabnost predlaganih detektorjev namena, pri čemer pokažemo, da: 1) presegajo detektorje namena, ki temeljijo na natančnem nastavitvi celotnega modela BERT-Large ali uporabi BERT kot fiksnega kodirja črnega polja na treh različnih naborih podatkov o zaznavanju namena; 2) dobički so še posebej izraziti pri nekaj posnetkih (tj. z le 10 ali 30 označenimi primeri na namen); 3) naše detektorje namena lahko usposabljamo v nekaj minutah na enem procesorju; in 4) so stabilni v različnih nastavitvah hiperparametrov. V upanju, da bomo olajšali in demokratizirali raziskave, osredotočene na odkrivanje namenov, objavljamo našo kodo in nov zahteven nabor podatkov o odkrivanju namenov z eno domeno, ki vsebuje 13.083 označenih primerov nad 77 nameni.</abstract_sk>
      <abstract_he>בניית מערכות שיחה בתחומים חדשים ובתוספת פונקציונליות דורשות דוגמנים יעילים משאבים שעובדים תחת מערכות נתונים נמוכות (כלומר, במערכות קטנות). מוטיבציה על ידי הדרישות האלה, אנו מכירים שיטות גילוי כוונות תומכות על ידי קודים משפטים כפולים מתאמנים מראש, כמו USE ובConveRT. אנחנו מראים את השימוש והאפשרות הרחבה של גלאי הכוונות המוצעים, מראים כי: 1) הם יוצאים מעל גלאי הכוונות המבוססים על התדרגות המלאה של המודל BERT-גדול או בשימוש BERT כקודד קופסה שחורה קבוע על שלושה קבוצות נתונים של זיהוי הכוונות מגוונים; 2) הרווחים מבטחים במיוחד במערכות קטנות (כלומר, עם רק 10 או 30 דוגמאות מצוינות לכוונה); 3) גלאי הכוונה שלנו יכולים להיות מאומנים בעוד כמה דקות על CPU אחד; ו-4) הם יציבים במערכות היפרפרמטרים שונות. בתקווה להקל ולדמוקרטיזם מחקר ממוקד על זיהוי כוונות, אנחנו משחררים את הקוד שלנו, כמו גם קבוצת נתונים חדשה מאתגרת לזהות כוונות במשטרה אחת שמכילה 13,083 דוגמאות מוצבעות מעל 77 כוונות.</abstract_he>
      <abstract_ha>Yi samun shiryoyin ayuka da aka haɗa cikin wurãre-daban, da kuma an ƙara wani aiki, yana ƙayyade misãlai masu da amfani da resource da su yi aiki a ƙarƙashin-data (misali, cikin tsari masu ƙaranci). Aka fara da wannan umarni, Munã ƙara hanyoyin bayani masu iya ƙaranci da aka baka ko-kodi biyu kamar shirin YUK da ConvRT. Tuna nuna amfani da amfani da mai shimfiɗawa wa masu shirya matsayin shiryarwa da aka yi niyyar da shi, suna nuna: 1) suna tafiya zaɓani a kan gyarata masu amfani da matsayin mai cikakken BERT-Babbar ko kuma suna yin amfani da BERT kamar kodi mai daidaita matsayin-boxen mai baƙi a kan daidaita danne-zane-zane taki uku daban-daban; 2) Ana ƙayyade matsayin da aka ƙayyade shi a cikin masu tsari kaɗan (misali, da misãlai 10 ko 30 wanda aka yi zartar da shi a gaba ɗaya). 3) za'a iya kõre masu so cikin guda dakika guda a kan CPU; kuma 4) suna madaidaici a kan kowane zaɓallin giperparameteri dabam. In a tsammãni ga sauƙi da kuma a Democratizi research, yana fassara kowanmu da ke so, da sami wani zane-zane-zane-zane-zane-danne da ke cikin kashi-guda, wanda ke samun misãlai 13,083 da aka sanar da shi a kan kashfa 77.</abstract_ha>
      <abstract_bo>དྲ་ཁོངས་གསར་པའི་ནང་དུ་གཏམ་གླེང་སྒྲུབ་གྱི་མ་ལག་གསར་པ་དང་ཁ་སྐོང་རྩིས་མཐུན་བྱས་པའི་རྒྱུ་དངོས་ཡིག་སྟོན་ནུས་ཡོད་པའི་མ་ལག་ལེན་དགོས Motivated by these requirements, we introduce intent detection methods backed by pretrained dual sentence encoders such as USE and ConveRT. ང་ཚོས་འཆར་བཀོད་པའི་དམིགས་བསལ་བྱ་རིམ་གྱི་སྤྱོད་སྤྱོད་དང་ཆེ་བའི་འཇུག་སྤྱོད་མཁན་ལ་མངོན་འཆར་བྱེད་ཀྱི་ཡོད། 2) རྒྱལ་སྤྱིར་བཏང་བ་ཡིན་པའི་སྒྲིག་འཛིན་ཉུང་ཅིག་གི་ནང་དུ་གཏོང་ཡོད། 3) ང་ཚོའི་དམིགས་བསལ་རྟོགས་པ་ཚོ་སྐར་ཆ་གཅིག་པུ་ཞིག་གི་ནང་སློབ་འཛུགས་བྱེད་སྲིད། and 4) they are stable across different hyperparameter settings. In hope of facilitating and democratizing research focused on intention detection, we release our code, as well as a new challenging single-domain intent detection dataset comprising 13,083 annotated examples over 77 intents.</abstract_bo>
      </paper>
    <paper id="6">
      <title>Accelerating <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">Natural Language Understanding</a> in Task-Oriented Dialog</title>
      <author><first>Ojas</first><last>Ahuja</last></author>
      <author><first>Shrey</first><last>Desai</last></author>
      <pages>46–53</pages>
      <abstract>Task-oriented dialog models typically leverage complex neural architectures and large-scale, pre-trained Transformers to achieve state-of-the-art performance on popular natural language understanding benchmarks. However, these models frequently have in excess of tens of millions of parameters, making them impossible to deploy on-device where <a href="https://en.wikipedia.org/wiki/Resource_efficiency">resource-efficiency</a> is a major concern. In this work, we show that a simple convolutional model compressed with structured pruning achieves largely comparable results to BERT on <a href="https://en.wikipedia.org/wiki/Automatic_terminal_information_service">ATIS</a> and Snips, with under 100 K parameters. Moreover, we perform acceleration experiments on <a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPUs</a>, where we observe our multi-task model predicts intents and slots nearly 63x faster than even DistilBERT.</abstract>
      <url hash="686fcd1b">2020.nlp4convai-1.6</url>
      <doi>10.18653/v1/2020.nlp4convai-1.6</doi>
      <bibkey>ahuja-desai-2020-accelerating</bibkey>
      <pwccode url="https://github.com/oja/pruned-nlu" additional="false">oja/pruned-nlu</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="9">
      <title>Automating Template Creation for Ranking-Based Dialogue Models</title>
      <author><first>Jingxiang</first><last>Chen</last></author>
      <author><first>Heba</first><last>Elfardy</last></author>
      <author><first>Simi</first><last>Wang</last></author>
      <author><first>Andrea</first><last>Kahn</last></author>
      <author><first>Jared</first><last>Kramer</last></author>
      <pages>71–78</pages>
      <abstract>Dialogue response generation models that use template ranking rather than direct sequence generation allow model developers to limit generated responses to pre-approved messages. However, manually creating templates is time-consuming and requires domain expertise. To alleviate this problem, we explore automating the process of creating dialogue templates by using <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a> to cluster historical utterances and selecting representative utterances from each <a href="https://en.wikipedia.org/wiki/Cluster_analysis">cluster</a>. Specifically, we propose an end-to-end model called Deep Sentence Encoder Clustering (DSEC) that uses an auto-encoder structure to jointly learn the utterance representation and construct template clusters. We compare this method to a random baseline that randomly assigns templates to clusters as well as a strong baseline that performs the sentence encoding and the utterance clustering sequentially. To evaluate the performance of the proposed method, we perform an automatic evaluation with two annotated customer service datasets to assess clustering effectiveness, and a human-in-the-loop experiment using a live customer service application to measure the acceptance rate of the generated templates. DSEC performs best in the automatic evaluation, beats both the sequential and random baselines on most metrics in the human-in-the-loop experiment, and shows promising results when compared to gold / manually created templates.</abstract>
      <url hash="e2fc267c">2020.nlp4convai-1.9</url>
      <attachment type="Software" hash="d3cfb0f6">2020.nlp4convai-1.9.Software.txt</attachment>
      <doi>10.18653/v1/2020.nlp4convai-1.9</doi>
      <attachment type="Software" hash="b5003fb1">2020.nlp4convai-1.9.Software.zip</attachment>
      <video href="http://slideslive.com/38929630" />
      <bibkey>chen-etal-2020-automating</bibkey>
    </paper>
    <paper id="13">
      <title>MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines<fixed-case>M</fixed-case>ulti<fixed-case>WOZ</fixed-case> 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines</title>
      <author><first>Xiaoxue</first><last>Zang</last></author>
      <author><first>Abhinav</first><last>Rastogi</last></author>
      <author><first>Srinivas</first><last>Sunkara</last></author>
      <author><first>Raghav</first><last>Gupta</last></author>
      <author><first>Jianguo</first><last>Zhang</last></author>
      <author><first>Jindong</first><last>Chen</last></author>
      <pages>109–117</pages>
      <abstract>MultiWOZ is a well-known task-oriented dialogue dataset containing over 10,000 annotated dialogues spanning 8 domains. It is extensively used as a benchmark for dialogue state tracking. However, recent works have reported presence of substantial noise in the dialogue state annotations. MultiWOZ 2.1 identified and fixed many of these erroneous <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> and user utterances, resulting in an improved version of this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. This work introduces MultiWOZ 2.2, which is a yet another improved version of this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. Firstly, we identify and fix dialogue state annotation errors across 17.3 % of the utterances on top of MultiWOZ 2.1. Secondly, we redefine the <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a> by disallowing vocabularies of slots with a large number of possible values (e.g., restaurant name, time of booking). In addition, we introduce slot span annotations for these slots to standardize them across recent models, which previously used custom string matching heuristics to generate them. We also benchmark a few state of the art dialogue state tracking models on the corrected dataset to facilitate comparison for future work. In the end, we discuss best practices for dialogue data collection that can help avoid annotation errors.</abstract>
      <url hash="5dc5d275">2020.nlp4convai-1.13</url>
      <doi>10.18653/v1/2020.nlp4convai-1.13</doi>
      <video href="http://slideslive.com/38929641" />
      <bibkey>zang-etal-2020-multiwoz</bibkey>
      <pwccode url="https://github.com/budzianowski/multiwoz" additional="false">budzianowski/multiwoz</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/100doh">100DOH</pwcdataset>
    </paper>
    <paper id="15">
      <title>Probing Neural Dialog Models for Conversational Understanding</title>
      <author><first>Abdelrhman</first><last>Saleh</last></author>
      <author><first>Tovly</first><last>Deutsch</last></author>
      <author><first>Stephen</first><last>Casper</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <author><first>Stuart</first><last>Shieber</last></author>
      <pages>132–143</pages>
      <abstract>The predominant approach to open-domain dialog generation relies on end-to-end training of neural models on chat datasets. However, this approach provides little insight as to what these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> learn (or do not learn) about engaging in <a href="https://en.wikipedia.org/wiki/Dialogue">dialog</a>. In this study, we analyze the internal representations learned by neural open-domain dialog systems and evaluate the quality of these <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> for learning basic conversational skills. Our results suggest that standard open-domain dialog systems struggle with answering questions, inferring <a href="https://en.wikipedia.org/wiki/Contradiction">contradiction</a>, and determining the topic of conversation, among other tasks. We also find that the dyadic, turn-taking nature of <a href="https://en.wikipedia.org/wiki/Dialogue">dialog</a> is not fully leveraged by these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. By exploring these limitations, we highlight the need for additional research into <a href="https://en.wikipedia.org/wiki/Computer_architecture">architectures</a> and <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training methods</a> that can better capture high-level information about dialog.</abstract>
      <url hash="9360d36d">2020.nlp4convai-1.15</url>
      <doi>10.18653/v1/2020.nlp4convai-1.15</doi>
      <video href="http://slideslive.com/38929635" />
      <bibkey>saleh-etal-2020-probing</bibkey>
      <pwccode url="https://github.com/AbdulSaleh/dialog-probing" additional="false">AbdulSaleh/dialog-probing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
  </volume>
</collection>