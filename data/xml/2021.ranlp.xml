<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.ranlp">
  <volume id="1" ingest-date="2021-11-09">
    <meta>
      <booktitle>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</booktitle>
      <editor><first>Ruslan</first><last>Mitkov</last></editor>
      <editor><first>Galia</first><last>Angelova</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Held Online</address>
      <month>September</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="3db98e19">2021.ranlp-1.0</url>
      <bibkey>ranlp-2021-deep</bibkey>
    </frontmatter>
    <paper id="6">
      <title>English-Arabic Cross-language Plagiarism Detection<fixed-case>E</fixed-case>nglish-<fixed-case>A</fixed-case>rabic Cross-language Plagiarism Detection</title>
      <author><first>Naif</first><last>Alotaibi</last></author>
      <author><first>Mike</first><last>Joy</last></author>
      <pages>44–52</pages>
      <abstract>The advancement of the <a href="https://en.wikipedia.org/wiki/Information_and_communications_technology">web and information technology</a> has contributed to the rapid growth of <a href="https://en.wikipedia.org/wiki/Digital_library">digital libraries</a> and <a href="https://en.wikipedia.org/wiki/Machine_translation">automatic machine translation tools</a> which easily translate texts from one language into another. These have increased the content accessible in different languages, which results in easily performing translated plagiarism, which are referred to as cross-language plagiarism. Recognition of plagiarism among texts in different languages is more challenging than identifying <a href="https://en.wikipedia.org/wiki/Plagiarism">plagiarism</a> within a corpus written in the same language. This paper proposes a new <a href="https://en.wikipedia.org/wiki/Scientific_technique">technique</a> for enhancing English-Arabic cross-language plagiarism detection at the <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence level</a>. This technique is based on semantic and syntactic feature extraction using <a href="https://en.wikipedia.org/wiki/Word_order">word order</a>, <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> and <a href="https://en.wikipedia.org/wiki/Word_alignment">word alignment</a> with multilingual encoders. Those <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>, and their combination with different <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning (ML) algorithms</a>, are then used in order to aid the task of classifying sentences as either <a href="https://en.wikipedia.org/wiki/Plagiarism">plagiarized</a> or non-plagiarized. The proposed approach has been deployed and assessed using datasets presented at SemEval-2017. Analysis of experimental data demonstrates that utilizing extracted <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> and their combinations with various ML classifiers achieves promising results.</abstract>
      <url hash="e4ac941f">2021.ranlp-1.6</url>
      <bibkey>alotaibi-joy-2021-english</bibkey>
    </paper>
    <paper id="9">
      <title>Enriching the <a href="https://en.wikipedia.org/wiki/Transformer">Transformer</a> with Linguistic Factors for Low-Resource Machine Translation</title>
      <author><first>Jordi</first><last>Armengol-Estapé</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Carlos</first><last>Escolano</last></author>
      <pages>73–78</pages>
      <abstract>Introducing factors, that is to say, word features such as linguistic information referring to the source tokens, is known to improve the results of neural machine translation systems in certain settings, typically in recurrent architectures. This study proposes enhancing the current state-of-the-art neural machine translation architecture, the Transformer, so that it allows to introduce external knowledge. In particular, our proposed modification, the Factored Transformer, uses linguistic factors that insert additional knowledge into the <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation system</a>. Apart from using different kinds of features, we study the effect of different architectural configurations. Specifically, we analyze the performance of combining words and <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> at the <a href="https://en.wikipedia.org/wiki/Embedding">embedding level</a> or at the <a href="https://en.wikipedia.org/wiki/Encoder">encoder level</a>, and we experiment with two different <a href="https://en.wikipedia.org/wiki/Combination">combination strategies</a>. With the best-found configuration, we show improvements of 0.8 BLEU over the baseline Transformer in the IWSLT German-to-English task. Moreover, we experiment with the more challenging FLoRes English-to-Nepali benchmark, which includes both extremely low-resourced and very distant languages, and obtain an improvement of 1.2 BLEU</abstract>
      <url hash="8d853930">2021.ranlp-1.9</url>
      <bibkey>armengol-estape-etal-2021-enriching</bibkey>
    </paper>
    <paper id="10">
      <title>A Multi-Pass Sieve Coreference Resolution for Indonesian<fixed-case>I</fixed-case>ndonesian</title>
      <author><first>Valentina Kania Prameswara</first><last>Artari</last></author>
      <author><first>Rahmad</first><last>Mahendra</last></author>
      <author><first>Meganingrum Arista</first><last>Jiwanggi</last></author>
      <author><first>Adityo</first><last>Anggraito</last></author>
      <author><first>Indra</first><last>Budi</last></author>
      <pages>79–85</pages>
      <abstract>Coreference resolution is an NLP task to find out whether the set of referring expressions belong to the same concept in discourse. A multi-pass sieve is a deterministic coreference model that implements several layers of sieves, where each <a href="https://en.wikipedia.org/wiki/Sieve_theory">sieve</a> takes a pair of correlated mentions from a collection of non-coherent mentions. The multi-pass sieve is based on the principle of high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a>, followed by increased <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> in each <a href="https://en.wikipedia.org/wiki/Sieve">sieve</a>. In this work, we examine the portability of the multi-pass sieve coreference resolution model to the <a href="https://en.wikipedia.org/wiki/Indonesian_language">Indonesian language</a>. We conduct the experiment on 201 Wikipedia documents and the multi-pass sieve system yields 72.74 % of MUC F-measure and 52.18 % of BCUBED F-measure.</abstract>
      <url hash="526e3641">2021.ranlp-1.10</url>
      <bibkey>artari-etal-2021-multi</bibkey>
    </paper>
    <paper id="12">
      <title>PyEuroVoc : A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors<fixed-case>P</fixed-case>y<fixed-case>E</fixed-case>uro<fixed-case>V</fixed-case>oc: A Tool for Multilingual Legal Document Classification with <fixed-case>E</fixed-case>uro<fixed-case>V</fixed-case>oc Descriptors</title>
      <author><first>Andrei-Marius</first><last>Avram</last></author>
      <author><first>Vasile</first><last>Pais</last></author>
      <author><first>Dan Ioan</first><last>Tufis</last></author>
      <pages>92–101</pages>
      <abstract>EuroVoc is a multilingual thesaurus that was built for organizing the legislative documentary of the European Union institutions. It contains thousands of categories at different levels of <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">specificity</a> and its descriptors are targeted by <a href="https://en.wikipedia.org/wiki/Legal_writing">legal texts</a> in almost thirty languages. In this work we propose a unified framework for <a href="https://en.wikipedia.org/wiki/EuroVoc">EuroVoc classification</a> on 22 languages by fine-tuning modern Transformer-based pretrained language models. We study extensively the performance of our trained <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> and show that they significantly improve the results obtained by a similar tool-JEX-on the same <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. The <a href="https://en.wikipedia.org/wiki/Source_code">code</a> and the fine-tuned models were open sourced, together with a programmatic interface that eases the process of loading the weights of a trained <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> and of classifying a new document.</abstract>
      <url hash="499b00f2">2021.ranlp-1.12</url>
      <bibkey>avram-etal-2021-pyeurovoc</bibkey>
      <pwccode url="https://github.com/racai-ai/EuroVoc-BERT" additional="true">racai-ai/EuroVoc-BERT</pwccode>
    </paper>
    <paper id="13">
      <title>TEASER : Towards Efficient Aspect-based SEntiment Analysis and Recognition<fixed-case>TEASER</fixed-case>: Towards Efficient Aspect-based <fixed-case>SE</fixed-case>ntiment Analysis and Recognition</title>
      <author><first>Vaibhav</first><last>Bajaj</last></author>
      <author><first>Kartikey</first><last>Pant</last></author>
      <author><first>Ishan</first><last>Upadhyay</last></author>
      <author><first>Srinath</first><last>Nair</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>102–110</pages>
      <abstract>Sentiment analysis aims to detect the overall sentiment, i.e., the polarity of a sentence, paragraph, or text span, without considering the entities mentioned and their aspects. Aspect-based sentiment analysis aims to extract the aspects of the given target entities and their respective sentiments. Prior works formulate this as a sequence tagging problem or solve this task using a span-based extract-then-classify framework where first all the opinion targets are extracted from the sentence, and then with the help of span representations, the targets are classified as positive, negative, or neutral. The sequence tagging problem suffers from issues like sentiment inconsistency and colossal search space. Whereas, Span-based extract-then-classify framework suffers from issues such as half-word coverage and overlapping spans. To overcome this, we propose a similar span-based extract-then-classify framework with a novel and improved heuristic. Experiments on the three benchmark datasets (Restaurant14, Laptop14, Restaurant15) show our model consistently outperforms the current state-of-the-art. Moreover, we also present a novel supervised movie reviews dataset (Movie20) and a pseudo-labeled movie reviews dataset (moviesLarge) made explicitly for this task and report the results on the novel Movie20 dataset as well.</abstract>
      <url hash="42af87f1">2021.ranlp-1.13</url>
      <bibkey>bajaj-etal-2021-teaser</bibkey>
    </paper>
    <paper id="15">
      <title>Litescale : A Lightweight Tool for Best-worst Scaling Annotation</title>
      <author><first>Valerio</first><last>Basile</last></author>
      <author><first>Christian</first><last>Cagnazzo</last></author>
      <pages>121–127</pages>
      <abstract>Best-worst Scaling (BWS) is a <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> for <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> based on comparing and ranking instances, rather than classifying or scoring individual instances. Studies have shown the efficacy of this <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> applied to NLP tasks in terms of a higher quality of the datasets produced by following it. In this system demonstration paper, we present Litescale, a free software library to create and manage BWS annotation tasks. Litescale computes the tuples to annotate, manages the users and the annotation process, and creates the final gold standard. The functionalities of Litescale can be accessed programmatically through a <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python module</a>, or via two alternative <a href="https://en.wikipedia.org/wiki/User_interface">user interfaces</a>, a textual console-based one and a graphical Web-based one. We further developed and deployed a fully online version of Litescale complete with <a href="https://en.wikipedia.org/wiki/Multi-user_software">multi-user support</a>.</abstract>
      <url hash="2b5d8b95">2021.ranlp-1.15</url>
      <bibkey>basile-cagnazzo-2021-litescale</bibkey>
    </paper>
    <paper id="17">
      <title>Cross-Lingual Wolastoqey-English Definition Modelling<fixed-case>E</fixed-case>nglish Definition Modelling</title>
      <author><first>Diego</first><last>Bear</last></author>
      <author><first>Paul</first><last>Cook</last></author>
      <pages>138–146</pages>
      <abstract>Definition modelling is the task of automatically generating a <a href="https://en.wikipedia.org/wiki/Dictionary_definition">dictionary-style definition</a> given a target word. In this paper, we consider cross-lingual definition generation. Specifically, we generate English definitions for Wolastoqey (Malecite-Passamaquoddy) words. Wolastoqey is an endangered, low-resource polysynthetic language. We hypothesize that sub-word representations based on byte pair encoding (Sennrich et al., 2016) can be leveraged to represent morphologically-complex Wolastoqey words and overcome the challenge of not having large corpora available for training. Our experimental results demonstrate that this approach outperforms baseline methods in terms of BLEU score.</abstract>
      <url hash="8e321b88">2021.ranlp-1.17</url>
      <bibkey>bear-cook-2021-cross</bibkey>
    </paper>
    <paper id="20">
      <title>On the Contribution of Per-ICD Attention Mechanisms to Classify Health Records in Languages with Fewer Resources than English<fixed-case>ICD</fixed-case> Attention Mechanisms to Classify Health Records in Languages with Fewer Resources than <fixed-case>E</fixed-case>nglish</title>
      <author><first>Alberto</first><last>Blanco</last></author>
      <author><first>Sonja</first><last>Remmer</last></author>
      <author><first>Alicia</first><last>Pérez</last></author>
      <author><first>Hercules</first><last>Dalianis</last></author>
      <author><first>Arantza</first><last>Casillas</last></author>
      <pages>165–172</pages>
      <abstract>We introduce a multi-label text classifier with per-label attention for the classification of Electronic Health Records according to the <a href="https://en.wikipedia.org/wiki/International_Classification_of_Diseases">International Classification of Diseases</a>. We apply the model on two Electronic Health Records datasets with Discharge Summaries in two languages with fewer resources than <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a> and <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a>. Our model leverages the BERT Multilingual model (specifically the <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, as the model have been trained with 104 languages, including <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a> and <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a>, with the largest <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia dumps</a>) to share the language modelling capabilities across the languages. With the per-label attention, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can compute the relevance of each word from the EHR towards the prediction of each label. For the experimental framework, we apply 157 labels from Chapter XI   Diseases of the Digestive System of the ICD, which makes the attention especially important as the model has to discriminate between similar diseases. 1 https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages</abstract>
      <url hash="d06ba45b">2021.ranlp-1.20</url>
      <bibkey>blanco-etal-2021-contribution</bibkey>
      <pwccode url="https://github.com/google-research/bert" additional="false">google-research/bert</pwccode>
    </paper>
    <paper id="21">
      <title>Can the Transformer Be Used as a Drop-in Replacement for RNNs in Text-Generating GANs?<fixed-case>RNN</fixed-case>s in Text-Generating <fixed-case>GAN</fixed-case>s?</title>
      <author><first>Kevin</first><last>Blin</last></author>
      <author><first>Andrei</first><last>Kucharavy</last></author>
      <pages>173–181</pages>
      <abstract>In this paper we address the problem of fine-tuned text generation with a limited computational budget. For that, we use a well-performing text generative adversarial network (GAN) architecture-Diversity-Promoting GAN (DPGAN), and attempted a drop-in replacement of the LSTM layer with a self-attention-based Transformer layer in order to leverage their efficiency. The resulting Self-Attention DPGAN (SADPGAN) was evaluated for performance, quality and diversity of generated text and stability. Computational experiments suggested that a transformer architecture is unable to drop-in replace the LSTM layer, under-performing during the pre-training phase and undergoing a complete mode collapse during the GAN tuning phase. Our results suggest that the transformer architecture need to be adapted before it can be used as a replacement for RNNs in text-generating GANs.</abstract>
      <url hash="ae4edfc2">2021.ranlp-1.21</url>
      <bibkey>blin-kucharavy-2021-transformer</bibkey>
    </paper>
    <paper id="22">
      <title>Predicting the Factuality of Reporting of News Media Using Observations about User Attention in Their YouTube Channels<fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ube Channels</title>
      <author><first>Krasimira</first><last>Bozhanova</last></author>
      <author><first>Yoan</first><last>Dinkov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Maria</first><last>Castaldo</last></author>
      <author><first>Tommaso</first><last>Venturini</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>182–189</pages>
      <abstract>We propose a novel framework for predicting the factuality of reporting of news media outlets by studying the user attention cycles in their YouTube channels. In particular, we design a rich set of features derived from the temporal evolution of the number of views, likes, dislikes, and comments for a video, which we then aggregate to the channel level. We develop and release a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for the task, containing observations of user attention on YouTube channels for 489 <a href="https://en.wikipedia.org/wiki/News_media">news media</a>. Our experiments demonstrate both complementarity and sizable improvements over state-of-the-art textual representations.</abstract>
      <url hash="858a220a">2021.ranlp-1.22</url>
      <bibkey>bozhanova-etal-2021-predicting</bibkey>
    </paper>
    <paper id="24">
      <title>A Psychologically Informed Part-of-Speech Analysis of Depression in <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a></title>
      <author><first>Ana-Maria</first><last>Bucur</last></author>
      <author><first>Ioana R.</first><last>Podina</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <pages>199–207</pages>
      <abstract>In this work, we provide an extensive part-of-speech analysis of the discourse of social media users with depression. Research in <a href="https://en.wikipedia.org/wiki/Psychology">psychology</a> revealed that depressed users tend to be self-focused, more preoccupied with themselves and ruminate more about their lives and emotions. Our work aims to make use of <a href="https://en.wikipedia.org/wiki/Data_set">large-scale datasets</a> and <a href="https://en.wikipedia.org/wiki/Computational_linguistics">computational methods</a> for a quantitative exploration of discourse. We use the publicly available depression dataset from the Early Risk Prediction on the Internet Workshop (eRisk) 2018 and extract part-of-speech features and several indices based on them. Our results reveal statistically significant differences between the depressed and non-depressed individuals confirming findings from the existing psychology literature. Our work provides insights regarding the way in which depressed individuals are expressing themselves on <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a>, allowing for better-informed <a href="https://en.wikipedia.org/wiki/Computational_model">computational models</a> to help monitor and prevent mental illnesses.</abstract>
      <url hash="41e9cd41">2021.ranlp-1.24</url>
      <bibkey>bucur-etal-2021-psychologically</bibkey>
    </paper>
    <paper id="28">
      <title>Evaluating Recognizing Question Entailment Methods for a Portuguese Community Question-Answering System about Diabetes Mellitus<fixed-case>P</fixed-case>ortuguese Community Question-Answering System about Diabetes Mellitus</title>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>João</first><last>Victor de Pinho Costa</last></author>
      <author><first>Isabela</first><last>Rigotto</last></author>
      <author><first>Vitoria</first><last>Portella</last></author>
      <author><first>Gabriel</first><last>Frota</last></author>
      <author><first>Ana</first><last>Luisa A. R. Guimarães</last></author>
      <author><first>Adalberto</first><last>Penna</last></author>
      <author><first>Isabela</first><last>Lee</last></author>
      <author><first>Tayane</first><last>A. Soares</last></author>
      <author><first>Sophia</first><last>Rolim</last></author>
      <author><first>Rossana</first><last>Cunha</last></author>
      <author><first>Celso</first><last>França</last></author>
      <author><first>Ariel</first><last>Santos</last></author>
      <author><first>Rivaney</first><last>F. Oliveira</last></author>
      <author><first>Abisague</first><last>Langbehn</last></author>
      <author><first>Daniel</first><last>Hasan Dalip</last></author>
      <author><first>Marcos</first><last>André Gonçalves</last></author>
      <author><first>Rodrigo</first><last>Bastos Fóscolo</last></author>
      <author><first>Adriana</first><last>Pagano</last></author>
      <pages>234–243</pages>
      <abstract>This study describes the development of a Portuguese Community-Question Answering benchmark in the domain of <a href="https://en.wikipedia.org/wiki/Diabetes">Diabetes Mellitus</a> using a Recognizing Question Entailment (RQE) approach. Given a premise question, RQE aims to retrieve semantically similar, already answered, archived questions. We build a new Portuguese benchmark corpus with 785 pairs between premise questions and archived answered questions marked with relevance judgments by medical experts. Based on the benchmark corpus, we leveraged and evaluated several RQE approaches ranging from traditional information retrieval methods to novel large pre-trained language models and ensemble techniques using learn-to-rank approaches. Our experimental results show that a supervised transformer-based method trained with multiple languages and for multiple tasks (MUSE) outperforms the alternatives. Our results also show that ensembles of methods (stacking) as well as a traditional (light) information retrieval method (BM25) can produce competitive results. Finally, among the tested <a href="https://en.wikipedia.org/wiki/Strategy">strategies</a>, those that exploit only the question (not the answer), provide the best effectiveness-efficiency trade-off. Code is publicly available.</abstract>
      <url hash="cf99b93f">2021.ranlp-1.28</url>
      <bibkey>castro-ferreira-etal-2021-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/medquad">MedQuAD</pwcdataset>
    </paper>
    <paper id="29">
      <title>On the Usability of Transformers-based Models for a French Question-Answering Task<fixed-case>F</fixed-case>rench Question-Answering Task</title>
      <author><first>Oralie</first><last>Cattan</last></author>
      <author><first>Christophe</first><last>Servan</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <pages>244–255</pages>
      <abstract>For many tasks, state-of-the-art results have been achieved with Transformer-based architectures, resulting in a paradigmatic shift in practices from the use of task-specific architectures to the fine-tuning of pre-trained language models. The ongoing trend consists in training <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> with an ever-increasing amount of data and parameters, which requires considerable resources. It leads to a strong search to improve <a href="https://en.wikipedia.org/wiki/Resource_efficiency">resource efficiency</a> based on algorithmic and hardware improvements evaluated only for <a href="https://en.wikipedia.org/wiki/English_language">English</a>. This raises questions about their usability when applied to small-scale learning problems, for which a limited amount of training data is available, especially for under-resourced languages tasks. The lack of appropriately sized corpora is a hindrance to applying data-driven and transfer learning-based approaches with strong instability cases. In this paper, we establish a state-of-the-art of the efforts dedicated to the usability of Transformer-based models and propose to evaluate these improvements on the question-answering performances of <a href="https://en.wikipedia.org/wiki/French_language">French language</a> which have few resources. We address the instability relating to data scarcity by investigating various training strategies with <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, <a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">hyperparameters optimization</a> and cross-lingual transfer. We also introduce a new compact model for French FrALBERT which proves to be competitive in low-resource settings.</abstract>
      <url hash="09c969d4">2021.ranlp-1.29</url>
      <bibkey>cattan-etal-2021-usability</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fquad">FQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/french-wikipedia">French Wikipedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/oscar">OSCAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="31">
      <title>Character-based Thai Word Segmentation with Multiple Attentions<fixed-case>T</fixed-case>hai Word Segmentation with Multiple Attentions</title>
      <author><first>Thodsaporn</first><last>Chay-intr</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>264–273</pages>
      <abstract>Character-based word-segmentation models have been extensively applied to <a href="https://en.wikipedia.org/wiki/Agglutinative_language">agglutinative languages</a>, including <a href="https://en.wikipedia.org/wiki/Thai_language">Thai</a>, due to their high performance. These <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> estimate <a href="https://en.wikipedia.org/wiki/Word_(computer_architecture)">word boundaries</a> from a <a href="https://en.wikipedia.org/wiki/Character_(computing)">character sequence</a>. However, a <a href="https://en.wikipedia.org/wiki/Character_(computing)">character unit</a> in sequences has no essential meaning, compared with word, subword, and character cluster units. We propose a Thai word-segmentation model that uses various types of information, including <a href="https://en.wikipedia.org/wiki/Word">words</a>, <a href="https://en.wikipedia.org/wiki/Subword">subwords</a>, and character clusters, from a character sequence. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> applies multiple attentions to refine segmentation inferences by estimating the significant relationships among characters and various unit types. The experimental results indicate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can outperform other state-of-the-art Thai word-segmentation models.</abstract>
      <url hash="d2d99700">2021.ranlp-1.31</url>
      <bibkey>chay-intr-etal-2021-character</bibkey>
      <pwccode url="https://github.com/tchayintr/thwcc-attn" additional="false">tchayintr/thwcc-attn</pwccode>
    </paper>
    <paper id="32">
      <title>Are Language-Agnostic Sentence Representations Actually Language-Agnostic?</title>
      <author><first>Yu</first><last>Chen</last></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <pages>274–280</pages>
      <abstract>With the emergence of pre-trained multilingual models, multilingual embeddings have been widely applied in various natural language processing tasks. Language-agnostic models provide a versatile way to convert linguistic units from different languages into a shared vector representation space. The relevant work on multilingual sentence embeddings has reportedly reached low <a href="https://en.wikipedia.org/wiki/Error_rate">error rate</a> in cross-lingual similarity search tasks. In this paper, we apply the pre-trained embedding models and the cross-lingual similarity search task in diverse scenarios, and observed large discrepancy in results in comparison to the original paper. Our findings on cross-lingual similarity search with different newly constructed multilingual datasets show not only correlation with observable language similarities but also strong influence from factors such as translation paths, which limits the interpretation of the language-agnostic property of the LASER model.</abstract>
      <url hash="69246c56">2021.ranlp-1.32</url>
      <bibkey>chen-avgustinova-2021-language</bibkey>
    </paper>
    <paper id="37">
      <title>Towards an Etymological Map of Romanian<fixed-case>R</fixed-case>omanian</title>
      <author><first>Alina Maria</first><last>Cristea</last></author>
      <author><first>Anca</first><last>Dinu</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <author><first>Simona</first><last>Georgescu</last></author>
      <author><first>Ana Sabina</first><last>Uban</last></author>
      <author><first>Laurentiu</first><last>Zoicas</last></author>
      <pages>315–323</pages>
      <abstract>In this paper we investigate the <a href="https://en.wikipedia.org/wiki/Etymology">etymology</a> of <a href="https://en.wikipedia.org/wiki/Romanian_language">Romanian words</a>. We start from the <a href="https://en.wikipedia.org/wiki/Romanian_lexicon">Romanian lexicon</a> and automatically extract information from multiple <a href="https://en.wikipedia.org/wiki/List_of_etymological_dictionaries">etymological dictionaries</a>. We evaluate the results and perform extensive quantitative and qualitative analyses with the goal of building an <a href="https://en.wikipedia.org/wiki/Etymology">etymological map</a> of the language.</abstract>
      <url hash="0b01377b">2021.ranlp-1.37</url>
      <bibkey>cristea-etal-2021-towards</bibkey>
    </paper>
    <paper id="40">
      <title>Event Prominence Extraction Combining a Knowledge-Based Syntactic Parser and a BERT Classifier for Dutch<fixed-case>BERT</fixed-case> Classifier for <fixed-case>D</fixed-case>utch</title>
      <author><first>Thierry</first><last>Desot</last></author>
      <author><first>Orphee</first><last>De Clercq</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>346–357</pages>
      <abstract>A core task in <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a> is event detection that identifies event triggers in sentences that are typically classified into event types. In this study an event is considered as the unit to measure <a href="https://en.wikipedia.org/wiki/Multiculturalism">diversity</a> and similarity in <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> in the framework of a <a href="https://en.wikipedia.org/wiki/Recommender_system">news recommendation system</a>. Current typology-based event detection approaches fail to handle the variety of events expressed in real-world situations. To overcome this, we aim to perform event salience classification and explore whether a transformer model is capable of classifying new information into less and more general prominence classes. After comparing a Support Vector Machine (SVM) baseline and our transformer-based classifier performances on several event span formats, we conceived multi-word event spans as syntactic clauses. Those are fed into our prominence classifier which is fine-tuned on pre-trained Dutch BERT word embeddings. On top of that we outperform a pipeline of a Conditional Random Field (CRF) approach to event-trigger word detection and the BERT-based classifier. To the best of our knowledge we present the first event extraction approach that combines an expert-based syntactic parser with a transformer-based classifier for <a href="https://en.wikipedia.org/wiki/Dutch_language">Dutch</a>.</abstract>
      <url hash="6ea7b948">2021.ranlp-1.40</url>
      <bibkey>desot-etal-2021-event</bibkey>
    </paper>
    <paper id="41">
      <title>Automatic Detection and Classification of Mental Illnesses from General Social Media Texts</title>
      <author><first>Anca</first><last>Dinu</last></author>
      <author><first>Andreea-Codrina</first><last>Moldovan</last></author>
      <pages>358–366</pages>
      <abstract>Mental health is getting more and more attention recently, <a href="https://en.wikipedia.org/wiki/Depression_(mood)">depression</a> being a very common illness nowadays, but also other disorders like <a href="https://en.wikipedia.org/wiki/Anxiety">anxiety</a>, <a href="https://en.wikipedia.org/wiki/Obsessive–compulsive_disorder">obsessive-compulsive disorders</a>, <a href="https://en.wikipedia.org/wiki/Feeding_disorder">feeding disorders</a>, <a href="https://en.wikipedia.org/wiki/Autism">autism</a>, or <a href="https://en.wikipedia.org/wiki/Attention_deficit_hyperactivity_disorder">attention-deficit / hyperactivity disorders</a>. The huge amount of data from <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> and the recent advances of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning models</a> provide valuable means to automatically detecting mental disorders from <a href="https://en.wikipedia.org/wiki/Plain_text">plain text</a>. In this article, we experiment with state-of-the-art methods on the SMHD mental health conditions dataset from <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a> (Cohan et al., 2018). Our contribution is threefold : using a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> consisting of more illnesses than most studies, focusing on general text rather than mental health support groups and classification by posts rather than individuals or groups. For the automatic classification of the diseases, we employ three deep learning models : BERT, RoBERTa and XLNET. We double the <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline</a> established by Cohan et al. (2018), on just a sample of their <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. We improve the results obtained by Jiang et al. (2020) on post-level classification. The accuracy obtained by the eating disorder classifier is the highest due to the pregnant presence of discussions related to calories, diets, recipes etc., whereas <a href="https://en.wikipedia.org/wiki/Depression_(mood)">depression</a> had the lowest F1 score, probably because <a href="https://en.wikipedia.org/wiki/Depression_(mood)">depression</a> is more difficult to identify in linguistic acts.</abstract>
      <url hash="bcbf0f49">2021.ranlp-1.41</url>
      <bibkey>dinu-moldovan-2021-automatic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smhd">SMHD</pwcdataset>
    </paper>
    <paper id="42">
      <title>A Pre-trained Transformer and CNN Model with Joint Language ID and Part-of-Speech Tagging for Code-Mixed Social-Media Text<fixed-case>CNN</fixed-case> Model with Joint Language <fixed-case>ID</fixed-case> and Part-of-Speech Tagging for Code-Mixed Social-Media Text</title>
      <author><first>Suman</first><last>Dowlagar</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>367–374</pages>
      <abstract>Code-mixing (CM) is a frequently observed phenomenon that uses multiple languages in an utterance or sentence. There are no strict grammatical constraints observed in <a href="https://en.wikipedia.org/wiki/Code-mixing">code-mixing</a>, and it consists of non-standard variations of spelling. The linguistic complexity resulting from the above factors made the <a href="https://en.wikipedia.org/wiki/Computational_linguistics">computational analysis</a> of the code-mixed language a challenging task. Language identification (LI) and part of speech (POS) tagging are the fundamental steps that help analyze the structure of the code-mixed text. Often, the LI and POS tagging tasks are interdependent in the code-mixing scenario. We project the problem of dealing with <a href="https://en.wikipedia.org/wiki/Multilingualism">multilingualism</a> and <a href="https://en.wikipedia.org/wiki/Grammar">grammatical structure</a> while analyzing the code-mixed sentence as a joint learning task. In this paper, we jointly train and optimize <a href="https://en.wikipedia.org/wiki/Language_recognition">language detection</a> and <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part of speech tagging models</a> in the code-mixed scenario. We used a Transformer with convolutional neural network architecture. We train a joint learning method by combining POS tagging and LI models on code-mixed social media text obtained from the ICON shared task.</abstract>
      <url hash="83b14d89">2021.ranlp-1.42</url>
      <bibkey>dowlagar-mamidi-2021-pre</bibkey>
    </paper>
    <paper id="46">
      <title>Knowledge Discovery in COVID-19 Research Literature<fixed-case>COVID</fixed-case>-19 Research Literature</title>
      <author><first>Ernesto L.</first><last>Estevanell-Valladares</last></author>
      <author><first>Suilan</first><last>Estevez-Velarde</last></author>
      <author><first>Alejandro</first><last>Piad-Morffis</last></author>
      <author><first>Yoan</first><last>Gutierrez</last></author>
      <author><first>Andres</first><last>Montoyo</last></author>
      <author><first>Rafael</first><last>Muñoz</last></author>
      <author><first>Yudivián</first><last>Almeida Cruz</last></author>
      <pages>402–410</pages>
      <abstract>This paper presents the preliminary results of an ongoing project that analyzes the growing body of scientific research published around the COVID-19 pandemic. In this research, a general-purpose semantic model is used to double annotate a batch of 500 sentences that were manually selected from the CORD-19 corpus. Afterwards, a baseline text-mining pipeline is designed and evaluated via a large batch of 100,959 sentences. We present a qualitative analysis of the most interesting facts automatically extracted and highlight possible future lines of development. The preliminary results show that general-purpose semantic models are a useful tool for discovering fine-grained knowledge in large corpora of scientific documents.</abstract>
      <url hash="bb0bc177">2021.ranlp-1.46</url>
      <bibkey>estevanell-valladares-etal-2021-knowledge</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
    </paper>
    <paper id="47">
      <title>Online Learning over Time in Adaptive Neural Machine Translation</title>
      <author><first>Thierry</first><last>Etchegoyhen</last></author>
      <author><first>David</first><last>Ponce</last></author>
      <author><first>Harritxu</first><last>Gete</last></author>
      <author><first>Victor</first><last>Ruiz</last></author>
      <pages>411–420</pages>
      <abstract>Adaptive Machine Translation purports to dynamically include <a href="https://en.wikipedia.org/wiki/User-generated_content">user feedback</a> to improve translation quality. In a post-editing scenario, user corrections of machine translation output are thus continuously incorporated into translation models, reducing or eliminating repetitive error editing and increasing the usefulness of <a href="https://en.wikipedia.org/wiki/Machine_translation">automated translation</a>. In <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>, this goal may be achieved via online learning approaches, where network parameters are updated based on each new sample. This type of <a href="https://en.wikipedia.org/wiki/Adaptation">adaptation</a> typically requires higher learning rates, which can affect the quality of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> over time. Alternatively, less aggressive online learning setups may preserve model stability, at the cost of reduced adaptation to user-generated corrections. In this work, we evaluate different online learning configurations over time, measuring their impact on user-generated samples, as well as separate in-domain and out-of-domain datasets. Results in two different domains indicate that mixed approaches combining <a href="https://en.wikipedia.org/wiki/Educational_technology">online learning</a> with periodic batch fine-tuning might be needed to balance the benefits of <a href="https://en.wikipedia.org/wiki/Educational_technology">online learning</a> with model stability.</abstract>
      <url hash="78a594ee">2021.ranlp-1.47</url>
      <bibkey>etchegoyhen-etal-2021-online</bibkey>
    </paper>
    <paper id="50">
      <title>Cross-lingual Offensive Language Identification for Low Resource Languages : The Case of <a href="https://en.wikipedia.org/wiki/Marathi_language">Marathi</a><fixed-case>M</fixed-case>arathi</title>
      <author><first>Saurabh Sampatrao</first><last>Gaikwad</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Christopher</first><last>Homan</last></author>
      <pages>437–443</pages>
      <abstract>The widespread presence of <a href="https://en.wikipedia.org/wiki/Profanity">offensive language</a> on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> motivated the development of <a href="https://en.wikipedia.org/wiki/System">systems</a> capable of recognizing such <a href="https://en.wikipedia.org/wiki/Content_(media)">content</a> automatically. Apart from a few notable exceptions, most research on automatic offensive language identification has dealt with <a href="https://en.wikipedia.org/wiki/English_language">English</a>. To address this shortcoming, we introduce MOLD, the Marathi Offensive Language Dataset. MOLD is the first <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of its kind compiled for <a href="https://en.wikipedia.org/wiki/Marathi_language">Marathi</a>, thus opening a new domain for research in low-resource Indo-Aryan languages. We present results from several <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> experiments on this dataset, including zero-short and other <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> experiments on state-of-the-art cross-lingual transformers from existing data in <a href="https://en.wikipedia.org/wiki/Bengali_language">Bengali</a>, <a href="https://en.wikipedia.org/wiki/English_language">English</a>, and <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>.</abstract>
      <url hash="0805223a">2021.ranlp-1.50</url>
      <bibkey>gaikwad-etal-2021-cross</bibkey>
      <pwccode url="https://github.com/tharindudr/mold" additional="false">tharindudr/mold</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mold">MOLD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="53">
      <title>Syntax and Themes : How Context Free Grammar Rules and Semantic Word Association Influence Book Success</title>
      <author><first>Henry</first><last>Gorelick</last></author>
      <author><first>Biddut Sarker</first><last>Bijoy</last></author>
      <author><first>Syeda</first><last>Jannatus Saba</last></author>
      <author><first>Sudipta</first><last>Kar</last></author>
      <author><first>Md Saiful</first><last>Islam</last></author>
      <author><first>Mohammad Ruhul</first><last>Amin</last></author>
      <pages>463–474</pages>
      <abstract>In this paper, we attempt to improve upon the state-of-the-art in predicting a novel’s success by modeling the lexical semantic relationships of its contents. We created the largest <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> used in such a project containing <a href="https://en.wikipedia.org/wiki/Lexical_analysis">lexical data</a> from 17,962 books from Project Gutenberg. We utilized domain specific feature reduction techniques to implement the most accurate <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to date for predicting book success, with our best model achieving an average <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 94.0 %. By analyzing the model parameters, we extracted the successful semantic relationships from books of 12 different genres. We finally mapped those semantic relations to a set of <a href="https://en.wikipedia.org/wiki/Theme_(narrative)">themes</a>, as defined in Roget’s Thesaurus and discovered the <a href="https://en.wikipedia.org/wiki/Theme_(narrative)">themes</a> that successful books of a given genre prioritize. At the end of the paper, we further showed that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> demonstrate similar performance for book success prediction even when <a href="https://en.wikipedia.org/wiki/Goodreads">Goodreads rating</a> was used instead of download count to measure success.</abstract>
      <url hash="e7f9fc2e">2021.ranlp-1.53</url>
      <bibkey>gorelick-etal-2021-syntax</bibkey>
    </paper>
    <paper id="55">
      <title>Apples to Apples : A Systematic Evaluation of Topic Models</title>
      <author><first>Ismail</first><last>Harrando</last></author>
      <author><first>Pasquale</first><last>Lisena</last></author>
      <author><first>Raphael</first><last>Troncy</last></author>
      <pages>483–493</pages>
      <abstract>From statistical to neural models, a wide variety of topic modelling algorithms have been proposed in the literature. However, because of the diversity of datasets and metrics, there have not been many efforts to systematically compare their performance on the same <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> and under the same conditions. In this paper, we present a selection of 9 topic modelling techniques from the state of the art reflecting a diversity of approaches to the task, an overview of the different <a href="https://en.wikipedia.org/wiki/Performance_metric">metrics</a> used to compare their performance, and the challenges of conducting such a comparison. We empirically evaluate the performance of these models on different settings reflecting a variety of real-life conditions in terms of dataset size, number of topics, and distribution of topics, following identical preprocessing and evaluation processes. Using both metrics that rely on the intrinsic characteristics of the dataset (different coherence metrics), as well as external knowledge (word embeddings and ground-truth topic labels), our experiments reveal several shortcomings regarding the common practices in topic models evaluation.</abstract>
      <url hash="52778976">2021.ranlp-1.55</url>
      <bibkey>harrando-etal-2021-apples</bibkey>
      <pwccode url="https://github.com/d2klab/tomodapi" additional="false">d2klab/tomodapi</pwccode>
    </paper>
    <paper id="57">
      <title>Semi-Supervised and Unsupervised Sense Annotation via Translations</title>
      <author><first>Bradley</first><last>Hauer</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <author><first>Yixing</first><last>Luan</last></author>
      <author><first>Arnob</first><last>Mallik</last></author>
      <author><first>Lili</first><last>Mou</last></author>
      <pages>504–513</pages>
      <abstract>Acquisition of multilingual training data continues to be a challenge in word sense disambiguation (WSD). To address this problem, <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised approaches</a> have been proposed to automatically generate sense annotations for training supervised WSD systems. We present three new methods for creating sense-annotated corpora which leverage translations, parallel bitexts, lexical resources, as well as contextual and synset embeddings. Our <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised method</a> applies <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> to transfer existing sense annotations to other languages. Our two unsupervised methods refine sense annotations produced by a knowledge-based WSD system via lexical translations in a parallel corpus. We obtain state-of-the-art results on standard WSD benchmarks.</abstract>
      <url hash="1aff20b4">2021.ranlp-1.57</url>
      <bibkey>hauer-etal-2021-semi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="63">
      <title>Application of Deep Learning Methods to SNOMED CT Encoding of Clinical Texts : From Data Collection to Extreme Multi-Label Text-Based Classification<fixed-case>SNOMED</fixed-case> <fixed-case>CT</fixed-case> Encoding of Clinical Texts: From Data Collection to Extreme Multi-Label Text-Based Classification</title>
      <author><first>Anton</first><last>Hristov</last></author>
      <author><first>Aleksandar</first><last>Tahchiev</last></author>
      <author><first>Hristo</first><last>Papazov</last></author>
      <author><first>Nikola</first><last>Tulechki</last></author>
      <author><first>Todor</first><last>Primov</last></author>
      <author><first>Svetla</first><last>Boytcheva</last></author>
      <pages>557–565</pages>
      <abstract>Concept normalization of clinical texts to standard <a href="https://en.wikipedia.org/wiki/Medical_classification">medical classifications</a> and <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontologies</a> is a task with high importance for <a href="https://en.wikipedia.org/wiki/Medical_research">healthcare and medical research</a>. We attempt to solve this problem through automatic SNOMED CT encoding, where <a href="https://en.wikipedia.org/wiki/SNOMED_CT">SNOMED CT</a> is one of the most widely used and comprehensive clinical term ontologies. Applying basic Deep Learning models, however, leads to undesirable results due to the unbalanced nature of the data and the extreme number of classes. We propose a classification procedure that features a multiple-step workflow consisting of label clustering, multi-cluster classification, and clusters-to-labels mapping. For multi-cluster classification, BioBERT is fine-tuned over our custom dataset. The clusters-to-labels mapping is carried out by a one-vs-all classifier (SVC) applied to every single cluster. We also present the steps for automatic dataset generation of textual descriptions annotated with SNOMED CT codes based on <a href="https://en.wikipedia.org/wiki/Public_data">public data</a> and <a href="https://en.wikipedia.org/wiki/Linked_open_data">linked open data</a>. In order to cope with the problem that our dataset is highly unbalanced, some data augmentation methods are applied. The results from the conducted experiments show high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and reliability of our approach for prediction of SNOMED CT codes relevant to a clinical text.</abstract>
      <url hash="3d24b69f">2021.ranlp-1.63</url>
      <bibkey>hristov-etal-2021-application</bibkey>
    </paper>
    <paper id="65">
      <title>Transfer Learning for Czech Historical Named Entity Recognition<fixed-case>C</fixed-case>zech Historical Named Entity Recognition</title>
      <author><first>Helena</first><last>Hubková</last></author>
      <author><first>Pavel</first><last>Kral</last></author>
      <pages>576–582</pages>
      <abstract>Nowadays, named entity recognition (NER) achieved excellent results on the standard corpora. However, big issues are emerging with a need for an application in a specific domain, because it requires a suitable annotated corpus with adapted NE tag-set. This is particularly evident in the historical document processing field. The main goal of this paper consists of proposing and evaluation of several transfer learning methods to increase the score of the Czech historical NER. We study several information sources, and we use two <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural nets</a> for NE modeling and recognition. We employ two corpora for evaluation of our transfer learning methods, namely Czech named entity corpus and Czech historical named entity corpus. We show that BERT representation with <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> and only the simple <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> trained on the union of corpora achieves excellent results.</abstract>
      <url hash="2589d962">2021.ranlp-1.65</url>
      <bibkey>hubkova-kral-2021-transfer</bibkey>
    </paper>
    <paper id="72">
      <title>Domain-Specific Japanese ELECTRA Model Using a Small Corpus<fixed-case>J</fixed-case>apanese <fixed-case>ELECTRA</fixed-case> Model Using a Small Corpus</title>
      <author><first>Youki</first><last>Itoh</last></author>
      <author><first>Hiroyuki</first><last>Shinnou</last></author>
      <pages>640–646</pages>
      <abstract>Recently, domain shift, which affects <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> due to differences in data between source and target domains, has become a serious issue when using <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning methods</a> to solve <a href="https://en.wikipedia.org/wiki/Natural-language_processing">natural language processing tasks</a>. With additional pretraining and fine-tuning using a target domain corpus, pretraining models such as BERT (Bidirectional Encoder Representations from Transformers) can address this issue. However, the additional pretraining of the BERT model is difficult because it requires significant computing resources. The efficiently learning an encoder that classifies token replacements accurately (ELECTRA) pretraining model replaces the BERT pretraining method’s masked language modeling with a method called replaced token detection, which improves the computational efficiency and allows the additional pretraining of the model to a practical extent. Herein, we propose a method for addressing the computational efficiency of pretraining models in domain shift by constructing an ELECTRA pretraining model on a Japanese dataset and additional pretraining this model in a downstream task using a corpus from the target domain. We constructed a pretraining model for ELECTRA in <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a> and conducted experiments on a document classification task using data from <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese news articles</a>. Results show that even a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> smaller than the pretrained model performs equally well.</abstract>
      <url hash="028cd3c4">2021.ranlp-1.72</url>
      <bibkey>itoh-shinnou-2021-domain</bibkey>
    </paper>
    <paper id="75">
      <title>Behavior of Modern Pre-trained Language Models Using the Example of Probing Tasks</title>
      <author><first>Ekaterina</first><last>Kalyaeva</last></author>
      <author><first>Oleg</first><last>Durandin</last></author>
      <author><first>Alexey</first><last>Malafeev</last></author>
      <pages>664–670</pages>
      <abstract>Modern transformer-based language models are revolutionizing <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the model during training. Thus, the aim of this study is to examine behavior of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model BERT</a> in the task of masked language modelling and to provide linguistic interpretation to the unexpected effects and errors produced by the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>. For this purpose, we used a new Russian-language dataset based on educational texts for learners of <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a> and annotated with the help of the National Corpus of the Russian language. In terms of quality metrics (the proportion of words, semantically related to the target word), the multilingual BERT is recognized as the best model. Generally, each <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> has distinct strengths in relation to a certain <a href="https://en.wikipedia.org/wiki/Phenomenon">linguistic phenomenon</a>. These observations have meaningful implications for research into <a href="https://en.wikipedia.org/wiki/Applied_linguistics">applied linguistics</a> and <a href="https://en.wikipedia.org/wiki/Pedagogy">pedagogy</a>, contribute to dialogue system development, automatic exercise making, text generation and potentially could improve the quality of existing linguistic technologies</abstract>
      <url hash="c2ca0bf4">2021.ranlp-1.75</url>
      <bibkey>kalyaeva-etal-2021-behavior</bibkey>
    </paper>
    <paper id="77">
      <title>Application of Mix-Up Method in Document Classification Task Using BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Naoki</first><last>Kikuta</last></author>
      <author><first>Hiroyuki</first><last>Shinnou</last></author>
      <pages>679–683</pages>
      <abstract>The mix-up method (Zhang et al., 2017), one of the methods for <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>, is known to be easy to implement and highly effective. Although the mix-up method is intended for image identification, it can also be applied to <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. In this paper, we attempt to apply the mix-up method to a document classification task using bidirectional encoder representations from transformers (BERT) (Devlin et al., 2018). Since BERT allows for two-sentence input, we concatenated word sequences from two documents with different labels and used the multi-class output as the supervised data with a one-hot vector. In an experiment using the livedoor news corpus, which is <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a>, we compared the accuracy of <a href="https://en.wikipedia.org/wiki/Document_classification">document classification</a> using two methods for selecting documents to be concatenated with that of ordinary document classification. As a result, we found that the proposed method is better than the normal classification when the documents with labels shortages are mixed preferentially. This indicates that how to choose documents for mix-up has a significant impact on the results.</abstract>
      <url hash="2c00c21a">2021.ranlp-1.77</url>
      <bibkey>kikuta-shinnou-2021-application</bibkey>
    </paper>
    <paper id="82">
      <title>Neural Machine Translation for Sinhala-English Code-Mixed Text<fixed-case>S</fixed-case>inhala-<fixed-case>E</fixed-case>nglish Code-Mixed Text</title>
      <author><first>Archchana</first><last>Kugathasan</last></author>
      <author><first>Sagara</first><last>Sumathipala</last></author>
      <pages>718–726</pages>
      <abstract>Code-mixing has become a moving method of communication among multilingual speakers. Most of the <a href="https://en.wikipedia.org/wiki/Social_media">social media content</a> of the multilingual societies are written in code-mixed text. However, most of the current <a href="https://en.wikipedia.org/wiki/Translation">translation systems</a> neglect to convert <a href="https://en.wikipedia.org/wiki/Code-mixing">code-mixed texts</a> to a standard language. Most of the user written code-mixed content in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> remains unprocessed due to the unavailability of linguistic resource such as <a href="https://en.wikipedia.org/wiki/Parallel_corpus">parallel corpus</a>. This paper proposes a Neural Machine Translation(NMT) model to translate the Sinhala-English code-mixed text to the <a href="https://en.wikipedia.org/wiki/Sinhala_language">Sinhala language</a>. Due to the limited resources available for Sinhala-English code-mixed(SECM) text, a parallel corpus is created with SECM sentences and Sinhala sentences. Srilankan social media sites contain SECM texts more frequently than the standard languages. The model proposed for code-mixed text translation in this study is a combination of Encoder-Decoder framework with LSTM units and Teachers Forcing Algorithm. The translated sentences from the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> are evaluated using BLEU(Bilingual Evaluation Understudy) metric. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieved a remarkable BLEU score for the <a href="https://en.wikipedia.org/wiki/Translation">translation</a>.</abstract>
      <url hash="a211d1d9">2021.ranlp-1.82</url>
      <bibkey>kugathasan-sumathipala-2021-neural</bibkey>
    <title_fr>Traduction automatique neuronale pour texte mixte cinghalais-anglais</title_fr>
      <title_es>Traducción automática neuronal para texto mixto cingalés e inglés</title_es>
      <title_ar>الترجمة الآلية العصبية للنص السنهالية الإنجليزية كود مختلط</title_ar>
      <title_pt>Tradução automática neural para texto misto de código cingalês-inglês</title_pt>
      <title_hi>सिंहली-अंग्रेजी कोड-मिश्रित पाठ के लिए तंत्रिका मशीन अनुवाद</title_hi>
      <title_ja>シンハラ語-英語コード-混合テキスト用ニューラル機械翻訳</title_ja>
      <title_ru>Нейронный машинный перевод для сингальского-английского кодово-смешанного текста</title_ru>
      <title_zh>僧伽罗语-英语代码-混文神经机器翻译</title_zh>
      <title_ga>Aistriúchán Meaisín Néarach do Théacs Cód-Mheasctha Siolóinis-Béarla</title_ga>
      <title_hu>Neurális gépi fordítás a singala-angol kód-vegyes szöveghez</title_hu>
      <title_ka>Name</title_ka>
      <title_el>Νευρική μηχανική μετάφραση για μικτό κείμενο κινχαλο-αγγλικό κώδικα</title_el>
      <title_lt>Name</title_lt>
      <title_mk>Name</title_mk>
      <title_it>Traduzione automatica neurale per testo misto in codice singala-inglese</title_it>
      <title_kk>Синхал- ағылшын код- аралас мәтін үшін нейрондық машинаның аудармасыName</title_kk>
      <title_ml>Neural Machine Translation for Sinhala-English Code-Mixed Text</title_ml>
      <title_ms>Name</title_ms>
      <title_mn>Синхал-Англи код-холбогдсон текст дээр мэдрэлийн машины хөгжүүлэлт</title_mn>
      <title_no>Neuralmaskinsomsetjing for sinhalsk- engelsk- kodeflekst tekst</title_no>
      <title_mt>Traduzzjoni ta’ Makkinarju Newrali għal test imħallat bil-Kodiċi Sinhala-Ingliż</title_mt>
      <title_ro>Traducere automată neurală pentru text mixt de cod singhala-engleză</title_ro>
      <title_si>Name</title_si>
      <title_pl>Neuronalne tłumaczenie maszynowe dla tekstu mieszanego w języku chińsko-angielskim</title_pl>
      <title_so>Turjumista baabuurta Neural machine for Sinhala-Ingiriis Code-Mixed Text</title_so>
      <title_sv>Neural maskinöversättning för singala-engelska kodblandad text</title_sv>
      <title_sr>Neuralna mašina prevoda za sinhalski engleski kodirani tekst</title_sr>
      <title_ur>Sinhala-English Code-Mixed Text کے لئے نیورال ماشین ترجمہ</title_ur>
      <title_ta>Name</title_ta>
      <title_uz>Name</title_uz>
      <title_vi>Dịch về máy thần kinh cho đơn vị</title_vi>
      <title_bg>Невроден машинен превод за синхала-английски кодов смесен текст</title_bg>
      <title_nl>Neurale machinevertaling voor Sinhalas-Engelse code-gemengde tekst</title_nl>
      <title_da>Neural maskinoversættelse for singala-engelsk kode-blandet tekst</title_da>
      <title_hr>Neuralno prevodenje stroja za sinhalski engleski kodirani tekst</title_hr>
      <title_de>Neuronale maschinelle Übersetzung für Singhala-Englisch Code-Mixed Text</title_de>
      <title_ko>스님 가로어 - 영어 코드 혼합 텍스트의 신경 번역</title_ko>
      <title_id>Translation Machine Neural for Sinhala-English Code-Mixed Text</title_id>
      <title_fa>ترجمه ماشین عصبی برای متن پیوند پیوند‌پیوند سینهال- انگلیسی</title_fa>
      <title_sw>Tafsiri ya Mashine ya Kijerumani kwa ajili ya Maandishi ya Miungano ya Kiingereza</title_sw>
      <title_tr>Sinhala-Iňlisçe Kod-Karışma Metin üçin Nural Maşynyň terjime edilmesi</title_tr>
      <title_af>Nural Masjien Vertaling vir Sinhala- Engels Kode- gemengde Teks</title_af>
      <title_sq>Teksti i përzier me kod sinhala-anglez</title_sq>
      <title_am>የነዌብ መኪን ትርጉም</title_am>
      <title_hy>Սինհալա-անգլերեն կոդի խառնված տեքստի նյարդային մեքենայի թարգմանություն</title_hy>
      <title_bn>সিঙ্গাল- ইংরেজী কোড- মিক্সেড টেক্সটের জন্য নিউরাল মেশিন অনুবাদ</title_bn>
      <title_bs>Neuralna mašina prevoda za sinhala-engleski kodirani tekst</title_bs>
      <title_az>Sinhal-İngilizə Kodu-Karışmış Metin üçün nöral Makine Çeviri</title_az>
      <title_ca>Traducció de màquines neuronals per text mixt de codi sinhala-anglès</title_ca>
      <title_et>Neuraalne masintõlge sinhala-inglise koodsega teksti jaoks</title_et>
      <title_cs>Neurální strojový překlad pro čínština-anglický kód smíšený text</title_cs>
      <title_fi>Neuraalinen konekäännös sinhala-englanti koodisekoitettu teksti</title_fi>
      <title_sk>Nevralni strojni prevod za sinhalo-angleško kodno mešano besedilo</title_sk>
      <title_he>Neural Machine Translation for Sinhala-English Code-Mixed Text</title_he>
      <title_ha>@ item Text character set</title_ha>
      <title_jv>Nyural Masukan Terjamahan kanggo kode-Nyunalan kang Teks Mi-Mixed</title_jv>
      <title_bo>སིན་ཧྲིལ་ཡིག་གི་སྤྱོད་པའི་མ་ལག་འཁྱེར་གྱི་ཚིག་ཡིག་གཟུགས་རྟགས་བཀལ་བ</title_bo>
      <abstract_ar>أصبح خلط الشفرات وسيلة مؤثرة للتواصل بين المتحدثين متعددي اللغات. تتم كتابة معظم محتوى الوسائط الاجتماعية للمجتمعات متعددة اللغات بنص مختلط بأكواد. ومع ذلك ، فإن معظم أنظمة الترجمة الحالية تتجاهل تحويل النصوص المختلطة بالشفرات إلى لغة قياسية. لا يزال معظم المحتوى المكتوب المختلط بالشفرات في وسائل التواصل الاجتماعي غير معالج بسبب عدم توفر الموارد اللغوية مثل المجموعة المتوازية. تقترح هذه الورقة نموذجًا للترجمة الآلية العصبية (NMT) لترجمة النص المختلط بالشفرة السنهالية الإنجليزية إلى اللغة السنهالية. نظرًا لمحدودية الموارد المتاحة لنص مختلط الأكواد (SECM) باللغة السنهالية الإنجليزية ، يتم إنشاء مجموعة موازية مع جمل SECM وجمل السنهالية. تحتوي مواقع الوسائط الاجتماعية السريلانكية على نصوص SECM بشكل متكرر أكثر من اللغات القياسية. النموذج المقترح لترجمة النص المختلط في هذه الدراسة هو مزيج من إطار عمل التشفير-فك التشفير مع وحدات LSTM وخوارزمية فرض المعلمين. يتم تقييم الجمل المترجمة من النموذج باستخدام مقياس BLEU (BLEU). حقق نموذجنا درجة BLEU ملحوظة في الترجمة.</abstract_ar>
      <abstract_fr>Le mixage de codes est devenu une méthode de communication émouvante pour les locuteurs multilingues. La plupart des contenus des réseaux sociaux des sociétés multilingues sont rédigés dans un texte mixte de code. Cependant, la plupart des systèmes de traduction actuels négligent de convertir les textes mixtes de code en une langue standard. La plupart du contenu mixte de code écrit par l'utilisateur sur les réseaux sociaux n'est pas traité en raison de l'indisponibilité de ressources linguistiques telles que des corpus parallèles. Cet article propose un modèle de traduction automatique neuronale (NMT) pour traduire le texte mixte cinghalais-anglais en langue cinghalaise. En raison des ressources limitées disponibles pour le texte mixte cinghalais-anglais (SECM), un corpus parallèle est créé avec des phrases SECM et des phrases en cinghalais. Les sites de réseaux sociaux srilankais contiennent des textes SECM plus fréquemment que les langues standard. Le modèle proposé pour la traduction de textes mixtes en code dans cette étude est une combinaison d'un framework Encoder-Decoder avec des unités LSTM et un algorithme de forçage des enseignants. Les phrases traduites du modèle sont évaluées à l'aide de la métrique BLEU (Bilingual Evaluation Understudy). Notre modèle a obtenu un score BLEU remarquable pour la traduction.</abstract_fr>
      <abstract_es>La mezcla de códigos se ha convertido en un método de comunicación en movimiento entre los hablantes multilingües. La mayor parte del contenido de las redes sociales de las sociedades multilingües está escrito en texto con código mixto. Sin embargo, la mayoría de los sistemas de traducción actuales no convierten textos con código mixto a un idioma estándar. La mayor parte del contenido de código mezclado escrito por el usuario en las redes sociales permanece sin procesar debido a la falta de disponibilidad de recursos lingüísticos, como el corpus paralelo. Este artículo propone un modelo de traducción automática neuronal (NMT) para traducir el texto con código mixto cingalés e inglés al idioma cingalés. Debido a los limitados recursos disponibles para el texto mixto cingalés e inglés (SECM), se crea un corpus paralelo con sentencias SECM y oraciones en cingalés. Los sitios de medios sociales de Sri Lanka contienen textos SECM con más frecuencia que los idiomas estándar. El modelo propuesto para la traducción de texto con código mixto en este estudio es una combinación del marco Encoder-Decoder con unidades LSTM y el Algoritmo Forcing de Maestros. Las frases traducidas del modelo se evalúan mediante la métrica BLEU (estudio sustituto de evaluación bilingüe). Nuestro modelo obtuvo una notable puntuación BLEU para la traducción.</abstract_es>
      <abstract_pt>A mistura de códigos tornou-se um método de comunicação em movimento entre falantes multilíngues. A maior parte do conteúdo de mídia social das sociedades multilíngues é escrita em texto misto de código. No entanto, a maioria dos sistemas de tradução atuais negligencia a conversão de textos de código misto para um idioma padrão. A maior parte do conteúdo de código misto escrito pelo usuário nas mídias sociais permanece não processado devido à indisponibilidade de recursos linguísticos, como corpus paralelo. Este artigo propõe um modelo de Neural Machine Translation (NMT) para traduzir o texto misto de código cingalês-inglês para o idioma cingalês. Devido aos recursos limitados disponíveis para texto cingalês-inglês code-mixed (SECM), um corpus paralelo é criado com sentenças SECM e sentenças Sinhala. Os sites de mídia social do Sri Lanka contêm textos SECM com mais frequência do que os idiomas padrão. O modelo proposto para tradução de texto misto de código neste estudo é uma combinação do framework Encoder-Decoder com unidades LSTM e Algoritmo de Força de Professores. As sentenças traduzidas do modelo são avaliadas usando a métrica BLEU (Bilingual Evaluation Understudy). Nosso modelo alcançou uma notável pontuação BLEU para a tradução.</abstract_pt>
      <abstract_ja>コードミキシングは、多言語話者間のコミュニケーションの移動方法となっている。 多言語社会のソーシャルメディアコンテンツのほとんどは、コードが混在したテキストで書かれています。 しかし、現在の翻訳システムのほとんどは、コードが混在したテキストを標準言語に変換することを無視しています。 並行コーパスなどの言語リソースが利用できないため、ソーシャルメディアで書かれたユーザーのコードミックスコンテンツのほとんどは未処理のままです。 本論文では、シンハラ語-英語のコードミックステキストをシンハラ語に翻訳するためのニューラル・マシン・トランスレーション（ NMT ）モデルを提案している。 Sinhala - English code - mix (SECM)テキストに利用できるリソースが限られているため、SECM文とSinhala文で並列コーパスが作成されます。 Srilankanソーシャルメディアサイトには、標準言語よりも頻繁にSECMテキストが含まれています。 この研究でコード混合テキスト翻訳のために提案されたモデルは、LSTMユニットとTeachers Forcing Algorithmとのエンコーダデコーダフレームワークの組み合わせである。 モデルからの翻訳された文章は、BLEU （ Bilingual Evaluation Understudy ）メトリックを使用して評価されます。 私たちのモデルは、翻訳で驚くべきBLEUスコアを達成しました。</abstract_ja>
      <abstract_ru>Перемешивание кодов стало подвижным методом общения между многоязычными носителями. Большая часть контента в социальных сетях многоязычных обществ написана смешанным текстом. Однако большинство нынешних систем перевода не преобразуют смешанные тексты в стандартный язык. Большая часть пользовательского контента со смешанным кодом в социальных сетях остается необработанным из-за отсутствия лингвистического ресурса, такого как параллельный корпус. В этой статье предлагается модель нейронного машинного перевода(НМП) для перевода сингальско-английского текста со смешанным кодом на сингальский язык. Из-за ограниченных ресурсов, доступных для сингальско-английского текста со смешанным кодом (SECM), создается параллельный корпус с предложениями SECM и сингальскими предложениями. На сайтах социальных сетей Шриланки тексты SECM встречаются чаще, чем на стандартных языках. Модель, предложенная для смешанного перевода текста в этом исследовании, представляет собой комбинацию фреймворка кодировщика-декодера с блоками LSTM и алгоритмом форсировки учителей. Переведенные предложения из модели оцениваются с помощью метрики BLEU(Bilingual Evaluation Understudy). Наша модель достигла замечательного балла BLEU за перевод.</abstract_ru>
      <abstract_hi>कोड-मिश्रण बहुभाषी वक्ताओं के बीच संचार का एक चलती विधि बन गया है। बहुभाषी समाजों की अधिकांश सोशल मीडिया सामग्री कोड-मिश्रित पाठ में लिखी गई है। हालांकि, अधिकांश वर्तमान अनुवाद प्रणालियां कोड-मिश्रित ग्रंथों को एक मानक भाषा में परिवर्तित करने की उपेक्षा करती हैं। सोशल मीडिया में अधिकांश उपयोगकर्ता लिखित कोड-मिश्रित सामग्री भाषाई संसाधन जैसे समानांतर कॉर्पस की अनुपलब्धता के कारण असंसाधित रहती है। यह पेपर सिंहली भाषा में सिंहली-अंग्रेजी कोड-मिश्रित पाठ का अनुवाद करने के लिए एक तंत्रिका मशीन अनुवाद (एनएमटी) मॉडल का प्रस्ताव करता है। सिंहली-अंग्रेजी कोड-मिश्रित (एसईसीएम) पाठ के लिए उपलब्ध सीमित संसाधनों के कारण, एसईसीएम वाक्यों और सिंहल वाक्यों के साथ एक समानांतर कॉर्पस बनाया गया है। श्रीलंकाई सोशल मीडिया साइटों में मानक भाषाओं की तुलना में अधिक बार एसईसीएम ग्रंथ होते हैं। इस अध्ययन में कोड-मिश्रित पाठ अनुवाद के लिए प्रस्तावित मॉडल एलएसटीएम इकाइयों और शिक्षकों को मजबूर करने वाले एल्गोरिथ्म के साथ एनकोडर-डिकोडर फ्रेमवर्क का एक संयोजन है। मॉडल से अनुवादित वाक्यों का मूल्यांकन BLEU (द्विभाषी मूल्यांकन Understudy) मीट्रिक का उपयोग करके किया जाता है। हमारे मॉडल ने अनुवाद के लिए एक उल्लेखनीय BLEU स्कोर हासिल किया।</abstract_hi>
      <abstract_zh>代码混为多语种言语之间一移之流也。 多言协会者社交媒体皆以代码杂文本为之。 然当时多译统忽将代码混合文本转换为准的言语。 语言资源(如并行语料库)者不可用,社交媒体中多用户所撰代码混合未决。 本文立神经机器翻译(NMT)模,合僧伽罗语-英语代码本翻译成僧伽罗语。 以僧伽罗语-英语代码混 (SECM) 文本有限,故用 SECM 句与僧伽罗语句创一并行语料库。 斯里兰卡社交媒体网站频含SECM本。 本研代码混合文本转换模型编码器-解码器框架与LSTM单元师强算法之组。 用BLEU(双语评)指标对模型译句评之。 形于译者著BLEU分。</abstract_zh>
      <abstract_ga>Is modh cumarsáide soghluaiste anois é códmheascadh i measc cainteoirí ilteangacha. Tá an chuid is mó d’ábhar meán sóisialta na gcumann ilteangach scríofa i dtéacs cód-mheasctha. Mar sin féin, déanann an chuid is mó de na córais aistriúcháin atá ann faoi láthair faillí téacsanna cód-mheasctha a thiontú go teanga chaighdeánach. Tá an chuid is mó den ábhar códmheasctha scríofa ag úsáideoirí ar na meáin shóisialta fós gan phróiseáil mar gheall ar easpa acmhainne teanga ar nós corpas comhthreomhar. Molann an páipéar seo múnla Neural Machine Translation (NMT) chun an téacs códmheasctha Siolóinis-Béarla a aistriú go dtí an Siolóinis. De bharr na n-acmhainní teoranta atá ar fáil do théacs cód-mheasctha Siolóinis-Béarla (SECM), cruthaítear corpas comhthreomhar le habairtí SECM agus abairtí Siolóine. Bíonn téacsanna SECM níos minice i suíomhanna meán sóisialta Srilankan ná sna teangacha caighdeánacha. Is é atá sa mhúnla atá molta d’aistriúchán téacs cód-mheasctha sa staidéar seo ná meascán de chreat Ionchódóra-Díchódóra le haonaid LSTM agus Algartam um Fhoréigean Múinteoirí. Déantar na habairtí aistrithe ón tsamhail a mheas trí úsáid a bhaint as méadrach BLEU(Bilingual Evaluation Understudy). Bhain ár múnla scór iontach BLEU amach don aistriúchán.</abstract_ga>
      <abstract_hu>A kódkeverés a többnyelvű beszélők közötti kommunikáció mozgó módszerévé vált. A többnyelvű társaságok közösségi média tartalmának nagy része kódkevert szövegben van írva. A jelenlegi fordítási rendszerek többsége azonban elhanyagolja a kódkeverékes szövegek standard nyelvre történő átalakítását. A közösségi médiában a felhasználók által írt kódkevert tartalom többsége feldolgozatlan marad, mivel nem érhető el nyelvi erőforrás, például a párhuzamos korpusz. A tanulmány egy Neural Machine Translation (NMT) modellt javasol a szinhala-angol kódkevert szöveg lefordítására a szinhala nyelvre. Mivel korlátozott erőforrások állnak rendelkezésre a szinhala-angol code-mixed(SECM) szöveghez, egy párhuzamos korpusz jön létre SECM mondatokkal és szinhala mondatokkal. A srilankani közösségi oldalak gyakrabban tartalmaznak SECM szövegeket, mint a normál nyelvek. A jelen tanulmányban a kódkevert szövegfordításra javasolt modell az Encoder-Decoder keretrendszer LSTM egységekkel és a Teachers Forcing Algorithm kombinációja. A modell lefordított mondatait BLEU (kétnyelvű értékelés alátámasztása) metrika segítségével értékeljük. Modellünk figyelemre méltó BLEU pontszámot ért el a fordításhoz.</abstract_hu>
      <abstract_el>Η ανάμειξη κώδικα έχει γίνει μια κινούμενη μέθοδος επικοινωνίας μεταξύ πολύγλωσσων ομιλητών. Το μεγαλύτερο μέρος του περιεχομένου των μέσων κοινωνικής δικτύωσης των πολύγλωσσων κοινωνιών είναι γραμμένο σε μικτό κείμενο κώδικα. Ωστόσο, τα περισσότερα από τα σημερινά μεταφραστικά συστήματα παραμελούν να μετατρέψουν κείμενα μικτού κώδικα σε μια τυποποιημένη γλώσσα. Το μεγαλύτερο μέρος του γραπτού περιεχομένου των χρηστών στα μέσα κοινωνικής δικτύωσης παραμένει ανεπεξέργαστο λόγω της μη διαθεσιμότητας γλωσσικού πόρου όπως το παράλληλο σώμα. Η παρούσα εργασία προτείνει ένα μοντέλο Νευρικής Μηχανικής Μετάφρασης (NMT) για τη μετάφραση του Σινχάλα-Αγγλικού κώδικα-μικτού κειμένου στη Σινχάλα γλώσσα. Λόγω των περιορισμένων διαθέσιμων πόρων για κείμενο Σινχάλα-Αγγλικού κώδικα-μικτού (SECM), δημιουργείται ένα παράλληλο σώμα με προτάσεις και προτάσεις Σινχάλα. Οι ιστοσελίδες κοινωνικής δικτύωσης του Σριμάνκαν περιέχουν κείμενα του SECM πιο συχνά από τις τυπικές γλώσσες. Το μοντέλο που προτείνεται για τη μετάφραση κώδικα-μικτού κειμένου σε αυτή τη μελέτη είναι ένας συνδυασμός πλαισίου κωδικοποιητή-αποκωδικοποιητή με μονάδες και αλγόριθμο επιβολής δασκάλων. Οι μεταφρασμένες προτάσεις από το μοντέλο αξιολογούνται χρησιμοποιώντας τη μετρική. Το μοντέλο μας πέτυχε αξιοσημείωτη βαθμολογία για τη μετάφραση.</abstract_el>
      <abstract_it>Il code-mixing è diventato un metodo di comunicazione commovente tra i parlanti multilingue. La maggior parte dei contenuti dei social media delle società multilingue sono scritti in testo codificato. Tuttavia, la maggior parte degli attuali sistemi di traduzione trascura di convertire testi misti in codice in una lingua standard. La maggior parte dei contenuti codificati scritti dall'utente nei social media rimane non elaborati a causa dell'indisponibilità di risorse linguistiche come il corpus parallelo. Questo articolo propone un modello Neural Machine Translation (NMT) per tradurre il testo in codice singhala-inglese nella lingua singhala. A causa delle limitate risorse disponibili per il testo singhala-inglese code-mixed (SECM), viene creato un corpus parallelo con frasi SECM e frasi singhala. I siti di social media Srilankan contengono testi SECM più frequentemente rispetto alle lingue standard. Il modello proposto per la traduzione di testo misto in questo studio è una combinazione di Encoder-Decoder framework con unità LSTM e Teachers Forcing Algorithm. Le frasi tradotte dal modello sono valutate utilizzando la metrica BLEU (Bilingual Evaluation Understudy). Il nostro modello ha ottenuto un notevole punteggio BLEU per la traduzione.</abstract_it>
      <abstract_kk>Кодты араластыру көптілік сөйлейтіншілер арасында байланыстыру әдісі болды. Көп тілдік қоғамдарының социаллық медиа мазмұнының көбі код аралас мәтінде жазылады. Бірақ, назардағы аудару жүйелерінің көпшілігі код араластырылған мәтіндерді стандартты тіліне аударуға болмайды. Пайдаланушылардың көпшілігі жазылған код араластырылған мазмұның көпшілігі әлемдік медиақтағы лингвистикалық ресурстардың параллель корпус сияқты мүмкіндігі болмайтын Бұл қағаз синхалы- ағылшын код араластырылған мәтінді синхалық тіліне аудару үшін нейралы машинаны аудару (NMT) үлгісін ұсынады. Синхал- ағылшын код араластырылған (SECM) мәтіннің шектелген ресурстарының себебі, параллелі корпус SECM сөздермен мен синхалық сөздермен құрылады. Шрилан социаллық медиа сайтында стандартты тілдерден көп SECM мәтіндері бар. Бұл зерттеулерде код араластырылған мәтін аудармасының үлгісі - LSTM бірліктері мен Мұғалімдері Алгоритмді бағыттауға арналған Encoder- декодер фреймінің біріктірімі. Үлгіден аударылған сөздерді BLEU (Екінші оқу астындағы оқу) метрикалық түрде тең алады. Біздің моделіміз аудармалардың белгілі BLEU нәтижесін жетті.</abstract_kk>
      <abstract_ka>მრავალენგური საუკეთესებელი კომუნიკაციის გადატანა. მრავალენგური საზოგადოებების სოციალური მედიაში მნიშვნელოვანი საზოგადოებების უფრო მეტი კოდის შესაბამისი ტექსტში დაწერა. მაგრამ მიმდინარე შეცვლის სისტემის უფრო მეტი არ უნდა კოდის შეცვლის ტექსტის სტანდარტული ენაში გადაცვლა. სოციალური მედიაში ყველაფერი მომხმარებელი კოდის შემთხვევაში მუშაობაში არ დაუწყებულია, რადგან ლენგურისტიკური რესურსების შესაძლებლობად, როგორც პარალელი კო ეს დოკუმენტი უნეიროლური მაქსინის შეცვლა (NMT) მოდელს სინჰალური ანგლისური კოდის შეცვლა სინჰალური ენაში. სინჰალური ანგლისური ტექსტისთვის შესაძლებელი რესურსების გამოსახულება (SECM) ტექსტისთვის, პარალელი კორპუსი SECM სიტყვებით და სინჰალური სიტყვებით შექმნა. შრილანკანის სოციალური მედია საზოგადოებში SECM ტექსტი უფრო ფარჯიშია, ვიდრე სტანდარული ენების. ამ სწავლაში კოდის შემდეგ ტექსტის გასაგულისხმებისთვის მოდელი არის LSTM ერთეულებით და სწავლებელი ალგორიტიმს შემდეგ Encoder-Decoder ფრამბემის კომბიზაცია. მეტრიკის გამოყენებული მოდელიდან გამოყენებული სიტყვები გამოყენებულია BLEU(მეტრიკური განსაზღვრება) მეტრიკის გამოყენებით. ჩვენი მოდელის შესაძლებელი BLEU წერტილის შესაძლებლობისთვის მიღება.</abstract_ka>
      <abstract_ms>Pengcampuran kod telah menjadi kaedah komunikasi bergerak di antara pembicara berbilang bahasa. Kebanyakan kandungan media sosial masyarakat berbilang bahasa ditulis dalam teks campuran kod. Namun, kebanyakan sistem terjemahan semasa mengabaikan untuk menukar teks campuran-kod ke bahasa piawai. Kebanyakan kandungan kod-campuran ditulis pengguna dalam media sosial masih belum diproses kerana tidak tersedia sumber bahasa seperti corpus selari. Kertas ini melaporkan model Terjemahan Mesin Neural (NMT) untuk menerjemahkan teks kod-campuran Sinhala-Inggeris ke bahasa Sinhala. Kerana sumber terbatas yang tersedia untuk teks kod-campuran (SECM) Bahasa Inggeris-Sinhala, korpus selari dicipta dengan kalimat SECM dan kalimat Sinhala. laman media sosial Srilankan mengandungi teks SECM lebih sering daripada bahasa piawai. Model yang diusulkan untuk terjemahan teks campuran-kod dalam kajian ini adalah kombinasi kerangka Pengekod-Pengekod dengan unit LSTM dan Algoritma Memaksa Guru. kalimat terjemahan dari model diteliti menggunakan metrik BLEU( Understudy Evaluation Bilingual). Model kami mencapai skor BLEU yang luar biasa untuk terjemahan.</abstract_ms>
      <abstract_ml>കോഡ് മിക്കിങ്ങ് പല ഭാഷയിലെ സംസാരിക്കുന്നവരുടെ ഇടയില്‍ നീങ്ങുന്ന ഒരു രീതിയായിരിക്കുന്നു. പല ഭാഷകങ്ങളുടെ സമൂഹത്തിന്റെ സാമൂഹ്യ മീഡിയയുടെ ഉള്ളടക്കം കോഡ് മിഷ്ടപ്പെട്ട വാചകത്തില്‍ എഴുതിയി എന്നാലും നിലവിലുള്ള പരിഭാഷപ്രവർത്തനങ്ങളിലേറെപ്പേരും ഒരു സാധാരണ ഭാഷയിലേക്ക് കലര്‍ത്തുന്നതിനെ അവഗണിക് സാമൂഹ്യ മാധ്യമങ്ങളില്‍ എഴുതിയ കോഡ് കലര്‍ത്തിയിരിക്കുന്ന ഉപയോക്താവിന്റെ അധികപേരും പാരാണല്‍ കോര്‍പ്പുസ് പോലുള്ള ഭാഷ ഈ പത്രത്തില്‍ സിങ്ഹാല- ഇംഗ്ലീഷ് കോഡ്- mixed text സിനാലാ ഭാഷയിലേക്ക് ഒരു നെയുറല്‍ മെഷീന്‍ പരിഭാഷ പ്രദര്‍ശിപ്പിക്കുന്നു. സിങ്ഹാല- ഇംഗ്ലീഷ് കോഡ് മിഷ്ടപ്പെട്ട വാക്കുകള്‍ സെസിഎം ടെക്സ്റ്റ് ചെയ്യുന്നതില്‍ സ്രിലാങ്കാന്‍ സോഷ്യല്‍ മീഡിയയുടെ സൈറ്റുകള്‍ സ്ഥാപിക്കുന്നു.  ഈ പഠനത്തില്‍ കോഡ്- mixed text translation-ന്റെ പ്രാദേശിപ്പിക്കുന്ന മോഡല്‍ എംഎസ്റ്റിം യൂണിറ്റുകളും ടീച്ചര്‍സ്റ്റര്‍ ബദ്ധപ്പെടുത്തുന്ന മോഡലില്‍ നിന്നുള്ള വാക്കുകള്‍ BLEU( ബിലിങ്കുള്ള വിവരങ്ങള്‍ അന്റ്റോള്‍ട്രിക്ക്) മെട്രിക്ക് ഉപയോഗിച്ച് വിലയ നമ്മുടെ മോഡല്‍ പരിഭാഷപ്പെടുത്തുന്നതിന് വേണ്ടി ഒരു മഹത്തായ ബില്ലു യു സ്കോര്‍ നേടി</abstract_ml>
      <abstract_mk>Мешањето на кодовите стана движечки метод на комуникација меѓу мултијазичните говорници. Повеќето социјални медиуми во мултијазичните општества се напишани во мешан текст. However, most of the current translation systems neglect to convert code-mixed texts to a standard language.  Повеќето од корисниците пишуваат мешана содржина во социјалните медиуми и понатаму не се процесирани поради недостигноста на јазички ресурси како што е паралелниот корпус. Овој документ предлага модел на неурална машина за преведување на синхалско- англискиот код- мешан текст на синхалскиот јазик. Поради ограничените ресурси достапни за текстот синхалско-англиски код- мешан (SECM), паралелен корпус се создава со реченици SECM и синхалски реченици. Сејтовите на сриланските социјални медиуми содржат тексти на SECM почесто од стандардните јазици. Моделот предложен за превод на текст со мешан код во оваа студија е комбинација на рамка на кодер-декодер со LSTM единици и Алгоритм на принудување на учителите. Преведените реченици од моделот се проценуваат користејќи метрика BLEU( Дивјазична подстудија за евалуација). Нашиот модел постигна извонреден БЛЕУ резултат за преводот.</abstract_mk>
      <abstract_lt>Kodų derinimas tapo judančiu daugiakalbių kalbėtojų ryšio metodu. Dauguma daugiakalbių visuomenės socialinių žiniasklaidų turinio įrašoma mišriame tekste. Vis dėlto dauguma dabartinių vertimo sistemų neatsižvelgia į kodų mišrių tekstų konvertavimą į standartinę kalbą. Dauguma naudotojų rašomo kodų mišraus turinio social in ėje žiniasklaidoje lieka neapdorotas dėl kalbinių išteklių, pavyzdžiui, paralelinio korpuso, nepakankamumo. Šiame dokumente siūlomas neurologinio mašinų vertimo (NMT) modelis, skirtas sinhališkai ir anglų kalbos kodų mišriam tekstui išversti į sinhališką kalbą. Atsižvelgiant į ribotus išteklius, skirtus tekstui „Sinhala-English code-mixed“ (SECM), kuriamas lygiagretus korpus su „SECM“ ir „Sinhala“ sakiniais. Srilanko socialinės žiniasklaidos svetainėse SECM tekstai dažniau nei standartinės kalbos. Šiame tyrime siūlomas kodų mišraus teksto vertimo model is yra kodų dekoderių sistemos derinys su LSTM vienetais ir mokytojų priverstiniu algoritmu. Iš modelio išversti sakiniai vertinami naudojant BLEU (dvikalbio vertinimo tyrimo) metriką. Our model achieved a remarkable BLEU score for the translation.</abstract_lt>
      <abstract_mn>Код холбоотой нь олон хэлний ярьдаг хүмүүсийн холбоотой хөдөлгөөн арга болсон. Олон хэл нийгмийн нийгмийн нийгмийн ихэнх мэдээлэл нь кодын төвөгтэй текст бичигддэг. Гэвч одоогийн орчин үеийн орчуулах системүүдийн ихэнх нь код холбогдсон текстүүдийг стандарт хэл рүү шилжүүлэхийг хүсэхгүй. Хэрэглэгчдийн ихэнх нь нийгмийн мэдээллийн хэрэглэгчид хэлний боловсрол зэрэгцээ корпус зэрэгцээ хэлний боловсрол байхгүй учраас хамгаалахгүй байна. Энэ цаас Синхал-Англи хэлний код холбогдсон текстийг синхал хэл руу орлуулахын тулд мэдрэлийн машин хөгжүүлэх загварыг санал болгодог. Синхал-Англи хэлний код-холбогдсон (SECM) текстүүдэд хязгаарлагдсан нөөц болохоор параллел корпус SECM өгүүлбэртэй болон синхал өгүүлбэртэй үүсгэдэг. Шриланчуудын нийгмийн хэвлэлийн сайтууд стандарт хэлээс илүү ихэвчлэн SECM текст агуулдаг. Энэ судалгаанд код холбогдсон текст орчуулалтын загвар нь LSTM нэгжүүд болон багш нар Алгоритмыг хүргэж байгаа Encoder-Decoder хэлбэртэй холбогдолтой юм. Загвараас орчуулсан өгүүлбэрүүдийг BLEU(Хоёрдугаар оюутнуудын үнэлгээ) метрик ашиглан дүгнэж байна. Бидний загвар нь хөрөнгө оруулахад гайхалтай БЛЕУ оноо олсон.</abstract_mn>
      <abstract_pl>Miksowanie kodu stało się poruszającą metodą komunikacji wśród wielojęzycznych mówców. Większość treści mediów społecznościowych wielojęzycznych społeczeństw jest napisana tekstem mieszanym z kodem. Jednak większość obecnych systemów tłumaczeń zaniedbuje konwersję tekstów mieszanych kodem na standardowy język. Większość treści pisanych przez użytkownika w mediach społecznościowych pozostaje nieprzetworzona ze względu na brak dostępności zasobów językowych, takich jak równoległy korpus. Niniejszy artykuł proponuje model neuronowego tłumaczenia maszynowego (NMT) do tłumaczenia tekstu mieszanego kodem chińsko-angielskim na język chiński. Ze względu na ograniczone zasoby dostępne dla tekstu chińsko-angielskiego kodu-mieszanego (SECM), tworzony jest równoległy korpus ze zdaniami SECM i zdaniami chińskimi. Serwisy społecznościowe w Srilankanie zawierają teksty SECM częściej niż standardowe języki. Model zaproponowany do tłumaczenia tekstu mieszanego kodem w niniejszym opracowaniu jest połączeniem frameworku Encoder-Decoder z jednostkami LSTM oraz algorytmem wymuszania nauczycieli. Przetłumaczone zdania z modelu są oceniane za pomocą metryki BLEU (Bilingual Evaluation Understudy). Nasz model osiągnął niezwykły wynik BLEU dla tłumaczenia.</abstract_pl>
      <abstract_no>Name Dei fleste sosiale media-innhaldet i fleirspråkssamanhengane er skriven i tekst med kodefeks. Dei fleste av dei gjeldande omsetjingssystemene vil likevel konvertera tekst med kode til eit standardspråk. Dei fleste av brukarane som er skriven med kode-blandet innhald i sosiale medier er ikkje prosessert på grunn av ikkje tilgjengeleg språksressurs som parallelle korpus. Denne papiret foreslår eit neuralmaskinsomsetjingsmodul (NMT) for å oversette teksten med kode mellom sinhalsk- engelsk til sinhalsk språk. På grunn av dei grenserte ressursane som er tilgjengelege for teksten med sinhalsk- engelsk kode- blandet (SECM), blir eit parallell korpus oppretta med SECM- setningar og sinhala- setningar. Srilankan-sosiale medianettstader inneheld SECM-tekstar oftast enn standardsspråka. Modellen foreslått for omsetjing av tekst mellom koden i denne studien er ein kombinasjon av koder- dekoderrammeverket med LSTM- einingar og lærarar som tvingar algoritme. Omsette setningane frå modellen blir evaluert med BLEU- metrisk. Modellen vårt oppnådd eit merkelig BLEU-poeng for omsetjinga.</abstract_no>
      <abstract_sr>Mešanje kodova postala je pokretna metoda komunikacije među višejezičkim govornicima. Većina društvenih medija multijezičkih društva napisana je u tekstu mešanom kodu. Međutim, većina trenutnih sustava prevođenja zaboravlja preobraćati tekste pomešane kod na standardni jezik. Većina korisnika napisanog kodiranog sadržaja u društvenim medijima ostaje neprocesovana zbog nepostupnosti jezičkih resursa poput paralelnog korpusa. Ovaj papir predlaže model neurološkog prevoda (NMT) za prevod teksta pomešanog koda sa sinhalama na sinhalački jezik. Zbog ograničenih resursa koje su dostupne za tekst sinhalanskog-engleskog koda pomešana (SECM), paralelni korpus se stvori sa rečenicama SECM-a i kaznama sinhalaca. Sriljanski društveni mediji sadrže tekstove SECM češće od standardnih jezika. Model predložen za prevod teksta u ovom ispitivanju je kombinacija okvira kodera-dekodera sa LSTM jedinicama i nastavnicima koji prisiljavaju algoritam. Prevedene rečenice iz modela procjenjuju se koristeći metričku BLEU (podstudiranje za dvostruku ocjenu). Naš model je postigao izvanredan rezultat BLEU za prevod.</abstract_sr>
      <abstract_ro>Mixarea codurilor a devenit o metodă mișcătoare de comunicare între vorbitorii multilingvi. Majoritatea conținutului social media al societăților multilingve sunt scrise în text mixt de coduri. Cu toate acestea, majoritatea sistemelor actuale de traducere neglijează convertirea textelor mixte de cod într-o limbă standard. Majoritatea conținutului amestecat de coduri scris de utilizator în rețelele sociale rămâne neprelucrat din cauza indisponibilității resurselor lingvistice, cum ar fi corpul paralel. Această lucrare propune un model Neural Machine Translation (NMT) pentru a traduce textul singhala-engleză în limba singhală. Datorită resurselor limitate disponibile pentru text singhala-engleză code-mixed (SECM), se creează un corpus paralel cu propoziții SECM și propoziții singhala. Site-urile social media Srilankan conțin texte SECM mai frecvent decât limbile standard. Modelul propus pentru traducerea textului mixt de coduri în acest studiu este o combinație de cadru Encoder-Decoder cu unități LSTM și algoritmul de forțare a profesorilor. Frazele traduse din model sunt evaluate folosind metrica BLEU (Bilingual Evaluation Understudy). Modelul nostru a obținut un scor BLEU remarcabil pentru traducere.</abstract_ro>
      <abstract_si>Name ගොඩක් භාෂාත්මක සමාජාත්මක සාමාජික මිඩියාව සමාව ලියලා තියෙන්නේ. නමුත්, ප්‍රස්තූත වාර්ථාව පද්ධතියක් ගොඩක් අවශ්‍ය භාෂාවක් වෙනුවෙන් කෝඩ් මික්ස් පාළුව සාමාජික මිඩියාවට ලියපු ප්‍රයෝජකයේ ගොඩක් ප්‍රයෝජකයෙන් ලියපු කෝඩ් මිස්සු සාමාජික සාමාජික සාමාජික සා මේ පැත්තේ සින්හාල්- ඉංග්‍රීසි කෝඩ- මික්ස් පාළුවක් සින්හාලි භාෂාවට පරිවර්තන කරන්න සින්හාල් භාෂා සින්හාල්-ඉංග්‍රීසි කෝඩ් මිශ්‍රීය (SECM) පාළුවට පුළුවන් සීමාන්‍ය සම්පූර්ණ සම්පූර්ණ සම්පූර්ණ සම්පූ ශ්‍රිලාන්කාන් සාමාජික මාධ්‍යමාධ්‍යමය සිටින් SECM පණිවිඩය සාමාන්‍ය භාෂාවට වඩා වඩා  මේ පරීක්ෂණයේ කෝඩ් මික්ස් පාළු භාවිතය සඳහා ප්‍රයෝජනය ප්‍රයෝජනයක් LSTM යුනිත්වය සහ ගුරුවර් අල්ගෝරිතම් සමග ස මොඩේල් වලින් භාවිත කතාවල් බිලින්ගුල් විශ්ලේෂණය බිලින්ගුල් විශ්ලේෂණය සඳහා විශ්ලේෂණය කර අපේ නිර්මාණය පරිවර්තනය වෙනුවෙන් ප්‍රශ්නයක් ලැබුනා.</abstract_si>
      <abstract_sv>Kodblandning har blivit en rörlig kommunikationsmetod bland flerspråkiga talare. Det mesta av det sociala medieinnehållet i de flerspråkiga samhällena är skrivet i kodblandad text. De flesta av de nuvarande översättningssystemen försummar dock att konvertera kodblandade texter till ett standardspråk. Det mesta av användarskrivet kodblandat innehåll i sociala medier förblir obearbetat på grund av otillgängligheten av språkliga resurser såsom parallell corpus. Denna uppsats föreslår en Neural Machine Translation (NMT) modell för att översätta den singhala-engelska kodblandade texten till singhala språket. På grund av de begränsade resurser som finns tillgängliga för singala-engelska kodblandad(SECM)-text skapas en parallell korpus med SECM-meningar och singala meningar. Srilankans sociala medier innehåller SECM-texter oftare än standardspråken. Den modell som föreslås för kodblandad textöversättning i denna studie är en kombination av Encoder-Decoder ramverk med LSTM-enheter och Teachers Forcing Algorithm. De översatta meningarna från modellen utvärderas med hjälp av BLEU (tvåspråkig utvärdering substudie). Vår modell uppnådde en anmärkningsvärd BLEU-poäng för översättningen.</abstract_sv>
      <abstract_so>Codeynta isku xiriirka waxay noqotay qaab dhaqdhaqaaq ah oo ku dhexeeya hadalka luuqadaha kala duduwan. Inta badan waxaa ku qoran macluumaadka bulshada ee bulshada kala duduwan qoraal ku qoran. Si kastaba ha ahaatee, nidaamka turjumista ee joogtada badankood waxay ka jeedaan in ay qoraal koonfureed u beddelaan luuqad standard ah. Inta badan ee isticmaalayaasha ku qoran kooxda isku xiran ee macluumaadka bulshada ayaa la baaraandegay sababtoo ah sababtoo ah aysan helin manfacada luqada sida korpus oo kale. Kanu warqaddan wuxuu soo jeedaa tarjumaadda Neural Machine (NMT) model to translate the Sinhala-Ingiriis code-mixed text to the Sinhala language. Sababta ay tahay maal xad ah oo ay ku jiraan qoraalka kooxda ee Sinhala-Ingiriis (SECM), waxaa la abuuraa qodob isku mid ah oo lagu qoray xukunka SECM iyo xafiiska Sinhala. Bogagga shabakada bulshada ee Srilankan waxaa ku yaala qoraal-qoraal SECM oo ka badan luuqadaha caadiga ah. Tusaalada lagu soo jeedo qoraalka koowaad-mixed-text turjumista waxbarashadan waa isku xir kookookoor-Decoder framework oo ay leeyihiin LSTM iyo Macallimiin qasab Algorithm. Tusaalada waxaa lagu qiimeeyaa qoraalka ku qoran BLEU(Waxbarashada ku qoran afka bilowga). Tusaalkayagii wuxuu gaadhay qiimo aad u fiican BLEU oo turjumista.</abstract_so>
      <abstract_ur>کڈ میکسنگ بہت سی زبان صحبت کرنے والوں کے درمیان تعامل کی حرکت کا طریقہ ہو گیا ہے۔ اکثر سیاسی میڈیا موجودات کی مختلف زبان اجتماعوں میں لکھی جاتی ہیں۔ However, most of the current translation systems ignore code-mixed texts to a standard language. اکثر کارساز کو سوسیلی میڈیا میں لکھا ہوا کڈ میکس منصوبات کی نسبت زبان منصوبات کے غیر قابل دسترسی کے باعث بغیر محسوس ہو رہے ہیں۔ This paper proposes a Neural Machine Translation (NMT) model to translate the Sinhala-English code-mixed text to the Sinhala language. Sinhala-English code-mixed (SECM) text کے لئے موجود ہونے کے لئے محدودہ سرمایہ کے سبب، ایک parallel corpus is created with SECM sentences and Sinhala sentences. سیرلنک سوسیلی میڈیا سائٹوں میں SECM پیغام استاندارڈ زبانوں سے زیادہ اضافہ ہے۔ اس تحقیقات میں کوڈ میکس ٹیکسٹ ترجمہ کے لئے پیشنهاد کی موڈل LSTM یونیٹوں اور استاد الگوریتم کے ساتھ Encoder-Decoder فرموٹ کی ترجمہ ہے. موڈل سے ترجمہ کلمات بلیوس (دوئلینگ ارزش اندر استرس) کے مطابق مطابق ارزش کی جاتی ہیں. ہماری مدل نے ترجمہ کے لئے ایک اثر بلیوس اسکور پہنچا۔</abstract_ur>
      <abstract_ta>பல மொழி பேச்சாளர்களுக்கு இடையே தொடர்பு முறையாகும். பல மொழிகளின் சமூகங்களின் பெரும்பாலான சமூக ஊடகங்களின் உள்ளடக்கங்கள் குறியீடு கலந்த உரையில் எழுதப்பட்டுள் ஆனால், தற்போதைய மொழிபெயர்ப்பு அமைப்பு சமூக ஊடகங்களில் பெரும்பாலான எழுதப்பட்ட குறியீடு கலக்கப்பட்ட உள்ளடக்கங்கள் செயல்படுத்தப்படாது இணைய கோர்புஸ் போன்ற மொழியின் கி @ info சிங்ஹாலா- ஆங்கிலத்தின் குறியீடு- கலக்கப்பட்ட (SECM) உரைக்கு கிடைக்கும் வரம்பு மூலங்கள் காரணத்தால், SECM வாக்குகள் மற்றும் சிங் ஸ்ரிலாங்கான் சமூக ஊடக தளங்களில் SECM உரைகள் நிலையான மொழிகளை விட அதிகமாக இருக்கிறது. இந்த ஆராய்ச்சியில் குறியீட்டு கலக்கப்பட்ட உரை மொழிபெயர்ப்பிற்கு பரிந்துரைக்கப்பட்ட மாதிரி LSTM அலகுகள் மற்றும் ஆசிரியர்கள் அல்gor மாதிரியில் இருந்து மொழிபெயர்ப்பின் வாக்குகள் BLEU (Bilingual evaluation Understudy) மெட்ரிக் பயன்படுத்தப்படும். நம்முடைய மாதிரி மொழிபெயர்ப்பிற்கான பிலியு மதிப்பெண் அடைந்தது.</abstract_ta>
      <abstract_mt>It-taħlit tal-kodiċi sar metodu ta’ komunikazzjoni li jiċċaqlaq fost il-kelliema multilingwi. Il-biċċa l-kbira tal-kontenut tal-midja soċjali tas-soċjetajiet multilingwi jinsab f’test imħallat bil-kodiċi. However, most of the current translation systems neglect to convert code-mixed texts to a standard language.  Most of the user written code-mixed content in social media remains unprocessed due to the unavailability of linguistic resource such as parallel corpus.  Dan id-dokument jipproponi mudell ta’ Traduzzjoni ta’ Magni Newrali (NMT) biex it-test imħallat bil-kodiċi Sinhala-Ingliż jiġi tradott fil-lingwa Sinhala. Minħabba r-riżorsi limitati disponibbli għat-test imħallat bil-kodiċi Sinhala-Ingliż (SECM), jinħoloq korpus parallel mas-sentenzi SECM u s-sentenzi Sinhala. Is-siti tal-midja soċjali Srilankan fihom testi tas-SECM aktar spiss mil-lingwi standard. Il-mudell propost għat-traduzzjoni tat-test imħallat bil-kodiċi f’dan l-istudju huwa kombinazzjoni ta’ qafas ta’ Encoder-Decoder ma’ unitajiet LSTM u Algorithm ta’ Forcing tal-għalliema. Is-sentenzi tradotti mill-mudell huma evalwati bl-użu tal-metrika BLEU(Substudju ta’ Evalwazzjoni Bilinguali). Il-mudell tagħna kiseb punteġġ notevoli BLEU għat-traduzzjoni.</abstract_mt>
      <abstract_uz>Kodlash qoidasi bir necha tilda gapiruvchilar orasidagi aloqa usuli bo'lgan. Ko'pchilik jamiyatlar jamiyatlarining ko'pchiligi jamiyatli tarkibi kodlash matnini yozishadi. Lekin, joriy tarjima tizimning ko'pchiligi kod- mixed matnlarni andoza tilga aylantirishga eʼtibor bermaydi. Name Name Name Name The model proposed for code-mixed text translation in this study is a combination of Encoder-Decoder framework with LSTM units and Teachers Forcing Algorithm.  Name Bizning modelimiz tarjima qilish uchun ajoyib BLEU scori topdi.</abstract_uz>
      <abstract_vi>Mã pha trộn đã trở thành một phương pháp liên lạc chuyển động giữa các diễn viên đa dạng. Phần lớn nội dung truyền thông xã hội của các xã hội đa dạng đều được viết bằng mã trộn. Tuy nhiên, hầu hết các hệ thống dịch chuyển hiện nay thiếu khả năng chuyển đổi các văn bản hỗn hợp mã thành ngôn ngữ tiêu chuẩn. Phần lớn nội dung mã trộn lẫn trong các phương tiện xã hội vẫn chưa được xử lý vì không có tài nguyên ngôn ngữ như tập thể song. Tờ giấy này đề xuất một mô hình Dịch về máy thần kinh (NMB) để dịch đoạn mã trộn của Sinhala-English sang ngôn ngữ SinHale. Do các nguồn lực hạn chế cho văn bản theo mã trộn (SecM) của SinHale-Anh, một tập thể song song được tạo ra với các câu thứ hai và SinHale. Các trang mạng xã hội bố Lan chứa các văn bản theo thứ hai thường xuyên hơn ngôn ngữ tiêu chuẩn. The model proposed for mã-mixed text translation in this study is a combination of Encoder-Decder structure with LSTM units and teachers Khởi tạo Algorithm. Các câu dịch ra từ mô hình được đánh giá bằng hệ thống LELIU(Bài Đánh giá giải) Mô hình của chúng tôi đã đạt được một số lượng đáng chú ý.</abstract_vi>
      <abstract_nl>Code-mixing is uitgegroeid tot een bewegende manier van communicatie tussen meertalige sprekers. De meeste social media content van de meertalige samenlevingen zijn geschreven in code-gemengde tekst. De meeste huidige vertaalsystemen verwaarlozen echter om code-gemengde teksten om te zetten naar een standaardtaal. Het grootste deel van de door de gebruiker geschreven code-mixed content in sociale media blijft onbewerkt vanwege de onbeschikbaarheid van taalkundige bronnen zoals parallelle corpus. Deze paper stelt een Neural Machine Translation (NMT) model voor om de Singhala-Engelse code-mixed tekst te vertalen naar de Singhala taal. Vanwege de beperkte middelen die beschikbaar zijn voor Sinhalas-Engelse code-mixed(SECM) tekst, wordt een parallel corpus gecreëerd met SECM zinnen en Sinhalas zinnen. Srilankan social media sites bevatten SECM teksten vaker dan de standaard talen. Het model dat in deze studie wordt voorgesteld voor code-gemengde tekst vertaling is een combinatie van Encoder-Decoder framework met LSTM eenheden en Teachers Forcing Algorithm. De vertaalde zinnen uit het model worden geëvalueerd met behulp van BLEU (Bilingual Evaluation Understudy) metric. Ons model behaalde een opmerkelijke BLEU score voor de vertaling.</abstract_nl>
      <abstract_hr>Mješanje kodova postalo je pokretni način komunikacije među višejezičkim govornicima. Većina sadržaja društvenih medija multijezičkih društva napisana je u tekstu mešanom kod. Međutim, većina trenutnih sustava prevođenja zaboravlja preobraćati tekste pomiješane kod na standardni jezik. Većina korisnika napisanog sadržaja mešanog koda u društvenim medijima ostaje neprocijeđena zbog nepostupnosti jezičkih resursa poput paralelnog korpusa. Ovaj papir predlaže model neurološkog prevoda (NMT) za prevod teksta pomiješanog sinhalačkog engleskog koda na sinhalački jezik. Zbog ograničenih resursa koje su dostupne za tekst sinhalski-engleski mješani kodovi (SECM), paralelni korpus je stvoren sa rečenicama SECM-a i kaznicama sinhala. Sriljanski društveni mediji sadrže tekstove SECM češće od standardnih jezika. Model koji je predložen za prevod teksta u ovom ispitivanju pomiješan kod je kombinacija okvira kodera-dekodera sa LSTM jedinicama i učiteljima koji prisiljavaju algoritam. Prevedene rečenice iz modela procjenjuju se koristeći metričku BLEU (podispitivanje za dvostruku procjenu). Naš model je postigao izvanredan rezultat BLEU za prevod.</abstract_hr>
      <abstract_bg>Смесването на кодове се превърна в движещ се метод за комуникация между многоезичните говорители. Повечето от съдържанието на социалните медии на многоезичните общества са написани в кодово смесен текст. Въпреки това, повечето от сегашните системи за превод пренебрегват конвертирането на смесени с кодове текстове на стандартен език. Повечето от потребителското писмено кодово смесено съдържание в социалните медии остава непробивано поради липсата на езиков ресурс като паралелен корпус. Настоящата статия предлага модел на неврален машинен превод (НМТ) за превод на синхалско-английски кодово смесен текст на синхалския език. Поради ограничените ресурси, налични за синхалско-английски кодов смесен текст, се създава паралелен корпус с изречения и синхалски изречения. Сайтовете на Сриланка в социалните медии съдържат текстове по-често от стандартните езици. Моделът, предложен за кодово-смесен текстов превод в настоящото изследване, е комбинация от рамка за кодиране-декодер с единици и алгоритъм на учителите. Преведените изречения от модела се оценяват с помощта на метрика. Нашият модел постигна забележителен резултат за превода.</abstract_bg>
      <abstract_da>Kodeblanding er blevet en bevægende kommunikationsmetode blandt flersprogede talere. Størstedelen af indholdet på sociale medier i de flersprogede samfund er skrevet i kode-blandet tekst. De fleste af de nuværende oversættelsessystemer undlader imidlertid at konvertere kodblandede tekster til et standardsprog. Det meste af det brugerskrevne kodeblandede indhold i sociale medier forbliver ubehandlet på grund af manglende tilgængelighed af sproglige ressourcer såsom parallel corpus. Denne artikel foreslår en Neural Machine Translation (NMT) model til at oversætte den singhala-engelske kodeblandede tekst til singhala sproget. På grund af de begrænsede ressourcer til rådighed for singhala-engelsk kode-blandet (SECM) tekst, oprettes et parallelt korpus med SECM sætninger og singhala sætninger. Srilankans sociale mediesider indeholder SECM-tekster oftere end standardsprogene. Den model, der foreslås for kode-blandet tekst oversættelse i dette studie, er en kombination af Encoder-Decoder ramme med LSTM enheder og Teachers Forcing Algorithm. De oversatte sætninger fra modellen evalueres ved hjælp af BLEU (Tosproget Evaluation Understudy) metric. Vores model opnåede en bemærkelsesværdig BLEU score for oversættelsen.</abstract_da>
      <abstract_id>Pengcampuran kode telah menjadi metode komunikasi bergerak di antara pembicara berbagai bahasa. Kebanyakan isi media sosial masyarakat berbilang bahasa ditulis dalam teks kode-campuran. Namun, kebanyakan sistem terjemahan saat ini mengabaikan untuk mengubah teks kode-campuran ke bahasa standar. Kebanyakan pengguna menulis isi kode-campuran dalam media sosial masih belum diproses karena tidak tersedia sumber daya bahasa seperti corpus paralel. Kertas ini mengusulkan model Translation Mesin Neural (NMT) untuk menerjemahkan teks kode-campuran Sinhala-Inggris ke bahasa Sinhala. Karena sumber daya terbatas yang tersedia untuk teks kode-mixed (SECM) bahasa Inggris-Sinhala, sebuah corpus paralel diciptakan dengan kalimat SECM dan kalimat Sinhala. Srilankan social media sites contain SECM texts more frequently than the standard languages.  Model yang diusulkan untuk terjemahan teks campuran kode dalam studi ini adalah kombinasi dari kerangka Encoder-Decoder dengan LSTM unit dan Algoritma Memaksa Guru. Kalimat terjemahan dari model diteliti menggunakan metrik BLEU(Bilingual Evaluation Understudy). Model kami mencapai nilai BLEU yang luar biasa untuk terjemahan.</abstract_id>
      <abstract_ko>코드 혼합은 이미 여러 언어 사용자 간의 교류의 일종의 이동 방식이 되었다.다언어 사회의 대다수 소셜 미디어 내용은 코드와 텍스트를 혼합하여 작성된 것이다.그러나 현재 대부분의 번역 시스템은 코드 혼합 텍스트를 표준 언어로 바꾸는 것을 소홀히 하고 있다.평행 자료 라이브러리 등 언어 자원을 사용할 수 없기 때문에 소셜 미디어에서 대다수 사용자가 작성한 코드 혼합 내용은 여전히 처리되지 않았다.본고는 승가로어-영어 코드 혼합 텍스트를 승가로어로 번역하는 신경기계번역(NMT) 모델을 제시했다.승가라어-영어 코드 혼합(SECM) 텍스트의 사용 가능한 자원이 제한되어 있기 때문에 SECM 문장과 승가라어 문장을 이용하여 평행 어료 라이브러리를 만들었다.스리랑카 소셜미디어 사이트에는 SECM 텍스트의 빈도가 표준어보다 높다.본고에서 제시한 코드 혼합 텍스트 번역 모델은 인코딩-디코딩 프레임워크, LSTM 단원과 강제 알고리즘을 결합시켰다.BLEU 메트릭을 사용하여 모델의 번역 문장을 평가합니다.우리의 모델은 번역 방면에서 현저한 BLEU 점수를 얻었다.</abstract_ko>
      <abstract_de>Code-Mixing ist zu einer bewegenden Methode der Kommunikation unter mehrsprachigen Sprechern geworden. Die meisten Social-Media-Inhalte der mehrsprachigen Gesellschaften sind in Code-Mixed-Text geschrieben. Allerdings vernachlässigen die meisten gängigen Übersetzungssysteme die Umwandlung von Code-Mixed-Texten in eine Standardsprache. Der Großteil der vom Benutzer geschriebenen Code-Mixed-Inhalte in sozialen Medien bleibt aufgrund der Nichtverfügbarkeit sprachlicher Ressourcen wie paralleler Korpus unverarbeitet. Diese Arbeit schlägt ein Neural Machine Translation (NMT) Modell vor, um den Singhala-Englischen Code-Mixed Text in die Singhala Sprache zu übersetzen. Aufgrund der begrenzten Ressourcen für Singhala-English code-mixed(SECM)-Text wird ein paralleler Korpus mit SECM-Sätzen und Singhala-Sätzen erstellt. Srilankan Social Media Seiten enthalten SECM Texte häufiger als die Standardsprachen. Das in dieser Studie vorgeschlagene Modell für die Übersetzung von Code-Mixed Text ist eine Kombination aus Encoder-Decoder Framework mit LSTM Einheiten und Teachers Forcing Algorithm. Die übersetzten Sätze aus dem Modell werden mit BLEU (Bilingual Evaluation Understudy) Metrik ausgewertet. Unser Modell erzielte eine bemerkenswerte BLEU-Punktzahl für die Übersetzung.</abstract_de>
      <abstract_fa>ترکیب قانونی یک روش حرکت در ارتباط بین زبان‌ها تبدیل شده است. بیشتر محتوای رسانه‌های اجتماعی از جامعه‌های زیادی زبان در متن پیوند پیوند متصل شده نوشته می‌شوند. ولی بیشتر سیستم‌های ترجمه فعلی به زبان استاندارد تبدیل کردن متن‌های ترجمه‌شده کد را نادیده می‌گیرند. بیشتر کاربر محتوای پیوسته‌ای از کد نوشته شده در رسانه‌های اجتماعی به دلیل غیرقابل دسترسی منابع زبان‌شناسی مانند کورپوس متفاوتی در حال حاضر است. این کاغذ یک مدل تغییرات ماشین عصبی (NMT) را پیشنهاد می‌دهد تا متن متصل شده‌ی کد انگلیسی سینهالی را به زبان سینهالی ترجمه کند. به دلیل منابع محدودیت برای متن رمز پیوند (SECM) سینهال-انگلیسی وجود دارد، یک کورپوس پارالی با جمله‌های SECM و جمله‌های سینهال ایجاد می‌شود. سایت‌های رسانه‌های اجتماعی سرلانکا متن‌های SECM بیشتر از زبان‌های استاندارد را دارند. Model proposed for code-mixed text translation in this study is a combination of Encoder-Decoder framework with LSTM units and teachers Forcing Algorithm. جمله‌های ترجمه‌شده از مدل با استفاده از متریک BLEU(تحقیق زیر ارزیابی دوگانه) ارزیابی می‌شوند. مدل ما یک امتیاز بلوئوپ فوق العاده برای ترجمه رسید.</abstract_fa>
      <abstract_sw>Miungano ya sheria imekuwa njia ya kuhamisha mawasiliano kati ya wazungumzaji wa lugha mbalimbali. Mitandao mengi ya mitandao ya kijamii ya jumuiya nyingi za lugha zimeandikwa kwa maandishi yanayochanganyika. Hata hivyo, mifumo mingi ya tafsiri ya sasa hupuuza kubadilisha maandishi yanayochanganyika kwa kodi kuwa lugha ya kawaida. Most of the user written code-mixed content in social media remains unprocessed due to the unavailability of linguistic resource such as parallel corpus.  Makala hii inapendekeza mfumo wa Tafsiri ya Mashine ya Neural (NMT) kutafsiri ujumbe wa kodi-Kiingereza kwa lugha ya Sinhala. Kutokana na rasilimali chache zinazopatikana kwa ujumbe wa kodi ya Sinhala-Kiingereza (SECM), makampuni yanatengenezwa kwa hukumu za SECM na hukumu za Sinhala. Tovuti za mitandao ya kijamii za Srilanki zina ujumbe wa SECM mara kwa mara kuliko lugha za kawaida. Mradi unapendekezwa kwa kutafsiri tafsiri ya maandishi yenye mchanganyiko wa kodi katika utafiti huu ni muunganiko wa mfumo wa mfumo wa Encoder na vifaa vya LSTM na Walimu wanaohamasisha Algorithi. sentensi zilizotafsiriwa kutoka kwa mtindo huo zinavutiwa kwa kutumia mbinu ya BLEU(Utafiti wa Uchunguzi wa Kilugha). Mfano wetu ulifanikiwa score ya BLEU ya kutangaza tafsiri.</abstract_sw>
      <abstract_af>Name Die meeste van die sosiale media inhoud van die multitaalse samelewing is geskryf in kode gemengde teks. Maar meeste van die huidige vertalingsstelsels verwerp om kode gemengde teks na 'n standaard taal te omskakel. Die meeste van die gebruiker het geskrywe kode gemengde inhoud in sosiale media oorgebly onverwerp vanweë die onbeskikbaarheid van lingvisse hulpbron soos parallele korpus. Hierdie papier voorstel 'n Neural Masjien Vertaling( NMT) model om die Sinhala- Engels kode- gemengde teks te vertaal na die Sinhala taal. Dus die beperkte hulpbron beskikbaar vir Sinhala- Engels kode- gemengde (SECM) teks, word 'n parallele korpus geskep met SECM setnings en Sinhala setnings. Srilankan sosiale media tuistes bevat SECM teks meer dikwels as die standaard tale. Die model voorgestel vir kode gemengde teks vertaling in hierdie studie is 'n kombinasie van Encoder-Decoder raamwerk met LSTM eenhede en onderwysers wat versterk Algoritme. Die vertaalde setinge van die model word uitgewerk met gebruik van BLEU (Bilinguele Evaluering Understudy) metrie. Ons model het 'n remarkante BLEU-poeker vir die vertaling bereik.</abstract_af>
      <abstract_sq>Përzierja e kodeve është bërë një metodë lëvizëse komunikimi midis folësve shumëgjuhës. Shumica e përmbajtjes së medias sociale të shoqërive shumëgjuhësore janë shkruar në tekst të përzier me kode. Megjithatë, shumica e sistemeve aktuale të përkthimit harrojnë të konvertojnë tekste të përziera me kod në një gjuhë standard. Most of the user written code-mixed content in social media remains unprocessed due to the unavailability of linguistic resource such as parallel corpus.  Ky dokument propozon një model të përkthimit të makinës nervore (NMT) për të përkthyer tekstin e përzier me kod sinhala-anglez në gjuhën sinhala. Për shkak të burimeve të kufizuara të disponueshme për tekstin e përzier me kod sinhala-anglez (SECM), krijohet një korpus paralel me frazat SECM dhe frazat sinhala. Faqet e medias sociale të Srilankës përmbajnë tekste SECM më shpesh se gjuhët standarde. Modeli i propozuar për përkthimin e tekstit të përzier me kode në këtë studim është një kombinim i kuadrit të koduesit-dekoderit me njësitë LSTM dhe Algoritmin e Forcimit të Mësuesve. Frazat e përkthyera nga modeli vlerësohen duke përdorur metrikën BLEU(Substudimi i vlerësimit dygjuhësor). Modeli ynë arriti një rezultat të jashtëzakonshëm BLEU për përkthimin.</abstract_sq>
      <abstract_tr>Ködleme karışmasy birnäçe dilli gürleşýänler arasynda habarlaşmak üçin bir hereket edildi. Birnäçe dilli jemgyýetlerin sosyal medýäniň köp bölegi koda karışan metinde ýazylýar. Ýöne häzirki terjime sistemleriniň köp bölegi, karmaşdyran metinleri standart dile üýtgetmekden çykarýar. Ullançylaryň köp bölegi sosyal medýäniň içinde ýazylýan ködlemeler, parallel korpus ýaly dil çeşmeleriň tapylmadygyna sebäbi işlenmediler. Bu kagyz Sinhalaly-Iňlisçe karışylyk metini Sinhalaly diline terjime etmek üçin bir nural maşynyň terjime(NMT) nusgasyny teklip edýär. Sinhalaly-iňlisçe karışyk(SECM) metin üçin meňzeşli çeşmeleriň sebäbi parallel korpus SECM sözleri we Sinhalaly sözleri bilen döredildi. Şrilanka sosialy medýdançalar standart dillerden köplenç SECM metinlerini bar. Bu aramda karışmış metin terjime üçin nusgala edildi. Bu aramda LSTM bir ködler we Mugallymlar Algoritmus bilen birleştirilýär. Modeliň terjime edilen sözleriň BLEU(Bilingual Taýýarlama Astynyň Çykyşy) metrikleri ulanylar. Biziň nusgymyz bu terjime üçin örän möhüm bir BLEU netijesi tapdy.</abstract_tr>
      <abstract_hy>Կոդի խառնուրդը դարձավ շարժվող հաղորդակցման մեթոդ բազմալեզվով խոսնակների միջև: Շատ լեզվով հասարակությունների սոցիալական լրատվամիջոցների պարունակությունը գրված է կոդի խառնված տեքստում: Այնուամենայնիվ, ներկայիս թարգմանման համակարգերի մեծամասնությունը անտեսում է կոդի խառնված տեքստերը ստանդարտ լեզվի: Most of the user written code-mixed content in social media remains unprocessed due to the unavailability of linguistic resource such as parallel corpus.  Այս թղթին առաջարկում է նյարդային մեքենայի թարգմանման (NMT) մոդել, որը կթարգմանի սինալա-անգլերեն կոդի խառնված տեքստը սինալայի լեզվին: Սինհալա-անգլերեն կոդ-խառնված (SEKM) տեքստի համար հասանելի սահմանափակ ռեսուրսների շնորհիվ զուգահեռ կորպուս է ստեղծվում SEKM նախադասություններով և սինալա նախադասություններով: Սրիլանյան սոցիալական լրատվամիջոցների կայքերը ավելի հաճախ են ներառում SEK տեքստերը, քան ստանդարտ լեզուները: Այս ուսումնասիրության մեջ կոդի խառնված տեքստի թարգմանման համար առաջարկված մոդելը կոդեր-կոդեր համակարգի է LSMT միավորների և ուսուցիչների պարտադրող ալգորիթմի հետ: Մոդելի թարգմանված նախադասությունները գնահատվում են օգտագործելով ԲԼԵՎ (Երկլեզու գնահատման ենթաուսումնասիրություն) մետրիկ: Մեր մոդելը հաջողվեց թարգմանության համար գլխավոր գնահատականներ:</abstract_hy>
      <abstract_az>Kod karışması çoxlu dilli danışanlar arasında hərəkət etmə yolu oldu. Çoxlu dil toplumların sosyal media məlumatının çoxu kodlu karışıqlı metinlərdə yazılır. Halbuki, a ğımdaki tercümə sistemlərinin çoxu kodla karışmış metinləri standart dilinə çevirməsini unutdur. İstifadəçilərin çoxu sosyal media içində yazılmış kodla karışılmış məlumatı paralel korpus kimi dil çoxluğunun mümkün olmadığı üçün istifadə edilməz. Bu kağıt Sinhal-İngilizce kodu karışıqlı metinləri Sinhal dilinə çevirmək üçün NMT modelini təklif edir. Sinhal-İngilizə kodu karıştırılmış (SECM) mətnlərinin müəyyən edilən müəyyən kaynaqlarına görə paralel korpus SECM sözləri və Sinhal sözləri ilə yaradılır. Şrilankı sosyal media sitələri standart dillərindən daha çox SECM metinləri içərir. Bu araşdırma içində Kod-karışıqlı metin çevirisi üçün təklif edilən model LSTM birlikləri ilə Kod-Dekoder çerçivesi və Muhəmmədlər Algoritimi zorlayırlar. Modeldən çevirilən cümlələr BLEU(İkinci Evaluation Understudy) metric vasitəsilə değerlənir. Bizim modelimiz tərcümə üçün gözəl bir BLEU nöqtəsi qəbul etdi.</abstract_az>
      <abstract_am>የኮድ ማቀናቀል በቋንቋ ቋንቋዎች መካከል የመንቀሳቀስ መልዕክት ሆኖአል፡፡ ብዙዎቹ የቋንቋዎች ማኅበረሰቦች ማኅበራዊ ሚዲያዎች በጽሑፍ በተለየ ጽሑፍ ተጽፎአል፡፡ ምንም እንኳን፣ የአሁኑ ትርጉም ስርዓቶች ብዙዎቹ የኮድ-mixed ጽሑፎችን ወደ standard ቋንቋ ለመለወጥ ተዉታል፡፡ በማኅበራዊ ማኅበራዊ አውታር የተጻፈው የጽሑፍ ኮድ የተቀላቀለ ጥያቄ ብዙዎቹም እንደተያያይደለ የቋንቋዊ ክፍተት እንዳይገኝ ነው፡፡ This paper proposes a Neural Machine Translation(NMT) model to translate the Sinhala-English code-mixed text to the Sinhala language.  በሲንሐላ-እንግሊዘኛ ኮድ-መቀላቀል (SECM) ጽሑፍ የተደረገ ሀብት ምክንያት በSECM ፍርድ እና በሲንሃላ ፍርድ የተደረገ የኮርፓስ ክፍል ነው፡፡ የሳሪንጋን ማኅበራዊ ሚዲያ ገጾች የSECM ጽሑፎች በዓላማዊ ቋንቋዎች ላይ አብዛኛዎች ናቸው፡፡ በዚህ ትምህርት ውስጥ ለcode-mixed text ትርጓሜ የተገኘው model - Encoder-Decoder framework - with LSTM units and Teachers Forcing Algorithm - a combination. ከሞዴል የተዘጋጁ ቃላት BLEU (Bilingual Evaluation Understudy) ሚትሪክ የተጠቃሚ ነው፡፡ ሞዴሌያችን ለትርጉም የBLEU score አግኝቷል፡፡</abstract_am>
      <abstract_bn>কোড মিশ্রণ বিভিন্ন ভাষাভাষীদের মধ্যে যোগাযোগের মাধ্যমে পরিণত হয়েছে। Most of the social media content of the multilingual societies are written in code-mixed text.  তবে বর্তমান অনুবাদ সিস্টেম স্বাভাবিক ভাষায় কোড- মিশ্রিত টেক্সট পরিবর্তন করার উপেক্ষা করেছে। সামাজিক প্রচার মাধ্যমে বেশীরভাগ ব্যবহারকারীর লেখা কোড-মিশ্রিত বিষয়বস্তু প্রক্রিয়া নেই, যেমন পার্যালেল কোর্পাসের মত ভাষ এই পত্রিকাটি সিনাহাল-ইংরেজী কোড-মিশ্রিত লেখাটিকে সিনাহালার ভাষায় অনুবাদ করার জন্য নেউরাল মেশিন অনুবাদ প্রস্তাব করে। সিঙ্গাল-ইংরেজি কোড মিশ্রিত (সেসিএম) লেখার জন্য সীমিত সম্পদের কারণে সিসিএমের বাক্য এবং সিঙ্গালার শাস্তি দিয়ে একটি পার্লিল কর্প শ্রিলাঙ্কান সামাজিক প্রচার মাধ্যমের সাইটের মধ্যে সেসিএম টেক্সট প্রায়শ স্ট্যান্ডার ভাষার চেয়ে ব এই গবেষণায় কোড মিশ্রিত টেক্সট অনুবাদের জন্য প্রস্তাবিত মডেল হচ্ছে এনকোডার-ডেকোডার ফ্রেম্যাকার্ক LSTM ইউনিট এবং শিক্ষকদের বাধ্য মডেল থেকে অনুবাদ করা বিলিউ (বিলিঙ্গুয়াল ইনভায়ালুয়েশন আন্ডার্টলেশন) মেট্রিক ব্যবহার করে মূল্যায়ন করা হয়েছে। আমাদের মডেল অনুবাদের জন্য একটি চমৎকার বিলিউ স্কোর অর্জন করেছে।</abstract_bn>
      <abstract_bs>Mješanje kodova postalo je pokretni metod komunikacije među višejezičkim govornicima. Većina sadržaja društvenih medija multijezičkih društva napisana je u tekstu pomiješanom kodom. Međutim, većina trenutnih sustava prevođenja zaboravlja preobraćati tekste pomiješane kod na standardni jezik. Većina korisnika napisanog sadržaja mešanog koda u društvenim medijima ostaje neprocijeđena zbog nepostupnosti jezičkih resursa poput paralelnog korpusa. Ovaj papir predlaže model neurološkog prevoda (NMT) za prevod teksta pomiješanog sinhala-engleskog koda na sinhala jezik. Zbog ograničenih resursa koje su dostupne za tekst sinhalanskog-engleskog koda pomiješanog (SECM), paralelni korpus je stvoren sa kaznama SECM-a i kaznama sinhalaca. Sriljanski društveni mediji sadrže tekstove SECM češće od standardnih jezika. Model predložen za prevod teksta u ovom ispitivanju između kodova je kombinacija okvira kodera-dekodera sa LSTM jedinicama i učiteljima koji prisiljavaju algoritam. Prevedene rečenice iz modela procjenjuju se koristeći metričku BLEU (podstudiranje za dvostruku procjenu). Naš model je postigao izvanredan rezultat BLEU za prevod.</abstract_bs>
      <abstract_cs>Mixování kódu se stalo pohyblivou metodou komunikace mezi vícejazyčnými mluvčími. Většina obsahu sociálních médií mnohojazyčných společností je napsána v textu smíšeném kódem. Nicméně většina současných překladatelských systémů zanedbává konverzi textů smíšených kódem do standardního jazyka. Většina uživatelů psaného kódově smíšeného obsahu v sociálních médiích zůstává nezpracována kvůli nedostupnosti jazykových zdrojů, jako je paralelní korpus. Tento příspěvek navrhuje model neuronového strojového překladu (NMT) pro překlad čínštiny-anglického kódu smíšeného textu do sinhálského jazyka. Vzhledem k omezeným zdrojům, které jsou k dispozici pro čínština-anglický text, je vytvořen paralelní korpus s větami SECM a sinhálskými větami. Srilankanské sociální média obsahují texty SECM častěji než standardní jazyky. Model navržený pro překlad kódového smíšeného textu v této práci je kombinací frameworku Encoder-Decoder s LSTM jednotkami a algoritmem vynucování učitelů. Přeložené věty z modelu jsou hodnoceny pomocí metriky BLEU (Bilingual Evaluation Understudy). Náš model dosáhl pozoruhodného skóre BLEU pro překlad.</abstract_cs>
      <abstract_ca>La combinació de codis s'ha convertit en un mètode mouent de comunicació entre parlants multilingües. La majoria del contingut dels mitjans socials de les societats multilingües està escrit en text combinat amb codis. Tot i així, la majoria dels sistemes de traducció actuals no converteixen els textos combinats amb codi en un llenguatge normal. La majoria del contingut escrit per codi mixt als mitjans socials encara no ha estat processat degut a la falta de recursos lingüístics com el corpus parallel. Aquest article propon un model de traducció neuronal de màquina (NMT) per traduir el text mixte entre codi sinàl i anglès al llenguatge sinàl. Gràcies a les limitades quantitats de recursos disponibles per al text mixte de codi sinhala-anglès (SECM), es crea un corpus paral·lel amb frases SECM i frases sinhala. Els llocs de mitjans socials Srilankan contenen textos SECM més sovint que les llengües habituals. El model proposat per la traducció de text combinat amb codis en aquest estudi és una combinació de marc de codificador-codificador amb unitats LSTM i algoritmes de for ça dels professors. Les frases traduïdes del model s'evaluen usant la mètrica BLEU. El nostre model va aconseguir una puntuació notable BLEU per a la traducció.</abstract_ca>
      <abstract_et>Koodide segamine on muutunud liikuvaks suhtlemisviisiks mitmekeelsete kõnelejate vahel. Enamik mitmekeelsete ühiskondade sotsiaalmeedia sisust on kirjutatud koodisegatekstina. Enamik praegustest tõlkesüsteemidest jätab siiski tähelepanuta koodisegatud tekstide teisendamise standardkeelde. Enamik kasutaja kirjutatud koodisega sisust sotsiaalmeedias on töötlemata keelelise ressursi, näiteks paralleelkorpuse puudumise tõttu. Käesolev töö pakub välja neuraalse masintõlke (NMT) mudeli tõlkimiseks sinhala-inglise koodisegatud teksti tõlkimiseks sinhala keelde. Kuna sinhala-inglise koodisegatud teksti (SECM) jaoks on kättesaadavad piiratud ressursid, luuakse paralleelne korpus SECM lausete ja sinhala lausetega. Srilankani sotsiaalmeedia saidid sisaldavad SECM tekste sagedamini kui tavakeeled. Käesolevas uuringus pakutakse välja koodisegatud tekstitõlke mudel kombinatsiooniks kodeeri-dekooderi raamistikust LSTM üksuste ja õpetajate sundlalgoritmiga. Mudeli tõlgitud lauseid hinnatakse BLEU (Bilingual Evaluation Understudy) meetri abil. Meie mudel saavutas märkimisväärse BLEU skoori tõlke.</abstract_et>
      <abstract_fi>Koodien sekoittamisesta on tullut monikielisten puhujien liikuttava viestintätapa. Suurin osa monikielisten yhteiskuntien sosiaalisen median sisällöstä on kirjoitettu koodisekoitettuna tekstinä. Suurin osa nykyisistä käännösjärjestelmistä laiminlyö kuitenkin koodisekoitettujen tekstien muuntamisen tavalliseksi kieleksi. Suurin osa käyttäjän kirjoittamasta koodisekoitetusta sisällöstä sosiaalisessa mediassa on edelleen käsittelemättä, koska kieliaineistoa, kuten rinnakkaiskorpusta, ei ole saatavilla. Tässä työssä ehdotetaan neurokonekäännösmallia (NMT) sinhala-englantilaisen koodisekoitetun tekstin kääntämiseksi sinhalan kielelle. Koska sinhala-englanti code mixed(SECM) -tekstiin on saatavilla rajallisia resursseja, rinnakkaiskorpus luodaan SECM-lauseilla ja sinhala-lauseilla. Srilankanin sosiaalisen median sivustot sisältävät SECM-tekstejä useammin kuin tavalliset kielet. Tässä tutkimuksessa koodisekoitetun tekstin kääntämiseen ehdotettu malli on yhdistelmä kooderi-dekooderikehystä LSTM-yksiköihin ja Opettajien pakkoalgoritmiin. Mallin käännetyt lauseet arvioidaan BLEU (Bilingual Evaluation Understudy) -metrillä. Mallimme saavutti huomattavan BLEU-pisteen käännöksessä.</abstract_fi>
      <abstract_he>Code-mixing has become a moving method of communication among multilingual speakers.  Most of the social media content of the multilingual societies are written in code-mixed text.  עם זאת, רוב מערכות התרגום הנוכחיות שוכחות להפוך טקסטים מעורבים בקוד לשפה סטנדרטית. רוב המשתמשים כתבו תוכן מעורבב קוד בתקשורת חברתית נשאר ללא התהליך בגלל הלא זמין של משאבים שפתיים כמו קורפוס מקביל. העבודה הזו מציעה מודל לתרגום מכונת נוירולית (NMT) כדי לתרגם את הטקסט המערובב של קוד סינהלי-אנגלי לשפה סינהלית. Due to the limited resources available for Sinhala-English code-mixed(SECM) text, a parallel corpus is created with SECM sentences and Sinhala sentences.  Srilankan social media sites contain SECM texts more frequently than the standard languages.  המודל המוצע לתרגום טקסט מעורב בקוד במחקר הזה הוא שילוב של מסגרת קוד-קוד עם יחידות LSTM ואלגוריתם מאלץ. המשפטים המתורגמים מהדוגמא מתערכים באמצעות מטריקה BLEU (Understudy of Bilingual Evaluation). Our model achieved a remarkable BLEU score for the translation.</abstract_he>
      <abstract_sk>Mešanje kod je postalo gibljiva metoda komunikacije med večjezičnimi govorniki. Večina vsebin družbenih medijev večjezičnih družb je napisana v kodno mešanih besedilih. Vendar večina sedanjih prevajalskih sistemov zanemarja pretvorbo besedil z mešanimi kodami v standardni jezik. Večina uporabnikov napisanih kodno mešanih vsebin v družbenih omrežjih ostaja nepredelana zaradi nedostopnosti jezikovnih virov, kot je vzporedni korpus. Ta prispevek predlaga model nevralnega strojnega prevajanja (NMT) za prevajanje besedila sinhalsko-angleškega kodno mešanega besedila v sinhalski jezik. Zaradi omejenih virov, ki so na voljo za besedilo SECM (sinhala-angleško kodno mešano), je ustvarjen vzporedni korpus s stavki SECM in sinhalskimi stavki. Srilankana socialna omrežja vsebujejo besedila SECM pogosteje kot standardni jeziki. Model, predlagan za prevajanje besedila z mešanimi kodami, je kombinacija okvira Encoder-Dekoder z enotami LSTM in algoritmom prisilnih učiteljev. Prevedeni stavki iz modela so ovrednoteni z uporabo metrike BLEU (dvojezična evalvacija Understudy). Naš model je dosegel izjemno oceno BLEU za prevod.</abstract_sk>
      <abstract_ha>Code-Mixe ya kasance wata shirin mai tafiyar da wa'azi a tsakanin masu saurãre bakan mulki-harshe. Babu mafi yawan mitandai na jamii da ke cikin jamii masu yawa na rubũtu cikin littãfin da aka haɗa. Babu kasa, mafi yawansu na fassarar da ake kai yanzu sunã ƙyãma wa ya mayar da matsayin kodi-da-haɗe zuwa wata harshe na daidaita. Babu mafi yawan mãsu amfani da ya rubutu kodi-da-haɗi cikin mitandai da jamii ba za'a yi cẽto ba don a zartar da shi kamar ba'a iya sãmun resource na linguistic kamar nauyi. @ item license (short name) Dukan da aka ƙayyade marufau wanda ke iya amfani da wa kodi na Sinhala-Ingiriya-Mixed (SCM), an halitta nau'in rubutu mai daidaita da cewa na SCM da cewa Sinhala. QFontDatabase @ action: button An ƙaddara salon da aka fassara daga misalin ayuka, a ƙaddara metric mai amfani da BLEU(Bayan Taimar Bayaniya Babil da Fassarar). Tuduniyarmu ya sami wani na'urar BLEU mai girma wa fassarar.</abstract_ha>
      <abstract_bo>སྐད་རིགས་སྐད་ཀྱི་མིང་འདྲ་བ་གཉིས་ཀྱི་སྤོར་བའི་ཐབས་ལམ་ཞིག་ཆགས་ཡོད། སྐད་རིགས་སྤྱི་ཚོགས་ཀྱི་ནང་དོན་མང་ཆེ་ཤོས་ཡོད་པའི་སྤྱི་ཚོགས་འབྲེལ་མཐུད་དྲ་རྒྱའི་ནང་དུ་རྟགས་བཀལ་ཡོད ཡིན་ནའང་ད་ལྟོའི་ཡིག་སྣོད་ཀྱི་ཆེ་ཆུང་ནི་ཡིག སྤྱི་ཚོགས་འབྲེལ་མཐུད་དྲ་རྒྱ་ནང་གི་སྤྱོད་མཁན་གྱི་ཡིག ཤོག་བྱང་འདིས་རང་ཉིད་ཀྱི་མིའུ་རྩིས་འཁོར་གྱི་སྤྱི་ཚོགས་ཀྱི་རྣམ་པ་ཞིག་སྔོན་སྲོལ་འདུག་ སིན་ཧྲིལ་ཡིག སིན་ཧྲིལ་ཡིག Srilankan social media sites contain SECM texts more frequently than the standard languages. The model proposed for code-mixed text translation in this study is a combination of Encoder-Decoder framework with LSTM units and Teachers Forcing Algorithm. The translated sentences from the model are evaluated using BLEU(Bilingual Evaluation Understudy) metric. ང་ཚོའི་མ་དབྱིབས་ཡིག་ཆ་གསར་བརྗོད་ཀྱི་ཚད་ལྡན་ཡུལ་ཆེན་ཤིག་རྙེད་ཐུབ་ཀྱི་ཡོད།</abstract_bo>
      <abstract_jv>Where Banyak akèh sing paling media sothik kanggo sampeyan akeh langkung sampeyan sampeyan karo akeh basa. Nanging, akeh langkung sistem anyar terjamah-sistem sing bisa ngulinakake Piye soko perusahaan liyane dadi karo kode-karo hal-hal nang media sotiné kuwi ora bisa nguasai perusahaan karo hal-hal nganggep Perintah iki supoyo sistem sing model de Terjamahan (NMT), nggo terjamahan kelompok Singhal-Inggris barang kelompok karo Singhal. Tanggal nggo oleh sing perusahaan nggo kelas kanggo kelas nggo teks Singhal-Inggris kode-Mixed(SESM), sampeyan karo perusahaan kelas kotak dipunanggo kelas SECm karo peranggo kelas SECm karo peranggo kelas Singhal. Sampeyan hukum sing mengko hukum sing larang SESM, luwih akeh luwih apik sing luwih apik sing luwih sedhaya. model supoyo nggo kode-karbote teks terjamahan kanggo nggambar urip iki dadi karo konbine karo koder-Dekoder karo perusahaan LTT karo Pak guru nggawe Algorithm Validity Modelèn dhéwé wis ngerasakno sing paling dhéwé nang tarjamahan.</abstract_jv>
      </paper>
    <paper id="83">
      <title>Multilingual Multi-Domain NMT for <a href="https://en.wikipedia.org/wiki/Languages_of_India">Indian Languages</a><fixed-case>NMT</fixed-case> for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Sourav</first><last>Kumar</last></author>
      <author><first>Salil</first><last>Aggarwal</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>727–733</pages>
      <abstract>India is known as the land of many tongues and dialects. Neural machine translation (NMT) is the current state-of-the-art approach for machine translation (MT) but performs better only with large datasets which Indian languages usually lack, making this approach infeasible. So, in this paper, we address the problem of data scarcity by efficiently training multilingual and multilingual multi domain NMT systems involving languages of the  . We are proposing the technique for using the joint domain and language tags in a multilingual setup. We draw three major conclusions from our experiments : (i) Training a multilingual system via exploiting <a href="https://en.wikipedia.org/wiki/Lexical_similarity">lexical similarity</a> based on <a href="https://en.wikipedia.org/wiki/Language_family">language family</a> helps in achieving an overall average improvement of. over bilingual baselines, (ii) Technique of incorporating domain information into the language tokens helps multilingual multi-domain system in getting a significant average improvement of     over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>, (iii) Multistage fine-tuning further helps in getting an improvement of -. for the language pair of interest.</abstract>
      <url hash="7ffffbd3">2021.ranlp-1.83</url>
      <bibkey>kumar-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="84">
      <title>Fiction in <a href="https://en.wikipedia.org/wiki/Russian_language">Russian Translation</a> : A Translationese Study<fixed-case>R</fixed-case>ussian Translation: A Translationese Study</title>
      <author><first>Maria</first><last>Kunilovskaya</last></author>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>734–743</pages>
      <abstract>This paper presents a translationese study based on the parallel data from the Russian National Corpus (RNC). We explored differences between literary texts originally authored in <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a> and fiction translated into <a href="https://en.wikipedia.org/wiki/Russian_language">Russian</a> from 11 languages. The texts are represented with frequency-based features that capture structural and lexical properties of language. Binary classification results indicate that literary translations can be distinguished from non-translations with an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> ranging from 82 to 92 % depending on the source language and feature set. Multiclass classification confirms that translations from distant languages are more distinct from non-translations than translations from languages that are typologically close to Russian. It also demonstrates that translations from same-family source languages share translationese properties. Structural features return more consistent results than <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> relying on external resources and capturing lexical properties of texts in both translationese detection and source language identification tasks.</abstract>
      <url hash="c36dff5b">2021.ranlp-1.84</url>
      <bibkey>kunilovskaya-etal-2021-fiction</bibkey>
    </paper>
    <paper id="86">
      <title>Sentiment Analysis in Code-Mixed Telugu-English Text with Unsupervised Data Normalization<fixed-case>T</fixed-case>elugu-<fixed-case>E</fixed-case>nglish Text with Unsupervised Data Normalization</title>
      <author><first>Siva Subrahamanyam Varma</first><last>Kusampudi</last></author>
      <author><first>Preetham</first><last>Sathineni</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>753–760</pages>
      <abstract>In a multilingual society, people communicate in more than one language, leading to Code-Mixed data. Sentimental analysis on Code-Mixed Telugu-English Text (CMTET) poses unique challenges. The unstructured nature of the Code-Mixed Data is due to the <a href="https://en.wikipedia.org/wiki/Informal_language">informal language</a>, <a href="https://en.wikipedia.org/wiki/Transliteration">informal transliterations</a>, and spelling errors. In this paper, we introduce an annotated dataset for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> in CMTET. Also, we report an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 80.22 % on this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> using novel <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised data normalization</a> with a Multilayer Perceptron (MLP) model. This proposed data normalization technique can be extended to any NLP task involving CMTET. Further, we report an increase of 2.53 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> due to this data normalization approach in our best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>.</abstract>
      <url hash="9bfe2ea8">2021.ranlp-1.86</url>
      <bibkey>kusampudi-etal-2021-sentiment</bibkey>
    </paper>
    <paper id="88">
      <title>Making Your Tweets More Fancy : Emoji Insertion to Texts</title>
      <author><first>Jingun</first><last>Kwon</last></author>
      <author><first>Naoki</first><last>Kobayashi</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>770–779</pages>
      <abstract>In the <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, users frequently use small images called <a href="https://en.wikipedia.org/wiki/Emoji">emojis</a> in their posts. Although using <a href="https://en.wikipedia.org/wiki/Emoji">emojis</a> in texts plays a key role in recent communication systems, less attention has been paid on their positions in the given texts, despite that users carefully choose and put an <a href="https://en.wikipedia.org/wiki/Emoji">emoji</a> that matches their post. Exploring positions of <a href="https://en.wikipedia.org/wiki/Emoji">emojis</a> in texts will enhance understanding of the relationship between <a href="https://en.wikipedia.org/wiki/Emoji">emojis</a> and texts. We extend an emoji label prediction task taking into account the information of emoji positions, by jointly learning the emoji position in a tweet to predict the emoji label. The results demonstrate that the position of <a href="https://en.wikipedia.org/wiki/Emoji">emojis</a> in texts is a good clue to boost the performance of emoji label prediction. Human evaluation validates that there exists a suitable emoji position in a tweet, and our proposed task is able to make tweets more fancy and natural. In addition, considering emoji position can further improve the performance for the irony detection task compared to the emoji label prediction. We also report the experimental results for the modified <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, due to the problem of the original <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for the first shared task to predict an <a href="https://en.wikipedia.org/wiki/Emoji">emoji label</a> in SemEval2018.</abstract>
      <url hash="7b90d5ba">2021.ranlp-1.88</url>
      <bibkey>kwon-etal-2021-making</bibkey>
    </paper>
    <paper id="89">
      <title>Addressing Slot-Value Changes in Task-oriented Dialogue Systems through Dialogue Domain Adaptation</title>
      <author><first>Tiziano</first><last>Labruna</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <pages>780–789</pages>
      <abstract>Recent task-oriented dialogue systems learn a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> from annotated dialogues, and such dialogues are in turn collected and annotated so that they are consistent with certain <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a>. However, in real scenarios, <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a> is subject to frequent changes, and initial training dialogues may soon become obsolete, resulting in a significant decrease in the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance. In this paper, we investigate the relationship between training dialogues and <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a>, and propose Dialogue Domain Adaptation, a methodology aiming at adapting initial training dialogues to changes intervened in the <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a>. We focus on slot-value changes (e.g., when new slot values are available to describe domain entities) and define an experimental setting for dialogue domain adaptation. First, we show that current state-of-the-art models for dialogue state tracking are still poorly robust to slot-value changes of the domain knowledge. Then, we compare different domain adaptation strategies, showing that simple techniques are effective to reduce the gap between training dialogues and <a href="https://en.wikipedia.org/wiki/Domain_knowledge">domain knowledge</a>.</abstract>
      <url hash="5bb8fd9f">2021.ranlp-1.89</url>
      <bibkey>labruna-magnini-2021-addressing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="90">
      <title>Developing a Clinical Language Model for <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a> : Continued Pretraining of Generic BERT with In-Domain Data<fixed-case>S</fixed-case>wedish: Continued Pretraining of Generic <fixed-case>BERT</fixed-case> with In-Domain Data</title>
      <author><first>Anastasios</first><last>Lamproudis</last></author>
      <author><first>Aron</first><last>Henriksson</last></author>
      <author><first>Hercules</first><last>Dalianis</last></author>
      <pages>790–797</pages>
      <abstract>The use of pretrained language models, fine-tuned to perform a specific downstream task, has become widespread in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. Using a generic language model in specialized domains may, however, be sub-optimal due to differences in language use and vocabulary. In this paper, it is investigated whether an existing, generic language model for <a href="https://en.wikipedia.org/wiki/Swedish_language">Swedish</a> can be improved for the clinical domain through continued pretraining with clinical text. The generic and domain-specific language models are fine-tuned and evaluated on three representative clinical NLP tasks : (i) identifying protected health information, (ii) assigning ICD-10 diagnosis codes to discharge summaries, and (iii) sentence-level uncertainty prediction. The results show that continued pretraining on in-domain data leads to improved performance on all three downstream tasks, indicating that there is a potential added value of domain-specific language models for clinical NLP.</abstract>
      <url hash="1f043155">2021.ranlp-1.90</url>
      <bibkey>lamproudis-etal-2021-developing</bibkey>
    </paper>
    <paper id="93">
      <title>Frustration Level Annotation in Latvian Tweets with Non-Lexical Means of Expression<fixed-case>L</fixed-case>atvian Tweets with Non-Lexical Means of Expression</title>
      <author><first>Viktorija</first><last>Leonova</last></author>
      <author><first>Janis</first><last>Zuters</last></author>
      <pages>814–823</pages>
      <abstract>We present a neural-network-driven model for annotating frustration intensity in customer support tweets, based on representing tweet texts using a bag-of-words encoding after processing with subword segmentation together with non-lexical features. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> was evaluated on tweets in English and Latvian languages, focusing on aspects beyond the pure <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words representations</a> used in previous research. The experimental results show that the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> can be successfully applied for texts in a non-English language, and that adding non-lexical features to tweet representations significantly improves performance, while subword segmentation has a moderate but positive effect on <a href="https://en.wikipedia.org/wiki/Statistical_model">model accuracy</a>. Our code and training data are publicly available.</abstract>
      <url hash="6de72a35">2021.ranlp-1.93</url>
      <bibkey>leonova-zuters-2021-frustration</bibkey>
    </paper>
    <paper id="94">
      <title>System Combination for Grammatical Error Correction Based on Integer Programming</title>
      <author><first>Ruixi</first><last>Lin</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <pages>824–829</pages>
      <abstract>In this paper, we propose a system combination method for grammatical error correction (GEC), based on nonlinear integer programming (IP). Our method optimizes a novel F score objective based on error types, and combines multiple end-to-end GEC systems. The proposed IP approach optimizes the selection of a single best system for each grammatical error type present in the data. Experiments of the IP approach on combining state-of-the-art standalone GEC systems show that the combined <a href="https://en.wikipedia.org/wiki/System">system</a> outperforms all standalone systems. It improves <a href="https://en.wikipedia.org/wiki/F-number">F0.5 score</a> by 3.61 % when combining the two best participating systems in the BEA 2019 shared task, and achieves <a href="https://en.wikipedia.org/wiki/F-number">F0.5 score</a> of 73.08 %. We also perform experiments to compare our IP approach with another state-of-the-art system combination method for GEC, demonstrating IP’s competitive combination capability.</abstract>
      <url hash="228f77b1">2021.ranlp-1.94</url>
      <bibkey>lin-ng-2021-system</bibkey>
      <pwccode url="https://github.com/nusnlp/gec_ip" additional="false">nusnlp/gec_ip</pwccode>
    </paper>
    <paper id="96">
      <title>Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues Using BERT<fixed-case>BERT</fixed-case></title>
      <author><first>Ye</first><last>Liu</last></author>
      <author><first>Wolfgang</first><last>Maier</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <author><first>Stefan</first><last>Ultes</last></author>
      <pages>839–845</pages>
      <abstract>This paper presents an automatic method to evaluate the naturalness of <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation</a> in <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue systems</a>. While this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> was previously rendered through expensive and time-consuming human labor, we present this novel task of automatic naturalness evaluation of generated language. By fine-tuning the BERT model, our proposed naturalness evaluation method shows robust results and outperforms the baselines : support vector machines, bi-directional LSTMs, and BLEURT. In addition, the training speed and evaluation performance of naturalness model are improved by <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> from quality and informativeness linguistic knowledge.</abstract>
      <url hash="e059c241">2021.ranlp-1.96</url>
      <bibkey>liu-etal-2021-naturalness</bibkey>
      <revision id="1" href="2021.ranlp-1.96v1" hash="e10078ae" />
      <revision id="2" href="2021.ranlp-1.96v2" hash="e059c241" date="2021-12-02">Revised author emails</revision>
    </paper>
    <paper id="101">
      <title>Active Learning for Interactive Relation Extraction in a French Newspaper’s Articles<fixed-case>F</fixed-case>rench Newspaper’s Articles</title>
      <author><first>Cyrielle</first><last>Mallart</last></author>
      <author><first>Michel</first><last>Le Nouy</last></author>
      <author><first>Guillaume</first><last>Gravier</last></author>
      <author><first>Pascale</first><last>Sébillot</last></author>
      <pages>886–894</pages>
      <abstract>Relation extraction is a subtask of natural langage processing that has seen many improvements in recent years, with the advent of complex pre-trained architectures. Many of these state-of-the-art approaches are tested against <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmarks</a> with labelled sentences containing tagged entities, and require important pre-training and fine-tuning on task-specific data. However, in a real use-case scenario such as in a newspaper company mostly dedicated to local information, <a href="https://en.wikipedia.org/wiki/Binary_relation">relations</a> are of varied, highly specific type, with virtually no annotated data for such <a href="https://en.wikipedia.org/wiki/Binary_relation">relations</a>, and many entities co-occur in a sentence without being related. We question the use of supervised state-of-the-art models in such a context, where resources such as time, <a href="https://en.wikipedia.org/wiki/Computer_performance">computing power</a> and <a href="https://en.wikipedia.org/wiki/Annotation">human annotators</a> are limited. To adapt to these constraints, we experiment with an active-learning based relation extraction pipeline, consisting of a binary LSTM-based lightweight model for detecting the relations that do exist, and a state-of-the-art model for relation classification. We compare several choices for classification models in this scenario, from basic word embedding averaging, to graph neural networks and Bert-based ones, as well as several active learning acquisition strategies, in order to find the most cost-efficient yet accurate approach in our French largest daily newspaper company’s use case.</abstract>
      <url hash="2d5a218b">2021.ranlp-1.101</url>
      <bibkey>mallart-etal-2021-active</bibkey>
    </paper>
    <paper id="102">
      <title>ROFF-A Romanian Twitter Dataset for Offensive Language<fixed-case>ROFF</fixed-case> - A <fixed-case>R</fixed-case>omanian <fixed-case>T</fixed-case>witter Dataset for Offensive Language</title>
      <author><first>Mihai</first><last>Manolescu</last></author>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <pages>895–900</pages>
      <abstract>This paper describes the annotation process of an offensive language data set for Romanian on social media. To facilitate comparable multi-lingual research on <a href="https://en.wikipedia.org/wiki/Offensive_language">offensive language</a>, the annotation guidelines follow some of the recent annotation efforts for other languages. The final <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> contains 5000 micro-blogging posts annotated by a large number of volunteer annotators. The <a href="https://en.wikipedia.org/wiki/Inter-annotator_agreement">inter-annotator agreement</a> and the initial automatic discrimination results we present are in line with earlier annotation efforts.</abstract>
      <url hash="85427b26">2021.ranlp-1.102</url>
      <bibkey>manolescu-coltekin-2021-roff</bibkey>
    </paper>
    <paper id="103">
      <title>Monitoring Fact Preservation, Grammatical Consistency and Ethical Behavior of Abstractive Summarization Neural Models</title>
      <author><first>Iva</first><last>Marinova</last></author>
      <author><first>Yolina</first><last>Petrova</last></author>
      <author><first>Milena</first><last>Slavcheva</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <author><first>Ivaylo</first><last>Radev</last></author>
      <author><first>Kiril</first><last>Simov</last></author>
      <pages>901–909</pages>
      <abstract>The paper describes a system for <a href="https://en.wikipedia.org/wiki/Automatic_summarization">automatic summarization</a> in English language of online news data that come from different non-English languages. The <a href="https://en.wikipedia.org/wiki/System">system</a> is designed to be used in production environment for <a href="https://en.wikipedia.org/wiki/Media_monitoring">media monitoring</a>. Automatic summarization can be very helpful in this domain when applied as a helper tool for journalists so that they can review just the important information from the <a href="https://en.wikipedia.org/wiki/News_broadcasting">news channels</a>. However, like every software solution, the <a href="https://en.wikipedia.org/wiki/Automatic_summarization">automatic summarization</a> needs performance monitoring and assured safe environment for the clients. In media monitoring environment the most problematic features to be addressed are : the <a href="https://en.wikipedia.org/wiki/Copyright">copyright issues</a>, the factual consistency, the style of the text and the <a href="https://en.wikipedia.org/wiki/Ethics">ethical norms</a> in <a href="https://en.wikipedia.org/wiki/Journalism">journalism</a>. Thus, the main contribution of our present work is that the above mentioned characteristics are successfully monitored in neural automatic summarization models and improved with the help of validation, fact-preserving and fact-checking procedures.</abstract>
      <url hash="3b8dfd1d">2021.ranlp-1.103</url>
      <bibkey>marinova-etal-2021-monitoring</bibkey>
    </paper>
    <paper id="116">
      <title>Improving Distantly Supervised Relation Extraction with Self-Ensemble Noise Filtering</title>
      <author><first>Tapas</first><last>Nayak</last></author>
      <author><first>Navonil</first><last>Majumder</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>1031–1039</pages>
      <abstract>Distantly supervised models are very popular for <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a> since we can obtain a large amount of training data using the distant supervision method without human annotation. In distant supervision, a sentence is considered as a source of a tuple if the sentence contains both entities of the tuple. However, this condition is too permissive and does not guarantee the presence of relevant relation-specific information in the sentence. As such, distantly supervised training data contains much <a href="https://en.wikipedia.org/wiki/Noise">noise</a> which adversely affects the performance of the <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>. In this paper, we propose a self-ensemble filtering mechanism to filter out the noisy samples during the training process. We evaluate our proposed <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a> on the New York Times dataset which is obtained via distant supervision. Our experiments with multiple state-of-the-art neural relation extraction models show that our proposed filtering mechanism improves the robustness of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> and increases their F1 scores.</abstract>
      <url hash="698ef6ca">2021.ranlp-1.116</url>
      <bibkey>nayak-etal-2021-improving</bibkey>
      <pwccode url="https://github.com/nayakt/SENF4DSRE" additional="false">nayakt/SENF4DSRE</pwccode>
    </paper>
    <paper id="119">
      <title>Transfer-based Enrichment of a Hungarian Named Entity Dataset<fixed-case>H</fixed-case>ungarian Named Entity Dataset</title>
      <author><first>Attila</first><last>Novák</last></author>
      <author><first>Borbála</first><last>Novák</last></author>
      <pages>1060–1067</pages>
      <abstract>In this paper, we present a major update to the first Hungarian named entity dataset, the Szeged NER corpus. We used zero-shot cross-lingual transfer to initialize the enrichment of entity types annotated in the corpus using three neural NER models : two of them based on the English OntoNotes corpus and one based on the Czech Named Entity Corpus finetuned from multilingual neural language models. The output of the <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> was automatically merged with the original NER annotation, and automatically and manually corrected and further enriched with additional <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a>, like qualifiers for various entity types. We present the evaluation of the zero-shot performance of the two OntoNotes-based models and a transformer-based new NER model trained on the training part of the final corpus. We release the corpus and the trained <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>.</abstract>
      <url hash="14c8d1e1">2021.ranlp-1.119</url>
      <bibkey>novak-novak-2021-transfer</bibkey>
    </paper>
    <paper id="132">
      <title>A Call for Clarity in Contemporary Authorship Attribution Evaluation</title>
      <author><first>Allen</first><last>Riddell</last></author>
      <author><first>Haining</first><last>Wang</last></author>
      <author><first>Patrick</first><last>Juola</last></author>
      <pages>1174–1179</pages>
      <abstract>Recent research has documented that results reported in frequently-cited authorship attribution papers are difficult to reproduce. Inaccessible code and data are often proposed as factors which block successful reproductions. Even when original materials are available, problems remain which prevent researchers from comparing the effectiveness of different <a href="https://en.wikipedia.org/wiki/Methodology">methods</a>. To solve the remaining problemsthe lack of fixed test sets and the use of inappropriately homogeneous corporaour paper contributes materials for five closed-set authorship identification experiments. The five experiments feature texts from 106 distinct authors. Experiments involve a range of contemporary non-fiction American English prose. These experiments provide the foundation for comparable and reproducible authorship attribution research involving contemporary writing.</abstract>
      <url hash="a2649043">2021.ranlp-1.132</url>
      <bibkey>riddell-etal-2021-call</bibkey>
    </paper>
    <paper id="133">
      <title>Varieties of Plain Language</title>
      <author><first>Allen</first><last>Riddell</last></author>
      <author><first>Yohei</first><last>Igarashi</last></author>
      <pages>1180–1187</pages>
      <abstract>Many organizations seek or need to produce documents that are written plainly. In the United States, the Plain Writing Act of 2010 requires that many federal agencies’ documents for the public are written in <a href="https://en.wikipedia.org/wiki/Plain_English">plain English</a>. In particular, the government’s Plain Language Action and Information Network (PLAIN) recommends that writers use short sentences and everyday words, as does the Securities and Exchange Commission’s Plain English Rule. Since the 1970s, American plain language advocates have moved away from <a href="https://en.wikipedia.org/wiki/Readability">readability measures</a> and favored <a href="https://en.wikipedia.org/wiki/Usability_testing">usability testing</a> and document design considerations. But in this paper we use quantitative measures of <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence length</a> and word difficulty that (1) reveal stylistic variation among PLAIN’s exemplars of plain writing, and (2) help us position PLAIN’s exemplars relative to documents written in other kinds of <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">accessible English</a> (e.g., The <a href="https://en.wikipedia.org/wiki/The_New_York_Times">New York Times</a>, Voice of America Special English, and Wikipedia) and one academic document likely to be perceived as difficult. Uncombined measures for sentences and vocabularyleft separate, unlike in traditional readability formulascan complement <a href="https://en.wikipedia.org/wiki/Usability_testing">usability testing</a> and document design considerations, and advance knowledge about different types of plainer English.</abstract>
      <url hash="a06c120d">2021.ranlp-1.133</url>
      <bibkey>riddell-igarashi-2021-varieties</bibkey>
    </paper>
    <paper id="134">
      <title>Word Discriminations for Vocabulary Inventory Prediction</title>
      <author><first>Frankie</first><last>Robertson</last></author>
      <pages>1188–1195</pages>
      <abstract>The aim of vocabulary inventory prediction is to predict a learner’s whole vocabulary based on a limited sample of query words. This paper approaches the problem starting from the 2-parameter Item Response Theory (IRT) model, giving each word in the vocabulary a difficulty and discrimination parameter. The discrimination parameter is evaluated on the sub-problem of question item selection, familiar from the fields of Computerised Adaptive Testing (CAT) and <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a>. Next, the effect of the discrimination parameter on <a href="https://en.wikipedia.org/wiki/Prediction">prediction</a> performance is examined, both in a binary classification setting, and in an information retrieval setting. Performance is compared with <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> based on <a href="https://en.wikipedia.org/wiki/Word_frequency">word frequency</a>. A number of different generalisation scenarios are examined, including generalising word difficulty and <a href="https://en.wikipedia.org/wiki/Discrimination">discrimination</a> using <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> with a predictor network and testing on out-of-dataset data.</abstract>
      <url hash="512a125b">2021.ranlp-1.134</url>
      <bibkey>robertson-2021-word</bibkey>
      <pwccode url="https://github.com/frankier/vocabirt" additional="false">frankier/vocabirt</pwccode>
    </paper>
    <paper id="135">
      <title>FrenLyS : A Tool for the Automatic Simplification of French General Language Texts<fixed-case>F</fixed-case>ren<fixed-case>L</fixed-case>y<fixed-case>S</fixed-case>: A Tool for the Automatic Simplification of <fixed-case>F</fixed-case>rench General Language Texts</title>
      <author><first>Eva</first><last>Rolin</last></author>
      <author><first>Quentin</first><last>Langlois</last></author>
      <author><first>Patrick</first><last>Watrin</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <pages>1196–1205</pages>
      <abstract>Lexical simplification (LS) aims at replacing words considered complex in a sentence by simpler equivalents. In this paper, we present the first automatic LS service for <a href="https://en.wikipedia.org/wiki/French_language">French</a>, FrenLys, which offers different techniques to generate, select and rank substitutes. The paper describes the different <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> proposed by our <a href="https://en.wikipedia.org/wiki/Tool">tool</a>, which includes both classical approaches (e.g. generation of candidates from <a href="https://en.wikipedia.org/wiki/Lexical_resource">lexical resources</a>, <a href="https://en.wikipedia.org/wiki/Frequency_filter">frequency filter</a>, etc.) and more innovative approaches such as the exploitation of CamemBERT, a model for <a href="https://en.wikipedia.org/wiki/French_language">French</a> based on the RoBERTa architecture. To evaluate the different <a href="https://en.wikipedia.org/wiki/Methodology">methods</a>, a new evaluation dataset for <a href="https://en.wikipedia.org/wiki/French_language">French</a> is introduced.</abstract>
      <url hash="9b946155">2021.ranlp-1.135</url>
      <bibkey>rolin-etal-2021-frenlys</bibkey>
    </paper>
    <paper id="137">
      <title>Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems<fixed-case>SAM</fixed-case>) for Evaluating Sentiment Transfer by Machine Translation Systems</title>
      <author><first>Hadeel</first><last>Saadany</last></author>
      <author><first>Constantin</first><last>Orăsan</last></author>
      <author><first>Emad</first><last>Mohamed</last></author>
      <author><first>Ashraf</first><last>Tantavy</last></author>
      <pages>1217–1226</pages>
      <abstract>In translating text where <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiment</a> is the main message, human translators give particular attention to sentiment-carrying words. The reason is that an incorrect translation of such words would miss the fundamental aspect of the source text, i.e. the author’s sentiment. In the online world, MT systems are extensively used to translate User-Generated Content (UGC) such as <a href="https://en.wikipedia.org/wiki/Review">reviews</a>, <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a>, and <a href="https://en.wikipedia.org/wiki/Social_media">social media posts</a>, where the main message is often the author’s positive or negative attitude towards the topic of the text. It is important in such scenarios to accurately measure how far an MT system can be a reliable real-life utility in transferring the correct affect message. This paper tackles an under-recognized problem in the field of machine translation evaluation which is judging to what extent automatic metrics concur with the gold standard of human evaluation for a correct translation of sentiment. We evaluate the efficacy of conventional quality metrics in spotting a mistranslation of sentiment, especially when it is the sole error in the MT output. We propose a numerical sentiment-closeness measure appropriate for assessing the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of a translated affect message in UGC text by an MT system. We will show that incorporating this sentiment-aware measure can significantly enhance the correlation of some available quality metrics with the human judgement of an accurate translation of sentiment.</abstract>
      <url hash="30b66b8b">2021.ranlp-1.137</url>
      <bibkey>saadany-etal-2021-sentiment</bibkey>
    </paper>
    <paper id="138">
      <title>Multilingual Epidemic Event Extraction : From Simple Classification Methods to Open Information Extraction (OIE) and Ontology<fixed-case>OIE</fixed-case>) and Ontology</title>
      <author><first>Sihem</first><last>Sahnoun</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <pages>1227–1233</pages>
      <abstract>There is an incredible amount of information available in the form of <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">textual documents</a> due to the growth of <a href="https://en.wikipedia.org/wiki/Source_text">information sources</a>. In order to get the information into an actionable way, it is common to use <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a> and more specifically the <a href="https://en.wikipedia.org/wiki/Event_extraction">event extraction</a>, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> became crucial in various domains even in public health. In this paper, we address the problem of the epidemic event extraction in potentially any language, so that we tested different corpuses on an existed multilingual system for tele-epidemiology : the Data Analysis for Information Extraction in any Language(DANIEL) system. We focused on the influence of the number of documents on the performance of the <a href="https://en.wikipedia.org/wiki/System">system</a>, on average results show that it is able to achieve a <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a> and <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> around 82 %, but when we resorted to the evaluation by event by checking whether it has been really detected or not, the results are not satisfactory according to this paper’s evaluation. Our idea is to propose a system that uses an <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a> which includes information in different languages and covers specific epidemiological concepts, it is also based on the multilingual open information extraction for the relation extraction step to reduce the expert intervention and to restrict the content for each text. We describe a methodology of five main stages : Pre-processing, relation extraction, named entity recognition (NER), event recognition and the matching between the information extracted and the <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a>.</abstract>
      <url hash="97082ded">2021.ranlp-1.138</url>
      <bibkey>sahnoun-lejeune-2021-multilingual</bibkey>
    </paper>
    <paper id="139">
      <title>Exploiting Domain-Specific Knowledge for Judgment Prediction Is No Panacea</title>
      <author><first>Olivier</first><last>Salaün</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>Karim</first><last>Benyekhlef</last></author>
      <pages>1234–1243</pages>
      <abstract>Legal judgment prediction (LJP) usually consists in a text classification task aimed at predicting the verdict on the basis of the fact description. The literature shows that the use of <a href="https://en.wikipedia.org/wiki/Article_(publishing)">articles</a> as input <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> helps improve the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> performance. In this work, we designed a verdict prediction task based on landlord-tenant disputes and we applied BERT-based models to which we fed different article-based features. Although the results obtained are consistent with the literature, the improvements with the articles are mostly obtained with the most frequent labels, suggesting that pre-trained and fine-tuned transformer-based models are not scalable as is for legal reasoning in real life scenarios as they would only excel in accurately predicting the most recurrent verdicts to the detriment of other legal outcomes.</abstract>
      <url hash="9df33d9e">2021.ranlp-1.139</url>
      <bibkey>salaun-etal-2021-exploiting</bibkey>
    </paper>
    <paper id="142">
      <title>A <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">Semi-Supervised Approach</a> to Detect Toxic Comments</title>
      <author><first>Ghivvago Damas</first><last>Saraiva</last></author>
      <author><first>Rafael</first><last>Anchiêta</last></author>
      <author><first>Francisco Assis Ricarte</first><last>Neto</last></author>
      <author><first>Raimundo</first><last>Moura</last></author>
      <pages>1261–1267</pages>
      <abstract>Toxic comments contain forms of non-acceptable language targeted towards groups or individuals. These types of comments become a serious concern for government organizations, <a href="https://en.wikipedia.org/wiki/Online_community">online communities</a>, and <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a>. Although there are some approaches to handle non-acceptable language, most of them focus on <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> and the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>. In this paper, we deal with toxic comment detection as a <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised strategy</a> over a heterogeneous graph. We evaluate the approach on a toxic dataset of the <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese language</a>, outperforming several graph-based methods and achieving competitive results compared to transformer architectures.</abstract>
      <url hash="3954ad1d">2021.ranlp-1.142</url>
      <bibkey>saraiva-etal-2021-semi</bibkey>
      <pwccode url="https://github.com/rafaelanchieta/toxic" additional="false">rafaelanchieta/toxic</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/told-br">ToLD-Br</pwcdataset>
    </paper>
    <paper id="147">
      <title>A Domain-Independent Holistic Approach to Deception Detection</title>
      <author><first>Sadat</first><last>Shahriar</last></author>
      <author><first>Arjun</first><last>Mukherjee</last></author>
      <author><first>Omprakash</first><last>Gnawali</last></author>
      <pages>1308–1317</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Deception">deception</a> in the text can be of different forms in different domains, including <a href="https://en.wikipedia.org/wiki/Fake_news">fake news</a>, rumor tweets, and <a href="https://en.wikipedia.org/wiki/Email_spam">spam emails</a>. Irrespective of the domain, the main intent of the deceptive text is to deceit the reader. Although domain-specific deception detection exists, domain-independent deception detection can provide a holistic picture, which can be crucial to understand how <a href="https://en.wikipedia.org/wiki/Deception">deception</a> occurs in the text. In this paper, we detect <a href="https://en.wikipedia.org/wiki/Deception">deception</a> in a domain-independent setting using deep learning architectures. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms the State-of-the-Art performance of most benchmark datasets with an overall <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 93.42 % and F1-Score of 93.22 %. The domain-independent training allows us to capture subtler nuances of deceptive writing style. Furthermore, we analyze how much in-domain data may be helpful to accurately detect <a href="https://en.wikipedia.org/wiki/Deception">deception</a>, especially for the cases where data may not be readily available to train. Our results and analysis indicate that there may be a universal pattern of <a href="https://en.wikipedia.org/wiki/Deception">deception</a> lying in-between the text independent of the domain, which can create a novel area of research and open up new avenues in the field of <a href="https://en.wikipedia.org/wiki/Deception">deception detection</a>.</abstract>
      <url hash="d366f906">2021.ranlp-1.147</url>
      <bibkey>shahriar-etal-2021-domain</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fakenewsnet">FakeNewsNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/liar">LIAR</pwcdataset>
    </paper>
    <paper id="148">
      <title>Towards Domain-Generalizable Paraphrase Identification by Avoiding the Shortcut Learning</title>
      <author><first>Xin</first><last>Shen</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>1318–1325</pages>
      <abstract>In this paper, we investigate the Domain Generalization (DG) problem for supervised Paraphrase Identification (PI). We observe that the performance of existing PI models deteriorates dramatically when tested in an out-of-distribution (OOD) domain. We conjecture that it is caused by shortcut learning, i.e., these <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> tend to utilize the cue words that are unique for a particular dataset or domain. To alleviate this issue and enhance the DG ability, we propose a PI framework based on Optimal Transport (OT). Our method forces the <a href="https://en.wikipedia.org/wiki/Computer_network">network</a> to learn the necessary <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> for all the words in the input, which alleviates the shortcut learning problem. Experimental results show that our method improves the DG ability for the PI models.</abstract>
      <url hash="587cec72">2021.ranlp-1.148</url>
      <bibkey>shen-lam-2021-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/parade">PARADE</pwcdataset>
    </paper>
    <paper id="152">
      <title>How to Obtain Reliable Labels for MBTI Classification from Texts?<fixed-case>MBTI</fixed-case> Classification from Texts?</title>
      <author><first>Sanja</first><last>Stajner</last></author>
      <author><first>Seren</first><last>Yenikent</last></author>
      <pages>1360–1368</pages>
      <abstract>Automatic detection of the Myers-Briggs Type Indicator (MBTI) from short posts attracted noticeable attention in the last few years. Recent studies showed that this is quite a difficult <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, especially on commonly used <a href="https://en.wikipedia.org/wiki/Twitter">Twitter data</a>. Obtaining MBTI labels is also difficult, as human annotation requires trained psychologists, and automatic way of obtaining them is through long questionnaires of questionable usability for the task. In this paper, we present a method for collecting reliable MBTI labels via only four carefully selected questions that can be applied to any type of textual data.</abstract>
      <url hash="1041f3c2">2021.ranlp-1.152</url>
      <bibkey>stajner-yenikent-2021-obtain</bibkey>
    </paper>
    <paper id="156">
      <title>Does BERT Understand <a href="https://en.wikipedia.org/wiki/Idiom">Idioms</a>? A Probing-Based Empirical Study of BERT Encodings of Idioms<fixed-case>BERT</fixed-case> Understand Idioms? A Probing-Based Empirical Study of <fixed-case>BERT</fixed-case> Encodings of Idioms</title>
      <author><first>Minghuan</first><last>Tan</last></author>
      <author><first>Jing</first><last>Jiang</last></author>
      <pages>1397–1407</pages>
      <abstract>Understanding idioms is important in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. In this paper, we study to what extent pre-trained BERT model can encode the meaning of a potentially idiomatic expression (PIE) in a certain context. We make use of a few existing <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and perform two probing tasks : PIE usage classification and idiom paraphrase identification. Our experiment results suggest that BERT indeed can separate the literal and idiomatic usages of a <a href="https://en.wikipedia.org/wiki/Proto-Indo-European_language">PIE</a> with high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>. It is also able to encode the <a href="https://en.wikipedia.org/wiki/Idiom_(language_structure)">idiomatic meaning</a> of a <a href="https://en.wikipedia.org/wiki/Proto-Indo-European_language">PIE</a> to some extent.</abstract>
      <url hash="2f9b144b">2021.ranlp-1.156</url>
      <bibkey>tan-jiang-2021-bert</bibkey>
    </paper>
    <paper id="158">
      <title>TR-SEQ : Named Entity Recognition Dataset for Turkish Search Engine Queries<fixed-case>TR</fixed-case>-<fixed-case>SEQ</fixed-case>: Named Entity Recognition Dataset for <fixed-case>T</fixed-case>urkish Search Engine Queries</title>
      <author><first>Berkay</first><last>Topçu</last></author>
      <author><first>İlknur</first><last>Durgar El-Kahlout</last></author>
      <pages>1417–1422</pages>
      <abstract>Recognizing named entities in short search engine queries is a difficult task due to their weaker <a href="https://en.wikipedia.org/wiki/Context_(language_use)">contextual information</a> compared to <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">long sentences</a>. Standard named entity recognition (NER) systems that are trained on grammatically correct and long sentences fail to perform well on such queries. In this study, we share our efforts towards creating a cleaned and labeled dataset of real Turkish search engine queries (TR-SEQ) and introduce an extended label set to satisfy the search engine needs. A NER system is trained by applying the state-of-the-art <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning method</a> BERT to the collected data and its high performance on <a href="https://en.wikipedia.org/wiki/Web_search_engine">search engine queries</a> is reported. Moreover, we compare our results with the state-of-the-art Turkish NER systems.</abstract>
      <url hash="c4f464ce">2021.ranlp-1.158</url>
      <bibkey>topcu-durgar-el-kahlout-2021-tr</bibkey>
    </paper>
    <paper id="161">
      <title>Contextual-Lexicon Approach for Abusive Language Detection</title>
      <author><first>Francielle</first><last>Vargas</last></author>
      <author><first>Fabiana</first><last>Rodrigues de Góes</last></author>
      <author><first>Isabelle</first><last>Carvalho</last></author>
      <author><first>Fabrício</first><last>Benevenuto</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>1438–1447</pages>
      <abstract>Since a lexicon-based approach is more elegant scientifically, explaining the solution components and being easier to generalize to other applications, this paper provides a new approach for offensive language and hate speech detection on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, which embodies a lexicon of implicit and explicit offensive and swearing expressions annotated with contextual information. Due to the severity of the social media abusive comments in <a href="https://en.wikipedia.org/wiki/Brazil">Brazil</a>, and the lack of research in <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese</a>, <a href="https://en.wikipedia.org/wiki/Brazilian_Portuguese">Brazilian Portuguese</a> is the language used to validate the models. Nevertheless, our method may be applied to any other language. The conducted experiments show the effectiveness of the proposed approach, outperforming the current baseline methods for the <a href="https://en.wikipedia.org/wiki/Portuguese_language">Portuguese language</a>.</abstract>
      <url hash="038bbcae">2021.ranlp-1.161</url>
      <bibkey>vargas-etal-2021-contextual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech-and-offensive-language">Hate Speech and Offensive Language</pwcdataset>
    </paper>
    <paper id="162">
      <title>Comparative Analysis of Fine-tuned Deep Learning Language Models for ICD-10 Classification Task for Bulgarian Language<fixed-case>ICD</fixed-case>-10 Classification Task for <fixed-case>B</fixed-case>ulgarian Language</title>
      <author><first>Boris</first><last>Velichkov</last></author>
      <author><first>Sylvia</first><last>Vassileva</last></author>
      <author><first>Simeon</first><last>Gerginov</last></author>
      <author><first>Boris</first><last>Kraychev</last></author>
      <author><first>Ivaylo</first><last>Ivanov</last></author>
      <author><first>Philip</first><last>Ivanov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Svetla</first><last>Boytcheva</last></author>
      <pages>1448–1454</pages>
      <abstract>The task of automatic diagnosis encoding into standard medical classifications and <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontologies</a>, is of great importance in medicine-both to support the daily tasks of physicians in the preparation and reporting of clinical documentation, and for automatic processing of clinical reports. In this paper we investigate the application and performance of different deep learning transformers for automatic encoding in <a href="https://en.wikipedia.org/wiki/ICD-10">ICD-10</a> of clinical texts in Bulgarian. The comparative analysis attempts to find which approach is more efficient to be used for fine-tuning of pretrained BERT family transformer to deal with a specific domain terminology on a rare language as <a href="https://en.wikipedia.org/wiki/Bulgarian_language">Bulgarian</a>. On the one side are used SlavicBERT and MultiligualBERT, that are pretrained for common vocabulary in Bulgarian, but lack <a href="https://en.wikipedia.org/wiki/Medical_terminology">medical terminology</a>. On the other hand in the analysis are used BioBERT, ClinicalBERT, SapBERT, BlueBERT, that are pretrained for medical terminology in English, but lack training for language models in <a href="https://en.wikipedia.org/wiki/Bulgarian_language">Bulgarian</a>, and more over for vocabulary in <a href="https://en.wikipedia.org/wiki/Cyrillic_script">Cyrillic</a>. In our research study all BERT models are fine-tuned with additional medical texts in Bulgarian and then applied to the classification task for encoding <a href="https://en.wikipedia.org/wiki/Medical_diagnosis">medical diagnoses</a> in Bulgarian into <a href="https://en.wikipedia.org/wiki/ICD-10">ICD-10 codes</a>. Big corpora of diagnosis in Bulgarian annotated with ICD-10 codes is used for the classification task. Such an analysis gives a good idea of which of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> would be suitable for <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> of a similar type and domain. The experiments and evaluation results show that both <a href="https://en.wikipedia.org/wiki/Methodology">approaches</a> have comparable accuracy.</abstract>
      <url hash="14d875db">2021.ranlp-1.162</url>
      <bibkey>velichkov-etal-2021-comparative</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
    </paper>
    <paper id="163">
      <title>Mistake Captioning : A Machine Learning Approach for Detecting Mistakes and Generating Instructive Feedback</title>
      <author><first>Anton</first><last>Vinogradov</last></author>
      <author><first>Andrew Miles</first><last>Byrd</last></author>
      <author><first>Brent</first><last>Harrison</last></author>
      <pages>1455–1462</pages>
      <abstract>Giving feedback to students is not just about marking their answers as correct or incorrect, but also finding mistakes in their thought process that led them to that incorrect answer. In this paper, we introduce a <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning technique</a> for mistake captioning, a task that attempts to identify mistakes and provide <a href="https://en.wikipedia.org/wiki/Feedback">feedback</a> meant to help learners correct these mistakes. We do this by training a sequence-to-sequence network to generate this <a href="https://en.wikipedia.org/wiki/Feedback">feedback</a> based on <a href="https://en.wikipedia.org/wiki/Expert">domain experts</a>. To evaluate this <a href="https://en.wikipedia.org/wiki/System">system</a>, we explore how <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> can be used on a Linguistics assignment studying Grimm’s Law. We show that our approach generates <a href="https://en.wikipedia.org/wiki/Feedback">feedback</a> that outperforms a <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> on a set of automated NLP metrics. In addition, we perform a series of case studies in which we examine successful and unsuccessful system outputs.</abstract>
      <url hash="c0482eca">2021.ranlp-1.163</url>
      <bibkey>vinogradov-etal-2021-mistake</bibkey>
    </paper>
    <paper id="164">
      <title>A Novel Machine Learning Based Approach for Post-OCR Error Detection<fixed-case>OCR</fixed-case> Error Detection</title>
      <author><first>Shafqat Mumtaz</first><last>Virk</last></author>
      <author><first>Dana</first><last>Dannélls</last></author>
      <author><first>Azam</first><last>Sheikh Muhammad</last></author>
      <pages>1463–1470</pages>
      <abstract>Post processing is the most conventional approach for correcting errors that are caused by Optical Character Recognition(OCR) systems. Two steps are usually taken to correct OCR errors : <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">detection</a> and <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">corrections</a>. For the first <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">task</a>, supervised machine learning methods have shown state-of-the-art performances. Previously proposed approaches have focused most prominently on combining lexical, contextual and statistical features for detecting errors. In this study, we report a novel system to <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">error detection</a> which is based merely on the n-gram counts of a candidate token. In addition to being simple and computationally less expensive, our proposed <a href="https://en.wikipedia.org/wiki/System">system</a> beats previous systems reported in the ICDAR2019 competition on <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">OCR-error detection</a> with notable margins. We achieved state-of-the-art F1-scores for eight out of the ten involved <a href="https://en.wikipedia.org/wiki/Languages_of_Europe">European languages</a>. The maximum improvement is for <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a> which improved from 0.69 to 0.90, and the minimum for <a href="https://en.wikipedia.org/wiki/Polish_language">Polish</a> from 0.82 to 0.84.</abstract>
      <url hash="e93275bd">2021.ranlp-1.164</url>
      <bibkey>virk-etal-2021-novel</bibkey>
    </paper>
    <paper id="169">
      <title>ComboNER : A Lightweight All-In-One POS Tagger, Dependency Parser and NER<fixed-case>C</fixed-case>ombo<fixed-case>NER</fixed-case>: A Lightweight All-In-One <fixed-case>POS</fixed-case> Tagger, Dependency Parser and <fixed-case>NER</fixed-case></title>
      <author><first>Aleksander</first><last>Wawer</last></author>
      <pages>1508–1514</pages>
      <abstract>The current <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> is strongly focused on raising <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>. The progress comes at a cost of super-heavy models with hundreds of millions or even billions of parameters. However, simple syntactic tasks such as part-of-speech (POS) tagging, dependency parsing or named entity recognition (NER) do not require the largest models to achieve acceptable results. In line with this assumption we try to minimize the size of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that jointly performs all three <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. We introduce ComboNER : a lightweight tool, orders of magnitude smaller than state-of-the-art <a href="https://en.wikipedia.org/wiki/Transformer">transformers</a>. It is based on pre-trained subword embeddings and <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network architecture</a>. ComboNER operates on <a href="https://en.wikipedia.org/wiki/Polish_language">Polish language data</a>. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> has outputs for <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">POS tagging</a>, dependency parsing and <a href="https://en.wikipedia.org/wiki/Non-return-to-zero">NER</a>. Our paper contains some insights from fine-tuning of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and reports its overall results.</abstract>
      <url hash="afeb77ad">2021.ranlp-1.169</url>
      <bibkey>wawer-2021-comboner</bibkey>
    </paper>
    <paper id="170">
      <title>Investigating Annotator Bias in Abusive Language Datasets</title>
      <author><first>Maximilian</first><last>Wich</last></author>
      <author><first>Christian</first><last>Widmer</last></author>
      <author><first>Gerhard</first><last>Hagerer</last></author>
      <author><first>Georg</first><last>Groh</last></author>
      <pages>1515–1525</pages>
      <abstract>Nowadays, <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a> use classification models to cope with hate speech and abusive language. The problem of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> is their vulnerability to <a href="https://en.wikipedia.org/wiki/Bias">bias</a>. A prevalent form of bias in hate speech and abusive language datasets is annotator bias caused by the annotator’s subjective perception and the complexity of the annotation task. In our paper, we develop a set of methods to measure annotator bias in <a href="https://en.wikipedia.org/wiki/Abusive_language">abusive language datasets</a> and to identify different perspectives on <a href="https://en.wikipedia.org/wiki/Abusive_language">abusive language</a>. We apply these <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> to four different abusive language datasets. Our proposed approach supports annotation processes of such <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> and future research addressing different perspectives on the perception of abusive language.</abstract>
      <url hash="fe3fbb54">2021.ranlp-1.170</url>
      <bibkey>wich-etal-2021-investigating</bibkey>
    </paper>
    <paper id="172">
      <title>Transformer with Syntactic Position Encoding for <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a></title>
      <author><first>Yikuan</first><last>Xie</last></author>
      <author><first>Wenyong</first><last>Wang</last></author>
      <author><first>Mingqian</first><last>Du</last></author>
      <author><first>Qing</first><last>He</last></author>
      <pages>1536–1544</pages>
      <abstract>It has been widely recognized that syntax information can help end-to-end neural machine translation (NMT) systems to achieve better <a href="https://en.wikipedia.org/wiki/Translation_(biology)">translation</a>. In order to integrate dependency information into Transformer based NMT, existing approaches either exploit words’ local head-dependent relations, ignoring their non-local neighbors carrying important context ; or approximate two words’ syntactic relation by their relative distance on the dependency tree, sacrificing exactness. To address these issues, we propose global positional encoding for dependency tree, a new scheme that facilitates syntactic relation modeling between any two words with keeping exactness and without immediate neighbor constraint. Experiment results on NC11 GermanEnglish, EnglishGerman and WMT EnglishGerman datasets show that our approach is more effective than the above two strategies. In addition, our experiments quantitatively show that compared with higher layers, lower layers of the model are more proper places to incorporate syntax information in terms of each layer’s preference to the syntactic pattern and the final performance.</abstract>
      <url hash="c2054ac3">2021.ranlp-1.172</url>
      <bibkey>xie-etal-2021-transformer</bibkey>
    </paper>
    <paper id="173">
      <title>Towards Sentiment Analysis of Tobacco Products’ Usage in <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a></title>
      <author><first>Venkata Himakar</first><last>Yanamandra</last></author>
      <author><first>Kartikey</first><last>Pant</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>1545–1552</pages>
      <abstract>Contemporary tobacco-related studies are mostly concerned with a single <a href="https://en.wikipedia.org/wiki/Social_media">social media platform</a> while missing out on a broader audience. Moreover, they are heavily reliant on labeled datasets, which are expensive to make. In this work, we explore sentiment and product identification on tobacco-related text from two <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a>. We release SentiSmoke-Twitter and SentiSmoke-Reddit datasets, along with a comprehensive annotation schema for identifying tobacco products’ sentiment. We then perform benchmarking text classification experiments using state-of-the-art models, including BERT, RoBERTa, and DistilBERT. Our experiments show <a href="https://en.wikipedia.org/wiki/F-number">F1 scores</a> as high as 0.72 for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment identification</a> in the Twitter dataset, 0.46 for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment identification</a>, and 0.57 for product identification using <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised learning</a> for <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a>.</abstract>
      <url hash="ea16cc3e">2021.ranlp-1.173</url>
      <bibkey>yanamandra-etal-2021-towards</bibkey>
    </paper>
    <paper id="175">
      <title>Sentence Structure and Word Relationship Modeling for Emphasis Selection</title>
      <author><first>Haoran</first><last>Yang</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>1559–1566</pages>
      <abstract>Emphasis Selection is a newly proposed <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> which focuses on choosing words for emphasis in short sentences. Traditional methods only consider the sequence information of a sentence while ignoring the rich sentence structure and word relationship information. In this paper, we propose a new framework that considers <a href="https://en.wikipedia.org/wiki/Sentence_structure">sentence structure</a> via a sentence structure graph and word relationship via a word similarity graph. The sentence structure graph is derived from the <a href="https://en.wikipedia.org/wiki/Parse_tree">parse tree</a> of a sentence. The word similarity graph allows <a href="https://en.wikipedia.org/wiki/Vertex_(graph_theory)">nodes</a> to share information with their neighbors since we argue that in emphasis selection, similar words are more likely to be emphasized together. Graph neural networks are employed to learn the representation of each node of these two <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a>. Experimental results demonstrate that our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> can achieve superior performance.</abstract>
      <url hash="ed2f11b3">2021.ranlp-1.175</url>
      <bibkey>yang-lam-2021-sentence</bibkey>
      <pwccode url="https://github.com/lhryang/emphasis-selection" additional="false">lhryang/emphasis-selection</pwccode>
    </paper>
    <paper id="176">
      <title>Utterance Position-Aware Dialogue Act Recognition</title>
      <author><first>Yuki</first><last>Yano</last></author>
      <author><first>Akihiro</first><last>Tamura</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <author><first>Hiroaki</first><last>Obayashi</last></author>
      <pages>1567–1574</pages>
      <abstract>This study proposes an utterance position-aware approach for a neural network-based dialogue act recognition (DAR) model, which incorporates positional encoding for utterance’s absolute or relative position. The proposed approach is inspired by the observation that some <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue acts</a> have tendencies of occurrence positions. The evaluations on the Switchboard corpus show that the proposed positional encoding of utterances statistically significantly improves the performance of DAR.</abstract>
      <url hash="fddce0ce">2021.ranlp-1.176</url>
      <bibkey>yano-etal-2021-utterance</bibkey>
    </paper>
    <paper id="180">
      <title>Generic Mechanism for Reducing Repetitions in Encoder-Decoder Models</title>
      <author><first>Ying</first><last>Zhang</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Tatsuya</first><last>Aoki</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>1606–1615</pages>
      <abstract>Encoder-decoder models have been commonly used for many <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> such as <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> and response generation. As previous research reported, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> suffer from generating redundant repetition. In this research, we propose a new mechanism for encoder-decoder models that estimates the semantic difference of a source sentence before and after being fed into the encoder-decoder model to capture the consistency between two sides. This <a href="https://en.wikipedia.org/wiki/Mechanism_(engineering)">mechanism</a> helps reduce repeatedly generated tokens for a variety of <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a>. Evaluation results on publicly available machine translation and response generation datasets demonstrate the effectiveness of our proposal.</abstract>
      <url hash="e735f19e">2021.ranlp-1.180</url>
      <bibkey>zhang-etal-2021-generic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/persona-chat-1">PERSONA-CHAT</pwcdataset>
    </paper>
    <paper id="182">
      <title>Delexicalized Cross-lingual Dependency Parsing for Xibe<fixed-case>X</fixed-case>ibe</title>
      <author><first>He</first><last>Zhou</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <pages>1626–1635</pages>
      <abstract>Manually annotating a <a href="https://en.wikipedia.org/wiki/Treebank">treebank</a> is time-consuming and labor-intensive. We conduct delexicalized cross-lingual dependency parsing experiments, where we train the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> on one language and test on our target language. As our test case, we use <a href="https://en.wikipedia.org/wiki/Xibe_language">Xibe</a>, a severely under-resourced Tungusic language. We assume that choosing a closely related language as the source language will provide better results than more distant relatives. However, it is not clear how to determine those closely related languages. We investigate three different methods : choosing the typologically closest language, using LangRank, and choosing the most similar language based on perplexity. We train parsing models on the selected languages using UDify and test on different genres of Xibe data. The results show that languages selected based on <a href="https://en.wikipedia.org/wiki/Linguistic_typology">typology</a> and perplexity scores outperform those predicted by LangRank ; <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a> is the optimal source language. In determining the source language, proximity to the target language is more important than large training sizes. Parsing is also influenced by genre differences, but they have little influence as long as the training data is at least as complex as the target.</abstract>
      <url hash="753b5775">2021.ranlp-1.182</url>
      <bibkey>zhou-kubler-2021-delexicalized</bibkey>
    </paper>
    </volume>
  <volume id="srw" ingest-date="2021-11-18">
    <meta>
      <booktitle>Proceedings of the Student Research Workshop Associated with RANLP 2021</booktitle>
      <editor><first>Souhila</first><last>Djabri</last></editor>
      <editor><first>Dinara</first><last>Gimadi</last></editor>
      <editor><first>Tsvetomila</first><last>Mihaylova</last></editor>
      <editor><first>Ivelina</first><last>Nikolova-Koleva</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Online</address>
      <month>September</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="d6c0aaa9">2021.ranlp-srw.0</url>
      <bibkey>ranlp-2021-student</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Towards Code-Mixed Hinglish Dialogue Generation<fixed-case>H</fixed-case>inglish Dialogue Generation</title>
      <author><first>Vibhav</first><last>Agarwal</last></author>
      <author><first>Pooja</first><last>Rao</last></author>
      <author><first>Dinesh Babu</first><last>Jayagopi</last></author>
      <pages>7–15</pages>
      <abstract>Code-mixed language plays a crucial role in communication in <a href="https://en.wikipedia.org/wiki/Multilingualism">multilingual societies</a>. Though the recent growth of web users has greatly boosted the use of such <a href="https://en.wikipedia.org/wiki/Mixed_language">mixed languages</a>, the current generation of dialog systems is primarily monolingual. This increase in usage of code-mixed language has prompted <a href="https://en.wikipedia.org/wiki/Dialog_(software)">dialog systems</a> in a similar language. We present our work in Code-Mixed Dialog Generation, an unexplored task in code-mixed languages, generating utterances in code-mixed language rather than a single language that is more often just English. We present a new synthetic corpus in code-mix for dialogs, CM-DailyDialog, by converting an existing English-only dialog corpus to a mixed Hindi-English corpus. We then propose a baseline approach where we show the effectiveness of using mBART like multilingual sequence-to-sequence transformers for code-mixed dialog generation. Our best performing dialog models can conduct coherent conversations in Hindi-English mixed language as evaluated by human and automatic metrics setting new benchmarks for the Code-Mixed Dialog Generation task.</abstract>
      <url hash="d661856f">2021.ranlp-srw.2</url>
      <bibkey>agarwal-etal-2021-towards-code-mixed</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lince">LinCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/phinc">PHINC</pwcdataset>
    </paper>
    <paper id="9">
      <title>Bilingual Terminology Extraction Using Neural Word Embeddings on Comparable Corpora</title>
      <author><first>Darya</first><last>Filippova</last></author>
      <author><first>Burcu</first><last>Can</last></author>
      <author><first>Gloria</first><last>Corpas Pastor</last></author>
      <pages>58–64</pages>
      <abstract>Term and glossary management are vital steps of preparation of every language specialist, and they play a very important role at the stage of education of translation professionals. The growing trend of efficient <a href="https://en.wikipedia.org/wiki/Time_management">time management</a> and constant time constraints we may observe in every job sector increases the necessity of the automatic glossary compilation. Many well-performing bilingual AET systems are based on processing parallel data, however, such parallel corpora are not always available for a specific domain or a language pair. Domain-specific, bilingual access to information and its retrieval based on comparable corpora is a very promising area of research that requires a detailed analysis of both available data sources and the possible extraction techniques. This work focuses on domain-specific automatic terminology extraction from comparable corpora for the English   Russian language pair by utilizing neural word embeddings.</abstract>
      <url hash="be75b7e3">2021.ranlp-srw.9</url>
      <bibkey>filippova-etal-2021-bilingual-terminology</bibkey>
    </paper>
    <paper id="10">
      <title>Web-sentiment analysis of public comments (public reviews) for languages with limited resources such as the Kazakh language<fixed-case>K</fixed-case>azakh language</title>
      <author><first>Dinara</first><last>Gimadi</last></author>
      <pages>65–68</pages>
      <abstract>In the pandemic period, the stay-at-home trend forced businesses to switch their activities to digital mode, for example, app-based payment methods, social distancing via social media platforms, and other digital means have become an integral part of our lives. Sentiment analysis of textual information in user comments is a topical task in emotion AI because user comments or reviews are not homogeneous, they contain sparse context behind, and are misleading both for human and computer. Barriers arise from the emotional language enriched with <a href="https://en.wikipedia.org/wiki/Slang">slang</a>, peculiar spelling, <a href="https://en.wikipedia.org/wiki/Transliteration">transliteration</a>, use of <a href="https://en.wikipedia.org/wiki/Emoji">emoji</a> and their symbolic counterparts, and <a href="https://en.wikipedia.org/wiki/Code-switching">code-switching</a>. For low resource languages <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> has not been worked upon extensively, because of an absence of ready-made tools and linguistic resources for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. This research focuses on developing a method for aspect-based sentiment analysis for Kazakh-language reviews in <a href="https://en.wikipedia.org/wiki/Google_Play">Android Google Play Market</a>.</abstract>
      <url hash="f54b8418">2021.ranlp-srw.10</url>
      <bibkey>gimadi-2021-web-sentiment</bibkey>
    </paper>
    <paper id="14">
      <title>Compiling a specialised corpus for <a href="https://en.wikipedia.org/wiki/Translation_studies">translation research</a> in the <a href="https://en.wikipedia.org/wiki/Biophysical_environment">environmental domain</a></title>
      <author><first>Anastasiia</first><last>Laktionova</last></author>
      <pages>94–98</pages>
      <abstract>The present study is an ongoing research that aims to investigate lexico-grammatical and stylistic features of texts in the environmental domain in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, their implications for translation into Ukrainian as well as the translation of key terminological units based on a specialised parallel and comparable corpora.</abstract>
      <url hash="df82887e">2021.ranlp-srw.14</url>
      <bibkey>laktionova-2021-compiling-specialised</bibkey>
    </paper>
    <paper id="15">
      <title>Paragraph Similarity Matches for Generating Multiple-choice Test Items</title>
      <author><first>Halyna</first><last>Maslak</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>99–108</pages>
      <abstract>Multiple-choice questions (MCQs) are widely used in <a href="https://en.wikipedia.org/wiki/Educational_assessment">knowledge assessment</a> in educational institutions, during work interviews, in entertainment quizzes and games. Although the research on the automatic or semi-automatic generation of multiple-choice test items has been conducted since the beginning of this millennium, most approaches focus on generating questions from a single sentence. In this research, a state-of-the-art method of creating questions based on multiple sentences is introduced. It was inspired by semantic similarity matches used in the translation memory component of <a href="https://en.wikipedia.org/wiki/Translation_management_system">translation management systems</a>. The performance of two <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning algorithms</a>, doc2vec and SBERT, is compared for the paragraph similarity task. The experiments are performed on the ad-hoc corpus within the <a href="https://en.wikipedia.org/wiki/Domain_name">EU domain</a>. For the automatic evaluation, a smaller corpus of manually selected matching paragraphs has been compiled. The results prove the good performance of Sentence Embeddings for the given <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>.</abstract>
      <url hash="af636f05">2021.ranlp-srw.15</url>
      <bibkey>maslak-mitkov-2021-paragraph-similarity</bibkey>
    </paper>
    <paper id="17">
      <title>Does local pruning offer task-specific models to learn effectively?</title>
      <author><first>Abhishek Kumar</first><last>Mishra</last></author>
      <author><first>Mohna</first><last>Chakraborty</last></author>
      <pages>118–125</pages>
      <abstract>The need to deploy large-scale pre-trained models on edge devices under limited computational resources has led to substantial research to compress these large <a href="https://en.wikipedia.org/wiki/Computer_simulation">models</a>. However, less attention has been given to compress the task-specific models. In this work, we investigate the different methods of unstructured pruning on task-specific models for Aspect-based Sentiment Analysis (ABSA) tasks. Specifically, we analyze differences in the learning dynamics of pruned models by using the standard pruning techniques to achieve high-performing <a href="https://en.wikipedia.org/wiki/Sparse_network">sparse networks</a>. We develop a hypothesis to demonstrate the effectiveness of local pruning over global pruning considering a simple CNN model. Later, we utilize the <a href="https://en.wikipedia.org/wiki/Hypothesis">hypothesis</a> to demonstrate the efficacy of the pruned state-of-the-art model compared to the over-parameterized state-of-the-art model under two settings, the first considering the baselines for the same <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> used for generating the hypothesis, i.e., <a href="https://en.wikipedia.org/wiki/Aspect_extraction">aspect extraction</a> and the second considering a different <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, i.e., <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. We also provide discussion related to the generalization of the pruning hypothesis.</abstract>
      <url hash="0c3c3ac6">2021.ranlp-srw.17</url>
      <bibkey>mishra-chakraborty-2021-local-pruning</bibkey>
      <pwccode url="https://github.com/abhishekkumarm98/local_vs_global-pruning" additional="false">abhishekkumarm98/local_vs_global-pruning</pwccode>
    </paper>
    <paper id="20">
      <title>A Dataset for Research on Modelling Depression Severity in Online Forum Data</title>
      <author><first>Isuri Anuradha</first><last>Nanomi Arachchige</last></author>
      <author><first>Vihangi Himaya</first><last>Jayasuriya</last></author>
      <author><first>Ruvan</first><last>Weerasinghe</last></author>
      <pages>144–153</pages>
      <abstract>People utilize <a href="https://en.wikipedia.org/wiki/Internet_forum">online forums</a> to either look for information or to contribute it. Because of their growing popularity, certain <a href="https://en.wikipedia.org/wiki/Internet_forum">online forums</a> have been created specifically to provide support, assistance, and opinions for people suffering from <a href="https://en.wikipedia.org/wiki/Mental_disorder">mental illness</a>. Depression is one of the most frequent psychological illnesses worldwide. People communicate more with <a href="https://en.wikipedia.org/wiki/Internet_forum">online forums</a> to find answers for their <a href="https://en.wikipedia.org/wiki/Mental_disorder">psychological disease</a>. However, there is no mechanism to measure the severity of depression in each post and give higher importance to those who are diagnosed more severely depressed. Despite the fact that numerous researches based on online forum data and the identification of depression have been conducted, the severity of depression is rarely explored. In addition, the absence of datasets will stymie the development of novel <a href="https://en.wikipedia.org/wiki/Medical_procedure">diagnostic procedures</a> for practitioners. From this study, we offer a dataset to support research on depression severity evaluation. The computational approach to measure an automatic process, identified severity of depression here is quite novel approach. Nonetheless, this elaborate measuring severity of depression in online forum posts is needed to ensure the measurement scales used in our research meets the expected norms of scientific research.</abstract>
      <url hash="fd7ddee0">2021.ranlp-srw.20</url>
      <bibkey>nanomi-arachchige-etal-2021-dataset-research</bibkey>
    </paper>
    <paper id="22">
      <title>On the Evolution of Word Order</title>
      <author><first>Idan</first><last>Rejwan</last></author>
      <author><first>Avi</first><last>Caciularu</last></author>
      <pages>162–166</pages>
      <abstract>Most <a href="https://en.wikipedia.org/wiki/Natural_language">natural languages</a> have a predominant or fixed word order. For example in <a href="https://en.wikipedia.org/wiki/English_language">English</a> the <a href="https://en.wikipedia.org/wiki/Word_order">word order</a> is usually Subject-Verb-Object. This work attempts to explain this phenomenon as well as other typological findings regarding <a href="https://en.wikipedia.org/wiki/Word_order">word order</a> from a functional perspective. In particular, we examine whether fixed word order provides a functional advantage, explaining why these <a href="https://en.wikipedia.org/wiki/Language">languages</a> are prevalent. To this end, we consider an evolutionary model of language and demonstrate, both theoretically and using <a href="https://en.wikipedia.org/wiki/Genetic_algorithm">genetic algorithms</a>, that a <a href="https://en.wikipedia.org/wiki/Language">language</a> with a fixed word order is optimal. We also show that adding information to the sentence, such as <a href="https://en.wikipedia.org/wiki/Marker_(linguistics)">case markers</a> and noun-verb distinction, reduces the need for fixed word order, in accordance with the typological findings.</abstract>
      <url hash="0cda3dd2">2021.ranlp-srw.22</url>
      <bibkey>rejwan-caciularu-2021-evolution-word</bibkey>
    </paper>
    <paper id="23">
      <title>EmoPars : A Collection of 30 K Emotion-Annotated Persian Social Media Texts<fixed-case>E</fixed-case>mo<fixed-case>P</fixed-case>ars: A Collection of 30<fixed-case>K</fixed-case> Emotion-Annotated <fixed-case>P</fixed-case>ersian Social Media Texts</title>
      <author><first>Nazanin</first><last>Sabri</last></author>
      <author><first>Reyhane</first><last>Akhavan</last></author>
      <author><first>Behnam</first><last>Bahrak</last></author>
      <pages>167–173</pages>
      <abstract>The wide reach of <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a>, such as <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>, have enabled many users to share their thoughts, opinions and emotions on various topics online. The ability to detect these <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> automatically would allow social scientists, as well as, businesses to better understand responses from nations and costumers. In this study we introduce a dataset of 30,000 Persian Tweets labeled with Ekman’s six basic emotions (Anger, <a href="https://en.wikipedia.org/wiki/Fear">Fear</a>, <a href="https://en.wikipedia.org/wiki/Happiness">Happiness</a>, <a href="https://en.wikipedia.org/wiki/Sadness">Sadness</a>, <a href="https://en.wikipedia.org/wiki/Hatred">Hatred</a>, and Wonder). This is the first publicly available emotion dataset in the <a href="https://en.wikipedia.org/wiki/Persian_language">Persian language</a>. In this paper, we explain the data collection and labeling scheme used for the creation of this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. We also analyze the created <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, showing the different <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> and characteristics of the data. Among other things, we investigate co-occurrence of different emotions in the <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, and the relationship between sentiment and emotion of textual instances. The dataset is publicly available at https://github.com/nazaninsbr/Persian-Emotion-Detection.</abstract>
      <url hash="25c566a6">2021.ranlp-srw.23</url>
      <bibkey>sabri-etal-2021-emopars-collection</bibkey>
      <pwccode url="https://github.com/nazaninsbr/persian-emotion-detection" additional="false">nazaninsbr/persian-emotion-detection</pwccode>
    </paper>
    <paper id="24">
      <title>A Review on Document Information Extraction Approaches</title>
      <author><first>Kanishka</first><last>Silva</last></author>
      <author><first>Thushari</first><last>Silva</last></author>
      <pages>174–179</pages>
      <abstract>Information extraction from <a href="https://en.wikipedia.org/wiki/Document">documents</a> has become great use of novel natural language processing areas. Most of the entity extraction methodologies are variant in a context such as <a href="https://en.wikipedia.org/wiki/Medicine">medical area</a>, <a href="https://en.wikipedia.org/wiki/Finance">financial area</a>, also come even limited to the given language. It is better to have one generic approach applicable for any document type to extract entity information regardless of language, context, and structure. Also, another issue in such research is <a href="https://en.wikipedia.org/wiki/Structural_analysis">structural analysis</a> while keeping the hierarchical, semantic, and heuristic features. Another problem identified is that usually, it requires a massive training corpus. Therefore, this research focus on mitigating such barriers. Several approaches have been identifying towards building document information extractors focusing on different disciplines. This research area involves <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, <a href="https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)">semantic analysis</a>, <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a>, and <a href="https://en.wikipedia.org/wiki/Conceptual_model">conceptual modelling</a>. This paper presents a review of the information extraction mechanism to construct a generic framework for document extraction with aim of providing a solid base for upcoming research.</abstract>
      <url hash="44a35513">2021.ranlp-srw.24</url>
      <bibkey>silva-silva-2021-review-document</bibkey>
    </paper>
    <paper id="26">
      <title>Question answering in <a href="https://en.wikipedia.org/wiki/Natural_language">Natural Language</a> : the Special Case of Temporal Expressions</title>
      <author><first>Armand</first><last>Stricker</last></author>
      <pages>184–192</pages>
      <abstract>Although general question answering has been well explored in recent years, temporal question answering is a <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> which has not received as much focus. Our work aims to leverage a popular approach used for general question answering, answer extraction, in order to find answers to temporal questions within a paragraph. To train our model, we propose a new dataset, inspired by SQuAD, a state-of-the-art question answering corpus, specifically tailored to provide rich temporal information by adapting the corpus WikiWars, which contains several documents on history’s greatest conflicts. Our evaluation shows that a pattern matching deep learning model, often used in general question answering, can be adapted to temporal question answering, if we accept to ask questions whose answers must be directly present within a text.</abstract>
      <url hash="afe63dc8">2021.ranlp-srw.26</url>
      <bibkey>stricker-2021-question-answering</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="27">
      <title>Building A Corporate Corpus For Threads Constitution</title>
      <author><first>Lionel</first><last>Tadonfouet Tadjou</last></author>
      <author><first>Fabrice</first><last>Bourge</last></author>
      <author><first>Tiphaine</first><last>Marie</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <author><first>Éric</first><last>de la Clergerie</last></author>
      <pages>193–202</pages>
      <abstract>In this paper we describe the process of build-ing a corporate corpus that will be used as a ref-erence for modelling and computing threadsfrom conversations generated using commu-nication and collaboration tools. The overallgoal of the reconstruction of threads is to beable to provide value to the collorator in var-ious use cases, such as higlighting the impor-tant parts of a running discussion, reviewingthe upcoming commitments or deadlines, etc. Since, to our knowledge, there is no avail-able corporate corpus for the French languagewhich could allow us to address this prob-lem of thread constitution, we present here amethod for building such corpora includingdifferent aspects and steps which allowed thecreation of a pipeline to pseudo-anonymisedata. Such a <a href="https://en.wikipedia.org/wiki/Pipeline_transport">pipeline</a> is a response to theconstraints induced by the General Data Pro-tection Regulation GDPR in Europe and thecompliance to the secrecy of correspondence.</abstract>
      <url hash="1ab62445">2021.ranlp-srw.27</url>
      <bibkey>tadonfouet-tadjou-etal-2021-building-corporate</bibkey>
    </paper>
    <paper id="29">
      <title>Toward Discourse-Aware Models for Multilingual Fake News Detection</title>
      <author><first>Francielle</first><last>Vargas</last></author>
      <author><first>Fabrício</first><last>Benevenuto</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>210–218</pages>
      <abstract>Statements that are intentionally misstated (or manipulated) are of considerable interest to researchers, government, security, and financial systems. According to <a href="https://en.wikipedia.org/wiki/Deception">deception literature</a>, there are reliable cues for detecting <a href="https://en.wikipedia.org/wiki/Deception">deception</a> and the belief that liars give off cues that may indicate their deception is near-universal. Therefore, given that deceiving actions require advanced <a href="https://en.wikipedia.org/wiki/Cognitive_development">cognitive development</a> that <a href="https://en.wikipedia.org/wiki/Honesty">honesty</a> simply does not require, as well as people’s cognitive mechanisms have promising guidance for deception detection, in this Ph.D. ongoing research, we propose to examine discourse structure patterns in multilingual deceptive news corpora using the Rhetorical Structure Theory framework. Considering that our work is the first to exploit multilingual discourse-aware strategies for fake news detection, the research community currently lacks multilingual deceptive annotated corpora. Accordingly, this paper describes the current progress in this thesis, including (i) the construction of the first multilingual deceptive corpus, which was annotated by specialists according to the Rhetorical Structure Theory framework, and (ii) the introduction of two new proposed rhetorical relations : INTERJECTION and IMPERATIVE, which we assume to be relevant for the fake news detection task.</abstract>
      <url hash="694dd7a0">2021.ranlp-srw.29</url>
      <bibkey>vargas-etal-2021-toward-discourse</bibkey>
    </paper>
    </volume>
</collection>