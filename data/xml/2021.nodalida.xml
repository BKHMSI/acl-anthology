<collection id="2021.nodalida">
  <volume id="main" ingest-date="2021-05-31">
    <meta>
      <booktitle>Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)</booktitle>
      <editor><first>Simon</first><last>Dobnik</last></editor>
      <editor><first>Lilja</first><last>&#216;vrelid</last></editor>
      <publisher>Link&#246;ping University Electronic Press, Sweden</publisher>
      <address>Reykjavik, Iceland (Online)</address>
      <month>May 31--2 June</month>
      <year>2021</year>
      <url hash="d102b425">2021.nodalida-main</url>
    </meta>
    <frontmatter>
      <url hash="4da07233">2021.nodalida-main.0</url>
      <bibkey>nodalida-2021-nordic</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Operationalizing a National Digital Library: The Case for a <fixed-case>N</fixed-case>orwegian Transformer Model</title>
      <author><first>Per E</first><last>Kummervold</last></author>
      <author><first>Javier</first><last>De la Rosa</last></author>
      <author><first>Freddy</first><last>Wetjen</last></author>
      <author><first>Svein Arne</first><last>Brygfjeld</last></author>
      <pages>20&#8211;29</pages>
      <abstract>In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library. The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both Norwegian Bokm&#229;l and Norwegian Nynorsk. Our model also improves the mBERT performance for other languages present in the corpus such as English, Swedish, and Danish. For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties. Therefore, we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.</abstract>
      <url hash="51be5fb2">2021.nodalida-main.3</url>
      <bibkey>kummervold-etal-2021-operationalizing</bibkey>
    </paper>
    <paper id="4">
      <title>Large-Scale Contextualised Language Modelling for <fixed-case>N</fixed-case>orwegian</title>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <author><first>Lilja</first><last>&#216;vrelid</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <pages>30&#8211;40</pages>
      <abstract>We present the ongoing NorLM initiative to support the creation and use of very large contextualised language models for Norwegian (and in principle other Nordic languages), including a ready-to-use software environment, as well as an experience report for data preparation and training. This paper introduces the first large-scale monolingual language models for Norwegian, based on both the ELMo and BERT frameworks. In addition to detailing the training process, we present contrastive benchmark results on a suite of NLP tasks for Norwegian. For additional background and access to the data, models, and software, please see: http://norlm.nlpl.eu</abstract>
      <url hash="a9eecc98">2021.nodalida-main.4</url>
      <bibkey>kutuzov-etal-2021-large</bibkey>
      <pwccode url="https://github.com/ltgoslo/NorBERT" additional="true">ltgoslo/NorBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/norec-fine">NoReC_fine</pwcdataset>
    </paper>
    <paper id="11">
      <title>A Baseline Document Planning Method for Automated Journalism</title>
      <author><first>Leo</first><last>Lepp&#228;nen</last></author>
      <author><first>Hannu</first><last>Toivonen</last></author>
      <pages>101&#8211;111</pages>
      <abstract>In this work, we present a method for content selection and document planning for automated news and report generation from structured statistical data such as that offered by the European Union&#8217;s statistical agency, EuroStat. The method is driven by the data and is highly topic-independent within the statistical dataset domain. As our approach is not based on machine learning, it is suitable for introducing news automation to the wide variety of domains where no training data is available. As such, it is suitable as a low-cost (in terms of implementation effort) baseline for document structuring prior to introduction of domain-specific knowledge.</abstract>
      <url hash="0f5954ac">2021.nodalida-main.11</url>
      <bibkey>leppanen-toivonen-2021-baseline</bibkey>
    </paper>
    <paper id="16">
      <title>Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification</title>
      <author><first>Samuel</first><last>R&#246;nnqvist</last></author>
      <author><first>Valtteri</first><last>Skantsi</last></author>
      <author><first>Miika</first><last>Oinonen</last></author>
      <author><first>Veronika</first><last>Laippala</last></author>
      <pages>157&#8211;165</pages>
      <abstract>This article studies register classification of documents from the unrestricted web, such as news articles or opinion blogs, in a multilingual setting, exploring both the benefit of training on multiple languages and the capabilities for zero-shot cross-lingual transfer. While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved. In this study, we show that training on multiple languages 1) benefits languages with limited amounts of register-annotated data, 2) on average achieves performance on par with monolingual models, and 3) greatly improves upon previous zero-shot results in Finnish, French and Swedish. The best results are achieved with the multilingual XLM-R model. As data, we use the CORE corpus series featuring register annotated data from the unrestricted web.</abstract>
      <url hash="271cadbf">2021.nodalida-main.16</url>
      <bibkey>ronnqvist-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="21">
      <title>De-identification of Privacy-related Entities in Job Postings</title>
      <author><first>Kristian N&#248;rgaard</first><last>Jensen</last></author>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>210&#8211;221</pages>
      <abstract>De-identification is the task of detecting privacy-related entities in text, such as person names, emails and contact data. It has been well-studied within the medical domain. The need for de-identification technology is increasing, as privacy-preserving data handling is in high demand in many domains. In this paper, we focus on job postings. We present JobStack, a new corpus for de-identification of personal data in job vacancies on Stackoverflow. We introduce baselines, comparing Long-Short Term Memory (LSTM) and Transformer models. To improve these baselines, we experiment with BERT representations, and distantly related auxiliary data via multi-task learning. Our results show that auxiliary data helps to improve de-identification performance. While BERT representations improve performance, surprisingly &#8220;vanilla&#8221; BERT turned out to be more effective than BERT trained on Stackoverflow-related data.</abstract>
      <url hash="a59b2641">2021.nodalida-main.21</url>
      <bibkey>jensen-etal-2021-de</bibkey>
      <pwccode url="https://github.com/kris927b/JobStack" additional="false">kris927b/JobStack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/jobstack">JobStack</pwcdataset>
    </paper>
    <paper id="28">
      <title><fixed-case>NLI</fixed-case> Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance</title>
      <author><first>Aarne</first><last>Talman</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
      <author><first>J&#246;rg</first><last>Tiedemann</last></author>
      <pages>276&#8211;287</pages>
      <abstract>Pre-trained neural language models give high performance on natural language inference (NLI) tasks. But whether they actually understand the meaning of the processed sequences is still unclear. We propose a new diagnostics test suite which allows to assess whether a dataset constitutes a good testbed for evaluating the models&#8217; meaning understanding capabilities. We specifically apply controlled corruption transformations to widely used benchmarks (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs. If model accuracy on the corrupted data remains high, then the dataset is likely to contain statistical biases and artefacts that guide prediction. Inversely, a large decrease in model accuracy indicates that the original dataset provides a proper challenge to the models&#8217; reasoning capabilities. Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks.</abstract>
      <url hash="c358f7c4">2021.nodalida-main.28</url>
      <bibkey>talman-etal-2021-nli</bibkey>
      <pwccode url="https://github.com/Helsinki-NLP/nli-data-sanity-check" additional="false">Helsinki-NLP/nli-data-sanity-check</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="33">
      <title>Towards cross-lingual application of language-specific <fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> tagging schemes</title>
      <author><first>Hinrik</first><last>Hafsteinsson</last></author>
      <author><first>Anton Karl</first><last>Ingason</last></author>
      <pages>321&#8211;325</pages>
      <abstract>We describe the process of conversion between the PoS tagging schemes of two languages, the Icelandic MIM-GOLD tagging scheme and the Faroese Sosialurin tagging scheme. These tagging schemes are functionally similar but use separate ways to encode fine-grained morphological information on tokenised text. As Faroese and Icelandic are lexically and grammatically similar, having a systematic method to convert between these two tagging schemes would be beneficial in the field of language technology, specifically in research on transfer learning between the two languages. As a product of our work, we present a provisional version of Icelandic corpora, prepared in the Faroese PoS tagging scheme, ready for use in cross-lingual NLP applications.</abstract>
      <url hash="a590242a">2021.nodalida-main.33</url>
      <bibkey>hafsteinsson-ingason-2021-towards</bibkey>
    </paper>
    <paper id="34">
      <title>Exploring the Importance of Source Text in Automatic Post-Editing for Context-Aware Machine Translation</title>
      <author><first>Chaojun</first><last>Wang</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>326&#8211;335</pages>
      <abstract>Accurate translation requires document-level information, which is ignored by sentence-level machine translation. Recent work has demonstrated that document-level consistency can be improved with automatic post-editing (APE) using only target-language (TL) information. We study an extended APE model that additionally integrates source context. A human evaluation of fluency and adequacy in English&#8211;Russian translation reveals that the model with access to source context significantly outperforms monolingual APE in terms of adequacy, an effect largely ignored by automatic evaluation metrics. Our results show that TL-only modelling increases fluency without improving adequacy, demonstrating the need for conditioning on source text for automatic post-editing. They also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably.</abstract>
      <url hash="1e1ba1c4">2021.nodalida-main.34</url>
      <bibkey>wang-etal-2021-exploring</bibkey>
      <pwccode url="https://github.com/zippotju/context-aware-bilingual-repair-for-neural-machine-translation" additional="false">zippotju/context-aware-bilingual-repair-for-neural-machine-translation</pwccode>
    </paper>
    <paper id="36">
      <title>Grapheme-Based Cross-Language Forced Alignment: Results with Uralic Languages</title>
      <author><first>Juho</first><last>Leinonen</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <pages>345&#8211;350</pages>
      <abstract>Forced alignment is an effective process to speed up linguistic research. However, most forced aligners are language-dependent, and under-resourced languages rarely have enough resources to train an acoustic model for an aligner. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple Uralic languages and English as an unrelated language. We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments.</abstract>
      <url hash="091b5464">2021.nodalida-main.36</url>
      <bibkey>leinonen-etal-2021-grapheme</bibkey>
      <pwccode url="https://github.com/aalto-speech/finnish-forced-alignment" additional="false">aalto-speech/finnish-forced-alignment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="40">
      <title>Decentralized <fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec Using Gossip Learning</title>
      <author><first>Abdul Aziz</first><last>Alkathiri</last></author>
      <author><first>Lodovico</first><last>Giaretta</last></author>
      <author><first>Sarunas</first><last>Girdzijauskas</last></author>
      <author><first>Magnus</first><last>Sahlgren</last></author>
      <pages>373&#8211;377</pages>
      <abstract>Advanced NLP models require huge amounts of data from various domains to produce high-quality representations. It is useful then for a few large public and private organizations to join their corpora during training. However, factors such as legislation and user emphasis on data privacy may prevent centralized orchestration and data sharing among these organizations. Therefore, for this specific scenario, we investigate how gossip learning, a massively-parallel, data-private, decentralized protocol, compares to a shared-dataset solution. We find that the application of Word2Vec in a gossip learning framework is viable. Without any tuning, the results are comparable to a traditional centralized setting, with a loss of quality as low as 4.3%. Furthermore, the results are up to 54.8% better than independent local training.</abstract>
      <url hash="58d0d53e">2021.nodalida-main.40</url>
      <bibkey>alkathiri-etal-2021-decentralized</bibkey>
    </paper>
    <paper id="41">
      <title>Multilingual <fixed-case>ELM</fixed-case>o and the Effects of Corpus Sampling</title>
      <author><first>Vinit</first><last>Ravishankar</last></author>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Lilja</first><last>&#216;vrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>378&#8211;384</pages>
      <abstract>Multilingual pretrained language models are rapidly gaining popularity in NLP systems for non-English languages. Most of these models feature an important corpus sampling step in the process of accumulating training data in different languages, to ensure that the signal from better resourced languages does not drown out poorly resourced ones. In this study, we train multiple multilingual recurrent language models, based on the ELMo architecture, and analyse both the effect of varying corpus size ratios on downstream performance, as well as the performance difference between monolingual models for each language, and broader multilingual language models. As part of this effort, we also make these trained models available for public use.</abstract>
      <url hash="c73e3d0a">2021.nodalida-main.41</url>
      <bibkey>ravishankar-etal-2021-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="46">
      <title>The <fixed-case>D</fixed-case>anish <fixed-case>G</fixed-case>igaword Corpus</title>
      <author><first>Leon</first><last>Str&#248;mberg-Derczynski</last></author>
      <author><first>Manuel</first><last>Ciosici</last></author>
      <author><first>Rebekah</first><last>Baglini</last></author>
      <author><first>Morten H.</first><last>Christiansen</last></author>
      <author><first>Jacob Aarup</first><last>Dalsgaard</last></author>
      <author><first>Riccardo</first><last>Fusaroli</last></author>
      <author><first>Peter Juel</first><last>Henrichsen</last></author>
      <author><first>Rasmus</first><last>Hvingelby</last></author>
      <author><first>Andreas</first><last>Kirkedal</last></author>
      <author><first>Alex Speed</first><last>Kjeldsen</last></author>
      <author><first>Claus</first><last>Ladefoged</last></author>
      <author><first>Finn &#197;rup</first><last>Nielsen</last></author>
      <author><first>Jens</first><last>Madsen</last></author>
      <author><first>Malte Lau</first><last>Petersen</last></author>
      <author><first>Jonathan Hvithamar</first><last>Rystr&#248;m</last></author>
      <author><first>Daniel</first><last>Varab</last></author>
      <pages>413&#8211;421</pages>
      <abstract>Danish language technology has been hindered by a lack of broad-coverage corpora at the scale modern NLP prefers. This paper describes the Danish Gigaword Corpus, the result of a focused effort to provide a diverse and freely-available one billion word corpus of Danish text. The Danish Gigaword corpus covers a wide array of time periods, domains, speakers&#8217; socio-economic status, and Danish dialects.</abstract>
      <url hash="d90398be">2021.nodalida-main.46</url>
      <revision id="1" href="2021.nodalida-main.46v1" hash="ae5c62ef" />
      <revision id="2" href="2021.nodalida-main.46v2" hash="d90398be" date="2021-06-04">This revision amends an incorrect name in one of the cited works.</revision>
      <bibkey>stromberg-derczynski-etal-2021-danish</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dagw">DAGW</pwcdataset>
    </paper>
    <paper id="47">
      <title><fixed-case>D</fixed-case>an<fixed-case>FEVER</fixed-case>: claim verification dataset for <fixed-case>D</fixed-case>anish</title>
      <author><first>Jeppe</first><last>N&#248;rregaard</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <pages>422&#8211;428</pages>
      <abstract>We present a dataset, DanFEVER, intended for multilingual misinformation research. The dataset is in Danish and has the same format as the well-known English FEVER dataset. It can be used for testing methods in multilingual settings, as well as for creating models in production for the Danish language.</abstract>
      <url hash="b787c42b">2021.nodalida-main.47</url>
      <bibkey>norregaard-derczynski-2021-danfever</bibkey>
      <pwccode url="https://github.com/StrombergNLP/danfever" additional="false">StrombergNLP/danfever</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/danfever">DanFEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
    </paper>
    <paper id="51">
      <title><fixed-case>N</fixed-case>or<fixed-case>D</fixed-case>ial: A Preliminary Corpus of Written <fixed-case>N</fixed-case>orwegian Dialect Use</title>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Petter</first><last>M&#230;hlum</last></author>
      <author><first>Samia</first><last>Touileb</last></author>
      <pages>445&#8211;451</pages>
      <abstract>Norway has a large amount of dialectal variation, as well as a general tolerance to its use in the public sphere. There are, however, few available resources to study this variation and its change over time and in more informal areas, on social media. In this paper, we propose a first step to creating a corpus of dialectal variation of written Norwegian. We collect a small corpus of tweets and manually annotate them as Bokm&#229;l, Nynorsk, any dialect, or a mix. We further perform preliminary experiments with state-of-the-art models, as well as an analysis of the data to expand this corpus in the future. Finally, we make the annotations available for future work.</abstract>
      <url hash="1f13a7c8">2021.nodalida-main.51</url>
      <bibkey>barnes-etal-2021-nordial</bibkey>
      <pwccode url="https://github.com/jerbarnes/norwegian_dialect" additional="false">jerbarnes/norwegian_dialect</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nordial">NorDial</pwcdataset>
    </paper>
    <paper id="52">
      <title>The <fixed-case>S</fixed-case>wedish <fixed-case>W</fixed-case>inogender Dataset</title>
      <author><first>Saga</first><last>Hansson</last></author>
      <author><first>Konstantinos</first><last>Mavromatakis</last></author>
      <author><first>Yvonne</first><last>Adesam</last></author>
      <author><first>Gerlof</first><last>Bouma</last></author>
      <author><first>Dana</first><last>Dann&#233;lls</last></author>
      <pages>452&#8211;459</pages>
      <abstract>We introduce the SweWinogender test set, a diagnostic dataset to measure gender bias in coreference resolution. It is modelled after the English Winogender benchmark, and is released with reference statistics on the distribution of men and women between occupations and the association between gender and occupation in modern corpus material. The paper discusses the design and creation of the dataset, and presents a small investigation of the supplementary statistics.</abstract>
      <url hash="4924bf48">2021.nodalida-main.52</url>
      <bibkey>hansson-etal-2021-swedish</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    </volume>
</collection>