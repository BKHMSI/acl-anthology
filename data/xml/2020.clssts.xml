<collection id="2020.clssts">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020)</booktitle>
      <editor><first>Kathy</first><last>McKeown</last></editor>
      <editor><first>Douglas W.</first><last>Oard</last></editor>
      <editor><first /><last>Elizabeth</last></editor>
      <editor><first>Richard</first><last>Schwartz</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-55-9</isbn>
    </meta>
    <frontmatter>
      <url hash="27b49c1f">2020.clssts-1.0</url>
      <bibkey>clssts-2020-cross</bibkey>
    </frontmatter>
    <paper id="4">
      <title><fixed-case>SEARCHER</fixed-case>: Shared Embedding Architecture for Effective Retrieval</title>
      <author><first>Joel</first><last>Barry</last></author>
      <author><first>Elizabeth</first><last>Boschee</last></author>
      <author><first>Marjorie</first><last>Freedman</last></author>
      <author><first>Scott</first><last>Miller</last></author>
      <pages>22&#8211;25</pages>
      <abstract>We describe an approach to cross lingual information retrieval that does not rely on explicit translation of either document or query terms. Instead, both queries and documents are mapped into a shared embedding space where retrieval is performed. We discuss potential advantages of the approach in handling polysemy and synonymy. We present a method for training the model, and give details of the model implementation. We present experimental results for two cases: Somali-English and Bulgarian-English CLIR.</abstract>
      <url hash="daff9c04">2020.clssts-1.4</url>
      <language>eng</language>
      <bibkey>barry-etal-2020-searcher</bibkey>
    </paper>
    <paper id="5">
      <title>Cross-lingual Information Retrieval with <fixed-case>BERT</fixed-case></title>
      <author><first>Zhuolin</first><last>Jiang</last></author>
      <author><first>Amro</first><last>El-Jaroudi</last></author>
      <author><first>William</first><last>Hartmann</last></author>
      <author><first>Damianos</first><last>Karakos</last></author>
      <author><first>Lingjun</first><last>Zhao</last></author>
      <pages>26&#8211;31</pages>
      <abstract>Multiple neural language models have been developed recently, e.g., BERT and XLNet, and achieved impressive results in various NLP tasks including sentence classification, question answering and document ranking. In this paper, we explore the use of the popular bidirectional language model, BERT, to model and learn the relevance between English queries and foreign-language documents in the task of cross-lingual information retrieval. A deep relevance matching model based on BERT is introduced and trained by finetuning a pretrained multilingual BERT model with weak supervision, using home-made CLIR training data derived from parallel corpora. Experimental results of the retrieval of Lithuanian documents against short English queries show that our model is effective and outperforms the competitive baseline approaches.</abstract>
      <url hash="632e3463">2020.clssts-1.5</url>
      <language>eng</language>
      <bibkey>jiang-etal-2020-cross</bibkey>
    </paper>
    <paper id="6">
      <title>A Comparison of Unsupervised Methods for Ad hoc Cross-Lingual Document Retrieval</title>
      <author><first>Elaine</first><last>Zosa</last></author>
      <author><first>Mark</first><last>Granroth-Wilding</last></author>
      <author><first>Lidia</first><last>Pivovarova</last></author>
      <pages>32&#8211;37</pages>
      <abstract>We address the problem of linking related documents across languages in a multilingual collection. We evaluate three diverse unsupervised methods to represent and compare documents: (1) multilingual topic model; (2) cross-lingual document embeddings; and (3) Wasserstein distance.We test the performance of these methods in retrieving news articles in Swedish that are known to be related to a given Finnish article.The results show that ensembles of the methods outperform the stand-alone methods, suggesting that they capture complementary characteristics of the documents</abstract>
      <url hash="ccf36348">2020.clssts-1.6</url>
      <language>eng</language>
      <bibkey>zosa-etal-2020-comparison</bibkey>
    </paper>
    </volume>
</collection>