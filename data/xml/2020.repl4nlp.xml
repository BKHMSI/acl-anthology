<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.repl4nlp">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 5th Workshop on Representation Learning for NLP</booktitle>
      <editor><first>Spandana</first><last>Gella</last></editor>
      <editor><first>Johannes</first><last>Welbl</last></editor>
      <editor><first>Marek</first><last>Rei</last></editor>
      <editor><first>Fabio</first><last>Petroni</last></editor>
      <editor><first>Patrick</first><last>Lewis</last></editor>
      <editor><first>Emma</first><last>Strubell</last></editor>
      <editor><first>Minjoon</first><last>Seo</last></editor>
      <editor><first>Hannaneh</first><last>Hajishirzi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="67f2d50b">2020.repl4nlp-1</url>
    </meta>
    <frontmatter>
      <url hash="d40b3f5b">2020.repl4nlp-1.0</url>
      <bibkey>repl4nlp-2020-representation</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Zero-Resource Cross-Domain Named Entity Recognition</title>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>1–6</pages>
      <abstract>Existing models for cross-domain named entity recognition (NER) rely on numerous unlabeled corpus or labeled NER training data in target domains. However, collecting data for low-resource target domains is not only expensive but also time-consuming. Hence, we propose a cross-domain NER model that does not use any <a href="https://en.wikipedia.org/wiki/Resource_(computing)">external resources</a>. We first introduce a Multi-Task Learning (MTL) by adding a new <a href="https://en.wikipedia.org/wiki/Loss_function">objective function</a> to detect whether tokens are named entities or not. We then introduce a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> called Mixture of Entity Experts (MoEE) to improve the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> for zero-resource domain adaptation. Finally, experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms strong unsupervised cross-domain sequence labeling models, and the performance of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is close to that of the state-of-the-art model which leverages extensive resources.</abstract>
      <url hash="7fe916c9">2020.repl4nlp-1.1</url>
      <doi>10.18653/v1/2020.repl4nlp-1.1</doi>
      <video href="http://slideslive.com/38929767" />
      <bibkey>liu-etal-2020-zero</bibkey>
    </paper>
    <paper id="2">
      <title>Encodings of Source Syntax : Similarities in NMT Representations Across Target Languages<fixed-case>NMT</fixed-case> Representations Across Target Languages</title>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <author><first>Anna</first><last>Rafferty</last></author>
      <pages>7–16</pages>
      <abstract>We train neural machine translation (NMT) models from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to six target languages, using NMT encoder representations to predict ancestor constituent labels of source language words. We find that NMT encoders learn similar source syntax regardless of NMT target language, relying on explicit morphosyntactic cues to extract syntactic features from source sentences. Furthermore, the NMT encoders outperform RNNs trained directly on several of the constituent label prediction tasks, suggesting that NMT encoder representations can be used effectively for natural language tasks involving <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a>. However, both the NMT encoders and the directly-trained RNNs learn substantially different syntactic information from a probabilistic context-free grammar (PCFG) parser. Despite lower overall accuracy scores, the PCFG often performs well on sentences for which the RNN-based models perform poorly, suggesting that RNN architectures are constrained in the types of syntax they can learn.</abstract>
      <url hash="09498e6e">2020.repl4nlp-1.2</url>
      <doi>10.18653/v1/2020.repl4nlp-1.2</doi>
      <video href="http://slideslive.com/38929768" />
      <bibkey>chang-rafferty-2020-encodings</bibkey>
    </paper>
    <paper id="3">
      <title>Learning Probabilistic Sentence Representations from Paraphrases</title>
      <author><first>Mingda</first><last>Chen</last></author>
      <author><first>Kevin</first><last>Gimpel</last></author>
      <pages>17–23</pages>
      <abstract>Probabilistic word embeddings have shown effectiveness in capturing notions of generality and <a href="https://en.wikipedia.org/wiki/Logical_consequence">entailment</a>, but there is very little work on doing the analogous type of investigation for sentences. In this paper we define <a href="https://en.wikipedia.org/wiki/Statistical_model">probabilistic models</a> that produce <a href="https://en.wikipedia.org/wiki/Probability_distribution">distributions</a> for sentences. Our best-performing <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> treats each word as a <a href="https://en.wikipedia.org/wiki/Linear_map">linear transformation operator</a> applied to a <a href="https://en.wikipedia.org/wiki/Normal_distribution">multivariate Gaussian distribution</a>. We train our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> and demonstrate that they naturally capture sentence specificity. While our proposed model achieves the best performance overall, we also show that <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">specificity</a> is represented by simpler <a href="https://en.wikipedia.org/wiki/Computer_architecture">architectures</a> via the norm of the sentence vectors. Qualitative analysis shows that our probabilistic model captures sentential entailment and provides ways to analyze the specificity and preciseness of individual words.</abstract>
      <url hash="09f3e76d">2020.repl4nlp-1.3</url>
      <doi>10.18653/v1/2020.repl4nlp-1.3</doi>
      <video href="http://slideslive.com/38929769" />
      <bibkey>chen-gimpel-2020-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="4">
      <title>Word Embeddings as Tuples of Feature Probabilities</title>
      <author><first>Siddharth</first><last>Bhat</last></author>
      <author><first>Alok</first><last>Debnath</last></author>
      <author><first>Souvik</first><last>Banerjee</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>24–33</pages>
      <abstract>In this paper, we provide an alternate perspective on <a href="https://en.wikipedia.org/wiki/Word_(group_theory)">word representations</a>, by reinterpreting the dimensions of the vector space of a <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> as a collection of <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>. In this reinterpretation, every component of the word vector is normalized against all the word vectors in the vocabulary. This idea now allows us to view each vector as an n-tuple (akin to a fuzzy set), where n is the dimensionality of the word representation and each element represents the probability of the word possessing a <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">feature</a>. Indeed, this representation enables the use fuzzy set theoretic operations, such as <a href="https://en.wikipedia.org/wiki/Union_(set_theory)">union</a>, <a href="https://en.wikipedia.org/wiki/Intersection_(set_theory)">intersection</a> and <a href="https://en.wikipedia.org/wiki/Subtraction">difference</a>. Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements. We compare the performance of this representation with various <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a>, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations.<tex-math>n</tex-math>-tuple (akin to a fuzzy set), where <tex-math>n</tex-math> is the dimensionality of the word representation and each element represents the probability of the word possessing a feature. Indeed, this representation enables the use fuzzy set theoretic operations, such as union, intersection and difference. Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements. We compare the performance of this representation with various benchmarks, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations.</abstract>
      <url hash="12eb4a8f">2020.repl4nlp-1.4</url>
      <doi>10.18653/v1/2020.repl4nlp-1.4</doi>
      <video href="http://slideslive.com/38929770" />
      <bibkey>bhat-etal-2020-word</bibkey>
    <title_ar>تضمين كلمة على أنها مجموعات من احتمالات الميزة</title_ar>
      <title_fr>Les intégrations de mots en tant que tuples de probabilités de caractéristiques</title_fr>
      <title_pt>Incorporações de palavras como tuplas de probabilidades de recursos</title_pt>
      <title_es>Incrustaciones de palabras como tuplas de probabilidades de características</title_es>
      <title_ja>フィーチャーの確率のタプルとしてのWord埋め込み</title_ja>
      <title_hi>Word एम्बेडिंग सुविधा संभावनाओं के Tuples के रूप में</title_hi>
      <title_zh>为特征概率元组词嵌之</title_zh>
      <title_ru>Встраивание слов в виде кортежей вероятностей признаков</title_ru>
      <title_ga>Leabú Focal mar Thuplaí de Dhóchúlachtaí Gné</title_ga>
      <title_ka>სიტყვების შესაძლებლობების სიტყვები როგორც სიტყვების შესაძლებლობა</title_ka>
      <title_hu>Szóbeágyazások, mint a funkciók valószínűségeinek tupletei</title_hu>
      <title_el>Ενσωματώσεις λέξεων ως Tuples πιθανών χαρακτηριστικών</title_el>
      <title_it>Embedding delle parole come Tuples of Feature Probabilities</title_it>
      <title_kk>Сөздің ендіру мүмкіндіктерінің мәліметтері ретінде</title_kk>
      <title_lt>žodžių įterpimas kaip galimų požymių vamzdeliai</title_lt>
      <title_mk>Вклучување на зборови како купки на веројатности</title_mk>
      <title_ms>Name</title_ms>
      <title_ml>വാക്ക് എംബെഡിങ്ങുകള്‍ സാധ്യതകളുടെ ടൂപ്ലുകളായി വാക്കുകള്‍</title_ml>
      <title_mt>L-inkorporazzjoni tal-kliem bħala Tuples of Feature Probabilities</title_mt>
      <title_mn>Үг нэвтрүүлэх боломжуудын Tuples of Feature Probabilities</title_mn>
      <title_no>Innebygging av ord som eksemplar av funksjonssannsynlighetar</title_no>
      <title_pl>Wkładanie słów jako Tuples prawdopodobieństw funkcji</title_pl>
      <title_ro>Încorporarea cuvintelor ca Tuples de probabilități ale caracteristicilor</title_ro>
      <title_sr>Времена речи као Вероватности функција</title_sr>
      <title_so>Heeganka hadalka sida tusaale ahaan suurtagalka ah</title_so>
      <title_si>Name</title_si>
      <title_sv>Ordinbäddningar som bitar av funktionssannolikheter</title_sv>
      <title_ta>வார்த்தை உட்பொதிக்கும் பண்புகளின் சிக்கல்களாக</title_ta>
      <title_ur>ویڈ انڈینگ کے طور پر ویڈ انڈینگ</title_ur>
      <title_uz>Comment</title_uz>
      <title_vi>Thiết lập chữ làm ống kính dự tính</title_vi>
      <title_nl>Woordinsluitingen als Tuples van Feature Waarschijnlijkheid</title_nl>
      <title_da>Word Embeddings as Tuples of Feature Sandsynligheder</title_da>
      <title_bg>Вградвания на думи като двойки от вероятности за характеристика</title_bg>
      <title_hr>Usporavanje riječi kao vrpce mogućnosti karaktera</title_hr>
      <title_ko>특징 확률 원조로서의 단어 삽입</title_ko>
      <title_de>Wort-Einbettungen als Tupel von Feature-Wahrscheinlichkeiten</title_de>
      <title_id>Pencampuran kata sebagai Tuples of Feature Probabilities</title_id>
      <title_fa>وارد کردن کلمات به عنوان مثال احتمالات ویژه</title_fa>
      <title_tr>Mahsuratyň Mumkinçiliki Däpli Sözler</title_tr>
      <title_sw>Matokeo ya Matokeo kama Tuzo ya Tafadhali</title_sw>
      <title_af>Woord Inbêding as Tuples van Funksie Moontlikheid</title_af>
      <title_sq>Përfshirja e fjalëve si Tuples of Feature Probabilities</title_sq>
      <title_am>የፊደል ቅርጽ ምርጫዎች</title_am>
      <title_hy>Բառերի ներգրավումը որպես առանձնահատկությունների խումբ</title_hy>
      <title_az>S칬zl칲k 캻fad톛l톛ri 칐l칞칲s칲 M칲mk칲nl칲kl톛ri kimi S칬zl칲k 캻fad톛l톛ri</title_az>
      <title_bn>বৈশিষ্ট্য সম্ভাবনার টাইপল হিসেবে শব্দ বিবরণ</title_bn>
      <title_bs>Uklapanje riječi kao vrpce mogućnosti karaktera</title_bs>
      <title_ca>Incorporació de paraules com tubles de probabilitats de característiques</title_ca>
      <title_cs>Vložení slov jako Tuples pravděpodobností funkcí</title_cs>
      <title_et>Sõnade põimimised funktsioonide tõenäosuste tuplitena</title_et>
      <title_fi>Sanaupotukset ominaisuustodennĂ¤kĂ¶isyyksien tupleina</title_fi>
      <title_ha>@ action</title_ha>
      <title_he>קידום מילים ככפולות של סבירות תכונות</title_he>
      <title_sk>Vgradnje besed kot vzorci verjetnosti lastnosti</title_sk>
      <title_bo>ཆོས་ཆོས་ལུགས་ཀྱི་དཔེ་དབྱིབས་སྔོན་སྒྲིག་ཀྱི་ཡིག་ཚན་བསྡུས་པ</title_bo>
      <title_jv>structural navigation</title_jv>
      <abstract_ar>في هذه الورقة ، نقدم منظورًا بديلاً لتمثيل الكلمات ، من خلال إعادة تفسير أبعاد فضاء متجه لتضمين كلمة كمجموعة من الميزات. في إعادة التفسير هذه ، يتم تطبيع كل مكون من متجه الكلمات مقابل جميع متجهات الكلمات في المفردات. تتيح لنا هذه الفكرة الآن عرض كل متجه على أنه n-tuple (على غرار مجموعة ضبابية) ، حيث n هي أبعاد تمثيل الكلمة ويمثل كل عنصر احتمال امتلاك الكلمة لميزة. في الواقع ، يتيح هذا التمثيل استخدام العمليات النظرية لمجموعة ضبابية ، مثل الاتحاد والتقاطع والاختلاف. على عكس المحاولات السابقة ، نظهر أن تمثيل الكلمات هذا يوفر فكرة تشابه غير متماثلة بطبيعتها وبالتالي فهي أقرب إلى أحكام التشابه البشري. نحن نقارن أداء هذا التمثيل بمعايير مختلفة ، ونستكشف بعض الخصائص الفريدة بما في ذلك اكتشاف الكلمات الوظيفية ، واكتشاف الكلمات متعددة المعاني ، وبعض التبصر في التفسير الذي توفره العمليات النظرية المحددة.</abstract_ar>
      <abstract_pt>Neste artigo, fornecemos uma perspectiva alternativa sobre representações de palavras, reinterpretando as dimensões do espaço vetorial de uma incorporação de palavras como uma coleção de recursos. Nesta reinterpretação, cada componente do vetor de palavras é normalizado em relação a todos os vetores de palavras no vocabulário. Essa ideia agora nos permite ver cada vetor como uma n-tupla (semelhante a um conjunto fuzzy), onde n é a dimensionalidade da representação da palavra e cada elemento representa a probabilidade da palavra possuir uma característica. De fato, essa representação possibilita o uso de operações teóricas de conjuntos fuzzy, como união, interseção e diferença. Ao contrário de tentativas anteriores, mostramos que essa representação de palavras fornece uma noção de semelhança inerentemente assimétrica e, portanto, mais próxima dos julgamentos de semelhança humanos. Comparamos o desempenho dessa representação com vários benchmarks e exploramos algumas das propriedades exclusivas, incluindo detecção de palavras funcionais, detecção de palavras polissêmicas e algumas informações sobre a interpretabilidade fornecida pelas operações teóricas de conjuntos.</abstract_pt>
      <abstract_fr>Dans cet article, nous proposons une perspective alternative sur les représentations de mots, en réinterprétant les dimensions de l'espace vectoriel d'un incorporation de mots comme un ensemble de caractéristiques. Dans cette réinterprétation, chaque composant du vecteur de mot est normalisé par rapport à tous les vecteurs de mots du vocabulaire. Cette idée nous permet maintenant de voir chaque vecteur comme un n-uplet (similaire à un ensemble flou), où n est la dimensionnalité de la représentation du mot et chaque élément représente la probabilité que le mot possède une caractéristique. En effet, cette représentation permet d'utiliser des opérations théoriques d'ensembles flous, telles que l'union, l'intersection et la différence. Contrairement aux tentatives précédentes, nous montrons que cette représentation des mots fournit une notion de similitude qui est intrinsèquement asymétrique et donc plus proche des jugements de similitude humaine. Nous comparons les performances de cette représentation avec divers points de référence et explorons certaines des propriétés uniques, notamment la détection de mots de fonction, la détection de mots polysémiques et un aperçu de l'interprétabilité fournie par les opérations théoriques des ensembles.</abstract_fr>
      <abstract_es>En este artículo, ofrecemos una perspectiva alternativa sobre las representaciones de palabras, reinterpretando las dimensiones del espacio vectorial de una palabra incrustada como una colección de características. En esta reinterpretación, cada componente del vector de palabras se normaliza con respecto a todos los vectores de palabras del vocabulario. Esta idea ahora nos permite ver cada vector como una n-tupla (similar a un conjunto difuso), donde n es la dimensionalidad de la representación de la palabra y cada elemento representa la probabilidad de que la palabra posea una característica. De hecho, esta representación permite el uso de operaciones teóricas de conjuntos difusos, como unión, intersección y diferencia. A diferencia de intentos anteriores, demostramos que esta representación de palabras proporciona una noción de similitud que es inherentemente asimétrica y, por lo tanto, más cercana a los juicios de similitud humana. Comparamos el rendimiento de esta representación con varios puntos de referencia y exploramos algunas de las propiedades únicas, incluida la detección de palabras de función, la detección de palabras polisémicas y algunos conocimientos sobre la interpretabilidad que proporcionan las operaciones teóricas de conjuntos.</abstract_es>
      <abstract_ja>本稿では、単語埋め込みのベクトル空間の次元を特徴の集合として再解釈することにより、単語表現に関する別の視点を提供する。 この再解釈では、単語ベクトルのすべての成分は、語彙内のすべての単語ベクトルに対して正規化される。 このアイデアにより、各ベクトルをnタプル（曖昧な集合に似ている）と見なすことができます。ここで、nは単語表現の次元数であり、各要素は、特徴を持つ単語の確率を表します。 実際、この表現は、結合、交差、差分などのファジィ集合論的演算の使用を可能にする。 これまでの試みとは異なり、この言葉の表現は、本質的に非対称であり、したがって人間の類似性の判断に近い類似性の概念を提供することを示している。 この表現のパフォーマンスを様々なベンチマークと比較し、機能単語の検出、多義的単語の検出、およびセット理論的操作によって提供される解釈可能性に関するいくつかの洞察を含む固有の特性のいくつかを探索します。</abstract_ja>
      <abstract_zh>本文中,重解向量空维度为特徵所集,供单词一视角。 词向量者,词汇之所有词向量规范化。 今许以向量为一n元组(类模糊集),其n单词之维数,元素单词有征概率。 实者,使能用模糊集论运算,如并集、交集、差分。 异乎前,单词有相似性名,非其名也,近人之相似性也。 校其性,原其独,包其功能,多义词其检测,与集论可解释性之见。</abstract_zh>
      <abstract_ru>В этой статье мы представляем альтернативную точку зрения на представления слов, переосмысливая размеры векторного пространства вложения слов как совокупность признаков. В этой повторной интерпретации каждый компонент слова-вектора нормируется относительно всех слов-векторов в словаре. Эта идея теперь позволяет рассматривать каждый вектор как n-кортеж (сродни нечеткому множеству), где n - размерность представления слова и каждый элемент представляет вероятность того, что слово обладает признаком. Действительно, это представление позволяет использовать теоретические операции нечеткого множества, такие как объединение, пересечение и разность. В отличие от предыдущих попыток, мы показываем, что это представление слов обеспечивает понятие сходства, которое по своей сути асимметрично и, следовательно, ближе к суждениям о человеческом сходстве. Мы сравниваем производительность этого представления с различными критериями и исследуем некоторые уникальные свойства, включая обнаружение функциональных слов, обнаружение многозначных слов и некоторое понимание интерпретируемости, обеспечиваемой множеством теоретических операций.</abstract_ru>
      <abstract_hi>इस पेपर में, हम शब्दों के प्रतिनिधित्व पर एक वैकल्पिक परिप्रेक्ष्य प्रदान करते हैं, सुविधाओं के संग्रह के रूप में एम्बेडिंग शब्द के वेक्टर स्पेस के आयामों को फिर से व्याख्या करके। इस पुनर्व्याख्या में, शब्द वेक्टर के प्रत्येक घटक को शब्दावली में सभी शब्द वैक्टरों के खिलाफ सामान्यीकृत किया जाता है। यह विचार अब हमें प्रत्येक वेक्टर को एन-टुपल (एक फजी सेट के समान) के रूप में देखने की अनुमति देता है, जहां एन शब्द प्रतिनिधित्व की आयामीता है और प्रत्येक तत्व एक विशेषता रखने वाले शब्द की संभावना का प्रतिनिधित्व करता है। दरअसल, यह प्रतिनिधित्व फजी सेट सैद्धांतिक संचालन, जैसे संघ, प्रतिच्छेदन और अंतर के उपयोग को सक्षम बनाता है। पिछले प्रयासों के विपरीत, हम दिखाते हैं कि शब्दों का यह प्रतिनिधित्व समानता की धारणा प्रदान करता है जो स्वाभाविक रूप से असममित है और इसलिए मानव समानता के फैसले के करीब है। हम विभिन्न बेंचमार्क के साथ इस प्रतिनिधित्व के प्रदर्शन की तुलना करते हैं, और फ़ंक्शन वर्ड डिटेक्शन, पॉलीसेमस शब्दों का पता लगाने और सेट सैद्धांतिक संचालन द्वारा प्रदान की गई व्याख्याक्षमता में कुछ अंतर्दृष्टि सहित कुछ अद्वितीय गुणों का पता लगाते हैं।</abstract_hi>
      <abstract_ga>Sa pháipéar seo, cuirimid peirspictíocht mhalartach ar fáil ar léirithe focal, trí thoisí an spáis veicteora a bhaineann le neadú focal a athléiriú mar bhailiúchán gnéithe. San athléiriú seo, normalaítear gach comhpháirt den fhocal veicteoir i gcoinne gach veicteoir focal sa stór focal. Ligeann an smaoineamh seo dúinn anois féachaint ar gach veicteoir mar n-tuple (cosúil le tacair doiléir), áit arb é n toiseachas léiriú an fhocail agus seasann gach eilimint don dóchúlacht go bhfuil gné ag an bhfocal. Go deimhin, cuireann an léiriú seo ar chumas oibríochtaí teoiriciúla tacair doiléir a úsáid, amhail aontas, trasnú agus difríocht. Murab ionann agus iarrachtaí roimhe seo, léirímid go soláthraíonn an léiriú seo ar fhocail nóisean cosúlachta atá neamhshiméadrach ó dhúchas agus mar sin níos gaire do bhreithiúnais chosúlachta daonna. Déanaimid comparáid idir feidhmíocht na hionadaíochta seo agus tagarmharcanna éagsúla, agus déanaimid iniúchadh ar roinnt de na hairíonna uathúla lena n-áirítear brath focal feidhme, braite focail ilghnéitheacha, agus roinnt léargas ar an inléiritheacht a sholáthraíonn oibríochtaí teoiriciúla socraithe.</abstract_ga>
      <abstract_el>Στην παρούσα εργασία, παρέχουμε μια εναλλακτική προοπτική για τις αναπαραστάσεις λέξεων, επανερμηνεύοντας τις διαστάσεις του διανυσματικού χώρου μιας λέξης που ενσωματώνεται ως συλλογή χαρακτηριστικών. Σε αυτή την επανερμηνεία, κάθε συστατικό του διανύσματος της λέξης κανονικοποιείται σε σχέση με όλα τα διανύσματα λέξεων στο λεξιλόγιο. Αυτή η ιδέα μας επιτρέπει τώρα να δούμε κάθε διάνυσμα ως ένα N-Tuple (παρόμοιο με ένα ασαφές σύνολο), όπου n είναι η διαστασιμότητα της λέξης αναπαράσταση και κάθε στοιχείο αντιπροσωπεύει την πιθανότητα της λέξης να κατέχει ένα χαρακτηριστικό. Πράγματι, αυτή η αναπαράσταση επιτρέπει τη χρήση θεωρητικών λειτουργιών ασαφής συνόλου, όπως η ένωση, η τομή και η διαφορά. Σε αντίθεση με προηγούμενες προσπάθειες, δείχνουμε ότι αυτή η αναπαράσταση των λέξεων παρέχει μια έννοια ομοιότητας η οποία είναι εγγενώς ασύμμετρη και ως εκ τούτου πιο κοντά στις ανθρώπινες εκτιμήσεις ομοιότητας. Συγκρίνουμε την απόδοση αυτής της αναπαράστασης με διάφορα κριτήρια αναφοράς, και εξερευνούμε μερικές από τις μοναδικές ιδιότητες, συμπεριλαμβανομένης της ανίχνευσης λέξεων συνάρτησης, της ανίχνευσης πολυσυναισθηματικών λέξεων, και κάποια κατανόηση της ερμηνείας που παρέχεται από τις θεωρητικές λειτουργίες συνόλων.</abstract_el>
      <abstract_hu>Ebben a tanulmányban alternatív perspektívát nyújtunk a szóreprezentációkra, úgy, hogy újraértelmezzük egy szó vektortérének dimenzióit, mint jellemzők gyűjteményét. Ebben az újraértelmezésben a szó vektorának minden összetevője normalizálódik a szókincs összes vektorával szemben. Ez az ötlet lehetővé teszi, hogy minden vektort n-tuple-ként tekintsünk (hasonló egy fuzzy halmazhoz), ahol n a szó ábrázolásának dimenziója és minden elem azt a valószínűséget jelenti, hogy a szó rendelkezik egy funkcióval. Valójában ez az ábrázolás lehetővé teszi a fuzzy halmaz elméleti műveletek használatát, mint például unió, metszés és különbség. A korábbi próbálkozásokkal ellentétben azt mutatjuk, hogy ez a szavak ábrázolása olyan hasonlóság fogalmát biztosítja, amely eredendően aszimmetrikus, és így közelebb kerül az emberi hasonlósági ítéletekhez. Összehasonlítjuk ennek az ábrázolásnak a teljesítményét különböző referenciaértékekkel, és feltárjuk néhány egyedi tulajdonságot, beleértve a függvényszó felismerését, a poliszemózus szavak felismerését, valamint a halmazelméleti műveletek értelmezhetőségét.</abstract_hu>
      <abstract_ka>ამ გვერდიში, ჩვენ გვაქვს სხვადასხვა პერსპექტიკური პერსპექტიკური სიტყვების გამოსახულების განზომილებების განზომილებით, რომლებიც სიტყვების კოლექტიკური განზომილებები ამ განსხვავებაში, ყველა სიტყვის გვექტორის კომპონენტი ნორმალიზებულია ყველა სიტყვის გვექტორის განმავლობაში. ამ იდეა ახლა გვაქვს, რომ ყოველ გვეკტორის n-tuple (როგორც მუშაობელი ნაწილი), სადაც n არის სიტყვის გამოსახულების განზომილებელობა და ყოველ ელემენტი გამოსახულება სიტყვის შესაძლებლობა. ეს გამოყენება შესაძლებელია გამოყენება თეორეტიკური операциები, როგორც საერთო, საშუალოდ და განსხვავება. წინა პროცემების განმავლობაში, ჩვენ ჩვენ ჩვენ ჩვენ აჩვენებთ, რომ ეს სიტყვების გამოსახულება იყოს სხვადასხვების წარმოდგენა, რომელიც საშუალოდ არიმეტრიული და ამიტომ ადამიანის სხვად ჩვენ ამ გამოსახულების გამოსახულებას განსხვავებული ბენქმარკებით შემდგენებთ და განსხვავებთ განსხვავებული განსხვავებების განსხვავებას, ფუნქციის სიტყვების განსხვავებას, პოლისემის სიტყვების განსხვავებას</abstract_ka>
      <abstract_it>In questo articolo, forniamo una prospettiva alternativa sulle rappresentazioni di parole, reinterpretando le dimensioni dello spazio vettoriale di una parola che incorpora come una raccolta di caratteristiche. In questa reinterpretazione, ogni componente del vettore della parola viene normalizzato rispetto a tutti i vettori della parola nel vocabolario. Questa idea ci permette ora di vedere ogni vettore come una n-tupla (simile ad un insieme sfocato), dove n è la dimensionalità della rappresentazione della parola e ogni elemento rappresenta la probabilità che la parola possieda una caratteristica. Infatti, questa rappresentazione consente l'uso di operazioni teoriche di set fuzzy, come unione, intersezione e differenza. A differenza dei tentativi precedenti, mostriamo che questa rappresentazione delle parole fornisce una nozione di somiglianza intrinsecamente asimmetrica e quindi più vicina ai giudizi di somiglianza umana. Confrontiamo le prestazioni di questa rappresentazione con vari parametri di riferimento ed esploriamo alcune delle proprietà uniche tra cui il rilevamento delle parole funzione, il rilevamento di parole polisemose e alcune informazioni sull'interpretabilità fornita dalle operazioni teoriche degli insiemi.</abstract_it>
      <abstract_lt>Šiame dokumente teikiame alternatyvią perspektyvą žodžių atspindėjimų atžvilgiu, iš naujo aiškindami vektoriaus erdvės matmenis žodžio įterpimo kaip savybių rinkinio. Šiame reinterpretacijoje kiekvienas žodžio vektoriaus komponentas normalizuojamas prieš visus žodžio vektorius žodyne. Ši idėja dabar leidžia mums žiūrėti į kiekvieną vektorių kaip n-dublą (panašų į apgaulingą rinkinį), kur n yra žodžio reprezentacijos matmenys ir kiekvienas element as rodo žodžio, turinčio savybę, tikimybę. Iš tiesų, šis atstovavimas leidžia naudoti netinkamas teorines operacijas, pvz., sąjungą, skerspjūvį ir skirtumą. Priešingai nei ankstesni bandymai, mes rodome, kad šis žodžių atstovavimas suteikia panašumo sąvoką, kuri iš esmės yra asimetriška ir todėl artimesnė žmogaus panašumo vertinimams. Palyginame šio atvaizdavimo rezultatus su įvairiais lyginamaisiais rodikliais ir tiriame kai kurias unikalias savybes, įskaitant funkcijos žodžių aptikimą, polizieminių žodžių aptikimą ir tam tikrą supratimą apie aiškinamumą, kurį suteikia teorinės operacijos.</abstract_lt>
      <abstract_mk>Во овој весник, ние обезбедуваме алтернативна перспектива за претставувањата на зборовите, со реинтерпретација на димензиите на векторниот простор на зборот вграден како колекција на карактеристики. In this reinterpretation, every component of the word vector is normalized against all the word vectors in the vocabulary.  Оваа идеја сега ни овозможува да го гледаме секој вектор како n-tuple (сличен на нејасниот сет), каде n е димензионалноста на репрезентацијата на зборот и секој елемент ја претставува веројатноста на зборот кој поседува функционалност. Всушност, ова претставување овозможува употреба на теоретски операции, како што се синдикат, премин и разлика. За разлика од претходните обиди, покажуваме дека ова претставување на зборовите обезбедува идеја за сличност која е природно асиметрична и оттука поблиску до судиите за човечка сличност. Ги споредуваме изведувањата на оваа претстава со различни референтни значки, и истражуваме некои од уникатните сопствености вклучувајќи детекција на функционалните зборови, детекција на полисемни зборови, и некои погледи во интерпретабилноста обезбедена од поставени теоретички опера</abstract_mk>
      <abstract_kk>Бұл қағазда сөздерді түсіндіру үшін басқа перспективті келтіреміз. Бұл сөздерді қайта түсіндіру үшін ендірілген сөздердің өлшемдерін қайта түсіндіреміз. Бұл қайта аудару үшін сөздің векторының әрбір компоненті сөздің векторының барлық сөздеріне қарсы нормализацияланады. Бұл идея қазір біз әрбір векторды n- tuple ретінде қарауға мүмкіндік береді (бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл б Шынымен, бұл түсініктеме теоретикалық операцияларды қолдану мүмкіндігін мүмкіндік береді, мысалы, бірлік, бөлшектеу және айырмашылығы. Алдыңғы әрекеттерге сәйкес, біз сөздердің көрсетілімі бұл сөздердің ұқсас тәртіпсіздігін көрсетеді, сондықтан адамдардың ұқсас тәртіпсіздігіне жақын көрсетеді. Біз бұл кескіндікті түрлі белгілерден салыстырып, функциялық сөздерді анықтау, полиземді сөздерді анықтау және теоретикалық операцияларды орнату үшін кейбір түсініктемелерді қалаймыз.</abstract_kk>
      <abstract_ml>ഈ പത്രത്തില്‍, വാക്കിന്റെ പ്രതിനിധികളെ വെക്റ്റര്‍ സ്ഥലത്തിന്റെ ഭാഗങ്ങള്‍ വീണ്ടും ചേര്‍ക്കുന്ന വാക്കുകളുടെ വിശേഷത്തിന്റെ വ്യത്യസ ഈ വാക്ക് വെക്റ്ററിന്റെ എല്ലാ ഭാഗങ്ങളും വാക്ക് വെക്റ്ററുകള്‍ക്കെതിരെ സാധാരണമാക്കിയിരിക്കുന്നു. ഈ ഐഡിയ ഇപ്പോള്‍ നമ്മള്‍ ഓരോ വെക്റ്റര്‍ക്കും n-ടുപ്പിള്‍ ആയി കാണാന്‍ അനുവദിക്കുന്നു. വാക്കിന്റെ പ്രതിനിധിയുടെ സ്ഥിതിയില്‍ നിന്നും വാക്കിന്റെ സാധ സത്യത്തില്‍, ഈ പ്രതിനിധിയില്‍ പ്രദര്‍ശിപ്പിക്കുന്നത് തിയോറിറ്റിക് പ്രവര്‍ത്തനങ്ങള്‍ സജ്ജീകരിക്കുന്നത് പോലെ മുമ്പുള്ള ശ്രമം വ്യത്യസ്തമായിരുന്നു, ഈ വാക്കുകളുടെ പ്രതിനിധിയില്‍ ഒരുപോലെയുള്ള ഒരു ആശയം കൊടുക്കുന്നു. അതിനാല്‍ അതിനാല്‍ മനുഷ്യരുട ഈ പ്രതിനിധിയുടെ പ്രഭാവം വ്യത്യസ്ത ബെന്‍മാര്‍ക്കുകളോടൊപ്പം താല്‍പ്പിക്കുകയും, ചില പ്രതിനിധികള്‍ പരിശോധിക്കുകയും, ഫങ്ഷന്‍ വാക്ക് കണ്ടുപിടിക്കു</abstract_ml>
      <abstract_ms>Dalam kertas ini, kita menyediakan perspektif alternatif pada perwakilan perkataan, dengan menerangkan semula dimensi ruang vektor perkataan yang ditambah sebagai koleksi ciri-ciri. Dalam penerangan semula ini, setiap komponen vektor perkataan normalisasi melawan semua vektor perkataan dalam vokbulari. Idea ini kini membolehkan kita melihat setiap vektor sebagai n-tuple (sama dengan set berlebihan), di mana n ialah dimensi perwakilan perkataan dan setiap elemen mewakili kebarangkalian perkataan yang mempunyai ciri. Indeed, this representation enables the use fuzzy set theoretic operations, such as union, intersection and difference.  Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements.  Kami membandingkan prestasi perwakilan ini dengan berbagai tanda referensi, dan mengeksplorasi beberapa ciri-ciri unik termasuk pengesan perkataan fungsi, pengesan perkataan polisemus, dan beberapa pandangan ke dalam pengenalan yang diberikan oleh set operasi teori.</abstract_ms>
      <abstract_mt>F’dan id-dokument, nagħtu perspettiva alternattiva dwar ir-rappreżentazzjonijiet tal-kliem, billi ninterpretaw mill-ġdid id-dimensjonijiet tal-ispazju tal-vetturi ta’ kelma inkorporata bħala ġabra ta’ karatteristiċi. F’din l-interpretazzjoni mill-ġdid, kull komponent tal-kelma vector huwa normalizzat kontra l-kelma vectors kollha fil-vokabulari. This idea now allows us to view each vector as an n-tuple (akin to a fuzzy set), where n is the dimensionality of the word representation and each element represents the probability of the word possessing a feature.  Tabilħaqq, din ir-rappreżentanza tippermetti l-użu ta’ operazzjonijiet teoretiċi ssettjati f’daqqa, bħall-unjoni, l-intersezzjoni u d-differenza. Għall-kuntrarju ta’ tentattivi preċedenti, nuru li din ir-rappreżentazzjoni tal-kliem tipprovdi kunċett ta’ similarità li huwa inerentement asimetriku u għalhekk eqreb lejn ġudizzji ta’ similarità umana. Aħna nqabblu l-prestazzjoni ta’ din ir-rappreżentazzjoni ma’ diversi punti ta’ riferiment, u nesploraw xi wħud mill-karatteristiċi uniċi inklużi l-individwazzjoni tal-kliem tal-funzjoni, l-individwazzjoni tal-kliem poliżimu, u xi ħarsa lejn l-interpretabbiltà pprovduta minn operazzjonijiet teoretiċi stabbiliti.</abstract_mt>
      <abstract_pl>W niniejszym artykule przedstawiamy alternatywną perspektywę reprezentacji słów poprzez reinterpretację wymiarów przestrzeni wektorowej osadzonego słowa jako zbiór cech. W tej reinterpretacji każdy składnik wektora słowa jest znormalizowany w stosunku do wszystkich wektorów słowa w słownictwie. Pomysł ten pozwala nam teraz spojrzeć na każdy wektor jako n-kropel (podobny do zbioru rozmytego), gdzie n jest wymiarowością reprezentacji słowa, a każdy element reprezentuje prawdopodobieństwo, że słowo posiada cechę. Rzeczywiście, ta reprezentacja umożliwia wykorzystanie operacji teoretycznych zbiorów rozmytych, takich jak unia, przecięcie i różnica. W przeciwieństwie do poprzednich prób, pokazujemy, że ta reprezentacja słów daje pojęcie podobieństwa, które jest z natury asymetryczne, a tym samym bliższe ludzkiej ocenie podobieństwa. Porównujemy wydajność tej reprezentacji z różnymi wskaźnikami referencyjnymi i badamy niektóre z unikalnych właściwości, w tym wykrywanie słów funkcyjnych, wykrywanie słów wielosemicznych i pewne wglądy w interpretowalność zapewnianą przez operacje teoretyczne zbiorów.</abstract_pl>
      <abstract_ro>În această lucrare, oferim o perspectivă alternativă asupra reprezentărilor cuvintelor, prin reinterpretarea dimensiunilor spațiului vectorial al unui cuvânt încorporat ca o colecție de caracteristici. În această reinterpretare, fiecare componentă a vectorului cuvântului este normalizată în raport cu toți vectorii de cuvânt din vocabular. Această idee ne permite acum să vedem fiecare vector ca un n-tuple (asemănător cu un set neclar), unde n este dimensiunea reprezentării cuvântului și fiecare element reprezintă probabilitatea ca cuvântul să aibă o caracteristică. Într-adevăr, această reprezentare permite utilizarea operațiunilor teoretice ale setului fuzzy, cum ar fi uniunea, intersecția și diferența. Spre deosebire de încercările anterioare, arătăm că această reprezentare a cuvintelor oferă o noțiune de similitudine care este inerent asimetrică și, prin urmare, mai aproape de judecățile de similitudine umane. Comparăm performanța acestei reprezentări cu diferite criterii de referință și explorăm unele dintre proprietățile unice, inclusiv detectarea cuvintelor funcționale, detectarea cuvintelor polisemoase și o perspectivă asupra interpretabilității oferite de operațiunile teoretice set.</abstract_ro>
      <abstract_mn>Энэ цаасан дээр бид үг илэрхийллийн тухай өөрчлөлт харагдаж, өөрчлөгдөж буй хэмжээсүүдийг өөрчлөгдөж, өөрчлөгдөж буй хэмжээсүүдийн хэмжээсүүдийг өөрчлөгдөж байна. Энэ дахин ойлголтын тулд, үгийн векторын бүх хэсэг нь үгийн бүх векторуудын эсрэг нормалтай байдаг. Энэ санаа одоо бидэнд вектор бүрийг n-tuple гэж үзэх боломжтой болгодог. n нь үгийн илэрхийллийн хэмжээсүүд ба элемент бүр үгийн магадлалыг илэрхийлдэг. Үнэндээ энэ илэрхийлэл нь холбоотой, холбоотой, ялгааг ашиглаж чадна. Өмнөх хичээлийн эсрэгээр бид энэ үгнүүдийн үзүүлэлт нь нэг төстэй төстэй ойлголтыг харуулж байна. Энэ нь хүний төстэй төстэй шүүмжүүдэд илүү ойрхон байдаг. Бид энэ илтгэлийн үйл ажиллагааг олон тоонуудыг харьцуулж, функцын үг олох, полизим үг олох, теоретикийн үйл ажиллагаанд өгсөн утгыг олох боломжтой байдлыг судалж байна.</abstract_mn>
      <abstract_no>I denne papiret gir vi ein alternativ perspektiv på ordrepresentasjonar ved å gjenoppretta dimensjonane av vektorrommet til eit ord innebygd som samling av funksjonar. I denne omsetjinga vert kvar komponent av ordvektoren normalisert mot alle ordvektorane i ordboka. Denne ideen gjer oss n å å sjå kvar vektor som n-tuple (liknande til eit utrygt sett), der n er dimensjonaliteten av ordrepresentasjonen og kvar element representerer sannsynligheten for ordet som har ein funksjon. Dette representasjonen slår på at bruken av tråd sett teoretiske operasjonar, som union, kryss av og forskjellen. I motsetjing til førre forsøk viser vi at denne representasjonen av ord gjev eit noe på likningar som er innehaldet asymmetrisk og derfor nærmere menneskelige forsøk. Vi samanliknar utviklinga av denne representasjonen med ulike benchmarker, og utforsk nokre av dei unike eigenskapane, inkludert funksjonsoppdaging av ord, oppdaging av polysemiske ord, og nokre innsikt i uttolkinga gjeven av teoretiske operasjonar.</abstract_no>
      <abstract_sr>U ovom papiru pružamo alternativnu perspektivu predstavljanja reèi, ponovno pretvaranjem dimenzija vektorskog prostora reèi koja se ukljuèuje kao kolekcija karakteristika. U ovoj reinterpretaciji, svaki komponent rečenog vektora se normalizira protiv svih rečenih vektora u rečniku. Ova ideja nam sada omogućava da gledamo svaki vektor kao n-tuple (sličan n a fuzzy set), gdje n je dimenzionalnost predstavljanja riječi i svaki element predstavlja verovatnoću reči koja posjeduje funkciju. Zapravo, ova predstava omogućava korištenje neobičnih teorijskih operacija, poput sindikata, prekidanja i razlike. Za razliku od prethodnih pokušaja, pokazujemo da ova predstavljanja reèi pruža pojam sliènosti koja je inherentno asimetrična i stoga bliže osuđivanju ljudske sliènosti. Uspoređujemo provedbu ove predstave sa različitim kriterijama, i istražujemo neke od jedinstvenih vlasništva uključujući otkrivanje funkcionalnih reči, otkrivanje polizemnih reči, i neke uvide o interpretabilnosti pruženoj teoretičkim operacijama.</abstract_sr>
      <abstract_si>මේ පැත්තේ, අපි වචන ප්‍රතිරූපයක් ගැන වෙනස් ප්‍රතිරූපයක් දෙනවා, වචන ප්‍රතිරූපයක් වගේ වෙක්ටර් අවසානයේ වචන අවසානයේ  මේ ආපහු ප්‍රවේශනයේදී, වචන වෙක්ටර්ගේ හැම අංකයක්ම සාමාන්‍ය වෙක්ටර් වලින් වචන වෙක්ටර් වලට සාමාන්‍ය මේ අදහසය දැන් අපිට අවශ්‍ය කරන්න පුළුවන් හැම වෙක්ටර්ටර්ටර්ටර්ටර්ටර්ටර්ටර්ටර්ටර්ටර්ටර් n- ටුප්ල් වගේ බලන්න (පුළුවන් සෙට් වග ඇත්තටම, මේ ප්‍රතිනිධානය ප්‍රයෝජනය කරන්න පුළුවන් වෙන්න පුළුවන් විදිහට සාධාන්‍ය ව්‍යාපෘතික වැඩ අපි පෙන්වන්නේ මුලින් උත්සාහ කරපු විදියට, මේ වචනේ ප්‍රතිචාරයක් ප්‍රතිචාරයක් ප්‍රතිචාරයක් වෙනවා කියලා ප්‍රතිචාරයක්  අපි මේ ප්‍රතිනිධානයේ විවිධ බෙන්ච්මාර්ක්ස් එක්ක සම්බන්ධ කරනවා, සමහර විශේෂ වචන පරික්ෂණය සමහර විශේෂ වචන පරික්ෂණය සමහර විශ</abstract_si>
      <abstract_so>Qoraalkan waxaynu ku siinaynaa aragti kale oo ku saabsan qofka hadalka la soo jeedo, kaas oo ku qoraya qaybaha goobta vectorka ee ereyga ku qoran waxyaabo badan. Qayb kasta oo qeyb ka mid ah waxqabadka ereyga waxaa lagu caadi karaa wax walba oo ka gees ah vectoryada hadalka ee hadalka ku qoran. Fikirkaasi wuxuu inagu ogolaan karaa inaan wax walbo ka aragno n-tijaab (sida saxda dhibaato ah), meesha ay ku jirto taxadirka hadalka, qayb kastana waxaa ka mid ah suurtagalka ereyga oo haysta tayo. Sida xaqiiqada ah muuqashadan waxaa suurtogal ah in isticmaalka la isticmaalayo la sameyn karo waxqabadyo cilmi ah, tusaale ahaan urur, kala duwan iyo kala duwan. Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements.  Dhaqashadan waxaynu isbarbardhignaa waxyaabaha kala duduwan, waxaana baaraynaa qaar gaar ah oo ah aqoonta hadalka shaqada, garitaanka hadalka polysemiga ah, iyo waxyaabaha qaar ka mid ah turjubaanka ay sameeyaan waxqabadka theoretika ah.</abstract_so>
      <abstract_sv>I den här uppsatsen ger vi ett alternativt perspektiv på ordrepresentationer genom att omtolka dimensionerna av vektorrymden hos ett ord som bäddas in som en samling funktioner. I denna omtolkning normaliseras varje komponent i ordvektorn mot alla ordvektorer i ordförrådet. Denna idé tillåter oss nu att se varje vektor som en n-tuple (besläktad med en fuzzy set), där n är dimensionen av ordet representation och varje element representerar sannolikheten för ordet innehar en funktion. Faktum är att denna representation möjliggör användning av fuzzy set teoretiska operationer, såsom union, skärning och skillnad. Till skillnad från tidigare försök visar vi att denna representation av ord ger en uppfattning om likhet som i sig är asymmetrisk och därmed närmare mänskliga likheter bedömningar. Vi jämför prestandan för denna representation med olika riktmärken, och utforskar några av de unika egenskaperna inklusive funktionsordsdetektion, detektering av polystemösa ord och viss insikt i tolkningen av uppsättningsteoretiska operationer.</abstract_sv>
      <abstract_ta>In this paper, we provide an alternate perspective on word representations, by reinterpreting the dimensions of the vector space of a word embedding as a collection of features.  இந்த மீண்டும் பொருளில், வார்த்தை நெறியின் ஒவ்வொரு பொருளும் சொல்வளத்தின் வார்த்தை நெறிக்கும் எதிராக இயல்பாக இந்த கருத்து ஒவ்வொரு நெறியையும் n- துப்பீட்டாக பார்க்க அனுமதிக்கிறது (குறிப்பாக்கும் அமைப்புகளுக்கு, அதில் n என்பது வார்த்தை குறிப்பிடும் தனிப உண்மையில், இந்த குறிப்பிட்ட பயன்பாட்டை செயல்படுத்த முடியும், யூனியன், இடைவெட்டு மற்றும் வேறுபாடு முந்தைய முயற்சிகளை வித்தியாசமாக, இந்த வார்த்தைகளின் பிரதிநிதியை காட்டுகிறோம் என்பதை நாம் காண்பிக்கிறோம், அது உட்பொழுது ஒத்த நாம் இந்த குறிப்பிட்ட செயல்பாட்டை பல குறிப்புகளுடன் ஒப்பிடுகிறோம், செயல்பாடு வார்த்தையை கண்டுபிடிக்க, பலவிதமான வார்த்தைகளை கண்டுபிடிக்க</abstract_ta>
      <abstract_ur>اس کاغذ میں، ہم کلمات کی تصویر پر ایک دوسری نظر دیتے ہیں، ایک کلمات کی جگہ کے اندازے دوبارہ تغییر دیتے ہیں اس دوبارہ تفسیر میں، کلمات ویکتور کی ہر قسمت ویکتوری کے مقابلہ میں سارے کلمات ویکتوروں کے مقابلہ میں عام کیا جاتا ہے. یہ ایڈیو اب ہمیں ہر ویکتور کو n-tuple کے مطابق دیکھنے کی اجازت دیتا ہے، جہاں n کلمات کی تصویر ہے اور ہر عنصر کلمات کی تصویر کے مطابق ایک ویکتوری کے مطابق ہے. بے شک، یہ نمایش اسے مضبوط استعمال کرنا امکان دیتی ہے، جیسے اتحادیہ، متفرقہ اور تفرقہ. پہلے کی کوشش کے مطابق، ہم دکھاتے ہیں کہ یہ کلمات کی نمونش ایک ایسی نظریہ پیش کرتا ہے جو اس کے دل میں برابر ہے اور اسی طرح انسان کی برابری کے فیصلے سے زیادہ قریب ہے ہم اس نمایش کی عملکرد کو مختلف بنچم مارک سے مقایسہ کرتے ہیں، اور بعض مختلف خصوصیات کا تحقیق کرتے ہیں، فنقش کلمات شناسایی، پالیس کلمات کی شناسایی، اور بعض نظریہ نظریہ عملکرد کے ذریعہ تفسیر کی تعبیر کے ذریعہ۔</abstract_ur>
      <abstract_uz>Bu qogʻozda, biz soʻzning tashkilotlarini o'zgartirish imkoniyatini o'zgartiraymiz va vektorning joylarini qaytadan qo'yish mumkin. Ushbu qaytadan qaytadan, so'zlar vektorining har bir komponent soʻzlarning hamma so'zlar vektorlariga qoʻllaniladi. Bu g'oya bizga har bir vectorni n-tutlik sifatida ko'rsatishga ruxsat beradi. Bu yerda so'zning chegaraligi va har bir element imkoniyatlarni qoʻllash mumkin. Hullas, bu tashkilotni foydalanish imkoniyatlariga, birlashtirish, birlashtirish va ўзгартириш imkoniyatini beradi. Oldingi jarayonlarni o'xshash ko'rsatganimiz, bu so'zlar tashkilotlarini ko'rsatumiz, bu asymmetrik va shunday qilib odamning bir xil xususiyatlariga яқин. Biz bu tashkilotning natijasini ko'plab bog'lamalar bilan birlashtiramiz, uning xossalarini qidirib, muloqat so'zlarni aniqlash, va bir necha narsalarni teoretik amallar qo'llash orzularini o'rganish mumkin.</abstract_uz>
      <abstract_vi>Trong tờ giấy này, chúng tôi cung cấp một góc nhìn khác nhau về các biểu tượng từ, bằng cách tái hiểu lại các kích thước của các chiều của các chữ nhúng vào như một bộ sưu tập các tính năng. Trong phiên dịch lại này, mỗi thành phần của véc- tơ từ được tổng hợp lại với tất cả các véc- tơ từ trong từ. Ý tưởng n ày cho phép chúng ta xem mỗi véc- tơ như một v (giống với một bộ màu lục, nơi n là chiều không của từ đại diện và mỗi nguyên tố là xác suất của từ sở hữu một tính năng. Thật ra, sự đại diện này cho phép sử dụng các thao tác lí thuyết trên tập thất, như liên kết, giao nhau và khác nhau. Không giống như những nỗ lực trước đây, chúng tôi cho thấy rằng sự mô tả từ ngữ này mang lại một khái niệm về nét giống nhau vốn không đồng nhất và gần hơn so với các phán đoán về nét giống người. Chúng tôi so sánh hiệu quả của sự đại diện này với nhiều tiêu chuẩn khác nhau, và khám phá một số tính chất độc nhất, gồm khả năng phát hiện từ hàm, phát hiện từ dạng polysemous, và một số hiểu biết về sự thể dịch được cung cấp bởi các thao định lý.</abstract_vi>
      <abstract_bg>В тази статия ние предлагаме алтернативна перспектива за представянето на думи, като реинтерпретираме размерите на векторното пространство на една дума, вградена като колекция от функции. При това повторно тълкуване всеки компонент на вектора на думата се нормализира спрямо всички вектори на думата в речника. Тази идея сега ни позволява да разглеждаме всеки вектор като n-тупъл (подобно на мъглив набор), където n е размерността на словото представяне и всеки елемент представлява вероятността думата да притежава дадена характеристика. Всъщност, това представяне позволява използването на мъгливи теоретични операции, като обединение, пресичане и разлика. За разлика от предишните опити, ние показваме, че това представяне на думите осигурява понятие за сходство, което по своята същност е асиметрично и следователно по-близо до човешките преценки за сходство. Сравняваме ефективността на това представяне с различни референтни показатели и изследваме някои от уникалните свойства, включително функция откриване на думи, откриване на многослойни думи и известно разбиране за интерпретацията, предоставена от теоретичните операции на множеството.</abstract_bg>
      <abstract_da>I denne artikel giver vi et alternativt perspektiv på ordrepræsentationer ved at gentolke dimensionerne af vektorrummet i et ord, der indlejres som en samling af funktioner. I denne nyfortolkning normaliseres hver komponent i ordet vektor mod alle ordvektorer i ordforrådet. Denne idé giver os nu mulighed for at se hver vektor som en n-tuple (beslægtet med et fuzzy sæt), hvor n er dimensionen af ordet repræsentation og hvert element repræsenterer sandsynligheden for ordet besidder en funktion. Faktisk muliggør denne repræsentation brugen af fuzzy sæt teoretiske operationer, såsom union, skæring og forskel. I modsætning til tidligere forsøg viser vi, at denne repræsentation af ord giver et begreb om lighed, der i sig selv er asymmetrisk og dermed tættere på menneskelige lighedsdomme. Vi sammenligner ydeevnen af denne repræsentation med forskellige benchmarks, og undersøger nogle af de unikke egenskaber, herunder funktion orddetektion, detektion af polystemøse ord og noget indsigt i fortolkningen af sætteteoretiske operationer.</abstract_da>
      <abstract_hr>U ovom papiru pružamo alternativnu perspektivu predstavljanja riječi, ponovno pretvaranjem dimenzija vektorskog prostora riječi uključujući kao kolekciju funkcija. U ovoj ponovnoj pretvaranju, svaka komponenta riječnog vektora normalizira se protiv svih riječnih vektora u rečniku. Ova ideja nam sada omogućava da gledamo svaki vektor kao n-tuple (sličan n a fuzzy set), gdje n je dimenzionalnost predstavljanja riječi i svaki element predstavlja vjerojatnost riječi koja posjeduje funkciju. Zapravo, ova predstava omogućava korištenje neobičnih teorijskih operacija, poput sindikata, prekidanja i razlike. Za razliku od prethodnih pokušaja, pokazujemo da ova predstavljanje riječi pruža pojam sličnosti koja je inherentno asimetrična i stoga bliže osuđivanju ljudske sličnosti. Uspoređujemo učinkovitost te predstave s različitim kriterijama, i istražujemo neke jedinstvene vlasti uključujući otkrivanje funkcionalnih riječi, otkrivanje polizemnih riječi, i neke uvide o interpretabilnosti pruženoj teoretičkim operacijama.</abstract_hr>
      <abstract_nl>In dit artikel bieden we een alternatief perspectief op woordrepresentaties, door de afmetingen van de vectorruimte van een woord te herinterpreteren als een verzameling van kenmerken. Bij deze herinterpretatie wordt elke component van de woordvector genormaliseerd ten opzichte van alle woordvectoren in de woordenschat. Dit idee stelt ons nu in staat om elke vector te bekijken als een n-tupel (vergelijkbaar met een fuzzy set), waarbij n de dimensionaliteit van de woordweergave is en elk element de waarschijnlijkheid vertegenwoordigt dat het woord een kenmerk bezit. Inderdaad, deze representatie maakt het gebruik van fuzzy set theoretische operaties mogelijk, zoals vereniging, kruising en verschil. In tegenstelling tot eerdere pogingen laten we zien dat deze voorstelling van woorden een idee van gelijkenis biedt die inherent asymmetrisch is en dus dichter bij menselijke gelijkenisoordelen ligt. We vergelijken de prestaties van deze representatie met verschillende benchmarks, en onderzoeken enkele van de unieke eigenschappen, waaronder functiewoorddetectie, detectie van polyemotionele woorden, en enig inzicht in de interpreteerbaarheid van verzameltheoretische bewerkingen.</abstract_nl>
      <abstract_de>In diesem Beitrag stellen wir eine alternative Perspektive auf Wortdarstellungen zur Verfügung, indem wir die Dimensionen des Vektorraums einer Worteinbettung als Sammlung von Merkmalen neu interpretieren. Bei dieser Neuinterpretation wird jede Komponente des Wortvektors gegenüber allen Wortvektoren im Vokabular normalisiert. Diese Idee erlaubt es uns nun, jeden Vektor als n-Tupel (ähnlich einer unscharfen Menge) zu betrachten, wobei n die Dimensionalität der Wortdarstellung ist und jedes Element die Wahrscheinlichkeit repräsentiert, dass das Wort ein Merkmal besitzt. Tatsächlich ermöglicht diese Darstellung die Verwendung von fuzzy set theoretischen Operationen, wie Vereinigung, Schnittmenge und Differenz. Im Gegensatz zu früheren Versuchen zeigen wir, dass diese Darstellung von Wörtern einen Begriff von Ähnlichkeit liefert, der inhärent asymmetrisch ist und daher näher an menschlichen Ähnlichkeitsurteilen ist. Wir vergleichen die Leistung dieser Repräsentation mit verschiedenen Benchmarks und untersuchen einige der einzigartigen Eigenschaften, einschließlich Funktionsworterkennung, Erkennung polyemotionaler Wörter und einige Einblicke in die Interpretierbarkeit von mengentheoretischen Operationen.</abstract_de>
      <abstract_id>Dalam kertas ini, kami menyediakan perspektif alternatif pada representation kata, dengan menggambarkan kembali dimensi ruang vektor dari sebuah kata yang memasukkan sebagai koleksi fitur. Dalam interpretasi ulang ini, setiap komponen dari vektor kata normalisasi melawan semua vektor kata dalam vokabular. Ide ini sekarang memungkinkan kita untuk melihat setiap vektor sebagai n-tuple (mirip dengan set kabur), di mana n adalah dimensionalitas representation kata dan setiap elemen mewakili kemungkinan kata yang memiliki fitur. Sebenarnya, perwakilan ini memungkinkan penggunaan operasi teori set kabur, seperti union, intersection dan perbedaan. Tidak seperti percobaan sebelumnya, kami menunjukkan bahwa perwakilan kata ini memberikan gagasan persamaan yang secara alami tidak simetri dan oleh itu lebih dekat dengan penilaian persamaan manusia. Kami membandingkan prestasi representation ini dengan berbagai benchmark, dan mengeksplorasi beberapa properti unik termasuk deteksi kata fungsi, deteksi kata polisemus, dan beberapa pandangan ke dalam interpretabilitas yang diberikan oleh set operasi teori.</abstract_id>
      <abstract_ko>본고에서 우리는 단어가 삽입된 벡터 공간의 차원을 하나의 특징으로 재해석함으로써 단어의 표시에 또 다른 시각을 제공했다.이런 재해석에서 단어 벡터의 모든 구성 부분은 어휘표의 모든 단어 벡터를 규범화한 것이다.이 아이디어는 현재 우리가 모든 벡터를 하나의 n원조(모호집과 유사)로 볼 수 있게 한다. 그 중에서 n은 단어가 표시하는 차원이고 모든 요소는 단어가 특징을 가진 확률을 나타낸다.사실상 이런 표시는 모호 집합론 연산을 사용할 수 있게 한다. 예를 들어 병합, 교화차 등이다.이전의 시도와 달리 우리는 이런 단어의 표시가 내재된 비대칭적인 유사성 개념을 제공하기 때문에 인류의 유사성 판단에 더욱 가깝다는 것을 보여준다.우리는 이러한 표현법의 성능을 각종 기준과 비교하고 허사 검측, 다의어 검측, 집합론 조작에 대한 해석 가능한 견해를 포함한 독특한 특성을 탐색했다.</abstract_ko>
      <abstract_fa>در این کاغذ، ما یک نگاه تغییر در مورد نمایش‌های کلمه را به عنوان مجموعه‌ی ویکتوری به عنوان مجموعه‌ی ویکتوری تغییر می‌دهیم. در این تغییرات، هر قسمتی از ویکتور کلمه نسبت به تمام ویکتور کلمه‌ها در کلمه‌ای معمولاً متعادل می‌شود. این ایده به ما اجازه می دهد که هر vektor را به عنوان یک مجموعه n-tuple ببینیم (مانند یک مجموعه غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر در حقیقت، این نمایش به استفاده از عملیات نظریه‌ای که به عنوان اتحادیه، تقسیم و تفاوت می‌دهد اجازه می‌دهد. برخلاف تلاش قبلی، نشان می دهیم که این نمایش کلمات یک نظر شبیه‌ای را می‌دهد که در اصل شبیه‌ای است و بنابراین به حکم‌های شبیه‌ای انسان نزدیک تر است. ما عملکرد این نمایش را با برچسب‌های مختلف مقایسه می‌کنیم، و بعضی از ویژه‌های متفاوتی را که شامل شناسایی کلمه‌های فعالیت، شناسایی کلمه‌های متفاوتی، و بعضی مشاهده‌ها در تعبیر قابلیت توسط عملکرد‌های نظریه‌ای قرار داده می‌</abstract_fa>
      <abstract_sw>Katika karatasi hii, tunatoa mtazamo mbadala wa uwakilishi wa maneno, kwa kuingiza upande wa nafasi ya vector wa neno linalojumuisha kama mkusanyiko wa tabia. Katika upande huu, kila sehemu ya vector wa neno linalazimika dhidi ya vector zote za maneno katika lugha hii. Wazo hili kwa sasa linaturuhusu kuona kila vector kama kituo cha n-tiba (akilinganisha n a seti yenye tatizo), ambapo kuna ukubwa wa uwakilishi wa neno na kila element inawakilisha uwezekano wa neno linalo tayari. Kwa hakika, uwakilishi huu unawezesha matumizi ya ajabu kutengeneza shughuli za nadharia, kama vile umoja, tofauti na tofauti. Tofauti na majaribio yaliyopita, tunaonyesha kuwa uwakilishi huu wa maneno yanatoa wazo la usawa wa watu ambao ni kwa kiasi kikubwa na hivyo karibu zaidi na maamuzi yanayofanana na binadamu. Tunawalinganisha ufanisi wa uwakilishi huu na misingi mbalimbali, na kutafuta baadhi ya utaalam wa kipekee ikiwa ni pamoja na kutambua neno la kazi, kutambua maneno ya kijamii, na baadhi ya uelewa wa tafsiri iliyotolewa na shughuli za nadharia.</abstract_sw>
      <abstract_tr>Bu kağıt içinde, kelime ifadeleri üzerinde başka bir perspektiv sunuyoruz, içinde bulunan bir kelime alanın vektör alanının ölçülerini bir toplantı olarak tekrar dönüştürerek. - - Bu tercüme, kelime vektörünün her parçası kelime vektörlerine karşı normaldir. Bu fikir şu and a her vektöre n-tuple gibi görünmemizi sağlar ve n kelime temsilcisinin ölçüsü ve her elemente bir özelliğin sahip olduğu kelimenin muhtemeleni gösterir. Çünkü bu representasyon, birlik, kesişikler we farklygy ýaly çalşyrlyk teoriýa işlerini mümkin edir. Önceki denemelere benzemediğimize göre, bu kelimelerin ifadesi içerisinde asymmetrik bir fikir sağlayan ve bu yüzden insan benzeri kararlarına daha yakın olduğunu gösteriyoruz. Biz bu temsilin etkinliğini farklı kayıtlar ile karşılaştırıp, fonksiyonlu kelime keşfetme, polizem kelimelerin tanımlaması ve teoretik operasyonlar tarafından verilen yorumluluklara göz önüne alıyoruz.</abstract_tr>
      <abstract_af>In hierdie papier, ons verskaf 'n alternatiewe perspektief op woord voorstellings deur die dimensies van die vektor ruimte van 'n woord ingesluit as 'n versameling van funksies te hervertrek. In hierdie herterpretasie, elke komponent van die woord vektor is normaliseer teen al die woord vektore in die woordeboek. Hierdie idee laat ons nou toe om elke vektor te besigtig as 'n n- tuple (gelyk a an 'n verdwyn stel), waar n is die dimensionaliteit van die woord verteenwoording en elke element verteenwoordig die waarskynlikheid van die woord besit 'n funksie. Werklik, hierdie verteenwoording laat die gebruik van verdwyn stel teorieese operasies, soos union, interseksie en verskil. Ongelyks soos vorige probeers, wys ons dat hierdie voorstelling van woorde 'n notie van gelykheid verskaf wat inherent asymmetries is en daarom naby aan menslike oordelinge. Ons vergelyk die prestasie van hierdie verteenwoording met verskeie benchmarke, en ondersoek sommige van die unieke eienskappe insluitend funksie woord opdekking, opdekking van polisemose woorde, en sommige inkennisse in die uitleggingsverklaring wat deur teorieese operasies verskaf word.</abstract_af>
      <abstract_sq>Në këtë letër, ne ofrojmë një perspektivë alternative mbi përfaqësimet e fjalëve, duke përsëritur dimensionet e hapësirës vectore të një fjale të përfshirë si një koleksion karakteristike. Në këtë përsëritje, çdo komponent i vektorit të fjalës është normalizuar kundër të gjitha vektorëve të fjalës në fjalorë. Kjo ide n a lejon tani të shohim çdo vektor si një n-tuple (si një set i ngatërruar), ku n është dimensionaliteti i përfaqësimit të fjalës dhe çdo element përfaqëson probabilitetin e fjalës që posedon një funksion. Në fakt, ky përfaqësim lejon përdorimin e operacioneve teorike të vështira, të tilla si bashkimi, ndërprerje dhe ndryshimi. Në ndryshim nga përpjekjet e mëparshme, ne tregojmë se ky përfaqësim i fjalëve ofron një koncept të ngjashmërisë që është në vetvete asimetrike dhe kështu më pranë gjykimeve të ngjashmërisë njerëzore. Ne e krahasojmë performancën e kësaj përfaqësimi me pika të ndryshme referimi dhe eksplorojmë disa nga pronësitë unike duke përfshirë zbulimin e fjalëve të funksionit, zbulimin e fjalëve polisemore dhe disa pamje në interpretueshmërinë e ofruar nga operacionet teorike të vendosura.</abstract_sq>
      <abstract_am>በዚህ ፕሮግራም፣ ለቃላት መልዕክቶች እናስቀራለን፡፡ በዚህ መግለጫ፣ የቃላት vector ሁሉም ክፍል በአብሪካዊው ቃላት vectors ላይ የተደገመ ነው፡፡ ይህም አሳብ እያንዳንዱን vector እንደ n-ጭብጥ (የጨዋታ መስመር) ማየት ይፈቅዳል፡፡ እርግጠኛ፣ ይህ መልዕክት የተጠቃሚ ተግባር፣ እንደ ዩኒያዊ፣ ግንኙነት እና ልዩነት የሚደረገውን ጥያቄ ያስችላል፡፡ Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements.  የዚህን መልዕክት አካሄዱን በተለያዩ ደብዳቤዎች እናስተያየዋለን፣ የቃላትን ማግኘት፣ የፖሊስቲካዊ ቃላትን ማግኘት እናደርጋለን፣ አንዳንዶችም የtheoretica ተግባር በተደረገው ትርጓሜ እናደርጋለን፡፡</abstract_am>
      <abstract_hy>Այս թղթի մեջ մենք տալիս ենք բառերի ներկայացումների փոխարինական տեսանկյուն, վերարտացոլով վեկտորի տարածության չափերը բառի ներառման որպես հատկանիշների հավաքածու: In this reinterpretation, every component of the word vector is normalized against all the word vectors in the vocabulary.  Այս գաղափարը հիմա թույլ է տալիս մեզ տեսնել յուրաքանչյուր վեկտոր որպես n-անգամ (նման է խառնաշփոթ համակարգին), որտեղ n բառի ներկայացման չափսերը և յուրաքանչյուր տարր ներկայացնում է բառի հավանականությունը, որն ունի հատկություն: Իրականում, այս ներկայացումը հնարավորություն է տալիս օգտագործել խառնաշփոթ տեսական գործողություններ, ինչպիսիք են միավորումը, խաչը և տարբերությունը: Ի տարբերություն նախորդ փորձերին, մենք ցույց ենք տալիս, որ բառերի ներկայացումը ստեղծում է նմանության գաղափար, որը բնական անհամաչափ է և հետևաբար ավելի մոտ մարդկային նմանության դատողություններին: Մենք համեմատում ենք այս ներկայացումի արտադրությունը տարբեր համեմատային նպատակների հետ և ուսումնասիրում ենք որոշ առանձնահատկություններ, ներառյալ ֆունկցիոնալ բառերի հայտնաբերումը, պոլիզեմային բառերի հայտնաբերումը և որոշ ընկալումներ տեսական գործողությունների միջո</abstract_hy>
      <abstract_az>Bu kağızda, sözlərin göstərilmələri barəsində başqa bir perspektiv təyin edirik, özelliklərin koleksiyonu olaraq içərisində olan bir sözün vektör boşluğunun ölçülərini yenidən dəyişdirərək. Bu tərzdə, sözlərin vektorunun hər komponenti sözlərin vektorlarına qarşı normalizlənir. Bu fikir indi hər vektörü n-tuple kimi görünməyə imkan verir, n sözlərin göstəricisinin ölçülüyü və hər elementi bir fəaliyyət sahibi sözlərin ehtimalın ı göstərir. Əslində, bu göstəricisi istifadə etməyi fərqli təriqli təriqli işləri, birlikləri, fərqli və fərqli kimi fərqli təriqli təriqlərə qadirdir. Əvvəlki çabaların bənzərinə baxmayaraq, bu sözlərin göstərilməsi, bənzər bir fikir göstərir ki, bənzər bir qədər asymetrik və buna görə də insanların bənzər hökmlərinə daha yaxın olur. Biz bu göstərişlərin performansını müxtəlif benchmarklərlə qarşılaşdırırıq, fərqli sözləri keşfetmək, polizm sözlərin keşfetməsi və teoriqli işləri təyin etmək üçün təfsil edilən təfsil edilməsi barəsində bəzi xüsusiyyətləri keşfetirik.</abstract_az>
      <abstract_bn>এই কাগজটিতে আমরা শব্দের প্রতিনিধিত্বের বিপরীত দৃষ্টিভঙ্গি প্রদান করি, ভেক্টরের স্থান পুনরায় বিশেষ করে একটি শব্দের সংগ্রহ হিসেবে প্রবেশ এই পুনঃপ্রতিষ্ঠানে শব্দ ভেক্টরের প্রত্যেক অংশ স্বাভাবিক ভেক্টরের বিরুদ্ধে স্বাভাবিক। এই চিন্তা এখন আমাদের প্রত্যেক ভেক্টর একটি n-টাপেল হিসেবে দেখতে পাচ্ছে (যেখানে একটি অজ্ঞাত সেটের মতো), যেখানে শব্দের প্রতিনিধিত্বের মাত্রার মাত্রা এব সত্যিই, এই প্রতিনিধিত্ব ব্যবহারকারীদের ব্যবহারের ক্ষেত্রে ততীতিহীন কার্যক্রম, যেমন ইউনিয়ন, বিভিন্ন বিভিন্ন পূর্ববর্তী প্রচেষ্টার ভিন্ন ভিন্ন ভিন্ন ভিন্ন প্রতিনিধিত্ব দেখাচ্ছি যে এই শব্দের প্রতিনিধিত্বের একটি ধারণা প্রদান করা হয়েছে যা প্র আমরা এই প্রতিনিধিত্বের প্রকৃতির তুলনা করি বিভিন্ন বেনমার্কের সাথে এবং কিছু বৈশিষ্ট্যের বৈশিষ্ট্য অনুসন্ধান করি, যার মধ্যে ফাংশন শব্দ আবিষ্কার, বহুব</abstract_bn>
      <abstract_bs>U ovom papiru pružamo alternativnu perspektivu predstavljanja riječi, ponovno pretvaranjem dimenzija vektorskog prostora riječi koja se uključuje kao kolekcija funkcija. U ovoj reinterpretaciji, svaki komponent riječi vektora se normalizira protiv svih vektora riječi u rečniku. Ova ideja nam sada omogućava da vidimo svaki vektor kao n-tuple (sličan n a fuzzy set), gdje n je dimenzionalnost predstavljanja riječi i svaki element predstavlja vjerojatnost riječi koja posjeduje funkciju. Zapravo, ova predstava omogućava korištenje nepristojnih teorijskih operacija, poput sindikata, prekidanja i razlike. Za razliku od prethodnih pokušaja, pokazujemo da ova predstavljanja riječi pruža pojam sličnosti koja je inherentno asimetrična i stoga bliže osuđivanju ljudske sličnosti. Uspoređujemo učinkovitost ove predstave sa različitim kriterijama, i istražujemo neke od jedinstvenih vlasništva, uključujući otkrivanje funkcionalnih riječi, otkrivanje polizemnih riječi, i neke uvide o interpretabilnosti pruženoj teoretičkim operacijama.</abstract_bs>
      <abstract_ca>En aquest paper, proporcionem una perspectiva alternativa a les representacions de paraules, reinterpretant les dimensions de l'espai vector d'una paraula incorporada com una col·lecció de característiques. En aquesta reinterpretació, cada component de la paraula vector es normalitza en contra de tots els paraules vectors del vocabulari. Aquesta idea ara ens permet veure cada vector com un n-tuple (semblant a un conjunt confus), on n és la dimensionalitat de la representació de paraula i cada element representa la probabilitat de la paraula que posseeix una característica. De fet, aquesta representació permet l'ús d'operacions teòriques confuses, com la unió, la intersecció i la diferència. A diferència dels intents anteriors, demostram que aquesta representació de paraules proporciona una noció de similitud que és inherentment asimètrica i, per tant, més a prop dels judicis de similitud humana. Comparem el desempeny d'aquesta representació amb diversos punts de referència, i explorem algunes de les propietats únices, incloent la detecció de paraules de funció, la detecció de paraules polisemoses i alguna comprensió de l'interpretabilitat proporcionada per operacions teòriques.</abstract_ca>
      <abstract_cs>V tomto článku poskytujeme alternativní pohled na slovní reprezentace reinterpretací rozměrů vektorového prostoru vloženého slova jako sbírku prvků. V této reinterpretaci je každá složka slovního vektoru normalizována proti všem slovním vektorům ve slovní zásobě. Tato myšlenka nám nyní umožňuje vidět každý vektor jako n-tuple (podobný rozmazané množině), kde n je dimenzionalita slova reprezentace a každý prvek představuje pravděpodobnost, že slovo má vlastnost. Tato reprezentace umožňuje použití fuzzy množinových teoretických operací, jako jsou sjednocení, průsečík a rozdíl. Na rozdíl od předchozích pokusů ukazujeme, že tato reprezentace slov poskytuje představu podobnosti, která je z podstaty asymetrická a tudíž blíže k lidským úsudkům podobnosti. Porovnáváme výkon této reprezentace s různými benchmarky a zkoumáme některé z jedinečných vlastností včetně detekce funkčních slov, detekce polyemozních slov a některý vhled do interpretovatelnosti poskytované množinovými teoretickými operacemi.</abstract_cs>
      <abstract_et>Käesolevas töös pakume alternatiivset perspektiivi sõnade esitustele, tõlgendades uuesti vektoriruumi mõõtmeid sõna manustamisel funktsioonide kogumina. Selles ümbertõlgendamises normaliseeritakse sõnavaraktori iga komponent kõigi sõnavaraktori sõnavaraktorite suhtes. See idee võimaldab meil nüüd vaadata iga vektorit n-tuplina (sarnane udusele hulgale), kus n on sõna representatsiooni dimensioonilisus ja iga element esindab sõna omamise tõenäosust. Tõepoolest, see esitamine võimaldab kasutada hägune hulk teoreetilisi operatsioone, nagu liit, ristumine ja erinevus. Erinevalt varasematest katsetest näitame, et see sõnade esitamine annab sarnasuse mõiste, mis on olemuslikult asümmeetriline ja seega lähemal inimese sarnasuse otsustele. Me võrdleme selle esituse jõudlust erinevate võrdlusnäitajatega ja uurime mõningaid unikaalseid omadusi, sealhulgas funktsioonisõna tuvastamist, polüsemoossete sõnade tuvastamist ja mõningast ülevaadet komplekti teoreetiliste operatsioonide tõlgendatavusest.</abstract_et>
      <abstract_fi>Tässä työssä tarjoamme vaihtoehtoisen näkökulman sanaesityksiin tulkitsemalla uudelleen sanan upottamisen vektoriavaruuden ulottuvuudet ominaisuuksien kokoelmana. Tässä uudelleentulkinnassa sanavektorin jokainen komponentti normalisoidaan sanaston kaikkia sanavektoreita vastaan. Tämän idean avulla voimme nyt tarkastella jokaista vektoria n-tuplana (samankaltainen sumea joukko), jossa n on sanan edustuksen ulottuvuus ja jokainen elementti edustaa todennäköisyyttä, että sana omistaa ominaisuuden. Itse asiassa tämä esitys mahdollistaa fuzzy joukko teoreettisia toimintoja, kuten liitos, leikkaus ja ero. Toisin kuin aikaisemmat yritykset, osoitamme, että tämä sanojen esittäminen tarjoaa samankaltaisuuden käsitteen, joka on luonnostaan epäsymmetrinen ja siten lähempänä ihmisen samankaltaisuusarvioita. Vertaamme tämän esityksen suorituskykyä erilaisiin vertailuarvoihin ja tutkimme joitain ainutlaatuisia ominaisuuksia, kuten funktiosanojen havaitsemista, polyemoisten sanojen havaitsemista ja joitakin näkemyksiä joukkoteoreettisten operaatioiden tulkinnasta.</abstract_fi>
      <abstract_jv>Enter the vector space of a word To This idée nung iné permet us to view every vector as a n n-taple (like a FBI set), Where n is the size of the word representation and every item represents the likely of the word Open source Awak dhéwé éntuk kiper perbudhak, kita ngomatngon kuwi tindakan nyebuturan gambar nggawe sapa ngono kuwi duluran sing gak bener tentang karo akeh apik lan dadi iki sak duluran gambar uwong. Awak dhéwé nggawe geraraning nggawe representasi iki gambar nggambar aturan kanggo ngilangno karo perusahaan winih sing nyimpen, gambar nggambar kelas kuwi operasi layar, jatatan kelas polisemus lan kelas pangrungu nggawe gerarané perusahaan theoretik.</abstract_jv>
      <abstract_ha>In this paper, we provide an alternate perspective on word representations, by reinterpreting the dimensions of the vector space of a word embedding as a collection of features.  @ info: whatsthis Wannan idãnun yanzu yana yarda mu nuna kõwa mai shiryarwa kamar n-tipple (akin da wani set mai zartar da aiki), inda n ke da sifilanci n a maganar kuma duk ƙanshi na ƙayyade sannanan maganar da ya ƙunsa da wani zafi. In da gaske, wannan shirin zai iya amfani da aikin mai zartar da amfani da matsayin teori, kamar shirin kwamfyuta, guda da sãɓãni. Babu motsi da jarrabi da suka gabãta, Munã nuna cewa wannan mai gayarwa ga kalmõmi yana da wani noti na daidaita wanda ke cikin asymmetric da kuma daga wannan yana makusanta ga masu daidaita ga mutane. Kana samfani da aikin wannan rubutu da mistakardan misãlai masu yawa, kuma Munã samfani wasu properties na daban, kamar kunnufi maganar aiki, da gane magana na Polsemi, da wani gane cikin fassarar da aka ƙayyade aikin teori.</abstract_ha>
      <abstract_he>בעיתון הזה, אנחנו מספקים פרספקטיבה חלופית על מיצוגי מילים, על ידי להפריע מחדש את המימדים של מרחב הוקטורים של מילה מוקפת כאוסף של תכונות. בפרשנות מחדש הזאת, כל רכיב של הוקטור המילה נורמלי נגד כל הוקטורים המילים במילים. הרעיון הזה מאפשר לנו עכשיו לראות כל ווקטור כn-כפול (דומה לקבוצה מעורפלת), שבו n הוא המימד של מייצג המילה וכל אלמנט מייצג את הסבירות של המילה שיש לה תכונה. למעשה, היציגה הזאת מאפשרת להשתמש במבצעים תיאורטיים מסודרים, כמו איגוד, חיצום וההבדל. בניגוד לנסיונות קודמות, אנו מראים שהמייצג הזה של מילים מספק רעיון של דמיון שהוא באופן טבעי אסימטרי ולכן קרוב יותר לשיפוטים של דמיון אנושי. We compare the performance of this representation with various benchmarks, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations.</abstract_he>
      <abstract_sk>V prispevku zagotavljamo alternativno perspektivo besednih predstavitev, tako da ponovno interpretiramo dimenzije vektorskega prostora besedne vdelave kot zbirko značilnosti. V tej ponovni razlagi se vsaka komponenta besednega vektorja normalizira glede na vse besedne vektorje v besedišču. Ta ideja nam zdaj omogoča, da vidimo vsak vektor kot n-tuplo (podobno meglenemu množici), kjer je n dimenzionalnost besedne reprezentacije in vsak element predstavlja verjetnost, da ima beseda značilnost. Ta predstavitev namreč omogoča uporabo teoretičnih operacij meglenih množic, kot so unija, presečišče in razlika. Za razliko od prejšnjih poskusov pokažemo, da ta predstavitev besed zagotavlja pojem podobnosti, ki je po sebi asimetričen in s tem bližje človeškim podobnim presojam. Učinkovitost te reprezentacije primerjamo z različnimi referenčnimi vrednostmi in raziskujemo nekatere edinstvene lastnosti, vključno z zaznavanjem funkcijskih besed, zaznavanjem poličemskih besed in nekaj vpogledov v interpretabilnost, ki jo zagotavljajo teoretične operacije množic.</abstract_sk>
      <abstract_bo>ང་ཚོས་ཤོག་བུ་འདིའི་ནང་དུ་ཡི་གེ་ལ་ངོས་ཚོའི་རྩ་སྒྲིག་ཕྱོགས་གཞན་གྱི་ལྟ་བ་ཞིག་བྱེད་ཀྱི་ཡོད་པ་ཚོའི་ནང་ནས་ཕན་ཚུན་གྱི་བར་སྟོང་ འདིའི་རྗེས་སུ་འབྱེད་སྐབས་ཐོག་གི་ཆ་ཤས་རེ་རེ་བ་དེ་ཚོའི་ཐོག་རིམ་ནང་གི་ཚགས་རྣམས་མེད་རྒྱུན་གྱིས་བཟོས་ཚར This idea now allows us to view each vector as a n n-tuple (akin to a fuzzy set), where n is the dimensionality of the word representation and each element represents the probability of the word possessing a feature. དངོས་འབྲེལ་བ་འདིས་སྤྱི་ཁྱད་པར་ཕྱིར་བཏོན་པའི་གཞི་རྩལ་བ་སྒྲིག་ལྟར་བཀོད་སྤྱོད་ལ་ནུས་ཡོད། ང་ཚོས་དུས་མའི་དཔའ་བཅས་ལ་འགྱུར་བ་དེ་ལྟ་བུའི་ནང་གི་ཡིག་གི་གསལ་བཤད་འདི་གི་དོན་དག་མི་འདྲ་བ་དང་། བྱས་ཙང་མི་འདྲ་བ་དང་མི་འདྲ་བ་བསྐྱེད་ We compare the performance of this representation with various benchmarks, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations.</abstract_bo>
      </paper>
    <paper id="5">
      <title>Compositionality and Capacity in Emergent Languages</title>
      <author><first>Abhinav</first><last>Gupta</last></author>
      <author><first>Cinjon</first><last>Resnick</last></author>
      <author><first>Jakob</first><last>Foerster</last></author>
      <author><first>Andrew</first><last>Dai</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <pages>34–38</pages>
      <abstract>Recent works have discussed the extent to which <a href="https://en.wikipedia.org/wiki/Emergence">emergent languages</a> can exhibit properties of <a href="https://en.wikipedia.org/wiki/Natural_language">natural languages</a> particularly learning compositionality. In this paper, we investigate the <a href="https://en.wikipedia.org/wiki/Learning_bias">learning biases</a> that affect the efficacy and <a href="https://en.wikipedia.org/wiki/Compositionality">compositionality</a> in multi-agent communication in addition to the communicative bandwidth. Our foremost contribution is to explore how the capacity of a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and <a href="https://en.wikipedia.org/wiki/Bandwidth_(signal_processing)">channel bandwidth</a> that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.</abstract>
      <url hash="994aa67f">2020.repl4nlp-1.5</url>
      <doi>10.18653/v1/2020.repl4nlp-1.5</doi>
      <video href="http://slideslive.com/38929771" />
      <revision id="1" href="2020.repl4nlp-1.5v1" hash="ca52c819" />
      <revision id="2" href="2020.repl4nlp-1.5v2" hash="994aa67f" date="2021-01-03">Fixed a citation.</revision>
      <bibkey>gupta-etal-2020-compositionality</bibkey>
    </paper>
    <paper id="6">
      <title>Learning Geometric Word Meta-Embeddings</title>
      <author><first>Pratik</first><last>Jawanpuria</last></author>
      <author><first>Satya Dev</first><last>N T V</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Bamdev</first><last>Mishra</last></author>
      <pages>39–44</pages>
      <abstract>We propose a geometric framework for learning meta-embeddings of words from different embedding sources. Our framework transforms the <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> into a common latent space, where, for example, simple averaging or concatenation of different embeddings (of a given word) is more amenable. The proposed latent space arises from two particular geometric transformations-source embedding specific orthogonal rotations and a common Mahalanobis metric scaling. Empirical results on several word similarity and word analogy benchmarks illustrate the efficacy of the proposed framework.</abstract>
      <url hash="9eae4748">2020.repl4nlp-1.6</url>
      <attachment type="Software" hash="89828011">2020.repl4nlp-1.6.Software.zip</attachment>
      <doi>10.18653/v1/2020.repl4nlp-1.6</doi>
      <video href="http://slideslive.com/38929772" />
      <bibkey>jawanpuria-etal-2020-learning</bibkey>
    </paper>
    <paper id="10">
      <title>Exploring the Limits of Simple Learners in Knowledge Distillation for <a href="https://en.wikipedia.org/wiki/Document_classification">Document Classification</a> with DocBERT<fixed-case>D</fixed-case>oc<fixed-case>BERT</fixed-case></title>
      <author><first>Ashutosh</first><last>Adhikari</last></author>
      <author><first>Achyudh</first><last>Ram</last></author>
      <author><first>Raphael</first><last>Tang</last></author>
      <author><first>William L.</first><last>Hamilton</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>72–77</pages>
      <abstract>Fine-tuned variants of BERT are able to achieve state-of-the-art <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on many natural language processing tasks, although at significant computational costs. In this paper, we verify BERT’s effectiveness for document classification and investigate the extent to which BERT-level effectiveness can be obtained by different baselines, combined with knowledge distillationa popular model compression method. The results show that BERT-level effectiveness can be achieved by a single-layer LSTM with at least 40 fewer FLOPS and only 3 % parameters. More importantly, this study analyzes the limits of knowledge distillation as we distill BERT’s knowledge all the way down to linear modelsa relevant baseline for the task. We report substantial improvement in <a href="https://en.wikipedia.org/wiki/Effectiveness">effectiveness</a> for even the simplest <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, as they capture the knowledge learnt by BERT.<tex-math>40\times</tex-math> fewer FLOPS and only <tex-math>{\sim}3\%</tex-math> parameters. More importantly, this study analyzes the limits of knowledge distillation as we distill BERT’s knowledge all the way down to linear models—a relevant baseline for the task. We report substantial improvement in effectiveness for even the simplest models, as they capture the knowledge learnt by BERT.</abstract>
      <url hash="051972bd">2020.repl4nlp-1.10</url>
      <doi>10.18653/v1/2020.repl4nlp-1.10</doi>
      <video href="http://slideslive.com/38929776" />
      <bibkey>adhikari-etal-2020-exploring</bibkey>
    </paper>
    <paper id="16">
      <title>Are All Languages Created Equal in Multilingual BERT?<fixed-case>BERT</fixed-case>?</title>
      <author><first>Shijie</first><last>Wu</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>120–130</pages>
      <abstract>Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks : <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entity Recognition</a> (99 languages), <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">Part-of-speech Tagging</a> and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these <a href="https://en.wikipedia.org/wiki/Language">languages</a> do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for low resource languages require more efficient pretraining techniques or more data.</abstract>
      <url hash="d94ad761">2020.repl4nlp-1.16</url>
      <doi>10.18653/v1/2020.repl4nlp-1.16</doi>
      <video href="http://slideslive.com/38929782" />
      <bibkey>wu-dredze-2020-languages</bibkey>
      <pwccode url="https://github.com/shijie-wu/crosslingual-nlp" additional="false">shijie-wu/crosslingual-nlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="22">
      <title>Evaluating Compositionality of Sentence Representation Models</title>
      <author><first>Hanoz</first><last>Bhathena</last></author>
      <author><first>Angelica</first><last>Willis</last></author>
      <author><first>Nathan</first><last>Dass</last></author>
      <pages>185–193</pages>
      <abstract>We evaluate the compositionality of general-purpose sentence encoders by proposing two different <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> to quantify compositional understanding capability of sentence encoders. We introduce a novel <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>, Polarity Sensitivity Scoring (PSS), which utilizes sentiment perturbations as a proxy for measuring <a href="https://en.wikipedia.org/wiki/Compositionality">compositionality</a>. We then compare results from PSS with those obtained via our proposed extension of a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> called Tree Reconstruction Error (TRE) (CITATION) where compositionality is evaluated by measuring how well a true representation producing model can be approximated by a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that explicitly combines representations of its primitives.</abstract>
      <url hash="4d42e5cc">2020.repl4nlp-1.22</url>
      <attachment type="Software" hash="4a706720">2020.repl4nlp-1.22.Software.zip</attachment>
      <doi>10.18653/v1/2020.repl4nlp-1.22</doi>
      <video href="http://slideslive.com/38929788" />
      <bibkey>bhathena-etal-2020-evaluating</bibkey>
    </paper>
    </volume>
</collection>