<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.repl4nlp">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 5th Workshop on Representation Learning for NLP</booktitle>
      <editor><first>Spandana</first><last>Gella</last></editor>
      <editor><first>Johannes</first><last>Welbl</last></editor>
      <editor><first>Marek</first><last>Rei</last></editor>
      <editor><first>Fabio</first><last>Petroni</last></editor>
      <editor><first>Patrick</first><last>Lewis</last></editor>
      <editor><first>Emma</first><last>Strubell</last></editor>
      <editor><first>Minjoon</first><last>Seo</last></editor>
      <editor><first>Hannaneh</first><last>Hajishirzi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="67f2d50b">2020.repl4nlp-1</url>
    </meta>
    <frontmatter>
      <url hash="d40b3f5b">2020.repl4nlp-1.0</url>
      <bibkey>repl4nlp-2020-representation</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Zero-Resource Cross-Domain Named Entity Recognition</title>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>1–6</pages>
      <abstract>Existing models for cross-domain named entity recognition (NER) rely on numerous unlabeled corpus or labeled NER training data in target domains. However, collecting data for low-resource target domains is not only expensive but also time-consuming. Hence, we propose a cross-domain NER model that does not use any <a href="https://en.wikipedia.org/wiki/Resource_(computing)">external resources</a>. We first introduce a Multi-Task Learning (MTL) by adding a new <a href="https://en.wikipedia.org/wiki/Loss_function">objective function</a> to detect whether tokens are named entities or not. We then introduce a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> called Mixture of Entity Experts (MoEE) to improve the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> for zero-resource domain adaptation. Finally, experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms strong unsupervised cross-domain sequence labeling models, and the performance of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is close to that of the state-of-the-art model which leverages extensive resources.</abstract>
      <url hash="7fe916c9">2020.repl4nlp-1.1</url>
      <doi>10.18653/v1/2020.repl4nlp-1.1</doi>
      <video href="http://slideslive.com/38929767" />
      <bibkey>liu-etal-2020-zero</bibkey>
    <title_ar>التعرف على الكيانات المسماة عبر المجالات ذات الموارد الصفرية</title_ar>
      <title_fr>Reconnaissance d'entités nommées inter-domaines sans ressources</title_fr>
      <title_pt>Reconhecimento de entidade nomeada entre domínios com recursos zero</title_pt>
      <title_es>Reconocimiento de entidades nombradas entre dominios sin recursos</title_es>
      <title_hi>शून्य-संसाधन क्रॉस-डोमेन नामित निकाय पहचान</title_hi>
      <title_zh>零资源跨域名实体识</title_zh>
      <title_ja>ゼロリソースのクロスドメインの名前付きエンティティの認識</title_ja>
      <title_ru>Распознавание именованных сущностей с междоменным именем нулевого ресурса</title_ru>
      <title_ukr>Розпізнавання міждоменних іменованих сутностей з нульовим ресурсом</title_ukr>
      <title_ga>Aitheantas Aonán Ainmnithe Trasfhearann Acmhainne</title_ga>
      <title_ka>ნულ- რესურსის გარეშე დიომენის სახელი ინტერტის განახლება</title_ka>
      <title_hu>Zéró erőforrások, tartományközi nevű entitások felismerése</title_hu>
      <title_el>Αναγνώριση οντότητας μηδενικού πόρου μεταξύ τομέων που ονομάζεται οντότητα</title_el>
      <title_isl>Zero-Resource Cross-Domain Named Entity Recognition</title_isl>
      <title_it>Riconoscimento di entità con nome a dominio tra risorse zero</title_it>
      <title_kk>Нөл- ресурс доменге аталған нысанды анықтау</title_kk>
      <title_lt>Nėra išteklių tarpdomeninis subjekto pripažinimas</title_lt>
      <title_mk>Препознавање на ентитет наречен преку нула ресурси</title_mk>
      <title_ms>Pengenalan Entiti Bernama Salib-Domain Sumber-Sifar</title_ms>
      <title_ml>Zero-Resource Cross-Domain Named Entity Recognition</title_ml>
      <title_mt>Rikonoxximent ta’ Entità Ismija minn Qasam ta’ Riżorsi Żero</title_mt>
      <title_mn>Нөлөө-нөөц олон домаар нэрлэгдсэн Entity Recognition</title_mn>
      <title_no>Null- ressurs- kryssdomen- oppkjenning av entitet namn</title_no>
      <title_pl>Rozpoznawanie podmiotów o nazwie między zasobami zerowymi</title_pl>
      <title_ro>Recunoașterea entității numite între domenii fără resurse zero</title_ro>
      <title_sr>Prepoznavanje entiteta imenovanog krstodomena nula resursa</title_sr>
      <title_si>සුන්න- සම්බන්ධ ක්‍රොස්- ඩොමේන් නම් නම් අන්තිත්වය අඳුරගන්න</title_si>
      <title_so>Zero-Resource Cross-Domain Named Entity Recognition</title_so>
      <title_sv>Identifiering av domännamnsövergripande enheter utan resurs</title_sv>
      <title_ta>சூழ்நிலை மூலம் கிருஸ்- டோமைன் பெயர் உள்ளீடு அறிதல்</title_ta>
      <title_ur>Zero-Resource Cross-Domain Named Entity Recognition</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Siêu Lãnh địa vô tài nguyên</title_vi>
      <title_bg>Разпознаване на междудомейново име на субекти с нулев ресурс</title_bg>
      <title_da>Anerkendelse af enheder på tværs af domæner</title_da>
      <title_nl>Zero-Resource Cross-Domain Named Entity Recognition</title_nl>
      <title_id>Pengenalan Entitas bernama Cross Domain Ressource-Zero</title_id>
      <title_ko>제로 자원 크로스 필드 이름 실체 식별</title_ko>
      <title_hr>Prepoznavanje entiteta imenovanog krstodomena nula resursa</title_hr>
      <title_de>Zero-Resource Cross-Domain Named Entity Erkennung</title_de>
      <title_tr>Açmak üçin Ululyk Açmak</title_tr>
      <title_fa>شناسایی واحد نامیده‌شده دامنه‌های منبع صفر</title_fa>
      <title_sw>Msalama Zero-rasilimali</title_sw>
      <title_af>Nuwe- hulpbron Kruis- Domein Genaamde Eenheidrekening</title_af>
      <title_sq>Njohja e njësisë me emër zero-Resource-Cross-Domain</title_sq>
      <title_am>ፋይል sን መክፈት አልቻለም፦ %s፦ %s</title_am>
      <title_bn>Zero- resource Cross- Domain নামের এন্টিটি স্বীকৃতি</title_bn>
      <title_hy>Զերոռեսուրսների խաչբ-բնագավառի կոչվող անհատականության ճանաչելը</title_hy>
      <title_bs>Prepoznavanje entiteta imenovanog preko domena bez resursa</title_bs>
      <title_cs>Rozpoznávání entit s názvem nulových zdrojů mezi doménami</title_cs>
      <title_et>Nullressursi domeenidevaheline nimega üksuste tuvastamine</title_et>
      <title_az>Sıfır-Ressours Üstünlü Adlı Entity Recognition</title_az>
      <title_ca>Recognició d'una entitat anomenada "Recognició d'un domini cru de recursos zero"</title_ca>
      <title_fi>Nollaresurssin verkkotunnusten välinen nimetty yksikkötunnistus</title_fi>
      <title_jv>Cage-Resolute</title_jv>
      <title_he>זיהוי ישות בשם אפס משאבים</title_he>
      <title_sk>Prepoznavanje meddomenskega imenovanega subjekta z ničelnimi viri</title_sk>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_fil>Zero-Resource Cross-Domain Named Entity Recognition</title_fil>
      <title_bo>Zero-Resource Cross-Domain Named Entity Recognition</title_bo>
      <abstract_ar>تعتمد النماذج الحالية للتعرف على الكيانات المسماة عبر المجالات (NER) على العديد من المجموعات غير المسماة أو بيانات تدريب NER المسمى في المجالات المستهدفة. ومع ذلك ، فإن جمع البيانات للمجالات المستهدفة منخفضة الموارد ليس مكلفًا فحسب ، بل يستغرق أيضًا وقتًا طويلاً. ومن ثم ، فإننا نقترح نموذج NER عبر المجالات لا يستخدم أي موارد خارجية. نقدم أولاً التعلم متعدد المهام (MTL) عن طريق إضافة وظيفة موضوعية جديدة لاكتشاف ما إذا كانت الرموز المميزة كيانات مسماة أم لا. نقدم بعد ذلك إطارًا يسمى مزيج خبراء الكيانات (MoEE) لتحسين المتانة للتكيف مع مجال الموارد الصفرية. أخيرًا ، تُظهر النتائج التجريبية أن نموذجنا يتفوق في الأداء على نماذج وضع العلامات التسلسلية عبر المجالات القوية غير الخاضعة للرقابة ، وأداء نموذجنا قريب من أداء النموذج الحديث الذي يستفيد من موارد واسعة النطاق.</abstract_ar>
      <abstract_es>Los modelos existentes para el reconocimiento de entidades nombradas (NER) entre dominios se basan en numerosos corpus no etiquetados o datos de entrenamiento de NER etiquetados en dominios objetivo. Sin embargo, la recopilación de datos para dominios de destino de bajos recursos no solo es costosa sino que también lleva mucho tiempo. Por lo tanto, proponemos un modelo NER entre dominios que no utilice recursos externos. Primero introducimos el aprendizaje multitarea (MTL) añadiendo una nueva función objetivo para detectar si los tokens se denominan entidades o no. A continuación, introducimos un marco denominado Mixture of Entity Experts (MoEE) para mejorar la solidez de la adaptación del dominio sin recursos. Por último, los resultados experimentales muestran que nuestro modelo supera a los modelos sólidos de etiquetado de secuencias entre dominios no supervisados, y el rendimiento de nuestro modelo se acerca al del modelo de última generación que aprovecha amplios recursos.</abstract_es>
      <abstract_fr>Les modèles existants pour la reconnaissance d'entités nommées (NER) interdomaines reposent sur de nombreux corpus non étiquetés ou des données d'apprentissage NER étiquetées dans des domaines cibles. Cependant, la collecte de données pour des domaines cibles à faibles ressources est non seulement coûteuse mais aussi chronophage. Nous proposons donc un modèle NER interdomaines qui n'utilise aucune ressource externe. Nous introduisons d'abord un apprentissage multitâche (MTL) en ajoutant une nouvelle fonction d'objectif pour détecter si les jetons sont des entités nommées ou non. Nous introduisons ensuite un framework appelé Mixture of Entity Experts (MoEE) pour améliorer la robustesse de l'adaptation de domaine sans ressource. Enfin, les résultats expérimentaux montrent que notre modèle surpasse les modèles puissants d'étiquetage de séquences interdomaines non supervisés, et que les performances de notre modèle sont proches de celles du modèle de pointe qui exploite des ressources étendues.</abstract_fr>
      <abstract_pt>Os modelos existentes para reconhecimento de entidade nomeada (NER) entre domínios dependem de vários corpus não rotulados ou dados de treinamento de NER rotulados em domínios de destino. No entanto, coletar dados para domínios de destino com poucos recursos não é apenas caro, mas também demorado. Assim, propomos um modelo de NER de domínio cruzado que não usa nenhum recurso externo. Primeiro introduzimos um Multi-Task Learning (MTL) adicionando uma nova função objetivo para detectar se os tokens são entidades nomeadas ou não. Em seguida, apresentamos uma estrutura chamada Mixture of Entity Experts (MoEE) para melhorar a robustez da adaptação de domínio de recurso zero. Finalmente, os resultados experimentais mostram que nosso modelo supera os fortes modelos de rotulagem de sequência de domínio cruzado não supervisionados, e o desempenho do nosso modelo é próximo ao do modelo de última geração que alavanca recursos extensivos.</abstract_pt>
      <abstract_zh>以跨域名实识 (NER) 今依式域中多未识者语料库或标 NER 练数。 然为低资源域所收数据,不惟成本高昂,且耗时。 是以不用外资之跨域NER。 首以益新函数检令牌是非名实,引入多任务学(MTL)。 然后引入实体专家混合(MoEE)框架,以崇零资源域之鲁棒性。 最后,实验结果表明,我优于强大者跨域序其表,而其性近于先入。</abstract_zh>
      <abstract_ja>クロスドメイン命名エンティティ認識（ ＮＥＲ ）のための既存のモデルは、標識されていない多数のコーパスまたは標識されたＮＥＲトレーニングデータを標的ドメインに依存する。しかしながら、低資源ターゲットドメインのデータ収集は、費用がかかるだけでなく、時間もかかります。そこで、外部リソースを一切使用しないクロスドメインNERモデルを提案する。まず、トークンが名前付きエンティティであるかどうかを検出するための新しい目標関数を追加して、マルチタスク学習（ MTL ）を導入します。次に、ゼロリソースドメイン適応の堅牢性を改善するために、Mixture of Entity Expert (MoEE)と呼ばれるフレームワークを導入します。最後に、実験結果は、私たちのモデルが強力な監視されていないクロスドメインシーケンス標識モデルを上回っており、私たちのモデルのパフォーマンスは、広範なリソースを活用する最先端のモデルに近いことを示しています。</abstract_ja>
      <abstract_ru>Существующие модели междоменного распознавания сущностей (NER) основаны на многочисленных немеченных корпусах или меченых обучающих данных NER в целевых доменах. Однако сбор данных для малоресурсных целевых доменов является не только дорогостоящим, но и трудоемким. Таким образом, мы предлагаем междоменную модель NER, которая не использует никаких внешних ресурсов. Сначала мы вводим многозадачное обучение (MTL), добавляя новую целевую функцию для определения того, являются ли токены именованными сущностями или нет. Затем мы вводим структуру под названием «Смешение экспертов по организациям» (Mixture of Entity Experts, MoEE), чтобы повысить надежность для адаптации домена с нулевыми ресурсами. Наконец, экспериментальные результаты показывают, что наша модель превосходит сильные неконтролируемые модели маркировки междоменных последовательностей, и производительность нашей модели близка к производительности самой современной модели, которая использует обширные ресурсы.</abstract_ru>
      <abstract_hi>क्रॉस-डोमेन नामित एंटिटी रिकग्निशन (एनईआर) के लिए मौजूदा मॉडल लक्ष्य डोमेन में कई अनलेबल कॉर्पस या लेबल किए गए एनईआर प्रशिक्षण डेटा पर निर्भर करते हैं। हालांकि, कम-संसाधन लक्ष्य डोमेन के लिए डेटा एकत्र करना न केवल महंगा है, बल्कि समय लेने वाला भी है। इसलिए, हम एक क्रॉस-डोमेन एनईआर मॉडल का प्रस्ताव करते हैं जो किसी भी बाहरी संसाधनों का उपयोग नहीं करता है। हम पहले एक मल्टी-टास्क लर्निंग (एमटीएल) को एक नया उद्देश्य फ़ंक्शन जोड़कर यह पता लगाने के लिए पेश करते हैं कि टोकन को एंटिटी नाम दिया गया है या नहीं। फिर हम शून्य-संसाधन डोमेन अनुकूलन के लिए मजबूती में सुधार करने के लिए इकाई विशेषज्ञों (एमओईई) के मिश्रण नामक एक रूपरेखा पेश करते हैं। अंत में, प्रयोगात्मक परिणामों से पता चलता है कि हमारा मॉडल मजबूत असुरक्षित क्रॉस-डोमेन अनुक्रम लेबलिंग मॉडल को मात देता है, और हमारे मॉडल का प्रदर्शन अत्याधुनिक मॉडल के करीब है जो व्यापक संसाधनों का लाभ उठाता है।</abstract_hi>
      <abstract_ga>Braitheann samhlacha atá ann faoi láthair maidir le haithint aonán ainmnithe tras-fearainn (NER) ar an iliomad corpas neamhlipéadaithe nó sonraí oiliúna NER lipéadaithe i réimsí sprice. Mar sin féin, ní hamháin go bhfuil sé costasach sonraí a bhailiú le haghaidh spriocfhearainn a bhfuil acmhainní ísle acu ach bíonn sé am-íditheach freisin. Mar sin, molaimid múnla NER tras-fearainn nach n-úsáideann aon acmhainní seachtracha. Ar dtús tugaimid isteach Foghlaim Ilthasc (MTL) trí fheidhm oibiachtúil nua a chur leis chun a fháil amach an bhfuil comharthaí ina n-eintitis ainmnithe nó nach bhfuil. Ansin tugaimid isteach creat ar a dtugtar Meascán de Shaineolaithe Aonán (MoEE) chun feabhas a chur ar stóinseacht oiriúnú fearainn gan acmhainní. Mar fhocal scoir, léiríonn torthaí turgnamhacha go n-éiríonn lenár múnla níos fearr ná múnlaí lipéadaithe seicheamh tras-fhearainn gan mhaoirseacht, agus tá feidhmíocht ár múnla gar do fheidhmíocht na samhla nua-aimseartha a ghiaráil acmhainní fairsinge.</abstract_ga>
      <abstract_ukr>Існуючі моделі міждоменного розпізнавання сутностей (NER) спираються на численні немечені корпуси або мічені навчальні дані NER у цільових доменах. Однак збір даних для малоресурсних цільових доменів є не тільки дорогим, але й тривалим завданням. Отже, ми пропонуємо міждоменну модель NER, яка не використовує жодних зовнішніх ресурсів. Спочатку ми вводимо багатозадачне навчання (MTL), додаючи нову цільову функцію, щоб визначити, чи є токени іменованими сутностями. Потім ми запроваджуємо структуру під назвою "Змішання експертів суб 'єктів господарювання" (Mixture of Entity Experts, MoEE) для підвищення надійності для адаптації домену з нульовими ресурсами. Нарешті, експериментальні результати показують, що наша модель перевершує потужні міждоменні моделі маркування послідовностей без нагляду, і продуктивність нашої моделі близька до результатів найсучаснішої моделі, яка використовує великі ресурси.</abstract_ukr>
      <abstract_ka>მსგავსი მოდელები, რომლებიც სახელ მონიშნული ინტერტის განაცნობისთვის (NER), უფრო რაოდენობით უცნობილი კოპუსზე დარწმუნდება ან მინიშნული NER განაცნობის მონაც მაგრამ მარტივი რესურსის მისაღების მონაცემების შექმნარება არა მხოლოდ საკმაოდ, მაგრამ მხოლოდ საკმაოდ. ამიტომ, ჩვენ მინდომენოთ გარეშე NER მოდელი, რომელიც არ გამოყენებს გარეშე რესურსები. ჩვენ პირველად მრავალური დავასწავლით (MTL) ახალი ობიექტიკური ფუნქციის დამატებით განვიცნობისთვის თუ არა იქნება იქნება ან არა. შემდეგ ჩვენ გავაჩვენოთ ფრამეტრი, რომელიც უნდა იყოს ინტერტიკოსპერტის შემთხვევა (MoEE) რომ უფრო მუშაობელობა ნულ რესურსის დემომინის აკაპრატიფიკაციის საბოლოოდ, ექსპერიმენტიური წარმოდგენები ჩვენი მოდელეში უფრო ძალიან გავაკეთებს ძალიან უფრო სუპერიმენტიური მოდელების ჩვენი მოდელეში, და ჩვენი მოდელეში გავაკეთება მხოლოდ მოდელეში, რომელი</abstract_ka>
      <abstract_isl>Fyrirliggjandi líkanir fyrir greiningu á samsvæði sem nefnist einingar (NER) byggjast á fjölda ómerkaðra líkama eða merkta NER þjálfunargögn á marksvæðum. Hins vegar er að safna gögnum fyrir marksvæði með lítið auðlysni ekki aðeins dýrt, heldur einnig tímabært. Hence, we propose a cross-domain NER model that does not use any external resources.  Við kynnum fyrst fjölverkunarleiðni með því a ð bæta við nýja hlutlæga virkni til að greina hvort tákn eru nefnd einingar eða ekki. Við leggjum síðan fram ramma sem nefnist blanding af einstaklingum sérfræðingum (MoEE) til a ð bæta stöðugleika fyrir aðlögun núll-auðlinda svæðis. Loksins sýna niðurstöður úr tilraunum að líkan okkar framleiðir sterka, óeftirfylgjandi, samsvöruðu, samsvöruðu, samsvöruðu, samsvöruðu, samsvöruðu, samsvöruðu, samsvöruðu, samsvöruðu, samsvöruðu, samsvöruðu, samsvöruðu, samsvöruðu og samsv</abstract_isl>
      <abstract_hu>A tartományközi nevű entitások felismerésének (NER) meglévő modelljei számos címke nélküli korpuszra vagy címkével ellátott NER képzési adatra támaszkodnak a céltartományokban. Az alacsony erőforrású céltartományok adatgyűjtése azonban nemcsak drága, hanem időigényes is. Ezért olyan területeken átnyúló NER modellt javasolunk, amely nem használ külső erőforrásokat. Először bevezetjük a Multi-Task Learning (MTL) funkciót egy új objektív függvénnyel, hogy felismerjük, hogy a tokenek entitások-e vagy sem. Ezt követően bevezetjük a Mixture of Entity Experts (MoEE) nevű keretrendszert, hogy javítsuk a nulla erőforrás-tartomány adaptáció robusztusságát. Végezetül a kísérleti eredmények azt mutatják, hogy modellünk felügyelet nélküli cross-domain szekvencia címkézési modelleket felülmúlja, és modellünk teljesítménye közel áll a korszerű modell teljesítményéhez, amely kiterjedt erőforrásokat használ.</abstract_hu>
      <abstract_el>Τα υπάρχοντα μοντέλα αναγνώρισης οντοτήτων μεταξύ των τομέων βασίζονται σε πολυάριθμα μη επισημασμένα σώματα ή επισημασμένα δεδομένα εκπαίδευσης στους τομείς-στόχους. Ωστόσο, η συλλογή δεδομένων για τομείς στόχου χαμηλού πόρου δεν είναι μόνο δαπανηρή αλλά και χρονοβόρα. Ως εκ τούτου, προτείνουμε ένα μοντέλο που δεν χρησιμοποιεί εξωτερικούς πόρους. Πρώτα εισάγουμε μια εκμάθηση πολλαπλών εργασιών (προσθέτοντας μια νέα συνάρτηση αντικειμενικού για να εντοπίσουμε αν τα σήματα ονομάζονται οντότητες ή όχι. Στη συνέχεια, εισάγουμε ένα πλαίσιο που ονομάζεται Μείγμα εμπειρογνωμόνων οντοτήτων (ΜEE) για τη βελτίωση της ανθεκτικότητας για προσαρμογή στον τομέα μηδενικού πόρου. Τέλος, τα πειραματικά αποτελέσματα δείχνουν ότι το μοντέλο μας ξεπερνά τα ισχυρά μοντέλα επισήμανσης αλληλουχιών χωρίς επίβλεψη μεταξύ τομέων, και η απόδοση του μοντέλου μας είναι κοντά σε αυτή του σύγχρονου μοντέλου που χρησιμοποιεί εκτεταμένους πόρους.</abstract_el>
      <abstract_it>I modelli esistenti per il riconoscimento cross-domain named entity recognition (NER) si basano su numerosi dati di formazione NER non contrassegnati o etichettati nei domini target. Tuttavia, la raccolta di dati per domini target a basso contenuto di risorse non è solo costosa, ma richiede anche tempo. Pertanto, proponiamo un modello NER cross-domain che non utilizza risorse esterne. Per prima cosa introduciamo un Multi-Task Learning (MTL) aggiungendo una nuova funzione obiettivo per rilevare se i token sono entità denominate o meno. Introdurremo quindi un framework chiamato Mixture of Entity Experts (MoEE) per migliorare la robustezza dell'adattamento del dominio a risorse zero. Infine, i risultati sperimentali mostrano che il nostro modello supera i forti modelli di etichettatura cross-domain non supervisionati e le prestazioni del nostro modello sono vicine a quelle del modello all'avanguardia che sfrutta vaste risorse.</abstract_it>
      <abstract_kk>Бірнеше доменге аталған нысандарды анықтау үлгілері (NER) көп келтірілмеген корпус не нақты домендерде NER оқыту деректеріне тәуелді. Бірақ, төмен ресурс мақсатты домендердің деректерін жинау ғана бағатты емес, сондай-ақ уақытты пайдалану мүмкін. Сондықтан біз кез келген сыртқы ресурстарды пайдалануға болмайтын NER үлгісін ұсынамыз. Біріншіден біріншіден бірнеше тапсырмаларды оқыту (MTL) дегенді таңдап, белгілер аталған нысандарды анықтау үшін жаңа мақсатты функциясын қосу үшін таңдаймыз. Содан кейін біз Нөл ресурс доменінің адаптациясын жасау үшін бір бірлік эксперттердің (МоEE) Mixture of Entity Experts деп аталатын қоршау бағдарламасын келтіреміз. Соңында, тәжірибелі нәтижелеріміздің моделіміздің көмектесілмеген көмектесілмеген көмектесілмеген үлгілер үлгілерін жасайды, және моделіміздің көмектесілі ресурстарды көмектесетін өзге</abstract_kk>
      <abstract_lt>Esami tarpsritinio subjekto pripažinimo (NER) modeliai grindžiami daugeliu nepažymėtų korpuso arba pažymėtų NER mokymo duomenų tikslinėse srityse. Tačiau duomenų rinkimas mažai išteklių turinčioms tikslinėms sritims yra ne tik brangus, bet ir brangus. Todėl mes siūlome įvairių sričių NER model į, kuris nenaudoja jokių išorės išteklių. Pirmiausia įvedame daugiafunkcinį mokymąsi (MTL), pridėdami naują objektyvią funkciją nustatyti, ar simboliai yra pavadinti subjektais, ar ne. We then introduce a framework called Mixture of Entity Experts (MoEE) to improve the robustness for zero-resource domain adaptation.  Galiausiai, eksperimentiniai rezultatai rodo, kad mūsų model is yra veiksmingesnis už tvirtus nepastebimus įvairių sričių sekos ženklinimo modelius, o mūsų modelio veiksmingumas yra artimas moderniausio modelio, kuris sutelkia didelius išteklius, veiksmingumui.</abstract_lt>
      <abstract_mk>Постојаните модели за препознавање на ентитетите наречени преку домен (NER) се потпираат на бројни неозначени корпуси или означени податоци за обука на NER во домените на целта. Сепак, собирањето на податоци за домените со ниски ресурси е не само скапо, туку и временско потрошување. Затоа предложуваме крстодомен модел НЕР кој не користи никакви надворешни ресурси. We first introduce a Multi-Task Learning (MTL) by adding a new objective function to detect whether tokens are named entities or not.  Потоа воведуваме рамка наречена Мешавина на експерти на ентитетите (MoEE) за подобрување на силноста за адаптација на доменот без нула ресурси. Конечно, експерименталните резултати покажуваат дека нашиот модел е поголем од силните ненадгледувани модели за означување на секвенца на преку домени, а резултатите на нашиот модел се блиски до оној на најновиот модел кој користи екстремни ресурси.</abstract_mk>
      <abstract_ms>Model yang wujud untuk pengenalan entiti bernama-domain salib (NER) bergantung pada banyak corpus tidak ditabel atau data latihan NER ditabel dalam domain sasaran. However, collecting data for low-resource target domains is not only expensive but also time-consuming.  Oleh itu, kami melamar model NER cross-domain yang tidak menggunakan mana-mana sumber luaran. Kami pertama-tama memperkenalkan Pelajaran Berbagai Tugas (MTL) dengan menambah fungsi objektif baru untuk mengesan sama ada token bernama entiti atau tidak. Kemudian kami memperkenalkan kerangka yang dipanggil Mixture of Entity Experts (MoEE) untuk meningkatkan kepekatan untuk penyesuaian domain sumber-sifar. Akhirnya, hasil percubaan menunjukkan bahawa model kita melampaui prestasi kuat model label jujukan cross-domain yang tidak diawasi, dan prestasi model kita dekat dengan model state-of-the-art yang menggunakan sumber luas.</abstract_ms>
      <abstract_mt>Il-mudelli eżistenti għar-rikonoxximent ta’ entitajiet imsejħa f’diversi dominji (NER) jiddependu fuq għadd ta’ korpi mingħajr tikketta jew dejta ta’ taħriġ NER ittikkettata f’dominji fil-mira. However, collecting data for low-resource target domains is not only expensive but also time-consuming.  Hence, we propose a cross-domain NER model that does not use any external resources.  L-ewwel jintroduċu Tagħlim Multikompiti (MTL) billi żżid funzjoni oġġettiva ġdida biex nidentifikaw jekk it-tokens humiex imsejħa entitajiet jew le. Imbagħad nintroduċu qafas imsejjaħ Taħlita ta’ Esperti ta’ Entitajiet (MoEE) biex itejbu r-robustezza għall-adattament tad-dominju ta’ riżorsi żero. Fl-aħħar nett, ir-riżultati sperimentali juru li l-mudell tagħna jipproduċi mudelli qawwija ta’ tikkettar tas-sekwenzi transdomestiċi mhux sorveljati, u l-prestazzjoni tal-mudell tagħna hija qrib dik tal-mudell l-aktar avvanzat li jiġġenera riżorsi estensivi.</abstract_mt>
      <abstract_mn>Бүтээгдэхүүний хүлээн зөвшөөрөх (NER) хэд хэдэн бичигдэггүй корпус эсвэл тэмдэглэгдсэн NER сургалтын өгөгдлийг зорилготой зорилготой. Гэхдээ бага нөөцийн зорилготой зорилготой хэсэгт өгөгдлийг цуглуулах нь зөвхөн үнэтэй биш, мөн цаг хугацааны хэрэглээ. Тиймээс бид ямар ч гадаад нөөцийг ашиглахгүй NER хэлбэртэй загварыг санал болгож байна. Бид эхлээд олон-Task Learning (MTL) гэдгийг нэмж, шинэ зорилготой функцыг нэмж, тэмдэглэл нь бүтээгдэхүүний нэр эсвэл биш эсэхийг олж мэдэх хэрэгтэй. Дараа нь бид Нэгдсэн Эрчим хүмүүсийн Mixture of Entity Experts (MoEE) нэртэй системийг тайлбарлаж, 0-нүүрстөрөгчийн зохицуулалтын хүчтэй байдлыг сайжруулахын тулд. Эцэст нь, туршилтын үр дүнд бидний загвар нь маш их баталгаагүй хэмжээний дарааллын загварыг дамжуулдаг. Бидний загварын үйл ажиллагаа нь маш их нөөцийг ашигладаг урлагийн төвшин загварын тухай ойрхон байдаг.</abstract_mn>
      <abstract_ml>സാധാരണ തിരിച്ചറിയുന്നതിനുള്ള ക്രോഡോമെന്‍ പേരുള്ള മോഡലുകള്‍ നിലവിലുള്ള ഉപകരണങ്ങളില്‍ അളവില്ലാത്ത കോര്‍പ്പുസില്‍ ആശ്രയിക എന്നാലും കുറഞ്ഞ വിഭവങ്ങളുടെ ലക്ഷ്യം ഡോമെന്‍സിനുള്ള ഡേറ്റാ സംഘടിപ്പിക്കുന്നത് വിലയേറ്റ മാത്രമല്ല, സമയ അതുകൊണ്ട്, നമ്മള്‍ പുറത്തുള്ള വിഭവങ്ങള്‍ ഉപയോഗിക്കാത്ത ഒരു ക്രിസ്റ്റ് ഡൊമെയിന്‍ പ്രൊദ്ദേശിക്കുന് ഒരു പുതിയ ലക്ഷ്യം ചേര്‍ക്കുന്നതിനാല്‍ നമ്മള്‍ ആദ്യം ഒരു പല- ടാസ്ക് ലിന്‍ഷന്‍ പരിചയപ്പെടുത്തുന്നു. ഒളിപ്പുകള്‍ വസ്തുക്കള്‍ എന് പൂര്‍ണ്ണമായ വിഭവങ്ങളുടെ ഡോമെയിനുള്ള പൂര്‍ത്തീകരണത്തിനുള്ള റോബ്സ്ട്രോസ്റ്റ് മെച്ചട്ടല്‍ എന്ന പേരുള്ള ഒരു ഫ്രെയിമെക അവസാനം, പരീക്ഷണ ഫലങ്ങള്‍ കാണിച്ചു കൊണ്ടിരിക്കുന്നത് നമ്മുടെ മോഡല്‍ ശക്തിയായി സൂക്ഷിക്കപ്പെടാത്ത ക്രോഡോമെന്‍ സെക്കന്‍സ് ലേബിള്‍ മോഡല്‍ പ്രവര്‍ത്തിപ്പ</abstract_ml>
      <abstract_no>Eksiste modeller for krysdomenet med namn på entitetskjenning (NER) er på mange ikkje- merkte korpus eller merkte NER- treningsdata i måldområde. Men samling av data for låg ressursmål- domene er ikkje berre dykka, men også tidsbruk. I tillegg foreslår vi eit NER-modell for cross-domain som ikkje brukar eksterne ressursar. Vi introduserer først ein fleire oppgåver- læring (MTL) ved å leggja til ein ny målfunksjon for å finna om teikn er namnet på einingar eller ikkje. Vi introduserer så eit rammeverk kalla Mixture of Entity Experts (MoEE) for å forbetra kraftigheten for tilpassing av null-ressursdomene. Eksperimentale resultat viser at modellen vårt utfører sterke mønsterelementer for merkelappe på krysdomenesekvens, og utviklinga av modellen vårt er nær det i modellen som leverer utvida ressursar.</abstract_no>
      <abstract_pl>Istniejące modele rozpoznawania między domenami nazwanych entity (NER) opierają się na licznych nieoznakowanych korpusach lub oznakowanych danych treningowych NER w domenach docelowych. Jednak zbieranie danych dla niskich zasobów domen docelowych jest nie tylko kosztowne, ale także czasochłonne. W związku z tym proponujemy model NER między domenami, który nie wykorzystuje żadnych zasobów zewnętrznych. Najpierw wprowadzamy wielozadaniowe uczenie się (MTL) poprzez dodanie nowej funkcji obiektywnej, aby wykryć, czy tokeny są nazwane podmiotami, czy nie. Następnie wprowadzamy ramy o nazwie Mieszanka Ekspertów Entity (MoEE), aby poprawić solidność adaptacji domen zerowych. Wreszcie, wyniki eksperymentalne pokazują, że nasz model przewyższa silne nienadzorowane modele znakowania sekwencji między domenami, a wydajność naszego modelu jest zbliżona do najnowocześniejszego modelu, który wykorzystuje szerokie zasoby.</abstract_pl>
      <abstract_ro>Modelele existente pentru recunoașterea entităților denumite între domenii (NER) se bazează pe numeroase date de instruire fără etichete sau etichetate NER în domeniile țintă. Cu toate acestea, colectarea datelor pentru domeniile țintă cu resurse reduse este nu numai costisitoare, ci și consumatoare de timp. Prin urmare, propunem un model NER cross-domeniu care nu utilizează resurse externe. Introducem mai întâi un Multi-Task Learning (MTL) prin adăugarea unei noi funcții obiective pentru a detecta dacă jetoanele sunt numite entități sau nu. Introducem apoi un cadru numit Mixture of Entity Experts (MoEE) pentru a îmbunătăți robustețea adaptării domeniului zero-resurse. În cele din urmă, rezultatele experimentale arată că modelul nostru depășește modelele puternice de etichetare a secvențelor cross-domain nesupravegheate, iar performanța modelului nostru este aproape de cea a modelului de ultimă generație, care valorifică resurse extinse.</abstract_ro>
      <abstract_sr>Postojeći modeli za prepoznavanje entiteta preko domena (NER) oslanjaju se na brojne nepobeležene korpuse ili označene podatke o obuci NER u ciljnim domenama. Međutim, skupljanje podataka za ciljne domene niskih resursa nije samo skupo, već i potrošenje vremena. Stoga predlažemo model NER preko domena koji ne koristi nikakve vanjske resurse. Prvo predstavljamo učenje multitaska (MTL) dodajući novu objektivnu funkciju kako bi otkrili da li su znakovi imenovani entiteti ili ne. Onda predstavljamo okvir koji se zove Mixture of Entity Experts (MoEE) kako bi poboljšali snagu za adaptaciju domena zero resursa. Konačno, eksperimentalni rezultati pokazuju da naš model iznosi jake neodređene modele označavanja preko domena, a izvršnost našeg modela je blizu onog modela stanja umjetnosti koji utiče na velike resurse.</abstract_sr>
      <abstract_si>විශිෂ්ට ඩොමේන් නම් අයිති අඳුරණය (NER) සඳහා තියෙන්නේ මොඩේල් අයිතියෙන් ඉතින් අයිති කොර්පුස් සඳහා ලබාගත්ත NER ප නමුත්, අඩුම සම්බන්ධ ලක්ෂිත ඩෝමේන්ස්ට දත්ත සම්බන්ධ වෙන්නේ ගොඩක් විතරක් නෙවෙයි ඒත් වෙලාව ඉතින්, අපි ප්‍රශ්නයක් ප්‍රශ්නයක් කරනවා නැති ප්‍රශ්නයක් නැති ප්‍රශ්නයක්. අපි මුලින්ම ගොඩක් වැඩක් ඉගෙනගන්න (MTL) අලුත් අක්‍රියාත්මක වැඩක් සම්පූර්ණ කරනවා ටොකෙන්ස් නම් නම් නැති වැඩක් ත ඊට පස්සේ අපි ඉන්තිත්වය විශ්වාසිකය (MoEE) කියලා කියලා කියලා ක්‍රියාමාර්ගයක් ප්‍රවේශනය කරන්න ශූර්යය සඳහා  අන්තිමට, පරීක්ෂණ ප්‍රතිචාර ප්‍රතිචාර ප්‍රතිචාරයක් පෙන්වන්නේ අපේ මොඩල් බොහොම විශේෂ විශේෂ විශේෂ නැති ප්‍රතිචාරයක් ලේබිල</abstract_si>
      <abstract_so>Tusaalada joogtada ah ee lagu magacaabay aqoonsiga entity (NER) waxay ku xiran yihiin qodob badan oo aan labeyn ama macluumaadyo waxbarashada NER lagu qoray goobaha waxqabadka. Si kastaba ha ahaatee soo ururinta macluumaadka meelaha loogu talagalay waxyaabaha yar ma ahan kharash oo kaliya laakiin xittaa isticmaalka wakhtiga. Sidaas darteed waxaynu soo jeedaynaa model NER oo aan isticmaaleyn wax qalab ah. We first introduce a Multi-Task Learning (MTL) by adding a new objective function to detect whether tokens are named entities or not.  Markaas waxaynu soo bandhignaynaa framework la odhan jiray Mixture of Entity Experts (MoEE) si aan u hagaajinno dharka habboon-resource domain. Ugu dambaysta waxaa muuqda qaababka tijaabada ah in modellkayagu uu sameeyaa modelalka labeecada oo xoog badan oo aan ilaalinayn, sameynta modellkayagana waa u dhow qaababka dowladda farshaxanka ah oo kordhaya raslimo badan.</abstract_so>
      <abstract_sv>Befintliga modeller för domänöverskridande namngiven entitetsigenkänning (NER) bygger på många omärkta korpus eller märkta NER-träningsdata i måldomäner. Insamling av data för lågresursmåldomäner är dock inte bara dyrt utan också tidskrävande. Därför föreslår vi en gränsöverskridande NER-modell som inte använder några externa resurser. Vi introducerar först en Multi-Task Learning (MTL) genom att lägga till en ny objektiv funktion för att upptäcka om tokens heter entiteter eller inte. Vi introducerar sedan ett ramverk som kallas Mixture of Entity Experts (MoEE) för att förbättra robustheten för nollresurs domänanpassning. Slutligen visar experimentella resultat att vår modell presterar bättre än starka oövervakade modeller för sekvensmärkning över domäner, och prestandan för vår modell ligger nära den för den senaste modellen som utnyttjar omfattande resurser.</abstract_sv>
      <abstract_ur>کرسٹ ڈومین کے لئے موجود موجود موجود ہیں جن کا نام انٹیٹی شناخت (NER) بہت سی ناپذیر کورپوس پر بھروسہ ہے یا موجود ڈومین میں نیر ترینسی ڈیٹ پر لکھی جاتی ہے. لیکن، کم رسورس موجود ڈومین کے لئے ڈیٹا جمع کرنا صرف گران نہیں ہے بلکہ زمان مصرف بھی ہے. اسی وجہ سے ہم ایک کرس ڈومین NER موڈل کو پیشنهاد کرتے ہیں جو کسی خارجی منبع کا استعمال نہیں کرتا۔ ہم پہلی بار ایک Multi-Task Learning (MTL) کو پہنچا دیتے ہیں تاکہ ایک نوی موجود функцион اضافہ کریں کہ معلوم کریں کہ آیا ٹوکینوں کا نام entities ہیں یا نہیں۔ اس کے بعد ہم ایک فرمود پیش کریں جن کا نام ایک ٹیٹی اکسپٹر (MOEE) ہے کہ صفر-سروسیز ڈومین کے سامان کے مطابق قوت کی تدبیر کرے۔ آخر میں، آزمائش کے نتائج دکھاتے ہیں کہ ہماری مدل مضبوط کرس ڈومین کے لابلینگ موڈل سے زیادہ مضبوط ہے، اور ہمارے مدل کی پرورش اس موڈل کے قریب ہے جو بڑے سرمایہ کے ذریعہ سے اضافہ کرتا ہے.</abstract_ur>
      <abstract_ta>பொருள் அடையாளம் (NER) பெயரிடப்பட்ட மாதிரிகளுக்கு இருக்கும் குறிப்பிடப்படாத கோப்புகள் அல்லது குறிப்பிட்ட NER பயிற்சி தரவை குறிப ஆயினும், குறைந்த மூலத்திற்கான தகவல் சேகரி அதனால், நாம் வெளி மூலங்களை பயன்படுத்தாத ஒரு குறுக்கும் களஞ்சியத்தின் பாதையை பரிந்துரைக்கிறோம். @ info பிறகு நாம் ஒரு சட்டத்தை குறிப்பிடுகிறோம் பூஜ்ஜியமான மூலத்தின் மேம்படுத்தலுக்கான தொகுப்பை மேம்படுத்துவதற்கு. இறுதியில், சோதனையின் முடிவுகள் காண்பிக்கப்படுகிறது நம் மாதிரி வலிமை பாதுகாப்பாக்கப்படாத குறிப்பு சிட்ட மாதிரிகளை செயல்படுத்துகிறது, நம் மாதிரியின் ச</abstract_ta>
      <abstract_uz>Name Lekin, qisqa manbalar lokal domene uchun maʼlumotni olishni faqat qiymati emas, balki vaqt ishlatishda ham qiymati. Shunday qilib, biz tashqi rasmlarni ishlatilmagan NER modelini rivojlanamiz. Biz birinchi marta bir necha vazifa o'rganish (MTL) qoʻshish uchun yangi obʼekt funksiyasini aniqlash. Keyin biz zero resource domen adaptarini o'zgartirish uchun qismini o'zgartirish mumkin. Endi, tajriba natijalari esa modelimizning ko'rsatadi, bizning modelimiz juda ko'p xavfsiz qo'llanmagan qo'llanmalar modellarini bajaradi, modelimizning bajarishi juda ko'proq rasmlarni qo'llab qo'shiladi.</abstract_uz>
      <abstract_vi>Người mẫu tồn tại cho việc nhận dạng thực thể tên miền chéo (NER) dựa trên nhiều dữ liệu tập đoàn chưa được dán nhãn hay nằm trên người trong miền đích. Tuy nhiên, việc thu thập dữ liệu cho khu vực mục tiêu ít tài nguyên không chỉ tốn kém mà còn tốn thời gian. Do đó, chúng tôi đề xuất một mô hình giao lãnh vực nằm ngoài lãnh thổ không sử dụng nguồn tài nguyên nào. Chúng ta sẽ giới thiệu một chương trình giáo dục đa nhiệm vụ (MTV) bằng cách thêm một hàm mục tiêu mới để xác định xem bản sao có tên thực thể hay không. Sau đó, chúng tôi sẽ tạo ra một cơ sở gọi là hỗn hợp các chuyên gia chi tiết, để cải thiện uy lực sửa chữa lãnh thổ bằng không. Cuối cùng, kết quả thử nghiệm cho thấy mô hình của chúng ta đạt được những mô hình không giám sát mạnh, và hiệu quả của mô hình của chúng ta gần với mô hình hiện đại, dùng đầy đủ nguồn lực.</abstract_vi>
      <abstract_bg>Съществуващите модели за междудомейново разпознаване на обекти (НЕР) разчитат на множество немаркирани корпуси или обозначени данни за обучение на НЕР в целевите области. Събирането на данни за целеви домейни с ниски ресурси обаче е не само скъпо, но и отнема време. Затова предлагаме междудомейнен модел, който не използва никакви външни ресурси. Първо въвеждаме обучение с множество задачи (МТЛ), като добавим нова обективна функция, за да открием дали токените са наименовани обекти или не. След това въвеждаме рамка, наречена смесване на експерти по субекти (MoEE), за да подобрим устойчивостта за адаптиране на областта с нулеви ресурси. И накрая, експерименталните резултати показват, че нашият модел превъзхожда силните модели за етикетиране на последователност без надзор, а ефективността на нашия модел е близка до тази на най-съвременния модел, който използва обширни ресурси.</abstract_bg>
      <abstract_nl>Bestaande modellen voor cross-domein named entity recognition (NER) zijn gebaseerd op tal van niet-gelabelde corpus- of gelabelde NER-trainingsgegevens in doeldomeinen. Het verzamelen van gegevens voor low-resource doeldomeinen is echter niet alleen duur, maar ook tijdrovend. Daarom stellen we een cross-domein NER model voor dat geen externe bronnen gebruikt. We introduceren eerst een Multi-Task Learning (MTL) door een nieuwe objectieve functie toe te voegen om te detecteren of tokens entiteiten zijn genoemd of niet. Vervolgens introduceren we een framework genaamd Mixture of Entity Experts (MoEE) om de robuustheid voor zero-resource domeinaanpassing te verbeteren. Tot slot tonen experimentele resultaten aan dat ons model beter presteert dan sterke, onbeheerde cross-domain sequence labeling modellen, en dat de prestaties van ons model dicht bij die van het state-of-the-art model liggen dat gebruik maakt van uitgebreide middelen.</abstract_nl>
      <abstract_da>Eksisterende modeller for cross-domænenavnet entity recognition (NER) er afhængige af talrige mærkede korpus- eller mærkede NER-træningsdata i måldomæner. Indsamling af data for måldomæner med lav ressource er dog ikke kun dyrt, men også tidskrævende. Derfor foreslår vi en NER-model på tværs af domæner, der ikke bruger eksterne ressourcer. Vi introducerer først en Multi-Task Learning (MTL) ved at tilføje en ny objektiv funktion til at registrere, om tokens er navngivne entiteter eller ej. Vi introducerer derefter en ramme kaldet Mixture of Entity Experts (MoEE) for at forbedre robustheden for nul-ressource domæne tilpasning. Endelig viser eksperimentelle resultater, at vores model overgår stærke uautoriserede cross-domain sekvens mærkningsmodeller, og ydeevnen af vores model er tæt på den state-of-the-art model, der udnytter omfattende ressourcer.</abstract_da>
      <abstract_hr>Postojeći modeli za prepoznavanje preko domena entiteta (NER) oslanjaju se na brojne nepoznate korpuse ili označene podatke o obuci NER u ciljnim domenama. Međutim, skupljanje podataka za ciljne domene niskih resursa nije samo skupo, već i potrošenje vremena. Stoga predlažemo preko domena NER model koji ne koristi nikakve vanjske resurse. Prvo predstavljamo učenje višezadataka (MTL) dodajući novu objektivnu funkciju kako bi otkrili da li su znakovi imenovani entiteti ili ne. Zatim predstavljamo okvir koji se zove Mixture of Entity Experts (MoEE) kako bi poboljšali snagu za adaptaciju domena nula resursa. Konačno, eksperimentalni rezultati pokazuju da naš model iznosi jake neodređene modele označavanja preko domena, a učinkovitost našeg modela je blizu tog modela stanja umjetnosti koji utječe na ogromne resurse.</abstract_hr>
      <abstract_fa>مدل‌های موجود برای شناسایی entities نامیده شده‌اند (NER) بر بسیاری از شرکت‌های نامیده‌شده یا اطلاعات آموزش NER در دامنهای هدف نامیده شده‌اند. با این حال، جمع داده‌ها برای دامنهای هدف کم منابع نه تنها گرون است، بلکه همچنین مصرف زمان است. بنابراین، ما یک مدل NER cross-domain پیشنهاد می کنیم که از هیچ منابع خارجی استفاده نمی کند. اولین بار یک یادگیری Multi Task (MTL) را با اضافه کردن یک تابع هدف جدید برای شناسایی که آیا نشانه‌ها به عنوان یک entity نامیده می‌شوند یا نه معرفی می‌کنیم. سپس ما یک چهارچوب به نام Mixture of Entity Experts (MoEE) را معرفی می‌کنیم تا نیرومندی برای تغییر دادن دامنۀ منبع صفر را بهتر کند. بالاخره، نتایج آزمایشی نشان می دهد که مدل ما مدل نقاشی قابل استفاده از مدل نقاشی قابل استفاده نیست، و عملکرد مدل ما نزدیک به مدل هنر است که منابع زیادی را تأثیر می دهد.</abstract_fa>
      <abstract_de>Bestehende Modelle zur domänenübergreifenden benannten Entity Recognition (NER) stützen sich auf zahlreiche nicht beschriftete Korpus- oder beschriftete NER-Trainingsdaten in Zieldomänen. Das Sammeln von Daten für ressourcenarme Zieldomänen ist jedoch nicht nur teuer, sondern auch zeitaufwendig. Daher schlagen wir ein domänenübergreifendes NER-Modell vor, das keine externen Ressourcen verwendet. Wir führen zunächst ein Multi-Task Learning (MTL) ein, indem wir eine neue Zielfunktion hinzufügen, um zu erkennen, ob Token benannte Entitäten sind oder nicht. Anschließend führen wir ein Framework namens Mixture of Entity Experts (MoEE) ein, um die Robustheit für Zero-Resource Domain Adaption zu verbessern. Schließlich zeigen experimentelle Ergebnisse, dass unser Modell starke unüberwachte Cross-Domain Sequence Labeling Modelle übertrifft, und die Leistung unseres Modells liegt nahe an der des State-of-the-Art Modells, das umfangreiche Ressourcen nutzt.</abstract_de>
      <abstract_ko>기존의 크로스 필드 이름 실체 식별(NER) 모델은 목표 영역에 표시되지 않은 대량의 자료 라이브러리나 표시된 NER 트레이닝 데이터에 의존한다.그러나 낮은 자원 목표 영역을 위해 데이터를 수집하는 것은 비용이 들 뿐만 아니라 시간도 소모된다.따라서 외부 리소스를 사용하지 않는 도메인 간 NER 모델을 제안했습니다.먼저 MTL(Multi-Task Learning)을 도입하여 토큰이 명명된 엔티티인지 확인하는 새로운 대상 함수를 추가합니다.그리고 우리는 혼합 실체 전문가(MoEE)라는 틀을 도입하여 제로 자원 영역이 스스로 적응하는 노봉성을 향상시켰다.마지막으로 실험 결과에 의하면 우리의 모델은 감독이 없는 전역 서열 표기 모델보다 우수하고 우리의 모델의 성능은 대량의 자원을 이용한 최신 모델에 가깝다.</abstract_ko>
      <abstract_tr>Çoklu-domenyň ady bir enti tanamak üçin bar modeller (NER) birnäçe ýazylmaz corpus ýa nusga sahypalarda NER eğitim maglumatlaryna etilýär. Ýöne, iň az resursy maksady alanlar üçin maglumatlary toplamak diňe wagt ullanýan däl, hem wagt ullanýan Şonuň üçin biz daşarydaky çeşmeleri ulanmaýan cross-domain NER modelini teklip edip görýäris. Ilkinji gezek, täze bir maksady fonksiýany a ňlamak üçin bir Multi-Task Learning (MTL) bilen tanyşyrlyk Sonra "Mixture of Entity Experts" (MoEE) nul-resource domain adaptation üçin güýçlidigini geliştirmek üçin bir çerçew girişdiririk. Soňunda, deneysel netijelerimiz modelimiz daşary edilmedik cross-domeny etiketleme modellerini çykarýar we modelimiz möhüm çeşmeleri etkileýän durumda ýakyn şekilde çykarýar.</abstract_tr>
      <abstract_sw>Existing models for cross-domain named entity recognition (NER) rely on numerous unlabeled corpus or labeled NER training data in target domains.  Hata hivyo, kukusanya taarifa kwa ajili ya maeneo ya malengo ya chini ya rasilimali si ghali tu bali pia matumizi ya muda. Kwa hiyo, tunapendekeza modeli ya NEW ambayo haitumii rasilimali za nje. Kwanza tunaanzisha mafunzo ya kazi nyingi (MTL) kwa kuongeza kazi mpya ya malengo ili kutambua kama alama zinaitwa au la. Kisha tukaanzisha mfumo unaoitwa Mixture of Experts of Entity (MoEE) ili kuboresha uchumi wa mazingira ya rasilimali sifuri. Mwisho, matokeo ya majaribio yanaonyesha kuwa mifano yetu inafanya mifano yenye nguvu isiyo na usalama wa mifumo ya mifumo ya mitandao ya ndani, na utendaji wa muundo wetu ni karibu na mifano ya sanaa inayotumia rasilimali nyingi.</abstract_sw>
      <abstract_af>Bestaande modele vir kruisdomein genaamde entiteit herken (NER) vertrou op veelvuldige ongeabelde korpus of etiketeerde NER onderwerp data in doel domeine. Maar die versameling van data vir lae-hulpbron-doel domeine is nie slegs koste nie, maar ook tyd-gebruik nie. Daarom, ons voorstel 'n kruisdomein NER model wat nie enige eksterne hulpbronne gebruik nie. Ons introduseer eerste 'n Multi-Task Learning (MTL) deur 'n nuwe objekte funksie by te voeg om te beskry of tekens genoem is of nie. Ons introduseer dan 'n raamwerk genoem Miskien van Entiteit-eksperte (MoEE) om die kragtigheid vir nul-hulpbron-domein-aanpassing te verbeter. Eindelik, eksperimentele resultate wys dat ons model sterk nie ondersteunde kruisdomein sekwensiemerking modele uitvoer nie, en die prestasie van ons model is naby aan wat van die staat-van-kunste model wat uitvoer uitbreidige hulpbronne.</abstract_af>
      <abstract_id>Model yang ada untuk pengenalan entitas bernama cross-domain (NER) bergantung pada banyak corpus tidak ditandai atau ditandai data pelatihan NER di domain target. Namun, mengumpulkan data untuk domain target sumber daya rendah tidak hanya mahal tapi juga memakan waktu. Oleh karena itu, kami mengusulkan model NER cross-domain yang tidak menggunakan sumber daya luar. Kami pertama kali memperkenalkan Multi-Task Learning (MTL) dengan menambahkan fungsi objektif baru untuk mendeteksi apakah token bernama entitas atau tidak. Kemudian kami memperkenalkan cadangan yang disebut Mixture of Entity Experts (MoEE) untuk meningkatkan kepekatan untuk adaptasi domain sumber daya nol. Akhirnya, hasil percobaan menunjukkan bahwa model kita melampaui prestasi yang kuat tidak diawasi model label urutan cross-domain, dan prestasi model kita dekat dengan model state-of-the-art yang menggunakan sumber daya ekstensif.</abstract_id>
      <abstract_sq>Modelet ekzistuese për njohjen e njësisë me emër ndër-domain (NER) mbështeten në korpus të shumtë pa etiketë apo të dhëna të trajnimit të NER në domenat objektiv. Megjithatë, mbledhja e të dhënave për domenat e shënjestrave me burime të ulëta nuk është vetëm e shtrenjtë por gjithashtu e konsumuese kohe. Kështu, ne propozojmë një model NER ndërdomeni që nuk përdor asnjë burim të jashtëm. Ne fillimisht prezantojmë një Mësim Multi-Task (MTL) duke shtuar një funksion objektiv të ri për të zbuluar nëse tokenat quhen njësi apo jo. Pastaj futim një kuadër të quajtur Përzierje e Ekspertëve të Njësisë (MoEE) për të përmirësuar fuqinë për përshtatjen e fushës zero-burim. Më së fundi, rezultatet eksperimentale tregojnë se modeli ynë mbizotëron modele të forta të shënuara të sekuencës së mbikëqyrura ndër-domeni, dhe performanca e modelit tonë është afër modelit më të lartë që nxjerr burime të gjerë.</abstract_sq>
      <abstract_bn>বৈশিষ্ট্য স্বীকৃতির (NER) নামের বিদ্যমান মডেলের জন্য বিদ্যমান মডেল বিশেষ অলাইন কোর্পাসের উপর নির্ভর করে অথবা লক্ষ্য ডোমেইনে নির তবে কম-সম্পদ টার্গেট ডোমেনের জন্য তথ্য সংগ্রহ করা শুধুমাত্র দামী নয়, কিন্তু সময়-ব্যবহারের জন্য। তাই, আমরা একটি ক্রস-ডোমেইন নতুন মডেল প্রস্তাব করি যা বাইরের কোন সম্পদ ব্যবহার করে না। আমরা প্রথমে একটি মাল্টিক টাস্ক শিক্ষা শিক্ষার সাথে পরিচয় করিয়ে দিচ্ছি নতুন একটি উদ্দেশ্য ফাংশন যোগ করে দেখানোর জন্য প্রতীকের নাম ন তারপর আমরা একটি ফ্রেম পরিচয় করিয়ে দেই যার নাম মিক্স্টার অফ এন্টিটি বিশেষজ্ঞদের (মোইE) নামে শুধুমাত্র সম্পদ ডোমেইনের প্রতিষ্ঠানের অবশেষে, পরীক্ষার ফলাফল দেখা যাচ্ছে যে আমাদের মডেল শক্তিশালী সংরক্ষিত ক্রস-ডোমেইনের সেকেন্ড লেবেলিং মডেলের প্রযোজ্য, আর আমাদের মডেলের প্রভাব হচ্ছে রাষ্ট্</abstract_bn>
      <abstract_am>Existing models for cross-domain named entity recognition (NER) rely on numerous unlabeled corpus or labeled NER training data in target domains.  ምንም እንኳን፣ ዳራዎችን ለታናሽ-resource አካባቢ ዶሜኖችን የሚሰበስብ ጥቅም ብቻ አይደለም ነገር ግን የጊዜው መጠቀም ነው፡፡ ስለዚህም የውጭ ሀብትን የማይጠቀም የዲሞላዊ አዲስ ሞዴል እናሳልጋለን፡፡ መጀመሪያ የብዙ ስራ ትምህርት (MTL) አዲስ አካባቢ функцን ለመጨመር ማረጋገጫ ምልክቶችን አካባቢዎች ወይም አለመሆኑን ለማረጋገጥ እናስጠጋለን፡፡ በኋላም የኢትዮጵያ Experts (MoEE) መግለጫ ለzero-resource domain አካባቢውን ለማድረግ የሚባልን ፍሬም እናሳውቃለን፡፡ በመጨረሻው፣ የሞከሩ ፍሬዎች ምሳሌያችን የተጠበቀውን የኮምፒዩተር የድምፅ ምርጫዎችን እንዲያሳየው እና የሞዴላታችን ውጤት ብዙዎችን ሀብትን በሚያሰናከል የዐርድ ሥርዓት ሞዴል ቅርብ ነው፡፡</abstract_am>
      <abstract_bs>Postojeći modeli za prepoznavanje preko domena imena entiteta (NER) oslanjaju se na brojne neopisene korpuse ili označene podatke o obuci NER u ciljnim domenama. Međutim, skupljanje podataka za ciljne domene niskih resursa nije samo skupo, već i potrošenje vremena. Stoga predlažemo preko domena NER model koji ne koristi nikakve vanjske resurse. Prvo predstavljamo učenje multitaska (MTL) dodajući novu objektivnu funkciju kako bi otkrili da li su znakovi imenovani entiteti ili ne. Zatim predstavljamo okvir nazvan Mjesak stručnjaka entiteta (MoEE) kako bi poboljšali snagu za adaptaciju domena nula resursa. Konačno, eksperimentalni rezultati pokazuju da naš model iznosi jake neodređene modele označavanja krstodomena sekvence, a učinkovitost našeg modela je blizu tog modela stanja umjetnosti koji utiče na ogromne resurse.</abstract_bs>
      <abstract_hy>Existing models for cross-domain named entity recognition (NER) rely on numerous unlabeled corpus or labeled NER training data in target domains.  Այնուամենայնիվ, ցածր ռեսուրսների նպատակային բնագավառների տվյալներ հավաքելը ոչ միայն թանկ է, այլ նաև ժամանակ է պահանջում: Հետևաբար, մենք առաջարկում ենք մի միջոցային ՆԵՌ մոդել, որը չի օգտագործում արտաքին ռեսուրսներ: Սկզբում մենք ներկայացնում ենք բազմախնդիրներ սովորելու (MTL), ավելացնելով նոր օբյեկտիվ ֆունկցիա, որպեսզի բացահայտենք, արդյոք նշանները անվանվում են միավորներ, թե ոչ: Այնուհետև մենք ներկայացնում ենք մի կառուցվածք, որը կոչվում է Մոգերի մասնագետների խառնուրդ (MoEE), որպեսզի բարելավենք զրո ռեսուրսների ոլորտի ադապտացիայի ուժեղությունը: Վերջապես, փորձարկվող արդյունքները ցույց են տալիս, որ մեր մոդելը արտադրում է ուժեղ, անվերահսկված հատվածային հաջորդականության պիտակավորման մոդելներ, և մեր մոդելի արտադրողությունը մոտ է նորագույն մոդելի, որը օգտագործում է էքսպանսիվ ռեսուրսներ:</abstract_hy>
      <abstract_az>Üstünlük domena adlı entitə tanıması (NER) üçün mövcud modellər çoxlu işarə edilməmiş korpus və ya məqsəd domenalarda etiketli NER təhsil məlumatlarına təvəkkül edir. Lakin, düşük ressurs məqsədilərinin məlumatlarını toplama yalnız vaxt istifadəsi mal deyil. Beləliklə, biz bir neçə-domani NER modeli təklif edirik ki, heç bir dış resursu istifadə etməz. Biz ilk dəfə çox-Task Öyrənməsi (MTL) ilə yeni məqsəd funksiyasını əlavə edirik ki, möcüzələrin adlandırılmış və ya yoxdur. Sonra Null ressurs domeini uyğunlaşdırmaq üçün güclüyü artırmaq üçün Entity Experts (MoEE) adlı bir frameworku təşkil edirik. Sonunda, təcrübə sonuçları göstərir ki, modellərimiz çox qüvvətli, çox dəyişdirilmiş, çox dəyişdirilmiş çox dəyişdirilmiş çox dəyişdirilmiş, modellərin təcrübəsi modellərinin çox yaxındır.</abstract_az>
      <abstract_et>Olemasolevad domeenidevahelise nimega üksuste tuvastamise mudelid tuginevad sihtvaldkondades arvukatele märgistamata korpustele või märgistatud NER koolitusandmetele. Andmete kogumine madala ressursiga sihtdomeenide kohta ei ole aga mitte ainult kallis, vaid ka aeganõudev. Seetõttu pakume välja valdkondadevahelise NER mudeli, mis ei kasuta välisressursse. Esiteks võtame kasutusele mitme ülesandega õppe (MTL), lisades uue objektiivfunktsiooni, et tuvastada, kas tokenid on nimetatud olemid või mitte. Seejärel võtame kasutusele raamistiku nimega Mixture of Entity Experts (MoEE), et parandada usaldusväärsust nullressursside valdkonnas kohanemiseks. Lõpuks näitavad eksperimentaalsed tulemused, et meie mudel ületab tugevaid järelevalveta valdkondadevahelisi järjestuste märgistamise mudeleid ning meie mudeli jõudlus on lähedane tipptasemel mudelil, mis kasutab ulatuslikke ressursse.</abstract_et>
      <abstract_cs>Stávající modely pro rozpoznávání entit mezi doménami (NER) spoléhají na mnoha neznačených korpusech nebo označených NER tréninkových dat v cílových doménách. Shromažďování dat pro cílové domény s nízkými zdroji je však nejen nákladné, ale také časově náročné. Proto navrhujeme cross-domain NER model, který nevyužívá žádné externí zdroje. Nejprve představujeme Multi-Task Learning (MTL) přidáním nové objektivní funkce pro zjištění, zda tokeny jsou pojmenovány entity nebo ne. Následně představíme rámec s názvem Mixture of Entity Experts (MoEE), který zlepšuje robustnost pro adaptaci domén s nulovými zdroji. Nakonec experimentální výsledky ukazují, že náš model překonává silné modely bez dohledu nad sekvencemi mezi doménami a výkon našeho modelu je blízko výkonu nejmodernějšího modelu, který využívá rozsáhlé zdroje.</abstract_cs>
      <abstract_fi>Nykyiset mallit eri toimialueiden nimetyn entiteetin tunnistamiseksi (NER) perustuvat lukuisiin merkitsemättömiin korpusiin tai merkittyihin NER-koulutustietoihin kohdetoimialoilla. Tietojen kerääminen vähävaraisista kohdeverkkotunnuksista on kuitenkin kallista ja myös aikaa vievää. Siksi ehdotamme monialaista NER-mallia, jossa ei käytetä ulkoisia resursseja. Otamme ensin käyttöön Multi-Task Learning (MTL) -toiminnon lisäämällä uuden objektiivifunktion, joka tunnistaa, ovatko poletit nimettyjä kokonaisuuksia vai eivät. Tämän jälkeen otamme käyttöön kehyksen nimeltä Mixture of Entity Experts (MoEE), joka parantaa resurssien nollatason sopeutumisen kestävyyttä. Kokeelliset tulokset osoittavat, että mallimme suoriutuu paremmin kuin vahvat valvomattomat monialaiset sekvenssimerkintämallit, ja mallimme suorituskyky on lähellä huippuluokan mallia, joka hyödyntää laajoja resursseja.</abstract_fi>
      <abstract_ca>Existing models for cross-domain named entity recognition (NER) rely on numerous unlabeled corpus or labeled NER training data in target domains.  Però la col·lecció de dades per dominys d'objectiu de baix recursos no només és cara, sinó també costa temps. Hence, we propose a cross-domain NER model that does not use any external resources.  Primer introduim una Multitask Learning (MTL) afegint una nova funció objectiva per detectar si les fitxes es diuen entitats o no. Llavors introduïm un marc anomenat Mistura d'Experts d'Entitats (MoEE) per millorar la robustet de l'adaptació al domini de recursos zero. Finalment, els resultats experimentals mostren que el nostre model supera models forts de etiqueta de seqüència transdominical sense supervisió, i el rendiment del nostre model està prop d'aquell del model d'última generació que aprofita recursos extensos.</abstract_ca>
      <abstract_ha>Ana ƙayyade motsalan wanda ke iya samar-komai da aka ambaci sunan ganin shaidar abun (NER) sai ka dõgara a kan wasu nau'in rubutu da ba'a rubutu ba ko kuma da aka rubũta data na tsarin NER a cikin wurãren aiki da ake amfani da. A lokacin da, samun data dõmin wurãren-resource bai zama mai amfani ba kawai kuma amma mai amfani da lokaci. Saboda haka, Munã buɗa wani misali na guda-komai wanda bai yi amfani da duk wuri na fita ba. @ info: whatsthis Sa'an nan kuma muka ƙara wani firam wanda ake kiran Mixture of Entity Expertaries (MoEEU) dõmin ya kyautata tufi wa adadi ga lokacin-resource. Haƙarami, matsalan jarrabai ke nuna cewa misalinmu yana samar da misãlai mai ƙaranci da ba'a tsare shi ba, kuma aikin misalin mu yana kusa ta daga misalin-na-sanar da ke ƙarfafa matabbata.</abstract_ha>
      <abstract_sk>Obstoječi modeli za meddomensko prepoznavanje entitet (NER) temeljijo na številnih neoznačenih korpusih ali označenih podatkih o usposabljanju NER na ciljnih področjih. Vendar pa zbiranje podatkov za ciljne domene z nizkimi viri ni le drago, ampak tudi dolgotrajno. Zato predlagamo meddomenski model NER, ki ne uporablja nobenih zunanjih virov. Najprej uvedemo večopravilno učenje (MTL), tako da dodamo novo ciljno funkcijo, da ugotovimo, ali so žetoni imenovani entiteti ali ne. Nato uvedemo okvir, imenovan Mešanica strokovnjakov za entitete (MoEE), da bi izboljšali robustnost prilagajanja področja brez virov. Nazadnje, eksperimentalni rezultati kažejo, da je naš model boljši od močnih nenadzorovanih modelov označevanja zaporedja med področji, učinkovitost našega modela pa je blizu učinkovitosti najsodobnejšega modela, ki uporablja obsežne vire.</abstract_sk>
      <abstract_jv>Saintin modes for inter-domain name Enty knowtion (NIR) Relay on number unbleed capitus or label BER Learning data in goal domain. politenessoffpolite"), and when there is a change ("assertivepoliteness Lakok, kita gunakake model karo-domain NeR sing ora nggambar barang kering Awak dhéwé nambah tanggal Multi-task Learning We would like to insert a frame that's karaoke Mixure of Entty Exprts (MoET) to refresh the botus for 0-source domain modification. Ulihke, dadi sing paling-paling nggambar na modelo sing bisa ngomong nik modelo sing bisa perusahaan maneh dumaten, lan akeh dumaten sing modelo iki dadi kapan yo sampek modelo state-of-the-arts modelo sing bisa ngelaran akeh banjur.</abstract_jv>
      <abstract_fil>May mga modelo para sa karamihan ng pagkakilala ng entity na pangalan ng cross-domain (NER) ay umaasa sa karamihan na hindi nabalitaan ng corpus o labeled ng NER training data sa target domains. Gayon ma'y ang pagpipisan ng mga data para sa mababang-resource target domains ay hindi lamang mahalaga, kundi ang pag-usap ng time. Dahil dito, ipinagbibigay natin ang cross-domain NER model na hindi gumagamit ng anomang external resources. Una namin ipinakilala ang Multi-Task Learning (MTL) sa pamamagitan ng pagdagdag ng bagong objektional na function upang makita kung ang mga tokens ay pangalan ng entities o hindi. Ipakikilala namin ang isang framework na tinatawag na Mixture of Entity Experts (MoEE) upang maganda ang robustness para sa pag-adaptasyon ng domain ng zero-resource. Sa katapusan, ang mga resulta ng eksperimento ay nagpapakita na ang modelo namin ay nagtataglay ng malakas na modelo ng mga labeling na walang pag-upervised na cross-domain sequence, at ang gawain ng modelo namin ay malapit sa modelo ng state-of-art na nagbibigay ng mga malaking resources.</abstract_fil>
      <abstract_bo>Existing models for cross-domain named entity recognition (NER) rely on numerous unlabeled corpus or labeled NER training data in target domains. ཡིན་ནའང་། རྒྱུ་དངོས་འབྲེལ་ཆུང་ཉུང་བའི་དམིགས་ཡུལ་གྱི་གནས་ཚུལ་སྒྲིག་འཇུག དེར་བརྟེན། ང་ཚོས་བློ་གཏད་ཆེན་པོ་ཞིག་གིས་ཡིག་ཆ་སྤྱོད་ཀྱི་རྒྱུ་དངོས་པོ་མ་རེད། ང་ཚོས་དང་པོ་ནས་མཐོང་སྣ་གྲངས་ཀྱི་ཤེས་འཇུག་པ་ཞིག་སྤྲོད་ནས་དམིགས་ཡུལ་གསར་པ་ཞིག་གིས་རྟོགས་བྱེད་ཀྱི་ཡོད་པ་མིན་འདུག འུ་ཚོས་ཀྱིས་Entity Experts(MoEE)གི་སྒྲིག་བཀོད་དང་མཐུན་རྐྱེན་གྱིས་རྐྱེན་སྟངས་ཅིག་གསལ་བཤད་བྱེད་ཀྱི་ཡོད། Finally, experimental results show that our model outperforms strong unsupervised cross-domain sequence labeling models, and the performance of our model is close to that of the state-of-the-art model which leverages extensive resources.</abstract_bo>
      <abstract_he>דוגמנים קיימים לזיהוי ישויות בשם חוצה תחום (NER) תלויים במספר קורפוס ללא סימנים או מידע אימון NER מסוים בתחומים המטרה. However, collecting data for low-resource target domains is not only expensive but also time-consuming.  לכן, אנחנו מציעים מודל NER חוצה ששותמש בשום משאבים חיצוניים. We first introduce a Multi-Task Learning (MTL) by adding a new objective function to detect whether tokens are named entities or not.  ואז אנחנו מציגים מסגרת שנקראת תערובת של מומחים של יחידות (MoEE) כדי לשפר את החזקה של התאמה לתחום אפס משאבים. Finally, experimental results show that our model outperforms strong unsupervised cross-domain sequence labeling models, and the performance of our model is close to that of the state-of-the-art model which leverages extensive resources.</abstract_he>
      </paper>
    <paper id="2">
      <title>Encodings of Source Syntax : Similarities in NMT Representations Across Target Languages<fixed-case>NMT</fixed-case> Representations Across Target Languages</title>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <author><first>Anna</first><last>Rafferty</last></author>
      <pages>7–16</pages>
      <abstract>We train neural machine translation (NMT) models from <a href="https://en.wikipedia.org/wiki/English_language">English</a> to six target languages, using NMT encoder representations to predict ancestor constituent labels of source language words. We find that NMT encoders learn similar source syntax regardless of NMT target language, relying on explicit morphosyntactic cues to extract syntactic features from source sentences. Furthermore, the NMT encoders outperform RNNs trained directly on several of the constituent label prediction tasks, suggesting that NMT encoder representations can be used effectively for natural language tasks involving <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a>. However, both the NMT encoders and the directly-trained RNNs learn substantially different syntactic information from a probabilistic context-free grammar (PCFG) parser. Despite lower overall accuracy scores, the PCFG often performs well on sentences for which the RNN-based models perform poorly, suggesting that RNN architectures are constrained in the types of syntax they can learn.</abstract>
      <url hash="09498e6e">2020.repl4nlp-1.2</url>
      <doi>10.18653/v1/2020.repl4nlp-1.2</doi>
      <video href="http://slideslive.com/38929768" />
      <bibkey>chang-rafferty-2020-encodings</bibkey>
    <title_ar>ترميزات بناء جملة المصدر: أوجه التشابه في تمثيلات NMT عبر اللغات المستهدفة</title_ar>
      <title_pt>Codificações da sintaxe de origem: semelhanças nas representações NMT em todos os idiomas de destino</title_pt>
      <title_fr>Encodages de la syntaxe source : similitudes entre les représentations NMT dans les langues cibles</title_fr>
      <title_es>Codificaciones de la sintaxis de origen: similitudes en las representaciones de NMT en los idiomas de destino</title_es>
      <title_ja>ソース構文のエンコーディング:ターゲット言語間のNMT表現の類似性</title_ja>
      <title_ru>Кодирование исходного синтаксиса: сходство в представлениях НБ на разных языках перевода</title_ru>
      <title_zh>源语法编码:跨言 NMT 相似性</title_zh>
      <title_hi>स्रोत सिंटैक्स के एन्कोडिंग: लक्ष्य भाषाओं में NMT प्रतिनिधित्व में समानताएं</title_hi>
      <title_ukr>Кодування синтаксису джерела: подібності в представленнях НБ на всіх мовах призначення</title_ukr>
      <title_ga>Ionchóduithe Comhréire Foinse: Cosúlachtaí i Léirithe NMT ar fud na Sprioctheangacha</title_ga>
      <title_ka>მხოლოდ სინტაქსის კოდირება: NMT- ს გამოსახულებების გარეშე მიმდინარე ენების გარეშე</title_ka>
      <title_isl>Kóðun á upprunalegum sameiningum: Líkindi í NMT-myndum um marktungur</title_isl>
      <title_el>Κωδικοποιήσεις Συντάξης πηγής: Ομοιότητες στις Αντιπροσωπείες σε όλες τις γλώσσες στόχου</title_el>
      <title_hu>Forrásszintaxis kódolása: hasonlóságok az NMT reprezentációkban a célnyelveken</title_hu>
      <title_it>Codifica della sintassi di origine: somiglianze nelle rappresentazioni NMT in tutte le lingue di destinazione</title_it>
      <title_kk>Көздегі синтаксисінің кодтамасы: NMT келтірілген кезде мақсатты тілдерінде ұқсас сәйкестіктер</title_kk>
      <title_lt>Šaltinio sintakso kodai: panašumas NMT atstovybėse visomis tikslinėmis kalbomis</title_lt>
      <title_mk>Кодирање на синтаксот на изворот: Сличности во претставувањата на NMT преку јазиците на целта</title_mk>
      <title_ms>Pengekodan Sintaks Sumber: Persamaan dalam Perwakilan NMT Melalui Bahasa Sasaran</title_ms>
      <title_ml>സ്രോതസ്സ് സിന്‍ടാക്സിന്‍റെ കോഡിങ്ങുകള്‍: NMT പ്രതിനിധികളില്‍ സമമാണു് ലക്ഷ്യമുള്ള ഭാഷകള്‍</title_ml>
      <title_mt>Kodiċijiet tas-Sintassa tas-Sors: Similaritajiet fir-Rappreżentazzjonijiet tal-NMT lil hinn mil-lingwi fil-mira</title_mt>
      <title_mn>Хэрэгцээ синтаксисийн кодлог: NMT төлөвлөгөөний үзүүлэлтийн хоорондоо төстэй хэл</title_mn>
      <title_pl>Kodowanie składni źródłowych: podobieństwa w reprezentacjach NMT w językach docelowych</title_pl>
      <title_no>Koding av kjeldesyntaks: Similar i NMT- representasjonar gjennom målspråk</title_no>
      <title_ro>Codificări ale sintaxei sursei: similarități în reprezentările NMT în limbile țintă</title_ro>
      <title_sr>Kodiranje izvornog sintaksa: Similacije u predstavljanju NMT preko ciljnih jezika</title_sr>
      <title_si>මූල සංකේතනයේ සංකේතනය: NMT ප්‍රතිස්ථාපනය ඇතුළට ලක්ෂණ භාෂාවට සිමිලාරිත්වය</title_si>
      <title_sv>Kodningar av källsyntax: Likheter i NMT-representationer över målspråk</title_sv>
      <title_so>Kaararka kaararka cashuurta Source: Similarities in NMT representations Across Target Languages</title_so>
      <title_ta>மூல ஒத்திசைவின் குறியீடு</title_ta>
      <title_ur>سورٹ سینٹکس کا اکنوڈینگ: NMT روشنی ٹرکیٹ زبانوں میں سیمیلاریٹ</title_ur>
      <title_uz>Manba syntax kodlash usuli:</title_uz>
      <title_vi>Bảng mã gốc cú pháp: Similaties in NMT Representations ngang ngửa Thư mục tiêu</title_vi>
      <title_bg>Кодиране на синтаксис на източника: сходства в представянето на НМТ през целевите езици</title_bg>
      <title_nl>Coderingen van bronsyntaxis: gelijkenissen in NMT-representaties in doeltalen</title_nl>
      <title_hr>Kodiranje izvornog sintaksa: sličnosti u predstavljanju NMT preko ciljnih jezika</title_hr>
      <title_da>Kodninger af kildesyntaks: Ligheder i NMT-repræsentationer på tværs af målsprog</title_da>
      <title_de>Kodierungen der Quellsyntax: Ähnlichkeiten in NMT-Darstellungen über Zielsprachen hinweg</title_de>
      <title_ko>소스 구문 인코딩: 대상 언어에서 NMT가 나타내는 유사성</title_ko>
      <title_fa>Encodings of Source Syntax: Similarities in NMT Representations Through Target Languages</title_fa>
      <title_id>Kodisan Sintaks Sumber: Similaritas dalam Representasi NMT Melalui Bahasa Sasaran</title_id>
      <title_tr>Menb Sentaksy Ködlemeleri: NMT Görkezilmeleri Taryh Dilleri</title_tr>
      <title_sw>Kodi za Syntax wa Chanzo: Vifanana katika Representations of NMT Across Lugha za Target</title_sw>
      <title_af>Enkodering van Bron Sintaks: Similarisies in NMT voorstellings tussen Teël Taal</title_af>
      <title_sq>Kodifikimi i sintaksës së burimit: ngjashmëri në përfaqësimet e NMT nëpërmjet gjuhëve objektive</title_sq>
      <title_am>ምስሉን በሌላ ስም አስቀምጥ</title_am>
      <title_hy>Սինտաքսի աղբյուրի կոդավորումները. Նմանությունը NMT-ի ներկայացումներում նպատակային լեզուներում</title_hy>
      <title_az>Qaynaq Sintaksi KodlamasńĪ: NMT ńįzl…ôri M…ôqs…ôd Dill…ôrinin ńįzl…ôri</title_az>
      <title_bn>সূত্র সিন্যাক্সের এনকোডিং: এনএমটি প্রতিনিধির সমতা</title_bn>
      <title_bs>Kodiranje izvornog sintaksa: sličnosti u predstavljanju NMT preko ciljnih jezika</title_bs>
      <title_ca>Codificacions de sintaxi de fonts: Similaritats en representacions de MTN a través de llengües d'objectiu</title_ca>
      <title_cs>Kódování zdrojové syntaxe: Podobnosti v NMT reprezentacích napříč cílovými jazyky</title_cs>
      <title_et>Allika süntaksi kodeeringud: sarnasused NMT esindustes sihtkeeltes</title_et>
      <title_fi>Lähdesyntaksin koodaukset: NMT-edustustojen samankaltaisuudet kohdekielillä</title_fi>
      <title_jv>Kodulasi Sistem Sentas: Similrases nang NMT Gambaran Akra TarGet Languages</title_jv>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_sk>Kodiranje sintakse vira: podobnosti v predstavitvah NMT v ciljnih jezikih</title_sk>
      <title_he>קודים של סינטקס מקור: דמיונים ביציגות NMT ברחבי שפות המטרה</title_he>
      <title_fil>Encoding ng Source Syntax: Similarities sa NMT Representations Across Target Languages</title_fil>
      <title_bo>འབྱུང་ཁུངས་ཀྱི་སྒྲིག་སྟངས་རྟགས：NMT འཆར་སྟོན་པའི་ནང་མཚོན་རྟགས་ཀྱི་དམིགས་ཡུལ་གྱི་སྐད་རིགས</title_bo>
      <abstract_ar>نقوم بتدريب نماذج الترجمة الآلية العصبية (NMT) من الإنجليزية إلى ست لغات مستهدفة ، وذلك باستخدام تمثيلات مشفر NMT للتنبؤ بالتسميات المكونة للأسلاف لكلمات لغة المصدر. وجدنا أن مشفرات NMT تتعلم بناء جملة مصدر مماثل بغض النظر عن اللغة الهدف NMT ، بالاعتماد على إشارات صريحة صريحة لاستخراج الميزات النحوية من الجمل المصدر. علاوة على ذلك ، تتفوق مشفرات NMT على RNNs المدربة مباشرة على العديد من مهام التنبؤ بالتسمية المكونة ، مما يشير إلى أنه يمكن استخدام تمثيلات مشفر NMT بشكل فعال لمهام اللغة الطبيعية التي تتضمن بناء الجملة. ومع ذلك ، فإن كلاً من مشفرات NMT و RNNs المدربة تدريباً مباشراً تتعلم معلومات نحوية مختلفة إلى حد كبير من محلل نحوي احتمالي خالٍ من السياق (PCFG). على الرغم من انخفاض درجات الدقة الإجمالية ، غالبًا ما يكون أداء PCFG جيدًا في الجمل التي تعمل بها النماذج المستندة إلى RNN بشكل ضعيف ، مما يشير إلى أن بنيات RNN مقيدة في أنواع بناء الجملة التي يمكن أن تتعلمها.</abstract_ar>
      <abstract_pt>Treinamos modelos de tradução automática neural (NMT) de inglês para seis idiomas de destino, usando representações de codificador NMT para prever rótulos constituintes ancestrais de palavras do idioma de origem. Descobrimos que os codificadores NMT aprendem sintaxe de origem semelhante, independentemente do idioma de destino da NMT, contando com pistas morfossintáticas explícitas para extrair recursos sintáticos das frases de origem. Além disso, os codificadores NMT superam os RNNs treinados diretamente em várias das tarefas de previsão de rótulos constituintes, sugerindo que as representações dos codificadores NMT podem ser usadas efetivamente para tarefas de linguagem natural envolvendo sintaxe. No entanto, tanto os codificadores NMT quanto os RNNs treinados diretamente aprendem informações sintáticas substancialmente diferentes de um analisador de gramática livre de contexto probabilístico (PCFG). Apesar das pontuações de precisão geral mais baixas, o PCFG geralmente funciona bem em frases para as quais os modelos baseados em RNN têm um desempenho ruim, sugerindo que as arquiteturas RNN são restritas nos tipos de sintaxe que podem aprender.</abstract_pt>
      <abstract_es>Entrenamos modelos de traducción automática neuronal (NMT) del inglés a seis idiomas de destino, utilizando representaciones de codificadores NMT para predecir las etiquetas constituyentes de los antepasados de las palabras del idioma de origen. Encontramos que los codificadores NMT aprenden una sintaxis de origen similar independientemente del idioma de destino de NMT, confiando en señales morfosintácticas explícitas para extraer características sintácticas de las oraciones fuente. Además, los codificadores de NMT superan a los RNN entrenados directamente en varias de las tareas de predicción de etiquetas constituyentes, lo que sugiere que las representaciones del codificador de NMT pueden usarse eficazmente para tareas de lenguaje natural que implican sintaxis. Sin embargo, tanto los codificadores de NMT como los RNN directamente entrenados aprenden información sintáctica sustancialmente diferente de un analizador probabilístico de gramática libre de contexto (PCFG). A pesar de las puntuaciones de precisión generales más bajas, el PCFG a menudo funciona bien en oraciones para las que los modelos basados en RNN tienen un rendimiento deficiente, lo que sugiere que las arquitecturas RNN están limitadas en los tipos de sintaxis que pueden aprender.</abstract_es>
      <abstract_fr>Nous entraînons des modèles de traduction automatique neuronale (NMT) de l'anglais vers six langues cibles, en utilisant des représentations de codeurs NMT pour prédire les étiquettes constitutives des ancêtres des mots de la langue source. Nous avons constaté que les encodeurs NMT apprennent une syntaxe source similaire quel que soit le langage cible NMT, en s'appuyant sur des indices morphosyntaxiques explicites pour extraire les caractéristiques syntaxiques des phrases sources. En outre, les codeurs NMT surpassent les RNN entraînés directement sur plusieurs tâches de prédiction d'étiquettes constituantes, ce qui suggère que les représentations de codeurs NMT peuvent être utilisées efficacement pour des tâches de langage naturel impliquant la syntaxe. Cependant, les codeurs NMT et les RNN entraînés directement apprennent des informations syntaxiques sensiblement différentes à partir d'un analyseur de grammaire sans contexte probabiliste (PCFG). Malgré des scores de précision globaux plus faibles, le PCFG fonctionne souvent bien sur les phrases pour lesquelles les modèles basés sur RNN sont peu performants, ce qui suggère que les architectures RNN sont limitées dans les types de syntaxe qu'elles peuvent apprendre.</abstract_fr>
      <abstract_ja>私たちは、ソース言語単語の祖先構成ラベルを予測するためにNMTエンコーダ表現を使用して、英語から6つのターゲット言語へのニューラルマシン翻訳（ NMT ）モデルをトレーニングします。NMTエンコーダは、NMTターゲット言語に関係なく同様のソース構文を学習し、明示的なモルホシンタクティックキューに依存して、ソース文から構文機能を抽出することがわかります。さらに、ＮＭＴエンコーダは、いくつかの構成ラベル予測タスクで直接訓練されたＲＮＮを上回り、ＮＭＴエンコーダ表現が構文を伴う自然言語タスクに効果的に使用され得ることを示唆する。しかしながら、ＮＭＴエンコーダ及び直接訓練されたＲＮＮの両方は、確率的文脈自由文法（ ＰＣＦＧ ）構文解析器から実質的に異なる構文情報を学習する。全体的な精度スコアが低いにもかかわらず、ＰＣＦＧは、ＲＮＮベースのモデルが不十分な文に対して良好なパフォーマンスを発揮することが多く、ＲＮＮアーキテクチャが学習可能な構文の種類に制約されていることを示唆する。</abstract_ja>
      <abstract_zh>吾以 NMT 编码器占言单词之先为标,将神经机器翻译 (NMT) 从英语训练至六者言之。 吾见 NMT 言何如,NMT 编码器皆能学类源语法,依显式形句法取句法于源句。 又NMT编码器性优于数RNNs,明NMT编码器所以及语法自然语言也。 然NMT 编码器与直训 RNN 自概率上下文无关语法 (PCFG) 解析器学者语法信息大异。 准确度数虽下,PCFG善於RNN,明RNN架构於可学之语法也。</abstract_zh>
      <abstract_hi>हम स्रोत भाषा शब्दों के पूर्वज घटक लेबल की भविष्यवाणी करने के लिए NMT एन्कोडर अभ्यावेदन का उपयोग करके अंग्रेजी से छह लक्ष्य भाषाओं तक तंत्रिका मशीन अनुवाद (NMT) मॉडल को प्रशिक्षित करते हैं। हम पाते हैं कि एनएमटी एनकोडर एनएमटी लक्ष्य भाषा की परवाह किए बिना समान स्रोत वाक्यविन्यास सीखते हैं, स्रोत वाक्यों से वाक्यात्मक विशेषताओं को निकालने के लिए स्पष्ट मोर्फोसिंटैक्टिक संकेतों पर भरोसा करते हैं। इसके अलावा, NMT encoders घटक लेबल भविष्यवाणी कार्यों में से कई पर सीधे प्रशिक्षित RNNs outperform, सुझाव है कि NMT एन्कोडर प्रतिनिधित्व वाक्यविन्यास शामिल प्राकृतिक भाषा कार्यों के लिए प्रभावी ढंग से इस्तेमाल किया जा सकता है। हालांकि, NMT encoders और सीधे प्रशिक्षित RNNs दोनों एक संभाव्य संदर्भ-मुक्त व्याकरण (PCFG) पार्सर से काफी अलग वाक्यात्मक जानकारी सीखते हैं। कम समग्र सटीकता स्कोर के बावजूद, PCFG अक्सर उन वाक्यों पर अच्छा प्रदर्शन करता है जिनके लिए RNN-आधारित मॉडल खराब प्रदर्शन करते हैं, यह सुझाव देते हुए कि RNN आर्किटेक्चर उन वाक्यविन्यास के प्रकारों में विवश हैं जो वे सीख सकते हैं।</abstract_hi>
      <abstract_ru>Мы обучаем модели нейронного машинного перевода (НМП) с английского на шесть целевых языков, используя представления кодера НМП для предсказания предков составляющих меток слов исходного языка. Мы обнаружили, что кодировщики NMT изучают похожий синтаксис исходного текста независимо от языка назначения NMT, полагаясь на явные морфосинтаксические сигналы для извлечения синтаксических признаков из исходных предложений. Кроме того, кодеры NMT превосходят RNN, обученные непосредственно по нескольким задачам предсказания составляющих меток, предполагая, что представления кодера NMT могут быть эффективно использованы для задач на естественном языке, включающих синтаксис. Тем не менее, как кодеры NMT, так и непосредственно обученные RNN извлекают существенно отличающуюся синтаксическую информацию из вероятностного контекстно-свободного грамматического (PCFG) анализатора. Несмотря на более низкие общие оценки точности, PCFG часто хорошо выполняет предложения, для которых модели на основе RNN работают плохо, предполагая, что архитектуры RNN ограничены в типах синтаксиса, который они могут выучить.</abstract_ru>
      <abstract_ukr>Ми тренуємо моделі нейронного машинного перекладу (НМП) з англійської мови на шість цільових мов, використовуючи представлення кодувальників НМП для прогнозування складових міток слів вихідної мови. Ми виявили, що кодери NMT вивчають подібний синтаксис джерела незалежно від цільової мови NMT, спираючись на явні морфосинтаксичні сигнали для видобування синтаксичних ознак з речень джерела. Крім того, кодери NMT перевершують RNN, навчені безпосередньо за декількома завданнями передбачення складових міток, що свідчить про те, що представлення кодера NMT можуть бути ефективно використані для завдань природної мови, що включають синтаксис. Однак, як кодувальники NMT, так і безпосередньо навчені RNN вивчають суттєво різну синтаксичну інформацію з ймовірнісного контекстно-вільного граматичного аналізатора (PCFG). Незважаючи на нижчі загальні оцінки точності, PCFG часто добре виконує речення, для яких моделі на основі RNN працюють погано, що свідчить про те, що архітектури RNN обмежені в типах синтаксису, який вони можуть вивчити.</abstract_ukr>
      <abstract_ga>Déanaimid oiliúint ar mhúnlaí néar-aistriúcháin meaisín (NMT) ón mBéarla go sé sprioctheanga, ag baint úsáide as uiríll ionchódóra NMT chun lipéid comhchodacha na bhfocal sa teanga foinse a thuar. Faighimid amach go bhfoghlaimíonn ionchódóirí NMT comhréir foinse comhchosúil beag beann ar sprioctheanga an NMT, ag brath ar leideanna soiléire morphosyntactic chun gnéithe comhréire a bhaint as abairtí foinse. Ina theannta sin, sáraíonn ionchódóirí an NMT na RNNanna atá oilte go díreach ar roinnt de na tascanna tuar lipéad comhábhar, rud a thugann le tuiscint gur féidir uiríll ionchódóra NMT a úsáid go héifeachtach le haghaidh tascanna teanga nádúrtha a bhaineann le comhréir. Mar sin féin, foghlaimíonn na hionchódóirí NMT agus na RNNanna a bhfuil oiliúint dhíreach orthu faisnéis chomhréire atá difriúil go substaintiúil ó pharsálaí dóchúlachta gramadaí saor ó chomhthéacs (PCFG). In ainneoin scóir chruinneas iomlán níos ísle, is minic a fheidhmíonn an PCFG go maith ar abairtí nach bhfeidhmíonn na samhlacha RNN-bhunaithe go dona ina leith, rud a thugann le tuiscint go bhfuil srian ar ailtireachtaí RNN leis na cineálacha comhréire ar féidir leo a fhoghlaim.</abstract_ga>
      <abstract_isl>Við þjálfum taugaþýðingu (neural machine translation - NMT) líkana frá ensku til sex marktungumál, með því að nota NMT kóðurstöður til að spá fyrir upprunalegum samsetningum á upprunalegum tungumál. Við finnum að NMT kóðunarmenn læra svipaða upprunalegu samsetningu óháð NMT marktungumáli, sem byggist á útskýrðum morphosyntactic vísbendingum til að draga úr upprunalegum setningum samsetningu. Auk þess geta NMT kóðarnir framkvæmt RNN sem þjálfaðir eru beint á nokkrum af forspárverkunum sem innihalda merkimiðann, sem bendir til þess að NMT kóðarnir geti verið notaðir á árangursríkan hátt fyrir náttúrulegar tungumál verkefni sem tengjast samsetningu. Hins vegar læra bæði NMT-kóðarnir og beint þjálfaðir RNN-kóðarnir verulega mismunandi samræmdar upplýsingar frá líklegri tengslulausri grammar (PCFG) greiningu. Þrátt fyrir lægri heildarnákvæmni framkvæmir PCFG oft vel setningar þar sem RNN-byggðar líkanir eru illa framkvæmdar, sem bendir til þess að RNN-arkitektur sé takmarkaðar í tegundum samræmingar sem þeir geta lært.</abstract_isl>
      <abstract_el>Εκπαιδεύουμε μοντέλα νευρωνικής μηχανικής μετάφρασης (NMT) από τα αγγλικά σε έξι γλώσσες-στόχους, χρησιμοποιώντας αναπαραστάσεις κωδικοποιητών για την πρόβλεψη των συστατικών ετικετών προγόνων λέξεων της γλώσσας προέλευσης. Διαπιστώνουμε ότι οι κωδικοποιητές μαθαίνουν παρόμοια σύνταξη πηγής ανεξάρτητα από τη γλώσσα προορισμού βασίζονται σε ρητές μορφοσυντακτικές ενδείξεις για την εξαγωγή συντακτικών χαρακτηριστικών από προτάσεις πηγής. Επιπλέον, οι κωδικοποιητές ξεπερνούν τα RNN που εκπαιδεύονται άμεσα σε αρκετές από τις εργασίες πρόβλεψης ετικετών, υποδηλώνοντας ότι οι αναπαραστάσεις κωδικοποιητών NMT μπορούν να χρησιμοποιηθούν αποτελεσματικά για εργασίες φυσικής γλώσσας που περιλαμβάνουν σύνταξη. Ωστόσο, τόσο οι κωδικοποιητές NMT όσο και οι άμεσα εκπαιδευμένοι μαθαίνουν σημαντικά διαφορετικές συντακτικές πληροφορίες από έναν πιθανό αναλυτή γραμματικής χωρίς πλαίσιο. Παρά τις χαμηλότερες συνολικές βαθμολογίες ακρίβειας, το PCFG συχνά αποδίδει καλά σε προτάσεις για τις οποίες τα μοντέλα βασισμένα σε RNN αποδίδουν κακή απόδοση, υποδηλώνοντας ότι οι αρχιτεκτονικές RNN περιορίζονται στους τύπους σύνταξης που μπορούν να μάθουν.</abstract_el>
      <abstract_hu>Neurális gépi fordítási (NMT) modelleket képzünk angolról hat célnyelvre, NMT kódoló reprezentációkkal a forrásnyelv szavak őseinek megjelenítésére. Úgy találjuk, hogy az NMT kódolók hasonló forrásszintaxist tanulnak az NMT célnyelvétől függetlenül, explicit morfoszintatikus utakra támaszkodva, hogy kivonják a szintaktikus jellemzőket a forrásszövegekből. Ezen túlmenően az NMT kódolók több alkotó címke-előrejelzési feladatra közvetlenül kiképzett RNN-eket is felülmúlják, ami azt sugallja, hogy az NMT kódolóreprezentációk hatékonyan használhatók a szintaxissal járó természetes nyelvi feladatokhoz. Azonban mind az NMT kódolók, mind a közvetlenül képzett RNN-ek lényegesen eltérő szintaktikus információkat tanulnak egy valószínűleg kontextusmentes nyelvtani (PCFG) elemzőből. Az alacsonyabb általános pontossági pontszámok ellenére a PCFG gyakran jól teljesít olyan mondatokon, amelyeknél az RNN-alapú modellek rosszul teljesítenek, ami arra utal, hogy az RNN architektúrák korlátozzák a tanulható szintaxistípusokat.</abstract_hu>
      <abstract_ka>ჩვენ ნეიროლური მანქანის გადაწყვეტილება (NMT) მოდელები ანგლისგან ქვსი მიზეზი ენაზე, NMT კოდერის გამოყენება გამოყენებული გამოყენებული გამოყენება, რომელიც წინაწყვეტის მ NMT კოდერები იგივეა სინტაქსის შესწავლობენ NMT მისაღების ენაზე, რომელიც მხოლოდ მოპროსინტაქტიკური სინტაქსის გამოყენებას დავუწავლობენ. დამატებით, NMT კოდერები მუშაობს RNN-ს, რომლებიც კონსტუტენტის წარმოდგენების რამდენიმე დასაწყვებაზე განაკეთებულია, რომ NMT კოდერების გამოსახულებები შეიძლება გამოყენება ეფექტიურად სინტ მაგრამ, ორივე NMT კოდერები და ექსტურად განაკეთებული RNN-ები გავისწავლენ ძალიან განსხვავებული სინტაქტიული ინფორმაცია პრობალისტიკური კონტექსტური განაკეთებული გრამიმარი (PC პროგრამის უფრო ნაკლები წარმოდგენებისთვის, პროგრამის პროგრამის მუშაობაში მუშაობა, რომლებიც პროგრამის მუშაობაში მოდელები ცოტა გამოყენებენ, რომლებიც პროგრამის არქტიქტურები შეუძლი</abstract_ka>
      <abstract_it>Formiamo modelli di traduzione automatica neurale (NMT) dall'inglese a sei lingue di destinazione, utilizzando rappresentazioni di encoder NMT per predire le etichette costituenti degli antenati delle parole della lingua di origine. Scopriamo che gli encoder NMT imparano sintassi sorgente simile indipendentemente dalla lingua di destinazione NMT, basandosi su indizi morfosintattici espliciti per estrarre caratteristiche sintattiche dalle frasi sorgente. Inoltre, gli encoder NMT superano gli RNN addestrati direttamente su molte delle attività di previsione delle etichette costituenti, suggerendo che le rappresentazioni degli encoder NMT possono essere utilizzate efficacemente per attività di linguaggio naturale che coinvolgono la sintassi. Tuttavia, sia gli encoder NMT che gli RNN direttamente addestrati imparano informazioni sintattiche sostanzialmente diverse da un parser probabilistic context-free grammatical (PCFG). Nonostante i punteggi di accuratezza complessiva inferiori, il PCFG spesso si comporta bene su frasi per le quali i modelli basati su RNN funzionano male, suggerendo che le architetture RNN sono limitate nei tipi di sintassi che possono imparare.</abstract_it>
      <abstract_kk>Біз ағылшын тілден алты мақсатты тілдерге невралдық машинаның аударуын (NMT) үлгілерін оқыдық. NMT кодтарының түсініктемелерін көзі тілдерінің түсініктемелерін қолдану үшін аты Біз NMT кодерлері NMT мақсат тіліне қарамастан ұқсас көзінің синтаксисін оқу үшін синтактикалық құқықтарды көзінен тарқатуға арналған морфосинтактикалық белгілеріне көмектеседі. Қосымша, NMT кодері бірнеше жарлық тапсырмалардың бірнеше жарлық тапсырмаларында тәуелді RNN кодерлерін жұмыс істейді. NMT кодерлерін синтаксисі арқылы тәуелді тіл тапсырмаларында қолданылатын тапсырмалар үшін Бірақ NMT кодерлері және тікелей оқылған RNN екеуі мүмкіндік контексті бос грамматикалық (PCFG) талдағы синтактикалық мәліметтерден бірнеше түрлі мәліметті үйренеді. Жалпы дұрыс нәтижелерін төмендету үшін, PCFG көбінесе, RNN негізінде үлгілер дұрыс істейтін сөздерді жақсы орындайды. RNN архитектураларын олардың үйрене алатын синтаксис түрлерінде шектеуге болады.</abstract_kk>
      <abstract_lt>Mes mokome nervinių mašinų vertimo (NMT) modelius iš anglų į šešias tikslines kalbas, naudojant NMT kodatoriaus atvaizdus, kad būtų galima prognozuoti pradinių sudedamųjų dalių etiketes šaltinio kalbos žodžiais. Nustatome, kad NMT koduotojai mokosi panašios šaltinio sintaksijos, nepriklausomai nuo NMT tikslinės kalbos, remdamiesi aiškiais morfosintaksiniais požymiais, kad ištrauktų sintaksinius požymius iš šaltinio sakinių. Furthermore, the NMT encoders outperform RNNs trained directly on several of the constituent label prediction tasks, suggesting that NMT encoder representations can be used effectively for natural language tasks involving syntax.  Tačiau tiek NMT kodatoriai, tiek tiesiogiai apmokyti RNN iš esmės mokosi skirtingą sintaktinę informaciją nei tikėtinas be konteksto gramatikos analizatorius (PCFG). Nepaisant mažesnių bendro tikslumo rezultatų, PCFG dažnai gerai veikia sakiniais, kurių RNN modeliai blogai veikia, ir tai rodo, kad RNN architektūros yra apribotos jų mokomų sintaksų tipais.</abstract_lt>
      <abstract_ms>Kami melatih model terjemahan mesin saraf (NMT) dari bahasa Inggeris ke enam bahasa sasaran, menggunakan perwakilan pengekod NMT untuk meramalkan label konstityen nenek moyang perkataan bahasa sumber. Kami mendapati bahawa pengekod NMT belajar sintaks sumber yang sama tidak kira-kira bahasa sasaran NMT, bergantung pada isyarat morfosintaksi eksplicit untuk mengekstrak ciri-ciri sintaks dari kalimat sumber. Furthermore, the NMT encoders outperform RNNs trained directly on several of the constituent label prediction tasks, suggesting that NMT encoder representations can be used effectively for natural language tasks involving syntax.  However, both the NMT encoders and the directly-trained RNNs learn substantially different syntactic information from a probabilistic context-free grammar (PCFG) parser.  Walaupun skor keseluruhan ketepatan lebih rendah, PCFG sering berkesan dengan baik pada kalimat yang mana model berdasarkan RNN berkesan buruk, menyarankan bahawa arkitektur RNN dikuasai dalam jenis sintaks yang mereka boleh belajar.</abstract_ms>
      <abstract_ml>നമ്മള്‍ ഇംഗ്ലീഷില്‍ നിന്നും ആറു ലക്ഷ്യ ഭാഷകളിലേക്കും ന്യൂറല്‍ മെഷീന്‍ പരിഭാഷകളെ പരിശീലിപ്പിക്കുന്നു. നിങ്ങളുടെ പൂര്‍വ്വപിതാക്കന്‍ NMT എന്‍റെ ലക്ഷ്യ ഭാഷയില്‍ നിന്നും പോലുള്ള സോര്‍സ് സിന്‍ട്രാക്സ് പഠിക്കുന്നത് പോലെയാണെന്ന് നമുക്ക് കണ്ടെത്തുന്നു. വ്യക്തമായ മൊര്‍ഫോ അതിനുശേഷം, NMT കോഡോര്‍ഡുകള്‍ RNNs നേരിട്ട് പരിശീലനം പ്രവര്‍ത്തിപ്പിച്ചു കൊണ്ടിരിക്കുന്ന പല പ്രവചനങ്ങളില്‍ നേരിട്ട് പ്രവര്‍ത്തിപ്പിക്കുന്നു. സി എന്നാലും, NMT കോഡോര്‍ഡുകളും നേരിട്ട് പരിശീലനം നല്‍കപ്പെട്ട RNNകളും ഒരു സാധ്യതയുള്ള കോണ്‍ട്ടിക്സ് ഫ്രീഡ് ഗ്രാമാരില്‍ നിന്നും വ്യത്യസ്തമാ പിസിഎഫിജിയില്‍ നിന്നും കുറച്ച് കൂടുതല്‍ തെളിവുള്ള സ്കോര്‍ട്ടുകള്‍ താഴേക്കാണെങ്കിലും, RNN-അടിസ്ഥാനമായ മോഡലുകള്‍ പ്രവര്‍ത്തിക്കുന്ന വാക്കുകളില്‍ പ</abstract_ml>
      <abstract_mt>We train neural machine translation (NMT) models from English to six target languages, using NMT encoder representations to predict ancestor constituent labels of source language words.  Aħna nsibu li l-kodifikaturi NMT jitgħallmu sintaks tas-sors simili irrispettivament mil-lingwa fil-mira NMT, billi jiddependu fuq sinjali morfosintattiċi espliċiti biex jiġu estratti karatteristiċi sintattiċi mis-sentenzi tas-sors. Barra minn hekk, il-kodifikaturi NMT jaqbżu l-RNNs imħarrġa direttament fuq diversi kompiti ta’ tbassir tat-tikketta kostitwenti, li jissuġġerixxu li r-rappreżentazzjonijiet tal-kodifikaturi NMT jistgħu jintużaw b’mod effettiv għal kompiti lingwistiċi naturali li jinvolvu s-sintaks. Madankollu, kemm il-kodifikaturi NMT kif ukoll l-RNNs imħarrġa direttament jitgħallmu informazzjoni sintetika sostanzjalment differenti minn analizzatur grammatiku probabilistiku ħieles mill-kuntest (PCFG). Minkejja punteġġi ta’ preċiżjoni globali aktar baxxi, il-PCFG spiss iwettaq prestazzjoni tajba fuq sentenzi li għalihom il-mudelli bbażati fuq RNN iwettqu prestazzjoni ħażina, u dan jissuġġerixxi li l-arkitetturi tal-RNN huma ristretti fit-tipi ta’ sintaks li jistgħu jitgħallmu.</abstract_mt>
      <abstract_mk>Ги обучуваме моделите на невропски машински превод (НМТ) од англиски на шест метни јазици, користејќи ги претставувањата на кодерот на НМТ за да ги предвидеме ознаките на предците на зборовите на изворниот јазик. Најдовме дека кодерите на НМТ учат слична синтаксика од изворот без оглед на јазикот на метата на НМТ, потпирајќи се на експлицитни морфосинтактички знаци за извлекување синтактички карактеристики од изворот на речениците. Покрај тоа, кодерите на НМТ ги надминуваат резултатите на РНН обучени директно на неколку од задачите за предвидување на компонентната етикета, што сугерира дека резултатите на кодерите на НМТ може да се користат ефикасно за природните јазични задачи кои вклучуваат синтак However, both the NMT encoders and the directly-trained RNNs learn substantially different syntactic information from a probabilistic context-free grammar (PCFG) parser.  И покрај пониските целокупни резултати на точноста, ПКФГ честопати функционира добро на речениците за кои моделите базирани на РНН функционираат лошо, што сугерира дека архитектурите на РНН се ограничени во видовите на синтаксија кои можат да ги научат</abstract_mk>
      <abstract_no>Vi treng neuralmaskinsomsetjingsmodular frå engelsk til seks målspråk med NMT-koderingsrepresentasjonar for å forhåndsvisa foreldra- konstituent- merkelappar med kjeldespråk. Vi finn at NMT-kodarar lærer liknande kjeldesyntaks uavhengig av NMT-målspråk, ved hjelp av eksplisitt morphosyntaktiske teikn for å pakka ut syntaktiske funksjonar frå kjeldesetningar. I tillegg kan NMT-koderinga utføra RNN-ar som vert trengte direkte på fleire av forensingsoppgåva for konstitusjonsmerkelappen, som tyder på at representasjonane av NMT-koderingskoderingsprogram kan brukast effektivt for naturspråksoppgåver med syntaks. Men både NMT-kodingane og dei direkte trengte RNN lærer mykje ulike syntaktiske informasjon frå eit sannsynlig kontekstfri grammar (PCFG). Til tross nedre overalt nøyaktighetspoeng, vil PCFG ofte utføra godt på setningar som RNN-baserte modelane utfører slik, som tyder på at RNN-arkitekturar er begrenset i typane syntaks dei kan lære.</abstract_no>
      <abstract_pl>Trenujemy modele neuronowego tłumaczenia maszynowego (NMT) z angielskiego do sześciu języków docelowych, wykorzystując reprezentacje koderów NMT do przewidywania etykiet składowych przodków słów języka źródłowego. Odkrywamy, że kodery NMT uczą się podobnej składni źródłowej niezależnie od języka docelowego NMT, opierając się na wyraźnych wskazówkach morfoskładni, aby wyodrębnić cechy składni ze zdań źródłowych. Ponadto kodery NMT przewyższają RNN przeszkolone bezpośrednio w kilku zadaniach przewidywania etykiet składowych, co sugeruje, że reprezentacje koderów NMT mogą być skutecznie wykorzystywane do zadań języka naturalnego obejmujących składnię. Jednak zarówno kodery NMT, jak i bezpośrednio przeszkolone RNN uczą się znacznie różnych informacji składniowych z prawdopodobnie kontekstowego parsera gramatyki (PCFG). Pomimo niższych ogólnych wyników dokładności PCFG często sprawdza się dobrze w zdaniach, dla których modele oparte na RNN działają słabo, co sugeruje, że architektury RNN są ograniczone w typach składni, których mogą się uczyć.</abstract_pl>
      <abstract_ro>Instruim modele de traducere automată neurală (NMT) din limba engleză în șase limbi țintă, folosind reprezentări codificatoare NMT pentru a prezice etichetele constitutive ale strămoșilor cuvintelor din limba sursă. Descoperim că codificatorii NMT învață sintaxa sursă similară indiferent de limba țintă NMT, bazându-se pe indicii morfosintactice explicite pentru a extrage caracteristici sintactice din propozițiile sursă. În plus, codificatorii NMT depășesc RNN instruiți direct pe mai multe dintre sarcinile constitutive de predicție a etichetelor, sugerând că reprezentările codificatorilor NMT pot fi utilizate eficient pentru sarcini de limbaj natural care implică sintaxa. Cu toate acestea, atât codificatorii NMT, cât și RNN-urile instruite direct învață informații sintactice substanțial diferite dintr-un parser de gramatică probabilistic context-free (PCFG). În ciuda scorurilor de precizie generală mai scăzute, PCFG performează adesea bine pe propoziții pentru care modelele bazate pe RNN performează slab, sugerând că arhitecturile RNN sunt constrânse în tipurile de sintaxă pe care le pot învăța.</abstract_ro>
      <abstract_mn>Бид англи хэлээс зургаан зорилготой хэл руу мэдрэлийн машин хөгжүүлэх загваруудыг суралцаж, NMT кодчуудын илэрхийллийг ашиглаж, эх үүсвэрийн хэл үгийн өвгүүдийн байгууллагын жагсаалтыг таамаглах зо NMT кодерчууд NMT зорилготой хэл дээр ч ижил эх үүсвэртэй синтаксис суралцаж байдаг. Мөн үүсвэрийн өгүүлбэрээс синтактик боломжуудыг гаргахад тодорхой морфосинтактик тэмдэглэгддэг. Үүнээс гадна NMT коддогч нь хэлбэрийн даалгаврын тухай NMT коддогчийн хэл даалгаврын тулд шууд хөгжүүлсэн хэд хэдэн загваруудыг ашиглаж болно. Гэхдээ NMT кодер болон шууд сургалтын RNN хоёулаа магадлалын аграммаас (PCFG) хуваалцагч болох синтактикийн мэдээллээс маш өөр өөр өөр мэдээллийг суралцаж байна. Дэлхийн тодорхойлолтыг багасгасан ч, PCFG нь ихэвчлэн RNN-ын загвар нь зөвхөн хийдэг өгүүлбэр дээр сайн ажилладаг. Тэд сурах боломжтой синтаксикт RNN архитектурууд хязгаарлагддаг.</abstract_mn>
      <abstract_sr>Vježbamo modele prevoda neuralnih mašina (NMT) iz engleskog na šest ciljnih jezika, koristeći predstave kodera NMT-a kako bi predvidjeli znakove predaka sastavljajuće od jezika izvora. Nalazimo da koderi NMT nauče sličnu izvornu sintaksu bez obzira na ciljni jezik NMT-a, oslanjajući se na pojasne morfosintaktičke znakove kako bi izvukli sintaktične funkcije iz izvornih rečenica. Nadalje, NMT koderi nadmašuju RNN-ove koji su trenirani direktno na nekoliko zadataka predviđanja etiketa, sugerirajući da se predstavljanja kodera NMT-a mogu koristiti efikasno za prirodne jezičke zadatake uključujući sintaks. Međutim, i koderi NMT-a i direktno obučeni RNN-i nauče značajno različite sintaktičke informacije iz mogućnosti analizatora gramatike bez konteksta (PCFG). Uprkos nižim ukupnim preciznim rezultatima, PCFG često dobro izvodi na rečenicama za koje se modeli na RNN-u loše izvode, sugerirajući da su arhitekture RNN ograničene u vrstama sintakse koje mogu naučiti.</abstract_sr>
      <abstract_si>අපි ඉංග්‍රීසියෙන් ඉන්ග්‍රීසියෙන් ඉලක්ක භාෂාවට හතරක් ඉන්න ප්‍රමාණය කරනවා, NMT කෝඩාර් ප්‍රමාණය ප්‍රයෝජනය කරනවා ප්‍රම අපි හොයාගන්නවා NMT කෝඩාර්ස් සමාන ප්‍රභාව සංකේතය NMT ලක්ෂ භාෂාව නැතුව, ප්‍රශේෂ විශේෂ සංකේතය සඳහා සංකේතය සංකේත එතකොට, NMT සංකේතකයෙන් ප්‍රශ්නයක් නිර්මාණය කරලා තියෙන්නේ RNN සංකේතකයෙන් ප්‍රශ්නයක් නිර්මාණය කරලා තියෙන්නේ, NMT සංකේතකයෙන් ප්‍රශ නමුත්, NMT සංකේතකය සහ ප්‍රතිකාරණයෙන් පුළුවන් පරීක්ෂණය කරපු RNN දෙන්නම් වෙනස් සංකේතක තොරතුරු සංකේතක විදිහට ඉගෙන ගන සාමාන්‍ය සාමාන්‍ය විශේෂතාවක් අඩුවෙන් තියෙනවා නමුත්, PCFG සාමාන්‍ය විශේෂතාවක් හොඳ විශේෂය කරනවා RNN-අධාරිත මොඩේල් වලට නරක කරනවා කියල</abstract_si>
      <abstract_so>Waxbaranaynaa tarjumaadda machine neural (NMT) noocyada afka Ingiriiska ilaa lix luqadood oo target ah, waxaynu isticmaalnaa noocyada NMT koordirada ah si aan u sii sheegno calaamada afka source ah. Waxaynu heli nahay in qodxadaha NMT ay bartaan sintada u eg sourceed, haba kastoo ay tahay luqada NMT, iyadoo ku kalsoonaya xujooyinka morphosyntactic si ay uga soo bixiso xarumaha rasmiga ah. Markaas waxaa kale oo qoraya qorshaha NMT oo ku qoran RNNs oo toos ku tababaraya shaqooyin badan oo ka mid ah shaqooyin la sii-sheego ee guud lab-barta, waxay ka jeedayaan in qoidaha kooban ee NMT ay si faa’iido leh u isticmaali karto shaqooyinka luuqada asalka ah ee la xiriira canshuuraha. Si kastaba ha ahaatee, kooxda NMT iyo wadanka toos loo tababaray RNNs waxay ka bartaan macluumaad kala duduwan oo ay ka heleen xarunta kooxda xorta ah (PCFG). Inta kastoo ay hoos u dhigto kooxaha saxda ah, PCFG waxey marar badan si fiican ugu sameynaysaa xarumaha lagu sameynayo tusaalayaasha RNN-ku saleysan si ay u jeedaan in dhismaha RNN lagu qasbo qaababka ay u baran karo.</abstract_so>
      <abstract_sv>Vi trﾃ､nar neurala maskinﾃｶversﾃ､ttningsmodeller (NMT) frﾃ･n engelska till sex mﾃ･lsprﾃ･k, med hjﾃ､lp av NMT-kodare representationer fﾃｶr att fﾃｶrutsﾃ､ga fﾃｶrfﾃ､ders konstituerande etiketter av kﾃ､llsprﾃ･ksord. Vi finner att NMT-kodare lﾃ､r sig liknande kﾃ､llsyntax oavsett NMT mﾃ･lsprﾃ･k, beroende pﾃ･ explicita morfosyntaktiska ledtrﾃ･dar fﾃｶr att extrahera syntaktiska funktioner frﾃ･n kﾃ､llmeningar. Dessutom ﾃｶvertrﾃ､ffar NMT-kodarna RNN:er som utbildats direkt pﾃ･ flera av de konstituerande etikettprediktionsuppgifterna, vilket tyder pﾃ･ att NMT-kodarerepresentationer kan anvﾃ､ndas effektivt fﾃｶr naturliga sprﾃ･kuppgifter som involverar syntax. Men bﾃ･de NMT-kodarna och de direkt utbildade RNN:erna lﾃ､r sig vﾃ､sentligen olika syntaktisk information frﾃ･n en sannolik kontextfri grammatik (PCFG) parser. Trots lﾃ､gre ﾃｶvergripande noggrannhetspoﾃ､ng presterar PCFG ofta bra pﾃ･ meningar fﾃｶr vilka RNN-baserade modeller presterar dﾃ･ligt, vilket tyder pﾃ･ att RNN-arkitekturer ﾃ､r begrﾃ､nsade i de typer av syntax de kan lﾃ､ra sig.</abstract_sv>
      <abstract_ta>நாம் ஆங்கிலத்திலிருந்து ஆறு இலக்கு மொழிகளிலிருந்து புதிய இயந்திரத் மொழிமாற்றி மாதிரிகளை பயிற்சி செய்கிறோம், NMT குறியீட்ட We find that NMT encoders learn similar source syntax regardless of NMT target language, relying on explicit morphosyntactic cues to extract syntactic features from source sentences.  மேலும், NMT குறியீடுகள் RNNs நேரடியாக பயிற்சி செய்யப்பட்ட பல குறியீட்டு முன்காட்சி பணிகளில் பயிற்சி செய்யும், NMT குறியீட்டு குறியீட்டு ப ஆனால், NMT குறியீடுகளும் நேரடியாக பயிற்சி படுத்தப்பட்ட RNNs இருவரும் ஒரு சாத்தியமான சூழ்நிலை இலவச வரைப்படம் (PCFG) பிராணிகளிலிருந்த மொத்த சரியான மதிப்பெண்கள் குறைந்த போதிலும், PCFG பெரும்பாலும் RNN-அடிப்படையான மாதிரிகள் தவறான செயல்படுத்தும் வாக்கியங்களின் மீது செயல்படுத்துகி</abstract_ta>
      <abstract_ur>ہم انگلیسی سے چھ موقع زبانوں تک نیورال ماشین کی ترجمہ (NMT) موڈل کو ترکین دیتے ہیں، NMT کوڈر کی تصویر کے مطابق استعمال کرتے ہیں جن کی استعمال کرتی ہے کہ سورج زبان کی لکھوں کی پیش بینی کریں. ہم دیکھتے ہیں کہ NMT کوڈروں نے NMT موقع کی زبان کے بغیر کسی طرح کی سینٹکس سیکھ لیا ہے، سورج جماعتوں سے سینٹکس فوائل نکالنے کے لئے صریح موقع سینٹکس کوڈروں پر بھروسہ کیا ہے۔ اور اس کے بعد NMT کوڈروں نے RNN کو مستقیما تربیت کی چند قسمت لابل کی پیش بینی کے کاموں پر تربیت کی ہے، اس کی پیش بینی کرتی ہے کہ NMT کوڈر کی پیش بینی کے لئے مفید طور پر استعمال کی جاتی ہے۔ However, both NMT encoders and the direct-trained RNN learn substantially different syntactic information from a probabilistic context-free grammar (PCFG) parser. اگرچہ نیچے سارے دقیق اسکور کے ذریعہ، PCFG بہت زیادہ اچھی باتوں پر عمل کرتا ہے جن کے لئے RNN بنیادی موڈلہ برابر عمل کرتے ہیں، اس کی پیشنهاد کرتا ہے کہ RNN معماری سینٹکس کے طریقوں میں تنگ ہیں جن کو وہ سیکھ سکتے ہیں.</abstract_ur>
      <abstract_uz>Biz inglizcha tildan 6 tildan foydalanuvchi tarjima modellarni (NMT) tildan foydalanamiz. YanMT kodlash imkoniyatlarini ishlatish uchun avto-to ʻgʻri soʻzlardan foydalanishi mumkin. Biz bilganmiz, NMT kodlash usulini NMT tilidan bir xil kodlash imkoniyatini o'rganadi, manba soʻzlaridan foydatik xususiyatlarini olib tashlash uchun oddiy morphosyntactik xususiyatlarini ishlatadi. Kodlash usuli Lekin, NMT kodlash usuli va toʻgʻri taʼminlovchi RNNNs haqida juda bog'liq kodlash muvaffaqiyatlaridan foydalanadi. Parametrlar. Despite lower overall accuracy scores, the PCFG often performs well on sentences for which the RNN-based models perform poorly, suggesting that RNN architectures are constrained in the types of syntax they can learn.</abstract_uz>
      <abstract_vi>Chúng tôi đào tạo các mô hình dịch máy thần kinh từ Anh sang sáu ngôn ngữ đích, sử dụng biểu tượng mã hóa NMT để dự đoán các ký tự của tổ tiên trong ngôn ngữ nguồn. Chúng tôi tìm thấy rằng mã gien NMB học tương tự nhiên nhiên nhiên nhiên từ ngôn ngữ mục tiêu NMT, dựa trên kĩ năng morphine rõ ràng để trích các tính năng cú pháp từ câu từ. Hơn nữa, bộ mã hóa NMT vượt trội những RNN được huấn luyện trực tiếp trong nhiều nhiệm vụ dự đoán của định giá ảnh, gợi ý rằng biểu tượng mã hóa NMT có thể được sử dụng hiệu quả cho các công việc ngôn ngữ tự nhiên gồm syntax. Tuy nhiên, cả bộ mã hóa NMB và trực tiếp huấn luyện RNNs học về những thông tin cú pháp khác nhau cơ bản từ một cha xứ không ngữ cảnh ngữ cảnh ngẫu nhiên (PCFG). Mặc dù tỉ số độ chính xác thấp hơn, PCFG thường làm tốt với các câu cho những mẫu dựa trên RNN không tốt lắm, gợi ý kiến kiến kiến kiến trúc RNN bị hạn chế trong các loại syntax mà họ có thể học được.</abstract_vi>
      <abstract_bg>Обучаваме модели на невронен машинен превод (НМТ) от английски на шест целеви езика, като използваме изображения на НМТ кодер за предсказване на съставните етикети на предците думи на изходния език. Откриваме, че кодерите научават подобен синтаксис на източника независимо от целевия език на НМТ, разчитайки на изрични морфосинтактични знаци, за да извличат синтактични характеристики от изходните изречения. Освен това, NMT кодерите надминават RNN, обучени директно по няколко от съставните задачи за прогнозиране на етикета, което предполага, че NMT кодерите могат да бъдат използвани ефективно за задачи с естествен език, включващи синтаксис. Въпреки това, както кодерите на NMT, така и директно обучените RNN научават значително различна синтактична информация от анализатор на вероятността без контекст граматика (PCFG). Въпреки по-ниските общи оценки за точност, PCFG често се представя добре на изречения, за които моделите на базата на RNN се представят зле, което предполага, че архитектурите на RNN са ограничени в типовете синтаксис, които могат да научат.</abstract_bg>
      <abstract_nl>We trainen neuronale machine translation (NMT) modellen van Engels naar zes doeltalen, met behulp van NMT encoder representaties om voorouderlijke constitutionele labels van brontaalwoorden te voorspellen. We vinden dat NMT encoders vergelijkbare bronsyntaxis leren ongeacht de NMT doeltaal, afhankelijk van expliciete morfosyntactische aanwijzingen om syntactische kenmerken uit bronzinnen te extraheren. Bovendien presteren de NMT-encoders beter dan RNN's die direct zijn getraind op verschillende van de constitutionele label voorspellingstaken, wat suggereert dat NMT-encoder representaties effectief kunnen worden gebruikt voor natuurlijke taaltaken waarbij syntaxis is betrokken. Echter, zowel de NMT encoders als de direct getrainde RNN's leren substantieel verschillende syntactische informatie van een probabilistische context-vrije grammatica (PCFG) parser. Ondanks lagere algehele nauwkeurigheidsscore's presteert de PCFG vaak goed op zinnen waarvoor de RNN-gebaseerde modellen slecht presteren, wat suggereert dat RNN-architecturen beperkt zijn in de typen syntaxis die ze kunnen leren.</abstract_nl>
      <abstract_da>Vi træner neurale maskinoversættelsesmodeller (NMT) fra engelsk til seks målsprog ved hjælp af NMT encoder repræsentationer til at forudsige forfædre bestanddele etiketter af kildesprogsord. Vi finder ud af, at NMT-kodere lærer lignende kildesyntaks uanset NMT-målsprog, afhængig af eksplicitte morfosyntaktiske signaler til at udtrække syntaktiske funktioner fra kildesætninger. Desuden overgår NMT-koderne RNN'er, der er uddannet direkte på flere af de indbyggende etiketforudsigelsesopgaver, hvilket tyder på, at NMT-koderepræsentationer kan bruges effektivt til naturlige sprogopgaver, der involverer syntaks. Men både NMT-koderne og de direkte uddannede RNN'er lærer betydeligt forskellige syntaktiske oplysninger fra en sandsynligvis kontekstfri grammatik (PCFG) parser. På trods af lavere samlede nøjagtighedsgrader klarer PCFG ofte godt på sætninger, hvor RNN-baserede modeller fungerer dårligt, hvilket tyder på, at RNN-arkitekturer er begrænset i de typer syntaks, de kan lære.</abstract_da>
      <abstract_hr>Vježbamo modele prevoda neuralnih strojeva (NMT) iz engleskog na šest ciljnih jezika, koristeći predstave kodera NMT-a kako bi predvidjeli znakove predaka koji su sastojni od jezika izvora. Nalazimo da NMT koderi uče sličnu izvornu sintaksu bez obzira na ciljni jezik NMT-a, uz objašnjenje morfosintaktičkih znakova kako bi izvukli sintaktične znakove iz izvornih rečenica. Nadalje, NMT koderi nadmašuju RNN-ove koji su trenirani direktno na nekoliko zadataka predviđanja oznake, sugerirajući da se predstavljanja kodera NMT-a mogu učinkovito koristiti za prirodne jezičke zadatake uključujući sintaks. Međutim, i koderi NMT-a i direktno obučeni RNN-i nauče značajno različite sintaktičke informacije iz mogućnosti analizatora gramatike bez konteksta (PCFG). Uprkos nižim ukupnim rezultatima točnosti, PCFG često dobro izvodi na rečenicama za koje se modeli na RNN-u loše izvode, sugerirajući da su arhitekture RNN ograničene u vrstama sintakse koje mogu naučiti.</abstract_hr>
      <abstract_id>Kami melatih model terjemahan mesin saraf (NMT) dari bahasa Inggris ke enam bahasa sasaran, menggunakan represensi kode NMT untuk memprediksi label konstitusi nenek moyang dari kata-kata bahasa sumber. Kami menemukan bahwa koder NMT belajar sintaks sumber yang sama tidak peduli bahasa sasaran NMT, bergantung pada isyarat morfosintaksi eksplisit untuk mengekstrak fitur sintaks dari kalimat sumber. Selain itu, pengkode NMT melebihi RNN dilatih secara langsung pada beberapa tugas prediksi label konstitusi, menyarankan bahwa representasi pengkode NMT dapat digunakan secara efektif untuk tugas bahasa alam yang melibatkan sintaks. Namun, kedua koder NMT dan RNN yang terlatih secara langsung belajar informasi sintaks yang sangat berbeda dari parser gramatika bebas konteks probabilis (PCFG). Meskipun skor akurasi umum lebih rendah, PCFG sering berhasil dengan baik pada kalimat yang mana model berdasarkan RNN berhasil buruk, menyarankan bahwa arsitektur RNN dikuasai dalam jenis sintaks yang dapat mereka pelajari.</abstract_id>
      <abstract_de>Wir trainieren neuronale maschinelle Übersetzungsmodelle (NMT) aus dem Englischen in sechs Zielsprachen, wobei NMT-Encoder-Repräsentationen verwendet werden, um die Beschriftungen der Ausgangssprache vorherzusagen. Wir finden, dass NMT-Encoder ähnliche Quellsyntax unabhängig von der NMT-Zielsprache erlernen und sich auf explizite morphosyntaktische Hinweise verlassen, um syntaktische Merkmale aus Quellsätzen zu extrahieren. Darüber hinaus übertreffen die NMT-Encoder RNNs, die direkt auf mehreren der konstituierenden Label-Vorhersageaufgaben trainiert wurden, was darauf hindeutet, dass NMT-Encoder-Repräsentationen effektiv für natürliche Sprachaufgaben verwendet werden können, die Syntax beinhalten. Sowohl die NMT-Encoder als auch die direkt trainierten RNNs lernen jedoch wesentlich unterschiedliche syntaktische Informationen aus einem probabilistischen kontextfreien Grammatikpareser (PCFG). Trotz niedrigerer Gesamtgenauigkeitswerte schneidet das PCFG häufig gut bei Sätzen ab, für die die RNN-basierten Modelle schlecht funktionieren, was darauf hindeutet, dass RNN-Architekturen in den Syntaxtypen eingeschränkt sind, die sie lernen können.</abstract_de>
      <abstract_ko>우리는 영어부터 6개 목표 언어까지의 신경기계번역(NMT) 모델을 훈련하고 NMT 인코더 표시를 사용하여 원시 언어 단어의 조상 성분 라벨을 예측했다.우리는 NMT의 목표 언어가 무엇이든지 간에 NMT 인코더는 비슷한 원본 문법을 배우고 현식의 형태 문법 단서에 의존하여 원본 문장에서 문법 특징을 추출하는 것을 발견했다.또한 NMT 인코더의 성능은 몇 개의 성분 라벨 예측 임무에서 직접 훈련한 RNN보다 우수하다. 이것은 NMT 인코더가 문법과 관련된 자연 언어 임무에 효과적으로 사용할 수 있음을 나타낸다.그러나 NMT 인코더와 직접 훈련된 RNN은 모두 확률 상하문 무관문법(PCFG) 해석기에서 본질적으로 다른 문법 정보를 배웠다.전반적으로 정확도 점수가 낮았음에도 PCFG는 RNN 기반 모델이 좋지 않은 문장에서 잘 표현되는 경우가 많은데, 이는 RNN 아키텍처가 배울 수 있는 문법 유형에 한계가 있음을 보여준다.</abstract_ko>
      <abstract_fa>ما مدل ترجمه ماشین عصبی (NMT) را از انگلیسی به شش زبان هدف آموزش می‌دهیم، با استفاده از نمایش‌های رمز‌کننده NMT برای پیش‌بینی برچسب‌های نمایش‌کننده‌ای از زبان منبع پیش‌بینی کردند. ما پیدا می‌کنیم که Encoders NMT از زبان هدف NMT متفاوت‌های منبع مشابه یاد می‌گیرند و بر نشانه‌های مورفوسینتیک روشن می‌باشند تا ویژگی‌های سنتاکتیک از جمله‌های منبع خارج کنند. علاوه بر این، رمزگذاری NMT از RNN‌ها مستقیماً روی چندتا از کارهای پیش‌بینی برچسب‌های پایداری آموزش داده می‌شود، پیشنهاد می‌دهد که نمایش‌دهندگان رمزگذاری NMT می‌توانند برای کار زبان طبیعی که شامل سنتاکس است کاربرد شود. با این حال، هر دو رمزگران NMT و RNN‌های مستقیم آموزش داده شده‌اند، اطلاعات سنتاکتیک بسیار متفاوت از یک جداکنده‌کننده‌ی گرم‌نامه بی‌احتمال (PCFG) را یاد می‌گیرند. با وجود امتياز دقيق عمومي پايين، PCFG اغلب در مورد جمله‌هايي که مدل‌هاي بنياد RNN بد انجام مي‌دهند خوب انجام مي‌دهد، پيشنهاد مي‌دهد که معماري‌هاي RNN در نوع سنتاکسي که مي‌توانند ياد بگيرند محدود مي‌شوند.</abstract_fa>
      <abstract_sw>Tunafundisha mifano ya kutafsiri mashine ya kijinsia (NMT) kutoka Kiingereza hadi lugha sita za malengo, kwa kutumia idadi ya NMT kuwaonyesha mababu ya lugha ya asili. Tunapata kwamba idadi za NMT wanajifunza syntax kama hiyo bila kujali lugha ya malengo ya NMT, wanategemea mfumo wa wazi wa morphosyntactic ili kuondoa vipengele vya ushirikiano kutoka kwenye sentensi za vyanzo. Zaidi ya hayo, taarifa za NMT zinaweza kutumika kwa ufanisi wa kazi za lugha za asili zinazohusiana na kodi la utabiri. Hata hivyo, jumbe za NMT na wale wa RNNN walio na mafunzo ya moja kwa moja wanajifunza taarifa tofauti za ushirikiano kutoka kwenye programu inayowezekana kuwa haina uhuru wa mazingira (PCFG). Pamoja na score za sahihi za jumla, PCFG mara nyingi hufanya vizuri kwenye hukumu ambazo mifano ya msingi wa RNN hufanya vibaya, wakipendekeza kwamba majengo ya RNN yanalazimishwa katika aina ya kodi wanazoweza kujifunza.</abstract_sw>
      <abstract_tr>Biz iñlis dilinden alty maksady dillere çevirip nuýral maşynyň terjimesini (NMT) örneklerini NMT ködlemelerini gözden geçirmek üçin atalaryň çeşme dilleriniň etiketlerini ulanýarys. NMT ködlemeleri NMT maksady dilinden hiç haýsyz şeklinde meňzeş ködler öwrenip bilýäris Mundan hem, NMT ködlemeler RNN ködlemeleri diňe görnüş etiket öňünde bilinmiş täbliklerde, NMT ködlemeleri sintaksy bilen täbli diller täblisa üçin ullanylabilir. Ýöne, NMT ködlemeleri we düzgün bilinmeli RNN ködlemeleri çalyşyrlybdyr. Muhtemelen bir kontekst boş gramatik (PCFG) çözümlerden örän farklı sintaktik maglumatlary öwrenip biler. Adaty düşürmegine rağmen, PCFG köplenç RNN tabanly nusgalarynyň çirkin işleýän sözlerinde gowy dowam edýär we RNN arhitekterilerini öwrenip biljek syntaks ýaly çarpylýandygyny maslahat berýär.</abstract_tr>
      <abstract_sq>We train neural machine translation (NMT) models from English to six target languages, using NMT encoder representations to predict ancestor constituent labels of source language words.  Ne zbulojmë se koduesit NMT mësojnë sintaksi të ngjashme burimi pavarësisht nga gjuha e objektivit NMT, duke u mbështetur në shenja morfosintaktike të qarta për të nxjerrë karakteristika sintaktike nga fjalët e burimit. Furthermore, the NMT encoders outperform RNNs trained directly on several of the constituent label prediction tasks, suggesting that NMT encoder representations can be used effectively for natural language tasks involving syntax.  Megjithatë, si koduesit NMT dhe RNN të trajnuar drejtpërdrejt mësojnë informacion sintaktik thelbësisht të ndryshëm nga një analizues gramatik probabilist pa kontekst (PCFG). Megjithë rezultatet më të ulëta të saktësisë së përgjithshme, PCFG shpesh funksionon mirë në fjalët për të cilat modelet me bazë RNN funksionojnë keq, duke sugjeruar se arkitekturat RNN janë të kufizuara në llojet e sintaksit që mund të mësojnë.</abstract_sq>
      <abstract_af>Ons tref neurale masjien vertaling (NMT) modele van Engels tot ses doel tale, gebruik NMT enkoder voorstellings om voorskou voorskou voorskou voorskou voorstellings van voorspoedige konstituent etikette van bron taal woorde. Ons vind dat NMT-koders lyk soos bron sintaks ongeag van NMT-doel taal, verwag op eksplisiese morfosyntaktike tekens om sintaktike funksies uit bron setnings te uitpak. Verder, die NMT-koders uitvoer RNN wat direk op verskeie van die konstituent etiket voorskou opdragte onderrig is, wat voorstel dat NMT-koders-voorskou kan effektief gebruik word vir natuurlike taal-opdragte wat by sintaks invoer word. Maar beide die NMT-koders en die direk-opgelei RNN leer substantieel verskillende sintaktisies inligting van 'n waarskynlik konteks-vry grammatiek (PCFG) ontwerker. Alhoewel die kleiner heeltemal presisie telling, doen die PCFG dikwels goed op setnings waarvan die RNN-gebaseerde modele sleg uitvoer, voorstel dat RNN-arkitekturke in die tipes sintaks wat hulle kan leer word.</abstract_af>
      <abstract_am>የናውሬል መሣሪያን ትርጉም (NMT) ሞዴላዎችን ከንግግሊዝኛ ጀምሮ እስከ ስድስት ተቃውሞ ቋንቋዎች እናስተምራለን፤ የNMT የኮንዶር ፕሬዚደንቶች የአባቶችን የቋንቋ ቋንቋ ቃላትን ለመቀበል እናሳውቃለን፡፡ አዲስ የNMT ቀለሞች ከNMT ቋንቋ ምንም እንኳን እንደሚማሩ እናገኛለን፡፡ ከዚህም በላይ የNMT የኮድ አካባቢዎች በአካባቢው ቋንቋ ስራቶች ላይ አካባቢ የመንግሥት የጽሑፍ ቅድሚያ ማድረጊያውን በመቀበል ያስተማሩታል፡፡ ምንም እንኳን የNMT የኮድ ኮድ እና ቀጥተኛ ተማሪ RNNs በተለየ የተለየ የSyntactic መረጃ ከቻይለኛዊ context-free grammar (PCFG) ተርጓሚ ይማራሉ። ምንም እንኳን አናሽ እርግጠኛ ጥያቄ ቢሆንም PCFG ብዙ ጊዜ የRNN-based ሞዴላዎች ክስ እንዲያደርጉበት በክፍሎች ላይ ጥሩ ያደርጋል፡፡ RNN-መሠረት መሠረት በሚማሩት በዓይነት ሲንካስብ ውስጥ ግንኙነት ነው፡፡</abstract_am>
      <abstract_hy>Մենք վարժեցնում ենք նյարդային մեքենայի թարգմանման (NMT) մոդելներ անգլերենից վեց նպատակային լեզուներին, օգտագործելով NMT-ի կոդավորման ներկայացումներ, որպեսզի կանխատեսենք նախնիների բառերի բաղադրիչները: We find that NMT encoders learn similar source syntax regardless of NMT target language, relying on explicit morphosyntactic cues to extract syntactic features from source sentences.  Furthermore, the NMT encoders outperform RNNs trained directly on several of the constituent label prediction tasks, suggesting that NMT encoder representations can be used effectively for natural language tasks involving syntax.  Այնուամենայնիվ, ինչպես ՆՄԹ-ի կոդերները, ինչպես նաև անմիջապես պատրաստված ՌՆԹ-ները սովորում են նշանակալի տարբեր սինտակտիկ տեղեկություններ հավանական կոնտեքստից ազատ գրամագրության (PՖԳ) վերլուծողից: Չնայած ավելի ցածր ընդհանուր ճշգրտության գնահատականներին, ՀԿՀՀ-ն հաճախ լավ է աշխատում այն նախադասությունների վրա, որոնց համար ՌՆՆ-ի հիմնված մոդելները վատ են աշխատում, և առաջարկում է, որ ՌՆ-ի ճարտարապետությունները սահմանափակված են այն սի</abstract_hy>
      <abstract_bn>We train neural machine translation (NMT) models from English to six target languages, using NMT encoder representations to predict ancestor constituent labels of source language words.  আমরা খুঁজে পাচ্ছি যে এনএমটি এনকোডার এনএমটি টার্গেট ভাষার ব্যাপারে একই রকম সোর্স সিন্ট্যাক্স শিখে থাকে, উৎসের শাস্তি থেকে সিন্ট্যা এছাড়াও এনএমটি এনকোডার বেশ কয়েকটি প্রতিষ্ঠান লেবেলের ভবিষ্যদ্বাণী কাজে সরাসরি প্রশিক্ষণ প্রদান করেছে এনএমটি এনকোডারের প্রতিনিধিত্ব সিন্ট্যাক্স তবে এনএমটি এনকোডার এবং সরাসরি প্রশিক্ষণ প্রশিক্ষিত RNNs উভয় সম্ভবত একটি প্রেক্ষাপট-মুক্ত গ্রামার (পিসিএফজি) পার্সার থেকে বিভিন্ন সি সাধারণত সঠিক স্কোরের পরিমাণ সত্ত্বেও পিসিএফজি প্রায়শই ভালোভাবে কাজ করে যায়, যার জন্য RNN ভিত্তিক মডেল খারাপ কাজ করে, তার পরামর্শ দেয়া হচ্ছে যে RNN-এর</abstract_bn>
      <abstract_ca>Ensenyem models de traducció neural de màquina (NMT) d'anglès a sis llengües destinataris, fent servir representacions del codificador NMT per predir etiquetes constitutives dels ancestres de paraules de llengüe de font. Descobrim que els codificadors NMT aprenen síntaxis de fonts similars sense importar el llenguatge d'objectiu NMT, confiant en indicacions morfosíntactiques explícites per extrair característiques sinàctiques de les frases d'origen. A més, els codificadors NMT superen els RNN entrenats directament en diverses de les tasques de predicció de l'etiqueta constitucional, suggerent que les representacions del codificador NMT poden ser utilitzades eficaçment per tasques de llenguatge natural que impliquen sintaxis. Tot i així, tant els codificadors NMT com els RNN entrenats directament aprenen informació sinàctica substancialment diferent d'un analitzador de gramàtica probabilista sense contest (PCFG). Malgrat els punts de precisió global més baixos, el PCFG sovint funciona bé en frases per les quals els models basats en RNN actuen malament, suggerent que les arquitectures RNN estan restringides en els tipus de sintaxi que poden aprendre.</abstract_ca>
      <abstract_bs>Vježbamo modele prevođenja neuralnih strojeva (NMT) od engleskog na šest ciljnih jezika, koristeći predstave kodera NMT-a kako bi predvidjeli znakove predaka koji su sastojni od jezika izvora. Nalazimo da koderi NMT nauče sličnu izvornu sintaksu bez obzira na ciljni jezik NMT-a, uz objašnjenje morfosintaktičkih znakova kako bi izvukli sintaktične znakove iz izvornih rečenica. Nadalje, NMT koderi nadmašuju RNN-ove koji su trenirani direktno na nekoliko zadataka predviđanja oznake, sugerirajući da se predstavljanja kodera NMT-a mogu koristiti efikasno za prirodne jezičke zadatake uključujući sintaks. Međutim, i koderi NMT-a i direktno obučeni RNN-i nauče značajno različite sintaktičke informacije iz mogućnosti analizatora gramatike bez konteksta (PCFG). Uprkos nižim ukupnim preciznim rezultatima, PCFG često dobro izvodi na rečenicama za koje se modeli na RNN-u loše izvode, sugerirajući da su arhitekture RNN ograničene u vrstama sintakse koje mogu naučiti.</abstract_bs>
      <abstract_az>Biz İngilizdən altı məqsəd dillərə nöral maşına çevirilən modelləri təhsil edirik, NMT kodlayıcı göstəriciləri NMT kodlayıcı dillərini təhsil etmək üçün ataların dəstəkli dillərin etiketlərini təhsil edirik. NMT kodlayıcıların NMT məqsəd dilindən heç vaxt eyni mənbə sintaksi öyrənməsini görürük, mənbə sözlərindən sintaktik özellikləri çıxartmaq üçün açıq morfosintaktik işaretlərə təvəkkül edirlər. Buna görə də NMT kodlayıcısı RNN-ləri müəyyən etiket tədbirlərinin bir neçəsində təhsil edilən tədbirlərin üstündə istifadə edir və NMT kodlayıcının sintaksi ilə təbiətli dil işləri üçün istifadə edilə bilər. Ancaq NMT kodlayıcıları və doğrudan təhsil edilmiş RNN-lər mümkün olaraqlıq kontekst boş grammatik (PCFG) ayırıcısından çox farklı sintaktik məlumatları öyrənirlər. Düşünüb tam nöqtələri istisna olmasına rağmen, PCFG sık-sık RNN tabanlı modellərinin pis işlədikləri cümlələrdə yaxşı işlər edir, RNN arhitektarlarının öyrənib biləcəkləri sintaks nöqtələrində sıkıldığını göstərir.</abstract_az>
      <abstract_cs>Trénujeme modely neuronového strojového překladu (NMT) z angličtiny do šesti cílových jazyků, pomocí reprezentací NMT kodérů předpovídáme štítky slov zdrojového jazyka. Zjišťujeme, že NMT kodéry se učí podobnou zdrojovou syntaxi bez ohledu na cílový jazyk NMT, spoléhají na explicitní morfosyntaktické návody k extrakci syntaktických prvků ze zdrojových vět. Navíc NMT kodéry překonávají RNN trénované přímo na několika úlohách predikce štítků, což naznačuje, že reprezentace NMT kodérů lze efektivně použít pro úlohy přirozeného jazyka zahrnující syntaxi. Nicméně, jak NMT kodéry, tak přímo trénované RNN se naučí podstatně odlišné syntaktické informace z pravděpodobnostního kontextového parseru gramatiky (PCFG). Navzdory nižšímu celkovému skóre přesnosti PCFG často funguje dobře u vět, pro které modely založené na RNN fungují špatně, což naznačuje, že architektury RNN jsou omezeny typy syntaxe, které se mohou naučit.</abstract_cs>
      <abstract_et>Me koolitame neuromasintõlke (NMT) mudeleid inglise keelest kuuesse sihtkeelde, kasutades NMT kodeerijate esitusi lähtekeelsete sõnade esivanemate koostisosade märgistuste prognoosimiseks. Leiame, et NMT kodeerijad õpivad sarnast lähtesüntaksit sõltumata NMT sihtkeelest, tuginedes selgetele morfosüntaktilistele vihjetele süntaktiliste omaduste ekstraheerimiseks lähtelausetest. Lisaks sellele on NMT kodeerijad suuremad RNN-idest, mida on koolitatud otseselt mitmete koostisosade etiketi ennustamise ülesannete täitmisel, mis viitab sellele, et NMT kodeerijate esitusi saab tõhusalt kasutada looduskeele ülesannete puhul, mis hõlmavad süntaksit. Siiski õpivad nii NMT kodeerijad kui ka otse koolitatud RNN-id oluliselt erinevat süntaktilist teavet tõenäoliselt kontekstivaba grammatika (PCFG) parserist. Vaatamata väiksematele üldistele täpsuskooriumidele toimib PCFG sageli hästi lausetel, mille RNN-põhised mudelid toimivad halvasti, mis viitab sellele, et RNN-arhitektuurid on piiratud süntaksitüüpide poolest, mida nad õpivad.</abstract_et>
      <abstract_fi>Koulutamme neurokonekäännösmalleja englannista kuuteen kohdekieleen käyttäen NMT-koodausesityksiä lähdekielen sanojen esi-isien tunnusmerkkien ennustamiseen. Havaitsemme, että NMT-kooderit oppivat samankaltaisen lähdesyntaksin NMT-kohdekielestä riippumatta luottaen eksplisiittisiin morfosyntaktisiin vihjeihin saadakseen syntaktisia ominaisuuksia lähdelauseista. Lisäksi NMT-kooderit suoriutuvat RNN:istä, jotka on koulutettu suoraan useisiin komponentin etiketin ennustustehtäviin, mikä viittaa siihen, että NMT-kooderin representaatioita voidaan käyttää tehokkaasti luonnollisen kielen tehtävissä, joihin liittyy syntaksia. Kuitenkin sekä NMT-kooderit että suoraan koulutetut RNN:t oppivat huomattavasti erilaista syntaktista tietoa todennäköisestä kontekstivapaasta kieliopin jäsentäjästä (PCFG). Pienemmistä kokonaistarkkuuspisteistä huolimatta PCFG suoriutuu usein hyvin lauseissa, joissa RNN-pohjaiset mallit suoriutuvat huonosti, mikä viittaa siihen, että RNN-arkkitehtuurit ovat rajallisia oppimiensa syntaksien suhteen.</abstract_fi>
      <abstract_jv>Awak dhéwé nglanggar tarjamahan Neral (NMT) model sing sampeyan ingkang sampeyan kanggo isih tarjamahan ingkang sampeyan. Ngawe ngubah koder NMT nggawe tarjamahan kanggo ngerasai etiket sing sabun nggo langgambar luwih dumadhi. Anyone underline Ngubah kok, NMT koder nggawe R-NNs seneng dipun ajeng-ajeng langgambar kelas supoyo barang nggawe etiket supoyo nggawe Perintasun NMT politenessoffpolite"), and when there is a change ("assertivepoliteness Nanging kabèh sing paling kelas nêmêr kesempatan kanggo dianggawe gerarané, PFG supoyo sak ngempek dhéwé babagan barang nggawe model sing bisa DNN dumateng egal, supoyo supoyo nambah sing arep architecture DNN dumateng egal kang didasakno sing bisa mékalungé</abstract_jv>
      <abstract_sk>Usposabljamo modele nevronskega strojnega prevajanja (NMT) iz angleščine v šest ciljnih jezikov z uporabo NMT kodirnih reprezentacij za napovedovanje oznak prednikov sestavnih elementov besed izvornega jezika. Ugotovili smo, da se NMT kodirniki naučijo podobne izvorne sintakse ne glede na ciljni jezik NMT, pri čemer se zanašajo na eksplicitne morfosintaktične namige za ekstrakcijo sintaktičnih značilnosti iz izvornih stavkov. Poleg tega NMT kodirniki presegajo RNN, usposobljene neposredno za več nalog napovedovanja nalepk sestavnih delov, kar kaže, da se lahko predstavitve NMT kodirnikov učinkovito uporabljajo za naloge naravnega jezika, ki vključujejo sintakso. Vendar pa se tako NMT kodirniki kot neposredno usposobljeni RNN naučijo bistveno različnih sintaktičnih informacij iz verjetnostnega razčlenjevalnika brez konteksta slovnice (PCFG). Kljub nižjim splošnim rezultatom natančnosti PCFG pogosto dobro deluje na stavkih, za katere modeli na osnovi RNN delujejo slabo, kar kaže, da so arhitekture RNN omejene v vrstah sintakse, ki se jih lahko naučijo.</abstract_sk>
      <abstract_ha>Tuna tafiyar da misalin tarjifani na masu neural (NMT) misalin ayuka daga Ingiriya zuwa 6 harsunan gaba ɗaya, kuma, tuna amfani da NMT kode masu motsi dõmin ya yi birnin alama masu cikin maganar source. We find that NMT encoders learn similar source syntax regardless of NMT target language, relying on explicit morphosyntactic cues to extract syntactic features from source sentences.  Furtheran, NMT kode ɗin na outrun RNNs tunkuɗe live on several of the itinin la'anar label in constituent, sunã shiryarwa cẽwa an iya amfani da kode wakin NMT masu cikin aikin ayuka na asili masu cikin shirya da shiryarwa. Babu kasa, kodi na NMT da wanda aka yi wa shirya RNNNs, suna sanar da mutane masu inganci dabam-daban syntactic daga wani mai yiwuwa na wata grammar da ba'a komai ba (PCFG). Parser Babu ƙaranci ga fassarar daidaita duk, PCFG yana aiki mai kyau a kan sonar da misãlai na RNN ke samar da shi maras daidai, kuma ana gaya cewa an ƙudura musamman RNN cikin nau'i-nau'in da za su iya sanar.</abstract_ha>
      <abstract_he>אנחנו מאמן דוגמנים של מכונות עצביות (NMT) מאנגלית לשש שפות מטרה, באמצעות מייצגים של קודם NMT כדי לחזות את תוויות המרכיבים של אבות של מילים שפת מקור. אנחנו מוצאים שמקודדים NMT לומדים סינטקס מקור דומה ללא קשר לשפה המטרה NMT, בסמוך על סימנים מורפוסינטקטיים ברורים כדי להוציא תכונות סינטקסיות משפטי מקור. חוץ מזה, הקודנים NMT יוצאים מעל RNN מואמנים ישירות על מספר משימות צפייה של התווים המרכיבים, שמצייעים שהציגורים של הקודנים NMT יכולים להשתמש באופן יעיל למשימות שפת טבעיות שכוללות סינטקס. עם זאת, גם הקודנים NMT וגם RNN המאמנים ישירות לומדים מידע סינטאקטי שונה באופן משמעותי מפרסם גרמטיקה ללא הקשר (PCFG). למרות נקודות מדויקות כלליות נמוכות יותר, PCFG לעתים קרובות פועלת היטב על משפטים שבהם הדוגמנים המבוססים על RNN פועלים בצורה גרועה, מה שמצייע שארכיטקטורות RNN מוגבלות בסינטקס שהם יכולים ללמוד.</abstract_he>
      <abstract_fil>Nagtuturo kami ng mga modelo ng paglilikat ng neural machine (NMT) mula sa Ingles hanggang sa anim na target languages, na ginagamit ang mga representasyon ng encoder ng NMT upang mag-unawa ng mga constituent labels ng mga salitang source language. Nasusumpungan natin na ang mga encoder ng NMT ay nangagtutunaw ng paraang source syntax na walang kailangan ng target language ng NMT, na nangangailangan sa mga explicit morphosyntactic cues upang makakuha ng syntactic features mula sa source sentences. Bukod dito, ang mga encoder ng NMT ay nagsigawa ng mga RNN na tinuturuan directo sa iba sa mga gawang panghuhuli ng label na constituent, na ibinibigay na ang mga representation ng encoder ng NMT ay maaaring gamitin na mabuti sa mga gawang natural na wika na kasama ng syntax. Gayon ma'y ang mga encoder ng NMT at ang mga directly-trained RNN ay nangagtuturo ng lubhang ibang impormasyong syntactic mula sa probabilistic context-free grammar (PCFG) parser. Malibang mababa ang buong mga puntuang precision, madalas ang PCFG ay gumagawa ng mabuti sa mga sentasyon na ang mga modelo na pinagsasaligan ng RNN ay walang kabuluhan, na ibinibigay na ang mga arkitektura ng RNN ay nagpipigilan sa mga uri ng syntax na kanilang maalaman.</abstract_fil>
      <abstract_bo>We train neural machine translation (NMT) models from English to six target languages, using NMT encoder representations to predict ancestor constituent labels of source language words. We find that NMT encoders learn similar source syntax regardless of NMT target language, relying on explicit morphosyntactic cues to extract syntactic features from source sentences. Furthermore, the NMT encoders outperform RNNs trained directly on several of the constituent label prediction tasks, suggesting that NMT encoder representations can be used effectively for natural language tasks involving syntax. However, both the NMT encoders and the directly-trained RNNs learn substantially different syntactic information from a probabilistic context-free grammar (PCFG) parser. ཉེར་བར་ནམ་ཡོད་ཚད་མང་ཆེ་བའི་accuracy scores་ཀྱང་དམའ་བའི་PCFG་གིས་ཡང་ཚང་མས་སྟབས་བདེ་ཞིག་ཡོད་པའི་ཚིག་རྣམས་RNN ལ་གཞི་རྟེན་པའི་མིག</abstract_bo>
      </paper>
    <paper id="3">
      <title>Learning Probabilistic Sentence Representations from Paraphrases</title>
      <author><first>Mingda</first><last>Chen</last></author>
      <author><first>Kevin</first><last>Gimpel</last></author>
      <pages>17–23</pages>
      <abstract>Probabilistic word embeddings have shown effectiveness in capturing notions of generality and <a href="https://en.wikipedia.org/wiki/Logical_consequence">entailment</a>, but there is very little work on doing the analogous type of investigation for sentences. In this paper we define <a href="https://en.wikipedia.org/wiki/Statistical_model">probabilistic models</a> that produce <a href="https://en.wikipedia.org/wiki/Probability_distribution">distributions</a> for sentences. Our best-performing <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> treats each word as a <a href="https://en.wikipedia.org/wiki/Linear_map">linear transformation operator</a> applied to a <a href="https://en.wikipedia.org/wiki/Normal_distribution">multivariate Gaussian distribution</a>. We train our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> and demonstrate that they naturally capture sentence specificity. While our proposed model achieves the best performance overall, we also show that <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">specificity</a> is represented by simpler <a href="https://en.wikipedia.org/wiki/Computer_architecture">architectures</a> via the norm of the sentence vectors. Qualitative analysis shows that our probabilistic model captures sentential entailment and provides ways to analyze the specificity and preciseness of individual words.</abstract>
      <url hash="09f3e76d">2020.repl4nlp-1.3</url>
      <doi>10.18653/v1/2020.repl4nlp-1.3</doi>
      <video href="http://slideslive.com/38929769" />
      <bibkey>chen-gimpel-2020-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    <title_ar>تعلم تمثيلات الجمل الاحتمالية من إعادة الصياغة</title_ar>
      <title_pt>Aprendendo representações probabilísticas de sentenças a partir de paráfrases</title_pt>
      <title_ja>パラフレーズから確率的文章表現を学習する</title_ja>
      <title_fr>Apprentissage des représentations probabilistes de phrases à partir de paraphrases</title_fr>
      <title_es>Aprender representaciones probabilísticas de oraciones a partir de paráfrasis</title_es>
      <title_zh>从释义中学概率句</title_zh>
      <title_ukr>Вивчення ймовірнісних речень з парафраз</title_ukr>
      <title_ru>Изучение вероятностных представлений предложений из парафраз</title_ru>
      <title_hi>पैराफ्रेज़ से संभावित वाक्य प्रतिनिधित्व सीखना</title_hi>
      <title_ga>Léirithe Pianbhreithe Dóchúla a Fhoghlaim ó Athróga</title_ga>
      <title_ka>პარაფრაზებიდან შესაძლებლობა სიტყვების გამოსახულება</title_ka>
      <title_isl>Learning Probabilistic Sentence Representations from Paraphrases</title_isl>
      <title_hu>A valószínűsítő mondatok megtanulása parafrázisokból</title_hu>
      <title_el>Μάθηση Πιθανιστικών Αντιπροσωπεύσεων Προτάσεων από Παραφράσεις</title_el>
      <title_it>Imparare le rappresentazioni probabilistiche delle frasi dalle parafrasi</title_it>
      <title_lt>Mokymasis tikėtinų sakinių atstovavimais iš parafrazių</title_lt>
      <title_mk>Научи веројатни реченици од парафрази</title_mk>
      <title_kk>Парафраздан ықтималдық сөздерді таңдау</title_kk>
      <title_ms>Mempelajari Perwakilan Sentensi Kemungkinan Dari Parafrasa</title_ms>
      <title_ml>പാരാഫ്രായസില്‍ നിന്നും പ്രതിനിധികള്‍ പഠിക്കുന്നതിന് സാധ്യമായ ശിക്ഷ</title_ml>
      <title_mt>Tagħlim Rappreżentazzjonijiet ta’ Sentenza Probabilistika minn Parafrażijiet</title_mt>
      <title_mn>Парафразын магадлалын өгүүлбэрийн суралцах</title_mn>
      <title_no>Læring av sannsynlige setningar frå parafraser</title_no>
      <title_pl>Uczenie się prawdopodobnych reprezentacji zdań z parafraz</title_pl>
      <title_ro>Învățarea reprezentărilor propoziției probabilistice din parafraze</title_ro>
      <title_sr>Naučenje mogućnosti predstavljanja kazne iz parafraze</title_sr>
      <title_si>පරාෆ්‍රේස් වලින් සංශ්‍ය වාර්තාව ප්‍රතිස්ථාපනය ඉගෙන ගන්න</title_si>
      <title_so>Barista xuquuqda ciqaabka suurtagalka ah ee Paraphrases</title_so>
      <title_ta>சாத்தியமான வாக்குறுதி பிரதிநிதிகளை கற்றுக்கொள்வது</title_ta>
      <title_sv>Att lära sig sannolika meningsrepresentationer från parafraser</title_sv>
      <title_ur>پارافریزز سے احتمالات سنٹنس نمایش سکھائی جاتی ہے</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Học thuyết đồn đại từ những cụm từ</title_vi>
      <title_bg>Изучаване на вероятни представи на изречения от парафрази</title_bg>
      <title_da>At lære sandsynlige sætningsrepræsentationer fra parafraser</title_da>
      <title_nl>Probabilistische zinsvertegenwoordigingen leren van parafrases</title_nl>
      <title_hr>Naučenje mogućnosti predstavljanja kazne iz parafraze</title_hr>
      <title_de>Wahrscheinliche Satzrepräsentationen aus Paraphrasen lernen</title_de>
      <title_ko>해석에서 확률 문장 표징을 배우다</title_ko>
      <title_id>Mempelajari Perwakilan Sentensi Kemungkinan Dari Parafrasa</title_id>
      <title_fa>یاد گرفتن نماینده‌های احتمالات فیلم از پارافریز</title_fa>
      <title_sw>Kujifunza Maandamano yanayoweza kuanzia Paraphras</title_sw>
      <title_af>Leer waarskynlik voorstellings van voorstellings van parafrase</title_af>
      <title_tr>Parafrazlerden olaryk sözler temsillerini öwrenmek</title_tr>
      <title_sq>Learning Probabilistic Sentence Representations from Paraphrases</title_sq>
      <title_am>ከፓራፍሬስ የስርዓት ተሟጋቾች ማስተማር</title_am>
      <title_bn>প্যারাফ্রাইস থেকে সম্ভাব্য শাস্তি প্রতিনিধি শিখা</title_bn>
      <title_az>Parafrazl톛rd톛n Muxluqatl캼q S칬z칲 G칬r칲nt칲l톛ri 칬yr톛nm톛k</title_az>
      <title_bs>Naučenje mogućnosti predstavljanja kazne iz parafraze</title_bs>
      <title_ca>Aprendre representacions de sentences probables a partir de parafrases</title_ca>
      <title_cs>Učení se pravděpodobných reprezentací vět z parafráz</title_cs>
      <title_et>Tõenäoliste lausete esitamise õppimine parafraasidest</title_et>
      <title_fi>Oppiminen todennäköisiä lauseesityksiä parafraaseista</title_fi>
      <title_hy>Գիտել հավանական նախադասությունների ներկայացումները պարաֆրեսից</title_hy>
      <title_jv>gagal</title_jv>
      <title_sk>Učenje verjetnih stavkov iz odstavkov</title_sk>
      <title_he>ללמוד מייצגים של גזרים סבירים</title_he>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_fil>Nagtuturo ng Probabilistic Sentence Representations mula sa Paraphrases</title_fil>
      <title_bo>བྱ་ཚུལ་གྱི་སྐད་ཆ་མཚོན་པའི་སྐད་བརྗོད་སྟོན་དག་སློང་བྱེད་ཀྱི་ཡོད།</title_bo>
      <abstract_fr>Les intégrations probabilistes de mots se sont révélées efficaces pour saisir les notions de généralité et d'implication, mais il y a très peu de travail sur le type d'enquête analogue pour les phrases. Dans cet article, nous définissons des modèles probabilistes qui produisent des distributions pour les phrases. Notre modèle le plus performant traite chaque mot comme un opérateur de transformation linéaire appliqué à une distribution gaussienne multivariée. Nous entraînons nos modèles sur les paraphrases et montrons qu'ils capturent naturellement la spécificité des phrases. Bien que notre modèle proposé atteigne les meilleures performances globales, nous montrons également que la spécificité est représentée par des architectures plus simples via la norme des vecteurs de phrases. L'analyse qualitative montre que notre modèle probabiliste saisit l'implication sententielle et fournit des moyens d'analyser la spécificité et la précision des mots individuels.</abstract_fr>
      <abstract_pt>Embeddings probabilísticos de palavras mostraram eficácia na captura de noções de generalidade e implicação, mas há muito pouco trabalho em fazer o tipo análogo de investigação para sentenças. Neste artigo definimos modelos probabilísticos que produzem distribuições para sentenças. Nosso modelo de melhor desempenho trata cada palavra como um operador de transformação linear aplicado a uma distribuição gaussiana multivariada. Treinamos nossos modelos em paráfrases e demonstramos que elas capturam naturalmente a especificidade da frase. Enquanto nosso modelo proposto alcança o melhor desempenho geral, também mostramos que a especificidade é representada por arquiteturas mais simples via norma dos vetores de sentença. A análise qualitativa mostra que nosso modelo probabilístico captura a vinculação sentencial e fornece maneiras de analisar a especificidade e precisão de palavras individuais.</abstract_pt>
      <abstract_ar>لقد أظهرت عمليات دمج الكلمات الاحتمالية فعالية في التقاط مفاهيم العمومية والضمانات ، ولكن هناك القليل جدًا من العمل على القيام بالنوع المماثل من التحقيق في الجمل. في هذا البحث نحدد النماذج الاحتمالية التي تنتج التوزيعات للجمل. يتعامل نموذجنا الأفضل أداءً مع كل كلمة على أنها عامل تحويل خطي مطبق على توزيع غاوسي متعدد المتغيرات. نقوم بتدريب نماذجنا على إعادة الصياغة وإظهار أنها تلتقط بشكل طبيعي خصوصية الجملة. بينما يحقق نموذجنا المقترح أفضل أداء بشكل عام ، فإننا نظهر أيضًا أن الخصوصية يتم تمثيلها من خلال أبنية أبسط عبر قاعدة متجهات الجملة. يُظهر التحليل النوعي أن نموذجنا الاحتمالي يلتقط الضمانات الوجدانية ويوفر طرقًا لتحليل خصوصية ودقة الكلمات الفردية.</abstract_ar>
      <abstract_es>Las incrustaciones probabilísticas de palabras han demostrado ser eficaces para captar nociones de generalidad e implicación, pero hay muy poco trabajo para hacer el tipo análogo de investigación de sentencias. En este artículo definimos modelos probabilísticos que producen distribuciones para las sentencias. Nuestro modelo de mejor rendimiento trata cada palabra como un operador de transformación lineal aplicado a una distribución gaussiana multivariante. Entrenamos nuestros modelos en paráfrasis y demostramos que capturan naturalmente la especificidad de las oraciones. Si bien nuestro modelo propuesto logra el mejor rendimiento en general, también demostramos que la especificidad está representada por arquitecturas más simples a través de la norma de los vectores de oración. El análisis cualitativo muestra que nuestro modelo probabilístico captura la implicación de la oración y proporciona formas de analizar la especificidad y precisión de las palabras individuales.</abstract_es>
      <abstract_zh>概率词嵌普遍性、蕴涵概有效性,然句类几无事。 本文中,定义成句概率模样。 以至善之形,视单词为用多元斯布之线性易算子。 吾以释义训模,证其自得句特异性。 虽体得其宜,而我亦明,句向量之范数,由其架构特异性。 定性分析表明,吾概率获感蕴涵,而给单词特殊性精确性之术。</abstract_zh>
      <abstract_ja>確率論的な単語埋め込みは、一般性と帰結の概念を取り込むのに有効であることが示されているが、文の類似の調査タイプを行う作業はほとんどない。本稿では、文の分布を生成する確率モデルを定義する。最良のパフォーマンスを発揮するモデルは、各単語を多変数ガウス分布に適用される線形変換演算子として扱います。私たちはパラフレーズに関するモデルをトレーニングし、それらが文章の特異性を自然に取り込むことを実証します。提案されたモデルは全体的に最高のパフォーマンスを達成しますが、文ベクトルのノルムを介してより単純なアーキテクチャによって特異性が表されることも示されています。定性的分析は、我々の確率論的モデルが意味的帰結を捉え、個々の単語の特異性と精度を分析する方法を提供することを示している。</abstract_ja>
      <abstract_hi>संभाव्य शब्द एम्बेडिंग ने व्यापकता और अनिवार्यता की धारणाओं को कैप्चर करने में प्रभावशीलता दिखाई है, लेकिन वाक्यों के लिए अनुरूप प्रकार की जांच करने पर बहुत कम काम है। इस पेपर में हम संभाव्य मॉडल को परिभाषित करते हैं जो वाक्यों के लिए वितरण का उत्पादन करते हैं। हमारा सबसे अच्छा प्रदर्शन करने वाला मॉडल प्रत्येक शब्द को एक रैखिक परिवर्तन ऑपरेटर के रूप में मानता है जो एक बहुचर गाऊसी वितरण पर लागू होता है। हम अपने मॉडल को पैराफ्रेज़ पर प्रशिक्षित करते हैं और प्रदर्शित करते हैं कि वे स्वाभाविक रूप से वाक्य विशिष्टता पर कब्जा करते हैं। जबकि हमारा प्रस्तावित मॉडल समग्र रूप से सबसे अच्छा प्रदर्शन प्राप्त करता है, हम यह भी दिखाते हैं कि विशिष्टता को वाक्य वैक्टर के आदर्श के माध्यम से सरल आर्किटेक्चर द्वारा दर्शाया जाता है। गुणात्मक विश्लेषण से पता चलता है कि हमारा संभाव्य मॉडल संवेदनशील अनिवार्यता को कैप्चर करता है और व्यक्तिगत शब्दों की विशिष्टता और सटीकता का विश्लेषण करने के तरीके प्रदान करता है।</abstract_hi>
      <abstract_ru>Вероятностные вложения слов показали эффективность в отражении понятий общности и влечения, но очень мало работы по проведению аналогичного типа расследования в отношении приговоров. В этой статье мы определяем вероятностные модели, которые производят распределения для предложений. Наша наиболее эффективная модель рассматривает каждое слово как оператор линейного преобразования, применяемый к многомерному распределению Гаусса. Мы тренируем наши модели на перефразах и демонстрируем, что они, естественно, отражают специфику предложения. Хотя наша предлагаемая модель достигает наилучшей производительности в целом, мы также показываем, что специфичность представлена более простыми архитектурами через норму векторов предложений. Качественный анализ показывает, что наша вероятностная модель захватывает дословное влечение и предоставляет способы анализа специфичности и точности отдельных слов.</abstract_ru>
      <abstract_ukr>Ймовірнісні вбудовування слів показали ефективність у фіксації понять загальності та тяжіння, але є дуже мало роботи щодо проведення аналогічного типу розслідування для вироків. У цій роботі ми визначаємо ймовірнісні моделі, які виробляють розподіли для речень. Наша найкраща модель розглядає кожне слово як оператор лінійного перетворення, застосований до багатовимірного розподілу Гауса. Ми тренуємо наші моделі на перефразах і демонструємо, що вони, природно, фіксують специфіку речення. Хоча наша запропонована модель досягає найкращої продуктивності в цілому, ми також показуємо, що специфіка представлена більш простими архітектурами через норму векторів речень. Якісний аналіз показує, що наша ймовірнісна модель фіксує припущення та надає шляхи аналізу специфічності та точності окремих слів.</abstract_ukr>
      <abstract_ga>Tá éifeachtacht léirithe ag leabaithe focal dóchúla maidir le coincheapa ginearáltachta agus eitice a ghabháil, ach is beag obair atá ar bun ar an gcineál imscrúdaithe ar aon dul le habairtí. Sa pháipéar seo sainímid samhlacha dóchúlachta a tháirgeann dáiltí d'abairtí. Déileálann ár múnla is fearr feidhmíochta le gach focal mar oibreoir claochlaithe líneach a chuirtear i bhfeidhm ar dháileadh ilvariate Gaussach. Cuirimid oiliúint ar ár múnlaí ar athfhrásaí agus léirímid go mbaineann siad sainiúlacht abairtí go nádúrtha. Cé go mbaineann ár múnla molta an fheidhmíocht is fearr amach ar an iomlán, léirímid freisin go léirítear sainiúlacht ag ailtireachtaí níos simplí trí norm na veicteoirí abairtí. Léiríonn anailís cháilíochtúil go nglacann ár múnla dóchúlachta gabháil chainte agus go soláthraíonn sé bealaí chun anailís a dhéanamh ar shainiúlacht agus beachtas na bhfocal aonair.</abstract_ga>
      <abstract_el>Οι πιθανές ενσωμάτωση λέξεων έχουν δείξει αποτελεσματικότητα στην καταγραφή των εννοιών της γενικότητας και της περιεκτικότητας, αλλά υπάρχει πολύ μικρή εργασία για την πραγματοποίηση του ανάλογου τύπου έρευνας για τις προτάσεις. Στην παρούσα εργασία καθορίζουμε πιθανολογικά μοντέλα που παράγουν διανομές για προτάσεις. Το μοντέλο μας με τις καλύτερες επιδόσεις αντιμετωπίζει κάθε λέξη ως έναν τελεστή γραμμικού μετασχηματισμού που εφαρμόζεται σε μια πολυμεταβλητή κατανομή Gaussian. Εκπαιδεύουμε τα μοντέλα μας σε παραφράσεις και αποδεικνύουμε ότι αποτυπώνουν φυσικά την ιδιαιτερότητα των προτάσεων. Ενώ το προτεινόμενο μοντέλο επιτυγχάνει την καλύτερη απόδοση συνολικά, καταδεικνύουμε επίσης ότι η ιδιαιτερότητα αντιπροσωπεύεται από απλούστερες αρχιτεκτονικές μέσω του κανόνα των διανυσμάτων προτάσεων. Η ποιοτική ανάλυση δείχνει ότι το πιθανολογικό μοντέλο μας συλλαμβάνει νοητική εμπλοκή και παρέχει τρόπους ανάλυσης της ιδιαιτερότητας και ακρίβειας των μεμονωμένων λέξεων.</abstract_el>
      <abstract_isl>Líklegt er að orðinnsetning hafi sýnt árangur við að ná í hugmyndir um almenni og tengsl, en það er mjög lítið að vinna við að gera svipaða gerð rannsóknar á dómum. Í þessu pappíri skilgreinum við líkurnar líkur sem framleiða dreifingu fyrir setningar. Our best-performing model treats each word as a linear transformation operator applied to a multivariate Gaussian distribution.  Viđ ūjálfum líkanir okkar á setningum og sũnum ađ ūeir náttúrulega taka á sértækni setninga. Meðan fyrirhuguð líkani nær bestu árangur í heild sýnum við einnig að sértækni er staðfest af einfaldara arhitektúrum með venju á setningaleikum. Gæðingargreining sýnir að líkamsþættismyndin okkar tekur upp dæmismynd og veitir leiðir til að greina sértækni og nákvæmni einstakra orða.</abstract_isl>
      <abstract_hu>A valószínűsíthető szóbeágyazások hatékonyságot mutattak az általánosság és a vonatkozás fogalmának megfogalmazásában, de nagyon kevés munka van a mondatok analóg vizsgálatának elvégzésén. Ebben a tanulmányban olyan valószínűségi modelleket határozunk meg, amelyek mondatok eloszlását hozzák létre. A legjobb teljesítményű modellünk minden szót lineáris transzformációs operátorként kezeli egy többváltozós gauss eloszlásra alkalmazott lineáris transzformációs operátorként. Modelljeinket parafrázisokra képezzük, és bebizonyítjuk, hogy természetesen megragadják a mondatspecifikusságot. Miközben javasolt modellünk a legjobb teljesítményt éri el összességében, azt is megmutatjuk, hogy a specifikációt egyszerűbb architektúrák képviselik a mondatvektorok normáján keresztül. Minőségi elemzés azt mutatja, hogy valószínűsíthető modellünk megragadja az érzékenységi vonatkozásokat és lehetőségeket nyújt az egyes szavak specificitásának és pontosságának elemzésére.</abstract_hu>
      <abstract_ka>შესაბამისი სიტყვები, რომელიც ჩვენებული სიტყვები, უფრო ძალიან მარტივი სამუშაო სიტყვების შესაბამისთვის ეფექტიურობა, რომელიც გენერალურობის და დასაბამისთვის შესაბამისთ ამ წიგნიში ჩვენ განსაზღვრებთ შესაბამისი მოდელები, რომლებიც წარმოიქმნენ წიგნილებების განსაზღვრება. ჩვენი ყველაზე საუკეთესო მოდელეში ყველა სიტყვა, როგორც ლინინერი ტრანფიგურაციის პოპერატორი, როგორც მულტივარიატური დაუსიანის გაყოფილებისთვი ჩვენ ჩვენი მოდელების პარაფრაზების შესაბამისად გავარწმუნეთ და გამოჩნეთ, რომ ისინი თავისუფალურად წესების განსაკუთრებულობას დავწყებენ თუმცა ჩვენი მოდელი უკეთესი პროცესტის უკეთესი პროცესტის მიღება, ჩვენ ასევე ჩვენ ჩვენ ჩვენ ჩვენ ჩვენ ჩვენი მოდელესტის განსაკუთრებულობა უფრო სუ კვალიტატიური ანალიზია, რომ ჩვენი შესაბამისი მოდელი წარმოდგენა სენტიალური დაწყვეტილება და დააწყვეტის გზები, რომლებიც ინდიველური სიტყვების განსაკუთრებულობა და წ</abstract_ka>
      <abstract_lt>Probabilistic word embeddings have shown effectiveness in capturing notions of generality and entailment, but there is very little work on doing the analogous type of investigation for sentences.  Šiame dokumente mes apibrėžiame probabilistinius modelius, kurie leidžia paskirstyti sakinius. Mūsų geriausiai veikiantis modelis vertina kiekvieną žodį kaip linijinės transformacijos operatorių, taikomą daugialypiam gausijos pasiskirstymui. Mokome savo modelius parafrazėmis ir įrodome, kad jie natūraliai apibūdina sakinio specifiškumą. Nors mūsų siūlomas model is pasiekia geriausių rezultatų apskritai, mes taip pat parodome, kad specifiškumą rodo paprastesnės architektūros pagal sakinių vektorių norm ą. Kokybinė analizė rodo, kad mūsų probabilistinis modelis apima bausmę ir pateikia būdus analizuoti atskirų žodžių specifiškumą ir tikslumą.</abstract_lt>
      <abstract_it>Le incorporazioni probabilistiche di parole hanno dimostrato efficacia nel catturare nozioni di generalità e implicazione, ma c'è molto poco lavoro per fare il tipo analogo di indagine per le frasi. In questo articolo definiamo modelli probabilistici che producono distribuzioni per frasi. Il nostro modello più performante tratta ogni parola come un operatore di trasformazione lineare applicato a una distribuzione gaussiana multivariata. Formiamo i nostri modelli sulle parafrasi e dimostriamo che catturano naturalmente la specificità della frase. Mentre il nostro modello proposto raggiunge le migliori prestazioni complessive, mostriamo anche che la specificità è rappresentata da architetture più semplici attraverso la norma dei vettori di frase. L'analisi qualitativa mostra che il nostro modello probabilistico cattura il coinvolgimento sentimentale e fornisce modi per analizzare la specificità e la precisione delle singole parole.</abstract_it>
      <abstract_kk>Мүмкіндік сөздерді ендіру үшін жалпы және жалпы түсініктерді түсініп, бірақ сөздердің аналогиялық түрін зерттеу үшін тым жұмыс істейді. Бұл қағазда сөйлемелер үшін таратылатын маңызды үлгілерді анықтаймыз. Біздің ең жақсы істеу үлгіміз әрбір сөз сызық түрлендіру операторы болып, көптеген гаусия үлестірімізге қолданылады. Өзіміздің үлгілерімізді парафразлар туралы оқыту және олардың тәуелдігін түсіндіреді. Біздің келтірілген үлгіміз ең жақсы жұмыс істеу үшін, сондай-ақ біз сөйлемелер векторының нормасы арқылы қарапайым архитектураларды көрсетеді. Сапаттылық анализ біздің ықтималдық үлгіміз сәттілікті түсіріп, әрбір сөздердің тәсілдігін және дұрыстығын анализдеу жолдарын береді.</abstract_kk>
      <abstract_ms>Pencampuran perkataan kemungkinan telah menunjukkan keefektivitas dalam menangkap pemikiran umum dan penyelesaian, tetapi terdapat sedikit kerja untuk melakukan jenis penyelidikan analog untuk kalimat. In this paper we define probabilistic models that produce distributions for sentences.  Model terbaik kami memperlakukan setiap perkataan sebagai operator pengubahan linear yang dilaksanakan kepada distribusi Gaussian berbilang variasi. Kami melatih model kami pada parafrasa dan menunjukkan bahawa mereka secara alami menangkap spesifik kalimat. Sementara model kami yang diusulkan mencapai prestasi terbaik secara keseluruhan, kami juga menunjukkan bahawa spesifik adalah mewakili oleh arkitektur yang lebih mudah melalui norma vektor kalimat. Analisis kualitif menunjukkan bahawa model kemungkinan kita menangkap penyelesaian kalimat dan menyediakan cara untuk menganalisis spesifik dan ketepatan perkataan individu.</abstract_ms>
      <abstract_mt>L-inkorporazzjoni probabbli tal-kliem uriet effikaċja fil-qbid ta’ kunċetti ta’ ġeneralità u involviment, iżda ftit li xejn hemm xogħol fuq it-twettiq tat-tip analogu ta’ investigazzjoni għas-sentenzi. F’dan id-dokument niddefinixxu mudelli probabilistiċi li jipproduċu distribuzzjonijiet għas-sentenzi. Il-mudell tagħna bl-a ħjar prestazzjoni jittratta kull kelma bħala operatur ta’ trasformazzjoni lineari applikat għal distribuzzjoni Gaussjana multivarjata. Aħna nħarrġu l-mudelli tagħna fuq parafrażijiet u nippruvaw li b’mod naturali jaqbdu l-ispeċifiċità tas-sentenza. While our proposed model achieves the best performance overall, we also show that specificity is represented by simpler architectures via the norm of the sentence vectors.  Analiżi kwalitattiva turi li l-mudell probabilistiku tagħna jaqbad l-involviment sentenzjali u jipprovdi modi biex tiġi analizzata l-ispeċifiċità u l-preċiżjoni tal-kliem individwali.</abstract_mt>
      <abstract_ml>സാധ്യമായ വാക്കുകള്‍ സാധ്യതയുടെയും വിവരങ്ങളുടെയും ആശയങ്ങള്‍ പിടികൂടുന്നതില്‍ സാധ്യതയുണ്ടായിരിക്കുന്നു. പക്ഷെ വാക്കുകള്‍ക്കുള്ള അന് ഈ പത്രത്തില്‍ നമ്മള്‍ ശിക്ഷയ്ക്ക് വിതരണം വരുത്തുന്ന സാധ്യതയുള്ള മോഡലുകള്‍ നിര്‍ണയിക്കുന്നു. നമ്മുടെ ഏറ്റവും നല്ല പ്രവര്‍ത്തനങ്ങളുടെ മോഡല്‍ ഓരോ വാക്കിനെയും നിരീക്ഷിക്കുന്നു. ഒരു ലൈയിര്‍ മാറ്റം മാറ്റങ്ങള്‍ ഓപ്പറേറ ഞങ്ങള്‍ നമ്മുടെ മോഡലുകളെ പരിശീലിപ്പിക്കുകയും പ്രധാനപ്പെടുത്തുകയും ചെയ്യുന്നത് സ്വാഭാവികമായി അവര്‍ വിധിയ നമ്മുടെ പ്രൊദ്ദേശിക്കപ്പെട്ട മോഡല്‍ എല്ലാവര്‍ക്കും ഏറ്റവും നല്ല പ്രവര്‍ത്തനങ്ങള്‍ എത്തുമ്പോള്‍ നമ്മള്‍ കാണിക്കുന്നു വാക്ക് വെക്റ നമ്മുടെ സാധ്യതയുടെ മോഡല്‍ വിവരങ്ങളുടെ പിടികൂടുകയും വ്യക്തിപരമായ വാക്കുകളുടെ പ്രത്യേക വിശേഷതയും വിശേഷിപ്പിക്കാന്‍ വഴികള്‍ നല്‍</abstract_ml>
      <abstract_mk>Веројатното вложување на зборови покажа ефикасност во зафатувањето на понимањата за генералност и вмешаност, но има многу малку работа на извршувањето на аналошкиот тип на истрага за казни. Во оваа хартија дефинираме веројатни модели кои произведуваат дистрибуција за реченици. Нашиот најдобар модел го третира секој збор како линијарен трансформациски оператор аплициран на мултиваријатна гауска дистрибуција. Ние ги тренираме нашите модели на парафрази и демонстрираме дека природно ја заземаат специфичноста на речениците. Додека нашиот предложен модел постигнува најдобра резултат вкупно, ние исто така покажуваме дека специфичноста е претставена од поедноставни архитектури преку нормата на векторите на речениците. Квалитативната анализа покажува дека нашиот веројатен модел зазема реченици и обезбедува начини за анализирање на специфичноста и прецизноста на индивидуалните зборови.</abstract_mk>
      <abstract_pl>Prawdopodobne osadzenia słów wykazały skuteczność w uchwyceniu pojęć ogólności i pociągnięcia, ale jest bardzo mało pracy nad przeprowadzeniem analogicznego rodzaju dochodzenia w odniesieniu do zdań. W artykule zdefiniowano modele probabilistyczne, które tworzą rozkłady zdań. Nasz najlepiej wydajny model traktuje każde słowo jako operator transformacji liniowej stosowany do wielowymiarowego rozkładu Gaussa. Szkolimy nasze modele na parafrazach i pokazujemy, że w naturalny sposób uwzględniają one specyfikę zdań. Chociaż proponowany model osiąga ogólnie najlepszą wydajność, pokazujemy również, że specyfikę reprezentują prostsze architektury poprzez normę wektorów zdań. Analiza jakościowa pokazuje, że nasz model prawdopodobieństwa uchwyca zaangażowanie sentencjalne i dostarcza sposobów analizy specyfiki i dokładności poszczególnych słów.</abstract_pl>
      <abstract_ro>Încorporările probabilistice de cuvinte au arătat eficiență în captarea noțiunilor de generalitate și implicare, însă există foarte puține lucrări pentru efectuarea tipului analog de investigare pentru propoziții. În această lucrare definim modele probabilistice care produc distribuții pentru propoziții. Modelul nostru cel mai performant trateaza fiecare cuvant ca un operator de transformare liniara aplicat unei distributii gaussiane multivariate. Ne antrenăm modelele pe parafraze și demonstrăm că acestea captează în mod natural specificitatea propoziției. În timp ce modelul nostru propus obține cea mai bună performanță generală, demonstrăm, de asemenea, că specificitatea este reprezentată de arhitecturi mai simple prin intermediul normei vectorilor de propoziții. Analiza calitativă arată că modelul nostru probabilistic captează implicarea sentimentală și oferă modalități de a analiza specificitatea și precizia cuvintelor individuale.</abstract_ro>
      <abstract_mn>Магадгүй үг нэвтрүүлэх нь ерөнхийлөгч, сэтгэл хөдлөлийн ойлголтын тухай үр дүнтэй байдлыг харуулж байна. Гэхдээ өгүүлбэрт адилхан судалгаа хийх нь маш бага ажил байдаг. Энэ цаасан дээр бид өгүүлбэрийн хуваарилцааны магадлал загварыг тодорхойлж байна. Бидний хамгийн шилдэг загвар нь үг бүр шулуун шилжүүлэлтийн оператор болгон Гауссийн хуваарилцаанд хэрэглэгддэг. Бид загваруудыг параграл дээр суралцаж өгүүлбэрийн тодорхойлолтыг харуулж байна. Бидний санал өгсөн загвар нь хамгийн сайн үйл ажиллагааг бүтээхэд бид мөн тодорхойлолтыг илүү энгийн архитектуруудын хувьд өгүүлбэл векторуудын нормоор илэрхийлэгддэг гэдгийг харуулж байна. Магадгүй боломжтой загвар магадгүй сэтгэл хөдлөлийг барьж, хэн нэгэн үгийг тодорхойлох болон тодорхойлох арга зам гаргадаг гэдгийг харуулж байна.</abstract_mn>
      <abstract_no>Sannsynleg ordinnbygging har vist effektivitet i å henta merknader om generelt og innbygging, men det er veldig lite arbeid på å gjera den analoge type forsøk for setningar. I denne papiret definerer vi sannsynlige modeller som produserer distribusjonar for setningar. Vårt beste utføringsmodul behandler kvar ord som ein lineær transformeringsoperator som er brukt til ein multivariert Gaussisk distribusjon. Vi treng modellen våre på parafraser og demonstrerer at dei naturleg hentar setningsspesifikat. Mens vår foreslått modell når det beste utviklinga i alt, viser vi også at spesifikasjonen er representert av enklare arkitektur gjennom normalen av setningsvektorar. Kvalitativ analyse viser at vår sannsynlige modell får sentensiell innhald og tilbyr måtar å analysera spesifikasjonen og presisjonen av individuelle ord.</abstract_no>
      <abstract_sr>Verovatno su uključene reči pokazale učinkovitost u hvatanju pojma generalnosti i želje, ali nema vrlo malo posla na radi analogne vrste istrage za rečenice. U ovom papiru definišemo verovatne modele koji proizvode distribuciju za rečenice. Naš najbolji model tretira svaku reč kao linearni transformacijski operator koji se primjenjuje na multivarijantu Gausijsku distribuciju. Treniramo naše modele na parafraze i pokazujemo da prirodno uhvate specifičnost kazne. Iako je naš predloženi model postigao najbolji izvršak ukupno, pokazujemo da je specifičnost predstavljena jednostavnijim arhitekturom preko norme vektora rečenice. Kvalitativna analiza pokazuje da naš verovatni model uhvati sentencijalnu želju i pruža načine da analiziramo specifičnost i preciznost pojedinačnih reči.</abstract_sr>
      <abstract_si>සමහර විශ්වාසික වචන සම්බන්ධතාවක් පෙන්වන්නේ සාමාන්‍යතාවක් සහ සැලසුම්බතාවක් සම්බන්ධතාවක් සඳහා ප්‍රශ්නයක් පෙන්වන මේ පත්තරේ අපි විශ්වාස කරන්න පුළුවන් විදිහට විතරයි. අපේ හොඳම ප්‍රමාණයක් හැම වචනයක්ම ලේනියාර් වෙනස් ක්‍රියාකරුවෙක් විදිහට ගෝසියාන් විතරයෙක් වෙනුවෙන්  අපි අපේ මොඩේල්ස් එක ප්‍රශ්නයක් කරනවා සහ ප්‍රශ්නයක් කරනවා ඔවුන් ස්වභාවිකයෙන් වාක්ය විශේෂතා අපේ ප්‍රාර්ථනා කරපු මොඩල් එක්ක හොඳම ප්‍රාර්ථනාවක් ලැබෙනවා නමුත්, අපි පෙන්වන්නේ විශේෂතාවක් වෙක්ටර්ස් වල සාම ප්‍රශ්ණ විශ්ලේෂණය පෙන්වන්නේ අපේ සංභාවිත විශ්වාසිකය මදුල්ය සංභාවිත විශ්ලේෂණය සහ විශ්වාසිත වචන ව</abstract_si>
      <abstract_so>Hadalka warqada ah oo suurtowda ah waxay muujiyaan faa’iido ku leh in la qabsado fikrada guud iyo cilmi la’aanta, laakiin waxaa jira shaqo aad u yar in la sameeyo qaababka baaritaanka ah ee loo baaraandegayo ciqaabaha. Qoraalkan waxaynu ku qornaa tusaalooyin suurtagal ah oo soo saara qayb lagu qaybiyo. Our best-performing model treats each word as a linear transformation operator applied to a multivariate Gaussian distribution.  Tusaalooyinkayada waxaynu ku tababarinnaa fasaxyada, waxaana muujinnaa inay si dabiicadda ah u qabsadaan xuquuq. Inta lagu talo galayo tusaalahayagii uu sameynayo tababarka ugu wanaagsan dhammaan, waxaynu sidoo kale tusnaynaa in gaar ah waxaa lagu muujiyaa dhismaha sahlan ee ku qoran qaababka qaababka xukunka. Shahaadada takhasuska ah waxay muuqataa in qaababkayaga suurtagalka ah uu qabsado qofka aan aqoon lahayn, wuxuuna bixiyaa jidooyin ay ku baaritaan gaarka ah iyo qiimeynta hadalka gaarka ah.</abstract_so>
      <abstract_sv>Probabilistiska ordinbäddningar har visat effektivitet när det gäller att fånga begreppen generalitet och involvering, men det finns mycket lite arbete med att göra den analoga typen av utredning av meningar. I denna uppsats definierar vi sannolikhetsmodeller som producerar fördelningar för meningar. Vår bäst presterande modell behandlar varje ord som en linjär transformationsoperator applicerad på en multivariat gaussisk distribution. Vi tränar våra modeller på parafraser och visar att de naturligt fångar meningsspecificitet. Samtidigt som vår föreslagna modell uppnår bästa prestanda totalt visar vi också att specificitet representeras av enklare arkitekturer via normen för meningsvektorer. Kvalitativ analys visar att vår sannolikhetsmodell fångar känslomässig innebörd och ger sätt att analysera specificiteten och precisionen hos enskilda ord.</abstract_sv>
      <abstract_ta>சாத்தியமான வார்த்தை உள்ளிடுதல் பொதுவான மற்றும் அறிவில்லாத கருத்துக்களை பிடித்துக் கொண்டுள்ளது, ஆனால் வாக்குகளுக்கு விளக்கமான இந்த காகிதத்தில் நாம் சாத்தியமான மாதிரிகளை வரையறுக்கிறோம் வாக்கியங்களுக்கு பங்கிடும். Our best-performing model treats each word as a linear transformation operator applied to a multivariate Gaussian distribution.  நாங்கள் எங்கள் மாதிரிகளை பயிற்சி செய்து வாக்கியத்தின் குறிப்பிட்டு பிடிப்பு காட்டுகிறோம். எங்கள் பரிந்துரைக்கப்பட்ட மாதிரி மொத்தத்தில் சிறந்த செயல்பாடு பெறும் போது, நாம் குறிப்பிட்ட குறிப்பிட்ட சுலபமான கட்டுக் தரமான ஆராய்ச்சி நமது சாத்தியமான மாதிரி உணர்வு அறிவில்லாதவரை பிடித்து தனிப்பட்ட வார்த்தைகளின் குறிப்பிட்ட மற்றும் மதிப்ப</abstract_ta>
      <abstract_ur>احتمالاً کلمات کے مطابق مطابق کلمات کے مطابق فعالیت کے ذریعے دکھائے گئے ہیں، لیکن کلمات کے مطابق برابر تحقیق کی طرح بہت کم کام ہے. ہم اس کاغذ میں احتمالات موڈل کی تعریف کرتے ہیں جو کلمات کے لئے تقسیم کرتی ہیں۔ ہماری بہترین نمونڈل ہر کلم کو linear transformation operator کے طور پر معلوم کرتا ہے جو ایک متعددی گوسی تقسیم پر لازم کیا گیا ہے. ہم اپنی مدلکوں کو پارافریز پر آموزش دیتے ہیں اور دکھاتے ہیں کہ وہ طبیعی طور پر فیصلہ ویژہ پکڑتے ہیں۔ اگرچہ ہماری پیشنهاد کی مدل سب سے بہترین عملکرد حاصل کرتی ہے، ہم بھی دکھاتے ہیں کہ ویکتروں کے معاملہ سے صریح معماری کے ذریعہ مشخص ہے۔ کیلوٹیٹیو تحلیل دکھاتا ہے کہ ہمارے احتمال کی موڈل سنٹیٹیول کے ذریعے پکڑتا ہے اور ایک شخصی کلمات کی خاص اور دقیق تحلیل کرنے کے لئے طریقے پیش کرتا ہے.</abstract_ur>
      <abstract_vi>Sự nhúng tay từ dự đoán đã cho thấy hiệu quả trong việc nắm bắt các khái niệm về định vị và tổng thống, nhưng rất ít việc thực hiện một loại điều tra tương tự về các câu án. Trong tờ giấy này chúng tôi xác định các mô hình hiển nhiên phân chia câu. Mẫu hiệu quả tốt nhất của chúng ta đối xử từng từ như một tổng thể chuyển đổi tuyến được áp dụng cho phân phối kiểu Gaussian. Chúng tôi huấn luyện các mô hình theo cách diễn giải và cho thấy họ chiếm đặc biệt bản án. Trong khi mẫu đề xuất đạt được hiệu suất tốt nhất, chúng tôi cũng cho thấy đặc tính rõ ràng được đại diện bởi các kiến trúc đơn giản thông qua tiêu chuẩn của các môi trường câu chữ. Phân tích chất lượng cho thấy mô hình thử nghiệm của chúng tôi nắm bắt mức án và cung cấp cách phân tích đặc tính và dứt khoát của từng từ.</abstract_vi>
      <abstract_uz>Probabilistic word embeddings have shown effectiveness in capturing notions of generality and entailment, but there is very little work on doing the analogous type of investigation for sentences.  Bu hujjatda biz gapirar uchun tarqatishni aniqlash mumkin. Bizning eng yaxshi bajarish modelimiz har bir so'zlarni chizish operatori ko'paytirish uchun multivariat Gaussiya tarqatishga qo'llangan. Biz modellarimizni paraphraslarda o'rganamiz va ular oddiy soʻzni tayyorlash mumkin. Ko'rib chiqarilgan modelimiz umuman eng yaxshi bajarish imkoniyatini bajarayotganda, biz xususiyatlarni qo'shilgan arxituvlar va gapir vektorining tabiiyasi orqali oddiy ishlatiladi. Qualitatik analytiki esa bizning imkoniyatli o'smirlarimiz o'zaro ilmiyni qabul qiladi va oddiy so'zlarning foydalanishini taʼminlashtirish va foydalanish usullarini beradi.</abstract_uz>
      <abstract_bg>Вероятно вграждането на думи показва ефективност при улавянето на понятията за общост и обвързване, но има много малко работа по извършването на аналогичен тип разследване за изречения. В тази статия дефинираме вероятностни модели, които произвеждат разпределения за изречения. Нашият най-ефективен модел третира всяка дума като линеен трансформационен оператор, приложен към мултивариатно гаусово разпределение. Ние обучаваме нашите модели на парафрази и демонстрираме, че те естествено улавят спецификата на изречението. Докато нашият предложен модел постига най-доброто представяне като цяло, ние също така показваме, че специфичността е представена от по-прости архитектури чрез нормата на векторите на изреченията. Качественият анализ показва, че вероятностният ни модел улавя сентенциалното обвързване и предоставя начини за анализ на спецификата и точността на отделните думи.</abstract_bg>
      <abstract_hr>Vjerojatno su uključivanje riječi pokazalo učinkovitost u uhvaćenju pojma o generalnosti i želji, ali nema vrlo malo posla na obavljanju analognog vrsta istrage za kazne. U ovom papiru definiramo vjerojatni modeli koji proizvode distribuciju za rečenice. Naš najbolji model tretira svaku riječ kao linearni transformacijski operator koji se primjenjuje na multivarijantu Gausijsku distribuciju. Vježbamo naše modele na parafraze i pokazujemo da prirodno uhvate specifičnost kazne. Iako naš predloženi model postigne najbolje učinkovito ukupno, pokazujemo da je specifičnost predstavljena jednostavnijim arhitekturom preko norme vektora kazne. Kvalitativna analiza pokazuje da naš vjerojatni model uhvati sentencijalnu želju i pruža načine za analiziranje specifičnosti i preciznosti pojedinačnih riječi.</abstract_hr>
      <abstract_nl>Waarschijnlijke woordinbeddingen hebben effectiviteit aangetoond bij het vastleggen van begrippen van generaliteit en implicatie, maar er is zeer weinig werk aan het doen van het analoge type onderzoek voor zinnen. In dit artikel definiëren we probabilistische modellen die verdelingen voor zinnen produceren. Ons best presterende model behandelt elk woord als een lineaire transformatieoperator die wordt toegepast op een multivariate Gaussiaanse verdeling. We trainen onze modellen op parafrases en tonen aan dat ze op natuurlijke wijze zinsspecificiteit bevatten. Hoewel ons voorgestelde model over het algemeen de beste prestaties behaalt, laten we ook zien dat specificiteit wordt vertegenwoordigd door eenvoudigere architecturen via de norm van de zinnenvectoren. Kwalitatieve analyse toont aan dat ons probabilistische model sententiële implicaties vastlegt en manieren biedt om de specificiteit en nauwkeurigheid van individuele woorden te analyseren.</abstract_nl>
      <abstract_da>Probabilistiske ordindlejringer har vist effektivitet i at fange begreber om generalitet og involvering, men der er meget lidt arbejde med at gøre den analoge type undersøgelse af sætninger. I denne artikel definerer vi sandsynlighedsmodeller, der producerer fordelinger af sætninger. Vores bedst ydende model behandler hvert ord som en lineær transformationsoperator anvendt på en multivariat gaussisk fordeling. Vi træner vores modeller i parafraser og demonstrerer, at de naturligt fanger sætningsspecifik. Mens vores foreslåede model opnår den bedste ydeevne generelt, viser vi også, at specificitet repræsenteres af enklere arkitekturer via normen for sætningsvektorer. Kvalitativ analyse viser, at vores sandsynlighedsmodel fanger sentimental involvering og giver måder at analysere specificiteten og præcisionen af individuelle ord.</abstract_da>
      <abstract_de>Wahrscheinliche Einbettungen von Wörtern haben gezeigt, dass es effektiv ist, Begriffe von Allgemeinheit und Einbeziehung zu erfassen, aber es gibt sehr wenig Arbeit an der Durchführung der analogen Art der Untersuchung von Sätzen. In diesem Beitrag definieren wir probabilistische Modelle, die Verteilungen für Sätze erzeugen. Unser leistungsstärkstes Modell behandelt jedes Wort als linearen Transformationsoperator, der auf eine multivariate Gaußsche Verteilung angewendet wird. Wir trainieren unsere Modelle auf Paraphrasen und zeigen, dass sie auf natürliche Weise Satzspezifität erfassen. Während unser vorgeschlagenes Modell insgesamt die beste Leistung erzielt, zeigen wir auch, dass Spezifität durch einfachere Architekturen über die Norm der Satzvektoren repräsentiert wird. Die qualitative Analyse zeigt, dass unser probabilistisches Modell sentential implication erfasst und Möglichkeiten bietet, die Spezifität und Genauigkeit einzelner Wörter zu analysieren.</abstract_de>
      <abstract_fa>احتمالاً جمع کردن کلمه‌های احتمالی در دستگیر نظریه‌های ژنرال و اراده‌ای نشان داده‌اند، ولی کار بسیار کمی در انجام دادن نوع تحقیقات آنالگویی برای جمله‌ها وجود دارد. در این کاغذ ما مدل احتمالاتی را تعریف می‌کنیم که توزیع برای جمله‌ها تولید می‌کنند. بهترین مدل عملکرد ما هر کلمه را به عنوان یک عملکرد تغییرات خطی به تقسیم گاوسی متفاوت درمان می‌کند. ما مدل‌هایمان را روی پارافریز آموزش می‌دهیم و نشان می‌دهیم که طبیعتاً ویژه‌های جمله را گرفته‌اند. در حالی که مدل پیشنهاد ما به بهترین عملکرد در کل رسیده است، ما هم نشان می دهیم که مشخصت توسط معماری ساده‌تر از طریق نورمی ویکتورهای جمله نمایش می‌شود. تحلیل کیفیت نشان می دهد که مدل احتمالی ما به وسیله احتمالی احتمالی دستگیر می کند و راه‌هایی برای تحلیل مشخص و دقیق کلمات شخصی را پیشنهاد می کند.</abstract_fa>
      <abstract_sw>Inawezekana kuandika maneno yanaonyesha ufanisi katika kuchukua mawazo ya jumla na wasiofahamu, lakini kuna kazi kidogo sana ya kufanya aina ya uchunguzi kwa ajili ya hukumu. Katika karatasi hii tunaelezea mifano yenye uwezekano ambao hutengeneza usambazaji kwa hukumu. Mfano wetu bora zaidi unakabiliwa na kila neno kama operesheni ya mabadiliko ya msitari imetumika kwa usambazaji wa Gaussia. Tunafundisha mifano yetu kwenye misingi na kuonyesha kwamba kwa uhalisia wanakamata hukumu maalum. Wakati muundo wetu wa pendekezo unafanikiwa ufanisi bora zaidi kwa ujumla, pia tunaonyesha kwamba utaalam unawakilishwa na majengo rahisi kupitia utamaduni wa vectors wa hukumu. Uchambuzi wa kiwango kinaonyesha kuwa mifano yetu inayowezekana inawakamata watu wasiojua hisia na inatoa njia za kuchambua uhakika na ukweli wa maneno binafsi.</abstract_sw>
      <abstract_tr>Muhtemelen sözler içinde jeneral we çykyş düşünjelerini yakalamak üçin etkinlik görkezilýär, ýöne sözler üçin analogy hili soruşturmagy üçin örän kiçi iş bar. Bu kagyzda sözler üçin daýlamak üçin mümkin nusgalary tassyklaýarys. Bizim en iyi şeklimiz, her kelime lineer bir dönüştürme operatörü olarak, bir çok farklı Gaussian dağıtımına uyguladı. Biz nusgalarymyzy parafrazler bilen öwredýäris we özüniň sözleriniň takyklygyny görkezýäris. Teklif edilen modelimiz en iyi performans ulaşırken, biz de bu şekilde çözüm vektörlerinin normaları ile basit bir arhitektür tarafından ifade edildiğini gösteriyoruz. Görkezilişi çözümler, mümkinçilik nusgamyz sentenci daşarylygyny çekip, hatda birnäçe sözlerin takyklygyny we Dyggatyny çözümlemek üçin ýoly saýlaýar.</abstract_tr>
      <abstract_af>Waarskynlik woord inbettings het effektief vertoon in die opvang van notisies van generelheid en aanhouding, maar daar is baie klein werk om die analoge tipe ondersoek vir setinge te doen. In hierdie papier definieer ons waarskynlike modele wat verspreidings vir setinge produseer. Ons beste-uitvoerde model behandel elke woord as 'n lineêre transformasie operator wat op 'n multivariate Gaussian verspreiding aangepas word. Ons tref ons modele op parafrase en wys dat hulle natuurlik setingespesifiekheid opneem. Alhoewel ons voorgestelde model die beste prestasie totaal bereik word, wys ons ook dat spesifiekheid verteenwoordig word deur eenvoudiger arkitektuure deur die norm van die setvektore. Kvalitatiewe analisie vertoon dat ons waarskynlik model sentensiele aanhouding kap en verskaf maniere om die spesifiekheid en presisiteit van individuele woorde te analyseer.</abstract_af>
      <abstract_am>የግንኙነት ቃላት አካባቢዎች የዘረኝነትን እና የውይይት አሳብ ማሳየት ፍላጎታቸውን አሳልፎአል፤ ነገር ግን ለፍርድ የመጠየቅ ምርመራ ዓይነት በጣም ጥቂት ነው፡፡ In this paper we define probabilistic models that produce distributions for sentences.  የመልካም ሞዴል የጋውስሲ አካባቢ የደረጃ ምርጫዎች የጋውስሲ አካባቢ ነው፡፡ ምሳሌዎቻችንን በተለየ ጽሑፎች እናስታውቃለን፡፡ በተዘጋጀው ሞዴላያችን በተሻለጠው የድምፅ አካባቢ ሲደርስ፣ ፍጥረት በተለየው የፍርድ መሠረት አካባቢዎች በኩል የሚታየው ነው ብለን እናሳያቸዋለን፡፡ ጥያቄ ትምህርት የሚያሳየው የእኛ ምሳሌ የግንኙነታችንን ማወቅ እንዲያሳየው የግንኙነት እና የአካል ቃሎችን ውስጥነት እና ውስጥነት ለማስተምር መንገድን እንዲሰጠው ነው፡፡</abstract_am>
      <abstract_ko>확률어는 일반성과 함축성 개념을 포획하는 데 유효성을 보였지만 문장에 대해 유사한 유형을 조사하는 작업은 드물었다.본고에서 우리는 문장의 분포가 발생하는 확률 모델을 정의했다.우리의 성능이 가장 좋은 모델은 모든 단어를 다원 고스 분포에 응용되는 선형 변환 연산자로 간주한다.우리는 우리의 모형에 대해 해석 훈련을 하고 문장의 특수성을 자연스럽게 잡았다는 것을 증명했다.우리가 제시한 모델은 전체적으로 최상의 성능에 도달했지만 문장 벡터의 범수를 통해 더욱 간단한 체계 구조는 특정성을 나타낼 수 있음을 나타냈다.정성분석에 의하면 우리의 확률모델은 문장의 내포를 포착하고 단일 단어의 특이성과 정확성을 분석하는 방법을 제공했다.</abstract_ko>
      <abstract_az>Əlbəttə ki, bənzər sözlər içərisində cümlələr üçün analog təşkil edilməsi üçün çox az iş görünür. Bu kağızda cümlələrə dağıtılmaq üçün mümkün olaraq modelləri təyin edirik. Bizim ən yaxşı modellərimiz hər sözü linear transformasyon operatörü olaraq çoxlu varis Gaussian dağılışına uyğunlaşdırır. Biz modellərimizi parafrazlərə təhsil edirik və onlar doğal olaraq cümlələrin məqsədilə tutduqlarını göstəririk. Bizim təbliğ etdiyimiz modelimiz ən yaxşı performans olaraq başa çatdığımız halda, biz də göstəririk ki, xüsusiyyət, cümlələr vektörlərin normu vasitəsilə daha basit arhitektürlər tarafından göstərildir. Kvalitativ analizi göstərir ki, ehtimallı modellərimiz sentencial hökmünü alır və individual sözlərin nöqsanlığı və nöqsanlığı analizi edə bilər.</abstract_az>
      <abstract_bn>Probabilistic word embeddings have shown effectiveness in capturing notions of generality and entailment, but there is very little work on doing the analogous type of investigation for sentences.  এই কাগজটিতে আমরা সম্ভাব্য মডেল নির্ধারণ করি যা শাস্তির বিতরণ উৎপাদন করে। আমাদের সর্বোত্তম মডেল প্রত্যেকটি শব্দের চিকিৎসা করা হচ্ছে লাইনিয়ার রান্নাল পরিবর্তন অপারেটর হিসেবে একটি বহুভূতিক গাউ আমরা আমাদের মডেল প্যারাফ্রেসের ব্যাপারে প্রশিক্ষণ দেই এবং প্রমাণিত করি যে তারা স্বাভাবিকভাবে বিশেষ বিষয় যখন আমাদের প্রস্তাবিত মডেল সারাজায়গায় সবচেয়ে ভালো প্রদর্শন করে, তখন আমরা দেখাচ্ছি যে বিশেষ কাঠামোর দ্বারা সুস্পষ্ট প্রতিনিধিত্ব কর বৈশিষ্ট্য বিশ্লেষণ দেখাচ্ছে যে আমাদের সম্ভাব্য মডেল অনেক বিজ্ঞানীকে গ্রেফতার করে এবং ব্যক্তিগত শব্দের বিশেষ বিশেষ এবং মূল</abstract_bn>
      <abstract_bs>Vjerojatno su uključene riječi pokazale učinkovitost u hvatanju pojma generalnosti i želje, ali nema vrlo malo posla na radi analogne vrste istrage za kazne. U ovom papiru definišemo verovatne modele koji proizvode distribuciju za rečenice. Naš najbolji model tretira svaku riječ kao linearni transformacijski operator koji se primjenjuje na multivarijantu Gausijsku distribuciju. Treniramo naše modele na parafraze i pokazujemo da prirodno uhvate specifičnost kazne. Iako naš predloženi model postigne najbolje izvršenje ukupno, također pokazujemo da je specifičnost predstavljena jednostavnijim arhitekturom preko norme vektora kazne. Kvalitativna analiza pokazuje da naš vjerojatni model uhvati sentencijalnu želju i pruža načine za analizu specifičnosti i preciznosti pojedinačnih riječi.</abstract_bs>
      <abstract_hy>Հավանական է, որ բառերի ներգրավումը ցույց է տալիս արդյունավետություն ընդհանուր և ներգրավման գաղափարների ընկալում, բայց շատ քիչ աշխատանք կա նախադասությունների նմանատիպ հետազոտության կատարման վրա: Այս թղթի մեջ մենք սահմանում ենք հավանական մոդելներ, որոնք արտադրում են բաշխման նախադասությունների համար: Մեր ամենաարդյունավետ մոդելը յուրաքանչյուր բառ վերաբերում է որպես գծային վերափոխման օպերատոր, որը կիրառվում է բազմատարբեր գաուսացի տարածման վրա: Մենք վարժեցնում ենք մեր մոդելները պարաֆրեզների վրա և ցույց ենք տալիս, որ դրանք բնական կերպ են ընդունում նախադասությունների հատուկ հատկությունը: While our proposed model achieves the best performance overall, we also show that specificity is represented by simpler architectures via the norm of the sentence vectors.  Qualitative analysis shows that our probabilistic model captures sentential entailment and provides ways to analyze the specificity and preciseness of individual words.</abstract_hy>
      <abstract_id>Probabilistic word embeddings have shown effectiveness in capturing notions of generality and entailment, but there is very little work on doing the analogous type of investigation for sentences.  Dalam kertas ini kita mendefinisikan model probabilis yang menghasilkan distribusi untuk kalimat. Model terbaik kami memperlakukan setiap kata sebagai operator transformasi linear yang diaplikasikan pada distribusi Gaussi multivariasi. We train our models on paraphrases and demonstrate that they naturally capture sentence specificity.  While our proposed model achieves the best performance overall, we also show that specificity is represented by simpler architectures via the norm of the sentence vectors.  Qualitative analysis shows that our probabilistic model captures sentential entailment and provides ways to analyze the specificity and preciseness of individual words.</abstract_id>
      <abstract_ca>L'incorporació probable de paraules ha demostrat eficacia en capturar nocions de generalitat i implicació, però hi ha molt poc treball en fer el tipus analògic d'investigació per a les frases. En aquest paper definim models probabilistes que produeixen distribucions per a frases. El nostre model de millor rendiment tracta cada paraula com un operador de transformació linear aplicat a una distribució gaussia multivariada. Ensenyem els nostres models en parafrases i demostrem que capturen naturalment la especificitat de frases. Mentre el nostre model proposat aconsegueix el millor rendiment en general, també demostrem que la especificitat és representada per arquitectures més senzilles a través de la norma dels vectors de frases. L'anàlisi qualitativa mostra que el nostre model probabilista captura l'involucració sentencial i proporciona maneres d'analitzar l'especificitat i la precisió de les paraules individuals.</abstract_ca>
      <abstract_cs>Pravděpodobné vkládání slov ukázalo účinnost při zachycování pojmů obecnosti a implikace, ale je velmi málo práce na provádění analogického typu vyšetřování vět. V tomto článku definujeme pravděpodobnostní modely, které produkují rozdělení vět. Náš nejlépe výkonný model považuje každé slovo za lineární transformační operátor aplikovaný na vícerozměrné Gaussovo rozdělení. Naše modely trénujeme na parafrázích a demonstrujeme, že přirozeně zachycují specificitu věty. Zatímco náš navržený model dosahuje celkově nejlepšího výkonu, ukazujeme také, že specificita je reprezentována jednoduššími architekturami prostřednictvím normy větových vektorů. Kvalitativní analýza ukazuje, že náš pravděpodobnostní model zachycuje sententiální implikaci a poskytuje způsoby analýzy specificity a přesnosti jednotlivých slov.</abstract_cs>
      <abstract_et>Tõenäoliselt on sõnade manustamine näidanud tõhusust üldiste ja kaasnevate mõistete tabamisel, kuid lausete analoogse uurimise tegemisel on väga vähe tööd. Selles töös määratleme tõenäosusmudelid, mis toodavad jaotusi lausetele. Meie parim mudel käsitleb iga sõna lineaarse transformatsiooni operaatorina, mida rakendatakse mitme muutujaga Gaussi jaotusele. Me treenime oma mudeleid parafraasidel ja näitame, et need loomulikult kajastavad lause spetsiifilisust. Kuigi meie pakutud mudel saavutab parima tulemuse üldiselt, näitame ka, et spetsiifilisust esindavad lihtsamad arhitektuurid lausevektorite normi kaudu. Kvalitatiivne analüüs näitab, et meie tõenäosusmudel kajastab sentimentaalset seotust ja pakub võimalusi analüüsida üksikute sõnade spetsiifilisust ja täpsust.</abstract_et>
      <abstract_fi>Todennäköiset sanaupotukset ovat osoittaneet tehokkuutta yleistymisen ja osallisuuden käsitteiden vangitsemisessa, mutta samankaltaisen tutkimuksen tekemisessä lauseille on hyvin vähän työtä. Tässä työssä määritellään todennäköisyysmalleja, jotka tuottavat lauseiden jakautumista. Parhaiten suorituskykyinen mallimme käsittelee jokaista sanaa lineaarisena muunnosoperaattorina, jota sovelletaan monimuuttujaan Gaussin jakeluun. Koulutamme mallimme parafraaseihin ja osoitamme, että ne luonnollisesti vangitsevat lausespesifisyyden. Vaikka ehdotettu malli saavuttaa parhaan suorituskyvyn kokonaisuutena, osoitamme myös, että spesifisyyttä edustavat yksinkertaisemmat arkkitehtuurit lausevektorien normin kautta. Laadullinen analyysi osoittaa, että todennäköisyysmallimme tallentaa sententiaalisen yhteyden ja tarjoaa tapoja analysoida yksittäisten sanojen spesifisyyttä ja tarkkuutta.</abstract_fi>
      <abstract_sq>Ndoshta përfshirjet e fjalëve kanë treguar efektshmëri në kapjen e koncepteve të gjeneralitetit dhe përfshirjes, por ka shumë pak punë në kryerjen e llojit analog të hetimit për dënime. Në këtë letër ne përcaktojmë modele probabiliste që prodhojnë shpërndarje për fraza. Modeli ynë më i mirë trajton çdo fjalë si një operator transformimi linear aplikuar në një shpërndarje multivariate Gausi. Ne trajnojmë modelet tona në parafraza dhe demonstrojmë se ato natyralisht kapin specificitetin e fjalëve. While our proposed model achieves the best performance overall, we also show that specificity is represented by simpler architectures via the norm of the sentence vectors.  Analiza kualitative tregon se modeli ynë probabilist kap përfshirjen e dënimit dhe ofron mënyra për të analizuar specificitetin dhe saktësinë e fjalëve individuale.</abstract_sq>
      <abstract_ha>Ana yiwuwa da aka shigar da magana sun nuna mafiya amfani ga fansar idãnun jama'a da masu da sani, kuma amma yana da aiki kaɗan a kan aikin jarrabi-nau'in zartarwa wa wa'anarnin. Ga wannan takardan da Muke ƙayyade misãlai masu yiwuwa da za'a gaura rabon zuwa gargaɗin. Our best-performing model treats each word as a linear transformation operator applied to a multivariate Gaussian distribution.  Tuna sanar da misalinmu kan parameteri kuma Muke nuna cewa, suna kãma cewa a kangara. A lokacin da misalinmu ke sami mafi kyaun aikin aiki a gaba ɗaya, ko kuma munã nuna cewa yana mai ƙayyade taskõki masu sauƙi daga matsayin masu sauki a tsakanin shiryarwa. Ana ƙayyade sifatiti yana nũna cewa misalinmu mai yiwuwa yana kãma ma'anar masu da aka sani kuma yana da hanyõyi dõmin ya yi analyi ga haske da inganci na maganar guda.</abstract_ha>
      <abstract_jv>embedding Nang kuwi iki, kita diwis ngerasakno model sing gawe nggawe Distribution kanggo Kemerdekaan luwih Kernel Awak dhéwé éntuk modellé awak dhéwé paraFranêt lan uwong ngomong nik kabèh perusahaan winih Alpha" for "a", "Bravo" for "b Ndeleksyon kaliterasi maneh diumbane gambar kuwi model sanal sing gawe nguasai angat seneng pisan karo perusahaan kanggo anara perspekaan karo perusahaan kanggo didalakno</abstract_jv>
      <abstract_sk>Verjetno so bile vključene besede učinkovite pri zajemanju pojmov splošnosti in vključenosti, vendar je zelo malo dela pri izvajanju podobne vrste preiskave stavkov. V prispevku smo opredelili verjetnostne modele, ki proizvajajo porazdelitve stavkov. Naš najboljši model obravnava vsako besedo kot linearnega transformacijskega operaterja, ki se uporablja za multivariatno gaussijsko porazdelitev. Naše modele treniramo na parafrazah in dokazujemo, da naravno zajemajo specifičnost stavka. Medtem ko naš predlagani model na splošno dosega najboljšo zmogljivost, pokažemo tudi, da specifičnost predstavljajo preprostejše arhitekture preko norme vektorjev stavkov. Kvalitativna analiza kaže, da naš verjetnostni model zajema sententialno vključenost in ponuja načine analize specifičnosti in natančnosti posameznih besed.</abstract_sk>
      <abstract_he>כנראה שהקישורים של מילים הראו יעילות בכניסה לתפוס רעיונות של גנרליות ומעורבות, אבל יש מעט מאוד עבודה על לעשות סוג אנלוגי של חקירה למשפטים. בעיתון הזה אנחנו מגדירים דוגמנים סבירותיים שמוצרים פיצוצים למשפטים. המודל הכי מוצלח שלנו מתייחס לכל מילה כמפעיל שינוי לינרי שמשתמש לפיתוח גאוסי רב-שונה. אנו מאמן את הדוגמנים שלנו על פראפרסמות ומוכיחים שהם באופן טבעי לתפוס ספציפיות משפטים. בעוד המודל המוצע שלנו משיג את ההופעה הטובה ביותר באופן כללי, אנחנו גם מראים שהספציפיות מייצגת על ידי ארכיטקטורות פשוטות יותר באמצעות נורמה של ווקטורי המשפט. ניתוח איכותי מראה שהמודל הסבירותי שלנו תופס התערבות משפטית ומספק דרכים לנתח את המיוחד והמדויק של מילים בודדות.</abstract_he>
      <abstract_bo>ཆེས་མཉམ་གྱི་ནང་དུ་འཇུག ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོས་ཚོར་མང་ཆོས་ཉིད་ཀྱི་རྣམ་པ་ལ་ངོས་འཛིན་བྱེད་པའི་རྒྱུ་རྐྱེན་ལ་ངོས་འཛ ང་ཚོའི་མ་དབྱིབས་མང་ཤོས་བྱས་པའི་རྣམ་གྲངས་རེ་རེར་བཞིན་པའི་འགྱུར་བ་སྤྱོད་མཁན་གྱི་རྩོམ་པ་ཞིག་ལ་ཉེན་སྤྱོད་ ང་ཚོས་རང་གི་མིག་ལམ་ལུགས་གཟུགས་རིས་ལ་བསླབས་བྱས་ནས་རང་ཉིད་ཀྱིས་ཡིག་གེ་བཤད་ཀྱི་ཡོད། ང་ཚོའི་འཆར་བཀོད་པའི་མ་དབུགས་སྔར་སྒྲིག་གི་རྐྱེན་ཚད་གང་ཡོད་པ་དེ་ཡིན་ནའང་ངེད་ཚོས་གསལ་བཤད་བྱས་ནི་དབུས་གཞུ རང་ཉིད་ཀྱི་ལས་འཚོལ་བ་ཡིན་པའི་དབྱེ་ཞིབ་དཔྱད་ནི་ང་ཚོའི་རྐྱེན་ཚད་མང་ཙམ་འཛིན་བྱེད་ཀྱི་ཡོད་པ་དང་།</abstract_bo>
      <abstract_fil>Mga probabilistic word embeddings ay nagpakita ng effectiveness sa pagkuha ng mga notisyon ng generelity at entailment, nguni't may totoong maliit na trabaho sa paggawa ng analogong uri ng pagsusulit para sa mga sentasyon. Sa papiro na ito ay inilalagay namin ang probabilistic models na nagbubuhat ng distribution para sa mga sentencia. Ang pinakamainam na modelo namin ay nagtataglay ng bawat salita bilang isang linear transformation operator na ginagamit sa multivariate Gaussian distribution. Nagtuturo kami ng aming mga modelo sa parafrases at nagpapakita na sila'y nararapat nakakuha ng pagkatalaga ng sentence. Samantalang ang aming inihahanap na model ay nagtataglay ng pinakamainam na performance sa buong paraan, ipinakikita din namin na ang specificity ay represented by simpler architectures via the norm of the sentence vectors. Ang kalitateniwang analisi ay nagpapakita na ang aming probabilistic model ay kumukuha ng sentencial entailment at nagbibigay ng mga paraan para analisan ang pagkatalaga at pagkahalaga ng mga salita.</abstract_fil>
      </paper>
    <paper id="4">
      <title>Word Embeddings as Tuples of Feature Probabilities</title>
      <author><first>Siddharth</first><last>Bhat</last></author>
      <author><first>Alok</first><last>Debnath</last></author>
      <author><first>Souvik</first><last>Banerjee</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>24–33</pages>
      <abstract>In this paper, we provide an alternate perspective on <a href="https://en.wikipedia.org/wiki/Word_(group_theory)">word representations</a>, by reinterpreting the dimensions of the vector space of a <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> as a collection of <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>. In this reinterpretation, every component of the word vector is normalized against all the word vectors in the vocabulary. This idea now allows us to view each vector as an n-tuple (akin to a fuzzy set), where n is the dimensionality of the word representation and each element represents the probability of the word possessing a <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">feature</a>. Indeed, this representation enables the use fuzzy set theoretic operations, such as <a href="https://en.wikipedia.org/wiki/Union_(set_theory)">union</a>, <a href="https://en.wikipedia.org/wiki/Intersection_(set_theory)">intersection</a> and <a href="https://en.wikipedia.org/wiki/Subtraction">difference</a>. Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements. We compare the performance of this representation with various <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a>, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations.<tex-math>n</tex-math>-tuple (akin to a fuzzy set), where <tex-math>n</tex-math> is the dimensionality of the word representation and each element represents the probability of the word possessing a feature. Indeed, this representation enables the use fuzzy set theoretic operations, such as union, intersection and difference. Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements. We compare the performance of this representation with various benchmarks, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations.</abstract>
      <url hash="12eb4a8f">2020.repl4nlp-1.4</url>
      <doi>10.18653/v1/2020.repl4nlp-1.4</doi>
      <video href="http://slideslive.com/38929770" />
      <bibkey>bhat-etal-2020-word</bibkey>
    <title_ar>تضمين كلمة على أنها مجموعات من احتمالات الميزة</title_ar>
      <title_fr>Les intégrations de mots en tant que tuples de probabilités de caractéristiques</title_fr>
      <title_pt>Incorporações de palavras como tuplas de probabilidades de recursos</title_pt>
      <title_es>Incrustaciones de palabras como tuplas de probabilidades de características</title_es>
      <title_ja>フィーチャーの確率のタプルとしてのWord埋め込み</title_ja>
      <title_hi>Word एम्बेडिंग सुविधा संभावनाओं के Tuples के रूप में</title_hi>
      <title_zh>为特征概率元组词嵌之</title_zh>
      <title_ru>Встраивание слов в виде кортежей вероятностей признаков</title_ru>
      <title_ukr>Вбудовування слів як кортежі ймовірностей особливостей</title_ukr>
      <title_ga>Leabú Focal mar Thuplaí de Dhóchúlachtaí Gné</title_ga>
      <title_ka>სიტყვების შესაძლებლობების სიტყვები როგორც სიტყვების შესაძლებლობა</title_ka>
      <title_isl>Orðbætingar sem flokkar líkur á eiginleikum</title_isl>
      <title_hu>Szóbeágyazások, mint a funkciók valószínűségeinek tupletei</title_hu>
      <title_el>Ενσωματώσεις λέξεων ως Tuples πιθανών χαρακτηριστικών</title_el>
      <title_it>Embedding delle parole come Tuples of Feature Probabilities</title_it>
      <title_kk>Сөздің ендіру мүмкіндіктерінің мәліметтері ретінде</title_kk>
      <title_lt>žodžių įterpimas kaip galimų požymių vamzdeliai</title_lt>
      <title_mk>Вклучување на зборови како купки на веројатности</title_mk>
      <title_ms>Name</title_ms>
      <title_ml>വാക്ക് എംബെഡിങ്ങുകള്‍ സാധ്യതകളുടെ ടൂപ്ലുകളായി വാക്കുകള്‍</title_ml>
      <title_mt>L-inkorporazzjoni tal-kliem bħala Tuples of Feature Probabilities</title_mt>
      <title_mn>Үг нэвтрүүлэх боломжуудын Tuples of Feature Probabilities</title_mn>
      <title_no>Innebygging av ord som eksemplar av funksjonssannsynlighetar</title_no>
      <title_pl>Wkładanie słów jako Tuples prawdopodobieństw funkcji</title_pl>
      <title_ro>Încorporarea cuvintelor ca Tuples de probabilități ale caracteristicilor</title_ro>
      <title_sr>Времена речи као Вероватности функција</title_sr>
      <title_so>Heeganka hadalka sida tusaale ahaan suurtagalka ah</title_so>
      <title_si>Name</title_si>
      <title_sv>Ordinbäddningar som bitar av funktionssannolikheter</title_sv>
      <title_ta>வார்த்தை உட்பொதிக்கும் பண்புகளின் சிக்கல்களாக</title_ta>
      <title_ur>ویڈ انڈینگ کے طور پر ویڈ انڈینگ</title_ur>
      <title_uz>Comment</title_uz>
      <title_vi>Thiết lập chữ làm ống kính dự tính</title_vi>
      <title_nl>Woordinsluitingen als Tuples van Feature Waarschijnlijkheid</title_nl>
      <title_da>Word Embeddings as Tuples of Feature Sandsynligheder</title_da>
      <title_bg>Вградвания на думи като двойки от вероятности за характеристика</title_bg>
      <title_hr>Usporavanje riječi kao vrpce mogućnosti karaktera</title_hr>
      <title_ko>특징 확률 원조로서의 단어 삽입</title_ko>
      <title_de>Wort-Einbettungen als Tupel von Feature-Wahrscheinlichkeiten</title_de>
      <title_id>Pencampuran kata sebagai Tuples of Feature Probabilities</title_id>
      <title_fa>وارد کردن کلمات به عنوان مثال احتمالات ویژه</title_fa>
      <title_tr>Mahsuratyň Mumkinçiliki Däpli Sözler</title_tr>
      <title_sw>Matokeo ya Matokeo kama Tuzo ya Tafadhali</title_sw>
      <title_af>Woord Inbêding as Tuples van Funksie Moontlikheid</title_af>
      <title_sq>Përfshirja e fjalëve si Tuples of Feature Probabilities</title_sq>
      <title_am>የፊደል ቅርጽ ምርጫዎች</title_am>
      <title_hy>Բառերի ներգրավումը որպես առանձնահատկությունների խումբ</title_hy>
      <title_az>S칬zl칲k 캻fad톛l톛ri 칐l칞칲s칲 M칲mk칲nl칲kl톛ri kimi S칬zl칲k 캻fad톛l톛ri</title_az>
      <title_bn>বৈশিষ্ট্য সম্ভাবনার টাইপল হিসেবে শব্দ বিবরণ</title_bn>
      <title_bs>Uklapanje riječi kao vrpce mogućnosti karaktera</title_bs>
      <title_ca>Incorporació de paraules com tubles de probabilitats de característiques</title_ca>
      <title_cs>Vložení slov jako Tuples pravděpodobností funkcí</title_cs>
      <title_et>Sõnade põimimised funktsioonide tõenäosuste tuplitena</title_et>
      <title_fi>Sanaupotukset ominaisuustodennĂ¤kĂ¶isyyksien tupleina</title_fi>
      <title_ha>@ action</title_ha>
      <title_he>קידום מילים ככפולות של סבירות תכונות</title_he>
      <title_sk>Vgradnje besed kot vzorci verjetnosti lastnosti</title_sk>
      <title_fil>Ang mga salitang imbedding bilang Tuples of Feature Probabilities</title_fil>
      <title_bo>ཆོས་ཆོས་ལུགས་ཀྱི་དཔེ་དབྱིབས་སྔོན་སྒྲིག་ཀྱི་ཡིག་ཚན་བསྡུས་པ</title_bo>
      <title_jv>structural navigation</title_jv>
      <abstract_ar>في هذه الورقة ، نقدم منظورًا بديلاً لتمثيل الكلمات ، من خلال إعادة تفسير أبعاد فضاء متجه لتضمين كلمة كمجموعة من الميزات. في إعادة التفسير هذه ، يتم تطبيع كل مكون من متجه الكلمات مقابل جميع متجهات الكلمات في المفردات. تتيح لنا هذه الفكرة الآن عرض كل متجه على أنه n-tuple (على غرار مجموعة ضبابية) ، حيث n هي أبعاد تمثيل الكلمة ويمثل كل عنصر احتمال امتلاك الكلمة لميزة. في الواقع ، يتيح هذا التمثيل استخدام العمليات النظرية لمجموعة ضبابية ، مثل الاتحاد والتقاطع والاختلاف. على عكس المحاولات السابقة ، نظهر أن تمثيل الكلمات هذا يوفر فكرة تشابه غير متماثلة بطبيعتها وبالتالي فهي أقرب إلى أحكام التشابه البشري. نحن نقارن أداء هذا التمثيل بمعايير مختلفة ، ونستكشف بعض الخصائص الفريدة بما في ذلك اكتشاف الكلمات الوظيفية ، واكتشاف الكلمات متعددة المعاني ، وبعض التبصر في التفسير الذي توفره العمليات النظرية المحددة.</abstract_ar>
      <abstract_pt>Neste artigo, fornecemos uma perspectiva alternativa sobre representações de palavras, reinterpretando as dimensões do espaço vetorial de uma incorporação de palavras como uma coleção de recursos. Nesta reinterpretação, cada componente do vetor de palavras é normalizado em relação a todos os vetores de palavras no vocabulário. Essa ideia agora nos permite ver cada vetor como uma n-tupla (semelhante a um conjunto fuzzy), onde n é a dimensionalidade da representação da palavra e cada elemento representa a probabilidade da palavra possuir uma característica. De fato, essa representação possibilita o uso de operações teóricas de conjuntos fuzzy, como união, interseção e diferença. Ao contrário de tentativas anteriores, mostramos que essa representação de palavras fornece uma noção de semelhança inerentemente assimétrica e, portanto, mais próxima dos julgamentos de semelhança humanos. Comparamos o desempenho dessa representação com vários benchmarks e exploramos algumas das propriedades exclusivas, incluindo detecção de palavras funcionais, detecção de palavras polissêmicas e algumas informações sobre a interpretabilidade fornecida pelas operações teóricas de conjuntos.</abstract_pt>
      <abstract_fr>Dans cet article, nous proposons une perspective alternative sur les représentations de mots, en réinterprétant les dimensions de l'espace vectoriel d'un incorporation de mots comme un ensemble de caractéristiques. Dans cette réinterprétation, chaque composant du vecteur de mot est normalisé par rapport à tous les vecteurs de mots du vocabulaire. Cette idée nous permet maintenant de voir chaque vecteur comme un n-uplet (similaire à un ensemble flou), où n est la dimensionnalité de la représentation du mot et chaque élément représente la probabilité que le mot possède une caractéristique. En effet, cette représentation permet d'utiliser des opérations théoriques d'ensembles flous, telles que l'union, l'intersection et la différence. Contrairement aux tentatives précédentes, nous montrons que cette représentation des mots fournit une notion de similitude qui est intrinsèquement asymétrique et donc plus proche des jugements de similitude humaine. Nous comparons les performances de cette représentation avec divers points de référence et explorons certaines des propriétés uniques, notamment la détection de mots de fonction, la détection de mots polysémiques et un aperçu de l'interprétabilité fournie par les opérations théoriques des ensembles.</abstract_fr>
      <abstract_es>En este artículo, ofrecemos una perspectiva alternativa sobre las representaciones de palabras, reinterpretando las dimensiones del espacio vectorial de una palabra incrustada como una colección de características. En esta reinterpretación, cada componente del vector de palabras se normaliza con respecto a todos los vectores de palabras del vocabulario. Esta idea ahora nos permite ver cada vector como una n-tupla (similar a un conjunto difuso), donde n es la dimensionalidad de la representación de la palabra y cada elemento representa la probabilidad de que la palabra posea una característica. De hecho, esta representación permite el uso de operaciones teóricas de conjuntos difusos, como unión, intersección y diferencia. A diferencia de intentos anteriores, demostramos que esta representación de palabras proporciona una noción de similitud que es inherentemente asimétrica y, por lo tanto, más cercana a los juicios de similitud humana. Comparamos el rendimiento de esta representación con varios puntos de referencia y exploramos algunas de las propiedades únicas, incluida la detección de palabras de función, la detección de palabras polisémicas y algunos conocimientos sobre la interpretabilidad que proporcionan las operaciones teóricas de conjuntos.</abstract_es>
      <abstract_ja>本稿では、単語埋め込みのベクトル空間の次元を特徴の集合として再解釈することにより、単語表現に関する別の視点を提供する。 この再解釈では、単語ベクトルのすべての成分は、語彙内のすべての単語ベクトルに対して正規化される。 このアイデアにより、各ベクトルをnタプル（曖昧な集合に似ている）と見なすことができます。ここで、nは単語表現の次元数であり、各要素は、特徴を持つ単語の確率を表します。 実際、この表現は、結合、交差、差分などのファジィ集合論的演算の使用を可能にする。 これまでの試みとは異なり、この言葉の表現は、本質的に非対称であり、したがって人間の類似性の判断に近い類似性の概念を提供することを示している。 この表現のパフォーマンスを様々なベンチマークと比較し、機能単語の検出、多義的単語の検出、およびセット理論的操作によって提供される解釈可能性に関するいくつかの洞察を含む固有の特性のいくつかを探索します。</abstract_ja>
      <abstract_zh>本文中,重解向量空维度为特徵所集,供单词一视角。 词向量者,词汇之所有词向量规范化。 今许以向量为一n元组(类模糊集),其n单词之维数,元素单词有征概率。 实者,使能用模糊集论运算,如并集、交集、差分。 异乎前,单词有相似性名,非其名也,近人之相似性也。 校其性,原其独,包其功能,多义词其检测,与集论可解释性之见。</abstract_zh>
      <abstract_ru>В этой статье мы представляем альтернативную точку зрения на представления слов, переосмысливая размеры векторного пространства вложения слов как совокупность признаков. В этой повторной интерпретации каждый компонент слова-вектора нормируется относительно всех слов-векторов в словаре. Эта идея теперь позволяет рассматривать каждый вектор как n-кортеж (сродни нечеткому множеству), где n - размерность представления слова и каждый элемент представляет вероятность того, что слово обладает признаком. Действительно, это представление позволяет использовать теоретические операции нечеткого множества, такие как объединение, пересечение и разность. В отличие от предыдущих попыток, мы показываем, что это представление слов обеспечивает понятие сходства, которое по своей сути асимметрично и, следовательно, ближе к суждениям о человеческом сходстве. Мы сравниваем производительность этого представления с различными критериями и исследуем некоторые уникальные свойства, включая обнаружение функциональных слов, обнаружение многозначных слов и некоторое понимание интерпретируемости, обеспечиваемой множеством теоретических операций.</abstract_ru>
      <abstract_hi>इस पेपर में, हम शब्दों के प्रतिनिधित्व पर एक वैकल्पिक परिप्रेक्ष्य प्रदान करते हैं, सुविधाओं के संग्रह के रूप में एम्बेडिंग शब्द के वेक्टर स्पेस के आयामों को फिर से व्याख्या करके। इस पुनर्व्याख्या में, शब्द वेक्टर के प्रत्येक घटक को शब्दावली में सभी शब्द वैक्टरों के खिलाफ सामान्यीकृत किया जाता है। यह विचार अब हमें प्रत्येक वेक्टर को एन-टुपल (एक फजी सेट के समान) के रूप में देखने की अनुमति देता है, जहां एन शब्द प्रतिनिधित्व की आयामीता है और प्रत्येक तत्व एक विशेषता रखने वाले शब्द की संभावना का प्रतिनिधित्व करता है। दरअसल, यह प्रतिनिधित्व फजी सेट सैद्धांतिक संचालन, जैसे संघ, प्रतिच्छेदन और अंतर के उपयोग को सक्षम बनाता है। पिछले प्रयासों के विपरीत, हम दिखाते हैं कि शब्दों का यह प्रतिनिधित्व समानता की धारणा प्रदान करता है जो स्वाभाविक रूप से असममित है और इसलिए मानव समानता के फैसले के करीब है। हम विभिन्न बेंचमार्क के साथ इस प्रतिनिधित्व के प्रदर्शन की तुलना करते हैं, और फ़ंक्शन वर्ड डिटेक्शन, पॉलीसेमस शब्दों का पता लगाने और सेट सैद्धांतिक संचालन द्वारा प्रदान की गई व्याख्याक्षमता में कुछ अंतर्दृष्टि सहित कुछ अद्वितीय गुणों का पता लगाते हैं।</abstract_hi>
      <abstract_ukr>У цій роботі ми надаємо альтернативний погляд на представлення слів, перетлумачивши розміри векторного простору вбудовування слова як набір ознак. У цій інтерпретації кожна складова вектора слів нормалізується проти всіх векторів слів у словнику. Ця ідея тепер дозволяє нам розглядати кожен вектор як n-кортеж (схожий на нечітку множину), де n - розмірність представлення слова, і кожен елемент представляє ймовірність того, що слово володіє ознакою. Дійсно, це представлення дозволяє використовувати теоретичні операції нечіткої множини, такі як об 'єднання, перетин і різниця. На відміну від попередніх спроб, ми показуємо, що це представлення слів забезпечує поняття подібності, яке за своєю суттю є асиметричним і, отже, ближчим до суджень про подібність людини. Ми порівнюємо продуктивність цього представлення з різними орієнтирами та досліджуємо деякі унікальні властивості, включаючи виявлення функціональних слів, виявлення полісемантичних слів та деяке розуміння інтерпретабельності, що забезпечується заданими теоретичними операціями.</abstract_ukr>
      <abstract_ga>Sa pháipéar seo, cuirimid peirspictíocht mhalartach ar fáil ar léirithe focal, trí thoisí an spáis veicteora a bhaineann le neadú focal a athléiriú mar bhailiúchán gnéithe. San athléiriú seo, normalaítear gach comhpháirt den fhocal veicteoir i gcoinne gach veicteoir focal sa stór focal. Ligeann an smaoineamh seo dúinn anois féachaint ar gach veicteoir mar n-tuple (cosúil le tacair doiléir), áit arb é n toiseachas léiriú an fhocail agus seasann gach eilimint don dóchúlacht go bhfuil gné ag an bhfocal. Go deimhin, cuireann an léiriú seo ar chumas oibríochtaí teoiriciúla tacair doiléir a úsáid, amhail aontas, trasnú agus difríocht. Murab ionann agus iarrachtaí roimhe seo, léirímid go soláthraíonn an léiriú seo ar fhocail nóisean cosúlachta atá neamhshiméadrach ó dhúchas agus mar sin níos gaire do bhreithiúnais chosúlachta daonna. Déanaimid comparáid idir feidhmíocht na hionadaíochta seo agus tagarmharcanna éagsúla, agus déanaimid iniúchadh ar roinnt de na hairíonna uathúla lena n-áirítear brath focal feidhme, braite focail ilghnéitheacha, agus roinnt léargas ar an inléiritheacht a sholáthraíonn oibríochtaí teoiriciúla socraithe.</abstract_ga>
      <abstract_el>Στην παρούσα εργασία, παρέχουμε μια εναλλακτική προοπτική για τις αναπαραστάσεις λέξεων, επανερμηνεύοντας τις διαστάσεις του διανυσματικού χώρου μιας λέξης που ενσωματώνεται ως συλλογή χαρακτηριστικών. Σε αυτή την επανερμηνεία, κάθε συστατικό του διανύσματος της λέξης κανονικοποιείται σε σχέση με όλα τα διανύσματα λέξεων στο λεξιλόγιο. Αυτή η ιδέα μας επιτρέπει τώρα να δούμε κάθε διάνυσμα ως ένα N-Tuple (παρόμοιο με ένα ασαφές σύνολο), όπου n είναι η διαστασιμότητα της λέξης αναπαράσταση και κάθε στοιχείο αντιπροσωπεύει την πιθανότητα της λέξης να κατέχει ένα χαρακτηριστικό. Πράγματι, αυτή η αναπαράσταση επιτρέπει τη χρήση θεωρητικών λειτουργιών ασαφής συνόλου, όπως η ένωση, η τομή και η διαφορά. Σε αντίθεση με προηγούμενες προσπάθειες, δείχνουμε ότι αυτή η αναπαράσταση των λέξεων παρέχει μια έννοια ομοιότητας η οποία είναι εγγενώς ασύμμετρη και ως εκ τούτου πιο κοντά στις ανθρώπινες εκτιμήσεις ομοιότητας. Συγκρίνουμε την απόδοση αυτής της αναπαράστασης με διάφορα κριτήρια αναφοράς, και εξερευνούμε μερικές από τις μοναδικές ιδιότητες, συμπεριλαμβανομένης της ανίχνευσης λέξεων συνάρτησης, της ανίχνευσης πολυσυναισθηματικών λέξεων, και κάποια κατανόηση της ερμηνείας που παρέχεται από τις θεωρητικές λειτουργίες συνόλων.</abstract_el>
      <abstract_isl>Í þessu pappíri gefum við annað horn á orðmyndir með því a ð endurteka stærð vektursvæðis orðsins sem safn eiginleika. In this reinterpretation, every component of the word vector is normalized against all the word vectors in the vocabulary.  Þessi hugmynd leyfir okkur n ú a ð líta á hvern vektur sem n-tvöfalt (svipað við fuzzy sett), þar sem n er stærð orðstjórnunar og hvert hlut er líkur á að orðið eigi eiginleika. Þessi mynd gerir reyndar möguleika á notkun fuzzy set theoretical aðgerða, svo sem sameiningu, millistykki og munur. Í mismunandi lagi við fyrri tilraunir sýnum við a ð þessi myndun orða veitir hugmynd um svipað sem er í sjálfum sér ósamrýmanlegt og því nærri manna svipað dóm. Við borum saman framkvæmd þessarar myndunar við ýmsa viðmiðunarmarka, og skoðum sumir einkum eiginleika þ.m.t. greiningu á orðum starfsemi, greiningu á fjölbreyttum orðum og einhverja upplýsinga um þýðingarhæfni sem gefin er með settum fræðilegum aðgerðum.</abstract_isl>
      <abstract_hu>Ebben a tanulmányban alternatív perspektívát nyújtunk a szóreprezentációkra, úgy, hogy újraértelmezzük egy szó vektortérének dimenzióit, mint jellemzők gyűjteményét. Ebben az újraértelmezésben a szó vektorának minden összetevője normalizálódik a szókincs összes vektorával szemben. Ez az ötlet lehetővé teszi, hogy minden vektort n-tuple-ként tekintsünk (hasonló egy fuzzy halmazhoz), ahol n a szó ábrázolásának dimenziója és minden elem azt a valószínűséget jelenti, hogy a szó rendelkezik egy funkcióval. Valójában ez az ábrázolás lehetővé teszi a fuzzy halmaz elméleti műveletek használatát, mint például unió, metszés és különbség. A korábbi próbálkozásokkal ellentétben azt mutatjuk, hogy ez a szavak ábrázolása olyan hasonlóság fogalmát biztosítja, amely eredendően aszimmetrikus, és így közelebb kerül az emberi hasonlósági ítéletekhez. Összehasonlítjuk ennek az ábrázolásnak a teljesítményét különböző referenciaértékekkel, és feltárjuk néhány egyedi tulajdonságot, beleértve a függvényszó felismerését, a poliszemózus szavak felismerését, valamint a halmazelméleti műveletek értelmezhetőségét.</abstract_hu>
      <abstract_ka>ამ გვერდიში, ჩვენ გვაქვს სხვადასხვა პერსპექტიკური პერსპექტიკური სიტყვების გამოსახულების განზომილებების განზომილებით, რომლებიც სიტყვების კოლექტიკური განზომილებები ამ განსხვავებაში, ყველა სიტყვის გვექტორის კომპონენტი ნორმალიზებულია ყველა სიტყვის გვექტორის განმავლობაში. ამ იდეა ახლა გვაქვს, რომ ყოველ გვეკტორის n-tuple (როგორც მუშაობელი ნაწილი), სადაც n არის სიტყვის გამოსახულების განზომილებელობა და ყოველ ელემენტი გამოსახულება სიტყვის შესაძლებლობა. ეს გამოყენება შესაძლებელია გამოყენება თეორეტიკური операциები, როგორც საერთო, საშუალოდ და განსხვავება. წინა პროცემების განმავლობაში, ჩვენ ჩვენ ჩვენ ჩვენ აჩვენებთ, რომ ეს სიტყვების გამოსახულება იყოს სხვადასხვების წარმოდგენა, რომელიც საშუალოდ არიმეტრიული და ამიტომ ადამიანის სხვად ჩვენ ამ გამოსახულების გამოსახულებას განსხვავებული ბენქმარკებით შემდგენებთ და განსხვავებთ განსხვავებული განსხვავებების განსხვავებას, ფუნქციის სიტყვების განსხვავებას, პოლისემის სიტყვების განსხვავებას</abstract_ka>
      <abstract_it>In questo articolo, forniamo una prospettiva alternativa sulle rappresentazioni di parole, reinterpretando le dimensioni dello spazio vettoriale di una parola che incorpora come una raccolta di caratteristiche. In questa reinterpretazione, ogni componente del vettore della parola viene normalizzato rispetto a tutti i vettori della parola nel vocabolario. Questa idea ci permette ora di vedere ogni vettore come una n-tupla (simile ad un insieme sfocato), dove n è la dimensionalità della rappresentazione della parola e ogni elemento rappresenta la probabilità che la parola possieda una caratteristica. Infatti, questa rappresentazione consente l'uso di operazioni teoriche di set fuzzy, come unione, intersezione e differenza. A differenza dei tentativi precedenti, mostriamo che questa rappresentazione delle parole fornisce una nozione di somiglianza intrinsecamente asimmetrica e quindi più vicina ai giudizi di somiglianza umana. Confrontiamo le prestazioni di questa rappresentazione con vari parametri di riferimento ed esploriamo alcune delle proprietà uniche tra cui il rilevamento delle parole funzione, il rilevamento di parole polisemose e alcune informazioni sull'interpretabilità fornita dalle operazioni teoriche degli insiemi.</abstract_it>
      <abstract_lt>Šiame dokumente teikiame alternatyvią perspektyvą žodžių atspindėjimų atžvilgiu, iš naujo aiškindami vektoriaus erdvės matmenis žodžio įterpimo kaip savybių rinkinio. Šiame reinterpretacijoje kiekvienas žodžio vektoriaus komponentas normalizuojamas prieš visus žodžio vektorius žodyne. Ši idėja dabar leidžia mums žiūrėti į kiekvieną vektorių kaip n-dublą (panašų į apgaulingą rinkinį), kur n yra žodžio reprezentacijos matmenys ir kiekvienas element as rodo žodžio, turinčio savybę, tikimybę. Iš tiesų, šis atstovavimas leidžia naudoti netinkamas teorines operacijas, pvz., sąjungą, skerspjūvį ir skirtumą. Priešingai nei ankstesni bandymai, mes rodome, kad šis žodžių atstovavimas suteikia panašumo sąvoką, kuri iš esmės yra asimetriška ir todėl artimesnė žmogaus panašumo vertinimams. Palyginame šio atvaizdavimo rezultatus su įvairiais lyginamaisiais rodikliais ir tiriame kai kurias unikalias savybes, įskaitant funkcijos žodžių aptikimą, polizieminių žodžių aptikimą ir tam tikrą supratimą apie aiškinamumą, kurį suteikia teorinės operacijos.</abstract_lt>
      <abstract_mk>Во овој весник, ние обезбедуваме алтернативна перспектива за претставувањата на зборовите, со реинтерпретација на димензиите на векторниот простор на зборот вграден како колекција на карактеристики. In this reinterpretation, every component of the word vector is normalized against all the word vectors in the vocabulary.  Оваа идеја сега ни овозможува да го гледаме секој вектор како n-tuple (сличен на нејасниот сет), каде n е димензионалноста на репрезентацијата на зборот и секој елемент ја претставува веројатноста на зборот кој поседува функционалност. Всушност, ова претставување овозможува употреба на теоретски операции, како што се синдикат, премин и разлика. За разлика од претходните обиди, покажуваме дека ова претставување на зборовите обезбедува идеја за сличност која е природно асиметрична и оттука поблиску до судиите за човечка сличност. Ги споредуваме изведувањата на оваа претстава со различни референтни значки, и истражуваме некои од уникатните сопствености вклучувајќи детекција на функционалните зборови, детекција на полисемни зборови, и некои погледи во интерпретабилноста обезбедена од поставени теоретички опера</abstract_mk>
      <abstract_kk>Бұл қағазда сөздерді түсіндіру үшін басқа перспективті келтіреміз. Бұл сөздерді қайта түсіндіру үшін ендірілген сөздердің өлшемдерін қайта түсіндіреміз. Бұл қайта аудару үшін сөздің векторының әрбір компоненті сөздің векторының барлық сөздеріне қарсы нормализацияланады. Бұл идея қазір біз әрбір векторды n- tuple ретінде қарауға мүмкіндік береді (бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл бұл б Шынымен, бұл түсініктеме теоретикалық операцияларды қолдану мүмкіндігін мүмкіндік береді, мысалы, бірлік, бөлшектеу және айырмашылығы. Алдыңғы әрекеттерге сәйкес, біз сөздердің көрсетілімі бұл сөздердің ұқсас тәртіпсіздігін көрсетеді, сондықтан адамдардың ұқсас тәртіпсіздігіне жақын көрсетеді. Біз бұл кескіндікті түрлі белгілерден салыстырып, функциялық сөздерді анықтау, полиземді сөздерді анықтау және теоретикалық операцияларды орнату үшін кейбір түсініктемелерді қалаймыз.</abstract_kk>
      <abstract_ml>ഈ പത്രത്തില്‍, വാക്കിന്റെ പ്രതിനിധികളെ വെക്റ്റര്‍ സ്ഥലത്തിന്റെ ഭാഗങ്ങള്‍ വീണ്ടും ചേര്‍ക്കുന്ന വാക്കുകളുടെ വിശേഷത്തിന്റെ വ്യത്യസ ഈ വാക്ക് വെക്റ്ററിന്റെ എല്ലാ ഭാഗങ്ങളും വാക്ക് വെക്റ്ററുകള്‍ക്കെതിരെ സാധാരണമാക്കിയിരിക്കുന്നു. ഈ ഐഡിയ ഇപ്പോള്‍ നമ്മള്‍ ഓരോ വെക്റ്റര്‍ക്കും n-ടുപ്പിള്‍ ആയി കാണാന്‍ അനുവദിക്കുന്നു. വാക്കിന്റെ പ്രതിനിധിയുടെ സ്ഥിതിയില്‍ നിന്നും വാക്കിന്റെ സാധ സത്യത്തില്‍, ഈ പ്രതിനിധിയില്‍ പ്രദര്‍ശിപ്പിക്കുന്നത് തിയോറിറ്റിക് പ്രവര്‍ത്തനങ്ങള്‍ സജ്ജീകരിക്കുന്നത് പോലെ മുമ്പുള്ള ശ്രമം വ്യത്യസ്തമായിരുന്നു, ഈ വാക്കുകളുടെ പ്രതിനിധിയില്‍ ഒരുപോലെയുള്ള ഒരു ആശയം കൊടുക്കുന്നു. അതിനാല്‍ അതിനാല്‍ മനുഷ്യരുട ഈ പ്രതിനിധിയുടെ പ്രഭാവം വ്യത്യസ്ത ബെന്‍മാര്‍ക്കുകളോടൊപ്പം താല്‍പ്പിക്കുകയും, ചില പ്രതിനിധികള്‍ പരിശോധിക്കുകയും, ഫങ്ഷന്‍ വാക്ക് കണ്ടുപിടിക്കു</abstract_ml>
      <abstract_ms>Dalam kertas ini, kita menyediakan perspektif alternatif pada perwakilan perkataan, dengan menerangkan semula dimensi ruang vektor perkataan yang ditambah sebagai koleksi ciri-ciri. Dalam penerangan semula ini, setiap komponen vektor perkataan normalisasi melawan semua vektor perkataan dalam vokbulari. Idea ini kini membolehkan kita melihat setiap vektor sebagai n-tuple (sama dengan set berlebihan), di mana n ialah dimensi perwakilan perkataan dan setiap elemen mewakili kebarangkalian perkataan yang mempunyai ciri. Indeed, this representation enables the use fuzzy set theoretic operations, such as union, intersection and difference.  Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements.  Kami membandingkan prestasi perwakilan ini dengan berbagai tanda referensi, dan mengeksplorasi beberapa ciri-ciri unik termasuk pengesan perkataan fungsi, pengesan perkataan polisemus, dan beberapa pandangan ke dalam pengenalan yang diberikan oleh set operasi teori.</abstract_ms>
      <abstract_mt>F’dan id-dokument, nagħtu perspettiva alternattiva dwar ir-rappreżentazzjonijiet tal-kliem, billi ninterpretaw mill-ġdid id-dimensjonijiet tal-ispazju tal-vetturi ta’ kelma inkorporata bħala ġabra ta’ karatteristiċi. F’din l-interpretazzjoni mill-ġdid, kull komponent tal-kelma vector huwa normalizzat kontra l-kelma vectors kollha fil-vokabulari. This idea now allows us to view each vector as an n-tuple (akin to a fuzzy set), where n is the dimensionality of the word representation and each element represents the probability of the word possessing a feature.  Tabilħaqq, din ir-rappreżentanza tippermetti l-użu ta’ operazzjonijiet teoretiċi ssettjati f’daqqa, bħall-unjoni, l-intersezzjoni u d-differenza. Għall-kuntrarju ta’ tentattivi preċedenti, nuru li din ir-rappreżentazzjoni tal-kliem tipprovdi kunċett ta’ similarità li huwa inerentement asimetriku u għalhekk eqreb lejn ġudizzji ta’ similarità umana. Aħna nqabblu l-prestazzjoni ta’ din ir-rappreżentazzjoni ma’ diversi punti ta’ riferiment, u nesploraw xi wħud mill-karatteristiċi uniċi inklużi l-individwazzjoni tal-kliem tal-funzjoni, l-individwazzjoni tal-kliem poliżimu, u xi ħarsa lejn l-interpretabbiltà pprovduta minn operazzjonijiet teoretiċi stabbiliti.</abstract_mt>
      <abstract_pl>W niniejszym artykule przedstawiamy alternatywną perspektywę reprezentacji słów poprzez reinterpretację wymiarów przestrzeni wektorowej osadzonego słowa jako zbiór cech. W tej reinterpretacji każdy składnik wektora słowa jest znormalizowany w stosunku do wszystkich wektorów słowa w słownictwie. Pomysł ten pozwala nam teraz spojrzeć na każdy wektor jako n-kropel (podobny do zbioru rozmytego), gdzie n jest wymiarowością reprezentacji słowa, a każdy element reprezentuje prawdopodobieństwo, że słowo posiada cechę. Rzeczywiście, ta reprezentacja umożliwia wykorzystanie operacji teoretycznych zbiorów rozmytych, takich jak unia, przecięcie i różnica. W przeciwieństwie do poprzednich prób, pokazujemy, że ta reprezentacja słów daje pojęcie podobieństwa, które jest z natury asymetryczne, a tym samym bliższe ludzkiej ocenie podobieństwa. Porównujemy wydajność tej reprezentacji z różnymi wskaźnikami referencyjnymi i badamy niektóre z unikalnych właściwości, w tym wykrywanie słów funkcyjnych, wykrywanie słów wielosemicznych i pewne wglądy w interpretowalność zapewnianą przez operacje teoretyczne zbiorów.</abstract_pl>
      <abstract_ro>În această lucrare, oferim o perspectivă alternativă asupra reprezentărilor cuvintelor, prin reinterpretarea dimensiunilor spațiului vectorial al unui cuvânt încorporat ca o colecție de caracteristici. În această reinterpretare, fiecare componentă a vectorului cuvântului este normalizată în raport cu toți vectorii de cuvânt din vocabular. Această idee ne permite acum să vedem fiecare vector ca un n-tuple (asemănător cu un set neclar), unde n este dimensiunea reprezentării cuvântului și fiecare element reprezintă probabilitatea ca cuvântul să aibă o caracteristică. Într-adevăr, această reprezentare permite utilizarea operațiunilor teoretice ale setului fuzzy, cum ar fi uniunea, intersecția și diferența. Spre deosebire de încercările anterioare, arătăm că această reprezentare a cuvintelor oferă o noțiune de similitudine care este inerent asimetrică și, prin urmare, mai aproape de judecățile de similitudine umane. Comparăm performanța acestei reprezentări cu diferite criterii de referință și explorăm unele dintre proprietățile unice, inclusiv detectarea cuvintelor funcționale, detectarea cuvintelor polisemoase și o perspectivă asupra interpretabilității oferite de operațiunile teoretice set.</abstract_ro>
      <abstract_mn>Энэ цаасан дээр бид үг илэрхийллийн тухай өөрчлөлт харагдаж, өөрчлөгдөж буй хэмжээсүүдийг өөрчлөгдөж, өөрчлөгдөж буй хэмжээсүүдийн хэмжээсүүдийг өөрчлөгдөж байна. Энэ дахин ойлголтын тулд, үгийн векторын бүх хэсэг нь үгийн бүх векторуудын эсрэг нормалтай байдаг. Энэ санаа одоо бидэнд вектор бүрийг n-tuple гэж үзэх боломжтой болгодог. n нь үгийн илэрхийллийн хэмжээсүүд ба элемент бүр үгийн магадлалыг илэрхийлдэг. Үнэндээ энэ илэрхийлэл нь холбоотой, холбоотой, ялгааг ашиглаж чадна. Өмнөх хичээлийн эсрэгээр бид энэ үгнүүдийн үзүүлэлт нь нэг төстэй төстэй ойлголтыг харуулж байна. Энэ нь хүний төстэй төстэй шүүмжүүдэд илүү ойрхон байдаг. Бид энэ илтгэлийн үйл ажиллагааг олон тоонуудыг харьцуулж, функцын үг олох, полизим үг олох, теоретикийн үйл ажиллагаанд өгсөн утгыг олох боломжтой байдлыг судалж байна.</abstract_mn>
      <abstract_no>I denne papiret gir vi ein alternativ perspektiv på ordrepresentasjonar ved å gjenoppretta dimensjonane av vektorrommet til eit ord innebygd som samling av funksjonar. I denne omsetjinga vert kvar komponent av ordvektoren normalisert mot alle ordvektorane i ordboka. Denne ideen gjer oss n å å sjå kvar vektor som n-tuple (liknande til eit utrygt sett), der n er dimensjonaliteten av ordrepresentasjonen og kvar element representerer sannsynligheten for ordet som har ein funksjon. Dette representasjonen slår på at bruken av tråd sett teoretiske operasjonar, som union, kryss av og forskjellen. I motsetjing til førre forsøk viser vi at denne representasjonen av ord gjev eit noe på likningar som er innehaldet asymmetrisk og derfor nærmere menneskelige forsøk. Vi samanliknar utviklinga av denne representasjonen med ulike benchmarker, og utforsk nokre av dei unike eigenskapane, inkludert funksjonsoppdaging av ord, oppdaging av polysemiske ord, og nokre innsikt i uttolkinga gjeven av teoretiske operasjonar.</abstract_no>
      <abstract_sr>U ovom papiru pružamo alternativnu perspektivu predstavljanja reèi, ponovno pretvaranjem dimenzija vektorskog prostora reèi koja se ukljuèuje kao kolekcija karakteristika. U ovoj reinterpretaciji, svaki komponent rečenog vektora se normalizira protiv svih rečenih vektora u rečniku. Ova ideja nam sada omogućava da gledamo svaki vektor kao n-tuple (sličan n a fuzzy set), gdje n je dimenzionalnost predstavljanja riječi i svaki element predstavlja verovatnoću reči koja posjeduje funkciju. Zapravo, ova predstava omogućava korištenje neobičnih teorijskih operacija, poput sindikata, prekidanja i razlike. Za razliku od prethodnih pokušaja, pokazujemo da ova predstavljanja reèi pruža pojam sliènosti koja je inherentno asimetrična i stoga bliže osuđivanju ljudske sliènosti. Uspoređujemo provedbu ove predstave sa različitim kriterijama, i istražujemo neke od jedinstvenih vlasništva uključujući otkrivanje funkcionalnih reči, otkrivanje polizemnih reči, i neke uvide o interpretabilnosti pruženoj teoretičkim operacijama.</abstract_sr>
      <abstract_si>මේ පැත්තේ, අපි වචන ප්‍රතිරූපයක් ගැන වෙනස් ප්‍රතිරූපයක් දෙනවා, වචන ප්‍රතිරූපයක් වගේ වෙක්ටර් අවසානයේ වචන අවසානයේ  මේ ආපහු ප්‍රවේශනයේදී, වචන වෙක්ටර්ගේ හැම අංකයක්ම සාමාන්‍ය වෙක්ටර් වලින් වචන වෙක්ටර් වලට සාමාන්‍ය මේ අදහසය දැන් අපිට අවශ්‍ය කරන්න පුළුවන් හැම වෙක්ටර්ටර්ටර්ටර්ටර්ටර්ටර්ටර්ටර්ටර්ටර්ටර් n- ටුප්ල් වගේ බලන්න (පුළුවන් සෙට් වග ඇත්තටම, මේ ප්‍රතිනිධානය ප්‍රයෝජනය කරන්න පුළුවන් වෙන්න පුළුවන් විදිහට සාධාන්‍ය ව්‍යාපෘතික වැඩ අපි පෙන්වන්නේ මුලින් උත්සාහ කරපු විදියට, මේ වචනේ ප්‍රතිචාරයක් ප්‍රතිචාරයක් ප්‍රතිචාරයක් වෙනවා කියලා ප්‍රතිචාරයක්  අපි මේ ප්‍රතිනිධානයේ විවිධ බෙන්ච්මාර්ක්ස් එක්ක සම්බන්ධ කරනවා, සමහර විශේෂ වචන පරික්ෂණය සමහර විශේෂ වචන පරික්ෂණය සමහර විශ</abstract_si>
      <abstract_so>Qoraalkan waxaynu ku siinaynaa aragti kale oo ku saabsan qofka hadalka la soo jeedo, kaas oo ku qoraya qaybaha goobta vectorka ee ereyga ku qoran waxyaabo badan. Qayb kasta oo qeyb ka mid ah waxqabadka ereyga waxaa lagu caadi karaa wax walba oo ka gees ah vectoryada hadalka ee hadalka ku qoran. Fikirkaasi wuxuu inagu ogolaan karaa inaan wax walbo ka aragno n-tijaab (sida saxda dhibaato ah), meesha ay ku jirto taxadirka hadalka, qayb kastana waxaa ka mid ah suurtagalka ereyga oo haysta tayo. Sida xaqiiqada ah muuqashadan waxaa suurtogal ah in isticmaalka la isticmaalayo la sameyn karo waxqabadyo cilmi ah, tusaale ahaan urur, kala duwan iyo kala duwan. Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements.  Dhaqashadan waxaynu isbarbardhignaa waxyaabaha kala duduwan, waxaana baaraynaa qaar gaar ah oo ah aqoonta hadalka shaqada, garitaanka hadalka polysemiga ah, iyo waxyaabaha qaar ka mid ah turjubaanka ay sameeyaan waxqabadka theoretika ah.</abstract_so>
      <abstract_sv>I den här uppsatsen ger vi ett alternativt perspektiv på ordrepresentationer genom att omtolka dimensionerna av vektorrymden hos ett ord som bäddas in som en samling funktioner. I denna omtolkning normaliseras varje komponent i ordvektorn mot alla ordvektorer i ordförrådet. Denna idé tillåter oss nu att se varje vektor som en n-tuple (besläktad med en fuzzy set), där n är dimensionen av ordet representation och varje element representerar sannolikheten för ordet innehar en funktion. Faktum är att denna representation möjliggör användning av fuzzy set teoretiska operationer, såsom union, skärning och skillnad. Till skillnad från tidigare försök visar vi att denna representation av ord ger en uppfattning om likhet som i sig är asymmetrisk och därmed närmare mänskliga likheter bedömningar. Vi jämför prestandan för denna representation med olika riktmärken, och utforskar några av de unika egenskaperna inklusive funktionsordsdetektion, detektering av polystemösa ord och viss insikt i tolkningen av uppsättningsteoretiska operationer.</abstract_sv>
      <abstract_ta>In this paper, we provide an alternate perspective on word representations, by reinterpreting the dimensions of the vector space of a word embedding as a collection of features.  இந்த மீண்டும் பொருளில், வார்த்தை நெறியின் ஒவ்வொரு பொருளும் சொல்வளத்தின் வார்த்தை நெறிக்கும் எதிராக இயல்பாக இந்த கருத்து ஒவ்வொரு நெறியையும் n- துப்பீட்டாக பார்க்க அனுமதிக்கிறது (குறிப்பாக்கும் அமைப்புகளுக்கு, அதில் n என்பது வார்த்தை குறிப்பிடும் தனிப உண்மையில், இந்த குறிப்பிட்ட பயன்பாட்டை செயல்படுத்த முடியும், யூனியன், இடைவெட்டு மற்றும் வேறுபாடு முந்தைய முயற்சிகளை வித்தியாசமாக, இந்த வார்த்தைகளின் பிரதிநிதியை காட்டுகிறோம் என்பதை நாம் காண்பிக்கிறோம், அது உட்பொழுது ஒத்த நாம் இந்த குறிப்பிட்ட செயல்பாட்டை பல குறிப்புகளுடன் ஒப்பிடுகிறோம், செயல்பாடு வார்த்தையை கண்டுபிடிக்க, பலவிதமான வார்த்தைகளை கண்டுபிடிக்க</abstract_ta>
      <abstract_ur>اس کاغذ میں، ہم کلمات کی تصویر پر ایک دوسری نظر دیتے ہیں، ایک کلمات کی جگہ کے اندازے دوبارہ تغییر دیتے ہیں اس دوبارہ تفسیر میں، کلمات ویکتور کی ہر قسمت ویکتوری کے مقابلہ میں سارے کلمات ویکتوروں کے مقابلہ میں عام کیا جاتا ہے. یہ ایڈیو اب ہمیں ہر ویکتور کو n-tuple کے مطابق دیکھنے کی اجازت دیتا ہے، جہاں n کلمات کی تصویر ہے اور ہر عنصر کلمات کی تصویر کے مطابق ایک ویکتوری کے مطابق ہے. بے شک، یہ نمایش اسے مضبوط استعمال کرنا امکان دیتی ہے، جیسے اتحادیہ، متفرقہ اور تفرقہ. پہلے کی کوشش کے مطابق، ہم دکھاتے ہیں کہ یہ کلمات کی نمونش ایک ایسی نظریہ پیش کرتا ہے جو اس کے دل میں برابر ہے اور اسی طرح انسان کی برابری کے فیصلے سے زیادہ قریب ہے ہم اس نمایش کی عملکرد کو مختلف بنچم مارک سے مقایسہ کرتے ہیں، اور بعض مختلف خصوصیات کا تحقیق کرتے ہیں، فنقش کلمات شناسایی، پالیس کلمات کی شناسایی، اور بعض نظریہ نظریہ عملکرد کے ذریعہ تفسیر کی تعبیر کے ذریعہ۔</abstract_ur>
      <abstract_uz>Bu qogʻozda, biz soʻzning tashkilotlarini o'zgartirish imkoniyatini o'zgartiraymiz va vektorning joylarini qaytadan qo'yish mumkin. Ushbu qaytadan qaytadan, so'zlar vektorining har bir komponent soʻzlarning hamma so'zlar vektorlariga qoʻllaniladi. Bu g'oya bizga har bir vectorni n-tutlik sifatida ko'rsatishga ruxsat beradi. Bu yerda so'zning chegaraligi va har bir element imkoniyatlarni qoʻllash mumkin. Hullas, bu tashkilotni foydalanish imkoniyatlariga, birlashtirish, birlashtirish va ўзгартириш imkoniyatini beradi. Oldingi jarayonlarni o'xshash ko'rsatganimiz, bu so'zlar tashkilotlarini ko'rsatumiz, bu asymmetrik va shunday qilib odamning bir xil xususiyatlariga яқин. Biz bu tashkilotning natijasini ko'plab bog'lamalar bilan birlashtiramiz, uning xossalarini qidirib, muloqat so'zlarni aniqlash, va bir necha narsalarni teoretik amallar qo'llash orzularini o'rganish mumkin.</abstract_uz>
      <abstract_vi>Trong tờ giấy này, chúng tôi cung cấp một góc nhìn khác nhau về các biểu tượng từ, bằng cách tái hiểu lại các kích thước của các chiều của các chữ nhúng vào như một bộ sưu tập các tính năng. Trong phiên dịch lại này, mỗi thành phần của véc- tơ từ được tổng hợp lại với tất cả các véc- tơ từ trong từ. Ý tưởng n ày cho phép chúng ta xem mỗi véc- tơ như một v (giống với một bộ màu lục, nơi n là chiều không của từ đại diện và mỗi nguyên tố là xác suất của từ sở hữu một tính năng. Thật ra, sự đại diện này cho phép sử dụng các thao tác lí thuyết trên tập thất, như liên kết, giao nhau và khác nhau. Không giống như những nỗ lực trước đây, chúng tôi cho thấy rằng sự mô tả từ ngữ này mang lại một khái niệm về nét giống nhau vốn không đồng nhất và gần hơn so với các phán đoán về nét giống người. Chúng tôi so sánh hiệu quả của sự đại diện này với nhiều tiêu chuẩn khác nhau, và khám phá một số tính chất độc nhất, gồm khả năng phát hiện từ hàm, phát hiện từ dạng polysemous, và một số hiểu biết về sự thể dịch được cung cấp bởi các thao định lý.</abstract_vi>
      <abstract_bg>В тази статия ние предлагаме алтернативна перспектива за представянето на думи, като реинтерпретираме размерите на векторното пространство на една дума, вградена като колекция от функции. При това повторно тълкуване всеки компонент на вектора на думата се нормализира спрямо всички вектори на думата в речника. Тази идея сега ни позволява да разглеждаме всеки вектор като n-тупъл (подобно на мъглив набор), където n е размерността на словото представяне и всеки елемент представлява вероятността думата да притежава дадена характеристика. Всъщност, това представяне позволява използването на мъгливи теоретични операции, като обединение, пресичане и разлика. За разлика от предишните опити, ние показваме, че това представяне на думите осигурява понятие за сходство, което по своята същност е асиметрично и следователно по-близо до човешките преценки за сходство. Сравняваме ефективността на това представяне с различни референтни показатели и изследваме някои от уникалните свойства, включително функция откриване на думи, откриване на многослойни думи и известно разбиране за интерпретацията, предоставена от теоретичните операции на множеството.</abstract_bg>
      <abstract_da>I denne artikel giver vi et alternativt perspektiv på ordrepræsentationer ved at gentolke dimensionerne af vektorrummet i et ord, der indlejres som en samling af funktioner. I denne nyfortolkning normaliseres hver komponent i ordet vektor mod alle ordvektorer i ordforrådet. Denne idé giver os nu mulighed for at se hver vektor som en n-tuple (beslægtet med et fuzzy sæt), hvor n er dimensionen af ordet repræsentation og hvert element repræsenterer sandsynligheden for ordet besidder en funktion. Faktisk muliggør denne repræsentation brugen af fuzzy sæt teoretiske operationer, såsom union, skæring og forskel. I modsætning til tidligere forsøg viser vi, at denne repræsentation af ord giver et begreb om lighed, der i sig selv er asymmetrisk og dermed tættere på menneskelige lighedsdomme. Vi sammenligner ydeevnen af denne repræsentation med forskellige benchmarks, og undersøger nogle af de unikke egenskaber, herunder funktion orddetektion, detektion af polystemøse ord og noget indsigt i fortolkningen af sætteteoretiske operationer.</abstract_da>
      <abstract_hr>U ovom papiru pružamo alternativnu perspektivu predstavljanja riječi, ponovno pretvaranjem dimenzija vektorskog prostora riječi uključujući kao kolekciju funkcija. U ovoj ponovnoj pretvaranju, svaka komponenta riječnog vektora normalizira se protiv svih riječnih vektora u rečniku. Ova ideja nam sada omogućava da gledamo svaki vektor kao n-tuple (sličan n a fuzzy set), gdje n je dimenzionalnost predstavljanja riječi i svaki element predstavlja vjerojatnost riječi koja posjeduje funkciju. Zapravo, ova predstava omogućava korištenje neobičnih teorijskih operacija, poput sindikata, prekidanja i razlike. Za razliku od prethodnih pokušaja, pokazujemo da ova predstavljanje riječi pruža pojam sličnosti koja je inherentno asimetrična i stoga bliže osuđivanju ljudske sličnosti. Uspoređujemo učinkovitost te predstave s različitim kriterijama, i istražujemo neke jedinstvene vlasti uključujući otkrivanje funkcionalnih riječi, otkrivanje polizemnih riječi, i neke uvide o interpretabilnosti pruženoj teoretičkim operacijama.</abstract_hr>
      <abstract_nl>In dit artikel bieden we een alternatief perspectief op woordrepresentaties, door de afmetingen van de vectorruimte van een woord te herinterpreteren als een verzameling van kenmerken. Bij deze herinterpretatie wordt elke component van de woordvector genormaliseerd ten opzichte van alle woordvectoren in de woordenschat. Dit idee stelt ons nu in staat om elke vector te bekijken als een n-tupel (vergelijkbaar met een fuzzy set), waarbij n de dimensionaliteit van de woordweergave is en elk element de waarschijnlijkheid vertegenwoordigt dat het woord een kenmerk bezit. Inderdaad, deze representatie maakt het gebruik van fuzzy set theoretische operaties mogelijk, zoals vereniging, kruising en verschil. In tegenstelling tot eerdere pogingen laten we zien dat deze voorstelling van woorden een idee van gelijkenis biedt die inherent asymmetrisch is en dus dichter bij menselijke gelijkenisoordelen ligt. We vergelijken de prestaties van deze representatie met verschillende benchmarks, en onderzoeken enkele van de unieke eigenschappen, waaronder functiewoorddetectie, detectie van polyemotionele woorden, en enig inzicht in de interpreteerbaarheid van verzameltheoretische bewerkingen.</abstract_nl>
      <abstract_de>In diesem Beitrag stellen wir eine alternative Perspektive auf Wortdarstellungen zur Verfügung, indem wir die Dimensionen des Vektorraums einer Worteinbettung als Sammlung von Merkmalen neu interpretieren. Bei dieser Neuinterpretation wird jede Komponente des Wortvektors gegenüber allen Wortvektoren im Vokabular normalisiert. Diese Idee erlaubt es uns nun, jeden Vektor als n-Tupel (ähnlich einer unscharfen Menge) zu betrachten, wobei n die Dimensionalität der Wortdarstellung ist und jedes Element die Wahrscheinlichkeit repräsentiert, dass das Wort ein Merkmal besitzt. Tatsächlich ermöglicht diese Darstellung die Verwendung von fuzzy set theoretischen Operationen, wie Vereinigung, Schnittmenge und Differenz. Im Gegensatz zu früheren Versuchen zeigen wir, dass diese Darstellung von Wörtern einen Begriff von Ähnlichkeit liefert, der inhärent asymmetrisch ist und daher näher an menschlichen Ähnlichkeitsurteilen ist. Wir vergleichen die Leistung dieser Repräsentation mit verschiedenen Benchmarks und untersuchen einige der einzigartigen Eigenschaften, einschließlich Funktionsworterkennung, Erkennung polyemotionaler Wörter und einige Einblicke in die Interpretierbarkeit von mengentheoretischen Operationen.</abstract_de>
      <abstract_id>Dalam kertas ini, kami menyediakan perspektif alternatif pada representation kata, dengan menggambarkan kembali dimensi ruang vektor dari sebuah kata yang memasukkan sebagai koleksi fitur. Dalam interpretasi ulang ini, setiap komponen dari vektor kata normalisasi melawan semua vektor kata dalam vokabular. Ide ini sekarang memungkinkan kita untuk melihat setiap vektor sebagai n-tuple (mirip dengan set kabur), di mana n adalah dimensionalitas representation kata dan setiap elemen mewakili kemungkinan kata yang memiliki fitur. Sebenarnya, perwakilan ini memungkinkan penggunaan operasi teori set kabur, seperti union, intersection dan perbedaan. Tidak seperti percobaan sebelumnya, kami menunjukkan bahwa perwakilan kata ini memberikan gagasan persamaan yang secara alami tidak simetri dan oleh itu lebih dekat dengan penilaian persamaan manusia. Kami membandingkan prestasi representation ini dengan berbagai benchmark, dan mengeksplorasi beberapa properti unik termasuk deteksi kata fungsi, deteksi kata polisemus, dan beberapa pandangan ke dalam interpretabilitas yang diberikan oleh set operasi teori.</abstract_id>
      <abstract_ko>본고에서 우리는 단어가 삽입된 벡터 공간의 차원을 하나의 특징으로 재해석함으로써 단어의 표시에 또 다른 시각을 제공했다.이런 재해석에서 단어 벡터의 모든 구성 부분은 어휘표의 모든 단어 벡터를 규범화한 것이다.이 아이디어는 현재 우리가 모든 벡터를 하나의 n원조(모호집과 유사)로 볼 수 있게 한다. 그 중에서 n은 단어가 표시하는 차원이고 모든 요소는 단어가 특징을 가진 확률을 나타낸다.사실상 이런 표시는 모호 집합론 연산을 사용할 수 있게 한다. 예를 들어 병합, 교화차 등이다.이전의 시도와 달리 우리는 이런 단어의 표시가 내재된 비대칭적인 유사성 개념을 제공하기 때문에 인류의 유사성 판단에 더욱 가깝다는 것을 보여준다.우리는 이러한 표현법의 성능을 각종 기준과 비교하고 허사 검측, 다의어 검측, 집합론 조작에 대한 해석 가능한 견해를 포함한 독특한 특성을 탐색했다.</abstract_ko>
      <abstract_fa>در این کاغذ، ما یک نگاه تغییر در مورد نمایش‌های کلمه را به عنوان مجموعه‌ی ویکتوری به عنوان مجموعه‌ی ویکتوری تغییر می‌دهیم. در این تغییرات، هر قسمتی از ویکتور کلمه نسبت به تمام ویکتور کلمه‌ها در کلمه‌ای معمولاً متعادل می‌شود. این ایده به ما اجازه می دهد که هر vektor را به عنوان یک مجموعه n-tuple ببینیم (مانند یک مجموعه غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر در حقیقت، این نمایش به استفاده از عملیات نظریه‌ای که به عنوان اتحادیه، تقسیم و تفاوت می‌دهد اجازه می‌دهد. برخلاف تلاش قبلی، نشان می دهیم که این نمایش کلمات یک نظر شبیه‌ای را می‌دهد که در اصل شبیه‌ای است و بنابراین به حکم‌های شبیه‌ای انسان نزدیک تر است. ما عملکرد این نمایش را با برچسب‌های مختلف مقایسه می‌کنیم، و بعضی از ویژه‌های متفاوتی را که شامل شناسایی کلمه‌های فعالیت، شناسایی کلمه‌های متفاوتی، و بعضی مشاهده‌ها در تعبیر قابلیت توسط عملکرد‌های نظریه‌ای قرار داده می‌</abstract_fa>
      <abstract_sw>Katika karatasi hii, tunatoa mtazamo mbadala wa uwakilishi wa maneno, kwa kuingiza upande wa nafasi ya vector wa neno linalojumuisha kama mkusanyiko wa tabia. Katika upande huu, kila sehemu ya vector wa neno linalazimika dhidi ya vector zote za maneno katika lugha hii. Wazo hili kwa sasa linaturuhusu kuona kila vector kama kituo cha n-tiba (akilinganisha n a seti yenye tatizo), ambapo kuna ukubwa wa uwakilishi wa neno na kila element inawakilisha uwezekano wa neno linalo tayari. Kwa hakika, uwakilishi huu unawezesha matumizi ya ajabu kutengeneza shughuli za nadharia, kama vile umoja, tofauti na tofauti. Tofauti na majaribio yaliyopita, tunaonyesha kuwa uwakilishi huu wa maneno yanatoa wazo la usawa wa watu ambao ni kwa kiasi kikubwa na hivyo karibu zaidi na maamuzi yanayofanana na binadamu. Tunawalinganisha ufanisi wa uwakilishi huu na misingi mbalimbali, na kutafuta baadhi ya utaalam wa kipekee ikiwa ni pamoja na kutambua neno la kazi, kutambua maneno ya kijamii, na baadhi ya uelewa wa tafsiri iliyotolewa na shughuli za nadharia.</abstract_sw>
      <abstract_tr>Bu kağıt içinde, kelime ifadeleri üzerinde başka bir perspektiv sunuyoruz, içinde bulunan bir kelime alanın vektör alanının ölçülerini bir toplantı olarak tekrar dönüştürerek. - - Bu tercüme, kelime vektörünün her parçası kelime vektörlerine karşı normaldir. Bu fikir şu and a her vektöre n-tuple gibi görünmemizi sağlar ve n kelime temsilcisinin ölçüsü ve her elemente bir özelliğin sahip olduğu kelimenin muhtemeleni gösterir. Çünkü bu representasyon, birlik, kesişikler we farklygy ýaly çalşyrlyk teoriýa işlerini mümkin edir. Önceki denemelere benzemediğimize göre, bu kelimelerin ifadesi içerisinde asymmetrik bir fikir sağlayan ve bu yüzden insan benzeri kararlarına daha yakın olduğunu gösteriyoruz. Biz bu temsilin etkinliğini farklı kayıtlar ile karşılaştırıp, fonksiyonlu kelime keşfetme, polizem kelimelerin tanımlaması ve teoretik operasyonlar tarafından verilen yorumluluklara göz önüne alıyoruz.</abstract_tr>
      <abstract_af>In hierdie papier, ons verskaf 'n alternatiewe perspektief op woord voorstellings deur die dimensies van die vektor ruimte van 'n woord ingesluit as 'n versameling van funksies te hervertrek. In hierdie herterpretasie, elke komponent van die woord vektor is normaliseer teen al die woord vektore in die woordeboek. Hierdie idee laat ons nou toe om elke vektor te besigtig as 'n n- tuple (gelyk a an 'n verdwyn stel), waar n is die dimensionaliteit van die woord verteenwoording en elke element verteenwoordig die waarskynlikheid van die woord besit 'n funksie. Werklik, hierdie verteenwoording laat die gebruik van verdwyn stel teorieese operasies, soos union, interseksie en verskil. Ongelyks soos vorige probeers, wys ons dat hierdie voorstelling van woorde 'n notie van gelykheid verskaf wat inherent asymmetries is en daarom naby aan menslike oordelinge. Ons vergelyk die prestasie van hierdie verteenwoording met verskeie benchmarke, en ondersoek sommige van die unieke eienskappe insluitend funksie woord opdekking, opdekking van polisemose woorde, en sommige inkennisse in die uitleggingsverklaring wat deur teorieese operasies verskaf word.</abstract_af>
      <abstract_sq>Në këtë letër, ne ofrojmë një perspektivë alternative mbi përfaqësimet e fjalëve, duke përsëritur dimensionet e hapësirës vectore të një fjale të përfshirë si një koleksion karakteristike. Në këtë përsëritje, çdo komponent i vektorit të fjalës është normalizuar kundër të gjitha vektorëve të fjalës në fjalorë. Kjo ide n a lejon tani të shohim çdo vektor si një n-tuple (si një set i ngatërruar), ku n është dimensionaliteti i përfaqësimit të fjalës dhe çdo element përfaqëson probabilitetin e fjalës që posedon një funksion. Në fakt, ky përfaqësim lejon përdorimin e operacioneve teorike të vështira, të tilla si bashkimi, ndërprerje dhe ndryshimi. Në ndryshim nga përpjekjet e mëparshme, ne tregojmë se ky përfaqësim i fjalëve ofron një koncept të ngjashmërisë që është në vetvete asimetrike dhe kështu më pranë gjykimeve të ngjashmërisë njerëzore. Ne e krahasojmë performancën e kësaj përfaqësimi me pika të ndryshme referimi dhe eksplorojmë disa nga pronësitë unike duke përfshirë zbulimin e fjalëve të funksionit, zbulimin e fjalëve polisemore dhe disa pamje në interpretueshmërinë e ofruar nga operacionet teorike të vendosura.</abstract_sq>
      <abstract_am>በዚህ ፕሮግራም፣ ለቃላት መልዕክቶች እናስቀራለን፡፡ በዚህ መግለጫ፣ የቃላት vector ሁሉም ክፍል በአብሪካዊው ቃላት vectors ላይ የተደገመ ነው፡፡ ይህም አሳብ እያንዳንዱን vector እንደ n-ጭብጥ (የጨዋታ መስመር) ማየት ይፈቅዳል፡፡ እርግጠኛ፣ ይህ መልዕክት የተጠቃሚ ተግባር፣ እንደ ዩኒያዊ፣ ግንኙነት እና ልዩነት የሚደረገውን ጥያቄ ያስችላል፡፡ Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements.  የዚህን መልዕክት አካሄዱን በተለያዩ ደብዳቤዎች እናስተያየዋለን፣ የቃላትን ማግኘት፣ የፖሊስቲካዊ ቃላትን ማግኘት እናደርጋለን፣ አንዳንዶችም የtheoretica ተግባር በተደረገው ትርጓሜ እናደርጋለን፡፡</abstract_am>
      <abstract_hy>Այս թղթի մեջ մենք տալիս ենք բառերի ներկայացումների փոխարինական տեսանկյուն, վերարտացոլով վեկտորի տարածության չափերը բառի ներառման որպես հատկանիշների հավաքածու: In this reinterpretation, every component of the word vector is normalized against all the word vectors in the vocabulary.  Այս գաղափարը հիմա թույլ է տալիս մեզ տեսնել յուրաքանչյուր վեկտոր որպես n-անգամ (նման է խառնաշփոթ համակարգին), որտեղ n բառի ներկայացման չափսերը և յուրաքանչյուր տարր ներկայացնում է բառի հավանականությունը, որն ունի հատկություն: Իրականում, այս ներկայացումը հնարավորություն է տալիս օգտագործել խառնաշփոթ տեսական գործողություններ, ինչպիսիք են միավորումը, խաչը և տարբերությունը: Ի տարբերություն նախորդ փորձերին, մենք ցույց ենք տալիս, որ բառերի ներկայացումը ստեղծում է նմանության գաղափար, որը բնական անհամաչափ է և հետևաբար ավելի մոտ մարդկային նմանության դատողություններին: Մենք համեմատում ենք այս ներկայացումի արտադրությունը տարբեր համեմատային նպատակների հետ և ուսումնասիրում ենք որոշ առանձնահատկություններ, ներառյալ ֆունկցիոնալ բառերի հայտնաբերումը, պոլիզեմային բառերի հայտնաբերումը և որոշ ընկալումներ տեսական գործողությունների միջո</abstract_hy>
      <abstract_az>Bu kağızda, sözlərin göstərilmələri barəsində başqa bir perspektiv təyin edirik, özelliklərin koleksiyonu olaraq içərisində olan bir sözün vektör boşluğunun ölçülərini yenidən dəyişdirərək. Bu tərzdə, sözlərin vektorunun hər komponenti sözlərin vektorlarına qarşı normalizlənir. Bu fikir indi hər vektörü n-tuple kimi görünməyə imkan verir, n sözlərin göstəricisinin ölçülüyü və hər elementi bir fəaliyyət sahibi sözlərin ehtimalın ı göstərir. Əslində, bu göstəricisi istifadə etməyi fərqli təriqli təriqli işləri, birlikləri, fərqli və fərqli kimi fərqli təriqli təriqlərə qadirdir. Əvvəlki çabaların bənzərinə baxmayaraq, bu sözlərin göstərilməsi, bənzər bir fikir göstərir ki, bənzər bir qədər asymetrik və buna görə də insanların bənzər hökmlərinə daha yaxın olur. Biz bu göstərişlərin performansını müxtəlif benchmarklərlə qarşılaşdırırıq, fərqli sözləri keşfetmək, polizm sözlərin keşfetməsi və teoriqli işləri təyin etmək üçün təfsil edilən təfsil edilməsi barəsində bəzi xüsusiyyətləri keşfetirik.</abstract_az>
      <abstract_bn>এই কাগজটিতে আমরা শব্দের প্রতিনিধিত্বের বিপরীত দৃষ্টিভঙ্গি প্রদান করি, ভেক্টরের স্থান পুনরায় বিশেষ করে একটি শব্দের সংগ্রহ হিসেবে প্রবেশ এই পুনঃপ্রতিষ্ঠানে শব্দ ভেক্টরের প্রত্যেক অংশ স্বাভাবিক ভেক্টরের বিরুদ্ধে স্বাভাবিক। এই চিন্তা এখন আমাদের প্রত্যেক ভেক্টর একটি n-টাপেল হিসেবে দেখতে পাচ্ছে (যেখানে একটি অজ্ঞাত সেটের মতো), যেখানে শব্দের প্রতিনিধিত্বের মাত্রার মাত্রা এব সত্যিই, এই প্রতিনিধিত্ব ব্যবহারকারীদের ব্যবহারের ক্ষেত্রে ততীতিহীন কার্যক্রম, যেমন ইউনিয়ন, বিভিন্ন বিভিন্ন পূর্ববর্তী প্রচেষ্টার ভিন্ন ভিন্ন ভিন্ন ভিন্ন প্রতিনিধিত্ব দেখাচ্ছি যে এই শব্দের প্রতিনিধিত্বের একটি ধারণা প্রদান করা হয়েছে যা প্র আমরা এই প্রতিনিধিত্বের প্রকৃতির তুলনা করি বিভিন্ন বেনমার্কের সাথে এবং কিছু বৈশিষ্ট্যের বৈশিষ্ট্য অনুসন্ধান করি, যার মধ্যে ফাংশন শব্দ আবিষ্কার, বহুব</abstract_bn>
      <abstract_bs>U ovom papiru pružamo alternativnu perspektivu predstavljanja riječi, ponovno pretvaranjem dimenzija vektorskog prostora riječi koja se uključuje kao kolekcija funkcija. U ovoj reinterpretaciji, svaki komponent riječi vektora se normalizira protiv svih vektora riječi u rečniku. Ova ideja nam sada omogućava da vidimo svaki vektor kao n-tuple (sličan n a fuzzy set), gdje n je dimenzionalnost predstavljanja riječi i svaki element predstavlja vjerojatnost riječi koja posjeduje funkciju. Zapravo, ova predstava omogućava korištenje nepristojnih teorijskih operacija, poput sindikata, prekidanja i razlike. Za razliku od prethodnih pokušaja, pokazujemo da ova predstavljanja riječi pruža pojam sličnosti koja je inherentno asimetrična i stoga bliže osuđivanju ljudske sličnosti. Uspoređujemo učinkovitost ove predstave sa različitim kriterijama, i istražujemo neke od jedinstvenih vlasništva, uključujući otkrivanje funkcionalnih riječi, otkrivanje polizemnih riječi, i neke uvide o interpretabilnosti pruženoj teoretičkim operacijama.</abstract_bs>
      <abstract_ca>En aquest paper, proporcionem una perspectiva alternativa a les representacions de paraules, reinterpretant les dimensions de l'espai vector d'una paraula incorporada com una col·lecció de característiques. En aquesta reinterpretació, cada component de la paraula vector es normalitza en contra de tots els paraules vectors del vocabulari. Aquesta idea ara ens permet veure cada vector com un n-tuple (semblant a un conjunt confus), on n és la dimensionalitat de la representació de paraula i cada element representa la probabilitat de la paraula que posseeix una característica. De fet, aquesta representació permet l'ús d'operacions teòriques confuses, com la unió, la intersecció i la diferència. A diferència dels intents anteriors, demostram que aquesta representació de paraules proporciona una noció de similitud que és inherentment asimètrica i, per tant, més a prop dels judicis de similitud humana. Comparem el desempeny d'aquesta representació amb diversos punts de referència, i explorem algunes de les propietats únices, incloent la detecció de paraules de funció, la detecció de paraules polisemoses i alguna comprensió de l'interpretabilitat proporcionada per operacions teòriques.</abstract_ca>
      <abstract_cs>V tomto článku poskytujeme alternativní pohled na slovní reprezentace reinterpretací rozměrů vektorového prostoru vloženého slova jako sbírku prvků. V této reinterpretaci je každá složka slovního vektoru normalizována proti všem slovním vektorům ve slovní zásobě. Tato myšlenka nám nyní umožňuje vidět každý vektor jako n-tuple (podobný rozmazané množině), kde n je dimenzionalita slova reprezentace a každý prvek představuje pravděpodobnost, že slovo má vlastnost. Tato reprezentace umožňuje použití fuzzy množinových teoretických operací, jako jsou sjednocení, průsečík a rozdíl. Na rozdíl od předchozích pokusů ukazujeme, že tato reprezentace slov poskytuje představu podobnosti, která je z podstaty asymetrická a tudíž blíže k lidským úsudkům podobnosti. Porovnáváme výkon této reprezentace s různými benchmarky a zkoumáme některé z jedinečných vlastností včetně detekce funkčních slov, detekce polyemozních slov a některý vhled do interpretovatelnosti poskytované množinovými teoretickými operacemi.</abstract_cs>
      <abstract_et>Käesolevas töös pakume alternatiivset perspektiivi sõnade esitustele, tõlgendades uuesti vektoriruumi mõõtmeid sõna manustamisel funktsioonide kogumina. Selles ümbertõlgendamises normaliseeritakse sõnavaraktori iga komponent kõigi sõnavaraktori sõnavaraktorite suhtes. See idee võimaldab meil nüüd vaadata iga vektorit n-tuplina (sarnane udusele hulgale), kus n on sõna representatsiooni dimensioonilisus ja iga element esindab sõna omamise tõenäosust. Tõepoolest, see esitamine võimaldab kasutada hägune hulk teoreetilisi operatsioone, nagu liit, ristumine ja erinevus. Erinevalt varasematest katsetest näitame, et see sõnade esitamine annab sarnasuse mõiste, mis on olemuslikult asümmeetriline ja seega lähemal inimese sarnasuse otsustele. Me võrdleme selle esituse jõudlust erinevate võrdlusnäitajatega ja uurime mõningaid unikaalseid omadusi, sealhulgas funktsioonisõna tuvastamist, polüsemoossete sõnade tuvastamist ja mõningast ülevaadet komplekti teoreetiliste operatsioonide tõlgendatavusest.</abstract_et>
      <abstract_fi>Tässä työssä tarjoamme vaihtoehtoisen näkökulman sanaesityksiin tulkitsemalla uudelleen sanan upottamisen vektoriavaruuden ulottuvuudet ominaisuuksien kokoelmana. Tässä uudelleentulkinnassa sanavektorin jokainen komponentti normalisoidaan sanaston kaikkia sanavektoreita vastaan. Tämän idean avulla voimme nyt tarkastella jokaista vektoria n-tuplana (samankaltainen sumea joukko), jossa n on sanan edustuksen ulottuvuus ja jokainen elementti edustaa todennäköisyyttä, että sana omistaa ominaisuuden. Itse asiassa tämä esitys mahdollistaa fuzzy joukko teoreettisia toimintoja, kuten liitos, leikkaus ja ero. Toisin kuin aikaisemmat yritykset, osoitamme, että tämä sanojen esittäminen tarjoaa samankaltaisuuden käsitteen, joka on luonnostaan epäsymmetrinen ja siten lähempänä ihmisen samankaltaisuusarvioita. Vertaamme tämän esityksen suorituskykyä erilaisiin vertailuarvoihin ja tutkimme joitain ainutlaatuisia ominaisuuksia, kuten funktiosanojen havaitsemista, polyemoisten sanojen havaitsemista ja joitakin näkemyksiä joukkoteoreettisten operaatioiden tulkinnasta.</abstract_fi>
      <abstract_jv>Enter the vector space of a word To This idée nung iné permet us to view every vector as a n n-taple (like a FBI set), Where n is the size of the word representation and every item represents the likely of the word Open source Awak dhéwé éntuk kiper perbudhak, kita ngomatngon kuwi tindakan nyebuturan gambar nggawe sapa ngono kuwi duluran sing gak bener tentang karo akeh apik lan dadi iki sak duluran gambar uwong. Awak dhéwé nggawe geraraning nggawe representasi iki gambar nggambar aturan kanggo ngilangno karo perusahaan winih sing nyimpen, gambar nggambar kelas kuwi operasi layar, jatatan kelas polisemus lan kelas pangrungu nggawe gerarané perusahaan theoretik.</abstract_jv>
      <abstract_ha>In this paper, we provide an alternate perspective on word representations, by reinterpreting the dimensions of the vector space of a word embedding as a collection of features.  @ info: whatsthis Wannan idãnun yanzu yana yarda mu nuna kõwa mai shiryarwa kamar n-tipple (akin da wani set mai zartar da aiki), inda n ke da sifilanci n a maganar kuma duk ƙanshi na ƙayyade sannanan maganar da ya ƙunsa da wani zafi. In da gaske, wannan shirin zai iya amfani da aikin mai zartar da amfani da matsayin teori, kamar shirin kwamfyuta, guda da sãɓãni. Babu motsi da jarrabi da suka gabãta, Munã nuna cewa wannan mai gayarwa ga kalmõmi yana da wani noti na daidaita wanda ke cikin asymmetric da kuma daga wannan yana makusanta ga masu daidaita ga mutane. Kana samfani da aikin wannan rubutu da mistakardan misãlai masu yawa, kuma Munã samfani wasu properties na daban, kamar kunnufi maganar aiki, da gane magana na Polsemi, da wani gane cikin fassarar da aka ƙayyade aikin teori.</abstract_ha>
      <abstract_he>בעיתון הזה, אנחנו מספקים פרספקטיבה חלופית על מיצוגי מילים, על ידי להפריע מחדש את המימדים של מרחב הוקטורים של מילה מוקפת כאוסף של תכונות. בפרשנות מחדש הזאת, כל רכיב של הוקטור המילה נורמלי נגד כל הוקטורים המילים במילים. הרעיון הזה מאפשר לנו עכשיו לראות כל ווקטור כn-כפול (דומה לקבוצה מעורפלת), שבו n הוא המימד של מייצג המילה וכל אלמנט מייצג את הסבירות של המילה שיש לה תכונה. למעשה, היציגה הזאת מאפשרת להשתמש במבצעים תיאורטיים מסודרים, כמו איגוד, חיצום וההבדל. בניגוד לנסיונות קודמות, אנו מראים שהמייצג הזה של מילים מספק רעיון של דמיון שהוא באופן טבעי אסימטרי ולכן קרוב יותר לשיפוטים של דמיון אנושי. We compare the performance of this representation with various benchmarks, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations.</abstract_he>
      <abstract_sk>V prispevku zagotavljamo alternativno perspektivo besednih predstavitev, tako da ponovno interpretiramo dimenzije vektorskega prostora besedne vdelave kot zbirko značilnosti. V tej ponovni razlagi se vsaka komponenta besednega vektorja normalizira glede na vse besedne vektorje v besedišču. Ta ideja nam zdaj omogoča, da vidimo vsak vektor kot n-tuplo (podobno meglenemu množici), kjer je n dimenzionalnost besedne reprezentacije in vsak element predstavlja verjetnost, da ima beseda značilnost. Ta predstavitev namreč omogoča uporabo teoretičnih operacij meglenih množic, kot so unija, presečišče in razlika. Za razliko od prejšnjih poskusov pokažemo, da ta predstavitev besed zagotavlja pojem podobnosti, ki je po sebi asimetričen in s tem bližje človeškim podobnim presojam. Učinkovitost te reprezentacije primerjamo z različnimi referenčnimi vrednostmi in raziskujemo nekatere edinstvene lastnosti, vključno z zaznavanjem funkcijskih besed, zaznavanjem poličemskih besed in nekaj vpogledov v interpretabilnost, ki jo zagotavljajo teoretične operacije množic.</abstract_sk>
      <abstract_fil>Sa papiro na ito, nagbibigay tayo ng isang alternating perspektivo tungkol sa mga salitang representation, sa pagbalik ng mga dimension ng vector space ng isang salitang nagbibigay bilang koleksyon ng mga karakter. Sa reinterpretation na ito, bawa't komponente ng word vector ay normalized laban sa lahat ng salitang vectors sa vocabulary. Itong ideya ngayon ay nagpapahintulot sa atin n a makita ang bawa't vektor bilang n-tuple (tulad sa isang maliit na set), na kung saan n ang dimensionality ng reprezentasyon ng salita at bawa't elemento ay nagtataglay ng probabilidad ng salitang may feature. Katotohanan, ang representasyon na ito ay nagpapakinabang sa paggamit ng mabangis na set ng teoretika na operasyon, tulad ng union, intersection at pagkakaiba. Sa ibang pagsubok ng una, ipinakikita natin na ang representasyon ng mga salitang ito ay nagbibigay ng pagiisip ng katulad na hindi-asymmetrico at kaya't lalong malapit sa mga hukom ng pagkatao. Ito'y iniuusap namin ang gawain ng representasyon na ito sa ibang mga benchmarks, at inihanap ang ilan sa ibang mga pag-aari na kasama ng pag-detection ng mga salita ng funksyon, pag-detection ng mga salita ng polisemo, at ang ibang pakikita sa pagliliwanag na ibinigay sa pamamagitan ng paglalagay ng mga operasyon ng teoretika.</abstract_fil>
      <abstract_bo>ང་ཚོས་ཤོག་བུ་འདིའི་ནང་དུ་ཡི་གེ་ལ་ངོས་ཚོའི་རྩ་སྒྲིག་ཕྱོགས་གཞན་གྱི་ལྟ་བ་ཞིག་བྱེད་ཀྱི་ཡོད་པ་ཚོའི་ནང་ནས་ཕན་ཚུན་གྱི་བར་སྟོང་ འདིའི་རྗེས་སུ་འབྱེད་སྐབས་ཐོག་གི་ཆ་ཤས་རེ་རེ་བ་དེ་ཚོའི་ཐོག་རིམ་ནང་གི་ཚགས་རྣམས་མེད་རྒྱུན་གྱིས་བཟོས་ཚར This idea now allows us to view each vector as a n n-tuple (akin to a fuzzy set), where n is the dimensionality of the word representation and each element represents the probability of the word possessing a feature. དངོས་འབྲེལ་བ་འདིས་སྤྱི་ཁྱད་པར་ཕྱིར་བཏོན་པའི་གཞི་རྩལ་བ་སྒྲིག་ལྟར་བཀོད་སྤྱོད་ལ་ནུས་ཡོད། ང་ཚོས་དུས་མའི་དཔའ་བཅས་ལ་འགྱུར་བ་དེ་ལྟ་བུའི་ནང་གི་ཡིག་གི་གསལ་བཤད་འདི་གི་དོན་དག་མི་འདྲ་བ་དང་། བྱས་ཙང་མི་འདྲ་བ་དང་མི་འདྲ་བ་བསྐྱེད་ We compare the performance of this representation with various benchmarks, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations.</abstract_bo>
      </paper>
    <paper id="5">
      <title>Compositionality and Capacity in Emergent Languages</title>
      <author><first>Abhinav</first><last>Gupta</last></author>
      <author><first>Cinjon</first><last>Resnick</last></author>
      <author><first>Jakob</first><last>Foerster</last></author>
      <author><first>Andrew</first><last>Dai</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <pages>34–38</pages>
      <abstract>Recent works have discussed the extent to which <a href="https://en.wikipedia.org/wiki/Emergence">emergent languages</a> can exhibit properties of <a href="https://en.wikipedia.org/wiki/Natural_language">natural languages</a> particularly learning compositionality. In this paper, we investigate the <a href="https://en.wikipedia.org/wiki/Learning_bias">learning biases</a> that affect the efficacy and <a href="https://en.wikipedia.org/wiki/Compositionality">compositionality</a> in multi-agent communication in addition to the communicative bandwidth. Our foremost contribution is to explore how the capacity of a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and <a href="https://en.wikipedia.org/wiki/Bandwidth_(signal_processing)">channel bandwidth</a> that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.</abstract>
      <url hash="994aa67f">2020.repl4nlp-1.5</url>
      <doi>10.18653/v1/2020.repl4nlp-1.5</doi>
      <video href="http://slideslive.com/38929771" />
      <revision id="1" href="2020.repl4nlp-1.5v1" hash="ca52c819" />
      <revision id="2" href="2020.repl4nlp-1.5v2" hash="994aa67f" date="2021-01-03">Fixed a citation.</revision>
      <bibkey>gupta-etal-2020-compositionality</bibkey>
    <title_ar>التكوين والقدرة في اللغات الناشئة</title_ar>
      <title_pt>Composicionalidade e Capacidade em Línguas Emergentes</title_pt>
      <title_fr>Compositionalité et capacité dans les langues émergentes</title_fr>
      <title_es>Composicionalidad y capacidad en lenguajes emergentes</title_es>
      <title_zh>新兴语言成性量</title_zh>
      <title_hi>आकस्मिक भाषाओं में संरचना और क्षमता</title_hi>
      <title_ru>Композиционность и потенциал в новых языках</title_ru>
      <title_ja>新興言語における構成と能力</title_ja>
      <title_ukr>Композиційність та спроможність у нових мовах</title_ukr>
      <title_ga>Comhshuíomh agus Cumas i dTeangacha Éigeandála</title_ga>
      <title_ka>კომპოზიციონალიტი და შესაძლებლობა მხოლოდ ენებში</title_ka>
      <title_hu>Összefoglaltság és kapacitás az új nyelvekben</title_hu>
      <title_el>Σύνθεση και ικανότητα σε αναδυόμενες γλώσσες</title_el>
      <title_isl>Samsetning og hæfni á nýjum tungum</title_isl>
      <title_it>Compositionalità e capacità nelle lingue emergenti</title_it>
      <title_lt>Sudėtingumas ir gebėjimai naujomis kalbomis</title_lt>
      <title_kk>Қазіргі тілдерде композиционалық және мүмкіндік</title_kk>
      <title_mk>Композиционалноста и способноста на развиени јазици</title_mk>
      <title_ms>Komposisi dan Kemampuan dalam Bahasa Kecemasan</title_ms>
      <title_mn>Гүйцэтгэлтэй хэлнүүдийн нэгтгэл болон чадвар</title_mn>
      <title_no>Komposisjonalitet og kapasitet i eksterne språk</title_no>
      <title_pl>Kompozycjonalność i umiejętność w językach wschodzących</title_pl>
      <title_ro>Compoziționalitatea și capacitatea în limbile emergente</title_ro>
      <title_sr>Kompositivnost i kapacitet na hitnim jezicima</title_sr>
      <title_si>ප්‍රශ්න භාෂාවල් වල සංවිධානය සහ ක්‍රියාත්මක</title_si>
      <title_so>Heeganka iyo awoodda</title_so>
      <title_ml>എമെന്‍റര്‍ജെന്റ് ഭാഷകളിലെ കോമ്പോസിഷനിറ്റിയും ശക്തിയും</title_ml>
      <title_mt>Kompożizzjonalità u Kapaċità f’Lingwi Emerġenti</title_mt>
      <title_sv>Kompositionalitet och kapacitet i nya språk</title_sv>
      <title_ta>வெளிப்படையான மொழிகளில் குறிப்பிடும் மற்றும் சக்தி</title_ta>
      <title_ur>اضطراری زبانوں میں کامپیوتنی اور قابلیت</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Thành phần và khả năng về ngôn ngữ Mới</title_vi>
      <title_bg>Композиционалност и капацитет в нововъзникващите езици</title_bg>
      <title_nl>Samenstelling en capaciteit in opkomende talen</title_nl>
      <title_hr>Kompozitivnost i sposobnost na hitnim jezicima</title_hr>
      <title_da>Kompositionalitet og kapacitet i nye sprog</title_da>
      <title_id>Komposisionalitas dan Kapasitas dalam Bahasa Darurat</title_id>
      <title_de>Kompositionalität und Fähigkeit in Emergent Languages</title_de>
      <title_fa>شرکت و توانایی در زبانهای فوری</title_fa>
      <title_ko>신흥 언어의 구성성과 능력</title_ko>
      <title_sw>Ushirikiano na Uwezeshaji katika lugha za Dharura</title_sw>
      <title_tr>Çagajyk Dillerde ýerleşdirim we Derjes</title_tr>
      <title_af>Komposisionaliteit en Kapasiteit in Emergente Taal</title_af>
      <title_sq>Kompozitiviteti dhe aftësia në gjuhët emergente</title_sq>
      <title_am>አቀማመጥ ቋንቋዎች</title_am>
      <title_az>Çətin Dillərdə birləşdirilmiş və Mümkünlük</title_az>
      <title_hy>Կեպոզիոնալիությունը և կարողությունը զարգացող լեզուներում</title_hy>
      <title_bn>জরুরী ভাষায় ক্ষমতা এবং ক্ষমতা</title_bn>
      <title_ca>La composicionalitat i la capacitat en llengües emergents</title_ca>
      <title_bs>Kompositivnost i kapacitet na hitnim jezicima</title_bs>
      <title_cs>Skladba a schopnost v rozvíjejících se jazycích</title_cs>
      <title_et>Kompositsionaalsus ja suutlikkus tekkivates keeltes</title_et>
      <title_fi>Uusien kielten koostumus ja kapasiteetti</title_fi>
      <title_jv>Sampeyan lan Kapasaasat nang langgambar</title_jv>
      <title_he>שילוב וכוחות בשפות חדשות</title_he>
      <title_sk>Sestavljanje in zmogljivost v nastajajočih jezikih</title_sk>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_fil>Pagsasama at kapangyarihan sa Emergent Languages</title_fil>
      <title_bo>Emergent Languages ནང་གི་ཆ་འཕྲིན་དང་སྒུལ་སྒྲིག་ཚད</title_bo>
      <abstract_ar>ناقشت الأعمال الحديثة المدى الذي يمكن أن تظهر به اللغات الناشئة خصائص اللغات الطبيعية وخاصة تكوين التعلم. في هذه الورقة ، قمنا بالتحقيق في التحيزات التعليمية التي تؤثر على الفعالية والتركيب في الاتصالات متعددة الوكلاء بالإضافة إلى عرض النطاق الترددي التواصلي. تتمثل مساهمتنا الأولى في استكشاف كيفية تأثير قدرة الشبكة العصبية على قدرتها على تعلم لغة تركيبية. نقدم بالإضافة إلى ذلك مجموعة من مقاييس التقييم التي نحلل بها اللغات المكتسبة. فرضيتنا هي أنه يجب أن يكون هناك نطاق محدد من سعة النموذج وعرض النطاق الترددي للقناة الذي يحفز البنية التركيبية في اللغة الناتجة وبالتالي يشجع التعميم المنهجي. بينما نرى دليلًا تجريبيًا على الجزء السفلي من هذا النطاق ، فمن الغريب أننا لم نعثر على دليل للجزء العلوي من النطاق ونعتقد أن هذا سؤال مفتوح للمجتمع.</abstract_ar>
      <abstract_es>Trabajos recientes han analizado hasta qué punto las lenguas emergentes pueden exhibir propiedades de las lenguas naturales, en particular el aprendizaje de la composicionalidad. En este artículo, investigamos los sesgos de aprendizaje que afectan la eficacia y la composicionalidad en la comunicación multiagente, además del ancho de banda comunicativo. Nuestra principal contribución es explorar cómo la capacidad de una red neuronal afecta su capacidad de aprender un lenguaje compositivo. Además, introducimos un conjunto de métricas de evaluación con las que analizamos los idiomas aprendidos. Nuestra hipótesis es que debe haber un rango específico de capacidad de modelo y ancho de banda de canal que induzca una estructura compositiva en el lenguaje resultante y, en consecuencia, fomente la generalización sistemática. Si bien vemos empíricamente evidencia para la parte inferior de este rango, curiosamente no encontramos evidencia para la parte superior del rango y creemos que esta es una pregunta abierta para la comunidad.</abstract_es>
      <abstract_pt>Trabalhos recentes discutiram até que ponto as línguas emergentes podem exibir propriedades das línguas naturais, particularmente a aprendizagem da composicionalidade. Neste artigo, investigamos os vieses de aprendizagem que afetam a eficácia e a composicionalidade na comunicação multiagente, além da largura de banda comunicativa. Nossa principal contribuição é explorar como a capacidade de uma rede neural afeta sua capacidade de aprender uma linguagem composicional. Além disso, introduzimos um conjunto de métricas de avaliação com as quais analisamos as línguas aprendidas. Nossa hipótese é que deve haver uma faixa específica de capacidade de modelo e largura de banda de canal que induza a estrutura composicional na linguagem resultante e, consequentemente, encoraje a generalização sistemática. Embora vejamos empiricamente evidências para a parte inferior desse intervalo, curiosamente não encontramos evidências para a parte superior do intervalo e acreditamos que essa é uma questão em aberto para a comunidade.</abstract_pt>
      <abstract_fr>Des travaux récents ont examiné la mesure dans laquelle les langues émergentes peuvent présenter les propriétés des langues naturelles, en particulier la composition de l'apprentissage. Dans cet article, nous étudions les biais d'apprentissage qui affectent l'efficacité et la composition de la communication multi-agents en plus de la bande passante de communication. Notre contribution principale consiste à explorer l'impact de la capacité d'un réseau de neurones sur sa capacité à apprendre un langage compositionnel. Nous introduisons également un ensemble de mesures d'évaluation avec lesquelles nous analysons les langues apprises. Notre hypothèse est qu'il devrait y avoir une plage spécifique de capacité de modèle et de bande passante de canal qui induit une structure compositionnelle dans le langage résultant et encourage par conséquent une généralisation systématique. Alors que nous voyons empiriquement des preuves pour le bas de cette fourchette, nous ne trouvons curieusement aucune preuve pour la partie supérieure de la fourchette et pensons qu'il s'agit d'une question ouverte pour la communauté.</abstract_fr>
      <abstract_ja>最近の研究では、新興言語が自然言語の性質を示すことができる程度について考察されており、特に複合性を学習することについて考察されている。本稿では、通信帯域幅に加えて、マルチエージェントコミュニケーションの有効性と構成性に影響を与える学習バイアスについて検討する。私たちの最も重要な貢献は、ニューラルネットワークの容量が構成言語を学習する能力にどのように影響するかを探ることです。さらに、学習した言語を分析するための一連の評価指標を紹介します。私たちの仮説は、結果として生じる言語の構成構造を誘導し、結果として体系的な一般化を促すモデル容量とチャネル帯域幅の特定の範囲があるべきであるということです。この範囲の下部には経験的に証拠が見られますが、興味深いことに、範囲の上部には証拠が見当たりません。これはコミュニティにとってオープンな質問であると考えています。</abstract_ja>
      <abstract_zh>近事论新兴语言多大程度可见自然语言属性,特为学组。 本文,我们研究了通带宽之外,还感动了多智能体通信中的有效性和成性的学习偏劣。 所贵者,探神经网络之力,所以感其学而言也。 引入评指标,以析所学之语。 吾之设也,宜有一特定之容,与道之广,以诱其言,以劝其泛化。 虽见证于经验,而怪其顶证,信于社区为一悬而未决也。</abstract_zh>
      <abstract_hi>हाल के कार्यों ने इस बात पर चर्चा की है कि किस हद तक उभरती भाषाएं प्राकृतिक भाषाओं के गुणों को प्रदर्शित कर सकती हैं, विशेष रूप से रचनात्मकता सीखने के लिए। इस पेपर में, हम सीखने के पूर्वाग्रहों की जांच करते हैं जो संचार बैंडविड्थ के अलावा बहु-एजेंट संचार में प्रभावकारिता और रचनात्मकता को प्रभावित करते हैं। हमारा सबसे महत्वपूर्ण योगदान यह पता लगाना है कि एक तंत्रिका नेटवर्क की क्षमता एक रचनात्मक भाषा सीखने की अपनी क्षमता को कैसे प्रभावित करती है। हम इसके अतिरिक्त मूल्यांकन मैट्रिक्स का एक सेट पेश करते हैं जिसके साथ हम सीखी गई भाषाओं का विश्लेषण करते हैं। हमारी परिकल्पना यह है कि मॉडल क्षमता और चैनल बैंडविड्थ की एक विशिष्ट श्रृंखला होनी चाहिए जो परिणामी भाषा में रचनात्मक संरचना को प्रेरित करती है और परिणामस्वरूप व्यवस्थित सामान्यीकरण को प्रोत्साहित करती है। जबकि हम अनुभवजन्य रूप से इस सीमा के नीचे के लिए सबूत देखते हैं, हम उत्सुकता से सीमा के शीर्ष भाग के लिए सबूत नहीं पाते हैं और मानते हैं कि यह समुदाय के लिए एक खुला सवाल है।</abstract_hi>
      <abstract_ru>В недавних работах обсуждалась степень, в которой новые языки могут проявлять свойства естественных языков, особенно композиционность обучения. В этой статье мы исследуем ошибки обучения, которые влияют на эффективность и композиционность в мульти-агентной коммуникации в дополнение к коммуникативной полосе пропускания. Наш главный вклад заключается в изучении того, как способность нейронной сети влияет на ее способность изучать композиционный язык. Дополнительно вводим набор оценочных метрик, с помощью которых анализируем изучаемые языки. Наша гипотеза заключается в том, что должен существовать определенный диапазон пропускной способности модели и полосы пропускания канала, который индуцирует композиционную структуру в результирующем языке и, следовательно, поощряет систематическое обобщение. Хотя мы эмпирически видим доказательства для нижней части этого диапазона, мы, как ни странно, не находим доказательств для верхней части диапазона и считаем, что это открытый вопрос для сообщества.</abstract_ru>
      <abstract_ukr>Останні роботи обговорювали, наскільки нові мови можуть проявляти властивості природних мов, особливо навчальну композиційність. У цій роботі ми досліджуємо упередження навчання, які впливають на ефективність та композиційність у мультиагентному спілкуванні на додаток до комунікативної пропускної здатності. Наш головний внесок полягає в дослідженні того, як здатність нейронної мережі впливає на її здатність вивчати композиційну мову. Додатково запроваджуємо набір оціночних показників, за допомогою яких аналізуємо вивчені мови. Наша гіпотеза полягає в тому, що має бути певний діапазон пропускної здатності моделі та пропускної здатності каналу, який індукує композиційну структуру в отриманій мові і, отже, заохочує систематичне узагальнення. Хоча ми емпірично бачимо докази для нижньої частини цього діапазону, ми цікаво не знаходимо доказів для верхньої частини діапазону і вважаємо, що це відкрите питання для спільноти.</abstract_ukr>
      <abstract_ga>Phléigh saothair le déanaí a mhéid is féidir le teangacha atá ag teacht chun cinn tréithe teangacha nádúrtha a thaispeáint, go háirithe trí chomhdhéanamh a fhoghlaim. Sa pháipéar seo, déanaimid imscrúdú ar na laofachtaí foghlama a théann i bhfeidhm ar éifeachtúlacht agus comhdhéanamh na cumarsáide ilghníomhairí de bhreis ar an bandaleithead cumarsáide. Is é an rud is tábhachtaí atá againn ná iniúchadh a dhéanamh ar an tionchar a bhíonn ag acmhainn néarghréasáin ar a chumas teanga cumadóireachta a fhoghlaim. Chomh maith leis sin tugaimid isteach sraith méadracht mheastóireachta lena ndéanaimid anailís ar na teangacha foghlamtha. Is é an hipitéis atá againn ná gur cheart go mbeadh raon sonrach acmhainne samhaltaithe agus bandaleithead cainéal ann a chothaíonn struchtúr comhdhéanamh sa teanga a leanann as agus a spreagann ginearálú córasach dá bharr. Cé go bhfeicimid fianaise eimpíreach ag bun an raoin seo, is aisteach an rud é nach bhfaighimid fianaise don chuid is airde den raon agus creidimid gur ceist oscailte í seo don phobal.</abstract_ga>
      <abstract_ka>ახალი სამუშაო სამუშაო სამუშაო განსაზღვრებულია, რომელიც ახალგაზრდებული ენები შეუძლია გამოჩვენოთ თავისუფალური ენების მნიშვნელობები, გან ამ დოკუმენტში, ჩვენ განსხვავებთ სწავლების წინასწორება, რომელიც ეფექტიურობა და კომპოზიციალობა მრავალ ადვნენტის კომუნიკაციაში დამატებით კომუნიკაციური ჩვენი საუკეთესო დამატება არის განსხვავება, როგორ ნეიროლური ქსელის შესაძლებლობა მისი შესაძლებლობას შესწავლა კომპოციოლური ენაზე. ჩვენ დამატებით გავაკეთებთ განსაზღვრებული მეტრიკის ნაწილი, რომელიც ჩვენ გავაკეთებთ სწავლილი ენების ანალიზაცია. ჩვენი ჰიპოტეზა არის, რომ მოდელური კანოლიციის და კანალური ბანდის კომპოციონალური კომპოციონალური კომპოციონალური სტრუქტურაცია შემდეგ სისტემალური გენერალზა თუმცა ჩვენ ემპერიკურად ვხედავთ წარმოდგენები ამ დისონტის ქვემოთ, ვიცით, რომ არ გვიძებნა წარმოდგენები დისონტის უფრო დისონტის ნაწილად და გვჯერა, რომ ეს არის გახსნი</abstract_ka>
      <abstract_isl>Nýlega hefur verið rætt um hversu mikið nýjar tungumál geta sýnt eiginleika náttúrulegra tungumál, sérstaklega að læra samsetningu. In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication in addition to the communicative bandwidth.  Fyrsta hlutverkið okkar er a ð skoða hvernig getu taugakerfisins hefur áhrif á getu þess til að læra samsetningastofu. Við leggjum einnig fram mat með því a ð greina lært tungumál. Áætlun okkar er a ð það ætti að vera ákveðin umfang af líkani hæfni og rásbandbreidd sem örvar samsetningastofu í því tungumáli og því hvetjar kerfisbundna almenningu. Á međan viđ sjáum reyndarlega sönnunargögn fyrir neđan á ūessu svæđi finnum viđ ekki sönnunargögn fyrir efsta hluta svæđisins og teljum ađ ūetta sé opin spurning fyrir samfélagiđ.</abstract_isl>
      <abstract_hu>A legutóbbi munkák megvitatták, hogy a kialakuló nyelvek milyen mértékben mutathatják be a természetes nyelvek tulajdonságait, különösen a kompozicionalitás tanulását. Jelen tanulmányban azokat a tanulási előítéleteket vizsgáljuk, amelyek a kommunikációs sávszélesség mellett befolyásolják a többügynökös kommunikáció hatékonyságát és kompozicionalitását. Legfontosabb hozzájárulásunk annak feltárása, hogy egy ideghálózat kapacitása hogyan befolyásolja kompozíciós nyelv tanulásának képességét. Emellett bemutatunk egy sor értékelési mutatót, amelyekkel elemezzük a tanult nyelveket. Hipotézisünk szerint a modellkapacitás és a csatorna sávszélességének meghatározott skálájára van szükség, amely kompozíciós struktúrát idéz elő a kapott nyelvben, és ennek következtében ösztönzi a szisztematikus általánosítást. Bár empirikusan bizonyítékokat látunk ennek a tartománynak az aljára vonatkozóan, érdekes módon nem találunk bizonyítékokat a tartomány felső részére, és úgy véljük, hogy ez nyitott kérdés a közösség számára.</abstract_hu>
      <abstract_el>Πρόσφατα έργα έχουν συζητήσει τον βαθμό στον οποίο οι αναδυόμενες γλώσσες μπορούν να παρουσιάσουν ιδιότητες των φυσικών γλωσσών ιδιαίτερα της εκμάθησης της συνθηματικότητας. Στην παρούσα εργασία, διερευνούμε τις μαθησιακές προκαταλήψεις που επηρεάζουν την αποτελεσματικότητα και τη σύνθεση στην επικοινωνία πολλαπλών παραγόντων εκτός από το επικοινωνιακό εύρος ζώνης. Η κύρια συμβολή μας είναι να διερευνήσουμε πώς η ικανότητα ενός νευρωνικού δικτύου επηρεάζει την ικανότητά του να μάθει μια σύνθετη γλώσσα. Επίσης εισάγουμε ένα σύνολο μετρήσεων αξιολόγησης με τις οποίες αναλύουμε τις διδαγμένες γλώσσες. Η υπόθεσή μας είναι ότι θα πρέπει να υπάρχει ένα συγκεκριμένο εύρος ικανότητας μοντέλου και εύρους ζώνης καναλιού που να προκαλεί σύνθεση στη γλώσσα που προκύπτει και συνεπώς ενθαρρύνει τη συστηματική γενίκευση. Ενώ εμπειρικά βλέπουμε στοιχεία για το κάτω μέρος αυτού του εύρους, περιέργως δεν βρίσκουμε στοιχεία για το πάνω μέρος του εύρους και πιστεύουμε ότι αυτό είναι μια ανοικτή ερώτηση για την κοινότητα.</abstract_el>
      <abstract_it>Recenti lavori hanno discusso in che misura le lingue emergenti possono mostrare le proprietà dei linguaggi naturali, in particolare l'apprendimento compositivo. In questo articolo, esaminiamo i pregiudizi di apprendimento che influenzano l'efficacia e la compositività nella comunicazione multi-agente oltre alla larghezza di banda comunicativa. Il nostro contributo principale è quello di esplorare come la capacità di una rete neurale influisce sulla sua capacità di imparare un linguaggio compositivo. Inoltre introduciamo una serie di metriche di valutazione con cui analizziamo le lingue apprese. La nostra ipotesi è che ci dovrebbe essere una gamma specifica di capacità del modello e larghezza di banda del canale che induce la struttura compositiva nel linguaggio risultante e di conseguenza incoraggia la generalizzazione sistematica. Mentre vediamo empiricamente prove per il fondo di questa gamma, curiosamente non troviamo prove per la parte superiore della gamma e crediamo che questa sia una domanda aperta per la comunità.</abstract_it>
      <abstract_lt>Neseniai atliktuose darbuose aptarta, kokiu mastu atsirandančios kalbos gali įrodyti natūralių kalbų savybes, ypač mokymosi sudėtingumą. Šiame dokumente mes tiriame mokymosi iškraipymus, kurie daro poveikį veiksmingumui ir sudėtingumui įvairių veiksnių komunikacijoje, be ryšių juostos plotio. Svarbiausias mūsų indėlis yra ištirti, kaip nervinio tinklo pajėgumai daro poveikį jo gebėjimui mokytis sudėtinės kalbos. Be to, įvedame vertinimo metrijų rinkinį, kuriame analizuojame išmoktas kalbas. Mūsų hipotezė yra ta, kad turėtų būti tam tikras modelio pajėgumų ir kanalo juostos plotis, kuris skatina sudėtinę struktūrą gauta kalba ir todėl skatina sisteminę generalizaciją. Nors empiriniu požiūriu matome įrodymus, kad šios srities apačioje, stebima, kad neradome įrodymų viršutinei srities daliai ir manome, kad tai yra atviras klausimas bendruomenei.</abstract_lt>
      <abstract_mk>Неодамнешните дела разговараа за степенот во кој развиените јазици можат да изложат сопствености на природните јазици, особено учењето на композицијалноста. Во овој весник, ги истражуваме предрасудите за учење кои влијаат на ефикасноста и композицијалноста во комуникацијата со мултиагенти, покрај комуникационалната ширина. Нашиот најголем придонес е да истражуваме како капацитетот на нервната мрежа влијае на нејзината способност да научи композициски јазик. Дополнително воведуваме сет метрики за евалуација со кои ги анализираме научените јазици. Нашата хипотеза е дека треба да постои специфичен опсег на моделен капацитет и ширина на каналот кој индуцира композициска структура во резултатот на јазикот и, како последица, охрабрува систематска генерализација. Иако емпирички гледаме докази за дното на овој период, љубопитно не најдовме докази за врвот на периодот и веруваме дека ова е отворено прашање за заедницата.</abstract_mk>
      <abstract_ms>Recent works have discussed the extent to which emergent languages can exhibit properties of natural languages particularly learning compositionality.  Dalam kertas ini, kami menyelidiki biases pembelajaran yang mempengaruhi efektivitas dan komposisi dalam komunikasi multi-ejen selain lebar band komunikatif. Kontribusi utama kita adalah untuk mengeksplorasi bagaimana kapasitas rangkaian saraf mempengaruhi kemampuannya untuk belajar bahasa komposisi. Kami tambahan memperkenalkan set metrik penilaian yang kami analisis bahasa yang telah belajar. Hipotesis kita adalah bahawa seharusnya ada julat spesifik kapasitas model dan lebar saluran yang mengakibatkan struktur komposisi dalam bahasa yang menghasilkan dan secara konsekuensi mengakibatkan generalisasi sistemik. Sementara kita empirik melihat bukti untuk bahagian bawah julat ini, kita tidak ingin tahu bukti untuk bahagian atas julat dan percaya bahawa ini adalah soalan terbuka bagi masyarakat.</abstract_ms>
      <abstract_mt>Ix-xogħlijiet reċenti ddiskutew sa liema punt il-lingwi emerġenti jistgħu juru proprjetajiet ta’ lingwi naturali b’mod partikolari l-kompożizzjoni tat-tagħlim. F’dan id-dokument, ninvestigaw il-preġudizzji tat-tagħlim li jaffettwaw l-effikaċja u l-kompożizzjoni fil-komunikazzjoni b’diversi aġenti flimkien mal-medda ta’ frekwenzi komunikattivi. Our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language.  Barra minn hekk jintroduċu sett ta’ metriċi ta’ evalwazzjoni li bihom tanalizza l-lingwi mitgħallma. L-ipoteżi tagħna hija li għandu jkun hemm firxa speċifika ta’ kapaċità mudell u wisa’ frekwenza tal-kanal li jinduċi struttura kompożittiva fil-lingwa li tirriżulta u konsegwentement tinkoraġġixxi ġeneralizzazzjoni sistematika. Filwaqt li empirikament naraw evidenza għall-qiegħ ta’ din il-firxa, b’mod kurjuż ma ssibx evidenza għall-parti ta’ fuq tal-firxa u nemmnu li din hija mistoqsija miftuħa għall-komunità.</abstract_mt>
      <abstract_ml>അടുത്ത പ്രവര്‍ത്തനങ്ങള്‍ നിലവിലുള്ള ഭാഷകളുടെ സ്വാഭാവിക ഭാഷകളുടെ ഗുണഗണങ്ങള്‍ പ്രദര്‍ശിപ്പിക്കുന്നത് പ്രത്യേകിച ഈ പത്രത്തില്‍ നമ്മള്‍ പഠിക്കുന്ന പ്രശ്നങ്ങള്‍ അന്വേഷിക്കുന്നു. മുള്‍ജെന്‍റിന്‍റെ വിവരങ്ങള്‍ക്ക് കൂടാതെ പല-എജന്‍റിന്‍റെ വിവ നമ്മുടെ ആദ്യത്തെ പങ്ക് പരിശോധിക്കുന്നതാണ് ന്യൂറല്‍ നെറ്റര്‍ നെറ്റ്‌വര്‍ക്കിന്‍റെ കഴിവ് എങ്ങനെയാണ് അതിന്‍റെ സംഘ നമ്മള്‍ ഒരു കൂടുതല്‍ വിലാസങ്ങള്‍ പരിശോധിക്കുന്ന മെറ്റിക്കളെ കൂടുതല്‍ പരിചയപ്പെടുത്തുന്നു. നമ്മുടെ ഹൈപ്പിറ്റസിസിയസ് എന്താണെന്നാല്‍ മോഡലിന്റെ കഴിവും ചാനല്‍ ബാന്‍ഡ് വീതിയും ഒരു പ്രത്യേക പരിധിയുണ്ടാക്കേണ്ടത് അതിന്റെ ഫലമായ ഭാഷയി ഈ പരിധിയുടെ അടിത്തട്ടില്‍ തെളിവുകള്‍ കാണുമ്പോള്‍ നമുക്ക് അത്ഭുതകരമായി തെളിവുകള്‍ കണ്ടെത്താന്‍ കഴിയില്ല. ഇത് സമൂഹത്തിന് വ്യക്തമായ ഒര</abstract_ml>
      <abstract_no>Nyleg har arbeida diskutert kor mykje utgåande språk kan visa eigenskapar for naturspråk spesielt læring av komponentsitet. I denne papiret undersøker vi læringsforsikta som påvirkar effektiviteten og komposisjonalitet i multi agent-kommunikasjon i tillegg til kommunikasjonsbreidden. Vårt første bidrag er å utforske korleis kapasiteten til ein neuralnettverk påvirkar den kapasiteten til å lære eit sammensatt språk. Vi introduserer tillegg eit sett av evalueringsmetrikar som vi analyserer lærte språk. Dette er at det bør vera eit spesifikke område av modellekapasitet og kanalsbandbreidde som induserer sammensatte struktur i resultatet språk og følgjer systematisk generalisering. Mens vi empirisk ser beviser for bunnen av dette området, finn vi ikkje beviser for toppen av området og tror at dette er eit opna spørsmål for samfunnet.</abstract_no>
      <abstract_mn>Саяхан ажиллагаанууд нь байгалийн хэлний шинж чанарыг харуулж чадах хэмжээний талаар ярилцаж байна. Ялангуяа бүтээлч байдлыг сурах боломжтой. Энэ цаасан дээр бид олон агентын холбоотой холбоотой холбоотой үйл ажиллагаанд нөлөөлдөг суралцах урвалыг судалж байна. Бидний хамгийн эхний үүрэг бол мэдрэлийн сүлжээний чадварыг биеийн хэл сурах чадварыг хэрхэн нөлөөлдөг талаар судлах юм. Мөн бид сургалтын хэлний шинжилгээнд үнэлэх хэмжээний хэмжээг илтгэнэ. Бидний тодорхойлолт бол загварын чадвар болон загварын загварын өргөн байх ёстой. Үүний үр дүнд хэл дээр бий болгож, системийн ерөнхийлөгчийг дэмжих ёстой. Бид энэ хэмжээний доор нь баталгааг харж байгаа ч энэ хэмжээний ихэнх хэсэг нь баталгаа олохгүй гэдэгт итгэдэг.</abstract_mn>
      <abstract_pl>Ostatnie prace omówiły stopień, w jakim języki powstające mogą wykazywać właściwości języków naturalnych, w szczególności uczenia się kompozycyjności. W niniejszym artykule badamy uprzedzenia uczenia się, które wpływają oprócz przepustowości komunikacyjnej na skuteczność i kompozycjonalność w komunikacji wieloagentowej. Naszym głównym wkładem jest zbadanie, w jaki sposób zdolność sieci neuronowej wpływa na jej zdolność do nauki języka kompozycyjnego. Dodatkowo wprowadzamy zestaw wskaźników oceny, za pomocą których analizujemy nauczone języki. Nasza hipoteza jest taka, że powinien istnieć określony zakres pojemności modelu i przepustowości kanałów, który indukuje strukturę kompozycyjną w wynikowym języku i w konsekwencji zachęca do systematycznego uogólniania. Podczas gdy empirycznie widzimy dowody na dolną część tego zakresu, ciekawe, nie znajdujemy dowodów na górną część zakresu i uważamy, że jest to pytanie otwarte dla społeczności.</abstract_pl>
      <abstract_ro>Lucrările recente au discutat măsura în care limbile emergente pot prezenta proprietăți ale limbilor naturale, în special învățarea compoziționalității. În această lucrare, investigăm prejudecățile de învățare care afectează eficacitatea și compoziționalitatea în comunicarea multi-agenți în plus față de lățimea de bandă comunicativă. Contribuția noastră principală este de a explora modul în care capacitatea unei rețele neurale influențează capacitatea sa de a învăța un limbaj compozițional. În plus, introducem un set de măsurători de evaluare cu care analizăm limbile învățate. Ipoteza noastră este că ar trebui să existe o gamă specifică de capacitate de model și lățime de bandă a canalului care induce structura compozițională în limbajul rezultat și, prin urmare, încurajează generalizarea sistematică. Deși vedem empiric dovezi pentru partea de jos a acestui interval, curios nu găsim dovezi pentru partea de sus a intervalului și credem că aceasta este o întrebare deschisă pentru comunitate.</abstract_ro>
      <abstract_kk>Жуырдағы жұмыс істері табиғи тілдер қасиеттерін көрсету үшін, осылай қасиеттерді оқытуға болады. Бұл қағазда біз көп агенттің коммуникациялық бандықтардың қосымша, көп агенттік коммуникациялық және композициялықтың эффективнігіне әсер ететін оқыту күйін зерттеуді. Біздің алдыңғы қатынасыз - невралдық желінің көмегімен композициялық тілді оқыту мүмкіндігіне қалай әсер етеді. Қосымша біз оқылған тілдерді анализ етіп жатқан метрикалық жиынын таңдаймыз. Біздің гипотезиямыз - үлгі көлемі мен арналдық бандығының бірнеше аумағы болуы керек. Бұл тілде композиционалық құрылымды түсіндіреді. Сонымен қатар, жүйелік жалпы жалпы түрлендір Біз осы аумақтың төмендегі дәлелдерді көргенде, біз көңіл-көңіл аумақтың жоғары бөлігін таба алмаймыз. Бұл қоғамдық үшін ашық сұрақ деп сенеміз.</abstract_kk>
      <abstract_sr>Nedavni radovi su razgovarali o tome koliko se pojavljuju jezici mogu pokazati vlasništva prirodnih jezika posebno učenje kompozicionalnosti. U ovom papiru istražujemo predrasude učenja koje utječu na efikasnost i kompozicionalnost u multiagentnim komunikacijskim komunikacijskim širokostima. Naš prvi doprinos je da istražimo kako kapacitet neuralne mreže utiče na svoju sposobnost naučiti kompozicionalni jezik. Dodatno predstavljamo skup procjene metrika s kojom analiziramo naučene jezike. Naša hipoteza je da bi trebalo da postoji određen niz modelnih kapaciteta i širokosti kanala koji inducira kompozicionalnu strukturu u rezultatnom jeziku i stoga potiče sistematsku generalizaciju. Dok praktično vidimo dokaze za dnu ovog dometa, znatiželjno ne pronađemo dokaze za najveći deo dometa i verujemo da je ovo otvoreno pitanje za zajednicu.</abstract_sr>
      <abstract_si>අලුත් වැඩේ කතා කරලා තියෙන්නේ ප්‍රශ්ණ භාෂාවට ප්‍රශ්නයක් ස්වභාවික භාෂාවට ප්‍රශ්නයක් පෙන් මේ පත්තරේ අපි පරීක්ෂණය කරන්නේ ඉගෙන ගන්න පුළුවන් ප්‍රශ්නයක් සහ සංවිධානයක් ගොඩක් නියෝජිත සම්බන්ධයක් සමග ස අපේ ප්‍රධාන සම්පූර්ණය තමයි ප්‍රශ්නය කරන්නේ න්‍යූරාල ජාලයේ ක්‍රියාත්මක කොහොමද ඒකේ සම්පූර්ණ භාෂ අපි තවත් විශ්වාස කරනවා විශ්වාස කරන්න පුළුවන් මෙට්‍රික්ස් එකක් අපි ඉගෙන ගත්ත භාෂාවක් විශ්ල අපේ විශ්වාසය තමයි විශේෂ ප්‍රමාණය සහ චැනල් බැන්ඩ්විඩ් වලින් විශේෂ ප්‍රමාණයක් තියෙන්න ඕනි කියලා, ඒ වගේම ප්‍රම අපි ප්‍රශ්නයකින් මේ ප්‍රශ්නයේ පහලට සාක්ෂියක් දැක්කොත්, අපි හිතන්නේ පුළුවන් සාක්ෂියක් හොයාගන්නේ නැහැ ඒ වගේම</abstract_si>
      <abstract_so>Shaqooyinka la soo dhowaaday waxay kala sheekeysteen hadba marka luqada soo degdegta ah ay muujin karaan xuquuqda luuqadaha dabiiciga ah, si gaar ah u barashada iskuulaadka. Warqaddan waxaynu baaritaan waxqabadka waxbarashada oo saameyn ku yeelan kara saameynta iyo iskaashatada wargelinta dhaqaalaha kala duduwan iyo bandhigga xiriirka. Kharashadaada ugu horeeyay waa in baaraandegista awoodda shabakadda neuro ay saameyn ku yeelan karto awooddiisa barashada luqada kooxaha ah. We additionally introduce a set of evaluation metrics with which we analyze the learned languages.  Fiiniyadeena waa in ay jiraan awoodda modeliga iyo ballaadhka bandhigga kanaleyda oo dhismaha kooxaha ah ee luqada la soo jeedo, taas darteed waxay ku dhiirrigelisaa koritaanka nidaamka ah. Intaan si fiican u aragno caddeynta meeshan ugu hoosaysa, si yaab leh uma helno caddeyn darafka ugu sareeya, waxaynu aaminsanahay in taasu waa su'aal furan.</abstract_so>
      <abstract_sv>Nya arbeten har diskuterat i vilken utsträckning framväxande språk kan uppvisa egenskaper hos naturliga språk, särskilt inlärning av kompositionalitet. I denna uppsats undersöker vi lärandefördomar som påverkar effektiviteten och kompositionaliteten i multi-agent kommunikation utöver den kommunikativa bandbredden. Vårt främsta bidrag är att undersöka hur förmågan hos ett neuralt nätverk påverkar dess förmåga att lära sig ett kompositionsspråk. Dessutom introducerar vi en uppsättning utvärderingsmetoder med vilka vi analyserar de lärda språken. Vår hypotes är att det bör finnas ett specifikt spektrum av modellkapacitet och kanalbandbredd som inducerar kompositionsstruktur i det resulterande språket och därmed uppmuntrar systematisk generalisering. Även om vi empiriskt ser bevis för botten av detta intervall, finner vi nyfiket nog inte bevis för den övre delen av intervallet och tror att detta är en öppen fråga för samhället.</abstract_sv>
      <abstract_ta>சமீபத்தில் செயல்கள் வெளிப்பட்ட மொழிகளின் தன்மைகளை வெளிப்படுத்த முடியும் அளவில் விவாதம் செய்துள்ளார் இந்த காகிதத்தில், நாம் கற்றுக் கொள்ளும் பொருட்களை தொடர்பு கூடுதலான பாண்ட் அகலத்திற்கு மேலும் பாதிக்கும் விளைவுகள் மற்ற எங்கள் முதல் பங்கு ஒரு புதிய வலைப்பின்னலின் இயல்பை எப்படி பாதிக்கிறது என்பது தெரிந்து கொள்ளும் பொருள் மொழியை கற் நாம் கூடுதலாக ஒரு சில மதிப்பீட்டு மெட்ரிக்களை அறிவிக்கிறோம், அதைக் கொண்டு கற்ற மொழிகளை ஆராய்க்கிறோம். எங்கள் துப்பாக்கியம் என்னவென்றால் ஒரு குறிப்பிட்ட மாதிரி சக்தி மற்றும் வழி பாண்ட் அகலம் இருக்க வேண்டும் அது முடிவு மொழியில் உள்ள ஒரு அமைப இந்த வரம்பின் கீழ் தெளிவான அத்தாட்சிகளை நாம் பார்க்கும் போது, நாம் விசித்தியமாக இந்த வீச்சின் மேல் பகுதிக்கு தெளிவான ஆதா</abstract_ta>
      <abstract_ur>اگلے دنیا کی باتیں بحث کر رہی ہیں جس طرح اگلوں کی زبانیں طبیعی زبانوں کی خصوصی باتوں کی تعلیم سکتی ہیں۔ ہم اس کاغذ میں تحقیق کرتے ہیں کہ تعلیم کی بغیر معاملات بانڈ ویڈ کے علاوہ بہت سی اژنٹ کی تعلیم کے مطابق اثرات اور ترکیبات کی تأثیر دیتے ہیں. ہمارا سب سے پہلے حصہ یہ ہے کہ ایک نئورل نیٹورک کی قابلیت کس طرح اس کی قابلیت کا تأثیر کرتا ہے کہ ایک پیچیدگی زبان سکھائے۔ ہم اضافہ کے ساتھ ایک سٹ کا ارزش متریک معلوم کرتے ہیں جس سے ہم علم زبانوں کو تحلیل کرتے ہیں۔ ہماری فرضی یہ ہے کہ ایک مخصوص مدل کی مقدار اور چانال بانڈ ویڈ ہونا چاہیے جو نتیجہ زبان میں کامپیوتر کی ساختار کرتا ہے اور اس کے نتیجہ میں سیستمالی عمومی ساختار کی تحمل کرتا ہے۔ حالانکہ ہم اس سیمہ کے نیچے دلیلیں دیکھتے ہیں، ہمیں معلوم ہے کہ اس سیمہ کے اوپر سے کوئی دلیل نہیں پاتے اور یہ سمجھتے ہیں کہ یہ جماعت کے لئے کھلی سوال ہے۔</abstract_ur>
      <abstract_uz>Yaqinda ochilgan vazifalar tuzuvchi tillar asl tilning xossalarini bajarish, hususiy kompyuterni o'rganish uchun xususiyatlarni koʻrsatish mumkin. Bu qogʻozda, biz kommunikasi bandwidetidan ortiq multi-agent aloqalarining effekti va kompaniyalariga qo'llaniladigan o'rganishni o'rganamiz. Bizning birinchi qanday paydo bo'ladi, neyron tarmoqning qobiliyati bir kompyuterni o'rganish qobiliyatini o'zgartiradi. Biz ko'proq o'rganish tillarini o'rganish uchun bir necha qiymatni o'rganamiz. Bizning fikrimiz, model qobiliyati va kanal bandeni kengligi bo'lishi kerak. Bu natijada kompyuterning tuzuvlarini yaratadi va shunday qilib tizim yaratishga foydalanadi. Biz shu darajadagi hujjatni ko'rib turganimizda, qiziqqacha, chegaraning eng yuqori qismda hujjatni topmaymiz va bu jamiyat uchun ochiq savol deb ishonamiz.</abstract_uz>
      <abstract_vi>Các tác phẩm gần đây đã thảo luận đến mức độ những ngôn ngữ phát triển có thể mang tính chất của ngôn ngữ tự nhiên, đặc biệt là học về phối hợp. Trong tờ giấy này, chúng tôi nghiên cứu các bệnh tật về học tập ảnh hưởng đến hiệu quả và độ hợp tác trong giao tiếp đa tác nhân, cùng với độ rộng dây chuyền truyền thông. Quan điểm quan trọng của chúng ta là tìm hiểu khả năng của mạng thần kinh tác động đến khả năng học ngôn ngữ phân phối. Thêm vào đó chúng tôi sẽ giới thiệu một bộ đo hồ sơ đánh giá mà chúng tôi phân tích các ngôn ngữ học. Theo giả thuyết của chúng ta, phải có một khoảng cách đặc biệt về khả năng mô hình và băng tần rộng dẫn dẫn đến cấu trúc cấu trúc phân phối trong ngôn ngữ kết quả và do đó thúc đẩy tổng thống. Trong khi chúng tôi có kinh nghiệm thấy bằng chứng về mặt sâu nhất của phạm vi này, chúng tôi tò mò không tìm thấy bằng chứng cho phần trên của phạm vi và tin rằng đây là một câu hỏi mở cho cộng đồng.</abstract_vi>
      <abstract_bg>Последни произведения обсъждат степента, до която възникващите езици могат да проявят свойствата на естествените езици, особено учебната композиционна способност. В настоящата статия изследваме обучителните пристрастия, които оказват влияние върху ефикасността и композицията в мултиагентната комуникация в допълнение към комуникационната честотна лента. Нашият основен принос е да изследваме как капацитетът на невронната мрежа влияе върху способността й да учи композиционен език. Освен това въвеждаме набор от показатели за оценка, с които анализираме научените езици. Нашата хипотеза е, че трябва да има специфичен диапазон на капацитета на модела и честотната лента на канала, който индуцира композиционната структура в получения език и следователно насърчава систематичното обобщаване. Въпреки че емпирично виждаме доказателства за дъното на този диапазон, ние любопитно не намираме доказателства за горната част на диапазона и вярваме, че това е отворен въпрос за общността.</abstract_bg>
      <abstract_nl>Recente werken hebben besproken in hoeverre opkomende talen eigenschappen kunnen vertonen van natuurlijke talen, met name het leren van compositionaliteit. In dit artikel onderzoeken we de leerbiases die naast de communicatieve bandbreedte invloed hebben op de effectiviteit en compositionaliteit in multi-agent communicatie. Onze belangrijkste bijdrage is om te onderzoeken hoe de capaciteit van een neuraal netwerk invloed heeft op het vermogen om een compositietaal te leren. Daarnaast introduceren we een set evaluatiestatistieken waarmee we de geleerde talen analyseren. Onze hypothese is dat er een specifiek bereik van modelcapaciteit en kanaalbandbreedte moet zijn dat compositiestructuur in de resulterende taal induceert en bijgevolg systematische generalisatie aanmoedigt. Hoewel we empirisch bewijs zien voor de onderkant van deze range, vinden we vreemd genoeg geen bewijs voor het bovenste deel van de range en geloven we dat dit een open vraag is voor de gemeenschap.</abstract_nl>
      <abstract_da>De seneste værker har diskuteret, i hvilket omfang nye sprog kan udvise egenskaber ved natursprog, især læring af komposition. I denne artikel undersøger vi de læringsfordele, der påvirker effektiviteten og kompositionaliteten i multi-agent kommunikation ud over den kommunikative båndbredde. Vores vigtigste bidrag er at undersøge, hvordan kapaciteten af et neuralt netværk påvirker dets evne til at lære et kompositionssprog. Vi introducerer desuden et sæt evalueringsmetrics, som vi analyserer de lærte sprog med. Vores hypotese er, at der bør være en specifik vifte af modelkapacitet og kanalbåndbredde, der inducerer kompositionsstruktur i det resulterende sprog og dermed fremmer systematisk generalisering. Mens vi empirisk ser beviser for bunden af dette interval, finder vi underligt nok ikke beviser for den øverste del af intervalen og mener, at dette er et åbent spørgsmål for samfundet.</abstract_da>
      <abstract_hr>Nedavni radovi su razgovarali o mjeri u kojoj se pojavljujući jezici mogu pokazati vlasništva prirodnih jezika posebno učenje kompozicionalnosti. U ovom papiru istražujemo predrasude učenja koje utječu na učinkovitost i kompozicionalnost u komunikaciji s višestrukim agentima u dodatnoj komunikacijskoj širini banda. Naš prvi doprinos je da istražimo kako kapacitet neuralne mreže utječe na svoju sposobnost naučiti kompozicionalni jezik. Dodatno predstavljamo skup procjene metrika s kojom analiziramo učene jezike. Naša hipoteza je da bi trebala postojati određen niz modelnih kapaciteta i širokosti kanala koji inducira kompozicionalnu strukturu u rezultatnom jeziku i stoga potiče sistematsku generalizaciju. Iako praktično vidimo dokaze za dnu ovog dometa, znatiželjno ne pronalazimo dokaze za najveći dio dometa i vjerujemo da je to otvoreno pitanje zajednice.</abstract_hr>
      <abstract_de>Neuere Arbeiten haben diskutiert, inwieweit emergente Sprachen Eigenschaften natürlicher Sprachen aufweisen können, insbesondere lernende Kompositionalität. In diesem Beitrag untersuchen wir die Lernverzerrungen, die neben der kommunikativen Bandbreite auch die Effektivität und Kompositionalität in der Multiagentenkommunikation beeinflussen. Unser wichtigster Beitrag ist es zu untersuchen, wie die Fähigkeit eines neuronalen Netzwerks seine Fähigkeit beeinflusst, eine kompositorische Sprache zu lernen. Zusätzlich führen wir eine Reihe von Bewertungsmetriken ein, mit denen wir die erlernten Sprachen analysieren. Unsere Hypothese ist, dass es einen spezifischen Bereich von Modellkapazität und Kanalbandbreite geben sollte, der kompositorische Struktur in der resultierenden Sprache induziert und folglich eine systematische Generalisierung fördert. Obwohl wir empirisch Beweise für den unteren Teil dieses Bereichs sehen, finden wir seltsamerweise keine Beweise für den oberen Teil des Bereichs und glauben, dass dies eine offene Frage für die Gemeinschaft ist.</abstract_de>
      <abstract_id>Pekerjaan baru-baru ini telah mendiskusikan seberapa besar bahasa muncul dapat menunjukkan properti bahasa alami terutama belajar komposisionalitas. Dalam kertas ini, kami menyelidiki bias belajar yang mempengaruhi efisiensi dan komposisionalitas dalam komunikasi multi-agen selain lebar band komunikatif. Kontribusi utama kita adalah untuk mengeksplorasi bagaimana kapasitas jaringan saraf mempengaruhi kemampuannya untuk belajar bahasa komposisi. Kami tambahan memperkenalkan set metrik evaluasi dengan yang kami analisis bahasa belajar. Hipotesis kita adalah bahwa seharusnya ada jangkauan spesifik kapasitas model dan lebar saluran yang menginduksi struktur komposisi dalam bahasa hasilnya dan konsekuensinya mendorong generalisasi sistematis. Sementara kita empiris melihat bukti untuk bagian bawah dari jangkauan ini, kita penasaran tidak menemukan bukti untuk bagian atas dari jangkauan dan percaya bahwa ini adalah pertanyaan terbuka bagi masyarakat.</abstract_id>
      <abstract_fa>کارهای اخیرا به اندازه‌ای که زبانهای آشکار می‌توانند ویژگی زبانهای طبیعی را نشان دهند، در خصوصاً تعلیم ترکیب یادگیری را بحث کردند. در این کاغذ، ما تحقیق می‌کنیم چهره‌های یادگیری که تأثیر فعالیت و ترکیبی در ارتباط بسیاری از ماموران بیشتر در اضافه به گسترده باند ارتباطی تأثیر می‌دهند. اولین مشترک ما اینه که بفهمیم که توانایی شبکه عصبی چگونه توانایی آن را برای یاد گرفتن زبان ترکیب تاثیر می دهد. ما به اضافه یک مجموعه متریک ارزیابی را معرفی می کنیم که با آن زبانهای یاد گرفته را تحلیل می کنیم. فرضیه ما این است که باید یک مجموعه خاصی از توانایی مدل و چهارچوب کانال وجود داشته باشد که ساختار ترکیبی را در زبان نتیجه می دهد و به همین نتیجه نسبت به ژنرالی سیستم تحویل می دهد. در حالی که ما مدرک پایین این محدوده را می بینیم، کنجکاو ما مدرک بالای محدوده را پیدا نمی کنیم و باور می کنیم که این سوال باز برای جامعه است.</abstract_fa>
      <abstract_tr>Ýakyndaky işleri tebigat dilleriniň hasaplaryny görkezip biljek ýagdaýyny gürrüň edýärler. Bu kagyzda, gürrüňli baglanyşyk baglanyşyklaryň üstine etkinleşen öwrenme biaslaryny maslahat edýäris. Biziň öňki täsirimiz, näyral şebekeniň özüniň kompozisyonal dilini öwrenmek üçin nähili etjek ukyplaryny gözlemekdir. Biz ýene öwrenmeli dilleri çözümleýän çözümler metriklerini tapdyryp tanyşdyrýarys. Biziň ýagdaýymyz şol bir nusga ukyplaryň we kanalyň çykyşynyň bir görnüşi bolmaly däldir. Bu näme üçin kompozisyonal strukturyň netijesi bolan dilde döredir we bu sebäpli sistematik jeneralizasyona sebep edýär. Biz munuň aşagynda kanlag görýän bolsaňyz, geň pikirimçe bu sahypyň iň üsti bölegi üçin kanlag tapmaýarys we bu jemgyýet üçin açyk soragdyr diýip ynanýarlar.</abstract_tr>
      <abstract_sw>Kazi za hivi karibuni zimejadiliana na kiwango ambacho lugha zinazojitokeza zinaweza kuonyesha utaalam wa lugha za asili hususani kujifunza umuhimu. Katika karatasi hii, tunachunguza upendeleo wa kujifunza unaoathiri ufanisi na ushirikiano katika mawasiliano ya wafanyakazi wengi zaidi ya bandia ya mawasiliano. Mchango wetu wa kwanza ni kutambua jinsi uwezo wa mtandao wa neura unavyoathiri uwezo wake wa kujifunza lugha ya makundi. Tunaonyesha mbinu za uchunguzi ambazo tunachambua lugha zilizosomwa. Utafiti wetu ni kuwa kuna uwezo maalum wa mifano na upana wa mkononi unaotengeneza muundo wa ujenzi katika lugha inayosababisha na hivyo huhamasisha uzalishaji wa mfumo. Wakati tunaona ushahidi wa chini ya eneo hili, kwa ajabu hatupati ushahidi kwa upande wa juu wa eneo hilo na tunaamini kuwa hili ni swali la wazi kwa jamii.</abstract_sw>
      <abstract_af>Onlangse werke het die uitbreiding gespreek waarop uitkomstige tale eienskappe van natuurlike tale, spesifieke onderwerp van komponibiliteit. In hierdie papier, ondersoek ons die onderwerp wat die effektiviteit en komposiasionaliteit in multi-agent kommunikasie in byvoeg van die kommunikasiewydte. Ons voorste bydrang is om te ondersoek hoe die kapasiteit van 'n neuralnetwerk sy kapasiteit invloek om 'n samenskaplike taal te leer. Ons introduseer ook 'n stel van evalueringsmetries waarmee ons die geleerde tale analyseer. Ons hipotesis is dat daar 'n spesifieke omvang van model kapasiteit en kanaal bandwydte moet wees wat samenskaplike struktuur in die resulteerde taal induseer en daarna sistematiese generalisering moet voorspoedig. Terwyl ons empiriese getuienis sien vir die onderkant van hierdie omvang, ons waarskynlik nie vind getuienis vir die boonste deel van die omvang en glo dat dit 'n oop vraag is vir die gemeenskap.</abstract_af>
      <abstract_ko>최근의 작업은 신흥 언어가 어느 정도에 자연 언어의 특성을 나타낼 수 있는지, 특히 학습 조합성을 논의했다.본고에서 우리는 다중지능체의 통신 효율과 구성성에 영향을 주는 학습 편차, 그리고 통신 대역폭을 연구했다.우리의 가장 중요한 공헌은 신경 네트워크의 능력이 합성 언어를 배우는 능력에 어떻게 영향을 미치는지 탐색하는 것이다.이 밖에 우리는 배운 언어를 분석하는 데 사용할 평가 지표를 도입했다.우리의 가설은 특정한 모델 용량과 통로 대역폭 범위를 가지고 생성된 언어에서 합성 구조를 유도하여 체계화 개괄을 장려해야 한다는 것이다.비록 우리는 경험적으로 이 범위의 밑바닥 증거를 보았지만, 이상하게도, 우리는 이 범위의 끝부분 증거를 찾지 못했고, 이것은 지역 사회에 있어서 현안으로 남아 미해결의 문제라고 믿는다.</abstract_ko>
      <abstract_am>በአሁኑ ጊዜ የቋንቋ ቋንቋዎች በተለየ ትምህርት በተለይም ትምህርት የተደረገውን የፍጥረት ቋንቋዎች የስብስብሰባዎችን ማሳየት የሚችሉበትን ስርዓት ያሳያል፡፡ በዚህ ፕሮግራም፣ የመምህርት ግንኙነት እና የብዙ አካባቢ ግንኙነት በተጨማሪው ግንኙነት ላይ ጥቅም የሚያደርገውን ትምህርት ልማት እናመርምራለን፡፡ የፊተኛይቱ ጥያቄያችን የናውሬው መረብ መቻላቸውን እንዴት እንደሚያሳውቅ የቋንቋውን ለመማር ነው፡፡ በተጨማሪም የተማሩት ቋንቋዎችን የምናስተምርበት የቁጥጥር ማተሚያዎችን እናሳውቃለን፡፡ ጉዳዩ የሞዴል ኃይል እና የቻይሎችን የስፋት ስፋት እንዲኖር ነው፡፡ በዚህ ክፍል በታች ያሉትን ማስረጃዎችን ባየን ጊዜ፣ ለክፍሉ በላይ ክፍል ማስረጃዎችን ማግኘት እናምናለን ይህ ለቤተሰብ የተገለጠ ጥያቄ ነው ብለን እናምናለን፡፡</abstract_am>
      <abstract_sq>Recent works have discussed the extent to which emergent languages can exhibit properties of natural languages particularly learning compositionality.  Në këtë letër, ne hetojmë paragjykimet e mësimit që ndikojnë në efektshmërinë dhe kompozitivitetin në komunikimin me shumë agjentë përveç gjerësisë së frekuencës komunikuese. Kontributi ynë më i madh është të eksplorojmë se si kapaciteti i një rrjeti nervor ndikon në aftësinë e tij për të mësuar një gjuhë kompozitive. Përveç kësaj prezantojmë një sërë metrikësh vlerësimi me të cilat analizojmë gjuhët e mësuara. Hipoteza jonë është se duhet të ketë një gamë specifike të kapacitetit të modelit dhe gjerësisë së kanalit që indukton strukturën kompozitive në gjuhën që rezulton dhe kështu inkurajon gjeneralizimin sistematik. Ndërsa ne empirikisht shohim prova për fund të këtij intervali, ne kureshtar nuk gjejmë prova për pjesën më të lartë të intervalit dhe besojmë se kjo është një pyetje e hapur për komunitetin.</abstract_sq>
      <abstract_hy>Վերջին աշխատանքները քննարկեցին, թե ինչքան են զարգացած լեզուները կարող ցույց տալ բնական լեզուների հատկությունները, հատկապես սովորելու բաղադրությունը: Այս թղթի մեջ մենք ուսումնասիրում ենք ուսումնասիրության կողմնականությունները, որոնք ազդում են բազմագործականների հաղորդակցման արդյունավետության և բազմագործականների կազմակերպության վրա, բացի հաղորդակցման լայնության: Մեր ամենակարևոր ներդրումն այն է, որ ուսումնասիրենք, թե ինչպես է նյարդային ցանցի ունակությունը ազդում նյարդային ցանցի ունակության վրա բաղադրական լեզու սովորելու ունակությունը: Մենք նաև ներկայացնում ենք մի շարք գնահատման մետրիկներ, որոնց օգնությամբ մենք վերլուծում ենք սովոր լեզուները: Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization.  Մինչդեռ մենք էմպրիկապես տեսնում ենք ապացույցներ այս տարածքի ներքևում, մենք հետաքրքիր է, որ ապացույցներ չենք գտնում տարածքի վերևում և հավատում ենք, որ սա հասարակության համար բաց հարց է:</abstract_hy>
      <abstract_az>Son işlər təbiətli dillərin özlərinə təbiətli dillərin xüsusiyyətlərini göstərə biləcəyi qədər mübahisə edirlər. Bu kağıtda, çoxlu-ağantlı iletişimlərin çoxlu-ağantlı iletişimlərin istifadə edən öyrənmə təsirlərini incidirik. Əvvəlki qismətimiz, nöral şəbəkənin qabiliyyətinin kompozisyonal dilini öyrənmək bacarılığını necə etkilediyini keşfetməkdir. Biz də öyrəndiyimiz dilləri analiz edirik. Bizim hipotezimiz, modellərin səviyyəsi və kanalların səviyyəsi olması lazımdır ki, bunun sonucu dildə kompozisyonal strukturları təşkil edir və bunun ardınca sistematik generalizasyonu təşkil edir. Biz bu səviyyənin aşağısına dair dəlilləri görürsək də, bu səviyyənin ən yüksək hissəsində dəlillər tapmırıq və bu toplum üçün açıq bir sual olduğuna inanırıq.</abstract_az>
      <abstract_bs>Nedavni radovi su razgovarali o mjeri u kojoj se pojavljujući jezici mogu pokazati vlasništva prirodnih jezika posebno učenje kompozicionalnosti. U ovom papiru istražujemo predrasude učenja koje utječu na učinkovitost i kompozicionalnost u multiagentskoj komunikaciji, dodatno komunikacijskoj širini banda. Naš prvi doprinos je da istražimo kako kapacitet neuralne mreže utiče na svoju sposobnost naučiti kompozicionalni jezik. Dodatno predstavljamo skup procjene metrika s kojom analiziramo učeni jezik. Naša hipoteza je da bi trebala postojati određen niz modelnih kapaciteta i širokosti kanala koji inducira kompozicionalnu strukturu u rezultatnom jeziku i stoga potiče sistematsku generalizaciju. Dok empirički vidimo dokaze za dnu ovog dometa, znatiželjno ne pronalazimo dokaze za najveći dio dometa i vjerujemo da je to otvoreno pitanje za zajednicu.</abstract_bs>
      <abstract_bn>সাম্প্রতিক কাজের বিষয়টি আলোচনা করেছে প্রাকৃতিক ভাষার বৈশিষ্ট্য শিক্ষা বিশেষ করে প্রকৃত ভাষার বৈশিষ্ট্য প্রদর এই কাগজটিতে আমরা শিক্ষার বিরুদ্ধে তদন্ত করি যে ব্যান্ডিভ ব্যান্ডউইডের পরিবর্তে মাল্টিএজেন্টের যোগাযোগের কার্যকর এবং সংগ আমাদের প্রথম অংশগ্রহণ হচ্ছে একটি নিউরেল নেটওয়ার্কের ক্ষমতা কিভাবে তার ক্ষমতার প্রভাব ফেলে দিয়েছে একটি জমিত ভাষা শিখ আমরা আরো কিছু মূল্যবান মেট্রিকে পরিচয় করিয়ে দিচ্ছি যার সাথে আমরা শিক্ষিত ভাষাকে বিশ্লেষণ করি। আমাদের ধারণা হচ্ছে যে মডেলের ক্ষমতা এবং চ্যানেল ব্যান্ডাউডের একটি নির্দিষ্ট বিভিন্ন ধরনের ক্ষমতা থাকা উচিত যা এর ফলে ভাষায় সংগঠন কাঠামো তৈ যখন আমরা এই রেঞ্চটির নিচে প্রমাণ দেখতে পাই, তখন আমরা অদ্ভুত কোন প্রমাণ খুঁজে পাই না এবং বিশ্বাস করি যে এটা সম্প্রদায়ের জন্য একটি সুস্পষ্ট প্রশ্</abstract_bn>
      <abstract_ca>Recent works have discussed the extent to which emergent languages can exhibit properties of natural languages particularly learning compositionality.  En aquest article, investigam les tendències d'aprenentatge que afecten l'eficiència i la composicionalitat en la comunicació multi agent a més de l'ampleur de banda comunicativa. La nostra contribució principal és explorar com la capacitat d'una xarxa neuronal afecta la seva habilitat d'aprendre un llenguatge compositiu. També introduïm un conjunt de mètriques d'evaluació amb les quals analitzem les llengües aprengutes. La nostra hipòtesi és que hauria d'haver una gama específica de capacitat model i banda de canal que indueix l'estructura de composició en el llenguatge resultant i, conseqüentment, estimula la generalització sistemàtica. Mentre empíricament veiem evidències per a la part inferior d'aquesta gamma, curiosament no trobem evidències per a la part superior de la gamma i creiem que aquesta és una pregunta oberta per a la comunitat.</abstract_ca>
      <abstract_cs>Nedávné práce diskutují o tom, do jaké míry mohou vznikající jazyky vykazovat vlastnosti přirozených jazyků, zejména učení kompozici. V tomto článku zkoumáme učební předsudky, které ovlivňují efektivitu a kompozicionalitu v multiagentní komunikaci kromě komunikační šířky pásma. Naším hlavním příspěvkem je zkoumat, jak kapacita neuronové sítě ovlivňuje její schopnost učit se kompoziční jazyk. Dále představujeme sadu hodnotících metrik, se kterými analyzujeme naučené jazyky. Naše hypotéza je, že by měla existovat specifická škála kapacity modelu a šířky pásma kanálu, která indukuje kompoziční strukturu ve výsledném jazyce a následně podporuje systematickou generalizaci. I když empiricky vidíme důkazy pro spodní část tohoto rozsahu, zvláštně nenajdeme důkazy pro horní část rozsahu a věříme, že je to otevřená otázka pro komunitu.</abstract_cs>
      <abstract_et>Hiljutised tööd on arutanud, mil määral võivad tekkivad keeled ilmutada looduskeelte omadusi, eriti õppida kompositsiooni. Käesolevas töös uurime õppimisharjumusi, mis mõjutavad lisaks kommunikatsiooni ribalaiusele ka efektiivsust ja kompositsioonivõimet multiagentide kommunikatsioonis. Meie peamine panus on uurida, kuidas neurovõrgu võime mõjutab selle võimet õppida kompositsioonikeelt. Lisaks tutvustame hindamismeetodeid, millega analüüsime õppitud keeli. Meie hüpotees on, et peaks olema konkreetne mudeli mahtuvuse ja kanali ribalaiuse vahemik, mis indutseerib kompositsioonistruktuuri tekkinud keeles ja seega soodustab süstemaatilist üldistamist. Kuigi me näeme empiiriliselt tõendeid selle vahemiku alumise osa kohta, ei leia me huvitaval kombel tõendeid vahemiku ülemise osa kohta ja usume, et see on kogukonna jaoks avatud küsimus.</abstract_et>
      <abstract_fi>Viimeaikaisissa teoksissa on keskusteltu siitä, missä määrin kehittyvät kielet voivat näyttää luontaisten kielten ominaisuuksia erityisesti oppivaa kompositiivisuutta. Tässä työssä tutkitaan oppimisvääristymiä, jotka vaikuttavat vuorovaikutuksen kaistanleveyden lisäksi moniagenttisen viestinnän tehokkuuteen ja kompositiivisuuteen. Tärkein panos on tutkia, miten neuroverkon kyky vaikuttaa sen kykyyn oppia kompositiivista kieltä. Lisäksi esittelemme joukon arviointimittareita, joilla analysoimme opittuja kieliä. Hypoteesimme on, että mallin kapasiteetin ja kanavan kaistanleveyden tulisi olla tietty alue, joka indusoi kompositiivista rakennetta syntyvässä kielessä ja siten kannustaa systemaattiseen yleistymiseen. Vaikka näemme empiirisesti todisteita tämän vaihteluvälin alareunasta, kummallista kyllä emme löydä näyttöä vaihteluvälin yläosasta ja uskomme, että tämä on avoin kysymys yhteisölle.</abstract_fi>
      <abstract_jv>Awak dhéwé éntuk sistem yang pisan neng pakan kanggo sabên langkung pawaran gar-sabên karo ngregani ingkang sapa-sabên. Nang kuwi iki, awak dhéwé ngerasakno biasane kanggo ngilanggar nggambar obah-obahan lan basa sing berarti bantuan liyane karo komunikasi liyane uga-ajan Awak dhéwé bener kanggo kelas piye kapan ning netwisan nerunal sing nguasai kapan bangsane sampeyan ingkang sampeyan. Awak dhéwé nambah éntuk nggawe soko akeh pisan maning, dadi awak dhéwé nggunakake tarjamahan. Kita suposipun punika dipun ngomong nik kabèh saben kelas yen manut karo ingkang dipun-ingkang kuwi, nik kabèh basa sing bisalakno ngono nggawe nguasakno sistem sing bisalakno ngono nggawe sistem sing bisalakno. Nanging awak dhéwé ngerti perusahaan kanggo ngerti apa ning acara iki, awak dhéwé nganggep nggawe perusahaan kanggo ngerti apa ning acara iki sakjané awak dhéwé iki perusahaan kanggo kowé.</abstract_jv>
      <abstract_sk>Nedavna dela so obravnavala obseg, v katerem lahko nastajajoči jeziki kažejo lastnosti naravnih jezikov, zlasti učne kompozicijske lastnosti. V prispevku raziskujemo učne pristranskosti, ki poleg komunikacijske pasovne širine vplivajo na učinkovitost in kompozicijnost komunikacije z več agenti. Naš glavni prispevek je raziskati, kako zmogljivost nevronskega omrežja vpliva na njegovo sposobnost učenja kompozicijskega jezika. Dodatno predstavljamo nabor meritev vrednotenja, s katerimi analiziramo učene jezike. Naša hipoteza je, da bi moral obstajati specifičen obseg zmogljivosti modela in pasovne širine kanala, ki inducira kompozicijsko strukturo v nastalem jeziku in posledično spodbuja sistematično generalizacijo. Čeprav empirično vidimo dokaze za dno tega obsega, zanimivo ne najdemo dokazov za zgornji del obsega in verjamemo, da je to odprto vprašanje za skupnost.</abstract_sk>
      <abstract_he>העבודות האחרונות דיברו על המידה שבה שפות מתפתחות יכולות להראות תכונות של שפות טבעיות במיוחד ללמוד מיוחד. בעיתון הזה, אנו חוקרים את ההתמחות ללימודים שמשפיעות על יעילות ומרכיבות בתקשורת multi-סוכנים בנוסף לרחבי הקשר. התרומה הראשונה שלנו היא לחקור איך היכולת של רשת עצבית משפיעה על היכולת שלה ללמוד שפה מורכבת. בנוסף, אנחנו מציגים קבוצה של מטריות הערכה שבה אנו מנתחים את השפות הלמדות. ההיפוטזה שלנו היא שיש טווח ספציפי של יכולת מודל ורחב ערוץ שעורר מבנה מורכב בשפה הנוצאה ולכן מעודד הגנרליזציה מערכתית. בזמן שאנחנו רואים בעצמנו ראיות למטה של הטווח הזה, אנו סקרנות לא מוצאים ראיות לחלק העליון של הטווח ואמינים שזו שאלה פתוחה לקהילה.</abstract_he>
      <abstract_ha>Haƙĩƙa, masu aiki da ke ƙara sun yi jayayya da gwargwadon da lugha masu fitarwa za'a iya nuna properties wa lugha masu natsuwa, da haske da za'a sanar da composition. Ga wannan takardan, Munã karatun karatun musamman da za ta yi amfani ga masu amfani da composition cikin kommunikasin masu multi-ajani bayan bada bandwidget. Bayanmu na farko da aikinsa ni'anar ka gane jinin abincin a haɗi na neura yana shagala awonsa ga karanta lugha mai haɗuwa. Tuna iya ƙara da wasu metrici masu ƙaddara, da shi Muke yi analyza lugha waɗanda aka sani. Kayyadanmu na cẽwa, za'a sami wani ma'abũcin shirin ayuka da bandwidi wanda ke samar da matsayin da ke cikin harshen ta ƙara, kuma a bayan haka, yana ƙaramar da halin na'ura. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.</abstract_ha>
      <abstract_bo>འཕྲལ In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication in addition to the communicative bandwidth. ང་ཚོའི་གཙོ་ཆེ་ཤོས་དུ་འདྲ་བ་དང་མཐུན་ཤུགས་ཀྱི་སྐོར་དང་འབྲེལ་རྩིས་ཤེས་ཀྱི་ཆ་རྐྱེན་ནི་རྟོགས་ཤིག་དང་། We additionally introduce a set of evaluation metrics with which we analyze the learned languages. ང་ཚོའི་གྲངས་སུ་མཐུན་པར་དབྱེ་བ་དང་རྒྱུ་ལམ་ཀྱི་ཆ་མཐུན་ཚད་དམིགས་འཛུགས་དགོས་པ་དེ་ནི་འབྱུང་པའི་སྐད་ཡིག ང་ཚོས་རང་ཉིད་ཀྱི་ཁྱབ་ཚད་འདིའི་མཇུག་གི་ལ་ལྟར་མཐོང་བའི་ལྟ་བ་ཡིད་དུས་ངེད་ཚོའི་རྐྱེན་ཚད་ལྟར་པར་མཐོང་སྣང་མེད་པར། འོས་ན</abstract_bo>
      <abstract_fil>Ang mga bagay na kasalukuyan ay nangagsalita ng kadakilaan kung saan ang mga wikang nagkakaroon ay makapagpapakita ng mga pag-aari ng natural languages, talagang sa pag-aaral ng komposibilidad. Sa papiro na ito, aming pinagsisiyasat ang mga bias ng pag-aaral na nagbibigay ng efficacy at compositionality sa multi-agent communication bukod sa communicative bandwidth. Ang pinakamaunang pagbibigay namin ay pag-isiyasat kung paano ang kapangyarihan ng neural network ay nagbibigay ng kapangyarihan niyang mag-aaral ng wikang komposiyonal. Dahil dito'y ipinakilala namin ang isang set ng evaluasyon na metrics na aming isinasali ang mga natutunan na wika. Ang aming hipotesis ay dapat magkaroon ng isang espesyal na talaan ng modelo na kapangyarihan at bandwidth ng kanal na nagbibigay ng salimuot na struktura sa resulting wika at kung magkagayo'y nagbibigay ng sistematika na generalizasyon. Samantalang nakikita natin ng mga prudente sa ilalim ng range na ito, hindi natin nakikita ng mga prudente para sa pinakamataas na bahagi ng range at naniniwala na ito ay isang bukas na tanong para sa komunidad.</abstract_fil>
      </paper>
    <paper id="6">
      <title>Learning Geometric Word Meta-Embeddings</title>
      <author><first>Pratik</first><last>Jawanpuria</last></author>
      <author><first>Satya Dev</first><last>N T V</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Bamdev</first><last>Mishra</last></author>
      <pages>39–44</pages>
      <abstract>We propose a geometric framework for learning meta-embeddings of words from different embedding sources. Our framework transforms the <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> into a common latent space, where, for example, simple averaging or concatenation of different embeddings (of a given word) is more amenable. The proposed latent space arises from two particular geometric transformations-source embedding specific orthogonal rotations and a common Mahalanobis metric scaling. Empirical results on several word similarity and word analogy benchmarks illustrate the efficacy of the proposed framework.</abstract>
      <url hash="9eae4748">2020.repl4nlp-1.6</url>
      <attachment type="Software" hash="89828011">2020.repl4nlp-1.6.Software.zip</attachment>
      <doi>10.18653/v1/2020.repl4nlp-1.6</doi>
      <video href="http://slideslive.com/38929772" />
      <bibkey>jawanpuria-etal-2020-learning</bibkey>
    <title_ar>تعلم هندسية كلمة ميتا Embeddings</title_ar>
      <title_fr>Apprentissage des méta-embeddings de mots géométriques</title_fr>
      <title_pt>Aprendendo Meta-Embeddings de Palavras Geométricas</title_pt>
      <title_es>Aprendizaje de metaincrustaciones de palabras geométricas</title_es>
      <title_ja>ジオメトリワードのメタ埋め込みの学習</title_ja>
      <title_zh>学几何词元嵌</title_zh>
      <title_ru>Изучение геометрического слова Meta-Embeddings</title_ru>
      <title_hi>ज्यामितीय शब्द मेटा-एम्बेडिंग सीखना</title_hi>
      <title_ukr>Навчання геометричного слова Мета-поглинання</title_ukr>
      <title_ga>Ag Foghlaim Meitea-Leabaithe Focal Geoiméadrach</title_ga>
      <title_isl>Name</title_isl>
      <title_ka>Name</title_ka>
      <title_hu>Geometriai szavak metabeágyazása</title_hu>
      <title_el>Μετα-ενσωμάτωση γεωμετρικών λέξεων εκμάθησης</title_el>
      <title_it>Imparare metaincorporazioni di parole geometriche</title_it>
      <title_kk>Геометриялық сөз мета- ендіруді үйрену</title_kk>
      <title_ml>ജിയോമിറ്റിക്കല്‍ വാക്ക് മെറ്റ- എംബെഡിങുകള്‍ പഠിക്കുന്നു</title_ml>
      <title_mk>Name</title_mk>
      <title_mt>Tagħlim Meta-Embeddings tal-Kliem Ġeometriċi</title_mt>
      <title_mn>Геометрийн үг мета-интервейтинг сурах</title_mn>
      <title_no>Lær geometriske ord- metainnbygging</title_no>
      <title_pl>Nauka geometrycznych metaosadzeń słowa</title_pl>
      <title_ro>Învățarea metaîncorporărilor geometrice de cuvinte</title_ro>
      <title_lt>Mokymasis geometrinių žodžių metaįdėjimais</title_lt>
      <title_ms>Mempelajari Meta-Embedding Kata Geometrik</title_ms>
      <title_si>භායාමිතික වචන මෙටා- ඇතුළුම් ඉගෙන ගන්න</title_si>
      <title_so>Barista hadalka Geometric Meta-Embedding</title_so>
      <title_sr>Naučenje geometrijskih reèi meta- integracija</title_sr>
      <title_ta>வடிவியல் வார்த்தை மெடா- உட்பொதிகளை கற்றுக்கொண்டிருக்கிறது</title_ta>
      <title_sv>Metainbäddningar av geometriska ord</title_sv>
      <title_ur>Geometric Word Meta- EmbeddingName</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Học về siêu ảnh từ ngắn</title_vi>
      <title_hr>Naučenje geometrijskih riječi meta- integracija</title_hr>
      <title_da>Metaindlejringer af geometriske ord</title_da>
      <title_bg>Учене на геометрични метавграждания на думи</title_bg>
      <title_nl>Geometrische woordmeta-embeddings leren</title_nl>
      <title_id>Mempelajari Meta-Embedding Kata Geometris</title_id>
      <title_ko>학습 기하학 단어 원 삽입</title_ko>
      <title_sw>Kujifunza neno la Geometric</title_sw>
      <title_af>Name</title_af>
      <title_de>Geometrische Wort-Meta-Einbettungen lernen</title_de>
      <title_sq>Mësimi i metaintegrimit të fjalëve gjeometrike</title_sq>
      <title_am>ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s</title_am>
      <title_fa>蹖丕丿 诏乇賮鬲賳 賲鬲丕- 丿丕禺賱 讴賱賲丕鬲 跇芯屑丕鬲乇蹖讴蹖</title_fa>
      <title_bn>জিওমিট্রিক শব্দ মেটা-এমবেডিং শিখানো</title_bn>
      <title_tr>Geometrik Sözi Meta-Girişini öwrenmek</title_tr>
      <title_hy>Գերոմետրական բառերի մետաներգրավման սովորելը</title_hy>
      <title_cs>Učení se geometrických slov meta-vložení</title_cs>
      <title_ca>Learning Geometric Word Meta-Embeddings</title_ca>
      <title_bs>Naučenje geometrijskih riječi meta- integracija</title_bs>
      <title_az>Geometrik Kelimi Meta-캻칞eri 칬yr톛nm톛k</title_az>
      <title_fi>Geometristen sanametaupotusten oppiminen</title_fi>
      <title_et>Geomeetriliste sõnade metapõimimiste õppimine</title_et>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_jv>politenessoffpolite"), and when there is a change ("assertivepoliteness</title_jv>
      <title_he>ללמוד מילים גאומטריות</title_he>
      <title_sk>Učenje geometričnih metavdelav besed</title_sk>
      <title_bo>Learning Geometric Word Meta-Embeddings</title_bo>
      <title_fil>Nagtuturo ng Meta-Embedding ng Geometric Word</title_fil>
      <abstract_ar>نقترح إطارًا هندسيًا لتعلم التضمينات الوصفية للكلمات من مصادر التضمين المختلفة. يحول إطار العمل الخاص بنا الزخارف إلى مساحة كامنة مشتركة ، حيث ، على سبيل المثال ، يكون المتوسط البسيط أو تسلسل الزخارف المختلفة (لكلمة معينة) أكثر ملاءمة. تنشأ المساحة الكامنة المقترحة من تحوليين هندسيين محددين - مصدر يتضمن دورات متعامدة محددة ومقياس Mahalanobis المتري المشترك. توضح النتائج التجريبية على العديد من الكلمات المتشابهة ومعايير قياس الكلمات فعالية الإطار المقترح.</abstract_ar>
      <abstract_fr>Nous proposons un cadre géométrique pour l'apprentissage des méta-intégrations de mots provenant de différentes sources d'intégration. Notre structure transforme les intégrations en un espace latent commun, où, par exemple, une simple moyenne ou une concaténation de différents intégrations (d'un mot donné) est plus facile. L'espace latent proposé résulte de deux transformations géométriques particulières : la source incorporant des rotations orthogonales spécifiques et une échelle métrique commune de Mahalanobis. Les résultats empiriques sur plusieurs points de référence de similarité de mots et d'analogie de mots illustrent l'efficacité du cadre proposé.</abstract_fr>
      <abstract_es>Proponemos un marco geométrico para aprender metaincrustaciones de palabras de diferentes fuentes de incrustación. Nuestro marco transforma las incrustaciones en un espacio latente común, donde, por ejemplo, el promedio simple o la concatenación de diferentes incrustaciones (de una palabra dada) es más fácil. El espacio latente propuesto surge de dos transformaciones geométricas particulares: la fuente que incorpora rotaciones ortogonales específicas y una escala métrica común de Mahalanobis. Los resultados empíricos en varios puntos de referencia de similitud de palabras y analogía de palabras ilustran la eficacia del marco propuesto.</abstract_es>
      <abstract_pt>Propomos uma estrutura geométrica para aprender meta-embeddings de palavras de diferentes fontes de embedding. Nossa estrutura transforma os embeddings em um espaço latente comum, onde, por exemplo, a média simples ou a concatenação de diferentes embeddings (de uma determinada palavra) é mais acessível. O espaço latente proposto surge de duas transformações geométricas particulares - fonte embutindo rotações ortogonais específicas e uma escala métrica comum de Mahalanobis. Resultados empíricos em vários benchmarks de similaridade de palavras e analogia de palavras ilustram a eficácia da estrutura proposta.</abstract_pt>
      <abstract_ja>さまざまな埋め込みソースからの単語のメタ埋め込みを学習するための幾何学的フレームワークを提案します。私たちのフレームワークは、埋め込みを共通の潜在スペースに変換します。たとえば、（特定の単語の）異なる埋め込みの単純な平均化または連結化は、より適しています。提案されている潜在空間は、2つの特定の幾何学的変換-特定の直交回転のソース埋め込みと一般的なマハラノビスメトリクススケーリングから生じる。いくつかの単語の類似性と単語の類似性ベンチマークに関する実証結果は、提案されたフレームワークの有効性を示している。</abstract_ja>
      <abstract_zh>吾言一何框架,学自异嵌单词。 其框架嵌为一同者潜于空间,如简均联(给定单词)之异嵌为宜。 其间源于两特定几何变易 - 源嵌特定者正交旋常见Mahalanobis度量缩放。 若干词相似性与词类验所出框架有效性。</abstract_zh>
      <abstract_ru>Предложен геометрический каркас для изучения мета-вложений слов из разных источников вложений. Наша структура преобразует вложения в общее латентное пространство, где, например, простое усреднение или конкатенация различных вложений (данного слова) более удобны. Предлагаемое скрытое пространство возникает в результате двух конкретных геометрических преобразований - встраивания источника конкретных ортогональных вращений и общего масштабирования метрики Махаланобиса. Эмпирические результаты по нескольким критериям подобия слов и аналогии слов иллюстрируют эффективность предлагаемой структуры.</abstract_ru>
      <abstract_hi>हम विभिन्न एम्बेडिंग स्रोतों से शब्दों के मेटा-एम्बेडिंग सीखने के लिए एक ज्यामितीय ढांचे का प्रस्ताव करते हैं। हमारा ढांचा एम्बेडिंग को एक सामान्य अव्यक्त स्थान में बदल देता है, जहां, उदाहरण के लिए, विभिन्न एम्बेडिंग (किसी दिए गए शब्द का) का सरल औसत या संयोजन अधिक अनुकूल है। प्रस्तावित अव्यक्त स्थान दो विशेष ज्यामितीय परिवर्तनों से उत्पन्न होता है - स्रोत विशिष्ट ऑर्थोगोनल रोटेशन और एक आम महालनोबिस मीट्रिक स्केलिंग को एम्बेड करता है। कई शब्द समानता और शब्द सादृश्य बेंचमार्क पर अनुभवजन्य परिणाम प्रस्तावित ढांचे की प्रभावकारिता को दर्शाते हैं।</abstract_hi>
      <abstract_ukr>Запропоновано геометричний каркас для вивчення мета-вузлів слів з різних джерел вбудовування. Наш фреймворк перетворює вбудовані елементи у загальний латентний простір, де, наприклад, просте усереднення або конкатенація різних вбудованих елементів (даного слова) є більш придатною. Запропонований прихований простір виникає внаслідок двох конкретних геометричних перетворень - вбудовування джерела конкретних ортогональних обертань і загального масштабування метрики Махаланобіса. Емпіричні результати щодо кількох критеріїв подібності слів та аналогії слів ілюструють ефективність запропонованої структури.</abstract_ukr>
      <abstract_ga>Molaimid creat geoiméadrach chun meiteabhrúite focal a fhoghlaim ó fhoinsí éagsúla leabú. Athraíonn ár gcreat na leabaithe go spás folaigh coiteann, mar a bhfuil, mar shampla, meánú simplí nó comhghaolú simplí de leabaithe éagsúla (de fhocal ar leith) níos éasca. Eascraíonn an spás folaigh atá beartaithe as dhá chlaochlú geoiméadrach ar leith - foinse ag neadú uainíochta orthogonal sonracha agus scálaithe méadrach Mahalanobis coiteann. Léiríonn torthaí eimpíreacha ar thagarmharcanna cosúlachta roinnt focal agus analaí focal éifeachtúlacht an chreata atá beartaithe.</abstract_ga>
      <abstract_ka>ჩვენ დავიწყებთ გეომეტრიკური ფრამეტრი სიტყვების მეტა-ინტებიზაციის სწავლებისთვის განსხვავებული ინტებიზაციის ფოსტებიდან. ჩვენი პარამეტრი გადატანქტირებს საერთო ლანტენტის სივრცე, სადაც, მაგალითად, განსხვავებული სიტყვის განსხვავებული განსხვავებული განსხვავებების განსხვავება ან გადასხვავება უფრო დასხვავ პროგრამეტრებული ლეტანტიური სივრცე იქნება ორი განსაკუთრებული გეომეტრიური ტრანფორმაციებიდან - გამოყენებული სპექტური ორტოდონალური ცვლილებებიდან და საერთო მაჰალანო თმორიკალური წარმოდგენები რამდენიმე სიტყვების განსხვავება და სიტყვების ანალოგიური ბანქმეპების გამოსახულება, რომლებიც გამოსახულებული ფრამეტრის ეფექ</abstract_ka>
      <abstract_el>Προτείνουμε ένα γεωμετρικό πλαίσιο για την εκμάθηση μετα-ενσωμάτωσης λέξεων από διαφορετικές πηγές ενσωμάτωσης. Το πλαίσιο μας μετατρέπει τις ενσωματώσεις σε έναν κοινό λανθάνοντα χώρο, όπου, για παράδειγμα, η απλή μέση μέτρηση ή η αλληλουχία διαφορετικών ενσωματώσεων (μιας δεδομένης λέξης) είναι πιο εύκολη. Ο προτεινόμενος λανθάνοντας χώρος προκύπτει από δύο συγκεκριμένους γεωμετρικούς μετασχηματισμούς στην πηγή που ενσωματώνουν συγκεκριμένες ορθογώνιες περιστροφές και μια κοινή μετρική κλιμάκωση του Μαχαλανόβη. Τα εμπειρικά αποτελέσματα σε διάφορα σημεία αναφοράς ομοιότητας λέξεων και αναλογιών λέξεων καταδεικνύουν την αποτελεσματικότητα του προτεινόμενου πλαισίου.</abstract_el>
      <abstract_hu>Geometriai keretrendszert javasolunk a különböző beágyazási forrásokból származó szavak metabeágyazásának tanulására. Keretrendszerünk a beágyazásokat egy közös látens térré alakítja át, ahol például a különböző beágyazások (egy adott szó) egyszerű átlagosítása vagy összekapcsolása alkalmasabb. A javasolt látens tér két különleges geometriai átalakításból ered: specifikus ortogonális forgásokat beágyazó forrás és egy közös Mahalanobis metrikus skálázás. A javasolt keretrendszer hatékonyságát több szóhasonlósági és szóanalógiai referenciaérték empirikus eredményei mutatják be.</abstract_hu>
      <abstract_it>Proponiamo un framework geometrico per imparare meta-embedding di parole da diverse fonti di embedding. Il nostro framework trasforma gli embedding in uno spazio latente comune, dove, ad esempio, la semplice media o concatenazione di diversi embedding (di una data parola) è più suscettibile. Lo spazio latente proposto deriva da due trasformazioni geometriche particolari - sorgente che incorpora rotazioni ortogonali specifiche e una scala metrica Mahalanobis comune. Risultati empirici su diversi parametri di somiglianza delle parole e analogia delle parole illustrano l'efficacia del quadro proposto.</abstract_it>
      <abstract_kk>Біз бірнеше ендіру көзінен сөздерді мета-ендіру үшін геометриялық фреймін ұсынамыз. Біздің фрейміміз ендіруді жалпы latent орынға аударады. Мысалы, түрлі ендірудің (келтірілген сөздің) бағдарламасының қарапайым ортаңбалауы не біріктіруі мүмкін. Келтірілген латенттік орын екі геометриялық түрлендірімінен келеді - көзі ортогоналық бұрылығын ендіру және жалпы Махаланобис метрикалық масштабынан келеді. Бірнеше сөздердің ұқсас пен сөздердің аналогиялық бағдарламаларының империялық нәтижелері келтірілген бағдарламаның эффективнігін көрсетеді.</abstract_kk>
      <abstract_lt>Siūlome geometrinę sistemą, skirtą mokymuisi meta įterpti žodžius iš skirtingų įterpiamųjų šaltinių. Mūsų sistema paverčia įdėjimus bendra latentine erdve, kur, pavyzdžiui, paprastesnis įvairių įdėjimų (tam tikro žodžio) vidurkis arba sutapimas yra patogesnis. Siūloma latentinė erdvė susidaro dėl dviejų konkrečių geometrinių transformacijų - šaltinio, kuriame yra specialios ortogoninės rotacijos ir bendras Mahalanobis metrinis skaliavimas. Imperijos rezultatai dėl kelių žodžių panašumo ir žodžių analogijos lyginamųjų rodiklių rodo siūlomos sistemos veiksmingumą.</abstract_lt>
      <abstract_mk>Предложуваме геометрична рамка за учење мета-вградувања на зборови од различни вградувачки извори. Нашата рамка ги трансформира внатрешностите во заеднички лантен простор, каде, на пример, едноставното просекување или концентрација на различни внатрешности (на одреден збор) е поудобно. Предложениот лантен простор се појавува од две специфични геометриски трансформации - извор кој вклучува специфични ортогонски ротации и заедничко метричко скалирање на Махаланобис. Империските резултати на неколку зборови сличност и споредби на зборови аналогија ја илустрираат ефикасноста на предложената рамка.</abstract_mk>
      <abstract_ms>Kami cadangan geometrik untuk mempelajari penerbangan meta perkataan dari sumber penerbangan yang berbeza. Bingkai-bingkai kami mengubah penyembedding kepada ruang yang tersembunyi umum, di mana, contohnya, rata-rata sederhana atau persatuan penyembedding yang berbeza (dari perkataan yang diberikan) lebih selesa. Ruang tersembunyi yang diusulkan muncul dari dua perubahan geometrik tertentu - sumber memasukkan putaran ortogonal tertentu dan skala metrik Mahalanobis umum. Keputusan empirik pada beberapa perkataan persamaan dan tanda referensi analogi perkataan menunjukkan kegunaan kerangka yang direncanakan.</abstract_ms>
      <abstract_ml>വ്യത്യസ്തമായ വാക്കുകള്‍ പഠിപ്പിക്കുന്നതിന് വേറെ വാക്കുകള്‍ പഠിപ്പിക്കാന്‍ വേണ്ടി ഒരു ജോമിത്രിക് ഫ്രെ നമ്മുടെ ഫ്രെയിമ്പുകള്‍ സാധാരണ സ്ഥലത്തേക്ക് മാറ്റുന്നു. ഉദാഹരണത്തിനായി വ്യത്യസ്ത വിഭാഗങ്ങളുടെ (കൊടുത്ത വാക്കിന്റെ വാക്കുകളില The proposed latent space arises from two particular geometric transformations - source embedding specific orthogonal rotations and a common Mahalanobis metric scaling.  ഒരുപാട് വാക്കുകളുടെയും വാക്കുകളുടെയും അന്യായോജി ബെങ്ക്മാര്‍ക്കുകളുടെയും സാമ്രിക്കല്‍ ഫലങ്ങള്‍ പ്രോദ്ദേശിപ്പി</abstract_ml>
      <abstract_mt>Aħna nipproponu qafas ġeometriku għat-tagħlim ta’ meta-inkorporazzjonijiet ta’ kliem minn sorsi differenti ta’ inkorporazzjoni. Il-qafas tagħna jittrasforma l-inkorporazzjonijiet fi spazju komuni moħbi, fejn, pereżempju, il-medja sempliċi jew il-konċentrazzjoni ta’ inkorporazzjonijiet differenti (ta’ kelma partikolari) hija aktar faċli. L-ispazju latenti propost jirriżulta minn żewġ trasformazzjonijiet ġeometriċi partikolari - sors li jinkorpora rotazzjonijiet ortogonali speċifiċi u skala metrika komuni tal-Mahalanobis. Riżultati empiriċi fuq bosta punti ta’ riferiment ta’ similarità tal-kliem u analogija tal-kliem juru l-effikaċja tal-qafas propost.</abstract_mt>
      <abstract_isl>Við leggjum fram margfeldismeðaltal fyrir a ð læra meta-innsetningu orða frá mismunandi innsetningarheimildum. Umsetning okkar breytir innsetningum í sameiginlegt leynt svæði, þar sem til dæmis einfalt meðaltal eða samsetning mismunandi innsetningar (af ákveðinu or ði) er hægari. Fyrirlagður leyndur pláss kemur fram úr tveimur sérstakum geometric breytingum - uppruna sem inniheldur sérstakar staðbundnar breytingar og algengar Mahalanobis mælingar. Reglulegar niðurstöður á nokkrum orðsvipuðum og orðsvipuðum benda til verkunar áætlaðrar rammar.</abstract_isl>
      <abstract_no>Vi foreslår eit geometrisk rammeverk for å læra metaintegrering av ord frå ulike innbyggingskjelder. Rammeverket vårt transformerer innbyggingane til ei felles latent plass, der for eksempel er enkelt gjennomsnittering eller samsvaring av ulike innbyggingane (av eit gitt ord) meir gjennomsnittbar. Den foreslåde latentrommet oppstår frå to spesielle geometriske transformasjonar – kjelde innebygd spesielle orthogonale rotasjonar og ein vanleg Mahalanobis metrisk skalering. Empiriske resultat på fleire ord liknande og ord-analogiske benchmarker illustrerer effektiviteten til den foreslåde rammeverket.</abstract_no>
      <abstract_mn>Бид геометрийн хэмжээсүүд сурах мета-хэмжээсүүдийг олон нэвтрүүлэх эх үүсвэрээс сурах гэсэн үг. Бидний үйл ажиллагааны хэлбэрийг нийтийн сүүлийн зай болгодог. Жишээлбэл, өөр өөр хэлбэрийн хэлбэрүүдийг хялбар дундаж эсвэл нэгтгэл болгодог. Хоёр тодорхой геометрийн өөрчлөлт гарч ирсэн талант орон зай нь тодорхой ортогон эргүүлэлт болон нийтлэг Махаланобисын метрийн хэмжээсүүд юм. Хэдэн хэдэн үг ижил хэмжээний харилцааны үр дүнг, үг аналогийн харилцааны харилцааны үр дүнг тайлбарладаг.</abstract_mn>
      <abstract_pl>Proponujemy geometryczne ramy do nauki meta-osadzeń słów z różnych źródeł osadzenia. Nasz framework przekształca osadzenia w wspólną przestrzeń utajoną, gdzie na przykład proste uśrednienie lub łączenie różnych osadzeń (danego słowa) jest bardziej przydatne. Proponowana przestrzeń utajona wynika z dwóch szczególnych transformacji geometrycznych, w których źródło osadza się określone obroty ortogonalne i wspólne skalowanie metryczne Mahalanobisa. Wyniki empiryczne dotyczące kilku punktów odniesienia do podobieństwa słów i analogii słów ilustrują skuteczność proponowanych ram.</abstract_pl>
      <abstract_ro>Propunem un cadru geometric pentru învățarea meta-încorporări de cuvinte din diferite surse de încorporare. Cadrul nostru transformă încorporările într-un spațiu latent comun, unde, de exemplu, media simplă sau concatenarea diferitelor încorporări (dintr-un anumit cuvânt) este mai ușoară. Spațiul latent propus rezultă din două transformări geometrice particulare - sursă care încorporează rotații ortogonale specifice și o scalare metrică comună Mahalanobis. Rezultatele empirice pe mai multe criterii de referință pentru analogia cuvintelor și analogia cuvintelor ilustrează eficacitatea cadrului propus.</abstract_ro>
      <abstract_sr>Predlažemo geometrijski okvir za učenje meta-ugrađenja reči iz različitih izvora ugrađenja. Naš okvir transformiše ugrađenje u zajednički latentni prostor, gde je, na primer, jednostavno prosječavanje ili zaključavanje različitih ugrađenja (od određene reči) prilagodljivije. Predloženi latentni prostor se pojavljuje iz dve posebne geometrske transformacije - izvora uključujući specifične ortogonalne rotacije i zajedničke Mahalanobis metričke skale. Empirièki rezultati na nekoliko reèi sliènosti i analogijskih kriterija reèi ukazuju na efikasnost predloženog okvira.</abstract_sr>
      <abstract_ta>நாம் வேறு வார்த்தைகளில் இருந்து மூலங்களில் இருந்து கற்றுக்கொள்ள ஒரு ஜியூமிட்ரிக் சட்டம் பரிந்துரைக்கிற எங்கள் சட்டத்தில் உள்ளடக்கங்களை ஒரு பொதுவான சமீபத்தில் இடைவெளியாக மாற்றுகிறது, எடுத்துக்காட்டாக, எளிதான சராசரி அல்லது வேறு வார்த்தைகளி பிரிந்துரைக்கப்பட்ட சமீபத்திலிருந்து இரண்டு குறிப்பிட்ட ஜியோமிட்ரிக் மாற்றங்களிலிருந்து வெளியேறும் - மூலம் குறிப்ப பல வார்த்தைகளின் ஒப்பிருப்பு மற்றும் வார்த்தை ஆராய்ச்சி பென்மார்க்குகளின் விளைவை குறிப்பிடுகிறது.</abstract_ta>
      <abstract_si>අපි වෙනස් සම්බන්ධ ව්‍යාපෘතියෙන් වචනය ඉගෙන ගන්න භාවිමිතික ප්‍රකාරයක් ප්‍රයෝජනය කරනවා. අපේ පරීක්ෂණය සාමාන්‍ය ලටෙන්ට් අවසානයක් වෙනස් කරනවා, උදාහරණයෙන්, වෙනස් පරීක්ෂණ සමාන්‍ය සහ සමාන්‍ය සමාන්‍ය සමාන්‍ය සමාන්‍ ප්‍රතිශ්ණාත්මක විශේෂ භාවිමිතික වෙනස් දෙකක් නිසා ප්‍රතිශ්ණාත්මක වෙනස් වෙනුවෙන් ප්‍රතිශ්ණාත්මක වි සමාන වචනය සහ වචනය බෙන්ච්මාර්ක්ස් වලින් සාමාන්‍ය ප්‍රතිචාරයක් ප්‍රතිචාර කරනවා ප්‍රතිචාර කරපු වචනය</abstract_si>
      <abstract_so>Waxaynu soo jeedaynaa qalabka geometrici si aan u barano meta-embedyada hadallada laga soo barto sourceoyin kala duduwan. Our framework transforms the embeddings into a common latent space, where, for example, simple averaging or concatenation of different embeddings (of a given word) is more amenable.  Xiliga la soo jeeday ee ugu dambeeya waxaa ka soo baxa labo isbedelyo oo gaar ah, taas oo ku jirta qalloocyo gaar ah iyo qiyaastii caadiga ah ee Mahalanobis. Midhaha faa’iidada ah ee ku saabsan hadal u eg iyo qoraal lammaane ah waxaa ka muuqata saameyn ku saabsan firaaqada la soo jeeday.</abstract_so>
      <abstract_sv>Vi föreslår ett geometriskt ramverk för att lära sig metainbäddningar av ord från olika inbäddningskällor. Vårt ramverk omvandlar inbäddningarna till ett gemensamt latent utrymme, där till exempel enkel genomsnittlig eller sammankoppling av olika inbäddningar (av ett givet ord) är mer lättillgänglig. Den föreslagna latenta rymden härrör från två speciella geometriska transformationer - källa som inbäddar specifika ortogonala rotationer och en gemensam mahalanobis metrisk skalning. Empiriska resultat på flera referensvärden för ordlikhet och ordanalogi illustrerar effekten av det föreslagna ramverket.</abstract_sv>
      <abstract_ur>ہم ایک جسمٹریک فرم کی پیشنهاد کرتے ہیں کہ کلمات کی مٹا-ایمبڈینگ کو مختلف ایمبڈینگ سوروں سے سکھائے۔ ہمارا فرمود انڈینگ کو ایک مشترک لاٹینٹ فضا میں تبدیل کرتا ہے، جہاں، مثال، مختلف انڈینگ کا سادھا آہستہ یا مشترک کرنا (ایک لفظ کا) زیادہ مناسب ہے۔ پیشنهاد کی لاٹینٹ فضا دو مخصوص جہامتریکی تبدیل سے آتی ہے - سورج جس میں مخصوص اورٹوگونال چلنے اور ایک مشترک مہالانوبیس منٹریک اسکیلینگ ہے. بہت سی کلمات کے مطابق اور کلمات کے مطابق بنچم کے نتیجے نشان دیتے ہیں کہ پیشنهاد فرمود کے مطابق ہے.</abstract_ur>
      <abstract_uz>Biz bir geometrik freymini o'rganish uchun boshqa murakkablaridagi so'zlarni o'rganish uchun Bizning freymimizning chegarasini bir necha yangi joyga aylantirish mumkin. Masalan, boshqa bir so'zlar (bir so'zning qismlaridan) oddiy ko'paytirish mumkin. The proposed latent space arises from two particular geometric transformations - source embedding specific orthogonal rotations and a common Mahalanobis metric scaling.  Name</abstract_uz>
      <abstract_vi>Chúng tôi đề nghị một khung hình hình hình hình học những từ được nhập từ các nguồn khác nhau. Cơ chế của chúng tôi biến sự nhúng tay vào một không gian tiềm ẩn phổ biến, ví dụ như sự trung bình đơn giản hoặc kết hợp của sự nhúng vào khác nhau (của một từ nào đó) dễ chấp nhận hơn. Các vùng đất tiềm ẩn được đề nghị là dựa trên hai biến đổi hình thể đặc biệt « nguồn được gắn vào các xoay chính xác và một lớp đo mét Mahalanobis phổ biến. Các kết quả từ về nhiều nét giống nhau và các tiêu chuẩn so sánh từ mô phỏng cho thấy hiệu quả của khung tranh.</abstract_vi>
      <abstract_bg>Предлагаме геометрична рамка за изучаване на мета-вграждания на думи от различни източници на вграждане. Нашата рамка трансформира вгражданията в общо латентно пространство, където например простото осредняване или конкатениране на различни вграждания (на дадена дума) е по-приемливо. Предложеното латентно пространство възниква от две конкретни геометрични трансформации - източник, вграждащ специфични правоъгълни ротации и общ метричен мащаб на Махаланобис. Емпирични резултати по няколко сравнителни показатели за сходство на думи и аналогия на думи илюстрират ефективността на предложената рамка.</abstract_bg>
      <abstract_da>Vi foreslår en geometrisk ramme for at lære meta-indlejringer af ord fra forskellige indlejringskilder. Vores ramme forvandler indlejringerne til et fælles latent rum, hvor f.eks. simpel gennemsnitlig eller sammenkobling af forskellige indlejringer (af et givet ord) er mere modtagelig. Det foreslåede latente rum stammer fra to særlige geometriske transformationer - kilde, der indlejrer specifikke ortogonale rotationer og en fælles Mahalanobis metrisk skalering. Empiriske resultater på flere referencer for ordlighed og ordanalogi illustrerer effektiviteten af den foreslåede ramme.</abstract_da>
      <abstract_hr>Predlažemo geometrički okvir za učenje meta-ugrađenja riječi iz različitih izvora ugrađenja. Naš okvir pretvara ugrađenje u zajednički latentni prostor, gdje je, na primjer, jednostavnije prosječanje ili zaključavanje različitih ugrađenja (od određene riječi) prilagodljivije. Predloženi latentni prostor proizilaze iz dvije posebne geometričke transformacije - izvora uključujući specifične ortogonalne rotacije i zajedničke Mahalanobis metričke skale. Empirički rezultati na nekoliko riječi sličnosti i analogijskih kriterija riječi pokazuju djelotvornost predloženog okvira.</abstract_hr>
      <abstract_nl>We stellen een geometrisch kader voor het leren van meta-embeddings van woorden uit verschillende embedding bronnen voor. Ons framework transformeert de embeddings in een gemeenschappelijke latente ruimte, waar bijvoorbeeld eenvoudig gemiddeld of aaneengesloten worden van verschillende embeddings (van een bepaald woord) gemakkelijker is. De voorgestelde latente ruimte ontstaat uit twee specifieke geometrische transformaties waarbij de bron specifieke orthogonale rotaties insluit en een gemeenschappelijke Mahalanobis metrische schaal. Empirische resultaten op verschillende benchmarks voor woordovereenkomsten en woordanalogie illustreren de doeltreffendheid van het voorgestelde kader.</abstract_nl>
      <abstract_id>Kami mengusulkan rangkaian geometris untuk belajar meta-embedding kata dari sumber embedding yang berbeda. kerangka kami mengubah penerbangan menjadi ruang yang tersembunyi umum, di mana, contohnya, rata-rata sederhana atau persatuan dari penerbangan yang berbeda (dari kata tertentu) lebih nyaman. Ruang laten yang diusulkan muncul dari dua transformasi geometri tertentu - sumber yang memasukkan rotasi ortogonal spesifik dan skala metrik Mahalanobis umum. Hasil kerajaan pada beberapa kata persamaan dan benchmark analogi kata menunjukkan efektivitas dari cadangan yang diusulkan.</abstract_id>
      <abstract_de>Wir schlagen ein geometrisches Framework vor, um Meta-Einbettungen von Wörtern aus verschiedenen Einbettungsquellen zu lernen. Unser Framework transformiert die Einbettungen in einen gemeinsamen latenten Raum, in dem beispielsweise eine einfache Mittelung oder Verkettung verschiedener Einbettungen (eines bestimmten Wortes) leichter möglich ist. Der vorgeschlagene latente Raum ergibt sich aus zwei speziellen geometrischen Transformationen in der Quelle, die spezifische orthogonale Rotationen und eine gemeinsame Mahalanobis metrische Skalierung einbetten. Empirische Ergebnisse mehrerer Benchmarks zur Wortähnlichkeit und Wortanalogie verdeutlichen die Wirksamkeit des vorgeschlagenen Rahmens.</abstract_de>
      <abstract_tr>Biz beýleki daşary çeşmeden sözlerin meta-daşarylygyny öwrenmek üçin geometrik çerçewçigi teklip edip görýäris. Bizim çerçevemiz içerikleri ortak bir latent uzaya dönüştürür. Mesela, farklı içeriklerin ortalaması ya da birleşmesi birden daha uygun bir yer. Mazmunlar ýetişdirilen latent seleňiz iki beýleki geometriň üýtgewinden döredildi - çeşme diýjek orthogonal terjimeleri we umumy Mahalanobis metriň derejesinden daşarylýar. Empirik netijeler birnäçe söz meňzeşliki we söz analogi benchmarklarynyň teklip eden çerýäniň etkinligini görkez.</abstract_tr>
      <abstract_ko>우리는 서로 다른 삽입원에서 온 단어의 삽입을 배우기 위해 기하학적 구조를 제시했다.우리의 프레임워크는 삽입을 공공의 잠재적 공간으로 전환할 것이다. 예를 들어, 이 공간에서 서로 다른 삽입을 간단하게 평균하거나 직렬로 연결하는 것이 더욱 적합하다.제시된 잠재적 공간은 두 가지 특수한 기하학적 변환인 특정한 정교 회전과 공통된 마씨 도량 표도에서 나온다.몇 개의 단어의 유사성과 단어 유형 비교 기준에 대한 실증 결과는 이 구조의 유효성을 나타냈다.</abstract_ko>
      <abstract_fa>ما یک چهارچوب geometric برای یادگیری متا-وارد کردن کلمات از منابع مختلف وارد کردن پیشنهاد می کنیم. چهارچوب ما پیوند‌ها را به یک فضای latent مشترک تغییر می‌دهد، که برای مثال، متوسط ساده یا متوسط پیوند‌های مختلف (از یک کلمه به عنوان یک کلمه) قابل تغییر می‌دهد. این فضا پیشنهاد latent از دو تغییرات ژومتریکی خاص پیدا می‌شود - منبع تغییرات orthogonal خاص و یک مقیاس متریکی Mahalanobis مشترک را پیدا می‌کند. نتایج امپراطوری بر چند کلمه شباهت و برچسب‌های شباهت کلمه‌های شباهت و برچسب‌های شباهت کلمه نشان می‌دهند فعالیت چهارچوب پیشنهاد.</abstract_fa>
      <abstract_sq>Ne propozojmë një kuadër gjeometrik për mësimin e meta-përfshirjeve të fjalëve nga burime të ndryshme përfshirjeje. Our framework transforms the embeddings into a common latent space, where, for example, simple averaging or concatenation of different embeddings (of a given word) is more amenable.  Hapësira e propozuar e fshehtë vjen nga dy transformime gjeometrike të veçanta - burimi që përfshin rotacione ortogonale të veçanta dhe një shkallëzim metrik të përbashkët Mahalanobis. Rezultatet imperiale mbi disa fjalë ngjashmëri dhe referenca të fjalës analogjike ilustrojnë efektshmërinë e kuadrit të propozuar.</abstract_sq>
      <abstract_am>የኢዮሜትሪክ ፍሬም ከልዩ መልዕክቶች የቃላትን መግለጫ ለመማር እናስጀምራለን፡፡ Our framework transforms the embeddings into a common latent space, where, for example, simple averaging or concatenation of different embeddings (of a given word) is more amenable.  በተዘጋጀው የአሁኑን ቦታ ከሁለት የተለየ የጂዮሜትሪክ ለውጦች እና የተለየ መሃላንባቢ ሚትሪክ መጠቀሚያ እና የተለየ መሐላኖብ መጠቀሚያ ነው፡፡ በአካባቢ ቃላት በሚመስል እና የቃላት አናውሎጂ benchmarks በተዘጋጀው የፍሬም ውጤት ያስታውሳል፡፡</abstract_am>
      <abstract_sw>We propose a geometric framework for learning meta-embeddings of words from different embedding sources.  Our framework transforms the embeddings into a common latent space, where, for example, simple averaging or concatenation of different embeddings (of a given word) is more amenable.  Sehemu ya hivi karibuni inayopendekezwa inatokana na mabadiliko mawili maalum ya geometric – chanzo kinachobeba mzunguko maalum wa kigongonal na mabadiliko ya hali ya kawaida ya Mahalanobis. Matokeo ya matumaini juu ya maneno kadhaa yanayofanana na misingi ya uchambuzi wa maneno yanaonyesha ufanisi wa mfumo wa pendekezo.</abstract_sw>
      <abstract_bn>আমরা ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিত্তিক উৎস থেকে শব্দ শিখার জন্য ভূমিট্রিক ফ্রেম প্রস্তাব করি। আমাদের ফ্রেম কাঠামোটি সাধারণ সাম্প্রতিক স্থানে পরিবর্তন করে, যেখানে উদাহরণস্বরূপ, বিভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন প্রস্তাবিত সাম্প্রতিক স্থান দুটি বিশেষ ভূমিট্রিক পরিবর্তন থেকে উৎস - বিশেষ ভূমিকম্পের আবর্তন এবং সাধারণ মাহালানোবিস মেট্রিক প্রস্তাবিত ফ্রেমের কার্যক্রমের প্রভাব তুলে ধরা হয়েছে।</abstract_bn>
      <abstract_hy>We propose a geometric framework for learning meta-embeddings of words from different embedding sources.  Մեր կառուցվածքը փոխակերպում է ներդրումները ընդհանուր թաքնված տարածություն, որտեղ, օրինակ, տարբեր ներդրումների պարզ միջինը կամ համեմատությունը ավելի հարմար է: Պատրաստված թաքնված տարածքը առաջացնում է երկու որոշակի երկրաչափ փոփոխություններից' աղբյուրից, որը ներառում է որոշակի օրթոգոնալ փոփոխություններ և ընդհանուր Մահալանոբիսի մետրական չափսեր: Իմպրիկական արդյունքները բառերի նմանության և բառերի նմանության հարաբերականների վրա ցույց են տալիս առաջարկած շրջանակի արդյունավետությունը:</abstract_hy>
      <abstract_az>Biz müxtəlif inşa mənbələrindən sözlərin meta-inşa edilməsini öyrənmək üçün geometrik framework ü təklif edirik. Bizim framework ımız inşallarını ortaq latent bir alana çevirir. Məsələn, farklı inşalların ortalaması və ya birlikləşməsi daha çox münasibdir. İstemləndirilən latent alan iki müəyyən geometrik dəyişiklikdən təşkil edilir - məxluqat orthogonal dəyişiklikdən və ortaq Mahalanobis metrik dəyişiklikdən istifadə edilir. Bircə sözlərin istifadə edilməsi və sözlərin analoji benchmarkları təklif edilmiş frameworklərin etkinliğini göstərir.</abstract_az>
      <abstract_bs>Predlažemo geometrički okvir za učenje meta-ugrađenja riječi iz različitih izvora ugrađenja. Naš okvir preobraća ugrađenje u zajednički latentni prostor, gdje je, na primjer, jednostavnije prosječavanje ili zaključavanje različitih ugrađenja (određene riječi) prilagodljivije. Predloženi latentni prostor proizilaze iz dvije posebne geometrske transformacije - izvora uključujući određene ortogonalne rotacije i zajedničke Mahalanobis metričke skale. Empirički rezultati na nekoliko riječi sličnosti i analogijskih kriterija riječi pokazuju djelotvornost predloženog okvira.</abstract_bs>
      <abstract_cs>Navrhujeme geometrický rámec pro učení se meta-vložení slov z různých vložených zdrojů. Náš framework transformuje vložení do společného latentního prostoru, kde je například jednoduché průměrování nebo řetězení různých vložení (daného slova) přístupnější. Navržený latentní prostor vychází ze dvou konkrétních geometrických transformací ze zdroje vkládajících specifické ortogonální rotace a ze společného Mahalanobisova metrického měřítka. Empirické výsledky na několika referenčních hodnotách slovní podobnosti a slovní analogie ukazují účinnost navrhovaného rámce.</abstract_cs>
      <abstract_et>Pakume välja geomeetrilise raamistiku erinevatest manustamisallikatest pärit sõnade metamanustamise õppimiseks. Meie raamistik muudab manustamised ühiseks varjatud ruumiks, kus näiteks erinevate manustamiste (antud sõna) lihtne keskmine või ühendamine on paremini mõeldud. Kavandatud latentne ruum tuleneb kahest erilisest geomeetrilisest muundusest - allikast, mis sisaldab spetsiifilisi ortogonaalseid pöördeid ja ühisest Mahalanobise meetrilisest skaleerimisest. Mitme sõna sarnasuse ja sõna analoogia võrdlusnäitajate empiirilised tulemused illustreerivad kavandatud raamistiku tõhusust.</abstract_et>
      <abstract_fi>Ehdotamme geometrista kehystä sanojen metaupottamiseen eri upotuslähteistä. Kehyksemme muuntaa upotukset yhteiseksi piileväksi tilaksi, jossa esimerkiksi erilaisten upotusten (tietyn sanan) yksinkertainen keskiarvo tai yhdistäminen on helpompi. Ehdotettu piilevä tila syntyy kahdesta erityisestä geometrisestä muunnoksesta - lähteestä, joka upottaa tiettyjä ortogonaalisia kiertoja ja yhteisestä Mahalanobis-metrisestä skaalauksesta. Empiiriset tulokset useista sanasamankaltaisuuksista ja sanaanalogiasta havainnollistavat ehdotetun viitekehyksen tehokkuutta.</abstract_fi>
      <abstract_af>Ons voorstel 'n geometriese raamwerk vir die leer van meta-inbettings van woorde van verskillende inbettingbronne. Ons raamwerk transformeer die inbêdings in 'n gemeenskaplike latent ruimte, waar, byvoorbeeld, eenvoudige middelverdiging of samelewing van verskillende inbêdings (van 'n gegewe woord) meer gemeenskaplik is. Die voorgestelde latente ruimte kom uit twee bepaalde geometriese transformasies - bron wat spesifieke orthogonale rotasies inbêer en 'n gemeenskaplike Mahalanobis metriese skaal. Enigeriese resultate op verskeie woorde gelykbaarheid en woord analogiese benchmarke illustreer die effektiviteit van die voorgestelde raamwerk.</abstract_af>
      <abstract_ca>Proposem un marc geomètric per aprendre metaintegracions de paraules de diverses fonts d'integració. Our framework transforms the embeddings into a common latent space, where, for example, simple averaging or concatenation of different embeddings (of a given word) is more amenable.  L'espai latent proposat surt de dues transformacions geomètriques concretes - fonts que incorporen rotacions ortogonals específices i una escala mètrica comú de Mahalanobis. Els resultats empírics en moltes paraules similars i punts de referència en paraules analògiques ilustren l'eficacia del marc proposat.</abstract_ca>
      <abstract_jv>Awak dhéwé gunakake sistem jeogras kanggo nggambar meta-embedding sing gambar apat karo percoffes embedding string" in "context_BAR_stringLink Laptop" and "Desktop Rejalaké empirhik karo akeh gambar dengané gambar lan kelangan langgar sampeyan gambar dadi nyong langgar an effek nggawe barang nggawe</abstract_jv>
      <abstract_ha>Munã goyyar da wani firam na geometric wa da za'a sanar da meta-embedded words daga sourcen da ke cikin. Firmakonmu ya musanya fili cikin fili mai daidai a yanzu, inda, misali, sauri mai sauƙi ko ko ko kokewa masu haɗi daban-daban (cikin wata magana ba'a ba'a ba ta sauce ba. @ info: whatsthis Mataimakin Empirical na kan wasu kalmõmi da ke daidaita da maganar analogy na nuna aikin firam da aka buƙata.</abstract_ha>
      <abstract_sk>Predlagamo geometrijski okvir za učenje metavdelav besed iz različnih virov vdelave. Naš okvir pretvori vdelave v skupni latentni prostor, kjer je na primer enostavno povprečje ali združevanje različnih vdelav (določene besede) bolj prilagodljivo. Predlagani latentni prostor izhaja iz dveh posebnih geometrijskih transformacij - vir vključuje specifične pravokotne rotacije in skupnega Mahalanobisovega metričnega skaliranja. Empirični rezultati več besednih podobnosti in referenčnih vrednosti besedne analogije ponazarjajo učinkovitost predlaganega okvira.</abstract_sk>
      <abstract_fil>Nagbibigay kami ng geometric framework para mag-aral ng meta-embedding ng mga salita mula sa ibang embedding sources. Ang aming framework ay nagbabago ng mga embeddings sa isang karaniwang latent space, na kung saan, halimbawa, ang simpleng pagbabagabag o pagbabagabag ng ibang embeddings (ng ibinigay na salita) ay lalong maayos. Ang inilagay na latent na lugar ay nanggagaling mula sa dalawang espesyal na pagbabago ng geometria - source na nagbibigay ng espesyal na pagbabago ng orthogonal at ng karaniwang pagbabago ng metrika ng Mahalanobis. Ang mga resulta ng empirical sa iba pang salitang similarity at salitang analogy benchmarks ay nagbibigay ng efficacy ng proposed framework.</abstract_fil>
      <abstract_he>אנו מציעים מסגרת גיאומטרית ללמוד מיטה-קישורים של מילים ממקורות קישורים שונים. המסגרת שלנו משנה את התכניות לחלל סגור משותף, שבו, לדוגמא, הממוצע הפשוט או השתלב של התכניות שונות (של מילה מסוימת) יותר נוח. The proposed latent space arises from two particular geometric transformations - source embedding specific orthogonal rotations and a common Mahalanobis metric scaling.  Empirical results on several word similarity and word analogy benchmarks illustrate the efficacy of the proposed framework.</abstract_he>
      <abstract_bo>We propose a geometric framework for learning meta-embeddings of words from different embedding sources. ང་ཚོའི་གཞུང་གཞིས་གྱིས་embeddings་འདི་སྤྱིར་བཏང་བའི་བར་སྟོང་ཞིག་བཟོ་བྱེད་ཀྱི་ཡོད། The proposed latent space arises from two particular geometric transformations - source embedding specific orthogonal rotations and a common Mahalanobis metric scaling. གཟུགས་རིས་ཀྱི་འབྲས་བ་མང་པོ་ཞིག</abstract_bo>
      </paper>
    <paper id="10">
      <title>Exploring the Limits of Simple Learners in Knowledge Distillation for <a href="https://en.wikipedia.org/wiki/Document_classification">Document Classification</a> with DocBERT<fixed-case>D</fixed-case>oc<fixed-case>BERT</fixed-case></title>
      <author><first>Ashutosh</first><last>Adhikari</last></author>
      <author><first>Achyudh</first><last>Ram</last></author>
      <author><first>Raphael</first><last>Tang</last></author>
      <author><first>William L.</first><last>Hamilton</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>72–77</pages>
      <abstract>Fine-tuned variants of BERT are able to achieve state-of-the-art <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on many natural language processing tasks, although at significant computational costs. In this paper, we verify BERT’s effectiveness for document classification and investigate the extent to which BERT-level effectiveness can be obtained by different baselines, combined with knowledge distillationa popular model compression method. The results show that BERT-level effectiveness can be achieved by a single-layer LSTM with at least 40 fewer FLOPS and only 3 % parameters. More importantly, this study analyzes the limits of knowledge distillation as we distill BERT’s knowledge all the way down to linear modelsa relevant baseline for the task. We report substantial improvement in <a href="https://en.wikipedia.org/wiki/Effectiveness">effectiveness</a> for even the simplest <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>, as they capture the knowledge learnt by BERT.<tex-math>40\times</tex-math> fewer FLOPS and only <tex-math>{\sim}3\%</tex-math> parameters. More importantly, this study analyzes the limits of knowledge distillation as we distill BERT’s knowledge all the way down to linear models—a relevant baseline for the task. We report substantial improvement in effectiveness for even the simplest models, as they capture the knowledge learnt by BERT.</abstract>
      <url hash="051972bd">2020.repl4nlp-1.10</url>
      <doi>10.18653/v1/2020.repl4nlp-1.10</doi>
      <video href="http://slideslive.com/38929776" />
      <bibkey>adhikari-etal-2020-exploring</bibkey>
    <title_ar>استكشاف حدود المتعلمين البسطاء في تقطير المعرفة لتصنيف المستندات باستخدام DocBERT</title_ar>
      <title_es>Explorando los límites de los estudiantes sencillos en la destilación de conocimientos para la clasificación de documentos con DocBert</title_es>
      <title_fr>Explorer les limites des apprenants simples dans la distillation des connaissances pour la classification de documents avec DocBert</title_fr>
      <title_pt>Explorando os limites de aprendizes simples na destilação de conhecimento para classificação de documentos com DocBERT</title_pt>
      <title_ja>DocBERTを使用した文書分類のためのナレッジディスティレーションにおける単純学習者の限界の探求</title_ja>
      <title_hi>DocBERT के साथ दस्तावेज़ वर्गीकरण के लिए ज्ञान आसवन में सरल शिक्षार्थियों की सीमाओं की खोज</title_hi>
      <title_zh>用 DocBERT 求简学者文档分类提炼之局限性</title_zh>
      <title_ru>Изучение пределов простых обучающихся в дистилляции знаний для классификации документов с помощью DocBERT</title_ru>
      <title_ukr>Дослідження меж простих учнів у перегоні знань для класифікації документів за допомогою DocBERT</title_ukr>
      <title_ga>Iniúchadh ar Theorainneacha na bhFoghlaimeoirí Simplí i nDriogadh an Eolais le haghaidh Aicmiú Doiciméad le DocBERT</title_ga>
      <title_ka>DocBERT</title_ka>
      <title_hu>Az egyszerű tanulók határainak feltárása a DocBERT dokumentumok osztályozásához</title_hu>
      <title_el>Εξερεύνηση των Όρων των Απλών Μαθητών στην απόσταξη Γνώσης για την ταξινόμηση εγγράφων με το DocBERT</title_el>
      <title_isl>Skoðun á takmörkum einfalda lærera í þekkingu til skilgreiningar skjala með DocBERT</title_isl>
      <title_it>Esplorare i limiti dei semplici apprendisti nella distillazione della conoscenza per la classificazione dei documenti con DocBERT</title_it>
      <title_kk>DocBERT құжатты классификациялау үшін білімді оқытушылардың шектерін зерттеу</title_kk>
      <title_mk>Истражување на границите на едноставните ученици во дистилација на знаење за класификација на документи со DocBERT</title_mk>
      <title_ms>Menjelaskan Had Pelajar Mudah dalam Distillasi Pengetahuan untuk Klasifikasi Dokumen dengan DocBERT</title_ms>
      <title_mt>L-esplorazzjoni tal-limiti ta’ Tagħlimiet Simpli fid-Distillazzjoni tal-Għarfien għall-Klassifikazzjoni tad-Dokumenti b’DocBERT</title_mt>
      <title_mn>DocBERT-тэй бичгийн классификацийн мэдлэгтэй хэмжээний сурагчийн хязгаарыг судлах</title_mn>
      <title_no>Eksplorerer grensene for enkle lærarar i kjennomslag for dokumentklassifikasjon med DocBERT</title_no>
      <title_lt>Dokumentų klasifikavimo paprastųjų mokytojų žinių distiliacijos ribos DocBERT tyrimas</title_lt>
      <title_pl>Badanie granic prostych uczniów w destylacji wiedzy dla klasyfikacji dokumentów za pomocą DocBERT</title_pl>
      <title_ro>Explorarea limitelor învățătorilor simpli în distilarea cunoștințelor pentru clasificarea documentelor cu DocBERT</title_ro>
      <title_sr>Ispitivanje granica jednostavnih učitelja u destilaciji znanja za klasifikaciju dokumenta sa DocBERT-om</title_sr>
      <title_ml>ഡോക്ടറിന്റെ ക്ലാസിഷന്‍ കൊണ്ട് ഡോക്ടറിന്റെ വിജ്ഞാനത്തിലെ എളുപ്പമുള്ള ലെയറുകളുടെ അതിരുകള്‍ പരിശോധിക്കുക</title_ml>
      <title_so>Qoraalka Limitka Learners of Simple Knowledge Distribution for Documentation Classification with DocBERT</title_so>
      <title_sv>Undersökning av gränserna för enkla elever i kunskapsdestillation för dokumentklassificering med DocBERT</title_sv>
      <title_si>DocBERT සමග සාමාන්‍ය ඉගෙනීම් සීමාව ප්‍රශ්නය කරනවා දස්සන විශාලනය සඳහා දස්සන විශාලනය</title_si>
      <title_ta>Comment</title_ta>
      <title_ur>DocBERT کے ساتھ دکھانے کے لئے علم دیسٹیل میں ساده یادگاروں کی محدودیت تحقیق کرتا ہے</title_ur>
      <title_vi>Khám phá giới hạn của những học giả đơn giản về sự phân tách tri thức cho việc mô tả tài liệu với DocBERT.</title_vi>
      <title_uz>Comment</title_uz>
      <title_nl>Het verkennen van de grenzen van eenvoudige leerkrachten in kennisdestillatie voor documentclassificatie met DocBERT</title_nl>
      <title_da>Undersøgelse af grænserne for enkle lærere i videndedestillation til dokumentklassificering med DocBERT</title_da>
      <title_bg>Изследване на границите на простите ученици в дестилацията на знанието за класификация на документи с ДокBERT</title_bg>
      <title_id>Menjelaskan Batas Pelajar sederhana dalam Distillasi Pengetahuan untuk Klasifikasi Dokumen dengan DocBERT</title_id>
      <title_de>Erforschung der Grenzen einfacher Lernender in der Wissensdestillation für die Dokumentenklassifizierung mit DocBERT</title_de>
      <title_hr>Ispitivanje ograničenja jednostavnih učitelja u destilaciji znanja za klasifikaciju dokumenta s DocBERT-om</title_hr>
      <title_ko>DocBERT로 단순 학습자가 문서 분류 지식 추출에서의 한계를 탐구하다</title_ko>
      <title_fa>تحقیق محدودیت دانش‌آموزان ساده در محدودیت دانش‌شناسی برای شناسایی سند با DocBERT</title_fa>
      <title_tr>DocBERT'i Sened Klassifikasiýasy üçin Bilişi Dahili Okuwçylaryň Limatlaryny Exploring</title_tr>
      <title_sw>Kuelezea Mipaka ya Wanafunzi Wepesi katika Kugawanywa kwa ufahamu kwa ajili ya Kutangaza Hukumu na DocBERT</title_sw>
      <title_af>Verskyn die grense van eenvoudige leerders in kennis verskiling vir dokumentklassifikasie met DocBERT</title_af>
      <title_sq>Shqyrtimi i kufijve të mësuesve të thjeshtë në distilimin e njohurive për klasifikimin e dokumentit me DocBERT</title_sq>
      <title_am>DocBERT</title_am>
      <title_hy>DocBER-ի միջոցով փաստաթղթի դասակարգում պարզ ուսանողների սահմանները ուսումնասիրելը</title_hy>
      <title_bn>ডকুমেন্টের সাথে ডকুমেন্ট ক্লাসিফেশনের জন্য জ্ঞান ডিস্ট্রিলেশনে সাধারণ শিক্ষকদের সীমা বিশ্লেষণ করা হচ্ছে</title_bn>
      <title_ca>Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT</title_ca>
      <title_cs>Zkoumání limitů jednoduchých učitelů v destilaci znalostí pro klasifikaci dokumentů s DocBERT</title_cs>
      <title_et>Teadmiste destilleerimise lihtsate õppijate piiride uurimine dokumentide klassifitseerimiseks DocBERTiga</title_et>
      <title_fi>Tiedon tislauksen yksinkertaisten oppijoiden rajat dokumenttien luokittelussa DocBERTin avulla</title_fi>
      <title_az>DocBERT ilə Döküman Klasifikasyonu üçün Bilən Yaxşı Öyrənənənlərin Sınırlarını Exploring</title_az>
      <title_bs>Ispitivanje ograničenja jednostavnih učitelja u destilaciji znanja za klasifikaciju dokumenta sa DocBERT-om</title_bs>
      <title_jv>Gambar Kebebasan limiting the Simple Learner in Learn Distiltion for Dokumen</title_jv>
      <title_he>לחקור את הגבולות של לומדים פשוטים במסגרת ידע עבור מסמכים מסוימים עם DocBERT</title_he>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_sk>Raziskovanje meja preprostih učencev pri destilaciji znanja za klasifikacijo dokumentov z DocBERT</title_sk>
      <title_bo>DocBERT ཡིས་ཡིག་ཆ་སྦྱར་བའི་སྐབས་ཆ་སྟབས་བདེ་རིགས་ཀྱི་ཚད་གཞི་བཙལ་ཞིབ་བྱེད་བཞིན་པ</title_bo>
      <title_fil>Iniginawa ang mga limits ng mga Simple Learners sa Knowledge Distillation para sa Document Classification na may DocBERT</title_fil>
      <abstract_ar>المتغيرات الدقيقة لـ BERT قادرة على تحقيق دقة متطورة في العديد من مهام معالجة اللغة الطبيعية ، على الرغم من التكاليف الحسابية الكبيرة. في هذه الورقة ، نتحقق من فعالية BERT لتصنيف المستندات ونبحث إلى أي مدى يمكن الحصول على فعالية على مستوى BERT من خلال خطوط أساس مختلفة ، جنبًا إلى جنب مع تقطير المعرفة - طريقة ضغط نموذج شائعة. تظهر النتائج أنه يمكن تحقيق فعالية مستوى BERT من خلال LSTM أحادي الطبقة مع 40 × أقل من FLOPS ومعلمات ∼3٪ فقط. والأهم من ذلك ، أن هذه الدراسة تحلل حدود تقطير المعرفة حيث أننا نختصر معرفة بيرت وصولاً إلى النماذج الخطية - وهو الأساس المناسب للمهمة. أبلغنا عن تحسن كبير في الفعالية حتى لأبسط النماذج ، حيث إنها تلتقط المعرفة التي تعلمتها BERT.</abstract_ar>
      <abstract_pt>Variantes afinadas do BERT são capazes de alcançar precisão de última geração em muitas tarefas de processamento de linguagem natural, embora com custos computacionais significativos. Neste artigo, verificamos a eficácia do BERT para classificação de documentos e investigamos até que ponto a eficácia do nível do BERT pode ser obtida por diferentes linhas de base, combinadas com a destilação do conhecimento – um método popular de compressão de modelos. Os resultados mostram que a eficácia do nível de BERT pode ser alcançada por um LSTM de camada única com pelo menos 40x menos FLOPS e apenas ∼3% de parâmetros. Mais importante, este estudo analisa os limites da destilação do conhecimento à medida que destilamos o conhecimento do BERT até os modelos lineares - uma linha de base relevante para a tarefa. Relatamos uma melhora substancial na eficácia até mesmo dos modelos mais simples, pois capturam o conhecimento aprendido pelo BERT.</abstract_pt>
      <abstract_es>Las variantes ajustadas de BERT son capaces de lograr una precisión de vanguardia en muchas tareas de procesamiento del lenguaje natural, aunque con costos computacionales significativos. En este artículo, verificamos la eficacia del BERT para la clasificación de documentos e investigamos hasta qué punto se puede obtener la eficacia del nivel de BERT mediante diferentes líneas de base, en combinación con la destilación de conocimientos, un método de compresión de modelos popular. Los resultados muestran que la eficacia del nivel de BERT se puede lograr mediante un LSTM de una sola capa con al menos 40 veces menos FLOPS y solo ~ 3% de parámetros. Y lo que es más importante, este estudio analiza los límites de la destilación del conocimiento a medida que destilamos el conocimiento de BERT hasta llegar a modelos lineales, una línea de base relevante para la tarea. Reportamos una mejora sustancial en la eficacia incluso para los modelos más simples, ya que capturan el conocimiento aprendido por BERT.</abstract_es>
      <abstract_fr>Les variantes affinées du BERT sont capables d'atteindre une précision de pointe pour de nombreuses tâches de traitement du langage naturel, mais à des coûts de calcul importants. Dans cet article, nous vérifions l'efficacité du BERT pour la classification des documents et étudions dans quelle mesure l'efficacité du niveau BERT peut être obtenue par différentes bases de référence, combinée à la distillation des connaissances, une méthode de compression de modèle populaire. Les résultats montrent que l'efficacité du niveau BERT peut être atteinte par un LSTM monocouche avec au moins 40 fois moins de FLOPS et seulement ∼ 3 % de paramètres. Plus important encore, cette étude analyse les limites de la distillation des connaissances alors que nous distillons les connaissances du BERT jusqu'aux modèles linéaires — une base de référence pertinente pour la tâche. Nous signalons une amélioration substantielle de l'efficacité, même pour les modèles les plus simples, car ils capturent les connaissances acquises par BERT.</abstract_fr>
      <abstract_ja>BERTの微調整されたバリアントは、多くの自然言語処理タスクで最先端の精度を達成することができますが、計算コストはかなり高くなります。本稿では、文書分類に対するBERTの有効性を検証し、一般的なモデル圧縮法である知識蒸留と組み合わせて、異なるベースラインによってBERTレベルの有効性を得ることができる程度を調査する。BERTレベルの有効性は、少なくとも40 ×フロップ数が少なく、パラメータがわずか3%の単層LSTMで達成できることが示されています。さらに重要なことに、この研究では、BERTの知識を線形モデルに至るまで蒸留する際の知識蒸留の限界を分析しています。これは、タスクに関連するベースラインです。私たちは、最も単純なモデルであっても、BERTによって学んだ知識を取り込むため、有効性の大幅な改善を報告しています。</abstract_ja>
      <abstract_hi>BERT के ठीक-ठाक संस्करण कई प्राकृतिक भाषा प्रसंस्करण कार्यों पर अत्याधुनिक सटीकता प्राप्त करने में सक्षम हैं, हालांकि महत्वपूर्ण कम्प्यूटेशनल लागतों पर। इस पेपर में, हम दस्तावेज़ वर्गीकरण के लिए BERT की प्रभावशीलता को सत्यापित करते हैं और उस सीमा की जांच करते हैं कि BERT-स्तर की प्रभावशीलता को विभिन्न आधार रेखाओं द्वारा किस हद तक प्राप्त किया जा सकता है, जो ज्ञान आसवन के साथ संयुक्त है- एक लोकप्रिय मॉडल संपीड़न विधि। परिणामों से पता चलता है कि BERT-स्तर की प्रभावशीलता को कम से कम 40× कम FLOPS और केवल ~3% पैरामीटर के साथ एक एकल-परत LSTM द्वारा प्राप्त किया जा सकता है। इससे भी महत्वपूर्ण बात यह है कि यह अध्ययन ज्ञान आसवन की सीमाओं का विश्लेषण करता है क्योंकि हम BERT के ज्ञान को रैखिक मॉडल के लिए सभी तरह से डिस्टिल करते हैं- कार्य के लिए एक प्रासंगिक आधार रेखा। हम यहां तक कि सबसे सरल मॉडल के लिए प्रभावशीलता में पर्याप्त सुधार की रिपोर्ट करते हैं, क्योंकि वे BERT द्वारा सीखे गए ज्ञान पर कब्जा करते हैं।</abstract_hi>
      <abstract_zh>BERT之微变体能处事于众自然语言之上,先进之准确性,虽计算成本高。 本验BERT于文档类之有效性,考之不同基线得BERT级有效性于多大程度,以合蒸馏(行之形压缩法)。 结果表明单LSTM可以得BERT级有效性,其FLOPS至少减40×,参数为∼3%。 重者,论其局限性,以吾BERT至于线性 - 事之基线也。 臣等言,虽至简,其有效性著,以其得BERT知也。</abstract_zh>
      <abstract_ru>Тонко настраиваемые варианты BERT способны достигать самой современной точности во многих задачах обработки естественного языка, хотя и при значительных вычислительных затратах. В этой статье мы проверяем эффективность BERT для классификации документов и исследуем, в какой степени эффективность на уровне BERT может быть получена с помощью различных базовых линий в сочетании с дистилляцией знаний - популярным методом сжатия модели. Результаты показывают, что эффективность на уровне BERT может быть достигнута с помощью однослойного LSTM с минимальным количеством ФЛОПОВ в 40раз меньше и параметрами всего 3%. Что более важно, в этом исследовании анализируются пределы дистилляции знаний, поскольку мы перегоняем знания БЕРТА вплоть до линейных моделей - актуальной базовой линии для этой задачи. Мы сообщаем о значительном повышении эффективности даже для самых простых моделей, поскольку они отражают знания, полученные БЕРТОМ.</abstract_ru>
      <abstract_ukr>Тонко налаштовані варіанти BERT здатні досягти найсучаснішої точності при виконанні багатьох завдань з обробки природної мови, хоча і зі значними обчислювальними витратами. У цій роботі ми перевіряємо ефективність BERT для класифікації документів і досліджуємо, наскільки ефективність на рівні BERT може бути досягнута різними вихідними даними в поєднанні з перегонкою знань - популярним методом стиснення моделі. Результати показують, що ефективність на рівні BERT може бути досягнута за допомогою одношарової LSTM з щонайменше 40 разів меншою КІЛЬКІСТЮ флопів і лише3% параметрами. Що ще важливіше, це дослідження аналізує межі перегонки знань, оскільки ми перегонюємо знання БЕРТА аж до лінійних моделей - відповідної базової лінії для завдання. Ми повідомляємо про суттєве підвищення ефективності навіть для найпростіших моделей, оскільки вони фіксують знання, отримані БЕРТОМ.</abstract_ukr>
      <abstract_ga>Tá leaganacha mionchoigeartaithe de chuid BERT in ann cruinneas den scoth a bhaint amach ar go leor tascanna próiseála teanga nádúrtha, cé go bhfuil costais shuntasacha ríomhaireachta ag baint leo. Sa pháipéar seo, dearbhaímid éifeachtacht CRET maidir le haicmiú doiciméad agus déanaimid imscrúdú ar a mhéid is féidir éifeachtúlacht leibhéal BERT a fháil trí bhunlínte éagsúla, in éineacht le driogadh eolais - modh comhbhrú samhla a bhfuil tóir air. Léiríonn na torthaí gur féidir éifeachtúlacht ag leibhéal BERT a bhaint amach trí LSTM aonchiseal a bhfuil 40 × níos lú FLOPS ar a laghad aige agus gan ach paraiméadair ∼3%. Níos tábhachtaí fós, déanann an staidéar seo anailís ar theorainneacha driogadh an eolais agus eolas CRET á dhriogadh an bealach ar fad síos go samhlacha líneacha - bunlíne ábhartha don tasc. Tuairiscímid go bhfuil feabhas suntasach ar éifeachtúlacht fiú na múnlaí is simplí, mar go nglacann siad an t-eolas a d'fhoghlaim BERT.</abstract_ga>
      <abstract_el>Οι εκλεπτυσμένες παραλλαγές του μπορούν να επιτύχουν ακρίβεια τελευταίας τεχνολογίας σε πολλές εργασίες επεξεργασίας φυσικής γλώσσας, αν και με σημαντικό υπολογιστικό κόστος. Στην παρούσα εργασία, επαληθεύουμε την αποτελεσματικότητα του για την ταξινόμηση εγγράφων και διερευνούμε τον βαθμό στον οποίο η αποτελεσματικότητα σε επίπεδο μπορεί να επιτευχθεί με διαφορετικές γραμμές βάσης, σε συνδυασμό με την απόσταξη γνώσης – μια δημοφιλής μέθοδος συμπίεσης μοντέλου. Τα αποτελέσματα δείχνουν ότι η αποτελεσματικότητα σε επίπεδο μπορεί να επιτευχθεί με ένα μονοστρωματικό LSTM με τουλάχιστον 40* λιγότερους FLOPS και μόνο 3% παραμέτρους. Πιο σημαντικό, η παρούσα μελέτη αναλύει τα όρια της απόσταξης γνώσης καθώς αποσταλούμε τη γνώση του μέχρι τα γραμμικά μοντέλα – μια σχετική βάση για το έργο. Αναφέρουμε σημαντική βελτίωση της αποτελεσματικότητας ακόμη και για τα πιο απλά μοντέλα, καθώς αποτυπώνουν τις γνώσεις που έχει μάθει ο BERT.</abstract_el>
      <abstract_isl>Breytingar á BERT geta náð nýjasta nákvæmni á mörgum náttúrulegum tungumál meðhöndlunarstöðum, þó við marktæka reikningarkostnaði. Í þessu pappíri athugam við virkni BERT við skilgreiningu skjala og rannsakum hversu mikið virkni BERT-gildis er hægt a ð ná með mismunandi grunnlínum, ásamt þekkingu destillation-vinsælri þungunarmeðferð. Niðurstöður sýna a ð virkni BERT-gildis er hægt að ná með einlaglegri LSTM með að minnsta kosti 40* færri FLOPS og aðeins 3% breytum. Mikilvægari er a ð þessi rannsókn greinir takmarka á þekkingu destillation þar sem við destillim þekkingu BERT allt að línulegum líkönum - viðeigandi upphafsgildi fyrir verkefnið. Við tilkynnum umtalsvert bati á virkni jafnvel einfaldasta líkana, þar sem þeir greina þekkingu sem BERT lært.</abstract_isl>
      <abstract_ka>BERT-ის კეთილური განრამეტრები შეუძლიათ წარმოიდგინოთ მნიშვნელოვანი ენის პროცესირების მუშაობაში, თუმცა მნიშვნელოვანი კომპუტაციალური პასუხში. ამ დოკუმენტში ჩვენ დავწერეთ BERT-ის ეფექტიურობა დოკუმენტის კლასიფიკაციაში და განსხვავებთ, რომლებიც BERT-დოკუმენტის ეფექტიურობა შეიძლება მიიღება განსხვავებული ფექტილიებით, რომლებიც ც წარმოდგენები ჩვენებს, რომ BERT- დონე ეფექტიფიკაცია ერთ- დონე LSTM-ით შეიძლება მიიღება, რომელიც მინუს 40* ცოტა FLOPS და მხოლოდ 3% პარამეტრებით. უფრო მნიშვნელოვანია, ეს სწავლის ანალიზაცია ცნობიერების დისტლიაციის ზომის, როგორც ჩვენ განსხვავებთ BERT-ის ცნობიერებას ყველაფერი მინუს ლეინერი მოდელების მინუს მ ჩვენ შეგვიყვანეთ მნიშვნელოვანი წარმოადგენება ეფექტურებაში ყველაზე უკეთესი მოდელებისთვის, როგორც ისინი შეიძლება BERT-ის მეცნიერება.</abstract_ka>
      <abstract_lt>Geriau pritaikyti BERT variantai gali pasiekti pažangiausią tikslumą daugelyje gamtinių kalbų apdorojimo užduočių, nors ir didelėmis skaičiavimo sąnaudomis. Šiame dokumente tikriname BERT veiksmingumą klasifikuojant dokumentus ir tiriame, kokiu mastu BERT veiksmingumą galima pasiekti naudojant skirtingas bazines linijas, kartu su žinių distiliavimu – populiariu modelio suspaudimo metodu. Rezultatai rodo, kad BERT lygio veiksmingumą galima pasiekti taikant viensluoksnį LSTM, kurio FLOPS yra mažiau kaip 40 * ir parametrai yra mažiau kaip 3 %. Svarbiausia, šiame tyrime analizuojamos žinių distiliacijos ribos, nes distiliuojame BERT žinias iki linijinių modelių, kurie yra svarbi užduoties bazė. Mes pranešame, kad net paprasčiausių modelių veiksmingumas gerokai pagerėjo, nes jie apima BERT įgytas žinias.</abstract_lt>
      <abstract_mk>Фино-прилагодени варијанти на БЕРТ можат да постигнат најсовремена точност на многу природни задачи за обработување јазик, иако со значителни пресметки. In this paper, we verify BERT's effectiveness for document classification and investigate the extent to which BERT-level effectiveness can be obtained by different baselines, combined with knowledge distillation-a popular model compression method.  Резултатите покажуваат дека ефективноста на нивото BERT може да се постигне со еден слој LSTM со најмалку 40* помалку FLOPS и само 3 отсто параметри. Поважно е, оваа студија ги анализира границите на дистилацијата на знаењето додека го дистилираме знаењето на БЕРТ до линијарни модели - релевантна основа за задачата. Ние известуваме значително подобрување на ефикасноста дури и за наједноставните модели, бидејќи тие го фатат знаењето научиено од БЕРТ.</abstract_mk>
      <abstract_hu>A BERT finomhangolt változatai számos természetes nyelvfeldolgozási feladatnál képesek a legkorszerűbb pontosságot elérni, bár jelentős számítási költségekkel. Ebben a tanulmányban ellenőrizzük a BERT hatékonyságát a dokumentumok osztályozásában, és megvizsgáljuk, hogy a BERT szintű hatékonysága milyen mértékben érhető el különböző alapvonalakkal, kombinálva a tudás desztillációval – ez egy népszerű modell tömörítési módszer. Az eredmények azt mutatják, hogy a BERT-szintű hatékonyságot egyrétegű LSTM segítségével lehet elérni, legalább 40* kevesebb FLOPS és mindössze 3%-os paraméter mellett. Ami még fontosabb, ez a tanulmány elemzi a tudás desztillációjának határait, amint a BERT tudását egészen lineáris modellekig lepároljuk – ez a feladat releváns alapja. Jelentős hatékonysági javulásról számoltunk be még a legegyszerűbb modellek esetében is, mivel azok megragadják a BERT által elsajátított ismereteket.</abstract_hu>
      <abstract_ms>Variansi BERT yang disesuaikan baik mampu mencapai ketepatan-state-of-the-art pada banyak tugas pemprosesan bahasa alam, walaupun pada biaya pengiraan yang signifikan. Dalam kertas ini, kami mengesahkan keefektivitas BERT untuk kelasukan dokumen dan menyelidiki ke mana keefektivitas aras BERT boleh diperoleh dengan garis dasar berbeza, bergabung dengan pengetahuan destilasi-kaedah pemampatan model populer. Hasilnya menunjukkan bahawa keefektivitas aras BERT boleh dicapai dengan LSTM lapisan tunggal dengan sekurang-kurangnya 40* kurang FLOPS dan hanya parameter 3%. More importantly, this study analyzes the limits of knowledge distillation as we distill BERT's knowledge all the way down to linear models-a relevant baseline for the task.  Kami melaporkan peningkatan yang besar dalam keefektivitas untuk model paling mudah, kerana mereka menangkap pengetahuan yang dipelajari oleh BERT.</abstract_ms>
      <abstract_kk>BERT бағдарламасының жақсы бапталған варианттары көптеген тілдерді өңдеу тапсырмаларында дұрыс жеткізе алады, бірақ есептеу бағаларының маңызды. Бұл қағазда біз BERT құжаттарды салыстыру үшін әсер етілігін тексеріп, BERT деңгейінің әсер етілігін түрлі негізгі сызықтар мен білім дистилациялау үлгісімен біріктіруге болады. Нәтижелер BERT деңгейіндегі жұмыс істеуі бір қабатты LSTM арқылы кемінде 40* төмен FLOPS және тек 3% параметрлері жеткізуге болады дегенді көрсетеді. Ең маңызды, бұл зерттеу білім дистилляциясының шектерін анализиру үшін BERT- тың білімін тапсырманың сызық үлгілеріне қарсы негізгі сызықтың шектерін өзгертеді. Біз ең қарапайым үлгілер үшін ең жақсы жұмыс істеу үшін ең жақсы жақсартылығын есептеп береміз. БЕРТ бағытталған білімдерді түсіндіреді.</abstract_kk>
      <abstract_it>Le varianti perfezionate di BERT sono in grado di ottenere una precisione all'avanguardia su molte attività di elaborazione del linguaggio naturale, anche se a costi computazionali significativi. In questo articolo, verifichiamo l'efficacia di BERT per la classificazione dei documenti e analizziamo in che misura l'efficacia a livello BERT può essere ottenuta da diverse linee di base, combinate con la distillazione della conoscenza, un metodo di compressione modello popolare. I risultati mostrano che l'efficacia a livello BERT può essere raggiunta con un LSTM monostrato con almeno il 40* in meno FLOPS e solo il 3% di parametri. Ancora più importante, questo studio analizza i limiti della distillazione della conoscenza mentre distillamo le conoscenze di BERT fino a modelli lineari, una base di riferimento rilevante per il compito. Riportiamo un sostanziale miglioramento dell'efficacia anche per i modelli più semplici, poiché catturano le conoscenze apprese da BERT.</abstract_it>
      <abstract_mt>Varjanti rfinati tal-BERT jistgħu jiksbu l-aktar preċiżjoni avvanzata fuq ħafna kompiti ta’ pproċessar tal-lingwi naturali, għalkemm bi spejjeż komputattivi sinifikanti. F'dan id-dokument, nivverifikaw l-effettività tal-BERT għall-klassifikazzjoni tad-dokumenti u ninvestigaw s a liema punt l-effettività fil-livell tal-BERT tista' tinkiseb minn linji bażi differenti, flimkien mad-distillazzjoni tal-għarfien - metodu ta' kompressjoni tal-mudell popolari. Ir-riżultati juru li l-effikaċja fil-livell BERT tista’ tinkiseb permezz ta’ LSTM ta’ saff wieħed b’mill-inqas 40* inqas FLOPS u parametri ta’ 3% biss. More importantly, this study analyzes the limits of knowledge distillation as we distill BERT's knowledge all the way down to linear models-a relevant baseline for the task.  Aħna nirrappurtaw titjib sostanzjali fl-effettività anke għall-mudelli l-aktar sempliċi, peress li dawn jaqbdu l-għarfien li tgħallem il-BERT.</abstract_mt>
      <abstract_ml>ബെര്‍ട്ടിയുടെ നല്ല മാറ്റങ്ങള്‍ ഈ പത്രത്തില്‍ നമ്മള്‍ രേഖയുടെ ക്ലാസ്ഫിക്ഷനിലേക്കുള്ള ബെര്‍ട്ടിന്റെ പ്രഭാവം പരിശോധിക്കുകയും വ്യത്യസ്തമായ ബെര്‍ട്ടി നില പ്രവർത്തികമാക്കാനുള്ള ഫലങ്ങള്‍ക്ക് കാണിക്കുന്നത് ബെര്‍ട്ടി- നില പ്രവർത്തികമാണെന്നാണ്. ഒരു ഏക സ്ഥാനത്ത് LSTM നിങ്ങള്‍ക്ക് ലഭ്യമാകുന്നത്. കുറഞ്ഞത് 40* ഫ കൂടുതല്‍ പ്രധാനപ്പെട്ടത്, ഈ പഠനത്തിന്റെ അറിവുകളുടെ പരിധികള്‍ വിശദീകരിക്കുന്നു. ബെര്‍ട്ടിയുടെ അറിവ് നമ്മള്‍ എല്ലാവര്‍ക്കും വേര്‍തിരിച് ബെര്‍ട്ടിയില്‍ പഠിക്കുന്ന വിജ്ഞാനം പിടികൂടുന്നത് പോലും സാധാരണ മോഡലുകള്‍ക്ക് വേണ്ടി വലിയ മെച്ചപ്പെടുത</abstract_ml>
      <abstract_ro>Variantele reglate fin ale BERT sunt capabile să obțină acuratețe de ultimă generație în multe sarcini de procesare a limbajului natural, deși cu costuri de calcul semnificative. În această lucrare, verificăm eficacitatea BERT pentru clasificarea documentelor și investigăm măsura în care eficacitatea la nivel BERT poate fi obținută prin diferite linii de bază, combinată cu distilarea cunoștințelor – o metodă populară de compresie a modelului. Rezultatele arată că eficiența la nivelul BERT poate fi obținută printr-un LSTM cu un singur strat cu cel puțin 40* mai puține FLOPS și cu doar 3% parametri. Mai important, acest studiu analizează limitele distilării cunoștințelor pe măsură ce distilăm cunoștințele BERT până la modele liniare - o bază relevantă pentru sarcină. Raportăm o îmbunătățire substanțială a eficacității chiar și pentru cele mai simple modele, deoarece acestea capturează cunoștințele învățate de BERT.</abstract_ro>
      <abstract_pl>Dostrojone warianty BERT są w stanie osiągnąć najnowocześniejszą dokładność w wielu zadaniach przetwarzania języka naturalnego, choć przy znacznych kosztach obliczeniowych. W niniejszym artykule weryfikujemy skuteczność BERT w zakresie klasyfikacji dokumentów i badamy, w jakim stopniu skuteczność BERT można uzyskać za pomocą różnych linii bazowych, w połączeniu z destylacją wiedzy – popularną metodą kompresji modelu. Wyniki pokazują, że skuteczność BERT można osiągnąć dzięki jednowarstwowemu LSTM z co najmniej 40* mniejszą liczbą FLOPS i tylko 3% parametrów. Co ważniejsze, niniejsze badanie analizuje granice destylacji wiedzy, gdy destylujemy wiedzę BERT aż do modeli liniowych, które stanowią istotną bazę podstawową do tego zadania. Raportujemy znaczną poprawę skuteczności nawet w przypadku najprostszych modeli, ponieważ wykorzystują one wiedzę zdobytą przez BERT.</abstract_pl>
      <abstract_no>Det finne innstillingane av BERT er i stand til å oppnå nøyaktighet på mange naturlege språkshandsamar oppgåver, men med signifikante datakostnader. I denne papiret stadfestar vi BERT effektiviteten for dokumentklassifikasjon og undersøker kor mykje BERT-nivåeffektiviteten kan hentast av ulike baselinjer, kombinert med kunnskapsdistillasjon-ein populært modellkomprimeringsmetode. Resultatet viser at effektiviteten for BERT-nivå kan oppnå av ein enkellag LSTM med minst 40* fjerre FLOPS og berre 3% parametrar. Dette studiet analyserer grensene for kunnskapselisering, som vi distiller BERT kunnskap heilt ned til lineære modeller og relevante baseline for oppgåva. Vi rapporterer stor forbedring i effektiviteten for sjølv dei enkleste modelane, som dei hentar kunnskapen lærte av BERT.</abstract_no>
      <abstract_sr>Dobri određeni varianti BERT-a mogu postići tačnost države umjetnosti na mnogim prirodnim zadacima obrađivanja jezika, iako na značajnim računalnim troškovima. U ovom papiru provjeravamo djelotvornost BERT-a za klasifikaciju dokumenta i istražujemo u kakvoj mjeri efikasnost nivoa BERT-a može biti dobijena različitim osnovnim linijama, kombinovana sa destilacijom znanja popularnom metodom kompresije modela. Rezultati pokazuju da je efikasnost nivoa BERT postignuta jednoslojnim LSTM sa najmanjim 40* manjim FLOPS-om i samo 3% parametara. Što je važnije, ova studija analizira granice destilacije znanja, dok destilavamo BERT znanje sve do linijskih modela, relevantnu početnu liniju za taj zadatak. Prijavljujemo značajno poboljšanje učinkovitosti čak i najjednostavnijim modelima, kao što su uhvatili znanje koje je naučio BERT.</abstract_sr>
      <abstract_so>BERT waxey awoodi karaan habka farsamada ah oo ku habboon, habase yeeshee kharashka xisaabta ah. Qoraalkan ayaannu xaqiijinnaa faa’iidada BERT ee warqadda qoraalka, waxaana baaraynaa sidoo kale qaababka ay faa’iidada heerka BERT ku heli karto saldhigyo kala duduwan, waxaana ku dari karnaa qalabka aqoonta-qaabka kooxaha. Abaalku waxay muuqataa in faa'iidada heerka BERT waxaa la gaadhi karaa hal layer oo LSTM ah ugu yaraan 40* oo ka yar FLOPS oo kaliya 3% maamul. Inta ugu muhiimsan, waxbarashadu wuxuu sawiranaa xadhigga aqoonta, sababtoo ah waxaynu u kala soocno aqoonta BERT oo dhan ilaa qaabka sawir ah, kaas oo ah saldhig ku saabsan shaqada. Waxaannu wargelinaynaa hagaajinta waxyaabaha ugu sahlan, sababtoo ah waxay qabsadaan aqoonta lagu baray BERT.</abstract_so>
      <abstract_si>BERT ගේ හොඳ සම්පූර්ණ විකල්පයක් සාමාන්‍ය භාෂාව ප්‍රක්‍රියාත්මක ක්‍රියාත්මක විසින් සම්පූර්ණය කරන්න පුළුවන්. මේ පත්තරේ අපි BERT ගේ විශේෂතාවක් පරීක්ෂණය සහ පරීක්ෂණය සඳහා පරීක්ෂණය කරන්න පුළුවන් වෙනස් පත්තර පරීක්ෂණය සඳහා පරීක්ෂණය සඳහා පරීක්ෂණය ප්‍රතිචාරය පෙන්වන්නේ BERT- මට්ටම් පරීක්ෂණතාවක් එක්ක ස්ථානයක් LSTM වලින් අඩුම 40* අඩුම FLOPS වලින් 3% විතරයි. වඩා වැදගත්, මේ පරීක්ෂණය විශ්ලේෂණය කරනවා දන්නවත් විශ්ලේෂණයේ සීමාව අපි BERT ගේ දන්නවත් විශ්ලේෂණය කරනවා කාර්ය වෙනුවෙන අපි ප්‍රශ්නයක් වෙනුවෙන් සාමාන්‍ය මෝඩේල් වලින් විශාල ප්‍රශ්නයක් වෙනුවෙන් වාර්තාව කරනවා, ඔවුන් BERT වල</abstract_si>
      <abstract_sv>Finjusterade varianter av BERT kan uppnå toppmodern noggrannhet på många bearbetningsuppgifter i naturligt språk, men till betydande beräkningskostnader. I denna uppsats verifierar vi BERT:s effektivitet för dokumentklassificering och undersöker i vilken utsträckning BERT-nivå effektivitet kan erhållas genom olika baslinjer, kombinerat med kunskapsdestillation – en populär modellkomprimeringsmetod. Resultaten visar att BERT-nivå effektivitet kan uppnås med en LSTM med minst 40* färre FLOPS och endast 3% parametrar. Ännu viktigare är att denna studie analyserar gränserna för kunskapsdestillation när vi destillerar BERT:s kunskap hela vägen ner till linjära modeller – en relevant baslinje för uppgiften. Vi rapporterar en betydande förbättring av effektiviteten även för de enklaste modellerna, eftersom de fångar den kunskap som BERT lärt sig.</abstract_sv>
      <abstract_ta>பிரெட்டின் நன்றாக மாறுபாடுகள் பல இயல்பான மொழி செயல்படுத்தல் பணிகளின் நிலையில்-கலை சரியான திட்டத்தை பெற முடியும், ஆனாலும் முக்க இந்த காகிதத்தில், நாம் பெர்டின் விளைவுகளை ஆவண வகுப்பாட்டிற்கு சரிபார்த்து பிரெட் மட்டத்தின் விளைவுகளை வேறு அடிப்பகுதிகளால் பெற முடியும் என்பதை அறிவ முடிவு மேலும் முக்கியமாக, இந்த ஆராய்ச்சி அறிவு பிரெட்டின் அறிவு வேறுபாட்டின் எல்லைகளை ஆய்வு செய்கிறது ஏனென்றால் நாம் பிரெட்டின் அறி நாம் சுலபமான மாதிரிகளுக்கு முன்னேற்றத்தை அறிவிக்கிறோம், பெர்ட் படிப்பினை பிடித்துக் கொள்ளும் போது.</abstract_ta>
      <abstract_mn>БЕРТ-ын сайн тохиромжтой хувилбар нь байгалийн хэлний процесс үйл ажиллагаанд маш чухал тооцоололтын зардалтай ч байдаг. Энэ цаасан дээр бид BERT-ын баримт хуваалцааны үр дүнг шалгаж, BERT-ийн түвшинд үр дүнтэй байдлыг өөр өөр суурь шугам дээр гаргаж чадна гэдгийг судалж, мэдлэгтэй хуваалцах нь алдартай загварын нэгтгэх арга юм. Үүний үр дүнд BERT-ийн түвшинд үр дүнтэй байдлыг нэг давхар LSTM-ээр хамгийн бага 40* бага FLOPS болон зөвхөн 3% параметр гаргаж чадна. Хамгийн чухал нь энэ судалгаа, бид Бертийн мэдлэгийг шулуун загвар руу шилжүүлж, ажлын үндсэн суурь шулууны хил хязгаарыг шинжилдэг. Бид бүр хамгийн хялбар загваруудын үр дүнтэй сайжруулалттай сайжруулалт өгдөг. БЕРТ-ын сурсан мэдлэгийг барьдаг.</abstract_mn>
      <abstract_ur>BERT کی بہترین تنظیم الفاظت بہت سی طبیعی زبان پردازی کے کاموں پر قادر ہیں، اگرچہ بہترین کمپیوٹریسی کے مطابق۔ ہم اس کاغذ میں BERT کے کامیابی کی تصدیق کریں اور کس طرح BERT-level فعالیت حاصل کر سکتے ہیں، جس طرح مختلف بنیس لینوں کے ذریعے، علم distillation-a popular model compression method کے ساتھ ملے جاتے ہیں. نتیجے دکھاتے ہیں کہ BERT-level effectiveness can be achieved by a single-layer LSTM with at least 40* less FLOPS and only 3% parameters. اور زیادہ اہم بات ہے کہ یہ تحقیق علم کے متفرق محدودیت کو تحقیق کرتا ہے جیسے ہم BERT کے علم کو تمام طریقے سے لائنیر موڈل تک پہنچا دیتے ہیں -ایک مسئلہ بنسلین کے لئے۔ ہم بہت سادھے نمڈلوں کے لئے اثبات کے ساتھ بہت اثبات کی تدبیر سناتے ہیں، جیسے وہ BERT کے ذریعہ علم کو پکڑتے ہیں.</abstract_ur>
      <abstract_vi>Chế độ biến hoàn chỉnh của BERT có thể đạt độ chính xác cao nhất trong nhiều công việc xử lý ngôn ngữ tự nhiên, mặc dù với giá trị bằng tính to án đáng kể. Trong tờ giấy này, chúng tôi kiểm tra hiệu quả của BERT cho việc phân loại tài liệu và tìm hiểu xem mức độ hiệu quả của BERT có thể lấy được từ những căn cứ khác nhau, kết hợp với khả năng chưng cất tri thức là một phương pháp nén nổi tiếng. Kết quả cho thấy hiệu quả của BERT có thể được thực hiện bởi một lớp duy nhất LSD với ít nhất 40* ít hơn FOPS và chỉ có 3+ tham khảo. Quan trọng hơn, nghiên cứu này phân tích các giới hạn của việc chưng cất tri thức khi chúng ta chưng cất các kiến thức của BERT cho đến các mô phỏng tuyến, một cơ s ở thích hợp cho nhiệm vụ này. Chúng tôi báo cáo sự hiệu quả đáng kể cả với những mô hình đơn giản nhất, vì chúng nắm bắt kiến thức được học bởi BERT.</abstract_vi>
      <abstract_uz>Fine-tuned variants of BERT are able to achieve state-of-the-art accuracy on many natural language processing tasks, although at significant computational costs.  Bu qogʻozda biz ҳужжат тафсирлаш учун BERT effektini tekshirishimiz va boshqa asboblar bilan BERT darajasining effektligini qidirish mumkin va o'zgarishni o'rganish mumkin. Faylni ajratish va umuman model kompression usuli bilan bogʻliq. Natijalar koʻrsatiladi, BERT- daraja effektligi bir qancha 40* FLOPS bilan bir qancha qanchalik boʻlgan LSTM tomonidan foydalanishi mumkin va faqat 3% parametr boʻlishi mumkin. Muhimlik bo'lsa, bu o'qituvchi, ma'lumotni ajratuvchi chegaralarini analyzer, chunki biz vazifaning muhim asosiy asosida BERT ta'minotini ko'rinishimiz mumkin. Biz oddiy modellar uchun juda katta yaxshi o'zgarishni hisoblash mumkin, chunki ular BERT orqali o'rganishni o'rganish mumkin.</abstract_uz>
      <abstract_bg>Фините варианти на BERT са в състояние да постигнат най-съвременна точност при много задачи за обработка на естествени езици, въпреки че при значителни изчислителни разходи. В тази статия ние проверяваме ефективността на БЕРТ за класификация на документи и изследваме степента, в която ефективността на нивото на БЕРТ може да бъде получена чрез различни базови линии, комбинирани с дестилация на знания - популярен модел метод на компресия. Резултатите показват, че ефективността на ниво BERT може да бъде постигната чрез еднослоен ЛСТМ с най-малко 40* по-малко FLOPS и само 3% параметри. По-важното е, че това проучване анализира границите на дестилацията на знанието, докато дестилираме знанията на BERT чак до линейни модели - релевантна база за задачата. Докладваме за значително подобрение на ефективността дори и за най-простите модели, тъй като те улавят знанията, придобити от BERT.</abstract_bg>
      <abstract_de>Fein abgestimmte Varianten von BERT sind in der Lage, bei vielen natürlichen Sprachverarbeitungsaufgaben den neuesten Stand der Technik zu erreichen, wenn auch mit erheblichen Rechenkosten. In diesem Beitrag überprüfen wir die Effektivität von BERT für die Dokumentenklassifizierung und untersuchen, inwieweit die Effektivität von BERT durch verschiedene Basislinien, kombiniert mit Wissensdestillation, einer beliebten Modellkompressionsmethode, erreicht werden kann. Die Ergebnisse zeigen, dass die Wirksamkeit auf BERT-Ebene durch ein einlagiges LSTM mit mindestens 40* weniger FLOPS und nur 3% Parametern erreicht werden kann. Noch wichtiger ist, dass diese Studie die Grenzen der Wissensdestillation analysiert, indem wir das Wissen von BERT bis hin zu linearen Modellen destillieren – eine relevante Grundlage für die Aufgabe. Wir berichten von einer deutlichen Verbesserung der Effektivität selbst für die einfachsten Modelle, da sie das von BERT erlernte Wissen erfassen.</abstract_de>
      <abstract_hr>Dobri određeni varianti BERT-a mogu postići tačnost države umjetnosti na mnogim prirodnim zadacima obrađivanja jezika, iako na značajnim računalnim troškovima. U ovom papiru provjeravamo učinkovitost BERT-a za klasifikaciju dokumenta i istražujemo u kakvoj mjeri učinkovitost nivoa BERT-a može dobiti različitim osnovnim linijama, zajedno s destilacijom znanja-popularnom metodom kompresije modela. Rezultati pokazuju da učinkovitost nivoa BERT može postići jednoslojnim LSTM s najmanjim 40* manjim FLOPS-om i samo 3% parametara. Što je važnije, ova istraživanja analizira granice destilacije znanja jer destilacija znanja BERT-a sve do linijskih modela - relevantna početna linija za zadatak. Prijavljujemo značajno poboljšanje učinkovitosti čak i najjednostavnijim modelima, jer su uhvatili znanje koje je naučio BERT.</abstract_hr>
      <abstract_nl>Fijn afgestemde varianten van BERT zijn in staat om state-of-the-art nauwkeurigheid te bereiken bij veel natuurlijke taalverwerkingstaken, hoewel tegen aanzienlijke rekenkosten. In dit artikel verifiëren we de effectiviteit van BERT voor documentclassificatie en onderzoeken we in hoeverre de effectiviteit op BERT-niveau kan worden verkregen door verschillende baselines, gecombineerd met kennisdestillatie – een populaire modelcompressiemethode. De resultaten tonen aan dat BERT-niveau effectiviteit kan worden bereikt door een enkellaagse LSTM met minimaal 40* minder FLOPS en slechts 3% parameters. Belangrijker nog is dat deze studie de grenzen van kennisdestillatie analyseert terwijl we de kennis van BERT tot aan lineaire modellen distilleren – een relevante basis voor de taak. We rapporteren een aanzienlijke verbetering van de effectiviteit zelfs voor de eenvoudigste modellen, omdat ze de kennis die BERT heeft geleerd vastleggen.</abstract_nl>
      <abstract_da>Finjusterede varianter af BERT er i stand til at opnå topmoderne nøjagtighed på mange naturlige sprogbehandlingsopgaver, selv om det koster betydelige beregningsomkostninger. I denne artikel kontrollerer vi BERT's effektivitet til dokumentklassificering og undersøger, i hvilket omfang BERT-niveau effektivitet kan opnås ved forskellige basislinjer kombineret med videndedestillation – en populær modelkomprimeringsmetode. Resultaterne viser, at BERT-niveau effektivitet kan opnås med et enkelt lag LSTM med mindst 40* færre FLOPS og kun 3% parametre. Endnu vigtigere er, at denne undersøgelse analyserer grænserne for videndedestillation, når vi destillerer BERT's viden hele vejen ned til lineære modeller – en relevant baseline for opgaven. Vi rapporterer om en betydelig forbedring af effektiviteten for selv de enkleste modeller, da de fanger den viden, BERT har lært.</abstract_da>
      <abstract_ko>비록 계산 원가가 매우 높지만, 미세한 조정을 거친 BERT 변체는 많은 자연 언어 처리 임무에서 가장 선진적인 정밀도를 실현할 수 있다.본고에서 우리는 BERT가 문서 분류에서의 유효성을 검증했고 유행하는 모델 압축 방법 지식을 결합하여 서로 다른 기선에서 BERT급의 유효성 정도를 연구했다.그 결과 단층 LSTM은 BERT급의 유효성을 실현할 수 있고 최소 40*회 트리거를 줄일 수 있으며 파라미터는 3%에 불과하다.더 중요한 것은 이 연구는 버트의 지식을 선형 모델로 추출하는 것이 임무의 관련 기선이기 때문에 지식 추출의 한계성을 분석했다.가장 간단한 모델이라도 BERT에서 배운 지식을 포착했기 때문에 그 유효성이 실질적으로 향상되었다고 보고했다.</abstract_ko>
      <abstract_id>Variansi BERT yang disesuaikan baik dapat mencapai akurasi state-of-the-art pada banyak tugas proses bahasa alam, meskipun dengan biaya komputasi yang signifikan. In this paper, we verify BERT's effectiveness for document classification and investigate the extent to which BERT-level effectiveness can be obtained by different baselines, combined with knowledge distillation-a popular model compression method.  Hasilnya menunjukkan bahwa efektivitas tingkat BERT dapat dicapai dengan LSTM lapisan tunggal dengan setidaknya 40* lebih sedikit FLOPS dan hanya 3% parameter. Yang lebih penting, penelitian ini menganalisis batas distillasi pengetahuan saat kita mendestilkan pengetahuan BERT sampai ke model linear-dasar yang relevan untuk tugas ini. Kami melaporkan peningkatan yang besar dalam efektivitas bahkan model paling sederhana, karena mereka menangkap pengetahuan yang belajar oleh BERT.</abstract_id>
      <abstract_sw>Mabadiliko mazuri ya BERT yanaweza kupata uhakika wa hali ya sanaa katika kazi nyingi za utaratibu wa lugha za asili, ingawa kwa gharama kubwa za hisabati. Katika karatasi hii, tunathibitisha ufanisi wa BERT kwa usambazaji wa dokumentari na kuchunguza kiwango ambacho ufanisi wa kiwango cha BERT kinaweza kupatikana na misingi tofauti, ukiunganishwa na utofauti wa maarifa-njia ya kompyuta maarufu. Matokeo yanaonyesha kuwa ufanisi wa kiwango cha BERT unaweza kufanikiwa na kiwango cha LSTM cha ngazi moja na angalau 40* wachache zaidi ya FLOPS na kipimo cha asilimia 3 tu. Kimuhimu zaidi, utafiti huu unachambua ukomo wa utofauti wa maarifa kwa sababu tunachambua maarifa ya BERT kwa njia zote kuelekea mifano ya msingi-msingi muhimu kwa kazi hiyo. Tunashiria maboresho makubwa katika ufanisi kwa hata mifano rahisi, kwa sababu wanajifunza maarifa yanayofunzwa na BERT.</abstract_sw>
      <abstract_tr>BERT'iň gowy görnüşli warianatlary tebigy diller işlemek üçin birnäçe düzgünliki başaryp bilýär. Haýsy hasaplamak töleginde hem. Bu kagyzda, BERT sened klasifikasyýasy üçin etkinliýetini barlaýarys we BERT derejesiniň etkinliýetini farklı baseçinler bilen berilip bilen meşhur bir nusga sykylama yöntemi bilen barlaýarys. Netijeler BERT derejesi etkinlik ýeke gatlak LSTM tarapyndan iň azyndan 40* iň az FLOPS we diňe 3% parameterler tarapyndan ýetip biler diýip görkezilýärler. Daha möhüm bolsa, bu studiýa BERT'yň bilgilerini çyzgylaşdyryp, işiň üçin çyzgylyk nusgalary çyzgylaşdyrýar. Biz BERT tarapyndan öwrenmän bilimi tutyp çykýan iň basit nusgalar üçin örän täsirli gelişmeleri bildirip bilýäris.</abstract_tr>
      <abstract_af>Fine-tuned variante van BERT kan staat-van-kuns-presies op baie natuurlike taal verwerking opdragte bereik, alhoewel op betekende rekenaar koste. In hierdie papier, ons bevestig BERT se effektiviteit vir dokumentklassifikasie en ondersoek die uitbreiding waarin BERT-vlak effektiviteit deur verskillende basilyne kan ontvang word, gekombineer met kennis distillation-a populêre model kompresie metode. Die resultate vertoon dat BERT- vlak effektiviteit kan word bereik deur 'n enkele- laag LSTM met ten minste 40* minder FLOPS en slegs 3% parameters. Nog belangrik, hierdie studie analyseer die grense van kennis destilasie a s ons verstel BERT se kennis al die pad af na lineêre modele-â relevante basislien vir die taak. Ons rapporteer substantiele verbetering in effektiviteit vir selfs die eenvoudige modele, soos hulle die kennis wat deur BERT geleer het, opgeneem.</abstract_af>
      <abstract_am>የBERT መለያየት በተለየ ብሔራዊ ቋንቋ አካባቢ ክፍል ምንም እንኳን በተለየ ቁጥጥር ውጤቶች ላይ የ-አርራሲ አካል ማግኘት ይችላል፡፡ በዚህ ፕሮግራም የBERT ሥርዓት ለሰነድ መግለጫ እና የBERT-ደረጃ ፍቃዱን በተለየ የደረጃ መግለጫ እንዴት እንደደረገው እናረጋግጣለን፡፡ ፍሬዎቹ የBERT-ደረጃ ጥቅም በአንዲት ደረጃ LSTM እንዲደርስ ነው፡፡ ይልቁንም በትምህርት፣ ይህ ትምህርት የእውቀትን ግንኙነት ያሳያል፤ የBERT እውቀትን ሁሉ ወደ ስራው ግንኙነት መሠረት የሚለይ ነው፡፡ በBERT የተማረውን እውቀት በመያዙ ምክንያት ለቀላል ምሳሌዎች እናደርጋለን፡፡</abstract_am>
      <abstract_fa>تغییرات نیکویی از BERT قادر هستند که دقیقات وضعیت هنری بر بسیاری از کارهای پردازش زبان طبیعی را به دست آورده باشند، اگرچه با هزینه های محاسباتی بزرگ. در این کاغذ، ما موثرت BERT را برای classification of document s verify and investigate the extent to which BERT-level effectiveness can be obtained by different baselines, combined with knowledge distillation-a popular model compression. نتیجه‌ها نشان می‌دهند که فعالیت سطح BERT با یک لایه LSTM با حداقل ۴۰* کمتر FLOPS و فقط ۳% پارامتر می‌تواند رسید. مهمتر است، این مطالعه محدودیت تفریح علمی را تحلیل می‌کند، در حالی که ما دانش BERT را به مدل خطی تفریح می‌دهیم، یک خطی بنیادی مربوط به کار است. ما گزارش می دهیم که بهترین پیشرفت عملی در موثرت برای حتی ساده ترین مدل، همانطور که آنها دانش را که توسط BERT یاد گرفته می‌گیرند دستگیر می‌کنند.</abstract_fa>
      <abstract_hy>ԲԵՌԹ-ի բարձրացված տարբերակները կարողանում են հասնել ամենաբարձր ճշգրտության շատ բնական լեզվի վերամշակման խնդիրների վրա, չնայած որ դրանք նշանակալի հաշվարկման արժեքներով: Այս թղթի մեջ մենք ստուգում ենք BER-ի արդյունավետությունը փաստաթղթերի դասակարգման համար և ուսումնասիրում ենք, թե որքանով BER-ի մակարդակի արդյունավետությունը կարող է ստանալ տարբեր հիմնական գծերով, համադրված գիտելիքների դիսլացիայի հետ, հայտնի մո Արդյունքները ցույց են տալիս, որ BER-ի մակարդակի արդյունավետությունը կարելի է հասնել մեկ շերտ LSMT-ի միջոցով, որը ունի առնվազն 40 * ավելի քիչ ֆլոպս և միայն 3 տոկոս պարամետրեր: Ավելի կարևոր է, որ այս ուսումնասիրությունը վերլուծում է գիտելիքի դիսլիլացիայի սահմանները, քանի որ մենք դիսլիլացնում ենք ԲԵՌԹ-ի գիտելիքները մինչև գծային մոդելներ, որն առաջադրանքի կարևոր հիմք է: Մենք զեկուցում ենք, որ արդյունավետության մեծ բարելավում է նույնիսկ ամենապարզ մոդելների համար, քանի որ նրանք վերցնում են BER-ի սովորած գիտելիքները:</abstract_hy>
      <abstract_sq>Variante të rregulluara të BERT janë në gjendje të arrijnë saktësinë më të lartë në shumë detyra natyrore të përpunimit të gjuhës, megjithëse me kosto llogaritare të rëndësishme. Në këtë letër, ne verifikojmë efektshmërinë e BERT për klasifikimin e dokumenteve dhe hetojmë s a efektshmëria e nivelit BERT mund të arrihet nga linja bazë të ndryshme, të kombinuara me distillacionin e njohurive-një metodë kompresimi i modelit popullor. Rezultatet tregojnë se efektshmëria e nivelit BERT mund të arrihet nga një LSTM me të paktën 40* më pak FLOPS dhe vetëm 3% parametra. Më e rëndësishme, ky studim analizon kufijtë e distillacionit të njohurive ndërsa ne distillojmë njohuritë e BERT deri në modelet lineare-një bazë e rëndësishme për detyrën. Ne raportojmë përmirësim thelbësor në efektshmëri edhe për modelet më të thjeshta, ndërsa ato kapin njohuritë e mësuara nga BERT.</abstract_sq>
      <abstract_bs>Dobri određeni varianti BERT-a mogu postići tačnost države umjetnosti na mnogim prirodnim zadacima obrađivanja jezika, iako na značajnim računalnim troškovima. U ovom papiru provjeravamo učinkovitost BERT-a za klasifikaciju dokumenta i istražujemo u kakvoj mjeri učinkovitost na nivou BERT-a može dobiti različitim osnovnim linijama, kombiniranom sa destilacijom znanja-popularnom metodom kompresije modela. Rezultati pokazuju da učinkovitost nivoa BERT može postići jednoslojnim LSTM sa najmanjim 40* manjim FLOPS-om i samo 3% parametara. Što je važnije, ova studija analizira granice destilacije znanja jer destilacija znanja BERT-a sve do linijskih modela - relevantna početna linija za zadatak. Prijavljujemo značajno poboljšanje učinkovitosti čak i najjednostavnijim modelima, jer uhvate znanje koje je naučio BERT.</abstract_bs>
      <abstract_ca>Les variants fins ajustades del BERT són capaços d'aconseguir la precisió més avançada en moltes tasques naturals de processament de llenguatges, encara que a costos computacionals significatius. En aquest paper, verifiquem l'eficacia del BERT en la classificació de document s i investigam fins a quin punt l'eficacia del BERT es pot obtenir per diferents línies de base, combinats amb la distillació del coneixement-un mètode de compressió popular del model. Els resultats mostran que l'eficacia del nivell BERT es pot aconseguir amb una LSTM de una sola capa amb al menys 40* menys FLOPS i només un 3% de paràmetres. El més important és que aquest estudi analitza els límits de la distillació del coneixement mentre destillem el coneixement de BERT fins a models linears, una base rellevant per a la tasca. Rapportem una millora substancial en l'eficacia fins i tot dels models més senzills, mentre capturan el coneixement aprengut per BERT.</abstract_ca>
      <abstract_bn>অনেক প্রাকৃতিক ভাষার প্রক্রিয়ার কাজের উপর ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভ এই কাগজটিতে আমরা ডকুমেন্ট শ্রেণীকরণের কার্যক্রম পরীক্ষা করি এবং তদন্ত করি বিবেরেট-স্তরের কার্যক্রম বিভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভ ফলাফল দেখা যাচ্ছে যে বেরেট-স্তরের কার্যকলাপ একটি একক স্তর এলস্টিএম দ্বারা পৌঁছাতে পারে যার মধ্যে অন্তত ৪০* কম ফ্লোপস এবং শুধুমাত্র ৩ আরো গুরুত্বপূর্ণ, এই গবেষণাটি জ্ঞানের বিভাগের সীমানা বিশ্লেষণ করে যেহেতু আমরা বিবের্টের জ্ঞান পুরোপুরি দিকে বিচ্ছিন্ন করি লাইনি আমরা প্রতিবেদন করছি এমনকি সবচেয়ে সহজ মডেলের কার্যকর কার্যকলাপের জন্য, যেহেতু তারা বাইরেটের কাছ থেকে জ্ঞানের শিক্ষ</abstract_bn>
      <abstract_az>BERT'nin yaxŇüńĪ d√ľz…ôldil…ôn d…ôyiŇüiklikl…ôri √ßoxlu t…ôbi…ôtli dil iŇül…ôm…ô iŇül…ôri il…ô m√ľ…ôyy…ôn edil…ôn t…ôrzd…ô, √ßoxlu t…ôbi…ôtli hesaplama maliyy…ôti il…ô baŇüa √ßata bil…ôrl…ôr. Bu kańüńĪzda, BERT'nin s …ôhif…ôsini bel…ô t…ôsdiql…ôyirik v…ô BERT s…ôviyy…ôsini m√ľxt…ôlif s…ôhif…ôl…ôrl…ô …ôld…ô ed…ô bil…ôc…ôyi m√ľxt…ôlif s…ôhif…ôl…ôr il…ô birlikd…ô m…ôŇühur modeli sńĪkńĪŇütńĪrma metodu il…ô birlikd…ô m√ľxt…ôlif s…ôhif…ôl…ôr t…ôsdiql…ôyirik. Sonu√ßlar bel…ô g√∂st…ôrir ki, BERT s…ôviyy…ôsi t…ôk-katlńĪ LSTM il…ô …ôn az 40* daha az FLOPS v…ô yalnńĪz 3% parametru il…ô yetiŇü…ô bil…ôr. Daha da √∂nemlidir, bu t…ôhsil, BERT'nin bilgisini linear modell…ôr…ô baxńĪb, bu iŇüin m…ôqs…ôdil…ô …ôlaq…ôli bir s …ôhif…ô √ß…ôkilm…ôsinin sńĪnńĪrlarńĪnńĪ analiz edir. Bel…ôlikl…ô, BERT tarafńĪndan √∂yr…ôndiyi elmi almaq √ľ√ß√ľn …ôn basit modell…ôr √ľ√ß√ľn √ßox yaxŇüńĪlńĪqlarńĪnńĪ bildiririk.</abstract_az>
      <abstract_et>BERTi täpsustatud variandid suudavad saavutada tipptasemel täpsuse paljudes looduskeelte töötlemise ülesannetes, kuigi märkimisväärsete arvutuskuludega. Käesolevas töös kontrollime BERTi efektiivsust dokumentide klassifitseerimisel ja uurime, mil määral saab BERT-taseme efektiivsust saavutada erinevate lähtejoonte abil, kombineerituna teadmiste destilleerimisega – populaarse mudeli kompressioonimeetodiga. Tulemused näitavad, et BERT-taseme efektiivsust saab saavutada ühekihilise LSTM-i abil, millel on vähemalt 40* vähem FLOPS-i ja ainult 3% parameetreid. Veelgi olulisem on see, et käesolev uuring analüüsib teadmiste destilleerimise piire, kui me destilleerime BERTi teadmisi kuni lineaarsete mudeliteni, mis on ülesande jaoks asjakohane lähtealus. Teatame märkimisväärsest tõhususe paranemisest isegi kõige lihtsamate mudelite puhul, sest need hõlmavad BERTi omandatud teadmisi.</abstract_et>
      <abstract_cs>Jemně vyladěné varianty BERT jsou schopny dosáhnout nejmodernější přesnosti u mnoha úloh zpracování přirozeného jazyka, i když za značné výpočetní náklady. V tomto článku ověřujeme účinnost BERT pro klasifikaci dokumentů a zkoumáme, do jaké míry lze dosáhnout účinnosti BERT různými základními liniemi v kombinaci s destilací znalostí – populární metodou komprese modelu. Výsledky ukazují, že účinnost BERT může být dosažena jednovrstvým LSTM s minimálně 40* méně FLOPS a pouze 3% parametry. Důležitější je, že tato studie analyzuje hranice destilace znalostí při destilaci znalostí BERT až do lineárních modelů – relevantní základní základ pro tento úkol. Zjišťujeme výrazné zlepšení efektivity i u těch nejjednodušších modelů, protože zachycují znalosti získané BERT.</abstract_cs>
      <abstract_fi>Hienoviritetyt BERT-versiot pystyvät saavuttamaan huipputason tarkkuuden monissa luonnollisen kielen käsittelytehtävissä, vaikkakin huomattavilla laskennallisilla kustannuksilla. Tässä työssä varmennetaan BERT:n tehokkuus asiakirjojen luokittelussa ja tutkitaan, missä määrin BERT-tason tehokkuus voidaan saavuttaa eri lähtölinjoilla yhdistettynä tietämyksen tislaukseen – suosittuun mallikompressiomenetelmään. Tulokset osoittavat, että BERT-tason tehokkuus voidaan saavuttaa yksikerroksisella LSTM:llä, jossa on vähintään 40* vähemmän FLOPS:iä ja vain 3% parametreja. Tärkeämpää tässä tutkimuksessa analysoidaan tietämyksen tislauksen rajoja tislattaessa BERT:n tietämystä aina lineaarisiin malleihin asti, mikä on oleellinen lähtökohta tehtävälle. Raportoimme, että jopa yksinkertaisimpien mallien tehokkuus on parantunut huomattavasti, sillä niissä hyödynnetään BERT:n oppimaa tietoa.</abstract_fi>
      <abstract_sk>Fino nastavljene različice BERT lahko dosežejo najsodobnejšo natančnost pri številnih nalogah obdelave naravnega jezika, čeprav z znatnimi računalniškimi stroški. V prispevku preverjamo učinkovitost BERT-a pri klasifikaciji dokumentov in raziskujemo, v kolikšni meri je učinkovitost BERT-a mogoče doseči z različnimi osnovnimi linijami, kombiniranimi z destilacijo znanja – priljubljeno modelno metodo kompresije. Rezultati kažejo, da je učinkovitost BERT mogoče doseči z enoslojnim LSTM z najmanj 40* manj FLOPS in le 3% parametri. Še pomembneje je, da ta študija analizira meje destilacije znanja, ko destiliramo BERT-ovo znanje vse do linearnih modelov – ustrezno osnovo za nalogo. Poročamo o znatnem izboljšanju učinkovitosti celo za najpreprostejše modele, saj zajemajo znanje, ki ga je pridobil BERT.</abstract_sk>
      <abstract_ha>Bayan variants da aka gyaɗa BERT na iya iya cika halin-halin-kunna kan aikin masu aiki masu cikin harshen kwance, kuma kõ da kuma, idan yana da ƙari mai ƙidãya. Ga wannan takardan, Munã tabbatar da aikin BERT wa fasalin takardar kuma munã jãyayya a tsakanin da za'a iya s ãmu mai amfani da wajen daraja na BERT-daraja daban-daban, da kuma a sami da zane-zane-zane-zane-da misalin-umare-zane-zane-zane-zane-zane-zane-zane-zane-zane-zan Mataimakin na nuna cewa, za'a iya kai amfani da alamar BERT-leveli da an haɗa lokacin LSM da ko da a ƙara 40*kaɗan FLLoPS kuma kawai -3%parameters kawai. Ga muhimu, wannan littafi yana analyza tsarin zane da ake rarraba ma'ani da BERT ko duk zuwa misalin linje-a-salin mai muhimmi ga aikin. Tuna rapi masu ƙari da mafiya amfani ga mafiya amfani, ko kuwa masu kammala masu sauki, kamar yadda suna sami ilmi na da BERT.</abstract_ha>
      <abstract_jv>variants of BERT are able to success Nang pepulan iki, kita ngupakan efek BERT kanggo kelas nggawe seneng pisan anyari karo perusahaan nggawe barang nggawe barang BERT-kalih efek dipunahan seneng dipunahan seneng dipunahan, sampek karo akeh awak dhéwé tanggal dipunahan seneng kesempresan model sing perusahaan populer Reject Awak dhéwé, akeh basa iki dipuluhayo perusahaan kanggo ngerasakno nggawe barang nggawe barang BERT kuwi dianggawe barang nggawe model linear-a perusahaan bakal terus nggawe operasi. Awak dhéwé ngertuan langgar bantuan nggawe lan jewisan kanggo model sing luwih apik, kaya ngono awak dhéwé kuwi nggawe ngerti BERT kuwi.</abstract_jv>
      <abstract_he>שינויים מיוחדים של BERT מצליחים להשיג מדויקת מוקדמת על משימות רבות של עיבוד שפת טבעית, למרות שבעלות חישובים משמעותיות. בעיתון הזה, אנו מאשרים את היעילות של BERT להקליטה מסמכים וחקירים את המידה עד כמה היעילות ברמה BERT יכולה להשיג על ידי שורות בסיסיים שונות, בשילוב עם מידע-שיטת דחוס מודל פופולרית. התוצאות מראות שיכולת להשיג יעילות ברמה BERT על ידי LSTM שכבה אחת עם לפחות 40* פחות FLOPS ופרמטרים של רק 3%. וחשוב יותר, המחקר הזה מנתח את הגבולות של הדיסטליה של ידע בזמן שאנחנו מדיסטים את ידע של BERT כל הדרך למטה לדוגמאות לינריות-בסיס רלוונטי למשימה. אנחנו מדווחים על שיפור משמעותי ביעילות אפילו לדוגמאות הפשוטות ביותר, כפי שהם תופסים את הידע שנלמד על ידי BERT.</abstract_he>
      <abstract_bo>BERT ཡི་མཛོད་ཀྱི་གཟུགས་འགྱུར་ཚད་ལ་ཕན་མེད་སྨྱུག་ལས་རང་བཞིན་སྐད་ཀྱི་ལས་སྦྱོར་ཀྱི་བྱ་བ་མང་ཙམ་རྙེད་ཐུབ་པ་ཡིན་ནའང་མ་ཚད་རྩ In this paper, we verify BERT's effectiveness for document classification and investigate the extent to which BERT-level effectiveness can be obtained by different baselines, combined with knowledge distillation-a popular model compression method. The results show that BERT-level effectiveness can be achieved by a single-layer LSTM with at least 40* fewer FLOPS and only  3% parameters. ཧ་ཅང་གལ་ཆེ་བའི་ལྟ་བ་འདིས་ངེད་ཚོའི་དབྱེ་བ་དེ་གསལ་བཤད་ཀྱི་ཚད་གཞི་རྩལ་ཞིབ་དཔྱད་ནས་ ང་ཚོས་BERT ལས་ཤེས་ཡོད་པའི་ཆེད་དུ་དམིགས་བསལ་ནུས་ཡོད་ཚད་ལྡན་གྱིས་རྐྱེན་ཡར་རྒྱས་གཏོང་བ་ཞིག་ཡོད།</abstract_bo>
      <abstract_fil>Ang mabuting variant ng BERT ay makatataglay ng state-of-the-art accuracy sa maraming natural language processing tasks, bagaman sa mahalagang computational costs. Sa papiro na ito, aming pinagtibay ang effectiveness ng BERT para s a klasifikasyon ng mga dokumento at pinagtibay ang extent na maaaring makakuha ng iba pang baselines ang effectiveness ng BERT-level, na kasama ng kaalamang distillation-a ng popular model compression method. Ang mga resulta ay nagpapakita na ang epektiveness ng BERT-level ay maaaring makatagumpay ng isang layer LSTM na may kahit 40* kahit kaunti ang FLOPS at lamang ang 3% parameters. Dahil pang importante, ang estudyantong ito ay nagtatali ng mga limites ng pagkakilala na distillation habang aming inililiwanag ang kaalaman ni BERT s a lahat ng daan sa linear models-a ang relevant baseline para sa gawain. Nagbibigay kami ng malaking pagbabago sa effectiveness para sa makatuwid baga ng mga pinakamasarap na modelo, habang kanilang tinatanggap ang kaalaman na natutunan ng BERT.</abstract_fil>
      </paper>
    <paper id="16">
      <title>Are All Languages Created Equal in Multilingual BERT?<fixed-case>BERT</fixed-case>?</title>
      <author><first>Shijie</first><last>Wu</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>120–130</pages>
      <abstract>Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks : <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named Entity Recognition</a> (99 languages), <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">Part-of-speech Tagging</a> and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these <a href="https://en.wikipedia.org/wiki/Language">languages</a> do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for low resource languages require more efficient pretraining techniques or more data.</abstract>
      <url hash="d94ad761">2020.repl4nlp-1.16</url>
      <doi>10.18653/v1/2020.repl4nlp-1.16</doi>
      <video href="http://slideslive.com/38929782" />
      <bibkey>wu-dredze-2020-languages</bibkey>
      <pwccode url="https://github.com/shijie-wu/crosslingual-nlp" additional="false">shijie-wu/crosslingual-nlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    <title_ar>هل جميع اللغات التي تم إنشاؤها متساوية في BERT متعدد اللغات؟</title_ar>
      <title_fr>Toutes les langues sont-elles créées égales dans le BERT multilingue ?</title_fr>
      <title_es>¿Se crean todos los idiomas iguales en BERT multilingüe?</title_es>
      <title_pt>Todos os idiomas são criados iguais no BERT multilíngue?</title_pt>
      <title_ja>多言語BERTで作成されたすべての言語は等しいですか？</title_ja>
      <title_zh>多言BERT中,诸言平等乎?</title_zh>
      <title_hi>क्या सभी भाषाओं को बहुभाषी BERT में समान बनाया गया है?</title_hi>
      <title_ru>Все ли языки созданы равными в многоязычном BERT?</title_ru>
      <title_ukr>Чи всі мови створені рівними в багатомовному BERT?</title_ukr>
      <title_ga>An bhfuil Gach Teanga Cruthaithe Comhionann i CRET Ilteangach?</title_ga>
      <title_ka>ყველა ენები მრავალენგური BERT-ში შექმნილი ტოლია?</title_ka>
      <title_el>Όλες οι γλώσσες δημιουργούνται ίσες στο πολύγλωσσο BERT;</title_el>
      <title_hu>Minden nyelv egyenlő a többnyelvű BERT-ben?</title_hu>
      <title_isl>Eru allar tungur búnar til jafnar í fjöltungu BERT?</title_isl>
      <title_it>Tutte le lingue sono create uguali in BERT multilingue?</title_it>
      <title_lt>Are All Languages Created Equal in Multilingual BERT?</title_lt>
      <title_kk>Бүкіл тілдер көптілік BERT- де тең құрылған ма?</title_kk>
      <title_mk>Are All Languages Created Equal in Multilingual BERT?</title_mk>
      <title_ml>എല്ലാ ഭാഷകളും ഒരുപോലെ സൃഷ്ടിക്കപ്പെട്ടിരിക്കുന്നുണ്ടോ?</title_ml>
      <title_ms>Adakah Semua Bahasa dicipta sama dalam BERT berbilang bahasa?</title_ms>
      <title_mt>Il-lingwi kollha jinħolqu ugwali fil-BERT Multilingwi?</title_mt>
      <title_mn>Бүх хэл олон хэлний BERT-д тэнцүү байдаг уу?</title_mn>
      <title_no>Er alle språk laga like i fleirspråk BERT?</title_no>
      <title_ro>Toate limbile sunt create egale în BERT multilingv?</title_ro>
      <title_si>සියළු භාෂාවල් සිර්මාණය කරලා තියෙන්නේ විශාල භාෂාව BERT වලද?</title_si>
      <title_pl>Czy wszystkie języki są równe w wielojęzycznym BERT?</title_pl>
      <title_sr>Jesu li svi jezici stvoreni jednaki u multijezičkom BERT-u?</title_sr>
      <title_so>Dhammaan luqadaha oo dhammu ma u abuureen sinnaan oo ku qoran luuqado kala duduwan BERT?</title_so>
      <title_sv>Skapas alla språk lika i flerspråkig BERT?</title_sv>
      <title_ta>அனைத்து மொழிகளும் சமமாக உருவாக்கப்பட்டுள்ளதா பல மொழிகள் BERT?</title_ta>
      <title_ur>کیا تمام زبانیں ملتی زبان BERT میں برابر بنائے گئے ہیں؟</title_ur>
      <title_uz>Hamma tillar bir necha tillar BERT tilida tenglar yaratilmi?</title_uz>
      <title_vi>Tất cả ngôn ngữ đều được tạo bằng ngôn ngữ rộng?</title_vi>
      <title_bg>Всички езици ли са създадени еднакви в многоезичния BERT?</title_bg>
      <title_da>Er alle sprog skabt lige i flersproget BERT?</title_da>
      <title_nl>Zijn alle talen gelijkwaardig gemaakt in meertalig BERT?</title_nl>
      <title_hr>Jesu li svi jezici stvoreni jednaki u mnogih jezika BERT?</title_hr>
      <title_de>Sind alle Sprachen im mehrsprachigen BERT gleich?</title_de>
      <title_id>Apakah semua bahasa diciptakan sama dalam BERT berbilang bahasa?</title_id>
      <title_ko>모든 언어는 다국어 환경에서 평등합니까?</title_ko>
      <title_sw>Lugha zote zimetengenezwa sawa katika lugha nyingi BERT?</title_sw>
      <title_fa>آیا همه زبانها در BERT Multilingual ایجاد شده‌اند؟</title_fa>
      <title_tr>Ehli Diller Birden Dilli BERT-de eynidir döredilen Dillermi?</title_tr>
      <title_af>Is al Taal gemaak gelyk in Multilingual BERT?</title_af>
      <title_sq>A janë të gjitha gjuhët krijuar të barabarta në BERT shumëgjuhës?</title_sq>
      <title_am>ቋንቋዎች ሁሉ እየተመሳሰሉ ብሬት?</title_am>
      <title_hy>Արդյո՞ք բոլոր լեզուները հավասար են ստեղծվել բազլեզու BER-ում:</title_hy>
      <title_az>B칲t칲n dill톛r 칞oxlu dil BERT i칞ind톛 eyni ola bil톛rmi?</title_az>
      <title_bn>সমস্ত ভাষা কি বহুভাষায় সমান সৃষ্টি করা হয়েছে?</title_bn>
      <title_bs>Jesu li svi jezici stvoreni jednaki u multijezičkom BERT-u?</title_bs>
      <title_ca>Totes les llengües es creen iguals en BERT multilingüe?</title_ca>
      <title_cs>Jsou všechny jazyky vytvořeny stejně ve vícejazyčném BERT?</title_cs>
      <title_et>Kas kõik keeled on loodud mitmekeelses BERT-is võrdseteks?</title_et>
      <title_fi>Ovatko kaikki kielet luotu tasa-arvoisiksi monikielisessä BERT:ssä?</title_fi>
      <title_jv>Apa Bapak Dino Sampeyan kanggo Ketoke Gak Nang Multi-Lingui BERT?</title_jv>
      <title_ha>Shin, Duk harshes ne An halitta Ekima da Akan cikin Birem na BaERT?</title_ha>
      <title_he>Are All Languages Created Equal in Multilingual BERT?</title_he>
      <title_sk>Ali so v večjezičnem BERT ustvarjeni vsi jeziki enaki?</title_sk>
      <title_fil>Lahat bang wika ang nilalang ng Equal sa Multilingual BERT?</title_fil>
      <title_bo>སྐད་ཡིག་ཆ་དབྱིབས་BERT ནང་དུ་དབྱིབས་ཡོད་པའི་སྐད་ཡིག་ཆ་ཡིན་ནམ</title_bo>
      <abstract_ar>أظهر BERT متعدد اللغات (mBERT) الذي تم تدريبه على 104 لغة أداءً جيدًا عبر اللغات بشكل مدهش في العديد من مهام البرمجة اللغوية العصبية ، حتى بدون إشارات صريحة عبر اللغات. ومع ذلك ، فقد ركزت هذه التقييمات على النقل عبر اللغات مع اللغات عالية الموارد ، والتي تغطي فقط ثلث اللغات التي تغطيها mBERT. نستكشف كيفية أداء mBERT على مجموعة أكبر من اللغات ، مع التركيز على جودة التمثيل للغات منخفضة الموارد ، والتي يتم قياسها من خلال الأداء داخل اللغة. نحن نأخذ في الاعتبار ثلاث مهام: التعرف على الكيانات المسماة (99 لغة) ، وعلامات جزء من الكلام ، وتحليل التبعية (54 لغة لكل لغة). تعمل mBERT بشكل أفضل من أو يمكن مقارنتها بخطوط الأساس للغات عالية الموارد ولكنها أسوأ بكثير بالنسبة للغات منخفضة الموارد. علاوة على ذلك ، فإن نماذج BERT أحادية اللغة لهذه اللغات تعمل بشكل أسوأ. يمكن تضييق فجوة الأداء بين BERT أحادية اللغة و mBERT عند إقرانها مع لغات مماثلة. وجدنا أن النماذج الأفضل للغات منخفضة الموارد تتطلب تقنيات تدريب مسبق أكثر كفاءة أو المزيد من البيانات.</abstract_ar>
      <abstract_es>El BERT multilingüe (mBert) capacitado en 104 idiomas ha demostrado un rendimiento multilingüe sorprendentemente bueno en varias tareas de PNL, incluso sin señales explícitas en varios idiomas. Sin embargo, estas evaluaciones se han centrado en la transferencia multilingüe con idiomas de alto nivel de recursos, cubriendo solo un tercio de los idiomas cubiertos por mBert. Exploramos el rendimiento de MBert en un conjunto de idiomas mucho más amplio, centrándonos en la calidad de la representación de idiomas de bajos recursos, medida por el rendimiento dentro del idioma. Consideramos tres tareas: Reconocimiento de entidades con nombre (99 idiomas), etiquetado de parte del discurso y análisis de dependencias (54 idiomas cada uno). mBert funciona mejor o es comparable a las líneas base en idiomas de recursos altos, pero tiene un rendimiento mucho peor para los idiomas de recursos bajos. Además, los modelos BERT monolingües para estos idiomas empeoran aún más. Junto con lenguajes similares, se puede reducir la brecha de rendimiento entre BERT monolingües y mBERT. Descubrimos que los mejores modelos para lenguajes de bajos recursos requieren técnicas de preentrenamiento más eficientes o más datos.</abstract_es>
      <abstract_fr>Le BERT multilingue (mBERT) formé sur 104 langues a montré des performances interlinguistiques étonnamment bonnes sur plusieurs tâches de PNL, même sans signaux multilingues explicites. Cependant, ces évaluations se sont concentrées sur le transfert interlinguistique avec des langues à ressources élevées, ne couvrant qu'un tiers des langues couvertes par MBerT. Nous explorons comment mBerT fonctionne sur un ensemble de langues beaucoup plus large, en nous concentrant sur la qualité de la représentation pour les langues à faibles ressources, mesurée par les performances au sein de la langue. Nous considérons trois tâches : la reconnaissance des entités nommées (99 langues), le balisage des parties vocales et l'analyse des dépendances (54 langues chacune). mBerT fait mieux ou est comparable aux lignes de base sur les langues à ressources élevées, mais est bien pire pour les langues à ressources faibles. De plus, les modèles BERT monolingues pour ces langues font encore pire. Associé à des langages similaires, l'écart de performance entre BERT et mBERT monolingues peut être réduit. Nous avons constaté que de meilleurs modèles pour les langues à faibles ressources nécessitent des techniques de pré-apprentissage plus efficaces ou davantage de données.</abstract_fr>
      <abstract_pt>O BERT multilíngue (mBERT) treinado em 104 idiomas mostrou surpreendentemente bom desempenho multilíngue em várias tarefas de PNL, mesmo sem sinais explícitos em vários idiomas. No entanto, essas avaliações se concentraram na transferência multilíngue com idiomas de alto recurso, cobrindo apenas um terço dos idiomas cobertos pelo mBERT. Exploramos como o mBERT funciona em um conjunto muito mais amplo de linguagens, focando na qualidade da representação para linguagens de poucos recursos, medida pelo desempenho dentro da linguagem. Consideramos três tarefas: Reconhecimento de Entidade Nomeada (99 idiomas), Marcação de Parte da Fala e Análise de Dependência (54 idiomas cada). O mBERT é melhor ou comparável às linhas de base em linguagens de alto recurso, mas é muito pior para linguagens de baixo recurso. Além disso, os modelos BERT monolíngues para esses idiomas são ainda piores. Emparelhado com idiomas semelhantes, a diferença de desempenho entre o BERT monolíngue e o mBERT pode ser reduzida. Descobrimos que modelos melhores para linguagens de poucos recursos exigem técnicas de pré-treinamento mais eficientes ou mais dados.</abstract_pt>
      <abstract_ja>104言語で訓練された多言語BERT （ mBERT ）は、明示的なクロスリンガルシグナルがなくても、いくつかのNLPタスクで驚くほど良好なクロスリンガルパフォーマンスを示しています。 しかし、これらの評価は、mBERTでカバーされている言語のわずか3分の1をカバーする、高リソース言語とのクロスリンガル転送に焦点を当てています。 私たちは、mBERTが言語内のパフォーマンスによって測定される低資源言語の表現の質に焦点を当てて、はるかに幅広い言語セットでどのようにパフォーマンスを発揮するかを探ります。 名前付きエンティティ認識（ 99言語）、音声部分タグ付け、依存関係解析（各54言語）の3つのタスクを検討しています。mBERTは、高リソース言語のベースラインよりも優れていますが、低リソース言語の方がはるかに悪いです。 さらに、これらの言語のための単一言語のBERTモデルはさらに悪化する。 類似の言語と組み合わせることで、単一言語のBERTとmBERTの間のパフォーマンスギャップを縮めることができます。 リソースの少ない言語のためのより良いモデルには、より効率的な事前トレーニング技術またはより多くのデータが必要であることがわかります。</abstract_ja>
      <abstract_hi>104 भाषाओं पर प्रशिक्षित बहुभाषी BERT (mBERT) ने कई NLP कार्यों पर आश्चर्यजनक रूप से अच्छा क्रॉस-भाषी प्रदर्शन दिखाया है, यहां तक कि स्पष्ट क्रॉस-लिंगुअल संकेतों के बिना भी। हालांकि, इन मूल्यांकनों ने उच्च-संसाधन भाषाओं के साथ क्रॉस-लिंगुअल हस्तांतरण पर ध्यान केंद्रित किया है, जिसमें mBERT द्वारा कवर की गई भाषाओं का केवल एक तिहाई हिस्सा शामिल है। हम यह पता लगाते हैं कि MBERT भाषाओं के एक बहुत व्यापक सेट पर कैसे प्रदर्शन करता है, कम-संसाधन भाषाओं के लिए प्रतिनिधित्व की गुणवत्ता पर ध्यान केंद्रित करता है, जिसे भाषा के भीतर के प्रदर्शन द्वारा मापा जाता है। हम तीन कार्यों पर विचार करते हैं: नामित एंटिटी रिकग्निशन (99 भाषाएं), पार्ट-ऑफ-स्पीच टैगिंग और निर्भरता पार्सिंग (प्रत्येक 54 भाषाएं)। mBERT उच्च संसाधन भाषाओं पर बेसलाइन से बेहतर या तुलनीय है, लेकिन कम संसाधन भाषाओं के लिए बहुत खराब करता है। इसके अलावा, इन भाषाओं के लिए मोनोलिंगुअल BERT मॉडल और भी बदतर करते हैं। इसी तरह की भाषाओं के साथ युग्मित, मोनोलिंगुअल BERT और mBERT के बीच प्रदर्शन अंतर को कम किया जा सकता है। हम पाते हैं कि कम संसाधन भाषाओं के लिए बेहतर मॉडल को अधिक कुशल प्रीट्रेनिंग तकनीकों या अधिक डेटा की आवश्यकता होती है।</abstract_hi>
      <abstract_ru>Многоязычный BERT (mBERT), обученный на 104 языках, показал удивительно хорошую кросс-лингвистическую производительность по нескольким задачам NLP, даже без явных кросс-лингвистических сигналов. Однако эти оценки были сосредоточены на межязыковой передаче с высокоресурсными языками, охватывающими только треть языков, охваченных mBERT. Мы исследуем, как mBERT работает на гораздо более широком наборе языков, фокусируясь на качестве представления для малоресурсных языков, измеряемого производительностью внутри языка. Мы рассматриваем три задачи: Распознавание именованных сущностей (99 языков), Тегирование части речи и Синтаксический анализ зависимостей (54 языка каждый). mBERT работает лучше или сопоставимо с базовыми линиями на языках с высоким уровнем ресурсов, но делает гораздо хуже для языков с низким уровнем ресурсов. Кроме того, одноязычные модели BERT для этих языков работают еще хуже. В сочетании с аналогичными языками, разрыв в производительности между одноязычным BERT и mBERT может быть сокращен. Мы обнаружили, что лучшие модели для языков с низким уровнем ресурсов требуют более эффективных методов предварительного обучения или большего объема данных.</abstract_ru>
      <abstract_zh>104种语言多BERT(mBERT)数NLP见惊人语,虽无明跨。 然其重者,以高资言语移,仅涵盖mBERT所涵盖三之一。 论 mBERT 博言集上,重低资源言质,以言量之。 臣等思三务:曰名实识(99 曰语),曰词性曰凭(曰语 54 曰)。 mBERT 优与高资言语之基线比,而于低资源言语,mBERT 者差得多。 语言单语BERT至更糟。 与类语配对,可缩小单语BERTmBERT之间性相去也。 臣等见低资源言语之善者,更须预练术更多之数。</abstract_zh>
      <abstract_ukr>Багатомовний BERT (mBERT), навчений 104 мовами, показав напрочуд хорошу міжмовну продуктивність для декількох завдань NLP, навіть без явних міжмовних сигналів. Однак ці оцінки зосереджені на міжмовному передачі з високоресурсними мовами, охоплюючи лише третину мов, що охоплюються mBERT. Ми досліджуємо, як mBERT працює на набагато ширшому наборі мов, зосереджуючись на якості представлення для мов з низьким рівнем ресурсів, що вимірюється продуктивністю в межах мови. Ми розглядаємо три завдання: розпізнавання іменованих сутностей (99 мов), тегування частини мовлення та аналіз залежностей (54 мови кожна). mBERT працює краще або порівнянно з базовими лініями на мовах з високим рівнем ресурсів, але робить набагато гірше для мов з низьким рівнем ресурсів. Крім того, одномовні моделі BERT для цих мов роблять ще гірше. У поєднанні з аналогічними мовами, розрив у продуктивності між одномовним BERT та mBERT можна скоротити. Ми виявили, що кращі моделі для мов з низьким рівнем ресурсів вимагають більш ефективних методів попереднього навчання або більшої кількості даних.</abstract_ukr>
      <abstract_ga>Tá feidhmíocht thrastheangach thar a bheith maith léirithe ag BERT (mBERT) atá oilte ar 104 teanga ar roinnt tascanna NLP, fiú gan comharthaí tras-teangacha follasacha. Mar sin féin, dhírigh na meastóireachtaí seo ar aistriú tras-teangach le teangacha ard-acmhainne, nach gclúdaíonn ach an tríú cuid de na teangacha atá clúdaithe ag mBERT. Fiosraíonn muid conas a fheidhmíonn MBERT ar shraith teangacha i bhfad níos leithne, ag díriú ar cháilíocht ionadaíochta teangacha íseal-acmhainne, arna thomhas ag feidhmíocht laistigh den teanga. Breathnaímid ar thrí thasc: Aitheantas Aonán Ainmnithe (99 teanga), Clibeáil Pháirt cainte agus Parsáil Spleáchais (54 teanga an ceann). Déanann mBERT níos fearr nó inchomparáide le bunlínte ar theangacha ard-acmhainne ach déanann sé i bhfad níos measa do theangacha lagacmhainne. Ina theannta sin, déantar níos measa fós ar mhúnlaí aonteangacha BERT do na teangacha seo. Agus teangacha comhchosúla acu, is féidir an bhearna feidhmíochta idir BERT aonteangach agus mBERT a laghdú. Feictear dúinn go dteastaíonn teicnící réamhoiliúint níos éifeachtaí nó níos mó sonraí ó mhúnlaí níos fearr do theangacha lagacmhainne.</abstract_ga>
      <abstract_ka>მრავალენგური BERT (mBERT) 104 ენაზე განსწავლილი შემდეგ გამოიყენება განსხვავებული მრავალენგური კონფიგურაცია NLP რამდენიმე სამუშაო დამუშაობაში, მაგრამ განსხვავებული მრავალენ მაგრამ, ეს შესაბამისი განსაზღვრებები უფრო მრავალური ტრანსტრესტის ენათებით, რომელიც მხოლოდ მესამე ენათების განსაზღვრებული mBERT-ის განსაზღვრებულია. ჩვენ განსხვავებთ, როგორ mBERT მუშაობს ძალიან უფრო დიდი ენაზე, როგორც განსხვავებული რესპერსოსების საფუძებლობაზე, როგორც მუშაობა ენაში მუშაობა. ჩვენ ვფიქრობთ სამი დავალება: სახელი ინტერტის განაცნობა (99 ენები), სიტყვების ნაწილი სიტყვება და დამხოლობა განაცნობა (54 ენები ყოველ). mBERT უფრო უკეთესია, ვიდრე ან შემდგომარებელია, მაგრამ უფრო უკეთესია რესურსის ენებისთვის. მაგრამ, მონოლენგური BERT მოდელები ამ ენებისთვის უკეთესია. მსგავსი ენებით დაკავშირებული, მონოლენგური BERT და mBERT-ის შორის განსხვავება შეიძლება შეიძლება გადასრულება. ჩვენ ვიფიქრობთ, რომ უკეთესი მოდელები ცოტა რესურსის ენათებისთვის უფრო ეფექტიური ტექნოგიები ან უფრო მეტად მონაცემები უნ</abstract_ka>
      <abstract_isl>Flertatunguleg BERT (mBERT) sem þjálfaður er á 104 tungumál hefur sýnt á óvart góða krosstungulækni á nokkrum NLP verkunum, jafnvel án útskýrðra krosstungulækna merkia. Hins vegar hafa þessi mat einbeitt sér a ð yfirfærslu á milli tungumál með tungumál sem eru háþættir, sem einungis umkringja þriðjung tungumála sem mBERT dækir. Við skoðum hvernig mBERT virkar á miklu breiðari tölu tungumál, einbeitingar á gæði myndunar fyrir tungumál með lítið efni, mæld með innri tölu tungumál. Við íhugum þrjár verkefni: Nafnaður samfélags þekking (99 tungumál), hluti af ræðumerkingu og greining háðunar (54 tungumál hvert). mBERT gerir betur en eða sambærilegt við grunnlínur á tungumálum með háa auðlinda en gerir miklu verra fyrir tungumál með lága auðlinda. Auk þess eru eintungulegar BERT líkanir fyrir þessi tungumál enn verri. Með sambærilegum tungumálum er hægt að minnka virkni milli eintungu BERT og mBERT. Við finnum að betri líkanir fyrir litla upprunalega tungumál þurfa árangursríkari for þjálfunarmeðferðir eða fleiri gögn.</abstract_isl>
      <abstract_el>Το πολύγλωσσο BERT (mBERT) εκπαιδευμένο σε 104 γλώσσες έχει δείξει εκπληκτικά καλή διαγώνια απόδοση σε διάφορες εργασίες NLP, ακόμη και χωρίς ρητά διαγώνια σήματα. Ωστόσο, οι αξιολογήσεις αυτές επικεντρώθηκαν στη διασυνοριακή μεταφορά με γλώσσες υψηλού επιπέδου, καλύπτοντας μόνο το ένα τρίτο των γλωσσών που καλύπτονται από το mBERT. Ερευνούμε πώς λειτουργεί το mBERT σε ένα πολύ ευρύτερο σύνολο γλωσσών, εστιάζοντας στην ποιότητα της αναπαράστασης για γλώσσες χαμηλής περιεκτικότητας, μετρημένη με την απόδοση εντός γλωσσών. Εξετάζουμε τρία καθήκοντα: Αναγνώριση Οντότητας (99 γλώσσες), Σήμανση Μέρος του λόγου και Ανάλυση Εξαρτήσεων (54 γλώσσες ο καθένας). Το mBERT κάνει καλύτερα ή συγκρίσιμα με τις γραμμές βάσης σε γλώσσες υψηλού πόρου, αλλά κάνει πολύ χειρότερα για γλώσσες χαμηλού πόρου. Επιπλέον, τα μονογλωσσικά μοντέλα BERT για αυτές τις γλώσσες κάνουν ακόμη χειρότερα. Σε συνδυασμό με παρόμοιες γλώσσες, το χάσμα απόδοσης μεταξύ μονογλωσσικού BERT και mBERT μπορεί να μειωθεί. Διαπιστώνουμε ότι τα καλύτερα μοντέλα για γλώσσες χαμηλής περιεκτικότητας απαιτούν πιο αποτελεσματικές τεχνικές προετοιμασίας ή περισσότερα δεδομένα.</abstract_el>
      <abstract_hu>A 104 nyelven képzett többnyelvű BERT (mBERT) meglepően jó teljesítményt mutatott többnyelvű, többnyelvű feladatnál is, még a kifejezett, többnyelvű jelek nélkül is. Ezek az értékelések azonban a nyelvek közötti transzferre összpontosítottak, nagy erőforrásokkal rendelkező nyelveken, amelyek az mBERT által lefedett nyelvek mindössze harmadát fedik le. Azt vizsgáljuk, hogyan teljesít az mBERT egy sokkal szélesebb nyelvcsoportban, fókuszálva az alacsony erőforrású nyelvek reprezentációjának minőségére, amelyet a nyelven belüli teljesítmény mér. Három feladatot veszünk figyelembe: Nevezett entitás felismerés (99 nyelv), Beszédrész címkézés és Függőség értelmezés (54 nyelv). Az mBERT jobb, mint vagy hasonlítható a nagy erőforrású nyelvek alapjaihoz, de sokkal rosszabb az alacsony erőforrású nyelvek esetében. Továbbá az egynyelvű BERT modellek ezekre a nyelvekre még rosszabbak. Hasonló nyelvekkel párosítva csökkenthető az egynyelvű BERT és mBERT közötti teljesítménykülönbség. Úgy találjuk, hogy az alacsony erőforrású nyelvek jobb modelljei hatékonyabb előképzési technikákat vagy több adatot igényelnek.</abstract_hu>
      <abstract_it>Il BERT multilingue (mBERT) addestrato su 104 lingue ha mostrato sorprendentemente buone prestazioni cross-lingual su diverse attività NLP, anche senza segnali cross-lingual espliciti. Tuttavia, queste valutazioni si sono concentrate sul trasferimento multilingue con lingue ad alto contenuto di risorse, coprendo solo un terzo delle lingue coperte da mBERT. Esploriamo le prestazioni di mBERT su un insieme molto più ampio di lingue, concentrandoci sulla qualità della rappresentazione per i linguaggi a basso contenuto di risorse, misurata dalle prestazioni interne al linguaggio. Consideriamo tre compiti: Riconoscimento delle entità nominate (99 lingue), Etichettatura Part-of-speech e Analisi delle dipendenze (54 lingue ciascuna). mBERT funziona meglio o comparabile alle linee di base sui linguaggi ad alta risorsa, ma fa molto peggio per i linguaggi a bassa risorsa. Inoltre, i modelli BERT monolingue per queste lingue fanno ancora peggio. In combinazione con linguaggi simili, il divario di prestazioni tra BERT monolingue e mBERT può essere ridotto. Troviamo che modelli migliori per linguaggi a basso contenuto di risorse richiedono tecniche di pre-formazione più efficienti o più dati.</abstract_it>
      <abstract_mk>Мултијазичниот БЕРТ (mBERT) обучен на 104 јазици покажа изненадувачки добра прекујазична перформанса на неколку НЛП задачи, дури и без експлицитни прекујазични сигнали. Сепак, овие проценки се фокусираа на прекујазичкиот трансфер со јазици со високи ресурси, кој покрива само третина од јазиците покриени од mBERT. Истражуваме како mBERT функционира на многу поширок набор јазици, фокусирајќи се на квалитетот на претставувањето на јазиците со ниски ресурси, мерен од внатрешната функционирање на јазиците. Размислуваме за три задачи: препознавање на ентитети (99 јазици), одбележување на дел од говорот и анализирање на зависноста (54 јазици секој). mBERT е подобар од или споредлив со базите на јазиците со високи ресурси, но е многу полош за јазиците со ниски ресурси. Furthermore, monolingual BERT models for these languages do even worse.  Со слични јазици, разликата во изведувањето помеѓу монојазичниот BERT и mBERT може да се намали. Најдовме дека подобрите модели за јазиците со ниски ресурси бараат поефикасни техники за предобука или повеќе податоци.</abstract_mk>
      <abstract_kk>104 тілінде бірнеше тілді BERT (mBERT) бірнеше NLP тапсырмаларында бірнеше тілді белгілі сигналдар болмаса да, бірнеше тілде бірнеше тілді жұмыс істеу үшін бірнеше тілді жұмыс істеу жақсы көрсе Бірақ бұл оқиғалар көптеген тілдерді көптеген ресурс тілдері мен көптеген тілдерді ауыстыруға көмектеседі, мBERT тілдерінің тек үшіншісін ауыстырады. МЕБЕРТ тілдердің көп кеңіл тілдерінде қалай істейтінін зерттеп, көп ресурс тілдерінің төменгі тілдерінің сапасына назар аударып, тілдердің ішінде өлшемімен көрсетеді. Біз үш тапсырма деп ойлаймыз: Аталған нысандарды анықтау (99 тіл), сөйлеу тегтерінің бөлігі мен тәуелдік талдау (әрбір 54 тіл). mBERT ресурс тілдерінің негізгі жолдарынан не салыстыруға болады, бірақ ресурс тілдерінің төмен тілдеріне көп жақсы болады. Қосымша, бұл тілдер үшін бірнеше тілді BERT үлгілері жақсы болады. Бұндай тілдермен біріктірілген, бір тілді BERT және mBERT арасындағы әрекеттердің арасындағы аралығы қысқартылады. Біз төмен ресурс тілдерінің жақсы үлгілерін табу үшін көмектесетін техникалар немесе көмектесетін деректерді талап етеді.</abstract_kk>
      <abstract_lt>Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals.  Tačiau atliekant šiuos vertinimus daugiausia dėmesio buvo skiriama tarpkalbiniam perdavimui didelių išteklių kalbomis, apimančiam tik trečdalį kalbų, kurioms taikomas mBERT. Tiriame, kaip mBERT veikia daug platesnėje kalbų grupėje, daugiausia dėmesio skiriant mažai išteklių turinčių kalbų atstovavimo kokybei, vertinamai pagal kalbų rezultatus. Mes svarstome tris užduotis: pavadintas subjekto pripažinimas (99 kalbos), kalbos dalies ženklinimas ir priklausomybės analizavimas (kiekviena 54 kalbos). mBERT yra geresnis už arba palyginamas su bazinėmis aukšto išteklio kalbomis, tačiau gerokai blogesnis mažo išteklio kalbomis. Be to, vienakalbiai BERT modeliai šioms kalboms dar blogėja. Kartu su panašiomis kalbomis galima sumažinti vienokalbinio BERT ir mBERT veiklos skirtumą. Mes manome, kad geresniems mažų išteklių kalbų modeliams reikia veiksmingesnių išankstinio mokymo metodų arba daugiau duomenų.</abstract_lt>
      <abstract_ml>104 ഭാഷകളില്‍ പഠിപ്പിക്കപ്പെട്ട പല ഭാഷ ബെര്‍ട്ടി (mBERT) വിശിഷ്ടമായി ക്രിസ്റ്റല്‍ ഭാഷ പ്രവര്‍ത്തനം കാണിച്ചിരിക്കുന്നു. കുറച്ചു NLP എന്നാലും ഈ വിലാസങ്ങള്‍ ഉയര്‍ന്ന വിഭവങ്ങളുടെ ഭാഷകളില്‍ മൂന്നിലൊന്ന് മൂല്ലില്‍ മാറ്റിയിരിക്കുന്നു. നമ്മള്‍ പരിശോധിക്കുന്നത് എംബെര്‍ട്ടി എങ്ങനെയാണ് ഏറ്റവും വിശാലമായ ഭാഷകളില്‍ പ്രവര്‍ത്തിക്കുന്നതെന്ന്, കുറഞ്ഞ വിഭവഭാഷകള്‍ക്കുള നമ്മള്‍ മൂന്നു ജോലികളെ വിചാരിക്കുന്നു: പേരിട്ട എന്റിറ്റി തിരിച്ചറിയുന്നത് (99 ഭാഷകള്‍), സംസാരിക്കുന്നതിന്റെ ഭാഗ ഉയര്‍ന്ന വിഭവഭാഷകളിലുള്ള അടിസ്ഥാനങ്ങളില്‍ മെച്ചപ്പെടുത്തുന്നതിനെക്കാള്‍ മെച്ചപ്പെട്ടിരിക്കുന്നു Furthermore, monolingual BERT models for these languages do even worse.  ഇതുപോലുള്ള ഭാഷകള്‍ കൊണ്ട് പ്രദര്‍ശിപ്പിക്കപ്പെട്ടിരിക്കുന്നു, മോണോളില്‍ ഭാഷ ബെര്‍ട്ടിയും എംബെര്‍ട്ടിയ കുറഞ്ഞ വിഭവങ്ങളുടെ ഭാഷകള്‍ക്ക് നല്ല മോഡല്‍ ആവശ്യമുണ്ടെന്ന് ഞങ്ങള്‍ കണ്ടെത്തുന്നു. അതില്‍ കൂടുതല്‍ സാധാരണമായി</abstract_ml>
      <abstract_mn>104 хэл дээр сургалтын олон хэлний BERT (mBERT) нь олон NLP даалгаврууд дээр гайхалтай хэлний даалгаврыг харуулсан. Гэхдээ хэлний олон хэлний сигналыг тодорхойлж чаддаггүй ч гэсэн. Гэхдээ эдгээр оюутнууд нь мBERT-ээр дүүрэн хэлний 3 дахь хэлний шилжүүлэлт дээр төвлөрсөн. МБЕРТ хэрхэн илүү өргөн хэл дээр ажилладаг вэ гэдгийг судалж, хэл доторх үйл ажиллагаагаар хэлбэрээр хэмжээний бага боловсролын хэл дээр төвлөрүүлэх чадварыг төвлөрүүлж байна. Бид 3 даалгавар бодож байна: нэрлэгдсэн Entity Recognition (99 languages), ярианы нэг хэсэг, хамааралтай талаар (54 languages each). mBERT нь бага боловсролын хэл дээрх суурь шугамнуудаас илүү сайн эсвэл харьцуулагдах боловч бага боловсролын хэл дээр илүү муу байдаг. Мөн эдгээр хэл дээр ганц хэлний BERT загварууд илүү муу болдог. Яг ижил хэлнүүдтэй холбогдсон, BERT болон mBERT-ын хоорондын үйл ажиллагааны ялгаа багасгаж болно. Бид бага эдийн засгийн хэл дээр илүү сайн загварууд илүү үр дүнтэй арга загварын техник эсвэл олон мэдээллийн хэрэгтэй.</abstract_mn>
      <abstract_no>Fleirspråk BERT (mBERT) trent på 104 språk har vist overraska godt krysspråk utvikling på fleire NLP- oppgåver, sjølv utan eksplisitt krysspråk- signaler. Desse evalueringane har imidlertid fokusert på krysspråk overføring med høg ressursspråk, som dekkar berre ein tredjedel av språka dekka av mBERT. Vi utforskar korleis mBERT utfører på ein mykje stor sett språk, fokuserer på kvaliteten til representasjon for låg ressursspråk, målt av innspråk. Vi ser på tre oppgåver: Namnet Entity Recognition (99 languages), Part of speech Tagging and Dependency Parsing (54 languages each). mBERT gjer bedre enn eller sammenlignbar med baselinjer på høg ressursspråk, men gjer mykje verre for låg ressursspråk. I tillegg gjer monospråk BERT-modeller for desse språka enda verre. Påkopla med liknande språk, kan utgangspunktet mellom monospråk BERT og mBERT reduserast. Vi finn at bedre modeller for låge ressursspråk krev meir effektiv trekkingsteknikk eller fleire data.</abstract_no>
      <abstract_pl>Wielojęzyczny BERT (mBERT) przeszkolony w językach 104 wykazał zaskakująco dobrą wydajność w zakresie wielu zadań NLP, nawet bez wyraźnych sygnałów wielojęzycznych. Oceny te koncentrowały się jednak na transferze między językami z językami wysokimi zasobami, obejmującymi tylko jedną trzecią języków objętych mBERT. Badamy, jak mBERT działa w znacznie szerszym zestawie języków, koncentrując się na jakości reprezentacji języków o niskich zasobach, mierzonej wydajnością wewnątrz języków. Rozważamy trzy zadania: Rozpoznawanie nazwanych podmiotów (99 języki), Tagowanie części mowy i Parsing zależności (54 języki każdy). mBERT radzi sobie lepiej niż lub porównywalnie z liniami bazowymi na językach o wysokich zasobach, ale znacznie gorzej w przypadku języków o niskich zasobach. Ponadto jednojęzyczne modele BERT dla tych języków są jeszcze gorsze. W połączeniu z podobnymi językami różnica wydajności pomiędzy jednojęzycznym BERT a mBERT może zostać zmniejszona. Odkrywamy, że lepsze modele języków o niskich zasobach wymagają bardziej efektywnych technik wstępnego treningu lub więcej danych.</abstract_pl>
      <abstract_ro>BERT multilingv (mBERT) instruit pe 104 limbi a demonstrat performanțe translingvistice surprinzător de bune în mai multe sarcini PNL, chiar și fără semnale translingve explicite. Cu toate acestea, aceste evaluări s-au axat pe transferul translingvistic cu limbi cu resurse ridicate, acoperind doar o treime din limbile acoperite de mBERT. Explorăm modul în care mBERT performează pe un set mult mai larg de limbi, concentrându-se pe calitatea reprezentării pentru limbile cu resurse reduse, măsurată prin performanțele lingvistice. Considerăm trei sarcini: Recunoașterea entităților denumite (99 de limbi), Etichetarea parțială și analizarea dependenței (54 de limbi fiecare). mBERT funcționează mai bine sau comparabil cu liniile de referință pentru limbile cu resurse ridicate, dar este mult mai rău pentru limbile cu resurse reduse. În plus, modelele BERT monolingve pentru aceste limbi sunt și mai rele. În asociere cu limbi similare, diferența de performanță dintre BERT monolingv și mBERT poate fi redusă. Considerăm că modelele mai bune pentru limbile cu resurse reduse necesită tehnici de pregătire mai eficiente sau mai multe date.</abstract_ro>
      <abstract_sr>Većina jezika BERT (mBERT) obučena na 104 jezika pokazala je iznenađujuće dobru cross-language performance na nekoliko NLP zadataka, čak i bez objašnjih cross-lingual signala. Međutim, ove procjene su se fokusirale na preko jezika sa visokim jezicima resursa, pokrivajući samo trećinu jezika pokrivenih mBERT-om. Istražujemo kako mBERT izvodi na mnogo širom setu jezika, fokusirajući se na kvalitetu zastupanja jezika niskih resursa, mjerenu iznutra jezika. Razmišljamo o tri zadatka: priznanje imenovanih podataka (99 jezika), oznake dio govora i razmatranje ovisnosti (54 jezika svakog). mBERT radi bolje od ili usporedno sa osnovnim linijama na jezicima visokih resursa, ali mnogo gore za jezike niskih resursa. Osim toga, monojezički BERT modeli za ove jezike su još gore. Povezano sa sličnim jezicima, praznina izvedbe između monojezičkog BERT i mBERT može biti sužana. Nalazimo da bolji modeli za jezike niskih resursa zahtevaju učinkovitije tehnike pretkivanja ili više podataka.</abstract_sr>
      <abstract_si>ගොඩක් භාෂාවක් BERT (mBERT) 104 භාෂාවට ප්‍රශ්නය කරලා තියෙන්නේ පුද්ගලික විශ්වාසයෙන් හොඳ භාෂාවක් ප්‍රශ්නය කරනවා NLP ව නමුත්, මේ විශ්ලේෂණය අවධානය කරලා තියෙන්නේ විශේෂ භාෂාවක් වලින් විශේෂ භාෂාවක් වලින් විශේෂ ව අපි පරීක්ෂණය කරනවා mBERT කොහොමද භාෂාවක් විශාලයෙන් කරන්නේ කියලා, අඩුම භාෂාව භාෂාවක් විශාලයෙන් ප්‍රතික්‍රියා Name mBERT විශාල භාෂාවට ප්‍රමාණ භාෂාවට වඩා හොඳයි නැත්නම් ප්‍රමාණ භාෂාවට සම්බන්ධ වෙන්න පුළ ඒ වගේම, මේ භාෂාවට තවත් වඩා වඩා වැඩියි. සමාන භාෂාවක් සමග සම්බන්ධ වෙලා තියෙන්නේ, එක භාෂාවක් BERT සහ mBERT අතර ප්‍රභාව අතර අතර අතර අතර අත අපිට හොයාගන්න පුළුවන් හොඳ මොඩේල් භාෂාවට වඩා වැඩි ප්‍රතික්‍රීය විද්‍යාවක් නැති විද්‍යාවක්</abstract_si>
      <abstract_so>Aqoonta afka 104 luqadood lagu tababariyey BERT (mBERT) waxay si yaab leh ugu muuqatay shaqooyin badan oo af-luuqad ah oo aad u fiican, xitaa xittaa aan calaamado cad oo luuqadaha kala baxsan. Si kastaba ha ahaatee qiimeyntaasi waxay ku kalsoonaaday wareejinta luuqadaha aad ku kala duwan, waxayna ku qoran yihiin saddex meelood oo luqadood oo mBERT ku qoran. Waxaynu baaraynaa sida mBERT u sameeyo luuqado aad u ballaadhan oo aad u ballaadhan, oo aad ugu fiirsanaynaa qiimaha u dhigashada luuqadaha hoose-resource ee lagu qiyaasay muuqashada afka gudaha ah. Waxaan ka fiirsanaynaa saddex shaqo: Aqoonsashada ganacsiga (99 luqadood), qeyb ka mid ah baayacsiga hadalka iyo jardiinada ku xiran (54 luqadood kasta). mBERT wuxuu ka wanaagsan yahay ama u eg yahay aasaaska luuqadaha sare ee luqadaha badan, laakiin waxay aad uga xumaysaa luuqadaha hoose ee resource. Intaas waxaa dheer, noocyada afka nooca ah ee BERT ayaa ka sii xumaa. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed.  Waxaan heli nahay in tusaalooyin wanaagsan oo luuqadaha hoose ee noocyada badan ay u baahan yihiin qalabka hore oo ka faa’iidada ah ama macluumaad ka badan.</abstract_so>
      <abstract_ms>BERT berbilang bahasa (mBERT) dilatih pada 104 bahasa telah menunjukkan prestasi salib bahasa yang mengejutkan yang baik pada beberapa tugas NLP, walaupun tanpa isyarat salib bahasa yang jelas. Namun, penilaian ini telah fokus pada pemindahan saling bahasa dengan bahasa sumber tinggi, meliputi hanya satu pertiga bahasa yang ditutup oleh mBERT. Kami mengeksplorasi bagaimana mBERT berfungsi pada set bahasa yang lebih luas, fokus pada kualiti perwakilan bahasa sumber rendah, diukur oleh prestasi dalam bahasa. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each).  mBERT melakukan lebih baik daripada atau dibandingkan dengan garis dasar pada bahasa sumber tinggi tetapi lebih buruk untuk bahasa sumber rendah. Lagipun, model BERT monobahasa untuk bahasa-bahasa ini lebih teruk. Berpasang dengan bahasa yang sama, jarak prestasi antara BERT dan mBERT monobahasa boleh ketat. Kami mendapati bahawa model yang lebih baik untuk bahasa sumber rendah memerlukan teknik pretraining lebih efisien atau lebih data.</abstract_ms>
      <abstract_mt>BERT multilingwi (mBERT) imħarreġ f’104 lingwa wera prestazzjoni translingwi tajba b’mod sorprendenti fuq diversi kompiti NLP, anki mingħajr sinjali translingwi espliċiti. Madankollu, dawn l-evalwazzjonijiet iffukaw fuq trasferiment translingwi b’lingwi b’riżorsi għoljin, li jkopru biss terz tal-lingwi koperti mill-mBERT. Aħna nesploraw kif l-mBERT jaħdem fuq sett aktar wiesa’ ta’ lingwi, b’enfasi fuq il-kwalità tar-rappreżentanza għal lingwi b’riżorsi baxxi, imkejla permezz tal-prestazzjoni fil-lingwi. Aħna nqisu tliet kompiti: Rikonoxximent tal-Entità bl-Isem (99 lingwa), Tagging tal-Parti tad-Diskors u Parsing tad-Dipendenza (54 lingwa kull waħda). mBERT jagħmel aħjar minn jew komparabbli ma’ linji bażi fuq lingwi b’riżorsi għoljin iżda jagħmel ħafna agħar għal lingwi b’riżorsi baxxi. Barra minn hekk, il-mudelli monolingwi BERT għal dawn il-lingwi jagħmlu saħansitra agħar. Flimkien ma’ lingwi simili, id-differenza fil-prestazzjoni bejn il-BERT monolingwi u l-mBERT tista’ titnaqqas. We find that better models for low resource languages require more efficient pretraining techniques or more data.</abstract_mt>
      <abstract_sv>Flerspråkig BERT (mBERT) utbildad på 104 språk har visat förvånansvärt bra tvärspråkliga prestanda på flera NLP-uppgifter, även utan uttryckliga tvärspråkliga signaler. Dessa utvärderingar har dock inriktats på transspråksöverföring med språk med hög resurs och omfattar endast en tredjedel av de språk som omfattas av mBERT. Vi undersöker hur mBERT presterar på en mycket bredare uppsättning språk, med fokus på kvaliteten på representation för språk med låg resurs, mätt med prestanda inom språket. Vi överväger tre uppgifter: Namngivnad entitetsigenkänning (99 språk), Delmärkning och Beroendetolkning (54 språk vardera). mBERT gör bättre än eller jämförbar med baslinjer på språk med hög resurs, men gör mycket sämre för språk med låg resurs. Dessutom är enspråkiga BERT-modeller för dessa språk ännu värre. I kombination med liknande språk kan prestandaklyftan mellan enspråkiga BERT och mBERT minskas. Vi finner att bättre modeller för språk med låg resurs kräver effektivare förberedelsetekniker eller mer data.</abstract_sv>
      <abstract_ta>104 மொழிகளில் பயிற்சி செய்யப்பட்ட பல மொழிகள் BERT (mBERT) வெளிப்படையான குறிப்பு மொழிக்குறியீடு குறியீட்டு குறியீடு குறியீடுகள் இல்ல ஆனால் இந்த மதிப்புகள் உயர்மூலத்தின் மொழிகளால் மொழிமொழிகள் மீது கவனம் செலுத்தப்பட்டுள்ளது, mBERT மூலம் மறைக்கப்பட்ட மூன்றில்  MBERT மிகவும் விரிவான மொழிகளில் எப்படி செயல்படுகிறது என்பதை நாம் கண்டுபிடிக்கிறோம். குறைந்த மூலத்தின் குறைந்த மொழிகளின் குறை நாம் மூன்று பணிகளை கருத்துக் கொள்கிறோம்: பெயரிடப்பட்ட பொருள் அடையாளம் (99 மொழிகள்), பேச்சின் பாகம் ஒட்டுதல் மற்றும் சார் மூலத்தின் மொழிகளில் அடிப்படைகளை விட மேலானதாகவோ அல்லது ஒப்பிடுவதாகவோ எம்பெர்ட் செய்கிறது ஆனால் குறைந்த மூலத்தின மேலும், இந்த மொழிகளுக்கு மாதிரி பிரெட் மாதிரிகள் கூட மோசமாக செய்கிறது. ஒத்த மொழிகளால் கொடுக்கப்பட்டுள்ளது, மோனோலிமொழி BERT மற்றும் mBERT இடையில் செயல்பாட்டின் வேறுபாடு குறைந்த குறைந்த மூலத்தின் மொழிகளுக்கு நல்ல மாதிரிகளை நாம் கண்டுபிடிக்க வேண்டும் என்று நாம் கண்டுபிடிக்கிறோ</abstract_ta>
      <abstract_ur>104 زبانوں پر تعلیم کی بہت سی زبان BERT (mBERT) نے بہت سی NLP کاموں پر بہترین کرسی زبان کی فعالیت دکھائی ہے، اگرچہ صریح کرسی زبان سیگنالوں کے بغیر۔ However, these evaluations have focused on cross-language transfer with high-resource languages, covering only a third of the languages covered by mBERT. ہم دیکھتے ہیں کہ mBERT کس طرح زیادہ گھیری زبانوں پر عمل کرتا ہے، کم منبع زبانوں کے معاملہ کی کیفیت پر تمرکز کرتا ہے، جو زبانوں کے اندر مطابق اندازہ کئے جاتے ہیں. ہم تین کام سمجھتے ہیں: نام رکھی ہوئی ایٹیٹی شناسی (۹۹ زبان), کلام کا ایک حصہ ٹاگ اور اعتمادی پارچینگ (54 زبان) ۔ mBERT اس سے بہتر یا برابر ہے جو بالا سروسیس زبانوں میں بنیس لین کے ساتھ ہے لیکن کم سروسیس زبانوں کے لئے بہت برا ہے. اور ان زبانوں کے لئے ایک زبان کے BERT موڈل بھی بدتر ہیں۔ برابر زبانوں کے ساتھ جوڑے ہوئے، ایک زبان BERT اور mBERT کے درمیان فعالیت فاصلہ تنگ کر سکتا ہے. ہم دیکھتے ہیں کہ نیچے منبع زبانوں کے لئے بہترین نمونڈل بہترین استعمال کی ضرورت ہے اور زیادہ دکھانے کے لئے۔</abstract_ur>
      <abstract_uz>104 tillarda o'rganilgan bir necha tillar BERT (mBERT) ta'minlovchi bo'lgan bir necha NLP vazifalarda juda yaxshi tilni ko'rsatadi. Ko'pchilik tillar imkoniyatlarida ko'proq tillar yoʻq. Lekin, bu qiymatlar eng yuqori manbalar tillari bilan bir necha tillar tarjima qiladi, va faqat mBERT tomonidan qo'llangan tillarning uchinchi qiymatiga qaraydi. Biz mBERT qanday ko'p kengaytirilgan tillar orqali bajarayotganimizni aniqlamiz, juda katta manbalar tillarining taʼminligini o'rganamiz, o'z tilning ichida o'zgartirish imkoniyatini o'rganamiz. Biz uchta vazifalarni ko'rsamiz: nomli tizimni tanlash (99 tillar), gapirish qismlarining qismlari va ishlatish qismlari (har bir 54 tillar). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages.  Ko'rsatganda, bu tillar uchun monolik BERT modellari ham yomon. Bu xil tillar bilan paytda, monolik tilda BERT va mBERT orasidagi gap qisqari mumkin. Biz o'ylaymiz, qisqa manbaning tillari uchun yaxshi modellari qo'shimcha teknologiya yoki ko'proq maʼlumot kerak.</abstract_uz>
      <abstract_vi>Nền giao hảo đa ngôn ngữ Berlin (mBERT) được đào tạo trên những ngôn ngữ 104 đã cho thấy khả năng ngôn ngữ khác nhau trong nhiều nhiệm vụ bục ngôn ngữ, mặc dù không có tín hiệu chữ. Tuy nhiên, phần đánh giá này tập trung vào việc chuyển giao ngôn ngữ ngữ khác nhau với ngôn ngữ cao, chỉ bao gồm một phần ba ngôn ngữ được trồng bởi mBERT. Chúng tôi tìm hiểu cách mBERT thực hiện thành công trên một loạt ngôn ngữ rộng hơn nhiều, tập trung vào chất lượng của sự phân phối ngôn ngữ thấp, đo được đo bằng khả năng ngôn ngữ nội bộ. Chúng tôi xem xét ba nhiệm vụ: Tênd Entity recognition (9s languages), Part of phát biểu TagChó biển và Độ thuộc phân tích (54 languages each). mBERT làm tốt hơn hay tương tự với bản nền của ngôn ngữ cao nguyên nhưng lại làm xấu hơn nhiều cho ngôn ngữ ít tài nguyên. Thêm vào đó, mô hình gìn đỗ nhỏ cho các ngôn ngữ này còn tệ hơn. Kết hợp với ngôn ngữ tương tự, khoảng trống giữa hỗn xược ngôn ngữ và mBERT có thể bị thu hẹp. Chúng tôi thấy rằng những mô hình tốt hơn cho ngôn ngữ ít nguồn cần một kỹ thuật tiên lượng hiệu quả hơn hoặc nhiều dữ liệu.</abstract_vi>
      <abstract_bg>Многоезичният BERT (mBERT), обучен на 104 езика, показа изненадващо добро междуезично представяне на няколко задачи от НЛП, дори без изрични междуезични сигнали. Тези оценки обаче са насочени към междуезичен трансфер с езици с висок ресурс, обхващащи само една трета от езиците, обхванати от mBERT. Проучваме как действа на много по-широк набор от езици, като се фокусираме върху качеството на представянето на езици с нисък ресурс, измерено чрез вътрешноезиково представяне. Ние разглеждаме три задачи: разпознаване на имена на субекти (99 езика), маркиране на част от речта и анализ на зависимостта (54 езика всеки). mBERT е по-добър или сравним с базовите линии на езиците с висок ресурс, но е много по-лош за езиците с нисък ресурс. Освен това моноезичните модели BERT за тези езици са още по-лоши. В съчетание с подобни езици разликата в ефективността между едноезичните BERT и mBERT може да бъде стеснена. Намираме, че по-добрите модели за езици с нисък ресурс изискват по-ефективни техники за предварително обучение или повече данни.</abstract_bg>
      <abstract_nl>Meertalig BERT (mBERT) getraind op 104 talen heeft verrassend goede cross-lingual prestaties getoond bij verschillende NLP taken, zelfs zonder expliciete cross-lingual signalen. Deze evaluaties hebben zich echter toegespitst op de meertalige overdracht met talen met veel middelen, die slechts een derde van de talen bestrijken die onder mBERT vallen. We onderzoeken hoe mBERT presteert op een veel bredere reeks talen, waarbij we ons richten op de kwaliteit van representatie voor talen met weinig resources, gemeten aan de hand van prestaties binnen de taal. We beschouwen drie taken: Named Entity Recognition (99 talen), Part-of-speech Tagging en Dependency Parsing (54 talen elk). mBERT doet het beter dan of vergelijkbaar met baselines op high resource talen, maar doet het veel slechter voor low resource talen. Bovendien doen eentalige BERT-modellen voor deze talen het nog slechter. In combinatie met vergelijkbare talen kan de prestatiekloof tussen eentalige BERT en mBERT worden verkleind. We vinden dat betere modellen voor talen met weinig resources efficiëntere pretraining technieken of meer data vereisen.</abstract_nl>
      <abstract_hr>Većina jezika BERT (mBERT) obučena na 104 jezika pokazala je iznenađujuće dobru međujezičku učinku na nekoliko NLP zadataka, čak i bez objašnjih međujezičkih signala. Međutim, te procjene su usredotočene na prekršeno jezički transfer sa visokim jezicima resursa, pokrivajući samo trećinu jezika pokrivenih mBERT-om. Istražujemo kako mBERT izvodi na mnogo širom setu jezika, fokusirajući se na kvalitetu zastupanja jezika niskih resursa, mjerenu iznutra jezika. Smatramo tri zadatke: priznanje imenovanih područja (99 jezika), označavanje dijela govora i razmatranje ovisnosti (54 jezika svaki). mBERT radi bolje od ili usporedno s početnim linijama na jezicima visokih resursa, ali mnogo gore za jezike niskih resursa. Osim toga, monojezički BERT modeli za te jezike čine još gore. Povezano s sličnim jezicima, praznina učinka između monojezičkog BERT i mBERT može se smanjiti. Nalazimo da bolji modeli za jezike niskih resursa zahtijevaju učinkovitije tehnike pretkivanja ili više podataka.</abstract_hr>
      <abstract_da>Flersproget BERT (mBERT) uddannet på 104 sprog har vist overraskende god tværsproget ydeevne på flere NLP-opgaver, selv uden eksplicitte tværsprogede signaler. Disse evalueringer har imidlertid fokuseret på tværsproget overførsel med sprog med høj ressource, som kun dækker en tredjedel af de sprog, der er omfattet af mBERT. Vi undersøger, hvordan mBERT præsterer på et langt bredere sæt sprog, med fokus på kvaliteten af repræsentation for sprog med lav ressource, målt ved intern sprogpræstation. Vi overvejer tre opgaver: Navngivet Entity Recognition (99 sprog), Part-of-Tale Tagging og Afhængighed Parsing (54 sprog hver). mBERT gør det bedre end eller sammenligneligt med basislinjer på sprog med høj ressource, men gør det meget værre for sprog med lav ressource. Endvidere er ensprogede BERT-modeller for disse sprog endnu værre. Sammen med lignende sprog kan ydelsesforskellen mellem ensproget BERT og mBERT indsnævres. Vi finder ud af, at bedre modeller for sprog med lave ressourcer kræver mere effektive foruddannelsesteknikker eller flere data.</abstract_da>
      <abstract_de>Mehrsprachiges BERT (mBERT), das auf 104-Sprachen trainiert wurde, hat bei mehreren NLP-Aufgaben überraschend gute crosslinguale Leistungen gezeigt, auch ohne explizite crosslinguale Signale. Diese Evaluationen konzentrierten sich jedoch auf den sprachübergreifenden Transfer mit ressourcenreichen Sprachen, wobei nur ein Drittel der von mBERT abgedeckten Sprachen abgedeckt wurde. Wir untersuchen, wie mBERT in einer viel breiteren Reihe von Sprachen funktioniert, wobei wir uns auf die Qualität der Repräsentation für ressourcenarme Sprachen konzentrieren, gemessen an der Leistung innerhalb der Sprache. Wir betrachten drei Aufgaben: Named Entity Recognition (99 Sprachen), Part-of-Speech Tagging und Dependency Parsing (54 Sprachen je). mBERT ist besser als oder vergleichbar mit Baselines auf High Resource Languages, aber viel schlechter für Low Resource Languages. Darüber hinaus sind einsprachige BERT-Modelle für diese Sprachen noch schlechter. In Kombination mit ähnlichen Sprachen kann die Leistungslücke zwischen einsprachigem BERT und mBERT verringert werden. Wir stellen fest, dass bessere Modelle für ressourcenarme Sprachen effizientere Vortrainingstechniken oder mehr Daten erfordern.</abstract_de>
      <abstract_id>Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals.  Namun, evaluasi ini telah fokus pada transfer saling bahasa dengan bahasa sumber daya tinggi, meliputi hanya satu pertiga bahasa yang ditutup oleh mBERT. Kami mengeksplorasi bagaimana mBERT berfungsi pada set bahasa yang jauh lebih luas, fokus pada kualitas representation untuk bahasa sumber daya rendah, diukur oleh prestasi dalam bahasa. Kami mempertimbangkan tiga tugas: Pengenalan Entitas bernama (99 bahasa), Tagging Bahagian-dari-pidato dan Parsing Dependensi (54 bahasa masing-masing). mBERT melakukan lebih baik dari atau dibandingkan dengan garis dasar pada bahasa sumber daya tinggi tetapi lebih buruk untuk bahasa sumber daya rendah. Selain itu, model BERT monobahasa untuk bahasa-bahasa ini bahkan lebih buruk. Berpasang dengan bahasa yang sama, ruang prestasi antara BERT monobahasa dan mBERT dapat diperingatkan. Kami menemukan bahwa model yang lebih baik untuk bahasa sumber daya rendah membutuhkan teknik pretraining lebih efisien atau lebih banyak data.</abstract_id>
      <abstract_ko>104개 언어에서 훈련된 다중 언어 BERT(mBERT)는 명확한 다중 언어 신호가 없어도 몇 개의 NLP 임무에서 놀라운 좋은 크로스 언어 성능을 나타냈다.그러나 이러한 평가는 mBERT가 포함하는 언어의 3분의 1만 포함하는 고자원 언어의 크로스 언어 이전에 중심을 두었다.우리는 mBERT가 어떻게 더욱 광범위한 언어집에서 집행되는지 연구하고 저자원 언어의 표현 품질에 중점을 두고 언어 내 성능을 통해 평가할 것이다.우리는 명명 실체 식별(99개 언어), 어성 표시와 의존성 분석(각 54개 언어) 세 가지 임무를 고려했다.고자원 언어에서는 mBERT의 성능이 베이스라인보다 우수하거나 베이스라인과 비슷하지만 저자원 언어에서는 mBERT의 성능이 훨씬 떨어진다.그 밖에 이 언어들의 단어 버트 모델은 더 나쁘게 표현되었다.비슷한 언어와 함께 사용하면 단어 BERT와 mBERT 간의 성능 격차를 줄일 수 있다.우리는 저자원 언어의 더 좋은 모델은 더욱 효과적인 예훈련 기술이나 더 많은 데이터를 필요로 하는 것을 발견했다.</abstract_ko>
      <abstract_sw>Lugha nyingi za BERT (mBERT) zilizofundishwa kwa lugha 104 imeonyesha utendaji mzuri wa lugha katika kazi kadhaa za NLP, hata bila ishara za wazi za lugha. Hata hivyo, utafiti huu umejikita kwenye usafirishaji wa lugha mbalimbali kwa lugha zilizo na rasilimali za juu, ukiandika tu theluthi ya lugha zilizoandikwa na mBERT. Tunagundua jinsi mBERT anavyofanya katika aina kubwa ya lugha, tukizingatia ubora wa uwakilishi kwa lugha ndogo ya rasilimali, zilizosawiwa na utendaji wa lugha za ndani ya lugha. Tunafahamu kazi tatu: Utambulisho wa Ujumbe (lugha 99), Sehemu ya Ujumbe wa kujieleza na Uchaguzi wa Kutegemea (lugha 54 kila mmoja). mBERT anafanya vizuri zaidi au linalinganisha na msingi wa lugha za rasilimali za juu lakini inafanya vibaya zaidi kwa lugha ndogo ya rasilimali. Zaidi ya hayo, mifano ya BERT kwa lugha hizi yanafanya vibaya zaidi. Ikiwa na lugha kama hizo, tofauti ya utendaji kati ya lugha ya kimonolinguli na mBERT inaweza kupunguzwa. Tunaona kuwa mifano bora kwa lugha za rasilimali chini zinahitaji mbinu zenye ufanisi zaidi za kutengeneza matumizi au taarifa zaidi.</abstract_sw>
      <abstract_tr>104 dilinde bilim taýýarlanan BERT (mBERT) birnäçe NLP täbliklerinde ajaýyp şekilde çykyş dillerde, hatda cross-lingual signallerden hem gowy täblikler görkezildi. Ýöne bu çykyşlar, mBERT tarapyndan örän diller ýokary çeşme dilleri bilen uluslary terjime etýärler. Biz mBERT dillerin nähili döwletlerde edip bilýändigini gözləýäris, dilleriň içinde ölçülen iň az resurslar dili üçin üýtgetmegiň kalitesine üns berýäris. Biz üç zady pikir edýäris: Adlanýan Entity Recognition (99 languages), sözleriniň bir bölümi etilamak we çykyş Parsing (54 languages each). mBERT ýokary ressurs dillerinde baz çyzlaryň üstine ýa-da görşikliklerinden has gowy edýär ýöne ýokary ressurs dilleri üçin has gowy däldir. Daşary hem bu diller üçin monodilli BERT nusgalary has hem erbet bolýar. Aynı diller bilen bölünip, monodil BERT we mBERT arasyndaky täsirler azalyp biler. Aýak ressurs dilleri üçin gowy nusgalaryň täsirli ýa-da köp maglumatlary gerek bolýar.</abstract_tr>
      <abstract_af>Veelvuldige BERT (mBERT) onderwerp op 104 tale het verwonderbaar goeie kruistale prestasie op verskeie NLP opdragte vertoon, selfs sonder eksplisiese kruistale signale. Hierdie evaluasies het tog gefokus op kruistale oordrag met hoë-hulpbron tale, wat slegs 'n derde van die tale wat deur mBERT bedek is. Ons ondersoek hoe mBERT uitvoer op 'n baie breideer stel tale, fokus op die kwaliteit van voorstelling vir lae hulpbron tale, gemeet deur binne-taal uitvoering. Ons beskou drie opdragte: Genaamde Eenheidwerkening (99 tale), Deel van spraak merking en afhanklikheid verwerking (54 tale elke). mBERT doen beter as of vergelykbaar met baselyne op hoë hulpbron tale maar doen baie verder vir lae hulpbron tale. Verder, monolinguele BERT-modele vir hierdie tale doen nog verder. Gepaar met gelyke tale, die prestasie gap tussen monolinglike BERT en mBERT kan vernietig word. Ons vind dat beter modele vir lae hulpbron tale meer effektief voortrekende teknike of meer data nodig.</abstract_af>
      <abstract_fa>BERT Multilingual (mBERT) که در 104 زبان آموزش یافته شده، عملکرد زیادی زبان را در چند کارهای NLP نشان داده است، حتی بدون سیگنال های زیادی روشن زبان. با این حال، این ارزیابها روی انتقال کلی زبان با زبانهای منابع بالا تمرکز شده‌اند، که فقط یک سوم از زبان‌هایی که توسط mBERT پوشش شده‌اند را پوشش می‌دهند. ما تحقیق می‌کنیم چگونه mBERT روی مجموعه‌ی زبان‌های بسیار گسترده‌تر انجام می‌دهد، تمرکز می‌کنیم روی کیفیت نمایش برای زبان‌های کم منابع، که توسط عملکرد زبان‌ها اندازه می‌شود. ما سه تا وظیفه را در نظر می گیریم: شناسایی عنوان یک واحد (۹۹ زبان), قسمتی از نقاشی سخنرانی و تحلیل بستگی (۴۴ زبان هر یک). mBERT بهتر از یا قابل مقایسه با خطوط بنیادی در زبانهای منابع بالا انجام می دهد اما برای زبانهای کم منابع خیلی بدتر است. علاوه بر این، مدل‌های یک زبان BERT برای این زبانها بیشتر بدتر می‌شود. با زبانهای مشابه، فاصله عملکرد بین یک زبان BERT و mBERT می تواند تنگ شود. ما پیدا می‌کنیم که مدل‌های بهتر برای زبان‌های منابع کم نیاز به تکنیک‌های تغییر یا اطلاعات بیشتری دارند.</abstract_fa>
      <abstract_am>በ104 ቋንቋዎች የተጠቃሚ ብERT (mBERT) የልዩ ቋንቋ ድረ-ቋንቋ ስርዓት በብዙ NLP ስራ ላይ የበለጠ ጥሩ ነው፤ ምንም ግልፅ የቋንቋ ቋንቋ ሲልክ ባይኖር ነው፡፡ ነገር ግን እነዚህ ምርጫዎች የቋንቋ ቋንቋ ቋንቋዎች በተለየ ላይ ተዘጋጅተዋል፤ mBERT የተከፈቱትን የቋንቋዎች ሲሶውን ብቻ ይሸፍናሉ። በቋንቋ ቋንቋዎች ላይ MBERT እንዴት እንደሚፈጸም እናደርጋለን፡፡ ሦስት ስራዎችን እናስባለን: ስም የስብሰባ ማውቀት (99 ቋንቋዎች), የንግግር ማተሚያ እና የድጋፍ ማዘጋጀት (በየ54 ቋንቋዎች) mBERT ከፍተኛ resource ቋንቋዎች ላይ በመሠረት መሠረት ይሻላል ወይም ይተካከላል ነገር ግን ለጥቂት resource ቋንቋዎች በጣም ክፋት ነው፡፡ ከዚህም በላይ ለዚህ ቋንቋዎች የሞላዊ BERT ሞዴል በጣም ክፋት ነው፡፡ በተለያዩ ቋንቋዎች ላይ የተመሳሳይ፣ በሞሎ ቋንቋ እና mBERT መካከል የፍጥረት ውጤት መቆጣጠር ይችላል፡፡ We find that better models for low resource languages require more efficient pretraining techniques or more data.</abstract_am>
      <abstract_sq>BERT shumëgjuhës (mBERT) i trajnuar në 104 gjuhë ka treguar shfaqje të mirë ndërgjuhësore në disa detyra NLP, edhe pa sinjale të qarta ndërgjuhësore. Megjithatë, këto vlerësime janë përqëndruar në transferimin ndërgjuhësor me gjuhë me burime të larta, duke mbuluar vetëm një të tretën e gjuhëve të mbuluara nga mBERT. Ne eksplorojmë se si mBERT funksionon në një sërë gjuhësh shumë më të gjerë, duke u përqëndruar në cilësinë e përfaqësimit për gjuhët me burime të ulta, të matura nga përfaqësimi brenda gjuhës. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each).  mBERT bën më mirë se apo të krahasueshme me linjat bazë në gjuhët e larta të burimeve por bën shumë më keq për gjuhët e ulëta të burimeve. Përveç kësaj, modelet monogjuhësore BERT për këto gjuhë janë edhe më keq. Të palëzuar me gjuhë të ngjashme, dallimi i performancës midis BERT monogjuhës dhe mBERT mund të ngushtohet. Ne zbulojmë se modelet më të mira për gjuhët e ulëta të burimeve kërkojnë teknika më të efektshme paratrajnimi apo më shumë të dhëna.</abstract_sq>
      <abstract_hy>Բազլեզու BERT (mBERT), որը ուսուցանվում է 104լեզուներով, զարմանալիորեն լավ փոխլեզվի արտադրողություն է ցույց տվել մի քանի ՆԼՊ-ի առաջադրանքների վրա, նույնիսկ առանց բացահայտ փոխլեզվի ազդանշանների: Այնուամենայնիվ, այս գնահատումները կենտրոնացել են բարձր ռեսուրսների լեզուներով լեզվի փոխանցման վրա, որը ներառում է միայն mBER-ի կողմից ծածկված լեզուներից մեկը: Մենք ուսումնասիրում ենք, թե ինչպես է mBER-ը աշխատում շատ ավելի լայն լեզուների վրա, կենտրոնացնելով ցածր ռեսուրսների լեզուների ներկայացման որակի վրա, չափված լեզուների ներկայացման արդյունքի վրա: Մենք հաշվի առնում ենք երեք հանձնարարություն' կոչված անհատականության ճանաչելը (99 լեզու), խոսքի մասը և կախվածության վերլուծությունը (54 լեզու յուրաքանչյուրը): mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages.  Ավելին, այս լեզուների համար BER-ի միալեզու մոդելները նույնիսկ ավելի վատ են: Նմանատիպ լեզուներով զուգավորված, կարելի է նվազեցնել BER-ի և mBER-ի միալեզուների արտահայտության տարբերությունը: Մենք հայտնաբերում ենք, որ ցածր ռեսուրսների լեզուների ավելի լավ մոդելները պահանջում են ավելի արդյունավետ նախադասության մեթոդներ կամ ավելի շատ տվյալներ:</abstract_hy>
      <abstract_az>104 dildə təhsil edilən çoxlu dil BERT (mBERT) çoxlu NLP işlərində təhsil edilən çoxlu dillərdə çoxlu yaxşı işlər göstərdi, hətta çoxlu dil sinyalləri olmadan. Ancaq bu değerlendirmələr, mBERT ilə örtülmüş dillərin üçüncüsünü örtüb çox dilli transfer dillərinə odaqlandı. Biz mBERT dillərin çox genişliyi ilə necə işlədiyini keşfetirik, düşük ressurs dillərinin göstərilməsinin keyfiyyətini, dillərin içində ölçülməsi ilə ölçülür. Biz üç işi düşünürük: Adlı Entity Recognition (99 dil), Sözlük Merking və bağlılıq Parsing (54 dil). mBERT yüksək ressurs dillərində baz xətirlərə qarşılaşdırılabilir, amma düşük ressurs dilləri üçün çox daha pisdir. Bu dillər üçün monodilli BERT modelləri daha pis edirlər. Aynı dillərlə birlikdə, monodil BERT və mBERT arasındakı performans boşluğu azaldırıla bilər. Düşük ressurs dillərinin daha yaxşı modelləri daha müvəffəqiyyətli pretraining tehnikləri və daha çox məlumatları lazımdır.</abstract_az>
      <abstract_bn>১০৪ ভাষায় প্রশিক্ষিত বহুভাষী বেরেট (এমবেরেট) বেশ কয়েকটি এনএলপি কাজের উপর বিস্ময়কর ভাষায় প্রদর্শন করেছে, এমনকি ব্যাপারে ক্রস-ভাষার সিগন্ তবে এই মূল্যবোধগুলো উচ্চ সম্পদের ভাষা দিয়ে বিভিন্ন ভাষার প্রতি মনোযোগ প্রদান করেছে, যা মাত্র তৃতীয় ভাষা মাত্র mBERT দ্বারা ঢ আমরা খুঁজে বের করি কিভাবে এমবিরেটি ভাষায় বেশী বিস্তারিত ভাষায় প্রদর্শন করে, যার প্রতিনিধিত্ব নির্ধারণের মান নিয়ে মনোযোগ দিয়েছে  আমরা তিনটি কাজ বিবেচনা করি: নাম এন্টিটি স্বীকার (৯৯ ভাষা), ভাষণের অংশ ট্যাগিং এবং নির্ভর পার্সিং (প্রত্যেক ৫৪ ভাষা)। এমবেরেটি উচ্চ সম্পদ ভাষার ভাষায় বেশি ভাল বা তুলনায় কাজ করে কিন্তু কম সম্পদ ভাষার জন্য অনেক খারাপ। এছাড়াও, এই ভাষাগুলোর জন্য মোনোলিভাল ভাষার মডেল আরো খারাপ। একই ভাষায় পুরস্কার প্রদান করা হয়েছে, মোনোলিভাষী ভাষার মধ্যে প্রদর্শনীর পার্থক্য কমে যাবে। আমরা খুঁজে পাচ্ছি যে কম সম্পদ ভাষার জন্য ভালো মডেলের কারণে বৃষ্টির প্রযুক্তি বা আরও কার্যকর তথ্য প্রয়োজন।</abstract_bn>
      <abstract_ca>El BERT multilingüe (mBERT) entrenat en 104 llengües ha demostrat sorprenentment bons resultats translingües en diverses tasques del NLP, fins i tot sense senyals translingües explícits. Tot i així, aquestes evaluacions s'han centrat en la transfer ència translingüística amb llengües d'alt recurso, cobrant només un terç de les llengües cobertes per mBERT. Explorem com mBERT funciona en un conjunt molt més ampli de llengües, centrant-nos en la qualitat de la representació de llengües amb baix recursos, mesurada pel rendiment interior del llenguatge. Considerem tres tasques: Recognició d'Entitats Nomada (99 llengües), Etiquetat de Part-of-speech i Analysis of Dependence (54 llengües cada una). mBERT fa millor que o comparable a les línies de base en llengües de recursos elevats, però fa molt pitjor per a llengües de recursos baixos. A més, els models monolingües BERT per aquestes llengües són encara pitjors. Aparat amb llengües similars, es pot reduir la diferència de performance entre el BERT monolingüe i el mBERT. Trobem que millors models de llengües de baix recursos requereixen tècniques de pré-formació més eficients o més dades.</abstract_ca>
      <abstract_et>104 keeles koolitatud mitmekeelne BERT (mBERT) on näidanud üllatavalt head keeleülest suutlikkust mitmetel uue õppekava ülesannetel, isegi ilma selgete keeleüleste signaalideta. Kõnealused hindamised on siiski keskendunud keeltevahelisele ülekandele suure ressursiga keeltega, mis hõlmavad vaid kolmandikku mBERTiga hõlmatud keeltest. Uurime, kuidas mBERT toimib palju laiemates keeltes, keskendudes vähese ressursiga keelte esindamise kvaliteedile, mida mõõdetakse keelesisese jõudluse põhjal. Me kaalume kolme ülesannet: Nimetatud olemi tuvastamine (99 keelt), kõneosa märgistamine ja sõltuvuse parsimine (54 keelt igaüks). mBERT on kõrge ressursiga keelte baasjoontest parem või võrreldav, kuid vähese ressursiga keelte puhul on see palju halvem. Lisaks on ühekeelsed BERT-mudelid nende keelte jaoks veelgi halvemad. Koos sarnaste keeltega on võimalik vähendada ühekeelse BERT ja mBERT jõudluse lõhet. Leiame, et vähese ressursiga keelte paremad mudelid nõuavad tõhusamaid eelõpetamismeetodeid või rohkem andmeid.</abstract_et>
      <abstract_cs>Vícejazyčný BERT (mBERT) trénovaný na 104 jazycích prokázal překvapivě dobrý výkon mezi jazyky při několika NLP úlohách, a to i bez explicitních vícejazyčných signálů. Tato hodnocení se však zaměřila na přenos mezi jazyky s vysokými zdroji a zahrnovala pouze třetinu jazyků, na které se vztahuje mBERT. Zkoumáme, jak mBERT funguje na mnohem širší sadě jazyků, se zaměřením na kvalitu reprezentace jazyků s nízkými zdroji, měřenou výkonností uvnitř jazyka. Zvažujeme tři úkoly: Rozpoznávání jmenovaných entit (99 jazyky), Tagování části řeči a Parsing závislosti (54 jazyky každý). mBERT je lepší než nebo srovnatelnější s základními liniemi na jazycích s vysokými zdroji, ale mnohem horší pro jazyky s nízkými zdroji. Jednojazyčné modely BERT pro tyto jazyky jsou navíc ještě horší. Ve spojení s podobnými jazyky lze výkonnostní rozdíl mezi jednojzyčným BERT a mBERT zúžit. Zjistili jsme, že lepší modely pro jazyky s nízkými zdroji vyžadují efektivnější techniky předškolení nebo více dat.</abstract_cs>
      <abstract_bs>Većina jezika BERT (mBERT) obučena na 104 jezika pokazala je iznenađujuće dobro cross-lingual performance na nekoliko NLP zadataka, čak i bez objašnjih cross-lingual signala. Međutim, ove procjene su se fokusirale na cross-lingual transfer sa visokim jezicima resursa, pokrivajući samo trećinu jezika pokrivenih mBERT-om. Istražujemo kako mBERT izvodi na mnogo širom setu jezika, fokusirajući se na kvalitetu zastupanja jezika niskih resursa, mjerenu iznutra jezika. Razmišljamo o tri zadatka: priznanje imenovanih podataka (99 jezika), označavanje dijela govora i razmatranje ovisnosti (54 jezika svakog). mBERT radi bolje od ili usporedno sa baznim linijama na jezicima visokih resursa, ali mnogo gore za jezike niskih resursa. Osim toga, monojezički BERT modeli za ove jezike su još gore. Povezano sa sličnim jezicima, praznina izvedbe između monojezičkog BERT i mBERT može biti sužana. Nalazimo da bolji modeli za jezike niskih resursa zahtijevaju učinkovitije tehnike pretkivanja ili više podataka.</abstract_bs>
      <abstract_fi>104 kielellä koulutettu monikielinen BERT (mBERT) on osoittanut yllättävän hyvää monikielistä suorituskykyä useissa NLP-tehtävissä, jopa ilman selkeitä monikielisiä signaaleja. Arvioinnissa on kuitenkin keskitytty monikieliseen siirtoon, jossa käytetään runsaasti resursseja sisältäviä kieliä, ja se kattaa vain kolmanneksen mBERT:n kattamista kielistä. Tutkimme, miten mBERT toimii paljon laajemmilla kielillä, keskittyen matalaresurssisten kielten edustuksen laatuun, mitattuna kielen sisäisellä suorituskyvyllä. Käsittelemme kolmea tehtävää: Nimettyjen entiteettien tunnistus (99 kieltä), puheen osamerkinnät ja riippuvuuden parsing (54 kieltä kukin). mBERT toimii paremmin tai vertailukelpoisemmin kuin korkean resurssin kielillä, mutta huonommin matalan resurssin kielillä. Lisäksi yksikieliset BERT-mallit näille kielille ovat vielä huonompia. Samankaltaisten kielten kanssa yhdistettynä monikielisen BERT:n ja mBERT:n suorituskykyeroa voidaan kaventaa. Havaitsemme, että paremmat mallit vähäresurssisille kielille vaativat tehokkaampia esikoulutustekniikoita tai enemmän dataa.</abstract_fi>
      <abstract_jv>Multilanguang BERT (mBERT) sing ditambah kanggo kowé barêng-barêng lan nganggo hasil, akeh basa sampeyan akeh bantuan pakan-jungang kapan kanggo ngerasakno NLP seneng nggawe gerangkat saben. Nanging, kuwi nggunakake kuwi wis dipun nggawe barang langkung banter, lan luwih banter-pakan langkung, sampek tanggal telu kanggo langkung weruh nggawe mBERT. Awak dhéwé ngerasakno piye mBERT nggawe akeh luwih akeh luwih dumadhi, ingkang nggawe kalite operasi kanggo langgambar akeh kaya bantuan, meh ngejer-seneng langgambar obah-langgambar. Name mBERT luwih luwih luwih karo ditambahak karo perusahaan banget kanggo langgar banter, pero akeh luwih akeh dumadhi kanggo langgar apa-apa. Nanging, model singular BERT kuwi nggawe luwih dumadhi. Sadurungé karo langkung sampeyan, akeh iso nggawe ning BERT lan mBERT iso nggawe gerarané. Awak dhéwé luwih-luwih akeh model sing luwih kanggo langgampun kudu supoyo nggawe teknik sing luwih apik dhéwé, ngono data sithik.</abstract_jv>
      <abstract_he>ברט רבות שפות (mBERT) מאומן על 104 שפות הופיע ביצועים צלולים שפותיים טובים באופן מפתיע על מספר משימות NLP, אפילו בלי אותות צלולות שפותיות ברורות. עם זאת, הערכות אלה התמקדו על העברה בין שפות עם שפות משאבים גבוהים, מכסה רק שליש מהשפות שכוסות על ידי mBERT. אנו חוקרים איך mBERT פועל על קבוצה הרבה יותר רחבה של שפות, מתמקדים באיכות המייצג לשפות עם משאבים נמוכים, למדוד על ידי ביצוע בתוך שפות. אנחנו שוקלים שלושה משימות: זיהוי איכות בשם (99 שפות), תווית חלק מהנאום ומחקר תלויות (54 שפות לכל אחד). mBERT עושה טוב יותר או שווה לשורות הבסיס בשפות משאבים גבוהות אבל עושה הרבה יותר גרוע לשפות משאבים נמוכות. בנוסף, דוגמני BERT monolingual לשפות אלה עושים אפילו גרועים יותר. עם שפות דומות, אפשר לצמצם את הפער בין BERT מונושפתי ומBERT. אנו מוצאים שדוגמנים טובים יותר לשפות משאבים נמוכות דורשים טכניקות לימוד מוקדם יעילות יותר או יותר נתונים.</abstract_he>
      <abstract_sk>Večjezični BERT (mBERT), usposobljen za 104 jezikov, je pokazal presenetljivo dobro medjezično uspešnost pri več nalogah NLP, tudi brez izrecnih medjezičnih signalov. Vendar so se te ocene osredotočile na medjezični prenos z jeziki z visokimi viri, ki zajemajo le tretjino jezikov, ki jih zajema mBERT. Raziskujemo, kako učinkovitost mBERT deluje v mnogo širšem naboru jezikov, pri čemer se osredotočamo na kakovost reprezentacije jezikov z nizkimi viri, merjeno z zmogljivostjo znotraj jezika. Obravnavamo tri naloge: prepoznavanje imenovanih subjektov (99 jezikov), označevanje dela govora in razčlenitev odvisnosti (54 jezikov). mBERT je boljši ali primerljiv z osnovnimi vrsticami na jezikih z visokimi viri, vendar je veliko slabši pri jezikih z nizkimi viri. Poleg tega so enojezični modeli BERT za te jezike še slabši. V kombinaciji s podobnimi jeziki se lahko zmanjša vrzel v zmogljivosti med enojezičnim BERT in mBERT. Ugotovili smo, da boljši modeli za jezike z nizkimi viri zahtevajo učinkovitejše predusposabljanje ali več podatkov.</abstract_sk>
      <abstract_ha>@ item license (short name) Haƙĩƙa, waɗannan qiymati sun yi zura kan transfer cikin lugha masu tsawo da harshen-nau'i, sunã rufe rubuci kawai na rubuci na harshen da mBERT ke rufe su. Tuna gane yadda mBERT ke aikata a kan wasu harshe masu yawa, sunã muhalli wa tsarin shaidar wa lugha masu ƙasan-resource, an ƙaddara shi da fassaran-harshe. Tuna ƙaddara aikin uku: Ana sunan Entity Recognition (99 languages), Babban-of-magana Taging and DeDeDedicated Parse (54 languages each). QXml Furan haka, misalin BERT-da'ura da ke cikin harshen waɗannan yana da mafi sharri. An yi da shirin da ke daidaita harshen, za'a ƙara tsakanin mai nuna tsakanin BERT da mBERT. Tuna gane cewa misalin mafiya alhẽri wa lugha masu ƙaranci na ƙayyade mafiya masu amfani ga ruwan ayuka ko da wasu data.</abstract_ha>
      <abstract_fil>Maraming linggo na BERT (mBERT) na tinuturuan sa 104 wika ay nagpakita ng kagilagilalas na gawa ng cross-lingua sa ibang gawain ng NLP, kahit walang explicit cross-lingual signals. Gayon ma'y ang mga evaluasyon na ito ay nagbigay-focus sa cross-lingual transfer na may mataas na mga wika ng mga resource, na tinatakpan lamang ang ikatlo sa mga wika na tinatakpan ng mBERT. Inisiyasat namin kung paano ang ginagawa ng mBERT sa isang malaki na talagang wika, na nagtitingnan sa kaligayahan ng representation para sa mababang mga wika ng mga resource, na sinukat sa loob ng wika. Iniisip natin ang tatlong gawa: Ang pangalang Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages bawa't isa). Mas mabuti ang mBERT kay sa mga baseline sa mataas na wika ng mga resource pero mas masama ang ibang wika ng mga resource. Bukod dito, ang monolingual BERT models para sa mga wikang ito ay mas masama pa. Pinagkailangan ng mga wikang paraan, ang pagkakailangan ng gawain sa pagitan ng monolingual BERT at mBERT ay maaaring mahihirap. Nasusumpungan natin na mas mabuting modelo para sa mababang mga wika ng ressourse ay nangangailangan ng mas efikasyente na teknisa ng pretraining o mas mabuting data.</abstract_fil>
      <abstract_bo>Multilingual BERT (mBERT) trained on 104 languages have shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. ཡིན་ནའང་། དབྱེ་ཞིབ་འདི་དག་ནི་སྐད་ཡིག་ཆ་མཐོ་རིམ་ཅན་གྱི་སྐད་རིགས་མཐོ་བསྒྱུར་ན་ལ་བློ་གཏོང་བ་ཡིན། ང་ཚོས་རང་ཉིད་ཀྱི་སྐད་རིགས་ཀྱི་ཆེ་ཆུང་བའི་སྐད་ཡིག་ལ་ཅི་ཞིབ་བྱེད་པ ང་ཚོས་བྱ་རིམ་འདི་གསུམ་ཀ་བསམ་བློ་གཏད། མིང་བཏགས་ཡོད་པའི་Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each). mBERT ཡིས་རྒྱུ་དངོས་ཐོན་ཁུངས་མཐོ་བའི་སྐད་རིགས་ལ་གཞི་རྟེན་འདི་དང་མཐུན་རྟགས་ནུས་པ་ཡིན་ནའང་དང་ཐོན་ཁུངས་ཀ ད་དུང་། སྐད་རིགས་འདི་དག་གི་ཆ་གཅིག་སྐད་ཀྱི་BERT མིག་དཔེ་དབྱིབས་ཀྱི་རྣམ་པ་དེ་ལས་ཉེན་ཁ་བརྗོད་པ་ར སྐད་རིགས་གཅིག་མཚུངས་པའི་སྐད་ཡིག་དང་མཉམ་དུ་བསྡད་ཡོད། BERT དང་ mBERT སྦྲེལ་བའི་བར་སྟོང་བར་ཐག ང་ཚོས་རྒྱ་ནག་ཏུ་མིག་དཔེ་གཟུགས་རིས་ཆ་ཉུང་བའི་སྐད་ཡིག་ཆ་ལ་ཕན་ཚུན་བྱེད་ཀྱི་ཐབས་ལམ་དང་། བྱས་ཙང་ས</abstract_bo>
      </paper>
    <paper id="22">
      <title>Evaluating Compositionality of Sentence Representation Models</title>
      <author><first>Hanoz</first><last>Bhathena</last></author>
      <author><first>Angelica</first><last>Willis</last></author>
      <author><first>Nathan</first><last>Dass</last></author>
      <pages>185–193</pages>
      <abstract>We evaluate the compositionality of general-purpose sentence encoders by proposing two different <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a> to quantify compositional understanding capability of sentence encoders. We introduce a novel <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>, Polarity Sensitivity Scoring (PSS), which utilizes sentiment perturbations as a proxy for measuring <a href="https://en.wikipedia.org/wiki/Compositionality">compositionality</a>. We then compare results from PSS with those obtained via our proposed extension of a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> called Tree Reconstruction Error (TRE) (CITATION) where compositionality is evaluated by measuring how well a true representation producing model can be approximated by a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that explicitly combines representations of its primitives.</abstract>
      <url hash="4d42e5cc">2020.repl4nlp-1.22</url>
      <attachment type="Software" hash="4a706720">2020.repl4nlp-1.22.Software.zip</attachment>
      <doi>10.18653/v1/2020.repl4nlp-1.22</doi>
      <video href="http://slideslive.com/38929788" />
      <bibkey>bhathena-etal-2020-evaluating</bibkey>
    <title_ar>تقييم تكوين نماذج تمثيل الجملة</title_ar>
      <title_fr>Évaluation de la compositionalité des modèles de représentation des phrases</title_fr>
      <title_es>Evaluación de la composición de los modelos de representación de oraciones</title_es>
      <title_pt>Avaliando a composicionalidade de modelos de representação de sentenças</title_pt>
      <title_ja>文章表現モデルの構成性の評価</title_ja>
      <title_zh>评句为形</title_zh>
      <title_ru>Оценка композиционности моделей представления предложений</title_ru>
      <title_hi>वाक्य प्रतिनिधित्व मॉडल की Compositionality का मूल्यांकन</title_hi>
      <title_ukr>Оцінка композиційності моделей речення</title_ukr>
      <title_ga>Ag Meastóireacht Comhshuíomh Múnlaí Léiriúcháin Pianbhreithe</title_ga>
      <title_ka>სიტყვის გამოსახულების მოდელების კომპოზიციონიალურობის განსაზღვრება</title_ka>
      <title_isl>Mat á samsetningu dæmismynda</title_isl>
      <title_el>Αξιολόγηση της συνθηματικότητας των μοντέλων αναπαράστασης προτάσεων</title_el>
      <title_hu>Az ítéletképviseleti modellek összetételének értékelése</title_hu>
      <title_it>Valutazione della composizione dei modelli di rappresentazione delle frasi</title_it>
      <title_kk>Сөзді келтіру үлгілерінің құрылғылығын оқу</title_kk>
      <title_lt>Sprendimų atstovavimo modelių sudėtingumo vertinimas</title_lt>
      <title_mk>Оценување на композиционалноста на моделите на претставување на речениците</title_mk>
      <title_mn>Хэрэглэгчийн төлөвлөгөө загварын нэгтгэлийг үнэлэх</title_mn>
      <title_ms>Mengevaluasi Komposisi Model Perwakilan Hukuman</title_ms>
      <title_mt>Evalwazzjoni tal-Kompożizzjoni tal-Mudelli tar-Rappreżentanza tas-Sentenzi</title_mt>
      <title_no>Evaluerer komposisjonalitet av uttrykk- representasjonsmodular</title_no>
      <title_pl>Ocena kompozycjonalności modeli reprezentacji zdań</title_pl>
      <title_ro>Evaluarea compoziționalității modelelor de reprezentare a sentințelor</title_ro>
      <title_sr>Procjenjivanje kompozicionalnosti modela predstavljanja kazne</title_sr>
      <title_si>වාර්තාව ප්‍රතිස්ථාපනය මදුල්ලයේ සංවිධානය අවශ්‍ය කරන්න</title_si>
      <title_so>Qiimeynta u dhigashada qaabilaadda xuquuqda</title_so>
      <title_sv>Utvärdering av meningsrepresentationsmodellernas sammansättning</title_sv>
      <title_ml>ശിക്ഷയുടെ പ്രതിനിധിയുടെ മോഡലുകളുടെ കോമ്പോണ്‍ടിഷനിറ്റിയിട്ടുണ്ടു്</title_ml>
      <title_ta>வாக்கியத்தின் முறைமைகளின் ஒதுக்கீட்டு செயல்பாடுகளை மதிப்பிடுகிறது</title_ta>
      <title_ur>Sentence Representation Models</title_ur>
      <title_uz>Comment</title_uz>
      <title_vi>Đánh giá thành phần của phiên bản</title_vi>
      <title_bg>Оценка на съставността на моделите за представяне на присъдата</title_bg>
      <title_nl>Evaluatie van de compositionaliteit van zinsrepresentatiemodellen</title_nl>
      <title_da>Evaluering af sammensætningen af sætningsrepræsentationsmodeller</title_da>
      <title_hr>Procjenjivanje kompozicionalnosti modela predstavljanja kazne</title_hr>
      <title_de>Bewertung der KompositionalitĂ¤t von SatzreprĂ¤sentationsmodellen</title_de>
      <title_ko>문장 표징 모델의 조합성 평가</title_ko>
      <title_fa>ارزیابی مجموعه نمونه‌های عبارت</title_fa>
      <title_sw>Kupima Ushirikiano wa Maandamano ya Mahakilishi</title_sw>
      <title_tr>Sözler Görkezilişi nusgala</title_tr>
      <title_af>Assebliseer Komposisionaliteit van Sentence Representation Models</title_af>
      <title_sq>Vlerësimi i përbërësisë së modeleve të përfaqësimit të dënimeve</title_sq>
      <title_am>ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s</title_am>
      <title_az>S칬zl칲 캻zl톛nm톛 Modell톛rinin 캻zl톛nm톛si</title_az>
      <title_bn>শাস্তি প্রতিনিধিত্ব মোডেলের কম্পোজিটিশনেলিটি মুছে ফেলা হচ্ছে</title_bn>
      <title_hy>Արժեցնել դատողության ներկայացման մոդելների համադրությունը</title_hy>
      <title_bs>Procjenjivanje kompozicionalnosti modela predstavljanja kazne</title_bs>
      <title_ca>Evaluation Compositionality of Sentence Representation Models</title_ca>
      <title_id>Evaluating Compositionality of Sentence Representation Models</title_id>
      <title_cs>Hodnocení kompoziciality modelů reprezentace vět</title_cs>
      <title_et>Kohtuotsuse esitamise mudelite koosseisu hindamine</title_et>
      <title_fi>Tuomioiden esittämismallien koostumuksen arviointi</title_fi>
      <title_jv>Ngubah Ukuhasar Sampeyan Sentense</title_jv>
      <title_he>הערכה המרכיבות של דוגמני מייצג גזרים</title_he>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_sk>Ocenjevanje sestavljenosti modelov zastopanja kazni</title_sk>
      <title_bo>གསལ་བརྗོད་ཀྱི་འཆར་སྟོན་པའི་མ་དབྱིབས་ཚུལ་རིམ་དཔྱད་བྱེད་བཞིན་པ</title_bo>
      <title_fil>Evaluating Compositionality of Sentence Representation Models</title_fil>
      <abstract_fr>Nous évaluons la compositionalité des codeurs de phrases à usage général en proposant deux mesures différentes pour quantifier la capacité de compréhension de la composition des codeurs de phrases. Nous introduisons une nouvelle métrique, Polarity Sensitivity Scoring (PSS), qui utilise les perturbations de sentiment comme indicateur pour mesurer la compositionalité. Nous comparons ensuite les résultats de PSS avec ceux obtenus via notre proposition d'extension d'une métrique appelée erreur de reconstruction d'arbre (TRE) (CITATION) où la compositionalité est évaluée en mesurant la capacité d'approximation d'un modèle produisant une représentation vraie par un modèle qui combine explicitement des représentations de ses primitives.</abstract_fr>
      <abstract_es>Evaluamos la composicionalidad de los codificadores de oraciones de uso general proponiendo dos métricas diferentes para cuantificar la capacidad de comprensión composicional de los codificadores de oraciones. Presentamos una métrica novedosa, Polarity Sensitivity Scoring (PSS), que utiliza las perturbaciones de sentimiento como un indicador para medir la composicionalidad. Luego comparamos los resultados de PSS con los obtenidos a través de nuestra propuesta de extensión de una métrica llamada Tree Reconstruction Error (TRE) (CITATION), donde la composicionalidad se evalúa midiendo qué tan bien se puede aproximar un modelo que produce representación real mediante un modelo que combina explícitamente representaciones de sus primitivos.</abstract_es>
      <abstract_ar>نقوم بتقييم تكوين مشفرات الجملة للأغراض العامة من خلال اقتراح مقياسين مختلفين لتحديد قدرة الفهم التركيبي لمشفرات الجملة. نقدم مقياسًا جديدًا ، وهو مقياس حساسية القطبية (PSS) ، والذي يستخدم اضطرابات المشاعر كبديل لقياس التركيب. نقوم بعد ذلك بمقارنة النتائج من PSS مع تلك التي تم الحصول عليها من خلال الامتداد المقترح لمقياس يسمى Tree Reconstruction Error (TRE) (CITATION) حيث يتم تقييم التركيب عن طريق قياس مدى جودة نموذج إنتاج التمثيل الحقيقي الذي يمكن تقريبه بواسطة نموذج يجمع بشكل صريح تمثيلات لـ بدائيه.</abstract_ar>
      <abstract_pt>Avaliamos a composicionalidade de codificadores de frases de uso geral propondo duas métricas diferentes para quantificar a capacidade de compreensão composicional de codificadores de frases. Introduzimos uma nova métrica, Polarity Sensitivity Scoring (PSS), que utiliza perturbações de sentimento como um proxy para medir a composicionalidade. Em seguida, comparamos os resultados do PSS com aqueles obtidos por meio de nossa proposta de extensão de uma métrica chamada Tree Reconstruction Error (TRE) (CITATION), onde a composicionalidade é avaliada medindo o quão bem um modelo de produção de representação verdadeira pode ser aproximado por um modelo que combina explicitamente representações de seus primitivos.</abstract_pt>
      <abstract_hi>हम वाक्य एन्कोडर की रचनात्मक समझ क्षमता को मापने के लिए दो अलग-अलग मीट्रिक का प्रस्ताव करके सामान्य-उद्देश्य वाक्य एनकोडर की रचनात्मकता का मूल्यांकन करते हैं। हम एक उपन्यास मीट्रिक, Polarity Sensitivity Scoreing (PSS) पेश करते हैं, जो compositionality को मापने के लिए एक प्रॉक्सी के रूप में भावनाओं का उपयोग करता है। फिर हम पीएसएस से परिणामों की तुलना ट्री पुनर्निर्माण त्रुटि (टीआरई) (प्रशस्ति पत्र) नामक मीट्रिक के हमारे प्रस्तावित विस्तार के माध्यम से प्राप्त किए गए लोगों के साथ करते हैं, जहां रचनात्मकता का मूल्यांकन यह मापकर किया जाता है कि एक सच्चे प्रतिनिधित्व उत्पादक मॉडल को एक मॉडल द्वारा कितनी अच्छी तरह से अनुमानित किया जा सकता है जो स्पष्ट रूप से इसके आदिमों के प्रतिनिधित्व को जोड़ता है।</abstract_hi>
      <abstract_ja>文エンコーダの構成的理解能力を定量化するための2つの異なる指標を提案することにより、汎用文エンコーダの構成性を評価する。我々は、組成性を測定するための代用としてセンチメントの摂動を利用する新規のメトリック、極性感度スコアリング（ PSS ）を導入する。次いで、ＰＳＳからの結果を、木再構築エラー（ ＴＲＥ ） （引用）と呼ばれるメトリックの提案された拡張を介して得られた結果と比較する。ここでは、真の表現生成モデルが、そのプリミティブの表現を明示的に組み合わせたモデルによってどの程度近似できるかを測定することによって、組成性が評価される。</abstract_ja>
      <abstract_zh>以二指标量化编码器之构图理解能力,以质编码器之构图性。 引入新指标,极性灵敏度评分(PSS)以情扰动为度分摄。 然后PSS与因吾议者为树重建差(TRE)(CITATION)之指标广而较之,其量实以成形者,显式合其基元。</abstract_zh>
      <abstract_ru>Мы оцениваем композиционность кодеров предложений общего назначения, предлагая две различные метрики для количественной оценки композиционной способности кодеров предложений к пониманию. Мы вводим романную метрику, Оценка Чувствительности Полярности (PSS), которая использует возмущения настроений как прокси для измерения композиционности. Затем мы сравниваем результаты PSS с результатами, полученными с помощью предложенного нами расширения метрики под названием Tree Reconstruction Error (TRE) (CITATION), где композиционность оценивается путем измерения того, насколько хорошо истинная модель получения представления может быть аппроксимирована моделью, которая явно объединяет представления ее примитивов.</abstract_ru>
      <abstract_ukr>Ми оцінюємо композиційність кодувальників речень загального призначення, пропонуючи дві різні метрики для кількісної оцінки композиційної здатності кодувальників речень до розуміння. Ми вводимо нову метрику, оцінку полярності чутливості (PSS), яка використовує збурення настроїв як проксі для вимірювання композиційності. Потім ми порівнюємо результати PSS з результатами, отриманими за допомогою нашого запропонованого розширення показника під назвою Помилка реконструкції дерева (TRE) (ЦИТУВАННЯ), де композиційність оцінюється шляхом вимірювання того, наскільки добре істинна модель представлення може бути апроксимована моделлю, яка явно поєднує представлення її примітивів.</abstract_ukr>
      <abstract_ga>Déanaimid measúnú ar chomhdhéanamh ionchódóirí pianbhreithe ginearálta trí dhá mhéadracht dhifriúla a mholadh chun cumas tuisceana comhdhéanaimh na n-ionchódóirí pianbhreithe a chainníochtú. Tugaimid isteach méadrach nua, Scóráil Íogaireachta Polarachta (PSS), a úsáideann suaiteanna meon mar sheachvótálaí chun comhdhéanamh a thomhas. Déanaimid comparáid ansin idir torthaí PSS agus iad siúd a fhaightear tríd an síneadh atá beartaithe againn ar mhéadrach ar a dtugtar Earráid Athfhoirgníochta Crann (TRE) (LUA) áit a ndéantar comhdhéanamh a mheas trí thomhas cé chomh maith agus is féidir samhail a tháirgeann léiriúchán fíor a chomhfhogasú trí mhúnla a chomhcheanglaíonn go sainráite léiriúcháin de a primitives.</abstract_ga>
      <abstract_ka>ჩვენ განსაზღვრებით სხვა მეტრიკის კომპოციციონალურობას, რომელიც კომპოციონციონციონციონციონციონციონციონციონციონციონციონციონციონციონციონ ჩვენ პრომენტიკური მეტრიკური, პოლარიტიკური სინცემულობის მოწყობილობა (PSS), რომელიც სენტიმენტის პერტუბურაციების გამოყენება როგორც პროქსი კომპოციოციალობ შემდეგ ჩვენ PSS-ის შემდეგ შემდეგ შემდეგ შემდეგ შემდეგ გადავადგინოთ მატრიკის გარეშექმნა შეცდომა (TRE) (CITATION) სადაც კომპოციოციალურობა განსაზღვრებულია, რომლებიც მართლა რესპეზიტაციის მოდელის გარეშექმნა მოდელის შესაძლებლობა</abstract_ka>
      <abstract_hu>Az általános célú mondatkódolók összetételét értékeljük két különböző mutatót javasolunk a mondatkódolók kompozíciós megértési képességének számszerűsítésére. Bemutatunk egy új metrikát, a Polarity Sensitivity Scoring (PSS), amely az érzelmi zavarokat használja a kompozíciós méréshez. Ezután összehasonlítjuk a PSS eredményeit a Tree Reconstruction Error (TRE) (CITATION) nevű metrika javasolt kiterjesztésével, ahol a kompozíciót úgy értékeljük, hogy egy valódi reprezentációt előállító modell milyen jól közelíthető meg egy olyan modell, amely kifejezetten kombinálja a primitívok reprezentációit.</abstract_hu>
      <abstract_isl>Við meta samsetningu almennra setninga kóðara með því að leggja fram tvær mismunandi mælingar til að mæla samsetningalega skilningaleika setninga kóðara. Við kynnum nýtt mælismæli, pólarætisnæmismæli (PSS), sem notar tilfinningatruflanir sem fulltrúa til a ð mæla samsetningu. Við borum síðan saman niðurstöður úr PSS og niðurstöður sem fengin eru með því a ð framkvæma mælikvarð sem kallast Tree Reconstruction Error (TRE) (CITATION) þar sem samsetningaráætlun er metin með því að mæla hversu vel hægt er að nálgast raunverulegum myndunarmönnum með líkani sem sameinar nákvæmlega myndun frumkominna.</abstract_isl>
      <abstract_el>Αξιολογούμε τη σύνθεση των κωδικοποιητών προτάσεων γενικής χρήσης προτείνοντας δύο διαφορετικές μετρήσεις για να ποσοτικοποιήσουμε την ικανότητα κατανόησης της σύνθεσης των κωδικοποιητών προτάσεων. Παρουσιάζουμε μια νέα μετρική, τη βαθμολογία πολικής ευαισθησίας (PSS), η οποία χρησιμοποιεί διαταραχές συναισθημάτων ως μεσολάβηση για τη μέτρηση της σύνθεσης. Στη συνέχεια, συγκρίνουμε τα αποτελέσματα από το PSS με εκείνα που λαμβάνονται μέσω της προτεινόμενης επέκτασης μιας μετρικής που ονομάζεται Σφάλμα Αναδόμησης Δέντρου (όπου αξιολογείται η σύνθεση μετρώντας πόσο καλά μπορεί να προσεγγιστεί ένα πραγματικό μοντέλο παραγωγής αναπαράστασης από ένα μοντέλο που συνδυάζει ρητά αναπαραστάσεις των πρωταρχικών του.</abstract_el>
      <abstract_it>Valutiamo la composizione degli encoder di frasi per uso generale proponendo due metriche diverse per quantificare la capacità di comprensione compositiva degli encoder di frasi. Introducemo una nuova metrica, Polarity Sensitivity Scoring (PSS), che utilizza le perturbazioni sentimentali come proxy per misurare la composizione. Confrontiamo quindi i risultati di PSS con quelli ottenuti tramite la nostra proposta estensione di una metrica chiamata Tree Reconstruction Error (TRE) (CITATION) dove la compositività viene valutata misurando quanto bene un modello che produce una rappresentazione reale possa essere approssimato da un modello che combina esplicitamente rappresentazioni dei suoi primitivi.</abstract_it>
      <abstract_kk>Біз жалпы мақсатты мәтін кодерлерінің құрылғысын оқу үшін екі түрлі метрикалық мәтін кодерлерінің көптеген түсініктерін есептеп ұсынамыз. Біз романдық метрикалық, Польшикалық сезімділігі (PSS) сәйкестігін көрсетедік. Бұл сезімдіктерді композициялықты өлшеу үшін прокси ретінде қолданады. Содан кейін біз PSS нәтижелерін біздің келтірілген метрикалық қайта құру қатесі (TRE) (CITATION) деп аталатын метрикалық кеңейтулерімізге салыстырамыз. Бұл композициялық қанша дұрыс кеңейту үлгісін түсіндіру үлгісін түсіндіру үлгісі қолданып</abstract_kk>
      <abstract_lt>Vertiname bendro tikslo sakinių kodatorių sudėtingumą siūlydami du skirtingus metrinius rodiklius, kad būtų galima kiekybiškai įvertinti sakinių kodatorių sudėtinį supratimą. Įdiegiame naują metrinį jautrumo poliarumui vertinimą (PSS), kuris naudoja jautrumo perturbacijas kaip kompozicijos matavimo rodmenį. Tada palyginame PSS rezultatus su rezultatais, gautais pasiūlius išplėsti metrinę klaidą, vadinamą medžio atkūrimo klaida (TRE) (CITATION), kurioje sudėtingumas vertinamas išmatuojant, kaip galima suderinti tikrąjį reprezentacinį model į modeliu, kuris aiškiai sujungia jo primitivų reprezentacijas.</abstract_lt>
      <abstract_ms>Kami menilai komposisi pengekod kalimat-tujuan umum dengan melamar dua metrik berbeza untuk kuantifikasikan kemampuan pemahaman komposisi pengekod kalimat. Kami memperkenalkan metrik baru, Skor Sensitivity Polariti (PSS), yang menggunakan gangguan perasaan sebagai proksi untuk mengukur komposisi. Kemudian kita membandingkan hasil dari PSS dengan hasil yang diterima melalui sambungan metrik yang dipanggil Ralat Pembangunan Pulih Pohon (TRE) (CITATION) di mana komposisi diukur dengan mengukur seberapa baik persembahan sebenar yang menghasilkan model boleh diharapkan oleh model yang secara eksplicit menggabungkan persembahan primitif.</abstract_ms>
      <abstract_mk>Ние ја проценуваме композицијата на кодирачите на реченици со општи цели со предложување на две различни метрики за квантификување на способноста за разбирање на композицијата на кодирачите на реченици. Ние воведуваме нова метричка оценка на чувствителност на поларитетот (PSS), која ги користи чувствителните пертурбирања како прокси за мерење на композицијалноста. Потоа ги споредуваме резултатите од ПСС со оние што се добиваат преку нашето предложено проширување на метричната грешка наречена ТрЕ (ЦИТАЦИЈА), каде композицијата се проценува со мерење колку добро може да се приближи вистинскиот модел кој произведува репрезентација со модел кој експлицитно ги комбинира претставувањата на неговите примити</abstract_mk>
      <abstract_mt>Aħna jevalwaw il-kompożizzjoni tal-kodifikaturi tas-sentenzi bi skop ġenerali billi nipproponu żewġ metriċi differenti biex nikkwantifikaw il-kapaċità ta’ fehim kompożittiv tal-kodifikaturi tas-sentenzi. Aħna nintroduċu Punteġġ Metriku ġdid tas-Sensittività tal-Polarità (PSS), li juża l-perturbazzjonijiet tas-sentimenti bħala proxy għall-kejl tal-kompożizzjoni. Imbagħad inqabblu r-riżultati mill-PSS ma’ dawk miksuba permezz tal-estensjoni proposta tagħna ta’ metrika msejħa Żball fir-Rikostruzzjoni tas-Siġar (TRE) (CITATION) fejn il-kompożizzjoni tiġi evalwata billi titkejjel kemm mudell li jipproduċi rappreżentanza vera jista’ jiġi approssimat b’mudell li jikkombina espliċitament ir-rappreżentazzjonijiet tal-primitivi tiegħu.</abstract_mt>
      <abstract_ml>നമ്മള്‍ വ്യത്യസ്തമായ രണ്ടു മെട്രിക്കുകള്‍ പരിഗണിക്കുന്നതിനാല്‍ വാക്കിന്റെ കോണ്‍കോഡോര്‍ഡുകള്‍ക്കുള്ള ബുദ്ധിമുട്ടിയുട നമ്മള്‍ ഒരു നോവല്‍ മെറ്റിക്ക്, പോളാരിറ്റി സെന്‍സിറ്റിവിറ്റി സ്കോരിങ്ങിനെ പരിചയപ്പെടുത്തുന്നു. അതിന്റെ വികാരങ്ങള്‍ സജ്ജീകരണത്തിന്  പിന്നെ ഞങ്ങള്‍ പിസ്എസില്‍ നിന്നും ഫലങ്ങള്‍ സമ്പാദിച്ചത് നമ്മുടെ പ്രൊദ്ദേശിക്കപ്പെട്ട മെട്രിക്കിന്‍റെ വികസിപ്പിക്കപ്പെട്ട മെറ്റിക്ക് പിശകുകളോടൊപ്പം തുല്യമാക്കുന്നു. അതിന്‍റെ പ</abstract_ml>
      <abstract_mn>Бид ерөнхий зорилготой өгүүлбэрийн кодеруудын нэгтгэлийг үнэлгээд өгүүлбэрийн кодеруудын чадварыг хэмжээгээр хэмжээгээр хоёр өөр метрик суралцаж байна. Бид шинэ метрик, Польширийн мэдрэмжтэй Скорринг (PSS) гэдгийг танилцуулдаг. Энэ нь мэдрэмжүүдийг бүтцийг хэмжихэд прокси гэж ашигладаг. Тэгээд бид PSS-ийн үр дүнг бидний санал дэвшүүлсэн метрийн нэмэлт хэмжээгээр харьцуулж байна. Энэ нь бидний анхны загваруудыг тодорхой нэгтгэдэг загвар юм.</abstract_mn>
      <abstract_no>Vi evaluerer komposisjonalitet av generelle teiknkodingar ved å foreslå to ulike metrikar for å kvantifisera komposisjonelle forståking av teiknkodingar. Vi introduserer eit nytt metrisk, polarisitetsfølsområde (PSS), som brukar følsområde som mellomtenar for å måla komposisjonalitet. Vi sammenliknar derfor resultatet frå PSS med dei som er fått gjennom vårt foreslått utviding av eit metrisk kalla Tre-rekonstruksjonsfeil (TRE) (CITATION) der komposisjonalitet er evaluert ved å måle kor godt ein sann representasjonsmodul kan bli tilnærmet av eit modell som eksplisisk kombinerer representasjonar av sine primitiv.</abstract_no>
      <abstract_pl>Oceniamy skład koderów zdań ogólnego przeznaczenia, proponując dwie różne wskaźniki do ilościowego określenia możliwości zrozumienia kompozycji koderów zdań. Wprowadzamy nową metrykę, Polarity Sensitivity Scoring (PSS), która wykorzystuje zakłócenia sentymentu jako zastępcę do pomiaru kompozycyjności. Następnie porównujemy wyniki z PSS z tymi uzyskanymi przez nasze proponowane rozszerzenie metryki o nazwie Error Rekonstrukcji Drzewa (TRE) (CITATION), gdzie kompozycyjność ocenia się poprzez pomiar, jak dobrze model produkujący reprezentację może być przybliżony przez model, który wyraźnie łączy reprezentacje jego pierwotnych.</abstract_pl>
      <abstract_ro>Evaluăm compoziționalitatea codificatorilor de propoziții cu scop general propunând două măsurători diferite pentru a cuantifica capacitatea de înțelegere compozițională a codificatorilor de propoziții. Introducem o nouă metrică, Polarity Sensitivity Scoring (PSS), care utilizează perturbările sentimentale ca proxy pentru măsurarea compoziționalității. Comparăm apoi rezultatele PSS cu cele obținute prin extensia propusă a unei metrice numită Tree Reconstruction Error (TRE) (CITATION), unde compoziționalitatea este evaluată prin măsurarea cât de bine un model producător de reprezentare reală poate fi aproximat de un model care combină explicit reprezentările primitivelor sale.</abstract_ro>
      <abstract_sr>Procjenjujemo kompoziciju kodera generalne rečenice predlažeći dve različite metrike za kvantifikaciju kompozicionalnog razumevanja sposobnosti kodera rečenica. Predstavljamo novu metričku, poljsku osetljivost (PSS), koja koristi perturbaciju osjećaja kao proksi za mjerenje kompozicionalnosti. Onda uspoređujemo rezultate PSS sa onima koji su dobili putem našeg predloženog produženja metričke greške rekonstrukcije drveta (TRE) (CITATION) gde se kompozicionalnost procjenjuje mjerenjem koliko dobro može približiti pravi model proizvodnje predstave model koji pojasno kombinuje predstave primitiva.</abstract_sr>
      <abstract_si>අපි සාමාන්‍ය-අරමුණ වාක්ෂි කේන්ඩාර්ගේ සංවිධානය අවශ්‍ය කරනවා වෙනස් මෙට්‍රික් දෙකක් ප්‍රයෝජනය කරනවා වචන කේන් අපි නියම මෙට්‍රික්, පෝලෝරිටි සංවේදනය ස්කෝරින්ග් (PSS) කිරීම් කරනවා, ඒකෙන් සංවේදනය ප්‍රොක්සියක් විදියට ප්‍ර අපි පස්සේ ප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිකාරය (TRE) (CITATION) කියලා ප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රති</abstract_si>
      <abstract_so>We evaluate the compositionality of general-purpose sentence encoders by proposing two different metrics to quantify compositional understanding capability of sentence encoders.  Waxaynu soo bandhignaynaa sawir qoraal ah oo polarity Sensitivity (PSS), kaas oo u isticmaalaya xisaabta xisaabta xisaabta ah sida proksi ah oo qiyaasa kooxda. Markaas waxaynu isbarbardhignaa resultiyada PSS iyo kuwii laga helay, tusaale ahaan oo si cad u soo bandhigaa tirada lagu magacaabay khaladda dhismaha geedka (TRE) (CITATION) meesha lagu qiimeeyo qofka la qiimeeyo, sida loo qiimeeyo muuqashada rasmiga ah oo u soo saari karo tusaale u dhow, kaas oo si cad u soo bandhiggan kara wakiilayaashiisa.</abstract_so>
      <abstract_sv>Vi utvärderar kompositionaliteten hos allmänna meningskoder genom att föreslå två olika mätvärden för att kvantifiera kompositionsförståelse hos meningskoder. Vi introducerar ett nytt mått, Polarity Sensitivity Scoring (PSS), som använder sentimentstörningar som en proxy för att mäta kompositionalitet. Vi jämför sedan resultat från PSS med de som erhållits via vår föreslagna förlängning av ett metriskt som kallas Tree Reconstruction Error (TRE) (CITATION) där kompositionaliteten utvärderas genom att mäta hur väl en verklig representationsproducerande modell kan approximeras av en modell som uttryckligen kombinerar representationer av dess primitiver.</abstract_sv>
      <abstract_ta>வாக்குறியீட்டின் குறியீட்டின் பொது சொற்கோவைக் குறியீட்டின் ஒழுங்கீட்டை நாம் மதிப்பிடுகிறோம் இரண்டு வேறு முற நாம் ஒரு புதிய மெட்ரிக், போலிரிட்டி உணர்வு மதிப்பெண்கள் (PSS) குறிப்பிடுகிறோம். இது உணர்வு குழப்பத்தை அளக்க ஒரு பதிலாக பயன்படுத்தும். பின்னர் நாம் PSS-ல் இருந்து முடிவுகளை ஒப்பிடுகிறோம் நம் பரிந்துரைக்கப்பட்ட மரத்தை மீட்ரிக் கட்டுப்பாட்டு பிழை (TRE) என்ற மெட்ரிக் பிழை (CITATION) என்று பெயர்க்கப்பட்ட மெட்ரிக் பிழை( சி</abstract_ta>
      <abstract_ur>ہم عمومی مطابق فیصلہ کوڈروں کی پیدائش کے مطابق دو مختلف میٹریک کی پیشنهاد کرتے ہیں کہ جمعیت سمجھنے کے قابلیت کا مقدار کریں ہم ایک نئی متریک، پولیریٹی سنسیتیویت اسکورینگ (PSS) کو معلوم کرتے ہیں، جسے احساسات پرٹربرسی کا استعمال کرتا ہے، ایک پیروکسی کے لئے پیروکسی کے طور پر۔ پھر ہم PSS کے نتائج کو ان لوگوں سے مقایسہ کرتے ہیں جو ہمارے پیشنهاد کے ذریعہ درخت Reconstruction Error (TRE) (CITATION) کے مطابق حاصل کئے گئے ہیں، جہاں پیدا کئے گئے ہیں، جہاں پیدا کئے جاتے ہیں کہ کس طرح ایک حقیقی نمونہ پیدا کرنے والی نمونڈل کے مطابق ایک نمونڈل سے محسوب کئے جاتے ہیں جو صریح طور پر اس کی اولی</abstract_ur>
      <abstract_uz>Biz maxfiy soʻz kodlash imkoniyatini qiymatlashimiz mumkin. Maxfiy soʻzlar kodlash imkoniyatini aniqlash uchun ikki boshqa metrik rivojlanishi mumkin. Biz bir novel metrik, Polarity Sensitivity Scoriya (PSS) bilan ishlatamiz. Bu hisob-turbatalarni qo'llash uchun foydalanadi. Keyin, biz PSS natijalaridan olingan narsalarni o'xshasiz va o'zgarishni tasavvur qilamiz, bu daraxtni qayta yuklash xatosi (TRE) (CITATION) deb nomlangan metrik rivojlanishimizdan foydalanamiz. Bu yerda kompaniyalarni qiymatlashtirish mumkin. Bu modelni hozirganish modellarining qiymatlarini aniqlash mumkin.</abstract_uz>
      <abstract_vi>Chúng tôi đánh giá độ hợp tác của bộ mã hóa câu chung bằng cách đề xuất hai âm lượng khác nhau để xác định khả năng nhận dạng cấu trúc của bộ mã hóa câu. Chúng tôi giới thiệu một loại mới về đo lường, độ nhạy vùng cực (PSS) dùng các lo ngại cảm xúc làm ủy nhiệm cho đo độ hợp tác. Sau đó chúng ta so sánh kết quả từ PSS với kết quả bằng việc mở rộng hệ thống đo lường gọi là lỗi tái cấu trúc cây (TRE) (Citation) nơi mà độ phối hợp được đánh giá bằng cách đo mức độ mô hình thực sự phân tích có thể được xác định gần với mô hình của mô hình được xác định tỉ mỉ bằng cách kết hợp các biểu tượng nguyên tử.</abstract_vi>
      <abstract_bg>Ние оценяваме композиционността на кодиращите изречения с общо предназначение, като предлагаме две различни показатели за количествено определяне на способността за разбиране на композицията на кодиращите изречения. Въвеждаме нов показател, който използва сентиментални смущения като прокси за измерване на композицията. След това сравняваме резултатите от ПСС с тези, получени чрез предложеното разширение на метрика, наречена Грешка при възстановяване на дърветата (ЦИТАЦИЯ), където композицията се оценява чрез измерване колко добре един модел, произвеждащ истинско представяне, може да бъде приближен чрез модел, който изрично комбинира представяне на неговите примитиви.</abstract_bg>
      <abstract_nl>We evalueren de compositionaliteit van algemene zinnencoders door twee verschillende metrics voor te stellen om compositioneel begrip van zinnencoders te kwantificeren. We introduceren een nieuwe metric, Polarity Sensitivity Scoring (PSS), die sentimentstoornissen gebruikt als een proxy voor het meten van compositionaliteit. Vervolgens vergelijken we resultaten van PSS met die verkregen via onze voorgestelde uitbreiding van een metric genaamd Tree Reconstruction Error (TRE) (CITATION), waarbij compositionaliteit wordt geëvalueerd door te meten hoe goed een echte representatie producerend model kan worden benaderd door een model dat expliciet representaties van zijn primitieven combineert.</abstract_nl>
      <abstract_hr>Procjenjujemo kompoziciju kodera opće svrhe, predlažeći dvije različite metrike za kvantificiranje sposobnosti kompozicionalnog razumijevanja kodera kazne. Upoznajemo novu metričku, poljsku osjetljivost (PSS), koja koristi perturbaciju osjećaja kao proxy za mjerenje kompozicionalnosti. Onda uspoređujemo rezultate PSS-a s onima koji su dobili putem našeg predloženog produženja metričke pogreške rekonstrukcije drveta (TRE) (CITATION) gdje se kompozicija procjenjuje mjerenjem koliko dobro može približiti pravi model proizvođa ča predstave koji pojasno kombinira predstave primitiva.</abstract_hr>
      <abstract_da>Vi evaluerer kompositionaliteten af generelle formål sætningskodere ved at foreslå to forskellige metrics til at kvantificere kompositionel forståelse evne af sætningskodere. Vi introducerer en ny metric, Polarity Sensitivity Scoring (PSS), som bruger sentimentforstyrrelser som en proxy til måling af kompositionalitet. Vi sammenligner derefter resultater fra PSS med resultaterne opnået via vores foreslåede udvidelse af en metric kaldet Tree Reconstruction Error (TRE) (CITATION), hvor kompositionaliteten vurderes ved at måle, hvor godt en sand repræsentationsproducerende model kan tilnærmes af en model, der eksplicit kombinerer repræsentationer af sine primitiver.</abstract_da>
      <abstract_de>Wir evaluieren die Zusammensetzung von universellen Satzkodierern, indem wir zwei verschiedene Metriken vorschlagen, um die kompositorische Verständnisfähigkeit von Satzkodierern zu quantifizieren. Wir stellen eine neuartige Metrik vor, Polarity Sensitivity Scoring (PSS), die Sentimentstörungen als Proxy für die Messung der Compositionalität nutzt. Wir vergleichen dann die Ergebnisse von PSS mit denen, die wir durch unsere vorgeschlagene Erweiterung einer Metrik namens Tree Reconstruction Error (TRE) (CITATION) erhalten haben, bei der die Compositionalität bewertet wird, indem gemessen wird, wie gut ein repräsentationsproduzierendes Modell durch ein Modell approximiert werden kann, das explizit Repräsentationen seiner Primitive kombiniert.</abstract_de>
      <abstract_id>Kami mengevaluasi komposisionalitas pengkode kalimat-tujuan umum dengan mengusulkan dua metrik berbeda untuk kuantifikasi kemampuan pemahaman komposisi pengkode kalimat. Kami memperkenalkan metrik baru, Skor Sensitivitas Polaritas (PSS), yang menggunakan perturbasi sentimen sebagai proksi untuk mengukur komposisionalitas. Kemudian kita membandingkan hasil dari PSS dengan hasil yang diperoleh melalui ekstensi kami yang diusulkan dari metrik yang disebut Tree Reconstruction Error (TRE) (CITATION) di mana komposisionalitas diuji dengan mengukur seberapa baik sebuah representation sejati yang menghasilkan model dapat didekatkan oleh model yang secara eksplicit menggabungkan representation primitif-nya.</abstract_id>
      <abstract_fa>ما با پیشنهاد دو متری متفاوت برای تعداد توانایی فهمیدن پیچیده‌ای از زبان‌دهندگان عمومی ارزیابی می‌کنیم. ما یک مقدار متریک نویسی را معرفی می‌کنیم، مقدار حساسی لهستانی (PSS) که از تغییرات احساسات به عنوان پروکسی برای اندازه اندازه‌گیری ترکیبی استفاده می‌کند. سپس نتیجه‌های PSS را با کسانی که از طریق کشور پیشنهاد ما به عنوان خطای بازسازی درخت (TRE) (CITATION) مقایسه می‌کنیم که با اندازه اندازه‌گیری که چقدر خوب یک نمونه‌ی تولید کننده‌ی نمونه‌ای واقعی می‌تواند توسط یک نمونه‌ای که به طور واضح نمونه‌های اولیه‌اش را ترکیب می‌کند، تقریبا می</abstract_fa>
      <abstract_ko>우리는 두 가지 서로 다른 도량 기준을 제시하여 문장 인코더의 합성 이해 능력을 계량화하여 통용 문장 인코더의 합성성을 평가한다.정서적 교란을 구성성을 측정하는 에이전트로 활용하는 극성민감평점(PSS)을 소개했다.그 다음에 우리는 PSS의 결과를 우리가 제시한 트리 재건 오차(Tree Reconstruction Error, TRE)의 도량 확장에서 얻은 결과와 비교한다. 그 중에서 합성성은 진정한 표시 생성 모델이 원어로 표시된 모델에 접근할 수 있는 정도를 측정함으로써 평가한다.</abstract_ko>
      <abstract_sw>Tunatathmini muungano wa kodi za hukumu kwa lengo la jumla kwa kupendekeza mbili tofauti ili kuhakikisha uwezo wa kuelewa viungo vya hukumu. Tunaonyesha mbinu za kitaifa, Uchunguzi wa Uchunguzi (PSS), ambazo hutumia uvunjifu wa hisia kama proksi kwa ajili ya kupima usawa. We then compare results from PSS with those obtained via our proposed extension of a metric called Tree Reconstruction Error (TRE) (CITATION) where compositionality is evaluated by measuring how well a true representation producing model can be approximated by a model that explicitly combines representations of its primitives.</abstract_sw>
      <abstract_af>Ons evalueer die komposiasionaliteit van algemene doel-teikenkodere deur twee verskillende metries te voorstel om komposisionele verstandigheid van teikenkodere te kvantifiseer. Ons introduseer 'n nuwe metriese, Polarisiteit Sensitiwiteitskouering (PSS), wat sentimente perturbasies gebruik as 'n volmag vir gemeeting van komposibiliteit. Ons vergelyk dan resultate van PSS met die wat deur ons voorgestelde uitbreiding van 'n metries genoem Boom Rekonstruksie Fout (TRE) (KITATION) vergelyk word waar komposiasionaliteit is evalueer deur te maak hoe goed 'n waar voorgestelde voorstelling produseerde model kan wees omtrent deur 'n model wat uitduidelik verbind voorstellings van sy primitives.</abstract_af>
      <abstract_tr>Biz umumy maksady sözlemçilerinin bir toparlygyny çykaryp, sözlemçilerin başaryp bilmek üçin iki farklı metrik teklif edip, çykyş kodleyicisini ölçüp barýarys. Biz bir roman metrik, Polaritet Sensitivitet Skorpyny (PSS) bilen tanyşýarys, bu duýgyny kompozisyonalitet ölçüsi üçin bir proxy diýip işleýär. Sonra PSS'den netijelerimizi, a ğaç Yeniden Yapılandırma Hatası (TRE) (CITATION) adlı metrik aracılığımızdan alınan bir metrik şeklinde karşılaştırıyoruz. Bu şekilde kompozisyonalitet, ilkinji şeklinde nähili doğru bir şekilde üretilen modelleri tarafından çözümlendirir ve bu şekilde örneklendirilen bir model tarafından çözümlendirilir.</abstract_tr>
      <abstract_sq>Ne vlerësojmë kompozitivitetin e koduesve të fjalëve me qëllim të përgjithshëm duke propozuar dy metrika të ndryshme për të kuantifikuar aftësinë e kuptimit kompozitiv të koduesve të fjalëve. We introduce a novel metric, Polarity Sensitivity Scoring (PSS), which utilizes sentiment perturbations as a proxy for measuring compositionality.  Ne pastaj krahasojmë rezultatet nga PSS me ato të fituara nëpërmjet zgjerimit tonë të propozuar të një metrike të quajtur Gabim i Rindërtimit të Pemës (TRE) (CITATION) ku kompozitiviteti vlerësohet duke masuar sa mirë një përfaqësim i vërtetë që prodhon modelin mund të përafërtohet nga një model që kombinon shprehësisht përfaqësimet e primitivëve të tij.</abstract_sq>
      <abstract_am>ሁለትን ልዩ ሜትሮዎች የቁጥጥር አካባቢ ማስተዋል አካባቢ የፍርድ አካላትን ለማስተካከል እናሳውቃለን፡፡ የሜትሮክ ሜትሪክ፣ ፖሊሲቲ ስህተት ስኮር (PSS) እናስጠቃለን፤ ይህም የስሜት ተቃውሞዎችን ለማሰናከል ፕሮክሲ ነው፡፡ ከዚህም በኋላ ከPSS ፍሬዎችን በተቀረበ የዛፍ መሠረት ስህተት (TRE) በተባለው ማረፊያ ስህተት (CITATION) እናስተካክል እናደርጋለን፡፡</abstract_am>
      <abstract_hy>Մենք գնահատում ենք ընդհանուր նպատակի նախադասությունների կոդավորիչների բաղադրությունը առաջարկելով երկու տարբեր մետրիկ նախադասությունների կոդավորիչների բաղադրական հասկանալու հնարավորության քանակությամբ չափելու համար: Մենք ներկայացնում ենք նոր մետրիկ, պոլարիտության զգացմունքի գնահատականը (PSS), որը օգտագործում է զգացմունքների խառնաշփումները որպես արտահայտություն համադրության չափման համար: Այնուհետև մենք համեմատում ենք PSS-ի արդյունքները այն արդյունքների հետ, որոնք ստացվել են ծառի վերականգնման սխալ կոչվող մետրիկայի մեր առաջարկած ընդլայնման միջոցով, որտեղ կոմպոզիցիոնալությունը գնահատվում է չափելով, թե ինչքան լավ կարող է իրական ներկայացումը արտադրող մոդելը մոտենալ մոդելի միջոցով, որը բացատր</abstract_hy>
      <abstract_bn>আমরা সাধারণ উদ্দেশ্যের ক্ষেত্রে বিভিন্ন দুটি ভিন্ন মেট্রিক প্রস্তাব করেছি যাতে আমরা বাক্য এনকোডারের সাথে বুঝতে পারি যে বি আমরা একটি উপন্যাসের মেট্রিক, পোলারিটি সেন্টিভিসি স্কোরিং (পিএসএস) পরিচয় করিয়ে দিচ্ছি, যা আবেগের প্রযুক্তি হিসেবে প্রক্সি ব্যবহার করে যাচ্ তারপর আমরা পিএসএস থেকে ফলাফল তুলনা করি যারা প্রস্তাব করেছে তাদের সাথে আমাদের প্রস্তাবিত একটি মেট্রিকের প্রস্তাবিত বিস্তারিত ব্যবস্থা (ট্রি) (সিটাটিয়েশন) যেখানে প্রতিনিধিত্বের মূল্য পরিমা</abstract_bn>
      <abstract_ca>Evaluam la composicionalitat dels codificadors de frases a propòsit general proposant dues mètriques diferents per quantificar la capacitat d'entendre la composició dels codificadors de frases. Introduïm una nova puntuació mètrica de sensibilitat polar (PSS), que utilitza les perturbacions sentimentals com a proxi per mesurar la composicionalitat. Llavors comparem els resultats del PSS amb els obtinguts a través de la nostra extensió proposada d'un mètric anomenat Error de Reconstrucció de l'Arbre (TRE) (CITATION), on la composicionalitat es valora mesurant com un model de producció de la veritable representació pot ser aproximat per un model que combina explícitament les representacions dels seus primitius.</abstract_ca>
      <abstract_bs>Procjenjujemo kompozicionalnost kodera generalne rečenice predlažeći dvije različite metrike za kvantifikaciju sposobnosti kompozicionalnog razumijevanja kodera rečenica. Predstavljamo novu metričku, poljsku osjećajnost (PSS), koja koristi perturbaciju osjećaja kao proksi za mjerenje kompozicionalnosti. Onda uspoređujemo rezultate PSS sa onima koji su dobili putem našeg predloženog produženja metričke pogreške rekonstrukcije drveta (TRE) (CITATION) u kojoj se kompozicionalnost procjenjuje mjerenjem koliko dobro može približiti pravi model proizvodnje predstave model koji pojasno kombinira predstave primitiva.</abstract_bs>
      <abstract_az>Biz cümlələr kodlayıcıların birləşdirilməsini təsdiqləyici iki müxtəlif metrik təklif edirik ki, cümlələr kodlayıcıların mümkünlüyünü kvantifikat edək. Biz yeni metrik, Polarity Sensitivity Scoring (PSS) ilə tanıyırıq ki, bu duyguları birləşdirilmək üçün proksisi olaraq istifadə edir. Sonra, PSS-dən sonuçlarını təklif etdiyimiz a ğac Yenidən Yapılandırma Hatası (TRE) (CITATION) vasitəsilə müəyyən edilənlərlə qarşılaşdırırıq ki, kompoziciyallığı ölçüb, həqiqət bir göstəriş modeli ilə yaxınlaşdırılabilir, bu modeli primitivlərin nümunələrini açıq-aydın birləşdirir.</abstract_az>
      <abstract_cs>Vyhodnocujeme kompozicionalitu univerzálních snímačů vět navržením dvou různých metrik pro kvantifikaci kompozičních schopností snímačů vět. Představujeme novou metriku Polarity Sensitivity Scoring (PSS), která využívá poruchy sentimentu jako proxy pro měření kompozice. Následně porovnáváme výsledky PSS s výsledky získanými prostřednictvím našeho navrhovaného rozšíření metriky nazvané Tree Reconstruction Error (CITATION), kde je kompozicionalita hodnocena měřením, jak dobře lze model produkující skutečnou reprezentaci aproximovat modelem, který explicitně kombinuje reprezentace jeho primitivů.</abstract_cs>
      <abstract_et>Üldotstarbeliste lausekodeerijate kompositsioonivõimet hindame, pakkudes välja kaks erinevat meetrit lausekodeerijate kompositsioonivõime kvantifitseerimiseks. Tutvustame uut meetrit Polarity Sensitivity Scoring (PSS), mis kasutab sentimentaalseid häireid kompositsiooni mõõtmiseks. Seejärel võrdleme PSS tulemusi nendega, mis on saadud meie kavandatud meetriku laienduse kaudu nimega Tree Reconstruction Error (TRE) (CITATION), kus kompositsioonilisust hinnatakse mõõtes, kui hästi saab tõelist representatsiooni tootvat mudelit lähendada mudeliga, mis selgesõnaliselt ühendab oma primitiivide representatsioonid.</abstract_et>
      <abstract_fi>Arvioimme yleiskäyttöisten lauseenkooderien kompositiivisuutta ehdottamalla kahta eri mittaria lauseenkooderien kompositiivisen ymmärtämiskyvyn kvantifioimiseksi. Esittelemme uuden mittarin, Polarity Sensitivity Scoring (PSS), joka hyödyntää tunteiden häiriöitä kompositiivisuuden mittaamisen proxynä. Tämän jälkeen vertaamme PSS:n tuloksia ehdotettuun Tree Reconstruction Error (TRE) (CITATION) -mittarin laajennukseen, jossa kompositiivisuutta arvioidaan mittaamalla, kuinka hyvin todellista esitystä tuottavaa mallia voidaan lähestyä mallilla, joka nimenomaisesti yhdistää sen primitiivejä.</abstract_fi>
      <abstract_jv>Anyone Awak dhéwé nggawe ngubah un bote Metik, polarity Sensitity Kernel</abstract_jv>
      <abstract_sk>Za kvantificiranje sposobnosti kompozicijskega razumevanja stavkov kodirnikov stavkov v splošnem namenu smo ocenili z dvema različnima metrikama. Predstavljamo novo merilo Polarity Sensitivity Scoring (PSS), ki uporablja motnje čustva kot približek merjenja kompozicijske kompozicije. Nato primerjamo rezultate PSS s tistimi, pridobljenimi z našo predlagano razširitvijo metrike Tree Reconstruction Error (TRE) (CITATION), kjer kompozicijnost ocenjujemo z merjenjem, kako dobro je mogoče model, ki ustvarja resnično reprezentacijo, približati model, ki eksplicitno združuje reprezentacije svojih primitivov.</abstract_sk>
      <abstract_he>אנו מעריכים את המרכיבות של קודפי משפטים למטרה כללית על ידי הצעה שתי מטריות שונות כדי לקוונטיב היכולת הבנה המרכיבית של קודפי משפטים. We introduce a novel metric, Polarity Sensitivity Scoring (PSS), which utilizes sentiment perturbations as a proxy for measuring compositionality.  ואז נשווה את התוצאות מ-PSS עם אלה שנקבלו באמצעות ההארכה המוצעת שלנו של מטריקה שנקראת שגיאה בהבנה מחדש של עץ (TRE) (CITATION) שבו המרכיבות מוערכת על ידי מידה היטב של מייצג אמיתי מודל יכול להתקרב על ידי מודל שמשולב באופן ברור מייצגים של פרימיטיביו.</abstract_he>
      <abstract_ha>Tuna ƙaddara composition of an kodi na jumla-goan da za'a yi amfani da kwamfyuta biyu masu motsi ko da za'a ƙayyade fahimci na daidaita ko-kode. Tuna ƙara wani littãfin na nowaya, Sura'awa na Polarity (PSS), wanda ke yi amfani da turbatori masu hisia kamar wata proksi ga haƙunsa da composition. Sa'an nan kuma, Munã samfan matsalar daga PSS da waɗanda aka samu da sami da kuma aka sami marubucin da aka faɗa wata metric wanda aka kallo na Gõdiya Tree (CITATIAN), a inda an evaluce composition da a ƙaddara jinsi masu tsari da gaskiya za'a iya sami shi da wani motel wanda ke samu da shi, mai bayyane na haɗi masu wakin mataimakiyinsa.</abstract_ha>
      <abstract_fil>Inihahalaga namin ang pagka-compositionality ng general-purpose sentence encoders sa pamamagitan ng pagbibigay ng dalawang ibang metrics upang pagbibigay ng pagkaunawa ng salimuot na kapangyarihan ng mga encoders ng sentence. Nagbibigay kami ng bagong metric, Polarity Sensitivity Scoring Kung magkagayo'y kami ay nakikipagkomparay ng mga resulta mula sa PSS sa mga natanggap sa pamamagitan ng aming inilagay na extension ng metric na tinatawag na Tree Reconstruction Error</abstract_fil>
      <abstract_bo>We evaluate the compositionality of general-purpose sentence encoders by proposing two different metrics to quantify compositional understanding capability of sentence encoders. We introduce a novel metric, Polarity Sensitivity Scoring (PSS), which utilizes sentiment perturbations as a proxy for measuring compositionality. We then compare results from PSS with those obtained via our proposed extension of a metric called Tree Reconstruction Error (TRE) (CITATION) where compositionality is evaluated by measuring how well a true representation producing model can be approximated by a model that explicitly combines representations of its primitives.</abstract_bo>
      </paper>
    </volume>
</collection>