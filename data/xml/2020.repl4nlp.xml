<collection id="2020.repl4nlp">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 5th Workshop on Representation Learning for NLP</booktitle>
      <editor><first>Spandana</first><last>Gella</last></editor>
      <editor><first>Johannes</first><last>Welbl</last></editor>
      <editor><first>Marek</first><last>Rei</last></editor>
      <editor><first>Fabio</first><last>Petroni</last></editor>
      <editor><first>Patrick</first><last>Lewis</last></editor>
      <editor><first>Emma</first><last>Strubell</last></editor>
      <editor><first>Minjoon</first><last>Seo</last></editor>
      <editor><first>Hannaneh</first><last>Hajishirzi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="67f2d50b">2020.repl4nlp-1</url>
    </meta>
    <frontmatter>
      <url hash="d40b3f5b">2020.repl4nlp-1.0</url>
      <bibkey>repl4nlp-2020-representation</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Zero-Resource Cross-Domain Named Entity Recognition</title>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>1&#8211;6</pages>
      <abstract>Existing models for cross-domain named entity recognition (NER) rely on numerous unlabeled corpus or labeled NER training data in target domains. However, collecting data for low-resource target domains is not only expensive but also time-consuming. Hence, we propose a cross-domain NER model that does not use any external resources. We first introduce a Multi-Task Learning (MTL) by adding a new objective function to detect whether tokens are named entities or not. We then introduce a framework called Mixture of Entity Experts (MoEE) to improve the robustness for zero-resource domain adaptation. Finally, experimental results show that our model outperforms strong unsupervised cross-domain sequence labeling models, and the performance of our model is close to that of the state-of-the-art model which leverages extensive resources.</abstract>
      <url hash="7fe916c9">2020.repl4nlp-1.1</url>
      <doi>10.18653/v1/2020.repl4nlp-1.1</doi>
      <video href="http://slideslive.com/38929767" />
      <bibkey>liu-etal-2020-zero</bibkey>
    </paper>
    <paper id="2">
      <title>Encodings of Source Syntax: Similarities in <fixed-case>NMT</fixed-case> Representations Across Target Languages</title>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <author><first>Anna</first><last>Rafferty</last></author>
      <pages>7&#8211;16</pages>
      <abstract>We train neural machine translation (NMT) models from English to six target languages, using NMT encoder representations to predict ancestor constituent labels of source language words. We find that NMT encoders learn similar source syntax regardless of NMT target language, relying on explicit morphosyntactic cues to extract syntactic features from source sentences. Furthermore, the NMT encoders outperform RNNs trained directly on several of the constituent label prediction tasks, suggesting that NMT encoder representations can be used effectively for natural language tasks involving syntax. However, both the NMT encoders and the directly-trained RNNs learn substantially different syntactic information from a probabilistic context-free grammar (PCFG) parser. Despite lower overall accuracy scores, the PCFG often performs well on sentences for which the RNN-based models perform poorly, suggesting that RNN architectures are constrained in the types of syntax they can learn.</abstract>
      <url hash="09498e6e">2020.repl4nlp-1.2</url>
      <doi>10.18653/v1/2020.repl4nlp-1.2</doi>
      <video href="http://slideslive.com/38929768" />
      <bibkey>chang-rafferty-2020-encodings</bibkey>
    </paper>
    <paper id="3">
      <title>Learning Probabilistic Sentence Representations from Paraphrases</title>
      <author><first>Mingda</first><last>Chen</last></author>
      <author><first>Kevin</first><last>Gimpel</last></author>
      <pages>17&#8211;23</pages>
      <abstract>Probabilistic word embeddings have shown effectiveness in capturing notions of generality and entailment, but there is very little work on doing the analogous type of investigation for sentences. In this paper we define probabilistic models that produce distributions for sentences. Our best-performing model treats each word as a linear transformation operator applied to a multivariate Gaussian distribution. We train our models on paraphrases and demonstrate that they naturally capture sentence specificity. While our proposed model achieves the best performance overall, we also show that specificity is represented by simpler architectures via the norm of the sentence vectors. Qualitative analysis shows that our probabilistic model captures sentential entailment and provides ways to analyze the specificity and preciseness of individual words.</abstract>
      <url hash="09f3e76d">2020.repl4nlp-1.3</url>
      <doi>10.18653/v1/2020.repl4nlp-1.3</doi>
      <video href="http://slideslive.com/38929769" />
      <bibkey>chen-gimpel-2020-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="4">
      <title>Word Embeddings as Tuples of Feature Probabilities</title>
      <author><first>Siddharth</first><last>Bhat</last></author>
      <author><first>Alok</first><last>Debnath</last></author>
      <author><first>Souvik</first><last>Banerjee</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>24&#8211;33</pages>
      <abstract>In this paper, we provide an alternate perspective on word representations, by reinterpreting the dimensions of the vector space of a word embedding as a collection of features. In this reinterpretation, every component of the word vector is normalized against all the word vectors in the vocabulary. This idea now allows us to view each vector as an <tex-math>n</tex-math>-tuple (akin to a fuzzy set), where <tex-math>n</tex-math> is the dimensionality of the word representation and each element represents the probability of the word possessing a feature. Indeed, this representation enables the use fuzzy set theoretic operations, such as union, intersection and difference. Unlike previous attempts, we show that this representation of words provides a notion of similarity which is inherently asymmetric and hence closer to human similarity judgements. We compare the performance of this representation with various benchmarks, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations.</abstract>
      <url hash="12eb4a8f">2020.repl4nlp-1.4</url>
      <doi>10.18653/v1/2020.repl4nlp-1.4</doi>
      <video href="http://slideslive.com/38929770" />
      <bibkey>bhat-etal-2020-word</bibkey>
    </paper>
    <paper id="5">
      <title>Compositionality and Capacity in Emergent Languages</title>
      <author><first>Abhinav</first><last>Gupta</last></author>
      <author><first>Cinjon</first><last>Resnick</last></author>
      <author><first>Jakob</first><last>Foerster</last></author>
      <author><first>Andrew</first><last>Dai</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <pages>34&#8211;38</pages>
      <abstract>Recent works have discussed the extent to which emergent languages can exhibit properties of natural languages particularly learning compositionality. In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication in addition to the communicative bandwidth. Our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.</abstract>
      <url hash="994aa67f">2020.repl4nlp-1.5</url>
      <doi>10.18653/v1/2020.repl4nlp-1.5</doi>
      <video href="http://slideslive.com/38929771" />
      <revision id="1" href="2020.repl4nlp-1.5v1" hash="ca52c819" />
      <revision id="2" href="2020.repl4nlp-1.5v2" hash="994aa67f" date="2021-01-03">Fixed a citation.</revision>
      <bibkey>gupta-etal-2020-compositionality</bibkey>
    </paper>
    <paper id="6">
      <title>Learning Geometric Word Meta-Embeddings</title>
      <author><first>Pratik</first><last>Jawanpuria</last></author>
      <author><first>Satya Dev</first><last>N T V</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Bamdev</first><last>Mishra</last></author>
      <pages>39&#8211;44</pages>
      <abstract>We propose a geometric framework for learning meta-embeddings of words from different embedding sources. Our framework transforms the embeddings into a common latent space, where, for example, simple averaging or concatenation of different embeddings (of a given word) is more amenable. The proposed latent space arises from two particular geometric transformations - source embedding specific orthogonal rotations and a common Mahalanobis metric scaling. Empirical results on several word similarity and word analogy benchmarks illustrate the efficacy of the proposed framework.</abstract>
      <url hash="9eae4748">2020.repl4nlp-1.6</url>
      <attachment type="Software" hash="89828011">2020.repl4nlp-1.6.Software.zip</attachment>
      <doi>10.18653/v1/2020.repl4nlp-1.6</doi>
      <video href="http://slideslive.com/38929772" />
      <bibkey>jawanpuria-etal-2020-learning</bibkey>
    </paper>
    <paper id="10">
      <title>Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with <fixed-case>D</fixed-case>oc<fixed-case>BERT</fixed-case></title>
      <author><first>Ashutosh</first><last>Adhikari</last></author>
      <author><first>Achyudh</first><last>Ram</last></author>
      <author><first>Raphael</first><last>Tang</last></author>
      <author><first>William L.</first><last>Hamilton</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>72&#8211;77</pages>
      <abstract>Fine-tuned variants of BERT are able to achieve state-of-the-art accuracy on many natural language processing tasks, although at significant computational costs. In this paper, we verify BERT&#8217;s effectiveness for document classification and investigate the extent to which BERT-level effectiveness can be obtained by different baselines, combined with knowledge distillation&#8212;a popular model compression method. The results show that BERT-level effectiveness can be achieved by a single-layer LSTM with at least <tex-math>40\times</tex-math> fewer FLOPS and only <tex-math>{\sim}3\%</tex-math> parameters. More importantly, this study analyzes the limits of knowledge distillation as we distill BERT&#8217;s knowledge all the way down to linear models&#8212;a relevant baseline for the task. We report substantial improvement in effectiveness for even the simplest models, as they capture the knowledge learnt by BERT.</abstract>
      <url hash="051972bd">2020.repl4nlp-1.10</url>
      <doi>10.18653/v1/2020.repl4nlp-1.10</doi>
      <video href="http://slideslive.com/38929776" />
      <bibkey>adhikari-etal-2020-exploring</bibkey>
    </paper>
    <paper id="16">
      <title>Are All Languages Created Equal in Multilingual <fixed-case>BERT</fixed-case>?</title>
      <author><first>Shijie</first><last>Wu</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>120&#8211;130</pages>
      <abstract>Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.</abstract>
      <url hash="d94ad761">2020.repl4nlp-1.16</url>
      <doi>10.18653/v1/2020.repl4nlp-1.16</doi>
      <video href="http://slideslive.com/38929782" />
      <bibkey>wu-dredze-2020-languages</bibkey>
      <pwccode url="https://github.com/shijie-wu/crosslingual-nlp" additional="false">shijie-wu/crosslingual-nlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="22">
      <title>Evaluating Compositionality of Sentence Representation Models</title>
      <author><first>Hanoz</first><last>Bhathena</last></author>
      <author><first>Angelica</first><last>Willis</last></author>
      <author><first>Nathan</first><last>Dass</last></author>
      <pages>185&#8211;193</pages>
      <abstract>We evaluate the compositionality of general-purpose sentence encoders by proposing two different metrics to quantify compositional understanding capability of sentence encoders. We introduce a novel metric, Polarity Sensitivity Scoring (PSS), which utilizes sentiment perturbations as a proxy for measuring compositionality. We then compare results from PSS with those obtained via our proposed extension of a metric called Tree Reconstruction Error (TRE) (CITATION) where compositionality is evaluated by measuring how well a true representation producing model can be approximated by a model that explicitly combines representations of its primitives.</abstract>
      <url hash="4d42e5cc">2020.repl4nlp-1.22</url>
      <attachment type="Software" hash="4a706720">2020.repl4nlp-1.22.Software.zip</attachment>
      <doi>10.18653/v1/2020.repl4nlp-1.22</doi>
      <video href="http://slideslive.com/38929788" />
      <bibkey>bhathena-etal-2020-evaluating</bibkey>
    </paper>
    </volume>
</collection>