<collection id="2020.crac">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference</booktitle>
      <editor><first>Maciej</first><last>Ogrodniczuk</last></editor>
      <editor><first>Vincent</first><last>Ng</last></editor>
      <editor><first>Yulia</first><last>Grishina</last></editor>
      <editor><first>Sameer</first><last>Pradhan</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="50191fb4">2020.crac-1.0</url>
      <bibkey>crac-2020-models</bibkey>
    </frontmatter>
    <paper id="2">
      <title>It&#8217;s absolutely divine! Can fine-grained sentiment analysis benefit from coreference resolution?</title>
      <author><first>Orphee</first><last>De Clercq</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>11&#8211;21</pages>
      <abstract>While it has been claimed that anaphora or coreference resolution plays an important role in opinion mining, it is not clear to what extent coreference resolution actually boosts performance, if at all. In this paper, we investigate the potential added value of coreference resolution for the aspect-based sentiment analysis of restaurant reviews in two languages, English and Dutch. We focus on the task of aspect category classification and investigate whether including coreference information prior to classification to resolve implicit aspect mentions is beneficial. Because coreference resolution is not a solved task in NLP, we rely on both automatically-derived and gold-standard coreference relations, allowing us to investigate the true upper bound. By training a classifier on a combination of lexical and semantic features, we show that resolving the coreferential relations prior to classification is beneficial in a joint optimization setup. However, this is only the case when relying on gold-standard relations and the result is more outspoken for English than for Dutch. When validating the optimal models, however, we found that only the Dutch pipeline is able to achieve a satisfying performance on a held-out test set and does so regardless of whether coreference information was included.</abstract>
      <url hash="01a14f14">2020.crac-1.2</url>
      <bibkey>de-clercq-hoste-2020-absolutely</bibkey>
    </paper>
    <paper id="3">
      <title>Anaphoric Zero Pronoun Identification: A Multilingual Approach</title>
      <author><first>Abdulrahman</first><last>Aloraini</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>22&#8211;32</pages>
      <abstract>Pro-drop languages such as Arabic, Chinese, Italian or Japanese allow morphologically null but referential arguments in certain syntactic positions, called anaphoric zero-pronouns. Much NLP work on anaphoric zero-pronouns (AZP) is based on gold mentions, but models for their identification are a fundamental prerequisite for their resolution in real-life applications. Such identification requires complex language understanding and knowledge of real-world entities. Transfer learning models, such as BERT, have recently shown to learn surface, syntactic, and semantic information,which can be very useful in recognizing AZPs. We propose a BERT-based multilingual model for AZP identification from predicted zero pronoun positions, and evaluate it on the Arabic and Chinese portions of OntoNotes 5.0. As far as we know, this is the first neural network model of AZP identification for Arabic; and our approach outperforms the stateof-the-art for Chinese. Experiment results suggest that BERT implicitly encode information about AZPs through their surrounding context.</abstract>
      <url hash="3b9e3990">2020.crac-1.3</url>
      <bibkey>aloraini-poesio-2020-anaphoric</bibkey>
    </paper>
    <paper id="4">
      <title>Predicting Coreference in <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentations</title>
      <author><first>Tatiana</first><last>Anikina</last></author>
      <author><first>Alexander</first><last>Koller</last></author>
      <author><first>Michael</first><last>Roth</last></author>
      <pages>33&#8211;38</pages>
      <abstract>This work addresses coreference resolution in Abstract Meaning Representation (AMR) graphs, a popular formalism for semantic parsing. We evaluate several current coreference resolution techniques on a recently published AMR coreference corpus, establishing baselines for future work. We also demonstrate that coreference resolution can improve the accuracy of a state-of-the-art semantic parser on this corpus.</abstract>
      <url hash="d61d8939">2020.crac-1.4</url>
      <bibkey>anikina-etal-2020-predicting</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>T</fixed-case>wi<fixed-case>C</fixed-case>onv: A Coreference-annotated Corpus of <fixed-case>T</fixed-case>witter Conversations</title>
      <author><first>Berfin</first><last>Akta&#351;</last></author>
      <author><first>Annalena</first><last>Kohnert</last></author>
      <pages>47&#8211;54</pages>
      <abstract>This article introduces TwiConv, an English coreference-annotated corpus of microblog conversations from Twitter. We describe the corpus compilation process and the annotation scheme, and release the corpus publicly, along with this paper. We manually annotated nominal coreference in 1756 tweets arranged in 185 conversation threads. The annotation achieves satisfactory annotation agreement results. We also present a new method for mapping the tweet contents with distributed stand-off annotations, which can easily be adapted to different annotation tasks.</abstract>
      <url hash="ef43979d">2020.crac-1.6</url>
      <bibkey>aktas-kohnert-2020-twiconv</bibkey>
      <pwccode url="https://github.com/berfingit/twiconv" additional="false">berfingit/twiconv</pwccode>
    </paper>
    <paper id="8">
      <title>Reference to Discourse Topics: Introducing &#8220;Global&#8221; Shell Nouns</title>
      <author><first>Fabian</first><last>Simonjetz</last></author>
      <pages>68&#8211;78</pages>
      <abstract>Shell nouns (SNs) are abstract nouns like &#8220;fact&#8221;, &#8220;issue&#8221;, and &#8220;decision&#8221;, which are capable of refer- ring to non-nominal antecedents, much like anaphoric pronouns. As an extension of classical anaphora resolution, the automatic detection of SNs alongside their respective antecedents has received a growing research interest in recent years but proved to be a challenging task. This paper critically examines the assumption prevalent in previous research that SNs are typically accompanied by a specific antecedent, arguing that SNs like &#8220;issue&#8221; and &#8220;decision&#8221; are frequently used to refer, not to specific antecedents, but to global discourse topics, in which case they are out of reach of previously proposed resolution strategies that are tailored to SNs with explicit antecedents. The contribution of this work is three-fold. First, the notion of global SNs is defined; second, their qualitative and quantitative impact on previous SN research is investigated; and third, implications for previous and future approaches to SN resolution are discussed.</abstract>
      <url hash="22cdfbab">2020.crac-1.8</url>
      <bibkey>simonjetz-2020-reference</bibkey>
    </paper>
    <paper id="10">
      <title>Partially-supervised Mention Detection</title>
      <author><first>Lesly</first><last>Miculicich</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <pages>91&#8211;98</pages>
      <abstract>Learning to detect entity mentions without using syntactic information can be useful for integration and joint optimization with other tasks. However, it is common to have partially annotated data for this problem. Here, we investigate two approaches to deal with partial annotation of mentions: weighted loss and soft-target classification. We also propose two neural mention detection approaches: a sequence tagging, and an exhaustive search. We evaluate our methods with coreference resolution as a downstream task, using multitask learning. The results show that the recall and F1 score improve for all methods.</abstract>
      <url hash="bd071624">2020.crac-1.10</url>
      <bibkey>miculicich-henderson-2020-partially</bibkey>
    </paper>
    <paper id="12">
      <title>Enhanced Labelling in Active Learning for Coreference Resolution</title>
      <author><first>Vebj&#248;rn</first><last>Espeland</last></author>
      <author><first>Beatrice</first><last>Alex</last></author>
      <author><first>Benjamin</first><last>Bach</last></author>
      <pages>111&#8211;121</pages>
      <abstract>In this paper we describe our attempt to increase the amount of information that can be retrieved through active learning sessions compared to previous approaches. We optimise the annotator&#8217;s labelling process using active learning in the context of coreference resolution. Using simulated active learning experiments, we suggest three adjustments to ensure the labelling time is spent as efficiently as possible. All three adjustments provide more information to the machine learner than the baseline, though a large impact on the F1 score over time is not observed. Compared to previous models, we report a marginal F1 improvement on the final coreference models trained using for two out of the three approaches tested when applied to the English OntoNotes 2012 Coreference Resolution data. Our best-performing model achieves 58.01 F1, an increase of 0.93 F1 over the baseline model.</abstract>
      <url hash="1549d32f">2020.crac-1.12</url>
      <bibkey>espeland-etal-2020-enhanced</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
    </paper>
    <paper id="13">
      <title>Reference in Team Communication for Robot-Assisted Disaster Response: An Initial Analysis</title>
      <author><first>Natalia</first><last>Skachkova</last></author>
      <author><first>Ivana</first><last>Kruijff-Korbayova</last></author>
      <pages>122&#8211;132</pages>
      <abstract>We analyze reference phenomena in a corpus of robot-assisted disaster response team communication. The annotation scheme we designed for this purpose distinguishes different types of entities, roles, reference units and relations. We focus particularly on mission-relevant objects, locations and actors and also annotate a rich set of reference links, including co-reference and various other kinds of relations. We explain the categories used in our annotation, present their distribution in the corpus and discuss challenging cases.</abstract>
      <url hash="de0424db">2020.crac-1.13</url>
      <bibkey>skachkova-kruijff-korbayova-2020-reference</bibkey>
    </paper>
    <paper id="14">
      <title>Resolving Pronouns in <fixed-case>T</fixed-case>witter Streams: Context can Help!</title>
      <author><first>Anietie</first><last>Andy</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>133&#8211;138</pages>
      <abstract>Many people live-tweet televised events like Presidential debates and popular TV-shows and discuss people or characters in the event. Naturally, many tweets make pronominal reference to these people/characters. We propose an algorithm for resolving personal pronouns that make reference to people involved in an event, in tweet streams collected during the event.</abstract>
      <url hash="099e87b5">2020.crac-1.14</url>
      <bibkey>andy-etal-2020-resolving</bibkey>
    </paper>
    <paper id="15">
      <title>Coreference Strategies in <fixed-case>E</fixed-case>nglish-<fixed-case>G</fixed-case>erman Translation</title>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Marie-Pauline</first><last>Krielke</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <pages>139&#8211;153</pages>
      <abstract>We present a study focusing on variation of coreferential devices in English original TED talks and news texts and their German translations. Using exploratory techniques we contemplate a diverse set of coreference devices as features which we assume indicate language-specific and register-based variation as well as potential translation strategies. Our findings reflect differences on both dimensions with stronger variation along the lines of register than between languages. By exposing interactions between text type and cross-linguistic variation, they can also inform multilingual NLP applications, especially machine translation.</abstract>
      <url hash="c9f60242">2020.crac-1.15</url>
      <bibkey>lapshinova-koltunski-etal-2020-coreference</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/parcorfull">ParCorFull</pwcdataset>
    </paper>
    </volume>
</collection>