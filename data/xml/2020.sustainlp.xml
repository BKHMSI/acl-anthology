<collection id="2020.sustainlp">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</booktitle>
      <editor><first>Nafise Sadat</first><last>Moosavi</last></editor>
      <editor><first>Angela</first><last>Fan</last></editor>
      <editor><first>Vered</first><last>Shwartz</last></editor>
      <editor><first>Goran</first><last>Glava&#353;</last></editor>
      <editor><first>Shafiq</first><last>Joty</last></editor>
      <editor><first>Alex</first><last>Wang</last></editor>
      <editor><first>Thomas</first><last>Wolf</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="29071254">2020.sustainlp-1.0</url>
      <bibkey>sustainlp-2020-sustainlp</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Knowing Right from Wrong: Should We Use More Complex Models for Automatic Short-Answer Scoring in <fixed-case>B</fixed-case>ahasa <fixed-case>I</fixed-case>ndonesia?</title>
      <author><first>Ali Akbar</first><last>Septiandri</last></author>
      <author><first>Yosef Ardhito</first><last>Winatmoko</last></author>
      <author><first>Ilham Firdausi</first><last>Putra</last></author>
      <pages>1&#8211;7</pages>
      <abstract>We compare three solutions to UKARA 1.0 challenge on automated short-answer scoring: single classical, ensemble classical, and deep learning. The task is to classify given answers to two questions, whether they are right or wrong. While recent development shows increasing model complexity to push the benchmark performances, they tend to be resource-demanding with mundane improvement. For the UKARA task, we found that bag-of-words and classical machine learning approaches can compete with ensemble models and Bi-LSTM model with pre-trained word2vec embedding from 200 million words. In this case, the single classical machine learning achieved less than 2% difference in F1 compared to the deep learning approach with 1/18 time for model training.</abstract>
      <url hash="737436b0">2020.sustainlp-1.1</url>
      <doi>10.18653/v1/2020.sustainlp-1.1</doi>
      <video href="https://slideslive.com/38939419" />
      <bibkey>septiandri-etal-2020-knowing</bibkey>
    </paper>
    <paper id="3">
      <title>Learning Informative Representations of Biomedical Relations with Latent Variable Models</title>
      <author><first>Harshil</first><last>Shah</last></author>
      <author><first>Julien</first><last>Fauqueur</last></author>
      <pages>19&#8211;28</pages>
      <abstract>Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence (mention-level) or across an entire corpus (pair-level). In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.</abstract>
      <url hash="cbaf98a3">2020.sustainlp-1.3</url>
      <doi>10.18653/v1/2020.sustainlp-1.3</doi>
      <video href="https://slideslive.com/38939422" />
      <bibkey>shah-fauqueur-2020-learning</bibkey>
      <pwccode url="https://github.com/BenevolentAI/RELVM" additional="false">BenevolentAI/RELVM</pwccode>
    </paper>
    <paper id="4">
      <title>End to End Binarized Neural Networks for Text Classification</title>
      <author><first>Kumar</first><last>Shridhar</last></author>
      <author><first>Harshil</first><last>Jain</last></author>
      <author><first>Akshat</first><last>Agarwal</last></author>
      <author><first>Denis</first><last>Kleyko</last></author>
      <pages>29&#8211;34</pages>
      <abstract>Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. A particular concern is that these networks pose high requirements for computing hardware and training budgets. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a network is one way of addressing the issue of the increasing complexity. In this paper, we propose an end to end binarized neural network for the task of intent and text classification. In order to fully utilize the potential of end to end binarization, both the input representations (vector embeddings of tokens statistics) and the classifier are binarized. We demonstrate the efficiency of such a network on the intent classification of short texts over three datasets and text classification with a larger dataset. On the considered datasets, the proposed network achieves comparable to the state-of-the-art results while utilizing 20-40% lesser memory and training time compared to the benchmarks.</abstract>
      <url hash="ab07164c">2020.sustainlp-1.4</url>
      <attachment type="OptionalSupplementaryMaterial" hash="1f601b55">2020.sustainlp-1.4.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.sustainlp-1.4</doi>
      <video href="https://slideslive.com/38939423" />
      <bibkey>shridhar-etal-2020-end</bibkey>
    </paper>
    <paper id="5">
      <title>Exploring the Boundaries of Low-Resource <fixed-case>BERT</fixed-case> Distillation</title>
      <author><first>Moshe</first><last>Wasserblat</last></author>
      <author><first>Oren</first><last>Pereg</last></author>
      <author><first>Peter</first><last>Izsak</last></author>
      <pages>35&#8211;40</pages>
      <abstract>In recent years, large pre-trained models have demonstrated state-of-the-art performance in many of NLP tasks. However, the deployment of these models on devices with limited resources is challenging due to the models&#8217; large computational consumption and memory requirements. Moreover, the need for a considerable amount of labeled training data also hinders real-world deployment scenarios. Model distillation has shown promising results for reducing model size, computational load and data efficiency. In this paper we test the boundaries of BERT model distillation in terms of model compression, inference efficiency and data scarcity. We show that classification tasks that require the capturing of general lexical semantics can be successfully distilled by very simple and efficient models and require relatively small amount of labeled training data. We also show that the distillation of large pre-trained models is more effective in real-life scenarios where limited amounts of labeled training are available.</abstract>
      <url hash="fd7284b7">2020.sustainlp-1.5</url>
      <doi>10.18653/v1/2020.sustainlp-1.5</doi>
      <video href="https://slideslive.com/38939426" />
      <bibkey>wasserblat-etal-2020-exploring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emotion">CARER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="6">
      <title>Efficient Estimation of Influence of a Training Instance</title>
      <author><first>Sosuke</first><last>Kobayashi</last></author>
      <author><first>Sho</first><last>Yokoi</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>41&#8211;47</pages>
      <abstract>Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model&#8217;s prediction would be changed if a training instance were not used. In this paper, we propose an efficient method for estimating the influence. Our method is inspired by dropout, which zero-masks a sub-network and prevents the sub-network from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving generalization.</abstract>
      <url hash="472a0c08">2020.sustainlp-1.6</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2924916b">2020.sustainlp-1.6.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.sustainlp-1.6</doi>
      <video href="https://slideslive.com/38939427" />
      <bibkey>kobayashi-etal-2020-efficient</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cifar-10">CIFAR-10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="7">
      <title>Efficient Inference For Neural Machine Translation</title>
      <author><first>Yi-Te</first><last>Hsu</last></author>
      <author><first>Sarthak</first><last>Garg</last></author>
      <author><first>Yi-Hsiu</first><last>Liao</last></author>
      <author><first>Ilya</first><last>Chatsviorkin</last></author>
      <pages>48&#8211;53</pages>
      <abstract>Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.</abstract>
      <url hash="7ad2b4df">2020.sustainlp-1.7</url>
      <doi>10.18653/v1/2020.sustainlp-1.7</doi>
      <video href="https://slideslive.com/38939429" />
      <bibkey>hsu-etal-2020-efficient</bibkey>
    </paper>
    <paper id="8">
      <title>Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm</title>
      <author><first>Alicia</first><last>Tsai</last></author>
      <author><first>Laurent</first><last>El Ghaoui</last></author>
      <pages>54&#8211;62</pages>
      <abstract>We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem. We solve it using a dedicated Frank-Wolfe algorithm. To generate a summary with k sentences, the algorithm only needs to execute approximately k iterations, making it very efficient for a long document. We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones. Our method achieves better results with both datasets and works especially well when combined with embeddings for highly paraphrased summaries.</abstract>
      <url hash="b22dc89c">2020.sustainlp-1.8</url>
      <attachment type="OptionalSupplementaryMaterial" hash="957e4570">2020.sustainlp-1.8.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.sustainlp-1.8</doi>
      <video href="https://slideslive.com/38939430" />
      <bibkey>tsai-el-ghaoui-2020-sparse</bibkey>
    </paper>
    <paper id="10">
      <title>A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings</title>
      <author><first>Cennet</first><last>Oguz</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>73&#8211;82</pages>
      <abstract>Learning-based slot filling - a key component of spoken language understanding systems - typically requires a large amount of in-domain hand-labeled data for training. In this paper, we propose a novel two-stage model architecture that can be trained with only a few in-domain hand-labeled examples. The first step is designed to remove non-slot tokens (i.e., O labeled tokens), as they introduce noise in the input of slot filling models. This step is domain-agnostic and therefore, can be trained by exploiting out-of-domain data. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as ELMO and BERT. We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset.</abstract>
      <url hash="ab915359">2020.sustainlp-1.10</url>
      <doi>10.18653/v1/2020.sustainlp-1.10</doi>
      <video href="https://slideslive.com/38939432" />
      <bibkey>oguz-vu-2020-two</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="11">
      <title>Early Exiting <fixed-case>BERT</fixed-case> for Efficient Document Ranking</title>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Rodrigo</first><last>Nogueira</last></author>
      <author><first>Yaoliang</first><last>Yu</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>83&#8211;88</pages>
      <abstract>Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for document ranking. With a slight modification, BERT becomes a model with multiple output paths, and each inference sample can exit early from these paths. In this way, computation can be effectively allocated among samples, and overall system latency is significantly reduced while the original quality is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x inference speedup with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.</abstract>
      <url hash="58f3e0f0">2020.sustainlp-1.11</url>
      <doi>10.18653/v1/2020.sustainlp-1.11</doi>
      <video href="https://slideslive.com/38939433" />
      <bibkey>xin-etal-2020-early</bibkey>
      <pwccode url="https://github.com/castorini/earlyexiting-monobert" additional="false">castorini/earlyexiting-monobert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="14">
      <title>A Little Bit Is Worse Than None: Ranking with Limited Training Data</title>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Andrew</first><last>Yates</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>107&#8211;112</pages>
      <abstract>Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC. We first answer the question: How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that &#8220;some&#8221; labeled in-domain data can be worse than none at all.</abstract>
      <url hash="8f1f5096">2020.sustainlp-1.14</url>
      <doi>10.18653/v1/2020.sustainlp-1.14</doi>
      <video href="https://slideslive.com/38939436" />
      <bibkey>zhang-etal-2020-little</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="16">
      <title>Load What You Need: Smaller Versions of Mutililingual <fixed-case>BERT</fixed-case></title>
      <author><first>Amine</first><last>Abdaoui</last></author>
      <author><first>Camille</first><last>Pradel</last></author>
      <author><first>Gr&#233;goire</first><last>Sigel</last></author>
      <pages>119&#8211;123</pages>
      <abstract>Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to extract smaller models that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller models that keep comparable results, while reducing up to 45% of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, distillation induced a 1.7% to 6% drop in the overall accuracy on the XNLI data set. The presented models and code are publicly available.</abstract>
      <url hash="f74dd40a">2020.sustainlp-1.16</url>
      <doi>10.18653/v1/2020.sustainlp-1.16</doi>
      <video href="https://slideslive.com/38939438" />
      <bibkey>abdaoui-etal-2020-load</bibkey>
      <pwccode url="https://github.com/Geotrend-research/smaller-transformers" additional="false">Geotrend-research/smaller-transformers</pwccode>
    </paper>
    <paper id="19">
      <title>Towards Accurate and Reliable Energy Measurement of <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Qingqing</first><last>Cao</last></author>
      <author><first>Aruna</first><last>Balasubramanian</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <pages>141&#8211;148</pages>
      <abstract>Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption. We conduct energy measurement experiments with four different models for a question answering task. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and energy consumption. We release the code and data at https://github.com/csarron/sustainlp2020-energy.</abstract>
      <url hash="8fd7694b">2020.sustainlp-1.19</url>
      <doi>10.18653/v1/2020.sustainlp-1.19</doi>
      <video href="https://slideslive.com/38939441" />
      <bibkey>cao-etal-2020-towards</bibkey>
      <pwccode url="https://github.com/csarron/sustainlp2020-energy" additional="false">csarron/sustainlp2020-energy</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="24">
      <title>Overview of the <fixed-case>S</fixed-case>ustai<fixed-case>NLP</fixed-case> 2020 Shared Task</title>
      <author><first>Alex</first><last>Wang</last></author>
      <author><first>Thomas</first><last>Wolf</last></author>
      <pages>174&#8211;178</pages>
      <abstract>We describe the SustaiNLP 2020 shared task: efficient inference on the SuperGLUE benchmark (Wang et al., 2019). Participants are evaluated based on performance on the benchmark as well as energy consumed in making predictions on the test sets. We describe the task, its organization, and the submitted systems. Across the six submissions to the shared task, participants achieved efficiency gains of 20&#215; over a standard BERT (Devlin et al., 2019) baseline, while losing less than an absolute point in performance.</abstract>
      <url hash="79668c0d">2020.sustainlp-1.24</url>
      <doi>10.18653/v1/2020.sustainlp-1.24</doi>
      <bibkey>wang-wolf-2020-overview</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/record">ReCoRD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
  </volume>
</collection>