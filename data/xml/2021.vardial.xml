<collection id="2021.vardial">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects</booktitle>
      <editor><first>Marcos</first><last>Zampieri</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Nikola</first><last>Ljube&#353;i&#263;</last></editor>
      <editor><first>J&#246;rg</first><last>Tiedemann</last></editor>
      <editor><first>Yves</first><last>Scherrer</last></editor>
      <editor><first>Tommi</first><last>Jauhiainen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Kiyv, Ukraine</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="5dcc52a6">2021.vardial-1.0</url>
      <bibkey>vardial-2021-nlp</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Hierarchical Transformer for Multilingual Machine Translation</title>
      <author><first>Albina</first><last>Khusainova</last></author>
      <author><first>Adil</first><last>Khan</last></author>
      <author><first>Ad&#237;n Ram&#237;rez</first><last>Rivera</last></author>
      <author><first>Vitaly</first><last>Romanov</last></author>
      <pages>12&#8211;20</pages>
      <abstract>The choice of parameter sharing strategy in multilingual machine translation models determines how optimally parameter space is used and hence, directly influences ultimate translation quality. Inspired by linguistic trees that show the degree of relatedness between different languages, the new general approach to parameter sharing in multilingual machine translation was suggested recently. The main idea is to use these expert language hierarchies as a basis for multilingual architecture: the closer two languages are, the more parameters they share. In this work, we test this idea using the Transformer architecture and show that despite the success in previous work there are problems inherent to training such hierarchical models. We demonstrate that in case of carefully chosen training strategy the hierarchical architecture can outperform bilingual models and multilingual models with full parameter sharing.</abstract>
      <url hash="9548e505">2021.vardial-1.2</url>
      <bibkey>khusainova-etal-2021-hierarchical</bibkey>
    </paper>
    <paper id="4">
      <title>Representations of Language Varieties Are Reliable Given Corpus Similarity Measures</title>
      <author><first>Jonathan</first><last>Dunn</last></author>
      <pages>28&#8211;38</pages>
      <abstract>This paper measures similarity both within and between 84 language varieties across nine languages. These corpora are drawn from digital sources (the web and tweets), allowing us to evaluate whether such geo-referenced corpora are reliable for modelling linguistic variation. The basic idea is that, if each source adequately represents a single underlying language variety, then the similarity between these sources should be stable across all languages and countries. The paper shows that there is a consistent agreement between these sources using frequency-based corpus similarity measures. This provides further evidence that digital geo-referenced corpora consistently represent local language varieties.</abstract>
      <url hash="b64e4e4d">2021.vardial-1.4</url>
      <bibkey>dunn-2021-representations</bibkey>
    </paper>
    <paper id="5">
      <title>Whit&#8217;s the Richt Pairt o Speech: <fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> tagging for <fixed-case>S</fixed-case>cots</title>
      <author><first>Harm</first><last>Lameris</last></author>
      <author><first>Sara</first><last>Stymne</last></author>
      <pages>39&#8211;48</pages>
      <abstract>In this paper we explore PoS tagging for the Scots language. Scots is spoken in Scotland and Northern Ireland, and is closely related to English. As no linguistically annotated Scots data were available, we manually PoS tagged a small set that is used for evaluation and training. We use English as a transfer language to examine zero-shot transfer and transfer learning methods. We find that training on a very small amount of Scots data was superior to zero-shot transfer from English. Combining the Scots and English data led to further improvements, with a concatenation method giving the best results. We also compared the use of two different English treebanks and found that a treebank containing web data was superior in the zero-shot setting, while it was outperformed by a treebank containing a mix of genres when combined with Scots data.</abstract>
      <url hash="ce78f7c0">2021.vardial-1.5</url>
      <bibkey>lameris-stymne-2021-whits</bibkey>
    </paper>
    <paper id="8">
      <title>Discriminating Between Similar Nordic Languages</title>
      <author><first>Ren&#233;</first><last>Haas</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <pages>67&#8211;75</pages>
      <abstract>Automatic language identification is a challenging problem. Discriminating between closely related languages is especially difficult. This paper presents a machine learning approach for automatic language identification for the Nordic languages, which often suffer miscategorisation by existing state-of-the-art tools. Concretely we will focus on discrimination between six Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokm&#229;l), Faroese and Icelandic.</abstract>
      <url hash="19384313">2021.vardial-1.8</url>
      <bibkey>haas-derczynski-2021-discriminating</bibkey>
      <pwccode url="https://github.com/StrombergNLP/NordicDSL" additional="true">StrombergNLP/NordicDSL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nordic-langid">nordic_langid</pwcdataset>
    </paper>
    <paper id="11">
      <title>Optimizing a Supervised Classifier for a Difficult Language Identification Problem</title>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>96&#8211;101</pages>
      <abstract>This paper describes the system developed by the Laboratoire d&#8217;analyse statistique des textes for the Dravidian Language Identification (DLI) shared task of VarDial 2021. This task is particularly difficult because the materials consists of short YouTube comments, written in Roman script, from three closely related Dravidian languages, and a fourth category consisting of several other languages in varying proportions, all mixed with English. The proposed system is made up of a logistic regression model which uses as only features n-grams of characters with a maximum length of 5. After its optimization both in terms of the feature weighting and the classifier parameters, it ranked first in the challenge. The additional analyses carried out underline the importance of optimization, especially when the measure of effectiveness is the Macro-F1.</abstract>
      <url hash="2f5da910">2021.vardial-1.11</url>
      <bibkey>bestgen-2021-optimizing</bibkey>
    </paper>
    <paper id="14">
      <title>Comparing Approaches to <fixed-case>D</fixed-case>ravidian Language Identification</title>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>120&#8211;127</pages>
      <abstract>This paper describes the submissions by team HWR to the Dravidian Language Identification (DLI) shared task organized at VarDial 2021 workshop. The DLI training set includes 16,674 YouTube comments written in Roman script containing code-mixed text with English and one of the three South Dravidian languages: Kannada, Malayalam, and Tamil. We submitted results generated using two models, a Naive Bayes classifier with adaptive language models, which has shown to obtain competitive performance in many language and dialect identification tasks, and a transformer-based model which is widely regarded as the state-of-the-art in a number of NLP tasks. Our first submission was sent in the closed submission track using only the training set provided by the shared task organisers, whereas the second submission is considered to be open as it used a pretrained model trained with external data. Our team attained shared second position in the shared task with the submission based on Naive Bayes. Our results reinforce the idea that deep learning methods are not as competitive in language identification related tasks as they are in many other text classification tasks.</abstract>
      <url hash="19975f07">2021.vardial-1.14</url>
      <bibkey>jauhiainen-etal-2021-comparing</bibkey>
    </paper>
    </volume>
</collection>