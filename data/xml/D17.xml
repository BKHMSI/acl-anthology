<?xml version='1.0' encoding='utf-8'?>
<collection id="D17">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</booktitle>
      <url hash="6382c144">D17-1</url>
      <editor><first>Martha</first><last>Palmer</last></editor>
      <editor><first>Rebecca</first><last>Hwa</last></editor>
      <editor><first>Sebastian</first><last>Riedel</last></editor>
      <doi>10.18653/v1/D17-1</doi>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Copenhagen, Denmark</address>
      <month>September</month>
      <year>2017</year>
    </meta>
    <frontmatter>
      <url hash="aa5988be">D17-1000</url>
      <bibkey>emnlp-2017-2017</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Monolingual Phrase Alignment on Parse Forests</title>
      <author><first>Yuki</first> <last>Arase</last></author>
      <author><first>Junichi</first> <last>Tsujii</last></author>
      <pages>1–11</pages>
      <url hash="fb6e14e5">D17-1001</url>
      <doi>10.18653/v1/D17-1001</doi>
      <attachment type="attachment" hash="6beb5674">D17-1001.Attachment.zip</attachment>
      <abstract>We propose an efficient method to conduct phrase alignment on <a href="https://en.wikipedia.org/wiki/Parse_forest">parse forests</a> for <a href="https://en.wikipedia.org/wiki/Paraphrase_detection">paraphrase detection</a>. Unlike previous studies, our method identifies syntactic paraphrases under linguistically motivated grammar. In addition, it allows phrases to non-compositionally align to handle <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> with non-homographic phrase correspondences. A <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> that provides gold parse trees and their phrase alignments is created. The experimental results confirm that the proposed method conducts highly accurate phrase alignment compared to human performance.</abstract>
      <video href="https://vimeo.com/238234373" />
      <bibkey>arase-tsujii-2017-monolingual</bibkey>
    </paper>
    <paper id="5">
      <title>Heterogeneous Supervision for Relation Extraction : A Representation Learning Approach</title>
      <author><first>Liyuan</first> <last>Liu</last></author>
      <author><first>Xiang</first> <last>Ren</last></author>
      <author><first>Qi</first> <last>Zhu</last></author>
      <author><first>Shi</first> <last>Zhi</last></author>
      <author><first>Huan</first> <last>Gui</last></author>
      <author><first>Heng</first> <last>Ji</last></author>
      <author><first>Jiawei</first> <last>Han</last></author>
      <pages>46–56</pages>
      <url hash="4bade836">D17-1005</url>
      <doi>10.18653/v1/D17-1005</doi>
      <abstract>Relation extraction is a fundamental task in <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a>. Most existing methods have heavy reliance on <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task : how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>.</abstract>
      <video href="https://vimeo.com/238228823" />
      <bibkey>liu-etal-2017-heterogeneous</bibkey>
      <pwccode url="https://github.com/LiyuanLucasLiu/ReHession" additional="false">LiyuanLucasLiu/ReHession</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="7">
      <title>Entity Linking for Queries by Searching Wikipedia Sentences<fixed-case>W</fixed-case>ikipedia Sentences</title>
      <author><first>Chuanqi</first> <last>Tan</last></author>
      <author><first>Furu</first> <last>Wei</last></author>
      <author><first>Pengjie</first> <last>Ren</last></author>
      <author><first>Weifeng</first> <last>Lv</last></author>
      <author><first>Ming</first> <last>Zhou</last></author>
      <pages>68–77</pages>
      <url hash="c5b45f68">D17-1007</url>
      <doi>10.18653/v1/D17-1007</doi>
      <abstract>We present a simple yet effective approach for linking entities in queries. The key idea is to search sentences similar to a query from Wikipedia articles and directly use the human-annotated entities in the similar sentences as candidate entities for the query. Then, we employ a rich set of features, such as link-probability, context-matching, word embeddings, and relatedness among candidate entities as well as their related entities, to rank the candidates under a regression based framework. The advantages of our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> lie in two aspects, which contribute to the ranking process and final linking result. First, it can greatly reduce the number of candidate entities by filtering out irrelevant entities with the words in the query. Second, we can obtain the query sensitive prior probability in addition to the static link-probability derived from all Wikipedia articles. We conduct experiments on two benchmark datasets on <a href="https://en.wikipedia.org/wiki/Entity_linking">entity linking</a> for queries, namely the ERD14 dataset and the GERDAQ dataset. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art systems</a> and yields 75.0 % in F1 on the ERD14 dataset and 56.9 % on the GERDAQ dataset.</abstract>
      <video href="https://vimeo.com/238228743" />
      <bibkey>tan-etal-2017-entity</bibkey>
    </paper>
    <paper id="8">
      <title>Train-O-Matic : Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data<fixed-case>O</fixed-case>-<fixed-case>M</fixed-case>atic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data</title>
      <author><first>Tommaso</first> <last>Pasini</last></author>
      <author><first>Roberto</first> <last>Navigli</last></author>
      <pages>78–88</pages>
      <url hash="04bc1d97">D17-1008</url>
      <doi>10.18653/v1/D17-1008</doi>
      <attachment type="attachment" hash="addb84a1">D17-1008.Attachment.zip</attachment>
      <abstract>Annotating large numbers of sentences with senses is the heaviest requirement of current <a href="https://en.wikipedia.org/wiki/Word-sense_disambiguation">Word Sense Disambiguation</a>. We present Train-O-Matic, a language-independent method for generating millions of sense-annotated training instances for virtually all meanings of words in a language’s vocabulary. The approach is fully automatic : no human intervention is required and the only type of <a href="https://en.wikipedia.org/wiki/Knowledge">human knowledge</a> used is a WordNet-like resource. Train-O-Matic achieves consistently state-of-the-art performance across gold standard datasets and languages, while at the same time removing the burden of manual annotation. All the <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training data</a> is available for research purposes at.<url>http://trainomatic.org</url>. </abstract>
      <video href="https://vimeo.com/238236475" />
      <bibkey>pasini-navigli-2017-train</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/senseval-2-1">Senseval-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/united-nations-parallel-corpus">United Nations Parallel Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="9">
      <title>Universal Semantic Parsing</title>
      <author><first>Siva</first> <last>Reddy</last></author>
      <author><first>Oscar</first> <last>Täckström</last></author>
      <author><first>Slav</first> <last>Petrov</last></author>
      <author><first>Mark</first> <last>Steedman</last></author>
      <author><first>Mirella</first> <last>Lapata</last></author>
      <pages>89–101</pages>
      <url hash="254916a6">D17-1009</url>
      <doi>10.18653/v1/D17-1009</doi>
      <attachment type="attachment" hash="ac425f22">D17-1009.Attachment.pdf</attachment>
      <abstract>Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a> can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to <a href="https://en.wikipedia.org/wiki/English_language">English</a>, and can not process <a href="https://en.wikipedia.org/wiki/Dependency_graph">dependency graphs</a>, which allow handling complex phenomena such as <a href="https://en.wikipedia.org/wiki/Control_flow">control</a>. In this work, we introduce UDepLambda, a semantic interface for UD, which maps <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> against <a href="https://en.wikipedia.org/wiki/Freebase">Freebase</a> and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> achieves a 4.9 F1 point improvement over the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> on GraphQuestions.</abstract>
      <video href="https://vimeo.com/238236598" />
      <bibkey>reddy-etal-2017-universal</bibkey>
      <pwccode url="https://github.com/sivareddyg/udeplambda" additional="false">sivareddyg/udeplambda</pwccode>
    </paper>
    <paper id="10">
      <title>Mimicking Word Embeddings using Subword RNNs<fixed-case>RNN</fixed-case>s</title>
      <author><first>Yuval</first> <last>Pinter</last></author>
      <author><first>Robert</first> <last>Guthrie</last></author>
      <author><first>Jacob</first> <last>Eisenstein</last></author>
      <pages>102–112</pages>
      <url hash="8c6d54fa">D17-1010</url>
      <doi>10.18653/v1/D17-1010</doi>
      <abstract>Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data. However, the effectiveness of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist. In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a <a href="https://en.wikipedia.org/wiki/Function_(mathematics)">function</a> from spellings to distributional embeddings. Unlike prior work, MIMICK does not require re-training on the original word embedding corpus ; instead, <a href="https://en.wikipedia.org/wiki/Machine_learning">learning</a> is performed at the type level. Intrinsic and extrinsic evaluations demonstrate the power of this simple approach. On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes. It is competitive with (and complementary to) a supervised character-based model in low resource settings.</abstract>
      <video href="https://vimeo.com/238234299" />
      <attachment type="presentation" hash="0bda3bea">D17-1010.Presentation.pdf</attachment>
      <bibkey>pinter-etal-2017-mimicking</bibkey>
      <pwccode url="https://github.com/yuvalpinter/Mimick" additional="true">yuvalpinter/Mimick</pwccode>
    </paper>
    <paper id="12">
      <title>Neural Machine Translation with Source-Side Latent Graph Parsing</title>
      <author><first>Kazuma</first> <last>Hashimoto</last></author>
      <author><first>Yoshimasa</first> <last>Tsuruoka</last></author>
      <pages>125–135</pages>
      <url hash="112c9bf5">D17-1012</url>
      <doi>10.18653/v1/D17-1012</doi>
      <abstract>This paper presents a novel neural machine translation model which jointly learns <a href="https://en.wikipedia.org/wiki/Translation">translation</a> and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, and thus the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> is optimized according to the translation objective. In experiments, we first show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> compares favorably with state-of-the-art sequential and pipelined syntax-based NMT models. We also show that the performance of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can be further improved by pre-training it with a small amount of treebank annotations. Our final <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble model</a> significantly outperforms the previous best models on the standard English-to-Japanese translation dataset.</abstract>
      <video href="https://vimeo.com/238234769" />
      <bibkey>hashimoto-tsuruoka-2017-neural</bibkey>
    </paper>
    <paper id="13">
      <title>Neural Machine Translation with Word Predictions</title>
      <author><first>Rongxiang</first> <last>Weng</last></author>
      <author><first>Shujian</first> <last>Huang</last></author>
      <author><first>Zaixiang</first> <last>Zheng</last></author>
      <author><first>Xinyu</first> <last>Dai</last></author>
      <author><first>Jiajun</first> <last>Chen</last></author>
      <pages>136–145</pages>
      <url hash="469dce79">D17-1013</url>
      <doi>10.18653/v1/D17-1013</doi>
      <abstract>In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and <a href="https://en.wikipedia.org/wiki/Decoder">decoder</a> carry the crucial information about the sentence. These <a href="https://en.wikipedia.org/wiki/Euclidean_vector">vectors</a> are generated by parameters which are updated by back-propagation of translation errors through time. We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. In this paper, we propose to use <a href="https://en.wikipedia.org/wiki/Word_prediction">word predictions</a> as a mechanism for direct supervision. More specifically, we require these <a href="https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)">vectors</a> to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and <a href="https://en.wikipedia.org/wiki/Code">decoder</a> without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English machine translation task show an average BLEU improvement by 4.53, respectively.</abstract>
      <bibkey>weng-etal-2017-neural</bibkey>
    </paper>
    <paper id="14">
      <title>Towards Decoding as Continuous Optimisation in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a></title>
      <author><first>Cong Duy Vu</first> <last>Hoang</last></author>
      <author><first>Gholamreza</first> <last>Haffari</last></author>
      <author><first>Trevor</first> <last>Cohn</last></author>
      <pages>146–156</pages>
      <url hash="dc97ea3c">D17-1014</url>
      <doi>10.18653/v1/D17-1014</doi>
      <attachment type="attachment" hash="e34782a0">D17-1014.Attachment.pdf</attachment>
      <abstract>We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. We reformulate decoding, a discrete optimization problem, into a continuous problem, such that <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">optimization</a> can make use of efficient gradient-based techniques. Our powerful decoding framework allows for more accurate decoding for standard neural machine translation models, as well as enabling decoding in intractable models such as intersection of several different NMT models. Our empirical results show that our decoding framework is effective, and can leads to substantial improvements in translations, especially in situations where <a href="https://en.wikipedia.org/wiki/Greedy_search">greedy search</a> and <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> are not feasible. Finally, we show how the <a href="https://en.wikipedia.org/wiki/Methodology">technique</a> is highly competitive with, and complementary to, <a href="https://en.wikipedia.org/wiki/Ranking">reranking</a>.</abstract>
      <video href="https://vimeo.com/238236392" />
      <bibkey>hoang-etal-2017-towards</bibkey>
    </paper>
    <paper id="15">
      <title>Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space</title>
      <author><first>Nikita</first> <last>Kitaev</last></author>
      <author><first>Dan</first> <last>Klein</last></author>
      <pages>157–166</pages>
      <url hash="6f4217ce">D17-1015</url>
      <doi>10.18653/v1/D17-1015</doi>
      <abstract>We present a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for locating regions in space based on <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language descriptions</a>. Starting with a 3D scene and a sentence, our model is able to associate words in the sentence with regions in the scene, interpret relations such as ‘on top of’ or ‘next to,’ and finally locate the region described in the sentence. All <a href="https://en.wikipedia.org/wiki/Component-based_software_engineering">components</a> form a single <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> that is trained end-to-end without prior knowledge of object segmentation. To evaluate our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, we construct and release a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> consisting of Minecraft scenes with <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourced natural language descriptions</a>. We achieve a 32 % relative error reduction compared to a strong neural baseline.</abstract>
      <video href="https://vimeo.com/238230308" />
      <bibkey>kitaev-klein-2017-misty</bibkey>
      <pwccode url="https://github.com/nikitakit/voxelworld" additional="false">nikitakit/voxelworld</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
    </paper>
    <paper id="16">
      <title>Continuous Representation of Location for <a href="https://en.wikipedia.org/wiki/Geolocation">Geolocation</a> and Lexical Dialectology using Mixture Density Networks</title>
      <author><first>Afshin</first> <last>Rahimi</last></author>
      <author><first>Timothy</first> <last>Baldwin</last></author>
      <author><first>Trevor</first> <last>Cohn</last></author>
      <pages>167–176</pages>
      <url hash="36cba80b">D17-1016</url>
      <doi>10.18653/v1/D17-1016</doi>
      <abstract>We propose a method for embedding two-dimensional locations in a continuous vector space using a neural network-based model incorporating mixtures of Gaussian distributions, presenting two model variants for text-based geolocation and lexical dialectology. Evaluated over <a href="https://en.wikipedia.org/wiki/Twitter">Twitter data</a>, the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms conventional regression-based geolocation and provides a better <a href="https://en.wikipedia.org/wiki/Measurement_uncertainty">estimate of uncertainty</a>. We also show the effectiveness of the <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representation</a> for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset.</abstract>
      <video href="https://vimeo.com/238228698" />
      <bibkey>rahimi-etal-2017-continuous</bibkey>
      <pwccode url="https://github.com/afshinrahimi/geomdn" additional="false">afshinrahimi/geomdn</pwccode>
    </paper>
    <paper id="17">
      <title>Obj2Text : Generating Visually Descriptive Language from Object Layouts<fixed-case>O</fixed-case>bj2<fixed-case>T</fixed-case>ext: Generating Visually Descriptive Language from Object Layouts</title>
      <author><first>Xuwang</first> <last>Yin</last></author>
      <author><first>Vicente</first> <last>Ordonez</last></author>
      <pages>177–187</pages>
      <url hash="76c6d54f">D17-1017</url>
      <doi>10.18653/v1/D17-1017</doi>
      <abstract>Generating captions for <a href="https://en.wikipedia.org/wiki/Digital_image">images</a> is a task that has recently received considerable attention. Another type of visual inputs are abstract scenes or object layouts where the only information provided is a set of objects and their locations. This type of <a href="https://en.wikipedia.org/wiki/Imagery">imagery</a> is commonly found in many applications in <a href="https://en.wikipedia.org/wiki/Computer_graphics">computer graphics</a>, <a href="https://en.wikipedia.org/wiki/Virtual_reality">virtual reality</a>, and <a href="https://en.wikipedia.org/wiki/Storyboard">storyboarding</a>. We explore in this paper OBJ2TEXT, a sequence-to-sequence model that encodes a set of objects and their locations as an input sequence using an LSTM network, and decodes this representation using an LSTM language model. We show in our paper that this model despite using a sequence encoder can effectively represent complex spatial object-object relationships and produce descriptions that are globally coherent and semantically relevant. We test our approach for the task of describing object layouts in the MS-COCO dataset by producing sentences given only object annotations. We additionally show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> combined with a state-of-the-art object detector can improve the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of an image captioning model.</abstract>
      <video href="https://vimeo.com/238230148" />
      <bibkey>yin-ordonez-2017-obj2text</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="18">
      <title>End-to-end Neural Coreference Resolution</title>
      <author><first>Kenton</first> <last>Lee</last></author>
      <author><first>Luheng</first> <last>He</last></author>
      <author><first>Mike</first> <last>Lewis</last></author>
      <author><first>Luke</first> <last>Zettlemoyer</last></author>
      <pages>188–197</pages>
      <url hash="2f8cf056">D17-1018</url>
      <doi>10.18653/v1/D17-1018</doi>
      <abstract>We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.</abstract>
      <video href="https://vimeo.com/238232979" />
      <bibkey>lee-etal-2017-end</bibkey>
      <pwccode url="https://github.com/kentonl/e2e-coref" additional="true">kentonl/e2e-coref</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="19">
      <title>Neural Net Models of Open-domain Discourse Coherence</title>
      <author><first>Jiwei</first> <last>Li</last></author>
      <author><first>Dan</first> <last>Jurafsky</last></author>
      <pages>198–209</pages>
      <url hash="f865769d">D17-1019</url>
      <doi>10.18653/v1/D17-1019</doi>
      <abstract>Discourse coherence is strongly associated with text quality, making it important to <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language generation and understanding</a>. Yet existing models of coherence focus on measuring individual aspects of <a href="https://en.wikipedia.org/wiki/Coherence_(linguistics)">coherence</a> (lexical overlap, rhetorical structure, entity centering) in narrow domains. In this paper, we describe domain-independent neural models of discourse coherence that are capable of measuring multiple aspects of <a href="https://en.wikipedia.org/wiki/Coherence_(linguistics)">coherence</a> in existing sentences and can maintain <a href="https://en.wikipedia.org/wiki/Coherence_(linguistics)">coherence</a> while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latent-variable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts.</abstract>
      <video href="https://vimeo.com/238234840" />
      <bibkey>li-jurafsky-2017-neural</bibkey>
    </paper>
    <paper id="20">
      <title>Affinity-Preserving Random Walk for Multi-Document Summarization</title>
      <author><first>Kexiang</first> <last>Wang</last></author>
      <author><first>Tianyu</first> <last>Liu</last></author>
      <author><first>Zhifang</first> <last>Sui</last></author>
      <author><first>Baobao</first> <last>Chang</last></author>
      <pages>210–220</pages>
      <url hash="56c66f70">D17-1020</url>
      <doi>10.18653/v1/D17-1020</doi>
      <abstract>Multi-document summarization provides users with a short text that summarizes the information in a set of related documents. This paper introduces affinity-preserving random walk to the summarization task, which preserves the affinity relations of sentences by an absorbing <a href="https://en.wikipedia.org/wiki/Random_walk_model">random walk model</a>. Meanwhile, we put forward adjustable affinity-preserving random walk to enforce the diversity constraint of <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a> in the random walk process. The ROUGE evaluations on DUC 2003 topic-focused summarization task and DUC 2004 generic summarization task show the good performance of our method, which has the best ROUGE-2 recall among the graph-based ranking methods.</abstract>
      <video href="https://vimeo.com/238231488" />
      <bibkey>wang-etal-2017-affinity</bibkey>
    </paper>
    <paper id="22">
      <title>Hierarchical Embeddings for Hypernymy Detection and Directionality</title>
      <author><first>Kim Anh</first> <last>Nguyen</last></author>
      <author><first>Maximilian</first> <last>Köper</last></author>
      <author><first>Sabine</first> <last>Schulte im Walde</last></author>
      <author><first>Ngoc Thang</first> <last>Vu</last></author>
      <pages>233–243</pages>
      <url hash="37e84236">D17-1022</url>
      <doi>10.18653/v1/D17-1022</doi>
      <abstract>We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> have shown limitations on prototypical hypernyms, HyperVec represents an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised measure</a> where <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> are learned in a specific order and capture the hypernymhyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state-of-the-art unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.</abstract>
      <bibkey>nguyen-etal-2017-hierarchical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/evalution">EVALution</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hyperlex">HyperLex</pwcdataset>
    </paper>
    <paper id="24">
      <title>Dict2vec : Learning Word Embeddings using Lexical Dictionaries<fixed-case>D</fixed-case>ict2vec : Learning Word Embeddings using Lexical Dictionaries</title>
      <author><first>Julien</first> <last>Tissier</last></author>
      <author><first>Christophe</first> <last>Gravier</last></author>
      <author><first>Amaury</first> <last>Habrard</last></author>
      <pages>254–263</pages>
      <url hash="567c405e">D17-1024</url>
      <doi>10.18653/v1/D17-1024</doi>
      <abstract>Learning <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> on large unlabeled corpus has been shown to be successful in improving many <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language tasks</a>. The most efficient and popular approaches learn or retrofit such <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representations</a> using additional <a href="https://en.wikipedia.org/wiki/Data">external data</a>. Resulting embeddings are generally better than their corpus-only counterparts, although such resources cover a fraction of words in the vocabulary. In this paper, we propose a new approach, Dict2vec, based on one of the largest yet refined datasource for describing words   natural language dictionaries. Dict2vec builds new word pairs from dictionary entries so that semantically-related words are moved closer, and negative sampling filters out pairs whose words are unrelated in dictionaries. We evaluate the word representations obtained using Dict2vec on eleven <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> for the word similarity task and on four <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> for a text classification task.</abstract>
      <bibkey>tissier-etal-2017-dict2vec</bibkey>
      <pwccode url="https://github.com/tca19/dict2vec" additional="false">tca19/dict2vec</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
    </paper>
    <paper id="26">
      <title>Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext</title>
      <author><first>John</first> <last>Wieting</last></author>
      <author><first>Jonathan</first> <last>Mallinson</last></author>
      <author><first>Kevin</first> <last>Gimpel</last></author>
      <pages>274–285</pages>
      <url hash="bbcbce2d">D17-1026</url>
      <doi>10.18653/v1/D17-1026</doi>
      <abstract>We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al. (2016b). We use <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> to generate sentential paraphrases via back-translation of bilingual sentence pairs. We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embeddings. We find that the data quality is stronger than prior work based on bitext and on par with manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains. We experiment with several language pairs and <a href="https://en.wikipedia.org/wiki/Data_source">data sources</a>, and develop a variety of data filtering techniques. In the process, we explore how neural machine translation output differs from <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">human-written sentences</a>, finding clear differences in length, the amount of <a href="https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)">repetition</a>, and the use of rare words.</abstract>
      <bibkey>wieting-etal-2017-learning</bibkey>
    </paper>
    <paper id="28">
      <title>Exploiting Morphological Regularities in Distributional Word Representations</title>
      <author><first>Arihant</first> <last>Gupta</last></author>
      <author><first>Syed Sarfaraz</first> <last>Akhtar</last></author>
      <author><first>Avijit</first> <last>Vajpayee</last></author>
      <author><first>Arjit</first> <last>Srivastava</last></author>
      <author><first>Madan Gopal</first> <last>Jhanwar</last></author>
      <author><first>Manish</first> <last>Shrivastava</last></author>
      <pages>292–297</pages>
      <url hash="863b8040">D17-1028</url>
      <doi>10.18653/v1/D17-1028</doi>
      <abstract>We present an unsupervised, language agnostic approach for exploiting morphological regularities present in high dimensional vector spaces. We propose a novel method for generating embeddings of words from their morphological variants using morphological transformation operators. We evaluate this approach on MSR word analogy test set with an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 85 % which is 12 % higher than the previous best known system.</abstract>
      <revision id="1" href="D17-1028v1" hash="40e7dc23" />
      <revision id="2" href="D17-1028v2" hash="863b8040">No description of the changes were recorded.</revision>
      <bibkey>gupta-etal-2017-exploiting</bibkey>
    </paper>
    <paper id="29">
      <title>Exploiting Word Internal Structures for Generic Chinese Sentence Representation<fixed-case>C</fixed-case>hinese Sentence Representation</title>
      <author><first>Shaonan</first> <last>Wang</last></author>
      <author><first>Jiajun</first> <last>Zhang</last></author>
      <author><first>Chengqing</first> <last>Zong</last></author>
      <pages>298–303</pages>
      <url hash="be18d45c">D17-1029</url>
      <doi>10.18653/v1/D17-1029</doi>
      <abstract>We introduce a novel mixed characterword architecture to improve Chinese sentence representations, by utilizing rich semantic information of word internal structures. Our <a href="https://en.wikipedia.org/wiki/Architecture">architecture</a> uses two key strategies. The first is a mask gate on characters, learning the relation among characters in a word. The second is a maxpooling operation on words, adaptively finding the optimal mixture of the atomic and compositional word representations. Finally, the proposed <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a> is applied to various sentence composition models, which achieves substantial performance gains over baseline models on sentence similarity task.</abstract>
      <bibkey>wang-etal-2017-exploiting</bibkey>
    </paper>
    <paper id="30">
      <title>High-risk learning : acquiring new word vectors from tiny data</title>
      <author><first>Aurélie</first> <last>Herbelot</last></author>
      <author><first>Marco</first> <last>Baroni</last></author>
      <pages>304–309</pages>
      <url hash="b9c6b33b">D17-1030</url>
      <doi>10.18653/v1/D17-1030</doi>
      <attachment type="attachment" hash="106236d0">D17-1030.Attachment.zip</attachment>
      <abstract>Distributional semantics models are known to struggle with <a href="https://en.wikipedia.org/wiki/Small_data">small data</a>. It is generally accepted that in order to learn ‘a good vector’ for a word, a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences’ worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.</abstract>
      <bibkey>herbelot-baroni-2017-high</bibkey>
      <pwccode url="https://github.com/minimalparts/nonce2vec" additional="false">minimalparts/nonce2vec</pwccode>
    </paper>
    <paper id="32">
      <title>VecShare : A Framework for Sharing Word Representation Vectors<fixed-case>V</fixed-case>ec<fixed-case>S</fixed-case>hare: A Framework for Sharing Word Representation Vectors</title>
      <author><first>Jared</first> <last>Fernandez</last></author>
      <author><first>Zhaocheng</first> <last>Yu</last></author>
      <author><first>Doug</first> <last>Downey</last></author>
      <pages>316–320</pages>
      <url hash="02077ac4">D17-1032</url>
      <doi>10.18653/v1/D17-1032</doi>
      <abstract>Many Natural Language Processing (NLP) models rely on distributed vector representations of words. Because the process of training word vectors can require large amounts of data and computation, NLP researchers and practitioners often utilize pre-trained embeddings downloaded from the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">Web</a>. However, finding the best <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> for a given <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> is difficult, and can be computationally prohibitive. We present a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a>, called VecShare, that makes it easy to share and retrieve <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> on the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">Web</a>. The <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> leverages a public data-sharing infrastructure to host embedding sets, and provides automated mechanisms for retrieving the embeddings most similar to a given corpus. We perform an experimental evaluation of VecShare’s similarity strategies, and show that they are effective at efficiently retrieving <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> that boost <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> in a document classification task. Finally, we provide an open-source Python library for using the VecShare framework.</abstract>
      <bibkey>fernandez-etal-2017-vecshare</bibkey>
    </paper>
    <paper id="33">
      <title>Word Re-Embedding via Manifold Dimensionality Retention</title>
      <author><first>Souleiman</first> <last>Hasan</last></author>
      <author><first>Edward</first> <last>Curry</last></author>
      <pages>321–326</pages>
      <url hash="c173a87a">D17-1033</url>
      <doi>10.18653/v1/D17-1033</doi>
      <attachment type="attachment" hash="22119458">D17-1033.Attachment.zip</attachment>
      <abstract>Word embeddings seek to recover a <a href="https://en.wikipedia.org/wiki/Euclidean_space">Euclidean metric space</a> by mapping words into vectors, starting from words co-occurrences in a corpus. Word embeddings may underestimate the similarity between nearby words, and overestimate it between distant words in the <a href="https://en.wikipedia.org/wiki/Euclidean_space">Euclidean metric space</a>. In this paper, we re-embed pre-trained <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> with a stage of <a href="https://en.wikipedia.org/wiki/Manifold_learning">manifold learning</a> which retains <a href="https://en.wikipedia.org/wiki/Dimensionality">dimensionality</a>. We show that this approach is theoretically founded in the metric recovery paradigm, and empirically show that it can improve on state-of-the-art embeddings in word similarity tasks 0.5-5.0 % points depending on the original space.</abstract>
      <bibkey>hasan-curry-2017-word</bibkey>
    </paper>
    <paper id="35">
      <title>Reporting Score Distributions Makes a Difference : Performance Study of LSTM-networks for Sequence Tagging<fixed-case>LSTM</fixed-case>-networks for Sequence Tagging</title>
      <author><first>Nils</first> <last>Reimers</last></author>
      <author><first>Iryna</first> <last>Gurevych</last></author>
      <pages>338–348</pages>
      <url hash="3d2ac663">D17-1035</url>
      <doi>10.18653/v1/D17-1035</doi>
      <abstract>In this paper we show that reporting a single performance score is insufficient to compare <a href="https://en.wikipedia.org/wiki/Non-deterministic_algorithm">non-deterministic approaches</a>. We demonstrate for common sequence tagging tasks that the seed value for the <a href="https://en.wikipedia.org/wiki/Random_number_generation">random number generator</a> can result in statistically significant (p &lt; 10 ^ -4) differences for state-of-the-art systems. For two recent <a href="https://en.wikipedia.org/wiki/System">systems</a> for NER, we observe an absolute difference of one percentage point <a href="https://en.wikipedia.org/wiki/F-score">F-score</a> depending on the selected seed value, making these <a href="https://en.wikipedia.org/wiki/System">systems</a> perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present <a href="https://en.wikipedia.org/wiki/Network_architecture">network architectures</a> that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.<tex-math>p &lt; 10^{-4}</tex-math>) differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point F₁-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters. </abstract>
      <bibkey>reimers-gurevych-2017-reporting</bibkey>
      <pwccode url="https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf" additional="true">UKPLab/emnlp2017-bilstm-cnn-crf</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="36">
      <title>Learning What’s Easy : Fully Differentiable Neural Easy-First Taggers</title>
      <author><first>André F. T.</first> <last>Martins</last></author>
      <author><first>Julia</first> <last>Kreutzer</last></author>
      <pages>349–362</pages>
      <url hash="ac26e4e2">D17-1036</url>
      <doi>10.18653/v1/D17-1036</doi>
      <attachment type="attachment" hash="0b95da3e">D17-1036.Attachment.pdf</attachment>
      <abstract>We introduce a novel neural easy-first decoder that learns to solve sequence tagging tasks in a flexible order. In contrast to previous easy-first decoders, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are end-to-end differentiable. The <a href="https://en.wikipedia.org/wiki/Codec">decoder</a> iteratively updates a sketch of the predictions over the sequence. At its core is an <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a> that controls which parts of the input are strategically the best to process next. We present a new constrained softmax transformation that ensures the same cumulative attention to every word, and show how to efficiently evaluate and backpropagate over it. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> compare favourably to BILSTM taggers on three sequence tagging tasks.</abstract>
      <bibkey>martins-kreutzer-2017-learning</bibkey>
    </paper>
    <paper id="37">
      <title>Incremental Skip-gram Model with Negative Sampling</title>
      <author><first>Nobuhiro</first> <last>Kaji</last></author>
      <author><first>Hayato</first> <last>Kobayashi</last></author>
      <pages>363–371</pages>
      <url hash="fe1e9420">D17-1037</url>
      <doi>10.18653/v1/D17-1037</doi>
      <attachment type="attachment" hash="6d3cefa5">D17-1037.Attachment.pdf</attachment>
      <abstract>This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SGNS, are multi-pass algorithms and thus can not perform incremental model update. To address this problem, we present a simple incremental extension of SGNS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.</abstract>
      <attachment type="poster" hash="22067a22">D17-1037.Poster.pdf</attachment>
      <bibkey>kaji-kobayashi-2017-incremental</bibkey>
    </paper>
    <paper id="38">
      <title>Learning to select data for <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> with Bayesian Optimization<fixed-case>B</fixed-case>ayesian Optimization</title>
      <author><first>Sebastian</first> <last>Ruder</last></author>
      <author><first>Barbara</first> <last>Plank</last></author>
      <pages>372–382</pages>
      <url hash="21a63e8b">D17-1038</url>
      <doi>10.18653/v1/D17-1038</doi>
      <abstract>Domain similarity measures can be used to gauge adaptability and select suitable <a href="https://en.wikipedia.org/wiki/Data">data</a> for <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to learn data selection measures using <a href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian Optimization</a> and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks : <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a>, and <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a>. We show the importance of complementing <a href="https://en.wikipedia.org/wiki/Similarity_measure">similarity</a> with diversity, and that learned measures areto some degreetransferable across models, domains, and even tasks.</abstract>
      <bibkey>ruder-plank-2017-learning</bibkey>
      <pwccode url="https://github.com/sebastianruder/learn-to-select-data" additional="false">sebastianruder/learn-to-select-data</pwccode>
    </paper>
    <paper id="39">
      <title>Unsupervised Pretraining for Sequence to Sequence Learning</title>
      <author><first>Prajit</first> <last>Ramachandran</last></author>
      <author><first>Peter</first> <last>Liu</last></author>
      <author><first>Quoc</first> <last>Le</last></author>
      <pages>383–391</pages>
      <url hash="b1a05e5b">D17-1039</url>
      <doi>10.18653/v1/D17-1039</doi>
      <attachment type="attachment" hash="4182342b">D17-1039.Attachment.zip</attachment>
      <abstract>This work presents a general unsupervised learning method to improve the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmarks</a> in <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> and abstractive summarization and find that it significantly improves the subsequent <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised models</a>. Our main result is that pretraining improves the <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a> of seq2seq models. We achieve state-of-the-art results on the WMT EnglishGerman task, surpassing a range of methods using both phrase-based machine translation and <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>. Our method achieves a significant improvement of 1.3 <a href="https://en.wikipedia.org/wiki/British_thermal_unit">BLEU</a> from th previous best models on both WMT’14 and WMT’15 EnglishGerman. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.</abstract>
      <bibkey>ramachandran-etal-2017-unsupervised</bibkey>
    </paper>
    <paper id="40">
      <title>Efficient Attention using a Fixed-Size Memory Representation</title>
      <author><first>Denny</first> <last>Britz</last></author>
      <author><first>Melody</first> <last>Guan</last></author>
      <author><first>Minh-Thang</first> <last>Luong</last></author>
      <pages>392–400</pages>
      <url hash="13001a28">D17-1040</url>
      <doi>10.18653/v1/D17-1040</doi>
      <abstract>The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step. In this work, we propose an alternative <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a> based on a fixed size memory representation that is more efficient. Our technique predicts a compact set of K attention contexts during <a href="https://en.wikipedia.org/wiki/Encoding_(memory)">encoding</a> and lets the <a href="https://en.wikipedia.org/wiki/Encoder">decoder</a> compute an efficient <a href="https://en.wikipedia.org/wiki/Lookup_table">lookup</a> that does not need to consult the <a href="https://en.wikipedia.org/wiki/Memory">memory</a>. We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20 % for real-world translation tasks and more for tasks with longer sequences. By visualizing <a href="https://en.wikipedia.org/wiki/Attention">attention scores</a> we demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> learn distinct, meaningful alignments.</abstract>
      <bibkey>britz-etal-2017-efficient</bibkey>
    </paper>
    <paper id="41">
      <title>Rotated Word Vector Representations and their Interpretability</title>
      <author><first>Sungjoon</first> <last>Park</last></author>
      <author><first>JinYeong</first> <last>Bak</last></author>
      <author><first>Alice</first> <last>Oh</last></author>
      <pages>401–411</pages>
      <url hash="48b7ae10">D17-1041</url>
      <doi>10.18653/v1/D17-1041</doi>
      <abstract>Vector representation of words improves performance in various NLP tasks, but the high dimensional word vectors are very difficult to interpret. We apply several <a href="https://en.wikipedia.org/wiki/Rotation_(mathematics)">rotation algorithms</a> to the vector representation of words to improve the <a href="https://en.wikipedia.org/wiki/Interpretability">interpretability</a>. Unlike previous approaches that induce sparsity, the rotated vectors are interpretable while preserving the expressive performance of the original vectors. Furthermore, any prebuilt word vector representation can be rotated for improved <a href="https://en.wikipedia.org/wiki/Interpreter_(computing)">interpretability</a>. We apply <a href="https://en.wikipedia.org/wiki/Rotation_(mathematics)">rotation</a> to skipgrams and glove and compare the expressive power and interpretability with the original <a href="https://en.wikipedia.org/wiki/Euclidean_vector">vectors</a> and the sparse overcomplete vectors. The results show that the rotated vectors outperform the original and the sparse overcomplete vectors for interpretability and expressiveness tasks.</abstract>
      <bibkey>park-etal-2017-rotated</bibkey>
      <pwccode url="https://github.com/SungjoonPark/factor_rotation" additional="false">SungjoonPark/factor_rotation</pwccode>
    </paper>
    <paper id="42">
      <title>A causal framework for explaining the predictions of black-box sequence-to-sequence models</title>
      <author><first>David</first> <last>Alvarez-Melis</last></author>
      <author><first>Tommi</first> <last>Jaakkola</last></author>
      <pages>412–421</pages>
      <url hash="97a807b5">D17-1042</url>
      <doi>10.18653/v1/D17-1042</doi>
      <attachment type="attachment" hash="564adc03">D17-1042.Attachment.zip</attachment>
      <abstract>We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an explanation consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with perturbed inputs, generating a graph over tokens from the responses, and solving a <a href="https://en.wikipedia.org/wiki/Partition_of_a_set">partitioning problem</a> to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.</abstract>
      <bibkey>alvarez-melis-jaakkola-2017-causal</bibkey>
    </paper>
    <paper id="43">
      <title>Piecewise Latent Variables for Neural Variational Text Processing</title>
      <author><first>Iulian Vlad</first> <last>Serban</last></author>
      <author><first>Alexander G.</first> <last>Ororbia</last></author>
      <author><first>Joelle</first> <last>Pineau</last></author>
      <author><first>Aaron</first> <last>Courville</last></author>
      <pages>422–432</pages>
      <url hash="6afbdb5d">D17-1043</url>
      <doi>10.18653/v1/D17-1043</doi>
      <attachment type="attachment" hash="abeaf83b">D17-1043.Attachment.zip</attachment>
      <abstract>Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> will learn to represent rich, multi-modal latent factors in real-world data, such as <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language text</a>. However, current models often assume simplistic priors on the latent variables-such as the uni-modal Gaussian distribution-which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This <a href="https://en.wikipedia.org/wiki/Probability_distribution">distribution</a> has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation</a> for <a href="https://en.wikipedia.org/wiki/Dialogue">dialogue</a>.</abstract>
      <bibkey>serban-etal-2017-piecewise</bibkey>
    </paper>
    <paper id="44">
      <title>Learning the Structure of Variable-Order CRFs : a finite-state perspective<fixed-case>CRF</fixed-case>s: a finite-state perspective</title>
      <author><first>Thomas</first> <last>Lavergne</last></author>
      <author><first>François</first> <last>Yvon</last></author>
      <pages>433–439</pages>
      <url hash="63e47ffc">D17-1044</url>
      <doi>10.18653/v1/D17-1044</doi>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">computational complexity</a> of linear-chain Conditional Random Fields (CRFs) makes it difficult to deal with very large label sets and long range dependencies. Such situations are not rare and arise when dealing with <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphologically rich languages</a> or joint labelling tasks. We extend here recent proposals to consider variable order CRFs. Using an effective finite-state representation of variable-length dependencies, we propose new ways to perform <a href="https://en.wikipedia.org/wiki/Feature_selection">feature selection</a> at large scale and report experimental results where we outperform strong baselines on a tagging task.</abstract>
      <bibkey>lavergne-yvon-2017-learning</bibkey>
    </paper>
    <paper id="47">
      <title>Recurrent Attention Network on Memory for Aspect Sentiment Analysis</title>
      <author><first>Peng</first> <last>Chen</last></author>
      <author><first>Zhongqian</first> <last>Sun</last></author>
      <author><first>Lidong</first> <last>Bing</last></author>
      <author><first>Wei</first> <last>Yang</last></author>
      <pages>452–461</pages>
      <url hash="da59277c">D17-1047</url>
      <doi>10.18653/v1/D17-1047</doi>
      <attachment type="attachment" hash="6577cba5">D17-1047.Attachment.zip</attachment>
      <abstract>We propose a novel framework based on <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> to identify the sentiment of opinion targets in a comment / review. Our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> adopts multiple-attention mechanism to capture <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment features</a> separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network</a>, which strengthens the expressive power of our model for handling more complications. The weighted-memory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence. We examine the merit of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on four <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> : two are from SemEval2014, i.e. reviews of restaurants and laptops ; a twitter dataset, for testing its performance on social media data ; and a Chinese news comment dataset, for testing its language sensitivity. The experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> consistently outperforms the state-of-the-art methods on different types of data.</abstract>
      <bibkey>chen-etal-2017-recurrent</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2">SemEval 2014 Task 4 Sub Task 2</pwcdataset>
    </paper>
    <paper id="49">
      <title>Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions from Reviews</title>
      <author><first>Lahari</first> <last>Poddar</last></author>
      <author><first>Wynne</first> <last>Hsu</last></author>
      <author><first>Mong Li</first> <last>Lee</last></author>
      <pages>472–481</pages>
      <url hash="04e40f2b">D17-1049</url>
      <doi>10.18653/v1/D17-1049</doi>
      <abstract>User generated content about products and services in the form of reviews are often diverse and even contradictory. This makes it difficult for users to know if an opinion in a review is prevalent or biased. We study the problem of searching for supporting opinions in the context of reviews. We propose a framework called SURF, that first identifies opinions expressed in a review, and then finds similar opinions from other reviews. We design a novel probabilistic graphical model that captures opinions as a combination of aspect, topic and sentiment dimensions, takes into account the preferences of individual authors, as well as the quality of the entity under review, and encodes the flow of thoughts in a review by constraining the aspect distribution dynamically among successive review segments. We derive a <a href="https://en.wikipedia.org/wiki/Similarity_measure">similarity measure</a> that considers both lexical and semantic similarity to find supporting opinions. Experiments on TripAdvisor hotel reviews and Yelp restaurant reviews show that our model outperforms existing methods for modeling opinions, and the proposed framework is effective in finding supporting opinions.</abstract>
      <bibkey>poddar-etal-2017-author</bibkey>
    </paper>
    <paper id="50">
      <title>Magnets for <a href="https://en.wikipedia.org/wiki/Sarcasm">Sarcasm</a> : Making Sarcasm Detection Timely, Contextual and Very Personal</title>
      <author><first>Aniruddha</first> <last>Ghosh</last></author>
      <author><first>Tony</first> <last>Veale</last></author>
      <pages>482–491</pages>
      <url hash="a5940364">D17-1050</url>
      <doi>10.18653/v1/D17-1050</doi>
      <abstract>Sarcasm is a pervasive phenomenon in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, permitting the concise communication of meaning, affect and <a href="https://en.wikipedia.org/wiki/Attitude_(psychology)">attitude</a>. Concision requires wit to produce and wit to understand, which demands from each party knowledge of norms, context and a speaker’s mindset. Insight into a speaker’s psychological profile at the time of production is a valuable source of context for sarcasm detection. Using a neural architecture, we show significant gains in detection accuracy when knowledge of the speaker’s mood at the time of production can be inferred. Our focus is on <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm detection</a> on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>, and show that the mood exhibited by a speaker over tweets leading up to a new post is as useful a cue for <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> as the topical context of the post itself. The work opens the door to an empirical exploration not just of <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> in text but of the <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcastic state of mind</a>.</abstract>
      <bibkey>ghosh-veale-2017-magnets</bibkey>
      <pwccode url="https://github.com/AniSkywalker/SarcasmDetection" additional="false">AniSkywalker/SarcasmDetection</pwccode>
    </paper>
    <paper id="51">
      <title>Identifying Humor in Reviews using Background Text Sources</title>
      <author><first>Alex</first> <last>Morales</last></author>
      <author><first>Chengxiang</first> <last>Zhai</last></author>
      <pages>492–501</pages>
      <url hash="b525e393">D17-1051</url>
      <doi>10.18653/v1/D17-1051</doi>
      <abstract>We study the problem of automatically identifying humorous text from a new kind of <a href="https://en.wikipedia.org/wiki/Text_file">text data</a>, i.e., <a href="https://en.wikipedia.org/wiki/Online_review">online reviews</a>. We propose a generative language model, based on the theory of incongruity, to model humorous text, which allows us to leverage background text sources, such as Wikipedia entry descriptions, and enables construction of multiple features for identifying humorous reviews. Evaluation of these <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> using <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> for classifying reviews into humorous and non-humorous reviews shows that the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> constructed based on the proposed generative model are much more effective than the major <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> proposed in the existing literature, allowing us to achieve almost 86 % accuracy. These humorous review predictions can also supply good indicators for identifying helpful reviews.</abstract>
      <bibkey>morales-zhai-2017-identifying</bibkey>
    </paper>
    <paper id="52">
      <title>Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision</title>
      <author><first>Leyi</first> <last>Wang</last></author>
      <author><first>Rui</first> <last>Xia</last></author>
      <pages>502–510</pages>
      <url hash="a9f3977b">D17-1052</url>
      <doi>10.18653/v1/D17-1052</doi>
      <abstract>Sentiment lexicon is an important tool for identifying the sentiment polarity of words and texts. How to automatically construct sentiment lexicons has become a research topic in the field of <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> and <a href="https://en.wikipedia.org/wiki/Opinion_mining">opinion mining</a>. Recently there were some attempts to employ <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning algorithms</a> to construct a sentiment lexicon with sentiment-aware word embedding. However, these <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> were normally trained under document-level sentiment supervision. In this paper, we develop a neural architecture to train a sentiment-aware word embedding by integrating the sentiment supervision at both document and word levels, to enhance the quality of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> as well as the sentiment lexicon. Experiments on the SemEval 2013-2016 datasets indicate that the sentiment lexicon generated by our approach achieves the state-of-the-art performance in both supervised and unsupervised sentiment classification, in comparison with several strong sentiment lexicon construction methods.</abstract>
      <bibkey>wang-xia-2017-sentiment</bibkey>
    </paper>
    <paper id="53">
      <title>Towards a Universal Sentiment Classifier in Multiple languages</title>
      <author><first>Kui</first> <last>Xu</last></author>
      <author><first>Xiaojun</first> <last>Wan</last></author>
      <pages>511–520</pages>
      <url hash="d1b1c43b">D17-1053</url>
      <doi>10.18653/v1/D17-1053</doi>
      <abstract>Existing sentiment classifiers usually work for only one specific language, and different classification models are used in different languages. In this paper we aim to build a universal sentiment classifier with a single <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification model</a> in multiple different languages. In order to achieve this goal, we propose to learn multilingual sentiment-aware word embeddings simultaneously based only on the labeled reviews in <a href="https://en.wikipedia.org/wiki/English_language">English</a> and unlabeled parallel data available in a few language pairs. It is not required that the parallel data exist between <a href="https://en.wikipedia.org/wiki/English_language">English</a> and any other language, because the <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment information</a> can be transferred into any language via pivot languages. We present the evaluation results of our universal sentiment classifier in five languages, and the results are very promising even when the parallel data between <a href="https://en.wikipedia.org/wiki/English_language">English</a> and the target languages are not used. Furthermore, the universal single classifier is compared with a few cross-language sentiment classifiers relying on direct parallel data between the source and target languages, and the results show that the performance of our universal sentiment classifier is very promising compared to that of different cross-language classifiers in multiple target languages.</abstract>
      <bibkey>xu-wan-2017-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/united-nations-parallel-corpus">United Nations Parallel Corpus</pwcdataset>
    </paper>
    <paper id="54">
      <title>Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network</title>
      <author><first>Zi-Yi</first> <last>Dou</last></author>
      <pages>521–526</pages>
      <url hash="9d0c0de9">D17-1054</url>
      <doi>10.18653/v1/D17-1054</doi>
      <abstract>Document-level sentiment classification is a fundamental problem which aims to predict a user’s overall sentiment about a product in a document. Several methods have been proposed to tackle the problem whereas most of them fail to consider the influence of users who express the sentiment and products which are evaluated. To address the issue, we propose a deep memory network for document-level sentiment classification which could capture the user and product information at the same time. To prove the effectiveness of our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a>, we conduct experiments on IMDB and Yelp datasets and the results indicate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can achieve better performance than several existing methods.</abstract>
      <bibkey>dou-2017-capturing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="55">
      <title>Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters</title>
      <author><first>Min</first> <last>Yang</last></author>
      <author><first>Jincheng</first> <last>Mei</last></author>
      <author><first>Heng</first> <last>Ji</last></author>
      <author><first>Wei</first> <last>Zhao</last></author>
      <author><first>Zhou</first> <last>Zhao</last></author>
      <author><first>Xiaojun</first> <last>Chen</last></author>
      <pages>527–533</pages>
      <url hash="6a1cc843">D17-1055</url>
      <doi>10.18653/v1/D17-1055</doi>
      <abstract>We study the problem of identifying the topics and sentiments and tracking their shifts from social media texts in different geographical regions during emergencies and disasters. We propose a location-based dynamic sentiment-topic model (LDST) which can jointly model topic, sentiment, time and Geolocation information. The experimental results demonstrate that LDST performs very well at discovering topics and sentiments from <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> and tracking their shifts in different geographical regions during emergencies and disasters. We will release the data and source code after this work is published.</abstract>
      <bibkey>yang-etal-2017-identifying</bibkey>
    </paper>
    <paper id="56">
      <title>Refining Word Embeddings for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a></title>
      <author><first>Liang-Chih</first> <last>Yu</last></author>
      <author><first>Jin</first> <last>Wang</last></author>
      <author><first>K. Robert</first> <last>Lai</last></author>
      <author><first>Xuejie</first> <last>Zhang</last></author>
      <pages>534–539</pages>
      <url hash="f32fc6b4">D17-1056</url>
      <doi>10.18653/v1/D17-1056</doi>
      <abstract>Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks. However, existing methods for learning context-based word embeddings typically fail to capture sufficient sentiment information. This may result in words with similar <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">vector representations</a> having an opposite sentiment polarity (e.g., good and bad), thus degrading <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> performance. Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., <a href="https://en.wikipedia.org/wiki/Word2vec">Word2vec</a> and GloVe). The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST).</abstract>
      <bibkey>yu-etal-2017-refining</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="58">
      <title>Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word Embeddings</title>
      <author><first>Raksha</first> <last>Sharma</last></author>
      <author><first>Arpan</first> <last>Somani</last></author>
      <author><first>Lakshya</first> <last>Kumar</last></author>
      <author><first>Pushpak</first> <last>Bhattacharyya</last></author>
      <pages>547–552</pages>
      <url hash="7119f261">D17-1058</url>
      <doi>10.18653/v1/D17-1058</doi>
      <abstract>Identification of intensity ordering among polar (positive or negative) words which have the same <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> can lead to a fine-grained sentiment analysis. For example, ‘master’, ‘seasoned’ and ‘familiar’ point to different intensity levels, though they all convey the same meaning (semantics), i.e., expertise : having a good knowledge of. In this paper, we propose a <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised technique</a> that uses sentiment bearing word embeddings to produce a continuous ranking among adjectives that share common semantics. Our system demonstrates a strong Spearman’s rank correlation of 0.83 with the gold standard ranking. We show that sentiment bearing word embeddings facilitate a more accurate intensity ranking system than other standard word embeddings (word2vec and GloVe). Word2vec is the state-of-the-art for intensity ordering task.</abstract>
      <bibkey>sharma-etal-2017-sentiment</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="59">
      <title>Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary Lookup, and Polarity Association<fixed-case>PU</fixed-case> Learning, Double Dictionary Lookup, and Polarity Association</title>
      <author><first>Yasheng</first> <last>Wang</last></author>
      <author><first>Yang</first> <last>Zhang</last></author>
      <author><first>Bing</first> <last>Liu</last></author>
      <pages>553–563</pages>
      <url hash="148a230e">D17-1059</url>
      <doi>10.18653/v1/D17-1059</doi>
      <abstract>Although many sentiment lexicons in different languages exist, most are not comprehensive. In a recent sentiment analysis application, we used a large Chinese sentiment lexicon and found that it missed a large number of sentiment words in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. This prompted us to make a new attempt to study sentiment lexicon expansion. This paper first poses the <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> as a PU learning problem, which is a new <a href="https://en.wikipedia.org/wiki/Formulation">formulation</a>. It then proposes a new PU learning method suitable for our problem using a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a>. The results are enhanced further with a new dictionary-based technique and a novel polarity classification technique. Experimental results show that the proposed approach outperforms baseline methods greatly.</abstract>
      <bibkey>wang-etal-2017-sentiment</bibkey>
    </paper>
    <paper id="60">
      <title>DeepPath : A Reinforcement Learning Method for Knowledge Graph Reasoning<fixed-case>D</fixed-case>eep<fixed-case>P</fixed-case>ath: A Reinforcement Learning Method for Knowledge Graph Reasoning</title>
      <author><first>Wenhan</first> <last>Xiong</last></author>
      <author><first>Thien</first> <last>Hoang</last></author>
      <author><first>William Yang</first> <last>Wang</last></author>
      <pages>564–573</pages>
      <url hash="ba36bbb2">D17-1060</url>
      <doi>10.18653/v1/D17-1060</doi>
      <abstract>We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths : we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a <a href="https://en.wikipedia.org/wiki/Reward_system">reward function</a> that takes the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, <a href="https://en.wikipedia.org/wiki/Diversity_(business)">diversity</a>, and <a href="https://en.wikipedia.org/wiki/Efficiency">efficiency</a> into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.</abstract>
      <video href="https://vimeo.com/238232302" />
      <bibkey>xiong-etal-2017-deeppath</bibkey>
      <pwccode url="https://github.com/xwhan/DeepPath" additional="true">xwhan/DeepPath</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nell-995">NELL-995</pwcdataset>
    </paper>
    <paper id="62">
      <title>Sentence Simplification with Deep Reinforcement Learning</title>
      <author><first>Xingxing</first> <last>Zhang</last></author>
      <author><first>Mirella</first> <last>Lapata</last></author>
      <pages>584–594</pages>
      <url hash="5b42dcee">D17-1062</url>
      <doi>10.18653/v1/D17-1062</doi>
      <abstract>Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms competitive simplification systems.<b>D</b>eep
      <b>RE</b>inforcement <b>S</b>entence <b>S</b>implification), explores the space of possible
      simplifications while learning to optimize a reward function that
      encourages outputs which are simple, fluent, and preserve the meaning of
      the input. Experiments on three datasets demonstrate that our model
      outperforms competitive simplification systems.
    </abstract>
      <video href="https://vimeo.com/238235599" />
      <bibkey>zhang-lapata-2017-sentence</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilarge">WikiLarge</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/asset">ASSET</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/turkcorpus">TurkCorpus</pwcdataset>
    </paper>
    <paper id="63">
      <title>Learning how to Active Learn : A Deep Reinforcement Learning Approach</title>
      <author><first>Meng</first> <last>Fang</last></author>
      <author><first>Yuan</first> <last>Li</last></author>
      <author><first>Trevor</first> <last>Cohn</last></author>
      <pages>595–605</pages>
      <url hash="4a60e58a">D17-1063</url>
      <doi>10.18653/v1/D17-1063</doi>
      <abstract>Active learning aims to select a small subset of data for <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> such that a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of <a href="https://en.wikipedia.org/wiki/Heuristic">heuristics</a> varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the <a href="https://en.wikipedia.org/wiki/Active_learning">active learning</a> as a reinforcement learning problem and explicitly learning a data selection policy, where the <a href="https://en.wikipedia.org/wiki/Policy">policy</a> takes the role of the <a href="https://en.wikipedia.org/wiki/Active_learning">active learning heuristic</a>. Importantly, our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> allows the selection policy learned using simulation to one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning algorithms.</abstract>
      <video href="https://vimeo.com/238234005" />
      <bibkey>fang-etal-2017-learning</bibkey>
      <pwccode url="https://github.com/mengf1/PAL" additional="false">mengf1/PAL</pwccode>
    </paper>
    <paper id="64">
      <title>Split and Rephrase</title>
      <author><first>Shashi</first> <last>Narayan</last></author>
      <author><first>Claire</first> <last>Gardent</last></author>
      <author><first>Shay B.</first> <last>Cohen</last></author>
      <author><first>Anastasia</first> <last>Shimorina</last></author>
      <pages>606–616</pages>
      <url hash="a1b30c7b">D17-1064</url>
      <doi>10.18653/v1/D17-1064</doi>
      <abstract>We propose a new sentence simplification task (Split-and-Rephrase) where the aim is to split a complex sentence into a meaning preserving sequence of shorter sentences. Like <a href="https://en.wikipedia.org/wiki/Sentence_simplification">sentence simplification</a>, splitting-and-rephrasing has the potential of benefiting both <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> and societal applications. Because shorter sentences are generally better processed by <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP systems</a>, it could be used as a preprocessing step which facilitates and improves the performance of <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a>, <a href="https://en.wikipedia.org/wiki/Semantic_role_labeling">semantic role labellers</a> and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation systems</a>. It should also be of use for people with reading disabilities because <a href="https://en.wikipedia.org/wiki/Italian_language">it</a> allows the conversion of longer sentences into shorter ones. This paper makes two contributions towards this new task. First, we create and make available a <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark</a> consisting of 1,066,115 tuples mapping a single complex sentence to a sequence of sentences expressing the same meaning. Second, we propose five <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> (vanilla sequence-to-sequence to semantically-motivated models) to understand the difficulty of the proposed task.</abstract>
      <video href="https://vimeo.com/238230265" />
      <bibkey>narayan-etal-2017-split</bibkey>
      <pwccode url="https://github.com/shashiongithub/Split-and-Rephrase" additional="true">shashiongithub/Split-and-Rephrase</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="65">
      <title>Neural Response Generation via <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">GAN</a> with an Approximate Embedding Layer<fixed-case>GAN</fixed-case> with an Approximate Embedding Layer</title>
      <author><first>Zhen</first> <last>Xu</last></author>
      <author><first>Bingquan</first> <last>Liu</last></author>
      <author><first>Baoxun</first> <last>Wang</last></author>
      <author><first>Chengjie</first> <last>Sun</last></author>
      <author><first>Xiaolong</first> <last>Wang</last></author>
      <author><first>Zhuoran</first> <last>Wang</last></author>
      <author><first>Chao</first> <last>Qi</last></author>
      <pages>617–626</pages>
      <url hash="17fe6ccf">D17-1065</url>
      <doi>10.18653/v1/D17-1065</doi>
      <abstract>This paper presents a Generative Adversarial Network (GAN) to model single-turn short-text conversations, which trains a sequence-to-sequence (Seq2Seq) network for response generation simultaneously with a discriminative classifier that measures the differences between human-produced responses and machine-generated ones. In addition, the proposed method introduces an approximate embedding layer to solve the non-differentiable problem caused by the sampling-based output decoding procedure in the Seq2Seq generative model. The GAN setup provides an effective way to avoid noninformative responses (a.k.a safe responses), which are frequently observed in traditional neural response generators. The experimental results show that the proposed approach significantly outperforms existing neural response generation models in diversity metrics, with slight increases in <a href="https://en.wikipedia.org/wiki/Relevance_(information_retrieval)">relevance scores</a> as well, when evaluated on both a <a href="https://en.wikipedia.org/wiki/Mandarin_Chinese">Mandarin corpus</a> and an <a href="https://en.wikipedia.org/wiki/English_language">English corpus</a>.</abstract>
      <video href="https://vimeo.com/238230011" />
      <bibkey>xu-etal-2017-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="66">
      <title>A Hybrid Convolutional Variational Autoencoder for Text Generation</title>
      <author><first>Stanislau</first> <last>Semeniuta</last></author>
      <author><first>Aliaksei</first> <last>Severyn</last></author>
      <author><first>Erhardt</first> <last>Barth</last></author>
      <pages>627–637</pages>
      <url hash="58ecec19">D17-1066</url>
      <doi>10.18653/v1/D17-1066</doi>
      <attachment type="attachment" hash="1f29c1c1">D17-1066.Attachment.zip</attachment>
      <abstract>In this paper we explore the effect of architectural choices on learning a variational autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid the issue of the VAE collapsing to a deterministic model.</abstract>
      <video href="https://vimeo.com/238230365" />
      <bibkey>semeniuta-etal-2017-hybrid</bibkey>
      <pwccode url="https://github.com/stas-semeniuta/textvae" additional="true">stas-semeniuta/textvae</pwccode>
    </paper>
    <paper id="68">
      <title>Measuring Thematic Fit with Distributional Feature Overlap</title>
      <author><first>Enrico</first> <last>Santus</last></author>
      <author><first>Emmanuele</first> <last>Chersoni</last></author>
      <author><first>Alessandro</first> <last>Lenci</last></author>
      <author><first>Philippe</first> <last>Blache</last></author>
      <pages>648–658</pages>
      <url hash="6dc3759c">D17-1068</url>
      <doi>10.18653/v1/D17-1068</doi>
      <abstract>In this paper, we introduce a new <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">distributional method</a> for modeling predicate-argument thematic fit judgments. We use a syntax-based DSM to build a prototypical representation of verb-specific roles : for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes. Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-the-art <a href="https://en.wikipedia.org/wiki/System">system</a>, and achieves better or comparable results to those reported in the literature for the other <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised systems</a>. Moreover, it provides an explicit representation of the <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> characterizing verb-specific semantic roles.</abstract>
      <video href="https://vimeo.com/238234243" />
      <bibkey>santus-etal-2017-measuring</bibkey>
      <pwccode url="https://github.com/esantus/Thematic_Fit" additional="false">esantus/Thematic_Fit</pwccode>
    </paper>
    <paper id="70">
      <title>Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</title>
      <author><first>Alexis</first> <last>Conneau</last></author>
      <author><first>Douwe</first> <last>Kiela</last></author>
      <author><first>Holger</first> <last>Schwenk</last></author>
      <author><first>Loïc</first> <last>Barrault</last></author>
      <author><first>Antoine</first> <last>Bordes</last></author>
      <pages>670–680</pages>
      <url hash="38658bcc">D17-1070</url>
      <doi>10.18653/v1/D17-1070</doi>
      <abstract>Many modern NLP systems rely on <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>, previously trained in an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised manner</a> on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised data</a> of the Stanford Natural Language Inference datasets can consistently outperform <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a> like SkipThought vectors on a wide range of transfer tasks. Much like how <a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a> uses <a href="https://en.wikipedia.org/wiki/ImageNet">ImageNet</a> to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> to other NLP tasks. Our <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> is publicly available.</abstract>
      <video href="https://vimeo.com/238236002" />
      <bibkey>conneau-etal-2017-supervised</bibkey>
      <pwccode url="https://github.com/facebookresearch/InferSent" additional="true">facebookresearch/InferSent</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="71">
      <title>Determining Semantic Textual Similarity using Natural Deduction Proofs</title>
      <author><first>Hitomi</first> <last>Yanaka</last></author>
      <author><first>Koji</first> <last>Mineshima</last></author>
      <author><first>Pascual</first> <last>Martínez-Gómez</last></author>
      <author><first>Daisuke</first> <last>Bekki</last></author>
      <pages>681–691</pages>
      <url hash="cee28437">D17-1071</url>
      <doi>10.18653/v1/D17-1071</doi>
      <abstract>Determining semantic textual similarity is a core research subject in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. Since vector-based models for sentence representation often use shallow information, capturing accurate <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> is difficult. By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of textual similarity. We propose a method for determining semantic textual similarity by combining shallow features with <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higher-order automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. Experiments show that our <a href="https://en.wikipedia.org/wiki/System">system</a> was able to outperform other logic-based systems and that <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> derived from the proofs are effective for learning textual similarity.</abstract>
      <video href="https://vimeo.com/238232412" />
      <bibkey>yanaka-etal-2017-determining</bibkey>
      <pwccode url="https://github.com/mynlp/ccg2lambda" additional="false">mynlp/ccg2lambda</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
    </paper>
    <paper id="72">
      <title>Multi-Grained Chinese Word Segmentation<fixed-case>C</fixed-case>hinese Word Segmentation</title>
      <author><first>Chen</first> <last>Gong</last></author>
      <author><first>Zhenghua</first> <last>Li</last></author>
      <author><first>Min</first> <last>Zhang</last></author>
      <author><first>Xinzhou</first> <last>Jiang</last></author>
      <pages>692–703</pages>
      <url hash="e3e93e72">D17-1072</url>
      <doi>10.18653/v1/D17-1072</doi>
      <abstract>Traditionally, word segmentation (WS) adopts the single-grained formalism, where a sentence corresponds to a single word sequence. However, Sproat et al. (1997) show that the inter-native-speaker consistency ratio over Chinese word boundaries is only 76 %, indicating single-grained WS (SWS) imposes unnecessary challenges on both manual annotation and statistical modeling. Moreover, WS results of different <a href="https://en.wikipedia.org/wiki/Granularity">granularities</a> can be complementary and beneficial for <a href="https://en.wikipedia.org/wiki/High-level_programming_language">high-level applications</a>. This work proposes and addresses multi-grained WS (MWS). We build a large-scale pseudo MWS dataset for model training and tuning by leveraging the annotation heterogeneity of three SWS datasets. Then we manually annotate 1,500 test sentences with true MWS annotations. Finally, we propose three benchmark approaches by casting MWS as constituent parsing and <a href="https://en.wikipedia.org/wiki/Sequence_labeling">sequence labeling</a>. Experiments and analysis lead to many interesting findings.</abstract>
      <bibkey>gong-etal-2017-multi</bibkey>
    </paper>
    <paper id="73">
      <title>Do n’t Throw Those Morphological Analyzers Away Just Yet : Neural Morphological Disambiguation for <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a><fixed-case>A</fixed-case>rabic</title>
      <author><first>Nasser</first> <last>Zalmout</last></author>
      <author><first>Nizar</first> <last>Habash</last></author>
      <pages>704–713</pages>
      <url hash="ce0fe45d">D17-1073</url>
      <doi>10.18653/v1/D17-1073</doi>
      <abstract>This paper presents a <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> for Arabic morphological disambiguation based on Recurrent Neural Networks (RNN). We train Long Short-Term Memory (LSTM) cells in several configurations and embedding levels to model the various morphological features. Our experiments show that these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> outperform state-of-the-art <a href="https://en.wikipedia.org/wiki/System">systems</a> without explicit use of <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a>. However, adding learning features from a <a href="https://en.wikipedia.org/wiki/Morphological_analysis">morphological analyzer</a> to model the space of possible analyses provides additional improvement. We make use of the resulting <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological models</a> for scoring and ranking the analyses of the morphological analyzer for morphological disambiguation. The results show significant gains in <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> across several evaluation metrics. Our <a href="https://en.wikipedia.org/wiki/System">system</a> results in 4.4 % absolute increase over the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> in full morphological analysis accuracy (30.6 % relative error reduction), and 10.6 % (31.5 % relative error reduction) for out-of-vocabulary words.</abstract>
      <bibkey>zalmout-habash-2017-dont</bibkey>
    </paper>
    <paper id="74">
      <title>Paradigm Completion for Derivational Morphology</title>
      <author><first>Ryan</first> <last>Cotterell</last></author>
      <author><first>Ekaterina</first> <last>Vylomova</last></author>
      <author><first>Huda</first> <last>Khayrallah</last></author>
      <author><first>Christo</first> <last>Kirov</last></author>
      <author><first>David</first> <last>Yarowsky</last></author>
      <pages>714–720</pages>
      <url hash="02de052d">D17-1074</url>
      <doi>10.18653/v1/D17-1074</doi>
      <abstract>The generation of complex derived word forms has been an overlooked problem in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> ; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of <a href="https://en.wikipedia.org/wiki/Morphological_derivation">derivational morphology</a>, and introduce the task of <a href="https://en.wikipedia.org/wiki/Morphological_derivation">derivational paradigm completion</a> as a parallel to <a href="https://en.wikipedia.org/wiki/Morphological_derivation">inflectional paradigm completion</a>. State-of-the-art neural models adapted from the inflection task are able to learn the range of derivation patterns, and outperform a <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">non-neural baseline</a> by 16.4 %. However, due to semantic, historical, and lexical considerations involved in <a href="https://en.wikipedia.org/wiki/Morphological_derivation">derivational morphology</a>, future work will be needed to achieve performance parity with inflection-generating systems.</abstract>
      <bibkey>cotterell-etal-2017-paradigm</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/nombank">NomBank</pwcdataset>
    </paper>
    <paper id="76">
      <title>Do LSTMs really work so well for PoS tagging?   A replication study<fixed-case>LSTM</fixed-case>s really work so well for <fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> tagging? – A replication study</title>
      <author><first>Tobias</first> <last>Horsmann</last></author>
      <author><first>Torsten</first> <last>Zesch</last></author>
      <pages>727–736</pages>
      <url hash="a31540b1">D17-1076</url>
      <doi>10.18653/v1/D17-1076</doi>
      <abstract>A recent study by Plank et al. (2016) found that LSTM-based PoS taggers considerably improve over the current state-of-the-art when evaluated on the corpora of the Universal Dependencies project that use a coarse-grained tagset. We replicate this study using a fresh collection of 27 corpora of 21 languages that are annotated with fine-grained tagsets of varying size. Our replication confirms the result in general, and we additionally find that the advantage of <a href="https://en.wikipedia.org/wiki/Linear_time-invariant_system">LSTMs</a> is even bigger for larger tagsets. However, we also find that for the very large tagsets of morphologically rich languages, hand-crafted morphological lexicons are still necessary to reach state-of-the-art performance.</abstract>
      <bibkey>horsmann-zesch-2017-lstms</bibkey>
    </paper>
    <paper id="77">
      <title>The Labeled Segmentation of Printed Books</title>
      <author><first>Lara</first> <last>McConnaughey</last></author>
      <author><first>Jennifer</first> <last>Dai</last></author>
      <author><first>David</first> <last>Bamman</last></author>
      <pages>737–747</pages>
      <url hash="7afed7ca">D17-1077</url>
      <doi>10.18653/v1/D17-1077</doi>
      <abstract>We introduce the task of book structure labeling : segmenting and assigning a fixed category (such as Table of Contents, Preface, Index) to the document structure of printed books. We manually annotate the page-level structural categories for a large dataset totaling 294,816 pages in 1,055 books evenly sampled from 1750-1922, and present empirical results comparing the performance of several classes of models. The best-performing <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, a bidirectional LSTM with rich features, achieves an overall <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 95.8 and a class-balanced macro F-score of 71.4.</abstract>
      <bibkey>mcconnaughey-etal-2017-labeled</bibkey>
    </paper>
    <paper id="79">
      <title>Word-Context Character Embeddings for Chinese Word Segmentation<fixed-case>C</fixed-case>hinese Word Segmentation</title>
      <author><first>Hao</first> <last>Zhou</last></author>
      <author><first>Zhenting</first> <last>Yu</last></author>
      <author><first>Yue</first> <last>Zhang</last></author>
      <author><first>Shujian</first> <last>Huang</last></author>
      <author><first>Xinyu</first> <last>Dai</last></author>
      <author><first>Jiajun</first> <last>Chen</last></author>
      <pages>760–766</pages>
      <url hash="0c297858">D17-1079</url>
      <doi>10.18653/v1/D17-1079</doi>
      <abstract>Neural parsers have benefited from automatically labeled data via dependency-context word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method improves state-of-the-art neural word segmentation models significantly, beating tri-training baselines for leveraging auto-segmented data.</abstract>
      <bibkey>zhou-etal-2017-word</bibkey>
    </paper>
    <paper id="80">
      <title>Segmentation-Free Word Embedding for Unsegmented Languages</title>
      <author><first>Takamasa</first> <last>Oshikiri</last></author>
      <pages>767–772</pages>
      <url hash="b2b349c8">D17-1080</url>
      <doi>10.18653/v1/D17-1080</doi>
      <abstract>In this paper, we propose a new pipeline of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> for unsegmented languages, called segmentation-free word embedding, which does not require <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a> as a preprocessing step. Unlike space-delimited languages, unsegmented languages, such as <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a> and <a href="https://en.wikipedia.org/wiki/Japanese_language">Japanese</a>, require <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a> as a preprocessing step. However, <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a>, that often requires manually annotated resources, is difficult and expensive, and unavoidable errors in <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmentation</a> affect downstream tasks. To avoid these problems in learning word vectors of unsegmented languages, we consider word co-occurrence statistics over all possible candidates of segmentations based on frequent character n-grams instead of segmented sentences provided by conventional word segmenters. Our experiments of noun category prediction tasks on raw Twitter, <a href="https://en.wikipedia.org/wiki/Sina_Weibo">Weibo</a>, and Wikipedia corpora show that the proposed method outperforms the conventional approaches that require <a href="https://en.wikipedia.org/wiki/Word_segmentation">word segmenters</a>.</abstract>
      <bibkey>oshikiri-2017-segmentation</bibkey>
    </paper>
    <paper id="81">
      <title>From Textbooks to Knowledge : A Case Study in Harvesting Axiomatic Knowledge from <a href="https://en.wikipedia.org/wiki/Textbook">Textbooks</a> to Solve Geometry Problems</title>
      <author><first>Mrinmaya</first> <last>Sachan</last></author>
      <author><first>Kumar</first> <last>Dubey</last></author>
      <author><first>Eric</first> <last>Xing</last></author>
      <pages>773–784</pages>
      <url hash="0f3330f2">D17-1081</url>
      <doi>10.18653/v1/D17-1081</doi>
      <abstract>Textbooks are rich sources of information. Harvesting structured knowledge from <a href="https://en.wikipedia.org/wiki/Textbook">textbooks</a> is a key challenge in many <a href="https://en.wikipedia.org/wiki/Educational_technology">educational applications</a>. As a case study, we present an approach for harvesting structured axiomatic knowledge from math textbooks. Our approach uses rich contextual and typographical features extracted from raw textbooks. It leverages the <a href="https://en.wikipedia.org/wiki/Redundancy_(information_theory)">redundancy</a> and shared ordering across multiple textbooks to further refine the harvested axioms. These <a href="https://en.wikipedia.org/wiki/Axiom">axioms</a> are then parsed into <a href="https://en.wikipedia.org/wiki/Rule_of_inference">rules</a> that are used to improve the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> in solving <a href="https://en.wikipedia.org/wiki/Geometry">geometry problems</a>.</abstract>
      <bibkey>sachan-etal-2017-textbooks</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/geos">GeoS</pwcdataset>
    </paper>
    <paper id="83">
      <title>Beyond Sentential Semantic Parsing : Tackling the Math SAT with a Cascade of Tree Transducers<fixed-case>SAT</fixed-case> with a Cascade of Tree Transducers</title>
      <author><first>Mark</first> <last>Hopkins</last></author>
      <author><first>Cristian</first> <last>Petrescu-Prahova</last></author>
      <author><first>Roie</first> <last>Levin</last></author>
      <author><first>Ronan</first> <last>Le Bras</last></author>
      <author><first>Alvaro</first> <last>Herrasti</last></author>
      <author><first>Vidur</first> <last>Joshi</last></author>
      <pages>795–804</pages>
      <url hash="f1afa5f6">D17-1083</url>
      <doi>10.18653/v1/D17-1083</doi>
      <abstract>We present an approach for answering questions that span multiple sentences and exhibit sophisticated cross-sentence anaphoric phenomena, evaluating on a rich source of such questions   the math portion of the Scholastic Aptitude Test (SAT). By using a tree transducer cascade as its basic architecture, our <a href="https://en.wikipedia.org/wiki/System">system</a> propagates uncertainty from multiple sources (e.g. coreference resolution or verb interpretation) until it can be confidently resolved. Experiments show the first-ever results 43 % <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> and 91 % <a href="https://en.wikipedia.org/wiki/Precision_(computer_science)">precision</a>) on SAT algebra word problems. We also apply our system to the public Dolphin algebra question set, and improve the state-of-the-art F1-score from 73.9 % to 77.0 %.</abstract>
      <bibkey>hopkins-etal-2017-beyond</bibkey>
    </paper>
    <paper id="84">
      <title>Learning Fine-Grained Expressions to Solve Math Word Problems</title>
      <author><first>Danqing</first> <last>Huang</last></author>
      <author><first>Shuming</first> <last>Shi</last></author>
      <author><first>Chin-Yew</first> <last>Lin</last></author>
      <author><first>Jian</first> <last>Yin</last></author>
      <pages>805–814</pages>
      <url hash="01aca850">D17-1084</url>
      <doi>10.18653/v1/D17-1084</doi>
      <abstract>This paper presents a novel template-based method to solve math word problems. This method learns the mappings between math concept phrases in math word problems and their math expressions from training data. For each equation template, we automatically construct a rich template sketch by aggregating information from various problems with the same <a href="https://en.wikipedia.org/wiki/Template_processor">template</a>. Our approach is implemented in a two-stage system. It first retrieves a few relevant equation system templates and aligns numbers in math word problems to those <a href="https://en.wikipedia.org/wiki/Template_processor">templates</a> for candidate equation generation. It then does a fine-grained inference to obtain the final answer. Experiment results show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> achieves an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 28.4 % on the linear Dolphin18 K benchmark, which is 10 % (54 % relative) higher than previous state-of-the-art systems while achieving an accuracy increase of 12 % (59 % relative) on the TS6 benchmark subset.</abstract>
      <bibkey>huang-etal-2017-learning</bibkey>
    </paper>
    <paper id="85">
      <title>Structural Embedding of Syntactic Trees for Machine Comprehension</title>
      <author><first>Rui</first> <last>Liu</last></author>
      <author><first>Junjie</first> <last>Hu</last></author>
      <author><first>Wei</first> <last>Wei</last></author>
      <author><first>Zi</first> <last>Yang</last></author>
      <author><first>Eric</first> <last>Nyberg</last></author>
      <pages>815–824</pages>
      <url hash="9dcddde4">D17-1085</url>
      <doi>10.18653/v1/D17-1085</doi>
      <abstract>Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods.</abstract>
      <bibkey>liu-etal-2017-structural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="86">
      <title>World Knowledge for Reading Comprehension : Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions<fixed-case>LSTM</fixed-case>s Using External Descriptions</title>
      <author><first>Teng</first> <last>Long</last></author>
      <author><first>Emmanuel</first> <last>Bengio</last></author>
      <author><first>Ryan</first> <last>Lowe</last></author>
      <author><first>Jackie Chi Kit</first> <last>Cheung</last></author>
      <author><first>Doina</first> <last>Precup</last></author>
      <pages>825–834</pages>
      <url hash="af762223">D17-1086</url>
      <doi>10.18653/v1/D17-1086</doi>
      <abstract>Humans interpret texts with respect to some background information, or <a href="https://en.wikipedia.org/wiki/World_knowledge">world knowledge</a>, and we would like to develop automatic reading comprehension systems that can do the same. In this paper, we introduce a <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> and several <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> to drive progress towards this goal. In particular, we propose the task of rare entity prediction : given a web document with several entities removed, models are tasked with predicting the correct missing entities conditioned on the document context and the lexical resources. This <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is challenging due to the diversity of language styles and the extremely large number of rare entities. We propose two recurrent neural network architectures which make use of <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">external knowledge</a> in the form of <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity descriptions</a>. Our experiments show that our hierarchical LSTM model performs significantly better at the rare entity prediction task than those that do not make use of external resources.</abstract>
      <bibkey>long-etal-2017-world</bibkey>
    </paper>
    <paper id="87">
      <title>Two-Stage Synthesis Networks for <a href="https://en.wikipedia.org/wiki/Transfer_learning">Transfer Learning</a> in Machine Comprehension</title>
      <author><first>David</first> <last>Golub</last></author>
      <author><first>Po-Sen</first> <last>Huang</last></author>
      <author><first>Xiaodong</first> <last>He</last></author>
      <author><first>Li</first> <last>Deng</last></author>
      <pages>835–844</pages>
      <url hash="449e4ade">D17-1087</url>
      <doi>10.18653/v1/D17-1087</doi>
      <abstract>We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network. Given a high performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs. Using the proposed synthesis network with a pretrained model on the SQuAD dataset, we achieve an <a href="https://en.wikipedia.org/wiki/F-number">F1 measure</a> of 46.6 % on the challenging NewsQA dataset, approaching performance of in-domain models (F1 measure of 50.0 %) and outperforming the out-of-domain baseline by 7.6 %, without use of provided annotations.</abstract>
      <bibkey>golub-etal-2017-two</bibkey>
      <pwccode url="https://github.com/davidgolub/QuestionGeneration" additional="true">davidgolub/QuestionGeneration</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="88">
      <title>Deep Neural Solver for Math Word Problems</title>
      <author><first>Yan</first> <last>Wang</last></author>
      <author><first>Xiaojiang</first> <last>Liu</last></author>
      <author><first>Shuming</first> <last>Shi</last></author>
      <pages>845–854</pages>
      <url hash="505802f7">D17-1088</url>
      <doi>10.18653/v1/D17-1088</doi>
      <abstract>This paper presents a deep neural solver to automatically solve math word problems. In contrast to previous statistical learning approaches, we directly translate math word problems to equation templates using a recurrent neural network (RNN) model, without sophisticated <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a>. We further design a hybrid model that combines the RNN model and a similarity-based retrieval model to achieve additional performance improvement. Experiments conducted on a large dataset show that the RNN model and the hybrid model significantly outperform state-of-the-art statistical learning methods for math word problem solving.</abstract>
      <bibkey>wang-etal-2017-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/math23k">Math23K</pwcdataset>
    </paper>
    <paper id="90">
      <title>Question Generation for Question Answering</title>
      <author><first>Nan</first> <last>Duan</last></author>
      <author><first>Duyu</first> <last>Tang</last></author>
      <author><first>Peng</first> <last>Chen</last></author>
      <author><first>Ming</first> <last>Zhou</last></author>
      <pages>866–874</pages>
      <url hash="2cdbb7c3">D17-1090</url>
      <doi>10.18653/v1/D17-1090</doi>
      <abstract>This paper presents how to generate questions from given passages using <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>, where large scale QA pairs are automatically crawled and processed from Community-QA website, and used as training data. The contribution of the paper is 2-fold : First, two types of question generation approaches are proposed, one is a retrieval-based method using convolution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN) ; Second, we show how to leverage the generated questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA</a> improvement can be achieved.</abstract>
      <bibkey>duan-etal-2017-question</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="91">
      <title>Learning to Paraphrase for Question Answering</title>
      <author><first>Li</first> <last>Dong</last></author>
      <author><first>Jonathan</first> <last>Mallinson</last></author>
      <author><first>Siva</first> <last>Reddy</last></author>
      <author><first>Mirella</first> <last>Lapata</last></author>
      <pages>875–886</pages>
      <url hash="ca4a0a89">D17-1091</url>
      <doi>10.18653/v1/D17-1091</doi>
      <abstract>Question answering (QA) systems are sensitive to the many different ways <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> expresses the same information need. In this paper we turn to <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is trained end-to-end using question-answer pairs as a <a href="https://en.wikipedia.org/wiki/Signal_processing">supervision signal</a>. A question and its <a href="https://en.wikipedia.org/wiki/Paraphrase">paraphrases</a> serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA</a> over <a href="https://en.wikipedia.org/wiki/Freebase">Freebase</a> and answer sentence selection. Experimental results on three <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> show that our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> consistently improves performance, achieving competitive results despite the use of simple <a href="https://en.wikipedia.org/wiki/Quality_assurance">QA models</a>.</abstract>
      <bibkey>dong-etal-2017-learning</bibkey>
    </paper>
    <paper id="92">
      <title>Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture<fixed-case>LSTM</fixed-case>-based Architecture</title>
      <author><first>Yuanliang</first> <last>Meng</last></author>
      <author><first>Anna</first> <last>Rumshisky</last></author>
      <author><first>Alexey</first> <last>Romanov</last></author>
      <pages>887–896</pages>
      <url hash="7f1a2d81">D17-1092</url>
      <doi>10.18653/v1/D17-1092</doi>
      <abstract>In this paper, we propose to use a set of simple, uniform in architecture LSTM-based models to recover different kinds of temporal relations from text. Using the shortest dependency path between entities as input, the same architecture is used to extract intra-sentence, cross-sentence, and document creation time relations. A double-checking technique reverses entity pairs in <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>, boosting the <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> of positive cases and reducing misclassifications between opposite classes. An efficient pruning algorithm resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-of-the-art methods by a large margin. We also conduct intrinsic evaluation and post state-of-the-art results on Timebank-Dense.</abstract>
      <bibkey>meng-etal-2017-temporal</bibkey>
    </paper>
    <paper id="93">
      <title>Ranking Kernels for Structures and Embeddings : A Hybrid Preference and Classification Model</title>
      <author><first>Kateryna</first> <last>Tymoshenko</last></author>
      <author><first>Daniele</first> <last>Bonadiman</last></author>
      <author><first>Alessandro</first> <last>Moschitti</last></author>
      <pages>897–902</pages>
      <url hash="acf570aa">D17-1093</url>
      <doi>10.18653/v1/D17-1093</doi>
      <abstract>Recent work has shown that Tree Kernels (TKs) and Convolutional Neural Networks (CNNs) obtain the state of the art in answer sentence reranking. Additionally, their combination used in Support Vector Machines (SVMs) is promising as it can exploit both the syntactic patterns captured by TKs and the embeddings learned by CNNs. However, the <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> are constructed according to a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification function</a>, which is not directly exploitable in the preference ranking algorithm of SVMs. In this work, we propose a new hybrid approach combining preference ranking applied to TKs and pointwise ranking applied to CNNs. We show that our approach produces better results on two well-known and rather different datasets : WikiQA for answer sentence selection and SemEval cQA for comment selection in Community Question Answering.</abstract>
      <bibkey>tymoshenko-etal-2017-ranking</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="94">
      <title>Recovering Question Answering Errors via Query Revision</title>
      <author><first>Semih</first> <last>Yavuz</last></author>
      <author><first>Izzeddin</first> <last>Gur</last></author>
      <author><first>Yu</first> <last>Su</last></author>
      <author><first>Xifeng</first> <last>Yan</last></author>
      <pages>903–909</pages>
      <url hash="65e8e8d1">D17-1094</url>
      <doi>10.18653/v1/D17-1094</doi>
      <attachment type="attachment" hash="972dbb66">D17-1094.Attachment.zip</attachment>
      <abstract>The existing factoid QA systems often lack a post-inspection component that can help models recover from their own mistakes. In this work, we propose to crosscheck the corresponding <a href="https://en.wikipedia.org/wiki/Binary_relation">KB relations</a> behind the predicted answers and identify potential inconsistencies. Instead of developing a new <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that accepts evidences collected from these relations, we choose to plug them back to the original questions directly and check if the revised question makes sense or not. A bidirectional LSTM is applied to encode revised questions. We develop a scoring mechanism over the revised question encodings to refine the predictions of a base QA system. This approach can improve the <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> of <a href="https://en.wikipedia.org/wiki/STAGG">STAGG</a> (Yih et al., 2015), one of the leading QA systems, from 52.5 % to 53.9 % on WEBQUESTIONS data.</abstract>
      <bibkey>yavuz-etal-2017-recovering</bibkey>
    </paper>
    <paper id="95">
      <title>An empirical study on the effectiveness of <a href="https://en.wikipedia.org/wiki/Digital_image">images</a> in Multimodal Neural Machine Translation</title>
      <author><first>Jean-Benoit</first> <last>Delbrouck</last></author>
      <author><first>Stéphane</first> <last>Dupont</last></author>
      <pages>910–919</pages>
      <url hash="ca6a1026">D17-1095</url>
      <doi>10.18653/v1/D17-1095</doi>
      <abstract>In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the <a href="https://en.wikipedia.org/wiki/Translation_(biology)">translation</a>. At every step, the <a href="https://en.wikipedia.org/wiki/Encoder">decoder</a> uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multi-modal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multi-modal translation task (English, image   German) and evaluate the ability of the model to make use of images to improve <a href="https://en.wikipedia.org/wiki/Translation">translation</a>. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while <a href="https://en.wikipedia.org/wiki/Translation">translating</a>.</abstract>
      <bibkey>delbrouck-dupont-2017-empirical</bibkey>
    </paper>
    <paper id="96">
      <title>Sound-Word2Vec : Learning Word Representations Grounded in Sounds<fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec: Learning Word Representations Grounded in Sounds</title>
      <author><first>Ashwin</first> <last>Vijayakumar</last></author>
      <author><first>Ramakrishna</first> <last>Vedantam</last></author>
      <author><first>Devi</first> <last>Parikh</last></author>
      <pages>920–925</pages>
      <url hash="a0764cfb">D17-1096</url>
      <doi>10.18653/v1/D17-1096</doi>
      <abstract>To be able to interact better with humans, it is crucial for machines to understand sound   a primary modality of human perception. Previous works have used <a href="https://en.wikipedia.org/wiki/Sound">sound</a> to learn <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a> for improved generic semantic similarity assessment. In this work, we treat <a href="https://en.wikipedia.org/wiki/Sound">sound</a> as a first-class citizen, studying downstream 6textual tasks which require aural grounding. To this end, we propose sound-word2vec   a new embedding scheme that learns specialized word embeddings grounded in sounds. For example, we learn that two seemingly (semantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make. Our embeddings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering <a href="https://en.wikipedia.org/wiki/Foley_(filmmaking)">Foley sound effects</a> (used in movies). Moreover, our embedding space captures interesting dependencies between words and <a href="https://en.wikipedia.org/wiki/Onomatopoeia">onomatopoeia</a> and outperforms prior work on aurally-relevant word relatedness datasets such as AMEN and ASLex.</abstract>
      <bibkey>vijayakumar-etal-2017-sound</bibkey>
    </paper>
    <paper id="97">
      <title>The Promise of Premise : Harnessing Question Premises in Visual Question Answering</title>
      <author><first>Aroma</first> <last>Mahendru</last></author>
      <author><first>Viraj</first> <last>Prabhu</last></author>
      <author><first>Akrit</first> <last>Mohapatra</last></author>
      <author><first>Dhruv</first> <last>Batra</last></author>
      <author><first>Stefan</first> <last>Lee</last></author>
      <pages>926–935</pages>
      <url hash="9b03a72a">D17-1097</url>
      <doi>10.18653/v1/D17-1097</doi>
      <abstract>In this paper, we make a simple observation that questions about images often contain premises   objects and relationships implied by the question   and that reasoning about premises can help Visual Question Answering (VQA) models respond more intelligently to irrelevant or previously unseen questions. When presented with a question that is irrelevant to an <a href="https://en.wikipedia.org/wiki/Image">image</a>, state-of-the-art VQA models will still answer purely based on learned language biases, resulting in non-sensical or even misleading answers. We note that a visual question is irrelevant to an <a href="https://en.wikipedia.org/wiki/Image">image</a> if at least one of its premises is false (i.e. not depicted in the image). We leverage this observation to construct a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> for Question Relevance Prediction and Explanation (QRPE) by searching for false premises. We train novel question relevance detection models and show that <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that reason about premises consistently outperform <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that do not. We also find that forcing standard VQA models to reason about premises during training can lead to improvements on tasks requiring compositional reasoning.</abstract>
      <bibkey>mahendru-etal-2017-promise</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="98">
      <title>Guided Open Vocabulary Image Captioning with Constrained Beam Search</title>
      <author><first>Peter</first> <last>Anderson</last></author>
      <author><first>Basura</first> <last>Fernando</last></author>
      <author><first>Mark</first> <last>Johnson</last></author>
      <author><first>Stephen</first> <last>Gould</last></author>
      <pages>936–945</pages>
      <url hash="8469fcaa">D17-1098</url>
      <doi>10.18653/v1/D17-1098</doi>
      <attachment type="attachment" hash="1fcd4b47">D17-1098.Attachment.zip</attachment>
      <abstract>Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the <a href="https://en.wikipedia.org/wiki/Machine_learning">learning algorithm</a>. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.</abstract>
      <bibkey>anderson-etal-2017-guided</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="99">
      <title>Zero-Shot Activity Recognition with Verb Attribute Induction</title>
      <author><first>Rowan</first> <last>Zellers</last></author>
      <author><first>Yejin</first> <last>Choi</last></author>
      <pages>946–958</pages>
      <url hash="1c96bf9e">D17-1099</url>
      <doi>10.18653/v1/D17-1099</doi>
      <abstract>In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs. For example, the verb salute has several properties, such as being a light movement, a <a href="https://en.wikipedia.org/wiki/Social_actions">social act</a>, and short in duration. We use these attributes as the internal mapping between visual and textual representations to reason about a previously unseen action. In contrast to much prior work that assumes access to gold standard attributes for zero-shot classes and focuses primarily on object attributes, our model uniquely learns to infer action attributes from dictionary definitions and distributed word representations. Experimental results confirm that action attributes inferred from <a href="https://en.wikipedia.org/wiki/Language">language</a> can provide a predictive signal for zero-shot prediction of previously unseen activities.</abstract>
      <bibkey>zellers-choi-2017-zero</bibkey>
      <pwccode url="https://github.com/uwnlp/verb-attributes" additional="true">uwnlp/verb-attributes</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="100">
      <title>Deriving continous grounded meaning representations from referentially structured multimodal contexts</title>
      <author><first>Sina</first> <last>Zarrieß</last></author>
      <author><first>David</first> <last>Schlangen</last></author>
      <pages>959–965</pages>
      <url hash="3c1a88ce">D17-1100</url>
      <doi>10.18653/v1/D17-1100</doi>
      <abstract>Corpora of referring expressions paired with their visual referents are a good source for learning word meanings directly grounded in visual representations. Here, we explore additional ways of extracting from them word representations linked to multi-modal context : through expressions that refer to the same object, and through expressions that refer to different objects in the same scene. We show that continuous meaning representations derived from these contexts capture complementary aspects of similarity,, even if not outperforming textual embeddings trained on very large amounts of raw text when tested on standard similarity benchmarks. We propose a new task for evaluating grounded meaning representationsdetection of potentially co-referential phrasesand show that it requires precise denotational representations of attribute meanings, which our method provides.</abstract>
      <bibkey>zarriess-schlangen-2017-deriving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/refcoco">RefCOCO</pwcdataset>
    </paper>
    <paper id="102">
      <title>Video Highlight Prediction Using Audience Chat Reactions</title>
      <author><first>Cheng-Yang</first> <last>Fu</last></author>
      <author><first>Joon</first> <last>Lee</last></author>
      <author><first>Mohit</first> <last>Bansal</last></author>
      <author><first>Alexander</first> <last>Berg</last></author>
      <pages>972–978</pages>
      <url hash="777cba37">D17-1102</url>
      <doi>10.18653/v1/D17-1102</doi>
      <attachment type="attachment" hash="6562c437">D17-1102.Attachment.zip</attachment>
      <abstract>Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis. We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both <a href="https://en.wikipedia.org/wiki/English_language">English</a> and traditional Chinese. We present a novel dataset based on <a href="https://en.wikipedia.org/wiki/League_of_Legends_World_Championship">League of Legends championships</a> recorded from North American and Taiwanese Twitch.tv channels (will be released for further research), and demonstrate strong results on these using multimodal, character-level CNN-RNN model architectures.</abstract>
      <bibkey>fu-etal-2017-video</bibkey>
    </paper>
    <paper id="104">
      <title>Evaluating Hierarchies of Verb Argument Structure with <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">Hierarchical Clustering</a></title>
      <author><first>Jesse</first> <last>Mu</last></author>
      <author><first>Joshua K.</first> <last>Hartshorne</last></author>
      <author><first>Timothy</first> <last>O’Donnell</last></author>
      <pages>986–991</pages>
      <url hash="ac27fd91">D17-1104</url>
      <doi>10.18653/v1/D17-1104</doi>
      <attachment type="attachment" hash="6f0f7f9d">D17-1104.Attachment.zip</attachment>
      <abstract>Verbs can only be used with a few specific arrangements of their arguments (syntactic frames). Most theorists note that verbs can be organized into a hierarchy of verb classes based on the frames they admit. Here we show that such a hierarchy is objectively well-supported by the patterns of verbs and frames in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, since a systematic hierarchical clustering algorithm converges on the same structure as the handcrafted taxonomy of VerbNet, a broad-coverage verb lexicon. We also show that the <a href="https://en.wikipedia.org/wiki/Hierarchy">hierarchies</a> capture meaningful psychological dimensions of generalization by predicting novel verb coercions by human participants. We discuss limitations of a simple hierarchical representation and suggest similar approaches for identifying the representations underpinning verb argument structure.</abstract>
      <bibkey>mu-etal-2017-evaluating</bibkey>
    </paper>
    <paper id="105">
      <title>Incorporating Global Visual Features into Attention-based Neural Machine Translation.</title>
      <author><first>Iacer</first> <last>Calixto</last></author>
      <author><first>Qun</first> <last>Liu</last></author>
      <pages>992–1003</pages>
      <url hash="eaf4bb7f">D17-1105</url>
      <doi>10.18653/v1/D17-1105</doi>
      <abstract>We introduce multi-modal, attention-based neural machine translation (NMT) models which incorporate visual features into different parts of both the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and the <a href="https://en.wikipedia.org/wiki/Codec">decoder</a>. Global image features are extracted using a pre-trained convolutional neural network and are incorporated (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate translations into <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/German_language">German</a>, how different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional <a href="https://en.wikipedia.org/wiki/Data">data</a> have a positive impact on multi-modal NMT models. We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this <a href="https://en.wikipedia.org/wiki/Data_set">data set</a>.</abstract>
      <bibkey>calixto-liu-2017-incorporating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="106">
      <title>Mapping Instructions and Visual Observations to Actions with <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a></title>
      <author><first>Dipendra</first> <last>Misra</last></author>
      <author><first>John</first> <last>Langford</last></author>
      <author><first>Yoav</first> <last>Artzi</last></author>
      <pages>1004–1015</pages>
      <url hash="d152a133">D17-1106</url>
      <doi>10.18653/v1/D17-1106</doi>
      <attachment type="attachment" hash="01f1d360">D17-1106.Attachment.pdf</attachment>
      <abstract>We propose to directly map raw visual observations and <a href="https://en.wikipedia.org/wiki/Text-based_user_interface">text input</a> to actions for <a href="https://en.wikipedia.org/wiki/Instruction_set_architecture">instruction execution</a>. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> to jointly reason about linguistic and visual input. We use <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> in a contextual bandit setting to train a neural network agent. To guide the agent’s exploration, we use reward shaping with different forms of <a href="https://en.wikipedia.org/wiki/Supervisor">supervision</a>. Our approach does not require intermediate representations, planning procedures, or training different <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a>. We evaluate in a simulated environment, and show significant improvements over <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> and common reinforcement learning variants.</abstract>
      <bibkey>misra-etal-2017-mapping</bibkey>
      <pwccode url="https://github.com/clic-lab/blocks" additional="false">clic-lab/blocks</pwccode>
    </paper>
    <paper id="107">
      <title>An analysis of <a href="https://en.wikipedia.org/wiki/Eye_movement">eye-movements</a> during reading for the detection of mild cognitive impairment</title>
      <author><first>Kathleen C.</first> <last>Fraser</last></author>
      <author><first>Kristina</first> <last>Lundholm Fors</last></author>
      <author><first>Dimitrios</first> <last>Kokkinakis</last></author>
      <author><first>Arto</first> <last>Nordlund</last></author>
      <pages>1016–1026</pages>
      <url hash="64331c13">D17-1107</url>
      <doi>10.18653/v1/D17-1107</doi>
      <abstract>We present a machine learning analysis of eye-tracking data for the detection of mild cognitive impairment, a decline in cognitive abilities that is associated with an increased risk of developing dementia. We compare two experimental configurations (reading aloud versus reading silently), as well as two methods of combining information from the two trials (concatenation and merging). Additionally, we annotate the words being read with information about their frequency and <a href="https://en.wikipedia.org/wiki/Syntactic_category">syntactic category</a>, and use these annotations to generate new <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a>. Ultimately, we are able to distinguish between participants with and without <a href="https://en.wikipedia.org/wiki/Cognitive_deficit">cognitive impairment</a> with up to 86 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>.</abstract>
      <bibkey>fraser-etal-2017-analysis</bibkey>
    </paper>
    <paper id="108">
      <title>A Structured Learning Approach to Temporal Relation Extraction</title>
      <author><first>Qiang</first> <last>Ning</last></author>
      <author><first>Zhili</first> <last>Feng</last></author>
      <author><first>Dan</first> <last>Roth</last></author>
      <pages>1027–1037</pages>
      <url hash="30ad47e4">D17-1108</url>
      <doi>10.18653/v1/D17-1108</doi>
      <abstract>Identifying temporal relations between events is an essential step towards <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>. However, the temporal relation between two events in a story depends on, and is often dictated by, relations among other events. Consequently, effectively identifying temporal relations between events is a challenging problem even for human annotators. This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge. As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">methods</a>. As we show, the proposed approach results in significant improvements on the two commonly used <a href="https://en.wikipedia.org/wiki/Data_set">data sets</a> for this problem.</abstract>
      <video href="https://vimeo.com/238231266" />
      <bibkey>ning-etal-2017-structured</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tempeval-3">TempEval-3</pwcdataset>
    </paper>
    <paper id="109">
      <title>Importance sampling for unbiased on-demand evaluation of knowledge base population</title>
      <author><first>Arun</first> <last>Chaganty</last></author>
      <author><first>Ashwin</first> <last>Paranjape</last></author>
      <author><first>Percy</first> <last>Liang</last></author>
      <author><first>Christopher D.</first> <last>Manning</last></author>
      <pages>1038–1048</pages>
      <url hash="40418b55">D17-1109</url>
      <doi>10.18653/v1/D17-1109</doi>
      <abstract>Knowledge base population (KBP) systems take in a large document corpus and extract entities and their relations. Thus far, KBP evaluation has relied on judgements on the pooled predictions of existing systems. We show that this <a href="https://en.wikipedia.org/wiki/Evaluation">evaluation</a> is problematic : when a new system predicts a previously unseen relation, it is penalized even if it is correct. This leads to significant bias against new systems, which counterproductively discourages innovation in the field. Our first contribution is a new importance-sampling based evaluation which corrects for this bias by annotating a new system’s predictions on-demand via <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a>. We show this eliminates bias and reduces variance using data from the 2015 TAC KBP task. Our second contribution is an implementation of our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> made publicly available as an online KBP evaluation service. We pilot the service by testing diverse state-of-the-art systems on the TAC KBP 2016 corpus and obtain accurate scores in a cost effective manner.</abstract>
      <video href="https://vimeo.com/238233745" />
      <bibkey>chaganty-etal-2017-importance</bibkey>
    </paper>
    <paper id="110">
      <title>PACRR : A Position-Aware Neural IR Model for Relevance Matching<fixed-case>PACRR</fixed-case>: A Position-Aware Neural <fixed-case>IR</fixed-case> Model for Relevance Matching</title>
      <author><first>Kai</first> <last>Hui</last></author>
      <author><first>Andrew</first> <last>Yates</last></author>
      <author><first>Klaus</first> <last>Berberich</last></author>
      <author><first>Gerard</first> <last>de Melo</last></author>
      <pages>1049–1058</pages>
      <url hash="84d1f3d4">D17-1110</url>
      <doi>10.18653/v1/D17-1110</doi>
      <abstract>In order to adopt <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> for <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a>, <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> are needed that can capture all relevant information required to assess the relevance of a document to a given user query. While previous works have successfully captured unigram term matches, how to fully employ position-dependent information such as proximity and term dependencies has been insufficiently explored. In this work, we propose a novel neural IR model named PACRR aiming at better modeling position-dependent interactions between a query and a document. Extensive experiments on six years’ TREC Web Track data confirm that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> yields better results under multiple benchmarks.</abstract>
      <video href="https://vimeo.com/238235171" />
      <bibkey>hui-etal-2017-pacrr</bibkey>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="111">
      <title>Globally Normalized Reader</title>
      <author><first>Jonathan</first> <last>Raiman</last></author>
      <author><first>John</first> <last>Miller</last></author>
      <pages>1059–1069</pages>
      <url hash="d2a93032">D17-1111</url>
      <doi>10.18653/v1/D17-1111</doi>
      <abstract>Rapid progress has been made towards question answering (QA) systems that can extract answers from text. Existing neural approaches make use of expensive bi-directional attention mechanisms or score all possible answer spans, limiting <a href="https://en.wikipedia.org/wiki/Scalability">scalability</a>. We propose instead to cast extractive QA as an iterative search problem : select the answer’s sentence, start word, and end word. This representation reduces the space of each search step and allows computation to be conditionally allocated to promising search paths. We show that globally normalizing the <a href="https://en.wikipedia.org/wiki/Decision-making">decision process</a> and back-propagating through <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> makes this <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a> viable and learning efficient. We empirically demonstrate the benefits of this approach using our model, Globally Normalized Reader (GNR), which achieves the second highest single model performance on the Stanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow. We also introduce a data-augmentation method to produce semantically valid examples by aligning named entities to a <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a> and swapping them with new entities of the same type. This method improves the performance of all <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> considered in this work and is of independent interest for a variety of NLP tasks.</abstract>
      <video href="https://vimeo.com/238233236" />
      <bibkey>raiman-miller-2017-globally</bibkey>
      <pwccode url="https://github.com/baidu-research/GloballyNormalizedReader" additional="false">baidu-research/GloballyNormalizedReader</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="112">
      <title>Speech segmentation with a neural encoder model of working memory</title>
      <author><first>Micha</first> <last>Elsner</last></author>
      <author><first>Cory</first> <last>Shain</last></author>
      <pages>1070–1080</pages>
      <url hash="9ec63546">D17-1112</url>
      <doi>10.18653/v1/D17-1112</doi>
      <attachment type="attachment" hash="dd4ce957">D17-1112.Attachment.zip</attachment>
      <abstract>We present the first unsupervised LSTM speech segmenter as a <a href="https://en.wikipedia.org/wiki/Cognitive_model">cognitive model</a> of the acquisition of words from unsegmented input. Cognitive biases toward phonological and syntactic predictability in <a href="https://en.wikipedia.org/wiki/Speech">speech</a> are rooted in the limitations of <a href="https://en.wikipedia.org/wiki/Memory">human memory</a> (Baddeley et al., 1998) ; compressed representations are easier to acquire and retain in memory. To model the biases introduced by these memory limitations, our system uses an LSTM-based encoder-decoder with a small number of hidden units, then searches for a segmentation that minimizes autoencoding loss. Linguistically meaningful segments (e.g. words) should share regular patterns of features that facilitate decoder performance in comparison to random segmentations, and we show that our learner discovers these <a href="https://en.wikipedia.org/wiki/Pattern">patterns</a> when trained on either phoneme sequences or raw acoustics. To our knowledge, ours is the first fully <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised system</a> to be able to segment both symbolic and acoustic representations of speech.</abstract>
      <video href="https://vimeo.com/238235902" />
      <bibkey>elsner-shain-2017-speech</bibkey>
    </paper>
    <paper id="113">
      <title>Speaking, Seeing, Understanding : Correlating semantic models with conceptual representation in the brain</title>
      <author><first>Luana</first> <last>Bulat</last></author>
      <author><first>Stephen</first> <last>Clark</last></author>
      <author><first>Ekaterina</first> <last>Shutova</last></author>
      <pages>1081–1091</pages>
      <url hash="26a5397d">D17-1113</url>
      <doi>10.18653/v1/D17-1113</doi>
      <abstract>Research in <a href="https://en.wikipedia.org/wiki/Computational_semantics">computational semantics</a> is increasingly guided by our understanding of human semantic processing. However, semantic models are typically studied in the context of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing system</a> performance. In this paper, we present a systematic evaluation and comparison of a range of widely-used, state-of-the-art semantic models in their ability to predict patterns of conceptual representation in the human brain. Our results provide new insights both for the design of computational semantic models and for further research in <a href="https://en.wikipedia.org/wiki/Cognitive_neuroscience">cognitive neuroscience</a>.</abstract>
      <video href="https://vimeo.com/238235824" />
      <bibkey>bulat-etal-2017-speaking</bibkey>
    </paper>
    <paper id="114">
      <title>Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video</title>
      <author><first>Haoran</first> <last>Li</last></author>
      <author><first>Junnan</first> <last>Zhu</last></author>
      <author><first>Cong</first> <last>Ma</last></author>
      <author><first>Jiajun</first> <last>Zhang</last></author>
      <author><first>Chengqing</first> <last>Zong</last></author>
      <pages>1092–1102</pages>
      <url hash="eed7fb9d">D17-1114</url>
      <doi>10.18653/v1/D17-1114</doi>
      <abstract>The rapid increase of the multimedia data over the <a href="https://en.wikipedia.org/wiki/Internet">Internet</a> necessitates multi-modal summarization from collections of text, image, audio and video. In this work, we propose an extractive Multi-modal Summarization (MMS) method which can automatically generate a textual summary given a set of <a href="https://en.wikipedia.org/wiki/Document">documents</a>, <a href="https://en.wikipedia.org/wiki/Image">images</a>, audios and <a href="https://en.wikipedia.org/wiki/Video">videos</a> related to a specific topic. The key idea is to bridge the semantic gaps between multi-modal contents. For <a href="https://en.wikipedia.org/wiki/Audio_signal">audio information</a>, we design an approach to selectively use its <a href="https://en.wikipedia.org/wiki/Transcription_(biology)">transcription</a>. For vision information, we learn joint representations of texts and images using a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a>. Finally, all the multi-modal aspects are considered to generate the textural summary by maximizing the salience, <a href="https://en.wikipedia.org/wiki/Redundancy_(engineering)">non-redundancy</a>, <a href="https://en.wikipedia.org/wiki/Readability">readability</a> and <a href="https://en.wikipedia.org/wiki/Coverage_(statistics)">coverage</a> through budgeted optimization of submodular functions. We further introduce an <a href="https://en.wikipedia.org/wiki/Multimedia_Messaging_Service">MMS corpus</a> in <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>. The experimental results on this <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> demonstrate that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms other competitive baseline methods.</abstract>
      <video href="https://vimeo.com/238234442" />
      <bibkey>li-etal-2017-multi</bibkey>
    </paper>
    <paper id="115">
      <title>Tensor Fusion Network for Multimodal Sentiment Analysis</title>
      <author><first>Amir</first> <last>Zadeh</last></author>
      <author><first>Minghai</first> <last>Chen</last></author>
      <author><first>Soujanya</first> <last>Poria</last></author>
      <author><first>Erik</first> <last>Cambria</last></author>
      <author><first>Louis-Philippe</first> <last>Morency</last></author>
      <pages>1103–1114</pages>
      <url hash="5b251a5d">D17-1115</url>
      <doi>10.18653/v1/D17-1115</doi>
      <abstract>Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of <a href="https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis">multimodal sentiment analysis</a> as modeling intra-modality and inter-modality dynamics. We introduce a novel <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, termed Tensor Fusion Networks, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in <a href="https://en.wikipedia.org/wiki/Online_video_platform">online videos</a> as well as accompanying gestures and <a href="https://en.wikipedia.org/wiki/Human_voice">voice</a>. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis.</abstract>
      <video href="https://vimeo.com/238236114" />
      <bibkey>zadeh-etal-2017-tensor</bibkey>
    </paper>
    <paper id="116">
      <title>ConStance : Modeling Annotation Contexts to Improve Stance Classification<fixed-case>C</fixed-case>on<fixed-case>S</fixed-case>tance: Modeling Annotation Contexts to Improve Stance Classification</title>
      <author><first>Kenneth</first> <last>Joseph</last></author>
      <author><first>Lisa</first> <last>Friedland</last></author>
      <author><first>William</first> <last>Hobbs</last></author>
      <author><first>David</first> <last>Lazer</last></author>
      <author><first>Oren</first> <last>Tsur</last></author>
      <pages>1115–1124</pages>
      <url hash="4d5d2e46">D17-1116</url>
      <doi>10.18653/v1/D17-1116</doi>
      <attachment type="attachment" hash="0ea88bd4">D17-1116.Attachment.zip</attachment>
      <abstract>Manual annotations are a prerequisite for many applications of <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>. However, weaknesses in the <a href="https://en.wikipedia.org/wiki/Annotation">annotation process</a> itself are easy to overlook. In particular, scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a>, and stance detection, such choices can impact results. Here, for the task of political stance detection on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>, we show that providing too little context can result in noisy and uncertain annotations, whereas providing too strong a <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a> may cause it to outweigh other signals. To characterize and reduce these biases, we develop ConStance, a general <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> for reasoning about <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> across information conditions. Given conflicting labels produced by multiple annotators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> for new instances. We show that the classifier learned by ConStance outperforms a variety of baselines at predicting political stance, while the model’s interpretable parameters shed light on the effects of each context.</abstract>
      <video href="https://vimeo.com/238231805" />
      <bibkey>joseph-etal-2017-constance</bibkey>
    </paper>
    <paper id="117">
      <title>Deeper Attention to Abusive User Content Moderation</title>
      <author><first>John</first> <last>Pavlopoulos</last></author>
      <author><first>Prodromos</first> <last>Malakasiotis</last></author>
      <author><first>Ion</first> <last>Androutsopoulos</last></author>
      <pages>1125–1135</pages>
      <url hash="bf9197a7">D17-1117</url>
      <doi>10.18653/v1/D17-1117</doi>
      <abstract>Experimenting with a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 1.6 M user comments from a news portal and an existing <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 115 K Wikipedia talk page comments, we show that an RNN operating on word embeddings outpeforms the previous state of the art in moderation, which used <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> or an MLP classifier with character or word n-grams. We also compare against a <a href="https://en.wikipedia.org/wiki/CNN">CNN</a> operating on <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>, and a word-list baseline. A novel, deep, classificationspecific attention mechanism improves the performance of the <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN</a> further, and can also highlight suspicious words for free, without including highlighted words in the training data. We consider both fully automatic and semi-automatic moderation.</abstract>
      <video href="https://vimeo.com/238232251" />
      <bibkey>pavlopoulos-etal-2017-deeper</bibkey>
    </paper>
    <paper id="118">
      <title>Outta Control : Laws of Semantic Change and Inherent Biases in Word Representation Models</title>
      <author><first>Haim</first> <last>Dubossarsky</last></author>
      <author><first>Daphna</first> <last>Weinshall</last></author>
      <author><first>Eitan</first> <last>Grossman</last></author>
      <pages>1136–1145</pages>
      <url hash="fc433fd9">D17-1118</url>
      <doi>10.18653/v1/D17-1118</doi>
      <abstract>This article evaluates three proposed laws of semantic change. Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition, in which no change can possibly have taken place. Our analysis shows that the effects reported in recent literature must be substantially revised : (i) the proposed negative correlation between meaning change and <a href="https://en.wikipedia.org/wiki/Word_frequency">word frequency</a> is shown to be largely an artefact of the models of word representation used ; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art ; and (iii) the proposed positive correlation between meaning change and <a href="https://en.wikipedia.org/wiki/Polysemy">polysemy</a> is largely an artefact of <a href="https://en.wikipedia.org/wiki/Word_frequency">word frequency</a>. These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on <a href="https://en.wikipedia.org/wiki/Word_frequency">word frequency</a>, and thus <a href="https://en.wikipedia.org/wiki/Word_frequency">word frequency</a> can not be evaluated as an independent factor with these representations.</abstract>
      <video href="https://vimeo.com/238235126" />
      <bibkey>dubossarsky-etal-2017-outta</bibkey>
    </paper>
    <paper id="119">
      <title>Human Centered NLP with User-Factor Adaptation<fixed-case>NLP</fixed-case> with User-Factor Adaptation</title>
      <author><first>Veronica</first> <last>Lynn</last></author>
      <author><first>Youngseo</first> <last>Son</last></author>
      <author><first>Vivek</first> <last>Kulkarni</last></author>
      <author><first>Niranjan</first> <last>Balasubramanian</last></author>
      <author><first>H. Andrew</first> <last>Schwartz</last></author>
      <pages>1146–1155</pages>
      <url hash="687aada8">D17-1119</url>
      <doi>10.18653/v1/D17-1119</doi>
      <abstract>We pose the general task of user-factor adaptation   adapting supervised learning models to real-valued user factors inferred from a background of their language, reflecting the idea that a piece of text should be understood within the context of the user that wrote it. We introduce a continuous adaptation technique, suited for real-valued user factors that are common in <a href="https://en.wikipedia.org/wiki/Social_science">social science</a> and bringing us closer to personalized NLP, adapting to each user uniquely. We apply this technique with known user factors including age, gender, and personality traits, as well as latent factors, evaluating over five tasks : POS tagging, PP-attachment, <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, sarcasm detection, and stance detection. Adaptation provides statistically significant benefits for 3 of the 5 tasks : up to +1.2 points for PP-attachment, +3.4 points for <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a>, and +3.0 points for <a href="https://en.wikipedia.org/wiki/List_of_human_positions">stance</a>.</abstract>
      <video href="https://vimeo.com/238233454" />
      <bibkey>lynn-etal-2017-human</bibkey>
    </paper>
    <paper id="120">
      <title>Neural Sequence Learning Models for Word Sense Disambiguation</title>
      <author><first>Alessandro</first> <last>Raganato</last></author>
      <author><first>Claudio</first> <last>Delli Bovi</last></author>
      <author><first>Roberto</first> <last>Navigli</last></author>
      <pages>1156–1167</pages>
      <url hash="a83fdb8e">D17-1120</url>
      <doi>10.18653/v1/D17-1120</doi>
      <abstract>Word Sense Disambiguation models exist in many flavors. Even though supervised ones tend to perform best in terms of <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, they often lose ground to more flexible knowledge-based solutions, which do not require training by a word expert for every <a href="https://en.wikipedia.org/wiki/Word-sense_disambiguation">disambiguation target</a>. To bridge this gap we adopt a different perspective and rely on <a href="https://en.wikipedia.org/wiki/Sequence_learning">sequence learning</a> to frame the disambiguation problem : we propose and study in depth a series of end-to-end neural architectures directly tailored to the task, from bidirectional Long Short-Term Memory to encoder-decoder models. Our extensive evaluation over standard benchmarks and in multiple languages shows that <a href="https://en.wikipedia.org/wiki/Sequence_learning">sequence learning</a> enables more versatile all-words models that consistently lead to state-of-the-art results, even against word experts with engineered features.</abstract>
      <bibkey>raganato-etal-2017-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="121">
      <title>Learning Word Relatedness over Time</title>
      <author><first>Guy D.</first> <last>Rosin</last></author>
      <author><first>Eytan</first> <last>Adar</last></author>
      <author><first>Kira</first> <last>Radinsky</last></author>
      <pages>1168–1178</pages>
      <url hash="95677937">D17-1121</url>
      <doi>10.18653/v1/D17-1121</doi>
      <attachment type="attachment" hash="85cbd31f">D17-1121.Attachment.zip</attachment>
      <abstract>Search systems are often focused on providing relevant results for the now, assuming both corpora and user needs that focus on the present. However, many corpora today reflect significant longitudinal collections ranging from 20 years of the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">Web</a> to hundreds of years of digitized newspapers and books. Understanding the temporal intent of the user and retrieving the most relevant historical content has become a significant challenge. Common search features, such as <a href="https://en.wikipedia.org/wiki/Query_expansion">query expansion</a>, leverage the relationship between terms but can not function well across all times when relationships vary temporally. In this work, we introduce a temporal relationship model that is extracted from <a href="https://en.wikipedia.org/wiki/Longitudinal_study">longitudinal data collections</a>. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> supports the task of identifying, given two words, when they relate to each other. We present an algorithmic framework for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> and show its application for the task of <a href="https://en.wikipedia.org/wiki/Query_expansion">query expansion</a>, achieving high gain.</abstract>
      <bibkey>rosin-etal-2017-learning</bibkey>
      <pwccode url="https://github.com/guyrosin/learning-word-relatedness" additional="false">guyrosin/learning-word-relatedness</pwccode>
    </paper>
    <paper id="122">
      <title>Inter-Weighted Alignment Network for Sentence Pair Modeling</title>
      <author><first>Gehui</first> <last>Shen</last></author>
      <author><first>Yunlun</first> <last>Yang</last></author>
      <author><first>Zhi-Hong</first> <last>Deng</last></author>
      <pages>1179–1189</pages>
      <url hash="bc12eb4e">D17-1122</url>
      <doi>10.18653/v1/D17-1122</doi>
      <abstract>Sentence pair modeling is a crucial problem in the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. In this paper, we propose a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to measure the similarity of a sentence pair focusing on the <a href="https://en.wikipedia.org/wiki/Interaction_information">interaction information</a>. We utilize the word level similarity matrix to discover fine-grained alignment of two sentences. It should be emphasized that each word in a sentence has a different importance from the perspective of semantic composition, so we exploit two novel and efficient strategies to explicitly calculate a weight for each word. Although the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> only use a sequential LSTM for sentence modeling without any external resource such as syntactic parser tree and additional lexicon features, experimental results show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves state-of-the-art performance on three datasets of two tasks.</abstract>
      <bibkey>shen-etal-2017-inter</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="123">
      <title>A Short Survey on Taxonomy Learning from Text Corpora : Issues, Resources and Recent Advances</title>
      <author><first>Chengyu</first> <last>Wang</last></author>
      <author><first>Xiaofeng</first> <last>He</last></author>
      <author><first>Aoying</first> <last>Zhou</last></author>
      <pages>1190–1203</pages>
      <url hash="ec8ca747">D17-1123</url>
      <doi>10.18653/v1/D17-1123</doi>
      <abstract>A <a href="https://en.wikipedia.org/wiki/Taxonomy_(general)">taxonomy</a> is a semantic hierarchy, consisting of concepts linked by is-a relations. While a large number of <a href="https://en.wikipedia.org/wiki/Taxonomy_(biology)">taxonomies</a> have been constructed from <a href="https://en.wikipedia.org/wiki/Web_of_Science">human-compiled resources</a> (e.g., Wikipedia), learning <a href="https://en.wikipedia.org/wiki/Taxonomy_(biology)">taxonomies</a> from <a href="https://en.wikipedia.org/wiki/Text_corpus">text corpora</a> has received a growing interest and is essential for long-tailed and domain-specific knowledge acquisition. In this paper, we overview recent advances on <a href="https://en.wikipedia.org/wiki/Taxonomy_(biology)">taxonomy construction</a> from free texts, reorganizing relevant subtasks into a complete <a href="https://en.wikipedia.org/wiki/Conceptual_framework">framework</a>. We also overview resources for evaluation and discuss challenges for future research.</abstract>
      <bibkey>wang-etal-2017-short</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/yago">YAGO</pwcdataset>
    </paper>
    <paper id="124">
      <title>Idiom-Aware Compositional Distributed Semantics</title>
      <author><first>Pengfei</first> <last>Liu</last></author>
      <author><first>Kaiyu</first> <last>Qian</last></author>
      <author><first>Xipeng</first> <last>Qiu</last></author>
      <author><first>Xuanjing</first> <last>Huang</last></author>
      <pages>1204–1213</pages>
      <url hash="91f0111b">D17-1124</url>
      <doi>10.18653/v1/D17-1124</doi>
      <abstract>Idioms are peculiar linguistic constructions that impose great challenges for representing the <a href="https://en.wikipedia.org/wiki/Semantics">semantics of language</a>, especially in current prevailing end-to-end neural models, which assume that the <a href="https://en.wikipedia.org/wiki/Semantics">semantics</a> of a phrase or sentence can be literally composed from its constitutive words. In this paper, we propose an idiom-aware distributed semantic model to build representation of sentences on the basis of understanding their contained idioms. Our models are grounded in the literal-first psycholinguistic hypothesis, which can adaptively learn semantic compositionality of a phrase literally or idiomatically. To better evaluate our models, we also construct an idiom-enriched sentiment classification dataset with considerable scale and abundant peculiarities of <a href="https://en.wikipedia.org/wiki/Idiom_(language_structure)">idioms</a>. The qualitative and quantitative experimental analyses demonstrate the efficacy of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>.</abstract>
      <bibkey>liu-etal-2017-idiom</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="125">
      <title>Macro Grammars and Holistic Triggering for Efficient Semantic Parsing</title>
      <author><first>Yuchen</first> <last>Zhang</last></author>
      <author><first>Panupong</first> <last>Pasupat</last></author>
      <author><first>Percy</first> <last>Liang</last></author>
      <pages>1214–1223</pages>
      <url hash="f8c570b0">D17-1125</url>
      <doi>10.18653/v1/D17-1125</doi>
      <attachment type="attachment" hash="5060d227">D17-1125.Attachment.zip</attachment>
      <abstract>To learn a <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a> from denotations, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WikiTableQuestions dataset, we first expand the search space of an existing <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to improve the state-of-the-art <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> from 38.7 % to 42.7 %, and then use <a href="https://en.wikipedia.org/wiki/Macro_(computer_science)">macro grammars</a> and holistic triggering to achieve an 11x speedup and an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 43.7 %.</abstract>
      <bibkey>zhang-etal-2017-macro</bibkey>
      <pwccode url="https://github.com/percyliang/sempre" additional="true">percyliang/sempre</pwccode>
    </paper>
    <paper id="126">
      <title>A Continuously Growing Dataset of Sentential Paraphrases</title>
      <author><first>Wuwei</first> <last>Lan</last></author>
      <author><first>Siyu</first> <last>Qiu</last></author>
      <author><first>Hua</first> <last>He</last></author>
      <author><first>Wei</first> <last>Xu</last></author>
      <pages>1224–1234</pages>
      <url hash="93fea87a">D17-1126</url>
      <doi>10.18653/v1/D17-1126</doi>
      <abstract>A major challenge in paraphrase research is the lack of <a href="https://en.wikipedia.org/wiki/Parallel_text">parallel corpora</a>. In this paper, we present a new method to collect large-scale sentential paraphrases from <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> by linking tweets through shared URLs. The main advantage of our method is its simplicity, as it gets rid of the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a> or human in the loop needed to select data before <a href="https://en.wikipedia.org/wiki/Annotation">annotation</a> and subsequent application of paraphrase identification algorithms in the previous work. We present the largest human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification. In addition, we show that more than 30,000 new sentential paraphrases can be easily and continuously captured every month at ~70 % precision, and demonstrate their utility for downstream NLP tasks through phrasal paraphrase extraction. We make our code and data freely available.</abstract>
      <bibkey>lan-etal-2017-continuously</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/twitter-news-url-corpus">TURL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pit">PIT</pwcdataset>
    </paper>
    <paper id="128">
      <title>A Joint Sequential and Relational Model for Frame-Semantic Parsing</title>
      <author><first>Bishan</first> <last>Yang</last></author>
      <author><first>Tom</first> <last>Mitchell</last></author>
      <pages>1247–1256</pages>
      <url hash="9c67e032">D17-1128</url>
      <doi>10.18653/v1/D17-1128</doi>
      <abstract>We introduce a new <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> for frame-semantic parsing that significantly improves the prior state of the art. Our model leverages the advantages of a deep bidirectional LSTM network which predicts semantic role labels word by word and a relational network which predicts semantic roles for individual text expressions in relation to a predicate. The two networks are integrated into a single <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> via knowledge distillation, and a unified graphical model is employed to jointly decode frames and semantic roles during <a href="https://en.wikipedia.org/wiki/Inference">inference</a>. Experiments on the standard FrameNet data show that our model significantly outperforms existing neural and non-neural approaches, achieving a 5.7 F1 gain over the current state of the art, for full frame structure extraction.</abstract>
      <bibkey>yang-mitchell-2017-joint</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="129">
      <title>Getting the Most out of AMR Parsing<fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Chuan</first> <last>Wang</last></author>
      <author><first>Nianwen</first> <last>Xue</last></author>
      <pages>1257–1268</pages>
      <url hash="ae5130aa">D17-1129</url>
      <doi>10.18653/v1/D17-1129</doi>
      <abstract>This paper proposes to tackle the AMR parsing bottleneck by improving two components of an AMR parser : concept identification and alignment. We first build a Bidirectional LSTM based concept identifier that is able to incorporate richer contextual information to learn sparse AMR concept labels. We then extend an HMM-based word-to-concept alignment model with graph distance distortion and a rescoring method during decoding to incorporate the structural information in the AMR graph. We show integrating the two components into an existing AMR parser results in consistently better performance over the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state of the art</a> on various datasets.</abstract>
      <bibkey>wang-xue-2017-getting</bibkey>
    </paper>
    <paper id="131">
      <title>An End-to-End Deep Framework for Answer Triggering with a Novel Group-Level Objective</title>
      <author><first>Jie</first> <last>Zhao</last></author>
      <author><first>Yu</first> <last>Su</last></author>
      <author><first>Ziyu</first> <last>Guan</last></author>
      <author><first>Huan</first> <last>Sun</last></author>
      <pages>1276–1282</pages>
      <url hash="8a80056d">D17-1131</url>
      <doi>10.18653/v1/D17-1131</doi>
      <abstract>Given a question and a set of answer candidates, answer triggering determines whether the candidate set contains any correct answers. If yes, it then outputs a correct one. In contrast to existing pipeline methods which first consider individual candidate answers separately and then make a prediction based on a threshold, we propose an end-to-end deep neural network framework, which is trained by a novel group-level objective function that directly optimizes the answer triggering performance. Our <a href="https://en.wikipedia.org/wiki/Loss_function">objective function</a> penalizes three potential types of error and allows training the <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> in an end-to-end manner. Experimental results on the WikiQA benchmark show that our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> outperforms the <a href="https://en.wikipedia.org/wiki/State_(polity)">state</a> of the arts by a 6.6 % absolute gain under F1 measure.</abstract>
      <bibkey>zhao-etal-2017-end</bibkey>
    </paper>
    <paper id="132">
      <title>Predicting Word Association Strengths</title>
      <author><first>Andrew</first> <last>Cattle</last></author>
      <author><first>Xiaojuan</first> <last>Ma</last></author>
      <pages>1283–1288</pages>
      <url hash="005a5625">D17-1132</url>
      <doi>10.18653/v1/D17-1132</doi>
      <abstract>This paper looks at the task of predicting word association strengths across three datasets ; WordNet Evocation (Boyd-Graber et al., 2006), University of Southern Florida Free Association norms (Nelson et al., 2004), and Edinburgh Associative Thesaurus (Kiss et al., 1973). We achieve results of r=0.357 and p=0.379, r=0.344 and p=0.300, and r=0.292 and p=0.363, respectively. We find Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) cosine similarities, as well as vector offsets, to be the highest performing features. Furthermore, we examine the usefulness of Gaussian embeddings (Vilnis and McCallum, 2014) for predicting word association strength, the first work to do so.</abstract>
      <bibkey>cattle-ma-2017-predicting</bibkey>
    </paper>
    <paper id="133">
      <title>Learning Contextually Informed Representations for Linear-Time Discourse Parsing</title>
      <author id="yang-liu-edinburgh"><first>Yang</first> <last>Liu</last></author>
      <author><first>Mirella</first> <last>Lapata</last></author>
      <pages>1289–1298</pages>
      <url hash="2def78da">D17-1133</url>
      <doi>10.18653/v1/D17-1133</doi>
      <abstract>Recent advances in RST discourse parsing have focused on two modeling paradigms : (a) high order parsers which jointly predict the tree structure of the discourse and the relations it encodes ; or (b) linear-time parsers which are efficient but mostly based on local features. In this work, we propose a linear-time parser with a novel way of representing <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">discourse constituents</a> based on <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> which takes into account global contextual information and is able to capture long-distance dependencies. Experimental results show that our <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> obtains state-of-the art performance on benchmark datasets, while being efficient (with <a href="https://en.wikipedia.org/wiki/Time_complexity">time complexity</a> linear in the number of sentences in the document) and requiring minimal feature engineering.</abstract>
      <bibkey>liu-lapata-2017-learning</bibkey>
    </paper>
    <paper id="134">
      <title>Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification</title>
      <author><first>Man</first> <last>Lan</last></author>
      <author><first>Jianxiang</first> <last>Wang</last></author>
      <author><first>Yuanbin</first> <last>Wu</last></author>
      <author><first>Zheng-Yu</first> <last>Niu</last></author>
      <author><first>Haifeng</first> <last>Wang</last></author>
      <pages>1299–1308</pages>
      <url hash="26ee37d3">D17-1134</url>
      <doi>10.18653/v1/D17-1134</doi>
      <abstract>We present a novel multi-task attention based neural network model to address implicit discourse relationship representation and identification through two types of <a href="https://en.wikipedia.org/wiki/Representation_learning">representation learning</a>, an attention based neural network for learning discourse relationship representation with two arguments and a multi-task framework for learning knowledge from annotated and unannotated corpora. The extensive experiments have been performed on two benchmark corpora (i.e., PDTB and CoNLL-2016 datasets). Experimental results show that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms the state-of-the-art systems on benchmark corpora.</abstract>
      <bibkey>lan-etal-2017-multi</bibkey>
    </paper>
    <paper id="135">
      <title>Chinese Zero Pronoun Resolution with Deep Memory Network<fixed-case>C</fixed-case>hinese Zero Pronoun Resolution with Deep Memory Network</title>
      <author><first>Qingyu</first> <last>Yin</last></author>
      <author><first>Yu</first> <last>Zhang</last></author>
      <author><first>Weinan</first> <last>Zhang</last></author>
      <author><first>Ting</first> <last>Liu</last></author>
      <pages>1309–1318</pages>
      <url hash="7e6e2148">D17-1135</url>
      <doi>10.18653/v1/D17-1135</doi>
      <abstract>Existing approaches for Chinese zero pronoun resolution typically utilize only syntactical and lexical features while ignoring semantic information. The fundamental reason is that zero pronouns have no descriptive information, which brings difficulty in explicitly capturing their semantic similarities with antecedents. Meanwhile, representing zero pronouns is challenging since they are merely gaps that convey no actual content. In this paper, we address this issue by building a deep memory network that is capable of encoding zero pronouns into vector representations with information obtained from their contexts and potential antecedents. Consequently, our resolver takes advantage of <a href="https://en.wikipedia.org/wiki/Semantics">semantic information</a> by using these continuous distributed representations. Experiments on the OntoNotes 5.0 dataset show that the proposed memory network could substantially outperform the state-of-the-art systems in various experimental settings.</abstract>
      <bibkey>yin-etal-2017-chinese</bibkey>
    </paper>
    <paper id="136">
      <title>How much progress have we made on RST discourse parsing? A replication study of recent results on the RST-DT<fixed-case>RST</fixed-case> discourse parsing? A replication study of recent results on the <fixed-case>RST</fixed-case>-<fixed-case>DT</fixed-case></title>
      <author><first>Mathieu</first> <last>Morey</last></author>
      <author><first>Philippe</first> <last>Muller</last></author>
      <author><first>Nicholas</first> <last>Asher</last></author>
      <pages>1319–1324</pages>
      <url hash="6eceaa47">D17-1136</url>
      <doi>10.18653/v1/D17-1136</doi>
      <abstract>This article evaluates purported progress over the past years in RST discourse parsing. Several studies report a relative error reduction of 24 to 51 % on all metrics that authors attribute to the introduction of distributed representations of discourse units. We replicate the standard evaluation of 9 <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a>, 5 of which use distributed representations, from 8 studies published between 2013 and 2017, using their predictions on the test set of the RST-DT. Our main finding is that most recently reported increases in RST discourse parser performance are an artefact of differences in implementations of the evaluation procedure. We evaluate all these <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a> with the standard Parseval procedure to provide a more accurate picture of the actual RST discourse parsers performance in standard evaluation settings. Under this more stringent procedure, the gains attributable to <a href="https://en.wikipedia.org/wiki/Distributed_representation">distributed representations</a> represent at most a 16 % relative error reduction on fully-labelled structures.</abstract>
      <bibkey>morey-etal-2017-much</bibkey>
    </paper>
    <paper id="137">
      <title>What is it? Disambiguating the different readings of the pronoun ‘it’</title>
      <author><first>Sharid</first> <last>Loáiciga</last></author>
      <author><first>Liane</first> <last>Guillou</last></author>
      <author><first>Christian</first> <last>Hardmeier</last></author>
      <pages>1325–1331</pages>
      <url hash="5056f64f">D17-1137</url>
      <doi>10.18653/v1/D17-1137</doi>
      <abstract>In this paper, we address the problem of predicting one of three functions for the English pronoun ‘it’ : <a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)">anaphoric</a>, event reference or <a href="https://en.wikipedia.org/wiki/Pleonasm">pleonastic</a>. This <a href="https://en.wikipedia.org/wiki/Disambiguation">disambiguation</a> is valuable in the context of <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> and <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>. We present experiments using a MAXENT classifier trained on <a href="https://en.wikipedia.org/wiki/Gold_standard_(test)">gold-standard data</a> and self-training experiments of an RNN trained on silver-standard data, annotated using the MAXENT classifier. Lastly, we report on an analysis of the strengths of these two <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>.</abstract>
      <bibkey>loaiciga-etal-2017-disambiguating</bibkey>
    </paper>
    <paper id="138">
      <title>Revisiting Selectional Preferences for Coreference Resolution</title>
      <author><first>Benjamin</first> <last>Heinzerling</last></author>
      <author><first>Nafise Sadat</first> <last>Moosavi</last></author>
      <author><first>Michael</first> <last>Strube</last></author>
      <pages>1332–1339</pages>
      <url hash="b8b21ece">D17-1138</url>
      <doi>10.18653/v1/D17-1138</doi>
      <abstract>Selectional preferences have long been claimed to be essential for <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>. However, they are modeled only implicitly by current <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolvers</a>. We propose a dependency-based embedding model of selectional preferences which allows fine-grained compatibility judgments with high coverage. Incorporating our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> improves performance, matching state-of-the-art results of a more complex system. However, it comes with a cost that makes it debatable how worthwhile are such improvements.</abstract>
      <bibkey>heinzerling-etal-2017-revisiting</bibkey>
    </paper>
    <paper id="139">
      <title>Learning to Rank Semantic Coherence for Topic Segmentation</title>
      <author><first>Liang</first> <last>Wang</last></author>
      <author><first>Sujian</first> <last>Li</last></author>
      <author><first>Yajuan</first> <last>Lv</last></author>
      <author><first>Houfeng</first> <last>Wang</last></author>
      <pages>1340–1344</pages>
      <url hash="d7d27652">D17-1139</url>
      <doi>10.18653/v1/D17-1139</doi>
      <abstract>Topic segmentation plays an important role for discourse parsing and <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a>. Due to the absence of training data, previous work mainly adopts <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a> to rank semantic coherence between paragraphs for <a href="https://en.wikipedia.org/wiki/Topic_segmentation">topic segmentation</a>. In this paper, we present an intuitive and simple idea to automatically create a quasi training dataset, which includes a large amount of text pairs from the same or different documents with different semantic coherence. With the training corpus, we design a symmetric CNN neural network to model text pairs and rank the semantic coherence within the learning to rank framework. Experiments show that our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> is able to achieve competitive performance over strong baselines on several real-world datasets.</abstract>
      <bibkey>wang-etal-2017-learning</bibkey>
    </paper>
    <paper id="140">
      <title>GRASP : Rich Patterns for Argumentation Mining<fixed-case>GRASP</fixed-case>: Rich Patterns for Argumentation Mining</title>
      <author><first>Eyal</first> <last>Shnarch</last></author>
      <author><first>Ran</first> <last>Levy</last></author>
      <author><first>Vikas</first> <last>Raykar</last></author>
      <author><first>Noam</first> <last>Slonim</last></author>
      <pages>1345–1350</pages>
      <url hash="22584951">D17-1140</url>
      <doi>10.18653/v1/D17-1140</doi>
      <attachment type="attachment" hash="4e7197c7">D17-1140.Attachment.zip</attachment>
      <abstract>GRASP (GReedy Augmented Sequential Patterns) is an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> for automatically extracting patterns that characterize subtle linguistic phenomena. To that end, GRASP augments each term of input text with multiple <a href="https://en.wikipedia.org/wiki/Linguistic_description">layers of linguistic information</a>. These different facets of the text terms are systematically combined to reveal rich patterns. We report highly promising experimental results in several challenging text analysis tasks within the field of Argumentation Mining. We believe that GRASP is general enough to be useful for other domains too. For example, each of the following sentences includes a claim for a [ topic ] : 1. Opponents often argue that the open primary is unconstitutional. [ Open Primaries ] 2. Prof. Smith suggested that <a href="https://en.wikipedia.org/wiki/Affirmative_action">affirmative action</a> devalues the accomplishments of the chosen. [ Affirmative Action ] 3. The majority stated that the First Amendment does not guarantee the right to offend others. [ Freedom of Speech ] These sentences share almost no words in common, however, they are similar at a more abstract level. A human observer may notice the following underlying common structure, or pattern : [ someone][argue / suggest / state][that][topic term][sentiment term ]. GRASP aims to automatically capture such underlying structures of the given data. For the above examples it finds the pattern [ noun][express][that][noun, topic][sentiment ], where [ express ] stands for all its (in)direct hyponyms, and [ noun, topic ] means a noun which is also related to the topic.</abstract>
      <bibkey>shnarch-etal-2017-grasp</bibkey>
    </paper>
    <paper id="141">
      <title>Patterns of Argumentation Strategies across Topics</title>
      <author><first>Khalid</first> <last>Al-Khatib</last></author>
      <author><first>Henning</first> <last>Wachsmuth</last></author>
      <author><first>Matthias</first> <last>Hagen</last></author>
      <author><first>Benno</first> <last>Stein</last></author>
      <pages>1351–1357</pages>
      <url hash="562b556b">D17-1141</url>
      <doi>10.18653/v1/D17-1141</doi>
      <abstract>This paper presents an analysis of <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argumentation strategies</a> in <a href="https://en.wikipedia.org/wiki/Editorial">news editorials</a> within and across topics. Given nearly 29,000 argumentative editorials from the <a href="https://en.wikipedia.org/wiki/The_New_York_Times">New York Times</a>, we develop two machine learning models, one for determining an editorial’s topic, and one for identifying evidence types in the editorial. Based on the distribution and structure of the identified types, we analyze the usage patterns of <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argumentation strategies</a> among 12 different topics. We detect several common patterns that provide insights into the manifestation of <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argumentation strategies</a>. Also, our experiments reveal clear correlations between the topics and the detected patterns.</abstract>
      <bibkey>al-khatib-etal-2017-patterns</bibkey>
    </paper>
    <paper id="142">
      <title>Using Argument-based Features to Predict and Analyse Review Helpfulness</title>
      <author><first>Haijing</first> <last>Liu</last></author>
      <author><first>Yang</first> <last>Gao</last></author>
      <author><first>Pin</first> <last>Lv</last></author>
      <author><first>Mengxue</first> <last>Li</last></author>
      <author><first>Shiqiang</first> <last>Geng</last></author>
      <author><first>Minglan</first> <last>Li</last></author>
      <author><first>Hao</first> <last>Wang</last></author>
      <pages>1358–1363</pages>
      <url hash="99a37567">D17-1142</url>
      <doi>10.18653/v1/D17-1142</doi>
      <attachment type="attachment" hash="9bbc4fb4">D17-1142.Attachment.rar</attachment>
      <abstract>We study the helpful product reviews identification problem in this paper. We observe that the evidence-conclusion discourse relations, also known as arguments, often appear in product reviews, and we hypothesise that some argument-based features, e.g. the percentage of argumentative sentences, the evidences-conclusions ratios, are good indicators of helpful reviews. To validate this hypothesis, we manually annotate arguments in 110 hotel reviews, and investigate the effectiveness of several combinations of argument-based features. Experiments suggest that, when being used together with the argument-based features, the state-of-the-art baseline features can enjoy a performance boost (in terms of F1) of 11.01 % in average.</abstract>
      <bibkey>liu-etal-2017-using</bibkey>
    </paper>
    <paper id="143">
      <title>Here’s My Point : Joint Pointer Architecture for Argument Mining</title>
      <author><first>Peter</first> <last>Potash</last></author>
      <author><first>Alexey</first> <last>Romanov</last></author>
      <author><first>Anna</first> <last>Rumshisky</last></author>
      <pages>1364–1373</pages>
      <url hash="652ddd3e">D17-1143</url>
      <doi>10.18653/v1/D17-1143</doi>
      <abstract>In order to determine argument structure in text, one must understand how individual components of the overall argument are linked. This work presents the first neural network-based approach to link extraction in <a href="https://en.wikipedia.org/wiki/Argument_mining">argument mining</a>. Specifically, we propose a novel architecture that applies Pointer Network sequence-to-sequence attention modeling to structural prediction in discourse parsing tasks. We then develop a joint model that extends this <a href="https://en.wikipedia.org/wiki/Computer_architecture">architecture</a> to simultaneously address the link extraction task and the classification of argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, showing far superior performance than the previously proposed corpus-specific and heavily feature-engineered models. Furthermore, our results demonstrate that jointly optimizing for both <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> is crucial for high performance.</abstract>
      <bibkey>potash-etal-2017-heres</bibkey>
    </paper>
    <paper id="144">
      <title>Identifying attack and support argumentative relations using <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a></title>
      <author><first>Oana</first> <last>Cocarascu</last></author>
      <author><first>Francesca</first> <last>Toni</last></author>
      <pages>1374–1379</pages>
      <url hash="3ad26940">D17-1144</url>
      <doi>10.18653/v1/D17-1144</doi>
      <abstract>We propose a deep learning architecture to capture argumentative relations of attack and support from one piece of text to another, of the kind that naturally occur in a debate. The architecture uses two (unidirectional or bidirectional) Long Short-Term Memory networks and (trained or non-trained) word embeddings, and allows to considerably improve upon existing techniques that use syntactic features and supervised classifiers for the same form of (relation-based) argument mining.</abstract>
      <bibkey>cocarascu-toni-2017-identifying</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="146">
      <title>Memory-augmented Neural Machine Translation</title>
      <author><first>Yang</first> <last>Feng</last></author>
      <author><first>Shiyue</first> <last>Zhang</last></author>
      <author><first>Andi</first> <last>Zhang</last></author>
      <author><first>Dong</first> <last>Wang</last></author>
      <author><first>Andrew</first> <last>Abel</last></author>
      <pages>1390–1399</pages>
      <url hash="b24e6d11">D17-1146</url>
      <doi>10.18653/v1/D17-1146</doi>
      <abstract>Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memory-augmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">NMT baseline</a> by 9.0 and 2.7 BLEU points on the two <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>, respectively. Additionally, we found this <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a> resulted in a much more effective OOV treatment compared to competitive methods.</abstract>
      <bibkey>feng-etal-2017-memory</bibkey>
    </paper>
    <paper id="147">
      <title>Dynamic Data Selection for Neural Machine Translation</title>
      <author><first>Marlies</first> <last>van der Wees</last></author>
      <author><first>Arianna</first> <last>Bisazza</last></author>
      <author><first>Christof</first> <last>Monz</last></author>
      <pages>1400–1410</pages>
      <url hash="75d77409">D17-1147</url>
      <doi>10.18653/v1/D17-1147</doi>
      <abstract>Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and <a href="https://en.wikipedia.org/wiki/Translation">translation</a> performance for phrase-based machine translation (PBMT). With the recent increase in popularity of <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation (NMT)</a>, we explore in this paper to what extent and how <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">NMT</a> can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for <a href="https://en.wikipedia.org/wiki/Pharmacodynamics">PBMT</a>, we show that gains are substantially lower for <a href="https://en.wikipedia.org/wiki/Pharmacodynamics">NMT</a>. Next, we introduce ‘dynamic data selection’ for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a <a href="https://en.wikipedia.org/wiki/Scientific_technique">technique</a> we call ‘gradual fine-tuning’, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.</abstract>
      <bibkey>van-der-wees-etal-2017-dynamic</bibkey>
      <pwccode url="https://github.com/marliesvanderwees/dds-nmt" additional="false">marliesvanderwees/dds-nmt</pwccode>
    </paper>
    <paper id="148">
      <title>Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search</title>
      <author><first>Leonard</first> <last>Dahlmann</last></author>
      <author><first>Evgeny</first> <last>Matusov</last></author>
      <author><first>Pavel</first> <last>Petrushkov</last></author>
      <author><first>Shahram</first> <last>Khadivi</last></author>
      <pages>1411–1420</pages>
      <url hash="9b7eb715">D17-1148</url>
      <doi>10.18653/v1/D17-1148</doi>
      <abstract>In this paper, we introduce a hybrid search for attention-based neural machine translation (NMT). A target phrase learned with statistical MT models extends a hypothesis in the NMT beam search when the attention of the NMT model focuses on the source words translated by this phrase. Phrases added in this way are scored with the NMT model, but also with SMT features including phrase-level translation probabilities and a target language model. Experimental results on German-to-English news domain and English-to-Russian e-commerce domain translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3 % BLEU absolute as compared to a strong NMT baseline.</abstract>
      <bibkey>dahlmann-etal-2017-neural</bibkey>
    </paper>
    <paper id="149">
      <title>Translating Phrases in Neural Machine Translation</title>
      <author><first>Xing</first> <last>Wang</last></author>
      <author><first>Zhaopeng</first> <last>Tu</last></author>
      <author><first>Deyi</first> <last>Xiong</last></author>
      <author><first>Min</first> <last>Zhang</last></author>
      <pages>1421–1431</pages>
      <url hash="b8d1226e">D17-1149</url>
      <doi>10.18653/v1/D17-1149</doi>
      <abstract>Phrases play an important role in <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a> and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> (Sag et al., 2002 ; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a> by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a>. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make <a href="https://en.wikipedia.org/wiki/Probability_estimation">probability estimations</a> for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese to English translation show that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves significant improvements over the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> on various <a href="https://en.wikipedia.org/wiki/Test_set">test sets</a>.</abstract>
      <bibkey>wang-etal-2017-translating</bibkey>
    </paper>
    <paper id="151">
      <title>Massive Exploration of Neural Machine Translation Architectures</title>
      <author><first>Denny</first> <last>Britz</last></author>
      <author><first>Anna</first> <last>Goldie</last></author>
      <author><first>Minh-Thang</first> <last>Luong</last></author>
      <author><first>Quoc</first> <last>Le</last></author>
      <pages>1442–1451</pages>
      <url hash="8401dbaf">D17-1151</url>
      <doi>10.18653/v1/D17-1151</doi>
      <abstract>Neural Machine Translation (NMT) has shown remarkable progress over the past few years, with production systems now being deployed to end-users. As the field is moving rapidly, it has become unclear which elements of NMT architectures have a significant impact on translation quality. In this work, we present a large-scale analysis of the sensitivity of NMT architectures to common hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on a WMT English to German translation task. Our experiments provide practical insights into the relative importance of factors such as embedding size, network depth, RNN cell type, residual connections, attention mechanism, and decoding heuristics. As part of this contribution, we also release an open-source NMT framework in <a href="https://en.wikipedia.org/wiki/TensorFlow">TensorFlow</a> to make it easy for others to reproduce our results and perform their own experiments.</abstract>
      <bibkey>britz-etal-2017-massive</bibkey>
      <pwccode url="https://github.com/google/seq2seq" additional="true">google/seq2seq</pwccode>
    </paper>
    <paper id="153">
      <title>Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback</title>
      <author><first>Khanh</first> <last>Nguyen</last></author>
      <author><first>Hal</first> <last>Daumé III</last></author>
      <author><first>Jordan</first> <last>Boyd-Graber</last></author>
      <pages>1464–1474</pages>
      <url hash="bb80ddbe">D17-1153</url>
      <doi>10.18653/v1/D17-1153</doi>
      <attachment type="attachment" hash="b7825a2f">D17-1153.Attachment.zip</attachment>
      <abstract>Machine translation is a natural candidate problem for <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> from human feedback : users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning algorithm</a> that improves neural machine translation systems from simulated human feedback. Our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder architecture (Luong et al., 2015). This <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.</abstract>
      <bibkey>nguyen-etal-2017-reinforcement</bibkey>
      <pwccode url="https://github.com/khanhptnk/bandit-nmt" additional="false">khanhptnk/bandit-nmt</pwccode>
    </paper>
    <paper id="154">
      <title>Towards Compact and Fast Neural Machine Translation Using a Combined Method</title>
      <author><first>Xiaowei</first> <last>Zhang</last></author>
      <author><first>Wei</first> <last>Chen</last></author>
      <author><first>Feng</first> <last>Wang</last></author>
      <author><first>Shuang</first> <last>Xu</last></author>
      <author><first>Bo</first> <last>Xu</last></author>
      <pages>1475–1481</pages>
      <url hash="f60b6cf4">D17-1154</url>
      <doi>10.18653/v1/D17-1154</doi>
      <abstract>Neural Machine Translation (NMT) lays intensive burden on computation and memory cost. It is a challenge to deploy NMT models on the devices with limited computation and memory budgets. This paper presents a four stage pipeline to compress <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and speed up the <a href="https://en.wikipedia.org/wiki/Code">decoding</a> for <a href="https://en.wikipedia.org/wiki/Nuclear_magnetic_resonance_spectroscopy">NMT</a>. Our method first introduces a compact architecture based on convolutional encoder and weight shared embeddings. Then weight pruning is applied to obtain a sparse model. Next, we propose a fast sequence interpolation approach which enables the greedy decoding to achieve performance on par with the <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a>. Hence, the time-consuming <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> can be replaced by simple greedy decoding. Finally, vocabulary selection is used to reduce the computation of softmax layer. Our final <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves 10 times <a href="https://en.wikipedia.org/wiki/Speedup">speedup</a>, 17 times parameters reduction, less than 35 MB storage size and comparable performance compared to the baseline model.</abstract>
      <bibkey>zhang-etal-2017-towards</bibkey>
    </paper>
    <paper id="155">
      <title>Instance Weighting for Neural Machine Translation Domain Adaptation</title>
      <author><first>Rui</first> <last>Wang</last></author>
      <author><first>Masao</first> <last>Utiyama</last></author>
      <author><first>Lemao</first> <last>Liu</last></author>
      <author><first>Kehai</first> <last>Chen</last></author>
      <author><first>Eiichiro</first> <last>Sumita</last></author>
      <pages>1482–1488</pages>
      <url hash="50e05506">D17-1155</url>
      <doi>10.18653/v1/D17-1155</doi>
      <abstract>Instance weighting has been widely applied to phrase-based machine translation domain adaptation. However, it is challenging to be applied to <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation (NMT)</a> directly, because <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">NMT</a> is not a <a href="https://en.wikipedia.org/wiki/Linear_model">linear model</a>. In this paper, two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT English-German / French tasks show that the proposed methods can substantially improve <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a> performance by up to 2.7-6.7 BLEU points, outperforming the existing <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a> by up to 1.6-3.6 BLEU points.</abstract>
      <bibkey>wang-etal-2017-instance</bibkey>
      <pwccode url="https://github.com/wangruinlp/nmt_instance_weighting" additional="false">wangruinlp/nmt_instance_weighting</pwccode>
    </paper>
    <paper id="156">
      <title>Regularization techniques for <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a></title>
      <author><first>Antonio Valerio</first> <last>Miceli Barone</last></author>
      <author><first>Barry</first> <last>Haddow</last></author>
      <author><first>Ulrich</first> <last>Germann</last></author>
      <author><first>Rico</first> <last>Sennrich</last></author>
      <pages>1489–1494</pages>
      <url hash="81d22445">D17-1156</url>
      <doi>10.18653/v1/D17-1156</doi>
      <abstract>We investigate techniques for supervised domain adaptation for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> is a major challenge. We investigate a number of techniques to reduce <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> and improve <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>, obtaining improvements on IWSLT datasets for <a href="https://en.wikipedia.org/wiki/German_language">EnglishGerman</a> and EnglishRussian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.</abstract>
      <bibkey>miceli-barone-etal-2017-regularization</bibkey>
    </paper>
    <paper id="158">
      <title>Using Target-side Monolingual Data for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a> through <a href="https://en.wikipedia.org/wiki/Multi-task_learning">Multi-task Learning</a></title>
      <author><first>Tobias</first> <last>Domhan</last></author>
      <author><first>Felix</first> <last>Hieber</last></author>
      <pages>1500–1505</pages>
      <url hash="438b9b83">D17-1158</url>
      <doi>10.18653/v1/D17-1158</doi>
      <abstract>The performance of Neural Machine Translation (NMT) models relies heavily on the availability of sufficient amounts of <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallel data</a>, and an efficient and effective way of leveraging the vastly available amounts of <a href="https://en.wikipedia.org/wiki/Monolingualism">monolingual data</a> has yet to be found. We propose to modify the decoder in a neural sequence-to-sequence model to enable <a href="https://en.wikipedia.org/wiki/Multi-task_learning">multi-task learning</a> for two strongly related tasks : target-side language modeling and <a href="https://en.wikipedia.org/wiki/Translation">translation</a>. The decoder predicts the next target word through two channels, a target-side language model on the lowest layer, and an attentional recurrent model which is conditioned on the source representation. This architecture allows joint training on both large amounts of monolingual and moderate amounts of bilingual data to improve <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a> performance. Initial results in the news domain for three language pairs show moderate but consistent improvements over a <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> trained on <a href="https://en.wikipedia.org/wiki/Multilingualism">bilingual data</a> only.</abstract>
      <bibkey>domhan-hieber-2017-using</bibkey>
      <pwccode url="https://github.com/awslabs/sockeye" additional="false">awslabs/sockeye</pwccode>
    </paper>
    <paper id="161">
      <title>Joint Concept Learning and Semantic Parsing from Natural Language Explanations</title>
      <author><first>Shashank</first> <last>Srivastava</last></author>
      <author><first>Igor</first> <last>Labutov</last></author>
      <author><first>Tom</first> <last>Mitchell</last></author>
      <pages>1527–1536</pages>
      <url hash="6f44042d">D17-1161</url>
      <doi>10.18653/v1/D17-1161</doi>
      <abstract>Natural language constitutes a predominant medium for much of <a href="https://en.wikipedia.org/wiki/Pedagogy">human learning and pedagogy</a>. We consider the problem of <a href="https://en.wikipedia.org/wiki/Concept">concept learning</a> from <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language explanations</a>, and a small number of labeled examples of the <a href="https://en.wikipedia.org/wiki/Concept">concept</a>. For example, in learning the concept of a phishing email, one might say ‘this is a phishing email because it asks for your bank account number’. Solving this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> involves both learning to interpret open ended natural language statements, and learning the <a href="https://en.wikipedia.org/wiki/Concept">concept</a> itself. We present a joint model for (1) language interpretation (semantic parsing) and (2) concept learning (classification) that does not require labeling statements with logical forms. Instead, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> prefers discriminative interpretations of statements in context of observable features of the data as a weak signal for <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a>. On a dataset of email-related concepts, our approach yields across-the-board improvements in <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> performance, with a 30 % relative improvement in F1 score over competitive methods in the low data regime.</abstract>
      <video href="https://vimeo.com/238233841" />
      <bibkey>srivastava-etal-2017-joint</bibkey>
    </paper>
    <paper id="162">
      <title>Grasping the Finer Point : A Supervised Similarity Network for Metaphor Detection</title>
      <author><first>Marek</first> <last>Rei</last></author>
      <author><first>Luana</first> <last>Bulat</last></author>
      <author><first>Douwe</first> <last>Kiela</last></author>
      <author><first>Ekaterina</first> <last>Shutova</last></author>
      <pages>1537–1546</pages>
      <url hash="58e2831d">D17-1162</url>
      <doi>10.18653/v1/D17-1162</doi>
      <abstract>The ubiquity of <a href="https://en.wikipedia.org/wiki/Metaphor">metaphor</a> in our everyday communication makes <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> an important problem for <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>. Yet, the majority of metaphor processing systems to date rely on hand-engineered features and there is still no consensus in the field as to which features are optimal for this task. In this paper, we present the first deep learning architecture designed to capture metaphorical composition. Our results demonstrate that <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> outperforms the existing <a href="https://en.wikipedia.org/wiki/Psychological_evaluation">approaches</a> in the metaphor identification task.</abstract>
      <video href="https://vimeo.com/238233405" />
      <bibkey>rei-etal-2017-grasping</bibkey>
    </paper>
    <paper id="163">
      <title>Identifying civilians killed by police with distantly supervised entity-event extraction</title>
      <author><first>Katherine</first> <last>Keith</last></author>
      <author><first>Abram</first> <last>Handler</last></author>
      <author><first>Michael</first> <last>Pinkham</last></author>
      <author><first>Cara</first> <last>Magliozzi</last></author>
      <author><first>Joshua</first> <last>McDuffie</last></author>
      <author><first>Brendan</first> <last>O’Connor</last></author>
      <pages>1547–1557</pages>
      <url hash="e956f178">D17-1163</url>
      <doi>10.18653/v1/D17-1163</doi>
      <abstract>We propose a new, socially-impactful task for <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> : from a <a href="https://en.wikipedia.org/wiki/Text_corpus">news corpus</a>, extract names of persons who have been killed by police. We present a newly collected police fatality corpus, which we release publicly, and present a model to solve this problem that uses EM-based distant supervision with <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> and convolutional neural network classifiers. Our model outperforms two off-the-shelf event extractor systems, and it can suggest candidate victim names in some cases faster than one of the major manually-collected police fatality databases.</abstract>
      <video href="https://vimeo.com/238233559" />
      <bibkey>keith-etal-2017-identifying</bibkey>
    </paper>
    <paper id="164">
      <title>Asking too much? The rhetorical role of questions in political discourse</title>
      <author><first>Justine</first> <last>Zhang</last></author>
      <author><first>Arthur</first> <last>Spirling</last></author>
      <author><first>Cristian</first> <last>Danescu-Niculescu-Mizil</last></author>
      <pages>1558–1572</pages>
      <url hash="eeb1ef51">D17-1164</url>
      <doi>10.18653/v1/D17-1164</doi>
      <abstract>Questions play a prominent role in <a href="https://en.wikipedia.org/wiki/Social_relation">social interactions</a>, performing <a href="https://en.wikipedia.org/wiki/Rhetoric">rhetorical functions</a> that go beyond that of simple <a href="https://en.wikipedia.org/wiki/Information_exchange">informational exchange</a>. The surface form of a question can signal the intention and background of the person asking it, as well as the nature of their relation with the interlocutor. While the informational nature of questions has been extensively examined in the context of question-answering applications, their rhetorical aspects have been largely understudied. In this work we introduce an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methodology</a> for extracting surface motifs that recur in questions, and for grouping them according to their latent rhetorical role. By applying this framework to the setting of question sessions in the <a href="https://en.wikipedia.org/wiki/Parliament_of_the_United_Kingdom">UK parliament</a>, we show that the resulting typology encodes key aspects of the political discoursesuch as the bifurcation in questioning behavior between government and opposition partiesand reveals new insights into the effects of a legislator’s tenure and political career ambitions.</abstract>
      <bibkey>zhang-etal-2017-asking</bibkey>
    </paper>
    <paper id="165">
      <title>Detecting Perspectives in Political Debates</title>
      <author><first>David</first> <last>Vilares</last></author>
      <author><first>Yulan</first> <last>He</last></author>
      <pages>1573–1582</pages>
      <url hash="ca5ba21e">D17-1165</url>
      <doi>10.18653/v1/D17-1165</doi>
      <abstract>We explore how to detect people’s perspectives that occupy a certain proposition. We propose a Bayesian modelling approach where topics (or propositions) and their associated perspectives (or viewpoints) are modeled as <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variables</a>. Words associated with topics or perspectives follow different <a href="https://en.wikipedia.org/wiki/Generative_grammar">generative routes</a>. Based on the extracted perspectives, we can extract the top associated sentences from text to generate a succinct summary which allows a quick glimpse of the main viewpoints in a document. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is evaluated on debates from the House of Commons of the UK Parliament, revealing perspectives from the debates without the use of labelled data and obtaining better results than previous related solutions under a variety of evaluations.</abstract>
      <video href="https://vimeo.com/238232359" />
      <bibkey>vilares-he-2017-detecting</bibkey>
      <pwccode url="https://github.com/aghie/lam" additional="false">aghie/lam</pwccode>
    </paper>
    <paper id="166">
      <title>i have a feeling trump will win.................. : Forecasting Winners and Losers from User Predictions on Twitter<fixed-case>T</fixed-case>witter</title>
      <author><first>Sandesh</first> <last>Swamy</last></author>
      <author><first>Alan</first> <last>Ritter</last></author>
      <author><first>Marie-Catherine</first> <last>de Marneffe</last></author>
      <pages>1583–1592</pages>
      <url hash="cf55823e">D17-1166</url>
      <doi>10.18653/v1/D17-1166</doi>
      <abstract>Social media users often make explicit predictions about upcoming events. Such statements vary in the degree of certainty the author expresses toward the outcome : Leonardo DiCaprio will win Best Actor vs. Leonardo DiCaprio may win or No way Leonardo wins !. Can popular beliefs on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> predict who will win? To answer this question, we build a corpus of tweets annotated for <a href="https://en.wikipedia.org/wiki/Veridicality">veridicality</a> on which we train a log-linear classifier that detects positive veridicality with high precision. We then forecast uncertain outcomes using the <a href="https://en.wikipedia.org/wiki/Wisdom_of_crowds">wisdom of crowds</a>, by aggregating users’ explicit predictions. Our method for forecasting winners is fully automated, relying only on a set of contenders as input. It requires no training data of past outcomes and outperforms sentiment and tweet volume baselines on a broad range of contest prediction tasks. We further demonstrate how our approach can be used to measure the reliability of individual accounts’ predictions and retrospectively identify surprise outcomes.</abstract>
      <video href="https://vimeo.com/238236876" />
      <bibkey>swamy-etal-2017-feeling</bibkey>
    </paper>
    <paper id="168">
      <title>Story Comprehension for Predicting What Happens Next</title>
      <author><first>Snigdha</first> <last>Chaturvedi</last></author>
      <author><first>Haoruo</first> <last>Peng</last></author>
      <author><first>Dan</first> <last>Roth</last></author>
      <pages>1603–1614</pages>
      <url hash="6e74b563">D17-1168</url>
      <doi>10.18653/v1/D17-1168</doi>
      <abstract>Automatic story comprehension is a fundamental challenge in <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">Natural Language Understanding</a>, and can enable computers to learn about <a href="https://en.wikipedia.org/wiki/Social_norm">social norms</a>, <a href="https://en.wikipedia.org/wiki/Human_behavior">human behavior</a> and <a href="https://en.wikipedia.org/wiki/Commonsense">commonsense</a>. In this paper, we present a story comprehension model that explores three distinct semantic aspects : (i) the sequence of events described in the story, (ii) its emotional trajectory, and (iii) its plot consistency. We judge the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>’s understanding of real-world stories by inquiring if, like humans, it can develop an expectation of what will happen next in a given story. Specifically, we use <a href="https://en.wikipedia.org/wiki/It_(2017_film)">it</a> to predict the correct ending of a given short story from possible alternatives. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> uses a <a href="https://en.wikipedia.org/wiki/Hidden-variable_theory">hidden variable</a> to weigh the <a href="https://en.wikipedia.org/wiki/Semantics">semantic aspects</a> in the context of the story. Our experiments demonstrate the potential of our approach to characterize these semantic aspects, and the strength of the hidden variable based approach. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> outperforms the state-of-the-art approaches and achieves best results on a publicly available dataset.</abstract>
      <video href="https://vimeo.com/238235959" />
      <bibkey>chaturvedi-etal-2017-story</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="171">
      <title>CRF Autoencoder for Unsupervised Dependency Parsing<fixed-case>CRF</fixed-case> Autoencoder for Unsupervised Dependency Parsing</title>
      <author><first>Jiong</first> <last>Cai</last></author>
      <author><first>Yong</first> <last>Jiang</last></author>
      <author><first>Kewei</first> <last>Tu</last></author>
      <pages>1638–1643</pages>
      <url hash="c35b74a1">D17-1171</url>
      <doi>10.18653/v1/D17-1171</doi>
      <abstract>Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> focuses on learning <a href="https://en.wikipedia.org/wiki/Generative_model">generative models</a>. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> for <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a> as well as a tractable learning algorithm. We evaluated the performance of our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> on eight multilingual treebanks and found that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieved comparable performance with state-of-the-art approaches.</abstract>
      <bibkey>cai-etal-2017-crf</bibkey>
    </paper>
    <paper id="172">
      <title>Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence</title>
      <author><first>Caio</first> <last>Corro</last></author>
      <author><first>Joseph</first> <last>Le Roux</last></author>
      <author><first>Mathieu</first> <last>Lacroix</last></author>
      <pages>1644–1654</pages>
      <url hash="f1f1935b">D17-1172</url>
      <doi>10.18653/v1/D17-1172</doi>
      <abstract>We present a new <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> for the joint task of tagging and non-projective dependency parsing. We demonstrate its usefulness with an application to discontinuous phrase-structure parsing where decoding lexicalized spines and syntactic derivations is performed jointly. The main contributions of this paper are (1) a reduction from joint tagging and non-projective dependency parsing to the Generalized Maximum Spanning Arborescence problem, and (2) a novel decoding algorithm for this problem through <a href="https://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field">Lagrangian relaxation</a>. We evaluate this <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and obtain state-of-the-art results despite strong independence assumptions.</abstract>
      <bibkey>corro-etal-2017-efficient</bibkey>
    </paper>
    <paper id="173">
      <title>Incremental Graph-based Neural Dependency Parsing</title>
      <author><first>Xiaoqing</first> <last>Zheng</last></author>
      <pages>1655–1665</pages>
      <url hash="af16b42b">D17-1173</url>
      <doi>10.18653/v1/D17-1173</doi>
      <abstract>Very recently, some studies on neural dependency parsers have shown advantage over the traditional ones on a wide variety of languages. However, for graph-based neural dependency parsing systems, they either count on the <a href="https://en.wikipedia.org/wiki/Long-term_memory">long-term memory</a> and attention mechanism to implicitly capture the high-order features or give up the global exhaustive inference algorithms in order to harness the <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> over a rich history of parsing decisions. The former might miss out the important <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> for specific headword predictions without the help of the explicit structural information, and the latter may suffer from the <a href="https://en.wikipedia.org/wiki/Error_propagation">error propagation</a> as false early structural constraints are used to create <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> when making future predictions. We explore the feasibility of explicitly taking high-order features into account while remaining the main advantage of global inference and <a href="https://en.wikipedia.org/wiki/Machine_learning">learning</a> for graph-based parsing. The proposed <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> first forms an initial <a href="https://en.wikipedia.org/wiki/Parse_tree">parse tree</a> by head-modifier predictions based on the <a href="https://en.wikipedia.org/wiki/First-order_logic">first-order factorization</a>. High-order features (such as grandparent, sibling, and uncle) then can be defined over the initial <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">tree</a>, and used to refine the parse tree in an iterative fashion. Experimental results showed that our model (called INDP) archived competitive performance to existing benchmark parsers on both English and Chinese datasets.</abstract>
      <bibkey>zheng-2017-incremental</bibkey>
    </paper>
    <paper id="174">
      <title>Neural Discontinuous Constituency Parsing</title>
      <author><first>Miloš</first> <last>Stanojević</last></author>
      <author><first>Raquel G.</first> <last>Alhama</last></author>
      <pages>1666–1676</pages>
      <url hash="239d3f97">D17-1174</url>
      <doi>10.18653/v1/D17-1174</doi>
      <abstract>One of the most pressing issues in discontinuous constituency transition-based parsing is that the relevant information for parsing decisions could be located in any part of the <a href="https://en.wikipedia.org/wiki/Call_stack">stack</a> or the <a href="https://en.wikipedia.org/wiki/Data_buffer">buffer</a>. In this paper, we propose a solution to this problem by replacing the structured perceptron model with a recursive neural model that computes a global representation of the configuration, therefore allowing even the most remote parts of the configuration to influence the parsing decisions. We also provide a detailed analysis of how this representation should be built out of sub-representations of its core elements (words, <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">trees</a> and stack). Additionally, we investigate how different types of swap oracles influence the results. Our model is the first neural discontinuous constituency parser, and it outperforms all the previously published models on three out of four datasets while on the fourth it obtains second place by a tiny difference.</abstract>
      <bibkey>stanojevic-alhama-2017-neural</bibkey>
    </paper>
    <paper id="175">
      <title>Stack-based Multi-layer Attention for Transition-based Dependency Parsing</title>
      <author><first>Zhirui</first> <last>Zhang</last></author>
      <author><first>Shujie</first> <last>Liu</last></author>
      <author><first>Mu</first> <last>Li</last></author>
      <author><first>Ming</first> <last>Zhou</last></author>
      <author><first>Enhong</first> <last>Chen</last></author>
      <pages>1677–1682</pages>
      <url hash="fb20fac2">D17-1175</url>
      <doi>10.18653/v1/D17-1175</doi>
      <abstract>Although sequence-to-sequence (seq2seq) network has achieved significant success in many NLP tasks such as <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> and <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a>, simply applying this approach to transition-based dependency parsing can not yield a comparable performance gain as in other state-of-the-art methods, such as stack-LSTM and head selection. In this paper, we propose a stack-based multi-layer attention model for seq2seq learning to better leverage structural linguistics information. In our method, two binary vectors are used to track the decoding stack in transition-based parsing, and multi-layer attention is introduced to capture multiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model.</abstract>
      <bibkey>zhang-etal-2017-stack</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="176">
      <title>Dependency Grammar Induction with Neural Lexicalization and Big Training Data</title>
      <author><first>Wenjuan</first> <last>Han</last></author>
      <author><first>Yong</first> <last>Jiang</last></author>
      <author><first>Kewei</first> <last>Tu</last></author>
      <pages>1683–1688</pages>
      <url hash="9aeca12c">D17-1176</url>
      <doi>10.18653/v1/D17-1176</doi>
      <attachment type="attachment" hash="2a111e4a">D17-1176.Attachment.pdf</attachment>
      <abstract>We study the impact of <a href="https://en.wikipedia.org/wiki/Big_data">big models</a> (in terms of the degree of lexicalization) and <a href="https://en.wikipedia.org/wiki/Big_data">big data</a> (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016). We find that L-DMV only benefits from very small degrees of <a href="https://en.wikipedia.org/wiki/Lexicalization">lexicalization</a> and moderate sizes of training corpora. L-NDMV can benefit from <a href="https://en.wikipedia.org/wiki/Big_data">big training data</a> and <a href="https://en.wikipedia.org/wiki/Lexicalization">lexicalization</a> of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a>.</abstract>
      <bibkey>han-etal-2017-dependency</bibkey>
    </paper>
    <paper id="177">
      <title>Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition</title>
      <author><first>Yong</first> <last>Jiang</last></author>
      <author><first>Wenjuan</first> <last>Han</last></author>
      <author><first>Kewei</first> <last>Tu</last></author>
      <pages>1689–1694</pages>
      <url hash="9c9631a5">D17-1177</url>
      <doi>10.18653/v1/D17-1177</doi>
      <abstract>Unsupervised dependency parsing aims to learn a <a href="https://en.wikipedia.org/wiki/Dependency_grammar">dependency parser</a> from unannotated sentences. Existing work focuses on either learning <a href="https://en.wikipedia.org/wiki/Generative_model">generative models</a> using the <a href="https://en.wikipedia.org/wiki/Expectation–maximization_algorithm">expectation-maximization algorithm</a> and its variants, or learning <a href="https://en.wikipedia.org/wiki/Discriminative_model">discriminative models</a> using the discriminative clustering algorithm. In this paper, we propose a new learning strategy that learns a <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> and a <a href="https://en.wikipedia.org/wiki/Discriminative_model">discriminative model</a> jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> and improve their learning results. We tested our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> on the UD treebank and achieved a state-of-the-art performance on thirty languages.</abstract>
      <bibkey>jiang-etal-2017-combining</bibkey>
    </paper>
    <paper id="178">
      <title>Effective Inference for Generative Neural Parsing</title>
      <author><first>Mitchell</first> <last>Stern</last></author>
      <author><first>Daniel</first> <last>Fried</last></author>
      <author><first>Dan</first> <last>Klein</last></author>
      <pages>1695–1700</pages>
      <url hash="74a04b42">D17-1178</url>
      <doi>10.18653/v1/D17-1178</doi>
      <abstract>Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these <a href="https://en.wikipedia.org/wiki/Generative_model">generative models</a>. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> while exploring significantly less of the <a href="https://en.wikipedia.org/wiki/Feasible_region">search space</a>. Applied to the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> of Choe and Charniak (2016), our <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference procedure</a> obtains 92.56 F1 on section 23 of the <a href="https://en.wikipedia.org/wiki/Penn_Treebank">Penn Treebank</a>, surpassing prior state-of-the-art results for single-model systems.</abstract>
      <bibkey>stern-etal-2017-effective</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="179">
      <title>Semi-supervised Structured Prediction with Neural CRF Autoencoder<fixed-case>CRF</fixed-case> Autoencoder</title>
      <author><first>Xiao</first> <last>Zhang</last></author>
      <author><first>Yong</first> <last>Jiang</last></author>
      <author><first>Hao</first> <last>Peng</last></author>
      <author><first>Kewei</first> <last>Tu</last></author>
      <author><first>Dan</first> <last>Goldwasser</last></author>
      <pages>1701–1711</pages>
      <url hash="0c0737fa">D17-1179</url>
      <doi>10.18653/v1/D17-1179</doi>
      <abstract>In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning of sequential structured prediction problems. Our NCRF-AE consists of two parts : an encoder which is a CRF model enhanced by <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a>, and a decoder which is a <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> trying to reconstruct the input. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> has a unified structure with different <a href="https://en.wikipedia.org/wiki/Loss_function">loss functions</a> for labeled and unlabeled data with shared parameters. We developed a variation of the EM algorithm for optimizing both the <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and the <a href="https://en.wikipedia.org/wiki/Codec">decoder</a> simultaneously by decoupling their parameters. Our Experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that our model can outperform competitive systems in both supervised and semi-supervised scenarios.</abstract>
      <bibkey>zhang-etal-2017-semi</bibkey>
      <pwccode url="https://github.com/cosmozhang/NCRF-AE" additional="false">cosmozhang/NCRF-AE</pwccode>
    </paper>
    <paper id="180">
      <title>TAG Parsing with <a href="https://en.wikipedia.org/wiki/Neural_network">Neural Networks</a> and Vector Representations of Supertags<fixed-case>TAG</fixed-case> Parsing with Neural Networks and Vector Representations of Supertags</title>
      <author><first>Jungo</first> <last>Kasai</last></author>
      <author><first>Bob</first> <last>Frank</last></author>
      <author><first>Tom</first> <last>McCoy</last></author>
      <author><first>Owen</first> <last>Rambow</last></author>
      <author><first>Alexis</first> <last>Nasr</last></author>
      <pages>1712–1722</pages>
      <url hash="187ef873">D17-1180</url>
      <doi>10.18653/v1/D17-1180</doi>
      <abstract>We present supertagging-based models for Tree Adjoining Grammar parsing that use neural network architectures and dense vector representation of supertags (elementary trees) to achieve state-of-the-art performance in unlabeled and labeled attachment scores. The shift-reduce parsing model eschews lexical information entirely, and uses only the 1-best supertags to parse a sentence, providing further support for the claim that supertagging is almost <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a>. We demonstrate that the embedding vector representations the <a href="https://en.wikipedia.org/wiki/Parsing">parser</a> induces for supertags possess linguistically interpretable structure, supporting analogies between grammatical structures like those familiar from recent work in <a href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional semantics</a>. This dense representation of supertags overcomes the drawbacks for statistical models of TAG as compared to CCG parsing, raising the possibility that <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">TAG</a> is a viable alternative for NLP tasks that require the assignment of richer structural descriptions to sentences.</abstract>
      <bibkey>kasai-etal-2017-tag</bibkey>
    </paper>
    <paper id="181">
      <title>Global Normalization of <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a> for Joint Entity and Relation Classification</title>
      <author><first>Heike</first> <last>Adel</last></author>
      <author><first>Hinrich</first> <last>Schütze</last></author>
      <pages>1723–1729</pages>
      <url hash="bf65a4aa">D17-1181</url>
      <doi>10.18653/v1/D17-1181</doi>
      <abstract>We introduce globally normalized convolutional neural networks for joint entity classification and <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a>. In particular, we propose a way to utilize a linear-chain conditional random field output layer for predicting entity types and relations between entities at the same time. Our experiments show that global normalization outperforms a locally normalized softmax layer on a benchmark dataset.</abstract>
      <bibkey>adel-schutze-2017-global</bibkey>
    </paper>
    <paper id="182">
      <title>End-to-End Neural Relation Extraction with Global Optimization</title>
      <author><first>Meishan</first> <last>Zhang</last></author>
      <author><first>Yue</first> <last>Zhang</last></author>
      <author><first>Guohong</first> <last>Fu</last></author>
      <pages>1730–1740</pages>
      <url hash="e7678a62">D17-1182</url>
      <doi>10.18653/v1/D17-1182</doi>
      <abstract>Neural networks have shown promising results for <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a>. State-of-the-art models cast the <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> as an <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end problem</a>, solved incrementally using a local classifier. Yet previous work using <a href="https://en.wikipedia.org/wiki/Statistical_model">statistical models</a> have demonstrated that <a href="https://en.wikipedia.org/wiki/Global_optimization">global optimization</a> can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate <a href="https://en.wikipedia.org/wiki/Syntax">syntactic information</a> to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is highly effective, achieving the best performances on two standard benchmarks.</abstract>
      <bibkey>zhang-etal-2017-end</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2005">ACE 2005</pwcdataset>
    </paper>
    <paper id="183">
      <title>KGEval : Accuracy Estimation of Automatically Constructed Knowledge Graphs<fixed-case>KGE</fixed-case>val: Accuracy Estimation of Automatically Constructed Knowledge Graphs</title>
      <author><first>Prakhar</first> <last>Ojha</last></author>
      <author><first>Partha</first> <last>Talukdar</last></author>
      <pages>1741–1750</pages>
      <url hash="8d7af69a">D17-1183</url>
      <doi>10.18653/v1/D17-1183</doi>
      <attachment type="attachment" hash="8f929b21">D17-1183.Attachment.zip</attachment>
      <abstract>Automatic construction of large knowledge graphs (KG) by mining web-scale text datasets has received considerable attention recently. Estimating accuracy of such automatically constructed KGs is a challenging problem due to their size and diversity. This important problem has largely been ignored in prior research   we fill this gap and propose KGEval. KGEval uses coupling constraints to bind facts and crowdsources those few that can infer large parts of the <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a>. We demonstrate that the <a href="https://en.wikipedia.org/wiki/Loss_function">objective</a> optimized by KGEval is submodular and NP-hard, allowing guarantees for our <a href="https://en.wikipedia.org/wiki/Approximation_algorithm">approximation algorithm</a>. Through experiments on real-world datasets, we demonstrate that KGEval best estimates KG accuracy compared to other baselines, while requiring significantly lesser number of human evaluations.</abstract>
      <bibkey>ojha-talukdar-2017-kgeval</bibkey>
    </paper>
    <paper id="184">
      <title>Sparsity and Noise : Where Knowledge Graph Embeddings Fall Short</title>
      <author><first>Jay</first> <last>Pujara</last></author>
      <author><first>Eriq</first> <last>Augustine</last></author>
      <author><first>Lise</first> <last>Getoor</last></author>
      <pages>1751–1756</pages>
      <url hash="97205390">D17-1184</url>
      <doi>10.18653/v1/D17-1184</doi>
      <abstract>Knowledge graph (KG) embedding techniques use structured relationships between entities to learn low-dimensional representations of entities and relations. One prominent goal of these approaches is to improve the quality of <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a> by removing errors and adding <a href="https://en.wikipedia.org/wiki/Missing_data">missing facts</a>. Surprisingly, most embedding techniques have been evaluated on benchmark datasets consisting of dense and reliable subsets of human-curated KGs, which tend to be fairly complete and have few errors. In this paper, we consider the problem of applying embedding techniques to KGs extracted from <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">text</a>, which are often incomplete and contain errors. We compare the sparsity and unreliability of different KGs and perform empirical experiments demonstrating how embedding approaches degrade as sparsity and <a href="https://en.wikipedia.org/wiki/Reliability_(statistics)">unreliability</a> increase.</abstract>
      <bibkey>pujara-etal-2017-sparsity</bibkey>
      <pwccode url="https://github.com/linqs/pujara-emnlp17" additional="false">linqs/pujara-emnlp17</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k">FB15k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18">WN18</pwcdataset>
    </paper>
    <paper id="185">
      <title>Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations</title>
      <author><first>Goran</first> <last>Glavaš</last></author>
      <author><first>Simone Paolo</first> <last>Ponzetto</last></author>
      <pages>1757–1767</pages>
      <url hash="cc987191">D17-1185</url>
      <doi>10.18653/v1/D17-1185</doi>
      <attachment type="attachment" hash="9f04fd97">D17-1185.Attachment.zip</attachment>
      <abstract>Detection of lexico-semantic relations is one of the central tasks of <a href="https://en.wikipedia.org/wiki/Computational_semantics">computational semantics</a>. Although some fundamental relations (e.g., hypernymy) are asymmetric, most existing models account for asymmetry only implicitly and use the same concept representations to support detection of symmetric and asymmetric relations alike. In this work, we propose the Dual Tensor model, a neural architecture with which we explicitly model the asymmetry and capture the translation between unspecialized and specialized word embeddings via a pair of <a href="https://en.wikipedia.org/wiki/Tensor">tensors</a>. Although our Dual Tensor model needs only unspecialized embeddings as input, our experiments on <a href="https://en.wikipedia.org/wiki/Hypernymy">hypernymy</a> and <a href="https://en.wikipedia.org/wiki/Meronymy">meronymy detection</a> suggest that it can outperform more complex and resource-intensive models. We further demonstrate that the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can account for <a href="https://en.wikipedia.org/wiki/Polysemy">polysemy</a> and that it exhibits stable performance across languages.</abstract>
      <bibkey>glavas-ponzetto-2017-dual</bibkey>
    </paper>
    <paper id="186">
      <title>Incorporating Relation Paths in Neural Relation Extraction</title>
      <author><first>Wenyuan</first> <last>Zeng</last></author>
      <author><first>Yankai</first> <last>Lin</last></author>
      <author><first>Zhiyuan</first> <last>Liu</last></author>
      <author><first>Maosong</first> <last>Sun</last></author>
      <pages>1768–1777</pages>
      <url hash="72172b85">D17-1186</url>
      <doi>10.18653/v1/D17-1186</doi>
      <abstract>Distantly supervised relation extraction has been widely used to find novel relational facts from <a href="https://en.wikipedia.org/wiki/Plain_text">plain text</a>. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which also provide rich useful information but not yet employed by <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a>. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with strong baselines. The source code of this paper can be obtained from.<url>https://github.com/thunlp/PathNRE</url>.
    </abstract>
      <bibkey>zeng-etal-2017-incorporating</bibkey>
      <pwccode url="https://github.com/thunlp/PathNRE" additional="false">thunlp/PathNRE</pwccode>
    </paper>
    <paper id="187">
      <title>Adversarial Training for Relation Extraction</title>
      <author><first>Yi</first> <last>Wu</last></author>
      <author><first>David</first> <last>Bamman</last></author>
      <author><first>Stuart</first> <last>Russell</last></author>
      <pages>1778–1783</pages>
      <url hash="4b07d56d">D17-1187</url>
      <doi>10.18653/v1/D17-1187</doi>
      <abstract>Adversarial training is a mean of <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularizing classification algorithms</a> by generating adversarial noise to the training data. We apply <a href="https://en.wikipedia.org/wiki/Adversarial_learning">adversarial training</a> in <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a> within the multi-instance multi-label learning framework. We evaluate various neural network architectures on two different <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>. Experimental results demonstrate that adversarial training is generally effective for both CNN and RNN models and significantly improves the <a href="https://en.wikipedia.org/wiki/Precision_(statistics)">precision</a> of predicted relations.</abstract>
      <bibkey>wu-etal-2017-adversarial</bibkey>
    </paper>
    <paper id="188">
      <title>Context-Aware Representations for Knowledge Base Relation Extraction</title>
      <author><first>Daniil</first> <last>Sorokin</last></author>
      <author><first>Iryna</first> <last>Gurevych</last></author>
      <pages>1784–1789</pages>
      <url hash="c1f2c399">D17-1188</url>
      <doi>10.18653/v1/D17-1188</doi>
      <abstract>We demonstrate that for sentence-level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation. Our architecture uses an LSTM-based encoder to jointly learn representations for all relations in a single sentence. We combine the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context representations</a> with an <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a> to make the final prediction. We use the <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata knowledge base</a> to construct a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of multiple relations per sentence and to evaluate our approach. Compared to a baseline system, our method results in an average error reduction of 24 on a held-out set of relations. The code and the dataset to replicate the experiments are made available at.<url>https://github.com/ukplab/</url>.
    </abstract>
      <bibkey>sorokin-gurevych-2017-context</bibkey>
    </paper>
    <paper id="189">
      <title>A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction</title>
      <author><first>Tianyu</first> <last>Liu</last></author>
      <author><first>Kexiang</first> <last>Wang</last></author>
      <author><first>Baobao</first> <last>Chang</last></author>
      <author><first>Zhifang</first> <last>Sui</last></author>
      <pages>1790–1795</pages>
      <url hash="68a32110">D17-1189</url>
      <doi>10.18653/v1/D17-1189</doi>
      <abstract>Distant-supervised relation extraction inevitably suffers from wrong labeling problems because it heuristically labels relational facts with <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a>. Previous sentence level denoise models do n’t achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable during training. To this end, we introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During <a href="https://en.wikipedia.org/wiki/Training">training</a>, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms other state-of-the-art systems.</abstract>
      <bibkey>liu-etal-2017-soft</bibkey>
    </paper>
    <paper id="190">
      <title>A <a href="https://en.wikipedia.org/wiki/Sequential_model">Sequential Model</a> for Classifying Temporal Relations between Intra-Sentence Events</title>
      <author><first>Prafulla Kumar</first> <last>Choubey</last></author>
      <author><first>Ruihong</first> <last>Huang</last></author>
      <pages>1796–1802</pages>
      <url hash="805113de">D17-1190</url>
      <doi>10.18653/v1/D17-1190</doi>
      <abstract>We present a <a href="https://en.wikipedia.org/wiki/Sequential_model">sequential model</a> for temporal relation classification between intra-sentence events. The key observation is that the overall <a href="https://en.wikipedia.org/wiki/Syntax">syntactic structure</a> and compositional meanings of the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">multi-word context</a> between events are important for distinguishing among fine-grained temporal relations. Specifically, our approach first extracts a sequence of context words that indicates the temporal relation between two events, which well align with the dependency path between two event mentions. The context word sequence, together with a parts-of-speech tag sequence and a dependency relation sequence that are generated corresponding to the word sequence, are then provided as input to bidirectional recurrent neural network (LSTM) models. The <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural nets</a> learn compositional syntactic and semantic representations of contexts surrounding the two events and predict the temporal relation between them. Evaluation of the proposed approach on TimeBank corpus shows that sequential modeling is capable of accurately recognizing temporal relations between events, which outperforms a neural net model using various discrete features as input that imitates previous feature based models.</abstract>
      <bibkey>choubey-huang-2017-sequential</bibkey>
    </paper>
    <paper id="191">
      <title>Deep Residual Learning for Weakly-Supervised Relation Extraction</title>
      <author><first>Yi Yao</first> <last>Huang</last></author>
      <author><first>William Yang</first> <last>Wang</last></author>
      <pages>1803–1807</pages>
      <url hash="a1b2d8d2">D17-1191</url>
      <doi>10.18653/v1/D17-1191</doi>
      <abstract>Deep residual learning (ResNet) is a new method for training very deep neural networks using <a href="https://en.wikipedia.org/wiki/Identity_mapping">identity mapping</a> for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolutional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that <a href="https://en.wikipedia.org/wiki/ResNet">ResNet</a> only works well for very deep networks, we found that even with 9 layers of CNNs, using <a href="https://en.wikipedia.org/wiki/Identity_mapping">identity mapping</a> could significantly improve the performance for distantly-supervised relation extraction.</abstract>
      <bibkey>huang-wang-2017-deep</bibkey>
    </paper>
    <paper id="192">
      <title>Noise-Clustered Distant Supervision for <a href="https://en.wikipedia.org/wiki/Relation_extraction">Relation Extraction</a> : A Nonparametric Bayesian Perspective<fixed-case>B</fixed-case>ayesian Perspective</title>
      <author><first>Qing</first> <last>Zhang</last></author>
      <author><first>Houfeng</first> <last>Wang</last></author>
      <pages>1808–1813</pages>
      <url hash="213d90a3">D17-1192</url>
      <doi>10.18653/v1/D17-1192</doi>
      <abstract>For the task of <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a>, distant supervision is an efficient approach to generate labeled data by aligning <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge base</a> with free texts. The essence of it is a challenging incomplete multi-label classification problem with sparse and noisy features. To address the challenge, this work presents a novel nonparametric Bayesian formulation for the task. Experiment results show substantially higher top precision improvements over the traditional state-of-the-art approaches.</abstract>
      <bibkey>zhang-wang-2017-noise</bibkey>
    </paper>
    <paper id="194">
      <title>Temporal dynamics of semantic relations in <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> : an application to predicting armed conflict participants</title>
      <author><first>Andrey</first> <last>Kutuzov</last></author>
      <author><first>Erik</first> <last>Velldal</last></author>
      <author><first>Lilja</first> <last>Øvrelid</last></author>
      <pages>1824–1829</pages>
      <url hash="78cf3941">D17-1194</url>
      <doi>10.18653/v1/D17-1194</doi>
      <abstract>This paper deals with using word embedding models to trace the temporal dynamics of semantic relations between pairs of words. The set-up is similar to the well-known analogies task, but expanded with a <a href="https://en.wikipedia.org/wiki/Spacetime">time dimension</a>. To this end, we apply incremental updating of the models with new training texts, including incremental vocabulary expansion, coupled with learned <a href="https://en.wikipedia.org/wiki/Transformation_matrix">transformation matrices</a> that let us map between members of the relation. The proposed approach is evaluated on the task of predicting insurgent armed groups based on <a href="https://en.wikipedia.org/wiki/Geographic_coordinate_system">geographical locations</a>. The gold standard data for the time span 19942010 is extracted from the UCDP Armed Conflicts dataset. The results show that the <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is feasible and outperforms the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>, but also that important work still remains to be done.</abstract>
      <bibkey>kutuzov-etal-2017-temporal</bibkey>
    </paper>
    <paper id="195">
      <title>Dynamic Entity Representations in Neural Language Models</title>
      <author><first>Yangfeng</first> <last>Ji</last></author>
      <author><first>Chenhao</first> <last>Tan</last></author>
      <author><first>Sebastian</first> <last>Martschat</last></author>
      <author><first>Yejin</first> <last>Choi</last></author>
      <author><first>Noah A.</first> <last>Smith</last></author>
      <pages>1830–1839</pages>
      <url hash="db765a4f">D17-1195</url>
      <doi>10.18653/v1/D17-1195</doi>
      <abstract>Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of <a href="https://en.wikipedia.org/wiki/Language_model">language model</a>, EntityNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is generative and flexible ; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a>, <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>, and <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity prediction</a>. Experimental results with all these tasks demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> consistently outperforms strong baselines and prior work.</abstract>
      <bibkey>ji-etal-2017-dynamic</bibkey>
      <pwccode url="https://github.com/smartschat/cort" additional="true">smartschat/cort</pwccode>
    </paper>
    <paper id="197">
      <title>Reference-Aware Language Models</title>
      <author><first>Zichao</first> <last>Yang</last></author>
      <author><first>Phil</first> <last>Blunsom</last></author>
      <author><first>Chris</first> <last>Dyer</last></author>
      <author><first>Wang</first> <last>Ling</last></author>
      <pages>1850–1859</pages>
      <url hash="eae1fceb">D17-1197</url>
      <doi>10.18653/v1/D17-1197</doi>
      <attachment type="attachment" hash="192622ac">D17-1197.Attachment.pdf</attachment>
      <abstract>We propose a general class of <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> that treat <a href="https://en.wikipedia.org/wiki/Reference">reference</a> as discrete stochastic latent variables. This decision allows for the creation of entity mentions by accessing external databases of referents (required by, e.g., dialogue generation) or past internal state (required to explicitly model coreferentiality). Beyond simple copying, our coreference model can additionally refer to a referent using <a href="https://en.wikipedia.org/wiki/Variation_(linguistics)">varied mention forms</a> (e.g., a reference to Jane can be realized as she), a characteristic feature of reference in <a href="https://en.wikipedia.org/wiki/Natural_language">natural languages</a>. Experiments on three representative applications show our model variants outperform models based on deterministic attention and standard language modeling baselines.</abstract>
      <bibkey>yang-etal-2017-reference</bibkey>
    </paper>
    <paper id="198">
      <title>A Simple <a href="https://en.wikipedia.org/wiki/Language_model">Language Model</a> based on PMI Matrix Approximations<fixed-case>PMI</fixed-case> Matrix Approximations</title>
      <author><first>Oren</first> <last>Melamud</last></author>
      <author><first>Ido</first> <last>Dagan</last></author>
      <author><first>Jacob</first> <last>Goldberger</last></author>
      <pages>1860–1865</pages>
      <url hash="7c864a9b">D17-1198</url>
      <doi>10.18653/v1/D17-1198</doi>
      <abstract>In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired <a href="https://en.wikipedia.org/wiki/Conditional_probability">conditional probabilities</a> from PMI at test time. Specifically, we show that with minor modifications to word2vec’s algorithm, we get principled <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a> to learn word embeddings.</abstract>
      <bibkey>melamud-etal-2017-simple</bibkey>
    </paper>
    <paper id="200">
      <title>Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels</title>
      <author><first>Lea</first> <last>Frermann</last></author>
      <author><first>György</first> <last>Szarvas</last></author>
      <pages>1873–1883</pages>
      <url hash="923da631">D17-1200</url>
      <doi>10.18653/v1/D17-1200</doi>
      <attachment type="attachment" hash="fa67f6ae">D17-1200.Attachment.zip</attachment>
      <abstract>Automatically understanding the <a href="https://en.wikipedia.org/wiki/Plot_(narrative)">plot of novels</a> is important both for informing <a href="https://en.wikipedia.org/wiki/Literary_criticism">literary scholarship</a> and applications such as summarization or <a href="https://en.wikipedia.org/wiki/Book_review">recommendation</a>. Various <a href="https://en.wikipedia.org/wiki/Scientific_modelling">models</a> have addressed this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, but their evaluation has remained largely intrinsic and qualitative. Here, we propose a principled and scalable <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> leveraging expert-provided semantic tags (e.g., mystery, pirates) to evaluate plot representations in an extrinsic fashion, assessing their ability to produce locally coherent groupings of novels (micro-clusters) in model space. We present a deep recurrent autoencoder model that learns richly structured multi-view plot representations, and show that they i) yield better micro-clusters than less structured representations ; and ii) are interpretable, and thus useful for further literary analysis or labeling of the emerging micro-clusters.</abstract>
      <bibkey>frermann-szarvas-2017-inducing</bibkey>
    </paper>
    <paper id="201">
      <title>Initializing Convolutional Filters with <a href="https://en.wikipedia.org/wiki/Semantic_feature">Semantic Features</a> for Text Classification</title>
      <author><first>Shen</first> <last>Li</last></author>
      <author><first>Zhe</first> <last>Zhao</last></author>
      <author><first>Tao</first> <last>Liu</last></author>
      <author><first>Renfen</first> <last>Hu</last></author>
      <author><first>Xiaoyong</first> <last>Du</last></author>
      <pages>1884–1889</pages>
      <url hash="a406de4c">D17-1201</url>
      <doi>10.18653/v1/D17-1201</doi>
      <attachment type="attachment" hash="d065f4ed">D17-1201.Attachment.pdf</attachment>
      <abstract>Convolutional Neural Networks (CNNs) are widely used in NLP tasks. This paper presents a novel weight initialization method to improve the CNNs for <a href="https://en.wikipedia.org/wiki/Text_classification">text classification</a>. Instead of randomly initializing the convolutional filters, we encode semantic features into them, which helps the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> focus on learning useful features at the beginning of the training. Experiments demonstrate the effectiveness of the initialization technique on seven text classification tasks, including <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> and <a href="https://en.wikipedia.org/wiki/Topic_and_comment">topic classification</a>.</abstract>
      <bibkey>li-etal-2017-initializing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="202">
      <title>Shortest-Path Graph Kernels for Document Similarity</title>
      <author><first>Giannis</first> <last>Nikolentzos</last></author>
      <author><first>Polykarpos</first> <last>Meladianos</last></author>
      <author><first>François</first> <last>Rousseau</last></author>
      <author><first>Yannis</first> <last>Stavrakas</last></author>
      <author><first>Michalis</first> <last>Vazirgiannis</last></author>
      <pages>1890–1900</pages>
      <url hash="46396332">D17-1202</url>
      <doi>10.18653/v1/D17-1202</doi>
      <abstract>In this paper, we present a novel document similarity measure based on the definition of a <a href="https://en.wikipedia.org/wiki/Graph_kernel">graph kernel</a> between pairs of documents. The proposed measure takes into account both the terms contained in the documents and the relationships between them. By representing each document as a graph-of-words, we are able to model these relationships and then determine how similar two documents are by using a modified shortest-path graph kernel. We evaluate our approach on two tasks and compare it against several baseline approaches using various <a href="https://en.wikipedia.org/wiki/Performance_metric">performance metrics</a> such as DET curves and macro-average F1-score. Experimental results on a range of <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> showed that our proposed approach outperforms traditional techniques and is capable of measuring more accurately the similarity between two documents.</abstract>
      <bibkey>nikolentzos-etal-2017-shortest</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/webkb">WebKB</pwcdataset>
    </paper>
    <paper id="203">
      <title>Adapting Topic Models using <a href="https://en.wikipedia.org/wiki/Lexical_analysis">Lexical Associations</a> with Tree Priors</title>
      <author><first>Weiwei</first> <last>Yang</last></author>
      <author><first>Jordan</first> <last>Boyd-Graber</last></author>
      <author><first>Philip</first> <last>Resnik</last></author>
      <pages>1901–1906</pages>
      <url hash="980736f4">D17-1203</url>
      <doi>10.18653/v1/D17-1203</doi>
      <abstract>Models work best when they are optimized taking into account the evaluation criteria that people care about. For <a href="https://en.wikipedia.org/wiki/Topic_model">topic models</a>, people often care about <a href="https://en.wikipedia.org/wiki/Interpretability">interpretability</a>, which can be approximated using measures of lexical association. We integrate lexical association into topic optimization using tree priors, which provide a flexible framework that can take advantage of both first order word associations and the higher-order associations captured by word embeddings. Tree priors improve topic interpretability without hurting <a href="https://en.wikipedia.org/wiki/Intrinsic_and_extrinsic_properties">extrinsic performance</a>.</abstract>
      <bibkey>yang-etal-2017-adapting</bibkey>
    </paper>
    <paper id="204">
      <title>Finding Patterns in Noisy Crowds : Regression-based Annotation Aggregation for Crowdsourced Data</title>
      <author><first>Natalie</first> <last>Parde</last></author>
      <author><first>Rodney</first> <last>Nielsen</last></author>
      <pages>1907–1912</pages>
      <url hash="5e151bd9">D17-1204</url>
      <doi>10.18653/v1/D17-1204</doi>
      <abstract>Crowdsourcing offers a convenient means of obtaining <a href="https://en.wikipedia.org/wiki/Data_(computing)">labeled data</a> quickly and inexpensively. However, crowdsourced labels are often noisier than expert-annotated data, making it difficult to aggregate them meaningfully. We present an aggregation approach that learns a <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression model</a> from crowdsourced annotations to predict aggregated labels for instances that have no expert adjudications. The predicted labels achieve a correlation of 0.594 with expert labels on our <a href="https://en.wikipedia.org/wiki/Data">data</a>, outperforming the best alternative aggregation method by 11.9 %. Our approach also outperforms the alternatives on third-party datasets.</abstract>
      <bibkey>parde-nielsen-2017-finding</bibkey>
    </paper>
    <paper id="205">
      <title>CROWD-IN-THE-LOOP : A Hybrid Approach for Annotating Semantic Roles<fixed-case>CROWD</fixed-case>-<fixed-case>IN</fixed-case>-<fixed-case>THE</fixed-case>-<fixed-case>LOOP</fixed-case>: A Hybrid Approach for Annotating Semantic Roles</title>
      <author><first>Chenguang</first> <last>Wang</last></author>
      <author><first>Alan</first> <last>Akbik</last></author>
      <author><first>Laura</first> <last>Chiticariu</last></author>
      <author><first>Yunyao</first> <last>Li</last></author>
      <author><first>Fei</first> <last>Xia</last></author>
      <author><first>Anbang</first> <last>Xu</last></author>
      <pages>1913–1922</pages>
      <url hash="9be16d5c">D17-1205</url>
      <doi>10.18653/v1/D17-1205</doi>
      <abstract>Crowdsourcing has proven to be an effective method for generating <a href="https://en.wikipedia.org/wiki/Labeled_data">labeled data</a> for a range of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP tasks</a>. However, multiple recent attempts of using <a href="https://en.wikipedia.org/wiki/Crowdsourcing">crowdsourcing</a> to generate gold-labeled training data for semantic role labeling (SRL) reported only modest results, indicating that SRL is perhaps too difficult a task to be effectively crowdsourced. In this paper, we postulate that while producing SRL annotation does require expert involvement in general, a large subset of SRL labeling tasks is in fact appropriate for the crowd. We present a novel <a href="https://en.wikipedia.org/wiki/Workflow">workflow</a> in which we employ a classifier to identify difficult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality.</abstract>
      <bibkey>wang-etal-2017-crowd</bibkey>
    </paper>
    <paper id="207">
      <title>Earth Mover’s Distance Minimization for Unsupervised Bilingual Lexicon Induction</title>
      <author><first>Meng</first> <last>Zhang</last></author>
      <author id="yang-liu-ict"><first>Yang</first> <last>Liu</last></author>
      <author><first>Huanbo</first> <last>Luan</last></author>
      <author><first>Maosong</first> <last>Sun</last></author>
      <pages>1934–1945</pages>
      <url hash="3a17fc61">D17-1207</url>
      <doi>10.18653/v1/D17-1207</doi>
      <abstract>Cross-lingual natural language processing hinges on the premise that there exists invariance across languages. At the word level, researchers have identified such <a href="https://en.wikipedia.org/wiki/Invariant_(mathematics)">invariance</a> in the word embedding semantic spaces of different languages. However, in order to connect the separate spaces, cross-lingual supervision encoded in parallel data is typically required. In this paper, we attempt to establish the cross-lingual connection without relying on any cross-lingual supervision. By viewing word embedding spaces as <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)">distributions</a>, we propose to minimize their earth mover’s distance, a measure of divergence between distributions. We demonstrate the success on the unsupervised bilingual lexicon induction task. In addition, we reveal an interesting finding that the earth mover’s distance shows potential as a measure of <a href="https://en.wikipedia.org/wiki/Language">language difference</a>.</abstract>
      <video href="https://vimeo.com/238232779" />
      <bibkey>zhang-etal-2017-earth</bibkey>
    </paper>
    <paper id="209">
      <title>Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</title>
      <author><first>Jasmijn</first> <last>Bastings</last></author>
      <author><first>Ivan</first> <last>Titov</last></author>
      <author><first>Wilker</first> <last>Aziz</last></author>
      <author><first>Diego</first> <last>Marcheggiani</last></author>
      <author><first>Khalil</first> <last>Sima’an</last></author>
      <pages>1957–1967</pages>
      <url hash="94ee509e">D17-1209</url>
      <doi>10.18653/v1/D17-1209</doi>
      <abstract>We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. We rely on graph-convolutional networks (GCNs), a recent class of <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> developed for modeling <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">graph-structured data</a>. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take <a href="https://en.wikipedia.org/wiki/Word_processor_(electronic_device)">word representations</a> as input and produce <a href="https://en.wikipedia.org/wiki/Word_processor_(electronic_device)">word representations</a> as output, so they can easily be incorporated as layers into standard <a href="https://en.wikipedia.org/wiki/Encoder">encoders</a> (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.</abstract>
      <revision id="1" href="D17-1209v1" hash="1d2c4800" />
      <revision id="2" href="D17-1209v2" hash="94ee509e" date="2020-07-07">Changed the name of one of the authors.</revision>
      <bibkey>bastings-etal-2017-graph</bibkey>
    </paper>
    <paper id="210">
      <title>Trainable Greedy Decoding for Neural Machine Translation</title>
      <author><first>Jiatao</first> <last>Gu</last></author>
      <author><first>Kyunghyun</first> <last>Cho</last></author>
      <author><first>Victor O.K.</first> <last>Li</last></author>
      <pages>1968–1978</pages>
      <url hash="0350ca50">D17-1210</url>
      <doi>10.18653/v1/D17-1210</doi>
      <abstract>Recent research in <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> has largely focused on two aspects ; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an <a href="https://en.wikipedia.org/wiki/Actor">actor</a> that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> using four language pairs and two decoding objectives and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal <a href="https://en.wikipedia.org/wiki/Overhead_(computing)">computational overhead</a>.</abstract>
      <video href="https://vimeo.com/238236435" />
      <bibkey>gu-etal-2017-trainable</bibkey>
    </paper>
    <paper id="211">
      <title>Satirical News Detection and Analysis using <a href="https://en.wikipedia.org/wiki/Attentional_control">Attention Mechanism</a> and <a href="https://en.wikipedia.org/wiki/Linguistic_description">Linguistic Features</a></title>
      <author><first>Fan</first> <last>Yang</last></author>
      <author><first>Arjun</first> <last>Mukherjee</last></author>
      <author><first>Eduard</first> <last>Dragut</last></author>
      <pages>1979–1989</pages>
      <url hash="0c65c54b">D17-1211</url>
      <doi>10.18653/v1/D17-1211</doi>
      <abstract>Satirical news is considered to be entertainment, but <a href="https://en.wikipedia.org/wiki/It_(2017_film)">it</a> is potentially deceptive and harmful. Despite the embedded genre in the article, not everyone can recognize the satirical cues and therefore believe the news as true news. We observe that satirical cues are often reflected in certain paragraphs rather than the whole document. Existing works only consider document-level features to detect the <a href="https://en.wikipedia.org/wiki/Satire">satire</a>, which could be limited. We consider paragraph-level linguistic features to unveil the <a href="https://en.wikipedia.org/wiki/Satire">satire</a> by incorporating <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> and <a href="https://en.wikipedia.org/wiki/Attentional_control">attention mechanism</a>. We investigate the difference between paragraph-level features and document-level features, and analyze them on a large satirical news dataset. The evaluation shows that the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> detects <a href="https://en.wikipedia.org/wiki/News_satire">satirical news</a> effectively and reveals what <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> are important at which level.</abstract>
      <video href="https://vimeo.com/238235500" />
      <bibkey>yang-etal-2017-satirical</bibkey>
      <pwccode url="https://github.com/fYYw/satire" additional="false">fYYw/satire</pwccode>
    </paper>
    <paper id="212">
      <title>Fine Grained Citation Span for References in Wikipedia<fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Besnik</first> <last>Fetahu</last></author>
      <author><first>Katja</first> <last>Markert</last></author>
      <author><first>Avishek</first> <last>Anand</last></author>
      <pages>1990–1999</pages>
      <url hash="8d5d377a">D17-1212</url>
      <doi>10.18653/v1/D17-1212</doi>
      <abstract>Verifiability is one of the core editing principles in <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a>, where editors are encouraged to provide <a href="https://en.wikipedia.org/wiki/Citation">citations</a> for the added content. For a Wikipedia article determining what content is covered by a <a href="https://en.wikipedia.org/wiki/Citation">citation</a> or the citation span is not trivial, an important aspect for automated citation finding for uncovered content, or fact assessments. We address the problem of determining the citation span in Wikipedia articles. We approach this <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> by classifying which textual fragments in an article are covered or hold true given a <a href="https://en.wikipedia.org/wiki/Citation">citation</a>. We propose a sequence classification approach where for a paragraph and a citation, we determine the citation span at a fine-grained level. We provide a thorough experimental evaluation and compare our approach against baselines adopted from the scientific domain, where we show improvement for all evaluation metrics.</abstract>
      <video href="https://vimeo.com/238233167" />
      <bibkey>fetahu-etal-2017-fine</bibkey>
    </paper>
    <paper id="213">
      <title>Identifying Semantic Edit Intentions from Revisions in Wikipedia<fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Diyi</first> <last>Yang</last></author>
      <author><first>Aaron</first> <last>Halfaker</last></author>
      <author><first>Robert</first> <last>Kraut</last></author>
      <author><first>Eduard</first> <last>Hovy</last></author>
      <pages>2000–2010</pages>
      <url hash="b88901f8">D17-1213</url>
      <doi>10.18653/v1/D17-1213</doi>
      <abstract>Most studies on human editing focus merely on syntactic revision operations, failing to capture the intentions behind revision changes, which are essential for facilitating the single and collaborative writing process. In this work, we develop in collaboration with Wikipedia editors a 13-category taxonomy of the semantic intention behind edits in Wikipedia articles. Using labeled article edits, we build a computational classifier of intentions that achieved a micro-averaged F1 score of 0.621. We use this model to investigate edit intention effectiveness : how different types of edits predict the retention of newcomers and changes in the quality of articles, two key concerns for Wikipedia today. Our analysis shows that the types of <a href="https://en.wikipedia.org/wiki/Wikipedia_community">edits</a> that users make in their first session predict their subsequent survival as <a href="https://en.wikipedia.org/wiki/Wikipedia_community">Wikipedia editors</a>, and articles in different stages need different types of <a href="https://en.wikipedia.org/wiki/Wikipedia_community">edits</a>.</abstract>
      <video href="https://vimeo.com/238233635" />
      <bibkey>yang-etal-2017-identifying-semantic</bibkey>
    </paper>
    <paper id="214">
      <title>Accurate Supervised and Semi-Supervised Machine Reading for Long Documents</title>
      <author><first>Daniel</first> <last>Hewlett</last></author>
      <author><first>Llion</first> <last>Jones</last></author>
      <author><first>Alexandre</first> <last>Lacoste</last></author>
      <author><first>Izzeddin</first> <last>Gur</last></author>
      <pages>2011–2020</pages>
      <url hash="b66ebdf8">D17-1214</url>
      <doi>10.18653/v1/D17-1214</doi>
      <abstract>We introduce a hierarchical architecture for <a href="https://en.wikipedia.org/wiki/Machine_reading">machine reading</a> capable of extracting precise information from long documents. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> divides the document into small, overlapping windows and encodes all windows in parallel with an RNN. It then attends over these window encodings, reducing them to a single <a href="https://en.wikipedia.org/wiki/Code">encoding</a>, which is decoded into an answer using a sequence decoder. This hierarchical approach allows the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> to scale to longer documents without increasing the number of sequential steps. In a <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised setting</a>, our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> achieves state of the art <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 76.8 on the WikiReading dataset. We also evaluate the model in a semi-supervised setting by downsampling the WikiReading training set to create increasingly smaller amounts of supervision, while leaving the full unlabeled document corpus to train a sequence autoencoder on document windows. We evaluate <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that can reuse autoencoder states and outputs without fine-tuning their weights, allowing for more efficient <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training</a> and <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference</a>.</abstract>
      <video href="https://vimeo.com/238231359" />
      <bibkey>hewlett-etal-2017-accurate</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikireading">WikiReading</pwcdataset>
    </paper>
    <paper id="215">
      <title>Adversarial Examples for Evaluating Reading Comprehension Systems</title>
      <author><first>Robin</first> <last>Jia</last></author>
      <author><first>Percy</first> <last>Liang</last></author>
      <pages>2021–2031</pages>
      <url hash="03c4c4dc">D17-1215</url>
      <doi>10.18653/v1/D17-1215</doi>
      <abstract>Standard accuracy metrics indicate that <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension systems</a> are making rapid progress, but the extent to which these <a href="https://en.wikipedia.org/wiki/System">systems</a> truly understand <a href="https://en.wikipedia.org/wiki/Language">language</a> remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of sixteen published <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> drops from an average of 75 % F1 score to 36 % ; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7 %. We hope our insights will motivate the development of new <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that understand language more precisely.</abstract>
      <video href="https://vimeo.com/238231419" />
      <bibkey>jia-liang-2017-adversarial</bibkey>
      <pwccode url="https://worksheets.codalab.org/worksheets/0xc86d3ebe69a3427d91f9aaa63f7d1e7d" additional="true">worksheets/0xc86d3ebe</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="216">
      <title>Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension</title>
      <author><first>Hongyu</first> <last>Lin</last></author>
      <author><first>Le</first> <last>Sun</last></author>
      <author><first>Xianpei</first> <last>Han</last></author>
      <pages>2032–2043</pages>
      <url hash="e09f2460">D17-1216</url>
      <doi>10.18653/v1/D17-1216</doi>
      <abstract>Reasoning with <a href="https://en.wikipedia.org/wiki/Commonsense_knowledge">commonsense knowledge</a> is critical for <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>. Traditional methods for commonsense machine comprehension mostly only focus on one specific kind of knowledge, neglecting the fact that <a href="https://en.wikipedia.org/wiki/Commonsense_reasoning">commonsense reasoning</a> requires simultaneously considering different kinds of commonsense knowledge. In this paper, we propose a multi-knowledge reasoning method, which can exploit heterogeneous knowledge for commonsense machine comprehension. Specifically, we first mine different kinds of knowledge (including event narrative knowledge, entity semantic knowledge and sentiment coherent knowledge) and encode them as inference rules with costs. Then we propose a multi-knowledge reasoning model, which selects <a href="https://en.wikipedia.org/wiki/Rule_of_inference">inference rules</a> for a specific reasoning context using attention mechanism, and reasons by summarizing all valid <a href="https://en.wikipedia.org/wiki/Rule_of_inference">inference rules</a>. Experiments on RocStories show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> outperforms traditional <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> significantly.</abstract>
      <video href="https://vimeo.com/238235420" />
      <bibkey>lin-etal-2017-reasoning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="218">
      <title>What is the Essence of a Claim? Cross-Domain Claim Identification</title>
      <author><first>Johannes</first> <last>Daxenberger</last></author>
      <author><first>Steffen</first> <last>Eger</last></author>
      <author><first>Ivan</first> <last>Habernal</last></author>
      <author><first>Christian</first> <last>Stab</last></author>
      <author><first>Iryna</first> <last>Gurevych</last></author>
      <pages>2055–2066</pages>
      <url hash="10a8c1cc">D17-1218</url>
      <doi>10.18653/v1/D17-1218</doi>
      <attachment type="attachment" hash="0fed2435">D17-1218.Attachment.zip</attachment>
      <abstract>Argument mining has become a popular research area in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent conceptualization of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.</abstract>
      <bibkey>daxenberger-etal-2017-essence</bibkey>
      <pwccode url="https://github.com/UKPLab/emnlp2017-claim-identification" additional="false">UKPLab/emnlp2017-claim-identification</pwccode>
    </paper>
    <paper id="219">
      <title>Identifying Where to Focus in <a href="https://en.wikipedia.org/wiki/Reading_comprehension">Reading Comprehension</a> for Neural Question Generation</title>
      <author><first>Xinya</first> <last>Du</last></author>
      <author><first>Claire</first> <last>Cardie</last></author>
      <pages>2067–2073</pages>
      <url hash="10705c18">D17-1219</url>
      <doi>10.18653/v1/D17-1219</doi>
      <attachment type="attachment" hash="607d7119">D17-1219.Attachment.pdf</attachment>
      <abstract>A first step in the task of automatically generating questions for testing <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a> is to identify question-worthy sentences, i.e. sentences in a text passage that humans find it worthwhile to ask questions about. We propose a hierarchical neural sentence-level sequence tagging model for this task, which existing approaches to question generation have ignored. The approach is fully data-driven   with no sophisticated NLP pipelines or any hand-crafted rules / features   and compares favorably to a number of baselines when evaluated on the SQuAD data set. When incorporated into an existing neural question generation system, the resulting end-to-end system achieves state-of-the-art performance for paragraph-level question generation for <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a>.<i>question-worthy</i> sentences, i.e.
      sentences in a text passage that humans find it worthwhile to ask
      questions about. We propose a hierarchical neural sentence-level sequence
      tagging model for this task, which existing approaches to question
      generation have ignored. The approach is fully data-driven — with no
      sophisticated NLP pipelines or any hand-crafted rules/features — and
      compares favorably to a number of baselines when evaluated on the SQuAD
      data set. When incorporated into an existing neural question generation
      system, the resulting end-to-end system achieves state-of-the-art
      performance for paragraph-level question generation for reading
      comprehension.
    </abstract>
      <bibkey>du-cardie-2017-identifying</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="221">
      <title>Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization</title>
      <author><first>Piji</first> <last>Li</last></author>
      <author><first>Wai</first> <last>Lam</last></author>
      <author><first>Lidong</first> <last>Bing</last></author>
      <author><first>Weiwei</first> <last>Guo</last></author>
      <author><first>Hang</first> <last>Li</last></author>
      <pages>2081–2090</pages>
      <url hash="dcfaa1c3">D17-1221</url>
      <doi>10.18653/v1/D17-1221</doi>
      <abstract>When people recall and digest what they have read for writing summaries, the important content is more likely to attract their attention. Inspired by this observation, we propose a cascaded attention based unsupervised model to estimate the <a href="https://en.wikipedia.org/wiki/Salience_(neuroscience)">salience information</a> from the text for compressive multi-document summarization. The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience. By adding sparsity constraints on the number of output vectors, we can generate condensed information which can be treated as word salience. Fine-grained and coarse-grained sentence compression strategies are incorporated to produce compressive summaries. Experiments on some benchmark data sets show that our <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> achieves better results than the state-of-the-art methods.</abstract>
      <bibkey>li-etal-2017-cascaded</bibkey>
    </paper>
    <paper id="222">
      <title>Deep Recurrent Generative Decoder for Abstractive Text Summarization</title>
      <author><first>Piji</first> <last>Li</last></author>
      <author><first>Wai</first> <last>Lam</last></author>
      <author><first>Lidong</first> <last>Bing</last></author>
      <author><first>Zihao</first> <last>Wang</last></author>
      <pages>2091–2100</pages>
      <url hash="08c15995">D17-1222</url>
      <doi>10.18653/v1/D17-1222</doi>
      <abstract>We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-of-the-art methods.</abstract>
      <bibkey>li-etal-2017-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/duc-2004">DUC 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lcsts">LCSTS</pwcdataset>
    </paper>
    <paper id="224">
      <title>Towards Automatic Construction of News Overview Articles by News Synthesis</title>
      <author><first>Jianmin</first> <last>Zhang</last></author>
      <author><first>Xiaojun</first> <last>Wan</last></author>
      <pages>2111–2116</pages>
      <url hash="6d1ca902">D17-1224</url>
      <doi>10.18653/v1/D17-1224</doi>
      <abstract>In this paper we investigate a new task of automatically constructing an overview article from a given set of <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> about a news event. We propose a news synthesis approach to address this task based on passage segmentation, <a href="https://en.wikipedia.org/wiki/Ranking">ranking</a>, selection and merging. Our proposed approach is compared with several typical multi-document summarization methods on the Wikinews dataset, and achieves the best performance on both automatic evaluation and manual evaluation.</abstract>
      <bibkey>zhang-wan-2017-towards</bibkey>
    </paper>
    <paper id="227">
      <title>When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size)</title>
      <author><first>Liang</first> <last>Huang</last></author>
      <author><first>Kai</first> <last>Zhao</last></author>
      <author><first>Mingbo</first> <last>Ma</last></author>
      <pages>2134–2139</pages>
      <url hash="7f736f55">D17-1227</url>
      <doi>10.18653/v1/D17-1227</doi>
      <abstract>In neural text generation such as <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a>, <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a>, and image captioning, <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> is widely used to improve the output text quality. However, in the neural generation setting, hypotheses can finish in different steps, which makes it difficult to decide when to end <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> to ensure <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">optimality</a>. We propose a provably optimal beam search algorithm that will always return the optimal-score complete hypothesis (modulo beam size), and finish as soon as the optimality is established. To counter neural generation’s tendency for shorter hypotheses, we also introduce a bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal. Experiments on <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> demonstrate that our principled beam search algorithm leads to improvement in BLEU score over previously proposed alternatives.</abstract>
      <bibkey>huang-etal-2017-finish</bibkey>
    </paper>
    <paper id="228">
      <title>Steering Output Style and Topic in Neural Response Generation</title>
      <author><first>Di</first> <last>Wang</last></author>
      <author><first>Nebojsa</first> <last>Jojic</last></author>
      <author><first>Chris</first> <last>Brockett</last></author>
      <author><first>Eric</first> <last>Nyberg</last></author>
      <pages>2140–2150</pages>
      <url hash="d0770995">D17-1228</url>
      <doi>10.18653/v1/D17-1228</doi>
      <attachment type="attachment" hash="7adfd310">D17-1228.Attachment.zip</attachment>
      <abstract>We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems : a faithfulness model and a decoding method based on <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">selective-sampling</a>. We also describe <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training and sampling algorithms</a> that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to to restrict style and topic without degrading output quality in conversational tasks.</abstract>
      <bibkey>wang-etal-2017-steering</bibkey>
      <pwccode url="https://github.com/digo/steering-response-style-and-topic" additional="false">digo/steering-response-style-and-topic</pwccode>
    </paper>
    <paper id="230">
      <title>Adversarial Learning for Neural Dialogue Generation</title>
      <author><first>Jiwei</first> <last>Li</last></author>
      <author><first>Will</first> <last>Monroe</last></author>
      <author><first>Tianlin</first> <last>Shi</last></author>
      <author><first>Sébastien</first> <last>Jean</last></author>
      <author><first>Alan</first> <last>Ritter</last></author>
      <author><first>Dan</first> <last>Jurafsky</last></author>
      <pages>2157–2169</pages>
      <url hash="808f1fe8">D17-1230</url>
      <doi>10.18653/v1/D17-1230</doi>
      <abstract>We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems : a <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> to produce response sequences, and a discriminatoranalagous to the human evaluator in the <a href="https://en.wikipedia.org/wiki/Turing_test">Turing test</a> to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the <a href="https://en.wikipedia.org/wiki/Discriminator">discriminator</a> are used to encourage the <a href="https://en.wikipedia.org/wiki/System">system</a> towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines</abstract>
      <bibkey>li-etal-2017-adversarial</bibkey>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="233">
      <title>Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems</title>
      <author><first>Lili</first> <last>Yao</last></author>
      <author><first>Yaoyuan</first> <last>Zhang</last></author>
      <author><first>Yansong</first> <last>Feng</last></author>
      <author><first>Dongyan</first> <last>Zhao</last></author>
      <author><first>Rui</first> <last>Yan</last></author>
      <pages>2190–2199</pages>
      <url hash="2a0d8edc">D17-1233</url>
      <doi>10.18653/v1/D17-1233</doi>
      <abstract>The study on human-computer conversation systems is a hot research topic nowadays. One of the prevailing methods to build the system is using the generative Sequence-to-Sequence (Seq2Seq) model through <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>. However, the standard Seq2Seq model is prone to generate <a href="https://en.wikipedia.org/wiki/Triviality_(mathematics)">trivial responses</a>. In this paper, we aim to generate a more meaningful and informative reply when answering a given question. We propose an implicit content-introducing method which incorporates additional information into the Seq2Seq model in a flexible way. Specifically, we fuse the general decoding and the auxiliary cue word information through our proposed hierarchical gated fusion unit. Experiments on real-life data demonstrate that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> consistently outperforms a set of competitive baselines in terms of BLEU scores and human evaluation.</abstract>
      <bibkey>yao-etal-2017-towards</bibkey>
    </paper>
    <paper id="234">
      <title>Affordable On-line Dialogue Policy Learning</title>
      <author><first>Cheng</first> <last>Chang</last></author>
      <author><first>Runzhe</first> <last>Yang</last></author>
      <author><first>Lu</first> <last>Chen</last></author>
      <author><first>Xiang</first> <last>Zhou</last></author>
      <author><first>Kai</first> <last>Yu</last></author>
      <pages>2200–2209</pages>
      <url hash="8e2cfc08">D17-1234</url>
      <doi>10.18653/v1/D17-1234</doi>
      <attachment type="attachment" hash="0a0f613e">D17-1234.Attachment.pdf</attachment>
      <abstract>The key to building an evolvable dialogue system in real-world scenarios is to ensure an affordable on-line dialogue policy learning, which requires the on-line learning process to be safe, efficient and economical. But in reality, due to the scarcity of real interaction data, the <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue system</a> usually grows slowly. Besides, the poor initial dialogue policy easily leads to bad user experience and incurs a failure of attracting users to contribute training data, so that the learning process is unsustainable. To accurately depict this, two <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">quantitative metrics</a> are proposed to assess safety and efficiency issues. For solving the unsustainable learning problem, we proposed a complete companion teaching framework incorporating the guidance from the human teacher. Since the human teaching is expensive, we compared various teaching schemes answering the question how and when to teach, to economically utilize teaching budget, so that make the online learning process affordable.</abstract>
      <bibkey>chang-etal-2017-affordable</bibkey>
    </paper>
    <paper id="235">
      <title>Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models</title>
      <author><first>Yuanlong</first> <last>Shao</last></author>
      <author><first>Stephan</first> <last>Gouws</last></author>
      <author><first>Denny</first> <last>Britz</last></author>
      <author><first>Anna</first> <last>Goldie</last></author>
      <author><first>Brian</first> <last>Strope</last></author>
      <author><first>Ray</first> <last>Kurzweil</last></author>
      <pages>2210–2219</pages>
      <url hash="3bae81eb">D17-1235</url>
      <doi>10.18653/v1/D17-1235</doi>
      <attachment type="attachment" hash="f6f6cf3a">D17-1235.Attachment.pdf</attachment>
      <abstract>Sequence-to-sequence models have been applied to the conversation response generation problem where the source sequence is the conversation history and the target sequence is the response. Unlike <a href="https://en.wikipedia.org/wiki/Translation">translation</a>, conversation responding is inherently creative. The generation of long, informative, coherent, and diverse responses remains a hard task. In this work, we focus on the <a href="https://en.wikipedia.org/wiki/Turn_(geometry)">single turn setting</a>. We add self-attention to the decoder to maintain <a href="https://en.wikipedia.org/wiki/Coherence_(statistics)">coherence</a> in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a>. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths.</abstract>
      <bibkey>shao-etal-2017-generating</bibkey>
    </paper>
    <paper id="236">
      <title>Bootstrapping incremental dialogue systems from minimal data : the generalisation power of dialogue grammars</title>
      <author><first>Arash</first> <last>Eshghi</last></author>
      <author><first>Igor</first> <last>Shalyminov</last></author>
      <author><first>Oliver</first> <last>Lemon</last></author>
      <pages>2220–2230</pages>
      <url hash="79206beb">D17-1236</url>
      <doi>10.18653/v1/D17-1236</doi>
      <abstract>We investigate an <a href="https://en.wikipedia.org/wiki/End-to-end_principle">end-to-end method</a> for automatically inducing task-based dialogue systems from small amounts of unannotated dialogue data. It combines an incremental semantic grammar-Dynamic Syntax and Type Theory with Records (DS-TTR)-with Reinforcement Learning (RL), where language generation and dialogue management are a joint decision problem. The systems thus produced are incremental : dialogues are processed word-by-word, shown previously to be essential in supporting natural, spontaneous dialogue. We hypothesised that the rich linguistic knowledge within the grammar should enable a combinatorially large number of dialogue variations to be processed, even when trained on very few dialogues. Our experiments show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can process 74 % of the Facebook AI bAbI dataset even when trained on only 0.13 % of the <a href="https://en.wikipedia.org/wiki/Data">data</a> (5 dialogues). It can in addition process 65 % of bAbI+, a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> we created by systematically adding incremental dialogue phenomena such as restarts and <a href="https://en.wikipedia.org/wiki/Self-reference">self-corrections</a> to bAbI. We compare our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> with a state-of-the-art retrieval model, MEMN2N. We find that, in terms of semantic accuracy, the MEMN2N model shows very poor robustness to the bAbI+ transformations even when trained on the full bAbI dataset.</abstract>
      <bibkey>eshghi-etal-2017-bootstrapping</bibkey>
    </paper>
    <paper id="239">
      <title>Challenges in Data-to-Document Generation</title>
      <author><first>Sam</first> <last>Wiseman</last></author>
      <author><first>Stuart</first> <last>Shieber</last></author>
      <author><first>Alexander</first> <last>Rush</last></author>
      <pages>2253–2263</pages>
      <url hash="07257479">D17-1239</url>
      <doi>10.18653/v1/D17-1239</doi>
      <attachment type="attachment" hash="0e915b82">D17-1239.Attachment.pdf</attachment>
      <abstract>Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of <a href="https://en.wikipedia.org/wiki/Record_(computer_science)">database records</a>. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.</abstract>
      <bibkey>wiseman-etal-2017-challenges</bibkey>
      <pwccode url="https://github.com/harvardnlp/data2text" additional="true">harvardnlp/data2text</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rotowire">RotoWire</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/robocup">RoboCup</pwcdataset>
    </paper>
    <paper id="240">
      <title>All that is English may be Hindi : Enhancing <a href="https://en.wikipedia.org/wiki/Language_identification">language identification</a> through automatic ranking of the likeliness of <a href="https://en.wikipedia.org/wiki/Loanword">word borrowing</a> in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a><fixed-case>E</fixed-case>nglish may be <fixed-case>H</fixed-case>indi: Enhancing language identification through automatic ranking of the likeliness of word borrowing in social media</title>
      <author><first>Jasabanta</first> <last>Patro</last></author>
      <author><first>Bidisha</first> <last>Samanta</last></author>
      <author><first>Saurabh</first> <last>Singh</last></author>
      <author><first>Abhipsa</first> <last>Basu</last></author>
      <author><first>Prithwish</first> <last>Mukherjee</last></author>
      <author><first>Monojit</first> <last>Choudhury</last></author>
      <author><first>Animesh</first> <last>Mukherjee</last></author>
      <pages>2264–2274</pages>
      <url hash="a1d07225">D17-1240</url>
      <doi>10.18653/v1/D17-1240</doi>
      <attachment type="attachment" hash="eef3b1f6">D17-1240.Attachment.pdf</attachment>
      <abstract>n this paper, we present a set of computational methods to identify the likeliness of a word being borrowed, based on the signals from <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. In terms of Spearman’s correlation values, our methods perform more than two times better (0.62) in predicting the borrowing likeliness compared to the best performing baseline (0.26) reported in literature. Based on this likeliness estimate we asked annotators to re-annotate the language tags of foreign words in predominantly native contexts. In 88 % of cases the annotators felt that the foreign language tag should be replaced by native language tag, thus indicating a huge scope for improvement of automatic language identification systems.</abstract>
      <bibkey>patro-etal-2017-english</bibkey>
    </paper>
    <paper id="241">
      <title>Multi-View Unsupervised User Feature Embedding for Social Media-based Substance Use Prediction</title>
      <author><first>Tao</first> <last>Ding</last></author>
      <author><first>Warren K.</first> <last>Bickel</last></author>
      <author><first>Shimei</first> <last>Pan</last></author>
      <pages>2275–2284</pages>
      <url hash="f0c3fed0">D17-1241</url>
      <doi>10.18653/v1/D17-1241</doi>
      <abstract>In this paper, we demonstrate how the state-of-the-art machine learning and text mining techniques can be used to build effective social media-based substance use detection systems. Since a substance use ground truth is difficult to obtain on a large scale, to maximize system performance, we explore different unsupervised feature learning methods to take advantage of a large amount of unsupervised social media data. We also demonstrate the benefit of using multi-view unsupervised feature learning to combine heterogeneous user information such as <a href="https://en.wikipedia.org/wiki/List_of_Facebook_features">Facebook likes</a> and status updates to enhance system performance. Based on our evaluation, our best models achieved 86 % <a href="https://en.wikipedia.org/wiki/Analysis_of_covariance">AUC</a> for predicting <a href="https://en.wikipedia.org/wiki/Tobacco_smoking">tobacco use</a>, 81 % for <a href="https://en.wikipedia.org/wiki/Alcoholic_drink">alcohol use</a> and 84 % for <a href="https://en.wikipedia.org/wiki/Substance_abuse">illicit drug use</a>, all of which significantly outperformed existing methods. Our investigation has also uncovered interesting relations between a <a href="https://en.wikipedia.org/wiki/User-generated_content">user’s social media behavior</a> (e.g., word usage) and <a href="https://en.wikipedia.org/wiki/Substance_use_disorder">substance use</a>.</abstract>
      <bibkey>ding-etal-2017-multi</bibkey>
    </paper>
    <paper id="242">
      <title>Demographic-aware word associations</title>
      <author><first>Aparna</first> <last>Garimella</last></author>
      <author><first>Carmen</first> <last>Banea</last></author>
      <author><first>Rada</first> <last>Mihalcea</last></author>
      <pages>2285–2295</pages>
      <url hash="01cd3db0">D17-1242</url>
      <doi>10.18653/v1/D17-1242</doi>
      <abstract>Variations of <a href="https://en.wikipedia.org/wiki/Word_association">word associations</a> across different groups of people can provide insights into people’s psychologies and their world views. To capture these variations, we introduce the task of demographic-aware word associations. We build a new gold standard dataset consisting of word association responses for approximately 300 stimulus words, collected from more than 800 respondents of different gender (male / female) and from different locations (India / United States), and show that there are significant variations in the word associations made by these groups. We also introduce a new demographic-aware word association model based on a neural net skip-gram architecture, and show how computational methods for measuring word associations that specifically account for writer demographics can outperform generic methods that are agnostic to such information.</abstract>
      <bibkey>garimella-etal-2017-demographic</bibkey>
    </paper>
    <paper id="243">
      <title>A Factored Neural Network Model for Characterizing Online Discussions in Vector Space</title>
      <author><first>Hao</first> <last>Cheng</last></author>
      <author><first>Hao</first> <last>Fang</last></author>
      <author><first>Mari</first> <last>Ostendorf</last></author>
      <pages>2296–2306</pages>
      <url hash="54404d9d">D17-1243</url>
      <doi>10.18653/v1/D17-1243</doi>
      <abstract>We develop a novel factored neural model that learns comment embeddings in an unsupervised way leveraging the structure of distributional context in online discussion forums. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> links different context with related language factors in the <a href="https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms">embedding space</a>, providing a way to interpret the factored embeddings. Evaluated on a community endorsement prediction task using a large collection of topic-varying Reddit discussions, the factored embeddings consistently achieve improvement over other text representations. Qualitative analysis shows that the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> captures community style and topic, as well as response trigger patterns.</abstract>
      <bibkey>cheng-etal-2017-factored</bibkey>
      <pwccode url="https://github.com/hao-cheng/factored_neural" additional="false">hao-cheng/factored_neural</pwccode>
    </paper>
    <paper id="244">
      <title>Dimensions of Interpersonal Relationships : Corpus and Experiments</title>
      <author><first>Farzana</first> <last>Rashid</last></author>
      <author><first>Eduardo</first> <last>Blanco</last></author>
      <pages>2307–2316</pages>
      <url hash="80088613">D17-1244</url>
      <doi>10.18653/v1/D17-1244</doi>
      <abstract>This paper presents a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> and experiments to determine dimensions of interpersonal relationships. We define a set of <a href="https://en.wikipedia.org/wiki/Dimension_(data_warehouse)">dimensions</a> heavily inspired by work in <a href="https://en.wikipedia.org/wiki/Social_science">social science</a>. We create a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> by retrieving pairs of people, and then annotating <a href="https://en.wikipedia.org/wiki/Dimension_(data_warehouse)">dimensions</a> for their relationships. A corpus analysis shows that <a href="https://en.wikipedia.org/wiki/Dimension_(data_warehouse)">dimensions</a> can be annotated reliably. Experimental results show that given a pair of people, values to <a href="https://en.wikipedia.org/wiki/Dimension_(data_warehouse)">dimensions</a> can be assigned automatically.</abstract>
      <bibkey>rashid-blanco-2017-dimensions</bibkey>
    </paper>
    <paper id="245">
      <title>Argument Mining on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> : Arguments, Facts and Sources<fixed-case>T</fixed-case>witter: Arguments, Facts and Sources</title>
      <author><first>Mihai</first> <last>Dusmanu</last></author>
      <author><first>Elena</first> <last>Cabrio</last></author>
      <author><first>Serena</first> <last>Villata</last></author>
      <pages>2317–2322</pages>
      <url hash="f0c8db15">D17-1245</url>
      <doi>10.18653/v1/D17-1245</doi>
      <abstract>Social media collect and spread on the Web personal opinions, facts, fake news and all kind of information users may be interested in. Applying argument mining methods to such heterogeneous data sources is a challenging open research issue, in particular considering the peculiarities of the language used to write textual messages on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. In addition, new issues emerge when dealing with arguments posted on such platforms, such as the need to make a distinction between personal opinions and actual facts, and to detect the source disseminating information about such facts to allow for provenance verification. In this paper, we apply <a href="https://en.wikipedia.org/wiki/Supervised_classification">supervised classification</a> to identify arguments on <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a>, and we present two new tasks for <a href="https://en.wikipedia.org/wiki/Argument_mining">argument mining</a>, namely facts recognition and source identification. We study the feasibility of the approaches proposed to address these tasks on a set of <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a> related to the Grexit and Brexit news topics.</abstract>
      <bibkey>dusmanu-etal-2017-argument</bibkey>
    </paper>
    <paper id="246">
      <title>Distinguishing Japanese Non-standard Usages from Standard Ones<fixed-case>J</fixed-case>apanese Non-standard Usages from Standard Ones</title>
      <author><first>Tatsuya</first> <last>Aoki</last></author>
      <author><first>Ryohei</first> <last>Sasano</last></author>
      <author><first>Hiroya</first> <last>Takamura</last></author>
      <author><first>Manabu</first> <last>Okumura</last></author>
      <pages>2323–2328</pages>
      <url hash="d55519bd">D17-1246</url>
      <doi>10.18653/v1/D17-1246</doi>
      <abstract>We focus on non-standard usages of common words on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. In the context of <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>, words sometimes have other usages that are totally different from their original. In this study, we attempt to distinguish non-standard usages on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> from standard ones in an unsupervised manner. Our basic idea is that non-standardness can be measured by the inconsistency between the expected meaning of the target word and the given context. For this purpose, we use context embeddings derived from <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. Our experimental results show that the model leveraging the context embedding outperforms other methods and provide us with findings, for example, on how to construct context embeddings and which corpus to use.</abstract>
      <bibkey>aoki-etal-2017-distinguishing</bibkey>
    </paper>
    <paper id="247">
      <title>Connotation Frames of Power and Agency in Modern Films</title>
      <author><first>Maarten</first> <last>Sap</last></author>
      <author><first>Marcella Cindy</first> <last>Prasettio</last></author>
      <author><first>Ari</first> <last>Holtzman</last></author>
      <author><first>Hannah</first> <last>Rashkin</last></author>
      <author><first>Yejin</first> <last>Choi</last></author>
      <pages>2329–2334</pages>
      <url hash="18905c57">D17-1247</url>
      <doi>10.18653/v1/D17-1247</doi>
      <attachment type="attachment" hash="2e35c325">D17-1247.Attachment.zip</attachment>
      <abstract>The framing of an action influences how we perceive its actor. We introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of <a href="https://en.wikipedia.org/wiki/Power_(social_and_political)">power</a> and <a href="https://en.wikipedia.org/wiki/Agency_(sociology)">agency</a> are implicitly projected on actors through their actions. We use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known <a href="https://en.wikipedia.org/wiki/Bechdel_test">Bechdel test</a>. Our contributions include an extended lexicon of connotation frames along with a <a href="https://en.wikipedia.org/wiki/User_interface">web interface</a> that provides a comprehensive analysis through the lens of connotation frames.</abstract>
      <bibkey>sap-etal-2017-connotation</bibkey>
    </paper>
    <paper id="248">
      <title>Controlling Human Perception of Basic User Traits</title>
      <author><first>Daniel</first> <last>Preoţiuc-Pietro</last></author>
      <author><first>Sharath</first> <last>Chandra Guntuku</last></author>
      <author><first>Lyle</first> <last>Ungar</last></author>
      <pages>2335–2341</pages>
      <url hash="d56efd18">D17-1248</url>
      <doi>10.18653/v1/D17-1248</doi>
      <attachment type="poster" hash="e9c0fd23">D17-1248.Poster.pdf</attachment>
      <abstract>Much of our <a href="https://en.wikipedia.org/wiki/Online_communication">online communication</a> is text-mediated and, lately, more common with <a href="https://en.wikipedia.org/wiki/Intelligent_agent">automated agents</a>. Unlike interacting with humans, these <a href="https://en.wikipedia.org/wiki/Intelligent_agent">agents</a> currently do not tailor their language to the type of person they are communicating to. In this pilot study, we measure the extent to which human perception of basic user trait information   gender and age   is controllable through text. Using automatic models of gender and age prediction, we estimate which tweets posted by a user are more likely to mis-characterize his traits. We perform multiple controlled crowdsourcing experiments in which we show that we can reduce the human prediction accuracy of gender to almost random   an over 20 % drop in accuracy. Our experiments show that it is practically feasible for multiple applications such as text generation, <a href="https://en.wikipedia.org/wiki/Automatic_summarization">text summarization</a> or <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> to be tailored to specific traits and perceived as such.</abstract>
      <bibkey>preotiuc-pietro-etal-2017-controlling</bibkey>
    </paper>
    <paper id="250">
      <title>Assessing Objective Recommendation Quality through Political Forecasting</title>
      <author><first>H. Andrew</first> <last>Schwartz</last></author>
      <author><first>Masoud</first> <last>Rouhizadeh</last></author>
      <author><first>Michael</first> <last>Bishop</last></author>
      <author><first>Philip</first> <last>Tetlock</last></author>
      <author><first>Barbara</first> <last>Mellers</last></author>
      <author><first>Lyle</first> <last>Ungar</last></author>
      <pages>2348–2357</pages>
      <url hash="18a39d74">D17-1250</url>
      <doi>10.18653/v1/D17-1250</doi>
      <abstract>Recommendations are often rated for their subjective quality, but few researchers have studied comment quality in terms of objective utility. We explore recommendation quality assessment with respect to both subjective (i.e. users’ ratings) and objective (i.e., did it influence? did <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> improve decisions?) metrics in a massive online geopolitical forecasting system, ultimately comparing linguistic characteristics of each quality metric. Using a variety of <a href="https://en.wikipedia.org/wiki/Software_feature">features</a>, we predict all types of quality with better <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> than the simple yet strong baseline of comment length. Looking at the most predictive content illustrates rater biases ; for example, forecasters are subjectively biased in favor of comments mentioning business transactions or dealings as well as material things, even though such comments do not indeed prove any more useful objectively. Additionally, more complex <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence constructions</a>, as evidenced by subordinate conjunctions, are characteristic of comments leading to objective improvements in <a href="https://en.wikipedia.org/wiki/Forecasting">forecasting</a>.</abstract>
      <bibkey>schwartz-etal-2017-assessing</bibkey>
    </paper>
    <paper id="251">
      <title>Never Abandon Minorities : Exhaustive Extraction of Bursty Phrases on <a href="https://en.wikipedia.org/wiki/Microblogging">Microblogs</a> Using Set Cover Problem</title>
      <author><first>Masumi</first> <last>Shirakawa</last></author>
      <author><first>Takahiro</first> <last>Hara</last></author>
      <author><first>Takuya</first> <last>Maekawa</last></author>
      <pages>2358–2367</pages>
      <url hash="713ad4f9">D17-1251</url>
      <doi>10.18653/v1/D17-1251</doi>
      <abstract>We propose a language-independent data-driven method to exhaustively extract bursty phrases of arbitrary forms (e.g., phrases other than simple noun phrases) from <a href="https://en.wikipedia.org/wiki/Microblogging">microblogs</a>. The burst (i.e., the rapid increase of the occurrence) of a phrase causes the burst of overlapping N-grams including incomplete ones. In other words, bursty incomplete N-grams inevitably overlap bursty phrases. Thus, the proposed method performs the extraction of bursty phrases as the <a href="https://en.wikipedia.org/wiki/Set_cover_problem">set cover problem</a> in which all bursty N-grams are covered by a minimum set of bursty phrases. Experimental results using Japanese Twitter data showed that the proposed method outperformed word-based, noun phrase-based, and segmentation-based methods both in terms of accuracy and coverage.</abstract>
      <bibkey>shirakawa-etal-2017-never</bibkey>
    </paper>
    <paper id="252">
      <title>Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision</title>
      <author><first>Haoruo</first> <last>Peng</last></author>
      <author><first>Ming-Wei</first> <last>Chang</last></author>
      <author><first>Wen-tau</first> <last>Yih</last></author>
      <pages>2368–2378</pages>
      <url hash="cd651c13">D17-1252</url>
      <doi>10.18653/v1/D17-1252</doi>
      <abstract>Neural networks have achieved state-of-the-art performance on several structured-output prediction tasks, trained in a fully supervised fashion. However, annotated examples in structured domains are often costly to obtain, which thus limits the applications of <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>. In this work, we propose Maximum Margin Reward Networks, a neural network-based framework that aims to learn from both explicit (full structures) and implicit supervision signals (delayed feedback on the correctness of the predicted structure). On <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> and <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a>, our model outperforms previous systems on the benchmark datasets, CoNLL-2003 and WebQuestionsSP.</abstract>
      <video href="https://vimeo.com/238234174" />
      <bibkey>peng-etal-2017-maximum</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="253">
      <title>The Impact of Modeling Overall Argumentation with Tree Kernels</title>
      <author><first>Henning</first> <last>Wachsmuth</last></author>
      <author><first>Giovanni</first> <last>Da San Martino</last></author>
      <author><first>Dora</first> <last>Kiesel</last></author>
      <author><first>Benno</first> <last>Stein</last></author>
      <pages>2379–2389</pages>
      <url hash="f15d3466">D17-1253</url>
      <doi>10.18653/v1/D17-1253</doi>
      <abstract>Several approaches have been proposed to model either the explicit sequential structure of an argumentative text or its implicit hierarchical structure. So far, the adequacy of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> of overall argumentation remains unclear. This paper asks what type of <a href="https://en.wikipedia.org/wiki/Structure_(mathematical_logic)">structure</a> is actually important to tackle downstream tasks in computational argumentation. We analyze patterns in the overall argumentation of texts from three corpora. Then, we adapt the idea of positional tree kernels in order to capture sequential and hierarchical argumentative structure together for the first time. In systematic experiments for three text classification tasks, we find strong evidence for the impact of both types of <a href="https://en.wikipedia.org/wiki/Structure">structure</a>. Our results suggest that either of <a href="https://en.wikipedia.org/wiki/Cofactor_(biochemistry)">them</a> is necessary while their combination may be beneficial.</abstract>
      <video href="https://vimeo.com/238236158" />
      <bibkey>wachsmuth-etal-2017-impact</bibkey>
    </paper>
    <paper id="254">
      <title>Learning Generic Sentence Representations Using <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a></title>
      <author><first>Zhe</first> <last>Gan</last></author>
      <author><first>Yunchen</first> <last>Pu</last></author>
      <author><first>Ricardo</first> <last>Henao</last></author>
      <author><first>Chunyuan</first> <last>Li</last></author>
      <author><first>Xiaodong</first> <last>He</last></author>
      <author><first>Lawrence</first> <last>Carin</last></author>
      <pages>2390–2400</pages>
      <url hash="461f32d7">D17-1254</url>
      <doi>10.18653/v1/D17-1254</doi>
      <abstract>We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes. The model is learned by using a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural network</a> as an <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> to map an input sentence into a <a href="https://en.wikipedia.org/wiki/Continuous_or_discrete_variable">continuous vector</a>, and using a <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">long short-term memory recurrent neural network</a> as a decoder. Several <a href="https://en.wikipedia.org/wiki/Task_(computing)">tasks</a> are considered, including sentence reconstruction and future sentence prediction. Further, a hierarchical encoder-decoder model is proposed to encode a sentence to predict multiple future sentences. By training our <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on a large collection of <a href="https://en.wikipedia.org/wiki/Novel">novels</a>, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> over competing methods.</abstract>
      <video href="https://vimeo.com/238233944" />
      <bibkey>gan-etal-2017-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
    </paper>
    <paper id="255">
      <title>Repeat before Forgetting : Spaced Repetition for Efficient and Effective Training of <a href="https://en.wikipedia.org/wiki/Neural_network">Neural Networks</a></title>
      <author><first>Hadi</first> <last>Amiri</last></author>
      <author><first>Timothy</first> <last>Miller</last></author>
      <author><first>Guergana</first> <last>Savova</last></author>
      <pages>2401–2410</pages>
      <url hash="db50717e">D17-1255</url>
      <doi>10.18653/v1/D17-1255</doi>
      <abstract>We present a novel approach for training <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">artificial neural networks</a>. Our approach is inspired by broad evidence in psychology that shows human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (spaced repetition). We investigate the analogy between training neural models and findings in psychology about human memory model and develop an efficient and effective <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> to train neural models. The core part of our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> is a cognitively-motivated scheduler according to which training instances and their reviews are spaced over time. Our <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> uses only 34-50 % of data per epoch, is 2.9-4.8 times faster than standard <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training</a>, and outperforms competing state-of-the-art baselines. Our <a href="https://en.wikipedia.org/wiki/Code">code</a> is available at.<url>scholar.harvard.edu/hadi/RbF/</url>. </abstract>
      <video href="https://vimeo.com/238235456" />
      <bibkey>amiri-etal-2017-repeat</bibkey>
    </paper>
    <paper id="256">
      <title>Part-of-Speech Tagging for <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> with Adversarial Neural Networks<fixed-case>T</fixed-case>witter with Adversarial Neural Networks</title>
      <author><first>Tao</first> <last>Gui</last></author>
      <author><first>Qi</first> <last>Zhang</last></author>
      <author><first>Haoran</first> <last>Huang</last></author>
      <author><first>Minlong</first> <last>Peng</last></author>
      <author><first>Xuanjing</first> <last>Huang</last></author>
      <pages>2411–2420</pages>
      <url hash="82f7b344">D17-1256</url>
      <doi>10.18653/v1/D17-1256</doi>
      <abstract>In this work, we study the problem of <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech tagging</a> for <a href="https://en.wikipedia.org/wiki/Twitter">Tweets</a>. In contrast to newswire articles, <a href="https://en.wikipedia.org/wiki/Twitter">Tweets</a> are usually informal and contain numerous out-of-vocabulary words. Moreover, there is a lack of large scale labeled datasets for this <a href="https://en.wikipedia.org/wiki/Domain_(mathematical_analysis)">domain</a>. To tackle these challenges, we propose a novel <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> to make use of out-of-domain labeled data, unlabeled in-domain data, and labeled in-domain data. Inspired by adversarial neural networks, the proposed method tries to learn common features through adversarial discriminator. In addition, we hypothesize that domain-specific features of target domain should be preserved in some degree. Hence, the proposed <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> adopts a sequence-to-sequence autoencoder to perform this <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a>. Experimental results on three different datasets show that our <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> achieves better performance than state-of-the-art methods.</abstract>
      <video href="https://vimeo.com/238235246" />
      <bibkey>gui-etal-2017-part</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tweebank">Tweebank</pwcdataset>
    </paper>
    <paper id="257">
      <title>Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings</title>
      <author><first>Bofang</first> <last>Li</last></author>
      <author><first>Tao</first> <last>Liu</last></author>
      <author><first>Zhe</first> <last>Zhao</last></author>
      <author><first>Buzhou</first> <last>Tang</last></author>
      <author><first>Aleksandr</first> <last>Drozd</last></author>
      <author><first>Anna</first> <last>Rogers</last></author>
      <author><first>Xiaoyong</first> <last>Du</last></author>
      <pages>2421–2431</pages>
      <url hash="fb90b1bf">D17-1257</url>
      <doi>10.18653/v1/D17-1257</doi>
      <abstract>The number of word embedding models is growing every year. Most of them are based on the co-occurrence information of words and their contexts. However, it is still an open question what is the best definition of <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a>. We provide a systematical investigation of 4 different <a href="https://en.wikipedia.org/wiki/Context_(language_use)">syntactic context types</a> and <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context representations</a> for learning <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. Comprehensive experiments are conducted to evaluate their effectiveness on 6 extrinsic and intrinsic tasks. We hope that this paper, along with the published code, would be helpful for choosing the best context type and representation for a given task.</abstract>
      <bibkey>li-etal-2017-investigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="258">
      <title>Does syntax help discourse segmentation? Not so much</title>
      <author><first>Chloé</first> <last>Braud</last></author>
      <author><first>Ophélie</first> <last>Lacroix</last></author>
      <author><first>Anders</first> <last>Søgaard</last></author>
      <pages>2432–2442</pages>
      <url hash="19465f59">D17-1258</url>
      <doi>10.18653/v1/D17-1258</doi>
      <abstract>Discourse segmentation is the first step in building <a href="https://en.wikipedia.org/wiki/Discourse_analysis">discourse parsers</a>. Most work on discourse segmentation does not scale to real-world discourse parsing across languages, for two reasons : (i) models rely on constituent trees, and (ii) experiments have relied on gold standard identification of sentence and token boundaries. We therefore investigate to what extent <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">constituents</a> can be replaced with universal dependencies, or left out completely, as well as how state-of-the-art <a href="https://en.wikipedia.org/wiki/Segment_(linguistics)">segmenters</a> fare in the absence of <a href="https://en.wikipedia.org/wiki/Sentence_(linguistics)">sentence boundaries</a>. Our results show that dependency information is less useful than expected, but we provide a fully scalable, robust model that only relies on part-of-speech information, and show that it performs well across languages in the absence of any gold-standard annotation.</abstract>
      <video href="https://vimeo.com/238232586" />
      <bibkey>braud-etal-2017-syntax</bibkey>
      <pwccode url="https://bitbucket.org/chloebt/discourse" additional="false">chloebt/discourse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="259">
      <title>Deal or No Deal? End-to-End Learning of Negotiation Dialogues</title>
      <author><first>Mike</first> <last>Lewis</last></author>
      <author><first>Denis</first> <last>Yarats</last></author>
      <author><first>Yann</first> <last>Dauphin</last></author>
      <author><first>Devi</first> <last>Parikh</last></author>
      <author><first>Dhruv</first> <last>Batra</last></author>
      <pages>2443–2453</pages>
      <url hash="7a936852">D17-1259</url>
      <doi>10.18653/v1/D17-1259</doi>
      <abstract>Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI</a>. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who can not observe each other’s reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for <a href="https://en.wikipedia.org/wiki/Negotiation">negotiation</a>, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available.</abstract>
      <video href="https://vimeo.com/238232142" />
      <bibkey>lewis-etal-2017-deal</bibkey>
    </paper>
    <paper id="260">
      <title>Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning<fixed-case>DQN</fixed-case> for Safe and Efficient On-line Dialogue Policy Learning</title>
      <author><first>Lu</first> <last>Chen</last></author>
      <author><first>Xiang</first> <last>Zhou</last></author>
      <author><first>Cheng</first> <last>Chang</last></author>
      <author><first>Runzhe</first> <last>Yang</last></author>
      <author><first>Kai</first> <last>Yu</last></author>
      <pages>2454–2464</pages>
      <url hash="026855b9">D17-1260</url>
      <doi>10.18653/v1/D17-1260</doi>
      <attachment type="attachment" hash="a80d3a85">D17-1260.Attachment.zip</attachment>
      <abstract>Hand-crafted rules and reinforcement learning (RL) are two popular choices to obtain dialogue policy. The rule-based policy is often reliable within predefined scope but not self-adaptable, whereas RL is evolvable with data but often suffers from a bad initial performance. We employ a companion learning framework to integrate the two approaches for on-line dialogue policy learning, in which a pre-defined rule-based policy acts as a teacher and guides a data-driven RL system by giving example actions as well as additional rewards. A novel agent-aware dropout Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teacher’s experiences. AAD-DQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by <a href="https://en.wikipedia.org/wiki/Dropping_out">dropout</a> to control the timing of consultation and learning. Simulation experiments showed that the proposed approach can significantly improve both safetyand efficiency of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus.<i>companion learning</i> framework to integrate the two approaches for <i>on-line</i> dialogue policy learning, in which a pre-defined rule-based policy acts as a “teacher” and guides a data-driven RL system by giving example actions as well as additional rewards. A novel <i>agent-aware dropout</i> Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teacher’s experiences. AAD-DQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by dropout to control the timing of consultation and learning. Simulation experiments showed that the proposed approach can significantly improve both <i>safety</i>

and <i>efficiency</i> of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus. </abstract>
      <video href="https://vimeo.com/238231562" />
      <bibkey>chen-etal-2017-agent</bibkey>
    </paper>
    <paper id="261">
      <title>Towards Debate Automation : a Recurrent Model for Predicting Debate Winners</title>
      <author><first>Peter</first> <last>Potash</last></author>
      <author><first>Anna</first> <last>Rumshisky</last></author>
      <pages>2465–2475</pages>
      <url hash="88f3fa28">D17-1261</url>
      <doi>10.18653/v1/D17-1261</doi>
      <abstract>In this paper we introduce a practical first step towards the creation of an automated debate agent : a state-of-the-art recurrent predictive model for predicting debate winners. By having an accurate <a href="https://en.wikipedia.org/wiki/Predictive_modelling">predictive model</a>, we are able to objectively rate the quality of a statement made at a specific turn in a debate. The <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is based on a <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network architecture</a> with <a href="https://en.wikipedia.org/wiki/Attention">attention</a>, which allows the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to effectively account for the entire debate when making its prediction. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves state-of-the-art <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on a dataset of debate transcripts annotated with <a href="https://en.wikipedia.org/wiki/Audience_measurement">audience favorability</a> of the debate teams. Finally, we discuss how future work can leverage our proposed model for the creation of an automated debate agent. We accomplish this by determining the model input that will maximize audience favorability toward a given side of a debate at an arbitrary turn.</abstract>
      <video href="https://vimeo.com/238236302" />
      <bibkey>potash-rumshisky-2017-towards</bibkey>
    </paper>
    <paper id="262">
      <title>Further Investigation into Reference Bias in Monolingual Evaluation of <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a></title>
      <author><first>Qingsong</first> <last>Ma</last></author>
      <author><first>Yvette</first> <last>Graham</last></author>
      <author><first>Timothy</first> <last>Baldwin</last></author>
      <author><first>Qun</first> <last>Liu</last></author>
      <pages>2476–2485</pages>
      <url hash="f6cdaaeb">D17-1262</url>
      <doi>10.18653/v1/D17-1262</doi>
      <abstract>Monolingual evaluation of Machine Translation (MT) aims to simplify human assessment by requiring assessors to compare the meaning of the MT output with a reference translation, opening up the task to a much larger pool of genuinely qualified evaluators. Monolingual evaluation runs the risk, however, of bias in favour of MT systems that happen to produce translations superficially similar to the reference and, consistent with this intuition, previous investigations have concluded monolingual assessment to be strongly biased in this respect. On re-examination of past analyses, we identify a series of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results for show no significant evidence of reference bias in monolingual evaluation of MT.</abstract>
      <bibkey>ma-etal-2017-investigation</bibkey>
      <pwccode url="https://github.com/qingsongma/percentage-refBias" additional="false">qingsongma/percentage-refBias</pwccode>
    </paper>
    <paper id="263">
      <title>A Challenge Set Approach to Evaluating <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a></title>
      <author><first>Pierre</first> <last>Isabelle</last></author>
      <author><first>Colin</first> <last>Cherry</last></author>
      <author><first>George</first> <last>Foster</last></author>
      <pages>2486–2496</pages>
      <url hash="9fdb957a">D17-1263</url>
      <doi>10.18653/v1/D17-1263</doi>
      <attachment type="attachment" hash="3e5f4f77">D17-1263.Attachment.zip</attachment>
      <abstract>Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and <a href="https://en.wikipedia.org/wiki/Error_analysis_(linguistics)">error analysis</a>. A challenge set consists of a small set of sentences, each hand-designed to probe a system’s capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting <a href="https://en.wikipedia.org/wiki/Analysis">analysis</a> provides not only a more fine-grained picture of the strengths of <a href="https://en.wikipedia.org/wiki/Nervous_system">neural systems</a>, but also insight into which linguistic phenomena remain out of reach.</abstract>
      <bibkey>isabelle-etal-2017-challenge</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="264">
      <title>Knowledge Distillation for Bilingual Dictionary Induction</title>
      <author><first>Ndapandula</first> <last>Nakashole</last></author>
      <author><first>Raphael</first> <last>Flauger</last></author>
      <pages>2497–2506</pages>
      <url hash="f924fbb3">D17-1264</url>
      <doi>10.18653/v1/D17-1264</doi>
      <abstract>Leveraging zero-shot learning to learn <a href="https://en.wikipedia.org/wiki/Function_(mathematics)">mapping functions</a> between <a href="https://en.wikipedia.org/wiki/Vector_space">vector spaces</a> of different languages is a promising approach to bilingual dictionary induction. However, <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> using this <a href="https://en.wikipedia.org/wiki/Methodology">approach</a> have not yet achieved high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. In this paper, we propose a bridging approach, where our main contribution is a knowledge distillation training objective. As teachers, rich resource translation paths are exploited in this <a href="https://en.wikipedia.org/wiki/Role">role</a>. And as learners, translation paths involving low resource languages learn from the teachers. Our training objective allows seamless addition of teacher translation paths for any given low resource pair. Since our approach relies on the quality of monolingual word embeddings, we also propose to enhance <a href="https://en.wikipedia.org/wiki/Vector_graphics">vector representations</a> of both the source and target language with <a href="https://en.wikipedia.org/wiki/Linguistic_description">linguistic information</a>. Our experiments on various languages show large performance gains from our distillation training objective, obtaining as high as 17 % accuracy improvements.</abstract>
      <bibkey>nakashole-flauger-2017-knowledge</bibkey>
    </paper>
    <paper id="265">
      <title>Machine Translation, it’s a question of style, innit? The case of English tag questions<fixed-case>E</fixed-case>nglish tag questions</title>
      <author><first>Rachel</first> <last>Bawden</last></author>
      <pages>2507–2512</pages>
      <url hash="cff5bb13">D17-1265</url>
      <doi>10.18653/v1/D17-1265</doi>
      <abstract>In this paper, we address the problem of generating English tag questions (TQs) (e.g. it is, is n’t it?) in Machine Translation (MT). We propose a post-edition solution, formulating the <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> as a multi-class classification task. We present (i) the automatic annotation of English TQs in a parallel corpus of subtitles and (ii) an approach using a series of <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> to predict TQ forms, which we use to post-edit state-of-the-art MT outputs. Our method provides significant improvements in English TQ translation when translating from <a href="https://en.wikipedia.org/wiki/Czech_language">Czech</a>, <a href="https://en.wikipedia.org/wiki/French_language">French</a> and <a href="https://en.wikipedia.org/wiki/German_language">German</a>, in turn improving the fluidity, naturalness, grammatical correctness and pragmatic coherence of MT output.</abstract>
      <bibkey>bawden-2017-machine</bibkey>
    </paper>
    <paper id="266">
      <title>Deciphering Related Languages</title>
      <author><first>Nima</first> <last>Pourdamghani</last></author>
      <author><first>Kevin</first> <last>Knight</last></author>
      <pages>2513–2518</pages>
      <url hash="274d2aed">D17-1266</url>
      <doi>10.18653/v1/D17-1266</doi>
      <abstract>We present a <a href="https://en.wikipedia.org/wiki/Methodology">method</a> for <a href="https://en.wikipedia.org/wiki/Translation">translating texts</a> between close language pairs. The <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> does not require parallel data, and it does not require the languages to be written in the same script. We show results for six language pairs : Afrikaans / Dutch, Bosnian / Serbian, Danish / Swedish, Macedonian / Bulgarian, Malaysian / Indonesian, and Polish / Belorussian. We report BLEU scores showing our method to outperform others that do not use parallel data.</abstract>
      <bibkey>pourdamghani-knight-2017-deciphering</bibkey>
    </paper>
    <paper id="267">
      <title>Identifying Cognate Sets Across Dictionaries of Related Languages</title>
      <author><first>Adam</first> <last>St Arnaud</last></author>
      <author><first>David</first> <last>Beck</last></author>
      <author><first>Grzegorz</first> <last>Kondrak</last></author>
      <pages>2519–2528</pages>
      <url hash="8c725132">D17-1267</url>
      <doi>10.18653/v1/D17-1267</doi>
      <abstract>We present a <a href="https://en.wikipedia.org/wiki/System">system</a> for identifying <a href="https://en.wikipedia.org/wiki/Cognate">cognate sets</a> across dictionaries of related languages. The likelihood of a <a href="https://en.wikipedia.org/wiki/Cognate">cognate relationship</a> is calculated on the basis of a rich set of <a href="https://en.wikipedia.org/wiki/Distinctive_feature">features</a> that capture both <a href="https://en.wikipedia.org/wiki/Semantic_similarity">phonetic and semantic similarity</a>, as well as the presence of regular sound correspondences. The similarity scores are used to cluster words from different languages that may originate from a common proto-word. When tested on the <a href="https://en.wikipedia.org/wiki/Algonquian_languages">Algonquian language family</a>, our system detects 63 % of cognate sets while maintaining cluster purity of 70 %.</abstract>
      <bibkey>st-arnaud-etal-2017-identifying</bibkey>
      <pwccode url="https://github.com/ajstarna/SemaPhoR" additional="false">ajstarna/SemaPhoR</pwccode>
    </paper>
    <paper id="268">
      <title>Learning Language Representations for Typology Prediction</title>
      <author><first>Chaitanya</first> <last>Malaviya</last></author>
      <author><first>Graham</first> <last>Neubig</last></author>
      <author><first>Patrick</first> <last>Littell</last></author>
      <pages>2529–2535</pages>
      <url hash="9797c967">D17-1268</url>
      <doi>10.18653/v1/D17-1268</doi>
      <abstract>One central mystery of neural NLP is what neural models know about their subject matter. When a neural machine translation system learns to translate from one language to another, does it learn the <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> or semantics of the languages? Can this knowledge be extracted from the <a href="https://en.wikipedia.org/wiki/System">system</a> to fill holes in human scientific knowledge? Existing typological databases contain relatively full feature specifications for only a few hundred languages. Exploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one NMT system from 1017 languages into <a href="https://en.wikipedia.org/wiki/English_language">English</a>, and use this to predict information missing from typological databases. Experiments show that the proposed method is able to infer not only syntactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages geographic and phylogenetic neighbors.</abstract>
      <bibkey>malaviya-etal-2017-learning</bibkey>
      <pwccode url="https://github.com/chaitanyamalaviya/lang-reps" additional="true">chaitanyamalaviya/lang-reps</pwccode>
    </paper>
    <paper id="269">
      <title>Cheap Translation for Cross-Lingual Named Entity Recognition</title>
      <author><first>Stephen</first> <last>Mayhew</last></author>
      <author><first>Chen-Tse</first> <last>Tsai</last></author>
      <author><first>Dan</first> <last>Roth</last></author>
      <pages>2536–2545</pages>
      <url hash="80e3ff35">D17-1269</url>
      <doi>10.18653/v1/D17-1269</doi>
      <abstract>Recent work in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> has attempted to deal with low-resource languages but still assumed a resource level that is not present for most <a href="https://en.wikipedia.org/wiki/Language">languages</a>, e.g., the availability of <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> in the target language. We propose a simple method for cross-lingual named entity recognition (NER) that works well in settings with very minimal resources. Our approach makes use of a <a href="https://en.wikipedia.org/wiki/Lexicon">lexicon</a> to translate annotated data available in one or several high resource language(s) into the target language, and learns a standard monolingual NER model there. Further, when <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> is available in the target language, our method can enhance <a href="https://en.wikipedia.org/wiki/Wikipedia">Wikipedia based methods</a> to yield state-of-the-art NER results ; we evaluate on 7 diverse languages, improving the <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> by an average of 5.5 % F1 points. With the minimal resources required, this is an extremely portable cross-lingual NER approach, as illustrated using a truly low-resource language, <a href="https://en.wikipedia.org/wiki/Uyghur_language">Uyghur</a>.<i>very</i> minimal resources. Our approach makes use of a lexicon to “translate” annotated data available in one or several high resource language(s) into the target language, and learns a standard monolingual NER model there. Further, when Wikipedia is available in the target language, our method can enhance Wikipedia based methods to yield state-of-the-art NER results; we evaluate on 7 diverse languages, improving the state-of-the-art by an average of 5.5% F1 points. With the minimal resources required, this is an extremely portable cross-lingual NER approach, as illustrated using a truly low-resource language, Uyghur. </abstract>
      <bibkey>mayhew-etal-2017-cheap</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/panlex">Panlex</pwcdataset>
    </paper>
    <paper id="270">
      <title>Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation</title>
      <author><first>Ivan</first> <last>Vulić</last></author>
      <author><first>Nikola</first> <last>Mrkšić</last></author>
      <author><first>Anna</first> <last>Korhonen</last></author>
      <pages>2546–2558</pages>
      <url hash="21eceef6">D17-1270</url>
      <doi>10.18653/v1/D17-1270</doi>
      <attachment type="attachment" hash="6d312bbb">D17-1270.Attachment.zip</attachment>
      <abstract>Existing approaches to automatic VerbNet-style verb classification are heavily dependent on <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a> and therefore limited to languages with mature <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP pipelines</a>. In this work, we propose a novel cross-lingual transfer method for inducing VerbNets for multiple languages. To the best of our knowledge, this is the first study which demonstrates how the architectures for learning <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> can be applied to this challenging syntactic-semantic task. Our method uses cross-lingual translation pairs to tie each of the six target languages into a bilingual vector space with English, jointly specialising the representations to encode the relational information from English VerbNet. A standard clustering algorithm is then run on top of the VerbNet-specialised representations, using <a href="https://en.wikipedia.org/wiki/Dimension_(vector_space)">vector dimensions</a> as <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> for learning verb classes. Our results show that the proposed cross-lingual transfer approach sets new state-of-the-art verb classification performance across all six target languages explored in this work.</abstract>
      <bibkey>vulic-etal-2017-cross</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/panlex">Panlex</pwcdataset>
    </paper>
    <paper id="271">
      <title>Classification of telicity using cross-linguistic annotation projection</title>
      <author><first>Annemarie</first> <last>Friedrich</last></author>
      <author><first>Damyana</first> <last>Gateva</last></author>
      <pages>2559–2565</pages>
      <url hash="b34bf948">D17-1271</url>
      <doi>10.18653/v1/D17-1271</doi>
      <abstract>This paper addresses the automatic recognition of telicity, an aspectual notion. A telic event includes a natural endpoint (she walked home), while an atelic event does not (she walked around). Recognizing this difference is a prerequisite for temporal natural language understanding. In <a href="https://en.wikipedia.org/wiki/English_language">English</a>, this classification task is difficult, as <a href="https://en.wikipedia.org/wiki/Telicity">telicity</a> is a covert linguistic category. In contrast, in <a href="https://en.wikipedia.org/wiki/Slavic_languages">Slavic languages</a>, <a href="https://en.wikipedia.org/wiki/Grammatical_aspect">aspect</a> is part of a verb’s meaning and even available in <a href="https://en.wikipedia.org/wiki/Machine-readable_dictionary">machine-readable dictionaries</a>. Our contributions are as follows. We successfully leverage additional silver standard training data in the form of projected annotations from parallel English-Czech data as well as context information, improving automatic telicity classification for English significantly compared to previous work. We also create a new data set of <a href="https://en.wikipedia.org/wiki/English_literature">English texts</a> manually annotated with <a href="https://en.wikipedia.org/wiki/Telicity">telicity</a>.</abstract>
      <bibkey>friedrich-gateva-2017-classification</bibkey>
      <pwccode url="https://github.com/annefried/telicity" additional="false">annefried/telicity</pwccode>
    </paper>
    <paper id="274">
      <title>Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures</title>
      <author><first>Lifu</first> <last>Huang</last></author>
      <author><first>Avirup</first> <last>Sil</last></author>
      <author><first>Heng</first> <last>Ji</last></author>
      <author><first>Radu</first> <last>Florian</last></author>
      <pages>2588–2597</pages>
      <url hash="8dac77f1">D17-1274</url>
      <doi>10.18653/v1/D17-1274</doi>
      <abstract>Slot Filling (SF) aims to extract the values of certain types of attributes (or slots, such as person : cities_of_residence) for a given entity from a large collection of source documents. In this paper we propose an effective DNN architecture for SF with the following new strategies : (1). Take a regularized dependency graph instead of a raw sentence as input to DNN, to compress the wide contexts between query and candidate filler ; (2). Incorporate two attention mechanisms : local attention learned from query and candidate filler, and global attention learned from external knowledge bases, to guide the model to better select indicative contexts to determine slot type. Experiments show that this framework outperforms <a href="https://en.wikipedia.org/wiki/State_of_the_art">state-of-the-art</a> on both <a href="https://en.wikipedia.org/wiki/Relation_extraction">relation extraction</a> (16 % absolute F-score gain) and slot filling validation for each individual <a href="https://en.wikipedia.org/wiki/System">system</a> (up to 8.5 % absolute F-score gain).</abstract>
      <bibkey>huang-etal-2017-improving</bibkey>
    </paper>
    <paper id="275">
      <title>Identifying Products in Online Cybercrime Marketplaces : A Dataset for Fine-grained Domain Adaptation</title>
      <author><first>Greg</first> <last>Durrett</last></author>
      <author><first>Jonathan K.</first> <last>Kummerfeld</last></author>
      <author><first>Taylor</first> <last>Berg-Kirkpatrick</last></author>
      <author><first>Rebecca</first> <last>Portnoff</last></author>
      <author><first>Sadia</first> <last>Afroz</last></author>
      <author><first>Damon</first> <last>McCoy</last></author>
      <author><first>Kirill</first> <last>Levchenko</last></author>
      <author><first>Vern</first> <last>Paxson</last></author>
      <pages>2598–2607</pages>
      <url hash="e7086b8b">D17-1275</url>
      <doi>10.18653/v1/D17-1275</doi>
      <attachment type="attachment" hash="d325fe54">D17-1275.Attachment.zip</attachment>
      <abstract>One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> and annotate <a href="https://en.wikipedia.org/wiki/Data">data</a> from four different forums. Each of these <a href="https://en.wikipedia.org/wiki/Internet_forum">forums</a> constitutes its own fine-grained domain in that the <a href="https://en.wikipedia.org/wiki/Internet_forum">forums</a> cover different market sectors with different properties, even though all forums are in the broad domain of <a href="https://en.wikipedia.org/wiki/Cybercrime">cybercrime</a>. We characterize these domain differences in the context of a <a href="https://en.wikipedia.org/wiki/Machine_learning">learning-based system</a> : <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised models</a> see decreased accuracy when applied to new forums, and standard techniques for <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised learning</a> and <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> have limited effectiveness on this data, which suggests the need to improve these techniques. We release a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> of 1,938 annotated posts from across the four forums.</abstract>
      <bibkey>durrett-etal-2017-identifying</bibkey>
      <pwccode url="https://github.com/ccied/ugforum-analysis" additional="false">ccied/ugforum-analysis</pwccode>
    </paper>
    <paper id="276">
      <title>Labeling Gaps Between Words : Recognizing Overlapping Mentions with Mention Separators</title>
      <author><first>Aldrian Obaja</first> <last>Muis</last></author>
      <author><first>Wei</first> <last>Lu</last></author>
      <pages>2608–2618</pages>
      <url hash="ddda7796">D17-1276</url>
      <doi>10.18653/v1/D17-1276</doi>
      <attachment type="attachment" hash="d9783728">D17-1276.Attachment.pdf</attachment>
      <abstract>In this paper, we propose a new <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> that is capable of recognizing overlapping mentions. We introduce a novel notion of mention separators that can be effectively used to capture how mentions overlap with one another. On top of a novel multigraph representation that we introduce, we show that efficient and exact <a href="https://en.wikipedia.org/wiki/Inference">inference</a> can still be performed. We present some theoretical analysis on the differences between our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and a recently proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for recognizing overlapping mentions, and discuss the possible implications of the differences. Through extensive empirical analysis on standard datasets, we demonstrate the effectiveness of our approach.</abstract>
      <bibkey>muis-lu-2017-labeling</bibkey>
    </paper>
    <paper id="277">
      <title>Deep Joint Entity Disambiguation with Local Neural Attention</title>
      <author><first>Octavian-Eugen</first> <last>Ganea</last></author>
      <author><first>Thomas</first> <last>Hofmann</last></author>
      <pages>2619–2629</pages>
      <url hash="b6854eba">D17-1277</url>
      <doi>10.18653/v1/D17-1277</doi>
      <attachment type="attachment" hash="bbb0a31c">D17-1277.Attachment.pdf</attachment>
      <abstract>We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> with more traditional approaches such as <a href="https://en.wikipedia.org/wiki/Graphical_model">graphical models</a> and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or state-of-the-art <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> at moderate computational costs.</abstract>
      <bibkey>ganea-hofmann-2017-deep</bibkey>
      <pwccode url="https://github.com/dalab/deep-ed" additional="true">dalab/deep-ed</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2004">ACE 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aquaint">AQUAINT</pwcdataset>
    </paper>
    <paper id="278">
      <title>MinIE : Minimizing Facts in Open Information Extraction<fixed-case>M</fixed-case>in<fixed-case>IE</fixed-case>: Minimizing Facts in Open Information Extraction</title>
      <author><first>Kiril</first> <last>Gashteovski</last></author>
      <author><first>Rainer</first> <last>Gemulla</last></author>
      <author><first>Luciano</first> <last>del Corro</last></author>
      <pages>2630–2640</pages>
      <url hash="d9abd69c">D17-1278</url>
      <doi>10.18653/v1/D17-1278</doi>
      <abstract>The goal of Open Information Extraction (OIE) is to extract surface relations and their arguments from natural-language text in an unsupervised, domain-independent manner. In this paper, we propose MinIE, an OIE system that aims to provide useful, compact extractions with high <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a> and <a href="https://en.wikipedia.org/wiki/Precision_and_recall">recall</a>. MinIE approaches these goals by (1) representing information about polarity, <a href="https://en.wikipedia.org/wiki/Linguistic_modality">modality</a>, <a href="https://en.wikipedia.org/wiki/Attribution_(psychology)">attribution</a>, and quantities with semantic annotations instead of in the actual extraction, and (2) identifying and removing parts that are considered overly specific. We conducted an experimental study with several real-world datasets and found that MinIE achieves competitive or higher <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">precision</a> and <a href="https://en.wikipedia.org/wiki/Precision_and_recall">recall</a> than most prior systems, while at the same time producing shorter, semantically enriched extractions.</abstract>
      <bibkey>gashteovski-etal-2017-minie</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/new-york-times-annotated-corpus">New York Times Annotated Corpus</pwcdataset>
    </paper>
    <paper id="279">
      <title>Scientific Information Extraction with Semi-supervised Neural Tagging</title>
      <author><first>Yi</first> <last>Luan</last></author>
      <author><first>Mari</first> <last>Ostendorf</last></author>
      <author><first>Hannaneh</first> <last>Hajishirzi</last></author>
      <pages>2641–2651</pages>
      <url hash="7e281770">D17-1279</url>
      <doi>10.18653/v1/D17-1279</doi>
      <abstract>This paper addresses the problem of extracting keyphrases from <a href="https://en.wikipedia.org/wiki/Scientific_literature">scientific articles</a> and categorizing them as corresponding to a task, process, or material. We cast the problem as sequence tagging and introduce <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised methods</a> to a neural tagging model, which builds on recent advances in <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art <a href="https://en.wikipedia.org/wiki/Information_extraction">information extraction</a> performance on the 2017 SemEval Task 10 ScienceIE task.</abstract>
      <bibkey>luan-etal-2017-scientific</bibkey>
    </paper>
    <paper id="280">
      <title>NITE : A Neural Inductive Teaching Framework for Domain Specific NER<fixed-case>NITE</fixed-case>: A Neural Inductive Teaching Framework for Domain Specific <fixed-case>NER</fixed-case></title>
      <author><first>Siliang</first> <last>Tang</last></author>
      <author><first>Ning</first> <last>Zhang</last></author>
      <author><first>Jinjiang</first> <last>Zhang</last></author>
      <author><first>Fei</first> <last>Wu</last></author>
      <author><first>Yueting</first> <last>Zhuang</last></author>
      <pages>2652–2657</pages>
      <url hash="ed8ff9a9">D17-1280</url>
      <doi>10.18653/v1/D17-1280</doi>
      <abstract>In domain-specific NER, due to insufficient labeled training data, <a href="https://en.wikipedia.org/wiki/Deep_learning">deep models</a> usually fail to behave normally. In this paper, we proposed a novel Neural Inductive TEaching framework (NITE) to transfer knowledge from existing domain-specific NER models into an arbitrary <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural network</a> in a teacher-student training manner. NITE is a general framework that builds upon <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> and <a href="https://en.wikipedia.org/wiki/Multiple_instance_learning">multiple instance learning</a>, which collaboratively not only transfers knowledge to a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep student network</a> but also reduces the noise from teachers. NITE can help <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning methods</a> to effectively utilize existing <a href="https://en.wikipedia.org/wiki/Resource_(computer_science)">resources</a> (i.e., <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>, labeled and unlabeled data) in a small domain. The experiment resulted on Disease NER proved that without using any labeled data, NITE can significantly boost the performance of a CNN-bidirectional LSTM-CRF NER neural network nearly over 30 % in terms of F1-score.</abstract>
      <bibkey>tang-etal-2017-nite</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-corpus">NCBI Disease Corpus</pwcdataset>
    </paper>
    <paper id="281">
      <title>Speeding up Reinforcement Learning-based Information Extraction Training using Asynchronous Methods</title>
      <author><first>Aditya</first> <last>Sharma</last></author>
      <author><first>Zarana</first> <last>Parekh</last></author>
      <author><first>Partha</first> <last>Talukdar</last></author>
      <pages>2658–2663</pages>
      <url hash="b7217e06">D17-1281</url>
      <doi>10.18653/v1/D17-1281</doi>
      <attachment type="attachment" hash="294a9b49">D17-1281.Attachment.zip</attachment>
      <abstract>RLIE-DQN is a recently proposed Reinforcement Learning-based Information Extraction (IE) technique which is able to incorporate external evidence during the extraction process. RLIE-DQN trains a single agent sequentially, training on one instance at a time. This results in significant training slowdown which is undesirable. We leverage recent advances in parallel RL training using <a href="https://en.wikipedia.org/wiki/Asynchronous_I/O">asynchronous methods</a> and propose RLIE-A3C. RLIE-A3C trains multiple agents in parallel and is able to achieve upto 6x training speedup over RLIE-DQN, while suffering no loss in average accuracy.</abstract>
      <bibkey>sharma-etal-2017-speeding</bibkey>
      <pwccode url="https://github.com/adi-sharma/RLIE_A3C" additional="false">adi-sharma/RLIE_A3C</pwccode>
    </paper>
    <paper id="282">
      <title>Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks</title>
      <author><first>Peng-Hsuan</first> <last>Li</last></author>
      <author><first>Ruo-Ping</first> <last>Dong</last></author>
      <author><first>Yu-Siang</first> <last>Wang</last></author>
      <author><first>Ju-Chieh</first> <last>Chou</last></author>
      <author><first>Wei-Yun</first> <last>Ma</last></author>
      <pages>2664–2669</pages>
      <url hash="cdaab090">D17-1282</url>
      <doi>10.18653/v1/D17-1282</doi>
      <attachment type="attachment" hash="adf688c8">D17-1282.Attachment.zip</attachment>
      <abstract>In this paper, we utilize the linguistic structures of texts to improve <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a> by BRNN-CNN, a special bidirectional recursive network attached with a convolutional network. Motivated by the observation that <a href="https://en.wikipedia.org/wiki/Named_entity">named entities</a> are highly related to <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">linguistic constituents</a>, we propose a constituent-based BRNN-CNN for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>. In contrast to classical sequential labeling methods, the system first identifies which text chunks are possible named entities by whether they are <a href="https://en.wikipedia.org/wiki/Constituent_(linguistics)">linguistic constituents</a>. Then it classifies these chunks with a constituency tree structure by recursively propagating syntactic and semantic information to each constituent node. This <a href="https://en.wikipedia.org/wiki/Method_(computer_programming)">method</a> surpasses current state-of-the-art on OntoNotes 5.0 with automatically generated parses.</abstract>
      <bibkey>li-etal-2017-leveraging</bibkey>
      <pwccode url="https://github.com/jacobvsdanniel/tf_rnn" additional="false">jacobvsdanniel/tf_rnn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="283">
      <title>Fast and Accurate Entity Recognition with Iterated Dilated Convolutions</title>
      <author><first>Emma</first> <last>Strubell</last></author>
      <author><first>Patrick</first> <last>Verga</last></author>
      <author><first>David</first> <last>Belanger</last></author>
      <author><first>Andrew</first> <last>McCallum</last></author>
      <pages>2670–2680</pages>
      <url hash="d3011067">D17-1283</url>
      <doi>10.18653/v1/D17-1283</doi>
      <abstract>Today when many practitioners run basic NLP on the entire <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a> and large-volume traffic, faster methods are paramount to saving time and energy costs. Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining per-token vector representations serving as input to labeling tasks such as NER (often followed by prediction in a linear-chain CRF). Though expressive and accurate, these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> fail to fully exploit <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU parallelism</a>, limiting their computational efficiency. This paper proposes a faster alternative to Bi-LSTMs for NER : Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of <a href="https://en.wikipedia.org/wiki/Parallel_computing">parallelism</a>, ID-CNNs permit fixed-depth convolutions to run in parallel across entire documents. We describe a distinct combination of network structure, parameter sharing and training procedures that enable dramatic 14-20x test-time speedups while retaining <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire document are more accurate than Bi-LSTM-CRFs while attaining 8x faster test time speeds.</abstract>
      <bibkey>strubell-etal-2017-fast</bibkey>
      <pwccode url="https://github.com/iesl/dilated-cnn-ner" additional="true">iesl/dilated-cnn-ner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="284">
      <title>Entity Linking via Joint Encoding of Types, Descriptions, and Context</title>
      <author><first>Nitish</first> <last>Gupta</last></author>
      <author><first>Sameer</first> <last>Singh</last></author>
      <author><first>Dan</first> <last>Roth</last></author>
      <pages>2681–2690</pages>
      <url hash="d7fec541">D17-1284</url>
      <doi>10.18653/v1/D17-1284</doi>
      <abstract>For accurate <a href="https://en.wikipedia.org/wiki/Entity_linking">entity linking</a>, we need to capture various information aspects of an entity, such as its description in a <a href="https://en.wikipedia.org/wiki/Knowledge_base">KB</a>, contexts in which it is mentioned, and structured knowledge. Additionally, a linking system should work on texts from different domains without requiring domain-specific training data or hand-engineered features. In this work we present a neural, modular entity linking system that learns a unified dense representation for each entity using multiple sources of information, such as its description, contexts around its mentions, and its fine-grained types. We show that the resulting entity linking system is effective at combining these sources, and performs competitively, sometimes out-performing current state-of-the-art systems across datasets, without requiring any domain-specific training data or hand-engineered features. We also show that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can effectively embed entities that are new to the KB, and is able to link its mentions accurately.</abstract>
      <bibkey>gupta-etal-2017-entity</bibkey>
    </paper>
    <paper id="285">
      <title>An Insight Extraction System on BioMedical Literature with Deep Neural Networks<fixed-case>B</fixed-case>io<fixed-case>M</fixed-case>edical Literature with Deep Neural Networks</title>
      <author><first>Hua</first> <last>He</last></author>
      <author><first>Kris</first> <last>Ganjam</last></author>
      <author><first>Navendu</first> <last>Jain</last></author>
      <author><first>Jessica</first> <last>Lundin</last></author>
      <author><first>Ryen</first> <last>White</last></author>
      <author><first>Jimmy</first> <last>Lin</last></author>
      <pages>2691–2701</pages>
      <url hash="7e4aefd2">D17-1285</url>
      <doi>10.18653/v1/D17-1285</doi>
      <abstract>Mining biomedical text offers an opportunity to automatically discover important facts and infer associations among them. As new scientific findings appear across a large collection of biomedical publications, our aim is to tap into this literature to automate biomedical knowledge extraction and identify important insights from them. Towards that goal, we develop a <a href="https://en.wikipedia.org/wiki/System">system</a> with novel <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a> to extract insights on biomedical literature. Evaluation shows our <a href="https://en.wikipedia.org/wiki/System">system</a> is able to provide insights with competitive accuracy of human acceptance and its relation extraction component outperforms previous work.</abstract>
      <bibkey>he-etal-2017-insight</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task 8</pwcdataset>
    </paper>
    <paper id="286">
      <title>Word Etymology as Native Language Interference</title>
      <author><first>Vivi</first> <last>Nastase</last></author>
      <author><first>Carlo</first> <last>Strapparava</last></author>
      <pages>2702–2707</pages>
      <url hash="0d166e3a">D17-1286</url>
      <doi>10.18653/v1/D17-1286</doi>
      <abstract>We present experiments that show the influence of <a href="https://en.wikipedia.org/wiki/First_language">native language</a> on <a href="https://en.wikipedia.org/wiki/Lexical_choice">lexical choice</a> when producing text in another language   in this particular case <a href="https://en.wikipedia.org/wiki/English_language">English</a>. We start from the premise that non-native English speakers will choose lexical items that are close to words in their native language. This leads us to an etymology-based representation of documents written by people whose mother tongue is an <a href="https://en.wikipedia.org/wiki/Indo-European_languages">Indo-European language</a>. Based on this <a href="https://en.wikipedia.org/wiki/Representation_(mathematics)">representation</a> we grow a <a href="https://en.wikipedia.org/wiki/Language_family">language family tree</a>, that matches closely the <a href="https://en.wikipedia.org/wiki/Indo-European_languages">Indo-European language tree</a>.</abstract>
      <bibkey>nastase-strapparava-2017-word</bibkey>
    </paper>
    <paper id="287">
      <title>A Simpler and More Generalizable Story Detector using Verb and Character Features</title>
      <author><first>Joshua</first> <last>Eisenberg</last></author>
      <author><first>Mark</first> <last>Finlayson</last></author>
      <pages>2708–2715</pages>
      <url hash="cb437665">D17-1287</url>
      <doi>10.18653/v1/D17-1287</doi>
      <abstract>Story detection is the task of determining whether or not a unit of text contains a story. Prior approaches achieved a maximum performance of 0.66 F1, and did not generalize well across different corpora. We present a new state-of-the-art <a href="https://en.wikipedia.org/wiki/Sensor">detector</a> that achieves a maximum performance of 0.75 <a href="https://en.wikipedia.org/wiki/F-number">F1</a> (a 14 % improvement), with significantly greater generalizability than previous work. In particular, our detector achieves performance above 0.70 <a href="https://en.wikipedia.org/wiki/F-number">F1</a> across a variety of combinations of lexically different corpora for training and testing, as well as dramatic improvements (up to 4,000 %) in performance when trained on a small, disfluent data set. The new <a href="https://en.wikipedia.org/wiki/Sensor">detector</a> uses two basic types of featuresones related to <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)">events</a>, and ones related to characterstotaling 283 specific features overall ; previous detectors used tens of thousands of <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>, and so this <a href="https://en.wikipedia.org/wiki/Sensor">detector</a> represents a significant simplification along with increased performance.</abstract>
      <bibkey>eisenberg-finlayson-2017-simpler</bibkey>
    </paper>
    <paper id="288">
      <title>Multi-modular domain-tailored OCR post-correction<fixed-case>OCR</fixed-case> post-correction</title>
      <author><first>Sarah</first> <last>Schulz</last></author>
      <author><first>Jonas</first> <last>Kuhn</last></author>
      <pages>2716–2726</pages>
      <url hash="e93db1d7">D17-1288</url>
      <doi>10.18653/v1/D17-1288</doi>
      <abstract>One of the main obstacles for many Digital Humanities projects is the low data availability. Texts have to be digitized in an expensive and time consuming process whereas Optical Character Recognition (OCR) post-correction is one of the time-critical factors. At the example of OCR post-correction, we show the adaptation of a generic system to solve a specific problem with little data. The system accounts for a diversity of errors encountered in OCRed texts coming from different time periods in the domain of literature. We show that the combination of different approaches, such as e.g. Statistical Machine Translation and <a href="https://en.wikipedia.org/wiki/Spell_checker">spell checking</a>, with the help of a ranking mechanism tremendously improves over single-handed approaches. Since we consider the accessibility of the resulting tool as a crucial part of Digital Humanities collaborations, we describe the <a href="https://en.wikipedia.org/wiki/Workflow">workflow</a> we suggest for efficient text recognition and subsequent automatic and manual post-correction</abstract>
      <bibkey>schulz-kuhn-2017-multi</bibkey>
    </paper>
    <paper id="289">
      <title>Learning to Predict Charges for Criminal Cases with Legal Basis</title>
      <author><first>Bingfeng</first> <last>Luo</last></author>
      <author><first>Yansong</first> <last>Feng</last></author>
      <author><first>Jianbo</first> <last>Xu</last></author>
      <author><first>Xiang</first> <last>Zhang</last></author>
      <author><first>Dongyan</first> <last>Zhao</last></author>
      <pages>2727–2736</pages>
      <url hash="eeee57ec">D17-1289</url>
      <doi>10.18653/v1/D17-1289</doi>
      <abstract>The charge prediction task is to determine appropriate charges for a given case, which is helpful for legal assistant systems where the user input is fact description. We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.</abstract>
      <bibkey>luo-etal-2017-learning</bibkey>
    </paper>
    <paper id="290">
      <title>Quantifying the Effects of Text Duplication on Semantic Models</title>
      <author><first>Alexandra</first> <last>Schofield</last></author>
      <author><first>Laure</first> <last>Thompson</last></author>
      <author><first>David</first> <last>Mimno</last></author>
      <pages>2737–2747</pages>
      <url hash="e236f24f">D17-1290</url>
      <doi>10.18653/v1/D17-1290</doi>
      <abstract>Duplicate documents are a pervasive problem in text datasets and can have a strong effect on <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised models</a>. Methods to remove duplicate texts are typically heuristic or very expensive, so it is vital to know when and why they are needed. We measure the sensitivity of two latent semantic methods to the presence of different levels of <a href="https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)">document repetition</a>. By artificially creating different forms of duplicate text we confirm several hypotheses about how repeated text impacts <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. While a small amount of duplication is tolerable, substantial over-representation of subsets of the text may overwhelm meaningful topical patterns.</abstract>
      <bibkey>schofield-etal-2017-quantifying</bibkey>
    </paper>
    <paper id="291">
      <title>Identifying Semantically Deviating Outlier Documents</title>
      <author><first>Honglei</first> <last>Zhuang</last></author>
      <author><first>Chi</first> <last>Wang</last></author>
      <author><first>Fangbo</first> <last>Tao</last></author>
      <author><first>Lance</first> <last>Kaplan</last></author>
      <author><first>Jiawei</first> <last>Han</last></author>
      <pages>2748–2757</pages>
      <url hash="4c171234">D17-1291</url>
      <doi>10.18653/v1/D17-1291</doi>
      <abstract>A document outlier is a document that substantially deviates in semantics from the majority ones in a corpus. Automatic identification of document outliers can be valuable in many <a href="https://en.wikipedia.org/wiki/Application_software">applications</a>, such as <a href="https://en.wikipedia.org/wiki/Screening_(medicine)">screening health records</a> for <a href="https://en.wikipedia.org/wiki/Medical_error">medical mistakes</a>. In this paper, we study the problem of mining semantically deviating document outliers in a given corpus. We develop a <a href="https://en.wikipedia.org/wiki/Generative_model">generative model</a> to identify frequent and characteristic semantic regions in the word embedding space to represent the given corpus, and a robust outlierness measure which is resistant to noisy content in documents. Experiments conducted on two real-world textual data sets show that our method can achieve an up to 135 % improvement over baselines in terms of <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a> at top-1 % of the outlier ranking.</abstract>
      <bibkey>zhuang-etal-2017-identifying</bibkey>
    </paper>
    <paper id="292">
      <title>Detecting and Explaining Causes From Text For a Time Series Event</title>
      <author><first>Dongyeop</first> <last>Kang</last></author>
      <author><first>Varun</first> <last>Gangal</last></author>
      <author><first>Ang</first> <last>Lu</last></author>
      <author><first>Zheng</first> <last>Chen</last></author>
      <author><first>Eduard</first> <last>Hovy</last></author>
      <pages>2758–2767</pages>
      <url hash="66bc6001">D17-1292</url>
      <doi>10.18653/v1/D17-1292</doi>
      <abstract>Explaining underlying causes or effects about events is a challenging but valuable task. We define a novel problem of generating explanations of a time series event by (1) searching cause and effect relationships of the time series with textual data and (2) constructing a connecting chain between them to generate an explanation. To detect causal features from text, we propose a novel method based on the Granger causality of time series between <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> extracted from text such as <a href="https://en.wikipedia.org/wiki/N-gram">N-grams</a>, topics, sentiments, and their composition. The generation of the sequence of causal entities requires a commonsense causative knowledge base with efficient <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a>. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.</abstract>
      <bibkey>kang-etal-2017-detecting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    <title_es>Detección y explicación de las causas del texto de un evento de serie temporal</title_es>
      <title_fr>Détecter et expliquer les causes à partir d'un texte pour un événement de série chronologique</title_fr>
      <title_ar>كشف وشرح الأسباب من النص لحدث السلاسل الزمنية</title_ar>
      <title_pt>Detectando e explicando as causas do texto para um evento de série temporal</title_pt>
      <title_ja>時系列イベントのためのテキストからの原因の検出と説明</title_ja>
      <title_hi>किसी समय श्रृंखला ईवेंट के लिए पाठ से कारणों का पता लगाना और समझाना</title_hi>
      <title_ru>Выявление и объяснение причин из текста для события временного ряда</title_ru>
      <title_zh>从文本中检释时序事故也</title_zh>
      <title_ga>Cúiseanna Ó Téacs a Bhrath agus a Mhíniú Le hAghaidh Ócáid Amshraith</title_ga>
      <title_ka>Comment</title_ka>
      <title_el>Ανίχνευση και εξήγηση αιτιών από κείμενο για ένα συμβάν χρονικής σειράς</title_el>
      <title_hu>Idősoros események szövegének okainak észlelése és magyarázata</title_hu>
      <title_mk>Детектирање и објаснување на причините од текстот за временски сериски настан</title_mk>
      <title_kk>Мәтіннен келесі уақыт тізімінің оқиғасын табу мен түсіндіру себептері</title_kk>
      <title_it>Rilevamento e spiegazione delle cause del testo per un evento di serie temporale</title_it>
      <title_lt>Teksto priežasčių nustatymas ir paaiškinimas laiko eilutės įvykiui</title_lt>
      <title_mt>L-identifikazzjoni u l-ispjegazzjoni tal-kawżi mit-test għal avveniment ta’ serje ta’ ħin</title_mt>
      <title_ml>വാചകത്തില്‍ നിന്നും കാരണങ്ങള്‍ കണ്ടുപിടിക്കുകയും വ്യക്തമാക്കുകയും ചെയ്യുന്നു</title_ml>
      <title_ms>Detecting and Explaining Causes From Text For a Time Series Event</title_ms>
      <title_mn>Хугацааны дараагийн үйл явдлын текстээс шалтгаан тогтоох болон тодорхойлох</title_mn>
      <title_no>Oppdag og utforskar grunnar frå teksten for ein tidseringshending</title_no>
      <title_pl>Wykrywanie i wyjaśnianie przyczyn tekstu dla zdarzenia serii czasowej</title_pl>
      <title_ro>Detectarea și explicarea cauzelor din text pentru un eveniment de serie de timp</title_ro>
      <title_sr>Otkrivanje i objašnjavanje uzroka teksta za događaj serije vremena</title_sr>
      <title_si>වෙලාව සිරීම් සැකසුම් සඳහා පරික්ෂණය සහ විස්තර කරන්න</title_si>
      <title_so>Detecting and Explaining Causes From Text For a Time Series Event</title_so>
      <title_sv>Identifiera och förklara orsaker till text för en tidsseriehändelse</title_sv>
      <title_ta>ஒரு நேர தொடர்ந்து நிகழ்வுக்கு உரையிலிருந்து காரணங்களை கண்டுபிடி</title_ta>
      <title_ur>ایک وقت سیریر ایڈمنٹ کے لئے پاکستان سے سبب شناسایی اور کھول رہی ہے</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Phát hiện và giải thích nguyên nhân từ văn bản cho sự kiện chuỗi thời gian</title_vi>
      <title_nl>Oorzaken detecteren en uitleggen van tekst voor een tijdreeksevenement</title_nl>
      <title_da>Registrering og forklaring af årsager fra tekst til en tidsseriebegivenhed</title_da>
      <title_bg>Откриване и обясняване на причини от текст за събитие от времева серия</title_bg>
      <title_hr>Otkrivanje i objašnjavanje uzroka teksta za događaj serije vremena</title_hr>
      <title_id>Mengeteksi dan menjelaskan penyebab Dari Teks Untuk Peristiwa Seri Waktu</title_id>
      <title_ko>시간 시퀀스 이벤트의 텍스트에서 원인 탐지 및 해석</title_ko>
      <title_de>Erkennen und Erklären von Ursachen aus Text für ein Zeitreihenereignis</title_de>
      <title_fa>شناسایی و توضیح دلایل از متن برای یک اتفاق مجموعه زمانی</title_fa>
      <title_tr>Metin Senediň Senediň Sebepleri Aňlap we Keşfedilýär</title_tr>
      <title_af>Name</title_af>
      <title_sq>Duke zbuluar dhe shpjeguar shkaqet nga teksti për një ngjarje serie kohore</title_sq>
      <title_sw>Kugundua na Kuelezea Sababu Kutoka Matambo kwa Tukio la Muda</title_sw>
      <title_am>Text For a time Series Event</title_am>
      <title_az>Bir vaxt seriyasńĪ olaraq m…ôtnd…ôn s…ôb…ôbl…ôri keŇüif v…ô aydńĪnlaŇüdńĪrma</title_az>
      <title_bn>টেক্সট থেকে ক্যাফেস সনাক্ত করা এবং ব্যাখ্যা করা হচ্ছে একটি সময় সিরিয়ের অনুষ্ঠানের জন্য</title_bn>
      <title_hy>Comment</title_hy>
      <title_bs>Otkrivanje i objašnjavanje uzroka teksta za događaj serije vremena</title_bs>
      <title_ca>Detectar i explicar les causes del text durant un eventde sèrie temporal</title_ca>
      <title_et>Ajaseeria sündmuse tekstist põhjuste tuvastamine ja selgitamine</title_et>
      <title_cs>Detekce a vysvětlení příčin textu pro událost časové řady</title_cs>
      <title_fi>Tekstistä johtuvien syiden tunnistaminen ja selittäminen aikasarjatapahtumassa</title_fi>
      <title_sk>Zaznavanje in pojasnjevanje vzrokov iz besedila za dogodek časovne serije</title_sk>
      <title_jv>politenessoffpolite"), and when there is a change ("assertive</title_jv>
      <title_he>גילוי ומסביר סיבות מהטקסט לאירוע סדרת זמן</title_he>
      <title_ha>@ action</title_ha>
      <title_bo>ཚིག་ཡི་གེ་ཚིག་དང་ཆ་རྐྱེན་འབྱུང་བའི་རྒྱུ་མཚན་བཙལ་བ</title_bo>
      <abstract_ar>يعد شرح الأسباب أو التأثيرات الأساسية للأحداث مهمة صعبة ولكنها قيّمة. نحدد مشكلة جديدة لتوليد تفسيرات لحدث السلاسل الزمنية من خلال (1) البحث عن علاقات السبب والنتيجة للسلسلة الزمنية مع البيانات النصية و (2) إنشاء سلسلة ربط بينهما لتوليد تفسير. لاكتشاف السمات السببية من النص ، نقترح طريقة جديدة تعتمد على السببية جرانجر للسلاسل الزمنية بين السمات المستخرجة من النص مثل N-grams والموضوعات والمشاعر وتكوينها. يتطلب إنشاء تسلسل الكيانات السببية قاعدة معرفة سببية منطقية مع تفكير فعال. لضمان قابلية تفسير جيدة واستخدام معجمي مناسب ، نجمع بين التمثيلات الرمزية والعصبية ، باستخدام خوارزمية التفكير العصبي المدربة على مجموعات سببية منطقية للتنبؤ بخطوة السبب التالية. يُظهر تحليلنا الكمي والبشري دليلًا تجريبيًا على أن طريقتنا تستخلص بنجاح علاقات سببية ذات مغزى بين السلاسل الزمنية ذات السمات النصية وتنتج تفسيرًا مناسبًا فيما بينها.</abstract_ar>
      <abstract_pt>Explicar causas ou efeitos subjacentes sobre eventos é uma tarefa desafiadora, mas valiosa. Definimos um novo problema de gerar explicações de um evento de série temporal (1) procurando relações de causa e efeito da série temporal com dados textuais e (2) construindo uma cadeia de conexão entre eles para gerar uma explicação. Para detectar características causais do texto, propomos um novo método baseado na causalidade de Granger de séries temporais entre características extraídas do texto, como N-grams, tópicos, sentimentos e sua composição. A geração da sequência de entidades causais requer uma base de conhecimento causativa de senso comum com raciocínio eficiente. Para garantir uma boa interpretabilidade e uso lexical apropriado, combinamos representações simbólicas e neurais, usando um algoritmo de raciocínio neural treinado em tuplas causais de senso comum para prever o próximo passo da causa. Nossas análises quantitativas e humanas mostram evidências empíricas de que nosso método extrai com sucesso relações de causalidade significativas entre séries temporais com recursos textuais e gera explicação apropriada entre elas.</abstract_pt>
      <abstract_es>Explicar las causas o los efectos subyacentes de los eventos es una tarea difícil pero valiosa. Definimos un problema novedoso de generar explicaciones de un evento de serie temporal mediante (1) la búsqueda de las relaciones de causa y efecto de la serie temporal con datos textuales y (2) la construcción de una cadena de conexión entre ellos para generar una explicación. Para detectar las características causales del texto, proponemos un método novedoso basado en la causalidad de Granger de las series temporales entre las características extraídas del texto, como N-gramas, temas, sentimientos y su composición. La generación de la secuencia de entidades causales requiere una base de conocimiento causal de sentido común con un razonamiento eficiente. Para garantizar una buena interpretabilidad y un uso léxico adecuado, combinamos representaciones simbólicas y neuronales, utilizando un algoritmo de razonamiento neuronal entrenado en tuplas causales de sentido común para predecir el siguiente paso de causa. Nuestros análisis cuantitativos y humanos muestran evidencia empírica de que nuestro método extrae con éxito relaciones de causalidad significativas entre series temporales con características textuales y genera una explicación adecuada entre ellas.</abstract_es>
      <abstract_fr>Expliquer les causes ou les effets sous-jacents des événements est une tâche difficile mais précieuse. Nous définissons un nouveau problème de génération d'explications d'un événement de série chronologique en (1) recherchant les relations de cause à effet de la série chronologique avec des données textuelles et (2) en construisant une chaîne de connexion entre elles pour générer une explication. Pour détecter les caractéristiques causales du texte, nous proposons une nouvelle méthode basée sur la causalité de Granger des séries temporelles entre des caractéristiques extraites du texte telles que des N-grammes, des sujets, des sentiments et leur composition. La génération de la séquence d'entités causales nécessite une base de connaissances causales de bon sens assortie d'un raisonnement efficace. Pour garantir une bonne interprétabilité et une utilisation lexicale appropriée, nous combinons des représentations symboliques et neuronales, à l'aide d'un algorithme de raisonnement neuronal formé sur des tuples causaux de bon sens pour prédire l'étape suivante de la cause. Notre analyse quantitative et humaine montre des preuves empiriques que notre méthode réussit à extraire des relations de causalité significatives entre des séries temporelles avec des caractéristiques textuelles et à générer une explication appropriée entre elles.</abstract_fr>
      <abstract_hi>घटनाओं के बारे में अंतर्निहित कारणों या प्रभावों की व्याख्या करना एक चुनौतीपूर्ण लेकिन मूल्यवान कार्य है। हम एक समय श्रृंखला घटना के स्पष्टीकरण उत्पन्न करने की एक उपन्यास समस्या को परिभाषित करते हैं (1) पाठ्य डेटा के साथ समय श्रृंखला के कारण और प्रभाव संबंधों की खोज और (2) एक स्पष्टीकरण उत्पन्न करने के लिए उनके बीच एक कनेक्टिंग श्रृंखला का निर्माण करना। पाठ से कारण सुविधाओं का पता लगाने के लिए, हम एन-ग्राम, विषयों, भावनाओं और उनकी संरचना जैसे पाठ से निकाली गई सुविधाओं के बीच समय श्रृंखला के ग्रेंजर कारण के आधार पर एक उपन्यास विधि का प्रस्ताव करते हैं। कारण संस्थाओं के अनुक्रम की पीढ़ी को कुशल तर्क के साथ एक सामान्य ज्ञान प्रेरक ज्ञान आधार की आवश्यकता होती है। अच्छी व्याख्या और उचित लेक्सिकल उपयोग सुनिश्चित करने के लिए हम अगले कारण चरण की भविष्यवाणी करने के लिए कॉमनसेंस कारण ट्यूपल्स पर प्रशिक्षित एक तंत्रिका तर्क एल्गोरिथ्म का उपयोग करके प्रतीकात्मक और तंत्रिका प्रतिनिधित्व को जोड़ते हैं। हमारे मात्रात्मक और मानव विश्लेषण से अनुभवजन्य सबूत पता चलता है कि हमारी विधि सफलतापूर्वक पाठ्य विशेषताओं के साथ समय श्रृंखला के बीच सार्थक कार्य-कारण संबंधों को निकालती है और उनके बीच उचित स्पष्टीकरण उत्पन्न करती है।</abstract_hi>
      <abstract_ja>イベントに関する根本的な原因や影響を説明することは、困難でありながら価値のある作業です。 時系列事象の説明を生成する新たな問題を、(1)テキストデータを用いて時系列の因果関係を検索することと、(2)それらの間に連鎖を構築して説明を生成することによって定義する。 テキストから因果的特徴を検出するために、N - gram、トピック、感情、およびそれらの構成などのテキストから抽出された特徴間の時系列のグレンジャー因果性に基づいた新規の方法を提案する。 因果的実体のシーケンスの生成には、効率的な推論を伴う共通の因果的知識ベースが必要である。 良好な解釈可能性と適切な語彙使用を確保するために、私たちは、次の原因ステップを予測するために、共通の因果関係タプルでトレーニングされたニューラル推論アルゴリズムを使用して、記号的表現とニューラル表現を組み合わせます。 私たちの定量的およびヒューマン分析は、私たちの方法がテキスト機能を持つ時系列間の有意義な因果関係をうまく抽出し、それらの間に適切な説明を生成するという経験的証拠を示しています。</abstract_ja>
      <abstract_zh>说事之本,一有挑战性而有直者也。 吾定义了一个新颖,即因(1)用文本数搜索时间序列的因果关系,及(2)在其间构连链以成解释以成时序事的解释。 检文本之因果特征,立一本于文本(如N-grams主题,情成)序者Granger因果关系新法也。 因果实之生,须有理常识性因果知识库。 以保良可解释性适词汇用法,合符号于神经,用常识性因果元组上训练神经推理算法以卜之。 吾定量与人分析表明,吾道成取文本有时序之间有义因果关系,而生其解释。</abstract_zh>
      <abstract_ru>Объяснение основных причин или последствий событий является сложной, но ценной задачей. Определяем новую задачу генерации объяснений события временного ряда путем (1) поиска причинно-следственных связей временного ряда с текстовыми данными и (2) построения связующей цепи между ними для генерации объяснения. Чтобы обнаружить причинно-следственные связи из текста, мы предлагаем новый метод, основанный на причинно-следственной связи Грейнджера временных рядов между признаками, извлеченными из текста, такими как N-граммы, темы, чувства и их состав. Генерация последовательности причинно-следственных связей требует наличия общепринятой базы причинно-следственных знаний с эффективной аргументацией. Для обеспечения хорошей интерпретируемости и соответствующего лексического использования мы объединяем символические и нейронные представления, используя алгоритм нейронного мышления, обученный на каузальных кортежах здравого смысла, чтобы предсказать следующий шаг причины. Наш количественный и человеческий анализ показывает эмпирические доказательства того, что наш метод успешно извлекает значимые причинно-следственные связи между временными рядами с текстовыми признаками и генерирует соответствующее объяснение между ними.</abstract_ru>
      <abstract_ga>Is tasc dúshlánach ach luachmhar é bunchúiseanna nó éifeachtaí imeachtaí a mhíniú. Sainmhínímid fadhb nua maidir le míniúcháin a ghiniúint ar imeacht sraith ama trí (1) caidreamh cúise agus éifeacht na sraithe ama a chuardach le sonraí téacsúla agus (2) slabhra ceangail a thógáil eatarthu chun míniú a ghiniúint. Chun gnéithe cúiseacha a bhrath ó théacs, molaimid modh úrnua bunaithe ar shraith ama cúisíochta Granger idir gnéithe a bhaintear as téacs mar N-gram, topaicí, mothúcháin, agus a gcomhdhéanamh. Chun seicheamh na n-eintiteas cúisíoch a ghiniúint, tá gá le bonn eolais cúisíoch tuisceana agus réasúnaíocht éifeachtach. Chun inléirmhíniú maith agus úsáid chuí foclóireachta a chinntiú cuirimid léiriúcháin siombalacha agus néaracha le chéile, ag baint úsáide as algartam réasúnaíochta néaracha atá oilte ar thuplaí cúiseacha le tuiscint choitianta chun an chéad chéim eile le cúis a thuar. Léiríonn ár n-anailís chainníochtúil agus daonna fianaise eimpíreach go n-éiríonn lenár modh gaolmhaireachtaí cúisíochta brí a bhaint amach idir sraitheanna ama le gnéithe téacsúla agus go gcruthaíonn sé míniú cuí eatarthu.</abstract_ga>
      <abstract_el>Η εξήγηση των υποκείμενων αιτιών ή επιπτώσεων σχετικά με τα γεγονότα είναι μια πρόκληση αλλά πολύτιμη εργασία. Καθορίζουμε ένα νέο πρόβλημα της δημιουργίας επεξηγήσεων ενός γεγονότος χρονοσειράς με (1) την αναζήτηση σχέσεων αιτίας και αποτελέσματος της χρονοσειράς με δεδομένα κειμένου και (2) την κατασκευή μιας αλυσίδας σύνδεσης μεταξύ τους για τη δημιουργία μιας εξήγησης. Για την ανίχνευση αιτιακών χαρακτηριστικών από κείμενο, προτείνουμε μια νέα μέθοδο βασισμένη στην αιτιώδη συνάφεια των χρονοσειρών μεταξύ χαρακτηριστικών που εξάγονται από κείμενο όπως Ν-γραφήματα, θέματα, συναισθήματα και τη σύνθεσή τους. Η δημιουργία της αλληλουχίας αιτιακών οντοτήτων απαιτεί μια λογική αιτιολογική βάση γνώσεων με αποτελεσματική συλλογιστική. Για να εξασφαλιστεί καλή ερμηνεία και κατάλληλη λεξική χρήση συνδυάζουμε συμβολικές και νευρωνικές αναπαραστάσεις, χρησιμοποιώντας έναν αλγόριθμο νευρωνικής λογικής εκπαιδευμένο σε αιτιολογικές τουαλέτες κοινής λογικής για να προβλέψουμε το επόμενο βήμα αιτίας. Η ποσοτική και ανθρώπινη ανάλυση μας δείχνει εμπειρικά στοιχεία ότι η μέθοδος μας εξάγει επιτυχώς ουσιαστικές αιτιώδεις σχέσεις μεταξύ χρονοσειρών με κειμενικά χαρακτηριστικά και παράγει την κατάλληλη εξήγηση μεταξύ τους.</abstract_el>
      <abstract_hu>Az eseményekkel kapcsolatos okok és hatások magyarázata kihívást jelentő, de értékes feladat. Az idősoros események magyarázatának generálásának új problémáját definiáljuk (1) az idősorok ok-okozati összefüggéseinek keresésével szöveges adatokkal és (2) egy összekötő lánc kialakításával, hogy magyarázatot generáljunk. Az okozati jellemzők felismerésére a szövegből egy új módszert javasolunk, amely az idősorok Granger okozati összefüggésén alapul a szövegből kivont jellemzők között, mint például N-gramm, témák, érzelmek és azok összetétele. Az okozati entitások sorozatának létrehozása hatékony érveléssel ellátott, közértelmes okozati tudásbázist igényel. A jó értelmezhetőség és a megfelelő lexikális használat biztosítása érdekében kombináljuk a szimbolikus és neurális reprezentációkat, egy közértelmű okozati tupletekre képzett neurális érvelési algoritmust használva, hogy megjósoljuk a következő ok lépést. Mennyiségi és emberi elemzésünk empirikus bizonyítékot mutat arra, hogy módszerünk sikeresen kivonja a szöveges jellemzőkkel rendelkező idősorok közötti értelmes okozati összefüggéseket és megfelelő magyarázatot generál közöttük.</abstract_hu>
      <abstract_ka>მოვლენების შესახებ მიზეზები ან ეფექტიები განახსნა შესაძლებელია, მაგრამ უფრო მნიშვნელოვანი დავალება. ჩვენ განსაზღვრებთ პრობლემა, რომელიც წარმოიქმნა დრო სერიის მოვლენების განახლებების შექმნა (1) საძებნა მიზეზი და ეფექტის შესახებების შესახებ, რომელიც ტექსტულ მონაცემებით და (2) დაკავშირებული კ ტექსტიდან მიზეზი ფუნქციების განახლებისთვის, ჩვენ დავიწყებთ პრომენტის მეტი, რომელიც გრანგერის წარმოდგენების წარმოდგენების წარმოდგენების წარმოდგენება ტექსტიდან გამოკლექტილი ფუ წარმოდგენების წარმოდგენების წარმოდგენების შემდეგ უნდა გამოსახულებელი წარმოდგენების უფრო გამოსახულებელი მეცნიერების ბაზა, რომელიც ეფექტი ჩვენ სიმბოლოგიური და ნეიროლური გამოყენებების გამოყენება, ნეიროლური პარამენტიური ალგორიტიმის გამოყენება, რომელიც საუბოლოო წარმოდგენების მიზეზი რამდენიმე წარმოდგენების გადა ჩვენი კოლუტატიური და ადამიანის ანალიზაცია გამოჩვენება ემპერიკური წარმოდგენება, რომ ჩვენი მეთოდი წარმოდგენელად გამოყენებს მნიშვნელოვანი წარმოდგენების შესახებ, რო</abstract_ka>
      <abstract_kk>Оқиғалар туралы негізгі себептерді немесе эффекттерді түсініп беру - әсер ететін, бірақ маңызды тапсырма. Біз уақыт сериясының түсініктемелерін жасау мәселесін (1) мәтін деректері мен (2) арасындағы түсініктемелерді жасау үшін уақыт сериясының себептері мен эффекттер қатынасын жасау үшін анықтаймыз. Мәтіннен негізгі мүмкіндіктерді табу үшін, N- граммалар, тақырыптар, сезімдер және олардың композициясы секілді мәтіннен таратылған мүмкіндіктер арасындағы Granger сериясының негізінде жаңа әдісін таңдаймыз Негізгі бірліктердің реттегін құру үшін көпшілікті білім негізі ефективті сезімдердің негізі керек. Жақсы түсініктемесін және қасиетті лексикалық қолдануды қамтамасыз үшін, біз символдық және невралдық түсініктемелерді біріктіреміз, көпшілікті түсініктеме алгоритмі көпшілікті Біздің санативтік және адамдардың анализиясыз текстік мүмкіндіктерімен уақыт сериясының арасындағы маңызды себептердің қатынасын сәтті түсіндіру және оның арасындағы дұрыс түсініктемелерд</abstract_kk>
      <abstract_it>Spiegare le cause o gli effetti sottostanti degli eventi è un compito impegnativo ma prezioso. Definiamo un nuovo problema di generare spiegazioni di un evento di serie temporali (1) cercando relazioni causa ed effetto della serie temporale con dati testuali e (2) costruendo una catena di collegamento tra di loro per generare una spiegazione. Per rilevare le caratteristiche causali dal testo, proponiamo un nuovo metodo basato sulla causalità Granger delle serie temporali tra caratteristiche estratte dal testo come N-grammi, argomenti, sentimenti e la loro composizione. La generazione della sequenza di entità causali richiede una base di conoscenza causale comune con ragionamento efficiente. Per garantire una buona interpretabilità e un uso lessicale appropriato combiniamo rappresentazioni simboliche e neurali, utilizzando un algoritmo di ragionamento neurale addestrato su tuples causali di buon senso per prevedere il prossimo passo causa. La nostra analisi quantitativa e umana mostra prove empiriche che il nostro metodo estrae con successo significative relazioni di causalità tra serie temporali con caratteristiche testuali e genera una spiegazione appropriata tra loro.</abstract_it>
      <abstract_lt>Pagrindinių priežasčių ar poveikio įvykiams paaiškinimas yra sudėtinga, bet vertinga užduotis. Mes apibrėžiame naują problem ą, susijusią su laiko serijos įvykio paaiškinimų sukūrimu (1) paieškos priežastimi ir poveikiu laiko serijos ryšiais su tekstiniais duomenimis ir (2) jų jungimo grandinės sukūrimu, kad būtų galima paaiškinti. Siekiant nustatyti priežastinius požymius iš teksto, siūlome naują metodą, pagrįstą Grangerio priežastiniu laiko eilutės ryšiu tarp požymių, ištrauktų iš teksto, pvz., N-gramų, temų, jausmų ir jų sudėties. Kad būtų sukurta priežastinių subjektų seka, reikia bendros priežastinių žinių bazės, kurioje būtų veiksmingai pagrįsta. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step.  Mūsų kiekybinė ir žmogaus analizė rodo empirinius įrodymus, kad mūsų metodas sėkmingai išgauna reikšmingus priežastinius santykius tarp laiko eilučių ir tekstinių savybių ir sukuria tinkamą jų paaiškinimą.</abstract_lt>
      <abstract_mk>Explaining underlying causes or effects about events is a challenging but valuable task.  Ние дефинираме нов проблем со генерирање објаснувања за временски сериски настан со (1) пребарување на причината и ефектот врски на временската серија со текстуални податоци и (2) изградба на ланец на поврзување помеѓу нив за генерирање објаснување. За да ги откриеме причинните карактеристики од текстот, предложуваме нов метод базиран на причината на временската серија Гренџер помеѓу карактеристиките извадени од текст како што се N-грами, теми, чувства и нивната композиција. Генерацијата на секвенцијата на причинските ентитети бара заедничка причинска база на знаење со ефикасно размислување. За да обезбедиме добра интерпретабилност и соодветна лексикална употреба ние комбинираме симболички и нервни претставувања, користејќи нервен размислувачки алгоритм обучен на заеднички причинни двојки за да го предвидеме следниот чекор на причината. Нашата квантитивна и човечка анализа покажува емпирички докази дека нашиот метод успешно извлекува значајни причински односи помеѓу временските серии со текстуални карактеристики и генерира соодветно објаснување помеѓу нив.</abstract_mk>
      <abstract_mt>L-ispjegazzjoni tal-kawżi jew l-effetti sottostanti dwar l-avvenimenti hija kompitu ta’ sfida iżda ta’ valur. Aħna niddefinixxu problem a ġdida li niġġeneraw spjegazzjonijiet ta’ avveniment tas-serje taż-żmien permezz ta’ (1) tfittxija ta’ relazzjonijiet ta’ kawża u effett tas-serje taż-żmien mad-dejta testwali u (2) il-bini ta’ katina ta’ konnessjoni bejniethom biex niġġeneraw spjegazzjoni. Biex jinstabu karatteristiċi kawżali mit-test, nipproponu metodu ġdid ibbażat fuq il-kawżalità Granger tas-serje taż-żmien bejn karatteristiċi estratti mit-test bħal N-grammi, suġġetti, sensazzjonijiet, u l-kompożizzjoni tagħhom. Il-ġenerazzjoni tas-sekwenza ta’ entitajiet kawżali teħtieġ bażi ta’ għarfien kawżali komuni b’raġunament effiċjenti. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step.  L-analiżi kwantitattiva u umana tagħna turi evidenza empirika li l-metodu tagħna jestrezzjona b’suċċess relazzjonijiet ta’ kawżalità sinifikanti bejn is-serje taż-żmien b’karatteristiċi testwali u jiġġenera spjegazzjoni xierqa bejniethom.</abstract_mt>
      <abstract_ms>Menjelaskan penyebab atau kesan asas mengenai peristiwa adalah tugas yang mencabar tetapi berharga. Kami menentukan masalah baru untuk menghasilkan penjelasan peristiwa siri masa dengan (1) mencari sebab dan kesan hubungan siri masa dengan data teks dan (2) membina rantai sambungan antara mereka untuk menghasilkan penjelasan. Untuk mengesan ciri-ciri penyebab dari teks, kami cadangkan kaedah baru berdasarkan penyebab Granger dari siri masa antara ciri-ciri yang diekstrak dari teks seperti N-gram, topik, perasaan, dan komposisi mereka. Generasi urutan entiti penyebab memerlukan pangkalan pengetahuan penyebab yang sama dengan alasan yang efisien. Untuk memastikan pengenalan yang baik dan penggunaan leksik yang sesuai, kita menggabungkan perwakilan simbolik dan saraf, menggunakan algoritma penyebab saraf dilatih pada dua penyebab umum untuk meramalkan langkah penyebab berikutnya. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.</abstract_ms>
      <abstract_ml>സംഭവങ്ങളെക്കുറിച്ച് അടിസ്ഥാനമായ കാരണങ്ങളോ പ്രഭാവങ്ങളോ വ്യക്തമാക്കുന്നത് വിലപിക്കുന്നത് വ ടെക്സ്ചുള്‍ ഡേറ്റായുള്ള സമയത്തിന്റെ കാരണവും പ്രഭാവങ്ങളുടെ ബന്ധങ്ങളും ടെക്സ്കൂള്‍ ഡാറ്റായുമായി ഒരു ബന്ധപ്പെടുത്തുന്ന ചങ്ങല്‍ ഉണ്ടാക്കുന്നതിന പദാവലിയില്‍ നിന്നുള്ള കാരണങ്ങള്‍ കണ്ടുപിടിക്കാന്‍ ഞങ്ങള്‍ ഒരു നോവല്‍ രീതിയില്‍ നിര്‍ദ്ദേശിക്കുന്നു. ഗ്രാന്‍ജെര്‍ സമയ സീരിയുടെ കാരണമാണ് പ്രേരിപ്പി കാരണ വസ്തുക്കളുടെ തലമുറയുടെ തലമുറയില്‍ ഒരു കമോണ്‍സണ്‍സെന്‍സ് കാരണവുമായ അറിവ് ബേസ് ആവശ്യമുണ്ട്. അതിന്റെ കാരണങ് നന്നായി വ്യാഖ്യാനിപ്പിക്കുന്നതും പ്രധാനപ്പെട്ട ലെക്സിക്കല്‍ ഉപയോഗിക്കുന്നതും ഉറപ്പ് വരുത്തുവാനും, അടുത്ത കാരണങ്ങള്‍ പ്രവചിക്കുന്നതിന് മു നമ്മുടെ കണക്കിന്റെയും മനുഷ്യന്റെയും വിശ്വാസവും കാണിച്ചുകൊണ്ടിരിക്കുന്നു നമ്മുടെ രീതി വിജയിച്ചു കൊണ്ട് നമ്മുടെ രീതിയില്‍ വി</abstract_ml>
      <abstract_mn>Үүний үндсэн шалтгаанууд эсвэл үйл явдлын нөлөөг тайлбарлах нь хэцүү, үнэ цэнэтэй ажил юм. Бид цаг хугацааны тодорхойлолтын тодорхойлолтын шинэ асуудлыг (1) хайлтын шалтгаан, нөлөөтэй харилцааны холбоотой (2) болон тэдний хоорондоо холбоотой хэлхээ бий болгож тайлбарлах боломжтой. Текстүүдийн шалтгаан чадварыг олохын тулд бид Н-грамм, сэтгэл, сэтгэл хөдлөл болон тэдний бүтээгдэхүүний хоорондын Грангер шалтгааны шалтгааныг санал болгож байна. Яагаад үйл явдлын дарааллын дарааллаар үр дүнтэй ойлголтын үр дүнтэй мэдлэгийн суурь хэрэгтэй. Үнэндээ сайн ойлголтын болон зөв хэлний хэрэглээний тулд бид символ болон мэдрэлийн төлөвлөгөөг нийтлэг ойлголтын алгоритм ашиглаж, дараагийн шалтгааныг таамаглах боломжтой түвшинд сургалтыг ашиглаж байна. Бидний хэмжээний болон хүн төрөлхтний шинжилгээ нь бидний арга зам нь мөн амжилттай хүчин зүйлсийн харилцааны хоорондын хугацааны харилцааны харилцааны харилцаа, мөн тэдний хоорондоо зөв тодорхойлолт</abstract_mn>
      <abstract_pl>Wyjaśnienie podstawowych przyczyn lub skutków zdarzeń jest trudnym, ale cennym zadaniem. Definiujemy nowatorski problem generowania wyjaśnień zdarzenia szeregu czasowego poprzez (1) wyszukiwanie relacji przyczynowo-skutkowych szeregu czasowego z danymi tekstowymi oraz (2) konstruowanie łańcucha łączącego między nimi w celu wygenerowania wyjaśnienia. Aby wykryć cechy przyczynowe z tekstu, proponujemy nowatorską metodę opartą na przyczynowości Grangera szeregów czasowych pomiędzy cechami ekstraktowanymi z tekstu, takimi jak N-gramy, tematy, sentymenty i ich skład. Generowanie sekwencji podmiotów przyczynowych wymaga zdrowego rozsądku bazy wiedzy przyczynowej z efektywnym rozumowaniem. Aby zapewnić dobrą interpretację i odpowiednie stosowanie leksykalii łączymy reprezentacje symboliczne i neuronowe, wykorzystując algorytm rozumowania neuronowego przeszkolony na krokach przyczynowych zdrowego rozsądku w celu przewidzenia kolejnego kroku przyczynowego. Nasza analiza ilościowa i ludzka pokazuje empiryczne dowody na to, że nasza metoda skutecznie wydobywa istotne związki przyczynowe między szeregami czasowymi z cechami tekstowymi i generuje odpowiednie wyjaśnienie między nimi.</abstract_pl>
      <abstract_ro>Explicarea cauzelor sau efectelor subiacente despre evenimente este o sarcină dificilă, dar valoroasă. Definim o nouă problemă de generare a explicațiilor unui eveniment de serie timpurie prin (1) căutarea relațiilor cauză-efect ale seriilor temporale cu datele textuale și (2) construirea unui lanț de conectare între ele pentru a genera o explicație. Pentru a detecta caracteristicile cauzale din text, propunem o metodă nouă bazată pe cauzalitatea Granger a seriilor de timp între caracteristicile extrase din text, cum ar fi N-grame, subiecte, sentimente și compoziția lor. Generarea secvenței entităților cauzale necesită o bază de cunoștințe cauzale de bun sens, cu raționament eficient. Pentru a asigura o interpretare bună și o utilizare lexicală adecvată combinăm reprezentări simbolice și neurale, folosind un algoritm de raționament neural instruit pe tuplele cauzale de bun sens pentru a prezice următorul pas cauzal. Analiza noastră cantitativă și umană arată dovezi empirice că metoda noastră extrage cu succes relații de cauzalitate semnificative între seriile de timp cu caracteristici textuale și generează explicații adecvate între ele.</abstract_ro>
      <abstract_no>Å forklare grunnleggjande grunnar eller effektar om hendingar er ein vanskeleg, men verdileg oppgåve. Vi definerer eit nytt problem med å laga forklaringar av ei tidseriefeil ved å søkja etter (1) søkja og effektforhold av tidserien med tekstverdiar og (2) som konstruerer ein tilkopling kjede mellom dei for å laga ein forklaring. For å oppdaga grunnleggjande funksjonar frå tekst, fører vi ein roman metode basert på Granger-grunnleggjande av tidserien mellom funksjonar som er ekstrahert frå teksten, som N-gramar, emner, sentimentar og komponenten sine. Generasjonen av sekvensen av grunnleggjande einingar krev ein vanleg forårsakeleg kunnskapsbasen med effektiv motivering. For å sikre på godt tolkingsbruk og tilpassande leksisk bruk kombinerer vi symbolske og neurale representasjonar ved å bruka ein neuralrasjonsgoritme trent på vanlegvis følgjande tuplar for å foregå neste grunnsteg. Kvantativne og menneskelige analyser våre viser empiriske beviser at metoden vårt fullførleg ekstraherer meaningfulle grunnlag mellom tidserien med tekstfunksjonar og gjer tilpassande forklaring mellom dei.</abstract_no>
      <abstract_sr>Objašnjavanje temeljnih uzroka ili efekta o događajima je izazovan ali vrijedan zadatak. Definiramo nov problem stvaranja objašnjenja događaja vremenskih serija (1) pretraživanjem uzroka i efekata vremenskih serija sa tekstualnim podacima i (2) izgradnjom lanca povezivanja između njih kako bi stvorili objašnjenje. Da bismo otkrili uzrokovane karakteristike teksta, predložili smo novu metodu baziranu na Grangerovoj uzrokovanosti vremenske serije između karakteristika izvučenih iz teksta poput N-grama, teme, osjećanja i njihove kompozicije. Generacija sekvence uzrokovanih entiteta zahteva zajedničku uzrokovanu bazu znanja sa efikasnim razumijevanjem. Da bismo osigurali dobru interpretabilnost i odgovarajuću leksičku upotrebu, kombinirali smo simbolične i neuralne predstave, koristeći algoritam neuralnog razuma obučen na običnim uzrokovanim tuplima da predvidimo sledeći korak uzrokovanja. Naša kvantitativna i ljudska analiza pokazuju empiričke dokaze da naša metoda uspešno izvlači značajne veze uzrokovanja između vremenskih serija sa tekstualnim karakteristikama i stvara odgovarajuće objašnjenje između njih.</abstract_sr>
      <abstract_si>අවස්ථාවය ගැන ප්‍රශ්නයක් විස්තර කරන්න ප්‍රශ්නයක් තමයි අවස්ථාවක් ගැන ප්‍රශ්නයක් විශ්ව අපි ප්‍රශ්නයක් තියෙන්නේ වෙලාව සිරීම් සිද්ධ විස්තරයක් නිර්මාණය කරන්න (1) පරීක්ෂණය සහ පරීක්ෂණය සඳහා වෙලාව සිද්ධ සිද්ධ සංවේදනය සමඟ ප ලිපිනයෙන් කාරණික අවශ්‍යතාවක් හොයාගන්න, අපි කාරණික විධානයක් අධාරිත විධානය කරන්න පුළුවන් N-ග්‍රාම්ස්, විධානය, සංවේද කාරණාත්මක විදියට පරීක්ෂණයක් අවශ්‍ය වෙනවා සාමාන්‍ය විදියට කාරණාත්මක විදියට දැනගන්න අධ්‍යය හොඳ අභිවාදය සහ හොඳ ලෙක්සිකල් භාවිතානය සම්බන්ධ කරන්න අපි සංකේතික සහ න්‍යූරාල් ප්‍රතිනිධානය සම්බන්ධ කරනවා, සාමාන්‍ය අ අපේ ප්‍රමාණය සහ මිනිස්සු විශ්ලේෂණය පෙන්වන්නේ අපේ විධානය සමහර විශේෂයෙන් අපේ විධානය සම්පූර්ණයෙන්ම විශේෂය සම්බ</abstract_si>
      <abstract_so>Caddaynta sababaha hoose ama saameyn ku saabsan dhacdooyinku waa mid adag laakiin waa shaqo muhiim ah. Waxaannu qoraynaa dhibaato saameyn ah oo ku saabsan abuurista dhacdooyinka safarka (1) oo raadinaya sabab iyo saameyn leh xiriirka tirada xiliga ah oo ku qoran taariikhda qoraalka iyo (2) dhisaya silsilad xiriir dhexdooda si uu u sameeyo fasax. Si aan u ogaano waxyaabaha sababta ah ee warqadda, waxaynu soo jeedaynaa qaab warqad ah oo ku saleysan sababaha wakhtiga badan oo u dhexeeya tababaro ka soo baxa qoraal, tusaale ahaan N-gram, madam, xisaabta iyo waxyaabaha ay leeyihiin. Qarniga waxyaabaha sababta ah waxay u baahan yihiin saldhig aqoonta sababta ah oo ku saabsan sabab faa’iido leh. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step.  Analyskayaga qiyaasta iyo dadka waxay muujiyaan caddeynta caqliga leh in qaabkanagu uu si liibaan u soo bixiyo xiriirka sababta ah ee la xiriira farsamada xilliga iyo farsamada qoriga ah dhexdooda, wuxuuna keenaa caddeyn sahlan.</abstract_so>
      <abstract_sv>Att förklara underliggande orsaker eller effekter om händelser är en utmanande men värdefull uppgift. Vi definierar ett nytt problem med att generera förklaringar av en tidsseriehändelse genom att (1) söka orsaks- och effektrelationer mellan tidsserien och textdata och (2) konstruera en förbindelsekedja mellan dem för att generera en förklaring. För att upptäcka orsakssamband från text föreslår vi en ny metod baserad på Grangers orsakssamband mellan egenskaper extraherade från text som N-gram, ämnen, känslor och deras sammansättning. Generationen av sekvensen av kausala entiteter kräver en gemensam kausativ kunskapsbas med effektivt resonemang. För att säkerställa god tolkning och lämplig lexikal användning kombinerar vi symboliska och neurala representationer, med hjälp av en neural resonansalgoritm utbildad på allmänsliga kausala tupler för att förutsäga nästa orsakssteg. Vår kvantitativa och mänskliga analys visar empiriska belägg för att vår metod framgångsrikt extraherar meningsfulla kausalitetsrelationer mellan tidsserier med textdrag och genererar lämplig förklaring mellan dem.</abstract_sv>
      <abstract_ta>நிகழ்வுகள் பற்றிய காரணங்கள் அல்லது விளைவுகளை விளக்குதல் ஒரு சவாலானது ஆனால் மதிப்புள்ள செயல். நாம் நேர தொடர் நிகழ்வின் விளக்கத்தை உருவாக்குவதற்கான புதிய பிரச்சனையை வரையறு உரையிலிருந்து காரணக் குணங்களை கண்டுபிடிக்க நாம் ஒரு புதிய முறைமையை பரிந்துரைக்கிறோம். N- grams, தலைப்புகள், உணர்வுகள், மற்றும் அமர்வு காரணங்களின் தொடர்ச்சியின் தலைமுறையில் தேவைப்படுகிறது தேவையான காரணங்களுடன் தேவைப்படுகிறது. நல்ல மொழிபெயர்ப்பு மற்றும் பொருத்தமான லெக்சிக்சியல் பயன்பாட்டை உறுதிப்படுத்த, நாம் குறியீடு மற்றும் புதிய பிரதிநிதிகளை ஒன்று சேர் எங்கள் அளவு மற்றும் மனித ஆராய்ச்சி பெரும் தெளிவான அத்தாட்சிகளை காண்பிக்கிறது நமது முறையில் வெற்றிகரமான காரணத்திற்கு இடையே நேர தொடர்</abstract_ta>
      <abstract_ur>اتفاقات کے بارے میں دلیل یا اثرات کا مفصل بیان کرنا ایک مشکل ہے لیکن ارزش کا کام ہے. ہم نے ایک نئی مسئلہ تفصیل دیتے ہیں تایمہ سیریے ایڈینٹ کی تفصیل پیدا کرنے کے ذریعہ تایمہ سیریے کے دلیل اور اثر رابطہ سے اور (2) ان کے درمیان ایک زنجیر پیدا کرنے کے لئے ایک تفصیل پیدا کرنے کے لئے۔ تاکسٹ سے کیسائل ویٹیوں کو پہچان کرنے کے لئے، ہم ایک نو طریقہ پیش کریں جو ٹیمکسٹ سے نکالا گیا ہے، ٹیمکسٹ، ٹیمکسٹ، احساسات اور ان کی ترکیب کے درمیان گرانگر کے سبب بنیاد ہے. کائنال ایٹنیٹیوں کی سفارش کی نسل ایک عام سمجھ کے باعث علم کی بنیاد ضروری ہے۔ اچھی تفسیر قابلیت اور مناسب لکسیکی استعمال کا مضبوطی کرنے کے لئے ہم سیمبولیکی اور نیورال نمونات کو ترکیب کرتے ہیں، ایک نیورال منطقی الگوریتم کے استعمال سے آپس کے دلیل قدم کی پیش بینی کرنے کے لئے تعلیم کی جاتی ہے. ہمارے مقدار اور انسان کی تحلیل مضبوط نشانیاں دکھاتے ہیں کہ ہماری طریقہ موفقیت کے ساتھ زمانہ سیریے کے درمیان معنی دلیل رابطہ نکالتا ہے اور ان کے درمیان مناسب توضیح پیدا کرتا ہے.</abstract_ur>
      <abstract_uz>@ info: whatsthis Biz bir vaqt seriya holatini (1) qidirish uchun va matn data bilan bogʻ'liq holatni aniqlashga va (2) orasidagi bir bogʻ'liq boshqaruvni yaratish uchun (1) muammolarni aniqlash. Matnning eng sabablar xossalarini aniqlash uchun biz "N-grams, madaniyalar, hissiyotlar va bir tuzuvdan chizib olingan xossalarning eng katta vaqt seriyasi darajasi asosida novel usulini anglatamiz. Bu sabablar suhbatlarining tarkibi avlodiqlarini tahlil qilish uchun faqat ma'lumot asosi kerak. Yaxshi tarjima qilishni va muhim leksikal foydalanishni ishlatish uchun biz keyingi sabablarni oldinga neyrolik va neural representalarni birlashtirish uchun neyrolik algoritni foydalanamiz. Bizning qiymatimiz va inson analytiklarimiz juda muvaffaqiyatli hujjatlarimizni ko'rsatadi. Ustunmiz muvaffaqiyatli muvaffaqiyatli vaqt seriya bilan ma'lumotlar bilan ma'noviy bogʻlanishimiz mumkin va texnologiya xususiyatlari bilan o'rganish va ularning</abstract_uz>
      <abstract_vi>Giải thích nguyên nhân hoặc ảnh hưởng của sự kiện là một nhiệm vụ khó khăn nhưng có giá trị. Chúng tôi xác định một vấn đề mới về việc giải thích một sự kiện về chuỗi thời gian bởi (1) các mối quan hệ nguyên nhân và hiệu quả của chuỗi thời gian với các dữ liệu cấu hình và (2) thiết lập một chuỗi nối giữa chúng để giải thích. Để phát hiện các tính năng hệ quả từ văn bản, chúng tôi đề xuất một phương pháp mới dựa trên sự nhân quả của chuỗi thời gian Granger giữa các tính năng được lấy ra từ văn bản như N-grams, chủ đề, tình cảm và cấu hình cùng nhau. Việc tạo ra chuỗi các sinh vật nguyên nhân đòi hỏi một căn cứ kiến thức nhân quả thông thường với một lý lẽ hiệu quả. Để đảm bảo sự giải thích tốt và sử dụng ngôn ngữ văn học thích hợp chúng ta kết hợp biểu hiện biểu tượng biểu tượng và thần kinh, sử dụng thuật to án thần kinh được đào tạo dựa trên các tuples hệ thống để dự đoán nguyên nhân tiếp theo. Phân tích con người và số lượng của chúng tôi cho thấy bằng chứng thực tế là phương pháp của chúng tôi tích cực kết hợp mối quan hệ nhân quả có ý nghĩa giữa các chuỗi thời gian với các tính chất kết cấu và giải thích thích thích thích thích thích thích thích hợp.</abstract_vi>
      <abstract_da>At forklare underliggende årsager eller virkninger om begivenheder er en udfordrende, men værdifuld opgave. Vi definerer et nyt problem med at generere forklaringer på en tidsseriebegivenhed ved at (1) søge årsag og effekt relationer af tidsserien med tekstdata og (2) konstruere en forbindelseskæde mellem dem for at generere en forklaring. For at opdage årsagsrelaterede træk fra tekst, foreslår vi en ny metode baseret på Granger årsagssammenhæng af tidsserier mellem træk udvundet fra tekst som N-gram, emner, følelser og deres sammensætning. Generationen af sekvensen af kausale enheder kræver en almindelig kausativ vidensbase med effektiv ræsonnement. For at sikre god fortolkning og passende leksikalsk brug kombinerer vi symbolske og neurale repræsentationer ved hjælp af en neural ræsonnement algoritme trænet på almindelige årsagstupler til at forudsige næste årsagstrin. Vores kvantitative og menneskelige analyse viser empirisk dokumentation for, at vores metode med succes udtrækker meningsfulde kausalitetsrelationer mellem tidsserier med teksttræk og genererer passende forklaringer mellem dem.</abstract_da>
      <abstract_hr>Objašnjavanje temeljnih uzroka ili učinka o događajima je izazovan ali vrijedan zadatak. Definiramo novi problem stvaranja objašnjenja događaja vremenskog serija (1) pretraživanjem uzroka i učinkovitih odnosa vremenskog serija s tekstualnim podacima i (2) izgrađivanjem lanca veze između njih kako bi stvorili objašnjenje. Da bismo otkrili uzrokovane karakteristike teksta, predložili smo novu metodu koja se temelji na uzrokovanju grangera vremenske serije između karakteristika izvlačenih iz teksta poput N-grama, teme, osjećanja i njihovog sastojaka. Generacija sekvence uzrokovanih entitata zahtijeva češće uzrokovanu bazu znanja s učinkovitom razumijevanjem. Da bismo osigurali dobru interpretabilnost i odgovarajuću leksičku uporabu, kombinirali smo simboličke i neuralne predstave, koristeći algoritam neuralnog razuma obučen na češće uzročne tuple kako bi predvidjeli sljedeći korak uzroka. Naša kvantitativna i ljudska analiza pokazuju empiričke dokaze da naša metoda uspješno izvlači značajne veze uzrokovanja između vremenskih serija sa tekstualnim karakteristikama i stvara odgovarajuće objašnjenje između njih.</abstract_hr>
      <abstract_nl>Het uitleggen van onderliggende oorzaken of effecten van gebeurtenissen is een uitdagende maar waardevolle taak. We definiëren een nieuw probleem van het genereren van verklaringen van een tijdreeks gebeurtenis door (1) oorzaak-effect relaties van de tijdreeks met tekstuele gegevens te zoeken en (2) een verbindingsketen tussen hen te construeren om een verklaring te genereren. Om causale kenmerken uit tekst te detecteren, stellen we een nieuwe methode voor die gebaseerd is op de Granger-causaliteit van tijdreeksen tussen kenmerken die zijn geëxtraheerd uit tekst zoals N-grammen, onderwerpen, sentimenten en hun samenstelling. Het genereren van de opeenvolging van causale entiteiten vereist een gezonde causatieve kennisbasis met efficiënte redenering. Om een goede interpreteerbaarheid en passend lexicaal gebruik te garanderen combineren we symbolische en neurale representaties, met behulp van een neuraal redeneeralgoritme getraind op gezonde causale tupels om de volgende oorzaak stap te voorspellen. Onze kwantitatieve en menselijke analyse tonen empirisch bewijs aan dat onze methode met succes betekenisvolle causaliteitsrelaties tussen tijdreeksen met tekstuele kenmerken extraheert en daartussen passende uitleg genereert.</abstract_nl>
      <abstract_bg>Обясняването на основните причини или ефекти за събитията е предизвикателна, но ценна задача. Определяме нов проблем за генериране на обяснения на събитие от времеви серии чрез (1) търсене на причинно-следствени връзки на времевите серии с текстови данни и (2) изграждане на свързваща верига между тях за генериране на обяснение. За да открием причинно-следствените особености от текста, предлагаме нов метод, базиран на причинно-следствената връзка на Грейнджър от времеви серии между особености, извлечени от текста като Н-грами, теми, сентименти и техния състав. Генерирането на последователността от причинно-следствени единици изисква обща причинно-следствена база от знания с ефективно разсъждение. За да осигурим добра интерпретация и подходящо лексикално използване, ние комбинираме символични и невронни представи, като използваме алгоритъм на невронно разсъждаване, обучен върху обикновени причинно-следствени тупли, за да предскажем следващата стъпка на причината. Нашият количествен и човешки анализ показва емпирични доказателства, че нашият метод успешно извлича значими причинно-следствени връзки между времевите серии с текстови характеристики и генерира подходящо обяснение между тях.</abstract_bg>
      <abstract_de>Ursachen oder Auswirkungen von Ereignissen zu erklären, ist eine herausfordernde, aber wertvolle Aufgabe. Wir definieren ein neuartiges Problem, Erklärungen eines Zeitreihenereignisses zu generieren, indem wir (1) Ursache-Wirkungsbeziehungen der Zeitreihen mit Textdaten suchen und (2) eine Verbindungskette zwischen ihnen konstruieren, um eine Erklärung zu generieren. Um kausale Merkmale aus Text zu erkennen, schlagen wir eine neuartige Methode vor, die auf der Granger-Kausalität von Zeitreihen zwischen Merkmalen, die aus Text extrahiert wurden, wie N-Gramm, Themen, Sentiments und deren Zusammensetzung basiert. Die Generierung der Sequenz kausaler Entitäten erfordert eine gesunde kausale Wissensbasis mit effizientem Argumentieren. Um eine gute Interpretierbarkeit und einen angemessenen lexikalischen Gebrauch zu gewährleisten, kombinieren wir symbolische und neuronale Repräsentationen, wobei ein neuronaler Argumentationsalgorithmus verwendet wird, der auf kausalen Tupeln des gesunden Menschenverstandes trainiert ist, um den nächsten Ursachenschritt vorherzusagen. Unsere quantitative und humane Analyse zeigen empirische Belege dafür, dass unsere Methode erfolgreich aussagekräftige Kausalitätsbeziehungen zwischen Zeitreihen mit textuellen Merkmalen extrahiert und entsprechende Erklärungen zwischen ihnen generiert.</abstract_de>
      <abstract_id>Menjelaskan penyebab atau efek dasar tentang peristiwa adalah tugas yang menantang tapi berharga. Kami mendefinisikan masalah baru untuk menghasilkan penjelasan dari peristiwa seri waktu dengan (1) mencari penyebab dan efek hubungan dari seri waktu dengan data tekstual dan (2) membangun rantai koneksi antara mereka untuk menghasilkan penjelasan. Untuk mendeteksi fitur penyebab dari teks, kami mengusulkan metode baru berdasarkan penyebab Granger dari seri waktu antara fitur yang diekstraksi dari teks seperti N-gram, topik, perasaan, dan komposisi mereka. Generasi urutan entitas penyebab membutuhkan dasar pengetahuan penyebab yang sama dengan alasan efisien. Untuk memastikan interpretasi yang baik dan penggunaan lexik yang sesuai kita menggabungkan representation simbolik dan saraf, menggunakan algoritma alasan saraf dilatih pada dua penyebab umum untuk memprediksi langkah penyebab berikutnya. Analisis kuantitatif dan manusia menunjukkan bukti empiris bahwa metode kita berhasil mengekstrak hubungan penyebab yang berarti antara siri waktu dengan ciri-ciri teks dan menghasilkan penjelasan yang sesuai antara mereka.</abstract_id>
      <abstract_ko>사건의 잠재적 원인이나 영향을 설명하는 것은 도전적이지만 가치 있는 임무이다.우리는 새로운 문제를 정의했다. (1) 시간 서열과 텍스트 데이터의 인과 관계를 검색하고 (2) 그들 사이에 연결 체인을 구축하여 해석을 생성하여 시간 서열 이벤트의 해석을 생성한다.텍스트에서 인과 특징을 검출하기 위해 우리는 시간 서열을 바탕으로 하는 그랜저 인과 관계를 바탕으로 하는 텍스트 특징 검출 방법을 제시했다. 이런 특징은 N-gram, 주제, 감정과 그들의 구성을 포함한다.인과 실체 서열의 생성은 효과적인 추리를 가진 상식 인과 지식 라이브러리를 필요로 한다.양호한 해석성과 적당한 어휘 사용을 확보하기 위해 우리는 기호와 신경표시를 결합시켜 상식적 인과원조를 바탕으로 하는 신경추리 알고리즘을 사용하여 다음 단계의 원인을 예측할 것이다.우리의 정량과 인위적인 분석에 의하면 우리의 방법은 텍스트의 특징을 가진 시간 서열 간의 의미 있는 인과 관계를 성공적으로 추출하고 그들 사이에 적당한 해석을 하였다.</abstract_ko>
      <abstract_fa>توضیح دلایل پایه‌ای یا اثرات در مورد رویدادها یک کار سخت اما ارزشمند است. ما مشکل نوی توضیح‌های سری زمان را توضیح می‌دهیم (۱) رابطه‌های جستجوی و تاثیر سری زمان با داده‌های متن و (۲) یک زنجیر ارتباط بین آنها برای توضیح‌گیری. برای شناسایی ویژه‌های دلایل از متن، ما یک روش رمانی را پیشنهاد می‌کنیم که بر اساس دلایل مجموعه زمان گرانگر بین ویژه‌هایی که از متن خارج شده‌اند مانند N-گرم، موضوع، احساسات و ترکیب آنها هستند. نسل تعادل موجودات باعث دلایل نیاز به پایگاه دانش باعث منطقی است. برای مطمئن کردن تعبیر قابلیت خوب و استفاده از زبان مناسب ما نمایش‌های نمایش‌شناسی و عصبی را ترکیب می‌کنیم، با استفاده از الگوریتم دلیل‌شناسی عصبی آموزش یافته شده روی تاپل‌های باعث دلیل معمولی برای پیش‌بینی قدم تحلیل مقداری و انسانی ما مدرک امپراتیک را نشان می دهند که روش ما با موفقیت رابطه‌های دلایل معنی بین سری زمان با ویژه‌های متن خارج می‌کند و توضیح مناسب بین آنها را ایجاد می‌کند.</abstract_fa>
      <abstract_af>Verduidelik onderstelde oorsaak of effekte van gebeurtenis is 'n pragtige maar waardelike taak. Ons definieer 'n nuwe probleem van genereer uitduidelings van 'n tydsreeks gebeurtenis deur (1) soektog oorsaak en effek verhouding van die tydsreeks met tekstuele data en (2) wat 'n koppeling ketting tussen hulle gebruik om 'n uitduidelik te genereer. Om oorsaaklike funksies van teks te ontdek, voorstel ons 'n roman metode gebaseer op die Granger oorsaaklikheid van tydsreeks tussen funksies uitgevoer van teks soos N-gramme, onderwerpe, sentimente en hulle komposisie. Die generasie van die volgorde van oorsaaklike entiteite benodig 'n gemeenskaplike oorsaaklike kennisbasis met effektief redening. Om goeie interpretasie en geskikte leksiese gebruik te verseker, moet ons simboliese en neurale voorstellings kombinieer, gebruik 'n neurale redening algoritme wat op gemeenskapste oorsaaklike tuples opgelei is om die volgende oorsaak stap te voorskou. Ons kvantitatiewe en menslike analisie wys empiriese getuienis dat ons metode suksesvol betekenlike oorsaaklikheidverhouding tussen tydsreeks met tekstuele funksies uittrek en gemaak toepassing tussen hulle.</abstract_af>
      <abstract_sw>Explaining underlying causes or effects about events is a challenging but valuable task.  Tunaweza kufafanua tatizo la riwaya la kutengeneza maelezo ya tukio la mfululizo wa muda na (1) kutafuta sababu na athari za mahusiano ya mfululizo wa mfululizo wa muda kwa takwimu za mfululizo na (2) kutengeneza mfumo wa kuunganisha kati yao ili kutengeneza maelezo. Ili kutambua vipengele vya sababu kutoka maandishi, tunapendekeza njia ya riwaya kwa kutumia sababu ya mfululizo wa muda mkubwa kati ya vipengele vilivyotolewa kutoka kwenye maandishi kama vile N-grams, mada, hisia na ujenzi wao. Kizazi cha mfululizo wa vifaa vinavyosababisha kinahitaji msingi wa maarifa yenye maana ya ufanisi. Ili kuhakikisha ufafanuzi mzuri na matumizi mazuri ya lexico tunaunganisha wakilishi wa alama na neura, kwa kutumia algorithi ya kisasa inayofundishwa na vifaa vya kusababisha hatua inayofuata. Uchambuzi wetu wa kiasi na binadamu unaonyesha ushahidi wa msisitizo kwamba njia yetu kwa mafanikio inaleta mahusiano ya sababu yenye maana kati ya mfululizo wa muda na tabia za msingi na inatengeneza maelezo sahihi kati yao.</abstract_sw>
      <abstract_tr>Vaqialar hakynda esasy sebäpleri ýa-da etkileri düşünmek çözümli ýöne değerli zadyr. Biz wagt seriýasynyň düşündirimlerini (1) gözlemek sebäbi we etki baglaýyşlaryny tekst verileri bilen tanyşdyrmak üçin (2) olaryň arasynda bir syýahat baglaýyşyny düzenlemek üçin tassyklandyrýarys. Metoden sebäbi karakterlerni tapmak üçin biz Granger zamanlaryň sebäbi üçin, N-gramlardan, temalardan, duýgulardan we olaryň kompozisyonundan çekilýän karakterleriň arasynda täze bir täze täze teklip berýäris. Sebäpli guramlaryň dizaýnlygynyň döwletlerine mümkin duýramçylyk sebäbi bilim bazyny etkinlik sebäbi gerek. Güzel bir yorumluluğa ve uygun bir lektik kullanımına güvenmek için simbolik ve neural temsilleri birleştirerek, sıradaki sebep adımını tahmin etmek için eğitilen neural reasoning algoritmi kullanarak Bizim miktarlarımız ve insan analizimiz empirik kanıtlarını gösteriyor ki, yöntemimiz zaman serileri metin özellikleri ile ifade eden ve aralarında uygun bir açıklama getirir.</abstract_tr>
      <abstract_sq>Shpjegimi i shkaqeve apo efekteve themelore rreth ngjarjeve është një detyrë e vështirë por e vlefshme. Ne përcaktojmë një problem të ri të gjenerimit të shpjegimeve të një ngjarje serie kohore me (1) kërkimin e lidhjeve shkak dhe efekt të serisë kohore me të dhënat tekstuale dhe (2) ndërtimin e një zinxhiri lidhjeje midis tyre për të gjeneruar një shpjegim. Për të zbuluar karakteristika shkaktuese nga teksti, propozojmë një metodë të re bazuar në shkaktueshmërinë Granger të serive kohore midis karakteristikave të nxjerra nga teksti të tilla si N-gram, temat, ndjenjat dhe përbërja e tyre. Gjenerimi i sekuencës së njësive shkakuese kërkon një bazë të përbashkët njohurie shkakuese me arsyetim të efektshëm. Për të siguruar interpretueshmërinë e mirë dhe përdorimin e duhur lexik ne kombinojmë përfaqësime simbolike dhe nervore, duke përdorur një algoritëm arsyetimi nervor të stërvitur në dubla të përbashkëta shkak për të parashikuar hapin tjetër të shkakut. Analiza jonë kuantitative dhe njerëzore tregon prova empirike se metoda jonë nxjerr me sukses marrëdhënie shkaktuese të kuptueshme midis serive kohore me karakteristika tekstuale dhe gjeneron shpjegime të përshtatshme midis tyre.</abstract_sq>
      <abstract_am>በጉዳዩ ላይ መግለጫ ወይም ማስታወቂያውን መግለጫ ግን የከበረ ነገር ነው፡፡ በ (1) የመፈለግ ምክንያት እና የጊዜውን ግንኙነት የጽሑፍ ዳታዎች እና (2) የመግለጫ ሰንሰንሰለትን በመፍጠር እና ትርጓሜን ለመፍጠር እናደርጋለን፡፡ ከጽሑፍ የሐሳብ ጥያቄዎችን ለማግኘት፣ ከ-ግራም፣ ጉዳዮች፣ እውቀት እና ክፍላቸውን በተለየ ጽሑፎች መካከል የግልፅ ጉዳይ የጊዜው ክፍተት በተመሳሳይ የመረጃ አካሄድ ልማድ እናሳውቃለን፡፡ የሐሳብ አካባቢዎች ትውልድ በጥቅም ምክንያት የውቀት አዋቂ መቀመጫ ያስፈልጋል፡፡ መልካም ትርጓሜ እና የሚገባውን የሌክሲካዊ ጥያቄ ለመጠበቅ እና የባሕላዊ መልዕክቶችን ለመቀበል፣ በአሁለተኛይቱ ጉዳይ ለመቀበል በተጠቃሚ የውይይት አካባቢ አልgoritም ለመጠቀም እናስቀናለን፡፡ ባሕላዊ እና የሰው አስተያይነታችን የሥርዓት ግንኙነትን በጽሑፍ ግንኙነት መካከል የሚያሳውቅ የሐሰት ግንኙነት እንዲያወጣ እና በመካከላቸው የሚገባውን ትርጓሜ እንዲያሳድጋል፡፡</abstract_am>
      <abstract_hy>Պատճառների վերաբերյալ հիմնական պատճառների կամ ազդեցությունների բացատրությունը դժվար, բայց արժեքավոր խնդիր է: We define a novel problem of generating explanations of a time series event by (1) searching cause and effect relationships of the time series with textual data and (2) constructing a connecting chain between them to generate an explanation.  Տեքստի պատճառային հատկանիշների հայտնաբերելու համար մենք առաջարկում ենք նոր մեթոդ, որը հիմնված է Գրենգերի ժամանակային շարքերի պատճառային հատկանիշների միջև, որոնք դուրս են գալիս տեքստից, ինչպիսիք են N-գրամերը, թեմաները, զգացմունքները և դրանց Պատճառական էակների հաջորդականության ստեղծումը պահանջում է ընդհանուր պատճառական գիտելիքների հիմք արդյունավետ մտածելով: Որպեսզի ապահովենք լավ մեկնաբանելիությունը և հարմար լեքսիկական օգտագործումը, մենք համադրում ենք սիմվոլիկ և նյարդային ներկայացումները, օգտագործելով նյարդային մտածողության ալգորիթմ, որը սովորեցվում է ընդհանուր պատճառի երկու կողմերով հաջորդ Մեր քանակական և մարդկային վերլուծությունները ցույց են տալիս էմպիրիկ ապացույցներ, որ մեր մեթոդը հաջողությամբ հանում է իմաստալից պատճառային հարաբերություններ ժամանակային շարքերի միջև տեքստային հատկությունների հետ և ստեղծում է հարմար բացատրություն դրանց</abstract_hy>
      <abstract_bn>ঘটনার ব্যাপারে কারণ বা প্রভাব ব্যাখ্যা হচ্ছে একটি চ্যালেঞ্জ কিন্তু মূল্যবান কাজ। টাইম সিরিজের অনুসন্ধানের কারণ এবং প্রভাবিত সময় সিরিজের সম্পর্কের সাথে টেক্সচুয়াল ডাটা এবং (২) তাদের মধ্যে একটি সংযোগ চেইন তৈরি করার জন্য (১) একটি ব্যাখ্য টেক্সট থেকে কারণের বৈশিষ্ট্য সনাক্ত করার জন্য আমরা প্রস্তাব করি গ্রেঞ্জারের সময় সিরিজের কারণের ভিত্তিতে গ্রেঞ্জারের সময় সিরিজের মাধ্যমে, যেমন এন-গ্রা কারণ বিষয়বস্তুর প্রজন্মের কারণে কার্যকর কারণের জন্য একটি কমিউনিসেন্সের কারণে জ্ঞানের কারণে প্রয়োজন। ভালো ব্যাখ্যা এবং যথেষ্ট লেক্সিক্যাল ব্যবহার নিশ্চিত করার জন্য আমরা প্রতীক এবং নিউরেল প্রতিনিধিদের সাথে একত্রিত করি, পরবর্তী কারণের ধাপের ভবিষ্যদ্ আমাদের পরিমাণ এবং মানুষ বিশ্লেষণ প্রমাণ দেখাচ্ছে যে আমাদের পদ্ধতি সফলভাবে সময় সিরিজের মাঝে মৌলিক ক কারণ সম্পর্ক বের করে এবং তাদের মধ্যে যুক্</abstract_bn>
      <abstract_ca>Explicar les causes o efectes subjacents dels esdeveniments és una tasca difícil però valiosa. Definim un nou problem a de generar explicacions d'un esdeveniment de sèrie temporal (1) buscant relacions de causa i efecte de la sèrie temporal amb dades textuals i (2) construint una cadena de connexió entre elles per generar una explicació. Per detectar característiques causals del text, proposem un mètode nou basat en la causalitat Granger de les sèries temporals entre característiques extraites del text com N-grams, temes, sentiments i la seva composició. La generació de seqüència d'entitats causals requereix una base comú de coneixements causals amb raonament eficient. Per assegurar una bona interpretabilitat i un bon ús lexical apropiat combinam representacions simbòlicas i neuronals, utilitzant un algoritme de raonament neural entrenat en dobles causals comuns per predir el següent pas de causa. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.</abstract_ca>
      <abstract_bs>Objašnjavanje temeljnih uzroka ili učinka o događajima je izazovan ali vrijedan zadatak. Definiramo novi problem stvaranja objašnjenja događaja vremenskih serija (1) pretraživanjem uzroka i učinkovitih odnosa vremenskih serija sa tekstualnim podacima i (2) izgrađivanjem lanca povezivanja između njih kako bi stvorili objašnjenje. Da bismo otkrili uzrokovane karakteristike teksta, predložili smo novu metodu koja se temelji na uzrokovanju Granger a vremenske serije između karakteristika izvlačenih iz teksta poput N-grama, teme, osjećanja i njihove kompozicije. Generacija sekvence uzrokovanih entiteta zahtijeva bazu povezanog smisla s efikasnim razumijevanjem. Da bismo osigurali dobru interpretabilnost i odgovarajuću leksičku uporabu, kombinirali smo simboličke i neuralne predstave, koristeći algoritam neuroralnog razuma obučen na češće uzrokovane tuple kako bi predvidjeli sljedeći korak cilja. Naša kvantitativna i ljudska analiza pokazuju empiričke dokaze da naša metoda uspješno izvlači značajne veze uzrokovanja između vremenskih serija sa tekstualnim karakteristikama i stvara odgovarajuće objašnjenje između njih.</abstract_bs>
      <abstract_az>Vaqialar bar…ôsind…ôki s…ôb…ôbl…ôri v…ô t…ôsirl…ôrini a √ßńĪq etm…ôk √ß…ôtin, amma qiym…ôtli iŇül…ôr. Biz vaxtlńĪ seri olaraq t…ôfsilatlarńĪnńĪ t…ôŇükil etm…ôk √ľ√ß√ľn yeni bir problemi t…ôŇükil edirik (1) vaxtńĪ serisi olaraq t…ôfsilatlarńĪnńĪ t…ôŇükil etm…ôk v…ô t…ôfsilatńĪnńĪ t…ôŇükil etm…ôk √ľ√ß√ľn onlarńĪn arasńĪnda bańülantńĪ zincirini t…ôŇükil etm…ôk √ľ√ß√ľn. M…ôtn t…ôs…ôvv√ľrl…ôrini keŇüfetm…ôk √ľ√ß√ľn, N-gram, m…ôs…ôl…ôl…ôr, duygularńĪ v…ô onlarńĪn kompozisyonu kimi m…ôtnl…ôrd…ôn √ßńĪxarńĪlan m…ôs…ôl…ôl…ôr arasńĪnda Granger s…ôviyy…ôsin…ô dayanan yeni bir metod t…ôklif edirik. M…ôxluqatńĪn s…ôviyy…ôtinin n…ôsili m√ľ…ôyy…ôn olunmuŇü d…ôyiŇüiklik fikirl…ôŇüdirilm…ôsi il…ô √ßoxluq t…ôsirli bir elm bazńĪ lazńĪmdńĪr. ńįyi yorumlayńĪcńĪ v…ô uyńüun leksik istifad…ô etm…ôk √ľ√ß√ľn simbolik v…ô n√∂ral g√∂st…ôricil…ôri birl…ôŇüdiririk. N√∂ral razńĪlńĪq algoritmi, t…ôhsil olunan, n√∂ral fikirl…ôŇüdirm…ôk algoritmi il…ô g…ôl…ôc…ôk s…ôb…ôb adńĪmńĪnńĪ t…ômin edir. Bizim kvantitatlńĪ v…ô insan analizimiz impirik kanńĪtlarńĪnńĪ g√∂st…ôrir ki, metodumuz m√∂vcud olaraq vaxt seril…ôri il…ô m…ôlumatlar arasńĪndakńĪ m…ôlumatlarńĪ t…ôŇükil edir v…ô onlarńĪn arasńĪnda uyńüun aydńĪnlńĪq yaradńĪr.</abstract_az>
      <abstract_et>Sündmuste põhjuste või tagajärgede selgitamine on keeruline, kuid väärtuslik ülesanne. Määratleme uue probleemi ajareasündmuse selgituste genereerimiseks (1) ajarea põhjuslike ja tagajärgede seoste otsimisel tekstiandmetega ja (2) nende vahel ühendusahela loomisel selgituse saamiseks. Tekstist põhjuslike omaduste tuvastamiseks pakume välja uudse meetodi, mis põhineb Grangeri aegridade põhjuslikul seosel tekstist ekstraheeritud omaduste vahel nagu N-grammid, teemad, tunded ja nende koostis. Põhjuslike üksuste jada tekitamine nõuab üldse mõistlikku põhjuslikku teadmistebaasi, millel on tõhus arutlus. Hea tõlgendatavuse ja sobiva leksikaalse kasutamise tagamiseks kombineerime sümboolseid ja neuraalseid esitusi, kasutades järgmise põhjuse etapi prognoosimiseks neuroarutluse algoritmi, mis on koolitatud põhjuslikel tuplitel. Meie kvantitatiivne ja inimlik analüüs näitab empiirilisi tõendeid selle kohta, et meie meetod kajastab edukalt tähenduslikke põhjuslikke seoseid aegridade vahel tekstiliste omadustega ja loob nende vahel sobiva selgituse.</abstract_et>
      <abstract_cs>Vysvětlit základní příčiny nebo účinky událostí je náročný, ale cenný úkol. Nový problém generování vysvětlení události časových řad definujeme (1) hledáním příčinových a účinkových vztahů časových řad s textovými daty a (2) konstruováním spojovacího řetězce mezi nimi pro generování vysvětlení. Pro detekci kauzálních rysů z textu navrhujeme novou metodu založenou na Grangerově kauzalitě časových řad mezi rysy extrahovanými z textu, jako jsou N-gramy, témata, sentimenty a jejich složení. Generování sekvence kauzálních entit vyžaduje zdravý rozumný kauzální znalostní základ s efektivním uvažováním. Pro zajištění dobré interpretovatelnosti a vhodného lexikálního využití kombinujeme symbolické a nervové reprezentace pomocí algoritmu neuronového uvažování trénovaného na zdravém rozumu kauzálních tulících k předpovědi dalšího kroku příčiny. Naše kvantitativní a lidská analýza ukazuje empirický důkaz, že naše metoda úspěšně extrahuje smysluplné kauzalitní vztahy mezi časovými řadami s textovými rysy a generuje mezi nimi vhodné vysvětlení.</abstract_cs>
      <abstract_fi>Tapahtuman taustalla olevien syiden tai vaikutusten selittäminen on haastava mutta arvokas tehtävä. Määrittelemme uuden ongelman aikasarjatapahtuman selittämisestä (1) etsimällä aikasarjojen syy-seuraussuhteita tekstitiedoilla ja (2) rakentamalla niiden välille yhdysketjun selityksen tuottamiseksi. Tekstin kausaalisten ominaisuuksien havaitsemiseksi ehdotamme uutta menetelmää, joka perustuu Grangerin kausaalisuuteen tekstistä poimittujen ominaisuuksien, kuten N-grammien, aiheiden, tunteiden ja niiden koostumuksen välillä. Syyllisten entiteettien sekvenssin luominen edellyttää järkevää kausaalista tietopohjaa tehokkaalla päättelyllä. Varmistaaksemme hyvän tulkittavuuden ja asianmukaisen sanaston käytön yhdistämme symbolisia ja neuroesityksiä käyttämällä järjetöntä kausaalituplia koulutettua hermopäättelyalgoritmia seuraavan syyn ennustamiseksi. Kvantitatiiviset ja inhimilliset analyysimme osoittavat empiiristä näyttöä siitä, että menetelmämme onnistuneesti poimii merkityksellisiä kausaalisuussuhteita aikasarjojen välillä tekstuaalisilla ominaisuuksilla ja tuottaa asianmukaisen selityksen niiden välille.</abstract_fi>
      <abstract_jv>string" in "context_BAR_stringLink Awak dhéwé éngawe pernik nganggo pernik nggawe kapan tanggal nggawe pernik (1) nggawe pernik nggawe pernik nggawe pernik nggawe pernik wektu nggawe data nggambar textual lan (2) nggawe dolanan nambah kebebasan dumateng sampek kanggo ngilanggar kebebasan Jejaring Genjer Jejaring Awakdhéwé kantarno karo hal-hal nganggo urip kuwi nggawe perusahaan empirhik punika dipuluhayo nggawe nguasai perusahaan seneng pisan kelangan anyar nggawe nguasai winih lan nganggo perusahaan anyar nggawe nguasai winih dhéwé</abstract_jv>
      <abstract_he>להסביר סיבות או השפעות בסיסיות על אירועים היא משימה מאתגרת אבל יקרה. אנחנו מגדירים בעיה חדשה של ליצור הסברים של אירוע סדרת הזמן על ידי (1) חיפוש סיבה ולהשפעה מערכות יחסים של סדרת הזמן עם נתונים טקסטיים (2) בניית שרשרת חיבור ביניהם כדי ליצור הסבר. כדי לגלות תכונות סיבות מהטקסט, אנו מציעים שיטה חדשה מבוססת על סיבות גראנג'ר של סדרת הזמן בין תכונות שמוצאות מהטקסט כמו N-גרם, נושאים, רגשות, והמייצב שלהם. דור הרצף של יחידות סיבות דורש בסיס ידע סיבותי משותף עם סיבה יעילה. כדי להבטיח אפשרות לפרשנות טובה ושימוש לקסי מתאים אנחנו משלבים מייצגים סמליים ועצביים, באמצעות אלגוריתם הגיון העצבי מאומן על כפולות סיבות משותפות כדי לחזות את צעד הסיבה הבא. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.</abstract_he>
      <abstract_ha>Yi bayyana bayani ga masu ƙari ko matsayin ayuka masu husũma masu hushi ne wani mai tsõron aiki kuma amma yana da kima. Tuna bayyana wani matsayi na nowaya da za'a ƙiƙira fassarar birnin da (1) ke nuna saba'a da mazaɓa da mazaɓa da danne-danne na littãfi da (2) za'a samar da wani ƙure mai haɗi a tsakanin su dõmin ya danne wani fassarar. To domin gane masu sabawa daga matsayin, za'a buƙata wata hanyoyi a kan hanyarwa na Gabbar da yawan shawarar lokaci tsakanin da aka samu masu tsari daga matsayin, kamar N-gram, madaidaita, da hisori da samun sunan. Umarnin da ke ƙaranci da ma'anar sabon, yana nufin wani ma'abũcin sanitar da ma'abũcin sanyi mai kamid. To, dõmin ka yi tabbatar da fassarar mai kyau da amfani da lissafi masu daidai, za mu koma da misalin alama da neura, idan an yi amfani da algoritm na rubura wanda aka sanar da shi a kan manyan shawarar da mataimaki na dabam. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.</abstract_ha>
      <abstract_sk>Razlaganje osnovnih vzrokov ali učinkov dogodkov je zahtevna, vendar dragocena naloga. Nov problem generiranja razlag dogodka časovne serije definiramo z (1) iskanjem vzročno-posledičnih odnosov časovne serije z besedilnimi podatki in (2) konstruiranjem povezovalne verige med njimi za ustvarjanje razlage. Za odkrivanje vzročnih značilnosti iz besedila predlagamo novo metodo, ki temelji na Grangerjevi vzročni zvezi časovnih vrst med značilnostmi, pridobljenimi iz besedila, kot so N-grami, teme, čustva in njihova sestava. Ustvarjanje zaporedja vzročnih entitet zahteva splošno vzročno bazo znanja z učinkovitim razlogom. Za zagotovitev dobre interpretabilnosti in ustrezne leksikalne uporabe združujemo simbolne in nevralne reprezentacije z uporabo algoritma nevralnega razmišljanja, usposobljenega na splošnih vzročnih tuplicah, da napovedamo naslednji korak vzroka. Naša kvantitativna in človeška analiza kaže empirične dokaze, da naša metoda uspešno izvleče smiselne vzročne povezave med časovnimi vrstami z besedilnimi značilnostmi in ustvari ustrezno razlago med njimi.</abstract_sk>
      <abstract_bo>བྱ་འགུལ་གྱི་གནད་དོན་གཞི་ལུགས་ཀྱི་རྒྱུ་མཚན་དང་ནུས་པ་ཚོར་འགྲེལ་བཤད་ནི་གནད ང་ཚོས་དུས་ཚོད་ལྟ་བུའི་འགྲེལ་བཤད་ཀྱི་དཀའ་ངལ ཚིག་ཡིག་ནས་རྒྱུ་རྐྱེན་གྱི་ཆོས་ཉིད་དེ་རྟོགས་དགོས་པ་ལས་ འུ་ཅག་གིས་ཚིག་ཡིག་གི་རྒྱུ་རྐྱེན་དང་། Granger་རྐྱེན་ཐོག་དང་། ཆོས་ཉིད་དེ་ཚོའི་ནང རྒྱུ་མཚན་དབྱེ་བ་ཚུགས་ཀྱི་གོ་རྗེས་ལྟར་མཚམས་འཇོག་ནུས་མཐུན་རྐྱེན་གྱིས་མཐུན་རྐྱེན་གྱི་གནས་ཚུལ་གཞི་གཅིག་དང To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step. ང་ཚོའི་གྲངས་འབོར་དང་མིའི་དབྱེ་ཞིབ</abstract_bo>
      </paper>
    <paper id="293">
      <title>A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential Data of MOOC<fixed-case>MOOC</fixed-case></title>
      <author><first>Zhuoxuan</first> <last>Jiang</last></author>
      <author><first>Shanshan</first> <last>Feng</last></author>
      <author><first>Gao</first> <last>Cong</last></author>
      <author><first>Chunyan</first> <last>Miao</last></author>
      <author><first>Xiaoming</first> <last>Li</last></author>
      <pages>2768–2773</pages>
      <url hash="40bf7cf7">D17-1293</url>
      <doi>10.18653/v1/D17-1293</doi>
      <abstract>Recent years have witnessed the proliferation of Massive Open Online Courses (MOOCs). With massive learners being offered <a href="https://en.wikipedia.org/wiki/Massive_open_online_course">MOOCs</a>, there is a demand that the forum contents within <a href="https://en.wikipedia.org/wiki/Massive_open_online_course">MOOCs</a> need to be classified in order to facilitate both learners and instructors. Therefore we investigate a significant <a href="https://en.wikipedia.org/wiki/Application_software">application</a>, which is to associate <a href="https://en.wikipedia.org/wiki/Internet_forum">forum threads</a> to subtitles of video clips. This task can be regarded as a document ranking problem, and the key is how to learn a distinguishable text representation from word sequences and learners’ behavior sequences. In this paper, we propose a novel cascade model, which can capture both the latent semantics and latent similarity by modeling <a href="https://en.wikipedia.org/wiki/Massive_open_online_course">MOOC data</a>. Experimental results on two real-world datasets demonstrate that our textual representation outperforms state-of-the-art unsupervised counterparts for the application.</abstract>
      <bibkey>jiang-etal-2017-novel</bibkey>
    </paper>
    <paper id="294">
      <title>Identifying the Provision of Choices in Privacy Policy Text</title>
      <author><first>Kanthashree</first> <last>Mysore Sathyendra</last></author>
      <author><first>Shomir</first> <last>Wilson</last></author>
      <author><first>Florian</first> <last>Schaub</last></author>
      <author><first>Sebastian</first> <last>Zimmeck</last></author>
      <author><first>Norman</first> <last>Sadeh</last></author>
      <pages>2774–2779</pages>
      <url hash="25a55f42">D17-1294</url>
      <doi>10.18653/v1/D17-1294</doi>
      <attachment type="attachment" hash="5fd9101c">D17-1294.Attachment.zip</attachment>
      <abstract>Websites’ and mobile apps’ privacy policies, written in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>, tend to be long and difficult to understand. Information privacy revolves around the fundamental principle of Notice and choice, namely the idea that users should be able to make informed decisions about what information about them can be collected and how it can be used. Internet users want control over their privacy, but their choices are often hidden in long and convoluted privacy policy texts. Moreover, little (if any) prior work has been done to detect the provision of choices in text. We address this challenge of enabling user choice by automatically identifying and extracting pertinent <a href="https://en.wikipedia.org/wiki/Choice_language">choice language</a> in <a href="https://en.wikipedia.org/wiki/Privacy_policy">privacy policies</a>. In particular, we present a two-stage architecture of classification models to identify opt-out choices in privacy policy text, labelling common varieties of choices with a mean F1 score of 0.735. Our techniques enable the creation of systems to help Internet users to learn about their choices, thereby effectuating notice and choice and improving <a href="https://en.wikipedia.org/wiki/Internet_privacy">Internet privacy</a>.</abstract>
      <bibkey>mysore-sathyendra-etal-2017-identifying</bibkey>
    </paper>
    <paper id="295">
      <title>An Empirical Analysis of Edit Importance between Document Versions</title>
      <author><first>Tanya</first> <last>Goyal</last></author>
      <author><first>Sachin</first> <last>Kelkar</last></author>
      <author><first>Manas</first> <last>Agarwal</last></author>
      <author><first>Jeenu</first> <last>Grover</last></author>
      <pages>2780–2784</pages>
      <url hash="21b0b364">D17-1295</url>
      <doi>10.18653/v1/D17-1295</doi>
      <abstract>In this paper, we present a novel approach to infer significance of various textual edits to documents. An author may make several edits to a document ; each edit varies in its impact to the content of the document. While some edits are surface changes and introduce negligible change, other edits may change the content / tone of the document significantly. In this paper, we perform an analysis on the human perceptions of edit importance while reviewing documents from one version to the next. We identify <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">linguistic features</a> that influence edit importance and model it in a <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression based setting</a>. We show that the predicted importance by our approach is highly correlated with the human perceived importance, established by a Mechanical Turk study.</abstract>
      <bibkey>goyal-etal-2017-empirical</bibkey>
    </paper>
    <paper id="296">
      <title>Transition-Based Disfluency Detection using <a href="https://en.wikipedia.org/wiki/Linear_time-invariant_system">LSTMs</a><fixed-case>LSTM</fixed-case>s</title>
      <author><first>Shaolei</first> <last>Wang</last></author>
      <author><first>Wanxiang</first> <last>Che</last></author>
      <author><first>Yue</first> <last>Zhang</last></author>
      <author><first>Meishan</first> <last>Zhang</last></author>
      <author><first>Ting</first> <last>Liu</last></author>
      <pages>2785–2794</pages>
      <url hash="994b24e2">D17-1296</url>
      <doi>10.18653/v1/D17-1296</doi>
      <abstract>In this paper, we model the problem of disfluency detection using a transition-based framework, which incrementally constructs and labels the disfluency chunk of input sentences using a new transition system without syntax information. Compared with sequence labeling methods, it can capture non-local chunk-level features ; compared with joint parsing and disfluency detection methods, it is free for noise in syntax. Experiments show that our <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> achieves state-of-the-art <a href="https://en.wikipedia.org/wiki/F-score">f-score</a> of 87.5 % on the commonly used English Switchboard test set, and a set of in-house annotated Chinese data.</abstract>
      <bibkey>wang-etal-2017-transition</bibkey>
      <pwccode url="https://github.com/hitwsl/transition_disfluency" additional="false">hitwsl/transition_disfluency</pwccode>
    </paper>
    <paper id="299">
      <title>A Study of Style in <a href="https://en.wikipedia.org/wiki/Machine_translation">Machine Translation</a> : Controlling the Formality of Machine Translation Output</title>
      <author><first>Xing</first> <last>Niu</last></author>
      <author><first>Marianna</first> <last>Martindale</last></author>
      <author><first>Marine</first> <last>Carpuat</last></author>
      <pages>2814–2819</pages>
      <url hash="5e4dbe7f">D17-1299</url>
      <doi>10.18653/v1/D17-1299</doi>
      <abstract>Stylistic variations of language, such as <a href="https://en.wikipedia.org/wiki/Formality">formality</a>, carry speakers’ intention beyond literal meaning and should be conveyed adequately in <a href="https://en.wikipedia.org/wiki/Translation">translation</a>. We propose to use lexical formality models to control the formality level of machine translation output. We demonstrate the effectiveness of our approach in empirical evaluations, as measured by <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">automatic metrics</a> and <a href="https://en.wikipedia.org/wiki/Human_factors_and_ergonomics">human assessments</a>.</abstract>
      <video href="https://vimeo.com/238231330" />
      <bibkey>niu-etal-2017-study</bibkey>
    </paper>
    <paper id="300">
      <title>Sharp Models on Dull Hardware : Fast and Accurate Neural Machine Translation Decoding on the <a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a><fixed-case>CPU</fixed-case></title>
      <author><first>Jacob</first> <last>Devlin</last></author>
      <pages>2820–2825</pages>
      <url hash="635fe82e">D17-1300</url>
      <doi>10.18653/v1/D17-1300</doi>
      <attachment type="attachment" hash="e9003edf">D17-1300.Attachment.zip</attachment>
      <abstract>Attentional sequence-to-sequence models have become the new standard for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, but one challenge of such <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> is a significant increase in training and decoding cost compared to phrase-based systems. In this work we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation (NMT), while achieving CPU decoding speed / throughput close to that of a phrasal decoder. We approach this problem from two angles : First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful <a href="https://en.wikipedia.org/wiki/Network_architecture">network architecture</a> which uses an RNN (GRU / LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This architecture achieves similar <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> to a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep recurrent model</a>, at a small fraction of the <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training and decoding cost</a>. By combining these techniques, our best <a href="https://en.wikipedia.org/wiki/System">system</a> achieves a very competitive <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 38.3 BLEU on WMT English-French NewsTest2014, while decoding at 100 words / sec on <a href="https://en.wikipedia.org/wiki/Thread_(computing)">single-threaded CPU</a>. We believe this is the best published accuracy / speed trade-off of an NMT system.</abstract>
      <video href="https://vimeo.com/238235696" />
      <bibkey>devlin-2017-sharp</bibkey>
    </paper>
    <paper id="302">
      <title>Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources<fixed-case>POS</fixed-case> Tagging without Cross-Lingual Resources</title>
      <author><first>Joo-Kyung</first> <last>Kim</last></author>
      <author><first>Young-Bum</first> <last>Kim</last></author>
      <author><first>Ruhi</first> <last>Sarikaya</last></author>
      <author><first>Eric</first> <last>Fosler-Lussier</last></author>
      <pages>2832–2838</pages>
      <url hash="9a5b9637">D17-1302</url>
      <doi>10.18653/v1/D17-1302</doi>
      <abstract>Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language. In this paper, we introduce a cross-lingual transfer learning model for <a href="https://en.wikipedia.org/wiki/Tag_(metadata)">POS tagging</a> without ancillary resources such as <a href="https://en.wikipedia.org/wiki/Parallel_corpora">parallel corpora</a>. The proposed cross-lingual model utilizes a common BLSTM that enables <a href="https://en.wikipedia.org/wiki/Knowledge_transfer">knowledge transfer</a> from other languages, and private BLSTMs for language-specific representations. The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language. Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.</abstract>
      <video href="https://vimeo.com/238231953" />
      <bibkey>kim-etal-2017-cross</bibkey>
    </paper>
    <paper id="303">
      <title>Image Pivoting for Learning Multilingual Multimodal Representations</title>
      <author><first>Spandana</first> <last>Gella</last></author>
      <author><first>Rico</first> <last>Sennrich</last></author>
      <author><first>Frank</first> <last>Keller</last></author>
      <author><first>Mirella</first> <last>Lapata</last></author>
      <pages>2839–2845</pages>
      <url hash="b9169a58">D17-1303</url>
      <doi>10.18653/v1/D17-1303</doi>
      <abstract>In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of <a href="https://en.wikipedia.org/wiki/Image_retrieval">image search</a> and <a href="https://en.wikipedia.org/wiki/Computer_vision">image understanding</a>. Our model learns a common <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a> for images and their descriptions in two different languages (which need not be parallel) by considering the <a href="https://en.wikipedia.org/wiki/Image">image</a> as a pivot between two languages. We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for <a href="https://en.wikipedia.org/wiki/German_language">German</a> and <a href="https://en.wikipedia.org/wiki/English_language">English</a>, and on semantic textual similarity of image descriptions in <a href="https://en.wikipedia.org/wiki/English_language">English</a>. In both cases we achieve state-of-the-art performance.</abstract>
      <video href="https://vimeo.com/238233708" />
      <bibkey>gella-etal-2017-image</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-2014">STS 2014</pwcdataset>
    </paper>
    <paper id="304">
      <title>Neural Machine Translation with Source Dependency Representation</title>
      <author><first>Kehai</first> <last>Chen</last></author>
      <author><first>Rui</first> <last>Wang</last></author>
      <author><first>Masao</first> <last>Utiyama</last></author>
      <author><first>Lemao</first> <last>Liu</last></author>
      <author><first>Akihiro</first> <last>Tamura</last></author>
      <author><first>Eiichiro</first> <last>Sumita</last></author>
      <author><first>Tiejun</first> <last>Zhao</last></author>
      <pages>2846–2852</pages>
      <url hash="ad81e73e">D17-1304</url>
      <doi>10.18653/v1/D17-1304</doi>
      <abstract>Source dependency information has been successfully introduced into <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation">statistical machine translation</a>. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel NMT with source dependency representation to improve translation performance of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NMT</a>, especially long sentences. Empirical results on NIST Chinese-to-English translation task show that our <a href="https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations">method</a> achieves 1.6 BLEU improvements on average over a strong <a href="https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations">NMT system</a>.</abstract>
      <video href="https://vimeo.com/238234744" />
      <bibkey>chen-etal-2017-neural</bibkey>
    </paper>
    <paper id="306">
      <title>Sequence Effects in Crowdsourced Annotations</title>
      <author><first>Nitika</first> <last>Mathur</last></author>
      <author><first>Timothy</first> <last>Baldwin</last></author>
      <author><first>Trevor</first> <last>Cohn</last></author>
      <pages>2860–2865</pages>
      <url hash="adb4e529">D17-1306</url>
      <doi>10.18653/v1/D17-1306</doi>
      <abstract>Manual data annotation is a vital component of <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP research</a>. When designing <a href="https://en.wikipedia.org/wiki/Annotation">annotation tasks</a>, properties of the annotation interface can unintentionally lead to artefacts in the resulting dataset, biasing the evaluation. In this paper, we explore sequence effects where <a href="https://en.wikipedia.org/wiki/Annotation">annotations</a> of an item are affected by the preceding items. Having assigned one label to an instance, the annotator may be less (or more) likely to assign the same label to the next. During rating tasks, seeing a low quality item may affect the score given to the next item either positively or negatively. We see clear evidence of both types of effects using auto-correlation studies over three different crowdsourced datasets. We then recommend a simple way to minimise sequence effects.</abstract>
      <video href="https://vimeo.com/238235659" />
      <bibkey>mathur-etal-2017-sequence</bibkey>
    </paper>
    <paper id="308">
      <title>The strange geometry of <a href="https://en.wikipedia.org/wiki/Skip-gram">skip-gram</a> with negative sampling</title>
      <author><first>David</first> <last>Mimno</last></author>
      <author><first>Laure</first> <last>Thompson</last></author>
      <pages>2873–2878</pages>
      <url hash="0453f394">D17-1308</url>
      <doi>10.18653/v1/D17-1308</doi>
      <abstract>Despite their ubiquity, <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> trained with skip-gram negative sampling (SGNS) remain poorly understood. We find that vector positions are not simply determined by <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a>, but rather occupy a narrow cone, diametrically opposed to the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context vectors</a>. We show that this geometric concentration depends on the ratio of positive to negative examples, and that it is neither theoretically nor empirically inherent in related embedding algorithms.</abstract>
      <video href="https://vimeo.com/238236230" />
      <bibkey>mimno-thompson-2017-strange</bibkey>
    </paper>
    <paper id="309">
      <title>Natural Language Processing with Small Feed-Forward Networks</title>
      <author><first>Jan A.</first> <last>Botha</last></author>
      <author><first>Emily</first> <last>Pitler</last></author>
      <author><first>Ji</first> <last>Ma</last></author>
      <author><first>Anton</first> <last>Bakalov</last></author>
      <author><first>Alex</first> <last>Salcianu</last></author>
      <author><first>David</first> <last>Weiss</last></author>
      <author><first>Ryan</first> <last>McDonald</last></author>
      <author><first>Slav</first> <last>Petrov</last></author>
      <pages>2879–2885</pages>
      <url hash="e87510f4">D17-1309</url>
      <doi>10.18653/v1/D17-1309</doi>
      <attachment type="attachment" hash="7a21c2e6">D17-1309.Attachment.zip</attachment>
      <abstract>We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like <a href="https://en.wikipedia.org/wiki/Mobile_phone">mobile phones</a>, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.</abstract>
      <video href="https://vimeo.com/238234701" />
      <bibkey>botha-etal-2017-natural</bibkey>
    </paper>
    <paper id="311">
      <title>Analogs of Linguistic Structure in Deep Representations</title>
      <author><first>Jacob</first> <last>Andreas</last></author>
      <author><first>Dan</first> <last>Klein</last></author>
      <pages>2893–2897</pages>
      <url hash="6f083f22">D17-1311</url>
      <doi>10.18653/v1/D17-1311</doi>
      <abstract>We investigate the compositional structure of message vectors computed by a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep network</a> trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to <a href="https://en.wikipedia.org/wiki/Negation">negation</a>, <a href="https://en.wikipedia.org/wiki/Logical_conjunction">conjunction</a>, and <a href="https://en.wikipedia.org/wiki/Logical_disjunction">disjunction</a>. Our results suggest that neural representations are capable of spontaneously developing a <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a> with functional analogues to qualitative properties of <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>.</abstract>
      <video href="https://vimeo.com/238231647" />
      <bibkey>andreas-klein-2017-analogs</bibkey>
      <pwccode url="https://github.com/jacobandreas/rnn-syn" additional="true">jacobandreas/rnn-syn</pwccode>
    </paper>
    <paper id="312">
      <title>A Simple <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">Regularization-based Algorithm</a> for Learning Cross-Domain Word Embeddings</title>
      <author><first>Wei</first> <last>Yang</last></author>
      <author><first>Wei</first> <last>Lu</last></author>
      <author><first>Vincent</first> <last>Zheng</last></author>
      <pages>2898–2904</pages>
      <url hash="8b7319d3">D17-1312</url>
      <doi>10.18653/v1/D17-1312</doi>
      <abstract>Learning word embeddings has received a significant amount of attention recently. Often, <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> are learned in an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised manner</a> from a large collection of text. The genre of the text typically plays an important role in the effectiveness of the resulting <a href="https://en.wikipedia.org/wiki/Embedding">embeddings</a>. How to effectively train word embedding models using data from different domains remains a problem that is less explored. In this paper, we present a simple yet effective method for learning <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> based on text from different domains. We demonstrate the effectiveness of our approach through extensive experiments on various down-stream NLP tasks.</abstract>
      <video href="https://vimeo.com/238231213" />
      <bibkey>yang-etal-2017-simple</bibkey>
    </paper>
    <paper id="313">
      <title>Learning what to read : Focused machine reading</title>
      <author><first>Enrique</first> <last>Noriega-Atala</last></author>
      <author><first>Marco A.</first> <last>Valenzuela-Escárcega</last></author>
      <author><first>Clayton</first> <last>Morrison</last></author>
      <author><first>Mihai</first> <last>Surdeanu</last></author>
      <pages>2905–2910</pages>
      <url hash="9644fd1f">D17-1313</url>
      <doi>10.18653/v1/D17-1313</doi>
      <abstract>Recent efforts in <a href="https://en.wikipedia.org/wiki/Bioinformatics">bioinformatics</a> have achieved tremendous progress in the machine reading of biomedical literature, and the assembly of the extracted biochemical interactions into large-scale models such as protein signaling pathways. However, batch machine reading of literature at today’s scale (PubMed alone indexes over 1 million papers per year) is unfeasible due to both cost and <a href="https://en.wikipedia.org/wiki/Overhead_(computing)">processing overhead</a>. In this work, we introduce a focused reading approach to guide the machine reading of biomedical literature towards what literature should be read to answer a biomedical query as efficiently as possible. We introduce a family of <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> for focused reading, including an intuitive, strong baseline, and a second approach which uses a reinforcement learning (RL) framework that learns when to explore (widen the search) or exploit (narrow it). We demonstrate that the RL approach is capable of answering more queries than the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>, while being more efficient, i.e., reading fewer documents.</abstract>
      <bibkey>noriega-atala-etal-2017-learning</bibkey>
    </paper>
    <paper id="314">
      <title>DOC : Deep Open Classification of Text Documents<fixed-case>DOC</fixed-case>: Deep Open Classification of Text Documents</title>
      <author><first>Lei</first> <last>Shu</last></author>
      <author><first>Hu</first> <last>Xu</last></author>
      <author><first>Bing</first> <last>Liu</last></author>
      <pages>2911–2916</pages>
      <url hash="a9953e26">D17-1314</url>
      <doi>10.18653/v1/D17-1314</doi>
      <abstract>Traditional <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> makes the <a href="https://en.wikipedia.org/wiki/Closed-world_assumption">closed-world assumption</a> that the classes appeared in the test data must have appeared in training. This also applies to text learning or <a href="https://en.wikipedia.org/wiki/Text_classification">text classification</a>. As learning is used increasingly in dynamic open environments where some new / test documents may not belong to any of the training classes, identifying these novel documents during <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a> presents an important problem. This <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.</abstract>
      <video href="https://vimeo.com/238232487" />
      <bibkey>shu-etal-2017-doc</bibkey>
    </paper>
    <paper id="315">
      <title>Charmanteau : Character Embedding Models For Portmanteau Creation<fixed-case>C</fixed-case>harmanteau: Character Embedding Models For Portmanteau Creation</title>
      <author><first>Varun</first> <last>Gangal</last></author>
      <author><first>Harsh</first> <last>Jhamtani</last></author>
      <author><first>Graham</first> <last>Neubig</last></author>
      <author><first>Eduard</first> <last>Hovy</last></author>
      <author><first>Eric</first> <last>Nyberg</last></author>
      <pages>2917–2922</pages>
      <url hash="9e451c92">D17-1315</url>
      <doi>10.18653/v1/D17-1315</doi>
      <attachment type="attachment" hash="382f55ee">D17-1315.Attachment.zip</attachment>
      <abstract>Portmanteaus are a word formation phenomenon where two words combine into a new word. We propose character-level neural sequence-to-sequence (S2S) methods for the task of portmanteau generation that are end-to-end-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channel-style model, which allows for the incorporation of unsupervised word lists, improving performance over a standard source-to-target model. This <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.</abstract>
      <video href="https://vimeo.com/238231770" />
      <bibkey>gangal-etal-2017-charmanteau</bibkey>
      <pwccode url="https://github.com/vgtomahawk/Charmanteau-CamReady" additional="false">vgtomahawk/Charmanteau-CamReady</pwccode>
    </paper>
    <paper id="316">
      <title>Using Automated Metaphor Identification to Aid in Detection and Prediction of First-Episode Schizophrenia</title>
      <author><first>E. Darío</first> <last>Gutiérrez</last></author>
      <author><first>Guillermo</first> <last>Cecchi</last></author>
      <author><first>Cheryl</first> <last>Corcoran</last></author>
      <author><first>Philip</first> <last>Corlett</last></author>
      <pages>2923–2930</pages>
      <url hash="c90ca03e">D17-1316</url>
      <doi>10.18653/v1/D17-1316</doi>
      <abstract>The diagnosis of serious mental health conditions such as <a href="https://en.wikipedia.org/wiki/Schizophrenia">schizophrenia</a> is based on the judgment of clinicians whose training takes several years, and can not be easily formalized into objective measures. However, previous research suggests there are disturbances in aspects of the language use of patients with schizophrenia. Using metaphor-identification and sentiment-analysis algorithms to automatically generate <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>, we create a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a>, that, with high accuracy, can predict which patients will develop (or currently suffer from) <a href="https://en.wikipedia.org/wiki/Schizophrenia">schizophrenia</a>. To our knowledge, this study is the first to demonstrate the utility of automated metaphor identification algorithms for detection or prediction of disease.</abstract>
      <video href="https://vimeo.com/238236651" />
      <bibkey>gutierrez-etal-2017-using</bibkey>
    </paper>
    <paper id="317">
      <title>Truth of Varying Shades : Analyzing Language in Fake News and Political Fact-Checking</title>
      <author><first>Hannah</first> <last>Rashkin</last></author>
      <author><first>Eunsol</first> <last>Choi</last></author>
      <author><first>Jin Yea</first> <last>Jang</last></author>
      <author><first>Svitlana</first> <last>Volkova</last></author>
      <author><first>Yejin</first> <last>Choi</last></author>
      <pages>2931–2937</pages>
      <url hash="2f684c63">D17-1317</url>
      <doi>10.18653/v1/D17-1317</doi>
      <abstract>We present an analytic study on the language of news media in the context of <a href="https://en.wikipedia.org/wiki/Fact-checking">political fact-checking</a> and fake news detection. We compare the language of real news with that of <a href="https://en.wikipedia.org/wiki/Satire">satire</a>, <a href="https://en.wikipedia.org/wiki/Hoax">hoaxes</a>, and <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a> to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a <a href="https://en.wikipedia.org/wiki/Case_study">case study</a> based on <a href="https://en.wikipedia.org/wiki/PolitiFact">PolitiFact.com</a> using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.</abstract>
      <video href="https://vimeo.com/238236521" />
      <bibkey>rashkin-etal-2017-truth</bibkey>
    </paper>
    <paper id="318">
      <title>Topic-Based Agreement and Disagreement in <a href="https://en.wikipedia.org/wiki/United_States_Electoral_College">US Electoral Manifestos</a><fixed-case>US</fixed-case> Electoral Manifestos</title>
      <author><first>Stefano</first> <last>Menini</last></author>
      <author><first>Federico</first> <last>Nanni</last></author>
      <author><first>Simone Paolo</first> <last>Ponzetto</last></author>
      <author><first>Sara</first> <last>Tonelli</last></author>
      <pages>2938–2944</pages>
      <url hash="f996fb87">D17-1318</url>
      <doi>10.18653/v1/D17-1318</doi>
      <abstract>We present a topic-based analysis of agreement and disagreement in <a href="https://en.wikipedia.org/wiki/Manifesto">political manifestos</a>, which relies on a new method for topic detection based on key concept clustering. Our approach outperforms both standard techniques like LDA and a state-of-the-art graph-based method, and provides promising initial results for this new task in <a href="https://en.wikipedia.org/wiki/Computational_social_science">computational social science</a>.</abstract>
      <video href="https://vimeo.com/238236263" />
      <bibkey>menini-etal-2017-topic</bibkey>
    </paper>
    <paper id="319">
      <title>Zipporah : a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora<fixed-case>Z</fixed-case>ipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora</title>
      <author><first>Hainan</first> <last>Xu</last></author>
      <author><first>Philipp</first> <last>Koehn</last></author>
      <pages>2945–2950</pages>
      <url hash="20eebbaa">D17-1319</url>
      <doi>10.18653/v1/D17-1319</doi>
      <abstract>We introduce <a href="https://en.wikipedia.org/wiki/Zipporah">Zipporah</a>, a fast and scalable data cleaning system. We propose a novel type of bag-of-words translation feature, and train <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression models</a> to classify good data and synthetic noisy data in the proposed <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">feature space</a>. The trained <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> is used to score parallel sentences in the data pool for <a href="https://en.wikipedia.org/wiki/Selection_bias">selection</a>. As shown in experiments, <a href="https://en.wikipedia.org/wiki/Zipporah">Zipporah</a> selects a high-quality parallel corpus from a large, mixed quality data pool. In particular, for one <a href="https://en.wikipedia.org/wiki/Noise_(signal_processing)">noisy dataset</a>, <a href="https://en.wikipedia.org/wiki/Zipporah">Zipporah</a> achieves a 2.1 BLEU score improvement with using 1/5 of the data over using the entire corpus.</abstract>
      <video href="https://vimeo.com/238236824" />
      <bibkey>xu-koehn-2017-zipporah</bibkey>
    </paper>
    <paper id="320">
      <title>Bringing Structure into Summaries : Crowdsourcing a Benchmark Corpus of Concept Maps</title>
      <author><first>Tobias</first> <last>Falke</last></author>
      <author><first>Iryna</first> <last>Gurevych</last></author>
      <pages>2951–2961</pages>
      <url hash="7fdea9e9">D17-1320</url>
      <doi>10.18653/v1/D17-1320</doi>
      <abstract>Concept maps can be used to concisely represent important information and bring structure into <a href="https://en.wikipedia.org/wiki/Collection_(artwork)">large document collections</a>. Therefore, we study a variant of <a href="https://en.wikipedia.org/wiki/Multi-document_summarization">multi-document summarization</a> that produces summaries in the form of concept maps. However, suitable evaluation datasets for this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> along with a <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline system</a> and proposed evaluation protocol to enable further research on this variant of <a href="https://en.wikipedia.org/wiki/Automatic_summarization">summarization</a>.</abstract>
      <bibkey>falke-gurevych-2017-bringing</bibkey>
      <pwccode url="https://github.com/UKPLab/emnlp2017-cmapsum-corpus" additional="false">UKPLab/emnlp2017-cmapsum-corpus</pwccode>
    </paper>
    <paper id="321">
      <title>Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog</title>
      <author><first>Satwik</first> <last>Kottur</last></author>
      <author><first>José</first> <last>Moura</last></author>
      <author><first>Stefan</first> <last>Lee</last></author>
      <author><first>Dhruv</first> <last>Batra</last></author>
      <pages>2962–2967</pages>
      <url hash="1f4551ed">D17-1321</url>
      <doi>10.18653/v1/D17-1321</doi>
      <attachment type="attachment" hash="2fbff537">D17-1321.Attachment.zip</attachment>
      <abstract>A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the <a href="https://en.wikipedia.org/wiki/Communication_protocol">protocols</a> developed by the agents, learned without any human supervision ! In this paper, using a Task &amp; Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one   showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a> does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.</abstract>
      <bibkey>kottur-etal-2017-natural</bibkey>
      <pwccode url="https://github.com/batra-mlp-lab/lang-emerge" additional="false">batra-mlp-lab/lang-emerge</pwccode>
    </paper>
    <paper id="322">
      <title>Depression and Self-Harm Risk Assessment in Online Forums</title>
      <author><first>Andrew</first> <last>Yates</last></author>
      <author><first>Arman</first> <last>Cohan</last></author>
      <author><first>Nazli</first> <last>Goharian</last></author>
      <pages>2968–2978</pages>
      <url hash="68111eda">D17-1322</url>
      <doi>10.18653/v1/D17-1322</doi>
      <abstract>Users suffering from <a href="https://en.wikipedia.org/wiki/Mental_disorder">mental health conditions</a> often turn to online resources for support, including specialized online support communities or general communities such as <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> and <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a>. In this work, we present a <a href="https://en.wikipedia.org/wiki/Software_framework">framework</a> for supporting and studying users in both types of <a href="https://en.wikipedia.org/wiki/Community">communities</a>. We propose <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for identifying posts in <a href="https://en.wikipedia.org/wiki/Peer_support">support communities</a> that may indicate a risk of <a href="https://en.wikipedia.org/wiki/Self-harm">self-harm</a>, and demonstrate that our approach outperforms strong previously proposed <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> for identifying such posts. Self-harm is closely related to <a href="https://en.wikipedia.org/wiki/Depression_(mood)">depression</a>, which makes identifying depressed users on <a href="https://en.wikipedia.org/wiki/Internet_forum">general forums</a> a crucial related task. We introduce a large-scale general forum dataset consisting of users with self-reported depression diagnoses matched with control users. We show how our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> can be applied to effectively identify <a href="https://en.wikipedia.org/wiki/Depression_(mood)">depressed users</a> from their use of language alone. We demonstrate that our method outperforms strong <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baselines</a> on this general forum dataset.</abstract>
      <bibkey>yates-etal-2017-depression</bibkey>
    </paper>
    <paper id="323">
      <title>Men Also Like Shopping : Reducing Gender Bias Amplification using Corpus-level Constraints</title>
      <author><first>Jieyu</first> <last>Zhao</last></author>
      <author><first>Tianlu</first> <last>Wang</last></author>
      <author><first>Mark</first> <last>Yatskar</last></author>
      <author><first>Vicente</first> <last>Ordonez</last></author>
      <author><first>Kai-Wei</first> <last>Chang</last></author>
      <pages>2979–2989</pages>
      <url hash="91e411c1">D17-1323</url>
      <doi>10.18653/v1/D17-1323</doi>
      <abstract>Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the <a href="https://en.wikipedia.org/wiki/World_Wide_Web">web</a>. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study <a href="https://en.wikipedia.org/wiki/Data">data</a> and <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> associated with multilabel object classification and visual semantic role labeling. We find that (a) <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> for these tasks contain significant <a href="https://en.wikipedia.org/wiki/Gender_bias">gender bias</a> and (b) <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on these <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a> further amplify existing bias. For example, the activity cooking is over 33 % more likely to involve females than males in a training set, and a trained <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> further amplifies the disparity to 68 % at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> based on <a href="https://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field">Lagrangian relaxation</a> for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5 % and 40.5 % for multilabel classification and visual semantic role labeling, respectively</abstract>
      <bibkey>zhao-etal-2017-men</bibkey>
      <pwccode url="https://github.com/uclanlp/reducingbias" additional="true">uclanlp/reducingbias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</booktitle>
      <url hash="a842145f">D17-2</url>
      <editor><first>Lucia</first><last>Specia</last></editor>
      <editor><first>Matt</first><last>Post</last></editor>
      <editor><first>Michael</first><last>Paul</last></editor>
      <doi>10.18653/v1/D17-2</doi>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Copenhagen, Denmark</address>
      <month>September</month>
      <year>2017</year>
    </meta>
    <frontmatter>
      <url hash="262196fb">D17-2000</url>
      <bibkey>emnlp-2017-2017-empirical</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The NLTK FrameNet API : Designing for Discoverability with a Rich Linguistic Resource<fixed-case>NLTK</fixed-case> <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et <fixed-case>API</fixed-case>: Designing for Discoverability with a Rich Linguistic Resource</title>
      <author><first>Nathan</first> <last>Schneider</last></author>
      <author><first>Chuck</first> <last>Wooters</last></author>
      <pages>1–6</pages>
      <url hash="d4e12a8b">D17-2001</url>
      <doi>10.18653/v1/D17-2001</doi>
      <abstract>A new <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python API</a>, integrated within the <a href="https://en.wikipedia.org/wiki/NLTK">NLTK suite</a>, offers access to the <a href="https://en.wikipedia.org/wiki/FrameNet">FrameNet 1.7 lexical database</a>. The lexicon (structured in terms of frames) as well as annotated sentences can be processed programatically, or browsed with human-readable displays via the interactive Python prompt.</abstract>
      <bibkey>schneider-wooters-2017-nltk</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="2">
      <title>Argotario : Computational Argumentation Meets Serious Games<fixed-case>A</fixed-case>rgotario: Computational Argumentation Meets Serious Games</title>
      <author><first>Ivan</first> <last>Habernal</last></author>
      <author><first>Raffael</first> <last>Hannemann</last></author>
      <author><first>Christian</first> <last>Pollak</last></author>
      <author><first>Christopher</first> <last>Klamm</last></author>
      <author><first>Patrick</first> <last>Pauli</last></author>
      <author><first>Iryna</first> <last>Gurevych</last></author>
      <pages>7–12</pages>
      <url hash="d8c880dd">D17-2002</url>
      <doi>10.18653/v1/D17-2002</doi>
      <abstract>An important skill in <a href="https://en.wikipedia.org/wiki/Critical_thinking">critical thinking</a> and <a href="https://en.wikipedia.org/wiki/Argumentation_theory">argumentation</a> is the ability to spot and recognize <a href="https://en.wikipedia.org/wiki/Fallacy">fallacies</a>. Fallacious arguments, omnipresent in argumentative discourse, can be deceptive, manipulative, or simply leading to ‘wrong moves’ in a discussion. Despite their importance, argumentation scholars and NLP researchers with focus on argumentation quality have not yet investigated <a href="https://en.wikipedia.org/wiki/Fallacy">fallacies</a> empirically. The nonexistence of resources dealing with <a href="https://en.wikipedia.org/wiki/Fallacy">fallacious argumentation</a> calls for scalable approaches to data acquisition and annotation, for which the serious games methodology offers an appealing, yet unexplored, alternative. We present Argotario, a <a href="https://en.wikipedia.org/wiki/Serious_game">serious game</a> that deals with fallacies in everyday argumentation. Argotario is a multilingual, open-source, platform-independent application with strong educational aspects, accessible at.<url>www.argotario.net</url>. </abstract>
      <bibkey>habernal-etal-2017-argotario</bibkey>
      <pwccode url="https://github.com/UKPLab/argotario" additional="false">UKPLab/argotario</pwccode>
    </paper>
    <paper id="3">
      <title>An Analysis and Visualization Tool for Case Study Learning of Linguistic Concepts</title>
      <author><first>Cecilia</first> <last>Ovesdotter Alm</last></author>
      <author><first>Benjamin</first> <last>Meyers</last></author>
      <author><first>Emily</first> <last>Prud’hommeaux</last></author>
      <pages>13–18</pages>
      <url hash="e456c56c">D17-2003</url>
      <doi>10.18653/v1/D17-2003</doi>
      <abstract>We present an <a href="https://en.wikipedia.org/wiki/Educational_technology">educational tool</a> that integrates computational linguistics resources for use in non-technical undergraduate language science courses. By using the tool in conjunction with evidence-driven pedagogical case studies, we strive to provide opportunities for students to gain an understanding of linguistic concepts and analysis through the lens of realistic problems in feasible ways. Case studies tend to be used in legal, business, and health education contexts, but less in the teaching and learning of linguistics. The approach introduced also has potential to encourage students across training backgrounds to continue on to computational language analysis coursework.</abstract>
      <bibkey>ovesdotter-alm-etal-2017-analysis</bibkey>
    </paper>
    <paper id="4">
      <title>GraphDocExplore : A Framework for the Experimental Comparison of Graph-based Document Exploration Techniques<fixed-case>G</fixed-case>raph<fixed-case>D</fixed-case>oc<fixed-case>E</fixed-case>xplore: A Framework for the Experimental Comparison of Graph-based Document Exploration Techniques</title>
      <author><first>Tobias</first> <last>Falke</last></author>
      <author><first>Iryna</first> <last>Gurevych</last></author>
      <pages>19–24</pages>
      <url hash="e8d5dc13">D17-2004</url>
      <doi>10.18653/v1/D17-2004</doi>
      <abstract>Graphs have long been proposed as a tool to browse and navigate in a collection of documents in order to support <a href="https://en.wikipedia.org/wiki/Exploratory_search">exploratory search</a>. Many techniques to automatically extract different types of <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a>, showing for example <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entities</a> or <a href="https://en.wikipedia.org/wiki/Concept">concepts</a> and different relationships between them, have been suggested. While experimental evidence that they are indeed helpful exists for some of them, it is largely unknown which type of <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> is most helpful for a specific exploratory task. However, carrying out experimental comparisons with human subjects is challenging and time-consuming. Towards this end, we present the GraphDocExplore framework. It provides an intuitive web interface for graph-based document exploration that is optimized for experimental user studies. Through a generic <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">graph interface</a>, different methods to extract <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a> from text can be plugged into the <a href="https://en.wikipedia.org/wiki/System">system</a>. Hence, they can be compared at minimal implementation effort in an environment that ensures controlled comparisons. The <a href="https://en.wikipedia.org/wiki/System">system</a> is publicly available under an open-source license.<i>GraphDocExplore</i>
      framework. It provides an intuitive web interface for graph-based document
      exploration that is optimized for experimental user studies. Through a
      generic graph interface, different methods to extract graphs from text can
      be plugged into the system. Hence, they can be compared at minimal
      implementation effort in an environment that ensures controlled
      comparisons. The system is publicly available under an open-source
      license.
    </abstract>
      <bibkey>falke-gurevych-2017-graphdocexplore</bibkey>
    </paper>
    <paper id="5">
      <title>SGNMT   A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies<fixed-case>SGNMT</fixed-case> – A Flexible <fixed-case>NMT</fixed-case> Decoding Platform for Quick Prototyping of New Models and Search Strategies</title>
      <author><first>Felix</first> <last>Stahlberg</last></author>
      <author><first>Eva</first> <last>Hasler</last></author>
      <author><first>Danielle</first> <last>Saunders</last></author>
      <author><first>Bill</first> <last>Byrne</last></author>
      <pages>25–30</pages>
      <url hash="0dc23c20">D17-2005</url>
      <doi>10.18653/v1/D17-2005</doi>
      <abstract>This paper introduces SGNMT, our experimental platform for machine translation research. SGNMT provides a generic interface to neural and symbolic scoring modules (predictors) with left-to-right semantic such as translation models like NMT, language models, translation lattices, n-best lists or other kinds of scores and constraints. Predictors can be combined with other <a href="https://en.wikipedia.org/wiki/Prediction">predictors</a> to form complex decoding tasks. SGNMT implements a number of <a href="https://en.wikipedia.org/wiki/Search_algorithm">search strategies</a> for traversing the space spanned by the predictors which are appropriate for different predictor constellations. Adding new predictors or decoding strategies is particularly easy, making it a very efficient tool for prototyping new research ideas. SGNMT is actively being used by students in the MPhil program in <a href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning</a>, Speech and Language Technology at the University of Cambridge for course work and these s, as well as for most of the research work in our group.</abstract>
      <bibkey>stahlberg-etal-2017-sgnmt</bibkey>
    </paper>
    <paper id="6">
      <title>StruAP : A Tool for Bundling Linguistic Trees through Structure-based Abstract Pattern<fixed-case>S</fixed-case>tru<fixed-case>AP</fixed-case>: A Tool for Bundling Linguistic Trees through Structure-based Abstract Pattern</title>
      <author><first>Kohsuke</first> <last>Yanai</last></author>
      <author><first>Misa</first> <last>Sato</last></author>
      <author><first>Toshihiko</first> <last>Yanase</last></author>
      <author><first>Kenzo</first> <last>Kurotsuchi</last></author>
      <author><first>Yuta</first> <last>Koreeda</last></author>
      <author><first>Yoshiki</first> <last>Niwa</last></author>
      <pages>31–36</pages>
      <url hash="e0e07cb6">D17-2006</url>
      <doi>10.18653/v1/D17-2006</doi>
      <abstract>We present a tool for developing tree structure patterns that makes it easy to define the relations among textual phrases and create a <a href="https://en.wikipedia.org/wiki/Search_engine_indexing">search index</a> for these newly defined <a href="https://en.wikipedia.org/wiki/Binary_relation">relations</a>. By using the proposed <a href="https://en.wikipedia.org/wiki/Tool">tool</a>, users develop tree structure patterns through abstracting <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">syntax trees</a>. The tool features (1) intuitive pattern syntax, (2) unique functions such as recursive call of patterns and use of lexicon dictionaries, and (3) whole workflow support for relation development and validation. We report the current implementation of the <a href="https://en.wikipedia.org/wiki/Tool">tool</a> and its effectiveness.</abstract>
      <bibkey>yanai-etal-2017-struap</bibkey>
    </paper>
    <paper id="7">
      <title>KnowYourNyms? A Game of Semantic Relationships<fixed-case>K</fixed-case>now<fixed-case>Y</fixed-case>our<fixed-case>N</fixed-case>yms? A Game of Semantic Relationships</title>
      <author><first>Ross</first> <last>Mechanic</last></author>
      <author><first>Dean</first> <last>Fulgoni</last></author>
      <author><first>Hannah</first> <last>Cutler</last></author>
      <author><first>Sneha</first> <last>Rajana</last></author>
      <author><first>Zheyuan</first> <last>Liu</last></author>
      <author><first>Bradley</first> <last>Jackson</last></author>
      <author><first>Anne</first> <last>Cocos</last></author>
      <author><first>Chris</first> <last>Callison-Burch</last></author>
      <author><first>Marianna</first> <last>Apidianaki</last></author>
      <pages>37–42</pages>
      <url hash="c902f5a8">D17-2007</url>
      <doi>10.18653/v1/D17-2007</doi>
      <abstract>Semantic relation knowledge is crucial for <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a>. We introduce KnowYourNyms?, a <a href="https://en.wikipedia.org/wiki/Browser_game">web-based game</a> for learning <a href="https://en.wikipedia.org/wiki/Semantic_Web">semantic relations</a>. While providing users with an engaging experience, the <a href="https://en.wikipedia.org/wiki/Application_software">application</a> collects large amounts of data that can be used to improve semantic relation classifiers. The data also broadly informs us of how people perceive the relationships between words, providing useful insights for research in <a href="https://en.wikipedia.org/wiki/Psychology">psychology</a> and <a href="https://en.wikipedia.org/wiki/Linguistics">linguistics</a>.</abstract>
      <bibkey>mechanic-etal-2017-knowyournyms</bibkey>
    </paper>
    <paper id="8">
      <title>The Projector : An Interactive Annotation Projection Visualization Tool</title>
      <author><first>Alan</first> <last>Akbik</last></author>
      <author><first>Roland</first> <last>Vollgraf</last></author>
      <pages>43–48</pages>
      <url hash="cd7d9493">D17-2008</url>
      <doi>10.18653/v1/D17-2008</doi>
      <abstract>Previous works proposed annotation projection in parallel corpora to inexpensively generate <a href="https://en.wikipedia.org/wiki/Treebank">treebanks</a> or propbanks for new languages. In this approach, <a href="https://en.wikipedia.org/wiki/Annotation">linguistic annotation</a> is automatically transferred from a resource-rich source language (SL) to translations in a target language (TL). However, annotation projection may be adversely affected by translational divergences between specific language pairs. For this reason, previous work often required careful qualitative analysis of projectability of specific annotation in order to define strategies to address quality and coverage issues. In this demonstration, we present THE PROJECTOR, an interactive GUI designed to assist researchers in such analysis : it allows users to execute and visually inspect annotation projection in a range of different settings. We give an overview of the <a href="https://en.wikipedia.org/wiki/Graphical_user_interface">GUI</a>, discuss use cases and illustrate how the tool can facilitate discussions with the research community.</abstract>
      <bibkey>akbik-vollgraf-2017-projector</bibkey>
    </paper>
    <paper id="9">
      <title>Interactive Visualization for Linguistic Structure</title>
      <author><first>Aaron</first> <last>Sarnat</last></author>
      <author><first>Vidur</first> <last>Joshi</last></author>
      <author><first>Cristian</first> <last>Petrescu-Prahova</last></author>
      <author><first>Alvaro</first> <last>Herrasti</last></author>
      <author><first>Brandon</first> <last>Stilson</last></author>
      <author><first>Mark</first> <last>Hopkins</last></author>
      <pages>49–54</pages>
      <url hash="4c73505f">D17-2009</url>
      <doi>10.18653/v1/D17-2009</doi>
      <abstract>We provide a visualization library and <a href="https://en.wikipedia.org/wiki/User_interface">web interface</a> for interactively exploring a parse tree or a <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">forest of parses</a>. The <a href="https://en.wikipedia.org/wiki/Library_(computing)">library</a> is not tied to any particular linguistic representation, but provides a <a href="https://en.wikipedia.org/wiki/Application_programming_interface">general-purpose API</a> for the interactive exploration of hierarchical linguistic structure. To facilitate rapid understanding of a complex structure, the <a href="https://en.wikipedia.org/wiki/Application_programming_interface">API</a> offers several important features, including expand / collapse functionality, positional and color cues, explicit visual support for sequential structure, and dynamic highlighting to convey node-to-text correspondence.</abstract>
      <bibkey>sarnat-etal-2017-interactive</bibkey>
    </paper>
    <paper id="10">
      <title>DLATK : Differential Language Analysis ToolKit<fixed-case>DLATK</fixed-case>: Differential Language Analysis <fixed-case>T</fixed-case>ool<fixed-case>K</fixed-case>it</title>
      <author><first>H. Andrew</first> <last>Schwartz</last></author>
      <author><first>Salvatore</first> <last>Giorgi</last></author>
      <author><first>Maarten</first> <last>Sap</last></author>
      <author><first>Patrick</first> <last>Crutchley</last></author>
      <author><first>Lyle</first> <last>Ungar</last></author>
      <author><first>Johannes</first> <last>Eichstaedt</last></author>
      <pages>55–60</pages>
      <url hash="d01c1f8b">D17-2010</url>
      <doi>10.18653/v1/D17-2010</doi>
      <abstract>We present Differential Language Analysis Toolkit (DLATK), an open-source python package and command-line tool developed for conducting social-scientific language analyses. While DLATK provides standard NLP pipeline steps such as <a href="https://en.wikipedia.org/wiki/Lexical_analysis">tokenization</a> or SVM-classification, its novel strengths lie in analyses useful for psychological, health, and social science : (1) incorporation of extra-linguistic structured information, (2) specified levels and units of analysis (e.g. document, user, community), (3) statistical metrics for continuous outcomes, and (4) robust, proven, and accurate pipelines for social-scientific prediction problems. DLATK integrates multiple popular packages (SKLearn, Mallet), enables interactive usage (Jupyter Notebooks), and generally follows object oriented principles to make it easy to tie in additional libraries or storage technologies.</abstract>
      <bibkey>schwartz-etal-2017-dlatk</bibkey>
    </paper>
    <paper id="11">
      <title>QUINT : Interpretable Question Answering over Knowledge Bases<fixed-case>QUINT</fixed-case>: Interpretable Question Answering over Knowledge Bases</title>
      <author><first>Abdalghani</first> <last>Abujabal</last></author>
      <author><first>Rishiraj</first> <last>Saha Roy</last></author>
      <author><first>Mohamed</first> <last>Yahya</last></author>
      <author><first>Gerhard</first> <last>Weikum</last></author>
      <pages>61–66</pages>
      <url hash="8197fe17">D17-2011</url>
      <doi>10.18653/v1/D17-2011</doi>
      <abstract>We present QUINT, a live system for <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> over <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a>. QUINT automatically learns role-aligned utterance-query templates from user questions paired with their answers. When QUINT answers a question, it visualizes the complete derivation sequence from the natural language utterance to the final answer. The derivation provides an explanation of how the <a href="https://en.wikipedia.org/wiki/Syntax_(programming_languages)">syntactic structure</a> of the question was used to derive the structure of a SPARQL query, and how the phrases in the question were used to instantiate different parts of the query. When an answer seems unsatisfactory, the <a href="https://en.wikipedia.org/wiki/Derivation_(logic)">derivation</a> provides valuable insights towards reformulating the question.</abstract>
      <bibkey>abujabal-etal-2017-quint</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="12">
      <title>Function Assistant : A Tool for NL Querying of APIs<fixed-case>NL</fixed-case> Querying of <fixed-case>API</fixed-case>s</title>
      <author><first>Kyle</first> <last>Richardson</last></author>
      <author><first>Jonas</first> <last>Kuhn</last></author>
      <pages>67–72</pages>
      <url hash="50778cd1">D17-2012</url>
      <doi>10.18653/v1/D17-2012</doi>
      <abstract>In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language</a>. The <a href="https://en.wikipedia.org/wiki/List_of_toolkits">toolkit</a> is designed to help end-users of a target API quickly find information about <a href="https://en.wikipedia.org/wiki/Subroutine">functions</a> through high-level natural language queries, or descriptions. For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the <a href="https://en.wikipedia.org/wiki/Application_programming_interface">API</a> using the semantic parsing approach of (Richardson and Kuhn, 2017). Translations are automatically learned from example text-code pairs in example <a href="https://en.wikipedia.org/wiki/Application_programming_interface">APIs</a>. The <a href="https://en.wikipedia.org/wiki/List_of_toolkits">toolkit</a> includes features for building translation pipelines and <a href="https://en.wikipedia.org/wiki/Query_language">query engines</a> for arbitrary source code projects. To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github.</abstract>
      <bibkey>richardson-kuhn-2017-function</bibkey>
      <pwccode url="" additional="true" />
    </paper>
    <paper id="13">
      <title>MoodSwipe : A Soft Keyboard that Suggests MessageBased on User-Specified Emotions<fixed-case>M</fixed-case>ood<fixed-case>S</fixed-case>wipe: A Soft Keyboard that Suggests <fixed-case>M</fixed-case>essage<fixed-case>B</fixed-case>ased on User-Specified Emotions</title>
      <author><first>Chieh-Yang</first> <last>Huang</last></author>
      <author><first>Tristan</first> <last>Labetoulle</last></author>
      <author><first>Ting-Hao</first> <last>Huang</last></author>
      <author><first>Yi-Pei</first> <last>Chen</last></author>
      <author><first>Hung-Chen</first> <last>Chen</last></author>
      <author><first>Vallari</first> <last>Srivastava</last></author>
      <author><first>Lun-Wei</first> <last>Ku</last></author>
      <pages>73–78</pages>
      <url hash="724bbf0f">D17-2013</url>
      <doi>10.18653/v1/D17-2013</doi>
      <abstract>We present MoodSwipe, a soft keyboard that suggests text messages given the user-specified emotions utilizing the real dialog data. The aim of MoodSwipe is to create a convenient user interface to enjoy the technology of <a href="https://en.wikipedia.org/wiki/Emotion_classification">emotion classification</a> and text suggestion, and at the same time to collect labeled data automatically for developing more advanced technologies. While users select the MoodSwipe keyboard, they can type as usual but sense the emotion conveyed by their text and receive suggestions for their message as a benefit. In MoodSwipe, the detected emotions serve as the medium for suggested texts, where viewing the latter is the incentive to correcting the former. We conduct several experiments to show the superiority of the emotion classification models trained on the dialog data, and further to verify good emotion cues are important context for text suggestion.</abstract>
      <bibkey>huang-etal-2017-moodswipe</bibkey>
    </paper>
    <paper id="15">
      <title>HeidelPlace : An Extensible Framework for <a href="https://en.wikipedia.org/wiki/Geoparsing">Geoparsing</a><fixed-case>H</fixed-case>eidel<fixed-case>P</fixed-case>lace: An Extensible Framework for Geoparsing</title>
      <author><first>Ludwig</first> <last>Richter</last></author>
      <author><first>Johanna</first> <last>Geiß</last></author>
      <author><first>Andreas</first> <last>Spitz</last></author>
      <author><first>Michael</first> <last>Gertz</last></author>
      <pages>85–90</pages>
      <url hash="82807576">D17-2015</url>
      <doi>10.18653/v1/D17-2015</doi>
      <abstract>Geographic information extraction from textual data sources, called <a href="https://en.wikipedia.org/wiki/Geoparsing">geoparsing</a>, is a key task in <a href="https://en.wikipedia.org/wiki/Text_processing">text processing</a> and central to subsequent spatial analysis approaches. Several geoparsers are available that support this task, each with its own (often limited or specialized) <a href="https://en.wikipedia.org/wiki/Gazetteer">gazetteer</a> and its own approaches to toponym detection and resolution. In this demonstration paper, we present HeidelPlace, an extensible framework in support of <a href="https://en.wikipedia.org/wiki/Geoparsing">geoparsing</a>. Key features of HeidelPlace include a generic gazetteer model that supports the integration of place information from different knowledge bases, and a pipeline approach that enables an effective combination of diverse modules tailored to specific geoparsing tasks. This makes HeidelPlace a valuable tool for testing and evaluating different gazetteer sources and geoparsing methods. In the demonstration, we show how to set up a geoparsing workflow with HeidelPlace and how it can be used to compare and consolidate the output of different geoparsing approaches.</abstract>
      <bibkey>richter-etal-2017-heidelplace</bibkey>
    </paper>
    <paper id="16">
      <title>Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation</title>
      <author><first>Alexander</first> <last>Panchenko</last></author>
      <author><first>Fide</first> <last>Marten</last></author>
      <author><first>Eugen</first> <last>Ruppert</last></author>
      <author><first>Stefano</first> <last>Faralli</last></author>
      <author><first>Dmitry</first> <last>Ustalov</last></author>
      <author><first>Simone Paolo</first> <last>Ponzetto</last></author>
      <author><first>Chris</first> <last>Biemann</last></author>
      <pages>91–96</pages>
      <url hash="42a54bd4">D17-2016</url>
      <doi>10.18653/v1/D17-2016</doi>
      <abstract>Interpretability of a <a href="https://en.wikipedia.org/wiki/Predictive_modelling">predictive model</a> is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our <a href="https://en.wikipedia.org/wiki/System">system</a>, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration.</abstract>
      <bibkey>panchenko-etal-2017-unsupervised</bibkey>
      <pwccode url="https://github.com/uhh-lt/wsd" additional="false">uhh-lt/wsd</pwccode>
    </paper>
    <paper id="18">
      <title>SupWSD : A Flexible Toolkit for Supervised Word Sense Disambiguation<fixed-case>S</fixed-case>up<fixed-case>WSD</fixed-case>: A Flexible Toolkit for Supervised Word Sense Disambiguation</title>
      <author><first>Simone</first> <last>Papandrea</last></author>
      <author><first>Alessandro</first> <last>Raganato</last></author>
      <author><first>Claudio</first> <last>Delli Bovi</last></author>
      <pages>103–108</pages>
      <url hash="26941b56">D17-2018</url>
      <doi>10.18653/v1/D17-2018</doi>
      <abstract>In this demonstration we present SupWSD, a <a href="https://en.wikipedia.org/wiki/Java_API">Java API</a> for supervised Word Sense Disambiguation (WSD). This toolkit includes the implementation of a state-of-the-art supervised WSD system, together with a Natural Language Processing pipeline for <a href="https://en.wikipedia.org/wiki/Data_preprocessing">preprocessing</a> and <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a>. Our aim is to provide an easy-to-use tool for the research community, designed to be modular, fast and scalable for training and testing on large datasets. The source code of SupWSD is available at.<url>http://github.com/SI3P/SupWSD</url>.
    </abstract>
      <bibkey>papandrea-etal-2017-supwsd</bibkey>
      <pwccode url="https://github.com/SI3P/SupWSD" additional="false">SI3P/SupWSD</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="21">
      <title>Interactive Visualization and Manipulation of Attention-based Neural Machine Translation</title>
      <author><first>Jaesong</first> <last>Lee</last></author>
      <author><first>Joong-Hwi</first> <last>Shin</last></author>
      <author><first>Jun-Seok</first> <last>Kim</last></author>
      <pages>121–126</pages>
      <url hash="4c9cf455">D17-2021</url>
      <doi>10.18653/v1/D17-2021</doi>
      <abstract>While <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation (NMT)</a> provides high-quality translation, it is still hard to interpret and analyze its behavior. We present an interactive interface for visualizing and intervening behavior of NMT, specifically concentrating on the behavior of beam search mechanism and attention component. The tool (1) visualizes <a href="https://en.wikipedia.org/wiki/Search_tree">search tree</a> and <a href="https://en.wikipedia.org/wiki/Attention">attention</a> and (2) provides interface to adjust <a href="https://en.wikipedia.org/wiki/Search_tree">search tree</a> and attention weight (manually or automatically) at real-time. We show the <a href="https://en.wikipedia.org/wiki/Tool">tool</a> gives various methods to understand <a href="https://en.wikipedia.org/wiki/Nonlinear_functional_analysis">NMT</a>.</abstract>
      <bibkey>lee-etal-2017-interactive</bibkey>
    </paper>
  </volume>
  <volume id="3">
    <meta>
      <booktitle>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</booktitle>
      <editor><first>Alexandra</first><last>Birch</last></editor>
      <editor><first>Nathan</first><last>Schneider</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Copenhagen, Denmark</address>
      <month>September</month>
      <year>2017</year>
    </meta>
    <paper id="1">
      <title>Acquisition, Representation and Usage of Conceptual Hierarchies</title>
      <author><first>Marius</first><last>Pasca</last></author>
      <url hash="9f1d40ea">D17-3001</url>
      <abstract>Through subsumption and <a href="https://en.wikipedia.org/wiki/Instantiation_principle">instantiation</a>, individual instances (artificial intelligence, the spotted pig) otherwise spanning a wide range of domains can be brought together and organized under conceptual hierarchies. The hierarchies connect more specific concepts (computer science subfields, gastropubs) to more general concepts (academic disciplines, restaurants) through IsA relations. Explicit or implicit properties applicable to, and defining, more general concepts are inherited by their more specific concepts, down to the instances connected to the lower parts of the <a href="https://en.wikipedia.org/wiki/Hierarchy">hierarchies</a>. Subsumption represents a crisp, universally-applicable principle towards consistently representing IsA relations in any knowledge resource. Yet knowledge resources often exhibit significant differences in their scope, representation choices and intended usage, to cause significant differences in their expected usage and impact on various <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a>. This tutorial examines the theoretical foundations of subsumption, and its practical embodiment through IsA relations compiled manually or extracted automatically. It addresses IsA relations from their formal definition ; through practical choices made in their representation within the larger and more widely-used of the available knowledge resources ; to their automatic acquisition from document repositories, as opposed to their manual compilation by human contributors ; to their impact in <a href="https://en.wikipedia.org/wiki/Textual_analysis">text analysis</a> and <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a>. As <a href="https://en.wikipedia.org/wiki/Web_search_engine">search engines</a> move away from returning a set of links and closer to returning results that more directly answer queries, IsA relations play an increasingly important role towards a better understanding of documents and queries.</abstract>
      <bibkey>pasca-2017-acquisition</bibkey>
    </paper>
    <paper id="2">
      <title>Computational Sarcasm</title>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Aditya</first><last>Joshi</last></author>
      <url hash="e2c784db">D17-3002</url>
      <abstract>Sarcasm is a form of <a href="https://en.wikipedia.org/wiki/Irony">verbal irony</a> that is intended to express contempt or ridicule. Motivated by challenges posed by sarcastic text to <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, computational approaches to <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> have witnessed a growing interest at NLP forums in the past decade. Computational sarcasm refers to automatic approaches pertaining to <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a>. The tutorial will provide a bird’s-eye view of the research in computational sarcasm for text, while focusing on significant milestones. The tutorial begins with linguistic theories of sarcasm, with a focus on incongruity : a useful notion that underlies <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> and other forms of <a href="https://en.wikipedia.org/wiki/Literal_and_figurative_language">figurative language</a>. Since the most significant work in computational sarcasm is sarcasm detection : predicting whether a given piece of text is sarcastic or not, sarcasm detection forms the focus hereafter. We begin our discussion on sarcasm detection with <a href="https://en.wikipedia.org/wiki/Data_set">datasets</a>, touching on strategies, challenges and nature of datasets. Then, we describe algorithms for sarcasm detection : rule-based (where a specific evidence of sarcasm is utilised as a rule), statistical classifier-based (where features are designed for a statistical classifier), a topic model-based technique, and deep learning-based algorithms for sarcasm detection. In case of each of these <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a>, we refer to our work on sarcasm detection and share our learnings. Since information beyond the text to be classified, contextual information is useful for sarcasm detection, we then describe approaches that use such information through conversational context or author-specific context. We then follow it by novel areas in computational sarcasm such as sarcasm generation, sarcasm v / s irony classification, etc. We then summarise the tutorial and describe future directions based on errors reported in past work. The tutorial will end with a demonstration of our work on sarcasm detection. This tutorial will be of interest to researchers investigating computational sarcasm and related areas such as <a href="https://en.wikipedia.org/wiki/Computational_humour">computational humour</a>, figurative language understanding, emotion and sentiment sentiment analysis, etc. The tutorial is motivated by our continually evolving survey paper of sarcasm detection, that is available on arXiv at : Joshi, Aditya, Pushpak Bhattacharyya, and Mark James Carman. Automatic Sarcasm Detection : A Survey. arXiv preprint arXiv:1602.03426 (2016).</abstract>
      <bibkey>bhattacharyya-joshi-2017-computational</bibkey>
    </paper>
    <paper id="3">
      <title>Graph-based Text Representations: Boosting Text Mining, <fixed-case>NLP</fixed-case> and Information Retrieval with Graphs</title>
      <author><first>Fragkiskos D.</first><last>Malliaros</last></author>
      <author><first>Michalis</first><last>Vazirgiannis</last></author>
      <abstract>Graphs or networks have been widely used as modeling tools in Natural Language Processing (NLP), Text Mining (TM) and Information Retrieval (IR). Traditionally, the unigram bag-of-words representation is applied; that way, a document is represented as a multiset of its terms, disregarding dependencies between the terms. Although several variants and extensions of this modeling approach have been proposed (e.g., the n-gram model), the main weakness comes from the underlying term independence assumption. The order of the terms within a document is completely disregarded and any relationship between terms is not taken into account in the final task (e.g., text categorization). Nevertheless, as the heterogeneity of text collections is increasing (especially with respect to document length and vocabulary), the research community has started exploring different document representations aiming to capture more fine-grained contexts of co-occurrence between different terms, challenging the well-established unigram bag-of-words model. To this direction, graphs constitute a well-developed model that has been adopted for text representation. The goal of this tutorial is to offer a comprehensive presentation of recent methods that rely on graph-based text representations to deal with various tasks in NLP and IR. We will describe basic as well as novel graph theoretic concepts and we will examine how they can be applied in a wide range of text-related application domains.

All the material associated to the tutorial will be available at: http://fragkiskosm.github.io/projects/graph_text_tutorial</abstract>
      <bibkey>malliaros-vazirgiannis-2017-graph</bibkey>
    </paper>
    <paper id="4">
      <title>Semantic Role Labeling</title>
      <author><first>Diego</first><last>Marcheggiani</last></author>
      <author><first>Michael</first><last>Roth</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <url hash="513498db">D17-3004</url>
      <abstract>This tutorial describes semantic role labelling (SRL), the task of mapping text to shallow semantic representations of eventualities and their participants. The tutorial introduces the SRL task and discusses recent research directions related to the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>. The audience of this tutorial will learn about the linguistic background and motivation for semantic roles, and also about a range of <a href="https://en.wikipedia.org/wiki/Computational_model">computational models</a> for this task, from early approaches to the current state-of-the-art. We will further discuss recently proposed variations to the traditional SRL task, including topics such as semantic proto-role labeling. We also cover techniques for reducing required annotation effort, such as methods exploiting unlabeled corpora (semi-supervised and unsupervised techniques), model adaptation across languages and domains, and methods for crowdsourcing semantic role annotation (e.g., question-answer driven SRL). Methods based on different machine learning paradigms, including <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>, generative Bayesian models, graph-based algorithms and bootstrapping style techniques. Beyond sentence-level SRL, we discuss work that involves semantic roles in discourse. In particular, we cover <a href="https://en.wikipedia.org/wiki/Data_set">data sets</a> and <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> related to the task of identifying implicit roles and linking them to discourse antecedents. We introduce different approaches to this task from the literature, including models based on <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a>, centering, and selectional preferences. We also review how new insights gained through them can be useful for the traditional SRL task.</abstract>
      <bibkey>marcheggiani-etal-2017-semantic</bibkey>
    </paper>
    <paper id="5">
      <title>Memory Augmented Neural Networks for Natural Language Processing</title>
      <author><first>Caglar</first><last>Gulcehre</last></author>
      <author><first>Sarath</first><last>Chandar</last></author>
      <url hash="507093f2">D17-3005</url>
      <abstract>Designing of general-purpose learning algorithms is a long-standing goal of <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial intelligence</a>. A general purpose AI agent should be able to have a memory that it can store and retrieve information from. Despite the success of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> in particular with the introduction of LSTMs and GRUs to this area, there are still a set of complex tasks that can be challenging for conventional <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>. Those tasks often require a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> to be equipped with an explicit, <a href="https://en.wikipedia.org/wiki/External_memory">external memory</a> in which a larger, potentially unbounded, set of facts need to be stored. They include but are not limited to, <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a>, <a href="https://en.wikipedia.org/wiki/Planning">planning</a>, episodic question-answering and learning compact algorithms. Recently two promising approaches based on <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> to this type of tasks have been proposed : Memory Networks and Neural Turing Machines. In this tutorial, we will give an overview of this new paradigm of <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a> with <a href="https://en.wikipedia.org/wiki/Memory">memory</a>. We will present a unified architecture for Memory Augmented Neural Networks (MANN) and discuss the ways in which one can address the external memory and hence read / write from it. Then we will introduce <a href="https://en.wikipedia.org/wiki/Neural_Turing_machine">Neural Turing Machines</a> and Memory Networks as specific instantiations of this general architecture. In the second half of the tutorial, we will focus on recent advances in MANN which focus on the following questions : How can we read / write from an extremely large memory in a scalable way? How can we design efficient non-linear addressing schemes? How can we do efficient <a href="https://en.wikipedia.org/wiki/Reason">reasoning</a> using large scale memory and an <a href="https://en.wikipedia.org/wiki/Episodic_memory">episodic memory</a>? The answer to any one of these questions introduces a variant of MANN. We will conclude the tutorial with several open challenges in MANN and its applications to NLP.We will introduce several <a href="https://en.wikipedia.org/wiki/Application_software">applications</a> of MANN in <a href="https://en.wikipedia.org/wiki/Neuro-linguistic_programming">NLP</a> throughout the tutorial. Few examples include <a href="https://en.wikipedia.org/wiki/Language_model">language modeling</a>, <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a>, visual question answering, and <a href="https://en.wikipedia.org/wiki/Dialogue_system">dialogue systems</a>. For updated information and material, please refer to our tutorial website : https://sites.google.com/view/mann-emnlp2017/.</abstract>
      <bibkey>gulcehre-chandar-2017-memory</bibkey>
    </paper>
    <paper id="7">
      <title>Cross-Lingual Word Representations : Induction and Evaluation</title>
      <author><first>Manaal</first><last>Faruqui</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <url hash="7346a164">D17-3007</url>
      <abstract>In recent past, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> as a field has seen tremendous utility of distributional word vector representations as <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> in downstream tasks. The fact that these <a href="https://en.wikipedia.org/wiki/Word_vector">word vectors</a> can be trained on unlabeled monolingual corpora of a language makes them an inexpensive resource in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>. With the increasing use of monolingual word vectors, there is a need for word vectors that can be used as efficiently across multiple languages as monolingually. Therefore, learning bilingual and multilingual word embeddings / vectors is currently an important research topic. These vectors offer an elegant and language-pair independent way to represent content across different languages. This tutorial aims to bring NLP researchers up to speed with the current techniques in cross-lingual word representation learning. We will first discuss how to induce cross-lingual word representations (covering both bilingual and multilingual ones) from various data types and resources (e.g., parallel data, comparable data, non-aligned monolingual data in different languages, dictionaries and theasuri, or, even, images, eye-tracking data). We will then discuss how to evaluate such <a href="https://en.wikipedia.org/wiki/Representation_(arts)">representations</a>, intrinsically and extrinsically. We will introduce researchers to state-of-the-art methods for constructing cross-lingual word representations and discuss their applicability in a broad range of downstream NLP applications. We will deliver a detailed survey of the current methods, discuss best training and evaluation practices and use-cases, and provide links to publicly available implementations, datasets, and pre-trained models.</abstract>
      <bibkey>faruqui-etal-2017-cross</bibkey>
    </paper>
  </volume>
</collection>