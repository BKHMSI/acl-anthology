<collection id="2019.icon">
  <volume id="1" ingest-date="2021-05-10">
    <meta>
      <booktitle>Proceedings of the 16th International Conference on Natural Language Processing</booktitle>
      <editor><first>Dipti Misra</first><last>Sharma</last></editor>
      <editor><first>Pushpak</first><last>Bhattacharya</last></editor>
      <publisher>NLP Association of India</publisher>
      <address>International Institute of Information Technology, Hyderabad, India</address>
      <month>December</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="8847b7d4">2019.icon-1.0</url>
      <bibkey>icon-2019-international</bibkey>
    </frontmatter>
    <paper id="2">
      <title>A Deep Ensemble Framework for Fake News Detection and Multi-Class Classification of Short Political Statements</title>
      <author><first>Arjun</first><last>Roy</last></author>
      <author><first>Kingshuk</first><last>Basak</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>9&#8211;17</pages>
      <abstract>Fake news, rumor, incorrect information, and misinformation detection are nowadays crucial issues as these might have serious consequences for our social fabrics. Such information is increasing rapidly due to the availability of enormous web information sources including social media feeds, news blogs, online newspapers etc. In this paper, we develop various deep learning models for detecting fake news and classifying them into the pre-defined fine-grained categories. At first, we develop individual models based on Convolutional Neural Network (CNN), and Bi-directional Long Short Term Memory (Bi-LSTM) networks. The representations obtained from these two models are fed into a Multi-layer Perceptron Model (MLP) for the final classification. Our experiments on a benchmark dataset show promising results with an overall accuracy of 44.87%, which outperforms the current state of the arts.</abstract>
      <url hash="d3430c1c">2019.icon-1.2</url>
      <bibkey>roy-etal-2019-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/liar">LIAR</pwcdataset>
    </paper>
    <paper id="4">
      <title>Introducing Aspects of Creativity in Automatic Poetry Generation</title>
      <author><first>Brendan</first><last>Bena</last></author>
      <author><first>Jugal</first><last>Kalita</last></author>
      <pages>26&#8211;35</pages>
      <abstract>Poetry Generation involves teaching systems to automatically generate text that resembles poetic work. A deep learning system can learn to generate poetry on its own by training on a corpus of poems and modeling the particular style of language. In this paper, we propose taking an approach that fine-tunes GPT-2, a pre-trained language model, to our downstream task of poetry generation. We extend prior work on poetry generation by introducing creative elements. Specifically, we generate poems that express emotion and elicit the same in readers, and poems that use the language of dreams&#8212;called dream poetry. We are able to produce poems that correctly elicit the emotions of sadness and joy 87.5 and 85 percent, respectively, of the time. We produce dreamlike poetry by training on a corpus of texts that describe dreams. Poems from this model are shown to capture elements of dream poetry with scores of no less than 3.2 on the Likert scale. We perform crowdsourced human-evaluation for all our poems. We also make use of the Coh-Metrix tool, outlining metrics we use to gauge the quality of text generated.</abstract>
      <url hash="2200f374">2019.icon-1.4</url>
      <bibkey>bena-kalita-2019-introducing</bibkey>
    </paper>
    <paper id="5">
      <title>Incorporating Sub-Word Level Information in Language Invariant Neural Event Detection</title>
      <author><first>Suhan</first><last>Prabhu</last></author>
      <author><first>Pranav</first><last>Goel</last></author>
      <author><first>Alok</first><last>Debnath</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>36&#8211;44</pages>
      <abstract>Detection of TimeML events in text have traditionally been done on corpora such as TimeBanks. However, deep learning methods have not been applied to these corpora, because these datasets seldom contain more than 10,000 event mentions. Traditional architectures revolve around highly feature engineered, language specific statistical models. In this paper, we present a Language Invariant Neural Event Detection (ALINED) architecture. ALINED uses an aggregation of both sub-word level features as well as lexical and structural information. This is achieved by combining convolution over character embeddings, with recurrent layers over contextual word embeddings. We find that our model extracts relevant features for event span identification without relying on language specific features. We compare the performance of our language invariant model to the current state-of-the-art in English, Spanish, Italian and French. We outperform the F1-score of the state of the art in English by 1.65 points. We achieve F1-scores of 84.96, 80.87 and 74.81 on Spanish, Italian and French respectively which is comparable to the current states of the art for these languages. We also introduce the automatic annotation of events in Hindi, a low resource language, with an F1-Score of 77.13.</abstract>
      <url hash="dea3fc5f">2019.icon-1.5</url>
      <bibkey>prabhu-etal-2019-incorporating</bibkey>
    </paper>
    <paper id="6">
      <title>Event Centric Entity Linking for <fixed-case>H</fixed-case>indi News Articles: A Knowledge Graph Based Approach</title>
      <author><first>Pranav</first><last>Goel</last></author>
      <author><first>Suhan</first><last>Prabhu</last></author>
      <author><first>Alok</first><last>Debnath</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>45&#8211;55</pages>
      <abstract>We describe the development of a knowledge graph from an event annotated corpus by presenting a pipeline that identifies and extracts the relations between entities and events from Hindi news articles. Due to the semantic implications of argument identification for events in Hindi, we use a combined syntactic argument and semantic role identification methodology. To the best of our knowledge, no other architecture exists for this purpose. The extracted combined role information is incorporated in a knowledge graph that can be queried via subgraph extraction for basic questions. The architectures presented in this paper can be used for participant extraction and event-entity linking in most Indo-Aryan languages, due to similar syntactic and semantic properties of event arguments.</abstract>
      <url hash="fad8c55e">2019.icon-1.6</url>
      <bibkey>goel-etal-2019-event</bibkey>
    </paper>
    <paper id="8">
      <title>Non-native Accent Partitioning for Speakers of <fixed-case>I</fixed-case>ndian Regional Languages</title>
      <author><first>Radha Krishna</first><last>Guntur</last></author>
      <author><first>Krishnan</first><last>Ramakrishnan</last></author>
      <author><first>Vinay Kumar</first><last>Mittal</last></author>
      <pages>65&#8211;74</pages>
      <abstract>Acoustic features extracted from the speech signal can help in identifying speaker related multiple information such as geographical origin, regional accent and nativity. In this paper, classification of native speakers of South Indian languages is carried out based upon the accent of their non-native language, i.e., English. Four South Indian languages: Kannada, Malayalam, Tamil, and Telugu are examined. A database of English speech from the native speakers of these languages, along with the native language speech data was collected, from a non-overlapping set of speakers. Segment level acoustic features F0 and Mel-frequency cepstral coefficients (MFCCs) are used. Accent partitioning of non-native English speech data is carried out using multiple classifiers: k-nearest neighbour (KNN), linear discriminant analysis (LDA) and support vector machine (SVM), for validation and comparison of results. Classification accuracies of 86.6% are observed using KNN, and 89.2% or more than 90% using SVM classifier. A study of acoustic feature F0 contour, related to L2 intonation, showed that native speakers of Kannada language are quite distinct as compared to those of Tamil or Telugu languages. It is also observed that identification of Malayalam and Kannada speakers from their English speech accent is relatively easier than Telugu or Tamil speakers.</abstract>
      <url hash="fbe59409">2019.icon-1.8</url>
      <attachment type="OptionalSupplementaryMaterial" hash="65c25af3">2019.icon-1.8.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>guntur-etal-2019-non</bibkey>
    </paper>
    <paper id="9">
      <title>A little perturbation makes a difference: Treebank augmentation by perturbation improves transfer parsing</title>
      <author><first>Ayan</first><last>Das</last></author>
      <author><first>Sudeshna</first><last>Sarkar</last></author>
      <pages>75&#8211;84</pages>
      <abstract>We present an approach for cross-lingual transfer of dependency parser so that the parser trained on a single source language can more effectively cater to diverse target languages. In this work, we show that the cross-lingual performance of the parsers can be enhanced by over-generating the source language treebank. For this, the source language treebank is augmented with its perturbed version in which controlled perturbation is introduced in the parse trees by stochastically reordering the positions of the dependents with respect to their heads while keeping the structure of the parse trees unchanged. This enables the parser to capture diverse syntactic patterns in addition to those that are found in the source language. The resulting parser is found to more effectively parse target languages with different syntactic structures. With English as the source language, our system shows an average improvement of 6.7% and 7.7% in terms of UAS and LAS over 29 target languages compared to the baseline single source parser trained using unperturbed source language treebank. This also results in significant improvement over the transfer parser proposed by (CITATION) that involves an &#8220;order-free&#8221; parser algorithm.</abstract>
      <url hash="22d7c447">2019.icon-1.9</url>
      <bibkey>das-sarkar-2019-little</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>S</fixed-case>anskrit Segmentation revisited</title>
      <author><first>Sriram</first><last>Krishnan</last></author>
      <author><first>Amba</first><last>Kulkarni</last></author>
      <pages>105&#8211;114</pages>
      <abstract>Computationally analyzing Sanskrit texts requires proper segmentation in the initial stages. There have been various tools developed for Sanskrit text segmentation. Of these, G&#233;rard Huet&#8217;s Reader in the Sanskrit Heritage Engine analyzes the input text and segments it based on the word parameters - phases like iic, ifc, Pr, Subst, etc., and sandhi (or transition) that takes place at the end of a word with the initial part of the next word. And it enlists all the possible solutions differentiating them with the help of the phases. The phases and their analyses have their use in the domain of sentential parsers. In segmentation, though, they are not used beyond deciding whether the words formed with the phases are morphologically valid. This paper tries to modify the above segmenter by ignoring the phase details (except for a few cases), and also proposes a probability function to prioritize the list of solutions to bring up the most valid solutions at the top.</abstract>
      <url hash="b4688758">2019.icon-1.12</url>
      <bibkey>krishnan-kulkarni-2019-sanskrit</bibkey>
    </paper>
    <paper id="15">
      <title>Dataset for Aspect Detection on Mobile reviews in <fixed-case>H</fixed-case>indi</title>
      <author><first>Pruthwik</first><last>Mishra</last></author>
      <author><first>Ayush</first><last>Joshi</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>130&#8211;134</pages>
      <abstract>In recent years Opinion Mining has become one of the very interesting fields of Language Processing. To extract the gist of a sentence in a shorter and efficient manner is what opinion mining provides. In this paper we focus on detecting aspects for a particular domain. While relevant research work has been done in aspect detection in resource rich languages like English, we are trying to do the same in a relatively resource poor Hindi language. Here we present a corpus of mobile reviews which are labelled with carefully curated aspects. The motivation behind Aspect detection is to get information on a finer level about the data. In this paper we identify all aspects related to the gadget which are present on the reviews given online on various websites. We also propose baseline models to detect aspects in Hindi text after conducting various experiments.</abstract>
      <url hash="74a9e224">2019.icon-1.15</url>
      <bibkey>mishra-etal-2019-dataset</bibkey>
    </paper>
    <paper id="18">
      <title>Towards Handling Verb Phrase Ellipsis in <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi Machine Translation</title>
      <author><first>Niyati</first><last>Bafna</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>150&#8211;159</pages>
      <abstract>English-Hindi machine translation systems have difficulty interpreting verb phrase ellipsis (VPE) in English, and commit errors in translating sentences with VPE. We present a solution and theoretical backing for the treatment of English VPE, with the specific scope of enabling English-Hindi MT, based on an understanding of the syntactical phenomenon of verb-stranding verb phrase ellipsis in Hindi (VVPE). We implement a rule-based system to perform the following sub-tasks: 1) Verb ellipsis identification in the English source sentence, 2) Elided verb phrase head identification 3) Identification of verb segment which needs to be induced at the site of ellipsis 4) Modify input sentence; i.e. resolving VPE and inducing the required verb segment. This system obtains 94.83 percent precision and 83.04 percent recall on subtask (1), tested on 3900 sentences from the BNC corpus. This is competitive with state-of-the-art results. We measure accuracy of subtasks (2) and (3) together, and obtain a 91 percent accuracy on 200 sentences taken from the WSJ corpus. Finally, in order to indicate the relevance of ellipsis handling to MT, we carried out a manual analysis of the English-Hindi MT outputs of 100 sentences after passing it through our system. We set up a basic metric (1-5) for this evaluation, where 5 indicates drastic improvement, and obtained an average of 3.55. As far as we know, this is the first attempt to target ellipsis resolution in the context of improving English-Hindi machine translation.</abstract>
      <url hash="7965028d">2019.icon-1.18</url>
      <bibkey>bafna-sharma-2019-towards</bibkey>
    </paper>
    <paper id="22">
      <title>Kunji : A Resource Management System for Higher Productivity in Computer Aided Translation Tools</title>
      <author><first>Priyank</first><last>Gupta</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Dipti Misra</first><last>Sharma</last></author>
      <author><first>Rashid</first><last>Ahmad</last></author>
      <pages>184&#8211;192</pages>
      <abstract>Complex NLP applications, such as machine translation systems, utilize various kinds of resources namely lexical, multiword, domain dictionaries, maps and rules etc. Similarly, translators working on Computer Aided Translation workbenches, also require help from various kinds of resources - glossaries, terminologies, concordances and translation memory in the workbenches in order to increase their productivity. Additionally, translators have to look away from the workbenches for linguistic resources like Named Entities, Multiwords, lexical and lexeme dictionaries in order to get help, as the available resources like concordances, terminologies and glossaries are often not enough. In this paper we present Kunji, a resource management system for translation workbenches and MT modules. This system can be easily integrated in translation workbenches and can also be used as a management tool for resources for MT systems. The described resource management system has been integrated in a translation workbench Transzaar. We also study the impact of providing this resource management system along with linguistic resources on the productivity of translators for English-Hindi language pair. When the linguistic resources like lexeme, NER and MWE dictionaries were made available to translators in addition to their regular translation memories, concordances and terminologies, their productivity increased by 15.61%.</abstract>
      <url hash="a20f5438">2019.icon-1.22</url>
      <bibkey>gupta-etal-2019-kunji</bibkey>
    </paper>
    <paper id="25">
      <title>Unsung Challenges of Building and Deploying Language Technologies for Low Resource Language Communities</title>
      <author><first>Pratik</first><last>Joshi</last></author>
      <author><first>Christain</first><last>Barnes</last></author>
      <author><first>Sebastin</first><last>Santy</last></author>
      <author><first>Simran</first><last>Khanuja</last></author>
      <author><first>Sanket</first><last>Shah</last></author>
      <author><first>Anirudh</first><last>Srinivasan</last></author>
      <author><first>Satwik</first><last>Bhattamishra</last></author>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <pages>211&#8211;219</pages>
      <abstract>In this paper, we examine and analyze the challenges associated with developing and introducing language technologies to low-resource language communities. While doing so we bring to light the successes and failures of past work in this area, challenges being faced in doing so, and what have they achieved. Throughout this paper, we take a problem-facing approach and describe essential factors which the success of such technologies hinges upon. We present the various aspects in a manner which clarify and lay out the different tasks involved, which can aid organizations looking to make an impact in this area. We take the example of Gondi, an extremely-low resource Indian language, to reinforce and complement our discussion.</abstract>
      <url hash="ceb13465">2019.icon-1.25</url>
      <bibkey>joshi-etal-2019-unsung</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>DRC</fixed-case>o<fixed-case>V</fixed-case>e: An Augmented Word Representation Approach using Distributional and Relational Context</title>
      <author><first>Md. Aslam</first><last>Parwez</last></author>
      <author><first>Muhammad</first><last>Abulaish</last></author>
      <author><first>Mohd</first><last>Fazil</last></author>
      <pages>220&#8211;229</pages>
      <abstract>Word representation using the distributional information of words from a sizeable corpus is considered efficacious in many natural language processing and text mining applications. However, distributional representation of a word is unable to capture distant relational knowledge, representing the relational semantics. In this paper, we propose a novel word representation approach using distributional and relational contexts, DRCoVe, which augments the distributional representation of a word using the relational semantics extracted as syntactic and semantic association among entities from the underlying corpus. Unlike existing approaches that use external knowledge bases representing the relational semantics for enhanced word representation, DRCoVe uses typed dependencies (aka syntactic dependencies) to extract relational knowledge from the underlying corpus. The proposed approach is applied over a biomedical text corpus to learn word representation and compared with GloVe, which is one of the most popular word embedding approaches. The evaluation results on various benchmark datasets for word similarity and word categorization tasks demonstrate the effectiveness of DRCoVe over the GloVe.</abstract>
      <url hash="e9f8c9f8">2019.icon-1.26</url>
      <bibkey>parwez-etal-2019-drcove</bibkey>
    </paper>
    <paper id="27">
      <title>A Deep Learning Approach for Automatic Detection of Fake News</title>
      <author><first>Tanik</first><last>Saikh</last></author>
      <author><first>Arkadipta</first><last>De</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>230&#8211;238</pages>
      <abstract>Fake news detection is a very prominent and essential task in the field of journalism. This challenging problem is seen so far in the field of politics, but it could be even more challenging when it is to be determined in the multi-domain platform. In this paper, we propose two effective models based on deep learning for solving fake news detection problem in online news contents of multiple domains. We evaluate our techniques on the two recently released datasets, namely Fake News AMT and Celebrity for fake news detection. The proposed systems yield encouraging performance, outperforming the current hand-crafted feature engineering based state-of-the-art system with a significant margin of 3.08% and 9.3% by the two models, respectively. In order to exploit the datasets, available for the related tasks, we perform cross-domain analysis (model trained on FakeNews AMT and tested on Celebrity and vice versa) to explore the applicability of our systems across the domains.</abstract>
      <url hash="166cb9aa">2019.icon-1.27</url>
      <bibkey>saikh-etal-2019-deep</bibkey>
    </paper>
    <paper id="28">
      <title>Samajh-Boojh: A Reading Comprehension system in <fixed-case>H</fixed-case>indi</title>
      <author><first>Shalaka</first><last>Vaidya</last></author>
      <author><first>Hiranmai</first><last>Sri Adibhatla</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>239&#8211;248</pages>
      <abstract>This paper presents a novel approach designed to answer questions on a reading comprehension passage. It is an end-to-end system which first focuses on comprehending the given passage wherein it converts unstructured passage into a structured data and later proceeds to answer the questions related to the passage using solely the aforementioned structured data. To the best of our knowledge, the proposed design is first of its kind which accounts for entire process of comprehending the passage and then answering the questions associated with the passage. The comprehension stage converts the passage into a Discourse Collection that comprises of the relation shared amongst logical sentences in given passage along with the key characteristics of each sentence. This design has its applications in academic domain , query comprehension in speech systems among others.</abstract>
      <url hash="c8c6e93b">2019.icon-1.28</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2325c695">2019.icon-1.28.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>vaidya-etal-2019-samajh</bibkey>
    </paper>
  </volume>
</collection>