<collection id="2020.fever">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Fact Extraction and VERification (FEVER)</booktitle>
      <editor><first>Christos</first><last>Christodoulopoulos</last></editor>
      <editor><first>James</first><last>Thorne</last></editor>
      <editor><first>Andreas</first><last>Vlachos</last></editor>
      <editor><first>Oana</first><last>Cocarascu</last></editor>
      <editor><first>Arpit</first><last>Mittal</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="ee435e98">2020.fever-1</url>
    </meta>
    <frontmatter>
      <url hash="c5f632b1">2020.fever-1.0</url>
      <bibkey>fever-2020-fact</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Simple Compounded-Label Training for Fact Extraction and Verification</title>
      <author><first>Yixin</first><last>Nie</last></author>
      <author><first>Lisa</first><last>Bauer</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>1&#8211;7</pages>
      <abstract>Automatic fact checking is an important task motivated by the need for detecting and preventing the spread of misinformation across the web. The recently released FEVER challenge provides a benchmark task that assesses systems&#8217; capability for both the retrieval of required evidence and the identification of authentic claims. Previous approaches share a similar pipeline training paradigm that decomposes the task into three subtasks, with each component built and trained separately. Although achieving acceptable scores, these methods induce difficulty for practical application development due to unnecessary complexity and expensive computation. In this paper, we explore the potential of simplifying the system design and reducing training computation by proposing a joint training setup in which a single sequence matching model is trained with compounded labels that give supervision for both sentence selection and claim verification subtasks, eliminating the duplicate computation that occurs when models are designed and trained separately. Empirical results on FEVER indicate that our method: (1) outperforms the typical multi-task learning approach, and (2) gets comparable results to top performing systems with a much simpler training setup and less training computation (in terms of the amount of data consumed and the number of model parameters), facilitating future works on the automatic fact checking task and its practical usage.</abstract>
      <url hash="0472bb38">2020.fever-1.1</url>
      <doi>10.18653/v1/2020.fever-1.1</doi>
      <video href="http://slideslive.com/38929663" />
      <bibkey>nie-etal-2020-simple</bibkey>
    </paper>
    <paper id="5">
      <title>Language Models as Fact Checkers?</title>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Belinda Z.</first><last>Li</last></author>
      <author><first>Sinong</first><last>Wang</last></author>
      <author><first>Wen-tau</first><last>Yih</last></author>
      <author><first>Hao</first><last>Ma</last></author>
      <author><first>Madian</first><last>Khabsa</last></author>
      <pages>36&#8211;41</pages>
      <abstract>Recent work has suggested that language models (LMs) store both common-sense and factual knowledge learned from pre-training data. In this paper, we leverage this implicit knowledge to create an effective end-to-end fact checker using a solely a language model, without any external knowledge or explicit retrieval components. While previous work on extracting knowledge from LMs have focused on the task of open-domain question answering, to the best of our knowledge, this is the first work to examine the use of language models as fact checkers. In a closed-book setting, we show that our zero-shot LM approach outperforms a random baseline on the standard FEVER task, and that our finetuned LM compares favorably with standard baselines. Though we do not ultimately outperform methods which use explicit knowledge bases, we believe our exploration shows that this method is viable and has much room for exploration.</abstract>
      <url hash="fcff8bf1">2020.fever-1.5</url>
      <doi>10.18653/v1/2020.fever-1.5</doi>
      <video href="http://slideslive.com/38929662" />
      <bibkey>lee-etal-2020-language</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    </volume>
</collection>