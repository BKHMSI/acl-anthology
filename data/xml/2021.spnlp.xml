<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.spnlp">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)</booktitle>
      <editor><first>Zornitsa</first><last>Kozareva</last></editor>
      <editor><first>Sujith</first><last>Ravi</last></editor>
      <editor><first>Andreas</first><last>Vlachos</last></editor>
      <editor><first>Priyanka</first><last>Agrawal</last></editor>
      <editor><first>André</first><last>Martins</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="3b7ac25c">2021.spnlp-1</url>
    </meta>
    <frontmatter>
      <url hash="32da2870">2021.spnlp-1.0</url>
      <bibkey>spnlp-2021-structured</bibkey>
    </frontmatter>
    <paper id="1">
      <title>RewardsOfSum : Exploring Reinforcement Learning Rewards for Summarisation<fixed-case>R</fixed-case>ewards<fixed-case>O</fixed-case>f<fixed-case>S</fixed-case>um: Exploring Reinforcement Learning Rewards for Summarisation</title>
      <author><first>Jacob</first><last>Parnell</last></author>
      <author><first>Inigo</first><last>Jauregi Unanue</last></author>
      <author><first>Massimo</first><last>Piccardi</last></author>
      <pages>1–11</pages>
      <abstract>To date, most abstractive summarisation models have relied on variants of the negative log-likelihood (NLL) as their training objective. In some cases, <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> has been added to train the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> with an objective that is closer to their evaluation measures (e.g. ROUGE). However, the <a href="https://en.wikipedia.org/wiki/Reward_system">reward function</a> to be used within the reinforcement learning approach can play a key role for performance and is still partially unexplored. For this reason, in this paper, we propose two reward functions for the task of abstractive summarisation : the first function, referred to as RwB-Hinge, dynamically selects the samples for the gradient update. The second <a href="https://en.wikipedia.org/wiki/Function_(mathematics)">function</a>, nicknamed RISK, leverages a small pool of strong candidates to inform the reward. In the experiments, we probe the proposed approach by fine-tuning an NLL pre-trained model over nine summarisation datasets of diverse size and nature. The experimental results show a consistent improvement over the negative log-likelihood baselines.</abstract>
      <url hash="cac9850f">2021.spnlp-1.1</url>
      <doi>10.18653/v1/2021.spnlp-1.1</doi>
      <bibkey>parnell-etal-2021-rewardsofsum</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
    <title_ar>RewardsOfSum: استكشاف مكافآت التعلم المعززة للتلخيص</title_ar>
      <title_fr>RewardsOfSum : Explorer les récompenses d'apprentissage par renforcement pour la synthèse</title_fr>
      <title_pt>RewardsOfSum: Explorando Recompensas de Aprendizado por Reforço para Resumir</title_pt>
      <title_es>RewardsOfSum: Explorando las recompensas de aprendizaje por refuerzo para resumir</title_es>
      <title_ja>RewardsOfSum ：要約のための強化学習報酬の探求</title_ja>
      <title_zh>赏总结:探总结</title_zh>
      <title_hi>RewardsOfSum: सारांशीकरण के लिए सुदृढीकरण सीखने के पुरस्कारों की खोज</title_hi>
      <title_ru>RewardsOfSum: Изучение наград за обучение подкреплению для подведения итогов</title_ru>
      <title_ga>RewardsOfSum: Ag Iniúchadh ar Fhoghlaim Treisithe Luaíochtaí le haghaidh Achoimre</title_ga>
      <title_ka>გადავიწყება</title_ka>
      <title_hu>RewardsOfSum: A megerősítési tanulási jutalmak feltárása az összefoglaláshoz</title_hu>
      <title_el>ΑνταμοιβέςOfSum: Εξερευνώντας Ανταμοιβές Μάθησης Ενίσχυσης για Σύνοψη</title_el>
      <title_it>RewardsOfSum: Esplorare le ricompense per l'apprendimento di rinforzo per la sintesi</title_it>
      <title_kk>Қайталау</title_kk>
      <title_mk>Одплатите од сумата: Истражување на наградите за зајакнување на учењето за резултат</title_mk>
      <title_lt>RewardsOfSum: Reinforcement Learning reward for Summary</title_lt>
      <title_ml>തിരിച്ചുവരുന്ന OfSum: Exploring Reinforcement Learning Rewards for Summary</title_ml>
      <title_mn>RewardsOfSum: Баталгаа суралцах үйл ажиллагааг судлах</title_mn>
      <title_mt>RigwardsOfSum: L-esplorazzjoni tar-Rigwardji tat-Tagħlim għat-Tisħiħ għas-Sommarju</title_mt>
      <title_pl>RewardsOfSum: Badanie nagród uczenia się wzmocnienia w celu podsumowania</title_pl>
      <title_no>GjenopprettaOfSum: Utforskar forstørring av forstørringar tilbake for sammendrag</title_no>
      <title_sr>Vratite se OfSum: istraživanje pojačanja učenja nagrade za sažetak</title_sr>
      <title_si>RewardsofSum: ප්‍රශ්නයක් විශ්වාස කරනවා ආපහු ප්‍රශ්නයක් ඉගෙනගන්න ආපහු ප්‍රශ්නයක්</title_si>
      <title_so>RewardsOfSum: Exploring Reinforcement Learning Rewards for Summary</title_so>
      <title_sv>RewardsOfSum: Exploring Förstärkning Lärande belöningar för Sammanfattning</title_sv>
      <title_ms>RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation</title_ms>
      <title_ta>மீண்டும் OfSum: Exploring Reinforcement Learning Rewards for Summary</title_ta>
      <title_ur>RewardsOfSum: reinforcement Learning Reward for Summarisation</title_ur>
      <title_ro>RecompensOfSum: Explorarea recompenselor de învățare pentru rezumare</title_ro>
      <title_uz>OfSum: Exploring Reinforcement Learning Rewards for Summary</title_uz>
      <title_vi>Phần bổ sung:</title_vi>
      <title_bg>Награди за обобщаване на знанията за подсилване</title_bg>
      <title_hr>RewardsOfSum: istraživanje pojačanja učenja nagrade za sažetak</title_hr>
      <title_da>RewardsOfSum: Udforsk forstærket læringsbelønninger til opsummering</title_da>
      <title_nl>RewardsOfSum: Het verkennen van Reinforcement Learning Rewards voor samenvatting</title_nl>
      <title_de>RewardsOfSum: Exploring Reinforcement Learning Belohnungen zur Zusammenfassung</title_de>
      <title_ko>보상 총결산: 탐색 강화 학습 보상 총결산</title_ko>
      <title_id>RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation</title_id>
      <title_fa>بازگشت‌سازیOfSum: تحقیق توسعه یادگیری پاداش یادگیری برای جمع‌سازی</title_fa>
      <title_af>Herstel OfSum: Ondersoek Verbevesting Leer Herstelling vir Opsomming</title_af>
      <title_sw>Upande wa Mpya: Kuchunguza Kufundisha Kufundisha Kujifunza</title_sw>
      <title_tr>Täzeleşme</title_tr>
      <title_sq>RewardsOfSum: Shqyrtimi i shpërblimeve të mësimit të forcuar për përmbledhjen</title_sq>
      <title_am>Sum: Exploring Reinforcement Learning Rewards for Summary</title_am>
      <title_bn>পুনরায় অফ সাম: সামারসংক্রান্ত শিক্ষা শিক্ষা প্রত্যাবর্তন করা হচ্ছে</title_bn>
      <title_hy>RerewsOFSum: Exploring BookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBook</title_hy>
      <title_bs>RewardsOfSum: istraživanje pojačanja učenja nagrade za sažetak</title_bs>
      <title_az>RewardsOfSum</title_az>
      <title_ca>RewardsOfSum: Explorar Rewards d'Aprendiment de reforç per a resumir</title_ca>
      <title_et>RewardsOfSum: Tugevdamise õppe uurimine</title_et>
      <title_fi>RewardsOfSum: Vahvistusoppimisen tutkiminen</title_fi>
      <title_cs>RewardsOfSum: Prozkoumání odměn za posílení učení pro shrnutí</title_cs>
      <title_jv>undo-type</title_jv>
      <title_he>פרסים של סכום: לחקור פרסים ללימודים מחדשים לסיום</title_he>
      <title_sk>RewardsOfSum: Raziskovanje okrepitve učenja Nagrade za povzetek</title_sk>
      <title_ha>Sum: Exploring reInheritor Learn Learn for Summary</title_ha>
      <title_bo>RewardsOfSum: བསྐྱར་གསོ་སྡུད་ཀློག་སྟངས་ལ་བསྡུས་དང་།</title_bo>
      <abstract_ar>حتى الآن ، اعتمدت معظم نماذج التلخيص التجريدية على متغيرات احتمالية السجل السلبي (NLL) كهدف تدريبي لها. في بعض الحالات ، تمت إضافة التعلم المعزز لتدريب النماذج بهدف أقرب إلى تدابير التقييم الخاصة بهم (مثل ROUGE). ومع ذلك ، فإن وظيفة المكافأة التي سيتم استخدامها في نهج التعلم المعزز يمكن أن تلعب دورًا رئيسيًا في الأداء ولا تزال غير مستكشفة جزئيًا. لهذا السبب ، في هذه الورقة ، نقترح وظيفتين للمكافأة لمهمة التلخيص التجريدي: الوظيفة الأولى ، المشار إليها باسم RwB-Hinge ، تختار ديناميكيًا العينات لتحديث التدرج. الوظيفة الثانية ، الملقبة بـ RISK ، تستفيد من مجموعة صغيرة من المرشحين الأقوياء لإبلاغ المكافأة. في التجارب ، قمنا بفحص النهج المقترح من خلال ضبط نموذج NLL مدرب مسبقًا على تسع مجموعات بيانات تلخيص ذات أحجام وطبيعة متنوعة. تظهر النتائج التجريبية تحسنًا ثابتًا على خطوط الأساس السلبية للسجل المحتمل.</abstract_ar>
      <abstract_es>Hasta la fecha, la mayoría de los modelos de resumen abstractivo se han basado en variantes de la probabilidad logarítmica negativa (NLL) como objetivo de entrenamiento. En algunos casos, se ha añadido el aprendizaje por refuerzo para entrenar a los modelos con un objetivo más cercano a sus medidas de evaluación (por ejemplo, ROUGE). Sin embargo, la función de recompensa que se utilizará dentro del enfoque de aprendizaje por refuerzo puede desempeñar un papel clave para el rendimiento y aún no se ha explorado parcialmente. Por esta razón, en este artículo proponemos dos funciones de recompensa para la tarea de resumen abstractivo: la primera función, denominada RWB-Hinge, selecciona dinámicamente las muestras para la actualización del gradiente. La segunda función, apodada RISK, aprovecha un pequeño grupo de candidatos fuertes para informar la recompensa. En los experimentos, probamos el enfoque propuesto ajustando un modelo previamente entrenado de NLL en nueve conjuntos de datos de resumen de diversos tamaños y naturaleza. Los resultados experimentales muestran una mejora constante con respecto a las líneas de base de probabilidad logarítmica negativa.</abstract_es>
      <abstract_fr>À ce jour, la plupart des modèles de synthèse abstraits se sont appuyés sur des variantes du log de vraisemblance négatif (NLL) comme objectif de formation. Dans certains cas, l'apprentissage par renforcement a été ajouté pour former les modèles avec un objectif plus proche de leurs mesures d'évaluation (par exemple ROUGE). Cependant, la fonction de récompense à utiliser dans le cadre de l'approche d'apprentissage par renforcement peut jouer un rôle clé pour la performance et est encore partiellement inexplorée. Pour cette raison, dans cet article, nous proposons deux fonctions de récompense pour la tâche de synthèse abstraite : la première fonction, appelée RWB-hinge, sélectionne dynamiquement les échantillons pour la mise à jour du gradient. La deuxième fonction, surnommée RISK, tire parti d'un petit bassin de candidats solides pour informer la récompense. Dans les expériences, nous étudions l'approche proposée en ajustant un modèle pré-entraîné NLL sur neuf ensembles de données de synthèse de différentes tailles et de différentes natures. Les résultats expérimentaux montrent une amélioration constante par rapport aux valeurs de base du log de vraisemblance négative.</abstract_fr>
      <abstract_pt>Até o momento, a maioria dos modelos de sumarização abstrativos baseou-se em variantes do log-likelihood negativo (NLL) como seu objetivo de treinamento. Em alguns casos, o aprendizado por reforço foi adicionado para treinar os modelos com um objetivo mais próximo de suas medidas de avaliação (por exemplo, ROUGE). No entanto, a função de recompensa a ser usada dentro da abordagem de aprendizado por reforço pode desempenhar um papel fundamental para o desempenho e ainda é parcialmente inexplorada. Por esta razão, neste artigo, propomos duas funções de recompensa para a tarefa de sumarização abstrativa: a primeira função, denominada RwB-Hinge, seleciona dinamicamente as amostras para a atualização do gradiente. A segunda função, apelidada de RISK, aproveita um pequeno grupo de candidatos fortes para informar a recompensa. Nos experimentos, sondamos a abordagem proposta ajustando um modelo pré-treinado de NLL em nove conjuntos de dados de sumarização de diversos tamanhos e naturezas. Os resultados experimentais mostram uma melhoria consistente sobre as linhas de base de probabilidade logarítmica negativa.</abstract_pt>
      <abstract_ja>これまで、ほとんどの抽象的な要約モデルは、トレーニング目標として負の対数尤度（ NLL ）のバリアントに依存してきた。 場合によっては、モデルを評価尺度に近い目標で訓練するために強化学習が追加されている（例：ルージュ）。 しかし、強化学習アプローチ内で使用される報酬関数は、パフォーマンスのための重要な役割を果たすことができ、まだ部分的には探求されていません。 このため、本稿では、抽象的な要約のタスクのための2つの報酬関数を提案する。RwB - Hingeと呼ばれる最初の関数は、勾配更新のためのサンプルを動的に選択する。 2つ目の関数はリスクと呼ばれ、強力な候補者の小さなプールを活用して報酬を通知します。 実験では、多様なサイズと性質の9つの要約データセットにわたってNLL事前にトレーニングされたモデルを微調整することによって、提案されたアプローチを探索する。 実験結果は、負の対数尤度ベースラインよりも一貫した改善を示す。</abstract_ja>
      <abstract_zh>迄今为止,多抽象总结模形,皆赖负对数似然(NLL)变体以为训练。 加之以化学,近之以估(如ROUGE)。 然于化学之赏函数可以起关键作用,而犹未穷也。 由此言之,抽象总结二奖函数:一函数谓之RwB-Hinge,动择梯度新样本。 第二职,绰号为RISK,因一小群强选以通奖。 实验之中,因九大小性质之总数集上微调NLL预练模形以探其术。 实验结果表明,比负对数似然基线,实验终改善。</abstract_zh>
      <abstract_hi>आज तक, अधिकांश अमूर्त सारांश मॉडल ने अपने प्रशिक्षण उद्देश्य के रूप में नकारात्मक लॉग-संभावना (एनएलएल) के रूपों पर भरोसा किया है। कुछ मामलों में, सुदृढीकरण सीखने को एक उद्देश्य के साथ मॉडल को प्रशिक्षित करने के लिए जोड़ा गया है जो उनके मूल्यांकन उपायों (जैसे रूज) के करीब है। हालांकि, सुदृढीकरण सीखने के दृष्टिकोण के भीतर उपयोग किए जाने वाले इनाम फ़ंक्शन प्रदर्शन के लिए एक महत्वपूर्ण भूमिका निभा सकते हैं और अभी भी आंशिक रूप से अज्ञात हैं। इस कारण से, इस पेपर में, हम अमूर्त सारांशीकरण के कार्य के लिए दो इनाम कार्यों का प्रस्ताव करते हैं: पहला फ़ंक्शन, जिसे आरडब्ल्यूबी-हिंज के रूप में जाना जाता है, गतिशील रूप से ग्रेडिएंट अपडेट के लिए नमूनों का चयन करता है। दूसरा फ़ंक्शन, जिसका उपनाम जोखिम है, इनाम को सूचित करने के लिए मजबूत उम्मीदवारों के एक छोटे से पूल का लाभ उठाता है। प्रयोगों में, हम विभिन्न आकार और प्रकृति के नौ सारांश डेटासेट पर एक एनएलएल पूर्व-प्रशिक्षित मॉडल को ठीक करके प्रस्तावित दृष्टिकोण की जांच करते हैं। प्रयोगात्मक परिणाम नकारात्मक लॉग-संभावना बेसलाइन पर एक सुसंगत सुधार दिखाते हैं।</abstract_hi>
      <abstract_ru>На сегодняшний день большинство моделей абстрактного обобщения опираются на варианты отрицательного логарифмического правдоподобия (НЛЛ) в качестве цели обучения. В некоторых случаях было добавлено дополнительное обучение для обучения моделей с целью, которая ближе к их оценке (например, ROUGE). Однако функция вознаграждения, которая будет использоваться в подходе к обучению подкреплению, может играть ключевую роль для эффективности и все еще частично не изучена. По этой причине в данной работе мы предлагаем две функции вознаграждения для задачи абстрактного обобщения: первая функция, называемая RwB-Hinge, динамически выбирает выборки для обновления градиента. Вторая функция, прозванная РИСК, использует небольшой пул сильных кандидатов, чтобы информировать награду. В экспериментах мы исследуем предлагаемый подход путем тонкой настройки предварительно обученной модели NLL для девяти сводных наборов данных различного размера и природы. Экспериментальные результаты показывают постоянное улучшение по сравнению с отрицательными исходными уровнями логарифмической правдоподобия.</abstract_ru>
      <abstract_ga>Go dtí seo, bhí an chuid is mó de na samhlacha achoimre teibí ag brath ar leaganacha de na dóchúlachtaí logála diúltacha (NLL) mar chuspóir oiliúna acu. I gcásanna áirithe, cuireadh leis an bhfoghlaim atreisithe chun na samhlacha a oiliúint le cuspóir atá níos gaire dá mbearta meastóireachta (e.g. ROUGE). Mar sin féin, féadfaidh an fheidhm luach saothair atá le húsáid laistigh den chur chuige foghlama treisithe príomhról a imirt maidir le feidhmíocht agus tá sé fós gan iniúchadh i bpáirt. Ar an ábhar sin, sa pháipéar seo, molaimid dhá fheidhm luaíochta don tasc a bhaineann le hachoimriú teibí: roghnaíonn an chéad fheidhm, ar a dtugtar RwB-Hinge, na samplaí don nuashonrú grádáin go dinimiciúil. Déanann an dara feidhm, ar a dtugtar RISK, giaráil ar líon beag iarrthóirí láidre chun an luach saothair a chur ar an eolas. Sna turgnaimh, déanaimid iniúchadh ar an gcur chuige atá beartaithe trí mhionchoigeartú a dhéanamh ar shamhail réamh-oilte NLL thar naoi tacar sonraí achoimriúcháin de mhéid agus nádúr éagsúil. Léiríonn na torthaí turgnamhacha feabhas comhsheasmhach thar na bonnlínte diúltacha dóchúlachta logála.</abstract_ga>
      <abstract_hu>Eddig a legtöbb absztraktív összefoglaló modell a negatív log-valószínűség (NLL) variánsaira támaszkodott képzési célként. Egyes esetekben kiegészítették a megerősített tanulást annak érdekében, hogy a modelleket olyan célkitűzéssel képezzék, amely közelebb áll az értékelési intézkedésekhez (pl. ROUGE). A megerősítő tanulási megközelítésben alkalmazott jutalmazási funkció azonban kulcsszerepet játszhat a teljesítmény szempontjából, és részben még mindig feltáratlan. Ezért ebben a tanulmányban két jutalomfüggvényt javasolunk az absztraktív összefoglalás feladatához: az első függvény, amelyet RwB-Hinge néven említenek, dinamikusan választja ki a gradiens frissítéséhez szükséges mintákat. A második funkció, a Kockázat beceneve, erős jelöltek kis csoportját használja fel a jutalom tájékoztatására. A kísérletek során a javasolt megközelítést úgy vizsgáljuk, hogy egy NLL előre képzett modell finomhangolásával kilenc, különböző méretű és természetű összefoglaló adatkészleten alapul. A kísérleti eredmények következetes javulást mutatnak a negatív log-valószínűség alapjaihoz képest.</abstract_hu>
      <abstract_el>Μέχρι σήμερα, τα περισσότερα μοντέλα αφηρημένης σύνοψης βασίστηκαν σε παραλλαγές της αρνητικής πιθανότητας καταγραφής (NLL) ως εκπαιδευτικό στόχο τους. Σε ορισμένες περιπτώσεις, προστέθηκε η ενισχυτική μάθηση για την κατάρτιση των μοντέλων με στόχο πιο κοντά στα μέτρα αξιολόγησης τους (π.χ. ROUGE). Ωστόσο, η λειτουργία ανταμοιβής που θα χρησιμοποιηθεί στο πλαίσιο της προσέγγισης ενίσχυσης μάθησης μπορεί να διαδραματίσει βασικό ρόλο για την απόδοση και εξακολουθεί εν μέρει να είναι ανεξερεύνητη. Για το λόγο αυτό, στην παρούσα εργασία, προτείνουμε δύο λειτουργίες ανταμοιβής για το έργο της αφηρημένης σύνοψης: η πρώτη συνάρτηση, που αναφέρεται ως αρθρωτή, επιλέγει δυναμικά τα δείγματα για την ενημέρωση της διαβάθμισης. Η δεύτερη λειτουργία, με το ψευδώνυμο ΚΙΝΔΥΝΟΣ, αξιοποιεί μια μικρή ομάδα ισχυρών υποψηφίων για να ενημερώσει την ανταμοιβή. Στα πειράματα, εξετάζουμε την προτεινόμενη προσέγγιση με την τελειοποίηση ενός προ-εκπαιδευμένου μοντέλου σε εννέα σύνολα δεδομένων σύνοψης διαφορετικού μεγέθους και φύσης. Τα πειραματικά αποτελέσματα δείχνουν συνεπή βελτίωση σε σχέση με τις αρνητικές γραμμές καταγραφής πιθανοτήτων βάσης.</abstract_el>
      <abstract_ka>ახლა, უფრო მეტი აბსტრაქტიური სიმბოლოების მოდელები გადარწმუნდება, როგორც საკუთარი საკუთარი საკუთარი მიზეზების გარიანტებით. რამდენიმე შემთხვევაში სწავლება დამატებულია, რომ მოდელები გასწავლათ მისი მიზეზი, რომელიც უფრო კიდევ მისი გაუმუშავება (მაგალითად ROUGE). მაგრამ, სამუშაო ფუნქცია, რომელიც სწავლად სწავლას შესაძლებლობად გამოიყენება, შეიძლება გავაკეთოთ სამუშაო პროლის გასაკეთებლად და უკვე ნაწილა ამ მიზეზით, ამ დომენტში, ჩვენ აბსტრაქტიგური სიმბოლოების დასაწყისთვის ორი სამუშაო ფუნქციების შესაძლებლობა: პირველი ფუნქცია, რომელიც RwB-Hinge, დინამიკურად განახლებელად მეორე ფუნქცია, სახელი RISK, ძალიან ძალიან კანდიდენტების მარტივი ბასონი იმპორტირება. ექსპერიმენტებში, ჩვენ შევცვალობთ წარმოიდგინული პროგრამა, რომელიც NLL-ს წარმოადგილებული მოდელის შესახებ, ცხრა განსხვავებული განსხვავებული განსხვავებული განსხვავებული მო ექსპერიმენტიური წარმოდგენების შესაძლებლობა მინდომენტიური ლოგური შესაძლებლობაზე შესაძლებელი დააწყება.</abstract_ka>
      <abstract_kk>Күніне, абстрактивтік жазу үлгілерінің көпшілігі негативті журнал мүмкіндігі (NLL) бақылау мақсаты ретінде тәуелді. Кейбір жағдайда, үлгілерді оқыту мақсаттарына жақын оқыту үшін қолданылады (мысалы, ROUGE). Бірақ қолданылатын көмектесу тәсілдерінде қолданылатын көмектесу функциясы оқу тәсілдерінің көмектесу үшін көмектесу рөлін ойлап, бірақ бөлімі күтпеген Бұл себептен, бұл қағазда, абстрактивті тұжырымдамасының тапсырмасының екі жоғары функциясын ұсынамыз: RwB- Hinge деп аталатын бірінші функциясы, динамикалық градиенттің жаңарту үшін үлгіле Екінші функциясы, RISK атауы үшін үлкен кандидаттардың кішкентай жиілігін қолданады. Тәжірибелерде, біз NLL алдын- ала оқылған үлгілерді тоғыз көлемі мен табиғатты қосымша деректер жинақтарының түрлі түрлі мәліметі мен қасиеттің түрлі түрлендіру арқылы ұсын Тәжірибелі нәтижелер негативті журнал мүмкіндіктерінің негативті жолдарында тұрақты жақсарту көрсетеді.</abstract_kk>
      <abstract_it>Ad oggi, la maggior parte dei modelli di sintesi astratti si è basata su varianti della probabilità di log negativa (NLL) come obiettivo formativo. In alcuni casi, è stato aggiunto il rafforzamento dell'apprendimento per formare i modelli con un obiettivo più vicino alle loro misure di valutazione (ad esempio ROUGE). Tuttavia, la funzione di ricompensa da utilizzare all'interno dell'approccio di apprendimento di rinforzo può svolgere un ruolo chiave per le prestazioni ed è ancora parzialmente inesplorata. Per questo motivo, in questo articolo, proponiamo due funzioni di ricompensa per il compito di sintesi astratta: la prima funzione, detta RwB-Hinge, seleziona dinamicamente i campioni per l'aggiornamento gradiente. La seconda funzione, soprannominata RISK, sfrutta un piccolo pool di candidati forti per informare la ricompensa. Negli esperimenti, sondiamo l'approccio proposto mettendo a punto un modello pre-addestrato NLL su nove set di dati di sintesi di diverse dimensioni e natura. I risultati sperimentali mostrano un miglioramento costante rispetto ai valori di riferimento negativi per la probabilità di log.</abstract_it>
      <abstract_lt>Iki šiol dauguma abstrakcinių santraukų modelių, kaip mokymo tikslas, rėmėsi neigiamos log tikimybės variantais. Kai kuriais atvejais mokymasis stiprinamas siekiant modelių mokymo tikslo, kuris yra artimesnis jų vertinimo priemonėms (pvz., ROUGE). Vis dėlto darbo užmokesčio funkcija, naudojama taikant mokymosi stiprinimu metodą, gali atlikti svarbų vaidmenį veiklos rezultatams ir vis dar iš dalies neištirta. Dėl šios priežasties šiame dokumente siūlome dvi atlyginimo funkcijas už abstrakcinės santraukos užduotį: pirmoji funkcija, vadinama RwB-Hinge, dinamiškai atrenka mėginius gradientui atnaujinti. Antroji funkcija, vadinama RISK, suteikia galimybę informuoti mažą grupę stiprių kandidatų apie atlygį. Eksperimentuose ištiriame siūlomą metodą tiksliai pritaikant iš anksto parengtą NLL model į į devynis įvairių dydžių ir gamtos duomenų rinkinius. Eksperimentiniai rezultatai rodo nuoseklų pagerėjimą palyginti su neigiama log tikimybės bazine verte.</abstract_lt>
      <abstract_mk>Досега, повеќето апстрактивни модели за резултат се потпираа на варијантите на негативната веројатност на логот (НЛЛ) како нивна обука. Во некои случаи, додадено е зајакнување на учењето за обука на моделите со цел кој е поблиску до нивните мерки за проценка (np. ROUGE). Сепак, функцијата на награда која треба да се користи во рамките на пристапот на зајакнување на учењето може да одигра клучна улога за изведувањето и сé уште е делумно неиспитана. Од оваа причина, во овој весник, предложуваме две функции на награда за задачата на апстрактивна резултатација: првата функција, наречена RwB-Hinge, динамично ги избира примероците за градијантното ажурирање. Втората функција, наречена РИСК, користи мал базен силни кандидати за да ја информира наградата. Во експериментите, го испитуваме предложениот пристап со финетизирање на предобучениот модел на НЛЛ преку девет резултати на податоци од различна големина и природа. Експерименталните резултати покажуваат константно подобрување во однос на базите на негативните лог-веројатности.</abstract_mk>
      <abstract_ms>Sehingga kini, kebanyakan model ringkasan abstraktif telah bergantung pada varian kemungkinan log negatif (NLL) sebagai objektif latihan mereka. In some cases, reinforcement learning has been added to train the models with an objective that is closer to their evaluation measures (e.g. ROUGE).  However, the reward function to be used within the reinforcement learning approach can play a key role for performance and is still partially unexplored.  Untuk sebab ini, dalam kertas ini, kami cadangkan dua fungsi hadiah untuk tugas ringkasan abstraktif: fungsi pertama, yang disebut sebagai RwB-Hinge, memilih secara dinamik sampel untuk kemaskini gradien. Fungsi kedua, bernama RISK, menggunakan sekumpulan kecil calon kuat untuk memberitahu hadiah. Dalam percubaan ini, kita menguji pendekatan yang direncanakan dengan menyesuaikan model NLL yang dilatih-dilatih lebih dari sembilan set data ringkasan saiz dan sifat berbeza. Hasil percubaan menunjukkan peningkatan konsisten atas garis dasar log-kemungkinan negatif.</abstract_ms>
      <abstract_ml>ഇന്നത്തേക്കുള്ള സമയത്ത്, ഏറ്റവും അസാധ്യതയില്ലാത്ത ചിഹ്നങ്ങളുടെ മോഡല്‍ അവരുടെ പരിശീലനത്തിന്റെ ലക്ഷ്യം ആയിരിക്കുന്നു. In some cases, reinforcement learning has been added to train the models with an objective that is closer to their evaluation measures (e.g. ROUGE).  എന്നാലും, കൂടുതല്‍ പഠിക്കുന്നതിനുള്ളില്‍ ഉപയോഗിക്കുന്ന പ്രതിഫലം പ്രദര്‍ശനത്തിനുള്ള താക്കോല്‍ പ്രവർത്തിക്കുന്നത ഈ പേപ്പറില്‍ നമ്മള്‍ രണ്ട് പ്രതിഫല പ്രവര്‍ത്തനങ്ങള്‍ പ്രായശ്ചിത്തം ചെയ്യുന്നു: ആദ്യത്തെ പ്രവര്‍ത്തനത്തിന് RwB-ഹിങ്ങ് എന്ന് വിളിക്കുന്നു. ഗ്രേഡിഗ്ര രണ്ടാമത്തെ പ്രവൃത്തിക്ക്, RISK എന്ന പേര്, പ്രതിഫലം അറിയിക്കാന്‍ ശക്തിയുള്ള പ്രാര്‍ത്ഥികളുടെ ഒരു ചെറിയ പൂള പരീക്ഷണങ്ങളില്‍, നമ്മള്‍ പ്രൊദ്ദേശിക്കപ്പെട്ട ഒരു NLL മുമ്പ് പരിശീലിക്കപ്പെട്ട മോഡലിനെ പരിശോധിക്കുന്നത് നമ്മള്‍ പരിശോധിക്ക പരീക്ഷണത്തിന്റെ ഫലങ്ങള്‍ നെഗറ്റീവ് ലോഗ് സാധ്യതയുള്ള അടിസ്ഥാനത്തില്‍ മെച്ചപ്പെടുത്തുന്നത് കാണിക്</abstract_ml>
      <abstract_mn>Өнөөдөр ихэнх абстрактив тодорхойлолтын загварууд нь тэдний сургалтын зорилго болгон сөрөг боломжтой (NLL) логины хувилбаруудыг хамаарна. Зарим тохиолдолд, загваруудыг оюутнуудын шалгалтын шалгалтын тулд хамгийн ойрхон зорилготой зорилгоор сургалтыг нэмэгдүүлсэн. Гэхдээ шагналын функцийг нэмэгдүүлэх суралцах аргын дотор ашиглах шагналын функц нь үйл ажиллагаанд хамгийн чухал үүрэг тоглож чадна. Гэхдээ хэсэг хэсэг нь тодорхойгүй. Ийм шалтгаанаас, энэ цаасан дээр бид abstractive summary-ын даалгаварын хоёр шагналын функцүүдийг санал болгож байна: RwB-Hinge гэдэг анхны функц, динамик градиентын жагсаалтын жишээг сонгож байна. Хоёр дахь функц, RISK нэртэй нэртэй нэртэй, шагналыг мэдээллийн тулд хүчирхэг хүндрэгчдийн жижиг хэмжээг ашигладаг. Эдгээр туршилтуудын тулд бид NLL сургалтын өмнө сургалтын загварыг 9 дахин олон хэмжээст болон байгалийн тодорхойлолтын хэмжээний өгөгдлийн сангуудыг судалж байна. Үүний туршилтын үр дүнд сөрөг лог магадлалын суурь шугам дээр тогтмол сайжруулалт гаргадаг.</abstract_mn>
      <abstract_pl>Do tej pory większość abstrakcyjnych modeli podsumowania opierała się na wariantach negatywnej prawdopodobieństwa logowania (NLL) jako celu szkoleniowego. W niektórych przypadkach dodano uczenie się wzmacniające w celu szkolenia modeli z celem bliższym do ich środków oceny (np. ROUGE). Jednakże funkcja nagrody, która ma być wykorzystywana w ramach podejścia do uczenia się wzmacniającego, może odgrywać kluczową rolę dla wydajności i jest nadal częściowo niezbadana. Z tego względu w niniejszym artykule proponujemy dwie funkcje nagrody dla zadania abstrakcyjnego podsumowania: pierwsza funkcja, zwana RwB-Zawiasem, dynamicznie dobiera próbki do aktualizacji gradientu. Druga funkcja, zwana RISK, wykorzystuje niewielką pulę silnych kandydatów do informowania o nagrodzie. W eksperymentach badamy proponowane podejście poprzez dostrojenie wstępnie przeszkolonego modelu NLL na dziewięciu zestawach danych podsumowujących o różnej wielkości i charakterze. Wyniki eksperymentalne wskazują na stałą poprawę w stosunku do ujemnych linii bazowych log-prawdopodobieństwa.</abstract_pl>
      <abstract_no>Til dag har dei fleste abstraktive sammendragsmodulane relieve på variantane av den negativ loggsannsynligheten (NLL) som opplæringsmålet. I enkelte tilfeller er stipringslæring lagt til for å lære modellen med eit mål som er nærmere deres evalueringsmål (f.eks. ROUGE). Dette funksjonen kan imidlertid brukast i læringstilnærminga for reinforcement kan spela ein nøkkelrolle for utviklinga og er fortsatt delvis uventa. I denne papiret foreslår vi to løftefunksjonar for oppgåva av abstraktive sammendrag: den første funksjonen, kalla som RwB-Hinge, dynamisk veljer prøver for oppdateringa av fargeovergangane. Den andre funksjonen, kallenamnet RISK, leverer ein liten pool av sterke kandidatar for å informere renten. I eksperimentene prøver vi den foreslåde tilnærminga ved å finne opp ein NLL-føretrained modell over ni samanseringsdata med ulike storleik og natur. Eksperimentale resultatet viser ein konsistent forbedring over den negative logsannsynlege baselinjene.</abstract_no>
      <abstract_mt>Sal-lum, il-biċċa l-kbira tal-mudelli abstrattivi ta’ sommarju ddependew fuq varjanti tal-probabbiltà negattiva ta’ reġistrazzjoni (NLL) bħala l-għan tat-taħriġ tagħhom. F’xi każijiet, it-tagħlim ta’ rinfurzar ġie miżjud biex jitħarrġu l-mudelli b’objettiv li huwa eqreb g ħall-miżuri ta’ evalwazzjoni tagħhom (pereżempju ROUGE). Madankollu, il-funzjoni ta’ premju li għandha tintuża fl-approċċ tat-tagħlim ta’ rinfurzar jista’ jkollha rwol ewlieni għall-prestazzjoni u għadha parzjalment mhux esplorata. Għal din ir-raġuni, f’dan id-dokument, qed nipproponu żewġ funzjonijiet ta’ premju għall-kompitu ta’ sommarju astrattiv: l-ewwel funzjoni, imsejħa RwB-Hinge, tagħżel dinamikament il-kampjuni għall-aġġornament tal-gradjenti. It-tieni funzjoni, magħrufa bħala RISK, tagħti spinta lil grupp żgħir ta’ kandidati b’saħħithom biex jinfurmaw il-premju. Fl-esperimenti, aħna nistudjaw l-approċċ propost billi nirranġaw mudell imħarreġ minn qabel tal-NLL fuq disa’ settijiet ta’ dejta ta’ sommarju ta’ daqs u natura varji. Ir-riżultati sperimentali juru titjib konsistenti fuq il-linji bażi negattivi tal-probabbiltà log.</abstract_mt>
      <abstract_ro>Până în prezent, cele mai multe modele abstractive de sinteză s-au bazat pe variante ale probabilității log negative (NLL) ca obiectiv de formare. În unele cazuri, s-a adăugat consolidarea învățării pentru a instrui modelele cu un obiectiv mai aproape de măsurile lor de evaluare (de exemplu, ROUGE). Cu toate acestea, funcția de recompensare care trebuie utilizată în cadrul abordării de învățare de consolidare poate juca un rol cheie pentru performanță și este încă parțial neexplorată. Din acest motiv, în această lucrare, propunem două funcții de recompensare pentru sarcina de rezumare abstractivă: prima funcție, denumită RwB-Balage, selectează dinamic eșantioanele pentru actualizarea gradientului. A doua funcție, poreclită RISK, mobilizează un mic grup de candidați puternici pentru a informa recompensa. În cadrul experimentelor, analizăm abordarea propusă prin reglarea fină a unui model pre-instruit NLL pe nouă seturi de date de rezumare de dimensiuni și natură diverse. Rezultatele experimentale arată o îmbunătăţire constantă faţă de valorile de referinţă ale probabilităţii jurnaliste negative.</abstract_ro>
      <abstract_sr>Do sada, većina apstraktivnih modela za sažetak oslanjaju se na variante negativne verovatnosti dnevnika (NLL) kao njihov cilj obuke. U nekim slučajevima, dodana je učenje pojačanja kako bi obučila modele objektivnim ciljem koji je bliži njihovim mjerama procjene (npr. ROUGE). Međutim, funkcija nagrade koja se koristi unutar pristupa pojačanja učenja može imati ključnu ulogu za izvođenje i još je djelomično neočekivana. Zbog ovog razloga, u ovom papiru predlažemo dve nagrade za zadatak abstraktivne sažetke: prva funkcija, pod nazivom RwB-Hinge, dinamički odabere uzorke za aktualizaciju gradienta. Druga funkcija, nadimak RISK, utiče na mali bazen jakih kandidata da obavijesti nagradu. U eksperimentima, istražujemo predloženi pristup kako bi finalizirali predobučeni model NLL preko devet rezimetriranih podataka različitih veličina i prirode. Eksperimentalni rezultati pokazuju konsekventno poboljšanje na osnovnim linijama negativne verovatnosti.</abstract_sr>
      <abstract_si>අවස්ථානයෙන්, ගොඩක් අවස්ථාවක් සංශ්‍ය සංශ්‍ය විද්‍යාපිත විද්‍යාපිත විද්‍යාපිත විද්‍යාපිත විද්‍යාපිත ව සමහර අවස්ථාවක් වලින්, විශ්වාස කරන්න පුළුවන් ඉගෙනගන්න ලැබුණා මොඩේල් එක්ක අභියෝගයක් සඳහා ඔවුන්ගේ විශ්වා නමුත්, ප්‍රතිචාර ප්‍රතිචාර ප්‍රයෝජනය විශ්වාස කරන්න පුළුවන් ප්‍රයෝජනය වෙනුවෙන් ප්‍රතිචාර ප්‍රයෝජන මේ හේතුවෙන්, මේ පත්තරයේ අපි ප්‍රතිචාර දෙකක් ප්‍රතිචාර කරන්න ප්‍රතිචාර දෙකක් ප්‍රතිචාර කරනවා: පළමු ප්‍රතිචාර ක්‍රියාව, RwB-හින දෙවෙනි ප්‍රකාර, RISK නාමය, ප්‍රතිචාරයක් තියෙන්න පුළුවන් පුළුවන් පුළුවන් පුළුවන් පුළුවන පරීක්ෂණයේ අපි ප්‍රශ්නයක් පරීක්ෂණය කරනවා NLL ප්‍රශ්නයක් ප්‍රශ්නයක් විවිධ ප්‍රමාණයක් සහ ස්වභාවිතයෙන් අනුවෙ පරීක්ෂණ ප්‍රතිචාර ප්‍රතිචාර ප්‍රතිචාර ප්‍රතිචාරයක් පෙන්වන්න පුළුවන් විශාලනයක් ව</abstract_si>
      <abstract_so>Maanta la joogo waxaa ku xiran qaababka habaarka ah oo ay ku xiran yihiin noocyo kala duduwan qoraalka diidmada ah (NLL) oo ah goal waxbarashadooda. Xaaladaha qaarkood waxaa lagu darsaday inaad kordhiso waxbarashada lagu barto tusaalayaasha, kaas oo ku dhow qaababka qiimeynta (tusaale ahaan ROUGE). Si kastaba ha ahaatee shaqada mushaarka lagu isticmaali karo habka kordhiska waxbarashadu waxay noqon kartaa qayb ka mid ah qayb la'aan. Sababtaas darteed waxan warqaddan ku qoran, waxaan u soo jeedaynaa laba shaqooyin oo mushaar ah oo u qoran shaqo la'aan oo la soo saaro cayiman ah: kooxda ugu horeeyay ee lagu magacaabay RwB-Hinge, oo lagu magacaabay dhaqdhaqaaqa samooyinka sawirida. Shaqooyinka labaad, magaca RISK, wuxuu soo saaraa balli yar oo ka mid ah kandidiyayaasha xoogga badan si uu mushahaarada u ogeysiiyo. Imtixaanka, waxaynu tijaabinaynaa dhaqdhaqaaqa la soo jeeday si fiican looga sameynayo model la tababaray NLL oo ka sarreeya sagaal xagaaminta macluumaad oo kala duduwan tirada iyo dabiicadda kala duduwan. resultinta imtixaanka waxaa ka muuqda horumarinta ku socota qorshaha suurtagalka diidiga ah.</abstract_so>
      <abstract_sv>Hittills har de flesta abstrakta sammanfattningsmodeller förlitat sig på varianter av negativ log-sannolikhet (NLL) som träningsmål. I vissa fall har förstärkt lärande lagts till för att utbilda modellerna med ett mål som ligger närmare deras utvärderingsåtgärder (t.ex. ROUGE). Den belöningsfunktion som används inom förstärkningsstrategin kan dock spela en nyckelroll för prestationen och är fortfarande delvis outforskad. Av denna anledning föreslår vi i denna uppsats två belöningsfunktioner för uppgiften med abstraktiv sammanfattning: den första funktionen, kallad RwB-Hinge, väljer dynamiskt proverna för gradientuppdateringen. Den andra funktionen, smeknamnet RISK, utnyttjar en liten pool av starka kandidater för att informera belöningen. I experimenten undersöker vi det föreslagna tillvägagångssättet genom att finjustera en NLL-förberedd modell över nio sammanfattningsdatauppsättningar av olika storlek och natur. De experimentella resultaten visar en konsekvent förbättring jämfört med den negativa log-sannolikhetsbaselinen.</abstract_sv>
      <abstract_ta>இந்த நேரத்திற்கு பெரும்பாலான சுருக்கம் மாதிரிகள் எதிர்மறை பதிவு சாத்தியமான (NLL) மாறிகளை அவர்களுடைய பயிற்சி பொருளாக நம் சில நிகழ்ச்சிகளில், கல்வி கற்றுக்கொள்ள மாதிரிகளை பயிற்சியுடன் சேர்க்கப்பட்டுள்ளது, அது அவர்களுடைய மதிப்பீட்டு அளவுகளுக்கு அருகில ஆயினும், வலுப்படுத்தல் முறையில் பயன்படுத்த வேண்டிய கூலி செயல்பாடு செயல்பாட்டிற்கான முக்கிய விளையாட்டை விளையாட இந்த காரணத்தில், நாம் செயல்பாட்டிற்கான இரண்டு கூலி செயல்பாடுகளை பரிந்துரைக்கிறோம்: முதல் செயல்பாடு, RwB- Hinge என்று குறிப்பிடப்பட்டால், தானாகவே சார் இரண்டாவது செயல்பாடு, RISK புனைப்பெயர், கூலி அறிவிப்பதற்கு ஒரு சிறிய குளியீட்டை வழங்குகிறது. பரிசோதனைகளில், நாம் பரிந்துரைக்கப்பட்ட செயல்பாட்டை சரியாக ஒரு NLL முன் பயிற்சி மாதிரியை ஒன்பது முறை சுருக்கி தகவல் அமைப்புகளில இந்த சோதனையின் முடிவுகள் எதிர்மறை பதிவு சாத்தியமான அடிப்படைகளை காட்டும்.</abstract_ta>
      <abstract_ur>یہاں تک، اکثر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غ بعض موقعیت میں، مدلس کی تعلیم کی زیادتی کے ساتھ اضافہ کی گئی ہے ایک موقعیت کے ساتھ جو ان کے ارزش اندازے کے قریب ہے (مثل ROUGE). However, the reward function to be used in reinforcement learning approach can play a key role for performance and still partially unexplored. اس وجہ سے، ہم اس کاغذ میں دو اجرت فناوری پیشنهاد کرتے ہیں کہ ابتراذی تعداد کے کام کے لئے دو اجرت فناوری: پہلی فناوری، RwB-Hinge کا نام لیا گیا ہے، سینامیک طور پر گراڈینٹ اوڈڈیٹ کے لئے نمونے انتخاب کرتے ہیں. دوسری فنقش، RISK کے نام کا نیک نام، مزدوری سے خبردار کرنے کے لئے ایک چھوٹا پائل ہے۔ آزمائش میں، ہم نے ایک NLL پیش آموزش کی مدل کو نو مختلف اندازے اور طبیعت کے ذریعہ سے اپنا پیش آموزش دینے والی ڈیٹ سٹ کے ذریعہ پیش آموزش دینے کے ذریعہ پیش آموزش دیے ہیں. آزمائش نتیجے منفی لاگ-احتمالات بنیس لین پر ایک ثابت قدم ترقی دکھاتے ہیں.</abstract_ur>
      <abstract_vi>Cho đến nay, hầu hết các mô hình tóm tắt trừu tượng đã dựa vào các biến thể của khả năng nhật ký âm (NLL) làm mục tiêu huấn luyện của chúng. Trong một số trường hợp, việc học gia cường đã được thêm vào để huấn luyện các mô- đun với một mục tiêu g ần hơn với các biện pháp đánh giá của chúng (v.d. ROSELE). Tuy nhiên, chức năng phần thưởng được sử dụng trong phương pháp huấn luyện gia tăng có thể đóng vai trò quan trọng trong việc vận hành và vẫn chưa được khám phá một phần. Vì vậy, trong tờ giấy này, chúng tôi đề xuất hai chức năng thưởng cho nhiệm vụ tổng hợp trừu tượng: chức năng đầu tiên, được gọi là RwB-Hinge, sẽ chọn theo động lực các mẫu cho lần cập nhật dốc. Lần thứ hai, biệt danh RISK, cầm một nhóm nhỏ các ứng viên mạnh để góp phần thưởng. Trong các thí nghiệm, chúng tôi thăm dò phương pháp được đề nghị bằng cách tinh chỉnh một mô hình Trước ICL hơn chín tập hợp dữ liệu với kích thước và tự nhiên khác nhau. Xét nghiệm kết quả cho thấy tỉ lệ tốt hơn so với các căn cứ về duyên khẩu âm.</abstract_vi>
      <abstract_uz>Bu yerda ko'pchilik muvaffaqiyatsiz modellari ularning taʼminlovchi maqsadiga (NLL) oʻzgarishga ishlatadi. Ba'zi holatda, o'rganishni qoʻshish qoʻshildi, modellarni qiymatning qiymatlariga quyidagi maqsad bilan o'rganish mumkin. Lekin, o'rganish usulida foydalanish muvaffaqiyatli o'rganish qoidasi bajarish uchun muhim roli o'ynashi mumkin va ammo qismi unutilmaydi. For this reason, in this paper, we propose two reward functions for the task of abstractive summarisation: the first function, referred to as RwB-Hinge, dynamically selects the samples for the gradient update.  Ikkinchi funksiyat RISK nomli, muammolar haqida o'zgartirish uchun kichkina qismi qo'shiladi. Tajribalar davomida, biz birinchi taʼminlovchi NLL modelini yaxshi ko'ra ko'proq taʼminlovchi 9 ta'minlovchi taʼminlovchi maʼlumotlar tarkibini o'rganish mumkin. Sertifikatlar natijalari negativ logning asosiy asosiy sonlarida davom etishni ko'rsatadi.</abstract_uz>
      <abstract_hr>Do sada, većina apstraktivnih modela za sažetak oslanjala se na variante negativne mogućnosti dnevnika (NLL) kao njihov cilj obuke. U nekim slučajevima dodana je učenje pojačanja za obuku modela s ciljem koji je bliži njihovim mjerama procjene (npr. ROUGE). Međutim, funkcija nagrade koja se koristi unutar pristupa pojačanja učenja može imati ključnu ulogu za učenje i još je djelomično neočekivana. Zbog ovog razloga, u ovom papiru predlažemo dvije nagrade za zadatak abstraktivne sažetke: prva funkcija, naziva se RwB-Hinge, dinamički odabere uzorke za aktualizaciju gradienta. Druga funkcija, nadimak RISK, utiče na mali bazen jakih kandidata da obavijesti nagradu. U eksperimentima, istražujemo predloženi pristup finaliziranjem predobučenog model NLL-a preko devet rezimetriranih podataka različitih veličina i prirode. Eksperimentalni rezultati pokazuju konsekventno poboljšanje na osnovnim linijama negativne mogućnosti dnevnika.</abstract_hr>
      <abstract_nl>Tot op heden hebben de meeste abstracte samenvattingsmodellen als trainingsdoelstelling gebruik gemaakt van varianten van de negatieve log-waarschijnlijkheid (NLL). In sommige gevallen is versterking learning toegevoegd om de modellen te trainen met een doel dat dichter bij hun evaluatiemaatregelen ligt (bijv. ROUGE). De beloningsfunctie die moet worden gebruikt binnen de versterkende leeraanpak kan echter een belangrijke rol spelen voor prestaties en is gedeeltelijk nog onontdekt. Daarom stellen we in dit artikel twee beloningsfuncties voor de taak van abstractieve samenvatting voor: de eerste functie, aangeduid als RwB-scharnier, selecteert dynamisch de samples voor de gradiënt update. De tweede functie, genaamd RISK, maakt gebruik van een kleine pool van sterke kandidaten om de beloning te informeren. In de experimenten onderzoeken we de voorgestelde aanpak door een NLL voorgetraind model af te stemmen over negen samenvattingsdatasets van verschillende grootte en aard. De experimentele resultaten tonen een consistente verbetering ten opzichte van de negatieve log-waarschijnlijkheid baselines.</abstract_nl>
      <abstract_id>Sehingga saat ini, kebanyakan model resumsi abstraktif telah bergantung pada varian dari kemungkinan log negatif (NLL) sebagai tujuan latihan mereka. Dalam beberapa kasus, pemerintahan belajar telah ditambah untuk melatih model dengan tujuan yang lebih dekat dengan tindakan evaluasi mereka (misalnya ROUGE). However, the reward function to be used within the reinforcement learning approach can play a key role for performance and is still partially unexplored.  Untuk alasan ini, di kertas ini, kami mengusulkan dua fungsi hadiah untuk tugas resumen abstraktif: fungsi pertama, disebut sebagai RwB-Hinge, dinamik memilih sampel untuk pemutakhiran gradien. The second function, nicknamed RISK, leverages a small pool of strong candidates to inform the reward.  Dalam eksperimen ini, kami menyelidiki pendekatan yang direncanakan dengan memperbaiki model NLL yang terlatih lebih dari sembilan dataset ringkasan ukuran dan alam yang berbeda. Hasil percobaan menunjukkan peningkatan konsisten atas garis dasar log-kemungkinan negatif.</abstract_id>
      <abstract_da>Hidtil har de fleste abstrakte opsummeringsmodeller baseret sig på varianter af den negative log-sandsynlighed (NLL) som deres træningsmål. I nogle tilfælde er der tilføjet forstærket læring for at uddanne modellerne med et mål, der ligger tættere på deres evalueringsforanstaltninger (f.eks. ROUGE). Belønningsfunktionen, der skal bruges inden for forstærkningslæringsmetoden, kan dog spille en nøglerolle for ydeevnen og er stadig delvist uudforsket. Derfor foreslår vi i denne artikel to belønningsfunktioner til opgaven med abstraktiv opsummering: Den første funktion, kaldet RwB-Hinge, udvælger dynamisk prøverne til gradient opdateringen. Den anden funktion, kaldet RISK, udnytter en lille pulje af stærke kandidater til at informere belønningen. I eksperimenterne undersøger vi den foreslåede tilgang ved at finjustere en NLL præ-trænet model over ni sammenfattende datasæt af forskellig størrelse og natur. De eksperimentelle resultater viser en konsekvent forbedring i forhold til de negative log-sandsynlighed basislinjer.</abstract_da>
      <abstract_ko>지금까지 대부분의 추상적인 총결산 모델은 마이너스 대수 유사(NLL) 변수에 의존하여 훈련 목표로 삼았다.어떤 경우 훈련 목표가 평가 지표에 더욱 가까워지도록 학습을 강화하는 모델(예를 들어 연지)을 추가했다.그러나 학습 방법을 강화하는 데 사용되는 장려 함수는 실적에 관건적인 역할을 할 수 있기 때문에 아직 일부는 탐색하지 못했다.따라서 본고에서 우리는 추상적으로 임무를 정리하는 데 사용되는 두 가지 보상 함수를 제시했다. 첫 번째 함수는 RwB 경첩이라고 하고 동적 선택 사다리 업데이트에 사용되는 견본이다.두 번째 기능은 리스크라는 별명으로 강력한 후보들을 이용해 보상을 알리는 것이다.실험에서 우리는 9개의 서로 다른 크기와 성질의 집합 데이터 집합의 NLL 예비 훈련 모델을 미세하게 조정함으로써 제시한 방법을 탐색했다.실험 결과 마이너스 대수의 유사 기선에 비해 이 방법은 일치된 개선을 보였다.</abstract_ko>
      <abstract_de>Bisher haben sich die meisten abstraktiven Zusammenfassungsmodelle auf Varianten der negativen Log-Likelihood (NLL) als Trainingsziel verlassen. In einigen Fällen wurde Verstärkungslernen hinzugefügt, um die Modelle mit einem Ziel zu trainieren, das näher an ihren Evaluationsmaßnahmen liegt (z.B. ROUGE). Die Belohnungsfunktion, die im Rahmen des Verstärkungslernens eingesetzt werden soll, kann jedoch eine Schlüsselrolle für die Leistung spielen und ist teilweise noch unerforscht. Aus diesem Grund schlagen wir in diesem Beitrag zwei Belohnungsfunktionen für die Aufgabe der abstraktiven Zusammenfassung vor: Die erste Funktion, genannt RwB-Scharnier, wählt dynamisch die Samples für die Verlaufsaktualisierung aus. Die zweite Funktion, genannt RISK, nutzt einen kleinen Pool starker Kandidaten, um die Belohnung zu informieren. In den Experimenten untersuchen wir den vorgeschlagenen Ansatz durch Feinabstimmung eines NLL-vortrainierten Modells über neun Zusammenfassungsdatensätze unterschiedlicher Größe und Natur. Die experimentellen Ergebnisse zeigen eine konsistente Verbesserung gegenüber den negativen Log-Likelihood Baselines.</abstract_de>
      <abstract_bg>Към днешна дата повечето абстрактни модели за обобщаване разчитат на варианти на отрицателната логаритметична вероятност (НЛЛ) като цел на обучение. В някои случаи е добавено засилено обучение за обучение на моделите с цел, която е по-близо до мерките за оценка (напр. Въпреки това, функцията за възнаграждение, която трябва да се използва в рамките на подхода за укрепване на обучението, може да играе ключова роля за изпълнението и все още е частично неизследвана. Поради тази причина в настоящата статия предлагаме две възнаграждаващи функции за задачата на абстрактно обобщаване: първата функция, наречена Панта, динамично избира образците за обновяване на градиента. Втората функция, наречена РИСК, използва малък набор от силни кандидати, за да информира наградата. В експериментите изследваме предложения подход чрез фина настройка на предварително обучен модел на НЛЛ върху девет обобщени набора от данни с различен размер и природа. Експерименталните резултати показват последователно подобрение спрямо базовите линии на отрицателната log-вероятност.</abstract_bg>
      <abstract_tr>Şu wagt, iň köp abstraktiw jemgyýet nusgalary negatif log-sanlykynyň (NLL) üýtgetmek maksady bolup geçirdi. Käbir ýagdaýda, nusgalary düzenlemek üçin köpräk öwrenmeler (meseläm ROUGE) düzenlemek golaýynda golaýlaşdyryldy. Ýöne, taýýarlanmak öwrenmek approwasynda ulanylýan täsirli işleýän fonksiýa performans üçin a çyk roli oýnap biler we heniz hem be ýleki şekilde tanamaýar. Bu sebäpden, bu kagyzda abstraktiw toplantyň zadynyň üçin iki täsirli fonksiyony teklip edip görýäris: ilkinji funksiýa, RwB-Hinge diýilip atlanýar, dinamik görnöşi üçin örnekleri saýlaýar. Ikinji faýly, RISK atlandyrylýan, tämiýeti biljek üçin güýçli kandidýalaryň kiçi howluny süýtgedýär. Deneylerde, NLL öňünden eğlenen nusga 9 topar ululyk we tebigatyň üstünde teklip eden nusgasyny çykaryp barýarys. Deneymeli netijeler negatif günlük mümkinçiliki üssüňlerde sürekli gelişmeleri görkezýär.</abstract_tr>
      <abstract_sw>Mpaka sasa, mifano mingi ya muhtasari usio na maana yametegemea tofauti ya uwezekano wa log hasi (NLL) kama lengo la mafunzo yao. Katika baadhi ya matukio mengine, mafunzo ya kuuza umeongezeka kufundisha mifano kwa lengo ambalo ni karibu zaidi na hatua za uchunguzi (kwa mfano, ROUGE). Hata hivyo, jukumu la malipo linalotumiwa ndani ya mbinu za kujifunza linaweza kucheza jukumu la msingi kwa ajili ya utendaji na bado haujajua. Kwa sababu hii, katika gazeti hili, tunapendekeza kazi mbili za malipo kwa ajili ya jukumu la muhtasari wa kidini: kazi ya kwanza, inayoitwa RwB-Hinge, kwa nguvu tunachagua mifano kwa ajili ya upya wa kisasa. Kifungu cha pili, kinachoitwa jina la RISK, kina kichwa kidogo cha wagombea wenye nguvu kutoa taarifa ya malipo. Katika majaribio hayo, tunajaribu mbinu zilizopendekezwa kwa kuunganisha muundo wa zamani wa NLL kwa zaidi ya seti za taarifa za muhtasari tisa za ukubwa na asili tofauti. Matokeo ya majaribio yanaonyesha maendeleo yanayoendelea zaidi ya uwezekano wa kuandika hasi.</abstract_sw>
      <abstract_af>Op dag het die meeste abstraktiewe opsomming modele op veranderinge van die negatiewe log-waarskynlik (NLL) as hul opsomming-objek opgelê. In sommige gevalle is die versterking leer bygevoeg om die modele te oefen met 'n doel wat naby is aan hulle evalueringsmaat (bv. ROUGE). Maar, die loon funksie wat binne die versterking leer toegang gebruik moet word, kan speel 'n sleutel rol vir prestasie en is nog gedeeltelik onverkondig. Vir hierdie rede, in hierdie papier, voorstel ons twee vergelde funksies vir die taak van abstraktiewe opsomming: die eerste funksie, verwys na as RwB- Hinge, dinamies kies die voorbeelde vir die gradient opdateer. Die tweede funksie, bynaam RISK, verwyder 'n klein pool van sterke kandidate na informasie die loon. In die eksperimente probeer ons die voorgestelde toegang deur 'n NLL voor-opgelei model te fin-tuning oor nege opsomming datastelle van verskeie grootte en natuur. Die eksperimentale resultate vertoon 'n konsistente verbetering oor die negatiewe log-waarskynlike basisline.</abstract_af>
      <abstract_hy>Այսօր վերացրական համառոտագրման մոդելների մեծ մասը հիմնված է բացասական լոգ-հավանականության (ՆԼԼ) տարբերակների վրա որպես իրենց ուսուցման նպատակ: Որոշ դեպքերում ուժեղացված ուսումնասիրությունը ավելացվել է, որպեսզի մոդելները վարժեցնեն նպատակով, որը ավելի մոտ է նրանց գնահատման չափումներին (օրինակ ROUge). Այնուամենայնիվ, վարձի գործառույթը, որը պետք է օգտագործվի ուսուցման ուժեղացման մոտեցում, կարող է ունենալ արտադրողության կարևոր դեր և դեռ մասամբ չի ուսումնասիրել: Այս պատճառով, այս թղթի մեջ մենք առաջարկում ենք երկու հատուկ ֆունկցիաներ վերացական համառոտագրման խնդրի համար. առաջին ֆունկցիան, որը կոչվում է RwB-Հինգ, դինամիկ կերպով ընտրում է նմուշները դասավորման վերականգնման համար: Երկրորդ ֆունկցիան, որը կոչվում է RIsk, օգտագործում է մի փոքրիկ հավաքածու ուժեղ թեկնածուներ, որպեսզի տեղեկացնեն պարգևը: Փորձարկումների ընթացքում մենք ուսումնասիրում ենք առաջարկած մոտեցումը, բարելավելով ՆԼԼ-ի նախապատրաստված մոդելը տարբեր չափերի և բնության ինն համառոտագրման տվյալների համակարգերի միջոցով: Փորձարկվող արդյունքները ցույց են տալիս, որ բացասական լոգ-հավանականության հիմնական գծերի հետ կապված բարելավում է:</abstract_hy>
      <abstract_am>To date, most abstractive summarisation models have relied on variants of the negative log-likelihood (NLL) as their training objective.  አንዳንድ ጉዳይ፣ ማስተማር ተጨማሪው ተጨማሪው በሞዴላዎችን ማስተምር የተጨመረ ነው፡፡ ነገር ግን የደመወዝ ሥራ በማድረግ ውስጥ የሚጠቀሙት የድጋፍ ማድረግ ማድረግ ማድረግ የሚችል የቁልፍ ሚዛን ይጫወታል፡፡ ስለዚህ ምክንያት፣ በዚህ ገጽ፣ ለጥያቄ አካባቢ ስርዓት ሁለት የዋጋ ፍትወቶችን እናሳውቃለን፤ የመጀመሪያው ክፍል RwB-Hinge የተባለውን ምሳሌዎችን ለቀዳሚ አዲስ ማሻሻሻል ይምረጣል፡፡ ሁለተኛይቱ ስርዓት RISK የተባለው የደመወዙን ለማሳወቅ የብርቱ አማካሪዎችን ታናሽ የውኃ ሙቀት ሰጥቷል፡፡ በተፈተናው ውስጥ የዘጠኝ አነስተኛነት ዳታዎችን በተለየ መጠን እና በሥርዓት ላይ የተለየውን የNLL ሞዴል በመጠቀም በተፈተናው ልማት እና በሥርዓት የተለየ ጥያቄዎችን እናሳውቃለን፡፡ የሞከሩ ውጤቶች የnegative የሎግ-ምናልባት መቀመጫዎች ላይ የሚተካክለውን ትክክል ያሳያል፡፡</abstract_am>
      <abstract_fa>تا حال، بیشترین مدل‌های جمع‌آوری مطلق به عنوان هدف آموزش آنها بر تغییرات احتمال منفی (NLL) اعتماد دارند. در بعضی موارد، یادگیری پشتیبانی برای آموزش مدلها با هدف نزدیکتر به اندازه‌های ارزیابی آنها (مثال ROUGE) اضافه شده است. ولی، عملکرد پاداش که در دستور یادگیری افزایش استفاده می‌شود، می‌تواند نقش کلید برای اجرا را را بازی کند و هنوز قسمتی غیر توضیح داده می‌شود. به خاطر این دلیل، در این کاغذ، دو تابع پاداش برای کار جمع‌آوری abstractive پیشنهاد می‌کنیم: اولین تابع، به عنوان RwB-Hinge، دینامیک نمونه‌ها را برای جدید‌سازی گراده انتخاب می‌کند. عملکرد دوم، اسم RISK، یک استخره کوچک از کاندیدای قوی برای اطلاعات پاداش را ارائه می دهد. در این آزمایشات، ما روش پیشنهاد را با تنظیم یک مدل پیش آموزش NLL بیش از نو مجموعه داده‌های جمع کردن اندازه و طبیعت مختلف تحقیق می‌کنیم. نتیجه آزمایشی بر اساس پایین‌های احتمال منفی بهتر شدن را نشان می‌دهد.</abstract_fa>
      <abstract_az>Şimdiye qədər, çox abstraktiv qurğulama modelleri onların təhsil məqsədili olaraq negatif log-likeliğinin variablarına təvəkkül edir. Bazı vaxtlarda, modelləri təhsil etmək üçün daha yaxın bir məqsəd ilə öyrənmək öyrənmək üçün artırmaq öyrənməsi artırıldı (məsələn, ROUGE). Ancaq mükafat funksiyası artırmaq öyrənmə tərzində istifadə ediləcək tərzdə performans üçün anahtar rolü oynaya bilər və hələ də bir qismi a çıq-aydın deyildir. Bu səbəbdən, bu kağızda abstraktiv təmizləmə görevi üçün iki mükafat funksiyasını təbliğ edirik: ilk funksiyası, RwB-Hinge adlandırılmış, dinamik olaraq gradient güncelləməsi üçün nümunələri seçir. İkinci funksiyası, RISK adlı nickname, mükafatı bildirmək üçün güclü kandidátların küçük bir havuna istifadə edir. İşlemlərdə, NLL əvvəlcə təhsil edilmiş modeli doqquz müxtəlif ölçü və təbiəti ilə təhsil edilmiş verilən verilən qurbanların təhsil edilməsini təsdiq edirik. Müxtəlif sonuçlar negatif log-likeliğinin səhifələrində müəyyən bir improvement göstərir.</abstract_az>
      <abstract_ca>To date, most abstractive summarisation models have relied on variants of the negative log-likelihood (NLL) as their training objective.  En alguns casos, s'ha afegit l'aprenentatge de reforç per formar els models amb un objectiu que és més proper a les seves mesures d'evaluació (per exemple ROUGE). No obstant això, la funció de recompensa que s'ha d'utilitzar en l'enfocament d'aprenentatge de reforç pot jugar un paper clau per al rendiment i encara no ha estat explorada en part. Per això, en aquest paper proposem dues funcions de recompensa per la tasca de resum abstractiu: la primera funció, anomenada RwB-Hinge, selecciona dinàmicament les mostres per a l'actualització del gradient. La segona funció, anomenada RISK, utilitza un petit grup de candidats forts per informar la recompensa. En els experiments, investigam l'enfocament proposat ajustando un model pré-entrenat de NLL sobre nou conjunts de dades de resum de mida i naturalesa diverses. Els resultats experimentals mostren una millora constantsobre les línies de base negatives de probabilitat de registre.</abstract_ca>
      <abstract_sq>Deri tani, shumica e modeleve abstraktive të përmbledhjes janë mbështetur në variantet e probabilitetit negativ të regjistrimit (NLL) si objektivin e tyre të trajnimit. Në disa raste, mësimi i forcimit është shtuar për të trajnuar modelet me një objektiv që është më pranë masave të tyre të vlerësimit (për shembull ROUGE). Megjithatë, funksioni i shpërblimit që do të përdoret brenda qasjes së forcimit të mësimit mund të luajë një rol kyç për performancën dhe është ende pjesërisht i papërshqyrtuar. For this reason, in this paper, we propose two reward functions for the task of abstractive summarisation: the first function, referred to as RwB-Hinge, dynamically selects the samples for the gradient update.  Funksioni i dytë, i quajtur RISK, nxjerr një grup të vogël kandidatësh të fortë për të informuar shpërblimin. Në eksperimente, ne vëzhgojmë qasjen e propozuar duke përshtatur një model të paratrajnuar NLL mbi nëntë grupe të dhënash të përmbledhur të madhësisë dhe natyrës së ndryshme. Rezultatet eksperimentale tregojnë një përmirësim konsistent lidhur me linjat bazë negative të probabilitetit të log.</abstract_sq>
      <abstract_cs>Dosud se většina abstraktivních souhrnných modelů spoléhala na varianty negativní log-pravděpodobnosti (NLL) jako cíl výcviku. V některých případech bylo přidáno posílení učení pro trénink modelů s cílem, který je blíže jejich hodnotícím opatřením (např. ROUGE). Funkce odměny, která má být použita v rámci přístupu k posilování učení, však může hrát klíčovou roli pro výkon a je stále částečně neprozkoumaná. Z tohoto důvodu v tomto článku navrhujeme dvě funkce odměny pro úkol abstraktivního shrnutí: první funkce, označovaná jako RwB-Hinge, dynamicky vybírá vzorky pro aktualizaci gradientu. Druhá funkce, přezdívaná RISK, využívá malou skupinu silných kandidátů, aby informovala odměnu. V experimentech zkoumáme navržený přístup jemným laděním NLL předtrénovaného modelu na devíti souhrnných datových sadách různé velikosti a povahy. Experimentální výsledky ukazují konzistentní zlepšení oproti negativním log-pravděpodobnosti základních linií.</abstract_cs>
      <abstract_fi>Tähän mennessä useimmat abstraktit yhteenvetomallit ovat käyttäneet koulutustavoitteena negatiivisen log-todennäköisyyden variantteja. Joissakin tapauksissa mallien kouluttamiseen on lisätty tehostettua oppimista tavoitteena, joka on lähempänä niiden arviointitoimenpiteitä (esim. ROUGE). Vahvistusoppimisessa käytettävällä palkitsemistoiminnolla voi kuitenkin olla keskeinen rooli suorituskyvyn kannalta, ja sitä ei ole vielä osittain tutkittu. Tästä syystä tässä työssä ehdotamme kahta palkitsemisfunktiota abstraktiivisen yhteenvedon tehtävään: ensimmäinen funktio, jota kutsutaan nimellä RwB-Hinge, valitsee dynaamisesti näytteet gradientin päivitystä varten. Toinen toiminto, lempinimeltään RISK, hyödyntää pientä vahvojen ehdokkaiden joukkoa ilmoittamaan palkkiosta. Kokeiluissa tutkitaan ehdotettua lähestymistapaa hienosäätämällä esikoulutettua NLL-mallia yhdeksään erikokoiseen ja luonteeltaan erikokoiseen yhteenvetoaineistoon. Kokeelliset tulokset osoittavat jatkuvan parannuksen negatiivisen log-todennäköisyyden perusviivoista.</abstract_fi>
      <abstract_bn>এখন পর্যন্ত নেতিবাচক লগ-সম্ভাবনা নির্ভর করে বেশীরভাগ সংক্রান্ত মডেল তাদের প্রশিক্ষণের লক্ষ্য হিসেবে নির্ভর করেছে। কিছু ক্ষেত্রে শিক্ষা বাড়িয়ে দিয়েছে মডেলের শিক্ষা প্রশিক্ষণের জন্য যোগ করা হয়েছে যা তাদের মূল্যায়নের কাছাকাছি (যেমন রোজের ক কিন্তু ক্ষেত্রে ব্যবহার করা পুরস্কারের কাজের ক্ষেত্রে ব্যবহার করা যায় তা প্রদর্শনের জন্য একটি গুরুত্বপূর্ণ ভূমিকা খেলতে প এই কারণের জন্য, এই কাগজটিতে আমরা অস্বীকৃতিক সংক্রান্ত কাজের জন্য দুটি পুরস্কার প্রস্তাব করছি: প্রথম ফাংশন, যার নাম রিউবি-হিঙ্গ বলা হচ্ছে, গ্রেডিয়েন্ড আপড দ্বিতীয় ফাংশন, রিএসকে নাম দেয়া হয়েছে, পুরস্কার জানানোর জন্য একটি ছোট পুলের প্রার্থীদের একটি পুল প্রদান করেছে। পরীক্ষার মধ্যে আমরা প্রস্তাবিত পদ্ধতি প্রমাণ করি একটি এনএল পূর্ব প্রশিক্ষিত মডেলের মাধ্যমে নয়টি সংক্ষিপ্ত তথ্যের বিভিন্ন আকার ও প পরীক্ষার ফলাফল নেতিবাচক লগ-সম্ভাবনার বেসাইনের ব্যাপারে একেবারে উন্নতি প্রদর্শন করে।</abstract_bn>
      <abstract_bs>Do sada, većina apstraktivnih modela za sažetak oslanjala se na variante negativne mogućnosti dnevnika (NLL) kao njihov cilj obuke. U nekim slučajevima, dodana je učenje pojačanja kako bi obučila modele objektivnim ciljem koji je bliži njihovim mjerama procjene (npr. ROUGE). Međutim, funkcija nagrade koja se koristi unutar pristupa pojačanja učenja može imati ključnu ulogu za izvođenje i još je djelomično neočekivana. Zbog ovog razloga, u ovom papiru predlažemo dve nagrade funkcije za zadatak abstraktivne sažetke: prva funkcija, pod nazivom RwB-Hinge, dinamički odabere uzorke za aktualizaciju gradienta. Druga funkcija, nadimak RISK, utiče na mali bazen jakih kandidata da obavijesti nagradu. U eksperimentima, istražujemo predloženi pristup tako što ćemo ispraviti predobučeni model NLL preko devet rezimetriranih podataka različitih veličina i prirode. Eksperimentalni rezultati pokazuju konsekventno poboljšanje na osnovnim linijama negativne mogućnosti dnevnika.</abstract_bs>
      <abstract_et>Praeguseks on enamik abstraktseid kokkuvõtlusmudeleid oma koolituse eesmärgina tuginenud negatiivse logitõenäosuse variantidele. Mõnel juhul on lisatud tugevdatud õppimine, et koolitada mudeleid eesmärgiga, mis on lähemal nende hindamismeetmetele (nt ROUGE). Siiski võib tugevdamisõppe meetodi raames kasutatav tasuvusfunktsioon mängida tulemuslikkuse seisukohalt võtmerolli ja seda on veel osaliselt uurimata. Sel põhjusel pakume selles töös välja kaks preemiafunktsiooni ülesandeks abstraktne kokkuvõte: esimene funktsioon, mida nimetatakse RwB-Hinge, valib dünaamiliselt näidised gradienti uuendamiseks. Teine funktsioon, hüüdnimega RISK, võimendab väikest tugevate kandidaatide kogumit, et teavitada tasu. Katsetes analüüsime kavandatud lähenemisviisi, täpsustades NLL eelõpetatud mudelit üheksa erineva suuruse ja iseloomuga kokkuvõtliku andmekogumi kaudu. Katsetulemused näitavad pidevat paranemist negatiivse logaritmilise tõenäosuse baasjoonega võrreldes.</abstract_et>
      <abstract_jv>Daftar, akeh di sistem absolute dadi resumen model sing wis dianggawe karo variant dadi log-like (NLL) nganggo nggawe aksil ngucap Slacky Nanging, bongkin nggo dianggap sistem kang nggawe Jejaring Awak dhéwé, ning basa iki, kita mulungi iki bakal nggawe bakal dhéwé basa sing perusahaan karo hal-perusahaan: nambah tanggal tuatah, nambah RwB-hint deep Nang ujaran ning rabi, kita ngubah akses nggawe layakno ning kebutuhan NLL kuwi model sing wis antara awak dhéwé dadi nggawe barang sampek kanggo kalagayut karo bumi. Rejalaké sing diperaksi wong kaé mpungasane kanggo ngilangno sistem sing gawe kaya nguasakno</abstract_jv>
      <abstract_sk>Do danes se je večina abstraktivnih modelov povzetka zanašala na različice negativne log verjetnosti (NLL) kot cilj usposabljanja. V nekaterih primerih je bilo dodano okrepitveno učenje za usposabljanje modelov s ciljem, ki je bližji njihovim ukrepom ocenjevanja (npr. ROUGE). Vendar pa lahko funkcija nagrajevanja, ki se uporablja v okviru pristopa krepitvenega učenja, igra ključno vlogo za uspešnost in je še vedno delno neraziskana. Zato v tem prispevku predlagamo dve nagradni funkciji za nalogo abstraktivnega povzetka: prva funkcija, imenovana RwB-Hinge, dinamično izbere vzorce za posodobitev gradienta. Druga funkcija, imenovana RISK, vzvodi majhen nabor močnih kandidatov za obveščanje o nagradi. V poskusih smo preučili predlagani pristop z natančnim nastavitvijo vnaprej usposobljenega modela NLL na devetih naborih povzetkov podatkov različnih velikosti in narave. Poskusni rezultati kažejo konsistentno izboljšanje glede na izhodiščne vrstice negativne log verjetnosti.</abstract_sk>
      <abstract_ha>Ga yanzu, mafi yawan motsi na ƙidãya masu kanana sun dõgara a kan variant na masu yiwur-rubutun-rubutun (NLL) kamar abun na tsari. In some cases, reinforcement learning has been added to train the models with an objective that is closer to their evaluation measures (e.g. ROUGE).  A lokacin da ake amfani da aikin ijãrar da aka ƙara cikin hanyarwa da za'a yi amfani da shi, ko kuma yana da rabon aiki wanda ba'a buƙata ba. Saboda haka, a cikin wannan takarda, Munã buɗar da misalin biyu na musamman wa aikin da aka tsare masu kanana: aikin na farko, wanda aka kallo na RwB-Hige, yana zãɓi misãlai na don a sake tsari na-sãɓa. Fara na biyu, wanda aka sunan RiSK, yana samar da wani abu mai ƙaranci daga kandida mai ƙarfi dõmin ya sanar da ijãra. Daga jarrabai, za'a jarraba kafin da za'a sami wata misalin na NLL a gaba-wa'anar da taki tara shekarar data masu turu'in girma da halin. Mataimakin jarrabai na nuna wata gyãra mai daidai a kan salummai mai yiwuwa-log-negative.</abstract_ha>
      <abstract_he>עד היום, רוב הדוגמנים המתוקפים המתוקפים הסמכו על שונות של סבירות השלילית של לוג-סבירות (NLL) כמטרה האימונים שלהם. במקרים מסוימים, הוספה למידת גיבוי כדי לאמן את הדוגמנים עם מטרה שהיא קרובה יותר לאמצעי הערכה שלהם (למשל ROUGE). בכל אופן, התפקיד הפרס שימש בתוך גישת הלימוד התגבורה יכול לשחק תפקיד מפתח לביצוע ועדיין לא חוקר חלקית. מסיבה זו, בעיתון הזה, אנו מציעים שתי תפקידים פרס למשימה של סכם אוסטרקטיבי: הפונקציה הראשונה, שנקראת RwB-Hinge, בוחרת דינמית את הדגימות למעדכון המדרגות. הפונקציה השנייה, בשם RISK, משתמשת בבריכה קטנה של מועמדים חזקים כדי להודיע על הפרס. בניסויים, אנו חוקרים את הגישה המוצעת על ידי שיפור מודל מאומן מראש של NLL מעל תשע קבוצות מידע של גודל ומגוון טבע. תוצאות הניסויים מראות שיפור קבוע מעל קווי הבסיס של סבירות לוג שליליים.</abstract_he>
      <abstract_bo>ད་ལྟ་མི་མང་ཆེ་བ་དག་གི་བསྡུས་བརྗོད་སྟངས་ཀྱི་ནང་དུ་ཉུང་འབྲེལ་བ་དང་མཉམ་དུ་འགྱུར་བ་ཡིན། གལ་སྲིད་ལ་ཤས་ཀྱི་ནང་དུ། མིག་རྩལ་བསྐྱེད་ཚད་སྒྲིག་གི་མཐུན་རྐྱེན་ཐབས་ལམ་ལ་ཁྱད་ནས་མཐུན་རྐྱེན་བཟོ་བྱས་ཡོད། ཡིན་ནའང་། རྒྱབ་སྐྱོར་གྱི་གཟུགས་རིས་སྟོན་པར་ལག་ལེན་འཐབ་དགོས་པ་དེ་ལས་ཀྱང་སྐྱོང་ཆེན་པོ་ཞིག་རྩེ་བ་ལས། འོན་ཀྱང་། རྒྱུ་མཚན་འདིའི་ནང་གི་ཤོག་བུ་འདིའི་ནང་དུ་ང་ཚོའི་རྗེས་སུ་འབྲངས་ཀ་གཉིས་འཆར་འདོད་པ་ཡིན། the first function, referred to as RwB-Hinge, dynamically selects the samples for the gradient update. དངོས་པོ་གཉིས་པ་་ཡིན་པའི་མིང་མིང RISK་་་་ཆ་རྐྱེན་སྐོར་ཆེན་པོ་ཆུང་ཀུ་ཉེ་ཆར་སྤྲོད་ཡོད། འུ་ཅག་གིས་བརྟག་ཞིབ་བྱས་པའི་སྔོན་གྲངས་སྒྲིག་འགོད་བྱས་པའི་གཟུགས་རིས་ལྟར་ཞིབ་བཤེར་བྱེད་ཀྱི་ཡོད། གྲུབ་ཕྱོགས་གྱི་གནད་སྡུད་དུ་ཚད་རྐྱེན་ཐོག་ནས་གནས་སྟངས་གང་ཟག་ཞིག་མཐོང་ནུས་</abstract_bo>
      </paper>
    <paper id="2">
      <title>SmBoP : Semi-autoregressive Bottom-up Semantic Parsing<fixed-case>S</fixed-case>m<fixed-case>B</fixed-case>o<fixed-case>P</fixed-case>: Semi-autoregressive Bottom-up Semantic Parsing</title>
      <author><first>Ohad</first><last>Rubin</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>12–21</pages>
      <abstract>The de-facto standard decoding method for <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a> in recent years has been to autoregressively decode the <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">abstract syntax tree</a> of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach : a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step t the top-K sub-trees of height   t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, <a href="https://en.wikipedia.org/wiki/Bottom-up_parsing">bottom-up parsing</a> allows to decode all <a href="https://en.wikipedia.org/wiki/Tree_(data_structure)">sub-trees</a> of a certain height in parallel, leading to <a href="https://en.wikipedia.org/wiki/Time_complexity">logarithmic runtime complexity</a> rather than <a href="https://en.wikipedia.org/wiki/Time_complexity">linear</a>. From a modeling perspective, a <a href="https://en.wikipedia.org/wiki/Bottom-up_parsing">bottom-up parser</a> learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP leads to a 2.2x speed-up in decoding time and a ~5x speed-up in training time, compared to a <a href="https://en.wikipedia.org/wiki/Semantic_parser">semantic parser</a> that uses autoregressive decoding. SmBoP obtains 71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+Grappa.</abstract>
      <url hash="a07509ac">2021.spnlp-1.2</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e4e5214e">2021.spnlp-1.2.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2021.spnlp-1.2</doi>
      <bibkey>rubin-berant-2021-smbop-semi</bibkey>
    </paper>
    <paper id="5">
      <title>Mode recovery in neural autoregressive sequence modeling</title>
      <author><first>Ilia</first><last>Kulikov</last></author>
      <author><first>Sean</first><last>Welleck</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <pages>44–52</pages>
      <abstract>Despite its wide use, recent studies have revealed unexpected and undesirable properties of neural autoregressive sequence models trained with <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood</a>, such as an unreasonably high affinity to short sequences after training and to infinitely long sequences at decoding time. We propose to study these phenomena by investigating how the modes, or local maxima, of a distribution are maintained throughout the full learning chain of the ground-truth, empirical, learned and decoding-induced distributions, via the newly proposed mode recovery cost. We design a tractable testbed where we build three types of ground-truth distributions : (1) an LSTM based structured distribution, (2) an unstructured distribution where probability of a sequence does not depend on its content, and (3) a product of these two which we call a semi-structured distribution. Our study reveals both expected and unexpected findings. First, starting with <a href="https://en.wikipedia.org/wiki/Data_collection">data collection</a>, mode recovery cost strongly relies on the ground-truth distribution and is most costly with the semi-structured distribution. Second, after learning, mode recovery cost from the ground-truth distribution may increase or decrease compared to <a href="https://en.wikipedia.org/wiki/Data_collection">data collection</a>, with the largest cost degradation occurring with the semi-structured ground-truth distribution. Finally, the ability of the decoding-induced distribution to recover modes from the learned distribution is highly impacted by the choices made earlier in the learning chain. We conclude that future research must consider the entire learning chain in order to fully understand the potentials and perils and to further improve neural autoregressive sequence models.<i>learning chain</i> of the ground-truth, empirical, learned and decoding-induced distributions, via the newly proposed <i>mode recovery cost</i>. We design a tractable testbed where we build three types of ground-truth distributions: (1) an LSTM based structured distribution, (2) an unstructured distribution where probability of a sequence does not depend on its content, and (3) a product of these two which we call a semi-structured distribution. Our study reveals both expected and unexpected findings. First, starting with data collection, mode recovery cost strongly relies on the ground-truth distribution and is most costly with the semi-structured distribution. Second, after learning, mode recovery cost from the ground-truth distribution may increase or decrease compared to data collection, with the largest cost degradation occurring with the semi-structured ground-truth distribution. Finally, the ability of the decoding-induced distribution to recover modes from the learned distribution is highly impacted by the choices made earlier in the learning chain. We conclude that future research must consider the entire learning chain in order to fully understand the potentials and perils and to further improve neural autoregressive sequence models.</abstract>
      <url hash="4141b8ad">2021.spnlp-1.5</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5cc13228">2021.spnlp-1.5.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.spnlp-1.5</doi>
      <bibkey>kulikov-etal-2021-mode</bibkey>
      <pwccode url="https://github.com/uralik/mode_recovery" additional="false">uralik/mode_recovery</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="6">
      <title>Using Hierarchical Class Structure to Improve Fine-Grained Claim Classification</title>
      <author><first>Erenay</first><last>Dayanik</last></author>
      <author><first>Andre</first><last>Blessing</last></author>
      <author><first>Nico</first><last>Blokker</last></author>
      <author><first>Sebastian</first><last>Haunss</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>53–60</pages>
      <abstract>The analysis of public debates crucially requires the classification of political demands according to hierarchical claim ontologies (e.g. for immigration, a supercategory Controlling Migration might have subcategories Asylum limit or Border installations). A major challenge for automatic claim classification is the large number and low frequency of such <a href="https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)">subclasses</a>. We address it by jointly predicting pairs of matching super- and subcategories. We operationalize this idea by (a) encoding <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">soft constraints</a> in the claim classifier and (b) imposing <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">hard constraints</a> via <a href="https://en.wikipedia.org/wiki/Integer_linear_programming">Integer Linear Programming</a>. Our experiments with different claim classifiers on a German immigration newspaper corpus show consistent performance increases for joint prediction, in particular for infrequent categories and discuss the complementarity of the two approaches.<i>claim ontologies</i> (e.g. for immigration, a supercategory “Controlling Migration” might have subcategories “Asylum limit” or “Border installations”). A major challenge for automatic claim classification is the large number and low frequency of such subclasses. We address it by jointly predicting pairs of matching super- and subcategories. We operationalize this idea by (a) encoding soft constraints in the claim classifier and (b) imposing hard constraints via Integer Linear Programming. Our experiments with different claim classifiers on a German immigration newspaper corpus show consistent performance increases for joint prediction, in particular for infrequent categories and discuss the complementarity of the two approaches.</abstract>
      <url hash="2c230191">2021.spnlp-1.6</url>
      <doi>10.18653/v1/2021.spnlp-1.6</doi>
      <bibkey>dayanik-etal-2021-using</bibkey>
    </paper>
    </volume>
</collection>