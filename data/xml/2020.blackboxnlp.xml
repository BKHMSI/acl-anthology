<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.blackboxnlp">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</booktitle>
      <editor><first>Afra</first><last>Alishahi</last></editor>
      <editor><first>Yonatan</first><last>Belinkov</last></editor>
      <editor><first>Grzegorz</first><last>Chrupała</last></editor>
      <editor><first>Dieuwke</first><last>Hupkes</last></editor>
      <editor><first>Yuval</first><last>Pinter</last></editor>
      <editor><first>Hassan</first><last>Sajjad</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="de8af8bb">2020.blackboxnlp-1.0</url>
      <bibkey>blackboxnlp-2020-blackboxnlp</bibkey>
    </frontmatter>
    <paper id="6">
      <title>Leveraging Extracted Model Adversaries for Improved Black Box Attacks</title>
      <author><first>Naveen Jafer</first><last>Nizar</last></author>
      <author><first>Ari</first><last>Kobren</last></author>
      <pages>57–67</pages>
      <abstract>We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our <a href="https://en.wikipedia.org/wiki/Stake_(Latter_Day_Saints)">approach</a> is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail. These perturbed inputs are used against the victim. In experiments we find that our method improves on the efficacy of the ADDANYa white box attackperformed on the approximate model by 25 % <a href="https://en.wikipedia.org/wiki/F-number">F1</a>, and the ADDSENT attacka black box attackby 11 % <a href="https://en.wikipedia.org/wiki/F-number">F1</a>.</abstract>
      <url hash="ac888724">2020.blackboxnlp-1.6</url>
      <doi>10.18653/v1/2020.blackboxnlp-1.6</doi>
      <bibkey>nizar-kobren-2020-leveraging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="14">
      <title>The elephant in the interpretability room : Why use attention as explanation when we have <a href="https://en.wikipedia.org/wiki/Salience_(neuroscience)">saliency methods</a>?</title>
      <author><first>Jasmijn</first><last>Bastings</last></author>
      <author><first>Katja</first><last>Filippova</last></author>
      <pages>149–155</pages>
      <abstract>There is a recent surge of interest in using <a href="https://en.wikipedia.org/wiki/Attention">attention</a> as explanation of model predictions, with mixed evidence on whether <a href="https://en.wikipedia.org/wiki/Attention">attention</a> can be used as such. While <a href="https://en.wikipedia.org/wiki/Attention">attention</a> conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use <a href="https://en.wikipedia.org/wiki/Attention">attention</a>, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.</abstract>
      <url hash="852122f3">2020.blackboxnlp-1.14</url>
      <doi>10.18653/v1/2020.blackboxnlp-1.14</doi>
      <video href="https://slideslive.com/38939764" />
      <bibkey>bastings-filippova-2020-elephant</bibkey>
    </paper>
    <paper id="16">
      <title>Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation</title>
      <author><first>Atticus</first><last>Geiger</last></author>
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <pages>163–173</pages>
      <abstract>We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods : the behavioral evaluation methods of (1) challenge test sets and (2) systematic generalization tasks, and the structural evaluation methods of (3) probes and (4) interventions. To facilitate this holistic evaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset focused on <a href="https://en.wikipedia.org/wiki/Logical_consequence">lexical entailment</a> and <a href="https://en.wikipedia.org/wiki/Negation">negation</a>. In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing <a href="https://en.wikipedia.org/wiki/Negation">negation</a>, but that MoNLI fine-tuning addresses this failure. In our structural evaluations, we look for evidence that our top-performing BERT-based model has learned to implement the monotonicity algorithm behind MoNLI. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the <a href="https://en.wikipedia.org/wiki/Causal_model">causal dynamics</a> of the model mirror the <a href="https://en.wikipedia.org/wiki/Causal_model">causal dynamics</a> of this <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> on subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.</abstract>
      <url hash="6f23d1c1">2020.blackboxnlp-1.16</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e8cf6ccc">2020.blackboxnlp-1.16.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.blackboxnlp-1.16</doi>
      <bibkey>geiger-etal-2020-neural</bibkey>
      <pwccode url="https://github.com/atticusg/MoNLI" additional="false">atticusg/MoNLI</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/help">HELP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="19">
      <title>Dissecting Lottery Ticket Transformers : Structural and Behavioral Study of Sparse Neural Machine Translation</title>
      <author><first>Rajiv</first><last>Movva</last></author>
      <author><first>Jason</first><last>Zhao</last></author>
      <pages>193–203</pages>
      <abstract>Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for <a href="https://en.wikipedia.org/wiki/NMT">NMT</a> while maintaining <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>. However, it is unclear how such pruning techniques affect a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>’s learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more <a href="https://en.wikipedia.org/wiki/Code">encoding</a>. Attention mechanisms remain remarkably consistent as sparsity increases.</abstract>
      <url hash="34fcea29">2020.blackboxnlp-1.19</url>
      <attachment type="OptionalSupplementaryMaterial" hash="d5a8b85c">2020.blackboxnlp-1.19.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.blackboxnlp-1.19</doi>
      <video href="https://slideslive.com/38939765" />
      <bibkey>movva-zhao-2020-dissecting</bibkey>
    <title_pt>Dissecando Transformadores de Bilhetes de Loteria: Estudo Estrutural e Comportamental da Tradução Automática Neural Esparsa</title_pt>
      <title_ar>تشريح محولات تذاكر اليانصيب: دراسة هيكلية وسلوكية لترجمة الآلة العصبية المتفرقة</title_ar>
      <title_es>Disección de transformadores de billetes de lotería: estudio estructural y de comportamiento de la traducción automática neuronal dispersa</title_es>
      <title_hi>विच्छेदन लॉटरी टिकट ट्रांसफॉर्मर: विरल तंत्रिका मशीन अनुवाद के संरचनात्मक और व्यवहार अध्ययन</title_hi>
      <title_ja>解剖宝くじトランスフォーマー：まばらな神経機械翻訳の構造と行動の研究</title_ja>
      <title_zh>析彩票变形金刚:疏神经机器译,行也</title_zh>
      <title_ga>Claochladáin Ticéad Crannchuir a Dhíroinnt: Staidéar Struchtúrtha agus Iompraíochta ar Aistriúchán Meaisíní Néaracha Gann</title_ga>
      <title_ka>ლოტერიის ტიკეტის ტრანფორმაციების გაყოფილი: სტრუქტურული და ქეგურაციის შესწავლება</title_ka>
      <title_el>Διάταξη μετασχηματιστών λαχειοφόρων εισιτηρίων: Δομική και Συμπεριφερειακή Μελέτη Σπανιών Νευρικών Μηχανικών Μεταφράσεων</title_el>
      <title_hu>Lottószelvény transzformátorok disszekciója: a ritka neurális gépi fordítás strukturális és viselkedési vizsgálata</title_hu>
      <title_it>Dissettare i trasformatori dei biglietti della lotteria: studio strutturale e comportamentale della traduzione automatica neurale sparsa</title_it>
      <title_kk>Лотериялық беттерді түрлендірушілерді бөліп тастау: Қосымшалық нейралық машинаның аудармасының структуралық және қасиеттерді зерттеу</title_kk>
      <title_lt>Loterijos bilietų keitikliai: struktūrinis ir elgsenos tyrimas sparčiojo nervinės mašinos vertimo būdu</title_lt>
      <title_mk>Трансформирачи на лотарски билети: Структурна и однесувачка студија за преведување на нервозни машини</title_mk>
      <title_ml>ലോട്ടറി ടിക്കറ്റി ട്രാന്‍സ്ഫോര്‍മാര്‍ വിതരണം ചെയ്യുന്നു: സ്പെയിന്‍സ് നെയുറല്‍ മെഷീന്‍ പരിഭാഷ</title_ml>
      <title_mn>Лотерийн салбарын шилжүүлэгчид бөлөөлөх: Структурал болон Behavioral Study of Sparse Neural Machine Translation</title_mn>
      <title_ms>Penukar Tiket Loteri Mempecah: Pelajaran Struktur dan Perilaku Terjemahan Mesin Neural Tersingkat</title_ms>
      <title_mt>Trasformaturi tal-Karti tal-Lotterija li Jqassmu: Studju Strutturali u ta’ l-Imġiba ta’ Traduzzjoni ta’ Magni Newrali Sparse</title_mt>
      <title_no>Avskjæring av lotteriske kartomformarar: Struktural og oppførsel- studie av omsetjing av avslag neuralmaskin</title_no>
      <title_pl>Analiza transformatorów losów loterii: badanie strukturalne i behawioralne rzadkiego neuronowego tłumaczenia maszynowego</title_pl>
      <title_ro>Disectarea transformatorilor de bilete de loterie: Studiul structural și comportamental al traducerii automate neurale rare</title_ro>
      <title_sr>Razvajanje Loterijskih kartica: Strukturalno i ponašanje istraživanja prijevoza neuroloških mašina</title_sr>
      <title_si>ලොට්‍රියි ටිකෙට් වෙනස් කරනවා: ස්ට්‍රූක්ටරුල් සහ ව්‍යාපෘතික අභ්‍යාසයේ ස්පාර්ස් න්‍යූරාල් මැෂ</title_si>
      <title_so>Dissecting Lottery Ticket Transformers: structural and Behavior Study of Sparse Neural Machine Translation</title_so>
      <title_sv>Dissecting Lottery Ticket Transformers: Struktur- och beteendestudie av sparsam neural maskinöversättning</title_sv>
      <title_ta>முழுமையான குறிப்பு மாற்றுபவர்களை விட்டுவிடு</title_ta>
      <title_ur>لوٹری ٹیکٹ تبدیل کرنے والوں کو تقسیم کرتا ہے: اسپارس نیورال ماشین تعلیم کی ساختاری اور رفتاری تحقیق</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Dịch biến hình Vé nhòe nhoẹt: Nghiên cứu cấu trúc và hành vi về máy móc thần kinh</title_vi>
      <title_bg>Разпространяване на лотарийни билетни трансформатори: структурно и поведенческо изследване на оскъдния неврален машинен превод</title_bg>
      <title_hr>Raskomadanje Loterijskih preobraćaja kartica: Strukturalno i ponašanje ispitivanja prijevoza neuroloških strojeva</title_hr>
      <title_da>Dissecting Lottery Ticket Transformers: Strukturel og adfærdsmæssig undersøgelse af sparsom neural maskinoversættelse</title_da>
      <title_nl>Het ontleden van loterijkaarttransformaties: Structurele en Gedragsstudie van schaarse neuronale machinevertaling</title_nl>
      <title_ko>복권 변압기 분석: 희소 신경 기계 번역의 구조와 행위 연구</title_ko>
      <title_fa>تقسیم کردن تغییرات برچسب‌های لوتری: مطالعه ساختاری و رفتاری از ترجمه ماشین عصبی فضایی</title_fa>
      <title_de>Analyse von Lotteriescheintransformatoren: Struktur- und Verhaltensstudie zur spärlichen neuronalen maschinellen Übersetzung</title_de>
      <title_tr>Loteriýa bilen terjimelerini pozmak: structural and Behavioral Study of Sparse Neural Machine Translation</title_tr>
      <title_sw>Utafiti wa Kiteknolojia: Utafiti wa Miundombinu na Utafiti wa Utafiti wa Kihispania Utafiti wa Mashine ya Kiurahisi</title_sw>
      <title_af>Skakel Lotterie Tikket Transformeerders: Struktuurale en Gedrag Studie van Sparse Neurale Masjien Vertaling</title_af>
      <title_am>Transformers: Structural and Behavior Study of Sparse Neural Machine translation</title_am>
      <title_az>Lotteri etiket transformatçıları parçalayır: Sərər nöral maşına çevirilməsinin strukturlu və davranışlıq öyrənməsi</title_az>
      <title_bn>লোটারি টিকেট ট্রান্সফর্মার বিভক্ত করা হচ্ছে: স্পের্স নিউরাল মেশিন অনুবাদ</title_bn>
      <title_id>Transformer Tiket Loteri Memotong: Pengelajaran Struktur dan Perilaku Terjemahan Mesin Neural Tanpa</title_id>
      <title_bs>Raspoređivanje Loterijskih preobraćaja kartica: Strukturalno i ponašanje proučavanja neuralnog preobraćanja</title_bs>
      <title_ca>Transformers de bitllets de loteria disseccionants: Estudio estructural i comportamental de traducció de màquines neuronals ràpides</title_ca>
      <title_hy>Բաժանող վիճակախաղի տոմսերի փոխակերպողները. Փոքր նյարդային մեքենայի թարգմանման կառուցվածքային և վարքագծային ուսումնասիրություն</title_hy>
      <title_cs>Disekce transformátorů loterijních lístků: Strukturální a behaviorální studie řídkého neuronového strojového překladu</title_cs>
      <title_et>Loteripiletite muundajate levitamine: hõreda neuroalse masintõlke struktuuri- ja käitumisuuring</title_et>
      <title_fi>Lottery Ticket Transformers: Harvoin hermojen konekäännöksen rakenne- ja käyttäytymistutkimus</title_fi>
      <title_sq>Transformuesit e biletave të shpërndarë të lotisë: Studimi strukturor dhe sjelljeje i përkthimit të makinës së shpejtë nervore</title_sq>
      <title_jv>Dissection</title_jv>
      <title_ha>Transformers: Fractural and Atomic Research of spaspastic Neural Machine Translate</title_ha>
      <title_sk>Disekacija loterijskih vstopnic transformatorjev: strukturna in vedenjska študija redkega živčnega strojnega prevajanja</title_sk>
      <title_he>משתני כרטיסי לוטריה מתפרקים: מחקר מבנה ותנהגות של תרגום של מכונות נוירות נמוכות</title_he>
      <title_bo>Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation</title_bo>
      <abstract_es>El trabajo reciente sobre la hipótesis del billete de lotería ha producido Transformers muy dispersos para NMT mientras se mantiene BLEU. Sin embargo, no está claro cómo estas técnicas de poda afectan las representaciones aprendidas de un modelo. Al sondear Transformers con más y más pesos de baja magnitud eliminados, descubrimos que la información semántica compleja es la primera en degradarse. El análisis de las activaciones internas revela que las capas superiores divergen más a lo largo de la poda, convirtiéndose gradualmente en menos complejas que sus contrapartes densas. Mientras tanto, las primeras capas de modelos dispersos comienzan a realizar más codificación. Los mecanismos de atención siguen siendo notablemente consistentes a medida que aumenta la dispersión.</abstract_es>
      <abstract_ar>أنتج العمل الأخير على فرضية بطاقة اليانصيب محولات متفرقة للغاية لـ NMT مع الحفاظ على BLEU. ومع ذلك ، فمن غير الواضح كيف تؤثر تقنيات التقليم هذه على التمثيلات المكتسبة للنموذج. من خلال فحص المحولات التي تحتوي على المزيد والمزيد من الأوزان المنخفضة الحجم التي تم تشذيبها بعيدًا ، نجد أن المعلومات الدلالية المعقدة هي أولًا يتم تدهورها. يكشف تحليل التنشيطات الداخلية أن الطبقات العليا تتباعد أكثر على مدار عملية التقليم ، وتصبح تدريجياً أقل تعقيدًا من نظيراتها الكثيفة. وفي الوقت نفسه ، تبدأ الطبقات المبكرة من النماذج المتفرقة في أداء المزيد من التشفير. تظل آليات الانتباه متسقة بشكل ملحوظ مع زيادة التباين.</abstract_ar>
      <abstract_pt>Trabalhos recentes sobre a hipótese do bilhete de loteria produziram Transformers altamente esparsos para NMT, mantendo o BLEU. No entanto, não está claro como essas técnicas de poda afetam as representações aprendidas de um modelo. Ao sondar Transformadores com pesos cada vez mais de baixa magnitude removidos, descobrimos que a informação semântica complexa é a primeira a ser degradada. A análise das ativações internas revela que as camadas mais altas divergem mais ao longo da poda, tornando-se gradualmente menos complexas do que suas contrapartes densas. Enquanto isso, as primeiras camadas de modelos esparsos começam a realizar mais codificação. Os mecanismos de atenção permanecem notavelmente consistentes à medida que a dispersão aumenta.</abstract_pt>
      <abstract_ja>宝くじの仮説に関する最近の研究では、BLEUを維持しながらNMTのための非常にまばらなトランスフォーマーが作られています。しかしながら、そのような枝刈り技術がモデルの学習された表現にどのように影響するかは不明である。より多くの低マグニチュードの重みを刈り取ったトランスフォーマーを探索することで、複雑な意味情報が最初に劣化することがわかります。内部活性化の分析により、高層層は枝刈りの過程で最も発散し、密度の高い層よりも徐々に複雑になっていくことが明らかになりました。一方、まばらなモデルの初期の層は、より多くの符号化を実行し始めます。注目メカニズムは、希少性が増加するにつれて著しく一貫したままである。</abstract_ja>
      <abstract_zh>近彩票伪事NMT生疏变形金刚,兼守BLEU。 然尚未详此类修剪之术,何以加于模形之学也。 探剪愈多低量级权变形金刚见语义先降。 内激活之分析表明,剪剪之际,高者最大,渐不如密者应物杂。 同时疏早期,更行更多编码。 随疏增益,意机犹一。</abstract_zh>
      <abstract_hi>लॉटरी टिकट परिकल्पना पर हाल के काम ने BLEU को बनाए रखते हुए NMT के लिए अत्यधिक विरल ट्रांसफॉर्मर का उत्पादन किया है। हालांकि, यह स्पष्ट नहीं है कि इस तरह की छंटाई तकनीकें मॉडल के सीखे गए अभ्यावेदन को कैसे प्रभावित करती हैं। अधिक से अधिक कम परिमाण वजन के साथ ट्रांसफॉर्मर की जांच करके, हम पाते हैं कि जटिल शब्दार्थ जानकारी को सबसे पहले नीचा दिखाया जाना है। आंतरिक सक्रियणों के विश्लेषण से पता चलता है कि उच्च परतें छंटाई के दौरान सबसे अधिक अलग हो जाती हैं, धीरे-धीरे उनके घने समकक्षों की तुलना में कम जटिल हो जाती हैं। इस बीच, विरल मॉडल की शुरुआती परतें अधिक एन्कोडिंग करना शुरू कर देती हैं। ध्यान तंत्र उल्लेखनीय रूप से सुसंगत रहते हैं क्योंकि स्पार्सिटी बढ़ जाती है।</abstract_hi>
      <abstract_ga>Tá obair le déanaí ar an hipitéis ticéad crannchuir tar éis Claochladáin an-bheag a tháirgeadh do NMT agus BLEU á chothabháil. Níl sé soiléir, áfach, conas a théann a leithéid de theicnící bearradh i bhfeidhm ar léirithe foghlamtha samhla. Trí iniúchadh a dhéanamh ar Chlaochladáin a bhfuil níos mó agus níos mó meáchain íseal-mhéid acu á ngearradh ar shiúl, feicimid go bhfuil faisnéis shéimeantach chasta le díghrádú ar dtús. Léiríonn anailís ar ghníomhartha inmheánacha gurb iad na sraitheanna níos airde an difríocht is mó le linn bearradh, ag éirí níos lú casta de réir a chéile ná a gcomhghleacaithe dlúth. Idir an dá linn, tosaíonn sraitheanna luath de mhúnlaí tanaí ag déanamh ionchódú níos mó. Fanann meicníochtaí aird thar a bheith comhsheasmhach de réir mar a mhéadaíonn an teimhneacht.</abstract_ga>
      <abstract_el>Πρόσφατες εργασίες σχετικά με την υπόθεση λαχειοφόρων λαχειοφόρων εισιτηρίων έχουν δημιουργήσει εξαιρετικά σπάνιους μετασχηματιστές για NMT διατηρώντας παράλληλα BLEU. Ωστόσο, δεν είναι σαφές πώς τέτοιες τεχνικές κλαδέματος επηρεάζουν τις διδαγμένες αναπαραστάσεις ενός μοντέλου. Εξετάζοντας τους μετασχηματιστές με όλο και περισσότερα βάρη χαμηλού μεγέθους που κόβονται μακριά, διαπιστώνουμε ότι οι σύνθετες σημασιολογικές πληροφορίες πρέπει πρώτα να υποβαθμιστούν. Η ανάλυση των εσωτερικών ενεργοποιήσεων αποκαλύπτει ότι τα υψηλότερα στρώματα αποκλίνουν περισσότερο κατά τη διάρκεια του κλαδέματος, γίνονται σταδιακά λιγότερο πολύπλοκα από τα πυκνά ομόλογά τους. Εν τω μεταξύ, τα πρώτα στρώματα των αραίων μοντέλων αρχίζουν να εκτελούν περισσότερη κωδικοποίηση. Οι μηχανισμοί προσοχής παραμένουν αξιοσημείωτα συνεπείς καθώς αυξάνεται η σπανιότητα.</abstract_el>
      <abstract_hu>A lottószelvény hipotézisének közelmúltbeli munkája rendkívül ritka transzformátorokat hozott létre NMT-hez, miközben fenntartja a BLEU-t. Nem világos azonban, hogy az ilyen metszési technikák hogyan befolyásolják a modell tanult reprezentációit. Egyre több és több alacsony nagyságrendű transzformátort vizsgálva úgy találjuk, hogy az összetett szemantikai információk elsőként romlanak le. A belső aktivációk elemzése azt mutatja, hogy a magasabb rétegek a metszés során a legtöbbször eltérnek, fokozatosan kevésbé bonyolultak, mint sűrű társaik. Eközben a ritka modellek korai rétegei elkezdenek több kódolást végezni. A figyelmeztetési mechanizmusok továbbra is rendkívül következetesek maradnak, ahogy a ritkaság növekszik.</abstract_hu>
      <abstract_it>Il recente lavoro sull'ipotesi dei biglietti della lotteria ha prodotto Transformers molto scarsi per NMT pur mantenendo BLEU. Tuttavia, non è chiaro come tali tecniche di potatura influenzino le rappresentazioni apprese di un modello. Analizzando Transformers con sempre più pesi di bassa magnitudine tagliati via, scopriamo che le informazioni semantiche complesse vengono prima degradate. L'analisi delle attivazioni interne rivela che gli strati più alti differiscono maggiormente nel corso della potatura, diventando gradualmente meno complessi rispetto alle loro controparti dense. Nel frattempo, i primi livelli di modelli rari iniziano ad eseguire più codifica. I meccanismi di attenzione rimangono notevolmente coerenti man mano che aumenta la scarsità.</abstract_it>
      <abstract_lt>Neseniai atliktas loterijos bilietų hipotezės tyrimas sukėlė labai retas NMT transformatorius ir kartu išlaikė BLEU. However, it is unclear how such pruning techniques affect a model's learned representations.  By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded.  Vidaus aktyvinimo analizė rodo, kad aukštesni sluoksniai labiausiai svyruojant skiriasi, laipsniškai tampa mažiau sudėtingi nei jų tankūs lygiaverčiai sluoksniai. Tuo tarpu pradedami naudoti ankstyvuosius nedidelių modelių sluoksnius. Atsižvelgiant į tai, kad sparčiai didėja, dėmesio mechanizmai išlieka nepaprastai nuoseklūs.</abstract_lt>
      <abstract_mk>Неодамнешната работа на хипотезата за лотарски билети произведе многу ретки Трансформери за НМТ, истовремено одржувајќи го БЛЕ. Сепак, не е јасно како ваквите техники на исцепување влијаат на научените претставувања на еден модел. Со истражување на Трансформерите со сé повеќе и пониски тегови, откриваме дека комплексните семантични информации прво се деградираат. Анализата на внатрешните активации открива дека повисоките слоеви се разликуваат најмногу во текот на прскањето, постепено станувајќи помалку комплексни од нивните густи колеги. Во меѓувреме, раните слоеви на мали модели почнуваат да извршуваат повеќе кодирање. Механизмите за внимание остануваат исклучително константни со зголемувањето на реткоста.</abstract_mk>
      <abstract_ms>Kerja baru-baru ini pada hipotesis tiket loteri telah menghasilkan Transformers yang sangat jarang untuk NMT semasa mengekalkan BLEU. Namun, tidak jelas bagaimana teknik pemotongan seperti ini mempengaruhi perwakilan belajar model. Dengan menyelidiki Transformers dengan berat-berat yang lebih rendah dan lebih rendah dipotong, kita mendapati bahawa maklumat semantik kompleks adalah pertama untuk dihina. Analisi aktivasi dalaman menunjukkan bahawa lapisan yang lebih tinggi bergerak paling dalam perjalanan pemotongan, secara perlahan-lahan menjadi lebih rumit daripada rakan-rakan yang padat. Sementara itu, lapisan awal model jarang mula melakukan pengekodan lebih. Mekanisme perhatian tetap sangat konsisten semasa kecepatan meningkat.</abstract_ms>
      <abstract_mt>Xogħol reċenti dwar l-ipoteżi tal-biljetti tal-lotterija pproduċa Transformers rari ħafna għall-NMT filwaqt li żammet il-BLEU. Madankollu, mhuwiex ċar kif dawn it-tekniki ta’ pruning jaffettwaw ir-rappreżentazzjonijiet miksuba ta’ mudell. Billi jinstabu Transformers b’piżijiet ta’ daqs dejjem aktar baxx imnaqqsa ’l bogħod, isibu li l-informazzjoni semantika kumplessa l-ewwel trid tiġi degradata. L-analiżi tal-attivazzjonijiet interni turi li saffi ogħla jvarjaw l-aktar matul il-pruning, u gradwalment isiru inqas kumplessi mill-kontropartijiet densi tagħhom. Sadanittant, saffi bikrin ta’ mudelli żgħar jibdew iwettqu aktar kodifikazzjoni. Il-mekkaniżmi ta’ attenzjoni jibqgħu konsistenti b’mod notevoli hekk kif tiżdied l-iskarsezza.</abstract_mt>
      <abstract_ml>ലോട്ടറി ടിക്കറ്റ് ഹൈപ്പിറ്റസിസിന്റെ അടുത്ത പ്രവര്‍ത്തിയ്ക്കുന്നത് ബിലിയു സൂക്ഷിച്ചുകൊണ്ടിരിക്കുമ്പോ എങ്കിലും ഒരു മോഡലിന്റെ പഠിച്ച പ്രതിനിധികളെ എങ്ങനെ ബാധിക്കുന്നുവെന്ന് അത് വ്യക്തമായിട്ടില്ല. ട്രാന്‍സ്ഫോര്‍മാര്‍ പരിശോധിക്കുന്നത് കൂടുതല്‍ കുറച്ചും കൂടുതല്‍ വലിപ്പമുള്ള തൂക്കങ്ങള്‍ നീക്കം ചെയ്യുന്നതാണെന്ന് നമുക ആന്തരീക പ്രവര്‍ത്തനങ്ങളുടെ അന്വേഷണം വെളിപ്പെടുത്തുന്നു, കൂടുതല്‍ തട്ടുകള്‍ ബുദ്ധിമുട്ടിയുടെ കാര്യത്തില്‍ ഏറ്റവും വികസിച് അതുകൊണ്ട്, സ്പാസ് മോഡലുകളുടെ ആദ്യമായ തലകള്‍ കൂടുതല്‍ കോഡിങ് പ്രവര്‍ത്തിപ്പിക്കാന്‍ തുടങ്ങി. ശ്രദ്ധിക്കുന്ന മെക്കിനസികങ്ങള്‍ സ്പെയിസിറ്റി വര്‍ദ്ധിപ്പിക്കുന്നത് പോലെയാണ്.</abstract_ml>
      <abstract_no>Nyleg arbeidet på lotteringshypotesien har produsert svært sparse transformerer for NMT under behandling av BLEU. Det er imidlertid ukjent korleis slike teknikk påvirkar ein modell lærte representasjonar. Ved å prøve transformerer med meir og meir låg størrelsesvekt, finn vi at komplekse semantiske informasjon først skal degraderast. Analyser av interne aktivasjonar viser at høgare lag forskjeller dei fleste over trekking, og gradvis blir mindre komplekse enn dei tette trekantane. I mellomtida startar tidlegare lag med sparse modeller med meir koding. Merknadsmekanismar blir merkelig konsistent når sparsitet øker.</abstract_no>
      <abstract_mn>Сүүлийн үед Лотерийн бичлэгийн таамаглалын талаар ажиллах нь БЛУ-г хадгалах үед NMT-ийн төлөөлөгчийн төлөөлөгчийн төлөөлөгчийг бүтээсэн. Гэхдээ ийм загварын сурсан үзүүлэлтийг хэрхэн нөлөөлж байгааг мэдэхгүй. Трансформацуудыг илүү бага хэмжээтэй судалгаагаар бид комплекс семантик мэдээллийг эхлээд ухамсарлах болно. Дотоод дахь үйл ажиллагааны шинжилгээс илүү өндөр давхар нь хамгийн их хэмжээний хувьд хуваагдаж, мөргөн хамтрагчдаас бага цогц болж байна. Гэвч эхний загварын давхар нь илүү кодлого хийж эхэлсэн. Хэрэв анхаарлын механизм нэмэгдэхэд гайхалтай байдаг.</abstract_mn>
      <abstract_ka>მხოლოდ ლოტერიის ჰიპოტეზაზე სამუშაო სამუშაო ტრანფორმეტრები NMT-სთვის გამოყენება BLEU-სთვის. მაგრამ არ არის წარმოიდგინელი, როგორ ასეთი წარმოდგინული ტექნოგიები მოდელის გასწავლილი რესპენტირებების შესახებ. ტრანფორმენტირების შესაბამისთვის უფრო და უფრო მეტი მანგნიტური მანგნიტური გაზრუნებით, ჩვენ ვიღებთ, რომ კომპლექსი სიმენტიკური ინფორმაცია პირველი და ინტერული აქტივაციების ანალიზი აღმოჩნდება, რომ უფრო მეტი სინამდვილეები უფრო მეტი სინამდვილეების განსხვავება, რომელიც უფრო მეტი სინამდვილეების განსხვავება საშუალოდ, საწყისო მოდელეების მონაცემები უფრო კოდირებას დაიწყება. მექანიზმი დაახლოებით უფრო შესაძლებელია, როგორც სწრაფად სწრაფად სწრაფად სწორებულია.</abstract_ka>
      <abstract_kk>Жуырдағы лотериялық билеттер гипотезасының жұмысы BLEU сақтау кезінде NMT үшін артық түрлендірушілерді жасады. Бірақ үлгісінің оқылған түсініктеріне қалай әсер ететін техникалар білмейді. Трансформацияларды біріншіден көп және төмен үлкендердің теңдігін теңдіру арқылы, біз комплекс семантикалық мәліметтер біріншіден деградицияланады. Ішкі белсенділіктердің анализиясы, қабаттардың көпшілігін бұл қабаттардың көпшілігін бұл көпшіліктерінде айырып, тұтас партнерінен артық көпшілікті Осы уақытта, кеңістік моделдердің алдыңғы қабаттары көп кодтамасын орындау бастады. Қарапайым механизмтері көтерілген кезде қарапайым болады.</abstract_kk>
      <abstract_pl>Ostatnie prace nad hipotezą losów loterii stworzyły bardzo rzadkie transformatory dla NMT przy jednoczesnym utrzymaniu BLEU. Nie jest jednak jasne, w jaki sposób takie techniki przycinania wpływają na nauczone reprezentacje modelu. Badając transformatory z coraz większą liczbą ciężarów niskiej wielkości, odkrywamy, że złożone informacje semantyczne są najpierw degradowane. Analiza aktywacji wewnętrznych ujawnia, że wyższe warstwy najbardziej różnią się w trakcie przycinania, stopniowo stając się mniej złożone niż ich gęste odpowiedniki. Tymczasem wczesne warstwy rzadkich modeli zaczynają wykonywać więcej kodowania. Mechanizmy uwagi pozostają niezwykle spójne wraz ze wzrostem słabości.</abstract_pl>
      <abstract_ro>Lucrările recente asupra ipotezei biletelor de loterie au produs transformatoare foarte rare pentru NMT, menținând în același timp BLEU. Cu toate acestea, nu este clar cum astfel de tehnici de tăiere afectează reprezentările învățate ale unui model. Prin sondarea Transformers cu greutăți din ce în ce mai mici tăiate departe, descoperim că informațiile semantice complexe sunt prima care sunt degradate. Analiza activărilor interne arată că straturile superioare diferă cel mai mult pe parcursul tăierii, devenind treptat mai puțin complexe decât omologii lor densi. Între timp, straturile timpurii de modele rare încep să efectueze mai multe codări. Mecanismele de atenție rămân remarcabil de consecvente pe măsură ce cantitatea scăzută crește.</abstract_ro>
      <abstract_sr>Nedavni rad na hipotezi karte loterije proizveo je vrlo rezervne transformere NMT-a dok održava BLEU. Međutim, nije jasno kako takve brižne tehnike utiču na naučene predstave model a. Probajući transformatore sa većim i visokim težinama niskog veličine, otkrivamo da će prvo biti smanjena kompleksna semantična informacija. Analiza unutrašnjih aktivacija pokazuje da se viši slojevi najviše razlikuju tokom pružanja, postupno postaju manje kompleksni od njihovih gustih kolega. U međuvremenu, rani sloji rezervnih modela počinju izvršiti više kodiranja. Mehanizmi pažnje ostaju izvanredno konsistentni dok se povećava sparsitet.</abstract_sr>
      <abstract_so>Shaqo ku saabsan tijaatarka tigidhada lottery waxay sameeyeen tarjumayaal aad u yar oo ay u dhaqdhaqaaqaan BLEU. Si kastaba ha ahaatee ma ahan sida qaababka caqliga ah ay u saameyso noocyada lagu bartay. Marka lagu imtixaamo turjumayaasha iyo miisaan badan oo hoos u yar, waxaynu aragnaa in macluumaad adag ee semantika marka hore la hoosaysiiyo. Analyska waxqabadka gudaha ah wuxuu muujiyaa in qasnadaha sare aad u kala bedelaan xiliga waxgarashada, si taxadar ah waxay u noqdaan wax ka yar murugaysan saaxiibbadooda hoose. Wakhtigaas waxaa bilaabaya inay sameeyaan codsiga badan. Meherka daryeelka waxyaabaha la'aanta ah waxay u sii socon yihiin sida kordhiya cimriga.</abstract_so>
      <abstract_sv>Nyligen arbete med lotteri lotter hypotesen har producerat mycket glesa Transformers för NMT samtidigt som BLEU upprätthålls. Det är dock oklart hur sådana beskärningstekniker påverkar en modells lärda representationer. Genom att sondera Transformers med fler och fler låga vikter avskurna, finner vi att komplex semantisk information först försämras. Analys av interna aktiveringar visar att högre skikt skiljer sig mest under beskärningen och gradvis blir mindre komplexa än deras täta motsvarigheter. Samtidigt börjar tidiga lager av glesa modeller utföra mer kodning. Uppmärksamhetsmekanismerna förblir anmärkningsvärt konsekventa i takt med att sparnivån ökar.</abstract_sv>
      <abstract_ta>சமீபத்தில் லாட்டரி டிக்கெட் துப்பாக்கத்தில் செயல்பாடு BLEU வைத்திருக்கும் போது NMT மாற்றுபவர்களை மிகவும் வெற ஆனால், இவ்வாறு புதிய தொழில்நுட்பம் எப்படி ஒரு மாதிரியின் கற்றப்பட்ட பங்குகளை பாதிக்கும் என்பது தெளிவா மாற்றுபவர்களை மேலும் அதிக குறைந்த அளவுகளை நீக்கி விட்டதால், சிக்கலான பெம்பெண்டிக் தகவல் முதலில் குறைந்துவிடும் என்பதை நா உள்ளார்ந்த செயல்பாடுகளின் ஆய்வு அதிக குறியீட்டை செய்ய மாதிரிகளின் ஆரம்ப அடுக்குகள் கவனம் முறைமைகள் மிகவும் பொருத்தமாக இருக்கும் வெளிப்பாடு அதிகரிக்கும் போது.</abstract_ta>
      <abstract_si>ලොට්‍රියි ටිකෙට් විශ්වාසයෙන් අලුත් වැඩේ නැවත NMT වෙනුවෙන් ගොඩක් ප්‍රමාණයක් නිර්මාණය කරලා තියෙ නමුත්, ඒ වගේම නැහැ මොඩල් එක්ක ඉගෙන ගත්ත ප්‍රතිනිධානයකට කොහොමද ප්‍රතිකාර කරන්නේ කියලා. ප්‍රමාණ කරණාකරුවන්ට වැඩිය හා වැඩියෙන් ප්‍රමාණය කරන්න, අපිට හොයාගන්න පුළුවන් සැමැන්තික තොරතුරු විනාශ කරනව ඇතුළු සක්‍රීය විශ්ලේෂණය ප්‍රකාශ කරනවා විශ්ලේෂණයෙන් විශ්ලේෂණය කරනවා කියලා වඩා විශ්වාස කරනවා කියලා වඩා ව අනුවෙන් වෙලාවෙන්, පළවෙනි ස්පර්ස් මෝඩේල්ස් වලින් තරම් සංකේතනය කරන්න පටන් ගන්නවා. බලාපොරොත්තු පද්ධතිය සාමාන්‍ය විශ්වාස කරන්න පුළුවන් වෙනවා.</abstract_si>
      <abstract_ur>لوتری ٹیکٹ کی فرضی پر اچھا کام NMT کے لئے بہت اچھا ترفنسر پیدا کیا گیا ہے جبکہ BLEU حفاظت کرتا ہے۔ لیکن یہ معلوم نہیں کہ ایک موڈل کی تعلیم کا کس طرح اثر کرتا ہے۔ ٹرانسفور کو آزمائش کے ذریعہ سے زیادہ اور زیادہ کم بڑائی وزن سے دور کر دیا گیا ہے، ہم دیکھتے ہیں کہ پیچیدہ سیمانٹی معلومات پہلے ذلیل ہونے والی ہے. داخلی فعالیت کا تحلیل ظاہر کرتا ہے کہ بالاترین لائٹوں سے زیادہ تغییر کرتا ہے اور ان کے گہرے کنٹوروں سے کم پیچیدہ ہوجاتا ہے۔ یہاں تک کہ اسپرس موڈل کے پہلے لہروں سے زیادہ اکڈینڈ کرنا شروع ہوتا ہے۔ توجه کے مکانیزوں باقی رہتے ہیں جب تکلیف اضافہ ہوتی ہے۔</abstract_ur>
      <abstract_uz>Name Lekin, bu narsalar modelning o'rganilgan tashkilotlariga qanday qo'llanmaydi. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded.  Ichki harakatlarni taʼminlovchi, yuqori qatlamlarning ko'p chegarasini o'zgartiradi, tez qismlaridan murakkab bo'ladi. Hozirga, kichkina modellarning birinchi qatlam kodlash usulini ishga tushirishni boshlaydi. Aniqlik mechanislari kichkina ko'paytirishda juda qiziqarli bo'ladi.</abstract_uz>
      <abstract_vi>Công trình gần đây về giả thuyết vé số đã sản xuất ra các biến hình của công ty NMT rất ít trong khi còn nguyên tiếng bíp. Tuy nhiên, không rõ làm thế nào các kỹ thuật cắt tỉa ảnh hưởng đến các biểu tượng được học hỏi. Bằng cách dò tìm các Transformers với các lượng trọng lượng thấp bị tỉa dần, chúng tôi thấy thông tin cơ bản đầu tiên bị thoái hoá. Phân tích kích hoạt nội bộ cho thấy các lớp cao khác biệt nhiều nhất trong quá trình cắt tỉa, dần dần trở nên phức tạp hơn so với các lớp đông đúc. Trong khi đó, lớp đầu của các mô hình rải rác bắt đầu đặt thêm mã số. Các cơ quan chú ý vẫn ổn định mỗi khi số lượng hẹp tăng.</abstract_vi>
      <abstract_bg>Последната работа по хипотезата за лотарийни билети доведе до изключително редки трансформатори за НМТ, като същевременно се поддържа Блеу. Въпреки това, не е ясно как такива техники за подрязване влияят на наученото представяне на модела. Чрез сондиране на трансформатори с все повече и повече тежести с ниска магнитуда, откриваме, че сложната семантична информация първо трябва да бъде деградирана. Анализът на вътрешните активирания разкрива, че по-високите слоеве се различават най-много в хода на подрязването, постепенно стават по-малко сложни от техните плътни колеги. Междувременно ранните слоеве от редки модели започват да изпълняват повече кодиране. Механизмите за внимание остават забележително последователни с увеличаването на оскъдността.</abstract_bg>
      <abstract_da>Nyligt arbejde med lotteri billet hypotesen har produceret meget sparsomme Transformers til NMT samtidig med at BLEU opretholdes. Det er imidlertid uklart, hvordan sådanne beskæringsteknikker påvirker en models lærte repræsentationer. Ved at undersøge Transformers med flere og flere lav størrelse vægte beskåret væk, finder vi, at komplekse semantiske oplysninger er først til at blive nedbrudt. Analyse af interne aktiveringer viser, at højere lag adskiller sig mest i løbet af beskæringen og gradvist bliver mindre komplekse end deres tætte modstykker. I mellemtiden begynder tidlige lag af sparsomme modeller at udføre mere kodning. Opmærksomhedsmekanismerne forbliver bemærkelsesværdigt konsekvente, efterhånden som sparsomheden stiger.</abstract_da>
      <abstract_hr>Nedavni rad na hipotezi karte loterije proizveo je vrlo rezervne transformere NMT tijekom održavanja BLEU-a. Međutim, nije jasno kako takve brižne tehnike utječu na naučene predstave model a. Provjeravajući transformere sa visokom i visokom težinom niskog veličine, otkrili smo da će prvo biti smanjena kompleksna semantička informacija. Analiza unutrašnjih aktivacija pokazuje da se viši slojevi najviše razlikuju tijekom pružanja, postupno postaju manje složeni od njihovih gustih kolega. U međuvremenu, rani sloji rezervnih modela počinju izvršiti više kodiranja. Mehanizmi pažnje ostaju izvanredno odgovarajući kad povećava rezervnost.</abstract_hr>
      <abstract_nl>Recent werk aan de loterij lot hypothese heeft zeer schaarse Transformers voor NMT geproduceerd met behoud van BLEU. Het is echter onduidelijk hoe dergelijke snoei technieken invloed hebben op de geleerde representaties van een model. Door Transformers te onderzoeken met steeds meer gewichten van lage grootte weggesneden, ontdekken we dat complexe semantische informatie eerst wordt gedegradeerd. Analyse van interne activeringen toont aan dat hogere lagen het meest uiteenlopen in de loop van het snoeien, geleidelijk minder complex worden dan hun dichte tegenhangers. Ondertussen beginnen vroege lagen van schaarse modellen meer codering uit te voeren. Aandacht mechanismen blijven opmerkelijk consistent naarmate de schaarste toeneemt.</abstract_nl>
      <abstract_de>Jüngste Arbeiten an der Lotteriescheinhypothese haben sehr spärliche Transformatoren für NMT unter Beibehaltung der BLEU produziert. Es ist jedoch unklar, wie solche Beschneidungstechniken die erlernten Darstellungen eines Modells beeinflussen. Indem wir Transformatoren mit immer mehr niederen Gewichten abtasten, stellen wir fest, dass komplexe semantische Informationen zuerst degradiert werden. Die Analyse der inneren Aktivierungen zeigt, dass höhere Schichten im Laufe des Beschnitts am meisten divergieren und allmählich weniger komplex werden als ihre dichten Pendants. Inzwischen beginnen frühe Schichten von spärlichen Modellen, mehr Codierung durchzuführen. Aufmerksamkeitsmechanismen bleiben bemerkenswert konsistent, wenn die Sparsität zunimmt.</abstract_de>
      <abstract_id>Pekerjaan baru-baru ini pada hipotesis tiket loteri telah menghasilkan Transformers sangat jarang untuk NMT sementara mempertahankan BLEU. Namun, tidak jelas bagaimana teknik pemotong tersebut mempengaruhi representation belajar model. Dengan menyelidiki Transformers dengan berat badan yang semakin rendah, kami menemukan bahwa informasi semantis kompleks adalah pertama untuk menjadi rendah. Analisi aktivasi interna mengungkapkan bahwa lapisan yang lebih tinggi paling bergerak sepanjang perjalanan pemotongan, secara perlahan-lahan menjadi lebih rumit dari rekan-rekan yang padat mereka. Sementara itu, lapisan awal dari model kecil mulai melakukan lebih banyak pengekodan. Mekanisme perhatian tetap sangat konsisten saat kecepatan meningkat.</abstract_id>
      <abstract_fa>اخیراً کار روی فرضیه بلیط لوتری در زمان حفظ BLEU، تغییر‌دهنده‌های زیادی برای NMT تولید کرده است. با این حال، این تکنیک‌های پاره‌گیری چگونه بر نمایش‌های یاد گرفته‌ی یک مدل تأثیر می‌دهد، مشخص نیست. با امتحان تغییر‌دهندگان با وزن‌های بیشتر و کمتر از ارتفاع پایین، می‌بینیم که اول اطلاعات سنتی‌های پیچیده‌ای نابود شده است. تحلیل فعالیت های داخلی نشان می دهد که لایه های بالاتر بیشتر در مسیر تغییر کردن و به تدریج کمتر از همکاران dense آنها پیچیده می شوند. در ضمن، لایه‌های اولین مدل‌های خاکستری شروع می‌کنند که کودکان بیشتری انجام دهند. مکانیسم توجه به طور کامل هماهنگی می‌ماند، در حالی که آرامش افزایش می‌یابد.</abstract_fa>
      <abstract_sw>Kazi za hivi karibuni kuhusu nadharia ya tiketi ya madini imetengeneza WaTransformers kwa ajili ya NMT wakati wa kuendelea BLEU. Hata hivyo, haijulikani jinsi mbinu hizi za akili zinavyoathiri uwakilishi wa modeli waliojifunza. Kwa kuwajaribu WaTransfers na mizani yenye kiwango cha chini imeondolewa, tunagundua kuwa taarifa tatizo ni ya kwanza ya kupunguza. Uchambuzi wa shughuli za ndani unaonyesha kuwa vipande vya juu vinavyotofautiana zaidi katika kipindi cha kuelewa, kwa taratibu kinakuwa tatizo kuliko wapenzi wao wenye msingi. Wakati huo huo, vipande vya mwanzo vya mifano ya uchimbaji vinaanza kufanya kodi zaidi. Attention mechanisms remain remarkably consistent as sparsity increases.</abstract_sw>
      <abstract_af>Onlangse werk op die loterie belet hipotees het baie sparse transformeerders vir NMT produseer terwyl BLEU behou. Maar dit is onbekend hoe sodanige pruning teknike 'n model se leer verteenwoordighede beïnvloor. Deur te probeer Transformers met meer en meer lae magnitude gewigte wat weg uitgebreek is, vind ons dat kompleks semantiese inligting eerste is om afgebreek te word. Analiseer van interne aktiwiteite vertoon dat hoëre lage mees oor die loop van pruning verskuif word, gradief minder kompleks word as hulle dense kunstenaars. Name Aangaande mekanisme bly remarkante konsistent as sparsiteit vergroot word.</abstract_af>
      <abstract_tr>Ýakynda loteriýan bilet teorisinde işlenýän işi BLEU tutulanda NMT üçin gaty uly gaýd edilen Transformerçiler üretildi. Ama bu kadar akıllı teknolojiler modelinin öğrenmiş temsillerine nähili etkisi yaratmaz. Transformerleri köp we azaltrak ağırlıklar bilen denedip, ilkinji gezek kompleks semantik maglumaty azaltmak üçin pikir edýäris. Daşary janlaşdyrmalaryň analizi ýokary katlaryň ýokary ýokary ýokary ýokary düşürip, ýokary ýokary ýakynlaşyklaryndan az karmaşık bolup görünýär. Bu arada, irden depler nusgalarynyň ködlemeleri başarmak üçin başlaýar. Seresap mekanizmalary ýuwaşlyk bilen aýratyn bir şekilde dowam edýärler.</abstract_tr>
      <abstract_am>በሎትራ ቲኪት hypothesis ላይ የሚሠራ ሥራ BLEU በመጠበቅ ጊዜ ለNMT የተለየ ፍላጻዎችን እጅግ ያሳያል፡፡ ነገር ግን እንዲህ ያሉ ብልሃት የሞዴል ተማሪዎችን እንዴት እንደሚያስጨንቁበት አይገለጽም፡፡ በተጨማሪና በሚያነካው ሚዛን በመፈታት፣ የተጨማሪው የsemantic መረጃ በመጀመሪያ እንዲያሳፍር እናገኛለን፡፡ የውስብ አካባቢዎች ትምህርት፣ ከፍተኛ ደረጃዎች በአስተዋይ ክፍል ላይ አብዛኛውን ይለየቃሉ፡፡ በዚያን ጊዜም፣ የቀድሞው ደረጃዎች የጭብጥ ምርጫዎች አካባቢ አካባቢ ማድረግ ይጀምራሉ፡፡ የጥያቄ አካባቢዎች እየጨመረ ቁጥጥር እንደሚያበዛ ይኖራል፡፡</abstract_am>
      <abstract_ko>최근 로또 가설에 관한 작업은 NMT에 고도로 드문 변압기를 만들어내면서 BLEU를 유지했다.그러나 이런 가위질 기술이 모델의 학습 표시에 어떻게 영향을 미치는지는 아직 분명하지 않다.점점 더 많은 저량급 권한을 가진 변압기를 탐측함으로써 우리는 복잡한 의미 정보가 먼저 강등되는 것을 발견했다.내부 활성화에 대한 분석에 따르면 더 높은 층은 가위질 과정에서 가장 크게 분화되고 밀집층보다 점점 복잡하지 않게 변한다.또한 희소 모형의 초기 층에서 더 많은 인코딩을 실행하기 시작합니다.희소도가 증가함에 따라 주의력 메커니즘은 현저한 일치성을 유지한다.</abstract_ko>
      <abstract_hy>Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU.  Այնուամենայնիվ, անհասկանալի չէ, թե ինչպես են նմանատիպ խզբզելու մեթոդները ազդում մոդելի սովոր ներկայացումների վրա: Երբ փորձում ենք վերափոխողներին ավելի ու ավելի ցածր քաշի վրա, մենք հայտնաբերում ենք, որ բարդ սեմանտիկ ինֆորմացիան առաջին հերթին դեգրոդացվում է: Ներքին ակտիվացիաների վերլուծությունը բացահայտում է, որ բարձր շերտերը ամենաբարձր տարբերակում են մաքրման ընթացքում, դառնալով ավելի քիչ բարդ, քան իրենց խտուն համեմատությունները: Մինչդեռ, փոքր մոդելների վաղ շերտերը սկսում են ավելի շատ կոդավորել: Ուշադրություն դարձնելու մեխանիզմները շարունակում են նշանակալի համապատասխան լինել, մինչ արագությունը աճում է:</abstract_hy>
      <abstract_bn>লোটারি টিকিট হিপাইথিসিসের সম্প্রতি কাজ বিলিউ রাখার সময় এনএমটির জন্য অনেক বেশী স্প্যারাস্ফার্মার তৈরি করেছে। তবে এটা পরিষ্কার নয় যে কিভাবে এই ধরনের বুদ্ধিমান প্রযুক্তিগুলো একটি মডেলের শিক্ষা প্রতিনিধিত্বের উপর প্ ট্রান্সফর্মারকে পরীক্ষা করার মাধ্যমে বেশী এবং কম মাত্রার ওজন ছিনিয়ে দেয়া হয়েছে, আমরা দেখতে পাচ্ছি যে জটিল সেম্যান্টিক তথ্য Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts.  এদিকে, স্প্যাস মডেলের প্রাথমিক ক্ষেত্রে আরো এনকোডিং করা শুরু করে। মনোযোগ প্রদান করা মেকিনসমূহ চমৎকার ব্যাপারে একত্রিত থাকে।</abstract_bn>
      <abstract_az>Qısa zamanda loteriya bileti hipotezi üzərində NMT üçün çox küçük Transformer yaratdı. Ancaq bu təklif metodların modelinin öyrəndiyi təsirlərinin necə etkisini bilmir. Transformers daha çox və daha düşük böyüklük ağırlığını təşviq edirək, kompleks semantik məlumatları ilk dəfə rüsvay edilməlidir. İçəri fəaliyyətlərin analizi göstərir ki, yüksək səviyyələr dəyişmək yolunda daha çox fəaliyyət edir, yavaş-yavaş onların yoxluq yoldaşlarından daha az kompleks olur. Bu sırada, əkin modellərin əvvəlki səviyyələri daha çox kodlamağa başlar. Dikkati mehānismi çoxluğunda çoxluğunda mövcuddur.</abstract_az>
      <abstract_bs>Nedavni rad na hipotezi karte loterije proizveo je visoko rezervne transformere NMT-a dok održava BLEU. Međutim, nije jasno kako takve tehnike pružanja utječu na naučene predstave model a. Probajući transformatore sa visokom i visokom težinom niskog veličine, otkrivamo da će prvo biti smanjena kompleksna semantička informacija. Analiza unutrašnjih aktivacija pokazuje da se viši slojevi najviše razlikuju tijekom pružanja, postupno postaju manje kompleksni od njihovih gustih kolega. U međuvremenu, rani sloji rezervnih modela počinju izvršiti više kodiranja. Mehanizmi pažnje ostaju nevjerojatno konsistentni dok se povećava sparsitet.</abstract_bs>
      <abstract_ca>Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU.  No obstant això, no és clar com aquestes tècniques de pruning afecten les representacions aprengutes d'un model. Investigant els Transformers amb més i més pesos de baixa magnitud, trobem que la informació semàntica complexa s'ha de degradar primer. L'anàlisi de l'activació interna revela que les capes més altes divergeixen més al llarg del pruning, tornant-se gradualment menys complexes que les seves denses contrapartides. Mentrestant, les primeres capes de models poc codificats comencen a fer més codificació. Els mecanismes d'atenció segueixen sorprenentment consistents a mesura que l'escassetat augmenta.</abstract_ca>
      <abstract_sq>Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU.  However, it is unclear how such pruning techniques affect a model's learned representations.  Duke vëzhguar Transformuesit me pesha më të vogla dhe më të vogla të rrënuara larg, ne gjejmë se informacioni kompleks semantik është i pari për të degraduar. Analiza e aktiviteteve të brendshme tregon se shtresa më të larta ndryshojnë më shumë gjatë rrjedhës së rrjedhjes, duke u bërë gradualisht më pak komplekse se homologët e tyre të dendur. Ndërkohë, nivelet e hershme të modeleve të vogla fillojnë të kryejnë më shumë kodim. Mekanizmat e vëmendjes mbeten jashtëzakonisht të konsistenta ndërsa pakësia rritet.</abstract_sq>
      <abstract_cs>Nedávná práce na hypotéze loterijních lístků vytvořila velmi řídké transformátory pro NMT při zachování BLEU. Není však jasné, jak tyto techniky prořezávání ovlivňují naučené reprezentace modelu. Zkoumáním transformátorů s více a více nízkými velikostmi odříznutými hmotnostmi zjišťujeme, že složité sémantické informace jsou nejprve degradovány. Analýza vnitřních aktivací ukazuje, že vyšší vrstvy se v průběhu prořezávání nejvíce liší a postupně se stávají méně složitými než jejich husté protějšky. Mezitím rané vrstvy řídkých modelů začínají provádět více kódování. Mechanismy pozornosti zůstávají pozoruhodně konzistentní s rostoucí řídkostí.</abstract_cs>
      <abstract_et>Hiljutine töö loteriipileti hüpoteesi on toonud NMT jaoks väga hõredaid transformaatoreid, säilitades samas BLEU. Siiski on ebaselge, kuidas sellised pügamismeetodid mõjutavad mudeli õppitud representatsioone. Uurides üha enam madala suurusega kaaluga transformaatoreid, leiame, et keeruline semantiline informatsioon tuleb esmalt halveneda. Sisemise aktiveerimise analüüs näitab, et kõrgemad kihid erinevad kõige enam pügamise käigus, muutudes järk-järgult vähem keeruliseks kui nende tihedad kolleegid. Samal ajal hakkavad hõredate mudelite varajased kihid rohkem kodeerima. Tähelepanu mehhanismid jäävad märkimisväärselt järjepidevaks, sest hõredus suureneb.</abstract_et>
      <abstract_fi>Viimeaikainen työ lottokuponin hypoteesista on tuottanut erittäin harvoja muuntajia NMT:lle säilyttäen BLEU:n. On kuitenkin epäselvää, miten tällaiset karsimistekniikat vaikuttavat mallin opittuihin representaatioihin. Tutkimalla muuntajia, joilla on yhä enemmän pienikokoisia painoja, huomaamme, että monimutkaista semanttista tietoa on ensin hajotettava. Sisäisten aktivaatioiden analyysi paljastaa, että korkeammat kerrokset eroavat eniten karsimisen aikana, vähitellen vähemmän monimutkaisia kuin niiden tiheät vastineet. Samaan aikaan harvojen mallien varhaiset kerrokset alkavat suorittaa enemmän koodausta. Huomiomekanismit ovat edelleen huomattavan johdonmukaisia, kun harvaluus kasvaa.</abstract_fi>
      <abstract_sk>Nedavno delo na hipotezi loterijskih vstopnic je ustvarilo zelo redke transformatorje za NMT, medtem ko je ohranilo BLEU. Vendar pa ni jasno, kako takšne tehnike obrezovanja vplivajo na znane predstavitve modela. S sondiranjem transformatorjev z vedno več težami nizke magnitude, ugotovimo, da je treba kompleksne semantične informacije najprej razgraditi. Analiza notranjih aktivacij razkriva, da se višje plasti med obrezovanjem najbolj razlikujejo in postopoma postajajo manj kompleksne od njihovih gostih kolegov. Medtem pa zgodnje plasti redkih modelov začnejo izvajati več kodiranja. Mehanizmi pozornosti ostajajo izjemno usklajeni, saj se redkost povečuje.</abstract_sk>
      <abstract_jv>Olo-luwih nggawe barang karo akeh kapan-kapan kuwi kapan kawit bagian sing nganggep bantuan Transformer kanggo NMT, sanes memperbudhakan CLUE Nanging, kuwi ora ngerti piye ngerti, teknik kuwi susahe nêmêr kuwi model model sing apik nyeanye Dijejer-jejer Ndelengke alam sing akeh tanggal dipunangke dipunangke alam luwih luwih-luwih jenis diolah punika dipunangke punika dipunangke dipunangke kapan politenessoffpolite"), and when there is a change ("assertivepoliteness Desaturan</abstract_jv>
      <abstract_he>העבודה האחרונה על ההיפתוזיה של כרטיסי הלוטו יצרה Transformers מאוד נדיר עבור NMT בזמן שמירה BLEU. בכל אופן, אינו ברור איך טכניקות טיפול כאלה משפיעות על מייצגים למדוגמנים. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded.  Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more encoding.  מנגנוני תשומת לב נשארים עקביים באופן יוצא דופן בזמן שהנדירות עולה.</abstract_he>
      <abstract_ha>Yin aikin nan da aka samu a kan mala'a mai Lottori ya sami mai girma Transformers wa NMT a lokacin da ke tsare BLEU. Amma, bã shi da gane yadda misãlai masu fahimta ke amfani da misãlai. Ga a jarraba Transformers da sikẽli masu ƙaranci ko ƙaranci, sai za mu gane cewa masu adadi na semantic ya zama kwanza a kunyatar. Anarari ga aikin aiki na guda ya bayyana cewa abubuwa masu sarrafa za'a gaura mafi yawa a tsakanin hankalin, kuma yana kasa sakan kammala musamman da tarakin nan bakin. A lokacin da za'a fara ƙananan masu motsi na tsumarni. Akwai matsayin saurãre yana daidai kamar an ƙara zafi.</abstract_ha>
      <abstract_bo>འཕྲལ་ཁམས་ཀྱི་སྐོར་གྱི་ལྕགས་གླེང་སྡུད་པར་བཟོ་བར་མཐུན་པ་ཁག་གིས་མཐུན་བཟོ་བཅོས་པ་དེ་ཡིན། ཡིན་ནའང་། དབྱིན་རྩལ་གྱི་ཐབས་ལམ་དེ་གིས་མི་ཤེས་པས་རྣམ་གྲངས་བསྡུར་བྱེད་ཀྱི་ཡོད། དབྱིབས་བཟོ་བ་དག་པ་ཞིག་གིས་མཐོ་དམའ་བའི་ཚད་ལྡན་བ་ཞིག་ནས དབྱེ་ཞིབ་ཀྱི་ནང་འཁོད་བྱ་འགུལ་གྱི་དཔྱད་ཞིག་ནི་བགོ་རིམ་མཐོ་ཚད་རྒྱ་ཆེ་མཐོ་ཁག་གི་ཡིག་རྟགས་ལ་ཕར་ཆེ་བ་བསྐྱེད་ཚད་འདྲ་བྱེད་ཀྱི་ ད་ནའང་ཡང་སྔོན་གྱི་བང་རིམ་པ་ཁང་གི་དབྱེ་རིས་མང་ཙམ་ཨང་ཀོ་གཏོང་འགོ་འཛུགས་བྱེད་ཀྱི་ཡོད། ཆེད་འཛིན་གྱི་ཐབས་ལམ་དེ་ཚོ་ཆེད་པོ་ཞིག་ཏུ་ཉར་ཆེན་པོ་ཞིག་ཏུ་ཉར་ཡོད།</abstract_bo>
      </paper>
    <paper id="21">
      <title>BERTs of a feather do not generalize together : Large variability in generalization across models with similar test set performance<fixed-case>BERT</fixed-case>s of a feather do not generalize together: Large variability in generalization across models with similar test set performance</title>
      <author><first>R. Thomas</first><last>McCoy</last></author>
      <author><first>Junghyun</first><last>Min</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <pages>217–227</pages>
      <abstract>If the same neural network architecture is trained multiple times on the same <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> ranging between 83.6 % and 84.8 %. In stark contrast, the same <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> varied widely in their <a href="https://en.wikipedia.org/wiki/Generalization">generalization</a> performance. For example, on the simple case of subject-object swap (e.g., determining that the doctor visited the lawyer does not entail the lawyer visited the doctor), <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> ranged from 0.0 % to 66.2 %. Such variation is likely due to the presence of many <a href="https://en.wikipedia.org/wiki/Maxima_and_minima">local minima</a> in the loss surface that are equally attractive to a low-bias learner such as a <a href="https://en.wikipedia.org/wiki/Neural_network">neural network</a> ; decreasing the variability may therefore require models with stronger <a href="https://en.wikipedia.org/wiki/Inductive_reasoning">inductive biases</a>.</abstract>
      <url hash="3e3b4902">2020.blackboxnlp-1.21</url>
      <doi>10.18653/v1/2020.blackboxnlp-1.21</doi>
      <video href="https://slideslive.com/38939766" />
      <bibkey>mccoy-etal-2020-berts</bibkey>
    </paper>
    <paper id="22">
      <title>Second-Order NLP Adversarial Examples<fixed-case>NLP</fixed-case> Adversarial Examples</title>
      <author><first>John</first><last>Morris</last></author>
      <pages>228–237</pages>
      <abstract>Adversarial example generation methods in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> rely on models like <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve, and associated metric ACCS, as tools for evaluating the <a href="https://en.wikipedia.org/wiki/Robustness_(computer_science)">robustness</a> of a <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraint</a> to second-order adversarial examples. To generate this <a href="https://en.wikipedia.org/wiki/Curve">curve</a>, we design an <a href="https://en.wikipedia.org/wiki/Adversarial_system">adversarial attack</a> to run directly on the semantic similarity models. We test on two <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraints</a>, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as <a href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraint</a> on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available here.</abstract>
      <url hash="2683e834">2020.blackboxnlp-1.22</url>
      <attachment type="OptionalSupplementaryMaterial" hash="30f2598d">2020.blackboxnlp-1.22.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.blackboxnlp-1.22</doi>
      <bibkey>morris-2020-second</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="25">
      <title>Investigating Novel Verb Learning in BERT : Selectional Preference Classes and Alternation-Based Syntactic Generalization<fixed-case>BERT</fixed-case>: Selectional Preference Classes and Alternation-Based Syntactic Generalization</title>
      <author><first>Tristan</first><last>Thrush</last></author>
      <author><first>Ethan</first><last>Wilcox</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>265–275</pages>
      <abstract>Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT’s few-shot learning capabilities for two aspects of English verbs : alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb / object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a>. For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias : verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively.</abstract>
      <url hash="82f892db">2020.blackboxnlp-1.25</url>
      <doi>10.18653/v1/2020.blackboxnlp-1.25</doi>
      <bibkey>thrush-etal-2020-investigating</bibkey>
      <pwccode url="https://github.com/TristanThrush/few-shot-lm-learning" additional="false">TristanThrush/few-shot-lm-learning</pwccode>
    </paper>
    <paper id="29">
      <title>Defining Explanation in an AI Context<fixed-case>AI</fixed-case> Context</title>
      <author><first>Tejaswani</first><last>Verma</last></author>
      <author><first>Christoph</first><last>Lingenfelder</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>314–322</pages>
      <abstract>With the increase in the use of <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI systems</a>, a need for explanation systems arises. Building an <a href="https://en.wikipedia.org/wiki/Explanation">explanation system</a> requires a definition of <a href="https://en.wikipedia.org/wiki/Explanation">explanation</a>. However, the natural language term explanation is difficult to define formally as it includes multiple perspectives from different domains such as <a href="https://en.wikipedia.org/wiki/Psychology">psychology</a>, <a href="https://en.wikipedia.org/wiki/Philosophy">philosophy</a>, and <a href="https://en.wikipedia.org/wiki/Cognitive_science">cognitive sciences</a>. We study multiple perspectives and aspects of explainability of recommendations or predictions made by <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI systems</a>, and provide a generic definition of <a href="https://en.wikipedia.org/wiki/Explanation">explanation</a>. The proposed <a href="https://en.wikipedia.org/wiki/Definition">definition</a> is ambitious and challenging to apply. With the intention to bridge the gap between theory and application, we also propose a possible architecture of an automated explanation system based on our definition of explanation.</abstract>
      <url hash="a3224d28">2020.blackboxnlp-1.29</url>
      <doi>10.18653/v1/2020.blackboxnlp-1.29</doi>
      <bibkey>verma-etal-2020-defining</bibkey>
    </paper>
    </volume>
</collection>