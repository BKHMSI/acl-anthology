<collection id="2020.blackboxnlp">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</booktitle>
      <editor><first>Afra</first><last>Alishahi</last></editor>
      <editor><first>Yonatan</first><last>Belinkov</last></editor>
      <editor><first>Grzegorz</first><last>Chrupa&#322;a</last></editor>
      <editor><first>Dieuwke</first><last>Hupkes</last></editor>
      <editor><first>Yuval</first><last>Pinter</last></editor>
      <editor><first>Hassan</first><last>Sajjad</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="de8af8bb">2020.blackboxnlp-1.0</url>
      <bibkey>blackboxnlp-2020-blackboxnlp</bibkey>
    </frontmatter>
    <paper id="6">
      <title>Leveraging Extracted Model Adversaries for Improved Black Box Attacks</title>
      <author><first>Naveen Jafer</first><last>Nizar</last></author>
      <author><first>Ari</first><last>Kobren</last></author>
      <pages>57&#8211;67</pages>
      <abstract>We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our approach is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail. These perturbed inputs are used against the victim. In experiments we find that our method improves on the efficacy of the ADDANY&#8212;a white box attack&#8212;performed on the approximate model by 25% F1, and the ADDSENT attack&#8212;a black box attack&#8212;by 11% F1.</abstract>
      <url hash="ac888724">2020.blackboxnlp-1.6</url>
      <doi>10.18653/v1/2020.blackboxnlp-1.6</doi>
      <bibkey>nizar-kobren-2020-leveraging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="14">
      <title>The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?</title>
      <author><first>Jasmijn</first><last>Bastings</last></author>
      <author><first>Katja</first><last>Filippova</last></author>
      <pages>149&#8211;155</pages>
      <abstract>There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.</abstract>
      <url hash="852122f3">2020.blackboxnlp-1.14</url>
      <doi>10.18653/v1/2020.blackboxnlp-1.14</doi>
      <video href="https://slideslive.com/38939764" />
      <bibkey>bastings-filippova-2020-elephant</bibkey>
    </paper>
    <paper id="16">
      <title>Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation</title>
      <author><first>Atticus</first><last>Geiger</last></author>
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <pages>163&#8211;173</pages>
      <abstract>We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods: the behavioral evaluation methods of (1) challenge test sets and (2) systematic generalization tasks, and the structural evaluation methods of (3) probes and (4) interventions. To facilitate this holistic evaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset focused on lexical entailment and negation. In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing negation, but that MoNLI fine-tuning addresses this failure. In our structural evaluations, we look for evidence that our top-performing BERT-based model has learned to implement the monotonicity algorithm behind MoNLI. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the causal dynamics of the model mirror the causal dynamics of this algorithm on subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.</abstract>
      <url hash="6f23d1c1">2020.blackboxnlp-1.16</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e8cf6ccc">2020.blackboxnlp-1.16.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.blackboxnlp-1.16</doi>
      <bibkey>geiger-etal-2020-neural</bibkey>
      <pwccode url="https://github.com/atticusg/MoNLI" additional="false">atticusg/MoNLI</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/help">HELP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="19">
      <title>Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation</title>
      <author><first>Rajiv</first><last>Movva</last></author>
      <author><first>Jason</first><last>Zhao</last></author>
      <pages>193&#8211;203</pages>
      <abstract>Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. However, it is unclear how such pruning techniques affect a model&#8217;s learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more encoding. Attention mechanisms remain remarkably consistent as sparsity increases.</abstract>
      <url hash="34fcea29">2020.blackboxnlp-1.19</url>
      <attachment type="OptionalSupplementaryMaterial" hash="d5a8b85c">2020.blackboxnlp-1.19.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.blackboxnlp-1.19</doi>
      <video href="https://slideslive.com/38939765" />
      <bibkey>movva-zhao-2020-dissecting</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>BERT</fixed-case>s of a feather do not generalize together: Large variability in generalization across models with similar test set performance</title>
      <author><first>R. Thomas</first><last>McCoy</last></author>
      <author><first>Junghyun</first><last>Min</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <pages>217&#8211;227</pages>
      <abstract>If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap (e.g., determining that &#8220;the doctor visited the lawyer&#8221; does not entail &#8220;the lawyer visited the doctor&#8221;), accuracy ranged from 0.0% to 66.2%. Such variation is likely due to the presence of many local minima in the loss surface that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases.</abstract>
      <url hash="3e3b4902">2020.blackboxnlp-1.21</url>
      <doi>10.18653/v1/2020.blackboxnlp-1.21</doi>
      <video href="https://slideslive.com/38939766" />
      <bibkey>mccoy-etal-2020-berts</bibkey>
    </paper>
    <paper id="22">
      <title>Second-Order <fixed-case>NLP</fixed-case> Adversarial Examples</title>
      <author><first>John</first><last>Morris</last></author>
      <pages>228&#8211;237</pages>
      <abstract>Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve, and associated metric ACCS, as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available here.</abstract>
      <url hash="2683e834">2020.blackboxnlp-1.22</url>
      <attachment type="OptionalSupplementaryMaterial" hash="30f2598d">2020.blackboxnlp-1.22.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.blackboxnlp-1.22</doi>
      <bibkey>morris-2020-second</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="25">
      <title>Investigating Novel Verb Learning in <fixed-case>BERT</fixed-case>: Selectional Preference Classes and Alternation-Based Syntactic Generalization</title>
      <author><first>Tristan</first><last>Thrush</last></author>
      <author><first>Ethan</first><last>Wilcox</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>265&#8211;275</pages>
      <abstract>Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT&#8217;s few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the model expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb/object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in fine-tuning. For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias: verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively.</abstract>
      <url hash="82f892db">2020.blackboxnlp-1.25</url>
      <doi>10.18653/v1/2020.blackboxnlp-1.25</doi>
      <bibkey>thrush-etal-2020-investigating</bibkey>
      <pwccode url="https://github.com/TristanThrush/few-shot-lm-learning" additional="false">TristanThrush/few-shot-lm-learning</pwccode>
    </paper>
    <paper id="29">
      <title>Defining Explanation in an <fixed-case>AI</fixed-case> Context</title>
      <author><first>Tejaswani</first><last>Verma</last></author>
      <author><first>Christoph</first><last>Lingenfelder</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>314&#8211;322</pages>
      <abstract>With the increase in the use of AI systems, a need for explanation systems arises. Building an explanation system requires a definition of explanation. However, the natural language term explanation is difficult to define formally as it includes multiple perspectives from different domains such as psychology, philosophy, and cognitive sciences. We study multiple perspectives and aspects of explainability of recommendations or predictions made by AI systems, and provide a generic definition of explanation. The proposed definition is ambitious and challenging to apply. With the intention to bridge the gap between theory and application, we also propose a possible architecture of an automated explanation system based on our definition of explanation.</abstract>
      <url hash="a3224d28">2020.blackboxnlp-1.29</url>
      <doi>10.18653/v1/2020.blackboxnlp-1.29</doi>
      <bibkey>verma-etal-2020-defining</bibkey>
    </paper>
    </volume>
</collection>