<collection id="K19">
  <volume id="1" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</booktitle>
      <url hash="a113dfa3">K19-1</url>
      <editor><first>Mohit</first><last>Bansal</last></editor>
      <editor><first>Aline</first><last>Villavicencio</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong, China</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="01341017">K19-1000</url>
      <bibkey>conll-2019-natural</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Investigating Cross-Lingual Alignment Methods for Contextualized Embeddings with Token-Level Evaluation</title>
      <author><first>Qianchu</first><last>Liu</last></author>
      <author><first>Diana</first><last>McCarthy</last></author>
      <author><first>Ivan</first><last>Vuli&#263;</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>33&#8211;43</pages>
      <abstract>In this paper, we present a thorough investigation on methods that align pre-trained contextualized embeddings into shared cross-lingual context-aware embedding space, providing strong reference benchmarks for future context-aware crosslingual models. We propose a novel and challenging task, Bilingual Token-level Sense Retrieval (BTSR). It specifically evaluates the accurate alignment of words with the same meaning in cross-lingual non-parallel contexts, currently not evaluated by existing tasks such as Bilingual Contextual Word Similarity and Sentence Retrieval. We show how the proposed BTSR task highlights the merits of different alignment methods. In particular, we find that using context average type-level alignment is effective in transferring monolingual contextualized embeddings cross-lingually especially in non-parallel contexts, and at the same time improves the monolingual space. Furthermore, aligning independently trained models yields better performance than aligning multilingual embeddings with shared vocabulary.</abstract>
      <url hash="7424b4ba">K19-1004</url>
      <doi>10.18653/v1/K19-1004</doi>
      <bibkey>liu-etal-2019-investigating</bibkey>
    </paper>
    <paper id="7">
      <title>Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models</title>
      <author><first>Grusha</first><last>Prasad</last></author>
      <author><first>Marten</first><last>van Schijndel</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <pages>66&#8211;76</pages>
      <abstract>Neural language models (LMs) perform well on tasks that require sensitivity to syntactic structure. Drawing on the syntactic priming paradigm from psycholinguistics, we propose a novel technique to analyze the representations that enable such success. By establishing a gradient similarity metric between structures, this technique allows us to reconstruct the organization of the LMs&#8217; syntactic representational space. We use this technique to demonstrate that LSTM LMs&#8217; representations of different types of sentences with relative clauses are organized hierarchically in a linguistically interpretable manner, suggesting that the LMs track abstract properties of the sentence.</abstract>
      <url hash="78a9008d">K19-1007</url>
      <doi>10.18653/v1/K19-1007</doi>
      <bibkey>prasad-etal-2019-using</bibkey>
      <pwccode url="https://github.com/grushaprasad/RNN-Priming" additional="false">grushaprasad/RNN-Priming</pwccode>
    </paper>
    <paper id="9">
      <title>Compositional Generalization in Image Captioning</title>
      <author><first>Mitja</first><last>Nikolaus</last></author>
      <author><first>Mostafa</first><last>Abdou</last></author>
      <author><first>Matthew</first><last>Lamm</last></author>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <pages>87&#8211;98</pages>
      <abstract>Image captioning models are usually evaluated on their ability to describe a held-out set of images, not on their ability to generalize to unseen concepts. We study the problem of compositional generalization, which measures how well a model composes unseen combinations of concepts when describing images. State-of-the-art image captioning models show poor generalization performance on this task. We propose a multi-task model to address the poor performance, that combines caption generation and image&#8211;sentence ranking, and uses a decoding mechanism that re-ranks the captions according their similarity to the image. This model is substantially better at generalizing to unseen combinations of concepts compared to state-of-the-art captioning models.</abstract>
      <url hash="5307b8c4">K19-1009</url>
      <attachment type="supplementary-material" hash="bd5c7969">K19-1009.Supplementary_Material.pdf</attachment>
      <doi>10.18653/v1/K19-1009</doi>
      <bibkey>nikolaus-etal-2019-compositional</bibkey>
      <pwccode url="https://github.com/mitjanikolaus/compositional-image-captioning" additional="false">mitjanikolaus/compositional-image-captioning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="10">
      <title>Representing Movie Characters in Dialogues</title>
      <author><first>Mahmoud</first><last>Azab</last></author>
      <author><first>Noriyuki</first><last>Kojima</last></author>
      <author><first>Jia</first><last>Deng</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>99&#8211;109</pages>
      <abstract>We introduce a new embedding model to represent movie characters and their interactions in a dialogue by encoding in the same representation the language used by these characters as well as information about the other participants in the dialogue. We evaluate the performance of these new character embeddings on two tasks: (1) character relatedness, using a dataset we introduce consisting of a dense character interaction matrix for 4,378 unique character pairs over 22 hours of dialogue from eighteen movies; and (2) character relation classification, for fine- and coarse-grained relations, as well as sentiment relations. Our experiments show that our model significantly outperforms the traditional Word2Vec continuous bag-of-words and skip-gram models, demonstrating the effectiveness of the character embeddings we introduce. We further show how these embeddings can be used in conjunction with a visual question answering system to improve over previous results.</abstract>
      <url hash="5c915b4b">K19-1010</url>
      <doi>10.18653/v1/K19-1010</doi>
      <bibkey>azab-etal-2019-representing</bibkey>
    </paper>
    <paper id="14">
      <title>Weird Inflects but <fixed-case>OK</fixed-case>: Making Sense of Morphological Generation Errors</title>
      <author><first>Kyle</first><last>Gorman</last></author>
      <author><first>Arya D.</first><last>McCarthy</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Ekaterina</first><last>Vylomova</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <author><first>Magdalena</first><last>Markowska</last></author>
      <pages>140&#8211;151</pages>
      <abstract>We conduct a manual error analysis of the CoNLL-SIGMORPHON Shared Task on Morphological Reinflection. This task involves natural language generation: systems are given a word in citation form (e.g., hug) and asked to produce the corresponding inflected form (e.g., the simple past hugged). We propose an error taxonomy and use it to annotate errors made by the top two systems across twelve languages. Many of the observed errors are related to inflectional patterns sensitive to inherent linguistic properties such as animacy or affect; many others are failures to predict truly unpredictable inflectional behaviors. We also find nearly one quarter of the residual &#8220;errors&#8221; reflect errors in the gold data.</abstract>
      <url hash="04d41e8d">K19-1014</url>
      <doi>10.18653/v1/K19-1014</doi>
      <bibkey>gorman-etal-2019-weird</bibkey>
    </paper>
    <paper id="15">
      <title>Learning to Represent Bilingual Dictionaries</title>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Yingtao</first><last>Tian</last></author>
      <author><first>Haochen</first><last>Chen</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Steven</first><last>Skiena</last></author>
      <author><first>Carlo</first><last>Zaniolo</last></author>
      <pages>152&#8211;162</pages>
      <abstract>Bilingual word embeddings have been widely used to capture the correspondence of lexical semantics in different human languages. However, the cross-lingual correspondence between sentences and words is less studied, despite that this correspondence can significantly benefit many applications such as crosslingual semantic search and textual inference. To bridge this gap, we propose a neural embedding model that leverages bilingual dictionaries. The proposed model is trained to map the lexical definitions to the cross-lingual target words, for which we explore with different sentence encoding techniques. To enhance the learning process on limited resources, our model adopts several critical learning strategies, including multi-task learning on different bridges of languages, and joint learning of the dictionary model with a bilingual word embedding model. We conduct experiments on two new tasks. In the cross-lingual reverse dictionary retrieval task, we demonstrate that our model is capable of comprehending bilingual concepts based on descriptions, and the proposed learning strategies are effective. In the bilingual paraphrase identification task, we show that our model effectively associates sentences in different languages via a shared embedding space, and outperforms existing approaches in identifying bilingual paraphrases.</abstract>
      <url hash="223c184b">K19-1015</url>
      <doi>10.18653/v1/K19-1015</doi>
      <bibkey>chen-etal-2019-learning</bibkey>
      <pwccode url="https://github.com/muhaochen/bilingual_dictionaries" additional="false">muhaochen/bilingual_dictionaries</pwccode>
    </paper>
    <paper id="16">
      <title>Improving Natural Language Understanding by Reverse Mapping Bytepair Encoding</title>
      <author><first>Chaodong</first><last>Tong</last></author>
      <author><first>Huailiang</first><last>Peng</last></author>
      <author><first>Qiong</first><last>Dai</last></author>
      <author><first>Lei</first><last>Jiang</last></author>
      <author><first>Jianghua</first><last>Huang</last></author>
      <pages>163&#8211;173</pages>
      <abstract>We propose a method called reverse mapping bytepair encoding, which maps named-entity information and other word-level linguistic features back to subwords during the encoding procedure of bytepair encoding (BPE). We employ this method to the Generative Pre-trained Transformer (OpenAI GPT) by adding a weighted linear layer after the embedding layer. We also propose a new model architecture named as the multi-channel separate transformer to employ a training process without parameter-sharing. Evaluation on Stories Cloze, RTE, SciTail and SST-2 datasets demonstrates the effectiveness of our approach.</abstract>
      <url hash="5393164f">K19-1016</url>
      <doi>10.18653/v1/K19-1016</doi>
      <bibkey>tong-etal-2019-improving</bibkey>
    </paper>
    <paper id="17">
      <title>Made for Each Other: Broad-Coverage Semantic Structures Meet Preposition Supersenses</title>
      <author><first>Jakob</first><last>Prange</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>174&#8211;185</pages>
      <abstract>Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) is a typologically-informed, broad-coverage semantic annotation scheme that describes coarse-grained predicate-argument structure but currently lacks semantic roles. We argue that lexicon-free annotation of the semantic roles marked by prepositions, as formulated by Schneider et al. (2018), is complementary and suitable for integration within UCCA. We show empirically for English that the schemes, though annotated independently, are compatible and can be combined in a single semantic graph. A comparison of several approaches to parsing the integrated representation lays the groundwork for future research on this task.</abstract>
      <url hash="93d2378c">K19-1017</url>
      <attachment type="supplementary-material" hash="c2f9ca34">K19-1017.Supplementary_Material.pdf</attachment>
      <attachment hash="7c1c8c1e">K19-1017.Attachment.pdf</attachment>
      <doi>10.18653/v1/K19-1017</doi>
      <bibkey>prange-etal-2019-made</bibkey>
      <pwccode url="https://github.com/jakpra/ucca-streusle" additional="false">jakpra/ucca-streusle</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="18">
      <title>Generating Timelines by Modeling Semantic Change</title>
      <author><first>Guy D.</first><last>Rosin</last></author>
      <author><first>Kira</first><last>Radinsky</last></author>
      <pages>186&#8211;195</pages>
      <abstract>Though languages can evolve slowly, they can also react strongly to dramatic world events. By studying the connection between words and events, it is possible to identify which events change our vocabulary and in what way. In this work, we tackle the task of creating timelines - records of historical &#8220;turning points&#8221;, represented by either words or events, to understand the dynamics of a target word. Our approach identifies these points by leveraging both static and time-varying word embeddings to measure the influence of words and events. In addition to quantifying changes, we show how our technique can help isolate semantic changes. Our qualitative and quantitative evaluations show that we are able to capture this semantic change and event influence.</abstract>
      <url hash="d01b3ef0">K19-1018</url>
      <attachment type="supplementary-material" hash="b210ac83">K19-1018.Supplementary_Material.zip</attachment>
      <doi>10.18653/v1/K19-1018</doi>
      <bibkey>rosin-radinsky-2019-generating</bibkey>
      <pwccode url="https://github.com/guyrosin/generating_timelines" additional="false">guyrosin/generating_timelines</pwccode>
    </paper>
    <paper id="19">
      <title>Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets</title>
      <author><first>Ohad</first><last>Rozen</last></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <author><first>Roee</first><last>Aharoni</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>196&#8211;205</pages>
      <abstract>Phenomenon-specific &#8220;adversarial&#8221; datasets have been recently designed to perform targeted stress-tests for particular inference types. Recent work (Liu et al., 2019a) proposed that such datasets can be utilized for training NLI and other types of models, often allowing to learn the phenomenon in focus and improve on the challenge dataset, indicating a &#8220;blind spot&#8221; in the original training data. Yet, although a model can improve in such a training process, it might still be vulnerable to other challenge datasets targeting the same phenomenon but drawn from a different distribution, such as having a different syntactic complexity level. In this work, we extend this method to drive conclusions about a model&#8217;s ability to learn and generalize a target phenomenon rather than to &#8220;learn&#8221; a dataset, by controlling additional aspects in the adversarial datasets. We demonstrate our approach on two inference phenomena &#8211; dative alternation and numerical reasoning, elaborating, and in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements.</abstract>
      <url hash="be6e30b7">K19-1019</url>
      <doi>10.18653/v1/K19-1019</doi>
      <bibkey>rozen-etal-2019-diversify</bibkey>
      <pwccode url="https://github.com/ohadrozen/generalization" additional="false">ohadrozen/generalization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="20">
      <title>Fully Unsupervised Crosslingual Semantic Textual Similarity Metric Based on <fixed-case>BERT</fixed-case> for Identifying Parallel Data</title>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <author><first>Michel</first><last>Simard</last></author>
      <pages>206&#8211;215</pages>
      <abstract>We present a fully unsupervised crosslingual semantic textual similarity (STS) metric, based on contextual embeddings extracted from BERT &#8211; Bidirectional Encoder Representations from Transformers (Devlin et al., 2019). The goal of crosslingual STS is to measure to what degree two segments of text in different languages express the same meaning. Not only is it a key task in crosslingual natural language understanding (XLU), it is also particularly useful for identifying parallel resources for training and evaluating downstream multilingual natural language processing (NLP) applications, such as machine translation. Most previous crosslingual STS methods relied heavily on existing parallel resources, thus leading to a circular dependency problem. With the advent of massively multilingual context representation models such as BERT, which are trained on the concatenation of non-parallel data from each language, we show that the deadlock around parallel resources can be broken. We perform intrinsic evaluations on crosslingual STS data sets and extrinsic evaluations on parallel corpus filtering and human translation equivalence assessment tasks. Our results show that the unsupervised crosslingual STS metric using BERT without fine-tuning achieves performance on par with supervised or weakly supervised approaches.</abstract>
      <url hash="d8604adc">K19-1020</url>
      <doi>10.18653/v1/K19-1020</doi>
      <bibkey>lo-simard-2019-fully</bibkey>
    </paper>
    <paper id="21">
      <title>On the Importance of Subword Information for Morphological Tasks in Truly Low-Resource Languages</title>
      <author><first>Yi</first><last>Zhu</last></author>
      <author><first>Benjamin</first><last>Heinzerling</last></author>
      <author><first>Ivan</first><last>Vuli&#263;</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>216&#8211;226</pages>
      <abstract>Recent work has validated the importance of subword information for word representation learning. Since subwords increase parameter sharing ability in neural models, their value should be even more pronounced in low-data regimes. In this work, we therefore provide a comprehensive analysis focused on the usefulness of subwords for word representation learning in truly low-resource scenarios and for three representative morphological tasks: fine-grained entity typing, morphological tagging, and named entity recognition. We conduct a systematic study that spans several dimensions of comparison: 1) type of data scarcity which can stem from the lack of task-specific training data, or even from the lack of unannotated data required to train word embeddings, or both; 2) language type by working with a sample of 16 typologically diverse languages including some truly low-resource ones (e.g. Rusyn, Buryat, and Zulu); 3) the choice of the subword-informed word representation method. Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and task-based models, where having sufficient in-task data is a more critical requirement.</abstract>
      <url hash="f2ca69bf">K19-1021</url>
      <doi>10.18653/v1/K19-1021</doi>
      <bibkey>zhu-etal-2019-importance</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="22">
      <title>Comparing Top-Down and Bottom-Up Neural Generative Dependency Models</title>
      <author><first>Austin</first><last>Matthews</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <pages>227&#8211;237</pages>
      <abstract>Recurrent neural network grammars generate sentences using phrase-structure syntax and perform very well on both parsing and language modeling. To explore whether generative dependency models are similarly effective, we propose two new generative models of dependency syntax. Both models use recurrent neural nets to avoid making explicit independence assumptions, but they differ in the order used to construct the trees: one builds the tree bottom-up and the other top-down, which profoundly changes the estimation problem faced by the learner. We evaluate the two models on three typologically different languages: English, Arabic, and Japanese. While both generative models improve parsing performance over a discriminative baseline, they are significantly less effective than non-syntactic LSTM language models. Surprisingly, little difference between the construction orders is observed for either parsing or language modeling.</abstract>
      <url hash="88970331">K19-1022</url>
      <doi>10.18653/v1/K19-1022</doi>
      <bibkey>matthews-etal-2019-comparing</bibkey>
    </paper>
    <paper id="23">
      <title>Representation Learning and Dynamic Programming for Arc-Hybrid Parsing</title>
      <author><first>Joseph</first><last>Le Roux</last></author>
      <author><first>Antoine</first><last>Rozenknop</last></author>
      <author><first>Mathieu</first><last>Lacroix</last></author>
      <pages>238&#8211;248</pages>
      <abstract>We present a new method for transition-based parsing where a solution is a pair made of a dependency tree and a derivation graph describing the construction of the former. From this representation we are able to derive an efficient parsing algorithm and design a neural network that learns vertex representations and arc scores. Experimentally, although we only train via local classifiers, our approach improves over previous arc-hybrid systems and reach state-of-the-art parsing accuracy.</abstract>
      <url hash="343c6884">K19-1023</url>
      <doi>10.18653/v1/K19-1023</doi>
      <bibkey>le-roux-etal-2019-representation</bibkey>
    </paper>
    <paper id="25">
      <title>Improving Neural Machine Translation by Achieving Knowledge Transfer with Sentence Alignment Learning</title>
      <author><first>Xuewen</first><last>Shi</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Wenguan</first><last>Wang</last></author>
      <author><first>Ping</first><last>Jian</last></author>
      <author><first>Yi-Kun</first><last>Tang</last></author>
      <pages>260&#8211;270</pages>
      <abstract>Neural Machine Translation (NMT) optimized by Maximum Likelihood Estimation (MLE) lacks the guarantee of translation adequacy. To alleviate this problem, we propose an NMT approach that heightens the adequacy in machine translation by transferring the semantic knowledge learned from bilingual sentence alignment. Specifically, we first design a discriminator that learns to estimate sentence aligning score over translation candidates, and then the learned semantic knowledge is transfered to the NMT model under an adversarial learning framework. We also propose a gated self-attention based encoder for sentence embedding. Furthermore, an N-pair training loss is introduced in our framework to aid the discriminator in better capturing lexical evidence in translation candidates. Experimental results show that our proposed method outperforms baseline NMT models on Chinese-to-English and English-to-German translation tasks. Further analysis also indicates the detailed semantic knowledge transfered from the discriminator to the NMT model.</abstract>
      <url hash="fd4a5cbf">K19-1025</url>
      <doi>10.18653/v1/K19-1025</doi>
      <bibkey>shi-etal-2019-improving</bibkey>
    </paper>
    <paper id="26">
      <title>Code-Switched Language Models Using Neural Based Synthetic Data from Parallel Sentences</title>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>271&#8211;280</pages>
      <abstract>Training code-switched language models is difficult due to lack of data and complexity in the grammatical structure. Linguistic constraint theories have been used for decades to generate artificial code-switching sentences to cope with this issue. However, this require external word alignments or constituency parsers that create erroneous results on distant languages. We propose a sequence-to-sequence model using a copy mechanism to generate code-switching data by leveraging parallel monolingual translations from a limited source of code-switching data. The model learns how to combine words from parallel sentences and identifies when to switch one language to the other. Moreover, it captures code-switching constraints by attending and aligning the words in inputs, without requiring any external knowledge. Based on experimental results, the language model trained with the generated sentences achieves state-of-the-art performance and improves end-to-end automatic speech recognition.</abstract>
      <url hash="66c08e25">K19-1026</url>
      <attachment type="supplementary-material" hash="409c588a">K19-1026.Supplementary_Material.pdf</attachment>
      <doi>10.18653/v1/K19-1026</doi>
      <bibkey>winata-etal-2019-code</bibkey>
    </paper>
    <paper id="27">
      <title>Unsupervised Neural Machine Translation with Future Rewarding</title>
      <author><first>Xiangpeng</first><last>Wei</last></author>
      <author><first>Yue</first><last>Hu</last></author>
      <author><first>Luxi</first><last>Xing</last></author>
      <author><first>Li</first><last>Gao</last></author>
      <pages>281&#8211;290</pages>
      <abstract>In this paper, we alleviate the local optimality of back-translation by learning a policy (takes the form of an encoder-decoder and is defined by its parameters) with future rewarding under the reinforcement learning framework, which aims to optimize the global word predictions for unsupervised neural machine translation. To this end, we design a novel reward function to characterize high-quality translations from two aspects: n-gram matching and semantic adequacy. The n-gram matching is defined as an alternative for the discrete BLEU metric, and the semantic adequacy is used to measure the adequacy of conveying the meaning of the source sentence to the target. During training, our model strives for earning higher rewards by learning to produce grammatically more accurate and semantically more adequate translations. Besides, a variational inference network (VIN) is proposed to constrain the corresponding sentences in two languages have the same or similar latent semantic code. On the widely used WMT&#8217;14 English-French, WMT&#8217;16 English-German and NIST Chinese-to-English benchmarks, our models respectively obtain 27.59/27.15, 19.65/23.42 and 22.40 BLEU points without using any labeled data, demonstrating consistent improvements over previous unsupervised NMT models.</abstract>
      <url hash="8444e843">K19-1027</url>
      <doi>10.18653/v1/K19-1027</doi>
      <bibkey>wei-etal-2019-unsupervised</bibkey>
    </paper>
    <paper id="28">
      <title>Automatically Extracting Challenge Sets for Non-Local Phenomena in Neural Machine Translation</title>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>291&#8211;303</pages>
      <abstract>We show that the state-of-the-art Transformer MT model is not biased towards monotonic reordering (unlike previous recurrent neural network models), but that nevertheless, long-distance dependencies remain a challenge for the model. Since most dependencies are short-distance, common evaluation metrics will be little influenced by how well systems perform on them. We therefore propose an automatic approach for extracting challenge sets rich with long-distance dependencies, and argue that evaluation using this methodology provides a complementary perspective on system performance. To support our claim, we compile challenge sets for English-German and German-English, which are much larger than any previously released challenge set for MT. The extracted sets are large enough to allow reliable automatic evaluation, which makes the proposed approach a scalable and practical solution for evaluating MT performance on the long-tail of syntactic phenomena.</abstract>
      <url hash="2410db87">K19-1028</url>
      <attachment type="supplementary-material" hash="9ef72412">K19-1028.Supplementary_Material.pdf</attachment>
      <doi>10.18653/v1/K19-1028</doi>
      <bibkey>choshen-abend-2019-automatically</bibkey>
    </paper>
    <paper id="30">
      <title>Improving Pre-Trained Multilingual Model with Vocabulary Expansion</title>
      <author><first>Hai</first><last>Wang</last></author>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Jianshu</first><last>Chen</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>316&#8211;327</pages>
      <abstract>Recently, pre-trained language models have achieved remarkable success in a broad range of natural language processing tasks. However, in multilingual setting, it is extremely resource-consuming to pre-train a deep language model over large-scale corpora for each language. Instead of exhaustively pre-training monolingual language models independently, an alternative solution is to pre-train a powerful multilingual deep language model over large-scale corpora in hundreds of languages. However, the vocabulary size for each language in such a model is relatively small, especially for low-resource languages. This limitation inevitably hinders the performance of these multilingual models on tasks such as sequence labeling, wherein in-depth token-level or sentence-level understanding is essential. In this paper, inspired by previous methods designed for monolingual settings, we investigate two approaches (i.e., joint mapping and mixture mapping) based on a pre-trained multilingual model BERT for addressing the out-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech tagging, named entity recognition, machine translation quality estimation, and machine reading comprehension. Experimental results show that using mixture mapping is more promising. To the best of our knowledge, this is the first work that attempts to address and discuss the OOV issue in multilingual settings.</abstract>
      <url hash="651a99e1">K19-1030</url>
      <doi>10.18653/v1/K19-1030</doi>
      <bibkey>wang-etal-2019-improving</bibkey>
    </paper>
    <paper id="31">
      <title>On the Relation between Position Information and Sentence Length in Neural Machine Translation</title>
      <author><first>Masato</first><last>Neishi</last></author>
      <author><first>Naoki</first><last>Yoshinaga</last></author>
      <pages>328&#8211;338</pages>
      <abstract>Long sentences have been one of the major challenges in neural machine translation (NMT). Although some approaches such as the attention mechanism have partially remedied the problem, we found that the current standard NMT model, Transformer, has difficulty in translating long sentences compared to the former standard, Recurrent Neural Network (RNN)-based model. One of the key differences of these NMT models is how the model handles position information which is essential to process sequential data. In this study, we focus on the position information type of NMT models, and hypothesize that relative position is better than absolute position. To examine the hypothesis, we propose RNN-Transformer which replaces positional encoding layer of Transformer by RNN, and then compare RNN-based model and four variants of Transformer. Experiments on ASPEC English-to-Japanese and WMT2014 English-to-German translation tasks demonstrate that relative position helps translating sentences longer than those in the training data. Further experiments on length-controlled training data reveal that absolute position actually causes overfitting to the sentence length.</abstract>
      <url hash="3899df70">K19-1031</url>
      <doi>10.18653/v1/K19-1031</doi>
      <bibkey>neishi-yoshinaga-2019-relation</bibkey>
    </paper>
    <paper id="32">
      <title>Word Recognition, Competition, and Activation in a Model of Visually Grounded Speech</title>
      <author><first>William N.</first><last>Havard</last></author>
      <author><first>Jean-Pierre</first><last>Chevrot</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>339&#8211;348</pages>
      <abstract>In this paper, we study how word-like units are represented and activated in a recurrent neural model of visually grounded speech. The model used in our experiments is trained to project an image and its spoken description in a common representation space. We show that a recurrent model trained on spoken sentences implicitly segments its input into word-like units and reliably maps them to their correct visual referents. We introduce a methodology originating from linguistics to analyse the representation learned by neural networks &#8211; the gating paradigm &#8211; and show that the correct representation of a word is only activated if the network has access to first phoneme of the target word, suggesting that the network does not rely on a global acoustic pattern. Furthermore, we find out that not all speech frames (MFCC vectors in our case) play an equal role in the final encoded representation of a given word, but that some frames have a crucial effect on it. Finally we suggest that word representation could be activated through a process of lexical competition.</abstract>
      <url hash="125a4792">K19-1032</url>
      <attachment type="supplementary-material" hash="4cbd29ef">K19-1032.Supplementary_Material.pdf</attachment>
      <doi>10.18653/v1/K19-1032</doi>
      <bibkey>havard-etal-2019-word</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="34">
      <title>Linguistic Analysis Improves Neural Metaphor Detection</title>
      <author><first>Kevin</first><last>Stowe</last></author>
      <author><first>Sarah</first><last>Moeller</last></author>
      <author><first>Laura</first><last>Michaelis</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <pages>362&#8211;371</pages>
      <abstract>In the field of metaphor detection, deep learning systems are the ubiquitous and achieve strong performance on many tasks. However, due to the complicated procedures for manually identifying metaphors, the datasets available are relatively small and fraught with complications. We show that using syntactic features and lexical resources can automatically provide additional high-quality training data for metaphoric language, and this data can cover gaps and inconsistencies in metaphor annotation, improving state-of-the-art word-level metaphor identification. This novel application of automatically improving training data improves classification across numerous tasks, and reconfirms the necessity of high-quality data for deep learning frameworks.</abstract>
      <url hash="cfca20ce">K19-1034</url>
      <doi>10.18653/v1/K19-1034</doi>
      <bibkey>stowe-etal-2019-linguistic</bibkey>
    </paper>
    <paper id="36">
      <title>A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification</title>
      <author><first>Ruizhe</first><last>Li</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Matthew</first><last>Collinson</last></author>
      <author><first>Xiao</first><last>Li</last></author>
      <author><first>Guanyi</first><last>Chen</last></author>
      <pages>383&#8211;392</pages>
      <abstract>Recognising dialogue acts (DA) is important for many natural language processing tasks such as dialogue generation and intention recognition. In this paper, we propose a dual-attention hierarchical recurrent neural network for DA classification. Our model is partially inspired by the observation that conversational utterances are normally associated with both a DA and a topic, where the former captures the social act and the latter describes the subject matter. However, such a dependency between DAs and topics has not been utilised by most existing systems for DA classification. With a novel dual task-specific attention mechanism, our model is able, for utterances, to capture information about both DAs and topics, as well as information about the interactions between them. Experimental results show that by modelling topic as an auxiliary task, our model can significantly improve DA classification, yielding better or comparable performance to the state-of-the-art method on three public datasets.</abstract>
      <url hash="6b2634ff">K19-1036</url>
      <doi>10.18653/v1/K19-1036</doi>
      <bibkey>li-etal-2019-dual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/switchboard-1-corpus">Switchboard-1 Corpus</pwcdataset>
    </paper>
    <paper id="37">
      <title>Mimic and Rephrase: Reflective Listening in Open-Ended Dialogue</title>
      <author><first>Justin</first><last>Dieter</last></author>
      <author><first>Tian</first><last>Wang</last></author>
      <author><first>Arun Tejasvi</first><last>Chaganty</last></author>
      <author><first>Gabor</first><last>Angeli</last></author>
      <author><first>Angel X.</first><last>Chang</last></author>
      <pages>393&#8211;403</pages>
      <abstract>Reflective listening&#8211;demonstrating that you have heard your conversational partner&#8211;is key to effective communication. Expert human communicators often mimic and rephrase their conversational partner, e.g., when responding to sentimental stories or to questions they don&#8217;t know the answer to. We introduce a new task and an associated dataset wherein dialogue agents similarly mimic and rephrase a user&#8217;s request to communicate sympathy (I&#8217;m sorry to hear that) or lack of knowledge (I do not know that). We study what makes a rephrasal response good against a set of qualitative metrics. We then evaluate three models for generating responses: a syntax-aware rule-based system, a seq2seq LSTM neural models with attention (S2SA), and the same neural model augmented with a copy mechanism (S2SA+C). In a human evaluation, we find that S2SA+C and the rule-based system are comparable and approach human-generated response quality. In addition, experiences with a live deployment of S2SA+C in a customer support setting suggest that this generation task is a practical contribution to real world conversational agents.</abstract>
      <url hash="82065a5a">K19-1037</url>
      <attachment hash="51f931cf">K19-1037.Attachment.zip</attachment>
      <doi>10.18653/v1/K19-1037</doi>
      <bibkey>dieter-etal-2019-mimic</bibkey>
    </paper>
    <paper id="40">
      <title>Leveraging Past References for Robust Language Grounding</title>
      <author><first>Subhro</first><last>Roy</last></author>
      <author><first>Michael</first><last>Noseworthy</last></author>
      <author><first>Rohan</first><last>Paul</last></author>
      <author><first>Daehyung</first><last>Park</last></author>
      <author><first>Nicholas</first><last>Roy</last></author>
      <pages>430&#8211;440</pages>
      <abstract>Grounding referring expressions to objects in an environment has traditionally been considered a one-off, ahistorical task. However, in realistic applications of grounding, multiple users will repeatedly refer to the same set of objects. As a result, past referring expressions for objects can provide strong signals for grounding subsequent referring expressions. We therefore reframe the grounding problem from the perspective of coreference detection and propose a neural network that detects when two expressions are referring to the same object. The network combines information from vision and past referring expressions to resolve which object is being referred to. Our experiments show that detecting referring expression coreference is an effective way to ground objects described by subtle visual properties, which standard visual grounding models have difficulty capturing. We also show the ability to detect object coreference allows the grounding model to perform well even when it encounters object categories not seen in the training data.</abstract>
      <url hash="3f58a196">K19-1040</url>
      <attachment hash="e59468dc">K19-1040.Attachment.pdf</attachment>
      <doi>10.18653/v1/K19-1040</doi>
      <bibkey>roy-etal-2019-leveraging</bibkey>
    </paper>
    <paper id="41">
      <title>Procedural Reasoning Networks for Understanding Multimodal Procedures</title>
      <author><first>Mustafa Sercan</first><last>Amac</last></author>
      <author><first>Semih</first><last>Yagcioglu</last></author>
      <author><first>Aykut</first><last>Erdem</last></author>
      <author><first>Erkut</first><last>Erdem</last></author>
      <pages>441&#8211;451</pages>
      <abstract>This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.</abstract>
      <url hash="90d91fbf">K19-1041</url>
      <doi>10.18653/v1/K19-1041</doi>
      <bibkey>amac-etal-2019-procedural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/recipeqa">RecipeQA</pwcdataset>
    </paper>
    <paper id="42">
      <title>On the Limits of Learning to Actively Learn Semantic Representations</title>
      <author><first>Omri</first><last>Koshorek</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <author><first>Yichu</first><last>Zhou</last></author>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>452&#8211;462</pages>
      <abstract>One of the goals of natural language understanding is to develop models that map sentences into meaning representations. However, training such models requires expensive annotation of complex structures, which hinders their adoption. Learning to actively-learn(LTAL) is a recent paradigm for reducing the amount of labeled data by learning a policy that selects which samples should be labeled. In this work, we examine LTAL for learning semantic representations, such as QA-SRL. We show that even an oracle policy that is allowed to pick examples that maximize performance on the test set (and constitutes an upper bound on the potential of LTAL), does not substantially improve performance compared to a random policy. We investigate factors that could explain this finding and show that a distinguishing characteristic of successful applications of LTAL is the interaction between optimization and the oracle policy selection process. In successful applications of LTAL, the examples selected by the oracle policy do not substantially depend on the optimization procedure, while in our setup the stochastic nature of optimization strongly affects the examples selected by the oracle. We conclude that the current applicability of LTAL for improving data efficiency in learning semantic meaning representations is limited.</abstract>
      <url hash="a5f02e26">K19-1042</url>
      <doi>10.18653/v1/K19-1042</doi>
      <bibkey>koshorek-etal-2019-limits</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl">QA-SRL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl-bank-2-0">QA-SRL Bank 2.0</pwcdataset>
    </paper>
    <paper id="43">
      <title>How Does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?</title>
      <author><first>Hila</first><last>Gonen</last></author>
      <author><first>Yova</first><last>Kementchedjhieva</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>463&#8211;471</pages>
      <abstract>Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun&#8217;s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While &#8220;embedding debiasing&#8221; methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words&#8217; context when training word embeddings is effective in removing it. Fixing the grammatical gender bias yields a positive effect on the quality of the resulting word embeddings, both in monolingual and cross-lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.</abstract>
      <url hash="3a49459c">K19-1043</url>
      <attachment hash="51988d78">K19-1043.Attachment.pdf</attachment>
      <doi>10.18653/v1/K19-1043</doi>
      <bibkey>gonen-etal-2019-grammatical</bibkey>
      <pwccode url="https://github.com/gonenhila/grammatical_gender" additional="false">gonenhila/grammatical_gender</pwccode>
    </paper>
    <paper id="47">
      <title>Detecting Frames in News Headlines and Its Application to Analyzing News Framing Trends Surrounding <fixed-case>U</fixed-case>.<fixed-case>S</fixed-case>. Gun Violence</title>
      <author><first>Siyi</first><last>Liu</last></author>
      <author><first>Lei</first><last>Guo</last></author>
      <author><first>Kate</first><last>Mays</last></author>
      <author><first>Margrit</first><last>Betke</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>504&#8211;514</pages>
      <abstract>Different news articles about the same topic often offer a variety of perspectives: an article written about gun violence might emphasize gun control, while another might promote 2nd Amendment rights, and yet a third might focus on mental health issues. In communication research, these different perspectives are known as &#8220;frames&#8221;, which, when used in news media will influence the opinion of their readers in multiple ways. In this paper, we present a method for effectively detecting frames in news headlines. Our training and performance evaluation is based on a new dataset of news headlines related to the issue of gun violence in the United States. This Gun Violence Frame Corpus (GVFC) was curated and annotated by journalism and communication experts. Our proposed approach sets a new state-of-the-art performance for multiclass news frame detection, significantly outperforming a recent baseline by 35.9% absolute difference in accuracy. We apply our frame detection approach in a large scale study of 88k news headlines about the coverage of gun violence in the U.S. between 2016 and 2018.</abstract>
      <url hash="a666a1db">K19-1047</url>
      <doi>10.18653/v1/K19-1047</doi>
      <bibkey>liu-etal-2019-detecting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gvfc">GVFC</pwcdataset>
    </paper>
    <paper id="49">
      <title>Learning Dense Representations for Entity Retrieval</title>
      <author><first>Daniel</first><last>Gillick</last></author>
      <author><first>Sayali</first><last>Kulkarni</last></author>
      <author><first>Larry</first><last>Lansing</last></author>
      <author><first>Alessandro</first><last>Presta</last></author>
      <author><first>Jason</first><last>Baldridge</last></author>
      <author><first>Eugene</first><last>Ie</last></author>
      <author><first>Diego</first><last>Garcia-Olano</last></author>
      <pages>528&#8211;537</pages>
      <abstract>We show that it is feasible to perform entity linking by training a dual encoder (two-tower) model that encodes mentions and entities in the same dense vector space, where candidate entities are retrieved by approximate nearest neighbor search. Unlike prior work, this setup does not rely on an alias table followed by a re-ranker, and is thus the first fully learned entity retrieval model. We show that our dual encoder, trained using only anchor-text links in Wikipedia, outperforms discrete alias table and BM25 baselines, and is competitive with the best comparable results on the standard TACKBP-2010 dataset. In addition, it can retrieve candidates extremely fast, and generalizes well to a new dataset derived from Wikinews. On the modeling side, we demonstrate the dramatic value of an unsupervised negative mining algorithm for this task.</abstract>
      <url hash="7fa8e51f">K19-1049</url>
      <doi>10.18653/v1/K19-1049</doi>
      <bibkey>gillick-etal-2019-learning</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>K</fixed-case>now<fixed-case>S</fixed-case>em<fixed-case>LM</fixed-case>: A Knowledge Infused Semantic Language Model</title>
      <author><first>Haoruo</first><last>Peng</last></author>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>550&#8211;562</pages>
      <abstract>Story understanding requires developing expectations of what events come next in text. Prior knowledge &#8211; both statistical and declarative &#8211; is essential in guiding such expectations. While existing semantic language models (SemLM) capture event co-occurrence information by modeling event sequences as semantic frames, entities, and other semantic units, this paper aims at augmenting them with causal knowledge (i.e., one event is likely to lead to another). Such knowledge is modeled at the frame and entity level, and can be obtained either statistically from text or stated declaratively. The proposed method, KnowSemLM, infuses this knowledge into a semantic LM by joint training and inference, and is shown to be effective on both the event cloze test and story/referent prediction tasks.</abstract>
      <url hash="954ecd24">K19-1051</url>
      <doi>10.18653/v1/K19-1051</doi>
      <bibkey>peng-etal-2019-knowsemlm</bibkey>
    </paper>
    <paper id="52">
      <title>Neural Attentive Bag-of-Entities Model for Text Classification</title>
      <author><first>Ikuya</first><last>Yamada</last></author>
      <author><first>Hiroyuki</first><last>Shindo</last></author>
      <pages>563&#8211;573</pages>
      <abstract>This study proposes a Neural Attentive Bag-of-Entities model, which is a neural network model that performs text classification using entities in a knowledge base. Entities provide unambiguous and relevant semantic signals that are beneficial for text classification. We combine simple high-recall entity detection based on a dictionary, to detect entities in a document, with a novel neural attention mechanism that enables the model to focus on a small number of unambiguous and relevant entities. We tested the effectiveness of our model using two standard text classification datasets (i.e., the 20 Newsgroups and R8 datasets) and a popular factoid question answering dataset based on a trivia quiz game. As a result, our model achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.</abstract>
      <url hash="bcba0737">K19-1052</url>
      <doi>10.18653/v1/K19-1052</doi>
      <bibkey>yamada-shindo-2019-neural</bibkey>
      <pwccode url="https://github.com/wikipedia2vec/wikipedia2vec" additional="true">wikipedia2vec/wikipedia2vec</pwccode>
    </paper>
    <paper id="55">
      <title><fixed-case>M</fixed-case>r<fixed-case>M</fixed-case>ep: Joint Extraction of Multiple Relations and Multiple Entity Pairs Based on Triplet Attention</title>
      <author><first>Jiayu</first><last>Chen</last></author>
      <author><first>Caixia</first><last>Yuan</last></author>
      <author><first>Xiaojie</first><last>Wang</last></author>
      <author><first>Ziwei</first><last>Bai</last></author>
      <pages>593&#8211;602</pages>
      <abstract>This paper focuses on how to extract multiple relational facts from unstructured text. Neural encoder-decoder models have provided a viable new approach for jointly extracting relations and entity pairs. However, these models either fail to deal with entity overlapping among relational facts, or neglect to produce the whole entity pairs. In this work, we propose a novel architecture that augments the encoder and decoder in two elegant ways. First, we apply a binary CNN classifier for each relation, which identifies all possible relations maintained in the text, while retaining the target relation representation to aid entity pair recognition. Second, we perform a multi-head attention over the text and a triplet attention with the target relation interacting with every token of the text to precisely produce all possible entity pairs in a sequential manner. Experiments on three benchmark datasets show that our proposed method successfully addresses the multiple relations and multiple entity pairs even with complex overlapping and significantly outperforms the state-of-the-art methods.</abstract>
      <url hash="ec4e994e">K19-1055</url>
      <doi>10.18653/v1/K19-1055</doi>
      <bibkey>chen-etal-2019-mrmep</bibkey>
    </paper>
    <paper id="56">
      <title>Effective Attention Modeling for Neural Relation Extraction</title>
      <author><first>Tapas</first><last>Nayak</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <pages>603&#8211;612</pages>
      <abstract>Relation extraction is the task of determining the relation between two entities in a sentence. Distantly-supervised models are popular for this task. However, sentences can be long and two entities can be located far from each other in a sentence. The pieces of evidence supporting the presence of a relation between two entities may not be very direct, since the entities may be connected via some indirect links such as a third entity or via co-reference. Relation extraction in such scenarios becomes more challenging as we need to capture the long-distance interactions among the entities and other words in the sentence. Also, the words in a sentence do not contribute equally in identifying the relation between the two entities. To address this issue, we propose a novel and effective attention model which incorporates syntactic information of the sentence and a multi-factor attention mechanism. Experiments on the New York Times corpus show that our proposed model outperforms prior state-of-the-art models.</abstract>
      <url hash="d92c42db">K19-1056</url>
      <doi>10.18653/v1/K19-1056</doi>
      <bibkey>nayak-ng-2019-effective</bibkey>
      <pwccode url="https://github.com/nusnlp/MFA4RE" additional="false">nusnlp/MFA4RE</pwccode>
    </paper>
    <paper id="57">
      <title>Exploiting the Entity Type Sequence to Benefit Event Detection</title>
      <author><first>Yuze</first><last>Ji</last></author>
      <author><first>Youfang</first><last>Lin</last></author>
      <author><first>Jianwei</first><last>Gao</last></author>
      <author><first>Huaiyu</first><last>Wan</last></author>
      <pages>613&#8211;623</pages>
      <abstract>Event Detection (ED) is one of the most important task in the field of information extraction. The goal of ED is to find triggers in sentences and classify them into different event types. In previous works, the information of entity types are commonly utilized to benefit event detection. However, the sequential features of entity types have not been well utilized yet in the existing ED methods. In this paper, we propose a novel ED approach which learns sequential features from word sequences and entity type sequences separately, and combines these two types of sequential features with the help of a trigger-entity interaction learning module. The experimental results demonstrate that our proposed approach outperforms the state-of-the-art methods.</abstract>
      <url hash="4ac784fe">K19-1057</url>
      <doi>10.18653/v1/K19-1057</doi>
      <bibkey>ji-etal-2019-exploiting</bibkey>
    </paper>
    <paper id="60">
      <title>Named Entity Recognition with Partially Annotated Training Data</title>
      <author><first>Stephen</first><last>Mayhew</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <author><first>Chen-Tse</first><last>Tsai</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>645&#8211;655</pages>
      <abstract>Supervised machine learning assumes the availability of fully-labeled data, but in many cases, such as low-resource languages, the only data available is partially annotated. We study the problem of Named Entity Recognition (NER) with partially annotated training data in which a fraction of the named entities are labeled, and all other tokens, entities or otherwise, are labeled as non-entity by default. In order to train on this noisy dataset, we need to distinguish between the true and false negatives. To this end, we introduce a constraint-driven iterative algorithm that learns to detect false negatives in the noisy set and downweigh them, resulting in a weighted training set. With this set, we train a weighted NER model. We evaluate our algorithm with weighted variants of neural and non-neural NER models on data in 8 languages from several language and script families, showing strong ability to learn from partial data. Finally, to show real-world efficacy, we evaluate on a Bengali NER corpus annotated by non-speakers, outperforming the prior state-of-the-art by over 5 points F1.</abstract>
      <url hash="3d93dd83">K19-1060</url>
      <doi>10.18653/v1/K19-1060</doi>
      <bibkey>mayhew-etal-2019-named</bibkey>
    </paper>
    <paper id="62">
      <title>Deep Structured Neural Network for Event Temporal Relation Extraction</title>
      <author><first>Rujun</first><last>Han</last></author>
      <author><first>I-Hung</first><last>Hsu</last></author>
      <author><first>Mu</first><last>Yang</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <author><first>Ralph</first><last>Weischedel</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>666&#8211;106</pages>
      <abstract>We propose a novel deep structured learning framework for event temporal relation extraction. The model consists of 1) a recurrent neural network (RNN) to learn scoring functions for pair-wise relations, and 2) a structured support vector machine (SSVM) to make joint predictions. The neural network automatically learns representations that account for long-term contexts to provide robust features for the structured model, while the SSVM incorporates domain knowledge such as transitive closure of temporal relations as constraints to make better globally consistent decisions. By jointly training the two components, our model combines the benefits of both data-driven learning and knowledge exploitation. Experimental results on three high-quality event temporal relation datasets (TCR, MATRES, and TB-Dense) demonstrate that incorporated with pre-trained contextualized embeddings, the proposed model achieves significantly better performances than the state-of-the-art methods on all three datasets. We also provide thorough ablation studies to investigate our model.</abstract>
      <url hash="c2f1e107">K19-1062</url>
      <doi>10.18653/v1/K19-1062</doi>
      <bibkey>han-etal-2019-deep</bibkey>
      <pwccode url="https://github.com/PlusLabNLP/Deep-Structured-EveEveTemp" additional="false">PlusLabNLP/Deep-Structured-EveEveTemp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tcr">TCR</pwcdataset>
    </paper>
    <paper id="68">
      <title>Memory Graph Networks for Explainable Memory-grounded Question Answering</title>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Pararth</first><last>Shah</last></author>
      <author><first>Anuj</first><last>Kumar</last></author>
      <author><first>Rajen</first><last>Subba</last></author>
      <pages>728&#8211;736</pages>
      <abstract>We introduce Episodic Memory QA, the task of answering personal user questions grounded on memory graph (MG), where episodic memories and related entity nodes are connected via relational edges. We create a new benchmark dataset first by generating synthetic memory graphs with simulated attributes, and by composing 100K QA pairs for the generated MG with bootstrapped scripts. To address the unique challenges for the proposed task, we propose Memory Graph Networks (MGN), a novel extension of memory networks to enable dynamic expansion of memory slots through graph traversals, thus able to answer queries in which contexts from multiple linked episodes and external knowledge are required. We then propose the Episodic Memory QA Net with multiple module networks to effectively handle various question types. Empirical results show improvement over the QA baselines in top-k answer prediction accuracy in the proposed task. The proposed model also generates a graph walk path and attention vectors for each predicted answer, providing a natural way to explain its QA reasoning.</abstract>
      <url hash="261ae358">K19-1068</url>
      <doi>10.18653/v1/K19-1068</doi>
      <bibkey>moon-etal-2019-memory</bibkey>
    </paper>
    <paper id="73">
      <title><fixed-case>TILM</fixed-case>: Neural Language Models with Evolving Topical Influence</title>
      <author><first>Shubhra Kanti</first><last>Karmaker Santu</last></author>
      <author><first>Kalyan</first><last>Veeramachaneni</last></author>
      <author><first>Chengxiang</first><last>Zhai</last></author>
      <pages>778&#8211;788</pages>
      <abstract>Content of text data are often influenced by contextual factors which often evolve over time (e.g., content of social media are often influenced by topics covered in the major news streams). Existing language models do not consider the influence of such related evolving topics, and thus are not optimal. In this paper, we propose to incorporate such topical-influence into a language model to both improve its accuracy and enable cross-stream analysis of topical influences. Specifically, we propose a novel language model called Topical Influence Language Model (TILM), which is a novel extension of a neural language model to capture the influences on the contents in one text stream by the evolving topics in another related (or possibly same) text stream. Experimental results on six different text stream data comprised of conference paper titles show that the incorporation of evolving topical influence into a language model is beneficial and TILM outperforms multiple baselines in a challenging task of text forecasting. In addition to serving as a language model, TILM further enables interesting analysis of topical influence among multiple text streams.</abstract>
      <url hash="9202a96c">K19-1073</url>
      <attachment hash="34ab3136">K19-1073.Attachment.zip</attachment>
      <doi>10.18653/v1/K19-1073</doi>
      <bibkey>karmaker-santu-etal-2019-tilm</bibkey>
    </paper>
    <paper id="74">
      <title>Pretraining-Based Natural Language Generation for Text Summarization</title>
      <author><first>Haoyu</first><last>Zhang</last></author>
      <author><first>Jingjing</first><last>Cai</last></author>
      <author><first>Jianjun</first><last>Xu</last></author>
      <author><first>Ji</first><last>Wang</last></author>
      <pages>789&#8211;797</pages>
      <abstract>In this paper, we propose a novel pretraining-based encoder-decoder framework, which can generate the output sequence based on the input sequence in a two-stage manner. For the encoder of our model, we encode the input sequence into context representations using BERT. For the decoder, there are two stages in our model, in the first stage, we use a Transformer-based decoder to generate a draft output sequence. In the second stage, we mask each word of the draft sequence and feed it to BERT, then by combining the input sequence and the draft representation generated by BERT, we use a Transformer-based decoder to predict the refined word for each masked position. To the best of our knowledge, our approach is the first method which applies the BERT into text generation tasks. As the first step in this direction, we evaluate our proposed method on the text summarization task. Experimental results show that our model achieves new state-of-the-art on both CNN/Daily Mail and New York Times datasets.</abstract>
      <url hash="37068e20">K19-1074</url>
      <doi>10.18653/v1/K19-1074</doi>
      <bibkey>zhang-etal-2019-pretraining</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="75">
      <title>Goal-Embedded Dual Hierarchical Model for Task-Oriented Dialogue Generation</title>
      <author><first>Yi-An</first><last>Lai</last></author>
      <author><first>Arshit</first><last>Gupta</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <pages>798&#8211;811</pages>
      <abstract>Hierarchical neural networks are often used to model inherent structures within dialogues. For goal-oriented dialogues, these models miss a mechanism adhering to the goals and neglect the distinct conversational patterns between two interlocutors. In this work, we propose Goal-Embedded Dual Hierarchical Attentional Encoder-Decoder (G-DuHA) able to center around goals and capture interlocutor-level disparity while modeling goal-oriented dialogues. Experiments on dialogue generation, response generation, and human evaluations demonstrate that the proposed model successfully generates higher-quality, more diverse and goal-centric dialogues. Moreover, we apply data augmentation via goal-oriented dialogue generation for task-oriented dialog systems with better performance achieved.</abstract>
      <url hash="660a197d">K19-1075</url>
      <attachment hash="a56086fc">K19-1075.Attachment.zip</attachment>
      <doi>10.18653/v1/K19-1075</doi>
      <bibkey>lai-etal-2019-goal</bibkey>
    </paper>
    <paper id="77">
      <title>In Conclusion Not Repetition: Comprehensive Abstractive Summarization with Diversified Attention Based on Determinantal Point Processes</title>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Marina</first><last>Litvak</last></author>
      <author><first>Natalia</first><last>Vanetik</last></author>
      <author><first>Zuying</first><last>Huang</last></author>
      <pages>822&#8211;832</pages>
      <abstract>Various Seq2Seq learning models designed for machine translation were applied for abstractive summarization task recently. Despite these models provide high ROUGE scores, they are limited to generate comprehensive summaries with a high level of abstraction due to its degenerated attention distribution. We introduce Diverse Convolutional Seq2Seq Model(DivCNN Seq2Seq) using Determinantal Point Processes methods(Micro DPPs and Macro DPPs) to produce attention distribution considering both quality and diversity. Without breaking the end to end architecture, DivCNN Seq2Seq achieves a higher level of comprehensiveness compared to vanilla models and strong baselines. All the reproducible codes and datasets are available online.</abstract>
      <url hash="e076bf61">K19-1077</url>
      <attachment type="supplementary-material" hash="ecf727d3">K19-1077.Supplementary_Material.zip</attachment>
      <doi>10.18653/v1/K19-1077</doi>
      <revision id="1" href="K19-1077v1" hash="53aee35e" />
      <revision id="2" href="K19-1077v2" date="2020-01-01" hash="e076bf61">The equation 11 should be opposite since we add this term into the loss function to minimize it not to maximize it. The code is right and this is just a erratum.</revision>
      <bibkey>li-etal-2019-conclusion</bibkey>
      <pwccode url="https://github.com/thinkwee/DPP_CNN_Summarization" additional="false">thinkwee/DPP_CNN_Summarization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bigpatent">BigPatent</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="81">
      <title><fixed-case>BIO</fixed-case>fid Dataset: Publishing a <fixed-case>G</fixed-case>erman Gold Standard for Named Entity Recognition in Historical Biodiversity Literature</title>
      <author><first>Sajawel</first><last>Ahmed</last></author>
      <author><first>Manuel</first><last>Stoeckel</last></author>
      <author><first>Christine</first><last>Driller</last></author>
      <author><first>Adrian</first><last>Pachzelt</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <pages>871&#8211;880</pages>
      <abstract>The Specialized Information Service Biodiversity Research (BIOfid) has been launched to mobilize valuable biological data from printed literature hidden in German libraries for over the past 250 years. In this project, we annotate German texts converted by OCR from historical scientific literature on the biodiversity of plants, birds, moths and butterflies. Our work enables the automatic extraction of biological information previously buried in the mass of papers and volumes. For this purpose, we generated training data for the tasks of Named Entity Recognition (NER) and Taxa Recognition (TR) in biological documents. We use this data to train a number of leading machine learning tools and create a gold standard for TR in biodiversity literature. More specifically, we perform a practical analysis of our newly generated BIOfid dataset through various downstream-task evaluations and establish a new state of the art for TR with 80.23% F-score. In this sense, our paper lays the foundations for future work in the field of information extraction in biology texts.</abstract>
      <url hash="e58ec986">K19-1081</url>
      <attachment type="supplementary-material" hash="18ff8a37">K19-1081.Supplementary_Material.zip</attachment>
      <doi>10.18653/v1/K19-1081</doi>
      <bibkey>ahmed-etal-2019-biofid</bibkey>
    </paper>
    <paper id="83">
      <title>Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes</title>
      <author><first>No&#233;mien</first><last>Kocher</last></author>
      <author><first>Christian</first><last>Scuito</last></author>
      <author><first>Lorenzo</first><last>Tarantino</last></author>
      <author><first>Alexandros</first><last>Lazaridis</last></author>
      <author><first>Andreas</first><last>Fischer</last></author>
      <author><first>Claudiu</first><last>Musat</last></author>
      <pages>890&#8211;899</pages>
      <abstract>In sequence modeling tasks the token order matters, but this information can be partially lost due to the discretization of the sequence into data points. In this paper, we study the imbalance between the way certain token pairs are included in data points and others are not. We denote this a token order imbalance (TOI) and we link the partial sequence information loss to a diminished performance of the system as a whole, both in text and speech processing tasks. We then provide a mechanism to leverage the full token order information&#8212;Alleviated TOI&#8212;by iteratively overlapping the token composition of data points. For recurrent networks, we use prime numbers for the batch size to avoid redundancies when building batches from overlapped data points. The proposed method achieved state of the art performance in both text and speech related tasks.</abstract>
      <url hash="b8f78ed8">K19-1083</url>
      <doi>10.18653/v1/K19-1083</doi>
      <bibkey>kocher-etal-2019-alleviating</bibkey>
      <pwccode url="https://github.com/nkcr/overlap-ml" additional="false">nkcr/overlap-ml</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="84">
      <title>Global Autoregressive Models for Data-Efficient Sequence Learning</title>
      <author><first>Tetiana</first><last>Parshakova</last></author>
      <author><first>Jean-Marc</first><last>Andreoli</last></author>
      <author><first>Marc</first><last>Dymetman</last></author>
      <pages>900&#8211;909</pages>
      <abstract>Standard autoregressive seq2seq models are easily trained by max-likelihood, but tend to show poor results under small-data conditions. We introduce a class of seq2seq models, GAMs (Global Autoregressive Models), which combine an autoregressive component with a log-linear component, allowing the use of global <i>a priori</i> features to compensate for lack of data. We train these models in two steps. In the first step, we obtain an <i>unnormalized</i> GAM that maximizes the likelihood of the data, but is improper for fast inference or evaluation. In the second step, we use this GAM to train (by distillation) a second autoregressive model that approximates the <i>normalized</i> distribution associated with the GAM, and can be used for fast inference and evaluation. Our experiments focus on language modelling under synthetic conditions and show a strong perplexity reduction of using the second autoregressive model over the standard one.</abstract>
      <url hash="932a38bc">K19-1084</url>
      <attachment hash="b96df6c5">K19-1084.Attachment.pdf</attachment>
      <attachment type="supplementary-material" hash="b96df6c5">K19-1084.Supplementary_Material.pdf</attachment>
      <doi>10.18653/v1/K19-1084</doi>
      <bibkey>parshakova-etal-2019-global</bibkey>
    </paper>
    <paper id="85">
      <title>Learning Analogy-Preserving Sentence Embeddings for Answer Selection</title>
      <author><first>A&#239;ssatou</first><last>Diallo</last></author>
      <author><first>Markus</first><last>Zopf</last></author>
      <author><first>Johannes</first><last>F&#252;rnkranz</last></author>
      <pages>910&#8211;919</pages>
      <abstract>Answer selection aims at identifying the correct answer for a given question from a set of potentially correct answers. Contrary to previous works, which typically focus on the semantic similarity between a question and its answer, our hypothesis is that question-answer pairs are often in analogical relation to each other. Using analogical inference as our use case, we propose a framework and a neural network architecture for learning dedicated sentence embeddings that preserve analogical properties in the semantic space. We evaluate the proposed method on benchmark datasets for answer selection and demonstrate that our sentence embeddings indeed capture analogical properties better than conventional embeddings, and that analogy-based question answering outperforms a comparable similarity-based technique.</abstract>
      <url hash="aac114e4">K19-1085</url>
      <doi>10.18653/v1/K19-1085</doi>
      <bibkey>diallo-etal-2019-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="86">
      <title>A Simple and Effective Method for Injecting Word-Level Information into Character-Aware Neural Language Models</title>
      <author><first>Yukun</first><last>Feng</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>920&#8211;928</pages>
      <abstract>We propose a simple and effective method to inject word-level information into character-aware neural language models. Unlike previous approaches which usually inject word-level information at the input of a long short-term memory (LSTM) network, we inject it into the softmax function. The resultant model can be seen as a combination of character-aware language model and simple word-level language model. Our injection method can also be used together with previous methods. Through the experiments on 14 typologically diverse languages, we empirically show that our injection method, when used together with the previous methods, works better than the previous methods, including a gating mechanism, averaging, and concatenation of word vectors. We also provide a comprehensive comparison of these injection methods.</abstract>
      <url hash="9c32eb7c">K19-1086</url>
      <doi>10.18653/v1/K19-1086</doi>
      <bibkey>feng-etal-2019-simple</bibkey>
    </paper>
    <paper id="87">
      <title>On Model Stability as a Function of Random Seed</title>
      <author><first>Pranava</first><last>Madhyastha</last></author>
      <author><first>Rishabh</first><last>Jain</last></author>
      <pages>929&#8211;939</pages>
      <abstract>In this paper, we focus on quantifying model stability as a function of random seed by investigating the effects of the induced randomness on model performance and the robustness of the model in general. We specifically perform a controlled study on the effect of random seeds on the behaviour of attention, gradient-based and surrogate model based (LIME) interpretations. Our analysis suggests that random seeds can adversely affect the consistency of models resulting in counterfactual interpretations. We propose a technique called Aggressive Stochastic Weight Averaging (ASWA) and an extension called Norm-filtered Aggressive Stochastic Weight Averaging (NASWA) which improves the stability of models over random seeds. With our ASWA and NASWA based optimization, we are able to improve the robustness of the original model, on average reducing the standard deviation of the model&#8217;s performance by 72%.</abstract>
      <url hash="8a2c1fe2">K19-1087</url>
      <attachment type="supplementary-material" hash="73abed49">K19-1087.Supplementary_Material.zip</attachment>
      <attachment hash="73abed49">K19-1087.Attachment.zip</attachment>
      <doi>10.18653/v1/K19-1087</doi>
      <bibkey>madhyastha-jain-2019-model</bibkey>
      <pwccode url="https://github.com/rishj97/ModelStability" additional="false">rishj97/ModelStability</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="88">
      <title>Studying Generalisability across Abusive Language Detection Datasets</title>
      <author><first>Steve Durairaj</first><last>Swamy</last></author>
      <author><first>Anupam</first><last>Jamatia</last></author>
      <author><first>Bj&#246;rn</first><last>Gamb&#228;ck</last></author>
      <pages>940&#8211;950</pages>
      <abstract>Work on Abusive Language Detection has tackled a wide range of subtasks and domains. As a result of this, there exists a great deal of redundancy and non-generalisability between datasets. Through experiments on cross-dataset training and testing, the paper reveals that the preconceived notion of including more non-abusive samples in a dataset (to emulate reality) may have a detrimental effect on the generalisability of a model trained on that data. Hence a hierarchical annotation model is utilised here to reveal redundancies in existing datasets and to help reduce redundancy in future efforts.</abstract>
      <url hash="bafcaa79">K19-1088</url>
      <doi>10.18653/v1/K19-1088</doi>
      <bibkey>swamy-etal-2019-studying</bibkey>
    </paper>
    <paper id="89">
      <title>Reduce &amp; Attribute: Two-Step Authorship Attribution for Large-Scale Problems</title>
      <author><first>Michael</first><last>Tschuggnall</last></author>
      <author><first>Benjamin</first><last>Murauer</last></author>
      <author><first>G&#252;nther</first><last>Specht</last></author>
      <pages>951&#8211;960</pages>
      <abstract>Authorship attribution is an active research area which has been prevalent for many decades. Nevertheless, the majority of approaches consider problem sizes of a few candidate authors only, making them difficult to apply to recent scenarios incorporating thousands of authors emerging due to the manifold means to digitally share text. In this study, we focus on such large-scale problems and propose to effectively reduce the number of candidate authors before applying common attribution techniques. By utilizing document embeddings, we show on a novel, comprehensive dataset collection that the set of candidate authors can be reduced with high accuracy. Moreover, we show that common authorship attribution methods substantially benefit from a preliminary reduction if thousands of authors are involved.</abstract>
      <url hash="26117bc1">K19-1089</url>
      <doi>10.18653/v1/K19-1089</doi>
      <bibkey>tschuggnall-etal-2019-reduce</bibkey>
    </paper>
    <paper id="93">
      <title>A Personalized Sentiment Model with Textual and Contextual Information</title>
      <author><first>Siwen</first><last>Guo</last></author>
      <author><first>Sviatlana</first><last>H&#246;hn</last></author>
      <author><first>Christoph</first><last>Schommer</last></author>
      <pages>992&#8211;1001</pages>
      <abstract>In this paper, we look beyond the traditional population-level sentiment modeling and consider the individuality in a person&#8217;s expressions by discovering both textual and contextual information. In particular, we construct a hierarchical neural network that leverages valuable information from a person&#8217;s past expressions, and offer a better understanding of the sentiment from the expresser&#8217;s perspective. Additionally, we investigate how a person&#8217;s sentiment changes over time so that recent incidents or opinions may have more effect on the person&#8217;s current sentiment than the old ones. Psychological studies have also shown that individual variation exists in how easily people change their sentiments. In order to model such traits, we develop a modified attention mechanism with Hawkes process applied on top of a recurrent network for a user-specific design. Implemented with automatically labeled Twitter data, the proposed model has shown positive results employing different input formulations for representing the concerned information.</abstract>
      <url hash="2c9348a4">K19-1093</url>
      <attachment hash="036a838a">K19-1093.Attachment.zip</attachment>
      <attachment type="supplementary-material" hash="d7a33241">K19-1093.Supplementary_Material.zip</attachment>
      <doi>10.18653/v1/K19-1093</doi>
      <bibkey>guo-etal-2019-personalized</bibkey>
    </paper>
    <paper id="94">
      <title>Cluster-Gated Convolutional Neural Network for Short Text Classification</title>
      <author><first>Haidong</first><last>Zhang</last></author>
      <author><first>Wancheng</first><last>Ni</last></author>
      <author><first>Meijing</first><last>Zhao</last></author>
      <author><first>Ziqi</first><last>Lin</last></author>
      <pages>1002&#8211;1011</pages>
      <abstract>Text classification plays a crucial role for understanding natural language in a wide range of applications. Most existing approaches mainly focus on long text classification (e.g., blogs, documents, paragraphs). However, they cannot easily be applied to short text because of its sparsity and lack of context. In this paper, we propose a new model called cluster-gated convolutional neural network (CGCNN), which jointly explores word-level clustering and text classification in an end-to-end manner. Specifically, the proposed model firstly uses a bi-directional long short-term memory to learn word representations. Then, it leverages a soft clustering method to explore their semantic relation with the cluster centers, and takes linear transformation on text representations. It develops a cluster-dependent gated convolutional layer to further control the cluster-dependent feature flows. Experimental results on five commonly used datasets show that our model outperforms state-of-the-art models.</abstract>
      <url hash="ce185d60">K19-1094</url>
      <doi>10.18653/v1/K19-1094</doi>
      <bibkey>zhang-etal-2019-cluster</bibkey>
    </paper>
    <paper id="96">
      <title>Predicting the Role of Political Trolls in Social Media</title>
      <author><first>Atanas</first><last>Atanasov</last></author>
      <author><first>Gianmarco</first><last>De Francisci Morales</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1023&#8211;1034</pages>
      <abstract>We investigate the political roles of &#8220;Internet trolls&#8221; in social media. Political trolls, such as the ones linked to the Russian Internet Research Agency (IRA), have recently gained enormous attention for their ability to sway public opinion and even influence elections. Analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. However, this analysis is manual and labor-intensive, thus making it impractical as a first-response tool for newly-discovered troll farms. In this paper, we show how to automate this analysis by using machine learning in a realistic setting. In particular, we show how to classify trolls according to their political role &#8212;left, news feed, right&#8212; by using features extracted from social media, i.e., Twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. Technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e., embeddings, for the trolls. Experiments on the &#8220;IRA Russian Troll&#8221; dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.</abstract>
      <url hash="5faa5564">K19-1096</url>
      <attachment hash="00fe9e31">K19-1096.Attachment.txt</attachment>
      <doi>10.18653/v1/K19-1096</doi>
      <bibkey>atanasov-etal-2019-predicting</bibkey>
      <pwccode url="https://github.com/amatanasov/conll_political_trolls" additional="false">amatanasov/conll_political_trolls</pwccode>
    </paper>
    <paper id="97">
      <title>Towards a Unified End-to-End Approach for Fully Unsupervised Cross-Lingual Sentiment Analysis</title>
      <author><first>Yanlin</first><last>Feng</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>1035&#8211;1044</pages>
      <abstract>Sentiment analysis in low-resource languages suffers from the lack of training data. Cross-lingual sentiment analysis (CLSA) aims to improve the performance on these languages by leveraging annotated data from other languages. Recent studies have shown that CLSA can be performed in a fully unsupervised manner, without exploiting either target language supervision or cross-lingual supervision. However, these methods rely heavily on unsupervised cross-lingual word embeddings (CLWE), which has been shown to have serious drawbacks on distant language pairs (e.g. English - Japanese). In this paper, we propose an end-to-end CLSA model by leveraging unlabeled data in multiple languages and multiple domains and eliminate the need for unsupervised CLWE. Our model applies to two CLSA settings: the traditional cross-lingual in-domain setting and the more challenging cross-lingual cross-domain setting. We empirically evaluate our approach on the multilingual multi-domain Amazon review dataset. Experimental results show that our model outperforms the baselines by a large margin despite its minimal resource requirement.</abstract>
      <url hash="a9db649b">K19-1097</url>
      <doi>10.18653/v1/K19-1097</doi>
      <bibkey>feng-wan-2019-towards</bibkey>
    </paper>
  </volume>
  <volume id="2" ingest-date="2019-11-03">
    <meta>
      <booktitle>Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning</booktitle>
      <url hash="1639259f">K19-2</url>
      <editor><first>Stephan</first><last>Oepen</last></editor>
      <editor><first>Omri</first><last>Abend</last></editor>
      <editor><first>Jan</first><last>Hajic</last></editor>
      <editor><first>Daniel</first><last>Hershcovich</last></editor>
      <editor><first>Marco</first><last>Kuhlmann</last></editor>
      <editor><first>Tim</first><last>O&#8217;Gorman</last></editor>
      <editor><first>Nianwen</first><last>Xue</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hong Kong</address>
      <month>November</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="2c016060">K19-2000</url>
      <bibkey>conll-2019-shared</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>MRP</fixed-case> 2019: Cross-Framework Meaning Representation Parsing</title>
      <author><first>Stephan</first><last>Oepen</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <author><first>Jan</first><last>Hajic</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Marco</first><last>Kuhlmann</last></author>
      <author><first>Tim</first><last>O&#8217;Gorman</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <author><first>Jayeol</first><last>Chun</last></author>
      <author><first>Milan</first><last>Straka</last></author>
      <author><first>Zdenka</first><last>Uresova</last></author>
      <pages>1&#8211;27</pages>
      <abstract>The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks. Five distinct approaches to the representation of sentence meaning in the form of directed graph were represented in the training and evaluation data for the task, packaged in a uniform abstract graph representation and serialization. The task received submissions from eighteen teams, of which five do not participate in the official ranking because they arrived after the closing deadline, made use of additional training data, or involved one of the task co-organizers. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu</abstract>
      <url hash="334f3476">K19-2001</url>
      <attachment hash="a11f02e8">K19-2001.Attachment.pdf</attachment>
      <doi>10.18653/v1/K19-2001</doi>
      <bibkey>oepen-etal-2019-mrp</bibkey>
    </paper>
    <paper id="3">
      <title>The <fixed-case>ERG</fixed-case> at <fixed-case>MRP</fixed-case> 2019: Radically Compositional Semantic Dependencies</title>
      <author><first>Stephan</first><last>Oepen</last></author>
      <author><first>Dan</first><last>Flickinger</last></author>
      <pages>40&#8211;44</pages>
      <abstract>The English Resource Grammar (ERG) is a broad-coverage computational grammar of English that outputs underspecified logical-form representations of meaning in a framework dubbed English Resource Semantics (ERS). Two of the target representations in the the 2019 Shared Task on Cross-Framework Meaning Representation Parsing (MRP 2019) derive graph-based simplifications of ERS, viz. Elementary Dependency Structures (EDS) and DELPH-IN MRS Bi-Lexical Dependencies (DM). As a point of reference outside the official MRP competition, we parsed the evaluation strings using the ERG and converted the resulting meaning representations to EDS and DM. These graphs yield higher evaluation scores than the purely data-driven parsers in the actual shared task, suggesting that the general-purpose linguistic knowledge about English grammar encoded in the ERG can add value when parsing into these meaning representations.</abstract>
      <url hash="b3459db6">K19-2003</url>
      <doi>10.18653/v1/K19-2003</doi>
      <bibkey>oepen-flickinger-2019-erg</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>SJTU</fixed-case>-<fixed-case>NICT</fixed-case> at <fixed-case>MRP</fixed-case> 2019: Multi-Task Learning for End-to-End Uniform Semantic Graph Parsing</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>45&#8211;54</pages>
      <abstract>This paper describes our SJTU-NICT&#8217;s system for participating in the shared task on Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conference for Computational Language Learning (CoNLL). Our system uses a graph-based approach to model a variety of semantic graph parsing tasks. Our main contributions in the submitted system are summarized as follows: 1. Our model is fully end-to-end and is capable of being trained only on the given training set which does not rely on any other extra training source including the companion data provided by the organizer; 2. We extend our graph pruning algorithm to a variety of semantic graphs, solving the problem of excessive semantic graph search space; 3. We introduce multi-task learning for multiple objectives within the same framework. The evaluation results show that our system achieved second place in the overall <tex-math>F_1</tex-math> score and achieved the best <tex-math>F_1</tex-math> score on the DM framework.</abstract>
      <url hash="c50a7818">K19-2004</url>
      <doi>10.18653/v1/K19-2004</doi>
      <bibkey>li-etal-2019-sjtu</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>CUHK</fixed-case> at <fixed-case>MRP</fixed-case> 2019: Transition-Based Parser with Cross-Framework Variable-Arity Resolve Action</title>
      <author><first>Sunny</first><last>Lai</last></author>
      <author><first>Chun Hei</first><last>Lo</last></author>
      <author><first>Kwong Sak</first><last>Leung</last></author>
      <author><first>Yee</first><last>Leung</last></author>
      <pages>104&#8211;113</pages>
      <abstract>This paper describes our system (RESOLVER) submitted to the CoNLL 2019 shared task on Cross-Framework Meaning Representation Parsing (MRP). Our system implements a transition-based parser with a directed acyclic graph (DAG) to tree preprocessor and a novel cross-framework variable-arity resolve action that generalizes over five different representations. Although we ranked low in the competition, we have shown the current limitations and potentials of including variable-arity action in MRP and concluded with directions for improvements in the future.</abstract>
      <url hash="39a18478">K19-2010</url>
      <attachment hash="eb8d53a8">K19-2010.Attachment.zip</attachment>
      <doi>10.18653/v1/K19-2010</doi>
      <bibkey>lai-etal-2019-cuhk</bibkey>
    </paper>
    <paper id="11">
      <title>Hitachi at <fixed-case>MRP</fixed-case> 2019: Unified Encoder-to-Biaffine Network for Cross-Framework Meaning Representation Parsing</title>
      <author><first>Yuta</first><last>Koreeda</last></author>
      <author><first>Gaku</first><last>Morio</last></author>
      <author><first>Terufumi</first><last>Morishita</last></author>
      <author><first>Hiroaki</first><last>Ozaki</last></author>
      <author><first>Kohsuke</first><last>Yanai</last></author>
      <pages>114&#8211;126</pages>
      <abstract>This paper describes the proposed system of the Hitachi team for the Cross-Framework Meaning Representation Parsing (MRP 2019) shared task. In this shared task, the participating systems were asked to predict nodes, edges and their attributes for five frameworks, each with different order of &#8220;abstraction&#8221; from input tokens. We proposed a unified encoder-to-biaffine network for all five frameworks, which effectively incorporates a shared encoder to extract rich input features, decoder networks to generate anchorless nodes in UCCA and AMR, and biaffine networks to predict edges. Our system was ranked fifth with the macro-averaged MRP F1 score of 0.7604, and outperformed the baseline unified transition-based MRP. Furthermore, post-evaluation experiments showed that we can boost the performance of the proposed system by incorporating multi-task learning, whereas the baseline could not. These imply efficacy of incorporating the biaffine network to the shared architecture for MRP and that learning heterogeneous meaning representations at once can boost the system performance.</abstract>
      <url hash="a040a48e">K19-2011</url>
      <doi>10.18653/v1/K19-2011</doi>
      <bibkey>koreeda-etal-2019-hitachi</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>&#218;FAL</fixed-case>-<fixed-case>O</fixed-case>slo at <fixed-case>MRP</fixed-case> 2019: Garage Sale Semantic Parsing</title>
      <author><first>Kira</first><last>Droganova</last></author>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Nikita</first><last>Mediankin</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <pages>158&#8211;165</pages>
      <abstract>This paper describes the &#218;FAL--Oslo system submission to the shared task on Cross-Framework Meaning Representation Parsing (MRP, Oepen et al. 2019). The submission is based on several third-party parsers. Within the official shared task results, the submission ranked 11th out of 13 participating systems.</abstract>
      <url hash="b353eb20">K19-2015</url>
      <attachment hash="291225cf">K19-2015.Attachment.zip</attachment>
      <doi>10.18653/v1/K19-2015</doi>
      <bibkey>droganova-etal-2019-ufal</bibkey>
    </paper>
    <paper id="16">
      <title>Peking at <fixed-case>MRP</fixed-case> 2019: Factorization- and Composition-Based Parsing for Elementary Dependency Structures</title>
      <author><first>Yufei</first><last>Chen</last></author>
      <author><first>Yajie</first><last>Ye</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <pages>166&#8211;176</pages>
      <abstract>We design, implement and evaluate two semantic parsers, which represent factorization- and composition-based approaches respectively, for Elementary Dependency Structures (EDS) at the CoNLL 2019 Shared Task on Cross-Framework Meaning Representation Parsing. The detailed evaluation of the two parsers gives us a new perception about parsing into linguistically enriched meaning representations: current neural EDS parsers are able to reach an accuracy at the inter-annotator agreement level in the same-epoch-and-domain setup.</abstract>
      <url hash="a8a2198e">K19-2016</url>
      <doi>10.18653/v1/K19-2016</doi>
      <bibkey>chen-etal-2019-peking</bibkey>
    </paper>
  </volume>
</collection>