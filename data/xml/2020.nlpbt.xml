<collection id="2020.nlpbt">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the First International Workshop on Natural Language Processing Beyond Text</booktitle>
      <editor><first>Giuseppe</first><last>Castellucci</last></editor>
      <editor><first>Simone</first><last>Filice</last></editor>
      <editor><first>Soujanya</first><last>Poria</last></editor>
      <editor><first>Erik</first><last>Cambria</last></editor>
      <editor><first>Lucia</first><last>Specia</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="54cecc5a">2020.nlpbt-1.0</url>
      <bibkey>nlpbt-2020-international</bibkey>
    </frontmatter>
    <paper id="7">
      <title><fixed-case>MAST</fixed-case>: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</title>
      <author><first>Aman</first><last>Khullar</last></author>
      <author><first>Udit</first><last>Arora</last></author>
      <pages>60&#8211;69</pages>
      <abstract>This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities &#8211; text, audio and video &#8211; in a multimodal video. Prior work on multimodal abstractive text summarization only utilized information from the text and video modalities. We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.</abstract>
      <url hash="a8bf05bd">2020.nlpbt-1.7</url>
      <doi>10.18653/v1/2020.nlpbt-1.7</doi>
      <video href="https://slideslive.com/38939781" />
      <bibkey>khullar-arora-2020-mast</bibkey>
      <pwccode url="https://github.com/amankhullar/mast" additional="false">amankhullar/mast</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/how2">How2</pwcdataset>
    </paper>
    <paper id="9">
      <title>Reasoning Over History: Context Aware Visual Dialog</title>
      <author><first>Muhammad</first><last>Shah</last></author>
      <author><first>Shikib</first><last>Mehri</last></author>
      <author><first>Tejas</first><last>Srinivasan</last></author>
      <pages>75&#8211;83</pages>
      <abstract>While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existing strong neural VQA models with the mechanisms that allow them to retain information from previous dialog turns. One strong VQA model is the MAC network, which decomposes a task into a series of attention-based reasoning steps. However, since the MAC network is designed for single-turn question answering, it is not capable of referring to past dialog turns. More specifically, it struggles with tasks that require reasoning over the dialog history, particularly coreference resolution. We extend the MAC network architecture with Context-aware Attention and Memory (CAM), which attends over control states in past dialog turns to determine the necessary reasoning operations for the current question. MAC nets with CAM achieve up to 98.25% accuracy on the CLEVR-Dialog dataset, beating the existing state-of-the-art by 30% (absolute). Our error analysis indicates that with CAM, the model&#8217;s performance particularly improved on questions that required coreference resolution.</abstract>
      <url hash="6697a871">2020.nlpbt-1.9</url>
      <doi>10.18653/v1/2020.nlpbt-1.9</doi>
      <video href="https://slideslive.com/38939783" />
      <bibkey>shah-etal-2020-reasoning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr-dialog">CLEVR-Dialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visdial">VisDial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
  </volume>
</collection>