<collection id="2021.repl4nlp">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)</booktitle>
      <editor><first>Anna</first><last>Rogers</last></editor>
      <editor><first>Iacer</first><last>Calixto</last></editor>
      <editor><first>Ivan</first><last>Vuli&#263;</last></editor>
      <editor><first>Naomi</first><last>Saphra</last></editor>
      <editor><first>Nora</first><last>Kassner</last></editor>
      <editor><first>Oana-Maria</first><last>Camburu</last></editor>
      <editor><first>Trapit</first><last>Bansal</last></editor>
      <editor><first>Vered</first><last>Shwartz</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="ee14b8fa">2021.repl4nlp-1</url>
    </meta>
    <frontmatter>
      <url hash="e0d00975">2021.repl4nlp-1.0</url>
      <bibkey>repl4nlp-2021-representation</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Improving Cross-lingual Text Classification with Zero-shot Instance-Weighting</title>
      <author><first>Irene</first><last>Li</last></author>
      <author><first>Prithviraj</first><last>Sen</last></author>
      <author><first>Huaiyu</first><last>Zhu</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>1&#8211;7</pages>
      <abstract>Cross-lingual text classification (CLTC) is a challenging task made even harder still due to the lack of labeled data in low-resource languages. In this paper, we propose zero-shot instance-weighting, a general model-agnostic zero-shot learning framework for improving CLTC by leveraging source instance weighting. It adds a module on top of pre-trained language models for similarity computation of instance weights, thus aligning each source instance to the target language. During training, the framework utilizes gradient descent that is weighted by instance weights to update parameters. We evaluate this framework over seven target languages on three fundamental tasks and show its effectiveness and extensibility, by improving on F1 score up to 4% in single-source transfer and 8% in multi-source transfer. To the best of our knowledge, our method is the first to apply instance weighting in zero-shot CLTC. It is simple yet effective and easily extensible into multi-source transfer.</abstract>
      <url hash="504832bc">2021.repl4nlp-1.1</url>
      <doi>10.18653/v1/2021.repl4nlp-1.1</doi>
      <bibkey>li-etal-2021-improving-cross</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mldoc">MLDoc</pwcdataset>
    </paper>
    <paper id="3">
      <title>Comprehension Based Question Answering using Bloom&#8217;s Taxonomy</title>
      <author><first>Pritish</first><last>Sahu</last></author>
      <author><first>Michael</first><last>Cogswell</last></author>
      <author><first>Ajay</first><last>Divakaran</last></author>
      <author><first>Sara</first><last>Rutherford-Quach</last></author>
      <pages>20&#8211;28</pages>
      <abstract>Current pre-trained language models have lots of knowledge, but a more limited ability to use that knowledge. Bloom&#8217;s Taxonomy helps educators teach children how to use knowledge by categorizing comprehension skills, so we use it to analyze and improve the comprehension skills of large pre-trained language models. Our experiments focus on zero-shot question answering, using the taxonomy to provide proximal context that helps the model answer questions by being relevant to those questions. We show targeting context in this manner improves performance across 4 popular common sense question answer datasets.</abstract>
      <url hash="11baef0f">2021.repl4nlp-1.3</url>
      <doi>10.18653/v1/2021.repl4nlp-1.3</doi>
      <bibkey>sahu-etal-2021-comprehension</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/social-iqa">Social IQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="5">
      <title>Learning Sparse Sentence Encoding without Supervision: An Exploration of Sparsity in Variational Autoencoders</title>
      <author><first>Victor</first><last>Prokhorov</last></author>
      <author><first>Yingzhen</first><last>Li</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>34&#8211;46</pages>
      <abstract>It has been long known that sparsity is an effective inductive bias for learning efficient representation of data in vectors with fixed dimensionality, and it has been explored in many areas of representation learning. Of particular interest to this work is the investigation of the sparsity within the VAE framework which has been explored a lot in the image domain, but has been lacking even a basic level of exploration in NLP. Additionally, NLP is also lagging behind in terms of learning sparse representations of large units of text e.g., sentences. We use the VAEs that induce sparse latent representations of large units of text to address the aforementioned shortcomings. First, we move in this direction by measuring the success of unsupervised state-of-the-art (SOTA) and other strong VAE-based sparsification baselines for text and propose a hierarchical sparse VAE model to address the stability issue of SOTA. Then, we look at the implications of sparsity on text classification across 3 datasets, and highlight a link between performance of sparse latent representations on downstream tasks and its ability to encode task-related information.</abstract>
      <url hash="8f51e476">2021.repl4nlp-1.5</url>
      <doi>10.18653/v1/2021.repl4nlp-1.5</doi>
      <bibkey>prokhorov-etal-2021-learning</bibkey>
      <pwccode url="https://github.com/VictorProkhorov/HSVAE" additional="false">VictorProkhorov/HSVAE</pwccode>
    </paper>
    <paper id="6">
      <title>Temporal-aware Language Representation Learning From Crowdsourced Labels</title>
      <author><first>Yang</first><last>Hao</last></author>
      <author><first>Xiao</first><last>Zhai</last></author>
      <author><first>Wenbiao</first><last>Ding</last></author>
      <author><first>Zitao</first><last>Liu</last></author>
      <pages>47&#8211;56</pages>
      <abstract>Learning effective language representations from crowdsourced labels is crucial for many real-world machine learning tasks. A challenging aspect of this problem is that the quality of crowdsourced labels suffer high intra- and inter-observer variability. Since the high-capacity deep neural networks can easily memorize all disagreements among crowdsourced labels, directly applying existing supervised language representation learning algorithms may yield suboptimal solutions. In this paper, we propose <i>TACMA</i>, a temporal-aware language representation learning heuristic for crowdsourced labels with multiple annotators. The proposed approach (1) explicitly models the intra-observer variability with attention mechanism; (2) computes and aggregates per-sample confidence scores from multiple workers to address the inter-observer disagreements. The proposed heuristic is extremely easy to implement in around 5 lines of code. The proposed heuristic is evaluated on four synthetic and four real-world data sets. The results show that our approach outperforms a wide range of state-of-the-art baselines in terms of prediction accuracy and AUC. To encourage the reproducible results, we make our code publicly available at <url>https://github.com/CrowdsourcingMining/TACMA</url>.</abstract>
      <url hash="4e51af9e">2021.repl4nlp-1.6</url>
      <doi>10.18653/v1/2021.repl4nlp-1.6</doi>
      <bibkey>hao-etal-2021-temporal</bibkey>
      <pwccode url="https://github.com/CrowdsourcingMining/TACMA" additional="false">CrowdsourcingMining/TACMA</pwccode>
    </paper>
    <paper id="12">
      <title>Knodle: Modular Weakly Supervised Learning with <fixed-case>P</fixed-case>y<fixed-case>T</fixed-case>orch</title>
      <author><first>Anastasiia</first><last>Sedova</last></author>
      <author><first>Andreas</first><last>Stephan</last></author>
      <author><first>Marina</first><last>Speranskaya</last></author>
      <author><first>Benjamin</first><last>Roth</last></author>
      <pages>100&#8211;111</pages>
      <abstract>Strategies for improving the training and prediction quality of weakly supervised machine learning models vary in how much they are tailored to a specific task or integrated with a specific model architecture. In this work, we introduce Knodle, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as separate, modular components. This modularization gives the training process access to fine-grained information such as data set characteristics, matches of heuristic rules, or elements of the deep learning model ultimately used for prediction. Hence, our framework can encompass a wide range of training methods for improving weak supervision, ranging from methods that only look at correlations of rules and output classes (independently of the machine learning model trained with the resulting labels), to those that harness the interplay of neural networks and weakly labeled data. We illustrate the benchmarking potential of the framework with a performance comparison of several reference implementations on a selection of datasets that are already available in Knodle.</abstract>
      <url hash="c9e270da">2021.repl4nlp-1.12</url>
      <doi>10.18653/v1/2021.repl4nlp-1.12</doi>
      <bibkey>sedova-etal-2021-knodle</bibkey>
      <pwccode url="https://github.com/knodle/knodle" additional="false">knodle/knodle</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sms-spam-collection-data-set">SMS Spam Collection Data Set</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="16">
      <title>Probing Cross-Modal Representations in Multi-Step Relational Reasoning</title>
      <author><first>Iuliia</first><last>Parfenova</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <author><first>Raquel</first><last>Fern&#225;ndez</last></author>
      <author><first>Sandro</first><last>Pezzelle</last></author>
      <pages>152&#8211;162</pages>
      <abstract>We investigate the representations learned by vision and language models in tasks that require relational reasoning. Focusing on the problem of assessing the relative size of objects in abstract visual contexts, we analyse both one-step and two-step reasoning. For the latter, we construct a new dataset of three-image scenes and define a task that requires reasoning at the level of the individual images and across images in a scene. We probe the learned model representations using diagnostic classifiers. Our experiments show that pretrained multimodal transformer-based architectures can perform higher-level relational reasoning, and are able to learn representations for novel tasks and data that are very different from what was seen in pretraining.</abstract>
      <url hash="2f2181c2">2021.repl4nlp-1.16</url>
      <doi>10.18653/v1/2021.repl4nlp-1.16</doi>
      <bibkey>parfenova-etal-2021-probing</bibkey>
      <pwccode url="https://github.com/jig-san/multi-step-size-reasoning" additional="false">jig-san/multi-step-size-reasoning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nlvr">NLVR</pwcdataset>
    </paper>
    <paper id="21">
      <title>Predicting the Success of Domain Adaptation in Text Similarity</title>
      <author><first>Nick</first><last>Pogrebnyakov</last></author>
      <author><first>Shohreh</first><last>Shaghaghian</last></author>
      <pages>206&#8211;212</pages>
      <abstract>Transfer learning methods, and in particular domain adaptation, help exploit labeled data in one domain to improve the performance of a certain task in another domain. However, it is still not clear what factors affect the success of domain adaptation. This paper models adaptation success and selection of the most suitable source domains among several candidates in text similarity. We use descriptive domain information and cross-domain similarity metrics as predictive features. While mostly positive, the results also point to some domains where adaptation success was difficult to predict.</abstract>
      <url hash="0ad1ab71">2021.repl4nlp-1.21</url>
      <doi>10.18653/v1/2021.repl4nlp-1.21</doi>
      <bibkey>pogrebnyakov-shaghaghian-2021-predicting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
    </paper>
    <paper id="26">
      <title>Deriving Contextualised Semantic Features from <fixed-case>BERT</fixed-case> (and Other Transformer Model) Embeddings</title>
      <author><first>Jacob</first><last>Turton</last></author>
      <author><first>Robert Elliott</first><last>Smith</last></author>
      <author><first>David</first><last>Vinson</last></author>
      <pages>248&#8211;262</pages>
      <abstract>Models based on the transformer architecture, such as BERT, have marked a crucial step forward in the field of Natural Language Processing. Importantly, they allow the creation of word embeddings that capture important semantic information about words in context. However, as single entities, these embeddings are difficult to interpret and the models used to create them have been described as opaque. Binder and colleagues proposed an intuitive embedding space where each dimension is based on one of 65 core semantic features. Unfortunately, the space only exists for a small data-set of 535 words, limiting its uses. Previous work (Utsumi, 2018, 2020; Turton et al., 2020) has shown that Binder features can be derived from static embeddings and successfully extrapolated to a large new vocabulary. Taking the next step, this paper demonstrates that Binder features can be derived from the BERT embedding space. This provides two things; (1) semantic feature values derived from contextualised word embeddings and (2) insights into how semantic features are represented across the different layers of the BERT model.</abstract>
      <url hash="eea0ed24">2021.repl4nlp-1.26</url>
      <doi>10.18653/v1/2021.repl4nlp-1.26</doi>
      <bibkey>turton-etal-2021-deriving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="29">
      <title>An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation</title>
      <author><first>Han</first><last>Guo</last></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>289&#8211;306</pages>
      <abstract>Recent advances in NLP systems, notably the pretraining-and-finetuning paradigm, have achieved great success in predictive accuracy. However, these systems are usually not well calibrated for uncertainty out-of-the-box. Many recalibration methods have been proposed in the literature for quantifying predictive uncertainty and calibrating model outputs, with varying degrees of complexity. In this work, we present a systematic study of a few of these methods. Focusing on the text classification task and finetuned large pretrained language models, we first show that many of the finetuned models are not well calibrated out-of-the-box, especially when the data come from out-of-domain settings. Next, we compare the effectiveness of a few widely-used recalibration methods (such as ensembles, temperature scaling). Then, we empirically illustrate a connection between distillation and calibration. We view distillation as a regularization term encouraging the student model to output uncertainties that match those of a teacher model. With this insight, we develop simple recalibration methods based on distillation with no additional inference-time cost. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches. Finally, we include ablations to understand the usefulness of components of our proposed method and examine the transferability of calibration via distillation.</abstract>
      <url hash="2dd50aa0">2021.repl4nlp-1.29</url>
      <doi>10.18653/v1/2021.repl4nlp-1.29</doi>
      <bibkey>guo-etal-2021-overview</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="32">
      <title>Direction is what you need: Improving Word Embedding Compression in Large Language Models</title>
      <author><first>Klaudia</first><last>Ba&#322;azy</last></author>
      <author><first>Mohammadreza</first><last>Banaei</last></author>
      <author><first>R&#233;mi</first><last>Lebret</last></author>
      <author><first>Jacek</first><last>Tabor</last></author>
      <author><first>Karl</first><last>Aberer</last></author>
      <pages>322&#8211;330</pages>
      <abstract>The adoption of Transformer-based models in natural language processing (NLP) has led to great success using a massive number of parameters. However, due to deployment constraints in edge devices, there has been a rising interest in the compression of these models to improve their inference time and memory footprint. This paper presents a novel loss objective to compress token embeddings in the Transformer-based models by leveraging an AutoEncoder architecture. More specifically, we emphasize the importance of the direction of compressed embeddings with respect to original uncompressed embeddings. The proposed method is task-agnostic and does not require further language modeling pre-training. Our method significantly outperforms the commonly used SVD-based matrix-factorization approach in terms of initial language model Perplexity. Moreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several downstream tasks from the GLUE benchmark, where we also outperform the baseline in most scenarios. Our code is public.</abstract>
      <url hash="400009e5">2021.repl4nlp-1.32</url>
      <doi>10.18653/v1/2021.repl4nlp-1.32</doi>
      <bibkey>balazy-etal-2021-direction</bibkey>
      <revision id="1" href="2021.repl4nlp-1.32v1" hash="445056a2" />
      <revision id="2" href="2021.repl4nlp-1.32v2" hash="400009e5" date="2021-08-12">Added a sponsor</revision>
      <pwccode url="https://github.com/MohammadrezaBanaei/orientation_based_embedding_compression" additional="false">MohammadrezaBanaei/orientation_based_embedding_compression</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
  </volume>
</collection>