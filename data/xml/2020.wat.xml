<collection id="2020.wat">
  <volume id="1" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the 7th Workshop on Asian Translation</booktitle>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Hideki</first><last>Nakayama</last></editor>
      <editor><first>Chenchen</first><last>Ding</last></editor>
      <editor><first>Raj</first><last>Dabre</last></editor>
      <editor><first>Anoop</first><last>Kunchukuttan</last></editor>
      <editor><first>Win Pa</first><last>Pa</last></editor>
      <editor><first>Ond&#345;ej</first><last>Bojar</last></editor>
      <editor><first>Shantipriya</first><last>Parida</last></editor>
      <editor><first>Isao</first><last>Goto</last></editor>
      <editor><first>Hidaya</first><last>Mino</last></editor>
      <editor><first>Hiroshi</first><last>Manabe</last></editor>
      <editor><first>Katsuhito</first><last>Sudoh</last></editor>
      <editor><first>Sadao</first><last>Kurohashi</last></editor>
      <editor><first>Pushpak</first><last>Bhattacharyya</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="e4d5969c">2020.wat-1.0</url>
      <bibkey>wat-2020-asian</bibkey>
    </frontmatter>
    <paper id="5">
      <title>Meta Ensemble for <fixed-case>J</fixed-case>apanese-<fixed-case>C</fixed-case>hinese Neural Machine Translation: <fixed-case>K</fixed-case>yoto-<fixed-case>U</fixed-case>+<fixed-case>ECNU</fixed-case> Participation to <fixed-case>WAT</fixed-case> 2020</title>
      <author><first>Zhuoyuan</first><last>Mao</last></author>
      <author><first>Yibin</first><last>Shen</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <author><first>Cheqing</first><last>Jin</last></author>
      <pages>64&#8211;71</pages>
      <abstract>This paper describes the Japanese-Chinese Neural Machine Translation (NMT) system submitted by the joint team of Kyoto University and East China Normal University (Kyoto-U+ECNU) to WAT 2020 (Nakazawa et al.,2020). We participate in APSEC Japanese-Chinese translation task. We revisit several techniques for NMT including various architectures, different data selection and augmentation methods, denoising pre-training, and also some specific tricks for Japanese-Chinese translation. We eventually perform a meta ensemble to combine all of the models into a single model. BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese-Chinese translation.</abstract>
      <url hash="e7252315">2020.wat-1.5</url>
      <bibkey>mao-etal-2020-meta</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="8">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>&#8217;s Participation in the <fixed-case>WAT</fixed-case> 2020 Indic Languages Multilingual Task</title>
      <author><first>Zhengzhe</first><last>Yu</last></author>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Xiaoyu</first><last>Chen</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <pages>92&#8211;97</pages>
      <abstract>This paper describes our work in the WAT 2020 Indic Multilingual Translation Task. We participated in all 7 language pairs (En&lt;-&gt;Bn/Hi/Gu/Ml/Mr/Ta/Te) in both directions under the constrained condition&#8212;using only the officially provided data. Using transformer as a baseline, our Multi-&gt;En and En-&gt;Multi translation systems achieve the best performances. Detailed data filtering and data domain selection are the keys to performance enhancement in our experiment, with an average improvement of 2.6 BLEU scores for each language pair in the En-&gt;Multi system and an average improvement of 4.6 BLEU scores regarding the Multi-&gt;En. In addition, we employed language independent adapter to further improve the system performances. Our submission obtains competitive results in the final evaluation.</abstract>
      <url hash="3d3cf08c">2020.wat-1.8</url>
      <bibkey>yu-etal-2020-hw</bibkey>
    </paper>
    <paper id="11">
      <title>Multimodal Neural Machine Translation for <fixed-case>E</fixed-case>nglish to <fixed-case>H</fixed-case>indi</title>
      <author><first>Sahinur Rahman</first><last>Laskar</last></author>
      <author><first>Abdullah Faiz Ur Rahman</first><last>Khilji</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>109&#8211;113</pages>
      <abstract>Machine translation (MT) focuses on the automatic translation of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-the-art results in the task of machine translation because of utilizing advanced deep learning techniques and handles issues like long-term dependency, and context-analysis. Nevertheless, NMT still suffers low translation quality for low resource languages. To encounter this challenge, the multi-modal concept comes in. The multi-modal concept combines textual and visual features to improve the translation quality of low resource languages. Moreover, the utilization of monolingual data in the pre-training step can improve the performance of the system for low resource language translations. Workshop on Asian Translation 2020 (WAT2020) organized a translation task for multimodal translation in English to Hindi. We have participated in the same in two-track submission, namely text-only and multi-modal translation with team name CNLP-NITS. The evaluated results are declared at the WAT2020 translation task, which reports that our multi-modal NMT system attained higher scores than our text-only NMT on both challenge and evaluation test set. For the challenge test data, our multi-modal neural machine translation system achieves Bilingual Evaluation Understudy (BLEU) score of 33.57, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.754141, Adequacy-Fluency Metrics (AMFM) score 0.787320 and for evaluation test data, BLEU, RIBES, and, AMFM score of 40.51, 0.803208, and 0.820980 for English to Hindi translation respectively.</abstract>
      <url hash="aa3f66e3">2020.wat-1.11</url>
      <bibkey>laskar-etal-2020-multimodal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hindi-visual-genome">Hindi Visual Genome</pwcdataset>
    </paper>
    <paper id="14">
      <title><fixed-case>WT</fixed-case>: Wipro <fixed-case>AI</fixed-case> Submissions to the <fixed-case>WAT</fixed-case> 2020</title>
      <author><first>Santanu</first><last>Pal</last></author>
      <pages>122&#8211;126</pages>
      <abstract>In this paper we present an English&#8211;Hindi and Hindi&#8211;English neural machine translation (NMT) system, submitted to the Translation shared Task organized at WAT 2020. We trained a multilingual NMT system based on transformer architecture. In this paper we show: (i) how effective pre-processing helps to improve performance, (ii) how synthetic data through back-translation from available monolingual data can help in overall translation performance, (iii) how language similarity can aid more onto it. Our submissions ranked 1st in both English to Hindi and Hindi to English translation achieving BLEU 20.80 and 29.59 respectively.</abstract>
      <url hash="d5c0308a">2020.wat-1.14</url>
      <bibkey>pal-2020-wt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="17">
      <title>The <fixed-case>ADAPT</fixed-case> Centre&#8217;s Neural <fixed-case>MT</fixed-case> Systems for the <fixed-case>WAT</fixed-case> 2020 Document-Level Translation Task</title>
      <author><first>Wandri</first><last>Jooste</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>142&#8211;146</pages>
      <abstract>In this paper we describe the ADAPT Centre&#8217;s submissions to the WAT 2020 document-level Business Scene Dialogue (BSD) Translation task. We only consider translating from Japanese to English for this task and we use the MarianNMT toolkit to train Transformer models. In order to improve the translation quality, we made use of both in-domain and out-of-domain data for training our Machine Translation (MT) systems, as well as various data augmentation techniques for fine-tuning the model parameters. This paper outlines the experiments we ran to train our systems and report the accuracy achieved through these various experiments.</abstract>
      <url hash="5ac897bd">2020.wat-1.17</url>
      <bibkey>jooste-etal-2020-adapt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/business-scene-dialogue">Business Scene Dialogue</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="19">
      <title>Improving <fixed-case>NMT</fixed-case> via Filtered Back Translation</title>
      <author><first>Nikhil</first><last>Jaiswal</last></author>
      <author><first>Mayur</first><last>Patidar</last></author>
      <author><first>Surabhi</first><last>Kumari</last></author>
      <author><first>Manasi</first><last>Patwardhan</last></author>
      <author><first>Shirish</first><last>Karande</last></author>
      <author><first>Puneet</first><last>Agarwal</last></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <pages>154&#8211;159</pages>
      <abstract>Document-Level Machine Translation (MT) has become an active research area among the NLP community in recent years. Unlike sentence-level MT, which translates the sentences independently, document-level MT aims to utilize contextual information while translating a given source sentence. This paper demonstrates our submission (Team ID - DEEPNLP) to the Document-Level Translation task organized by WAT 2020. This task focuses on translating texts from a business dialog corpus while optionally utilizing the context present in the dialog. In our proposed approach, we utilize publicly available parallel corpus from different domains to train an open domain base NMT model. We then use monolingual target data to create filtered pseudo parallel data and employ Back-Translation to fine-tune the base model. This is further followed by fine-tuning on the domain-specific corpus. We also ensemble various models to improvise the translation performance. Our best models achieve a BLEU score of 26.59 and 22.83 in an unconstrained setting and 15.10 and 10.91 in the constrained settings for En-&gt;Ja &amp; Ja-&gt;En direction, respectively.</abstract>
      <url hash="29250156">2020.wat-1.19</url>
      <bibkey>jaiswal-etal-2020-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="20">
      <title>A Parallel Evaluation Data Set of Software Documentation with Document Structure Annotation</title>
      <author><first>Bianka</first><last>Buschbeck</last></author>
      <author><first>Miriam</first><last>Exel</last></author>
      <pages>160&#8211;169</pages>
      <abstract>This paper accompanies the software documentation data set for machine translation, a parallel evaluation data set of data originating from the SAP Help Portal, that we released to the machine translation community for research purposes. It offers the possibility to tune and evaluate machine translation systems in the domain of corporate software documentation and contributes to the availability of a wider range of evaluation scenarios. The data set comprises of the language pairs English to Hindi, Indonesian, Malay and Thai, and thus also increases the test coverage for the many low-resource language pairs. Unlike most evaluation data sets that consist of plain parallel text, the segments in this data set come with additional metadata that describes structural information of the document context. We provide insights into the origin and creation, the particularities and characteristics of the data set as well as machine translation results.</abstract>
      <url hash="9f2ed0b7">2020.wat-1.20</url>
      <bibkey>buschbeck-exel-2020-parallel</bibkey>
      <pwccode url="https://github.com/SAP/software-documentation-data-set-for-machine-translation" additional="false">SAP/software-documentation-data-set-for-machine-translation</pwccode>
    </paper>
    </volume>
</collection>