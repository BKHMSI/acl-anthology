<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.wat">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</booktitle>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Hideki</first><last>Nakayama</last></editor>
      <editor><first>Isao</first><last>Goto</last></editor>
      <editor><first>Hideya</first><last>Mino</last></editor>
      <editor><first>Chenchen</first><last>Ding</last></editor>
      <editor><first>Raj</first><last>Dabre</last></editor>
      <editor><first>Anoop</first><last>Kunchukuttan</last></editor>
      <editor><first>Shohei</first><last>Higashiyama</last></editor>
      <editor><first>Hiroshi</first><last>Manabe</last></editor>
      <editor><first>Win Pa</first><last>Pa</last></editor>
      <editor><first>Shantipriya</first><last>Parida</last></editor>
      <editor><first>Ondřej</first><last>Bojar</last></editor>
      <editor><first>Chenhui</first><last>Chu</last></editor>
      <editor><first>Akiko</first><last>Eriguchi</last></editor>
      <editor><first>Kaori</first><last>Abe</last></editor>
      <editor><first>Yusuke</first><last>Oda</last></editor>
      <editor><first>Katsuhito</first><last>Sudoh</last></editor>
      <editor><first>Sadao</first><last>Kurohashi</last></editor>
      <editor><first>Pushpak</first><last>Bhattacharyya</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="949b3b17">2021.wat-1</url>
    </meta>
    <frontmatter>
      <url hash="659551df">2021.wat-1.0</url>
      <bibkey>wat-2021-asian</bibkey>
    </frontmatter>
    <paper id="4">
      <title>NICT’s Neural Machine Translation Systems for the WAT21 Restricted Translation Task<fixed-case>NICT</fixed-case>’s Neural Machine Translation Systems for the <fixed-case>WAT</fixed-case>21 Restricted Translation Task</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>62–67</pages>
      <abstract>This paper describes our <a href="https://en.wikipedia.org/wiki/System">system</a> (Team ID : nictrb) for participating in the WAT’21 restricted machine translation task. In our submitted <a href="https://en.wikipedia.org/wiki/System">system</a>, we designed a new <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training approach</a> for restricted machine translation. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary. With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, as well as <a href="https://en.wikipedia.org/wiki/Mathematical_model">model ensembling</a>, which further improved the final translation performance.</abstract>
      <url hash="d801187b">2021.wat-1.4</url>
      <doi>10.18653/v1/2021.wat-1.4</doi>
      <bibkey>li-etal-2021-nicts</bibkey>
    </paper>
    <paper id="6">
      <title>NECTEC’s Participation in WAT-2021<fixed-case>NECTEC</fixed-case>’s Participation in <fixed-case>WAT</fixed-case>-2021</title>
      <author><first>Zar Zar</first><last>Hlaing</last></author>
      <author><first>Ye Kyaw</first><last>Thu</last></author>
      <author><first>Thazin</first><last>Myint Oo</last></author>
      <author><first>Mya</first><last>Ei San</last></author>
      <author><first>Sasiporn</first><last>Usanavasin</last></author>
      <author><first>Ponrudee</first><last>Netisopakul</last></author>
      <author><first>Thepchai</first><last>Supnithi</last></author>
      <pages>74–82</pages>
      <abstract>In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021. Basically, our models are based on <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural methods</a> for both directions of English-Myanmar and Myanmar-English language pairs. Most of the existing Neural Machine Translation (NMT) models mainly focus on the conversion of sequential data and do not directly use syntactic information. However, we conduct multi-source neural machine translation (NMT) models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus. The multi-source translation is an approach to exploit multiple inputs (e.g. in two different formats) to increase translation accuracy. The RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment. The experimental results showed that the proposed models of RNN-based architecture outperform the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline model</a> for English-to-Myanmar translation task, and the multi-source and shared-multi-source transformer models yield better translation results than the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>.</abstract>
      <url hash="70ed53ac">2021.wat-1.6</url>
      <doi>10.18653/v1/2021.wat-1.6</doi>
      <bibkey>hlaing-etal-2021-nectecs</bibkey>
    </paper>
    <paper id="11">
      <title>Zero-pronoun Data Augmentation for Japanese-to-English Translation<fixed-case>J</fixed-case>apanese-to-<fixed-case>E</fixed-case>nglish Translation</title>
      <author><first>Ryokan</first><last>Ri</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Yoshimasa</first><last>Tsuruoka</last></author>
      <pages>117–123</pages>
      <abstract>For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding <a href="https://en.wikipedia.org/wiki/Pronoun">pronoun</a> in the target side of the English sentence. However, although fully resolving zero pronouns often needs <a href="https://en.wikipedia.org/wiki/Context_(language_use)">discourse context</a>, in some cases, the local context within a sentence gives clues to the inference of the zero pronoun. In this study, we propose a data augmentation method that provides additional training signals for the translation model to learn correlations between <a href="https://en.wikipedia.org/wiki/Context_(language_use)">local context</a> and zero pronouns. We show that the proposed method significantly improves the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of zero pronoun translation with <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> experiments in the conversational domain.</abstract>
      <url hash="a0e5659f">2021.wat-1.11</url>
      <doi>10.18653/v1/2021.wat-1.11</doi>
      <bibkey>ri-etal-2021-zero</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/business-scene-dialogue">Business Scene Dialogue</pwcdataset>
    </paper>
    <paper id="13">
      <title>TMU NMT System with Japanese BART for the Patent task of WAT 2021<fixed-case>TMU</fixed-case> <fixed-case>NMT</fixed-case> System with <fixed-case>J</fixed-case>apanese <fixed-case>BART</fixed-case> for the Patent task of <fixed-case>WAT</fixed-case> 2021</title>
      <author><first>Hwichan</first><last>Kim</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>133–137</pages>
      <abstract>In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. One of the pre-trained models, BART (Lewis et al., 2020), was shown to improve translation accuracy via fine-tuning with bilingual data. However, they experimented only Romanian!English translation using <a href="https://en.wikipedia.org/wiki/BART">English BART</a>. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</abstract>
      <url hash="3aba8b3d">2021.wat-1.13</url>
      <doi>10.18653/v1/2021.wat-1.13</doi>
      <bibkey>kim-komachi-2021-tmu</bibkey>
    <title_ar>نظام TMU NMT مع BART الياباني لمهمة براءة اختراع WAT 2021</title_ar>
      <title_pt>Sistema TMU NMT com BART japonês para a tarefa de patente do WAT 2021</title_pt>
      <title_fr>Système TMU NMT avec BART japonais pour la tâche de brevet du WAT 2021</title_fr>
      <title_es>Sistema TMU NMT con BART japonés para la tarea de patentes de WAT 2021</title_es>
      <title_ja>WAT 2021の特許業務のための日本のBARTを備えたTMU NMTシステム</title_ja>
      <title_zh>TMU NMT系统与日本BART同成WAT 2021专利</title_zh>
      <title_hi>WAT 2021 के पेटेंट कार्य के लिए जापानी BART के साथ TMU NMT सिस्टम</title_hi>
      <title_ru>Система TMU NMT с японским BART для патентной задачи WAT 2021</title_ru>
      <title_ga>Córas TMU NMT le BART na Seapáine le haghaidh tasc Paitinne WAT 2021</title_ga>
      <title_el>Σύστημα TMU NMT με ιαπωνικό BART για το έργο ευρεσιτεχνίας του WAT 2021</title_el>
      <title_ka>TMU NMT სისტემა იაპონური BART-ის პოტენტის დავალებისთვის WAT 2021</title_ka>
      <title_hu>TMU NMT rendszer japán BART-vel a WAT 2021 szabadalmi feladatához</title_hu>
      <title_kk>WAT 2021 патенттік тапсырманың жапон BART жүйесі TMU NMT жүйесі</title_kk>
      <title_lt>TMU NMT sistema su Japonijos BART, skirta 2021 m. WAT patentų uždaviniui atlikti</title_lt>
      <title_it>TMU NMT System con BART giapponese per il compito di brevetto di WAT 2021</title_it>
      <title_mt>Sistema TMU NMT b’BART Ġappuniż għall-kompitu tal-Privattivi tal-WAT 2021</title_mt>
      <title_mn>TMU NMT System with Japanese BART for the Patent task of WAT 2021</title_mn>
      <title_mk>TMU NMT систем со јапонски BART за задачата на патентот на WAT 2021</title_mk>
      <title_ms>Sistem NMT TMU dengan BART Jepun untuk tugas Paten WAT 2021</title_ms>
      <title_no>TMU NMT- systemet med japansk BART for patentoppgåva av WAT 2021</title_no>
      <title_ml>ജാപ്പനീസ് ബാര്‍ട്ടിനുള്ള ടിഎംഎംടി സിസ്റ്റം വാട്ട് 2021-ന്റെ പാതിന്റ് ജോലി</title_ml>
      <title_ro>Sistemul TMU NMT cu BART japonez pentru sarcina de brevetare a WAT 2021</title_ro>
      <title_sr>TMU NMT sistem sa japanskim BART za patentni zadatak WAT 2021.</title_sr>
      <title_pl>System TMU NMT z japońskim BART do zadania patentowego WAT 2021</title_pl>
      <title_si>TMU NMT පද්ධතිය ජාපානි BART එක්ක WAT 2021 ගේ පැටෙන්ට් වැඩේ වෙනුවෙන්.</title_si>
      <title_so>TMU NMT System with Japanese BART for the Patent task of WAT 2021</title_so>
      <title_sv>TMU NMT System med japansk BART för patentuppgiften för WAT 2021</title_sv>
      <title_ta>ஜாப்பானிய BART உடன் TMU NMT அமைப்பு</title_ta>
      <title_ur>WAT 2021 کے پیٹینٹ کام کے لئے جاپانی BART کے ساتھ TMU NMT سیسٹم</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Giao thông TMU NMT với BART Nhật Bản cho công việc sáng chế của WAT 2021</title_vi>
      <title_da>TMU NMT System med japansk BART til patentopgaven af WAT 2021</title_da>
      <title_bg>ТМУ НМТ Система с японски БАРТ за патентната задача на ВАТ 2021</title_bg>
      <title_de>TMU NMT System mit japanischem BART für die Patentaufgabe von WAT 2021</title_de>
      <title_id>Sistem TMU NMT dengan BART Jepang untuk tugas Paten dari WAT 2021</title_id>
      <title_ko>TMU NMT 시스템은 일본 BART와 협력하여 WAT 2021 특허 작업 수행</title_ko>
      <title_fa>سیستم TMU NMT با BART ژاپنی برای کار patent از WAT 2021</title_fa>
      <title_nl>TMU NMT Systeem met Japanse BART voor de Octrooitaak van WAT 2021</title_nl>
      <title_hr>TMU NMT sustav sa japanskim BART za patentni zadatak WAT 2021.</title_hr>
      <title_tr>Japonça BART bilen TMU NMT sistemi WAT 2021'iň Patent işi üçin</title_tr>
      <title_sq>TMU NMT System with Japanese BART for the Patent task of WAT 2021</title_sq>
      <title_af>TMU NMT Stelsel met Japaanse BART vir die Patent taak van WAT 2021</title_af>
      <title_am>ከጃፓንኛ BART ጋር TMU NMT ስርዓት ለWAT 2021 ለPatent ስራ</title_am>
      <title_hy>ԹՄՄ ՆՄԹ համակարգը ճապոնական Բարթ-ով, որն օգտագործում է պայթենտի աշխատանքի համար</title_hy>
      <title_az>TMU NMT System with Japanese BART for the Patent task of WAT 2021</title_az>
      <title_bn>ওয়াট ২০১২ সালের প্যাটেন্ট কাজের জন্য জাপানী বার্টের সাথে TMU NMT সিস্টেম</title_bn>
      <title_bs>TMU NMT sustav sa japanskim BART za patentni zadatak WAT 2021.</title_bs>
      <title_sw>Mfumo wa TMU NMT na BART wa Japani kwa ajili ya kazi ya Patients ya WAT 2021</title_sw>
      <title_ca>TMU NMT System with Japanese BART for the Patent task of WAT 2021</title_ca>
      <title_et>TMU NMT süsteem Jaapani BARTiga WAT 2021 patendiülesandeks</title_et>
      <title_cs>TMU NMT systém s japonským BART pro patentový úkol WAT 2021</title_cs>
      <title_fi>TMU NMT -järjestelmä japanilaisen BART:n kanssa WAT 2021 -patenttitehtävään</title_fi>
      <title_jv>MU NMT System karo BaRT japongan kanggo nggawe patent kanggo WAT 2020 1</title_jv>
      <title_sk>TMU NMT System z japonskim BART za patentno nalogo WAT 2021</title_sk>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_he>מערכת TMU NMT עם BART יפני למשימת הפטנטים של WAT 2021</title_he>
      <title_bo>TMU NMT མ་ལག་གི་རྒྱ་ནག་གི་BART སྤྱད་ནས་ WAT 2021 ཡི་patent task of the WAT 2021</title_bo>
      <abstract_ar>في هذه الورقة ، نقدم نظام TMU Neural Machine Translation (NMT) المقدم لمهمة براءات الاختراع (الكورية اليابانية والإنجليزية اليابانية) في ورشة العمل الثامنة حول الترجمة الآسيوية (ناكازاوا وآخرون ، 2021). في الآونة الأخيرة ، اقترحت العديد من الدراسات نماذج مدربة مسبقًا لوحدات فك التشفير باستخدام بيانات أحادية اللغة. تم عرض أحد النماذج المدربة مسبقًا ، BART (Lewis et al. ، 2020) ، لتحسين دقة الترجمة من خلال الضبط الدقيق للبيانات ثنائية اللغة. ومع ذلك ، فقد جربوا الترجمة الرومانية فقط الإنجليزية باستخدام BART الإنجليزية. في هذه الورقة ، قمنا بفحص فعالية BART اليابانية باستخدام مجموعة مكتب براءات الاختراع اليابانية 2.0. تشير تجاربنا إلى أن تقنية BART اليابانية يمكنها أيضًا تحسين دقة الترجمة في الترجمات اليابانية الكورية واليابانية والإنجليزية.</abstract_ar>
      <abstract_fr>Dans cet article, nous présentons notre système de traduction automatique neuronale TMU (NMT) soumis pour la tâche de brevet (coréen japonais et anglais japonais) du 8e atelier sur la traduction asiatique (Nakazawa et al., 2021). Récemment, plusieurs études ont proposé des modèles encodeur-décodeur pré-entraînés utilisant des données monolingues. Il a été démontré que l'un des modèles pré-entraînés, BART (Lewis et al., 2020), améliorait la précision de la traduction grâce à un réglage précis avec des données bilingues. Cependant, ils n'ont expérimenté que le roumain ! Traduction en anglais en utilisant le BART anglais. Dans cet article, nous examinons l'efficacité du BART japonais en utilisant le Corpus 2.0 de l'Office des brevets du Japon. Nos expériences indiquent que le BART japonais peut également améliorer la précision des traductions en coréen japonais et en anglais japonais.</abstract_fr>
      <abstract_es>En este artículo, presentamos nuestro sistema de traducción automática neuronal (NMT) TMU presentado para la tarea de patentes (coreano, japonés e inglés japonés) del octavo taller de traducción asiática (Nakazawa et al., 2021). Recientemente, varios estudios propusieron modelos de codificador-decodificador previamente entrenados que utilizan datos monolingües. Se demostró que uno de los modelos previamente entrenados, BART (Lewis et al., 2020), mejora la precisión de la traducción mediante el ajuste fino con datos bilingües. Sin embargo, ¡solo experimentaron rumano! Traducción al inglés con BART en inglés. En este artículo, examinamos la eficacia del BART japonés utilizando el Corpus 2.0 de la Oficina de Patentes de Japón. Nuestros experimentos indican que el BART japonés también puede mejorar la precisión de la traducción en las traducciones de coreano, japonés e inglés japonés.</abstract_es>
      <abstract_pt>Neste artigo, apresentamos nosso sistema TMU Neural Machine Translation (NMT) enviado para a tarefa de Patentes (Japonês Coreano e Japonês Inglês) do 8º Workshop sobre Tradução Asiática (Nakazawa et al., 2021). Recentemente, vários estudos propuseram modelos codificadores-decodificadores pré-treinados usando dados monolíngues. Um dos modelos pré-treinados, o BART (Lewis et al., 2020), demonstrou melhorar a precisão da tradução por meio do ajuste fino com dados bilíngues. No entanto, eles experimentaram apenas a tradução romeno!inglês usando o inglês BART. Neste artigo, examinamos a eficácia do BART japonês usando o Japan Patent Office Corpus 2.0. Nossos experimentos indicam que o BART japonês também pode melhorar a precisão das traduções em coreano japonês e inglês japonês.</abstract_pt>
      <abstract_hi>इस पेपर में, हम एशियाई अनुवाद पर 8 वीं कार्यशाला के पेटेंट कार्य (कोरियाई जापानी और अंग्रेजी जापानी) के लिए प्रस्तुत हमारे टीएमयू न्यूरल मशीन ट्रांसलेशन (एनएमटी) सिस्टम को पेश करते हैं (नाकाज़ावा एट अल। हाल ही में, कई अध्ययनों ने मोनोलिंगुअल डेटा का उपयोग करके पूर्व-प्रशिक्षित एन्कोडर-डिकोडर मॉडल का प्रस्ताव दिया। पूर्व-प्रशिक्षित मॉडलों में से एक, BART (लुईस एट अल, 2020), द्विभाषी डेटा के साथ ठीक-ट्यूनिंग के माध्यम से अनुवाद सटीकता में सुधार करने के लिए दिखाया गया था। हालांकि, उन्होंने केवल रोमानियाई प्रयोग किया! अंग्रेजी BART का उपयोग करके अंग्रेजी अनुवाद। इस पेपर में, हम जापान पेटेंट ऑफिस कॉर्पस 2.0 का उपयोग करके जापानी BART की प्रभावशीलता की जांच करते हैं। हमारे प्रयोगों से संकेत मिलता है कि जापानी BART भी कोरियाई जापानी और अंग्रेजी जापानी अनुवाद दोनों में अनुवाद सटीकता में सुधार कर सकता है।</abstract_hi>
      <abstract_ja>本稿では、第8回アジア翻訳ワークショップ（ Nakazawa et al., 2021 ）の特許業務（韓国語、日本語、英語、日本語）のために提出されたTMU神経機械翻訳（ NMT ）システムについて紹介する。最近、いくつかの研究では、モノリンガルデータを使用した事前に訓練されたエンコーダデコーダモデルが提案されています。事前に訓練されたモデルの一つであるBART （ Lewis et al., 2020 ）は、バイリンガルデータを用いた微調整を介して翻訳精度を向上させることが示された。しかし、彼らはルーマニア語のみを実験した！英語BARTを用いた英訳。本稿では、日本特許庁コーパス2.0を用いた日本語BARTの有効性について検討する。私たちの実験では、日本語のBARTは韓国語の日本語訳と英語の日本語訳の両方で翻訳の精度を向上させることもできることが示されています。</abstract_ja>
      <abstract_zh>本文引为第8届亚洲译研讨会专利(韩语日语与英语日语)交TMU神经机器翻译(NMT)系统(Nakazawa等,2021)。 近者,几项讲求用单语数预训练编码器 - 解码器模形。 其一先训者 BART(Lewis 等,2020 年)证可因双语数以重译准确性。 但试之罗马尼亚语! 用英语 BART 英语翻译。 本文,究日本专利局用日本专利局语料库2.0日本BART有效性。 吾实验之明,日语BART可以崇韩语日语英语日语翻译之准确性。</abstract_zh>
      <abstract_ru>В этой статье мы представляем нашу систему нейронного машинного перевода (НМП) TMU, представленную для патентной задачи (корейский японский и английский японский) 8-го семинара по азиатскому переводу (Nakazawa et al., 2021). Недавно в нескольких исследованиях были предложены предварительно обученные модели кодировщик-декодер с использованием одноязычных данных. Было показано, что одна из предварительно обученных моделей, BART (Lewis et al., 2020), повышает точность перевода за счет точной настройки с двуязычными данными. Однако они экспериментировали только на румынском языке!Английский перевод с использованием английского языка BART. В этой статье мы исследуем эффективность японского языка BART с использованием японского патентного ведомства Corpus 2.0. Наши эксперименты показывают, что японский БАРТ также может улучшить точность перевода как в корейском японском, так и в английском японском переводе.</abstract_ru>
      <abstract_ga>Sa pháipéar seo, tugaimid isteach ár gcóras TMU Neural Machine Translation (NMT) a cuireadh isteach don tasc Paitinne (Seapáinis na Cóiré agus Béarla na Seapáine) den 8ú Ceardlann ar Aistriúchán na hÁise (Nakazawa et al., 2021). Le déanaí, mhol roinnt staidéar samhlacha réamh-oilte ionchódóra-díchódóra ag baint úsáide as sonraí aonteangacha. Léiríodh go bhfuil ceann de na samhlacha réamhoilte, BART (Lewis et al., 2020), chun cruinneas an aistriúcháin a fheabhsú trí mhionchoigeartú a dhéanamh ar shonraí dátheangacha. Mar sin féin, rinne siad tástáil ar aistriúchán Béarla Rómáinis amháin ag baint úsáide as Béarla BART. Sa pháipéar seo, scrúdaímid éifeachtacht BART na Seapáine ag baint úsáide as Japan Paitinn Office Corpus 2.0. Léiríonn ár dturgnaimh gur féidir le BART na Seapáine feabhas a chur ar chruinneas aistriúcháin in aistriúcháin Cóiréis Seapánacha agus Béarla araon.</abstract_ga>
      <abstract_el>Στην παρούσα εργασία, παρουσιάζουμε το σύστημα Νευρικής Μηχανικής Μετάφρασης (NMT) που υποβλήθηκε για το έργο Διπλώματος ευρεσιτεχνίας (Κορεατικά Ιαπωνικά και Αγγλικά Ιαπωνικά) του 8ου Εργαστηρίου Ασιατικής Μετάφρασης (Νακαζάβα κ.α., 2021). Πρόσφατα, αρκετές μελέτες πρότειναν προ-εκπαιδευμένα μοντέλα κωδικοποιητών-αποκωδικοποιητών χρησιμοποιώντας μονογλωσσικά δεδομένα. Ένα από τα προ-εκπαιδευμένα μοντέλα, το BART (Lewis et al., 2020), αποδείχθηκε ότι βελτιώνει την ακρίβεια της μετάφρασης μέσω της τελειοποίησης με δίγλωσσα δεδομένα. Ωστόσο, πειραματίστηκαν μόνο ρουμανικά! Αγγλική μετάφραση χρησιμοποιώντας την αγγλική BART. Σε αυτή την εργασία, εξετάζουμε την αποτελεσματικότητα του ιαπωνικού BART χρησιμοποιώντας το Ιαπωνικό Γραφείο Διπλωμάτων Ευρεσιτεχνίας Corpus 2.0. Τα πειράματά μας δείχνουν ότι τα ιαπωνικά μπορούν επίσης να βελτιώσουν την ακρίβεια της μετάφρασης τόσο σε κορεατικές ιαπωνικές όσο και σε αγγλικές ιαπωνικές μεταφράσεις.</abstract_el>
      <abstract_it>In questo articolo presentiamo il nostro sistema di traduzione automatica neurale TMU (NMT) presentato per il compito di brevetto (coreano giapponese e inglese giapponese) dell'8° Workshop sulla traduzione asiatica (Nakazawa et al., 2021). Recentemente, diversi studi hanno proposto modelli di encoder-decoder pre-addestrati utilizzando dati monolingue. Uno dei modelli pre-addestrati, BART (Lewis et al., 2020), ha dimostrato di migliorare l'accuratezza della traduzione tramite la messa a punto con i dati bilingui. Tuttavia, hanno sperimentato solo rumeno! Traduzione inglese usando l'inglese BART. In questo articolo esaminiamo l'efficacia del BART giapponese utilizzando Japan Patent Office Corpus 2.0. I nostri esperimenti indicano che il giapponese BART può anche migliorare l'accuratezza della traduzione sia in giapponese coreano che in giapponese inglese.</abstract_it>
      <abstract_hu>Ebben a tanulmányban bemutatjuk a TMU Neural Machine Translation (NMT) rendszerünket, amelyet az Ázsiai Fordításról szóló 8. Workshop on Asian Translation (Nakazawa et al., 2021) szabadalmi feladatára benyújtottak (koreai japán és angol japán). A közelmúltban több tanulmány előre képzett útmérő-dekódoló modelleket javasolt egynyelvű adatok felhasználásával. Az egyik előre képzett modell, a BART (Lewis et al., 2020) kimutatták, hogy a kétnyelvű adatok finomhangolásával javítja a fordítási pontosságot. De csak románul kísérleteztek! Angol fordítás angol BART használatával. Ebben a tanulmányban a japán BART hatékonyságát vizsgáljuk a Japan Patent Office Corpus 2.0 alkalmazással. Kísérleteink azt mutatják, hogy a japán BART a koreai japán és angol japán fordítások pontosságát is javíthatja.</abstract_hu>
      <abstract_lt>Šiame dokumente pristatome mūsų TMU neurologinių mašinų vertimo (NMT) sistemą, pateiktą 8-ojo seminaro „Azijos vertimas“ (Nakazawa ir kt., 2021 m.). Neseniai keliuose tyrimuose siūlomi iš anksto parengti kodavimo kodavimo modeliai, naudojantys vienkalbius duomenis. Įrodyta, kad vienas iš iš iš anksto apmokytų modelių, BART (Lewis et al., 2020), gerina vertimo tikslumą tiksliau derinant su dvikalbiais duomenimis. Tačiau jie eksperimentavo tik rumunų! English translation using English BART.  Šiame dokumente nagrinėjame Japonijos BART veiksmingumą naudojant Japonijos patentų biurą Corpus 2.0. Mūsų eksperimentai rodo, kad Japonijos BART taip pat gali pagerinti vertimo tikslumą Korėjos japonų ir anglų kalbomis.</abstract_lt>
      <abstract_kk>Бұл қағазда, Азия аудармасының 8- ші жұмысының патенттік тапсырмасына (Корея жапон және жапон тілінде) жүйеңізді таңдап береміз. Жуырда бірнеше зерттеулерді бірнеше тілдік деректерді қолдану үшін алдын- оқылған кодер- декодер үлгілерін қолданылады. БаRT (Lewis et al., 2020) бағдарламасының бірінші оқылған үлгілері, екі тілі деректерді түзету арқылы аудармалардың дұрыстығын жақсарту үшін көрсетілді. Бірақ олар тек Румынша тәжірибеледі! Ағылшын BART қолданып ағылшын тілінің аудармасы. Бұл қағазда, Япония патенттер офисы Корпус 2.0 қолданатын Япония BART әрекетін тексереміз. Біздің тәжірибелеріміз жапон BART және Корея жапон және ағылшын жапон аудармаларында аудармалардың дұрыстығын жасай алады деп белгіледі.</abstract_kk>
      <abstract_ms>Dalam kertas ini, kami memperkenalkan sistem TMU Neural Machine Translation (NMT) kami dihantar untuk tugas Patent (Korea Jepun dan Inggeris Jepun) dari Workshop ke-8 tentang Translation Asia (Nakazawa et al., 2021). Baru-baru ini, beberapa kajian mencadangkan model pengekod-dekoder terlatih-terlatih menggunakan data monobahasa. Salah satu model pra-dilatih, BART (Lewis et al., 2020), dipaparkan untuk meningkatkan ketepatan terjemahan melalui penyesuaian dengan data dua bahasa. However, they experimented only Romanian! Terjemahan Bahasa Inggeris menggunakan BART Bahasa Inggeris. Dalam kertas ini, kami memeriksa kegunaan BART Jepun menggunakan Pejabat Paten Jepun Corpus 2.0. Eksperimen kami menunjukkan bahawa BART Jepun juga boleh meningkatkan ketepatan terjemahan dalam kedua-dua terjemahan Jepun Korea dan bahasa Inggeris Jepun.</abstract_ms>
      <abstract_ka>ამ დოკუნში ჩვენ ჩვენი TMU ნეიროლური მაქსინური განაცვლის (NMT) სისტემა, რომელიც აზიანეთის განაცვლის 8-ი სამუშაო სამუშაო სამუშაო (კორეული იაპონეთი და იაპონეთი წაპონეთ მიმდინარე, რამდენიმე კვლევები მოძლევა მონოლენგური მონაცემების გამოყენებით წინასწარმოვიდგინეთ კოდერების რეკოდერების მოდელები. ერთი მოდელეში, BART (Lewis et al., 2020) გამოჩვენებულია, რომ უფრო უფრო მეტად გადაწყვეტა წესიერება ორიენგური მონაცემებით. მაგრამ, ისინი ექსპერიმენტირებენ მხოლოდ პრომინული! English translation using English BART. ჩვენ ამ დოკუნში წაპონეთის BART-ის ეფექტიურობას გამოყენებთ წაპონეთის პეტენტის კოპუს 2.0. ჩვენი ექსპერიმენტები აჩვენებს, რომ იაპონური BART შეუძლია ასევე უფრო უფრო უფრო მეტადება თავისწორეული იაპონური და ანგლიური წონური</abstract_ka>
      <abstract_mk>In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021).  Неодамна, неколку студии предложија предобучени модели за декодирање на кодери користејќи монојазични податоци. Еден од предобучените модели, БАРТ (Lewis и други, 2020), покажа дека ја подобрува преведувачката точност преку финетизирање со двојјазични податоци. Сепак, тие експериментираа само романски! Англиски превод користејќи англиски BART. Во овој весник ја испитуваме ефикасноста на Јапонската Барт користејќи Јапонска патентска канцеларија Корпус 2.0. Нашите експерименти покажуваат дека јапонскиот Барт, исто така, може да ја подобри преведувачката точност на корејски јапонски и англиски јапонски преведувања.</abstract_mk>
      <abstract_mt>F’dan id-dokument, aħna nintroduċu s-sistema tagħna tat-Traduzzjoni tal-Magni Newrali tat-TMU (NMT) ippreżentata għall-kompitu tal-Privattivi (Ġappuniż Korean u Ġappuniż Ingliż) tat-8 Workshop dwar it-Traduzzjoni Asjatika (Nakazawa et al., 2021). Dan l-aħħar, diversi studji pproponu mudelli ta’ kodifikatur-dekoder imħarrġa minn qabel bl-użu ta’ dejta monolingwi. Wieħed mill-mudelli mħarrġa minn qabel, BART (Lewis et al., 2020), intwera li jtejjeb il-pre ċiżjoni tat-traduzzjoni permezz ta’ aġġustament fin b’dejta bilingwi. Madankollu, esperimentaw biss ir-Rumen! Traduzzjoni bl-Ingliż bl-użu tal-BART bl-Ingliż. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0.  Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</abstract_mt>
      <abstract_ml>ഈ പത്രത്തില്‍ ഞങ്ങള്‍ നമ്മുടെ ടിഎം യു ന്യൂറല്‍ മെഷീന്‍ പരിഭാഷ (NMT) സിസ്റ്റമിനെ പരിചയപ്പെടുത്തുന്നു. ഏഷ്യയിലെ പരിഭാഷണത്തിന്റെ എട്ടാം വര്‍ക്കാര്‍ക്ക് വേണ അടുത്തുതന്നെ, പല പഠനങ്ങളും മുമ്പ് പരിശീലിക്കപ്പെട്ട കോഡെര്‍ ഡെക്കോഡെര്‍ മോഡലുകള്‍ പരിശോധിച്ചു മുമ്പ് പരിശീലിക്കപ്പെട്ട മോഡലുകളില്‍ ഒരാള്‍ ബാര്‍ട്ടി (ലെവിസ് et al., 2020), രണ്ടു ഭാഷ വിവരങ്ങള്‍ മുഖേന പരിഭാഷപ്രകാരം മെച്ചപ്പെടു എന്നാലും, അവര്‍ റൊമാനിയന്‍ മാത്രം പരീക്ഷിച്ചു! ഇംഗ്ലീഷ് BART ഉപയോഗിച്ച് ഇംഗ്ലീഷ് പരിഭാഷ ഈ പത്രത്തില്‍, ജപ്പാന്‍ പാപ്റ്റന്റ് ഓഫീസ് 2.0 ഉപയോഗിച്ച് ജപ്പാന്‍ ഭാര്‍ട്ടിന്റെ പ്രഭാവം പരിശോധിക്കുന് Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</abstract_ml>
      <abstract_mn>Энэ цаасан дээр бид TMU мэдрэлийн машин хөгжүүлэх (NMT) системийг Азийн хөгжүүлэх (Nakazawa et al., 2021) 8-р ажлын Патент даалгаврын ажлын төлөө хийсэн. Саяхан олон судалгаанууд нэг хэлний өгөгдлийг ашиглаж сургалтын өмнө сургалтын кодлогч-декодлогч загварыг санал болгосон. БАРТ (Lewis et al., 2020) дасгал сургалтын нэг загвар нь хоёр хэлний өгөгдлийг сайжруулахын аргаар хөгжүүлсэн орнуудын тодорхойлолтыг сайжруулах боломжтой болсон. Гэхдээ тэд зөвхөн Румын зөвхөн туршилт хийсэн. Англи хэлний БАРТ ашиглаж Англи хэлний орчуулалт. Энэ цаасанд бид Японы Патент Оффис Корпус 2.0 ашиглан Япон Бартын үр дүнг шалгаж байна. Бидний туршилтууд Япон БАРТ нь Корейн Япон болон Англи Япон хэлбэрээр орчуулах зөв байдлыг илүү сайжруулж чадна.</abstract_mn>
      <abstract_ro>În această lucrare, prezentăm sistemul nostru TMU Neural Machine Translation (NMT) depus pentru sarcina de brevetare (coreeană japoneză și engleză japoneză) al celui de-al 8-lea atelier de traducere asiatică (Nakazawa et al., 2021). Recent, mai multe studii au propus modele pre-instruite de encoder-decoder utilizând date monolingve. Unul dintre modelele pre-instruite, BART (Lewis et al., 2020), a demonstrat că îmbunătățește acuratețea traducerii prin ajustarea fină cu date bilingve. Cu toate acestea, au experimentat doar români! Traducere în engleză folosind limba engleză BART. În această lucrare, examinăm eficacitatea BART japoneză folosind Japan Patent Office Corpus 2.0. Experimentele noastre indică faptul că BART japoneză poate îmbunătăți precizia traducerii atât în traducerile japoneze coreene, cât și în engleză japoneză.</abstract_ro>
      <abstract_sr>U ovom papiru predstavljamo naš sistem Neuralnog prevoda (NMT) TMU koji je predan za patentni zadatak (Korejski japanski i engleski japanski) 8. radionice o azijskom prevodu (Nakazawa et al., 2021). Nedavno, nekoliko studija predložilo je predobučene modele kodera-dekodera koristeći monojezičke podatke. Jedan od predobučenih modela, BART (Lewis et al., 2020), pokazuje se da je poboljšao preciznost prevoda putem fino-tuniranja sa dvojezičkim podacima. Međutim, eksperimentirali su samo rumunski! Engleski prevod koristeći engleski BART. U ovom papiru pregledamo učinkovitost japanskog BART korištenja Japanske patentne kancelarije korpusa 2.0. Naši eksperimenti ukazuju na to da japanski BART takođe može poboljšati preciznost prevoda na korejskim japanskim i engleskim japanskim prevodima.</abstract_sr>
      <abstract_pl>W niniejszym artykule przedstawiamy nasz system neuronowego tłumaczenia maszynowego TMU (NMT) zgłoszony do zadania patentowego (koreański japoński i angielski japoński) VIII Warsztatu Tłumaczenia Azjatyckiego (Nakazawa et al., 2021). Ostatnio kilka badań zaproponowało wstępnie przeszkolone modele kodera-dekodera z wykorzystaniem danych jednojęzycznych. Wykazano, że jeden z wstępnie przeszkolonych modeli, BART (Lewis et al., 2020), poprawia dokładność tłumaczenia poprzez dostosowanie danych dwujęzycznych. Jednak eksperymentowali tylko po rumuńsku! Tłumaczenie angielskie przy użyciu angielskiego BART. W niniejszym artykule badamy skuteczność japońskiego BART przy użyciu Japońskiego Urzędu Patentowego Corpus 2.0. Nasze eksperymenty wskazują, że japoński BART może również poprawić dokładność tłumaczenia zarówno w koreańskim, jak i angielskim tłumaczeniu japońskim.</abstract_pl>
      <abstract_no>I denne papiret introduserer vi vårt TMU Neural Machine Translation (NMT) system som er sendt til patentoppgåva (Koreansk og engelsk japansk) av 8. arbeidsområdet på Asian Translation (Nakazawa et al., 2021). Nyleg har fleire studier foreslått først trengte koderingsmodeller med monospråk- data. Ein av dei først trengte modelane, BART (Lewis et al., 2020), vert vist til å forbedra omsetjingskokretasjonen ved å finne opp med bilinguelt data. Men dei eksperimenterte berre romnisk! Engelsk omsetjing med engelsk BART. I denne papiret undersøker vi effektiviteten av japansk BART med Japan Patent Office Corpus 2.0. Våre eksperimenter tyder på at Japansk BART kan også forbedra omsetjingsakratitet i både Koreanske og engelske japanske omsetjingar.</abstract_no>
      <abstract_so>Qoraalkan waxaan ku soo bandhignaa nidaamka tarjumaadda ee TMU Neural Machine (NMT) ee loo soo dhiibay shaqada bukaanka (Koreaniya Japanese iyo Ingiriis) oo ah 8aad Workshop on Translation Aasiya (Nakazawa et al., 2021). Muddii u dhowaad, waxbarasho badan ayaa lagu soo jeeday modelal koordiyuhu uu ku isticmaalayo macluumaad luuqad ah. Mid ka mid ah modellada horay loo tababaray, BART (Lewis et al., 2020), waxaa looga muujiyey inuu kordhiso saxda turjumista via fine-tuning with labada luuqadood. Si kastaba ha ahaatee, waxay jirrabeen Romanian oo keliya! Turjumista Ingiriis ee isticmaalaya Ingiriis BART. Warqadan waxaynu baaritaan waxyaabaha ay japaniya BART ku leeyihiin isticmaalka xafiiska bukaanka Japan Korpus 2.0. Imtixaanadayada waxay muujinayaan in Jabanees BART sidoo kale uu kordhin karo saxda turjumaadda ee Koreaniya iyo turjumaadda Ingiriis ee Jabanees.</abstract_so>
      <abstract_ta>இந்த காக்கியத்தில், நாம் எங்கள் டிஎம்யு நெருக்கர் இயந்திரம் மொழிபெயர்ப்பு (NMT) அமைப்பை குறிப்பிடுகிறோம் எட்டாவது ஆசிய மொழிபெயர்ப்பின் மொழிபெயர்ப்ப சமீபத்தில், பல ஆராய்ச்சிகள் மோனோலிங்கல் தரவை பயன்படுத்தி முன் பயிற்சி குறியீட்டு மாதிரிகளை பரிந்துரைக்கப முன்பயிற்சிக்கப்பட்ட மாதிரிகளில் ஒன்று, BART (லீவி et al., 2020), இரு மொழிகள் தரவுடன் மொழிபெயர்ப்பு சரியான தெளிவை மேம்படுத்துவதற்கு  ஆனாலும், அவர்கள் ரோமானியன் மட்டும் சோதனைப்படுத்தினார்கள்! English translation using English BART.  இந்த காகிதத்தில், நாம் ஜப்பான் பாதுகாப்பு அலுவலகம் 2.0 பயன்படுத்தி ஜாப்பான் பார்ட் விளைவுகளை பரிசோதி எங்கள் சோதனைகள் குறிப்பிடுகிறது ஜப்பானிய பார்ட் மொழிபெயர்ப்பு சரியை மேலும் மொழிபெயர்ப்பு தெளிவாக்க முடி</abstract_ta>
      <abstract_si>මේ පැත්තට, අපි අපේ TMU න්‍යුරල් මැෂින් අවවාදය (NMT) පද්ධතිය ප්‍රදේශය (කෝරියාන් ජාපානි සහ ඉංග්‍රීසි ජාපානි වලින්) 8ම වැඩසටහන් අ අවසානයෙන්, විශේෂ අධ්‍යානය ප්‍රීක්ෂණා කරලා තියෙන්නේ ප්‍රීක්ෂණා කරපු කෝඩෝර් ඩිකොඩර් ම ප්‍රධානය කරපු මොඩේල් එකක්, BART (Luis et al., 2020යි), පෙන්වන්න පුළුවන් විදිහට පරිවර්තන ක්‍රියාත්මක විශේෂය කරන්න පුළුවන ඒත් ඔවුන් රෝමානියාන් විතරයි පරීක්ෂා කරලා! ඉංග්‍රීසි BART භාවිතානය කරන්න ඉංග්‍රීසි භාවිතානය. මේ පත්තරේ අපි ජාපාන් පැටෙන්ට් කාර්පුස් 2.0 භාවිතා කරන්න ජාපාන් බාර්ට් එකේ ප්‍රශ්ණතාවක් පරීක් අපේ පරීක්ෂණය පෙන්වන්නේ ජාපාන් බාර්ට් වලින් කෝරියාන් ජාපානි වලින් ඉංග්‍රීසි ජාපානි වලි</abstract_si>
      <abstract_sv>I denna uppsats presenterar vi vårt TMU Neural Machine Translation (NMT) system som lämnats in för patentuppgiften (koreansk japansk och engelsk japansk) i 8:e Workshop on Asian Translation (Nakazawa et al., 2021). Nyligen har flera studier föreslagit färdigutbildade encoder-avkodarmodeller med enspråkiga data. En av de förberedda modellerna, BART (Lewis et al., 2020), visade sig förbättra översättningens noggrannhet genom finjustering med tvåspråkiga data. Men de experimenterade bara rumänska! Engelska översättning med engelska BART. I denna uppsats undersöker vi effektiviteten av japansk BART med hjälp av Japan Patent Office Corpus 2.0. Våra experiment visar att japanska BART också kan förbättra översättningens noggrannhet i både koreanska japanska och engelska japanska översättningar.</abstract_sv>
      <abstract_ur>اس کاغذ میں ہم نے اپنی TMU نیورال ماشین ترجمہ (NMT) سیستم کو آسیا ترجمہ کے 8م کارشاپ کے لئے پیش کیا ہے۔ اچھا، بہت سی تحقیقات پیش آموزش کی پیش آموزش دی گئی ایک زبان دکھانے کے مطابق ایک کوڈر-ڈیکوڈر موڈل پیش کیے گئے ہیں. پہلے آموزش کی مدل میں سے ایک BART (Lewis et al., 2020) کو دکھایا گیا تھا کہ دو زبان اولاد کے ذریعہ مطابق ترجمہ کی دقیقیقیت کو بہتر کرنے کے لئے۔ لیکن وہ صرف رومانی آزمائش کرتے ہیں۔ انگلیسی BART کے مطابق انگلیسی ترجمہ. اس کاغذ میں، ہم جاپانی پٹینٹ ऑفیس کورپوس 2.0 کے مطابق جاپانی برٹ کی فعالیت کی تحقیق کرتے ہیں. ہماری آزمائش نشان دیتی ہے کہ جاپانی BART بھی کوریا ژاپنی اور انگلیسی ژاپنی ترجمہ میں ترجمہ دقیق بھی بہتر کر سکتا ہے.</abstract_ur>
      <abstract_uz>Bu hujjatda biz Asiy tarjima daftarining 8- toʻplami (Nakazawa et al, 2021) uchun Patent vazifasi (Koriya Yaponiya va Ingliz Yaponcha) uchun tarjima qiladigan TMU Neural Mashine tarjima (NMT) tizimini ko'rsamiz. Yaqinda ko'pchilik o'rganishlar monolingual maʼlumot yordamida bir necha ta'minlovchi kodekoder modellarini talab qiladi. Birinchi taʼminlovchi modellardan biri BART (Lewis et al., 2020), ikkita tillar yordamida tarjima tayyorligini oshirish mumkin. Lekin, улар фақат Румий синовчи бўлган. Inglizcha tarjima qilindiName In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0.  Bizning imtiyozlarimiz, Yaponcha BART xitoycha Japoniya va Ingliz Japoniya tarjimalarining ikkita tarjimalarini oshirish imkoniyatini oshirish mumkin.</abstract_uz>
      <abstract_vi>Trong tờ giấy này, chúng tôi xin giới thiệu hệ thống dịch máy thần kinh TMU (NMB) được giao cho công việc sáng chế (Nhật Bản Hàn Quốc và Anh Quốc) của 8th Workshop in Asian Translation (Nakazawa et al., 2021). Gần đây, nhiều nghiên cứu đề xuất mô hình mã hóa đã được đào tạo. Một trong những mô hình được huấn luyện trước, BART (Lewis et al., 2020) đã được cho thấy cải thiện độ chính xác của dịch qua việc tinh chỉnh hai thứ bằng độ chính xác. Tuy nhiên, họ chỉ thử nghiệm Romani! Dịch bằng tiếng Anh BART. Trong tờ giấy này, chúng tôi kiểm tra hiệu quả của Nhật BART sử dụng Nhật bản sáng chế Corpus 2.0. Những thí nghiệm của chúng tôi cho thấy rằng bên Nhật BART cũng có thể cải thiện độ chính xác dịch trong cả tiếng Nhật Triều Tiên và Anh.</abstract_vi>
      <abstract_bg>В настоящата статия представяме нашата система за невронен машинен превод (НМТ), подадена за патентна задача (корейски японски и английски японски) на 8-ма работилница по азиатски превод (Наказава и др., 2021). Наскоро няколко проучвания предлагат предварително обучени модели кодер-декодер, използващи едноезични данни. Доказано е, че един от предварително обучените модели подобрява точността на превода чрез фина настройка с двуезични данни. Но те експериментираха само румънски! Английски превод с помощта на английски BART. В настоящата статия изследваме ефективността на японския БАРТ, използвайки Японското патентно ведомство Корпус 2.0. Нашите експерименти показват, че японският може да подобри точността на превода както в корейския, така и в английския японски превод.</abstract_bg>
      <abstract_nl>In dit artikel introduceren we ons TMU Neural Machine Translation (NMT) systeem dat is ingediend voor de Patenttaak (Koreaans Japans en Engels Japans) van 8e Workshop on Asian Translation (Nakazawa et al., 2021). Onlangs hebben verschillende studies voorgetrainde encoder-decoder modellen voorgesteld met eentalige gegevens. Een van de voorgetrainde modellen, BART (Lewis et al., 2020), bleek de vertaalnauwkeurigheid te verbeteren door finetuning met tweetalige gegevens. Ze experimenteerden echter alleen met Roemeens! Engelse vertaling met behulp van het Engels BART. In dit artikel onderzoeken we de effectiviteit van Japanse BART met behulp van Japan Patent Office Corpus 2.0. Uit onze experimenten blijkt dat Japans BART ook vertaalnauwkeurigheid kan verbeteren in zowel Koreaans Japans als Engels Japans vertalingen.</abstract_nl>
      <abstract_hr>U ovom papiru predstavljamo naš sistem za neurološki prevod (NMT) TMU koji je podignut za zadatak patenta (korejski japanski i engleski japanski) 8. radionice o azijskom prevodu (Nakazawa et al., 2021). Nedavno je nekoliko ispitivanja predložilo predobučene modele kodera-dekodera koristeći monojezičke podatke. Jedan od predobučenih modela, BART (Lewis et al., 2020), pokazalo je kako bi poboljšao preciznost prevođenja putem ispravnog korištenja s dvojezičkim podacima. Međutim, oni su eksperimentirali samo rumunski! English translation using English BART. U ovom papiru pregledamo učinkovitost japanskog BART korištenja Japanskog patentnog ureda korpusa 2.0. Naši eksperimenti ukazuju na to da japanski BART također može poboljšati preciznost prevoda na korejskim japanskim i engleskim japanskim prevodima.</abstract_hr>
      <abstract_da>I denne artikel introducerer vi vores TMU Neural Machine Translation (NMT) system indsendt til patentopgaven (koreansk japansk og engelsk japansk) på 8. workshop om asiatisk oversættelse (Nakazawa et al., 2021). For nylig foreslog flere undersøgelser præ-trænede encoder-dekoder modeller ved hjælp af ensprogede data. En af de forududdannede modeller, BART (Lewis et al., 2020), viste sig at forbedre oversættelsesnøjagtigheden ved finjustering med tosprogede data. Men de eksperimenterede kun rumænsk! Engelsk oversættelse ved hjælp af engelsk BART. I denne artikel undersøger vi effektiviteten af japansk BART ved hjælp af Japan Patent Office Corpus 2.0. Vores eksperimenter viser, at japansk BART også kan forbedre oversættelsesnøjagtigheden i både koreansk japansk og engelsk japansk oversættelse.</abstract_da>
      <abstract_de>In diesem Beitrag stellen wir unser TMU Neural Machine Translation (NMT) System vor, das für die Patentaufgabe (Koreanisch Japanisch und Englisch Japanisch) des achten Workshops zur asiatischen Übersetzung (Nakazawa et al., 2021) eingereicht wurde. Kürzlich schlugen mehrere Studien vortrainierte Encoder-Decoder-Modelle vor, die monolinguale Daten verwenden. Eines der vortrainierten Modelle, BART (Lewis et al., 2020), verbesserte die Übersetzungsgenauigkeit durch Feinabstimmung mit zweisprachigen Daten. Allerdings experimentierten sie nur rumänisch! Englische Übersetzung mit Hilfe von BART. In diesem Beitrag untersuchen wir die Wirksamkeit des japanischen BART unter Verwendung des japanischen Patentamts Corpus 2.0. Unsere Experimente zeigen, dass Japanisch BART auch die Übersetzungsgenauigkeit sowohl in Koreanisch-Japanisch- als auch in Englisch-Japanisch-Übersetzungen verbessern kann.</abstract_de>
      <abstract_ko>본고에서 제8회 아시아번역세미나(Nakazawa 등, 2021년)의 특허 임무(아사히와 영일)를 위해 제출한 TMU 신경기계번역(NMT) 시스템을 소개한다.최근 일부 연구에서는 단어 데이터를 사용하는 예비 트레이닝 인코더인 디코더 모델을 제시했다.BART(Lewis et al., 2020)는 사전 훈련을 거친 모델로 이중 언어 데이터를 미세하게 조정하여 번역 정밀도를 높일 수 있다.그러나 그들은 루마니아어만 시험했다!영어 BART를 사용하여 영어로 번역합니다.본고에서 우리는 일본 특허국 어료고 2.0을 이용하여 일본 BART의 유효성을 검증한다.우리의 실험은 일본어 BART도 한일과 영일 번역의 정확성을 높일 수 있음을 나타냈다.</abstract_ko>
      <abstract_id>In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021).  Baru-baru ini, beberapa studi mengusulkan model koder-dekoder yang dilatih-dilatih menggunakan data monobahasa. Salah satu model prapelatih, BART (Lewis et al., 2020), ditunjukkan untuk meningkatkan akurasi terjemahan melalui fine-tuning dengan data dua bahasa. Namun, mereka hanya eksperimen Rumania! Terjemahan Inggris menggunakan BART Inggris. Dalam kertas ini, kami memeriksa efektivitas dari BART Jepang menggunakan Korpus Paten Jepang 2.0. Eksperimen kami menunjukkan bahwa BART Jepang juga dapat meningkatkan akurasi terjemahan dalam terjemahan Jepang Korea dan bahasa Inggris Jepang.</abstract_id>
      <abstract_fa>در این کاغذ، سیستم ترجمه ماشین عصبی TMU (NMT) ما را معرفی می‌کنیم که برای کار patent (ژاپنی ژاپنی و ژاپنی ژاپنی کوریه) هشتم کارشناسی در ترجمه آسیا (Nakazawa et al., 2021) فرستاده شده است. اخیرا، چند مطالعه پیش از آموزش مدل‌های رمزبندی پیش آموزش داده شده با استفاده از داده‌های یک زبان. یکی از مدلهای پیش آموزش داده شده، BART (Lewis et al., 2020) نشان داده شد که دقیق ترجمه را با اطلاعات دو زبان بهتر کند. ولی اونا فقط رومانی آزمایش کردند! ترجمه انگلیسی با استفاده از BART انگلیسی. در این کاغذ، ما موثیت BART ژاپنی را با استفاده از اداره پتانس ژاپن Corpus 2.0 تحقیق می کنیم. آزمایشات ما نشان می دهند که BART ژاپنی هم می تواند دقیق ترجمه را در ترجمه ژاپنی ژاپنی و ژاپنی ژاپنی بهتر کند.</abstract_fa>
      <abstract_tr>Bu kagyzda, biz TMU NMT-iň näral Maşynyň terjimelerini (NMT) sistemamyzy Aziýa terjimelerinde 8-nji Iýpet bellenilýär (Nakazawa et al., 2021). Soňky wagtlar, birnäçe öňki bilim öňki arkalanmış kodeýan nusgalary monodil maglumaty ullanýar. BART (Lewis et al., 2020) öňündeki bilim sistemasynda terjime edilen hatlaryň dogrylygyny ýüzeltmek üçin görkezildi. Ýöne olar diňe rumunça synanyşdylar! Iňlisçe BART ulanan Iňlisçe terjime edildi. Bu kagyzda Japon Patent Ofis Korpus 2.0 ulanarak Japon BART'yň etkinliýetini barlap bardyk. Biziň deneylerimiz Japon BART-yň hem Koreýan we Iňlis dilinde terjime edilmegiň dogrylygyny gowurap biljekdigini aýdýar.</abstract_tr>
      <abstract_af>In hierdie papier, ons introduseer ons TMU Neurale Masjien Vertaling (NMT) stelsel voorgestel vir die Patent-taak (Koreaanse Japanse en Engelse Japanse) van die 8de Werkshop op Asies Vertaling (Nakazawa et al., 2021). Onlangs het verskeie studie voorgestel vooraf-opgelei koder-dekoder modele gebruik van monolinglike data. Een van die voorafgevorderde modele, BART (Lewis et al., 2020), was vertoon om vertaling presisiteit te verbeter deur fyn-tuning met twee-tale data. Maar hulle het slegs Romaniese eksperimenteer! Engels vertaling gebruik Engels BART. In hierdie papier, ons ondersoek die effektiviteit van japanse BART deur Japan Patent Office Corpus 2.0 te gebruik. Ons eksperimente wys dat Japanse BART ook vertaling presies kan verbeter in Koreaanse Japanse en Engelse vertalings.</abstract_af>
      <abstract_sw>Katika karatasi hii, tunautambulisha mfumo wetu wa Tafsiri ya Mashine ya Kifaransa (NMT) uliotolewa kwa ajili ya kazi ya Wazapani (Kijapani na Kiingereza) wa warsha ya 8 kuhusu Tafsiri ya Asia (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data.  Moja ya mifano ya zamani ya mafunzo, BART (Lewis et al., 2020), ilionyesha kuongeza uhakika wa tafsiri kwa kutumia taarifa za lugha mbili. Hata hivyo, walijaribu WaRomania pekee! Tafsiri ya Kiingereza kwa kutumia Kiingereza BART. Katika karatasi hii, tunachunguza ufanisi wa BART wa Japani kwa kutumia Ofisi ya Wagonjwa wa Japan 2.0. Majaribio yetu yanaonyesha kuwa BART ya Japani inaweza pia kuboresha ukweli wa tafsiri katika tafsiri za Kijapani na Kiingereza.</abstract_sw>
      <abstract_sq>Në këtë letër, ne prezantojmë sistemin tonë TMU Neural Machine Translation (NMT) të paraqitur për detyrën e patentave (Korean Japanese and English Japanese) të Workshop të 8-të mbi Translation Asian (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. Një nga modelet e paratrajnuar, BART (Lewis et al., 2020), u tregua se përmirëson saktësinë e përkthimit nëpërmjet rregullimit me të dhënat dygjuhëse. Megjithatë, ata eksperimentuan vetëm rumun! Përkthimi anglez duke përdorur anglisht BART. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0.  Eksperimentet tona tregojnë se BART japonez mund gjithashtu të përmirësojë saktësinë e përkthimit në përkthimet japoneze dhe angleze.</abstract_sq>
      <abstract_az>Bu kańüńĪzda TMU Neural Machine Translation (NMT) sistemimizi Aziya √áeviri (Nakazawa et al., 2021) 8. Workshop on 8. Workshop for the Patent Task (Korean Japanese and English Japanese) il…ô t…ôblińü edirik. Son zamanlarda, bir ne√ß…ô t…ôhsil …ôvv…ôlc…ô t…ôhsil edilmiŇü koder-dekoder modell…ôri monodil veril…ônl…ôrd…ôn istifad…ô edir. √Ėn t…ôhsil edilmiŇü modell…ôrd…ôn biri BART (Lewis et al., 2020), iki dil m…ôlumatlarńĪ il…ô t…ôhsil edilm…ôsi il…ô t…ôhsil edilm…ôsi √ľ√ß√ľn t…ôhsil edilmiŇüdir. Ancaq onlar yalnńĪz Rumun t…ôcr√ľb…ôl…ôrini t…ôcr√ľb…ô etdil…ôr! ńįngiliz…ô BART vasit…ôsil…ô ńįngiliz…ô terc√ľm…ôsi. Bu kańüńĪzda Japon Patent Office Corpus 2.0 vasit…ôsil…ô Japon BART'un etkinlińüini incidirik. Bizim t…ôcr√ľb…ôl…ôrimiz Japonca BART h…ôm√ßin in Kore Japonca v…ô ńįngilizce √ß…ôtinlikl…ôrd…ô d…ô d…ôyiŇüiklik ed…ô bil…ôr.</abstract_az>
      <abstract_am>በዚህ ገጽ፣ በእስያ ትርጉም ላይ ስምንተኛው workshop (ናkazawa et al., 2021) ለPatent ሥራ (የቆሬያዊ ጃፓንኛ እና እንግሊዘኛ) የTMU ኔural machine ትርጉም (NMT) ስርዓታችንን እናሳውቃለን፡፡ በቅርብ ጊዜ ብዙዎች ተማርከቶች በሞሎልቋል ዳታ የተጠቃሚ የencoder-decoder models በመጠቀም ይገልጻሉ፡፡ የቀድሞው ተማሪ ሞዴል BART (Lewis et al., 2020) በሁለት ቋንቋዎች ዳታ በመጠቀም ትርጉም እርግጠኛ ማድረግ ተገልጦአል፡፡ ነገር ግን ሮማኒያን ብቻ ፈተናቸው፡፡ እንግሊዘኛ ትርጉም BART በተጠቃሚ በዚህ ገጽ የጃፓን ፓንቲ ኮርፓስ 2.0 በመጠቀም የጃፓን BART ጥያቄን መርምረናል፡፡ ፈተናዎቻችን የጃፓን BART እና በቆሬያዊ ጃፓንኛ እና ኢንጂፓን ትርጓሜዎች ሁለቱም ትርጓሜዎችን ማሻሻል እንደሚችል ያሳያል፡፡</abstract_am>
      <abstract_hy>Այս թղթի մեջ մենք ներկայացնում ենք մեր ԹՄԱ-ի նյարդային մեքենայի թարգմանման (ՆՄԹ) համակարգը, որը ներկայացվել է Ասիական թարգմանման 8-րդ աշխատասենյակի համար (Նակազավա և այլն., 2021 թ.): Վերջերս, մի քանի ուսումնասիրություններ առաջարկեցին նախապատրաստված կոդեր-կոդեր մոդելներ, որոնք օգտագործում են միալեզու տվյալներ: Պարզվեց, որ նախապատրաստված մոդելներից մեկը, Բարթը (Leouis et al., 2020), բարելավում է թարգմանման ճշգրտությունը երկլեզու տվյալների հետ կապված կերպով: Այնուամենայնիվ, նրանք փորձեցին միայն ռոմաներեն: Անգլերեն թարգմանություն օգտագործելով անգլերեն Բարտ: Այս թղթի մեջ մենք ուսումնասիրում ենք Ճապոնական Բարտի արդյունավետությունը՝ օգտագործելով Ճապոնիայի փաստաբանական գրասենյակ Կորպուս 2.0: Մեր փորձարկումները ցույց են տալիս, որ ճապոնական Բարթը կարող է նաև բարելավել թարգմանման ճշգրտությունը կորեացի ճապոնացի և անգլերենի թարգմանություններում:</abstract_hy>
      <abstract_bn>এই কাগজটিতে আমরা আমাদের টিএমউ নিউরাল মেশিন অনুবাদ (এনএমটি) সিস্টেমের পরিচয় প্রদান করেছি প্যাটেন্ট কাজের (কোরিয়ান জাপানী এবং ইংরেজী জাপানীয়) যা এশিয়ার সম্প্রতি বেশ কিছু গবেষণা প্রশিক্ষণের পূর্বে প্রশিক্ষিত এনকোডার-ডেকোডার মডেল প্রস্তাব করা হয়েছে মোনোলিভা প্রাক্তন প্রশিক্ষিত মডেলের মধ্যে একটি বার্ট (লেউস এন্ট এল ২০২০), দুই ভাষার তথ্যের মাধ্যমে অনুবাদের সঠিকভাবে উন্নত করা হয়েছে। তবে, তারা শুধুমাত্র রোমানীয় পরীক্ষা করেছে! ইংরেজি অনুবাদ বিআরটি ব্যবহার করে। এই পত্রিকায় আমরা জাপানের প্যাটেন্ট অফিস ব্যবহার করে জাপানী বার্টের কার্যকরী পরীক্ষা করি। আমাদের পরীক্ষা নির্দেশ করছে যে জাপানী বার্টি কোরিয়ান জাপানী এবং ইংরেজি ভাষায় অনুবাদের সঠিকভাবে উন্নত করতে পারে।</abstract_bn>
      <abstract_ca>En aquest article introduïm el nostre sistema de traducció de màquines neuronals (NMT) submetit a la tasca de brevets (japonès coreans i anglès) de la 8ª taller sobre traducció asiàtica (Nakazawa et al., 2021). Recentment, molts estudis van proposar models de codificador pré-entrenats que utilitzaven dades monolingües. Un dels models pré-entrenats, BART (Lewis et al., 2020), va demostrar que millora la precisió de la traducció ajustant-se a les dades bilingües. No obstant això, només experimentaven rumà! traducció anglesa amb BART anglès. En aquest paper examinem l'eficacia del BART japonès utilitzant el Japan Patent Office Corpus 2.0. Els nostres experiments indican que el BART japonès també pot millorar la precisió de la traducció tant en coreans japonès com en anglès.</abstract_ca>
      <abstract_bs>U ovom papiru predstavljamo naš sistem neurološkog prevoda (NMT) TMU koji je predan za zadatak patenta (korejski japanski i engleski japanski) 8. radionice o azijskom prevodu (Nakazawa et al., 2021). Nedavno, nekoliko ispitivanja predložilo je predobučene modele kodera-dekodera koristeći monojezičke podatke. Jedan od predobučenih modela, BART (Lewis et al., 2020), pokazuje se da će poboljšati preciznost prevođenja putem fine-tuning sa dvojezičkim podacima. Međutim, oni su eksperimentirali samo rumunski! Engleski prevod koristeći engleski BART. U ovom papiru pregledavamo učinkovitost japanskog BART korpusa 2.0. Naši eksperimenti ukazuju na to da japanski BART također može poboljšati preciznost prevoda na korejskim japanskim i engleskim prevodima.</abstract_bs>
      <abstract_cs>V tomto článku představujeme náš systém TMU Neural Machine Translation (NMT) předložený k patentovému úkolu (korejská japonština a anglická japonština) osmého workshopu o asijském překladu (Nakazawa et al., 2021). V poslední době několik studií navrhlo předem trénované modely kodéru-dekodéru s využitím jednojjazyčných dat. Bylo ukázáno, že jeden z předškolených modelů BART (Lewis et al., 2020) zlepšuje přesnost překladu díky jemnému ladění s dvojjazyčnými daty. Nicméně experimentovali pouze rumunsky! Anglický překlad pomocí angličtiny BART. V tomto článku zkoumáme efektivitu japonského BART s využitím japonského patentového úřadu Corpus 2.0. Naše experimenty ukazují, že japonština BART může také zlepšit přesnost překladu v korejském japonštině a anglickém japonštině.</abstract_cs>
      <abstract_fi>Tässä artikkelissa esittelemme TMU Neural Machine Translation (NMT) -järjestelmämme, joka on toimitettu 8. Aasian kääntämisen työpajan patenttitehtävään (Nakazawa et al., 2021). Viime aikoina useissa tutkimuksissa on ehdotettu esikoulutettuja kooderi-dekooderimalleja, joissa käytetään monikielistä dataa. Yhden esikoulutetun mallin, BART (Lewis et al., 2020), osoitettiin parantavan käännöksen tarkkuutta hienosäätämällä kaksikielisiä tietoja. Mutta he kokeilivat vain romaniaa! Englanninkielinen käännös käyttäen englantia BART. Tässä artikkelissa tarkastellaan japanilaisen BART:n tehokkuutta Japanin patenttiviraston Corpus 2.0:n avulla. Kokemuksemme osoittavat, että japanin BART voi myös parantaa käännösten tarkkuutta sekä korean japanin että englannin japanin käännöksissä.</abstract_fi>
      <abstract_et>Käesolevas töös tutvustame TMU neuroalse masintõlke (NMT) süsteemi, mis on esitatud 8. Aasia tõlke seminari patendiülesandeks (Korea jaapani ja inglise jaapani keeles) (Nakazawa et al., 2021). Hiljuti pakuti mitmes uuringus välja eelnevalt koolitatud kodeerija-dekooderi mudelid, mis kasutasid ühekeelseid andmeid. Üks eelkoolitud mudelitest, BART (Lewis et al., 2020), tõendas tõlke täpsust kahekeelsete andmetega peenhäälestuse kaudu. Kuid nad katsetasid ainult rumeenia keelt! Inglise tõlge inglise keeles BART. Käesolevas töös uurime Jaapani BART efektiivsust, kasutades Jaapani Patendiameti Corpus 2.0. Meie eksperimendid näitavad, et jaapani BART võib parandada ka tõlke täpsust nii korea jaapani kui ka inglise jaapani tõlketes.</abstract_et>
      <abstract_he>בעיתון הזה, אנחנו מציגים את מערכת המכונה העצבית TMU (NMT) שלנו שנשלחה למשימת הפטנטים (יפנית וקוריאנית) של Workshop 8 על התרגום אסיאטי (Nakazawa et al., 2021). לאחרונה, מספר מחקרים הציעו מודלים קודם-קודם מאומנים מראש בשימוש בנתונים מונולשונים. אחד מהדוגמנים המאמנים מראש, BART (Lewis et al., 2020), הוכח לשפר את מדויקת התרגום דרך התאים עם נתונים שתיים שפתיים. עם זאת, הם ניסו רק רומני! English translation using English BART.  בעיתון הזה, אנחנו בודקים את היעילות של BART היפני באמצעות משרד פטנטים יפני קורפוס 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</abstract_he>
      <abstract_sk>V tem prispevku predstavljamo sistem TMU nevral strojnega prevajanja (NMT), ki je bil predložen za nalogo patenta (korejska japonščina in angleščina japonščina) 8. delavnice o azijskem prevajanju (Nakazawa et al., 2021). V zadnjem času je več študij predlagalo vnaprej usposobljene modele kodirnikov-dekoderjev z uporabo enojezičnih podatkov. Eden od vnaprej usposobljenih modelov, BART (Lewis et al., 2020), je izboljšal natančnost prevajanja prek finega nastavitve z dvojezičnimi podatki. Vendar so eksperimentirali samo romunsko! Angleški prevod z uporabo angleškega BART. V tem prispevku preučujemo učinkovitost japonskega BART z uporabo japonskega patentnega urada Corpus 2.0. Naši eksperimenti kažejo, da lahko japonski BART izboljša natančnost prevodov v korejski japonski in angleški japonski prevodi.</abstract_sk>
      <abstract_jv>Nang mapun iki, kéné gunakake sistem tanggal (NMT) sing nyimpen kanggo nggawe patent kanggo nganggo dolanan sing wis pitik (japongan karo japongan ingkang Korea) ning wis asai Workspace nang pitik-terjamahan asita (nakutawa et al, 2020). plug-in-action metadata Nanging, dheweke entuk-ingkang rumane mungkin ! Terjamahan Inggris nang nggambar LPRT. Nang pemilih iki, awak dhéwé isih bakal nggawe barang japon barang nggambar barang patent Ofis 2.0 nggambar barang japon. Awak dhéwé éntuk ngerti barang, barang Hapon barang bisa nggawe turuné sak pangan anyar tentang kanggo barang Hapon karo Perancis Inggris.</abstract_jv>
      <abstract_ha>A cikin wannan takardan, Munã introduce na tsarin TMU Tarjima na Neural Mashine (NMT) wanda aka yi wajen aikin Nagon Farawa (Koriya Japanese da Ingiriya) na 8. workworkload on Asian Translate (Nakazawa et al., 2021). A yanzu, wasu fitina masu amfani da data na monoli'ura da aka yi amfani da shiryoyin kode-kode-kode. Babu wani daga shiryoyin ayuka da aka yi amfani da shi gaba-tuned, BArT (Lewi et al., 2020), aka nuna shi to improve translation uranci through fin-tuning with data bilin languages. To, a cikin fitinar ba su zamo ba fãce runtsũma. @ item Spelling dictionary Ga wannan takardan, Munã jarraba aikin BAT na Jamapi da Corbas 2.0. Kayan jarrabayanmu, yana madaidaita cẽwa BERT za'a iya samar da fassarar taƙaita a cikin fassarar Yahũdiya da Ingiriya.</abstract_ha>
      <abstract_bo>ང་ཚོའི་ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོའི་TMU Neural Machine Translation (NMT)རིགས་འདིས་ཨ་རེ་ཤི་ཡིག་སྣོད་ཀྱི་ལས་འགུལ་ལ་བཤད་པ་སོགས། འཕྲལ་ཁམས་ཀྱི་ལྟ་བ་དག་གི་སྔོན་སྒྲིག་འཛུགས་པའི་སྔོན་སྒྲིག་འཛུགས་པའི་ཨིན་ཀོ་ཌིར་སྤྱོད་པའི་མ་དཔེ་གཞུང་མང་པོ་ཞི སྔོན་གྲངས་འཛིན་པའི་མིག་གཟུགས་རིས་གཅིག་ནི་BART (Lewis et al., 2020)ནང་དུ་སྔོན་གྲངས་སྒྲིག ཡིན་ནའང་། ཁོང་ཚོས་རོ་མ་ཡིན་ལས་བརྟག་ཞིབ་བྱེད་པ་རེད། English translation using English BART. ང་ཚོས་ཤོག་བྱང་འདིའི་ནང་དུ་ཉེ་ཧོང་གི་ཡུལ་སྤྱོད་པའི་ཉེ་ཧོང་གི་BART་གི་ལྕགས་འབྱུང་བ་ཞིག ང་ཚོའི་བརྟག་ཞིག་གིས་ཉེ་ཧོང་གི་རྨང་གཞིའི་ནང་ནས་སྐད་ཡིག་ཆ་ཉེ་ཧོང་དང་ཨིན་ཇིའི་སྐད་ཡིག</abstract_bo>
      </paper>
    <paper id="18">
      <title>IITP at WAT 2021 : System description for English-Hindi Multimodal Translation Task<fixed-case>IITP</fixed-case> at <fixed-case>WAT</fixed-case> 2021: System description for <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi Multimodal Translation Task</title>
      <author><first>Baban</first><last>Gain</last></author>
      <author><first>Dibyanayan</first><last>Bandyopadhyay</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>161–165</pages>
      <abstract>Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the <a href="https://en.wikipedia.org/wiki/Translation">translation</a> by removing <a href="https://en.wikipedia.org/wiki/Ambiguity">ambiguity</a> on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT-2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 <a href="https://en.wikipedia.org/wiki/BLEU">BLEU points</a> for Evaluation and Challenge subset, respectively.</abstract>
      <url hash="bf4e62ef">2021.wat-1.18</url>
      <doi>10.18653/v1/2021.wat-1.18</doi>
      <bibkey>gain-etal-2021-iitp</bibkey>
    </paper>
    <paper id="20">
      <title>TMEKU System for the WAT2021 Multimodal Translation Task<fixed-case>TMEKU</fixed-case> System for the <fixed-case>WAT</fixed-case>2021 Multimodal Translation Task</title>
      <author><first>Yuting</first><last>Zhao</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <pages>174–180</pages>
      <abstract>We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT). The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use.</abstract>
      <url hash="8c1f0d63">2021.wat-1.20</url>
      <doi>10.18653/v1/2021.wat-1.20</doi>
      <bibkey>zhao-etal-2021-tmeku</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k-entities">Flickr30K Entities</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="21">
      <title>Optimal Word Segmentation for <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation</a> into Dravidian Languages<fixed-case>D</fixed-case>ravidian Languages</title>
      <author><first>Prajit</first><last>Dhar</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>181–190</pages>
      <abstract>Dravidian languages, such as <a href="https://en.wikipedia.org/wiki/Kannada">Kannada</a> and <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a>, are notoriously difficult to translate by state-of-the-art neural models. This stems from the fact that these <a href="https://en.wikipedia.org/wiki/Language">languages</a> are morphologically very rich as well as being low-resourced. In this paper, we focus on subword segmentation and evaluate Linguistically Motivated Vocabulary Reduction (LMVR) against the more commonly used SentencePiece (SP) for the task of translating from <a href="https://en.wikipedia.org/wiki/English_language">English</a> into four different <a href="https://en.wikipedia.org/wiki/Dravidian_languages">Dravidian languages</a>. Additionally we investigate the optimal subword vocabulary size for each language. We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality.</abstract>
      <url hash="61626921">2021.wat-1.21</url>
      <doi>10.18653/v1/2021.wat-1.21</doi>
      <bibkey>dhar-etal-2021-optimal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
    </paper>
    <paper id="22">
      <title>Itihasa : A <a href="https://en.wikipedia.org/wiki/Text_corpus">large-scale corpus</a> for Sanskrit to English translation<fixed-case>S</fixed-case>anskrit to <fixed-case>E</fixed-case>nglish translation</title>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>191–197</pages>
      <abstract>This work introduces <a href="https://en.wikipedia.org/wiki/Itihasa">Itihasa</a>, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The <a href="https://en.wikipedia.org/wiki/Shloka">shlokas</a> are extracted from two <a href="https://en.wikipedia.org/wiki/Indian_epic_poetry">Indian epics</a> viz., The <a href="https://en.wikipedia.org/wiki/Ramayana">Ramayana</a> and The <a href="https://en.wikipedia.org/wiki/Mahabharata">Mahabharata</a>. We first describe the motivation behind the curation of such a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.</abstract>
      <url hash="544fabc2">2021.wat-1.22</url>
      <doi>10.18653/v1/2021.wat-1.22</doi>
      <bibkey>aralikatte-etal-2021-itihasa</bibkey>
      <revision id="1" href="2021.wat-1.22v1" hash="9f1bffa5" />
      <revision id="2" href="2021.wat-1.22v2" hash="544fabc2" date="2021-10-11">Fixed typo</revision>
      <pwcdataset url="https://paperswithcode.com/dataset/itihasa">Itihasa</pwcdataset>
    </paper>
    <paper id="23">
      <title>NICT-5’s Submission To WAT 2021 : MBART Pre-training And In-Domain Fine Tuning For Indic Languages<fixed-case>NICT</fixed-case>-5’s Submission To <fixed-case>WAT</fixed-case> 2021: <fixed-case>MBART</fixed-case> Pre-training And In-Domain Fine Tuning For Indic Languages</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Abhisek</first><last>Chakrabarty</last></author>
      <pages>198–204</pages>
      <abstract>In this paper we describe our submission to the multilingual Indic language translation wtask MultiIndicMT under the team name NICT-5. This task involves <a href="https://en.wikipedia.org/wiki/Translation">translation</a> from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training.</abstract>
      <url hash="284ad0ea">2021.wat-1.23</url>
      <doi>10.18653/v1/2021.wat-1.23</doi>
      <bibkey>dabre-chakrabarty-2021-nict</bibkey>
    </paper>
    <paper id="24">
      <title>How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task<fixed-case>GPU</fixed-case> in 100 hours? <fixed-case>C</fixed-case>o<fixed-case>AS</fixed-case>ta<fixed-case>L</fixed-case> at <fixed-case>M</fixed-case>ulti<fixed-case>I</fixed-case>ndic<fixed-case>MT</fixed-case> Shared Task</title>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>Héctor Ricardo</first><last>Murrieta Bello</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Marcel</first><last>Bollmann</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>205–211</pages>
      <abstract>This work shows that competitive <a href="https://en.wikipedia.org/wiki/Translation_(geometry)">translation</a> results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a> for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metrics</a>.</abstract>
      <url hash="397e106b">2021.wat-1.24</url>
      <doi>10.18653/v1/2021.wat-1.24</doi>
      <bibkey>aralikatte-etal-2021-far</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
    </paper>
    <paper id="26">
      <title>Language Relatedness and Lexical Closeness can help Improve Multilingual NMT : IITBombay@MultiIndicNMT WAT2021<fixed-case>NMT</fixed-case>: <fixed-case>IITB</fixed-case>ombay@<fixed-case>M</fixed-case>ulti<fixed-case>I</fixed-case>ndic<fixed-case>NMT</fixed-case> <fixed-case>WAT</fixed-case>2021</title>
      <author><first>Jyotsana</first><last>Khatri</last></author>
      <author><first>Nikhil</first><last>Saini</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>217–223</pages>
      <abstract>Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for <a href="https://en.wikipedia.org/wiki/Multilingualism">multiple languages</a>. This paper describes our submission (Team ID : CFILT-IITB) for the MultiIndicMT : An Indic Language Multilingual Task at WAT 2021. We train multilingual NMT systems by sharing <a href="https://en.wikipedia.org/wiki/Encoder">encoder and decoder parameters</a> with language embedding associated with each token in both <a href="https://en.wikipedia.org/wiki/Encoder">encoder</a> and decoder. Furthermore, we demonstrate the use of transliteration (script conversion) for <a href="https://en.wikipedia.org/wiki/Indo-Aryan_languages">Indic languages</a> in reducing the lexical gap for training a multilingual NMT system. Further, we show improvement in performance by training a multilingual NMT system using languages of the same family, i.e., <a href="https://en.wikipedia.org/wiki/Lingua_franca">related languages</a>.</abstract>
      <url hash="6bfbb574">2021.wat-1.26</url>
      <doi>10.18653/v1/2021.wat-1.26</doi>
      <bibkey>khatri-etal-2021-language</bibkey>
    </paper>
    <paper id="27">
      <title>Samsung R&amp;D Institute Poland submission to WAT 2021 Indic Language Multilingual Task<fixed-case>S</fixed-case>amsung <fixed-case>R</fixed-case>&amp;<fixed-case>D</fixed-case> Institute <fixed-case>P</fixed-case>oland submission to <fixed-case>WAT</fixed-case> 2021 Indic Language Multilingual Task</title>
      <author><first>Adam</first><last>Dobrowolski</last></author>
      <author><first>Marcin</first><last>Szymański</last></author>
      <author><first>Marcin</first><last>Chochowski</last></author>
      <author><first>Paweł</first><last>Przybysz</last></author>
      <pages>224–232</pages>
      <abstract>This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&amp;D Institute Poland. The task covered translation between 10 Indic Languages (Bengali, <a href="https://en.wikipedia.org/wiki/Gujarati_language">Gujarati</a>, <a href="https://en.wikipedia.org/wiki/Hindi">Hindi</a>, <a href="https://en.wikipedia.org/wiki/Kannada">Kannada</a>, <a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a>, <a href="https://en.wikipedia.org/wiki/Marathi_language">Marathi</a>, <a href="https://en.wikipedia.org/wiki/Odia_language">Oriya</a>, <a href="https://en.wikipedia.org/wiki/Punjabi_language">Punjabi</a>, <a href="https://en.wikipedia.org/wiki/Tamil_language">Tamil</a> and <a href="https://en.wikipedia.org/wiki/Telugu_language">Telugu</a>) and <a href="https://en.wikipedia.org/wiki/English_language">English</a>. We combined a variety of techniques : <a href="https://en.wikipedia.org/wiki/Transliteration">transliteration</a>, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models. We applied an effective approach to low-resource training that consist of pretraining on backtranslations and tuning on parallel corpora. We experimented with two different domain-adaptation techniques which significantly improved translation quality when applied to monolingual corpora. We researched and applied a novel approach for finding the best <a href="https://en.wikipedia.org/wiki/Hyperparameter">hyperparameters</a> for ensembling a number of <a href="https://en.wikipedia.org/wiki/Translation_(biology)">translation models</a>. All techniques combined gave significant improvement-up to +8 <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> over baseline results. The quality of the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> has been confirmed by the human evaluation where SRPOL models scored best for all 5 manually evaluated languages.</abstract>
      <url hash="27e2db9d">2021.wat-1.27</url>
      <doi>10.18653/v1/2021.wat-1.27</doi>
      <bibkey>dobrowolski-etal-2021-samsung</bibkey>
    </paper>
    <paper id="28">
      <title>Multilingual Machine Translation Systems at WAT 2021 : One-to-Many and Many-to-One Transformer based NMT<fixed-case>WAT</fixed-case> 2021: One-to-Many and Many-to-One Transformer based <fixed-case>NMT</fixed-case></title>
      <author><first>Shivam</first><last>Mhaskar</last></author>
      <author><first>Aditya</first><last>Jain</last></author>
      <author><first>Aakash</first><last>Banerjee</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>233–237</pages>
      <abstract>In this paper, we present the details of the systems that we have submitted for the WAT 2021 MultiIndicMT : An Indic Language Multilingual Task. We have submitted two separate multilingual NMT models : one for <a href="https://en.wikipedia.org/wiki/English_language">English</a> to 10 Indic languages and another for 10 Indic languages to English. We discuss the implementation details of two separate multilingual NMT approaches, namely one-to-many and many-to-one, that makes use of a shared decoder and a shared encoder, respectively. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration.</abstract>
      <url hash="57129c8f">2021.wat-1.28</url>
      <doi>10.18653/v1/2021.wat-1.28</doi>
      <bibkey>mhaskar-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="30">
      <title>ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task<fixed-case>ANVITA</fixed-case> Machine Translation System for <fixed-case>WAT</fixed-case> 2021 <fixed-case>M</fixed-case>ulti<fixed-case>I</fixed-case>ndic<fixed-case>MT</fixed-case> Shared Task</title>
      <author><first>Pavanpankaj</first><last>Vegi</last></author>
      <author><first>Sivabhavani</first><last>J</last></author>
      <author><first>Biswajit</first><last>Paul</last></author>
      <author><first>Chitra</first><last>Viswanathan</last></author>
      <author><first>Prasanna Kumar</first><last>K R</last></author>
      <pages>244–249</pages>
      <abstract>This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions : EnglishIndic and IndicEnglish ; Indic set comprised of 10 Indian languages. ANVITA-1.0 MT system comprised of two multi-lingual NMT models one for the EnglishIndic directions and other for the IndicEnglish directions with shared encoder-decoder, catering 10 language pairs and twenty translation directions. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization. Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training. ANVITA-1.0 achieved highest AM-FM score for <a href="https://en.wikipedia.org/wiki/Bengali_language">EnglishBengali</a>, 2nd for <a href="https://en.wikipedia.org/wiki/Tamil_language">EnglishTamil</a> and 3rd for <a href="https://en.wikipedia.org/wiki/Indian_English">EnglishHindi</a>, BengaliEnglish directions on official test set. In general, performance achieved by ANVITA for the IndicEnglish directions are relatively better than that of EnglishIndic directions for all the 10 language pairs when evaluated using BLEU and RIBES, although the same trend is not observed consistently when AM-FM based evaluation was carried out. As compared to <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>, RIBES and AM-FM based scoring placed ANVITA relatively better among all the task participants.</abstract>
      <url hash="04b835ef">2021.wat-1.30</url>
      <doi>10.18653/v1/2021.wat-1.30</doi>
      <bibkey>vegi-etal-2021-anvita</bibkey>
    </paper>
  </volume>
</collection>