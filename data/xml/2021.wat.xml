<collection id="2021.wat">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</booktitle>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Hideki</first><last>Nakayama</last></editor>
      <editor><first>Isao</first><last>Goto</last></editor>
      <editor><first>Hideya</first><last>Mino</last></editor>
      <editor><first>Chenchen</first><last>Ding</last></editor>
      <editor><first>Raj</first><last>Dabre</last></editor>
      <editor><first>Anoop</first><last>Kunchukuttan</last></editor>
      <editor><first>Shohei</first><last>Higashiyama</last></editor>
      <editor><first>Hiroshi</first><last>Manabe</last></editor>
      <editor><first>Win Pa</first><last>Pa</last></editor>
      <editor><first>Shantipriya</first><last>Parida</last></editor>
      <editor><first>Ond&#345;ej</first><last>Bojar</last></editor>
      <editor><first>Chenhui</first><last>Chu</last></editor>
      <editor><first>Akiko</first><last>Eriguchi</last></editor>
      <editor><first>Kaori</first><last>Abe</last></editor>
      <editor><first>Yusuke</first><last>Oda</last></editor>
      <editor><first>Katsuhito</first><last>Sudoh</last></editor>
      <editor><first>Sadao</first><last>Kurohashi</last></editor>
      <editor><first>Pushpak</first><last>Bhattacharyya</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="949b3b17">2021.wat-1</url>
    </meta>
    <frontmatter>
      <url hash="659551df">2021.wat-1.0</url>
      <bibkey>wat-2021-asian</bibkey>
    </frontmatter>
    <paper id="4">
      <title><fixed-case>NICT</fixed-case>&#8217;s Neural Machine Translation Systems for the <fixed-case>WAT</fixed-case>21 Restricted Translation Task</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>62&#8211;67</pages>
      <abstract>This paper describes our system (Team ID: nictrb) for participating in the WAT&#8217;21 restricted machine translation task. In our submitted system, we designed a new training approach for restricted machine translation. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary. With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the model, as well as model ensembling, which further improved the final translation performance.</abstract>
      <url hash="d801187b">2021.wat-1.4</url>
      <doi>10.18653/v1/2021.wat-1.4</doi>
      <bibkey>li-etal-2021-nicts</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>NECTEC</fixed-case>&#8217;s Participation in <fixed-case>WAT</fixed-case>-2021</title>
      <author><first>Zar Zar</first><last>Hlaing</last></author>
      <author><first>Ye Kyaw</first><last>Thu</last></author>
      <author><first>Thazin</first><last>Myint Oo</last></author>
      <author><first>Mya</first><last>Ei San</last></author>
      <author><first>Sasiporn</first><last>Usanavasin</last></author>
      <author><first>Ponrudee</first><last>Netisopakul</last></author>
      <author><first>Thepchai</first><last>Supnithi</last></author>
      <pages>74&#8211;82</pages>
      <abstract>In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021. Basically, our models are based on neural methods for both directions of English-Myanmar and Myanmar-English language pairs. Most of the existing Neural Machine Translation (NMT) models mainly focus on the conversion of sequential data and do not directly use syntactic information. However, we conduct multi-source neural machine translation (NMT) models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus. The multi-source translation is an approach to exploit multiple inputs (e.g. in two different formats) to increase translation accuracy. The RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment. The experimental results showed that the proposed models of RNN-based architecture outperform the baseline model for English-to-Myanmar translation task, and the multi-source and shared-multi-source transformer models yield better translation results than the baseline.</abstract>
      <url hash="70ed53ac">2021.wat-1.6</url>
      <doi>10.18653/v1/2021.wat-1.6</doi>
      <bibkey>hlaing-etal-2021-nectecs</bibkey>
    </paper>
    <paper id="11">
      <title>Zero-pronoun Data Augmentation for <fixed-case>J</fixed-case>apanese-to-<fixed-case>E</fixed-case>nglish Translation</title>
      <author><first>Ryokan</first><last>Ri</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Yoshimasa</first><last>Tsuruoka</last></author>
      <pages>117&#8211;123</pages>
      <abstract>For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding pronoun in the target side of the English sentence. However, although fully resolving zero pronouns often needs discourse context, in some cases, the local context within a sentence gives clues to the inference of the zero pronoun. In this study, we propose a data augmentation method that provides additional training signals for the translation model to learn correlations between local context and zero pronouns. We show that the proposed method significantly improves the accuracy of zero pronoun translation with machine translation experiments in the conversational domain.</abstract>
      <url hash="a0e5659f">2021.wat-1.11</url>
      <doi>10.18653/v1/2021.wat-1.11</doi>
      <bibkey>ri-etal-2021-zero</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/business-scene-dialogue">Business Scene Dialogue</pwcdataset>
    </paper>
    <paper id="13">
      <title><fixed-case>TMU</fixed-case> <fixed-case>NMT</fixed-case> System with <fixed-case>J</fixed-case>apanese <fixed-case>BART</fixed-case> for the Patent task of <fixed-case>WAT</fixed-case> 2021</title>
      <author><first>Hwichan</first><last>Kim</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>133&#8211;137</pages>
      <abstract>In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. One of the pre-trained models, BART (Lewis et al., 2020), was shown to improve translation accuracy via fine-tuning with bilingual data. However, they experimented only Romanian!English translation using English BART. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</abstract>
      <url hash="3aba8b3d">2021.wat-1.13</url>
      <doi>10.18653/v1/2021.wat-1.13</doi>
      <bibkey>kim-komachi-2021-tmu</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>IITP</fixed-case> at <fixed-case>WAT</fixed-case> 2021: System description for <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi Multimodal Translation Task</title>
      <author><first>Baban</first><last>Gain</last></author>
      <author><first>Dibyanayan</first><last>Bandyopadhyay</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>161&#8211;165</pages>
      <abstract>Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the translation by removing ambiguity on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT - 2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU points for Evaluation and Challenge subset, respectively.</abstract>
      <url hash="bf4e62ef">2021.wat-1.18</url>
      <doi>10.18653/v1/2021.wat-1.18</doi>
      <bibkey>gain-etal-2021-iitp</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>TMEKU</fixed-case> System for the <fixed-case>WAT</fixed-case>2021 Multimodal Translation Task</title>
      <author><first>Yuting</first><last>Zhao</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <pages>174&#8211;180</pages>
      <abstract>We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT). The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use.</abstract>
      <url hash="8c1f0d63">2021.wat-1.20</url>
      <doi>10.18653/v1/2021.wat-1.20</doi>
      <bibkey>zhao-etal-2021-tmeku</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k-entities">Flickr30K Entities</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="21">
      <title>Optimal Word Segmentation for Neural Machine Translation into <fixed-case>D</fixed-case>ravidian Languages</title>
      <author><first>Prajit</first><last>Dhar</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>181&#8211;190</pages>
      <abstract>Dravidian languages, such as Kannada and Tamil, are notoriously difficult to translate by state-of-the-art neural models. This stems from the fact that these languages are morphologically very rich as well as being low-resourced. In this paper, we focus on subword segmentation and evaluate Linguistically Motivated Vocabulary Reduction (LMVR) against the more commonly used SentencePiece (SP) for the task of translating from English into four different Dravidian languages. Additionally we investigate the optimal subword vocabulary size for each language. We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality.</abstract>
      <url hash="61626921">2021.wat-1.21</url>
      <doi>10.18653/v1/2021.wat-1.21</doi>
      <bibkey>dhar-etal-2021-optimal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
    </paper>
    <paper id="22">
      <title>Itihasa: A large-scale corpus for <fixed-case>S</fixed-case>anskrit to <fixed-case>E</fixed-case>nglish translation</title>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Anders</first><last>S&#248;gaard</last></author>
      <pages>191&#8211;197</pages>
      <abstract>This work introduces Itihasa, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The shlokas are extracted from two Indian epics viz., The Ramayana and The Mahabharata. We first describe the motivation behind the curation of such a dataset and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this corpus and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.</abstract>
      <url hash="544fabc2">2021.wat-1.22</url>
      <doi>10.18653/v1/2021.wat-1.22</doi>
      <bibkey>aralikatte-etal-2021-itihasa</bibkey>
      <revision id="1" href="2021.wat-1.22v1" hash="9f1bffa5" />
      <revision id="2" href="2021.wat-1.22v2" hash="544fabc2" date="2021-10-11">Fixed typo</revision>
      <pwcdataset url="https://paperswithcode.com/dataset/itihasa">Itihasa</pwcdataset>
    </paper>
    <paper id="23">
      <title><fixed-case>NICT</fixed-case>-5&#8217;s Submission To <fixed-case>WAT</fixed-case> 2021: <fixed-case>MBART</fixed-case> Pre-training And In-Domain Fine Tuning For Indic Languages</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Abhisek</first><last>Chakrabarty</last></author>
      <pages>198&#8211;204</pages>
      <abstract>In this paper we describe our submission to the multilingual Indic language translation wtask &#8220;MultiIndicMT&#8221; under the team name &#8220;NICT-5&#8221;. This task involves translation from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training.</abstract>
      <url hash="284ad0ea">2021.wat-1.23</url>
      <doi>10.18653/v1/2021.wat-1.23</doi>
      <bibkey>dabre-chakrabarty-2021-nict</bibkey>
    </paper>
    <paper id="24">
      <title>How far can we get with one <fixed-case>GPU</fixed-case> in 100 hours? <fixed-case>C</fixed-case>o<fixed-case>AS</fixed-case>ta<fixed-case>L</fixed-case> at <fixed-case>M</fixed-case>ulti<fixed-case>I</fixed-case>ndic<fixed-case>MT</fixed-case> Shared Task</title>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>H&#233;ctor Ricardo</first><last>Murrieta Bello</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Marcel</first><last>Bollmann</last></author>
      <author><first>Anders</first><last>S&#248;gaard</last></author>
      <pages>205&#8211;211</pages>
      <abstract>This work shows that competitive translation results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single GPU for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and metrics.</abstract>
      <url hash="397e106b">2021.wat-1.24</url>
      <doi>10.18653/v1/2021.wat-1.24</doi>
      <bibkey>aralikatte-etal-2021-far</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
    </paper>
    <paper id="26">
      <title>Language Relatedness and Lexical Closeness can help Improve Multilingual <fixed-case>NMT</fixed-case>: <fixed-case>IITB</fixed-case>ombay@<fixed-case>M</fixed-case>ulti<fixed-case>I</fixed-case>ndic<fixed-case>NMT</fixed-case> <fixed-case>WAT</fixed-case>2021</title>
      <author><first>Jyotsana</first><last>Khatri</last></author>
      <author><first>Nikhil</first><last>Saini</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>217&#8211;223</pages>
      <abstract>Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for multiple languages. This paper describes our submission (Team ID: CFILT-IITB) for the MultiIndicMT: An Indic Language Multilingual Task at WAT 2021. We train multilingual NMT systems by sharing encoder and decoder parameters with language embedding associated with each token in both encoder and decoder. Furthermore, we demonstrate the use of transliteration (script conversion) for Indic languages in reducing the lexical gap for training a multilingual NMT system. Further, we show improvement in performance by training a multilingual NMT system using languages of the same family, i.e., related languages.</abstract>
      <url hash="6bfbb574">2021.wat-1.26</url>
      <doi>10.18653/v1/2021.wat-1.26</doi>
      <bibkey>khatri-etal-2021-language</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>S</fixed-case>amsung <fixed-case>R</fixed-case>&amp;<fixed-case>D</fixed-case> Institute <fixed-case>P</fixed-case>oland submission to <fixed-case>WAT</fixed-case> 2021 Indic Language Multilingual Task</title>
      <author><first>Adam</first><last>Dobrowolski</last></author>
      <author><first>Marcin</first><last>Szyma&#324;ski</last></author>
      <author><first>Marcin</first><last>Chochowski</last></author>
      <author><first>Pawe&#322;</first><last>Przybysz</last></author>
      <pages>224&#8211;232</pages>
      <abstract>This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&amp;D Institute Poland. The task covered translation between 10 Indic Languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil and Telugu) and English. We combined a variety of techniques: transliteration, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models. We applied an effective approach to low-resource training that consist of pretraining on backtranslations and tuning on parallel corpora. We experimented with two different domain-adaptation techniques which significantly improved translation quality when applied to monolingual corpora. We researched and applied a novel approach for finding the best hyperparameters for ensembling a number of translation models. All techniques combined gave significant improvement - up to +8 BLEU over baseline results. The quality of the models has been confirmed by the human evaluation where SRPOL models scored best for all 5 manually evaluated languages.</abstract>
      <url hash="27e2db9d">2021.wat-1.27</url>
      <doi>10.18653/v1/2021.wat-1.27</doi>
      <bibkey>dobrowolski-etal-2021-samsung</bibkey>
    </paper>
    <paper id="28">
      <title>Multilingual Machine Translation Systems at <fixed-case>WAT</fixed-case> 2021: One-to-Many and Many-to-One Transformer based <fixed-case>NMT</fixed-case></title>
      <author><first>Shivam</first><last>Mhaskar</last></author>
      <author><first>Aditya</first><last>Jain</last></author>
      <author><first>Aakash</first><last>Banerjee</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>233&#8211;237</pages>
      <abstract>In this paper, we present the details of the systems that we have submitted for the WAT 2021 MultiIndicMT: An Indic Language Multilingual Task. We have submitted two separate multilingual NMT models: one for English to 10 Indic languages and another for 10 Indic languages to English. We discuss the implementation details of two separate multilingual NMT approaches, namely one-to-many and many-to-one, that makes use of a shared decoder and a shared encoder, respectively. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration.</abstract>
      <url hash="57129c8f">2021.wat-1.28</url>
      <doi>10.18653/v1/2021.wat-1.28</doi>
      <bibkey>mhaskar-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>ANVITA</fixed-case> Machine Translation System for <fixed-case>WAT</fixed-case> 2021 <fixed-case>M</fixed-case>ulti<fixed-case>I</fixed-case>ndic<fixed-case>MT</fixed-case> Shared Task</title>
      <author><first>Pavanpankaj</first><last>Vegi</last></author>
      <author><first>Sivabhavani</first><last>J</last></author>
      <author><first>Biswajit</first><last>Paul</last></author>
      <author><first>Chitra</first><last>Viswanathan</last></author>
      <author><first>Prasanna Kumar</first><last>K R</last></author>
      <pages>244&#8211;249</pages>
      <abstract>This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions: English&#8594;Indic and Indic&#8594;English; Indic set comprised of 10 Indian languages. ANVITA-1.0 MT system comprised of two multi-lingual NMT models one for the English&#8594;Indic directions and other for the Indic&#8594;English directions with shared encoder-decoder, catering 10 language pairs and twenty translation directions. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization. Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training. ANVITA-1.0 achieved highest AM-FM score for English&#8594;Bengali, 2nd for English&#8594;Tamil and 3rd for English&#8594;Hindi, Bengali&#8594;English directions on official test set. In general, performance achieved by ANVITA for the Indic&#8594;English directions are relatively better than that of English&#8594;Indic directions for all the 10 language pairs when evaluated using BLEU and RIBES, although the same trend is not observed consistently when AM-FM based evaluation was carried out. As compared to BLEU, RIBES and AM-FM based scoring placed ANVITA relatively better among all the task participants.</abstract>
      <url hash="04b835ef">2021.wat-1.30</url>
      <doi>10.18653/v1/2021.wat-1.30</doi>
      <bibkey>vegi-etal-2021-anvita</bibkey>
    </paper>
  </volume>
</collection>