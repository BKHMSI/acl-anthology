<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.semeval">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Fourteenth Workshop on Semantic Evaluation</booktitle>
      <editor><first>Aurelie</first><last>Herbelot</last></editor>
      <editor><first>Xiaodan</first><last>Zhu</last></editor>
      <editor><first>Alexis</first><last>Palmer</last></editor>
      <editor><first>Nathan</first><last>Schneider</last></editor>
      <editor><first>Jonathan</first><last>May</last></editor>
      <editor><first>Ekaterina</first><last>Shutova</last></editor>
      <publisher>International Committee for Computational Linguistics</publisher>
      <address>Barcelona (online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="d141a7a7">2020.semeval-1.0</url>
      <bibkey>semeval-2020-semantic</bibkey>
    </frontmatter>
    <paper id="6">
      <title>Discovery Team at SemEval-2020 Task 1 : Context-sensitive Embeddings Not Always Better than Static for Semantic Change Detection<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 1: Context-sensitive Embeddings Not Always Better than Static for Semantic Change Detection</title>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Syrielle</first><last>Montariol</last></author>
      <author><first>Elaine</first><last>Zosa</last></author>
      <author><first>Lidia</first><last>Pivovarova</last></author>
      <pages>67–73</pages>
      <abstract>This paper describes the approaches used by the Discovery Team to solve SemEval-2020 Task 1-Unsupervised Lexical Semantic Change Detection. The proposed method is based on clustering of BERT contextual embeddings, followed by a comparison of cluster distributions across time. The best results were obtained by an <a href="https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)">ensemble</a> of this method and static Word2Vec embeddings. According to the official results, our approach proved the best for <a href="https://en.wikipedia.org/wiki/Latin">Latin</a> in Subtask 2.</abstract>
      <url hash="083ea248">2020.semeval-1.6</url>
      <bibkey>martinc-etal-2020-discovery</bibkey>
      <doi>10.18653/v1/2020.semeval-1.6</doi>
    </paper>
    <paper id="7">
      <title>GM-CTSC at SemEval-2020 Task 1 : Gaussian Mixtures Cross Temporal Similarity Clustering<fixed-case>GM</fixed-case>-<fixed-case>CTSC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 1: <fixed-case>G</fixed-case>aussian Mixtures Cross Temporal Similarity Clustering</title>
      <author><first>Pierluigi</first><last>Cassotti</last></author>
      <author><first>Annalina</first><last>Caputo</last></author>
      <author><first>Marco</first><last>Polignano</last></author>
      <author><first>Pierpaolo</first><last>Basile</last></author>
      <pages>74–80</pages>
      <abstract>This paper describes the system proposed by the Random team for SemEval-2020 Task 1 : Unsupervised Lexical Semantic Change Detection. We focus our <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> on the <a href="https://en.wikipedia.org/wiki/Detection_theory">detection problem</a>. Given the semantics of words captured by temporal word embeddings in different time periods, we investigate the use of <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised methods</a> to detect when the target word has gained or lost senses. To this end, we define a new <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> based on Gaussian Mixture Models to cluster the target similarities computed over the two periods. We compare the proposed approach with a number of <a href="https://en.wikipedia.org/wiki/Similarity_(geometry)">similarity-based thresholds</a>. We found that, although the performance of the detection methods varies across the word embedding algorithms, the combination of Gaussian Mixture with Temporal Referencing resulted in our best system.</abstract>
      <url hash="51866b0c">2020.semeval-1.7</url>
      <bibkey>cassotti-etal-2020-gm</bibkey>
      <doi>10.18653/v1/2020.semeval-1.7</doi>
    </paper>
    <paper id="10">
      <title>RIJP at SemEval-2020 Task 1 : Gaussian-based Embeddings for Semantic Change Detection<fixed-case>RIJP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 1: <fixed-case>G</fixed-case>aussian-based Embeddings for Semantic Change Detection</title>
      <author><first>Ran</first><last>Iwamoto</last></author>
      <author><first>Masahiro</first><last>Yukawa</last></author>
      <pages>98–104</pages>
      <abstract>This paper describes the model proposed and submitted by our RIJP team to SemEval 2020 Task1 : Unsupervised Lexical Semantic Change Detection. In the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, words are represented by <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian distributions</a>. For Subtask 1, the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieved average scores of 0.51 and 0.70 in the evaluation and post-evaluation processes, respectively. The higher score in the post-evaluation process than that in the evaluation process was achieved owing to appropriate parameter tuning. The results indicate that the proposed Gaussian-based embedding model is able to express <a href="https://en.wikipedia.org/wiki/Semantic_shift">semantic shifts</a> while having a low computational</abstract>
      <url hash="23708d8a">2020.semeval-1.10</url>
      <bibkey>iwamoto-yukawa-2020-rijp</bibkey>
      <doi>10.18653/v1/2020.semeval-1.10</doi>
    </paper>
    <paper id="14">
      <title>UiO-UvA at SemEval-2020 Task 1 : Contextualised Embeddings for Lexical Semantic Change Detection<fixed-case>U</fixed-case>i<fixed-case>O</fixed-case>-<fixed-case>U</fixed-case>v<fixed-case>A</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 1: Contextualised Embeddings for Lexical Semantic Change Detection</title>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Mario</first><last>Giulianelli</last></author>
      <pages>126–134</pages>
      <abstract>We apply contextualised word embeddings to lexical semantic change detection in the SemEval-2020 Shared Task 1. This paper focuses on Subtask 2, ranking words by the degree of their semantic drift over time. We analyse the performance of two contextualising architectures (BERT and ELMo) and three change detection algorithms. We find that the most effective algorithms rely on the cosine similarity between averaged token embeddings and the pairwise distances between token embeddings. They outperform strong baselines by a large margin (in the post-evaluation phase, we have the best Subtask 2 submission for SemEval-2020 Task 1), but interestingly, the choice of a particular <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a> depends on the distribution of gold scores in the test set.</abstract>
      <url hash="6f8c0547">2020.semeval-1.14</url>
      <bibkey>kutuzov-giulianelli-2020-uio</bibkey>
      <doi>10.18653/v1/2020.semeval-1.14</doi>
      <pwccode url="https://github.com/akutuzov/semeval2020" additional="false">akutuzov/semeval2020</pwccode>
    </paper>
    <paper id="15">
      <title>BMEAUT at SemEval-2020 Task 2 : <a href="https://en.wikipedia.org/wiki/Lexical_analysis">Lexical Entailment</a> with Semantic Graphs<fixed-case>BMEAUT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 2: Lexical Entailment with Semantic Graphs</title>
      <author><first>Ádám</first><last>Kovács</last></author>
      <author><first>Kinga</first><last>Gémes</last></author>
      <author><first>Andras</first><last>Kornai</last></author>
      <author><first>Gábor</first><last>Recski</last></author>
      <pages>135–141</pages>
      <abstract>In this paper we present a novel rule-based, language independent method for determining lexical entailment relations using semantic representations built from Wiktionary definitions. Combined with a simple WordNet-based method our system achieves top scores on the English and Italian datasets of the Semeval-2020 task Predicting Multilingual and Cross-lingual (graded) Lexical Entailment (Glava et al., 2020). A detailed error analysis of our output uncovers future di- rections for improving both the semantic parsing method and the inference process on semantic graphs.</abstract>
      <url hash="002844d9">2020.semeval-1.15</url>
      <bibkey>kovacs-etal-2020-bmeaut</bibkey>
      <doi>10.18653/v1/2020.semeval-1.15</doi>
    </paper>
    <paper id="16">
      <title>BRUMS at SemEval-2020 Task 3 : Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity<fixed-case>BRUMS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 3: Contextualised Embeddings for Predicting the (Graded) Effect of Context in Word Similarity</title>
      <author><first>Hansi</first><last>Hettiarachchi</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <pages>142–149</pages>
      <abstract>This paper presents the team BRUMS submission to SemEval-2020 Task 3 : Graded Word Similarity in Context. The system utilises state-of-the-art contextualised word embeddings, which have some task-specific adaptations, including stacked embeddings and average embeddings. Overall, the <a href="https://en.wikipedia.org/wiki/Software_development_process">approach</a> achieves good evaluation scores across all the languages, while maintaining simplicity. Following the final rankings, our approach is ranked within the top 5 solutions of each language while preserving the 1st position of Finnish subtask 2.</abstract>
      <url hash="3b0c34a9">2020.semeval-1.16</url>
      <bibkey>hettiarachchi-ranasinghe-2020-brums</bibkey>
      <doi>10.18653/v1/2020.semeval-1.16</doi>
      <pwccode url="https://github.com/HHansi/Semeval-2020-Task3" additional="false">HHansi/Semeval-2020-Task3</pwccode>
    </paper>
    <paper id="19">
      <title>UZH at SemEval-2020 Task 3 : Combining BERT with WordNet Sense Embeddings to Predict Graded Word Similarity Changes<fixed-case>UZH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 3: Combining <fixed-case>BERT</fixed-case> with <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Sense Embeddings to Predict Graded Word Similarity Changes</title>
      <author><first>Li</first><last>Tang</last></author>
      <pages>166–170</pages>
      <abstract>CoSimLex is a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> that can be used to evaluate the ability of context-dependent word embed- dings for modeling subtle, graded changes of meaning, as perceived by humans during reading. At SemEval-2020, task 3, subtask 1 is about predicting the (graded) effect of context in word similarity, using CoSimLex to quantify such a change of <a href="https://en.wikipedia.org/wiki/Similarity_measure">similarity</a> for a pair of words, from one context to another. Here, a meaning shift is composed of two aspects, a) discrete changes observed between different word senses, and b) more subtle changes of meaning representation that are not captured in those discrete changes. Therefore, this SemEval task was designed to allow the evaluation of <a href="https://en.wikipedia.org/wiki/System">systems</a> that can deal with a mix of both situations of <a href="https://en.wikipedia.org/wiki/Semantic_shift">semantic shift</a>, as they occur in the human perception of meaning. The described system was developed to improve the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">BERT baseline</a> provided with the task, by reducing distortions in the BERT semantic space, compared to the human semantic space. To this end, complementarity between 768- and 1024-dimensional BERT embeddings, and average word sense vectors were used. With this <a href="https://en.wikipedia.org/wiki/System">system</a>, after some fine-tuning, the baseline performance of 0.705 (uncentered Pearson correlation with human semantic shift data from 27 annotators) was enhanced by more than 6 %, to 0.7645. We hope that this work can make a contribution to further our understanding of the semantic vector space of human perception, as it can be modeled with context-dependent word embeddings in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing systems</a>.</abstract>
      <url hash="f01314b6">2020.semeval-1.19</url>
      <bibkey>tang-2020-uzh</bibkey>
      <doi>10.18653/v1/2020.semeval-1.19</doi>
    </paper>
    <paper id="23">
      <title>DCC-Uchile at SemEval-2020 Task 1 : Temporal Referencing Word Embeddings<fixed-case>DCC</fixed-case>-Uchile at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 1: Temporal Referencing Word Embeddings</title>
      <author><first>Frank D.</first><last>Zamora-Reina</last></author>
      <author><first>Felipe</first><last>Bravo-Marquez</last></author>
      <pages>194–200</pages>
      <abstract>We present a system for the task of unsupervised lexical change detection : given a target word and two corpora spanning different periods of time, automatically detects whether the word has lost or gained senses from one corpus to another. Our system employs the temporal referencing method to obtain compatible representations of target words in different periods of time. This is done by concatenating corpora of different periods and performing a temporal referencing of target words i.e., treating occurrences of target words in different periods as two independent tokens. Afterwards, we train <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> on the joint corpus and compare the referenced vectors of each target word using <a href="https://en.wikipedia.org/wiki/Cosine">cosine similarity</a>. Our submission was ranked 7th among 34 teams for subtask 1, obtaining an average accuracy of 0.637, only 0.050 points behind the first ranked system.</abstract>
      <url hash="7b1c00b5">2020.semeval-1.23</url>
      <bibkey>zamora-reina-bravo-marquez-2020-dcc</bibkey>
      <doi>10.18653/v1/2020.semeval-1.23</doi>
    </paper>
    <paper id="26">
      <title>SST-BERT at SemEval-2020 Task 1 : Semantic Shift Tracing by Clustering in BERT-based Embedding Spaces<fixed-case>SST</fixed-case>-<fixed-case>BERT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 1: Semantic Shift Tracing by Clustering in <fixed-case>BERT</fixed-case>-based Embedding Spaces</title>
      <author><first>Vani</first><last>Kanjirangat</last></author>
      <author><first>Sandra</first><last>Mitrovic</last></author>
      <author><first>Alessandro</first><last>Antonucci</last></author>
      <author><first>Fabio</first><last>Rinaldi</last></author>
      <pages>214–221</pages>
      <abstract>Lexical semantic change detection (also known as semantic shift tracing) is a task of identifying words that have changed their meaning over time. Unsupervised semantic shift tracing, focal point of SemEval2020, is particularly challenging. Given the <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised setup</a>, in this work, we propose to identify <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clusters</a> among different occurrences of each target word, considering these as representatives of different word meanings. As such, disagreements in obtained clusters naturally allow to quantify the level of <a href="https://en.wikipedia.org/wiki/Semantic_change">semantic shift</a> per each target word in four target languages. To leverage this idea, <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering</a> is performed on contextualized (BERT-based) embeddings of word occurrences. The obtained results show that our approach performs well both measured separately (per language) and overall, where we surpass all provided SemEval baselines.</abstract>
      <url hash="f4371019">2020.semeval-1.26</url>
      <bibkey>kanjirangat-etal-2020-sst</bibkey>
      <doi>10.18653/v1/2020.semeval-1.26</doi>
      <pwccode url="https://github.com/vanikanjirangat/SST_BERT-SEMEVAL_TASK1" additional="true">vanikanjirangat/SST_BERT-SEMEVAL_TASK1</pwccode>
    </paper>
    <paper id="27">
      <title>TemporalTeller at SemEval-2020 Task 1 : Unsupervised Lexical Semantic Change Detection with Temporal Referencing<fixed-case>T</fixed-case>emporal<fixed-case>T</fixed-case>eller at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Referencing</title>
      <author><first>Jinan</first><last>Zhou</last></author>
      <author><first>Jiaxin</first><last>Li</last></author>
      <pages>222–231</pages>
      <abstract>This paper describes our TemporalTeller system for SemEval Task 1 : Unsupervised Lexical Semantic Change Detection. We develop a unified framework for the common semantic change detection pipelines including preprocessing, learning word embeddings, calculating vector distances and determining threshold. We also propose Gamma Quantile Threshold to distinguish between changed and stable words. Based on our system, we conduct a comprehensive comparison among BERT, <a href="https://en.wikipedia.org/wiki/Skip-gram">Skip-gram</a>, Temporal Referencing and alignment-based methods. Evaluation results show that <a href="https://en.wikipedia.org/wiki/Skip-gram">Skip-gram</a> with Temporal Referencing achieves the best performance of 66.5 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">classification accuracy</a> and 51.8 % Spearman’s Ranking Correlation.</abstract>
      <url hash="0eeca120">2020.semeval-1.27</url>
      <bibkey>zhou-li-2020-temporalteller</bibkey>
      <doi>10.18653/v1/2020.semeval-1.27</doi>
    <title_fr>TemporalTeller à SemEval-2020 Tâche 1 : Détection de changement sémantique lexical non supervisée avec référencement temporel</title_fr>
      <title_ar>TemporalTeller في SemEval-2020 المهمة 1: الكشف عن التغيير المعجمي غير الخاضع للرقابة مع الإحالة الزمنية</title_ar>
      <title_es>TemporalTeller en SemEval-2020 Tarea 1: Detección de cambios semánticos léxicos no supervisados con referencia temporal</title_es>
      <title_pt>TemporalTeller na SemEval-2020 Tarefa 1: Detecção de Mudança Semântica Lexical Não Supervisionada com Referenciamento Temporal</title_pt>
      <title_ja>SemEval -2020のTemporalTellerタスク1 ：時間参照を使用した監督されていない語彙セマンティック変化検出</title_ja>
      <title_zh>SemEval-2020 务 1 者 TemporalTeller:用时引无监督词汇语义更检测</title_zh>
      <title_hi>TemLeval-2020 कार्य 1 पर TemporalTeller: अस्थायी संदर्भ के साथ Unsupervised Lexical Semantic Change Detection</title_hi>
      <title_ru>TemporalTeller на SemEval-2020 Задача 1: Неконтролируемое обнаружение лексических семантических изменений с временной привязкой</title_ru>
      <title_ga>Aithriseoir Sealadach ag SemEval-2020 Tasc 1: Brath Athrú Séimeantach Léacsúil gan Mhaoirseacht le Tagairtí Ama</title_ga>
      <title_ka>TemporalTeller at SemEval-2020 Task 1: Unupervised Lexical Semantic Change Detection with Temporal Referencing</title_ka>
      <title_el>Καθήκον 1: Ανίχνευση Λεξικής Σημματικής Αλλαγής χωρίς παρακολούθηση με Χρονική Αναφορά</title_el>
      <title_hu>TemporalTeller a SemEval-2020 1. feladat: Felügyeletlen Lexikus szemantikus változások észlelése időszakos hivatkozással</title_hu>
      <title_it>TemporalTeller a SemEval-2020 Task 1: Rilevazione del cambiamento semantico lessicale non supervisionata con riferimento temporale</title_it>
      <title_kk>Температуралық Teller 1- тапсырмасында: Температуралық референциясы арқылы тексерілмеген лексикалық семантикалық өзгерістерді анықтау</title_kk>
      <title_mk>TemporalTeller на SemEval-2020 задача 1: Ненадгледувано детективирање на лексикална семантична промена со температурна референција</title_mk>
      <title_lt>TemporalTeller at SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Referencing</title_lt>
      <title_ms>Teller Temporal pada SemEval-2020 Tugas 1: Pengesanan Ubah Semantik Leksikal Tidak Dikawal dengan Referensi Temporal</title_ms>
      <title_ml>സെമ്എവാല്‍- 2020 ടാസ്ക് 1- ല്‍ നിലവിലുള്ള ടെല്‍ലറ്‍: നേരത്തെ റെഫറന്‍സിങ്ങുമായി ലെക്സിക്സിക്കല്‍ സെമാന്റിക് മാറ്റ</title_ml>
      <title_mt>TemporalTeller f’SemEval-2020 Task 1: Detezzjoni ta’ Bidla Semantika Lessika Mhux Sorveljata b’Referenza Temporali</title_mt>
      <title_no>TemporalTeller på semiEval- 2020 oppgåve 1: Utgjeven leksisk semiantisk endring med mellombels referanse</title_no>
      <title_pl>TemporalTeller w SemEval-2020 Zadanie 1: Nienadzorowane wykrywanie zmian semantycznych z odniesieniem czasowym</title_pl>
      <title_mn>SemEval-2020 Task 1: Температур сэтгэл хөдлөлийн өөрчлөлтийн тодорхойлолт</title_mn>
      <title_ro>TemporalTeller la SemEval-2020 Sarcina 1: Detectarea schimbărilor semantice lexice nesupravegheate cu referință temporală</title_ro>
      <title_sr>Temporalni Teller na zadatku 1. za polu-Eval-2020: neodređena detekcija leksičkih semantičkih promjena sa vremenskim referencijama</title_sr>
      <title_si>Temporal Teleler at Halfeval-202 Job 1: Unurervised Lexical semantic Changing Detection with Temporal Reference</title_si>
      <title_so>Teller at SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Reference</title_so>
      <title_sv>TemporalTeller på SemEval-2020 Uppgift 1: Uppföljd Lexisk Semantic Change Detection med Temporal Referencing</title_sv>
      <title_ta>SemEval- 2020 பணியில் தற்காலிக தொலைபேசி</title_ta>
      <title_ur>ٹاکس ۱ پر تیموال ٹیلر: تیموال ریفرنسینگ کے ساتھ غیر قابل تغییر کی جگہ</title_ur>
      <title_uz>Vaqtincha toĘ»xtatish</title_uz>
      <title_vi>Tạm biệt Bố trí tại SemEvol-2020 Task 1: trinh sát tinh thần giao dịch bằng thời gian tham khảo cố thời gian</title_vi>
      <title_bg>Задача 1: Неконтролирано откриване на лексикални семантични промени с времево препращане</title_bg>
      <title_hr>Temporalni ispitivač na zadatku 1. za pola Eval-2020: neodržavana detekcija leksičke semantičke promjene s vremenskim referencijama</title_hr>
      <title_nl>Tijdelijke Teller bij SemEval-2020 Taak 1: Onbewaakte Lexische Semantische Veranderdetectie met Tijdelijke Referentie</title_nl>
      <title_da>TemporalTeller på SemEval-2020 Opgave 1: Ikke-overvåget Lexical Semantic Change Detection med Temporal Referencing</title_da>
      <title_ko>SemEval-20 작업 1: 시간 참조를 사용하여 감독되지 않은 단어의 의미 변화 감지</title_ko>
      <title_de>TemporalTeller bei SemEval-2020 Aufgabenstellung 1: Unservised Lexical Semantic Change Detection with Temporal Referencing</title_de>
      <title_fa>Teller TemporalTeller at SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Referencing</title_fa>
      <title_sw>Kituo cha muda mfupi katika kazi ya SemEval-2020 1: Utafiti wa Mabadiliko ya Kitendo cha Kilexico na Kumbuliwa kwa muda mfupi</title_sw>
      <title_tr>Wagtlaýyn Teller at SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Referencing</title_tr>
      <title_af>Tydelike Teller by SemEval- 2020 Opdrag 1: Onondersteunde Leksiese Semantiese Verandering Opdekking met Temporale Verwysing</title_af>
      <title_sq>TemporalTeller në SemEval-2020 Task 1: Detektimi i Ndryshimit Semantik Leksikal pa mbikqyrje me Referencë Temporale</title_sq>
      <title_id>TemporalTeller di SemEval-2020 Task 1: Deteksi Perubahan Semantik Lexik tanpa diawasi dengan Referensi Temporal</title_id>
      <title_az>SemEval-2020 Task 1'd…ô TemporalTeller: Temporal Referencing il…ô mΟΦ…ôyy…ôn edilm…ômi≈ü Lexical Semantic Change Detection</title_az>
      <title_bn>সেমEval- 2020 টাস্ক ১-এ Temporal Teller: অনভারওয়েড লেক্সিক্সিকাল সেম্যান্টিক পরিবর্তনের ডিটেক্টরেশন Temporal Referation সহ</title_bn>
      <title_bs>Temporalni Teller na zadatku 1. poluvremeno-2020: neodržavana detekcija leksičkih semantičkih promjena s vremenskim referencijama</title_bs>
      <title_am>Teller at SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Reference</title_am>
      <title_hy>Տեմպորալ</title_hy>
      <title_et>TemporalTeller SemEval-2020 ülesanne 1: järelevalveta leksiliste semantiliste muutuste tuvastamine ajalise viitamisega</title_et>
      <title_cs>Úkol 1: Nekontrolovaná lexikální sémantická detekce změn s časovým referenčním systémem</title_cs>
      <title_fi>TemporalTeller SemEval-2020 -tapahtumassa Tehtävä 1: valvomaton Lexical Semantic Change Detection with Temporal Referencing</title_fi>
      <title_ca>TemporalTeller en SemEval-2020 tasca 1: Detecció de canvis Semàtics Lexics sense supervisió amb referencia temporal</title_ca>
      <title_jv>Time oralteller at semi-2020 task 1</title_jv>
      <title_he>TemporalTeller at SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Referencing</title_he>
      <title_sk>TemporalTeller na SemEval-2020 Naloga 1: Nenadzorovano leksično semantično zaznavanje sprememb s časovnim referenciranjem</title_sk>
      <title_ha>TemporalTeller at SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Referencing</title_ha>
      <title_bo>TemporalTeller at SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Referencing</title_bo>
      <abstract_ar>تصف هذه الورقة نظام TemporalTeller الخاص بنا لمهمة SemEval 1: اكتشاف التغيير المعجمي غير الخاضع للإشراف. نقوم بتطوير إطار عمل موحد لخطوط أنابيب الكشف عن التغيير الدلالي الشائعة بما في ذلك المعالجة المسبقة ، وتعلم زخارف الكلمات ، وحساب مسافات المتجهات وتحديد العتبة. نقترح أيضًا عتبة جاما الكمية للتمييز بين الكلمات المتغيرة والمستقرة. استنادًا إلى نظامنا ، نجري مقارنة شاملة بين أساليب BERT و Skip-gram والمراجع الزمنية والطرق القائمة على المحاذاة. تظهر نتائج التقييم أن Skip-gram مع الإسناد الزمني يحقق أفضل أداء بنسبة 66.5٪ من دقة التصنيف و 51.8٪ Spearman's Ranking Correlation.</abstract_ar>
      <abstract_fr>Cet article décrit notre système TemporalTeller pour SemEval Task 1 : Unsupervised Lexical Semantic Change Detection. Nous développons un cadre unifié pour les pipelines de détection de changement sémantique courants, y compris le prétraitement, l'apprentissage des intégrations de mots, le calcul de distances vectorielles et la détermination de seuils. Nous proposons également un seuil quantile gamma pour faire la distinction entre les mots modifiés et les mots stables. Sur la base de notre système, nous effectuons une comparaison complète entre les méthodes BERT, Skip-gram, Temporal Referencing et basées sur l'alignement. Les résultats de l'évaluation montrent que Skip-gram avec référencement temporel permet d'obtenir les meilleures performances avec une précision de classification de 66,5 % et une corrélation de classement de Spearman de 51,8 %.</abstract_fr>
      <abstract_es>Este artículo describe nuestro sistema TemporalTeller para la Tarea 1 de SemEval: Detección de cambios semánticos léxicos no supervisados. Desarrollamos un marco unificado para las canalizaciones comunes de detección de cambios semánticos que incluyen el preprocesamiento, el aprendizaje de incrustaciones de palabras, el cálculo de distancias vectoriales y la determinación del umbral. También proponemos el Umbral Cuantil Gamma para distinguir entre palabras cambiadas y estables. Basado en nuestro sistema, realizamos una comparación exhaustiva entre los métodos BERT, Skip-gram, referencia temporal y alineación. Los resultados de la evaluación muestran que Skip-gram con referencia temporal logra el mejor rendimiento de 66,5% de precisión de clasificación y 51,8% de correlación de clasificación de Spearman.</abstract_es>
      <abstract_pt>Este artigo descreve nosso sistema TemporalTeller para SemEval Tarefa 1: Detecção de Mudança Semântica Lexical Não Supervisionada. Desenvolvemos uma estrutura unificada para os pipelines comuns de detecção de alterações semânticas, incluindo pré-processamento, aprendizado de incorporação de palavras, cálculo de distâncias vetoriais e determinação de limites. Também propomos o Gamma Quantile Threshold para distinguir entre palavras alteradas e estáveis. Com base em nosso sistema, realizamos uma comparação abrangente entre BERT, Skip-gram, Referenciamento Temporal e métodos baseados em alinhamento. Os resultados da avaliação mostram que Skip-gram com Referenciamento Temporal alcança o melhor desempenho de 66,5% de precisão de classificação e 51,8% de Correlação de Ranking de Spearman.</abstract_pt>
      <abstract_zh>本文言SemEval务1者TemporalTeller系统:无监词汇语义变检。 臣等为常见之语义变检管道开一统之框架,预处理、学嵌、计算矢量距定阈值。 又有伽玛分位数阈值以别定单词。 基于我统,我之于BERT,Skip-gram之于我,参之于齐。 论结果表明,有时而参者Skip-gram成66.5%之类准确率与51.8%之Spearman名相关性之最善者也。</abstract_zh>
      <abstract_ru>В этой статье описывается наша система TemporalTeller для задачи SemEval 1: Неконтролируемое обнаружение лексических семантических изменений. Мы разрабатываем единый фреймворк для общих конвейеров обнаружения семантических изменений, включая предварительную обработку, изучение вложений слов, вычисление векторных расстояний и определение порога. Мы также предлагаем Gamma Quantile Threshold, чтобы различать измененные и стабильные слова. Основываясь на нашей системе, мы проводим комплексное сравнение методов BERT, Skip-gram, Temporal Referencing и alignment-based. Результаты оценки показывают, что Скип-грамма с временной привязкой достигает наилучших показателей точности классификации 66,5% и корреляции рейтинга Спирмена 51,8%.</abstract_ru>
      <abstract_ja>本稿では、SemEval Task 1: Unsupervised Lexical Semantic Change DetectionのTemporalTellerシステムについて説明します。私たちは、前処理、単語埋め込みの学習、ベクトル距離の計算、閾値の決定を含む、共通の意味変化検出パイプラインのための統一されたフレームワークを開発します。また、変化した単語と安定した単語を区別するために、Gamma Quantile Thresholdを提案しています。当社のシステムに基づいて、BERT、SKIP - GRAM、Temporal Referencecing、およびアライメントベースの方法の包括的な比較を行います。評価結果は、テンポラルリファレンスを使用したスキップグラムが、66.5%の分類精度と51.8%のSpearman 's Ranking Correlationの最高のパフォーマンスを達成することを示しています。</abstract_ja>
      <abstract_hi>यह पेपर SemEval कार्य 1 के लिए हमारे TemporalTeller प्रणाली का वर्णन करता है: Unsupervised Lexical Semantic Change Detection. हम सामान्य शब्दार्थ परिवर्तन का पता लगाने पाइपलाइनों के लिए एक एकीकृत रूपरेखा विकसित करते हैं जिसमें प्रीप्रोसेसिंग, शब्द एम्बेडिंग सीखना, वेक्टर दूरी की गणना करना और थ्रेशोल्ड का निर्धारण करना शामिल है। हम परिवर्तित और स्थिर शब्दों के बीच अंतर करने के लिए गामा क्वांट्री थ्रेशोल्ड का भी प्रस्ताव करते हैं। हमारी प्रणाली के आधार पर, हम BERT, Skip-gram, Temporal Referencing और संरेखण-आधारित तरीकों के बीच एक व्यापक तुलना करते हैं। मूल्यांकन परिणामों से पता चलता है कि टेम्पोरल रेफरेंसिंग के साथ स्किप-ग्राम 66.5% वर्गीकरण सटीकता और 51.8% स्पीयरमैन की रैंकिंग सहसंबंध का सबसे अच्छा प्रदर्शन प्राप्त करता है।</abstract_hi>
      <abstract_ga>Déanann an páipéar seo cur síos ar ár gcóras TemporalTeller le haghaidh Tasc SemEval 1: Brath Athrú Séimeantach Léacsúil gan Mhaoirseacht. Forbraímid creat aontaithe le haghaidh na bpíblínte braite athraithe shéimeantacha coitianta lena n-áirítear réamhphróiseáil, foghlaim leabú focal, ríomh achair veicteoirí agus an tairseach a chinneadh. Molaimid freisin Gáma Cainníochtúil Tairseach chun idirdhealú a dhéanamh idir focail athraithe agus focail chobhsaí. Bunaithe ar ár gcóras, déanaimid comparáid chuimsitheach i measc BERT, Skip-gram, Tagairtí Sealadacha agus modhanna atá bunaithe ar ailíniú. Léiríonn torthaí meastóireachta go mbaineann Skip-gram le Tagairtí Ama an fheidhmíocht is fearr de chruinneas aicmithe 66.5% agus 51.8% Comhghaol Rangaithe Spearman.</abstract_ga>
      <abstract_ka>ჩვენი სამუშაო ტემპორალური ტელერილის სისტემა სამუშაო სამუშაო დავალებისთვის 1: არ განსხვავებული ლექსიკური Semantic Change Detection. ჩვენ განვითარებთ ერთადერთი ფრამეტრი საერთო სემონტიკური ცვლილების განვითარება ფესვილებისთვის, რომელიც პრეპროცესი, სწავლობა სიტყვების შემდეგება, განვითარება გვეკტო ჩვენ ასევე დამმა კვანტალური სიტყვების განსხვავება განცვლებული და სტაბლური სიტყვების შორის. ჩვენი სისტემაზე, ჩვენ ვაკეთებთ BERT, Skip-gram, Temporal Referencing and alignment-based methods. შედეგი განსაზღვრების შედეგი დააჩვენება, რომ ტემპორალური განსაზღვრებით Skip-gram მიიღება 66,5% კლასიფიკაციის სიმართლე და 51,8% სპერმანის კოლიფიკაციის კოლიფიკაციის კოლიფიკაცია.</abstract_ka>
      <abstract_hu>Ez a tanulmány bemutatja TemporalTeller rendszerünket a SemEval feladat 1: Felügyelet nélküli lexikai szemantikus változások észleléséhez. Egységes keretrendszert dolgozunk ki a közös szemantikai változások észlelésére, beleértve az előfeldolgozást, a szóbeágyazásokat, a vektortávolságok kiszámítását és a küszöb meghatározását. Azt is javasoljuk, hogy különbséget tegyünk a megváltozott és stabil szavak között. Rendszerünk alapján átfogó összehasonlítást végzünk a BERT, Skip-gram, Temporal Referencing és az igazítás alapú módszerek között. Az értékelési eredmények azt mutatják, hogy a Temporal Referencing Skip-gram 66,5%-os osztályozási pontossággal és 51,8%-os Spearman rangsorolási korrelációval éri el a legjobb teljesítményt.</abstract_hu>
      <abstract_el>Αυτή η εργασία περιγράφει το σύστημα για την εργασία 1: Μη εποπτευόμενη Λεξική Σημαντική Ανίχνευση Αλλαγών. Αναπτύσσουμε ένα ενοποιημένο πλαίσιο για τους κοινούς αγωγούς ανίχνευσης σημασιολογικών αλλαγών, συμπεριλαμβανομένων της προεπεξεργασίας, της ενσωμάτωσης λέξεων εκμάθησης, του υπολογισμού διανυσματικών αποστάσεων και του προσδιορισμού του κατώτατου ορίου. Προτείνουμε επίσης το ποσοτικό όριο Γάμμα για να διακρίνουμε μεταξύ αλλαγμένων και σταθερών λέξεων. Με βάση το σύστημά μας, διεξάγουμε μια ολοκληρωμένη σύγκριση μεταξύ μεθόδων και μεθόδων που βασίζονται στην ευθυγράμμιση. Τα αποτελέσματα αξιολόγησης δείχνουν ότι το πρόγραμμα με προσωρινή αναφορά επιτυγχάνει την καλύτερη απόδοση της ακρίβειας ταξινόμησης 66,5% και της συσχέτισης κατάταξης 51,8% του Σπίαρμαν.</abstract_el>
      <abstract_it>Questo articolo descrive il nostro sistema TemporalTeller per SemEval Task 1: Rilevazione di cambiamenti semantici lessicali non supervisionati. Sviluppiamo un framework unificato per le pipeline comuni di rilevamento dei cambiamenti semantici, tra cui il preelaborazione, l'apprendimento delle incorporazioni di parole, il calcolo delle distanze vettoriali e la determinazione della soglia. Proponiamo anche Gamma Quantile Threshold per distinguere tra parole modificate e parole stabili. Sulla base del nostro sistema, eseguiamo un confronto completo tra BERT, Skip-gram, Temporal Referencing e metodi basati sull'allineamento. I risultati della valutazione mostrano che Skip-gram con riferimento temporale raggiunge le migliori prestazioni del 66,5% di accuratezza della classificazione e del 51,8% di correlazione del ranking di Spearman.</abstract_it>
      <abstract_kk>Бұл қағаз біздің бірінші жалпы тапсырма үшін уақытша теллер жүйесімізді таңдайды: сақталмаған лексикалық бірінші өзгерістерді анықтау. Біз біріктірілген семантикалық өзгертулерді табу құрылғыларының біріктірілген қоршауын құрамыз. Алдын- ала өзгерту, сөздерді ендіру, вектордың қашықтығын есептеп, шегін аны Сонымен қатар, өзгертілген және stable сөздер арасындағы айырмашылығын Gamma Quantile деңгейін ұсынамыз. Жүйеміздің негізінде BERT, Skip-gram, Temporal Referencing және alignment-based әдістер арасындағы толық салыстырып тұрмыз. Температуралық референцияларды өткізу граммасының 66,5% классификациялық дұрыстығын және 51,8% Спеармандың референциялық коррекциясының ең дұрыстығын көрсетеді.</abstract_kk>
      <abstract_ml>ഈ പത്രത്തില്‍ സെമ്എവാല്‍ ടാസ്ക് 1- നുള്ള നമ്മുടെ ടെമ്പ്പോര്‍ട്ടല്‍ സിസ്റ്റം വിശദീകരിക്കുന്നു സെമാന്റിക് മാറ്റങ്ങള്‍ കണ്ടെത്തുന്നതിനുള്ള പൈപ്പെലിനുകള്‍ക്കുള്ള ഒരു യൂണിക്കേറ്റ് ഫ്രെയിമെക്ക് ഞങ്ങള്‍ നിര്‍മ്മിക്കുന്നു. പ ഞങ്ങള്‍ ഗാമ ക്വാന്റില്‍ ത്രെഷോള്‍ഡിനെയും മാറ്റുന്നതും സ്ഥിരമായ വാക്കുകള്‍ക്കിടയില്‍ വേര്‍പെടുത്താന നമ്മുടെ സിസ്റ്റത്തിന്റെ അടിസ്ഥാനത്തില്‍, ബെര്‍ട്ടി, സ്കിപ്പ്-ഗ്രാമിന്റെ ഇടയില്‍ നമ്മള്‍ ഒരു പൂര്‍ണ്ണമായ തുല്യമാണ് നടത്തുന് Evaluation results show that Skip-gram with Temporal Referencing achieves the best performance of 66.5% classification accuracy and 51.8% Spearman's Ranking Correlation.</abstract_ml>
      <abstract_ms>Kertas ini menggambarkan sistem TemporalTeller untuk SemEval Task 1: Pengesanan Perubahan Semantik Leksikal Tidak Dipengawasi. Kami mengembangkan kerangka bersatu untuk garis paip pengesan perubahan semantik yang biasa termasuk pemprosesan awal, belajar penyampaian perkataan, mengira jarak vektor dan menentukan ambang. Kami juga melamar batas kuantil Gamma untuk membezakan antara perkataan yang berubah dan perkataan yang stabil. Berdasarkan sistem kita, kita melakukan perbandingan yang meliputi antara BERT, Skip-gram, Temporal Referencing dan kaedah berdasarkan alignment. Keputusan penilaian menunjukkan bahawa Skip-gram dengan Referensi Temporal mencapai prestasi terbaik kepakuan kelas 66.5% dan 51.8% Korelasi Ranking Spearman.</abstract_ms>
      <abstract_mt>Dan id-dokument jiddeskrivi s-sistema TemporalTeller tagħna għal SemEval Task 1: Sejbien ta’ Bidla Semantika Lessika Mhux Sorveljata. Aħna niżviluppaw qafas unifikat għall-pipelines komuni għall-identifikazzjoni tal-bidliet semantiċi inklużi l-ipproċessar minn qabel, it-tagħlim tal-inkorporazzjoni tal-kliem, il-kalkolu tad-distanzi tal-vetturi u d-determinazzjoni tal-limitu. Aħna nipproponu wkoll Limitu Kwantili Gamma biex niddistingwu bejn kliem mibdul u stabbli. Based on our system, we conduct a comprehensive comparison among BERT, Skip-gram, Temporal Referencing and alignment-based methods.  Ir-riżultati tal-evalwazzjoni juru li Skip-gram b’Referenza Temporali jikseb l-aħjar prestazzjoni ta’ preċiżjoni tal-klassifikazzjoni ta’ 66.5% u 51.8% tal-Korelazzjoni tal-Klassifikazzjoni ta’ Spearman.</abstract_mt>
      <abstract_mn>Энэ цаас бидний SemEval Task 1-ийн Температик Телейлер системийг тайлбарладаг: Хэрэглэгдэхгүй Лексик Semantic Change Detection. Бид ерөнхий semantic өөрчлөлтийн шинэчлэлийн хоолойн шулуунуудын нэгтгэл хөрвөл бүтээж, аль боловсруулах, суралцах үг нэвтрүүлэх, векторын зайг тооцоолж, хязгаарыг тооцоолж байна. Мөн бид Gamma Quantile хэмжээний хэмжээг өөрчлөгдсөн, тогтвортой үгсийн хоорондоо ялгах гэсэн санал өгдөг. Бид системийн үндсэнээр BERT, Skip-gram, Temporal Referencing and alignment-based Methods хоорондоо бүрэн харьцуулсан. Тайлбарлалтын үр дүнд Температур сэтгэл хөдлөлтэй Skip-грамм нь 66.5% хувьд хуваалтын зөв байдал болон Спеарманын түвшин зөвхөн 51.8% байдалтай хамгийн сайн үйл ажиллагааг гаргадаг.</abstract_mn>
      <abstract_pl>Niniejszy artykuł opisuje nasz system TemporalTeller dla SemEval Task 1: Niemonitorowane Lexical Semantic Change Detection. Opracowujemy ujednolicone ramy dla wspólnych rurociągów wykrywania zmian semantycznych, w tym wstępnego przetwarzania, uczenia się osadzeń słów, obliczania odległości wektorów i określania progu. Proponujemy również Gamma Quantile Threshold, aby rozróżnić zmienione i stabilne słowa. W oparciu o nasz system przeprowadzamy kompleksowe porównanie metod BERT, Skip-gram, Temporal Referencing oraz opartych na wyrównaniu. Wyniki oceny pokazują, że Skip-gram z referencją czasową osiąga najlepszą wydajność klasyfikacji 66,5% i korelację rankingową 51,8% Spearmana.</abstract_pl>
      <abstract_mk>Овој весник го опишува нашиот систем на ТемпорталТелер за SemEval задача 1: Ненадгледувано детекција на лексикална семантична промена. Развиваме единствена рамка за заедничките семантични гасоводи за детекција на промени, вклучувајќи го препроцесот, учењето на вложување на зборови, пресметувањето на векторните дистанци и одредувањето на прагот. Ние, исто така, предложуваме Гама квантилна граница за разлика помеѓу променетите и стабилните зборови. На основа на нашиот систем, спроведуваме комплетна споредба помеѓу BERT, Skip-gram, Temporal Referencing и alignment-based methods. Резултатите од евалуацијата покажуваат дека Скип-грам со привремено референцирање ја постигнува најдобрата резултат на точноста на класификацијата од 66,5 отсто и 51,8 отсто од рангирањето на претседателот.</abstract_mk>
      <abstract_no>Denne papiret beskriver vår mellombels teljarsystemet for halvEval oppgåve 1: Utgjeven leksisk semantisk endring. Vi utviklar eit enkelt rammeverk for det vanlege semantiske oppdagingsrutenettet, inkludert førehandsaming, læring av ordinnbygging, rekna ut vektoravstandane og bestemmer grenseverdi. Vi foreslår også Gamma Quantile- grenseverdi for å distisera mellom endra og stabile ord. Basert på systemet vårt, gjer vi ein komplett sammenligning mellom BERT, Hopp-gram, Temporale Referensing og Alimentasjonsbasert metodar. Evalueringsresultat viser at Skip-gram med Temporale Referensing gjer det beste utviklinga av 66,5 % klassifikasjons nøyaktighet og 51,8 % Spearman sin Ranking Correlation.</abstract_no>
      <abstract_ro>Această lucrare descrie sistemul nostru TemporalTeller pentru SemEval Task 1: Detectarea schimbărilor semantice Lexicale nesupravegheate. Dezvoltăm un cadru unificat pentru conductele comune de detectare a schimbărilor semantice, inclusiv pre-procesarea, învățarea încorporărilor de cuvinte, calcularea distanțelor vectoriale și determinarea pragului. De asemenea, propunem pragul Gamma Quantile pentru a distinge între cuvintele modificate și stabile. Pe baza sistemului nostru, efectuăm o comparație cuprinzătoare între metodele BERT, Skip-gram, Referință temporală și aliniere bazate pe metode. Rezultatele evaluării arată că Skip-gram cu referință temporală obține cea mai bună performanță de 66,5% precizie de clasificare și 51,8% corelație de clasificare a lui Spearman.</abstract_ro>
      <abstract_sr>Ovaj papir opisuje naš privremeni telefonski sistem za poluvremeni zadatak 1: neodređena detekcija leksičkih semantičkih promjena. Razvijamo ujedinjeni okvir za zajedničke semantičke promene detektivne cijevi, uključujući preobradu, učenje rečenica, izračunavanje udaljenosti vektora i određivanje praga. Takoðe predlažemo Gamma Quantile Threshold da se razlikuje izmeðu promjene i stabilne reèi. Na osnovu našeg sustava, vodimo kompleksno usporedbe među BERT-om, Skip-gramom, Temporalnim referencijama i metodama na osnovu usporedbe. Rezultati procjene pokazuju da Skip-gram sa Temporalnim referencijama postiže najbolji učinkovit 66,5% tačnosti klasifikacije i 51,8% korelacije Spearmana Ranking.</abstract_sr>
      <abstract_so>Kanu wuxuu ku qoran yahay nidaamka ku meelgaarka ah ee SemEval shaqo 1: Unsupervised Lexical Semantic Change. Waxaynu horumarinaynaa firaaqad isku dayn ah oo loo qorayo baaritaanka isbedelka ee caadiga ah, kuwaas oo ka mid ah baaraandegista, barashada hadalka, xisaabta fogaanka wadooyinka iyo go'aanka. Sidoo kale waxaan horumarinaynaa Gamma Quantile Threshold inuu kala sooco hadallada is-beddelan iyo si stabile ah. Sida uu nidaamka ku saleysan yahay, waxaynu ku sameynaa isbardhig hoose ah BERT, Skip-gram, hababka ku meelgaarka ah ee soo jeedidda iyo isbedelka. Evaluation results show that Skip-gram with Temporal Referencing achieves the best performance of 66.5% classification accuracy and 51.8% Spearman's Ranking Correlation.</abstract_so>
      <abstract_si>මේ පැත්තේ අපේ තාමාවික ටෙල්ලර් පද්ධතිය සෙම්වෙල් වැඩක් 1 ගැන විස්තර කරනවා: සුරක්ෂිත ලෙක්සිකාල් සෙම්පැන් අපි සාමාන්‍ය සෙමාන්ටික වෙනස් පරීක්ෂණ පායිප්ලින්ස් එක්ක සංවිධානයක් විස්තර කරනවා, පරීක්ෂණය සමඟ පරීක්ෂණය සඳහා  අපි ගැමා ක්වාන්ටිල් ත්‍රෙෂෝල්ඩ් වලින් වෙනස් වෙනුවෙන් ස්ථිර වචන වලින් වෙනස් කරන්න ප්‍රශ්නය අපේ පද්ධතියේ අධාරිත, අපි BERT, Skip-gram, Temporal Reference and Aliment-based විද්‍යාවක් සම්පූර්ණ විද්‍යාවක් කරනවා. විශ්ලේෂණ ප්‍රතිචාරයක් පෙන්වන්නේ ස්කිප් ග්‍රාම්ම් එක්ක තමයාලාවක් ප්‍රතිචාරණය සඳහා 66.5% විශේෂණ විශේෂණය සමාන්ත</abstract_si>
      <abstract_sv>Denna uppsats beskriver vårt TemporalTeller-system för SemEval Task 1: Oserverad Lexical Semantic Change Detection. Vi utvecklar ett enhetligt ramverk för gemensamma system för detektering av semantiska förändringar inklusive förbehandling, inlärning av ordinbäddningar, beräkning av vektoravstånd och bestämning av tröskel. Vi föreslår också Gamma Quantile Threshold för att skilja mellan ändrade och stabila ord. Baserat på vårt system genomför vi en omfattande jämförelse mellan BERT, Skip-gram, Temporal Referencing och justeringsbaserade metoder. Utvärderingsresultat visar att Skip-gram med Temporal Referencing uppnår den bästa prestandan med 66,5% klassificeringsnoggrannhet och 51,8% Spearmans Ranking Correlation.</abstract_sv>
      <abstract_lt>This paper describes our TemporalTeller system for SemEval Task 1: Unsupervised Lexical Semantic Change Detection.  We develop a unified framework for the common semantic change detection pipelines including preprocessing, learning word embeddings, calculating vector distances and determining threshold.  Taip pat siūlome gamos kiekybinę ribą skirti pakeistus ir stabilius žodžius. Remiantis mūsų sistema, atliekame išsamų BERT, Skip-gram, Temporarinės nuorodos ir suderinimo metodų palyginimą. Vertinimo rezultatai rodo, kad „Skip-gram“ su laikina nuoroda geriausiai atitinka 66,5 % klasifikacijos tikslumą ir 51,8 % Spearmano rango koreliaciją.</abstract_lt>
      <abstract_ta>இந்த தாள் SemEval பணிக்கான எங்கள் தற்காலிக தொலைபேசி அமைப்பை விவரிக்கும்: லெக்சிக்சியல் செமான்டிக் மாற்றம் கண்டுபி நாம் பொதுவான மாற்றம் கண்டுபிடிப்பு பைப்பெல்களுக்கான ஒரு ஒன்றிணைக்கப்பட்ட சட்டத்தை உருவாக்குகிறோம், முன்செயல்படுத்தல், சொல்லு உள் நாம் காமா குவாண்டிக் குறிப்பு முறையிலும் நிலையான வார்த்தைகளுக்கும் இடையே பிரித்துக் கொள் எங்கள் கணினியை அடிப்படையில், நாங்கள் BERT, ஸ்கிப்- கிராம், தற்காலிக குறிப்பு மற்றும் ஒழுங்கு முறைமைகளில் ஒரு முழு ஒப்பீடு ச தற்காலிக க குறிப்பிடும் விளைவுகள் காட்டுகிறது 66.5% வகுப்பு சரியான செயல்பாட்டை பெறுகிறது மற்றும் 51.8% ஸ்பெயர்மான் விரிவான தொடர்பு</abstract_ta>
      <abstract_ur>This paper describes our TemporalTeller system for SemEval Task 1: Unsupervised Lexical Semantic Change Detection. ہم نے ایک متحدہ فرم ایجاد کر دی ہے ایک متحدہ سیمنٹی تبدیل پیپ لین کے لئے جو پہلے پرپرپرس کرتی ہے، کلمات ابڈینگ کی تعلیم کرتی ہے، ویکتور فاصلہ کا شمار کرتی ہے اور درش کی تعلیم کرتی ہے. ہم نے بھی Gamma Quantile Threshold کو پیشنهاد کرتا ہے کہ تغییر اور ثابت کلمات کے درمیان اختلاف کرے۔ ہمارے سیستم پر، ہم BERT، Skip-gram، Temporal Referencing اور Alignment-based روش کے درمیان ایک کامل مقایسہ کرتے ہیں. ارزیابی نتائج دکھاتے ہیں کہ Temporal Referencing کے ساتھ Skip-gram کو 66.5% کلاسیفوں کی دقیق اور 51.8% Spearman کی Ranking Correlation کے سب سے بہترین فعالیت حاصل کرتا ہے.</abstract_ur>
      <abstract_uz>Name Biz birlashtirilgan semantik o'zgarishni o'rganish qoidalari uchun bir yetarli freymni yaratishmiz, preprocessor, so'zlarni o'rganish, vektorning orasidagi maslahatlarni hisoblash va chegarani aniqlash. Va biz Gamma Quantil Threshold so'zlarni o'zgartirish va stabil so'zlarni o'zgartirish uchun o'zgarishni davom qilamiz. Based on our system, we conduct a comprehensive comparison among BERT, Skip-gram, Temporal Referencing and alignment-based methods.  Tasdiqlash natijalari esa vaqt Referensiya bilan Skip grammatika 66.5% darajalashtirish imkoniyatini bajaradi va 51.8% Spearman chegarasining birinchi bogʻlanishini bajaradi.</abstract_uz>
      <abstract_vi>Tờ giấy này mô tả hệ thống TemporalTeller tạo ra nhiệm vụ SemEvol 1: trinh sát tinh thần giao dịch. Chúng tôi phát triển một cơ sở thống nhất cho những đường ống phát hiện biến đổi theo ngữ nghĩa chung, bao gồm tiền xử lý, nhập từ học, tính khoảng cách vector và quyết định ngưỡng. Chúng tôi cũng đề nghị Gamma Quantico Ngưỡng mộ để phân biệt giữa từ đã thay đổi và ổn định. Dựa trên hệ thống của chúng tôi, chúng tôi tiến hành một cuộc so sánh toàn diện giữa loại BERT, Skip-gram, Thời Báo Quyền và các phương pháp định vị. Kết quả đánh giá cho thấy khả năng dịch chuyển thời gian đạt đến hiệu quả cao nhất của độ chính xác phân hạng cao cao cao cao cao cao cao cao và 51.8 Rất nhiều người viết được</abstract_vi>
      <abstract_bg>Тази статия описва нашата система за Задача 1: Откриване на лексикални семантични промени без надзор. Разработваме единна рамка за общите тръбопроводи за откриване на семантични промени, включващи предварителна обработка, изучаващи вграждания на думи, изчисляване на векторни разстояния и определяне на прага. Предлагаме също така количествен праг Гама за разграничаване между променени и стабилни думи. Въз основа на нашата система провеждаме цялостно сравнение между методите, базирани на подравняване, времеви референции и методи. Резултатите от оценката показват, че Скип-грам с времева референция постига най-доброто представяне с 66,5% точност на класификацията и 51,8% корелация на Спиърман.</abstract_bg>
      <abstract_da>Denne artikel beskriver vores TemporalTeller system til SemEval Opgave 1: Uservised Lexical Semantic Change Detection. Vi udvikler en samlet ramme for de fælles semantiske ændringsdetekteringsrørledninger, herunder forbehandling, indlæring af ord, beregning af vektorafstande og bestemmelse af tærskel. Vi foreslår også Gamma Quantile Threshold for at skelne mellem ændrede og stabile ord. Baseret på vores system foretager vi en omfattende sammenligning mellem BERT, Skip-gram, Temporal Referencing og justeringsbaserede metoder. Evalueringsresultater viser, at Skip-gram med Temporal Referencing opnår den bedste præstation med 66,5% klassificeringsnøjagtighed og 51,8% Spearmans Ranking Correlation.</abstract_da>
      <abstract_de>Dieser Beitrag beschreibt unser TemporalTeller System für SemEval Task 1: Unsupervised Lexical Semantic Change Detection. Wir entwickeln ein einheitliches Framework für die gemeinsamen Pipelines zur semantischen Änderungserkennung einschließlich Vorverarbeitung, Lernwort-Einbettungen, Berechnung von Vektorabständen und Bestimmung von Schwellenwerten. Wir schlagen auch Gamma Quantile Threshold vor, um zwischen veränderten und stabilen Wörtern zu unterscheiden. Basierend auf unserem System führen wir einen umfassenden Vergleich zwischen BERT, Skip-gram, Temporal Referencing und alignment-basierten Methoden durch. Auswertungsergebnisse zeigen, dass Skip-gram mit Temporal Referencing die beste Leistung von 66,5% Klassifikationsgenauigkeit und 51,8% Spearmans Rankingkorrelation erreicht.</abstract_de>
      <abstract_nl>Dit artikel beschrijft ons TemporalTeller-systeem voor SemEval Task 1: Unsupervised Lexical Semantic Change Detection. We ontwikkelen een uniform framework voor de gemeenschappelijke semantische veranderdetectiepipelines, inclusief preprocessing, het leren van woord embeddings, het berekenen van vectorafstanden en het bepalen van drempelwaarden. We stellen ook Gamma Quantile Threshold voor om onderscheid te maken tussen veranderde en stabiele woorden. Op basis van ons systeem voeren we een uitgebreide vergelijking uit tussen BERT, Skip-gram, Temporal Referencing en op uitlijning gebaseerde methoden. Evaluatieresultaten tonen aan dat Skip-gram met Tijdelijke Referentie de beste prestaties behaalt van 66,5% classificatienauwkeurigheid en 51,8% Spearman's Ranking Correlatie.</abstract_nl>
      <abstract_hr>Ovaj papir opisuje naš sustav privremenog telefoniranja za poluvremeni zadatak 1: otkrivanje nepotrebne leksičke semantičke promjene. Razvijamo ujedinjeni okvir za zajedničke semantičke promjene otkrivanja cijevi, uključujući preprocessiranje, učenje priključenja riječi, izračunavanje udaljenosti vektora i određivanje praga. Također predlažemo Gamma Quantile Threshold da se razlikuje između promjene i stabilne riječi. Na temelju našeg sustava, vodimo sveobuhvatljivu usporedbu među BERT-om, Skip-gramom, Temporalnim referencijama i metodama na temelju usklađenja. Rezultati procjene pokazuju da Skip-gram s Temporalnim referencijama postiže najbolji učinkovit 66,5% točnosti klasifikacije i 51,8% korelacije Spearmana Ranking.</abstract_hr>
      <abstract_sw>Gazeti hili linaelezea mfumo wetu wa Teller kwa ajili ya kazi ya SemEval 1: Utafiti wa Mabadiliko ya Kitendo cha Kilexico usio na mamlaka. Tunaendeleza mfumo wa pamoja kwa ajili ya mabadiliko yanayofanana na mabadiliko ya kimapenzi ikiwa ni pamoja na upasuaji, kujifunza maneno yanayoingia, kuhesabu umbali wa barabara na kuamua ufumbuzi. Pia tunapendekeza Shambulio la Kutetea Gamma kutafautisha kati ya maneno mabadiliko na yenye imara. Kwa msingi wa mfumo wetu, tunafanya ulinganisho mkubwa kati ya BERT, gram za Skip, Uvumbuzi wa muda na njia za upasuaji. Matokeo ya Uchunguzi yanaonyesha kwamba Uchunguzi wa Kupitia Ukimwengu wa muda mrefu unafanikiwa ufanisi bora wa uangalizi wa asilimia 66.5 na asilimia 51.8 ya Kuhusiana na Ujapani.</abstract_sw>
      <abstract_fa>این کاغذ سیستم تللر TemporalTeller ما را برای کار نیمه‌سالگی ۱ توصیف می‌کند: شناسایی تغییر متفاوت سکسیکی غیرقابل حفاظت است. ما یک چهارچوب متحد برای شناسایی لوله های تغییر semantic مشترک توسعه می کنیم، شامل پیش پردازش، یاد گرفتن کلمات متحد، محاسبه فاصله های ویکتور و تعیین سطح. ما همچنین پیشنهاد می‌کنیم که تفاوت بین کلمات تغییر و استوار گاما باشد. بر اساس سیستم ما، ما یک مقایسه کامل بین BERT، Skip-gram، Referencing Temporary and alignment-based methods conduct a comprehensive comparison among BERT, Skip-gram, Temporal Referencing and alignment-based methods. نتیجه ارزیابی نشان می دهد که Skip-gram با تفصیل زمانی بهترین عملکرد دقیق مختصات 66.5 درصد و 51.8 درصد اصلاح Ranking Spearman را می رساند.</abstract_fa>
      <abstract_ko>본고는 우리의TemporalTeller시스템이SemEval 작업 1에 사용되는 감독 어휘의 의미 변화 검측을 기술하였다.우리는 흔히 볼 수 있는 의미 변화 검측 파이프를 위해 예처리, 학습 단어 삽입, 벡터 거리 계산과 한도값 확정을 포함한 통일된 구조를 개발했다.우리는 또한 가마의 분위수 한도값을 제시하여 변화어와 안정어를 구분했다.우리 시스템을 바탕으로 BERT, Skip-gram, 시제 인용과 정렬 방법을 전면적으로 비교했다.평가 결과 시간 참조를 바탕으로 한 스킵-gram 분류 정확도가 66.5%로 가장 높았고, 스피어맨 서열 관련성이 51.8%로 가장 높았다.</abstract_ko>
      <abstract_sq>Ky artikull përshkruan sistemin tonë TemporalTeller për SemEval Task 1: Detektim i Ndryshimit Semantik Lexik i Pambikqyrur. Ne zhvillojmë një kuadër të unifikuar për tubacionet e përbashkëta për zbulimin e ndryshimeve semantike duke përfshirë paraprocesimin, mësimin e përfshirjes së fjalëve, llogaritjen e distancave të vektorit dhe përcaktimin e pragut. Ne propozojmë gjithashtu pragun kwantitativ Gamma për të dalluar midis fjalëve të ndryshuara dhe të qëndrueshme. Bazuar në sistemin tonë, ne kryejmë një krahasim të plotë midis BERT, Skip-gram, Referencës Temporale dhe metodave të bazuara në rregullim. Rezultatet e vlerësimit tregojnë se Skip-gram me Referencën Temporale arrin performancën më të mirë të saktësisë s ë klasifikimit 66.5% dhe korrelacionit të rangut 51.8% të Spearmanit.</abstract_sq>
      <abstract_am>ይህ ፕሮግራም የስምኤል ስራ ስራችንን የራምፕዩተርቴል ስርዓታችንን ይናገራል፡፡ የስሜናዊ ለውጥ አካባቢዎች፣ የቃላትን መግለጫ፣ የሜትሮር ርቀት ቁጥጥር እናቆጥረዋለን፡፡ ጋማ ቃንቲል ቴርሻል በተለወጠ እና በጽኑ ቃላት መካከል ለመለየት እናዘጋጀዋለን፡፡ በሥርዓታችን በመሠረት፣ BERT፣ Skip-gram፣ የጥምመት ህትመት እና በተመሳሳይ ሥርዓት እናደርጋለን፡፡ የውጤት ውጤቶች ሲኪ-ግራም ከቴምፕዩተርት ጉዳይ ጋር የ66.5 በመቶ ክፍተት ትክክለኛውን ማድረግ እንዲያገኝ እና የስፓራን ቀዳሚ ግንኙነት 51.8 በመቶ ነው፡፡</abstract_am>
      <abstract_id>Kertas ini menjelaskan sistem TemporalTeller kita untuk SemEval Task 1: Deteksi Perubahan Semantik Lexik Tidak Disupervisi. Kami mengembangkan cadangan yang bersatu untuk saluran pipa deteksi perubahan semantis umum termasuk preproses, mempelajari embedding kata, menghitung jarak vektor dan menentukan ambang. Kami juga mengusulkan Gamma Quantile Threshold untuk membedakan antara kata-kata yang berubah dan stabil. Based on our system, we conduct a comprehensive comparison among BERT, Skip-gram, Temporal Referencing and alignment-based methods.  Evaluation results show that Skip-gram with Temporal Referencing achieves the best performance of 66.5% classification accuracy and 51.8% Spearman's Ranking Correlation.</abstract_id>
      <abstract_af>Hierdie papier beskryf ons tydelike Teller stelsel vir semiEval taak 1: Ononderwerp Leksiese Semantiese Verandering Opdekking. Ons ontwikkel 'n eenvoudige raamwerk vir die gemeenskaplike semantiese veranderinge beskrywing pyplyn insluitend voorafverwerking, leer woord inbêding, bereken vektorafstande en bepaal drukking. Ons voorstel ook Gamma Quantile Threshold om tussen verander en stabile woorde te verkies. Basies op ons stelsel, doen ons 'n kompensiewe vergelyking tussen BERT, Skip-gram, Temporale Verwysing en Alimentasie-gebaseerde metodes. Evalueringsresultate wys dat Skip-gram met Temporale Verwysing die beste prestasie van 66.5% klassifikasie presisie en 51.8% Spearman se Ranking Korrektuuring bereik.</abstract_af>
      <abstract_tr>Bu kagyz SemEval Taýgyzy 1-nji üçin Wagtlaýyn Wagtlaýyn Sistemimizi tassyýar: Wagtlaýynmadyk Biz ortak semantik değişiklikler tanımlama pipetlerini ön işleme, kelime içerimlerini öğrenmek, vektör uzaklarını hesaplamak ve eşiği kesirlemek için birleşmiş bir çerçevemi geliştirdik. Biz de Gamma Küçük Threshold'u düzenli ve sabit sözler arasynda farklı etmek için teklif ediyoruz. Sistemimize görä, BERT'yň, Grammatlary, Temporary Referenslar we çyzygymyz ýaly derejesi we çyzygymyz arasynda döredilişip barýarys. Taýýarlama netijesi Temporal Referensiýasy bilen Skip-gram 66.5% klasifikasyň dogrylygyny we 51.8% Spearman Deňleşik Doraglygyny ýetip bilen meňzeýär.</abstract_tr>
      <abstract_az>Bu kağıt, 1. Ölçüsü üçün Temporal Teller Sistemimizi təsdiqləyir: Ölçüsüz Siqsi Semantik Değişiklik Görünüş. Biz ortaq semantik dəyişiklik keşfetmə boru çubuqlarının birləşdirilməsi üçün birləşdiririk, əvvəlcə işləmə, sözlər içərisində öyrənmək, vektör uzaqlarını hesablamaq və sütunu çəkmək üçün. Biz də Gamma Quantile Threshold təklif edirik ki, dəyişdirilmiş və sabit sözlərin arasını ayırmaq üçün. Sistemimizə baxmayaraq, BERT, Skip-gram, Temporal Referencing və alignment-based metodları arasında bütün kompleks bir hörmət çəkirik. Qıymet sonuçları Temporal Referencing ilə Skip-gram'un 66.5% klasifikasiya doğruluğunun ən yaxşı performansını və 51.8% Spearman Ranking Correlation'nin ən yaxşı performansını göstərir.</abstract_az>
      <abstract_ca>Aquest article descriu el nostre sistema TemporalTeller per a SemEval Task 1: Detecció de canvis Semàtics Lèxics sense supervisió. Desenvolvem un marc unificat per a les conductes comunes de detecció del canvi semàntic, incloent la preprocessió, l'aprenentatge d'incorporacions de paraules, el calcul de distàncies de vectors i la determinació del umbre. We also propose Gamma Quantile Threshold to distinguish between changed and stable words.  Sobre la base del nostre sistema, fem una comparació completa entre els mètodes BERT, Skip-gram, Referència Temporal i alliniament. Els resultats de l'evaluació demostren que Skip-gram amb Referencing Temporal aconsegueix el millor rendiment de la precisió de classificació del 66,5% i la correlació de rangs del 51,8%.</abstract_ca>
      <abstract_cs>Tento článek popisuje náš systém TemporalTeller pro SemEval Task 1: Nehlídaná Lexical Sémantic Change Detection. Vyvíjíme jednotný rámec pro běžné sémantické detekční potrubí včetně předzpracování, učení slov vložení, výpočtu vektorových vzdáleností a stanovení prahové hodnoty. Navrhujeme také Gamma Quantile Threshold pro rozlišení mezi změněnými a stabilními slovy. Na základě našeho systému provádíme komplexní srovnání metod BERT, Skip-gram, Temporal Referencing a založených na zarovnání. Výsledky hodnocení ukazují, že Skip-gram s časovým referencím dosahuje nejlepšího výkonu 66,5% klasifikační přesnosti a 51,8% Spearmanovy hodnocení korelace.</abstract_cs>
      <abstract_hy>Այս հոդվածը նկարագրում է մեր "ԹեմպորալԹելլերի" համակարգը, որը օգտագործվում է "Սեմեվալ" առաջին հանձնարարության համար' անվերահսկված լեքսիկական սեմատիկ փոփոխությունների հայտնաբերման համար: Մենք զարգանում ենք միավոր շրջանակ ընդհանուր սեմանտիկ փոփոխությունների հայտնաբերման խողովակաշարերի համար, ներառյալ նախամշակումը, բառերի ներդրումը սովորելը, վեկտորի հեռավորությունները հաշվարկելը և սահմանումը: We also propose Gamma Quantile Threshold to distinguish between changed and stable words.  Հաշվի առնելով մեր համակարգին, մենք կատարում ենք ընդհանուր համեմատություն BERT-ի, SkIP-գրամի, Ժամանակական համեմատության և հարմարեցման հիմնված մեթոդների միջև: Արժեքի արդյունքները ցույց են տալիս, որ Թեմպորմալ հաղորդակցման միջոցով "Անցած գրամը" հասնում է 66.5 տոկոսի դասակարգման ճշգրտության լավագույն արդյունքին և 51.8 տոկոսի "Սփրմենի դասակարգման կորելացիայի" լավագու</abstract_hy>
      <abstract_bn>এই পত্রিকা আমাদের টেম্পুরাল টেলার সিস্টেমের ব্যাখ্যা করছে সেমইভাল কাজের জন্য ১: লেক্সিক্সিক্সিয়াল সেম্যান্ আমরা সাধারণ সেম্পেন্টিক পরিবর্তনের পাইপেলেনের জন্য একটি একত্রিত ফ্রেম তৈরি করি, যার মধ্যে রয়েছে প্রাপ্যাসেসিং, শব্দ প্রবেশ, ভেক্টর দূরত্ব আমরা গামা কোয়ান্টিল ট্রাশোল্ডের প্রস্তাব করছি পরিবর্তন এবং স্থিরীয় শব্দের মধ্যে আলাদা করার জন্য। আমাদের সিস্টেমের ভিত্তিতে আমরা বিবের্ট, স্কিপ-গ্রাম, টামপার্টাল রিফেন্সিং এবং সামান্য ভিত্তিক পদ্ধতির মধ্যে একটি সম্পূ পরিস্কারের ফলাফল দেখা যাচ্ছে যে টেম্পুরাল রেফারেন্সিং এর স্কিপ-গ্রাম ৬৬.</abstract_bn>
      <abstract_bs>Ovaj papir opisuje naš sistem privremenog telefoniranja za poluvremeni zadatak 1: neodređeno detekcije leksičkih semantičkih promjena. Mi razvijamo ujedinjeni okvir za zajedničke semantičke promjene detektivne cijevi, uključujući preobradu, učenje priključenja riječi, izračunavanje udaljenosti vektora i određivanje praga. Također predlažemo Gamma Quantile Threshold da se razlikuje između promjene i stabilne riječi. Na temelju našeg sustava, vodimo sveobuhvatljivu usporedbu među BERT-om, Skip-gramom, Temporalnim referencijama i metodama na temelju poravnanja. Rezultati procjene pokazuju da Skip-gram s Temporalnim referencijama postigne najbolje učinkovito preciznosti klasifikacije 66,5% i 51,8% isprave Spearmana.</abstract_bs>
      <abstract_fi>Tässä artikkelissa kuvataan TemporalTeller-järjestelmäämme SemEval Task 1: Unsupervised Lexical Semantic Change Detection varten. Kehitämme yhtenäisen kehyksen yhteisille semanttisten muutosten havaitsemisputkille, mukaan lukien esikäsittely, sanaupotusten oppiminen, vektorietäisyyksien laskeminen ja kynnyksen määrittäminen. Ehdotamme myös Gamma Quantile Threshold -kynnystä muutettujen ja vakaiden sanojen erottamiseksi toisistaan. Järjestelmämme pohjalta teemme kattavan vertailun BERT-, Skip-gram-, Temporal Referencing- ja linjauspohjaisten menetelmien välillä. Arviointitulokset osoittavat, että Skip-gram with Temporal Referencing saavuttaa parhaan suorituskyvyn 66,5% luokitustarkkuudella ja 51,8% Spearmanin sijoituskorrelaatiolla.</abstract_fi>
      <abstract_et>Käesolevas dokumendis kirjeldatakse meie TemporalTelleri süsteemi SemEval ülesande 1: järelevalveta Lexical Semantic Change Detection jaoks. Töötame välja ühtse raamistiku ühistele semantiliste muutuste tuvastamise torustikele, sealhulgas eeltöötlus, sõnade manustamise õppimine, vektorikauguse arvutamine ja läve määramine. Samuti pakume Gamma kvantiilne läve, et eristada muudetud ja stabiilseid sõnu. Oma süsteemi põhjal teostame põhjaliku võrdluse BERT-, Skip-gram-, Temporal Referencing- ja joonduspõhiste meetodite vahel. Hindamise tulemused näitavad, et ajalise viitamisega Skip-gramm saavutab parima tulemuse 66,5% klassifikatsioonitäpsuse ja 51,8% Spearmani järjestuse korrelatsiooni.</abstract_et>
      <abstract_ha>@ info: status Tuna samar da firam wanda ke iya haɗa wa domin canza canza na semantiki, kamar shirin da za'a yi amfani da shi, da za'a sanar da maganar da za'a shiga, yana ƙidãya tsakanin hanya da ke ƙayyade sauri. Kayya, Muke goyyade Gamma Quantile Threshld dõmin ya rarrabe a tsakanin musanya da magana masu tabbatacce. Basan da na'urarmu, za'a samu'a da BERT, skiip-gram, Reference na Temam da shiryoyin masu daidaita. Ana nuna cewa, skiip-gram da Reference na Temam zai ci mafi kyaun performance na fasalin daraja na 66.5% da 51.8% Spearman's Ranning Correlated.</abstract_ha>
      <abstract_jv>Perintah sing dibertuju kelompok nggambar kelompok 1 Awakdhö Awak dhéwé éntukno Gamma Cantil threshold nggawe ngubah tanggal gamma karo pawaran stabil Ngawe sistem dadi, kita hukum sumulakno akeh komparasi karo BERT, Skip-gram, templal Referning lan alignment-supported method. Wulangan</abstract_jv>
      <abstract_sk>Ta prispevek opisuje naš sistem TemporalTeller za SemEval Task 1: Nenadzorovano leksično zaznavanje semantičnih sprememb. Razvijamo enoten okvir za skupne cevovode za zaznavanje semantičnih sprememb, ki vključujejo predobdelavo, učenje vgradnje besed, izračun vektorskih razdalj in določanje praga. Predlagamo tudi kvantitativni prag gama za razlikovanje med spremenjenimi in stabilnimi besedami. Na podlagi našega sistema izvajamo celovito primerjavo metod BERT, Skip-gram, Temporal Referencing in poravnave. Rezultati ocenjevanja kažejo, da Skip-gram s časovnim referenčenjem dosega najboljšo zmogljivost 66,5% natančnosti klasifikacije in 51,8% Spearmanove korelacije.</abstract_sk>
      <abstract_bo>ཤོག་བྱང་འདིས་ང་ཚོའི་དུས་མཚམས་འཇོག་པའི་མ་དངོས་པོ་༡་གི་འགྲེལ་བཤད་ཀྱི་ཡོད། ང་ཚོས་མཐུན ང་ཚོས་Gamma Quantile Threshold་ལ་བསྒྱུར་བཅོས་དང་ཐག་རིང་གི་ཐ་སྙད་དུ་གཏོང་བྱེད་ཀྱི་ཡོད། ང་ཚོའི་མ་ལག་གི་གཞི་རྟེན་ནས་ BERT དང་Skip-gram、Temporal Referencing and alignment-based methods་ལ་ཡོད་པའི་མཉམ་སྒྲིག་ཐད་ཀར་ཡོད་པའི་ལྟ་བུའི་ནང་དུ་གཞི་བཞག་ཡོད། evaluation results show that Skip-gram with Temporal Referencing achieves the best performance of 66.5% classification accuracy and 51.8% Spearman's Ranking Correlation.</abstract_bo>
      <abstract_he>This paper describes our TemporalTeller system for SemEval Task 1: Unsupervised Lexical Semantic Change Detection.  אנחנו מפתחים מסגרת מאוחדת לצינורות של זיהוי שינויים סמנטיים משותפים, כולל התהליך הקדמי, לימוד מילים קישורים, חישוב מרחקי ויקטורים וקבע גבול. אנחנו גם מציעים מגבלת גומה קוונטילית כדי להבדיל בין מילים משתנות לבין מילים יציבות. בהתבסס על המערכת שלנו, אנו מבצעים השוואה מורכבת בין BERT, Skip-gram, Referencing זמני ושיטות מבוססות על התאמה. תוצאות הערכה מראות שסקיפ-גרם עם התייחסות זמנית משיג את ההופעה הטובה ביותר של מדויקת סיווג של 66.5% ו-51.8% הקשר של ספירמן.</abstract_he>
      </paper>
    <paper id="35">
      <title>Ferryman at SemEval-2020 Task 3 : Bert with TFIDF-Weighting for Predicting the Effect of Context in Word Similarity<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 3: Bert with <fixed-case>TFIDF</fixed-case>-Weighting for Predicting the Effect of Context in Word Similarity</title>
      <author><first>Weilong</first><last>Chen</last></author>
      <author><first>Xin</first><last>Yuan</last></author>
      <author><first>Sai</first><last>Zhang</last></author>
      <author><first>Jiehui</first><last>Wu</last></author>
      <author><first>Yanru</first><last>Zhang</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <pages>281–285</pages>
      <abstract>Word similarity is widely used in machine learning applications like <a href="https://en.wikipedia.org/wiki/Web_search_engine">searching engine</a> and <a href="https://en.wikipedia.org/wiki/Recommender_system">recommendation</a>. Measuring the changing meaning of the same word between two different sentences is not only a way to handle complex features in word usage (such as sentence syntax and semantics), but also an important method for different word polysemy modeling. In this paper, we present the <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> proposed by team Ferryman. Our system is based on the Bidirectional Encoder Representations from Transformers (BERT) model combined with term frequency-inverse document frequency (TF-IDF), applying the method on the provided datasets called CoSimLex, which covers four different languages including <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Croatian_language">Croatian</a>, <a href="https://en.wikipedia.org/wiki/Slovene_language">Slovene</a>, and <a href="https://en.wikipedia.org/wiki/Finnish_language">Finnish</a>. Our team Ferryman wins the the first position for English task and the second position for <a href="https://en.wikipedia.org/wiki/Finnish_language">Finnish</a> in the subtask 1.</abstract>
      <url hash="a09a8129">2020.semeval-1.35</url>
      <bibkey>chen-etal-2020-ferryman</bibkey>
      <doi>10.18653/v1/2020.semeval-1.35</doi>
    </paper>
    <paper id="37">
      <title>JUSTMasters at SemEval-2020 Task 3 : Multilingual Deep Learning Model to Predict the Effect of Context in Word Similarity<fixed-case>JUSTM</fixed-case>asters at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 3: Multilingual Deep Learning Model to Predict the Effect of Context in Word Similarity</title>
      <author><first>Nour</first><last>Al-khdour</last></author>
      <author><first>Mutaz</first><last>Bni Younes</last></author>
      <author><first>Malak</first><last>Abdullah</last></author>
      <author><first>Mohammad</first><last>AL-Smadi</last></author>
      <pages>292–300</pages>
      <abstract>There is a growing research interest in studying word similarity. Without a doubt, two similar words in a context may considered different in another context. Therefore, this paper investigates the effect of the <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a> in word similarity. The SemEval-2020 workshop has provided a shared task (Task 3 : Predicting the (Graded) Effect of Context in Word Similarity). In this task, the organizers provided unlabeled datasets for four languages, <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Croatian_language">Croatian</a>, <a href="https://en.wikipedia.org/wiki/Finnish_language">Finnish</a> and <a href="https://en.wikipedia.org/wiki/Slovene_language">Slovenian</a>. Our team, JUSTMasters, has participated in this competition in the two subtasks : A and B. Our approach has used a weighted average ensembling method for different pretrained embeddings techniques for each of the four languages. Our proposed model outperformed the baseline models in both subtasks and acheived the best result for subtask 2 in <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Finnish_language">Finnish</a>, with score 0.725 and 0.68 respectively. We have been ranked the sixth for subtask 1, with scores for <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Croatian_language">Croatian</a>, <a href="https://en.wikipedia.org/wiki/Finnish_language">Finnish</a>, and <a href="https://en.wikipedia.org/wiki/Slovene_language">Slovenian</a> as follows : 0.738, 0.44, 0.546, 0.512.</abstract>
      <url hash="0b3f7007">2020.semeval-1.37</url>
      <bibkey>al-khdour-etal-2020-justmasters</bibkey>
      <doi>10.18653/v1/2020.semeval-1.37</doi>
    </paper>
    <paper id="38">
      <title>Will_Go at SemEval-2020 Task 3 : An Accurate Model for Predicting the (Graded) Effect of Context in Word Similarity Based on BERT<fixed-case>W</fixed-case>ill_<fixed-case>G</fixed-case>o at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 3: An Accurate Model for Predicting the (Graded) Effect of Context in Word Similarity Based on <fixed-case>BERT</fixed-case></title>
      <author><first>Wei</first><last>Bao</last></author>
      <author><first>Hongshu</first><last>Che</last></author>
      <author><first>Jiandong</first><last>Zhang</last></author>
      <pages>301–306</pages>
      <abstract>Natural Language Processing (NLP) has been widely used in the <a href="https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)">semantic analysis</a> in recent years. Our paper mainly discusses a <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> to analyze the effect that <a href="https://en.wikipedia.org/wiki/Context_(language_use)">context</a> has on human perception of similar words, which is the third task of SemEval 2020. We apply several methods in calculating the distance between two embedding vector generated by Bidirectional Encoder Representation from Transformer (BERT). Our team will go won the 1st place in Finnish language track of subtask1, the second place in English track of subtask1.</abstract>
      <url hash="2cf85893">2020.semeval-1.38</url>
      <bibkey>bao-etal-2020-will</bibkey>
      <doi>10.18653/v1/2020.semeval-1.38</doi>
    </paper>
    <paper id="41">
      <title>SemEval-2020 Task 6 : Definition Extraction from Free Text with the DEFT Corpus<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 6: Definition Extraction from Free Text with the <fixed-case>DEFT</fixed-case> Corpus</title>
      <author><first>Sasha</first><last>Spala</last></author>
      <author><first>Nicholas</first><last>Miller</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Carl</first><last>Dockhorn</last></author>
      <pages>336–345</pages>
      <abstract>Research on definition extraction has been conducted for well over a decade, largely with significant constraints on the type of definitions considered. In this work, we present DeftEval, a SemEval shared task in which participants must extract definitions from free text using a term-definition pair corpus that reflects the complex reality of definitions in natural language. Definitions and glosses in free text often appear without explicit indicators, across sentences boundaries, or in an otherwise complex linguistic manner. DeftEval involved 3 distinct subtasks : 1) Sentence classification, 2) <a href="https://en.wikipedia.org/wiki/Sequence_labeling">sequence labeling</a>, and 3) relation extraction.</abstract>
      <url hash="fb203f3c">2020.semeval-1.41</url>
      <bibkey>spala-etal-2020-semeval</bibkey>
      <doi>10.18653/v1/2020.semeval-1.41</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/deft-corpus">DEFT Corpus</pwcdataset>
    </paper>
    <paper id="42">
      <title>IIE-NLP-NUT at SemEval-2020 Task 4 : Guiding PLM with Prompt Template Reconstruction Strategy for ComVE<fixed-case>IIE</fixed-case>-<fixed-case>NLP</fixed-case>-<fixed-case>NUT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: Guiding <fixed-case>PLM</fixed-case> with Prompt Template Reconstruction Strategy for <fixed-case>C</fixed-case>om<fixed-case>VE</fixed-case></title>
      <author><first>Luxi</first><last>Xing</last></author>
      <author><first>Yuqiang</first><last>Xie</last></author>
      <author><first>Yue</first><last>Hu</last></author>
      <author><first>Wei</first><last>Peng</last></author>
      <pages>346–353</pages>
      <abstract>This paper introduces our systems for the first two subtasks of SemEval Task4 : Commonsense Validation and Explanation. To clarify the intention for judgment and inject contrastive information for selection, we propose the input reconstruction strategy with prompt templates. Specifically, we formalize the <a href="https://en.wikipedia.org/wiki/Question_answering">subtasks</a> into the multiple-choice question answering format and construct the input with the prompt templates, then, the final prediction of question answering is considered as the result of subtasks. Experimental results show that our approaches achieve significant performance compared with the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline systems</a>. Our approaches secure the third rank on both <a href="https://en.wikipedia.org/wiki/Standard_score">official test sets</a> of the first two subtasks with an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 96.4 and an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 94.3 respectively.</abstract>
      <url hash="e64ce198">2020.semeval-1.42</url>
      <bibkey>xing-etal-2020-iie</bibkey>
      <doi>10.18653/v1/2020.semeval-1.42</doi>
    </paper>
    <paper id="46">
      <title>BUT-FIT at SemEval-2020 Task 4 : Multilingual Commonsense<fixed-case>BUT</fixed-case>-<fixed-case>FIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: Multilingual Commonsense</title>
      <author><first>Josef</first><last>Jon</last></author>
      <author><first>Martin</first><last>Fajcik</last></author>
      <author><first>Martin</first><last>Docekal</last></author>
      <author><first>Pavel</first><last>Smrz</last></author>
      <pages>374–390</pages>
      <abstract>We participated in all three subtasks. In subtasks A and B, our submissions are based on pretrained language representation models (namely ALBERT) and <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a>. We experimented with solving the task for another language, <a href="https://en.wikipedia.org/wiki/Czech_language">Czech</a>, by means of multilingual models and machine translated dataset, or translated model inputs. We show that with a strong machine translation system, our <a href="https://en.wikipedia.org/wiki/System">system</a> can be used in another language with a small <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy loss</a>. In subtask C, our submission, which is based on pretrained sequence-to-sequence model (BART), ranked 1st in <a href="https://en.wikipedia.org/wiki/BLEU">BLEU score ranking</a>, however, we show that the correlation between <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> and human evaluation, in which our submission ended up 4th, is low. We analyse the metrics used in the evaluation and we propose an additional score based on model from subtask B, which correlates well with our manual ranking, as well as reranking method based on the same principle. We performed an error and dataset analysis for all subtasks and we present our findings.</abstract>
      <url hash="ab673fcf">2020.semeval-1.46</url>
      <bibkey>jon-etal-2020-fit</bibkey>
      <doi>10.18653/v1/2020.semeval-1.46</doi>
      <pwccode url="https://github.com/cepin19/semeval2020_task4" additional="false">cepin19/semeval2020_task4</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
    </paper>
    <paper id="49">
      <title>Masked Reasoner at SemEval-2020 Task 4 : Fine-Tuning RoBERTa for Commonsense Reasoning<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: Fine-Tuning <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a for Commonsense Reasoning</title>
      <author><first>Daming</first><last>Lu</last></author>
      <pages>411–414</pages>
      <abstract>This paper describes the masked reasoner system that participated in SemEval-2020 Task 4 : Commonsense Validation and Explanation. The <a href="https://en.wikipedia.org/wiki/System">system</a> participated in the subtask B.We proposes a novel <a href="https://en.wikipedia.org/wiki/Methodology">method</a> to fine-tune RoBERTa by masking the most important word in the statement. We believe that the confidence of the system in recovering that word is positively correlated to the score the masked language model gives to the current statement-explanation pair. We evaluate the importance of each word using InferSent and do the masked fine-tuning on RoBERTa. Then we use the fine-tuned model to predict the most plausible explanation. Our <a href="https://en.wikipedia.org/wiki/System">system</a> is fast in training and achieved 73.5 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>.</abstract>
      <url hash="eb1a8361">2020.semeval-1.49</url>
      <bibkey>lu-2020-masked</bibkey>
      <doi>10.18653/v1/2020.semeval-1.49</doi>
    </paper>
    <paper id="52">
      <title>UoR at SemEval-2020 Task 4 : Pre-trained Sentence Transformer Models for Commonsense Validation and Explanation<fixed-case>U</fixed-case>o<fixed-case>R</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: Pre-trained Sentence Transformer Models for Commonsense Validation and Explanation</title>
      <author><first>Thanet</first><last>Markchom</last></author>
      <author><first>Bhuvana</first><last>Dhruva</last></author>
      <author><first>Chandresh</first><last>Pravin</last></author>
      <author><first>Huizhi</first><last>Liang</last></author>
      <pages>430–436</pages>
      <abstract>SemEval Task 4 Commonsense Validation and Explanation Challenge is to validate whether a system can differentiate natural language statements that make sense from those that do not make sense. Two <a href="https://en.wikipedia.org/wiki/Task_(project_management)">subtasks</a>, A and B, are focused in this work, i.e., detecting against-common-sense statements and selecting explanations of why they are false from the given options. Intuitively, commonsense validation requires additional knowledge beyond the given statements. Therefore, we propose a system utilising pre-trained sentence transformer models based on BERT, RoBERTa and DistillBERT architectures to embed the statements before classification. According to the results, these embeddings can improve the performance of the typical MLP and LSTM classifiers as downstream models of both subtasks compared to regular tokenised statements. These embedded statements are shown to comprise additional information from external resources which help validate common sense in <a href="https://en.wikipedia.org/wiki/Natural_language">natural language</a>.</abstract>
      <url hash="4da66650">2020.semeval-1.52</url>
      <bibkey>markchom-etal-2020-uor</bibkey>
      <doi>10.18653/v1/2020.semeval-1.52</doi>
    </paper>
    <paper id="53">
      <title>BUT-FIT at SemEval-2020 Task 5 : Automatic Detection of Counterfactual Statements with Deep Pre-trained Language Representation Models<fixed-case>BUT</fixed-case>-<fixed-case>FIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 5: Automatic Detection of Counterfactual Statements with Deep Pre-trained Language Representation Models</title>
      <author><first>Martin</first><last>Fajcik</last></author>
      <author><first>Josef</first><last>Jon</last></author>
      <author><first>Martin</first><last>Docekal</last></author>
      <author><first>Pavel</first><last>Smrz</last></author>
      <pages>437–444</pages>
      <abstract>This paper describes BUT-FIT’s submission at SemEval-2020 Task 5 : Modelling Causal Reasoning in Language : Detecting Counterfactuals. The challenge focused on detecting whether a given statement contains a counterfactual (Subtask 1) and extracting both antecedent and consequent parts of the counterfactual from the text (Subtask 2). We experimented with various state-of-the-art language representation models (LRMs). We found RoBERTa LRM to perform the best in both subtasks. We achieved the first place in both exact match and F1 for Subtask 2 and ranked second for Subtask 1.</abstract>
      <url hash="cf93293d">2020.semeval-1.53</url>
      <bibkey>fajcik-etal-2020-fit</bibkey>
      <doi>10.18653/v1/2020.semeval-1.53</doi>
      <pwccode url="https://github.com/MFajcik/SemEval_2020_Task-5" additional="false">MFajcik/SemEval_2020_Task-5</pwccode>
    </paper>
    <paper id="58">
      <title>ACNLP at SemEval-2020 Task 6 : A Supervised Approach for Definition Extraction<fixed-case>ACNLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 6: A Supervised Approach for Definition Extraction</title>
      <author><first>Fabien</first><last>Caspani</last></author>
      <author><first>Pirashanth</first><last>Ratnamogan</last></author>
      <author><first>Mathis</first><last>Linger</last></author>
      <author><first>Mhamed</first><last>Hajaiej</last></author>
      <pages>479–486</pages>
      <abstract>We describe our contribution to two of the subtasks of SemEval 2020 Task 6, DeftEval : Extracting term-definition pairs in free text. The system for Subtask 1 : Sentence Classification is based on a transformer architecture where we use <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> to fine-tune a pretrained model on the downstream task, and the one for Subtask 3 : Relation Classification uses a Random Forest classifier with handcrafted dedicated features. Our <a href="https://en.wikipedia.org/wiki/System">systems</a> respectively achieve 0.830 and 0.994 <a href="https://en.wikipedia.org/wiki/Standard_score">F1-scores</a> on the <a href="https://en.wikipedia.org/wiki/Standard_score">official test set</a>, and we believe that the insights derived from our study are potentially relevant to help advance the research on definition extraction.</abstract>
      <url hash="371695b1">2020.semeval-1.58</url>
      <bibkey>caspani-etal-2020-acnlp</bibkey>
      <doi>10.18653/v1/2020.semeval-1.58</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/deft-corpus">DEFT Corpus</pwcdataset>
    </paper>
    <paper id="60">
      <title>CN-HIT-IT.NLP at SemEval-2020 Task 4 : Enhanced Language Representation with Multiple Knowledge Triples<fixed-case>CN</fixed-case>-<fixed-case>HIT</fixed-case>-<fixed-case>IT</fixed-case>.<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: Enhanced Language Representation with Multiple Knowledge Triples</title>
      <author><first>Yice</first><last>Zhang</last></author>
      <author><first>Jiaxuan</first><last>Lin</last></author>
      <author><first>Yang</first><last>Fan</last></author>
      <author><first>Peng</first><last>Jin</last></author>
      <author><first>Yuanchao</first><last>Liu</last></author>
      <author><first>Bingquan</first><last>Liu</last></author>
      <pages>494–500</pages>
      <abstract>This paper describes our <a href="https://en.wikipedia.org/wiki/System">system</a> that participated in the SemEval-2020 task 4 : Commonsense Validation and Explanation. For this task, it is obvious that external knowledge, such as <a href="https://en.wikipedia.org/wiki/Knowledge_graph">Knowledge graph</a>, can help the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> understand commonsense in natural language statements. But how to select the right triples for statements remains unsolved, so how to reduce the interference of irrelevant triples on <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance is a research focus. This paper adopt a modified K-BERT as the language encoder, to enhance language representation through triples from <a href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graphs</a>. Experiments show that our <a href="https://en.wikipedia.org/wiki/Methodology">method</a> is better than <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> without external knowledge, and is slightly better than the original K-BERT. We got an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy score</a> of 0.97 in subtaskA, ranking 1/45, and got an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy score</a> of 0.948, ranking 2/35.</abstract>
      <url hash="c18d8eff">2020.semeval-1.60</url>
      <bibkey>zhang-etal-2020-cn</bibkey>
      <doi>10.18653/v1/2020.semeval-1.60</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="62">
      <title>CS-NLP Team at SemEval-2020 Task 4 : Evaluation of State-of-the-art NLP Deep Learning Architectures on Commonsense Reasoning Task<fixed-case>CS</fixed-case>-<fixed-case>NLP</fixed-case> Team at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: Evaluation of State-of-the-art <fixed-case>NLP</fixed-case> Deep Learning Architectures on Commonsense Reasoning Task</title>
      <author><first>Sirwe</first><last>Saeedi</last></author>
      <author><first>Aliakbar</first><last>Panahi</last></author>
      <author><first>Seyran</first><last>Saeedi</last></author>
      <author><first>Alvis</first><last>C Fong</last></author>
      <pages>507–515</pages>
      <abstract>In this paper, we investigate a commonsense inference task that unifies <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a> and <a href="https://en.wikipedia.org/wiki/Commonsense_reasoning">commonsense reasoning</a>. We describe our attempt at SemEval-2020 Task 4 competition : Commonsense Validation and Explanation (ComVE) challenge. We discuss several state-of-the-art deep learning architectures for this challenge. Our system uses prepared labeled textual datasets that were manually curated for three different natural language inference subtasks. The goal of the first subtask is to test whether a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> can distinguish between natural language statements that make sense and those that do not make sense. We compare the performance of several <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> and fine-tuned classifiers. Then, we propose a method inspired by <a href="https://en.wikipedia.org/wiki/Question_answering">question / answering tasks</a> to treat a <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification problem</a> as a <a href="https://en.wikipedia.org/wiki/Multiple_choice">multiple choice question task</a> to boost the performance of our experimental results (96.06 %), which is significantly better than the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a>. For the second subtask, which is to select the reason why a statement does not make sense, we stand within the first six teams (93.7 %) among 27 participants with very competitive results. Our result for last subtask of generating reason against the nonsense statement shows many potentials for future researches as we applied the most powerful generative model of language (GPT-2) with 6.1732 BLEU score among first four teams.</abstract>
      <url hash="51abcbf9">2020.semeval-1.62</url>
      <bibkey>saeedi-etal-2020-cs</bibkey>
      <doi>10.18653/v1/2020.semeval-1.62</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="65">
      <title>JBNU at SemEval-2020 Task 4 : BERT and UniLM for Commonsense Validation and Explanation<fixed-case>JBNU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: <fixed-case>BERT</fixed-case> and <fixed-case>U</fixed-case>ni<fixed-case>LM</fixed-case> for Commonsense Validation and Explanation</title>
      <author><first>Seung-Hoon</first><last>Na</last></author>
      <author><first>Jong-Hyeon</first><last>Lee</last></author>
      <pages>527–534</pages>
      <abstract>This paper presents our contributions to the SemEval-2020 Task 4 Commonsense Validation and Explanation (ComVE) and includes the experimental results of the two Subtasks B and C of the SemEval-2020 Task 4. Our systems rely on pre-trained language models, i.e., BERT (including its variants) and UniLM, and rank 10th and 7th among 27 and 17 systems on Subtasks B and C, respectively. We analyze the commonsense ability of the existing pretrained language models by testing them on the SemEval-2020 Task 4 ComVE dataset, specifically for Subtasks B and C, the explanation subtasks with multi-choice and sentence generation, respectively.</abstract>
      <url hash="d61d668f">2020.semeval-1.65</url>
      <bibkey>na-lee-2020-jbnu</bibkey>
      <doi>10.18653/v1/2020.semeval-1.65</doi>
    </paper>
    <paper id="67">
      <title>KaLM at SemEval-2020 Task 4 : Knowledge-aware Language Models for Comprehension and Generation<fixed-case>K</fixed-case>a<fixed-case>LM</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: Knowledge-aware Language Models for Comprehension and Generation</title>
      <author><first>Jiajing</first><last>Wan</last></author>
      <author><first>Xinting</first><last>Huang</last></author>
      <pages>543–550</pages>
      <abstract>This paper presents our <a href="https://en.wikipedia.org/wiki/Strategy">strategies</a> in SemEval 2020 Task 4 : Commonsense Validation and <a href="https://en.wikipedia.org/wiki/Explanation">Explanation</a>. We propose a novel way to search for evidence and choose the different large-scale pre-trained models as the backbone for three subtasks. The results show that our evidence-searching approach improves <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance on commonsense explanation task. Our <a href="https://en.wikipedia.org/wiki/Team">team</a> ranks 2nd in subtask C according to human evaluation score.</abstract>
      <url hash="9c03f3cc">2020.semeval-1.67</url>
      <bibkey>wan-huang-2020-kalm</bibkey>
      <doi>10.18653/v1/2020.semeval-1.67</doi>
      <pwccode url="https://github.com/huangxt39/KaLM" additional="false">huangxt39/KaLM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
    </paper>
    <paper id="70">
      <title>LMVE at SemEval-2020 Task 4 : Commonsense Validation and Explanation Using Pretraining Language Model<fixed-case>LMVE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: Commonsense Validation and Explanation Using Pretraining Language Model</title>
      <author><first>Shilei</first><last>Liu</last></author>
      <author><first>Yu</first><last>Guo</last></author>
      <author><first>BoChao</first><last>Li</last></author>
      <author><first>Feiliang</first><last>Ren</last></author>
      <pages>562–568</pages>
      <abstract>This paper introduces our <a href="https://en.wikipedia.org/wiki/System">system</a> for commonsense validation and <a href="https://en.wikipedia.org/wiki/Explanation">explanation</a>. For Sen-Making task, we use a novel pretraining language model based architecture to pick out one of the two given statements that is againstcommon sense. For Explanation task, we use a hint sentence mechanism to improve the performance greatly. In addition, we propose a subtask level transfer learning to share information between subtasks.</abstract>
      <url hash="07b413d2">2020.semeval-1.70</url>
      <bibkey>liu-etal-2020-lmve</bibkey>
      <doi>10.18653/v1/2020.semeval-1.70</doi>
    </paper>
    <paper id="73">
      <title>SSN-NLP at SemEval-2020 Task 4 : Text Classification and Generation on Common Sense Context Using Neural Networks<fixed-case>SSN</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: Text Classification and Generation on Common Sense Context Using Neural Networks</title>
      <author><first>Rishivardhan</first><last>K.</last></author>
      <author><first>Kayalvizhi</first><last>S</last></author>
      <author><first>Thenmozhi</first><last>D.</last></author>
      <author><first>Raghav</first><last>R.</last></author>
      <author><first>Kshitij</first><last>Sharma</last></author>
      <pages>580–584</pages>
      <abstract>Common sense validation deals with testing whether a <a href="https://en.wikipedia.org/wiki/System">system</a> can differentiate natural language statements that make sense from those that do not make sense. This paper describes the our approach to solve this challenge. For common sense validation with <a href="https://en.wikipedia.org/wiki/Multiple_choice">multi choice</a>, we propose a stacking based approach to classify sentences that are more favourable in terms of common sense to the particular statement. We have used majority voting classifier methodology amongst three models such as Bidirectional Encoder Representations from Transformers (BERT), Micro Text Classification (Micro TC) and XLNet. For sentence generation, we used Neural Machine Translation (NMT) model to generate explanatory sentences.</abstract>
      <url hash="19903604">2020.semeval-1.73</url>
      <bibkey>k-etal-2020-ssn</bibkey>
      <doi>10.18653/v1/2020.semeval-1.73</doi>
    </paper>
    <paper id="77">
      <title>UAICS at SemEval-2020 Task 4 : Using a Bidirectional Transformer for Task a<fixed-case>UAICS</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: Using a Bidirectional Transformer for Task a</title>
      <author><first>Ciprian-Gabriel</first><last>Cusmuliuc</last></author>
      <author><first>Lucia-Georgiana</first><last>Coca</last></author>
      <author><first>Adrian</first><last>Iftene</last></author>
      <pages>609–613</pages>
      <abstract>Commonsense Validation and Explanation has been a difficult task for machines since the dawn of <a href="https://en.wikipedia.org/wiki/Computing">computing</a>. Although very trivial to humans it poses a high <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">complexity</a> for machines due to the necessity of <a href="https://en.wikipedia.org/wiki/Inference">inference</a> over a pre-existing knowledge base. In order to try and solve this problem the SemEval 2020 Task 4-Commonsense Validation and Explanation (ComVE) aims to evaluate systems capable of multiple stages of ComVE. The challenge includes 3 <a href="https://en.wikipedia.org/wiki/Task_(project_management)">tasks</a> (A, B and C), each with it’s own requirements. Our team participated only in task A which required selecting the statement that made the least sense. We choose to use a bidirectional transformer in order to solve the challenge, this paper presents the details of our method, runs and result.</abstract>
      <url hash="fc98e437">2020.semeval-1.77</url>
      <bibkey>cusmuliuc-etal-2020-uaics</bibkey>
      <doi>10.18653/v1/2020.semeval-1.77</doi>
    </paper>
    <paper id="79">
      <title>Warren at SemEval-2020 Task 4 : ALBERT and Multi-Task Learning for Commonsense Validation<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 4: <fixed-case>ALBERT</fixed-case> and Multi-Task Learning for Commonsense Validation</title>
      <author><first>Yuhang</first><last>Wu</last></author>
      <author><first>Hao</first><last>Wu</last></author>
      <pages>620–625</pages>
      <abstract>This paper describes our <a href="https://en.wikipedia.org/wiki/System">system</a> in subtask A of SemEval 2020 Shared Task 4. We propose a <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning model</a> based on MTL(Multi-Task Learning) to enhance the prediction ability of commonsense validation. The experimental results demonstrate that our <a href="https://en.wikipedia.org/wiki/System">system</a> outperforms the single-task text classification model. We combine MTL and ALBERT pretrain model to achieve an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 0.904 and our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is ranked 16th on the final leader board of the competition among the 45 teams.</abstract>
      <url hash="334d7cf7">2020.semeval-1.79</url>
      <bibkey>wu-wu-2020-warren</bibkey>
      <doi>10.18653/v1/2020.semeval-1.79</doi>
    </paper>
    <paper id="83">
      <title>ETHAN at SemEval-2020 Task 5 : Modelling Causal Reasoning in Language Using Neuro-symbolic Cloud Computing<fixed-case>ETHAN</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 5: Modelling Causal Reasoning in Language Using Neuro-symbolic Cloud Computing</title>
      <author><first>Len</first><last>Yabloko</last></author>
      <pages>645–652</pages>
      <abstract>I present ETHAN : Experimental Testing of Hybrid AI Node implemented entirely on free cloud computing infrastructure. The ultimate goal of this research is to create modular reusable hybrid neuro-symbolic architecture for <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence</a>. As a test case I model natural language comprehension of causal relations from open domain text corpus that combines semi-supervised language model (Huggingface Transformers) with constituency and dependency parsers (Allen Institute for Artificial Intelligence.)</abstract>
      <url hash="e284d87b">2020.semeval-1.83</url>
      <bibkey>yabloko-2020-ethan</bibkey>
      <doi>10.18653/v1/2020.semeval-1.83</doi>
    </paper>
    <paper id="84">
      <title>Ferryman as SemEval-2020 Task 5 : Optimized BERT for Detecting Counterfactuals<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 5: Optimized <fixed-case>BERT</fixed-case> for Detecting Counterfactuals</title>
      <author><first>Weilong</first><last>Chen</last></author>
      <author><first>Yan</first><last>Zhuang</last></author>
      <author><first>Peng</first><last>Wang</last></author>
      <author><first>Feng</first><last>Hong</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>Yanru</first><last>Zhang</last></author>
      <pages>653–657</pages>
      <abstract>The main purpose of this article is to state the effect of using different <a href="https://en.wikipedia.org/wiki/Scientific_method">methods</a> and <a href="https://en.wikipedia.org/wiki/Scientific_modelling">models</a> for <a href="https://en.wikipedia.org/wiki/Counterfactual_conditional">counterfactual determination</a> and detection of causal knowledge. Nowadays, counterfactual reasoning has been widely used in various fields. In the realm of natural language process(NLP), counterfactual reasoning has huge potential to improve the correctness of a sentence. In the shared Task 5 of detecting counterfactual in SemEval 2020, we pre-process the officially given dataset according to case conversion, extract stem and abbreviation replacement. We use last-5 bidirectional encoder representation from bidirectional encoder representation from transformer (BERT)and term frequencyinverse document frequency (TF-IDF) vectorizer for counterfactual detection. Meanwhile, multi-sample dropout and <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross validation</a> are used to improve versatility and prevent problems such as poor generosity caused by <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>. Finally, our team Ferryman ranked the 8th place in the sub-task 1 of this competition.</abstract>
      <url hash="989b7417">2020.semeval-1.84</url>
      <bibkey>chen-etal-2020-ferryman-semeval</bibkey>
      <doi>10.18653/v1/2020.semeval-1.84</doi>
    </paper>
    <paper id="86">
      <title>Lee at SemEval-2020 Task 5 : ALBERT Model Based on the Maximum Ensemble Strategy and Different Data Sampling Methods for Detecting Counterfactual Statements<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 5: <fixed-case>ALBERT</fixed-case> Model Based on the Maximum Ensemble Strategy and Different Data Sampling Methods for Detecting Counterfactual Statements</title>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Yuhang</first><last>Wu</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Haiyan</first><last>Ding</last></author>
      <pages>664–669</pages>
      <abstract>This article describes the system submitted to SemEval 2020 Task 5 : Modelling Causal Reasoning in Language : Detecting Counterfactuals. In this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>, we only participate in the subtask A which is detecting counterfactual statements. In order to solve this sub-task, first of all, because of the problem of data balance, we use the undersampling and oversampling methods to process the data set. Second, we used the ALBERT model and the maximum ensemble method based on the ALBERT model. Our <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> achieved a F1 score of 0.85 in subtask A.</abstract>
      <url hash="b887af8a">2020.semeval-1.86</url>
      <bibkey>li-etal-2020-lee</bibkey>
      <doi>10.18653/v1/2020.semeval-1.86</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="87">
      <title>NLU-Co at SemEval-2020 Task 5 : NLU / SVM Based Model Apply Tocharacterise and Extract Counterfactual Items on Raw Data<fixed-case>NLU</fixed-case>-Co at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 5: <fixed-case>NLU</fixed-case>/<fixed-case>SVM</fixed-case> Based Model Apply Tocharacterise and Extract Counterfactual Items on Raw Data</title>
      <author><first>Elvis</first><last>Mboning Tchiaze</last></author>
      <author><first>Damien</first><last>Nouvel</last></author>
      <pages>670–676</pages>
      <abstract>In this article, we try to solve the problem of classification of counterfactual statements and extraction of antecedents / consequences in raw data, by mobilizing on one hand Support vector machine (SVMs) and on the other hand Natural Language Understanding (NLU) infrastructures available on the market for conversational agents. Our experiments allowed us to test different <a href="https://en.wikipedia.org/wiki/Pipeline_(computing)">pipelines</a> of two known <a href="https://en.wikipedia.org/wiki/Computing_platform">platforms</a> (Snips NLU and Rasa NLU). The results obtained show that a Rasa NLU pipeline, built with a well-preprocessed dataset and tuned algorithms, allows to model accurately the structure of a <a href="https://en.wikipedia.org/wiki/Counterfactual_conditional">counterfactual event</a>, in order to facilitate the identification and the extraction of its components.</abstract>
      <url hash="47fdd5e0">2020.semeval-1.87</url>
      <bibkey>mboning-tchiaze-nouvel-2020-nlu</bibkey>
      <doi>10.18653/v1/2020.semeval-1.87</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="89">
      <title>YNU-oxz at SemEval-2020 Task 5 : Detecting Counterfactuals Based on Ordered Neurons LSTM and Hierarchical Attention Network<fixed-case>YNU</fixed-case>-oxz at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 5: Detecting Counterfactuals Based on Ordered Neurons <fixed-case>LSTM</fixed-case> and Hierarchical Attention Network</title>
      <author><first>Xiaozhi</first><last>Ou</last></author>
      <author><first>Shengyan</first><last>Liu</last></author>
      <author><first>Hongling</first><last>Li</last></author>
      <pages>683–689</pages>
      <abstract>This paper describes the system and results of our team’s participation in SemEval-2020 Task5 : Modelling Causal Reasoning in Language : Detecting Counterfactuals, which aims to simulate counterfactual semantics and reasoning in natural language. This <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> contains two subtasks : Subtask1Detecting counterfactual statements and Subtask2Detecting antecedent and consequence. We only participated in Subtask1, aiming to determine whether a given sentence is counterfactual. In order to solve this task, we proposed a system based on Ordered Neurons LSTM (ON-LSTM) with Hierarchical Attention Network (HAN) and used Pooling operation for dimensionality reduction. Finally, we used the K-fold approach as the ensemble method. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieved an <a href="https://en.wikipedia.org/wiki/F-number">F1 score</a> of 0.7040 in Subtask1 (Ranked 16/27).</abstract>
      <url hash="bbe53f67">2020.semeval-1.89</url>
      <bibkey>ou-etal-2020-ynu</bibkey>
      <doi>10.18653/v1/2020.semeval-1.89</doi>
    </paper>
    <paper id="90">
      <title>BERTatDE at SemEval-2020 Task 6 : Extracting Term-definition Pairs in Free Text Using Pre-trained Model<fixed-case>BERT</fixed-case>at<fixed-case>DE</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 6: Extracting Term-definition Pairs in Free Text Using Pre-trained Model</title>
      <author><first>Huihui</first><last>Zhang</last></author>
      <author><first>Feiliang</first><last>Ren</last></author>
      <pages>690–696</pages>
      <abstract>Definition extraction is an important task in Nature Language Processing, and it is used to identify the terms and definitions related to terms. The <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> contains sentence classification task (i.e., classify whether it contains definition) and sequence labeling task (i.e., find the boundary of terms and definitions). The paper describes our system BERTatDE1 in sentence classification task (subtask 1) and sequence labeling task (subtask 2) in the definition extraction (SemEval-2020 Task 6). We use BERT to solve the multi-domain problems including the uncertainty of term boundary that is, different areas have different ways to definite the domain related terms. We use <a href="https://en.wikipedia.org/wiki/BERT">BERT</a>, BiLSTM and <a href="https://en.wikipedia.org/wiki/Attention">attention</a> in subtask 1 and our best result achieved 79.71 % in F1 and the eighteenth place in subtask 1. For the subtask 2, we use BERT, BiLSTM and CRF to <a href="https://en.wikipedia.org/wiki/Sequence_labeling">sequence labeling</a>, and achieve 40.73 % in Macro-averaged F1.</abstract>
      <url hash="ced65874">2020.semeval-1.90</url>
      <bibkey>zhang-ren-2020-bertatde</bibkey>
      <doi>10.18653/v1/2020.semeval-1.90</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/deft-corpus">DEFT Corpus</pwcdataset>
    </paper>
    <paper id="92">
      <title>Defx at SemEval-2020 Task 6 : Joint Extraction of Concepts and Relations for Definition Extraction<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 6: Joint Extraction of Concepts and Relations for Definition Extraction</title>
      <author><first>Marc</first><last>Hübner</last></author>
      <author><first>Christoph</first><last>Alt</last></author>
      <author><first>Robert</first><last>Schwarzenberg</last></author>
      <author><first>Leonhard</first><last>Hennig</last></author>
      <pages>704–709</pages>
      <abstract>Definition Extraction systems are a valuable knowledge source for both humans and algorithms. In this paper we describe our submissions to the DeftEval shared task (SemEval-2020 Task 6), which is evaluated on an English textbook corpus. We provide a detailed explanation of our <a href="https://en.wikipedia.org/wiki/System">system</a> for the joint extraction of definition concepts and the relations among them. Furthermore we provide an ablation study of our model variations and describe the results of an error analysis.</abstract>
      <url hash="14648690">2020.semeval-1.92</url>
      <bibkey>hubner-etal-2020-defx</bibkey>
      <doi>10.18653/v1/2020.semeval-1.92</doi>
      <pwccode url="https://github.com/DFKI-NLP/defx" additional="false">DFKI-NLP/defx</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/deft-corpus">DEFT Corpus</pwcdataset>
    </paper>
    <paper id="97">
      <title>UPB at SemEval-2020 Task 6 : Pretrained Language Models for Definition Extraction<fixed-case>UPB</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 6: Pretrained Language Models for Definition Extraction</title>
      <author><first>Andrei-Marius</first><last>Avram</last></author>
      <author><first>Dumitru-Clementin</first><last>Cercel</last></author>
      <author><first>Costin</first><last>Chiru</last></author>
      <pages>737–745</pages>
      <abstract>This work presents our contribution in the context of the 6th task of SemEval-2020 : Extracting Definitions from Free Text in Textbooks (DeftEval). This competition consists of three subtasks with different levels of granularity : (1) classification of sentences as definitional or non-definitional, (2) labeling of definitional sentences, and (3) relation classification. We use various pretrained language models (i.e., BERT, XLNet, RoBERTa, SciBERT, and ALBERT) to solve each of the three subtasks of the competition. Specifically, for each language model variant, we experiment by both freezing its weights and fine-tuning them. We also explore a multi-task architecture that was trained to jointly predict the outputs for the second and the third subtasks. Our best performing <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> evaluated on the DeftEval dataset obtains the 32nd place for the first subtask and the 37th place for the second subtask. The code is available for further research at :<url>https://github.com/avramandrei/DeftEval</url>
      </abstract>
      <url hash="43d211d7">2020.semeval-1.97</url>
      <bibkey>avram-etal-2020-upb</bibkey>
      <doi>10.18653/v1/2020.semeval-1.97</doi>
      <pwccode url="https://github.com/avramandrei/DeftEval" additional="true">avramandrei/DeftEval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/deft-corpus">DEFT Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="104">
      <title>Buhscitu at SemEval-2020 Task 7 : Assessing Humour in Edited News Headlines Using Hand-Crafted Features and Online Knowledge Bases<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 7: Assessing Humour in Edited News Headlines Using Hand-Crafted Features and Online Knowledge Bases</title>
      <author><first>Kristian Nørgaard</first><last>Jensen</last></author>
      <author><first>Nicolaj Filrup</first><last>Rasmussen</last></author>
      <author><first>Thai</first><last>Wang</last></author>
      <author><first>Marco</first><last>Placenti</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>824–832</pages>
      <abstract>This paper describes a system that aims at assessing humour intensity in edited news headlines as part of the 7th task of SemEval-2020 on Humor, <a href="https://en.wikipedia.org/wiki/Emphasis_(typography)">Emphasis</a> and <a href="https://en.wikipedia.org/wiki/Sentimentality">Sentiment</a>. Various factors need to be accounted for in order to assess the <a href="https://en.wikipedia.org/wiki/Funniness">funniness</a> of an edited headline. We propose an architecture that uses hand-crafted features, <a href="https://en.wikipedia.org/wiki/Knowledge_base">knowledge bases</a> and a <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> to understand <a href="https://en.wikipedia.org/wiki/Humour">humour</a>, and combines them in a <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression model</a>. Our <a href="https://en.wikipedia.org/wiki/System">system</a> outperforms two <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baselines</a>. In general, automatic humour assessment remains a difficult task.</abstract>
      <url hash="8e995fbd">2020.semeval-1.104</url>
      <bibkey>jensen-etal-2020-buhscitu</bibkey>
      <doi>10.18653/v1/2020.semeval-1.104</doi>
    </paper>
    <paper id="105">
      <title>Hasyarasa at SemEval-2020 Task 7 : Quantifying Humor as Departure from Expectedness<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 7: Quantifying Humor as Departure from Expectedness</title>
      <author><first>Ravi Theja</first><last>Desetty</last></author>
      <author><first>Ranit</first><last>Chatterjee</last></author>
      <author><first>Smita</first><last>Ghaisas</last></author>
      <pages>833–842</pages>
      <abstract>This paper describes our system submission Hasyarasa for the SemEval-2020 Task-7 : Assessing Humor in Edited News Headlines. This <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> has two subtasks. The goal of Subtask 1 is to predict the mean funniness of the edited headline given the original and the edited headline. In Subtask 2, given two edits on the original headline, the goal is to predict the funnier of the two. We observed that the departure from expected state/ actions of situations/ individuals is the cause of <a href="https://en.wikipedia.org/wiki/Humour">humor</a> in the edited headlines. We propose two novel features : Contextual Semantic Distance and Contextual Neighborhood Distance to estimate this departure and thus capture the contextual absurdity and hence the <a href="https://en.wikipedia.org/wiki/Humour">humor</a> in the edited headlines. We have used these <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> together with a Bi-LSTM Attention based model and have achieved 0.53310 RMSE for Subtask 1 and 60.19 % <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> for Subtask 2.</abstract>
      <url hash="956d6e35">2020.semeval-1.105</url>
      <bibkey>desetty-etal-2020-hasyarasa</bibkey>
      <doi>10.18653/v1/2020.semeval-1.105</doi>
    </paper>
    <paper id="110">
      <title>YNU-HPCC at SemEval-2020 Task 7 : Using an Ensemble BiGRU Model to Evaluate the Humor of Edited News Titles<fixed-case>YNU</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 7: Using an Ensemble <fixed-case>B</fixed-case>i<fixed-case>GRU</fixed-case> Model to Evaluate the Humor of Edited News Titles</title>
      <author><first>Joseph</first><last>Tomasulo</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>871–875</pages>
      <abstract>This paper describes an ensemble model designed for Semeval-2020 Task 7. The task is based on the Humicroedit dataset that is comprised of news titles and one-word substitutions designed to make them humorous. We use <a href="https://en.wikipedia.org/wiki/BERT">BERT</a>, <a href="https://en.wikipedia.org/wiki/FastText">FastText</a>, Elmo, and Word2Vec to encode these titles then pass them to a bidirectional gated recurrent unit (BiGRU) with attention. Finally, we used <a href="https://en.wikipedia.org/wiki/XGBoost">XGBoost</a> on the concatenation of the results of the different <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to make predictions.</abstract>
      <url hash="a6a02927">2020.semeval-1.110</url>
      <bibkey>tomasulo-etal-2020-ynu</bibkey>
      <doi>10.18653/v1/2020.semeval-1.110</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/humicroedit">Humicroedit</pwcdataset>
    </paper>
    <paper id="113">
      <title>NLP_UIOWA at SemEval-2020 Task 8 : You’re Not the Only One Cursed with Knowledge-Multi Branch Model Memotion Analysis<fixed-case>NLP</fixed-case>_<fixed-case>UIOWA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 8: You’re Not the Only One Cursed with Knowledge - Multi Branch Model Memotion Analysis</title>
      <author><first>Ingroj</first><last>Shrestha</last></author>
      <author><first>Jonathan</first><last>Rusert</last></author>
      <pages>891–900</pages>
      <abstract>We propose hybrid models (HybridE and HybridW) for meme analysis (SemEval 2020 Task 8), which involves sentiment classification (Subtask A), humor classification (Subtask B), and scale of semantic classes (Subtask C). The hybrid model consists of BLSTM and CNN for text and image processing respectively. HybridE provides equal weight to BLSTM and CNN performance, while HybridW provides weightage based on the performance of BLSTM and CNN on a validation set. The performances (macro F1) of our hybrid model on Subtask A are 0.329 (HybridE), 0.328 (HybridW), on Subtask B are 0.507 (HybridE), 0.512 (HybridW), and on Subtask C are 0.309 (HybridE), 0.311 (HybridW).</abstract>
      <url hash="78e353cb">2020.semeval-1.113</url>
      <bibkey>shrestha-rusert-2020-nlp</bibkey>
      <doi>10.18653/v1/2020.semeval-1.113</doi>
    </paper>
    <paper id="117">
      <title>CS-Embed at SemEval-2020 Task 9 : The Effectiveness of Code-switched Word Embeddings for Sentiment Analysis<fixed-case>CS</fixed-case>-Embed at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 9: The Effectiveness of Code-switched Word Embeddings for Sentiment Analysis</title>
      <author><first>Frances Adriana</first><last>Laureano De Leon</last></author>
      <author><first>Florimond</first><last>Guéniat</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <pages>922–927</pages>
      <abstract>The growing popularity and applications of <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> of social media posts has naturally led to <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis of posts</a> written in multiple languages, a practice known as <a href="https://en.wikipedia.org/wiki/Code-switching">code-switching</a>. While recent research into code-switched posts has focused on the use of multilingual word embeddings, these embeddings were not trained on code-switched data. In this work, we present <a href="https://en.wikipedia.org/wiki/Word_embedding">word-embeddings</a> trained on <a href="https://en.wikipedia.org/wiki/Code-switching">code-switched tweets</a>, specifically those that make use of <a href="https://en.wikipedia.org/wiki/Spanish_language">Spanish</a> and English, known as <a href="https://en.wikipedia.org/wiki/Spanglish">Spanglish</a>. We explore the embedding space to discover how they capture the meanings of words in both languages. We test the effectiveness of these embeddings by participating in SemEval 2020 Task 9 : <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> on Code-Mixed Social Media Text. We utilised them to train a <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment classifier</a> that achieves an F-1 score of 0.722. This is higher than the <a href="https://en.wikipedia.org/wiki/Baseline_(surveying)">baseline</a> for the <a href="https://en.wikipedia.org/wiki/Surveying">competition</a> of 0.656, with our team (codalab username francesita) ranking 14 out of 29 participating teams, beating the <a href="https://en.wikipedia.org/wiki/Baseline_(surveying)">baseline</a>.<i>Sentiment Analysis on Code-Mixed Social Media Text</i>. We utilised them to train a sentiment classifier that achieves an F-1 score of 0.722. This is higher than the baseline for the competition of 0.656, with our team (codalab username francesita) ranking 14 out of 29 participating teams, beating the baseline.</abstract>
      <url hash="5aa787f3">2020.semeval-1.117</url>
      <bibkey>laureano-de-leon-etal-2020-cs</bibkey>
      <doi>10.18653/v1/2020.semeval-1.117</doi>
      <pwccode url="https://github.com/francesita/CS-Embed-SemEval2020" additional="false">francesita/CS-Embed-SemEval2020</pwccode>
    </paper>
    <paper id="118">
      <title>FII-UAIC at SemEval-2020 Task 9 : Sentiment Analysis for Code-Mixed Social Media Text Using CNN<fixed-case>FII</fixed-case>-<fixed-case>UAIC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text Using <fixed-case>CNN</fixed-case></title>
      <author><first>Lavinia</first><last>Aparaschivei</last></author>
      <author><first>Andrei</first><last>Palihovici</last></author>
      <author><first>Daniela</first><last>Gîfu</last></author>
      <pages>928–933</pages>
      <abstract>The <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> for Code-Mixed Social Media Text task at the SemEval 2020 competition focuses on <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> in code-mixed social media text, specifically, on the combination of English with Spanish (Spanglish) and Hindi (Hinglish). In this paper, we present a system able to classify <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a>, from Spanish and English languages, into positive, negative and neutral. Firstly, we built a classifier able to provide corresponding <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment labels</a>. Besides the <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment labels</a>, we provide the <a href="https://en.wikipedia.org/wiki/Linguistic_description">language labels</a> at the <a href="https://en.wikipedia.org/wiki/Linguistic_description">word level</a>. Secondly, we generate a word-level representation, using Convolutional Neural Network (CNN) architecture. Our solution indicates promising results for the Sentimix Spanglish-English task (0.744), the team, Lavinia_Ap, occupied the 9th place. However, for the Sentimix Hindi-English task (0.324) the results have to be improved.</abstract>
      <url hash="5bc816a5">2020.semeval-1.118</url>
      <bibkey>aparaschivei-etal-2020-fii</bibkey>
      <doi>10.18653/v1/2020.semeval-1.118</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/sentimix">SentiMix</pwcdataset>
    </paper>
    <paper id="123">
      <title>NLP-CIC at SemEval-2020 Task 9 : Analysing Sentiment in Code-switching Language Using a Simple Deep-learning Classifier<fixed-case>NLP</fixed-case>-<fixed-case>CIC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 9: Analysing Sentiment in Code-switching Language Using a Simple Deep-learning Classifier</title>
      <author><first>Jason</first><last>Angel</last></author>
      <author><first>Segun Taofeek</first><last>Aroyehun</last></author>
      <author><first>Antonio</first><last>Tamayo</last></author>
      <author><first>Alexander</first><last>Gelbukh</last></author>
      <pages>957–962</pages>
      <abstract>Code-switching is a phenomenon in which two or more languages are used in the same message. Nowadays, it is quite common to find messages with languages mixed in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. This phenomenon presents a challenge for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. In this paper, we use a standard <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural network model</a> to predict the sentiment of tweets in a blend of Spanish and English languages. Our simple approach achieved a F1-score of 0:71 on test set on the competition. We analyze our best model capabilities and perform <a href="https://en.wikipedia.org/wiki/Error_detection_and_correction">error analysis</a> to expose important difficulties for classifying sentiment in a code-switching setting.</abstract>
      <url hash="de54a9f8">2020.semeval-1.123</url>
      <bibkey>angel-etal-2020-nlp</bibkey>
      <doi>10.18653/v1/2020.semeval-1.123</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/sentimix">SentiMix</pwcdataset>
    </paper>
    <paper id="124">
      <title>Palomino-Ochoa at SemEval-2020 Task 9 : Robust System Based on Transformer for Code-Mixed Sentiment Classification<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 9: Robust System Based on Transformer for Code-Mixed Sentiment Classification</title>
      <author><first>Daniel</first><last>Palomino</last></author>
      <author><first>José</first><last>Ochoa-Luna</last></author>
      <pages>963–967</pages>
      <abstract>We present a transfer learning system to perform a mixed Spanish-English sentiment classification task. Our proposal uses the state-of-the-art language model BERT and embed it within a ULMFiT transfer learning pipeline. This combination allows us to predict the polarity detection of code-mixed (English-Spanish) tweets. Thus, among 29 submitted systems, our approach (referred to as dplominop) is ranked 4th on the Sentimix Spanglish test set of SemEval 2020 Task 9. In fact, our <a href="https://en.wikipedia.org/wiki/System">system</a> yields the weighted-F1 score value of 0.755 which can be easily reproduced   the source code and implementation details are made available.</abstract>
      <url hash="aff7d188">2020.semeval-1.124</url>
      <bibkey>palomino-ochoa-luna-2020-palomino</bibkey>
      <doi>10.18653/v1/2020.semeval-1.124</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/sentimix">SentiMix</pwcdataset>
    </paper>
    <paper id="125">
      <title>ULD@NUIG at SemEval-2020 Task 9 : Generative Morphemes with an Attention Model for Sentiment Analysis in Code-Mixed Text<fixed-case>ULD</fixed-case>@<fixed-case>NUIG</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 9: Generative Morphemes with an Attention Model for Sentiment Analysis in Code-Mixed Text</title>
      <author><first>Koustava</first><last>Goswami</last></author>
      <author><first>Priya</first><last>Rani</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Theodorus</first><last>Fransen</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <pages>968–974</pages>
      <abstract>Code mixing is a common phenomena in <a href="https://en.wikipedia.org/wiki/Multilingualism">multilingual societies</a> where people switch from one language to another for various reasons. Recent advances in public communication over different <a href="https://en.wikipedia.org/wiki/Social_media">social media sites</a> have led to an increase in the frequency of code-mixed usage in <a href="https://en.wikipedia.org/wiki/Written_language">written language</a>. In this paper, we present the Generative Morphemes with Attention (GenMA) Model sentiment analysis system contributed to SemEval 2020 Task 9 SentiMix. The system aims to predict the sentiments of the given English-Hindi code-mixed tweets without using word-level language tags instead inferring this automatically using a <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphological model</a>. The system is based on a novel deep neural network (DNN) architecture, which has outperformed the baseline F1-score on the test data-set as well as the validation data-set. Our results can be found under the user name koustava on the Sentimix Hindi English page.</abstract>
      <url hash="caa55042">2020.semeval-1.125</url>
      <bibkey>goswami-etal-2020-uld</bibkey>
      <doi>10.18653/v1/2020.semeval-1.125</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/sentimix">SentiMix</pwcdataset>
    </paper>
    <paper id="129">
      <title>ECNU at SemEval-2020 Task 7 : Assessing Humor in Edited News Headlines Using BiLSTM with Attention<fixed-case>ECNU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 7: Assessing Humor in Edited News Headlines Using <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> with Attention</title>
      <author><first>Tiantian</first><last>Zhang</last></author>
      <author><first>Zhixuan</first><last>Chen</last></author>
      <author><first>Man</first><last>Lan</last></author>
      <pages>995–1000</pages>
      <abstract>In this paper we describe our <a href="https://en.wikipedia.org/wiki/System">system</a> submitted to SemEval 2020 Task 7 : Assessing Humor in Edited News Headlines. We participated in all subtasks, in which the main goal is to predict the mean funniness of the edited headline given the original and the edited headline. Our system involves two similar sub-networks, which generate <a href="https://en.wikipedia.org/wiki/Vector_graphics">vector representations</a> for the original and edited headlines respectively. And then we do a subtract operation of the outputs from two sub-networks to predict the funniness of the edited headline.</abstract>
      <url hash="7cdd3cbb">2020.semeval-1.129</url>
      <bibkey>zhang-etal-2020-ecnu</bibkey>
      <doi>10.18653/v1/2020.semeval-1.129</doi>
    </paper>
    <paper id="130">
      <title>ELMo-NB at SemEval-2020 Task 7 : Assessing Sense of Humor in EditedNews Headlines Using ELMo and NB<fixed-case>ELM</fixed-case>o-<fixed-case>NB</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 7: Assessing Sense of Humor in <fixed-case>E</fixed-case>dited<fixed-case>N</fixed-case>ews Headlines Using <fixed-case>ELM</fixed-case>o and <fixed-case>NB</fixed-case></title>
      <author><first>Enas</first><last>Khwaileh</last></author>
      <author><first>Muntaha A.</first><last>Al-As’ad</last></author>
      <pages>1001–1007</pages>
      <abstract>Our approach is constructed to improve on a couple of aspects ; preprocessing with an emphasis on humor sense detection, using embeddings from state-of-the-art language model(Elmo), and ensembling the results came up with using machine learning model Na ve Bayes(NB) with a deep learning pre-trained models. Elmo-NB participation has scored (0.5642) on the competition leader board, where results were measured by Root Mean Squared Error (RMSE).</abstract>
      <url hash="578f4d22">2020.semeval-1.130</url>
      <bibkey>khwaileh-al-asad-2020-elmo</bibkey>
      <doi>10.18653/v1/2020.semeval-1.130</doi>
    </paper>
    <paper id="131">
      <title>Ferryman at SemEval-2020 Task 7 : Ensemble Model for Assessing Humor in Edited News Headlines<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 7: Ensemble Model for Assessing Humor in Edited News Headlines</title>
      <author><first>Weilong</first><last>Chen</last></author>
      <author><first>Jipeng</first><last>Li</last></author>
      <author><first>Chenghao</first><last>Huang</last></author>
      <author><first>Wei</first><last>Bai</last></author>
      <author><first>Yanru</first><last>Zhang</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <pages>1008–1012</pages>
      <abstract>Natural language processing (NLP) has been applied to various <a href="https://en.wikipedia.org/wiki/Field_(computer_science)">fields</a> including <a href="https://en.wikipedia.org/wiki/Text_classification">text classification</a> and <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. In the shared task of assessing the funniness of edited news headlines, which is a part of the SemEval 2020 competition, we preprocess datasets by replacing abbreviation, stemming words, then merge three models including Light Gradient Boosting Machine (LightGBM), Long Short-Term Memory (LSTM), and Bidirectional Encoder Representation from Transformer (BERT) by taking the average to perform the best. Our team Ferryman wins the 9th place in Sub-task 1 of Task 7-Regression.</abstract>
      <url hash="15f8aaba">2020.semeval-1.131</url>
      <bibkey>chen-etal-2020-ferryman-semeval-2020</bibkey>
      <doi>10.18653/v1/2020.semeval-1.131</doi>
    </paper>
    <paper id="132">
      <title>Funny3 at SemEval-2020 Task 7 : Humor Detection of Edited Headlines with LSTM and TFIDF Neural Network System<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 7: Humor Detection of Edited Headlines with <fixed-case>LSTM</fixed-case> and <fixed-case>TFIDF</fixed-case> Neural Network System</title>
      <author><first>Xuefeng</first><last>Luo</last></author>
      <author><first>Kuan</first><last>Tang</last></author>
      <pages>1013–1018</pages>
      <abstract>This paper presents a neural network system where we participate in the first task of SemEval-2020 shared task 7 Assessing the Funniness of Edited News Headlines. Our target is to create to <a href="https://en.wikipedia.org/wiki/Neural_network">neural network model</a> that can predict the funniness of edited headlines. We build our model using a combination of LSTM and TF-IDF, then a feed-forward neural network. The <a href="https://en.wikipedia.org/wiki/System">system</a> manages to slightly improve RSME scores regarding our mean score baseline.</abstract>
      <url hash="e6fd220b">2020.semeval-1.132</url>
      <bibkey>luo-tang-2020-funny3</bibkey>
      <doi>10.18653/v1/2020.semeval-1.132</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/humicroedit">Humicroedit</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sarc">SARC</pwcdataset>
    </paper>
    <paper id="133">
      <title>HumorAAC at SemEval-2020 Task 7 : Assessing the Funniness of Edited News Headlines through Regression and Trump Mentions<fixed-case>H</fixed-case>umor<fixed-case>AAC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 7: Assessing the Funniness of Edited News Headlines through Regression and Trump Mentions</title>
      <author><first>Anna-Katharina</first><last>Dick</last></author>
      <author><first>Charlotte</first><last>Weirich</last></author>
      <author><first>Alla</first><last>Kutkina</last></author>
      <pages>1019–1025</pages>
      <abstract>In this paper we describe our contribution to the Semeval-2020 Humor Assessment task. We essentially use three different <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> that are passed into a <a href="https://en.wikipedia.org/wiki/Ridge_regression">ridge regression</a> to determine a funniness score for an edited news headline : statistical, count-based features, semantic features and contextual information. For deciding which one of two given edited headlines is funnier, we additionally use <a href="https://en.wikipedia.org/wiki/Score_(statistics)">scoring information</a> and <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>. Our work was mostly concentrated on investigating <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a>, rather than improving <a href="https://en.wikipedia.org/wiki/Prediction">prediction</a> based on pre-trained language models. The resulting <a href="https://en.wikipedia.org/wiki/System">system</a> is task-specific, lightweight and performs above the <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">majority baseline</a>. Our experiments indicate that <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> related to socio-cultural context, in our case mentions of Donald Trump, generally perform better than context-independent features like headline length.</abstract>
      <url hash="d2378d58">2020.semeval-1.133</url>
      <bibkey>dick-etal-2020-humoraac</bibkey>
      <doi>10.18653/v1/2020.semeval-1.133</doi>
    </paper>
    <paper id="136">
      <title>MLEngineer at SemEval-2020 Task 7 : BERT-Flair Based Humor Detection Model (BFHumor)<fixed-case>MLE</fixed-case>ngineer at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 7: <fixed-case>BERT</fixed-case>-Flair Based Humor Detection Model (<fixed-case>BFH</fixed-case>umor)</title>
      <author><first>Fara</first><last>Shatnawi</last></author>
      <author><first>Malak</first><last>Abdullah</last></author>
      <author><first>Mahmoud</first><last>Hammad</last></author>
      <pages>1041–1048</pages>
      <abstract>Task 7, Assessing the Funniness of Edited News Headlines, in the International Workshop SemEval2020 introduces two sub-tasks to predict the funniness values of edited news headlines from the Reddit website. This paper proposes the BFHumor model of the MLEngineer team that participates in both sub-tasks in this competition. The BFHumor’s model is defined as a BERT-Flair based humor detection model that is a combination of different pre-trained models with various Natural Language Processing (NLP) techniques. The Bidirectional Encoder Representations from Transformers (BERT) regressor is considered the primary pre-trained model in our approach, whereas Flair is the main NLP library. It is worth mentioning that the BFHumor model has been ranked 4th in sub-task1 with a root mean square error (RMSE) value of 0.51966, and it is 0.02 away from the first ranked model. Also, the team is ranked 12th in the sub-task2 with an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 0.62291, which is 0.05 away from the top-ranked model. Our results indicate that the BFHumor model is one of the top models for detecting humor in the text.</abstract>
      <url hash="6e3f087b">2020.semeval-1.136</url>
      <bibkey>shatnawi-etal-2020-mlengineer</bibkey>
      <doi>10.18653/v1/2020.semeval-1.136</doi>
    </paper>
    <paper id="140">
      <title>UTFPR at SemEval-2020 Task 7 : Using Co-occurrence Frequencies to Capture Unexpectedness<fixed-case>UTFPR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 7: Using Co-occurrence Frequencies to Capture Unexpectedness</title>
      <author><first>Gustavo Henrique</first><last>Paetzold</last></author>
      <pages>1066–1070</pages>
      <abstract>We describe the UTFPR system for SemEval-2020’s Task 7 : Assessing Humor in Edited News Headlines. Ours is a minimalist unsupervised system that uses word co-occurrence frequencies from large corpora to capture unexpectedness as a mean to capture funniness. Our <a href="https://en.wikipedia.org/wiki/System">system</a> placed 22nd on the shared task’s Task 2. We found that our approach requires more text than we used to perform reliably, and that unexpectedness alone is not sufficient to gauge funniness for humorous content that targets a diverse target audience.</abstract>
      <url hash="b9487b83">2020.semeval-1.140</url>
      <bibkey>paetzold-2020-utfpr</bibkey>
      <doi>10.18653/v1/2020.semeval-1.140</doi>
    </paper>
    <paper id="141">
      <title>WUY at SemEval-2020 Task 7 : Combining BERT and Naive Bayes-SVM for Humor Assessment in Edited News Headlines<fixed-case>WUY</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 7: Combining <fixed-case>BERT</fixed-case> and Naive <fixed-case>B</fixed-case>ayes-<fixed-case>SVM</fixed-case> for Humor Assessment in Edited News Headlines</title>
      <author><first>Cheng</first><last>Zhang</last></author>
      <author><first>Hayato</first><last>Yamana</last></author>
      <pages>1071–1076</pages>
      <abstract>This paper describes our participation in SemEval 2020 Task 7 on assessment of humor in edited news headlines, which includes two subtasks, estimating the humor of micro-editd news headlines (subtask A) and predicting the more humorous of the two edited headlines (subtask B). To address these tasks, we propose two <a href="https://en.wikipedia.org/wiki/System">systems</a>. The first system adopts a regression-based fine-tuned single-sequence bidirectional encoder representations from transformers (BERT) model with easy data augmentation (EDA), called BERT+EDA. The second system adopts a hybrid of a regression-based fine-tuned sequence-pair BERT model and a combined <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> and support vector machine (SVM) model estimated on term frequencyinverse document frequency (TFIDF) features, called BERT+NB-SVM. In this case, no additional training datasets were used, and the BERT+NB-SVM model outperformed BERT+EDA. The official root-mean-square deviation (RMSE) score for subtask A is 0.57369 and ranks 31st out of 48, whereas the best RMSE of BERT+NB-SVM is 0.52429, ranking 7th. For subtask B, we simply use a sequence-pair BERT model, the official accuracy of which is 0.53196 and ranks 25th out of 32.</abstract>
      <url hash="96d21553">2020.semeval-1.141</url>
      <bibkey>zhang-yamana-2020-wuy</bibkey>
      <doi>10.18653/v1/2020.semeval-1.141</doi>
    </paper>
    <paper id="144">
      <title>BERT at SemEval-2020 Task 8 : Using BERT to Analyse Meme Emotions<fixed-case>BERT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 8: Using <fixed-case>BERT</fixed-case> to Analyse Meme Emotions</title>
      <author><first>Adithya</first><last>Avvaru</last></author>
      <author><first>Sanath</first><last>Vobilisetty</last></author>
      <pages>1094–1099</pages>
      <abstract>Sentiment analysis, being one of the most sought after research problems within Natural Language Processing (NLP) researchers. The range of problems being addressed by <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> is increasing. Till now, most of the research focuses on predicting sentiment, or sentiment categories like <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a>, <a href="https://en.wikipedia.org/wiki/Humour">humor</a>, offense and motivation on text data. But, there is very limited research that is focusing on predicting or analyzing the sentiment of <a href="https://en.wikipedia.org/wiki/Internet_meme">internet memes</a>. We try to address this problem as part of Task 8 of SemEval 2020 : Memotion Analysis. We have participated in all the three tasks under Memotion Analysis. Our system built using state-of-the-art Transformer-based pre-trained Bidirectional Encoder Representations from Transformers (BERT) performed better compared to baseline models for the two tasks A and C and performed close to the baseline model for task B. In this paper, we present the data used, steps used by us for data cleaning and preparation, the fine-tuning process for BERT based model and finally predict the sentiment or sentiment categories. We found that the sequence models like Long Short Term Memory(LSTM) and its variants performed below par in predicting the sentiments. We also performed a comparative analysis with other Transformer based models like DistilBERT and XLNet.</abstract>
      <url hash="f49a3216">2020.semeval-1.144</url>
      <bibkey>avvaru-vobilisetty-2020-bert</bibkey>
      <doi>10.18653/v1/2020.semeval-1.144</doi>
    </paper>
    <paper id="146">
      <title>CSECU_KDE_MA at SemEval-2020 Task 8 : A Neural Attention Model for Memotion Analysis<fixed-case>CSECU</fixed-case>_<fixed-case>KDE</fixed-case>_<fixed-case>MA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 8: A Neural Attention Model for Memotion Analysis</title>
      <author><first>Abu Nowshed</first><last>Chy</last></author>
      <author><first>Umme Aymun</first><last>Siddiqua</last></author>
      <author><first>Masaki</first><last>Aono</last></author>
      <pages>1106–1111</pages>
      <abstract>A <a href="https://en.wikipedia.org/wiki/Meme">meme</a> is a pictorial representation of an idea or theme. In the age of emerging volume of <a href="https://en.wikipedia.org/wiki/Social_media">social media platforms</a>, <a href="https://en.wikipedia.org/wiki/Meme">memes</a> are spreading rapidly from person to person and becoming a trending ways of opinion expression. However, due to the multimodal characteristics of meme contents, detecting and analyzing the underlying emotion of a <a href="https://en.wikipedia.org/wiki/Meme">meme</a> is a formidable task. In this paper, we present our approach for detecting the emotion of a <a href="https://en.wikipedia.org/wiki/Meme">meme</a> defined in the SemEval-2020 Task 8. Our team CSECU_KDE_MA employs an attention-based neural network model to tackle the problem. Upon extracting the text contents from a <a href="https://en.wikipedia.org/wiki/Meme">meme</a> using an optical character reader (OCR), we represent it using the distributed representation of words. Next, we perform the <a href="https://en.wikipedia.org/wiki/Convolution">convolution</a> based on multiple kernel sizes to obtain the higher-level feature sequences. The <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">feature sequences</a> are then fed into the attentive time-distributed bidirectional LSTM model to learn the long-term dependencies effectively. Experimental results show that our proposed neural model obtained competitive performance among the participants’ systems.</abstract>
      <url hash="78ff8d8a">2020.semeval-1.146</url>
      <bibkey>chy-etal-2020-csecu</bibkey>
      <doi>10.18653/v1/2020.semeval-1.146</doi>
    </paper>
    <paper id="149">
      <title>Hitachi at SemEval-2020 Task 8 : Simple but Effective Modality Ensemble for Meme Emotion Recognition<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 8: Simple but Effective Modality Ensemble for Meme Emotion Recognition</title>
      <author><first>Terufumi</first><last>Morishita</last></author>
      <author><first>Gaku</first><last>Morio</last></author>
      <author><first>Shota</first><last>Horiguchi</last></author>
      <author><first>Hiroaki</first><last>Ozaki</last></author>
      <author><first>Toshinori</first><last>Miyoshi</last></author>
      <pages>1126–1134</pages>
      <abstract>Users of <a href="https://en.wikipedia.org/wiki/Social_networking_service">social networking services</a> often share their emotions via multi-modal content, usually <a href="https://en.wikipedia.org/wiki/Image">images</a> paired with text embedded in them. SemEval-2020 task 8, Memotion Analysis, aims at automatically recognizing these <a href="https://en.wikipedia.org/wiki/Emotion">emotions</a> of so-called <a href="https://en.wikipedia.org/wiki/Internet_meme">internet memes</a>. In this paper, we propose a simple but effective Modality Ensemble that incorporates visual and textual deep-learning models, which are independently trained, rather than providing a single multi-modal joint network. To this end, we first fine-tune four pre-trained <a href="https://en.wikipedia.org/wiki/Visual_system">visual models</a> (i.e., Inception-ResNet, PolyNet, SENet, and PNASNet) and four textual models (i.e., <a href="https://en.wikipedia.org/wiki/BERT">BERT</a>, GPT-2, Transformer-XL, and XLNet). Then, we fuse their predictions with <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble methods</a> to effectively capture cross-modal correlations. The experiments performed on dev-set show that both visual and textual features aided each other, especially in subtask-C, and consequently, our system ranked 2nd on subtask-C.</abstract>
      <url hash="10d22014">2020.semeval-1.149</url>
      <bibkey>morishita-etal-2020-hitachi-semeval-2020</bibkey>
      <doi>10.18653/v1/2020.semeval-1.149</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="154">
      <title>Memebusters at SemEval-2020 Task 8 : Feature Fusion Model for Sentiment Analysis on Memes Using Transfer Learning<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 8: Feature Fusion Model for Sentiment Analysis on Memes Using Transfer Learning</title>
      <author><first>Mayukh</first><last>Sharma</last></author>
      <author><first>Ilanthenral</first><last>Kandasamy</last></author>
      <author><first>W.b.</first><last>Vasantha</last></author>
      <pages>1163–1171</pages>
      <abstract>In this paper, we describe our deep learning system used for SemEval 2020 Task 8 : Memotion analysis. We participated in all the subtasks i.e Subtask A : Sentiment classification, Subtask B : Humor classification, and Subtask C : Scales of semantic classes. Similar multimodal architecture was used for each subtask. The proposed <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a> makes use of <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> for images and text feature extraction. The extracted <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">features</a> are then fused together using stacked bidirectional Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) model with attention mechanism for final predictions. We also propose a single model for predicting semantic classes (Subtask B) as well as their scales (Subtask C) by branching the final output of the post LSTM dense layers. Our model was ranked 5 in Subtask B and ranked 8 in Subtask C and performed nicely in Subtask A on the leader board. Our system makes use of <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> for <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a> and fusion of image and text features for predictions.</abstract>
      <url hash="a2867821">2020.semeval-1.154</url>
      <bibkey>sharma-etal-2020-memebusters</bibkey>
      <doi>10.18653/v1/2020.semeval-1.154</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/memotion-analysis">Memotion Analysis</pwcdataset>
    </paper>
    <paper id="157">
      <title>SIS@IIITH at SemEval-2020 Task 8 : An Overview of Simple Text Classification Methods for Meme Analysis<fixed-case>SIS</fixed-case>@<fixed-case>IIITH</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 8: An Overview of Simple Text Classification Methods for Meme Analysis</title>
      <author><first>Sravani</first><last>Boinepelli</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <pages>1190–1194</pages>
      <abstract>Memes are steadily taking over the feeds of the public on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. There is always the threat of malicious users on the internet posting offensive content, even through <a href="https://en.wikipedia.org/wiki/Meme">memes</a>. Hence, the automatic detection of offensive images / memes is imperative along with detection of offensive text. However, this is a much more complex task as it involves both <a href="https://en.wikipedia.org/wiki/Sensory_cue">visual cues</a> as well as <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">language understanding</a> and <a href="https://en.wikipedia.org/wiki/Context_(language_use)">cultural / context knowledge</a>. This paper describes our approach to the task of SemEval-2020 Task 8 : Memotion Analysis. We chose to participate only in Task A which dealt with Sentiment Classification, which we formulated as a text classification problem. Through our experiments, we explored multiple training models to evaluate the performance of simple text classification algorithms on the raw text obtained after running <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">OCR</a> on meme images. Our submitted <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieved an <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> of 72.69 % and exceeded the existing baseline’s Macro F1 score by 8 % on the official test dataset. Apart from describing our official submission, we shall elucidate how different <a href="https://en.wikipedia.org/wiki/Statistical_model">classification models</a> respond to this task.</abstract>
      <url hash="ad908cd7">2020.semeval-1.157</url>
      <bibkey>boinepelli-etal-2020-sis</bibkey>
      <doi>10.18653/v1/2020.semeval-1.157</doi>
    </paper>
    <paper id="159">
      <title>UoR at SemEval-2020 Task 8 : Gaussian Mixture Modelling (GMM) Based Sampling Approach for Multi-modal Memotion Analysis<fixed-case>U</fixed-case>o<fixed-case>R</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 8: <fixed-case>G</fixed-case>aussian Mixture Modelling (<fixed-case>GMM</fixed-case>) Based Sampling Approach for Multi-modal Memotion Analysis</title>
      <author><first>Zehao</first><last>Liu</last></author>
      <author><first>Emmanuel</first><last>Osei-Brefo</last></author>
      <author><first>Siyuan</first><last>Chen</last></author>
      <author><first>Huizhi</first><last>Liang</last></author>
      <pages>1201–1207</pages>
      <abstract>Memes are widely used on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. They usually contain multi-modal information such as <a href="https://en.wikipedia.org/wiki/Image">images</a> and <a href="https://en.wikipedia.org/wiki/Text_(literary_theory)">texts</a>, serving as valuable data sources to analyse opinions and sentiment orientations of online communities. The provided memes data often face an imbalanced data problem, that is, some classes or labelled sentiment categories significantly outnumber other classes. This often results in difficulty in applying machine learning techniques where balanced labelled input data are required. In this paper, a Gaussian Mixture Model sampling method is proposed to tackle the problem of <a href="https://en.wikipedia.org/wiki/Social_class">class imbalance</a> for the memes sentiment classification task. To utilise both text and image data, a multi-modal CNN-LSTM model is proposed to jointly learn latent features for positive, negative and neutral category predictions. The experiments show that the re-sampling model can slightly improve the <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> on the trial data of sub-task A of Task 8. The multi-modal CNN-LSTM model can achieve macro F1 score 0.329 on the test set.</abstract>
      <url hash="13b03497">2020.semeval-1.159</url>
      <bibkey>liu-etal-2020-uor</bibkey>
      <doi>10.18653/v1/2020.semeval-1.159</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/memotion-analysis">Memotion Analysis</pwcdataset>
    </paper>
    <paper id="162">
      <title>BAKSA at SemEval-2020 Task 9 : Bolstering CNN with Self-Attention for Sentiment Analysis of Code Mixed Text<fixed-case>BAKSA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 9: Bolstering <fixed-case>CNN</fixed-case> with Self-Attention for Sentiment Analysis of Code Mixed Text</title>
      <author><first>Ayush</first><last>Kumar</last></author>
      <author><first>Harsh</first><last>Agarwal</last></author>
      <author><first>Keshav</first><last>Bansal</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>1221–1226</pages>
      <abstract>Sentiment Analysis of code-mixed text has diversified applications in <a href="https://en.wikipedia.org/wiki/Opinion_mining">opinion mining</a> ranging from tagging user reviews to identifying social or political sentiments of a sub-population. In this paper, we present an ensemble architecture of convolutional neural net (CNN) and self-attention based LSTM for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> of code-mixed tweets. While the CNN component helps in the classification of positive and negative tweets, the self-attention based LSTM, helps in the classification of neutral tweets, because of its ability to identify correct <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment</a> among multiple <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment bearing units</a>. We achieved F1 scores of 0.707 (ranked 5th) and 0.725 (ranked 13th) on Hindi-English (Hinglish) and Spanish-English (Spanglish) datasets, respectively. The submissions for Hinglish and Spanglish tasks were made under the usernames ayushk and harsh_6 respectively.</abstract>
      <url hash="c7d202b1">2020.semeval-1.162</url>
      <bibkey>kumar-etal-2020-baksa</bibkey>
      <doi>10.18653/v1/2020.semeval-1.162</doi>
      <pwccode url="https://github.com/keshav22bansal/BAKSA_IITK" additional="false">keshav22bansal/BAKSA_IITK</pwccode>
    </paper>
    <paper id="164">
      <title>Deep Learning Brasil-NLP at SemEval-2020 Task 9 : Sentiment Analysis of Code-Mixed Tweets Using Ensemble of Language Models<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 9: Sentiment Analysis of Code-Mixed Tweets Using Ensemble of Language Models</title>
      <author><first>Manoel</first><last>Veríssimo dos Santos Neto</last></author>
      <author><first>Ayrton</first><last>Amaral</last></author>
      <author><first>Nádia</first><last>Silva</last></author>
      <author><first>Anderson</first><last>da Silva Soares</last></author>
      <pages>1233–1238</pages>
      <abstract>In this paper, we describe a <a href="https://en.wikipedia.org/wiki/Methodology">methodology</a> to predict <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment</a> in code-mixed tweets (hindi-english). Our team called verissimo.manoel in CodaLab developed an approach based on an ensemble of four models (MultiFiT, BERT, ALBERT, and XLNET). The final <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification algorithm</a> was an ensemble of some predictions of all softmax values from these four <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a>. This <a href="https://en.wikipedia.org/wiki/Software_architecture">architecture</a> was used and evaluated in the context of the SemEval 2020 challenge (task 9), and our <a href="https://en.wikipedia.org/wiki/System">system</a> got 72.7 % on the F1 score.</abstract>
      <url hash="b0bac4b5">2020.semeval-1.164</url>
      <bibkey>verissimo-dos-santos-neto-etal-2020-deep</bibkey>
      <doi>10.18653/v1/2020.semeval-1.164</doi>
    </paper>
    <paper id="170">
      <title>IUST at SemEval-2020 Task 9 : Sentiment Analysis for Code-Mixed Social Media Text Using Deep Neural Networks and Linear Baselines<fixed-case>IUST</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text Using Deep Neural Networks and Linear Baselines</title>
      <author><first>Soroush</first><last>Javdan</last></author>
      <author><first>Taha</first><last>Shangipour ataei</last></author>
      <author><first>Behrouz</first><last>Minaei-Bidgoli</last></author>
      <pages>1270–1275</pages>
      <abstract>Sentiment Analysis is a well-studied field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a>. However, the rapid growth of <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> and noisy content within them poses significant challenges in addressing this problem with well-established methods and tools. One of these challenges is <a href="https://en.wikipedia.org/wiki/Code_mixing">code-mixing</a>, which means using different languages to convey thoughts in social media texts. Our group, with the name of IUST(username : TAHA), participated at the SemEval-2020 shared task 9 on Sentiment Analysis for Code-Mixed Social Media Text, and we have attempted to develop a system to predict the sentiment of a given code-mixed tweet. We used different preprocessing techniques and proposed to use different methods that vary from NBSVM to more complicated deep neural network models. Our best performing method obtains an F1 score of 0.751 for the Spanish-English sub-task and 0.706 over the Hindi-English sub-task.</abstract>
      <url hash="94093d17">2020.semeval-1.170</url>
      <bibkey>javdan-etal-2020-iust</bibkey>
      <doi>10.18653/v1/2020.semeval-1.170</doi>
    </paper>
    <paper id="174">
      <title>MeisterMorxrc at SemEval-2020 Task 9 : Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets<fixed-case>M</fixed-case>eister<fixed-case>M</fixed-case>orxrc at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 9: Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets</title>
      <author><first>Qi</first><last>Wu</last></author>
      <author><first>Peng</first><last>Wang</last></author>
      <author><first>Chenghao</first><last>Huang</last></author>
      <pages>1294–1297</pages>
      <abstract>Natural language processing (NLP) has been applied to various <a href="https://en.wikipedia.org/wiki/Field_(computer_science)">fields</a> including <a href="https://en.wikipedia.org/wiki/Text_classification">text classification</a> and <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. In the shared task of <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> of code-mixed tweets, which is a part of the SemEval-2020 competition, we preprocess datasets by replacing <a href="https://en.wikipedia.org/wiki/Emoji">emoji</a> and deleting uncommon characters and so on, and then fine-tune the Bidirectional Encoder Representation from Transformers(BERT) to perform the best. After exhausting top3 submissions, Our team MeisterMorxrc achieves an averaged F1 score of 0.730 in this task, and and our codalab username is MeisterMorxrc</abstract>
      <url hash="fadd8bba">2020.semeval-1.174</url>
      <bibkey>wu-etal-2020-meistermorxrc</bibkey>
      <doi>10.18653/v1/2020.semeval-1.174</doi>
    </paper>
    <paper id="181">
      <title>WESSA at SemEval-2020 Task 9 : Code-Mixed Sentiment Analysis Using Transformers<fixed-case>WESSA</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 9: Code-Mixed Sentiment Analysis Using Transformers</title>
      <author><first>Ahmed</first><last>Sultan</last></author>
      <author><first>Mahmoud</first><last>Salim</last></author>
      <author><first>Amina</first><last>Gaber</last></author>
      <author><first>Islam</first><last>El Hosary</last></author>
      <pages>1342–1347</pages>
      <abstract>In this paper, we describe our system submitted for SemEval 2020 Task 9, <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> for Code-Mixed Social Media Text alongside other experiments. Our best performing system is a Transfer Learning-based model that fine-tunes XLM-RoBERTa, a transformer-based multilingual masked language model, on monolingual English and Spanish data and Spanish-English code-mixed data. Our <a href="https://en.wikipedia.org/wiki/System">system</a> outperforms the official task baseline by achieving a 70.1 % average F1-Score on the official leaderboard using the test set. For later submissions, our <a href="https://en.wikipedia.org/wiki/System">system</a> manages to achieve a 75.9 % average F1-Score on the test set using CodaLab username ahmed0sultan.</abstract>
      <url hash="093170e3">2020.semeval-1.181</url>
      <bibkey>sultan-etal-2020-wessa</bibkey>
      <doi>10.18653/v1/2020.semeval-1.181</doi>
    </paper>
    <paper id="183">
      <title>Zyy1510 Team at SemEval-2020 Task 9 : Sentiment Analysis for Code-Mixed Social Media Text with Sub-word Level Representations<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text with Sub-word Level Representations</title>
      <author><first>Yueying</first><last>Zhu</last></author>
      <author><first>Xiaobing</first><last>Zhou</last></author>
      <author><first>Hongling</first><last>Li</last></author>
      <author><first>Kunjie</first><last>Dong</last></author>
      <pages>1354–1359</pages>
      <abstract>This paper reports the zyy1510 team’s work in the International Workshop on Semantic Evaluation (SemEval-2020) shared task on <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment analysis</a> for Code-Mixed (Hindi-English, English-Spanish) Social Media Text. The purpose of this task is to determine the polarity of the text, dividing it into one of the three labels positive, negative and neutral. To achieve this goal, we propose an ensemble model of word n-grams-based Multinomial Naive Bayes (MNB) and sub-word level representations in LSTM (Sub-word LSTM) to identify the sentiments of code-mixed data of Hindi-English and English-Spanish. This <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble model</a> combines the advantage of rich sequential patterns and the intermediate features after convolution from the LSTM model, and the polarity of keywords from the MNB model to obtain the final sentiment score. We have tested our system on Hindi-English and English-Spanish code-mixed social media data sets released for the task. Our model achieves the F1 score of 0.647 in the Hindi-English task and 0.682 in the English-Spanish task, respectively.</abstract>
      <url hash="fc22ac55">2020.semeval-1.183</url>
      <bibkey>zhu-etal-2020-zyy1510</bibkey>
      <doi>10.18653/v1/2020.semeval-1.183</doi>
    </paper>
    <paper id="188">
      <title>SemEval-2020 Task 12 : Multilingual Offensive Language Identification in Social Media (OffensEval 2020)<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Multilingual Offensive Language Identification in Social Media (<fixed-case>O</fixed-case>ffens<fixed-case>E</fixed-case>val 2020)</title>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Sara</first><last>Rosenthal</last></author>
      <author><first>Pepa</first><last>Atanasova</last></author>
      <author><first>Georgi</first><last>Karadzhov</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <author><first>Zeses</first><last>Pitenis</last></author>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <pages>1425–1447</pages>
      <abstract>We present the results and the main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020). The task included three subtasks corresponding to the hierarchical taxonomy of the OLID schema from OffensEval-2019, and it was offered in five languages : <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>, <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Greek_language">Greek</a>, and <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a>. OffensEval-2020 was one of the most popular tasks at SemEval-2020, attracting a large number of participants across all subtasks and languages : a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers.</abstract>
      <url hash="7c06d352">2020.semeval-1.188</url>
      <bibkey>zampieri-etal-2020-semeval</bibkey>
      <doi>10.18653/v1/2020.semeval-1.188</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/dkhate">DKhate</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="189">
      <title>Galileo at SemEval-2020 Task 12 : Multi-lingual Learning for Offensive Language Identification Using Pre-trained Language Models<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Multi-lingual Learning for Offensive Language Identification Using Pre-trained Language Models</title>
      <author><first>Shuohuan</first><last>Wang</last></author>
      <author><first>Jiaxiang</first><last>Liu</last></author>
      <author><first>Xuan</first><last>Ouyang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <pages>1448–1455</pages>
      <abstract>This paper describes <a href="https://en.wikipedia.org/wiki/Galileo_(spacecraft)">Galileo</a>’s performance in SemEval-2020 Task 12 on detecting and categorizing <a href="https://en.wikipedia.org/wiki/Profanity">offensive language</a> in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. For Offensive Language Identification, we proposed a multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive language categorization, we proposed a knowledge distillation method trained on soft labels generated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A-Offensive Language Identification, we ranked first in terms of average F1 scores in all languages. We are also the only team which ranked among the top three across all languages. We also took the first place in Sub-task B-Automatic Categorization of Offense Types and Sub-task C-Offence Target Identification.</abstract>
      <url hash="fd7b4056">2020.semeval-1.189</url>
      <bibkey>wang-etal-2020-galileo</bibkey>
      <doi>10.18653/v1/2020.semeval-1.189</doi>
    </paper>
    <paper id="191">
      <title>Aschern at SemEval-2020 Task 11 : It Takes Three to Tango : RoBERTa, CRF, and Transfer Learning<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 11: It Takes Three to Tango: <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a, <fixed-case>CRF</fixed-case>, and Transfer Learning</title>
      <author><first>Anton</first><last>Chernyavskiy</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1462–1468</pages>
      <abstract>We describe our <a href="https://en.wikipedia.org/wiki/System">system</a> for SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. We developed ensemble models using RoBERTa-based neural architectures, additional CRF layers, <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> between the two subtasks, and advanced post-processing to handle the multi-label nature of the task, the consistency between nested spans, repetitions, and labels from similar spans in training. We achieved sizable improvements over baseline fine-tuned RoBERTa models, and the official evaluation ranked our system 3rd (almost tied with the 2nd) out of 36 teams on the span identification subtask with an F1 score of 0.491, and 2nd (almost tied with the 1st) out of 31 teams on the technique classification subtask with an F1 score of 0.62.</abstract>
      <url hash="a197e0fd">2020.semeval-1.191</url>
      <bibkey>chernyavskiy-etal-2020-aschern</bibkey>
      <doi>10.18653/v1/2020.semeval-1.191</doi>
      <pwccode url="https://github.com/aschern/semeval2020_task11" additional="false">aschern/semeval2020_task11</pwccode>
    </paper>
    <paper id="198">
      <title>AdelaideCyC at SemEval-2020 Task 12 : Ensemble of Classifiers for Offensive Language Detection in Social Media<fixed-case>A</fixed-case>delaide<fixed-case>C</fixed-case>y<fixed-case>C</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Ensemble of Classifiers for Offensive Language Detection in Social Media</title>
      <author><first>Mahen</first><last>Herath</last></author>
      <author><first>Thushari</first><last>Atapattu</last></author>
      <author><first>Hoang Anh</first><last>Dung</last></author>
      <author><first>Christoph</first><last>Treude</last></author>
      <author><first>Katrina</first><last>Falkner</last></author>
      <pages>1516–1523</pages>
      <abstract>This paper describes the systems our team (AdelaideCyC) has developed for SemEval Task 12 (OffensEval 2020) to detect offensive language in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. The challenge focuses on three subtasks   offensive language identification (subtask A), offense type identification (subtask B), and offense target identification (subtask C). Our team has participated in all the three subtasks. We have developed machine learning and deep learning-based ensembles of models. We have achieved F1-scores of 0.906, 0.552, and 0.623 in subtask A, B, and C respectively. While our performance scores are promising for subtask A, the results demonstrate that subtask B and C still remain challenging to classify.</abstract>
      <url hash="929044de">2020.semeval-1.198</url>
      <bibkey>herath-etal-2020-adelaidecyc</bibkey>
      <doi>10.18653/v1/2020.semeval-1.198</doi>
    </paper>
    <paper id="202">
      <title>GruPaTo at SemEval-2020 Task 12 : Retraining mBERT on Social Media and Fine-tuned Offensive Language Models<fixed-case>G</fixed-case>ru<fixed-case>P</fixed-case>a<fixed-case>T</fixed-case>o at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Retraining m<fixed-case>BERT</fixed-case> on Social Media and Fine-tuned Offensive Language Models</title>
      <author><first>Davide</first><last>Colla</last></author>
      <author><first>Tommaso</first><last>Caselli</last></author>
      <author><first>Valerio</first><last>Basile</last></author>
      <author><first>Jelena</first><last>Mitrović</last></author>
      <author><first>Michael</first><last>Granitzer</last></author>
      <pages>1546–1554</pages>
      <abstract>We introduce an approach to multilingual Offensive Language Detection based on the mBERT transformer model. We download extra training data from <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>, and <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a>, and use it to re-train the model. We then fine-tuned the model on the provided training data and, in some configurations, implement transfer learning approach exploiting the typological relatedness between <a href="https://en.wikipedia.org/wiki/English_language">English</a> and <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>. Our systems obtained good results across the three languages (.9036 for EN,.7619 for DA, and.7789 for TR).</abstract>
      <url hash="09931e41">2020.semeval-1.202</url>
      <bibkey>colla-etal-2020-grupato</bibkey>
      <doi>10.18653/v1/2020.semeval-1.202</doi>
    </paper>
    <paper id="203">
      <title>GUIR at SemEval-2020 Task 12 : Domain-Tuned Contextualized Models for Offensive Language Detection<fixed-case>GUIR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Domain-Tuned Contextualized Models for Offensive Language Detection</title>
      <author><first>Sajad</first><last>Sotudeh</last></author>
      <author><first>Tong</first><last>Xiang</last></author>
      <author><first>Hao-Ren</first><last>Yao</last></author>
      <author><first>Sean</first><last>MacAvaney</last></author>
      <author><first>Eugene</first><last>Yang</last></author>
      <author><first>Nazli</first><last>Goharian</last></author>
      <author><first>Ophir</first><last>Frieder</last></author>
      <pages>1555–1561</pages>
      <abstract>Offensive language detection is an important and challenging task in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>. We present our submissions to the OffensEval 2020 shared task, which includes three English sub-tasks : identifying the presence of offensive language (Sub-task A), identifying the presence of target in offensive language (Sub-task B), and identifying the categories of the target (Sub-task C). Our experiments explore using a domain-tuned contextualized language model (namely, BERT) for this task. We also experiment with different <a href="https://en.wikipedia.org/wiki/Component-based_software_engineering">components</a> and configurations (e.g., a multi-view SVM) stacked upon BERT models for specific <a href="https://en.wikipedia.org/wiki/Subroutine">sub-tasks</a>. Our submissions achieve <a href="https://en.wikipedia.org/wiki/F-number">F1 scores</a> of 91.7 % in <a href="https://en.wikipedia.org/wiki/Task_(computing)">Sub-task</a> A, 66.5 % in Sub-task B, and 63.2 % in Sub-task C. We perform an ablation study which reveals that domain tuning considerably improves the classification performance. Furthermore, error analysis shows common misclassification errors made by our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and outlines research directions for future.</abstract>
      <url hash="f32d0e10">2020.semeval-1.203</url>
      <bibkey>sotudeh-etal-2020-guir</bibkey>
      <doi>10.18653/v1/2020.semeval-1.203</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="204">
      <title>IIITG-ADBU at SemEval-2020 Task 12 : Comparison of BERT and BiLSTM in Detecting Offensive Language<fixed-case>IIITG</fixed-case>-<fixed-case>ADBU</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Comparison of <fixed-case>BERT</fixed-case> and <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> in Detecting Offensive Language</title>
      <author><first>Arup</first><last>Baruah</last></author>
      <author><first>Kaushik</first><last>Das</last></author>
      <author><first>Ferdous</first><last>Barbhuiya</last></author>
      <author><first>Kuntal</first><last>Dey</last></author>
      <pages>1562–1568</pages>
      <abstract>Task 12 of SemEval 2020 consisted of 3 subtasks, namely offensive language identification (Subtask A), categorization of offense type (Subtask B), and offense target identification (Subtask C). This paper presents the results our <a href="https://en.wikipedia.org/wiki/Classifier_(linguistics)">classifiers</a> obtained for the <a href="https://en.wikipedia.org/wiki/English_language">English language</a> in the 3 subtasks. The <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> used by us were BERT and BiLSTM. On the test set, our BERT classifier obtained macro F1 score of 0.90707 for subtask A, and 0.65279 for subtask B. The BiLSTM classifier obtained macro F1 score of 0.57565 for subtask C. The paper also performs an analysis of the errors made by our <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a>. We conjecture that the presence of few misleading instances in the dataset is affecting the performance of the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a>. Our analysis also discusses the need of <a href="https://en.wikipedia.org/wiki/Context_(language_use)">temporal context</a> and <a href="https://en.wikipedia.org/wiki/World_knowledge">world knowledge</a> to determine the offensiveness of few comments.</abstract>
      <url hash="64833cb4">2020.semeval-1.204</url>
      <bibkey>baruah-etal-2020-iiitg-adbu-semeval</bibkey>
      <doi>10.18653/v1/2020.semeval-1.204</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech-and-offensive-language">Hate Speech and Offensive Language</pwcdataset>
    </paper>
    <paper id="208">
      <title>NUIG at SemEval-2020 Task 12 : Pseudo Labelling for Offensive Content Classification<fixed-case>NUIG</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Pseudo Labelling for Offensive Content Classification</title>
      <author><first>Shardul</first><last>Suryawanshi</last></author>
      <author><first>Mihael</first><last>Arcan</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <pages>1598–1604</pages>
      <abstract>This work addresses the classification problem defined by sub-task A (English only) of the OffensEval 2020 challenge. We used a semi-supervised approach to classify given tweets into an offensive (OFF) or not-offensive (NOT) class. As the OffensEval 2020 dataset is loosely labelled with confidence scores given by <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised models</a>, we used last year’s offensive language identification dataset (OLID) to label the OffensEval 2020 dataset. Our approach uses a pseudo-labelling method to annotate the current <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. We trained four text classifiers on the OLID dataset and the classifier with the highest macro-averaged F1-score has been used to pseudo label the OffensEval 2020 dataset. The same model which performed best amongst four text classifiers on <a href="https://en.wikipedia.org/wiki/Online_analytical_processing">OLID dataset</a> has been trained on the combined dataset of <a href="https://en.wikipedia.org/wiki/Online_analytical_processing">OLID</a> and pseudo labelled OffensEval 2020. We evaluated the classifiers with precision, recall and macro-averaged F1-score as the primary evaluation metric on the OLID and OffensEval 2020 datasets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details :.<url>http://creativecommons.org/licenses/by/4.0/</url>.</abstract>
      <url hash="d796fa3e">2020.semeval-1.208</url>
      <bibkey>suryawanshi-etal-2020-nuig</bibkey>
      <doi>10.18653/v1/2020.semeval-1.208</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="213">
      <title>UHH-LT at SemEval-2020 Task 12 : Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection<fixed-case>UHH</fixed-case>-<fixed-case>LT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection</title>
      <author><first>Gregor</first><last>Wiedemann</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>1638–1644</pages>
      <abstract>Fine-tuning of pre-trained transformer networks such as BERT yield state-of-the-art results for text classification tasks. Typically, <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> is performed on <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">task-specific training datasets</a> in a supervised manner. One can also fine-tune in unsupervised manner beforehand by further pre-training the masked language modeling (MLM) task. Hereby, in-domain data for <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised MLM</a> resembling the actual classification target dataset allows for domain adaptation of the <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a>. In this paper, we compare current pre-trained transformer networks with and without MLM fine-tuning on their performance for offensive language detection. Our MLM fine-tuned RoBERTa-based classifier officially ranks 1st in the SemEval 2020 Shared Task 12 for the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>. Further experiments with the ALBERT model even surpass this result.</abstract>
      <url hash="aeb6b4e3">2020.semeval-1.213</url>
      <bibkey>wiedemann-etal-2020-uhh</bibkey>
      <doi>10.18653/v1/2020.semeval-1.213</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="214">
      <title>EL-BERT at SemEval-2020 Task 10 : A Multi-Embedding Ensemble Based Approach for Emphasis Selection in Visual Media<fixed-case>EL</fixed-case>-<fixed-case>BERT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 10: A Multi-Embedding Ensemble Based Approach for Emphasis Selection in Visual Media</title>
      <author><first>Chandresh</first><last>Kanani</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>1645–1651</pages>
      <abstract>In <a href="https://en.wikipedia.org/wiki/Media_(communication)">visual media</a>, text emphasis is the strengthening of words in a text to convey the intent of the author. Text emphasis in visual media is generally done by using different colors, backgrounds, or font for the text ; it helps in conveying the actual meaning of the message to the readers. Emphasis selection is the task of choosing candidate words for <a href="https://en.wikipedia.org/wiki/Emphasis_(typography)">emphasis</a>, it helps in automatically designing <a href="https://en.wikipedia.org/wiki/Poster">posters</a> and other media contents with <a href="https://en.wikipedia.org/wiki/Writing">written text</a>. If we consider only the text and do not know the intent, then there can be multiple valid emphasis selections. We propose the use of <a href="https://en.wikipedia.org/wiki/Musical_ensemble">ensembles</a> for emphasis selection to improve over single emphasis selection models. We show that the use of multi-embedding helps in enhancing the results for base models. To show the efficacy of proposed approach we have also done a comparison of our results with state-of-the-art models.</abstract>
      <url hash="a4c4fdbc">2020.semeval-1.214</url>
      <bibkey>kanani-etal-2020-el</bibkey>
      <doi>10.18653/v1/2020.semeval-1.214</doi>
    </paper>
    <paper id="218">
      <title>LAST at SemEval-2020 Task 10 : Finding Tokens to Emphasise in Short Written Texts with Precomputed Embedding Models and LightGBM<fixed-case>LAST</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 10: Finding Tokens to Emphasise in Short Written Texts with Precomputed Embedding Models and <fixed-case>L</fixed-case>ight<fixed-case>GBM</fixed-case></title>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>1671–1677</pages>
      <abstract>To select tokens to be emphasised in short texts, a system mainly based on precomputed embedding models, such as <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> and ELMo, and LightGBM is proposed. Its performance is low. Additional analyzes suggest that its effectiveness is poor at predicting the highest emphasis scores while they are the most important for the challenge and that it is very sensitive to the specific instances provided during learning.</abstract>
      <url hash="f6a1f955">2020.semeval-1.218</url>
      <bibkey>bestgen-2020-last</bibkey>
      <doi>10.18653/v1/2020.semeval-1.218</doi>
    </paper>
    <paper id="220">
      <title>Randomseed19 at SemEval-2020 Task 10 : Emphasis Selection for Written Text in Visual Media<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 10: Emphasis Selection for Written Text in Visual Media</title>
      <author><first>Aleksandr</first><last>Shatilov</last></author>
      <author><first>Denis</first><last>Gordeev</last></author>
      <author><first>Alexey</first><last>Rey</last></author>
      <pages>1685–1690</pages>
      <abstract>This paper describes our approach to emphasis selection for <a href="https://en.wikipedia.org/wiki/Writing">written text</a> in <a href="https://en.wikipedia.org/wiki/Visual_media">visual media</a> as a solution for SemEval 2020 Task 10. We used an ensemble of several different Transformer-based models and cast the task as a sequence labeling problem with two tags : ‘I’ as ‘emphasized’ and ‘O’ as ‘non-emphasized’ for each token in the text.</abstract>
      <url hash="aab27287">2020.semeval-1.220</url>
      <bibkey>shatilov-etal-2020-randomseed19</bibkey>
      <doi>10.18653/v1/2020.semeval-1.220</doi>
    </paper>
    <paper id="224">
      <title>YNU-HPCC at SemEval-2020 Task 10 : Using a Multi-granularity Ordinal Classification of the BiLSTM Model for Emphasis Selection<fixed-case>YNU</fixed-case>-<fixed-case>HPCC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 10: Using a Multi-granularity Ordinal Classification of the <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> Model for Emphasis Selection</title>
      <author><first>Dawei</first><last>Liao</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>1710–1715</pages>
      <abstract>In this study, we propose a multi-granularity ordinal classification method to address the problem of emphasis selection. In detail, the <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> is learned from Embeddings from Language Model (ELMO) to extract feature vector representation. Then, the ordinal classifica-tions are implemented on four different multi-granularities to approximate the continuous em-phasize values. Comparative experiments were conducted to compare the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> with baseline in which the <a href="https://en.wikipedia.org/wiki/Problem_solving">problem</a> is transformed to label distribution problem.</abstract>
      <url hash="daa59896">2020.semeval-1.224</url>
      <bibkey>liao-etal-2020-ynu</bibkey>
      <doi>10.18653/v1/2020.semeval-1.224</doi>
    </paper>
    <paper id="229">
      <title>JUST at SemEval-2020 Task 11 : Detecting Propaganda Techniques Using BERT Pre-trained Model<fixed-case>JUST</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 11: Detecting Propaganda Techniques Using <fixed-case>BERT</fixed-case> Pre-trained Model</title>
      <author><first>Ola</first><last>Altiti</last></author>
      <author><first>Malak</first><last>Abdullah</last></author>
      <author><first>Rasha</first><last>Obiedat</last></author>
      <pages>1749–1755</pages>
      <abstract>This paper presents the submission to semeval-2020 task 11, Detection of Propaganda Techniques in News Articles. Knowing that there are two subtasks in this competition, we have participated in the Technique Classification subtask (TC), which aims to identify the propaganda techniques used in a specific propaganda span. We have used and implemented various <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> to detect <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a>. Our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is based on BERT uncased pre-trained language model as it has achieved state-of-the-art performance on multiple NLP benchmarks. The performance results of our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> have scored 0.55307 F1-Score, which outperforms the baseline model provided by the organizers with 0.2519 F1-Score, and our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is 0.07 away from the best performing team. Compared to other participating systems, our submission is ranked 15th out of 31 participants.</abstract>
      <url hash="bdfcdc1e">2020.semeval-1.229</url>
      <bibkey>altiti-etal-2020-just</bibkey>
      <doi>10.18653/v1/2020.semeval-1.229</doi>
    </paper>
    <paper id="232">
      <title>NLFIIT at SemEval-2020 Task 11 : Neural Network Architectures for Detection of Propaganda Techniques in News Articles<fixed-case>NLFIIT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 11: Neural Network Architectures for Detection of Propaganda Techniques in News Articles</title>
      <author><first>Matej</first><last>Martinkovic</last></author>
      <author><first>Samuel</first><last>Pecar</last></author>
      <author><first>Marian</first><last>Simko</last></author>
      <pages>1771–1778</pages>
      <abstract>Since <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a> became more common technique in <a href="https://en.wikipedia.org/wiki/News">news</a>, it is very important to look for possibilities of its automatic detection. In this paper, we present neural model architecture submitted to the SemEval-2020 Task 11 competition : Detection of Propaganda Techniques in News Articles. We participated in both subtasks, propaganda span identification and propaganda technique classification. Our model utilizes recurrent Bi-LSTM layers with pre-trained word representations and also takes advantage of self-attention mechanism. Our model managed to achieve score 0.405 F1 for subtask 1 and 0.553 F1 for subtask 2 on test set resulting in 17th and 16th place in subtask 1 and subtask 2, respectively.</abstract>
      <url hash="a6ad55e6">2020.semeval-1.232</url>
      <bibkey>martinkovic-etal-2020-nlfiit</bibkey>
      <doi>10.18653/v1/2020.semeval-1.232</doi>
    </paper>
    <paper id="233">
      <title>PsuedoProp at SemEval-2020 Task 11 : Propaganda Span Detection Using BERT-CRF and Ensemble Sentence Level Classifier<fixed-case>P</fixed-case>suedo<fixed-case>P</fixed-case>rop at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 11: Propaganda Span Detection Using <fixed-case>BERT</fixed-case>-<fixed-case>CRF</fixed-case> and Ensemble Sentence Level Classifier</title>
      <author><first>Aniruddha</first><last>Chauhan</last></author>
      <author><first>Harshita</first><last>Diddee</last></author>
      <pages>1779–1785</pages>
      <abstract>This paper explains our teams’ submission to the Shared Task of Fine-Grained Propaganda Detection in which we propose a sequential BERT-CRF based Span Identification model where the fine-grained detection is carried out only on the articles that are flagged as containing propaganda by an ensemble SLC model. We propose this setup bearing in mind the practicality of this approach in identifying propaganda spans in the exponentially increasing content base where the fine-tuned analysis of the entire data repository may not be the optimal choice due to its massive computational resource requirements. We present our analysis on different <a href="https://en.wikipedia.org/wiki/Electoral_system">voting ensembles</a> for the SLC model. Our <a href="https://en.wikipedia.org/wiki/System">system</a> ranks 14th on the test set and 22nd on the development set and with an F1 score of 0.41 and 0.39 respectively.</abstract>
      <url hash="f8778a6e">2020.semeval-1.233</url>
      <bibkey>chauhan-diddee-2020-psuedoprop</bibkey>
      <doi>10.18653/v1/2020.semeval-1.233</doi>
    </paper>
    <paper id="234">
      <title>SkoltechNLP at SemEval-2020 Task 11 : Exploring Unsupervised Text Augmentation for Propaganda Detection<fixed-case>S</fixed-case>koltech<fixed-case>NLP</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 11: Exploring Unsupervised Text Augmentation for Propaganda Detection</title>
      <author><first>Daryna</first><last>Dementieva</last></author>
      <author><first>Igor</first><last>Markov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>1786–1792</pages>
      <abstract>This paper presents a solution for the Span Identification (SI) task in the Detection of Propaganda Techniques in News Articles competition at SemEval-2020. The goal of the SI task is to identify specific fragments of each article which contain the use of at least one propaganda technique. This is a binary sequence tagging task. We tested several approaches finally selecting a fine-tuned BERT model as our <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline model</a>. Our main contribution is an investigation of several unsupervised data augmentation techniques based on distributional semantics expanding the original small training dataset as applied to this BERT-based sequence tagger. We explore various expansion strategies and show that they can substantially shift the balance between <a href="https://en.wikipedia.org/wiki/Precision_(computer_science)">precision</a> and <a href="https://en.wikipedia.org/wiki/Recall_(memory)">recall</a>, while maintaining comparable levels of the F1 score.</abstract>
      <url hash="852acda9">2020.semeval-1.234</url>
      <bibkey>dementieva-etal-2020-skoltechnlp</bibkey>
      <doi>10.18653/v1/2020.semeval-1.234</doi>
    </paper>
    <paper id="237">
      <title>syrapropa at SemEval-2020 Task 11 : BERT-based Models Design for Propagandistic Technique and Span Detection<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 11: <fixed-case>BERT</fixed-case>-based Models Design for Propagandistic Technique and Span Detection</title>
      <author><first>Jinfen</first><last>Li</last></author>
      <author><first>Lu</first><last>Xiao</last></author>
      <pages>1808–1816</pages>
      <abstract>This paper describes the BERT-based models proposed for two subtasks in SemEval-2020 Task 11 : Detection of Propaganda Techniques in News Articles. We first build the <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> for Span Identification (SI) based on SpanBERT, and facilitate the detection by a deeper <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> and a sentence-level representation. We then develop a hybrid model for the Technique Classification (TC). The hybrid model is composed of three submodels including two BERT models with different training methods, and a feature-based Logistic Regression model. We endeavor to deal with imbalanced dataset by adjusting <a href="https://en.wikipedia.org/wiki/Cost_function">cost function</a>. We are in the seventh place in SI subtask (0.4711 of F1-measure), and in the third place in TC subtask (0.6783 of F1-measure) on the development set.</abstract>
      <url hash="4cbddd57">2020.semeval-1.237</url>
      <bibkey>li-xiao-2020-syrapropa</bibkey>
      <doi>10.18653/v1/2020.semeval-1.237</doi>
    </paper>
    <paper id="238">
      <title>Team DiSaster at SemEval-2020 Task 11 : Combining BERT and Hand-crafted Features for Identifying Propaganda Techniques in News<fixed-case>D</fixed-case>i<fixed-case>S</fixed-case>aster at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 11: Combining <fixed-case>BERT</fixed-case> and Hand-crafted Features for Identifying Propaganda Techniques in News</title>
      <author><first>Anders</first><last>Kaas</last></author>
      <author><first>Viktor Torp</first><last>Thomsen</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>1817–1822</pages>
      <abstract>The identification of communication techniques in <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> such as <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a> is important, as such techniques can influence the opinions of large numbers of people. Most work so far focused on the <a href="https://en.wikipedia.org/wiki/Identification_(biology)">identification</a> at the news article level. Recently, a new <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> and shared task has been proposed for the identification of propaganda techniques at the finer-grained span level. This paper describes our system submission to the subtask of technique classification (TC) for the SemEval 2020 shared task on detection of propaganda techniques in news articles. We propose a method of combining neural BERT representations with hand-crafted features via stacked generalization. Our model has the added advantage that it combines the power of contextual representations from BERT with simple span-based and article-based global features. We present an ablation study which shows that even though BERT representations are very powerful also for this task, BERT still benefits from being combined with carefully designed task-specific features.</abstract>
      <url hash="753aa8ba">2020.semeval-1.238</url>
      <bibkey>kaas-etal-2020-team</bibkey>
      <doi>10.18653/v1/2020.semeval-1.238</doi>
    </paper>
    <paper id="240">
      <title>TTUI at SemEval-2020 Task 11 : Propaganda Detection with Transfer Learning and Ensembles<fixed-case>TTUI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 11: Propaganda Detection with Transfer Learning and Ensembles</title>
      <author><first>Moonsung</first><last>Kim</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <pages>1829–1834</pages>
      <abstract>In this paper, we describe our approaches and <a href="https://en.wikipedia.org/wiki/System">systems</a> for the SemEval-2020 Task 11 on propaganda technique detection. We fine-tuned BERT and RoBERTa pre-trained models then merged them with an average ensemble. We conducted several experiments for input representations dealing with long texts and preserving context as well as for the imbalanced class problem. Our system ranked 20th out of 36 teams with 0.398 F1 in the SI task and 14th out of 31 teams with 0.556 F1 in the TC task.</abstract>
      <url hash="78cb2802">2020.semeval-1.240</url>
      <bibkey>kim-bethard-2020-ttui</bibkey>
      <doi>10.18653/v1/2020.semeval-1.240</doi>
    </paper>
    <paper id="241">
      <title>UAIC1860 at SemEval-2020 Task 11 : Detection of Propaganda Techniques in News Articles<fixed-case>UAIC</fixed-case>1860 at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 11: Detection of Propaganda Techniques in News Articles</title>
      <author><first>Vlad</first><last>Ermurachi</last></author>
      <author><first>Daniela</first><last>Gifu</last></author>
      <pages>1835–1840</pages>
      <abstract>The Detection of Propaganda Techniques in News Articles task at the SemEval 2020 competition focuses on detecting and classifying <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a>, pervasive in <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news article</a>. In this paper, we present a system able to evaluate on sentence level, three traditional text representation techniques for these study goals, using : tf*idf, word and character n-grams. Firstly, we built a binary classifier able to provide corresponding propaganda labels, propaganda or non-propaganda. Secondly, we build a multilabel multiclass model to identify applied <a href="https://en.wikipedia.org/wiki/Propaganda">propaganda</a>.</abstract>
      <url hash="38229a7c">2020.semeval-1.241</url>
      <bibkey>ermurachi-gifu-2020-uaic1860</bibkey>
      <doi>10.18653/v1/2020.semeval-1.241</doi>
    </paper>
    <paper id="242">
      <title>UMSIForeseer at SemEval-2020 Task 11 : Propaganda Detection by Fine-Tuning BERT with Resampling and Ensemble Learning<fixed-case>UMSIF</fixed-case>oreseer at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 11: Propaganda Detection by Fine-Tuning <fixed-case>BERT</fixed-case> with Resampling and Ensemble Learning</title>
      <author><first>Yunzhe</first><last>Jiang</last></author>
      <author><first>Cristina</first><last>Garbacea</last></author>
      <author><first>Qiaozhu</first><last>Mei</last></author>
      <pages>1841–1846</pages>
      <abstract>We describe our participation at the SemEval 2020 Detection of Propaganda Techniques in News Articles-Techniques Classification (TC) task, designed to categorize textual fragments into one of the 14 given propaganda techniques. Our <a href="https://en.wikipedia.org/wiki/Solution">solution</a> leverages pre-trained BERT models. We present our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> implementations, evaluation results and analysis of these results. We also investigate the potential of combining <a href="https://en.wikipedia.org/wiki/Language_model">language models</a> with <a href="https://en.wikipedia.org/wiki/Sample-rate_conversion">resampling</a> and ensemble learning methods to deal with data imbalance and improve performance.</abstract>
      <url hash="95d18f74">2020.semeval-1.242</url>
      <bibkey>jiang-etal-2020-umsiforeseer</bibkey>
      <doi>10.18653/v1/2020.semeval-1.242</doi>
    </paper>
    <paper id="243">
      <title>UNTLing at SemEval-2020 Task 11 : Detection of Propaganda Techniques in English News Articles<fixed-case>UNTL</fixed-case>ing at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 11: Detection of Propaganda Techniques in <fixed-case>E</fixed-case>nglish News Articles</title>
      <author><first>Maia</first><last>Petee</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <pages>1847–1852</pages>
      <abstract>Our system for the PropEval task explores the ability of semantic features to detect and label propagandistic rhetorical techniques in English news articles. For Subtask 2, labeling identified propagandistic fragments with one of fourteen technique labels, our system attains a micro-averaged F1 of 0.40 ; in this paper, we take a detailed look at the fourteen labels and how well our semantically-focused model detects each of them. We also propose strategies to fill the gaps.</abstract>
      <url hash="6bdf11e1">2020.semeval-1.243</url>
      <bibkey>petee-palmer-2020-untling</bibkey>
      <doi>10.18653/v1/2020.semeval-1.243</doi>
    </paper>
    <paper id="250">
      <title>Amsqr at SemEval-2020 Task 12 : Offensive Language Detection Using <a href="https://en.wikipedia.org/wiki/Neural_network">Neural Networks</a> and Anti-adversarial Features<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Offensive Language Detection Using Neural Networks and Anti-adversarial Features</title>
      <author><first>Alejandro</first><last>Mosquera</last></author>
      <pages>1898–1905</pages>
      <abstract>This paper describes a method and <a href="https://en.wikipedia.org/wiki/System">system</a> to solve the problem of detecting offensive language in <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> using anti-adversarial features. Our submission to the SemEval-2020 task 12 challenge was generated by an stacked ensemble of neural networks fine-tuned on the OLID dataset and additional external sources. For Task-A (English), text normalisation filters were applied at both graphical and lexical level. The normalisation step effectively mitigates not only the natural presence of <a href="https://en.wikipedia.org/wiki/Variation_(linguistics)">lexical variants</a> but also intentional attempts to bypass moderation by introducing out of vocabulary words. Our approach provides strong F1 scores for both 2020 (0.9134) and 2019 (0.8258) challenges.</abstract>
      <url hash="3950af80">2020.semeval-1.250</url>
      <bibkey>mosquera-2020-amsqr</bibkey>
      <doi>10.18653/v1/2020.semeval-1.250</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="258">
      <title>Hitachi at SemEval-2020 Task 12 : Offensive Language Identification with Noisy Labels Using Statistical Sampling and Post-Processing<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Offensive Language Identification with Noisy Labels Using Statistical Sampling and Post-Processing</title>
      <author><first>Manikandan</first><last>Ravikiran</last></author>
      <author><first>Amin Ekant</first><last>Muljibhai</last></author>
      <author><first>Toshinori</first><last>Miyoshi</last></author>
      <author><first>Hiroaki</first><last>Ozaki</last></author>
      <author><first>Yuta</first><last>Koreeda</last></author>
      <author><first>Sakata</first><last>Masayuki</last></author>
      <pages>1961–1967</pages>
      <abstract>In this paper, we present our participation in SemEval-2020 Task-12 Subtask-A (English Language) which focuses on offensive language identification from noisy labels. To this end, we developed a hybrid system with the BERT classifier trained with tweets selected using Statistical Sampling Algorithm (SA) and Post-Processed (PP) using an offensive wordlist. Our developed system achieved 34th position with Macro-averaged F1-score (Macro-F1) of 0.90913 over both offensive and non-offensive classes. We further show comprehensive results and <a href="https://en.wikipedia.org/wiki/Error_analysis_(linguistics)">error analysis</a> to assist future research in offensive language identification with noisy labels.</abstract>
      <url hash="0a6cca3d">2020.semeval-1.258</url>
      <bibkey>ravikiran-etal-2020-hitachi</bibkey>
      <doi>10.18653/v1/2020.semeval-1.258</doi>
    </paper>
    <paper id="263">
      <title>IR3218-UI at SemEval-2020 Task 12 : Emoji Effects on Offensive Language IdentifiCation<fixed-case>IR</fixed-case>3218-<fixed-case>UI</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Emoji Effects on Offensive Language <fixed-case>I</fixed-case>dentifi<fixed-case>C</fixed-case>ation</title>
      <author><first>Sandy</first><last>Kurniawan</last></author>
      <author><first>Indra</first><last>Budi</last></author>
      <author><first>Muhammad Okky</first><last>Ibrohim</last></author>
      <pages>1998–2005</pages>
      <abstract>In this paper, we present our approach and the results of our participation in OffensEval 2020. There are three sub-tasks in OffensEval 2020 namely offensive language identification (sub-task A), automatic categorization of offense types (sub-task B), and offense target identification (sub-task C). We participated in sub-task A of English OffensEval 2020. Our approach emphasizes on how the <a href="https://en.wikipedia.org/wiki/Emoji">emoji</a> affects offensive language identification. Our model used LSTM combined with GloVe pre-trained word vectors to identify offensive language on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. The best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> obtained macro F1-score of 0.88428.</abstract>
      <url hash="faa9f188">2020.semeval-1.263</url>
      <bibkey>kurniawan-etal-2020-ir3218</bibkey>
      <doi>10.18653/v1/2020.semeval-1.263</doi>
    </paper>
    <paper id="266">
      <title>JCT at SemEval-2020 Task 12 : Offensive Language Detection in Tweets Using Preprocessing Methods, Character and Word N-grams<fixed-case>JCT</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Offensive Language Detection in Tweets Using Preprocessing Methods, Character and Word N-grams</title>
      <author><first>Moshe</first><last>Uzan</last></author>
      <author><first>Yaakov</first><last>HaCohen-Kerner</last></author>
      <pages>2017–2022</pages>
      <abstract>In this paper, we describe our submissions to SemEval-2020 contest. We tackled subtask 12-Multilingual Offensive Language Identification in <a href="https://en.wikipedia.org/wiki/Social_media">Social Media</a>. We developed different models for four languages : <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>, <a href="https://en.wikipedia.org/wiki/Greek_language">Greek</a>, and <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a>. We applied three supervised machine learning methods using various combinations of character and word n-gram features. In addition, we applied various combinations of basic preprocessing methods. Our best submission was a model we built for offensive language identification in <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a> using <a href="https://en.wikipedia.org/wiki/Random_Forest">Random Forest</a>. This <a href="https://en.wikipedia.org/wiki/Model_(person)">model</a> was ranked at the 6 position out of 39 submissions. Our result is lower by only 0.0025 than the result of the team that won the 4 place using entirely non-neural methods. Our experiments indicate that char ngram features are more helpful than word ngram features. This phenomenon probably occurs because <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a> are more characterized by <a href="https://en.wikipedia.org/wiki/Character_(computing)">characters</a> than by words, <a href="https://en.wikipedia.org/wiki/Twitter">tweets</a> are short, and contain various special sequences of characters, e.g., <a href="https://en.wikipedia.org/wiki/Hashtag">hashtags</a>, <a href="https://en.wikipedia.org/wiki/Shortcut_(computing)">shortcuts</a>, slang words, and typos.</abstract>
      <url hash="7c457f64">2020.semeval-1.266</url>
      <bibkey>uzan-hacohen-kerner-2020-jct</bibkey>
      <doi>10.18653/v1/2020.semeval-1.266</doi>
    </paper>
    <paper id="273">
      <title>Lee at SemEval-2020 Task 12 : A BERT Model Based on the Maximum Self-ensemble Strategy for Identifying Offensive Language<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: A <fixed-case>BERT</fixed-case> Model Based on the Maximum Self-ensemble Strategy for Identifying Offensive Language</title>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Xiaobing</first><last>Zhou</last></author>
      <author><first>Zichen</first><last>Zhang</last></author>
      <pages>2067–2072</pages>
      <abstract>This article describes the <a href="https://en.wikipedia.org/wiki/System">system</a> submitted to SemEval 2020 Task 12 : OffensEval 2020. This task aims to identify and classify offensive languages in different languages on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. We only participate in the English part of subtask A, which aims to identify offensive languages in English. To solve this task, we propose a BERT model system based on the transform mechanism, and use the maximum self-ensemble to improve <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> performance. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieved a macro F1 score of 0.913(ranked 13/82) in subtask A.</abstract>
      <url hash="30f9cbdf">2020.semeval-1.273</url>
      <bibkey>li-etal-2020-lee-semeval</bibkey>
      <doi>10.18653/v1/2020.semeval-1.273</doi>
    </paper>
    <paper id="274">
      <title>LIIR at SemEval-2020 Task 12 : A Cross-Lingual Augmentation Approach for Multilingual Offensive Language Identification<fixed-case>LIIR</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: A Cross-Lingual Augmentation Approach for Multilingual Offensive Language Identification</title>
      <author><first>Erfan</first><last>Ghadery</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>2073–2079</pages>
      <abstract>This paper presents our system entitled ‘LIIR’ for SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval 2). We have participated in sub-task A for <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>, <a href="https://en.wikipedia.org/wiki/Greek_language">Greek</a>, <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, and <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish languages</a>. We adapt and fine-tune the <a href="https://en.wikipedia.org/wiki/BERT">BERT</a> and Multilingual Bert models made available by Google AI for English and non-English languages respectively. For the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>, we use a combination of two fine-tuned BERT models. For other languages we propose a cross-lingual augmentation approach in order to enrich training data and we use Multilingual BERT to obtain sentence representations.</abstract>
      <url hash="2541f3e5">2020.semeval-1.274</url>
      <bibkey>ghadery-moens-2020-liir</bibkey>
      <doi>10.18653/v1/2020.semeval-1.274</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="283">
      <title>SalamNET at SemEval-2020 Task 12 : Deep Learning Approach for Arabic Offensive Language Detection<fixed-case>S</fixed-case>alam<fixed-case>NET</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Deep Learning Approach for <fixed-case>A</fixed-case>rabic Offensive Language Detection</title>
      <author><first>Fatemah</first><last>Husain</last></author>
      <author><first>Jooyeon</first><last>Lee</last></author>
      <author><first>Sam</first><last>Henry</last></author>
      <author><first>Ozlem</first><last>Uzuner</last></author>
      <pages>2133–2139</pages>
      <abstract>This paper describes SalamNET, an Arabic offensive language detection system that has been submitted to SemEval 2020 shared task 12 : Multilingual Offensive Language Identification in Social Media. Our approach focuses on applying multiple deep learning models and conducting in depth error analysis of results to provide system implications for future development considerations. To pursue our goal, a Recurrent Neural Network (RNN), a Gated Recurrent Unit (GRU), and Long-Short Term Memory (LSTM) models with different design architectures have been developed and evaluated. The SalamNET, a Bi-directional Gated Recurrent Unit (Bi-GRU) based model, reports a macro-F1 score of 0.83 %</abstract>
      <url hash="af3d59f6">2020.semeval-1.283</url>
      <bibkey>husain-etal-2020-salamnet</bibkey>
      <doi>10.18653/v1/2020.semeval-1.283</doi>
    </paper>
    <paper id="285">
      <title>Sonal.kumari at SemEval-2020 Task 12 : Social Media Multilingual Offensive Text Identification and Categorization Using Neural Network Models<fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Social Media Multilingual Offensive Text Identification and Categorization Using Neural Network Models</title>
      <author><first>Sonal</first><last>Kumari</last></author>
      <pages>2146–2154</pages>
      <abstract>In this paper, we present our approaches and results for SemEval-2020 Task 12, Multilingual Offensive Language Identification in Social Media (OffensEval 2020). The OffensEval 2020 had three subtasks : A) Identifying the tweets to be offensive (OFF) or non-offensive (NOT) for Arabic, Danish, <a href="https://en.wikipedia.org/wiki/English_language">English</a>, Greek, and Turkish languages, B) Detecting if the offensive tweet is targeted (TIN) or untargeted (UNT) for the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>, and C) Categorizing the offensive targeted tweets into three classes, namely : individual (IND), Group (GRP), or Other (OTH) for the <a href="https://en.wikipedia.org/wiki/English_language">English language</a>. We participate in all the subtasks A, B, and C. In our solution, first we use the pre-trained BERT model for all <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">subtasks</a>, A, B, and C and then we apply the BiLSTM model with attention mechanism (Attn-BiLSTM) for the same. Our result demonstrates that the pre-trained model is not giving good results for all types of languages and is compute and memory intensive whereas the Attn-BiLSTM model is fast and gives good accuracy with fewer resources. The Attn-BiLSTM model is giving better accuracy for <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> and <a href="https://en.wikipedia.org/wiki/Greek_language">Greek</a> where the pre-trained model is not able to capture the complete context of these languages due to lower vocab-size.</abstract>
      <url hash="c5930894">2020.semeval-1.285</url>
      <bibkey>kumari-2020-sonal</bibkey>
      <doi>10.18653/v1/2020.semeval-1.285</doi>
    </paper>
    <paper id="287">
      <title>SSN_NLP_MLRG at SemEval-2020 Task 12 : Offensive Language Identification in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>, <a href="https://en.wikipedia.org/wiki/Greek_language">Greek</a> Using BERT and Machine Learning Approach<fixed-case>SSN</fixed-case>_<fixed-case>NLP</fixed-case>_<fixed-case>MLRG</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Offensive Language Identification in <fixed-case>E</fixed-case>nglish, <fixed-case>D</fixed-case>anish, <fixed-case>G</fixed-case>reek Using <fixed-case>BERT</fixed-case> and Machine Learning Approach</title>
      <author><first>A</first><last>Kalaivani</last></author>
      <author><first>Thenmozhi</first><last>D.</last></author>
      <pages>2161–2170</pages>
      <abstract>Offensive language identification is to detect the hurtful tweets, <a href="https://en.wikipedia.org/wiki/Pejorative">derogatory comments</a>, swear words on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a>. As an emerging growth of social media communication, offensive language detection has received more attention in the last years ; we focus to perform the task on <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a> and <a href="https://en.wikipedia.org/wiki/Greek_language">Greek</a>. We have investigated which can be effect more on pre-trained models BERT (Bidirectional Encoder Representation from Transformer) and Machine Learning Approaches. Our investigation shows the difference performance between the three languages and to identify the best performance is evaluated by the <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification algorithms</a>. In the shared task SemEval-2020, our team SSN_NLP_MLRG submitted for three languages that are Subtasks A, B, C in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, Subtask A in <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a> and Subtask A in <a href="https://en.wikipedia.org/wiki/Greek_language">Greek</a>. Our team SSN_NLP_MLRG obtained the <a href="https://en.wikipedia.org/wiki/F1_score">F1 Scores</a> as 0.90, 0.61, 0.52 for the Subtasks A, B, C in <a href="https://en.wikipedia.org/wiki/English_language">English</a>, 0.56 for the Subtask A in <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a> and 0.67 for the Subtask A in Greek respectively.</abstract>
      <url hash="d7f35faf">2020.semeval-1.287</url>
      <bibkey>kalaivani-d-2020-ssn</bibkey>
      <doi>10.18653/v1/2020.semeval-1.287</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="289">
      <title>TAC at SemEval-2020 Task 12 : Ensembling Approach for Multilingual Offensive Language Identification in Social Media<fixed-case>TAC</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Ensembling Approach for Multilingual Offensive Language Identification in Social Media</title>
      <author><first>Talha</first><last>Anwar</last></author>
      <author><first>Omer</first><last>Baig</last></author>
      <pages>2177–2182</pages>
      <abstract>Usage of <a href="https://en.wikipedia.org/wiki/Profanity">offensive language</a> on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> is getting more common these days, and there is a need of a mechanism to detect it and control it. This paper deals with offensive language detection in five different languages ; <a href="https://en.wikipedia.org/wiki/English_language">English</a>, <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>, <a href="https://en.wikipedia.org/wiki/Greek_language">Greek</a> and <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a>. We presented an almost similar <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble pipeline</a> comprised of <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning and deep learning models</a> for all five languages. Three <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> and four <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning models</a> were used in the <a href="https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)">ensemble</a>. In the OffensEval-2020 competition our model achieved <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> of 0.85, 0.74, 0.68, 0.81, and 0.9 for <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, <a href="https://en.wikipedia.org/wiki/Turkish_language">Turkish</a>, <a href="https://en.wikipedia.org/wiki/Danish_language">Danish</a>, Greek and English language tasks respectively.</abstract>
      <url hash="9b3b6acd">2020.semeval-1.289</url>
      <bibkey>anwar-baig-2020-tac</bibkey>
      <doi>10.18653/v1/2020.semeval-1.289</doi>
      <pwccode url="https://github.com/talhaanwarch/offenseeval2020" additional="false">talhaanwarch/offenseeval2020</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="295">
      <title>UoB at SemEval-2020 Task 12 : Boosting BERT with Corpus Level Information<fixed-case>U</fixed-case>o<fixed-case>B</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>E</fixed-case>val-2020 Task 12: Boosting <fixed-case>BERT</fixed-case> with Corpus Level Information</title>
      <author><first>Wah Meng</first><last>Lim</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <pages>2216–2221</pages>
      <abstract>Pre-trained language model word representation, such as <a href="https://en.wikipedia.org/wiki/BERT">BERT</a>, have been extremely successful in several Natural Language Processing tasks significantly improving on the state-of-the-art. This can largely be attributed to their ability to better capture <a href="https://en.wikipedia.org/wiki/Semantics">semantic information</a> contained within a sentence. Several tasks, however, can benefit from information available at a corpus level, such as Term Frequency-Inverse Document Frequency (TF-IDF). In this work we test the effectiveness of integrating this <a href="https://en.wikipedia.org/wiki/Information">information</a> with BERT on the task of identifying abuse on <a href="https://en.wikipedia.org/wiki/Social_media">social media</a> and show that integrating this <a href="https://en.wikipedia.org/wiki/Information">information</a> with BERT does indeed significantly improve performance. We participate in Sub-Task A (abuse detection) wherein we achieve a score within two points of the top performing team and in Sub-Task B (target detection) wherein we are ranked 4 of the 44 participating teams.</abstract>
      <url hash="709f5882">2020.semeval-1.295</url>
      <bibkey>lim-tayyar-madabushi-2020-uob</bibkey>
      <doi>10.18653/v1/2020.semeval-1.295</doi>
    </paper>
    </volume>
</collection>