<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.iwdp">
  <volume id="1" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the Second International Workshop of Discourse Processing</booktitle>
      <editor><first>Qun</first><last>Liu</last></editor>
      <editor><first>Deyi</first><last>Xiong</last></editor>
      <editor><first>Shili</first><last>Ge</last></editor>
      <editor><first>Xiaojun</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="64d9f206">2020.iwdp-1.0</url>
      <bibkey>iwdp-2020-international</bibkey>
    </frontmatter>
    <paper id="5">
      <title>Context-Aware Word Segmentation for Chinese Real-World Discourse<fixed-case>C</fixed-case>hinese Real-World Discourse</title>
      <author><first>Kaiyu</first><last>Huang</last></author>
      <author><first>Junpeng</first><last>Liu</last></author>
      <author><first>Jingxiang</first><last>Cao</last></author>
      <author><first>Degen</first><last>Huang</last></author>
      <pages>22–28</pages>
      <abstract>Previous neural approaches achieve significant progress for Chinese word segmentation (CWS) as a sentence-level task, but it suffers from limitations on real-world scenario. In this paper, we address this issue with a context-aware method and optimize the <a href="https://en.wikipedia.org/wiki/Solution">solution</a> at document-level. This paper proposes a three-step strategy to improve the performance for discourse CWS. First, the method utilizes an auxiliary segmenter to remedy the limitation on pre-segmenter. Then the context-aware algorithm computes the <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence</a> of each split. The maximum probability path is reconstructed via this <a href="https://en.wikipedia.org/wiki/Algorithm">algorithm</a>. Besides, in order to evaluate the performance in <a href="https://en.wikipedia.org/wiki/Discourse">discourse</a>, we build a new <a href="https://en.wikipedia.org/wiki/Benchmarking">benchmark</a> consisting of the latest news and Chinese medical articles. Extensive experiments on this <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark</a> show that our proposed method achieves a competitive performance on a document-level real-world scenario for CWS.</abstract>
      <url hash="061c4146">2020.iwdp-1.5</url>
      <attachment type="Dataset" hash="ba4de41c">2020.iwdp-1.5.Dataset.rar</attachment>
      <bibkey>huang-etal-2020-context</bibkey>
    </paper>
    <paper id="6">
      <title>Neural Abstractive Multi-Document Summarization : Hierarchical or Flat Structure?</title>
      <author><first>Ye</first><last>Ma</last></author>
      <author><first>Lu</first><last>Zong</last></author>
      <pages>29–37</pages>
      <abstract>With regards to WikiSum (CITATION) that empowers applicative explorations of Neural Multi-Document Summarization (MDS) to learn from large scale dataset, this study develops two hierarchical Transformers (HT) that describe both the cross-token and cross-document dependencies, at the same time allow extended length of input documents. By incorporating word- and paragraph-level multi-head attentions in the decoder based on the parallel and vertical architectures, the proposed parallel and vertical hierarchical Transformers (PHT &amp; VHT) generate summaries utilizing context-aware word embeddings together with static and dynamics paragraph embeddings, respectively. A comprehensive evaluation is conducted on WikiSum to compare PHT &amp; VHT with established models and to answer the question whether hierarchical structures offer more promising performances than flat structures in the MDS task. The results suggest that our hierarchical models generate summaries of higher quality by better capturing cross-document relationships, and save more memory spaces in comparison to flat-structure models. Moreover, we recommend PHT given its practical value of higher <a href="https://en.wikipedia.org/wiki/Time_complexity">inference speed</a> and greater memory-saving capacity.</abstract>
      <url hash="0b7d8d4d">2020.iwdp-1.6</url>
      <bibkey>ma-zong-2020-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisum">WikiSum</pwcdataset>
    <title_ar>تلخيص متعدد المستندات تجريدي عصبي: هيكل هرمي أم مسطح؟</title_ar>
      <title_pt>Resumo Neural Abstrativo Multi-Documento: Estrutura Hierárquica ou Plana?</title_pt>
      <title_es>Resumen abstractivo de varios documentos neuronales: ¿estructura jerárquica o plana?</title_es>
      <title_fr>Synthèse multi-documents abstraite neuronale : structure hiérarchique ou plate ?</title_fr>
      <title_ja>ニューラル抽象的マルチドキュメントの要約：階層構造または平坦構造？</title_ja>
      <title_zh>神经抽象多文档摘要:层构与平面结构?</title_zh>
      <title_hi>तंत्रिका Abstractive बहु दस्तावेज़ Summarization: पदानुक्रमित या फ्लैट संरचना?</title_hi>
      <title_ru>Нейронное абстрактное обобщение нескольких документов: иерархическая или плоская структура?</title_ru>
      <title_ga>Achoimre Ildhoiciméad Neural Abstract: Ordlathach nó Struchtúr Comhréidh?</title_ga>
      <title_ka>ნეირალური აბსტრაქტიგური მრავალური დოკუმენტის კომპანიზაცია: ჰიერარიქური ან ფლანტიური სტრუქტურა?</title_ka>
      <title_el>Νευρική αφηρημένη Σύνοψη πολλών εγγράφων: Ιεραρχική ή Επίπεδη Δομή;</title_el>
      <title_hu>Neurális absztraktív többdokumentumos összefoglalás: hierarchikus vagy lapos struktúra?</title_hu>
      <title_it>Sintesi astratta neurale multi-documento: struttura gerarchica o piatta?</title_it>
      <title_kk>Нейралық абстрактивті көптеген құжаттардың тұжырымдамасы: хиерархикалық не кәдімгі құрылымы?</title_kk>
      <title_lt>Neuralinė abstrakti daugiadokumentų santrauka: Hierarchinė ar plokšti struktūra?</title_lt>
      <title_mk>Неурална апстрактивна мултидокументарна резервација: хиерархиска или рамна структура?</title_mk>
      <title_ms>Penapisan Multi-Dokumen Absraktif Neural: Struktur Hierarkik atau Flat?</title_ms>
      <title_ml>Neural Abstractive Multi-Document Summarization: Hierarchical or Flat Structure?</title_ml>
      <title_mn>Сэтгэл санааны абстрактив олон бичгийн нийлүүлэлт: Хиерархик эсвэл төвөгтэй бүтэц?</title_mn>
      <title_no>Neuralabstraktiv sammendrag for fleire dokument: hierarkisk eller flat struktur?</title_no>
      <title_mt>Sommarju Newrali Abstrattiv Multi-Dokument: Struttura Erarkika jew Ċatta?</title_mt>
      <title_ro>Rezumat abstractiv neural multi-document: Structură ierarhică sau plată?</title_ro>
      <title_pl>Abstrakcyjne podsumowanie wielu dokumentów neuronowych: hierarchiczna lub płaska struktura?</title_pl>
      <title_sr>Neuralna abstraktivna sažetka višestrukog dokumenta: hijerarhička ili plava struktura?</title_sr>
      <title_si>නිර්මාණික විශේෂ විශේෂ විශේෂ විශේෂය: හියාර්චිකල් නැත්තම් සංස්ථාපනය?</title_si>
      <title_sv>Neural Abstraktiv Sammanfattning av flera dokument: hierarkisk eller platt struktur?</title_sv>
      <title_so>Hierarchical or Flat structure?</title_so>
      <title_ta>Neural Abstractive Multi-Document Summarization: Hierarchical or Flat Structure?</title_ta>
      <title_ur>Neural Abstractive Multi-Document Summarization: Hierarchical or Flat Structure?</title_ur>
      <title_uz>Neural Abstractive Multi-Document Summarization: Hierarchical or Flat Structure?</title_uz>
      <title_vi>Về tổng kết đa tài liệu thần kinh:</title_vi>
      <title_hr>Neuralna abstraktivna skupljanja višestrukih dokumenta: hijerarhička ili plava struktura?</title_hr>
      <title_nl>Neuronale abstracte multi-document samenvatting: hiërarchische of platte structuur?</title_nl>
      <title_bg>Нервно абстрактивно многодокументално обобщение: йерархична или плоска структура?</title_bg>
      <title_da>Neural Abstraktiv Multi-Document Summarisation: Hierarkisk eller flad struktur?</title_da>
      <title_de>Neuronale abstraktive Zusammenfassung mehrerer Dokumente: Hierarchische oder flache Struktur?</title_de>
      <title_fa>تعداد مجموعه‌ای از سند‌های عصبی: ساختار معمولی یا ساختار معمولی؟</title_fa>
      <title_ko>신경 추상 다중 문서 요약: 차원 구조인가 편평 구조인가?</title_ko>
      <title_id>Penapisan Multi-Dokumen Abstraktif Neural: Struktur Hierarkis atau Flat?</title_id>
      <title_sw>Ujumbe wa nyaraka nyingi za Abstractive Neural: Miundomo ya Nyaraka au Nyaraka?</title_sw>
      <title_tr>Näsaz Abstraktiw Çot-Sened Toplaýyşy: Hierarhiýal ýa Taýik Structure?</title_tr>
      <title_af>Neural Abstractive Multi- Document Opsomming: Hierarchical of Flat Structure?</title_af>
      <title_am>የኩነቶች Abstractive Multi-Document ማጠቃለያ: Hierarchical or Flat Structure?</title_am>
      <title_az>Nöral Abstraktiv Çox-Dəstə Toplaşdırma: Hiyerarşik ya da Flat Structure?</title_az>
      <title_sq>Përshkrimi i Multi-Dokumenteve Abstraktive Neural: Struktura Hierarkike apo e Flat ë?</title_sq>
      <title_bs>Neuralna abstraktivna sažetka višestrukih dokumenta: hijerarhička ili plava struktura?</title_bs>
      <title_cs>Neurální abstrakční multi-dokumentové shrnutí: Hierarchická nebo plochá struktura?</title_cs>
      <title_ca>Resumen Neural Abstractive Multi-Document: Hierarchical or Flat Structure?</title_ca>
      <title_hy>Նյարդային բազմաթիվ փաստաթղթերի համառոտագրություն' հիերարքիկական կամ հարկի կառուցվածք:</title_hy>
      <title_et>Neuraalne abstraktne mitme dokumendi kokkuvõte: hierarhiline või tasane struktuur?</title_et>
      <title_bn>নিউরেল আবত্ত্রিক্যাটিভ মাল্টার-ডকুমেন্ট সামার্জার: হায়েরার্কিয়াল বা ফ্ল্যাট কাঠামোর?</title_bn>
      <title_fi>Neural Abstractive Multi-Document Summarization: Hierarkkinen vai tasainen rakenne?</title_fi>
      <title_jv>Samurasi Njuaral absolute Multi-document Cumaring:</title_jv>
      <title_he>סדרת מסמכים רבים נוירואלית אסטרקטיבית: מבנה הייררכי או שטוח?</title_he>
      <title_ha>KCharselect unicode block name</title_ha>
      <title_sk>Povzetek večdokumentov: hierarhična ali ravna struktura?</title_sk>
      <title_bo>དཔེ་དབྱིབས་བསྒྱུར་བའི་སྣ་མང་ཡིག་གི་བཅུད་སྡུད་ཚོང：Hierarchical or Flat Structure?</title_bo>
      <abstract_ar>فيما يتعلق بـ WikiSum (CITATION) الذي يمكّن الاستكشافات التطبيقية للتلخيص العصبي متعدد المستندات (MDS) للتعلم من مجموعة البيانات واسعة النطاق ، طورت هذه الدراسة محولين هرميين (HT) يصفان تبعيات الرموز المتقاطعة والوثيقة المتقاطعة ، على تسمح في نفس الوقت بإطالة طول مستندات الإدخال. من خلال دمج الانتباه متعدد الرؤوس على مستوى الكلمات والفقرة في وحدة فك التشفير استنادًا إلى البنى المتوازية والعمودية ، تولد المحولات الهرمية المتوازية والرأسية المقترحة (PHT &amp; VHT) ملخصات باستخدام عمليات دمج الكلمات الواعية بالسياق جنبًا إلى جنب مع حفلات الزفاف الثابتة والديناميكية ، على التوالى. يتم إجراء تقييم شامل على WikiSum لمقارنة PHT و VHT بالنماذج المعمول بها وللإجابة على سؤال ما إذا كانت الهياكل الهرمية تقدم أداءً واعدًا أكثر من الهياكل المسطحة في مهمة MDS. تشير النتائج إلى أن نماذجنا الهرمية تولد ملخصات ذات جودة أعلى من خلال التقاط علاقات عبر المستندات بشكل أفضل ، وتوفير المزيد من مساحات الذاكرة مقارنة بنماذج الهيكل المسطح. علاوة على ذلك ، نوصي باستخدام PHT نظرًا لقيمتها العملية المتمثلة في زيادة سرعة الاستدلال وزيادة سعة توفير الذاكرة.</abstract_ar>
      <abstract_fr>En ce qui concerne WikiSum (CITATION) qui permet des explorations applicatives de Neural Multi-Document Summarization (MDS) pour apprendre à partir d'un ensemble de données à grande échelle, cette étude développe deux transformateurs hiérarchiques (HT) qui décrivent à la fois les dépendances entre jetons et entre documents, en même temps permettent longueur des documents d'entrée. En incorporant des attentions multi-têtes au niveau des mots et des paragraphes dans le décodeur sur la base des architectures parallèle et verticale, les transformateurs hiérarchiques parallèles et verticaux proposés (PHT &amp; VHT) génèrent des résumés utilisant des incorporations de mots sensibles au contexte ainsi que des paragraphes statiques et dynamiques les intégrations, respectivement. Une évaluation complète est menée sur WikiSum pour comparer les PHT et VHT avec les modèles établis et pour répondre à la question de savoir si les structures hiérarchiques offrent des performances plus prometteuses que les structures plates dans la tâche MDS. Les résultats suggèrent que nos modèles hiérarchiques génèrent des résumés de meilleure qualité en capturant mieux les relations entre documents et en économisant davantage d'espaces mémoire par rapport aux modèles à structure plate. De plus, nous recommandons PHT étant donné sa valeur pratique de vitesse d'inférence plus élevée et de capacité d'économie de mémoire supérieure.</abstract_fr>
      <abstract_es>Con respecto a WikiSum (CITATION) que permite las exploraciones aplicativas de la sumarización neuronal de documentos múltiples (MDS) para aprender de un conjunto de datos a gran escala, este estudio desarrolla dos transformadores jerárquicos (HT) que describen tanto las dependencias entre tokens como entre documentos, al mismo tiempo permiten longitud de los documentos de entrada. Al incorporar atenciones de varios cabezales a nivel de palabras y párrafos en el decodificador basadas en las arquitecturas paralela y vertical, los Transformadores jerárquicos paralelos y verticales (PHT y VHT) propuestos generan resúmenes utilizando incrustaciones de palabras sensibles al contexto junto con párrafos estáticos y dinámicos incrustaciones, respectivamente. Se lleva a cabo una evaluación exhaustiva en WikiSum para comparar PHT y VHT con modelos establecidos y responder a la pregunta de si las estructuras jerárquicas ofrecen rendimientos más prometedores que las estructuras planas en la tarea MDS. Los resultados sugieren que nuestros modelos jerárquicos generan resúmenes de mayor calidad al capturar mejor las relaciones entre documentos y ahorrar más espacio de memoria en comparación con los modelos de estructura plana. Además, recomendamos PHT dado su valor práctico de mayor velocidad de inferencia y mayor capacidad de ahorro de memoria.</abstract_es>
      <abstract_pt>Com relação ao WikiSum (CITATION) que capacita explorações de aplicativos de sumarização multi-documentos neurais (MDS) para aprender com conjuntos de dados em grande escala, este estudo desenvolve dois transformadores hierárquicos (HT) que descrevem as dependências entre tokens e documentos cruzados, em ao mesmo tempo permitem um comprimento maior de documentos de entrada. Ao incorporar atenções multi-cabeça de nível de palavra e parágrafo no decodificador com base nas arquiteturas paralela e vertical, os transformadores hierárquicos paralelos e verticais propostos (PHT e VHT) geram resumos utilizando incorporações de palavras com reconhecimento de contexto juntamente com incorporações de parágrafos estáticos e dinâmicos, respectivamente. Uma avaliação abrangente é realizada no WikiSum para comparar PHT &amp; VHT com modelos estabelecidos e para responder à pergunta se as estruturas hierárquicas oferecem desempenhos mais promissores do que as estruturas planas na tarefa MDS. Os resultados sugerem que nossos modelos hierárquicos geram resumos de maior qualidade capturando melhor as relações entre documentos e economizam mais espaço de memória em comparação aos modelos de estrutura plana. Além disso, recomendamos o PHT devido ao seu valor prático de maior velocidade de inferência e maior capacidade de economia de memória.</abstract_pt>
      <abstract_ja>大規模なデータセットから学習するニューラルマルチドキュメントサマリゼーション（ MDS ）の応用的探索を可能にするWikiSum （引用）に関して、本研究は、クロストークンとクロスドキュメントの両方の依存関係を記述する2つの階層トランスフォーマー（ HT ）を開発すると同時に、入力ドキュメントの長さを延長することを可能にする。 並列および垂直アーキテクチャに基づいてデコーダに単語レベルおよび段落レベルの多頭の注意を組み込むことにより、提案されている並列および垂直階層トランスフォーマー（ PHTおよびVHT ）は、文脈認識の単語埋め込みと静的およびダイナミクスの段落埋め込みをそれぞれ利用して要約を生成する。 ウィキサムでは、PHTとVHTを確立されたモデルと比較し、階層構造がMDSタスクのフラット構造よりも有望なパフォーマンスを提供するかどうかの問題に答えるために、包括的な評価が行われます。 その結果、当社の階層モデルは、ドキュメント間の関係をよりよくキャプチャすることにより、より高品質の要約を生成し、平面構造モデルと比較してより多くのメモリ空間を節約できることが示唆されています。 さらに、より高い推論速度とより大きなメモリ節約容量の実用的な価値を考慮して、PHTをお勧めします。</abstract_ja>
      <abstract_zh>夫WikiSum(CITATION)者,使神经多文档摘要(MDS)之用,可以大集中学习,本开两变形金刚(HT),言交令牌跨文档,而许广输文档之长。 因其并行垂架构之解码器,入单词段多头注意,拟议并行,与垂直分变形金刚(PHT &amp;VHT)分上下文感知词嵌静动态段落嵌成摘要。 WikiSum上周评,以比PHT、VHT之法式,层次结构于MDS之任,孰与平面结构哉? 结果表明,比于平面,则吾分而益得跨文档以成高质量之摘要,而省多内存空间。 此外,吾辈荐PHT,以其有高理速内存节省能力之用也。</abstract_zh>
      <abstract_ru>Что касается WikiSum (ЦИТИРОВАНИЕ), которая позволяет прикладным исследованиям нейронного многодокументного суммирования (MDS) учиться на крупномасштабном наборе данных, это исследование разрабатывает два иерархических трансформатора (HT), которые описывают как кросс-токен, так и кросс-документарную зависимость, в то же время позволяя увеличить длину входных документов. Путем включения словесных и абзацевых многоглавных обращений внимания в декодер на основе параллельной и вертикальной архитектур, предлагаемые параллельные и вертикальные иерархические трансформаторы (PHT иVHT) генерируют сводки, используя контекстно-зависимые вложения слов вместе с вложениями статических и динамических абзацев, соответственно. На WikiSum проводится комплексная оценка для сравнения PHT иVHT с установленными моделями и ответа на вопрос, предлагают ли иерархические структуры более многообещающие характеристики, чем плоские структуры в задаче MDS. Результаты показывают, что наши иерархические модели генерируют сводки более высокого качества, лучше фиксируя взаимосвязи между документами, и экономят больше пространства памяти по сравнению с моделями с плоской структурой. Кроме того, мы рекомендуем PHT, учитывая его практическую ценность - более высокую скорость вывода и большую память-сберегающую способность.</abstract_ru>
      <abstract_hi>WikiSum (CITATION) के संबंध में जो बड़े पैमाने पर डेटासेट से सीखने के लिए तंत्रिका बहु-दस्तावेज़ सारांशीकरण (एमडीएस) के लागू अन्वेषणों को सशक्त बनाता है, यह अध्ययन दो पदानुक्रमित ट्रांसफॉर्मर (एचटी) विकसित करता है जो क्रॉस-टोकन और क्रॉस-दस्तावेज़ निर्भरता दोनों का वर्णन करते हैं, एक ही समय में इनपुट दस्तावेजों की विस्तारित लंबाई की अनुमति देते हैं। समानांतर और ऊर्ध्वाधर आर्किटेक्चर के आधार पर डिकोडर में शब्द- और पैराग्राफ-स्तरीय बहु-सिर ध्यान को शामिल करके, प्रस्तावित समानांतर और ऊर्ध्वाधर पदानुक्रमित ट्रांसफॉर्मर (PHT &amp; VHT) क्रमशः स्थैतिक और गतिशीलता पैराग्राफ एम्बेडिंग के साथ संदर्भ-जागरूक शब्द एम्बेडिंग का उपयोग करके सारांश उत्पन्न करते हैं। स्थापित मॉडल के साथ PHT और VHT की तुलना करने के लिए WikiSum पर एक व्यापक मूल्यांकन किया जाता है और इस सवाल का जवाब देने के लिए कि क्या पदानुक्रमित संरचनाएं MDS कार्य में फ्लैट संरचनाओं की तुलना में अधिक आशाजनक प्रदर्शन प्रदान करती हैं। परिणाम बताते हैं कि हमारे पदानुक्रमित मॉडल क्रॉस-दस्तावेज़ संबंधों को बेहतर ढंग से कैप्चर करके उच्च गुणवत्ता के सारांश उत्पन्न करते हैं, और फ्लैट-संरचना मॉडल की तुलना में अधिक मेमोरी स्पेस बचाते हैं। इसके अलावा, हम पीएचटी को उच्च अनुमान गति और अधिक स्मृति-बचत क्षमता के अपने व्यावहारिक मूल्य को देखते हुए सलाह देते हैं।</abstract_hi>
      <abstract_ga>Maidir le WikiSum (CITATION) a chumasaíonn taiscéalaíochtaí feidhmitheacha ar Achoimriú Néarach Ildhoiciméad (MDS) chun foghlaim ó thacar sonraí ar scála mór, forbraíonn an staidéar seo dhá Chlaochladán ordlathach (HT) a chuireann síos ar na spleáchais tras-chomharthaí agus trasdhoiciméid, ag ceadóidh an t-am céanna fad leathnaithe na ndoiciméad ionchuir. Trí airdí ilcheann ar leibhéal focal agus alt a ionchorprú sa díchódóir bunaithe ar na hailtireachtaí comhthreomhara agus ingearacha, gineann na Trasfhoirmeoirí ordlathacha comhthreomhara agus ingearacha (PHT &amp; VHT) achoimrí ag baint úsáide as leabaithe focal atá feasach ar an gcomhthéacs mar aon le leabaithe ailt statacha agus dinimic, faoi seach. Déantar meastóireacht chuimsitheach ar WikiSum chun PHT &amp; VHT a chur i gcomparáid le samhlacha seanbhunaithe agus chun an cheist a fhreagairt an dtugann struchtúir ordlathacha feidhmíocht níos bisiúla ná struchtúir chomhréidh sa tasc MDS. Tugann na torthaí le tuiscint go ngineann ár múnlaí ordlathacha achoimrí ar chaighdeán níos airde trí ghaolmhaireachtaí trasdhoiciméid a ghabháil níos fearr, agus go sábhálann siad níos mó spásanna cuimhne i gcomparáid le samhlacha le struchtúr comhréidh. Ina theannta sin, molaimid PHT i bhfianaise a luach praiticiúil a bhaineann le luas tátail níos airde agus cumas coigilte cuimhne níos fearr.</abstract_ga>
      <abstract_hu>A WikiSum (CITATION) tekintetében, amely lehetővé teszi a Neural Multi-Document Summarization (MDS) alkalmazási feltárásainak lehetővé tételét, hogy nagyméretű adatkészletből tanulhassanak, ez a tanulmány két hierarchikus transzformátort (HT) fejleszt ki, amelyek leírják mind a kereszttoken, mind a keresztdokumentum függőségeket, ugyanakkor lehetővé teszik a beviteli dokumentumok hosszabb hosszúságát. A párhuzamos és függőleges architektúrákon alapuló szó- és bekezdésszintű többfejű figyelmeztetések beépítésével a javasolt párhuzamos és függőleges hierarchikus transzformátorok (PHT &amp;VHT) összefoglalókat hoznak létre kontextustudatos szóbeágyazásokkal, statikus és dinamikus bekezdésbeágyazásokkal együtt. Átfogó értékelést végeznek a WikiSumon, hogy összehasonlítsák a PHT és VHT modellekkel és válaszoljanak arra a kérdésre, hogy a hierarchikus struktúrák ígéretesebb teljesítményt nyújtanak-e, mint a lapos struktúrák az MDS feladatban. Az eredmények arra utalnak, hogy hierarchikus modelleink jobb minőségű összefoglalókat hoznak létre a dokumentumok közötti kapcsolatok jobb rögzítésével, és több memóriahelyet takarítanak meg a lapos szerkezetű modellekhez képest. Ezenkívül a PHT-t ajánljuk, mivel gyakorlati értéke a nagyobb következtetési sebesség és a nagyobb memória-takarékos kapacitás.</abstract_hu>
      <abstract_el>Όσον αφορά το WikiSum (CITATION) που δίνει τη δυνατότητα στις εφαρμογές εξερεύνησης της Νευρικής Συνοψίας Πολυεγγράφων (ΜΔΣ) να μάθουν από το σύνολο δεδομένων μεγάλης κλίμακας, η παρούσα μελέτη αναπτύσσει δύο ιεραρχικούς Μετασχηματιστές (που περιγράφουν τόσο τις διασταυρούμενες όσο και τις διασταυρούμενες εξαρτήσεις εγγράφων, επιτρέποντας ταυτόχρονα το εκτεταμένο μήκος των εγγράφων εισόδου. Με την ενσωμάτωση προσοχής πολλαπλών κεφαλιών σε επίπεδο λέξης και παραγράφου στον αποκωδικοποιητή με βάση τις παράλληλες και κάθετες αρχιτεκτονικές, οι προτεινόμενοι παράλληλοι και κάθετοι ιεραρχικοί μετασχηματιστές (δημιουργούν περιλήψεις χρησιμοποιώντας ενσωμάτωση λέξεων με επίγνωση του περιβάλλοντος μαζί με στατικές και δυναμικές ενσωμάτωση παραγράφου αντίστοιχα. Μια ολοκληρωμένη αξιολόγηση πραγματοποιείται στο WikiSum για να συγκρίνει το PHT &amp;VHT με καθιερωμένα μοντέλα και να απαντήσει στο ερώτημα αν οι ιεραρχικές δομές προσφέρουν πιο ελπιδοφόρες επιδόσεις από τις επίπεδες δομές στο έργο MDS. Τα αποτελέσματα δείχνουν ότι τα ιεραρχικά μοντέλα μας δημιουργούν περιλήψεις υψηλότερης ποιότητας με την καλύτερη καταγραφή των σχέσεων μεταξύ εγγράφων και εξοικονομούν περισσότερους χώρους μνήμης σε σύγκριση με τα μοντέλα επίπεδης δομής. Επιπλέον, συστήνουμε δεδομένης της πρακτικής αξίας της υψηλότερης ταχύτητας συμπερασμάτων και της μεγαλύτερης ικανότητας εξοικονόμησης μνήμης.</abstract_el>
      <abstract_ka>WikiSum-ის (CITATION) შესახებ, რომელიც ნეიროლური მრავალური დოკუმენტის კომპანიზაციაციის (MDS) პროგრამეტური განსხვავებას უფრო ძალიან გავისწავლა დიდი მაგალითა მონაცემების კომპანიზაციაზე, ეს სწავლად განვითარებს ორი hiერაქტიური ტრანფორმე პარალელი და გერტიკალური აქტიქტიკურების ბაზედ მრავალური დაახლოებით სიტყვა- და პარაგრაფიკალური დონეზე მრავალური დაახლოებით, პროგრალელი და გერტიკალური აქტიქტიკურები (PHT &amp;VHT) დაახლოებით კონტექსტური დაახლოებით სიტყვა,  WikiSum-ში ყველაფერი განსაზღვრება გავაკეთება PHT &amp;VHT-ის შემდგენისთვის, რომელიც განსაზღვრებული მოდელთან და დასაუბრუნოთ კითხვას თუ არა იერაქტიკალური სტრუქტურები უფრო მუშაობელი პ წარმოდგენების შესახებ, რომ ჩვენი ჰიერაქტიკური მოდელები უფრო მეტი კაalitეტის საზოგადოებების შესახებ უფრო მეტი დოკუმენტის შესახებ, და უფრო მეტი მეხსიერი სივრცე და დამატებით, ჩვენ შევძლებთ PHT-ს პრაქტიკური მნიშვნელობა, რომელიც უფრო მეტი ინფრენციის სიჩქარე და უფრო მეტი მეხსიერების შესაძლებლობა.</abstract_ka>
      <abstract_it>Per quanto riguarda WikiSum (CITATION) che consente alle esplorazioni applicative di Neural Multi-Document Summarization (MDS) di imparare da set di dati su larga scala, questo studio sviluppa due Transformer gerarchici (HT) che descrivono sia le dipendenze cross-token che cross-document, consentendo allo stesso tempo una lunghezza estesa dei documenti di input. Incorporando nel decoder attenzioni multi-testa a livello di parola e paragrafo basate sulle architetture parallele e verticali, i trasformatori gerarchici paralleli e verticali proposti (PHT &amp;VHT) generano riassunti utilizzando incorporazioni di parole consapevoli del contesto insieme a incorporazioni di paragrafi statici e dinamici, rispettivamente. Una valutazione completa è condotta su WikiSum per confrontare PHT &amp;VHT con modelli consolidati e per rispondere alla domanda se le strutture gerarchiche offrono prestazioni più promettenti rispetto alle strutture piatte nel compito MDS. I risultati suggeriscono che i nostri modelli gerarchici generano riassunti di qualità superiore catturando meglio le relazioni tra documenti e risparmiando più spazio di memoria rispetto ai modelli a struttura piatta. Inoltre, raccomandiamo PHT dato il suo valore pratico di maggiore velocità di inferenza e maggiore capacità di risparmio di memoria.</abstract_it>
      <abstract_lt>With regards to WikiSum (CITATION) that empowers applicative explorations of Neural Multi-Document Summarization (MDS) to learn from large scale dataset, this study develops two hierarchical Transformers (HT) that describe both the cross-token and cross-document dependencies, at the same time allow extended length of input documents.  Įtraukiant žodžių ir punktų daugiapakopį dėmesį į lygiagrečią ir vertikalią architektūrą grindžiamą dekoderį, siūlomi lygiagrečiai ir vertikalūs hierarchiniai transformatoriai (PHT &amp;VHT) sukuria santraukas, kuriose naudojami konteksto suprantami žodžių įterpimai kartu atitinkamai statiniai ir dinamiški punktų įterpimai. Visapusiškas WikiSum vertinimas atliekamas siekiant palyginti PHT &amp;VHT su nustatytais modeliais ir atsakyti į klausimą, ar hierarchinės struktūros teikia daug žadančių rezultatų nei plokščios struktūros MDS užduotyje. The results suggest that our hierarchical models generate summaries of higher quality by better capturing cross-document relationships, and save more memory spaces in comparison to flat-structure models.  Be to, rekomenduojame PHT, atsižvelgiant į jos praktinę vertę – didesnį išvados greitį ir didesnį atminties taupymo pajėgumą.</abstract_lt>
      <abstract_mk>Што се однесува до WikiSum (CITATION) кој овозможува апликативните експерирации на неуралната мултидокументарна резултатација (MDS) да научат од голем набор на податоци, оваа студија развива два хиерархични трансформери (HT) кои ги опишуваат и раскрсните и раскрсните зависности на документите, истовремено овозможуваат проширена должина на ввод Со вклучување на внимание на зборови и параграфски нивоа на мултиглави во декодерот базиран на паралелните и вертикалните архитектури, предложените паралелни и вертикални хиерархични трансформери (PHT &amp;VHT) генерираат резултати користејќи контекстно свесни зборови вклучени заедно со статички и динамички параграфски На WikiSum се спроведува сеопфатна проценка за споредување на PHT &amp;VHT со воспоставени модели и за одговор на прашањето дали хиерархичните структури нудат повеќе ветувачки изведби отколку рамни структури во задачата MDS. The results suggest that our hierarchical models generate summaries of higher quality by better capturing cross-document relationships, and save more memory spaces in comparison to flat-structure models.  Покрај тоа, препорачуваме PHT со оглед на својата практична вредност на повисока брзина на конференција и поголем капацитет за зачувување на меморијата.</abstract_mk>
      <abstract_kk>WikiSum (CITATION) дегеннен қарай, көпшілікті көпшілікті құжаттар тұжырымдамасын (MDS) үлкен масштабтағы деректер жиындан үйрену үшін қолданатын зерттеулерді көмектеседі, бұл зерттеулер екі иерархиялық түрлендіруші (HT) жасайды. Бұл зерттеулер бірдей кезде,  Параллельді және тік архитектураларда негіздеген декодердің бірнеше сөз- мен абран деңгейінің қатынасын қосу арқылы, параллель және тік иерархиялық түрлендірушері (PHT &amp;VHT) қолданылатын мәтіндіктерді қолданып, контексті түсінікті сөздерді ендіру арқылы,  PHT &amp;VHT дегенді орнатылған үлгілерімен салыстыру үшін WikiSum- да толық оқиға жұмыс істейді, және иерархиялық құрылғылар MDS тапсырмасындағы жеткілікті құрылғылардан артық құрылғылардан артық оқиға Нәтижелер біздің иерархикалық үлгілеріміз құжаттың қатынасын жақсы түсіндіру арқылы, жады бос орындарын бір құрылғы үлгілерімен салыстыру арқылы жоғары сапаттамасын құрады. Қосымша, PHT- ті өзінің практикалық мәнін өзінің жылдамдығын және жады сақтау мүмкіндігін көмектесетін.</abstract_kk>
      <abstract_ms>Berkaitan dengan WikiSum (CITATION) yang membenarkan pengeksplorasi aplikatif Penapisan Dokumen-Berbilang Neural (MDS) untuk belajar dari set data skala besar, kajian ini mengembangkan dua Penukar Hierarkikal (HT) yang menggambarkan dependensi salib-token dan salib-dokumen, pada masa yang sama membenarkan panjang panjang dokumen input. Dengan memasukkan perhatian berbilang-kepala aras perkataan dan paragraf dalam dekoder berdasarkan arkitektur selari dan menegak, Penukar Hierarkikal selari dan menegak (PHT &amp;VHT) yang diusulkan menghasilkan ringkasan menggunakan penyembedding perkataan yang sedar-konteks bersama-sama dengan penyembedding paragraf statik dan dinamik, berdasarkan. Evaluasi meliputi dilakukan pada WikiSum untuk membandingkan PHT &amp;VHT dengan model yang ditetapkan dan untuk menjawab soalan sama ada struktur hierarkis menawarkan prestasi yang lebih berjanji daripada struktur rata dalam tugas MDS. Hasilnya menunjukkan bahawa model hierarkis kita menghasilkan ringkasan kualiti yang lebih tinggi dengan menangkap lebih baik hubungan salib dokumen, dan simpan lebih banyak ruang memori dibandingkan dengan model struktur rata. Selain itu, kami cadangkan PHT mengingati nilai praktik kelajuan kesimpulan yang lebih tinggi dan kapasitas penyimpanan ingatan yang lebih besar.</abstract_ms>
      <abstract_ml>വികിസ്സുമിന്റെ (CITATION) കാര്യത്തില്‍ നെയുറല്‍ പല-രേഖകളുടെ സംഘടനയുടെ പ്രയോഗത്തില്‍ പഠിക്കാന്‍ സാധിക്കുന്ന പ്രയോഗത്തിനുള്ള പരിശോധനങ്ങള്‍ക്ക് സാധ്യതയുണ്ട്. വലിയ സ്കേല്‍ ഡാറ്റാസറില്‍ നിന്നും പഠി പാരാളലിലും വെര്‍ട്ടിക്കല്‍ സ്ഥാനത്തിനും അടിസ്ഥാനമായി ഡെകോഡെറിലും വാക്ക്- നിലനിര്‍മ്മിക്കുന്ന പല തലയിലുള്ള ശ്രദ്ധ കൂട്ടത്തില്‍ ചേര്‍ക്കുന്നതിനാല്‍ പ്രൊദ്ദേശിക്കപ്പെട്ട പാലാള്‍ലെയിലും വെ PHT &amp;VHT-നെ സ്ഥാപിച്ച മോഡലുകളോടൊപ്പം തുല്യമാക്കുവാനും എംഡിഎസ് ജോലിയിലെ ഫ്ലാറ്റ് ഘടനകളെക്കാള്‍ ഹിയെരാര്‍ക്കിക്കല്‍ സ്ഥാപിക്കുന്ന പണിയു അതിന്റെ ഫലങ്ങള്‍ക്ക് നിര്‍ദേശിക്കുന്നത് നമ്മുടെ ഹീയറാര്‍ക്കിക്കല്‍ മോഡലുകള്‍ കൂടുതല്‍ ഗുരുതപ്രകാരം ഉണ്ടാക്കുന്നു. ക്രിസ്റ്റ് രേഖകളുടെ ബന അതുകൊണ്ട്, പിഎച്ചിറ്റിന്റെ പ്രാകൃതിക വേഗത്തിന്റെയും മെമ്മറി സൂക്ഷിക്കുന്ന കഴിവിന്റെയും കൂടുതല്‍ മൂല്</abstract_ml>
      <abstract_mt>Fir-rigward ta’ WikiSum (CITATION) li jagħti s-setgħa lill-esplorazzjonijiet applikattivi tas-Sommarju Multi-Dokumenti Newrali (MDS) biex jitgħallmu minn sett ta’ dejta fuq skala kbira, dan l-istudju jiżviluppa żewġ Trasformaturi ġerarkiċi (HT) li jiddeskrivu kemm id-dipendenzi trasversali kif ukoll dawk trasversali tad-dokumenti, fl-istess ħin jippermettu tul estiż ta’ dokumenti ta’ input. Permezz tal-inkorporazzjoni ta’ attenzjoni b’ħafna ras fil-livell tal-kliem u tal-paragrafu fid-dekoder ibbażat fuq l-arkitetturi paralleli u vertikali, it-Trasformaturi ġerarkiċi paralleli u vertikali proposti (PHT &amp;VHT) jiġġeneraw sommarji li jużaw inkorporazzjonijiet tal-kliem konxji mill-kuntest flimkien ma’ inkorporazzjonijiet tal-paragrafi statiċi u dinamiċi, rispettivament. Twettqet evalwazzjoni komprensiva fuq WikiSum biex titqabbel PHT &amp;VHT ma’ mudelli stabbiliti u biex titwieġeb il-mistoqsija jekk l-istrutturi ġerarkiċi joffrux prestazzjonijiet aktar promettenti minn strutturi ċatti fil-kompitu tal-MDS. Ir-riżultati jissuġġerixxu li l-mudelli ġerarkiċi tagħna jiġġeneraw sommarji ta’ kwalità ogħla billi jinqabdu aħjar relazzjonijiet bejn id-dokumenti, u jiffrankaw aktar spazji tal-memorja meta mqabbla ma’ mudelli ta’ struttura ċatta. Barra minn hekk, nirrakkomandaw lill-PHT minħabba l-valur prattiku tiegħu ta’ veloċità ogħla ta’ inferenza u kapaċità akbar ta’ ffrankar tal-memorja.</abstract_mt>
      <abstract_mn>WikiSum (CITATION) гэдэг нь мэдээллийн хэмжээгээр суралцах боломжтой мэдээллийн хэмжээгээс илүү их хэмжээний өгөгдлийн санаас суралцах боломжтой ажиллагааны судалгааны талаар энэ судалгаа хоёр төвөгтэй шилжүүлэгч (HT) боловсруулдаг. Энэ судалгаа хоёр давхар зураг болон олон баримт бичгийн хамаарал хамаара Параллел болон босоо архитектурууд дээр үндсэн үг болон параграл түвшинд олон толгой удирдлагааг багтааж, санал болгосон параграл болон босоо архитектур түвшигчид (PHT &amp;VHT) нь контекст ойлгомжтой үгийг багтааж байдаг. WikiSum дээр PHT &amp;VHT-г бүтээгдэхүүн загвартай харьцуулахын тулд бүрэн дүгнэлт хийгддэг. Иерактикийн бүтэц нь MDS-ийн ажлын бүтээгдэхүүнээс илүү амлалтай үйл ажиллагааг өгөх эсэхийг асуухад хариулт өгдөг. Үүний үр дүнд бидний төрөлхтний загвар нь бичил баримтын харилцааны илүү өндөр чанарын жинхэнэ хэмжээсүүдийг бий болгодог гэдгийг илүү ойлгож байна. Мөн бид PHT-г илүү өндөр халдварын хурд, санамж хадгалах чадварын үнэ цэнэтэй болгон тайлбарладаг.</abstract_mn>
      <abstract_pl>W odniesieniu do WikiSum (CITATION), który umożliwia aplikacyjne badania neuronowego podsumowania wielu dokumentów (MDS) do uczenia się z dużej skali zbioru danych, opracowano dwa hierarchiczne transformatory (HT), które opisują zarówno zależności między tokenami, jak i między dokumentami, jednocześnie umożliwiają wydłużenie długości dokumentów wejściowych. Dzięki włączeniu uwagi wielogłowicowej na poziomie słów i akapitów w dekoderze opartej na architekturze równoległej i pionowej, proponowane równoległe i pionowe hierarchiczne transformatory (PHT &amp;VHT) generują podsumowania wykorzystujące kontekstowe osadzenia słów wraz z osadzeniami akapitów statycznych i dynamicznych, odpowiednio. Przeprowadzona jest kompleksowa ocena na WikiSum, aby porównać PHT &amp;VHT z ustalonymi modelami i odpowiedzieć na pytanie, czy struktury hierarchiczne oferują bardziej obiecujące wydajności niż struktury płaskie w zadaniu MDS. Wyniki sugerują, że nasze hierarchiczne modele generują podsumowania o wyższej jakości poprzez lepsze przechwytywanie relacji między dokumentami i oszczędzają więcej miejsc pamięci w porównaniu z modelami struktury płaskiej. Ponadto zalecamy PHT ze względu na jego praktyczną wartość większej prędkości wnioskowania i większej pojemności oszczędzania pamięci.</abstract_pl>
      <abstract_ro>În ceea ce privește WikiSum (CITATION) care permite explorărilor aplicative ale Rezumării Neurale Multi-Document (MDS) să învețe din setul de date la scară largă, acest studiu dezvoltă două Transformatoare ierarhice (HT) care descriu atât dependențele cross-token, cât și cross-document, permițând în același timp o lungime extinsă a documentelor de intrare. Prin încorporarea atențiilor multi-cap la nivel de cuvânt și paragraf în decoder bazate pe arhitecturile paralele și verticale, transformatoarele ierarhice paralele și verticale propuse (PHT &amp;VHT) generează rezumate utilizând încorporări conștiente de context cu cuvinte încorporate împreună cu încorporări statice și dinamice de paragrafe, respectiv. O evaluare cuprinzătoare este efectuată pe WikiSum pentru a compara PHT &amp;VHT cu modelele stabilite și pentru a răspunde la întrebarea dacă structurile ierarhice oferă performanțe mai promițătoare decât structurile plate în sarcina MDS. Rezultatele sugerează că modelele noastre ierarhice generează rezumate de calitate superioară prin captarea mai bună a relațiilor între documente și economisirea mai multor spații de memorie în comparație cu modelele cu structură plată. Mai mult decât atât, recomandăm PHT având în vedere valoarea sa practică de viteză de inferență mai mare și capacitate mai mare de economisire a memoriei.</abstract_ro>
      <abstract_sr>S obzirom na WikiSum (CITATION) koji omogućava aplikacijske istraživanja Neuralne višestruke dokumentacije (MDS) da nauče iz velike skupine podataka, ova istraživanja razvija dva hijerarhičkog transformera (HT) koji opisuju i krstotokene i krstodokumenata zavisnosti, istovremeno omogućavaju produženu dužinu ulaznih dokumenta. Uključujući višeglavnu pažnju na reči i nivou paragrafa na dekoderu na temelju paralelnih i vertikalnih arhitektura, predloženi paralelni i vertikalni hijerarhički transformatori (PHT &amp;VHT) stvaraju sažetke korištenje uključujući kontekstsvesne riječi zajedno sa stavnim i dinamičkim stavljanjem paragrafa. Svobuhvatna procjena je provedena na WikiSumu kako bi usporedila PHT &amp;VHT sa uspostavljenim modelima i odgovorila na pitanje da li hijerarhijske strukture nude obećavajuće izvode nego ravne strukture u zadatku MDS-a. Rezultati ukazuju na to da naše hijerarhijske modele stvaraju sažetke višeg kvaliteta boljim prihvaćanjem odnosa preko dokumenta i spašavaju više mesta pamćenja u usporedbi sa modelima ravne strukture. Preporučujemo PHT s obzirom na praktičnu vrijednost veće brzine infekcije i veće kapacitete spašavanja pamćenja.</abstract_sr>
      <abstract_no>For WikiSum (CITATION) that empowers applicative explorations of Neural Multi-Document Summarization (MDS) to learn from large scale dataset, this study develops to hierarchical Transformers (HT) that describes both the cross-token and cross-document dependencies, at the same time allow extended length of input documents. Ved å inkludere fleire opplysningar på ordet- og avsnittnivå i dekoderen basert på parallelle og vertikale arkitekturar, vil dei foreslåde parallelle og vertikale hierarkiske transformerande (PHT &amp;VHT) laga samandringar som brukar innbygging av ordet i kontekst- opplysningar saman med innbygging av statiske og dynamiske paragrafer. Eit komplett evaluering gjev på WikiSum for å sammenligne PHT &amp;VHT med oppretta modeller og for å svara på spørsmålet om hierarkiske strukturar gjev meir promising utviklingar enn flate strukturar i MDS- oppgåva. Resultatet tyder på at hierarkiske modeller våre lagar sammendragar med høgare kvalitet ved at det er bedre å henta kryssdokumentrelasjonar og lagra meir minnerområde i sammenligning med flatstrukturmodeller. I tillegg anbefaler vi PHT å gje praktiske verdien til høgare infeksjonsfartet og større minne-lagringsmessighet.</abstract_no>
      <abstract_si>සමග විකිසුම (CITATION) සමග විකිසියම් සම්බන්ධයෙන් සම්බන්ධ කරනවා න්‍යූරාල් ගොඩක් ලොකු දත්ත සම්බන්ධයෙන් ඉගෙන ගන්න, මේ පරීක්ෂණය සම්බන්ධයෙන් විසින්ධ විසින්ධ විසින්ධ දෙක By inkorporating word- and paragraf-level Multi-head Attensions in the decoder based on the Parallel and Upper Archives, the advised Parallel and Upper Archive Transferers (PHT &amp; VHT) genere summes use ContextKnow word Embdings with static and ynamics Paragraf Embdings, in a way. Note this is a KRunner keyword;% 1 is a KRunner keyword;% 2 is a KRunner keyword;% 3 is a KRunner keyword A comprehersive evalution is managed on WikiSum to compare PHT &amp; VHT with a set of Models and to the Answer of the ask if the hiyerarArchic Strukts are more Promoting Perfections than the Flat Strukts in the MDS job. ප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍ර තවත්, අපි PHT ප්‍රශ්නය කරන්න පුළුවන් විශාල වේගය සහ වැඩිය මතකය බේරගන්න පුළුවන්</abstract_si>
      <abstract_so>Kuwii WikiSum (CITATION) ee uu awoodo dalbasho applikative ah oo ku saabsan qoraalka koowaad ee Neural Multi-Documentation (MDS) inuu ka barto macluumaad aad u weyn, kaasi waxbarashadu wuxuu horumariyaa laba hierarchical Transformers (HT), kaas oo ku sawira iskuul-token iyo iskuul-document ku xiran, isla markaasna wuxuu ruqsan yahay dhererka wargeyska. Hadal- iyo darafka darafka badan ee qodcodka ku qoran meelaha la mid ah iyo meelaha vertikal ah ee la soo jeeday waxay sameeyaan summariyo ku qoran isticmaalaya qoraal-aqoon ah oo ku qoran qoraal-qoraal, waxaana ku qoran qoraalka saxda iyo dynamics, kuwaas oo la soo jeeday parallel iyo vertical hierarchical hierarchical hierarchical hierarchical hierarchical (PHT &amp;VHT). WikiSum waxaa lagu sameeyaa qiimeyn aasaasi ah si uu u simo PHT &amp;VHT sameyno modello la dhisay iyo in uu u jawaabo su'aalaha, in dhismaha hierarchical ay sameeyaan wax ka sii ballan badan dhismaha bannaanta ee shaqada MDS. The results suggest that our hierarchical models generate summaries of higher quality by better capturing cross-document relationships, and save more memory spaces in comparison to flat-structure models.  Sidoo kale waxaan ku talinaynaa in PHT la siiyo qiimaheeda caadiga ah oo aad u dheer, iyo awooddiisa badbaadada xusuusta.</abstract_so>
      <abstract_sv>När det gäller WikiSum (CITATION) som gör det möjligt för applikativa utforskningar av Neural Multi-Document Summarization (MDS) att lära av storskalig datamängd, utvecklar denna studie två hierarkiska Transformers (HT) som beskriver både cross-token och cross-document beroenden, samtidigt tillåter utökad längd av indata dokument. Genom att införliva ord- och styckenivåuppmärksamheter på flera huvuden i dekoden baserat på parallella och vertikala arkitekturer genererar de föreslagna parallella och vertikala hierarkiska transformatorerna (PHT &amp;VHT) sammanfattningar med hjälp av kontextmedvetna ordinbäddningar tillsammans med statiska respektive dynamiska styckeinbäddningar. En omfattande utvärdering görs på WikiSum för att jämföra PHT &amp;VHT med etablerade modeller och för att besvara frågan om hierarkiska strukturer erbjuder mer lovande prestanda än platta strukturer i MDS-uppgiften. Resultaten tyder på att våra hierarkiska modeller genererar sammanfattningar av högre kvalitet genom att bättre fånga korsdokumentsrelationer och spara mer minnesutrymme jämfört med flat structure modeller. Dessutom rekommenderar vi PHT med tanke på dess praktiska värde av högre inferenshastighet och större minnesbesparande kapacitet.</abstract_sv>
      <abstract_ta>விகிச்சும் (CITATION) பற்றியால் நியூரல் பல- ஆவண சுருக்கம் கற்ற பயன்பாட்டின் வெளியீடுகளை அதிகாரமாக்குகிறது, பெரிய அளவு தரவு அமைப்பிலிருந்து கற்றுக் கொள்ள இந்த படிப்பாடு இரண்டு மாற்றங்களை உருவாக்குகிறது, அது க இணைய மற்றும் செங்குத்து அடிப்படைகளை அடிப்படையில் சேர்க்கும் வார்த்தை- மற்றும் paragraph- மட்டத்தில் பல தலைப்பு கவனம் Name The results suggest that our hierarchical models generate summaries of higher quality by better capturing cross-document relationships, and save more memory spaces in comparison to flat-structure models.  மேலும், நாம் PHT அதன் செயல்பாடு மதிப்பை அதிக நோய் வேகம் மற்றும் அதிகமான நினைவு சேமிப்பு சக்தி</abstract_ta>
      <abstract_ur>ویکیسسم (CITATION) کے بارے میں جو نیورل Multi-Document Summarization (MDS) کی کاربرد کی تحقیقات پر قادر ہے کہ بڑے اسکیل ڈاٹ سٹ سے سیکھ سکیں، اس تحقیقات دو حیراتیکل تغییرات (HT) کو تغییر دیتا ہے جو کروسٹٹوکنے اور کروسٹوکیم ڈوکیم ڈوکیمٹ ڈوکیمٹ ڈوکیمٹ ڈوکیمٹ ڈوکیمٹ کلمات- اور پاراگراف- سطح کے متعدد-سر کی توجه کے ذریعہ دکھانے کے لئے مختلف اور متعدد معمارات پر بنیاد رکھتے ہیں، پیغمبر کے مطابق مختلف اور متعدد افراطیکل تغییرات (PHT &amp;VHT) کے ذریعہ متوجہ ہونے والی کلمات کے مطابق استٹیکل اور ڈینامیک پاراگراف امڈینگ کے ساتھ پیدا ویکیس سم پر ایک محکم ارزیابی ہے کہ PHT &amp;VHT کو ثابت کئے ہوئے موڈل کے ساتھ مقایسہ کرنے کے لئے اور پوچھنے کے لئے جواب دیں کہ کیا ہیرارک ساخترات MDS کے کام میں فالت ساخترات سے زیادہ وعدہ دینے والی عملکرد پیش کرتی ہیں. نتیجے ان کی توصیف کرتے ہیں کہ ہمارے حیراتیکل موڈلے بلند کیفیت کے ساتھ بہترین ترکیب کے ذریعہ سے زیادہ مہمانی جگہ رکھتے ہیں اور فلاٹ ساختاری موڈلے کے مقابلہ میں زیادہ مہمانی جگہ رکھتے ہیں. اور ہم نے PHT کو اس کے عمدہ مقدار کے ساتھ مہربانی دیتے ہیں کہ اس کی عمدہ مہربانی میں زیادہ سرعت اور زیادہ مہربانی ذخیره کی قابلیت۔</abstract_ur>
      <abstract_uz>Name Name Name Natijalar esa, bizning hierarchik modellarimiz quyidagi darajalarni kichik darajada yaratish va cross-hujjatning huquqlarini olib tashlash va boshqa xotira joylarini flat-structural modellariga kamaytirish imkoniyatini saqlash mumkin. Ko'rib, biz PHT'ning asosiy qiymatini eng yaxshi qiymatga va xotira saqlash imkoniyatini bajarishimizni talab qilamiz.</abstract_uz>
      <abstract_vi>Đối với WikiLSum (độc lập) cho phép sử dụng các khám phá các tiêu chí thần kinh đa tài liệu (MDS) để học từ bộ dữ liệu quy mô lớn, nghiên cứu này phát triển hai Dịch hoá phân cấp (HT) mà mô tả các mối quan hệ giữa các quỹ hình và các tài liệu chéo, đồng thời cho phép dài lâu dài các tài liệu nhập. Bằng cách nhập vào các chức năng đa chữ và đoạn của các bộ giải mã dựa trên các cấu trúc song song và dọc, cấu trúc cấu trúc cấu trúc cấu trúc song song song song và thẳng đứng (PHT (PHT/ VHT) tạo ra các bản tóm tắt nhờ kết hợp nhận diện từ ngữ cảnh cùng với cấu trúc phân tích tích tĩnh và tính chất. Một đánh giá to àn diện được thực hiện trên WikiLSum về việc so sánh PHT, VHT với các mô hình đã xác định và để trả lời câu hỏi liệu cấu trúc cấp cao có đem lại triển vọng nhiều hơn cấu trúc phẳng trong nhiệm vụ MDS. Kết quả cho thấy các mô hình cấp dưới của chúng ta tạo ra các bản tóm tắt chất cao hơn bằng cách nắm giữ các mối quan hệ chéo tài liệu, và tiết kiệm nhiều khoảng bộ nhớ hơn so với các mô hình cấu trúc phẳng. Hơn nữa, chúng tôi đề nghị triết học dựa trên giá trị thực tế của tốc độ nhận ra cao và khả năng tiết kiệm trí nhớ.</abstract_vi>
      <abstract_bg>По отношение на Уикисумата (ЦИТАЦИЯ), която дава възможност за апликативни проучвания на невронното многодокументално обобщаване (МДС), за да се научи от широкомащабния набор от данни, това проучване разработва два йерархични трансформатора (ХТ), които описват както зависимостта на кръстосаните символи, така и на кръстосаните документи, като в същото време позволяват удължена продължителност на входните документи. Чрез включването на вниманието на няколко глави на ниво дума и абзац в декодера въз основа на паралелни и вертикални архитектури, предложените паралелни и вертикални йерархични трансформатори (PHT &amp;VHT) генерират резюмета, използвайки контекстното вграждане на думи заедно със статични и динамични вграждания на абзаци съответно. Проведена е цялостна оценка на Уикисум, за да се сравни PHT &amp;VHT с утвърдени модели и да се отговори на въпроса дали йерархичните структури предлагат по-обещаващи резултати от плоските структури в задачата МДС. Резултатите показват, че нашите йерархични модели генерират резюмета с по-високо качество чрез по-добро улавяне на междудокументни взаимоотношения и спестяват повече пространство в паметта в сравнение с моделите с плоска структура. Освен това препоръчваме ПХТ предвид практическата му стойност на по-висока скорост на заключение и по-голям капацитет за спестяване на памет.</abstract_bg>
      <abstract_da>Med hensyn til WikiSum (CITATION), der gør det muligt for applikationer at udforske Neural Multi-Document Summarization (MDS) til at lære af store datasæt, udvikler dette studie to hierarkiske Transformers (HT), der beskriver både cross-token og cross-document afhængigheder, samtidig tillader længere længde af inputdokumenter. Ved at indarbejde ord- og afsnitsniveau flerhovedets opmærksomhed i dekoden baseret på parallelle og lodrette arkitekturer, genererer de foreslåede parallelle og vertikale hierarkiske transformatorer (PHT &amp;VHT) resuméer ved hjælp af kontekstbevidste ordindlejringer sammen med henholdsvis statiske og dynamiske afsnitsindlejringer. En omfattende evaluering foretages på WikiSum for at sammenligne PHT &amp;VHT med etablerede modeller og for at besvare spørgsmålet om hierarkiske strukturer giver mere lovende resultater end flade strukturer i MDS-opgaven. Resultaterne tyder på, at vores hierarkiske modeller genererer resuméer af højere kvalitet ved bedre at fange tværs-dokumentrelationer og spare flere hukommelsespladser sammenlignet med flade strukturer modeller. Desuden anbefaler vi PHT i betragtning af dens praktiske værdi af højere inference hastighed og større hukommelsesbesparende kapacitet.</abstract_da>
      <abstract_hr>S obzirom na WikiSum (CITATION) koji omogućava primjene istraživanja Neuralne multidokumentacije (MDS) da nauče iz velike skupine podataka, ova istraživanja razvija dva hijerarhičkog transformatora (HT) koji opisuju i krstotokene i krstodokumenata zavisnosti, istovremeno omogućavaju produženu dužinu ulaznih dokumenta. Uključujući višeglavne pozornosti riječi i razine paragrafa u dekoderu na temelju paralelnih i vertikalnih arhitektura, predloženi paralelni i vertikalni hijerarhički transformatori (PHT &amp;VHT) stvaraju sažetke koji koriste uključujuće kontekstski svesne riječi zajedno s ugrađenjem staničnih i dinamičkih paragrafa. Svobuhvatna procjena je provedena na WikiSum kako bi usporedila PHT &amp;VHT s utvrđenim modelima i odgovorila na pitanje da li hijerarhijske strukture nude obećavajuće učinke nego ravne strukture u zadatku MDS-a. Rezultati ukazuju na to da naše hijerarhijske modele stvaraju sažetke višeg kvaliteta boljim uhvaćenjem odnosa o krstodokumentima i spašavaju više mjesta pamćenja u usporedbi s modelima ravne strukture. Preporučujemo PHT s obzirom na praktičnu vrijednost veće brzine infekcije i veće kapacitete spašavanja pamćenja.</abstract_hr>
      <abstract_nl>Met betrekking tot WikiSum (CITATION) dat applicatieve verkenningen van Neural Multi-Document Summarization (MDS) mogelijk maakt om te leren van grootschalige dataset, ontwikkelt deze studie twee hiërarchische Transformers (HT) die zowel de cross-token als cross-document afhankelijkheden beschrijven, tegelijkertijd een langere lengte van invoerdocumenten mogelijk maken. De voorgestelde parallelle en verticale hiërarchische Transformers (PHT &amp;VHT) genereren samenvattingen door gebruik te maken van contextbewuste woordinsluitingen samen met statische en dynamische alinea-insluitingen. Een uitgebreide evaluatie wordt uitgevoerd op WikiSum om PHT &amp;VHT te vergelijken met gevestigde modellen en om de vraag te beantwoorden of hiërarchische structuren veelbelovende prestaties bieden dan platte structuren in de MDS-taak. De resultaten suggereren dat onze hiërarchische modellen samenvattingen van hogere kwaliteit genereren door documentoverschrijdende relaties beter vast te leggen en meer geheugenruimte te besparen in vergelijking met vlakke structuren modellen. Bovendien raden we PHT aan vanwege de praktische waarde van hogere inferentiesnelheid en grotere geheugenbesparende capaciteit.</abstract_nl>
      <abstract_de>Im Hinblick auf WikiSum (CITATION), das die applikative Erforschung der neuronalen Multi-Document Summarization (MDS) ermöglicht, aus großen Datensätzen zu lernen, entwickelt diese Studie zwei hierarchische Transformatoren (HT), die sowohl die tokenübergreifenden als auch dokumentübergreifenden Abhängigkeiten beschreiben und gleichzeitig eine längere Länge von Eingabedokumenten ermöglichen. Die vorgeschlagenen parallelen und vertikalen hierarchischen Transformatoren (PHT &amp;VHT) erzeugen Zusammenfassungen, die kontextbewusste Worteinbettungen zusammen mit statischen und dynamischen Absatzeinbettungen verwenden, indem sie Wort- und Absatzaufmerksamkeiten in den Decoder integrieren. Auf WikiSum wird eine umfassende Evaluation durchgeführt, um PHT &amp;VHT mit etablierten Modellen zu vergleichen und die Frage zu beantworten, ob hierarchische Strukturen in der MDS-Aufgabe vielversprechendere Leistungen bieten als flache Strukturen. Die Ergebnisse deuten darauf hin, dass unsere hierarchischen Modelle durch bessere Erfassung dokumentübergreifender Zusammenhänge qualitativ hochwertigere Zusammenfassungen generieren und im Vergleich zu flachen Strukturmodellen mehr Speicherplatz sparen. Darüber hinaus empfehlen wir PHT aufgrund seines praktischen Wertes einer höheren Inferenzgeschwindigkeit und einer größeren Speicherkapazität.</abstract_de>
      <abstract_ko>WikiSum(인용문)에 관해서는 신경 다중 문서 요약(MDS)의 응용 탐색을 대규모 데이터로부터 집중적으로 학습할 수 있도록 한다. 본 연구는 두 개의 차원 변환기(HT)를 개발하여 태그와 문서에 대한 의존 관계를 설명하고 입력 문서의 길이를 연장할 수 있도록 한다.병행과 수직 구조를 바탕으로 하는 디코더에 단어와 단락급 다제목 주의를 추가함으로써 제시된 병행과 수직 차원 변환기(PHT &amp; VHT)는 각각 상하문 감지 단어 삽입과 정적 및 동적 단락 삽입을 이용하여 요약을 생성한다.WikiSum에서 종합 평가를 실시하여 PHT와 VHT를 이미 세워진 모델과 비교하고 MDS 작업에서 층구조가 편평한 구조보다 더 전망적인 성능을 제공하는지에 대한 질문에 대답한다.그 결과 평면 구조 모델에 비해 우리의 층별 모델은 크로스 문서 관계를 더욱 잘 포획하여 더욱 높은 품질의 요약을 생성하고 더 많은 메모리 공간을 절약하는 것으로 나타났다.또한 PHT는 더 높은 추리 속도와 더 큰 저장 용량을 가지고 있기 때문에 추천합니다.</abstract_ko>
      <abstract_id>Mengenai WikiSum (CITATION) yang memungkinkan eksplorasi aplikatif dari Penapisan Multi-Dokumen Neural (MDS) untuk belajar dari set data skala besar, penelitian ini mengembangkan dua Transformer Hierarkis (HT) yang menjelaskan kedua ketergantuan cross-token dan cross-document, pada saat yang sama memungkinkan panjang panjang dokumen masukan. By incorporating word- and paragraph-level multi-head attentions in the decoder based on the parallel and vertical architectures, the proposed parallel and vertical hierarchical Transformers (PHT &amp;VHT) generate summaries utilizing context-aware word embeddings together with static and dynamics paragraph embeddings, respectively.  Evaluasi komprensif dilakukan di WikiSum untuk membandingkan PHT &amp;VHT dengan model yang ditetapkan dan untuk menjawab pertanyaan apakah struktur hierarkis menawarkan prestasi yang lebih berjanji daripada struktur rata dalam tugas MDS. Hasilnya menunjukkan bahwa model hierarkis kita menghasilkan ringkasan kualitas yang lebih tinggi dengan menangkap lebih baik hubungan saling dokumen, dan menyimpan lebih banyak ruang memori dibandingkan dengan model struktur rata. Selain itu, kami merekomendasikan PHT mengingat nilai praktis kecepatan inferensi yang lebih tinggi dan kapasitas penyimpanan ingatan yang lebih besar.</abstract_id>
      <abstract_sw>Kuhusu WikiSum (CITATION) ambayo inawezesha kutekeleza matumizi ya Ujumbe wa Makala ya Njerumani (MDS) kujifunza kutoka kwenye seti kubwa ya data, utafiti huu unaendelea kuwa na Transformers wawili wa kiutawala (HT) ambao huelezea namna ya kupitia alama na kutumia nyaraka za kuvuka, wakati huo huo huruhusu muda wa mrefu wa nyaraka za input. Kwa kuunganisha maneno- na paragraph yenye kiwango kikubwa katika kodi kwa kutumia miundombinu yanayofanana na yenye msingi, mapendekezo yanatengeneza muhtasari wa mabadiliko ya kimaenzi na wa juu (PHT &amp;VHT) kwa kutumia ujumbe wa maneno yanayofahamika kwa muktadha pamoja na paragraph ya takwimu na dynamics. A comprehensive evaluation is conducted on WikiSum to compare PHT &amp;VHT with established models and to answer the question whether hierarchical structures offer more promising performances than flat structures in the MDS task.  The results suggest that our hierarchical models generate summaries of higher quality by better capturing cross-document relationships, and save more memory spaces in comparison to flat-structure models.  Zaidi ya hayo, tunapendekeza PHT kufuatia thamani yake ya uhalisia ya kiwango cha kuongezeka kwa kasi na uwezo wa kuokoa kumbukumbu.</abstract_sw>
      <abstract_fa>در مورد ویکیسوم (CITATION) که بر تحقیقات کاربردی از جمع‌سازی چند سند عصبی (MDS) توان می‌دهد تا از مجموعه داده‌های مقیاس بزرگ یاد بگیرد، این تحقیقات دو تغییر‌سازی (HT) را توسعه می‌دهد که هر دو بستگی‌های متصل‌ترکیب و متصل‌ترکیب سند را توصیف می‌کند، در همین زمان طول مدت با توجه به توجه کلمه- و پاراگراف- سطح متعدد سر در dekoder بر اساس معماری متعدد و عمودی، ترجمه‌کننده‌های متعدد و متعدد عمودی (PHT &amp;VHT) جمعیت‌ها را تولید می‌کنند که با توجه‌های پاراگراف‌های ثابت و دینامیک‌ها با استفاده می‌کنند. یک ارزیابی کامل در ویکیسوم برای مقایسه PHT &amp;VHT با مدل‌های ساخته شده و برای پاسخ دادن به سؤال این است که آیا ساختارهای دایره‌آرایشی عملکرد‌های قول‌دهنده‌تر از ساختارهای پایین در کار MDS پیشنهاد می‌دهند. نتیجه‌ها پیشنهاد می‌دهند که مدل‌های دایره‌آفرینی ما با بهتر گرفتن رابطه‌های دایره‌سند، جمع‌آوری‌های کیفیت بالاتر را تولید می‌کند، و فضای حافظه بیشتری را در مقایسه با مدل‌های ساختار پایین ذخیره می به علاوه، ما توصیه می کنیم که PHT با ارزش عملیاتش از سرعت بیماری بالاتر و توانایی ذخیره حافظه بیشتری داشته باشد.</abstract_fa>
      <abstract_af>Met betrekking tot WikiSum (KYTATION) wat aansoek toepassiende uitsoek van Neural Multi-Document Opsomming (MDS) om uit groot skaal datastel te leer, word hierdie studie twee hierarkies Transformeerders (HT) ontwikkel wat beide die kruistoken en kruisdokument afhanklikhede beskryf, op dieselfde tyd toelaat uitgebreide lengte van invoer dokumente toe. Deur te inkorporeer woord- en paragraaf- vlak multi- hoof aandag in die dekoder gebaseer op die parallele en vertikale arkitektuur, die voorgestelde parallele en vertikale hierarkies Transformeerders (PHT &amp;VHT) genereer opsommings wat gebruik konteks- wees woord inbettings saam met statiese en dinamiese paragraaf inbettings, respektief. 'n Kompleksie evaluasie is gedoen op WikiSum om PHT &amp;VHT te vergelyk met geïnstalleerde modele en om die vraag te antwoord of hierarkies strukture meer beloftende uitvoerings aanbied as plat strukture in die MDS taak. Die resultate stel voorstel dat ons hierarkies modele opsommings van hoër kwaliteit genereer deur beter kruisdokument verhouding te vang en meer geheue spasies stoor in vergelyking met plat-struktuur modele. Ook, ons beveel PHT gegee sy praktiese waarde van hoër inferensie spoed en groter geheue- stoor kapasiteit.</abstract_af>
      <abstract_tr>WikiSum (CITATION) ýagdaýynda näyral Çok-Sened Toplaýyşynyň (MDS) uly ölçek veri setirinden öwrenmesine mümkin edýän işlemleri üçin bu arşiw iki iýerarhiýal terjimeleri (HT) gelinýär. Bu arşiw hem çapda-token we çapda-sened baglanyşlaryny tanyýan, hem-de keçersiz sened boýunlaryna mümkin edýär. parallel we dikel arhitektarlaryň daýanýan söz- we paragraf-derejesi multi-kelli dykglaýyşlaryň içine (PHT &amp;VHT) maslahat edilýän parallel we dikel iýerarhiýal terjimeleri (süýşik we bellenen söz integrlerini statik we dinamik paragraf ködlemeleri bilen ullanýarlar. WikiSumda daşary çykyş edildi, PHT &amp;VHT düzümlendirilen nusgalary bilen karşılaştyrmak we soragyna jogap bermek üçin, iýerarhiýalyk strukturalar MDS göreviniň flat strukturalaryndan has söz berýän performanslary üýtgetmekden has baglanýar. Netijenler biziň iýerarhiýa nusgalarymyz çyk-sened baglaşyklaryny gowy ýazmak üçin ýokary kaliwatlygyny döredip, we ýeterlik ýagtylyk nusgalarymyzy flat-struktur modellere görä gaýd etmek üçin ullanyşýar. Üstelik, biz PHT'i ýokary azalyk tizliginiň praktik mykdaryny we hatyra gaýd etmäge rugsat bererdik.</abstract_tr>
      <abstract_hy>Ինչ վերաբերում է WiKiSum-ին (CITItion), որը հնարավորություն է տալիս նյարդային բազմաթիվ փաստաթղթերի համառոտագրման (MDS) ծրագրային ուսումնասիրություններին սովորել մեծ մասշտաբով տվյալների համակարգում, այս ուսումնասիրությունը զարգանում է երկու հիերարխիկ տրանֆորմերներ (HTML), որոնք նկարագրում են երկու թղթի և երկու փաստաթղթի Օգտագրելով բառերի և պարագծերի մակարդակի բազմագլխավոր ուշադրությունը դեկոդերի մեջ, որը հիմնված է զուգահեռ և ուղղահայաց ճարտարապետությունների վրա, առաջարկված զուգահեռ և ուղղահայաց հիերարխիկ տրանֆորմերները (PHT և VHTML) ստեղծում են համառոտագրություններ, որոնք օգտագործում են կոնտեքստի գիտակցած WiKiSum-ում կատարվում է ընդհանուր գնահատում, որպեսզի համեմատենք PHT-ն և VHTML-ը հաստատված մոդելների հետ և պատասխանենք հարցին, թե արդյոք հիերարխիկ կառուցվածքները ավելի խոստացնող արտադրություններ են տալիս, քան MDS-ի գործում հաստատվ Արդյունքները ցույց են տալիս, որ մեր հիերարխիկ մոդելները ստեղծում են ավելի բարձր որակի համառոտագրություններ' ավելի լավ վերցնելով թղթափաստաթղթերի միջև հարաբերությունները և ավելի շատ հիշողության տարածքներ պահպանելով հարաբերականների մոդելների հետ: Ավելին, մենք խորհուրդ ենք տալիս PHT-ին, հաշվի առնելով իր գործնական արժեքը ավելի բարձր հետևողության արագությունը և ավելի մեծ հիշողության խնայողության ունակությունը:</abstract_hy>
      <abstract_az>WikiSum (CITATION) barəsində, böyük ölçü verilən qurğudan öyrənmək üçün nöral çox-döküm Toplaşdırma (MDS) təşkil edilən təşkil keşfetmələrinə güclü olan iki hiyerarşik transformatörü (HT) təşkil edir, bu təşkil həmçinin hər ikisini çox-token və çox-döküm bağlılıqlarını təşkil edir. Sözləri - və paragraf-seviyyəti çox-başlıq dikkatini, paralel və vertical arhitektarları üzərində dayanan dekoderdə, təklif edilən paralel və vertical hiyerarşik transformatörlər (PHT &amp;VHT) təklif edilən təklif-bildirilmiş sözləri birlikdə statik və dinamik paragrafı içərisində istifadə edirlər. WikiSum'da, PHT &amp;VHT'i qurulmuş modellərlə qarşılaşdırmaq üçün, hiyerarşik strukturları MDS işində daha çox vəd verici performanslar təklif edir və yoxdur. Sonuçlarımız hiyerarşik modellərimizin yüksək keyfiyyətinin istifadələrini daha yaxşı döküm əlaqələrini alıb, daha çox hafıza alanlarını düz struktur modellərinə qarşılaşdırmaq üçün qoruyub saxlayır. Daha sonra, PHT'nin praktik qiymətini yüksək infeksiya sürəti və daha böyük hafıza qurtarma kapasitəsini təsdiq edirik.</abstract_az>
      <abstract_bs>S obzirom na WikiSum (CITATION) koji omogućava aplikacijske istraživanja Neuralne multidokumentacije (MDS) da nauče iz velike skupine podataka, ova istraživanja razvija dva hijerarhičkog transformera (HT) koji opisuju i krstotokene i krstodokumenata zavisnosti, istovremeno omogućavaju produženu dužinu ulaznih dokumenta. Uključujući višeglavnu pozornost riječi i nivoa paragrafa na dekoderu na temelju paralelnih i vertikalnih arhitektura, predloženi paralelni i vertikalni hijerarhički transformatori (PHT &amp;VHT) stvaraju sažetke koji koriste uključujuće kontekstski svesne riječi zajedno s stavnim i dinamičkim stavljanjem paragrafa. Svobuhvatna procjena je provedena na WikiSumu kako bi usporedila PHT &amp;VHT sa utvrđenim modelima i odgovorila na pitanje da li hijerarhijske strukture nude obećavajuće učinke nego ravne strukture u zadatku MDS-a. Rezultati ukazuju na to da naše hijerarhijske modele generiraju sažetke višeg kvaliteta boljim uhvaćenjem odnosa preko dokumenta i spašavaju više mjesta pamćenja u usporedbi s modelima ravne strukture. Preporučujemo PHT s obzirom na praktičnu vrijednost veće brzine infekcije i veće kapacitete spašavanja pamćenja.</abstract_bs>
      <abstract_am>With regards to WikiSum (CITATION) that empowers applicative explorations of Neural Multi-Document Summarization (MDS) to learn from large scale dataset, this study develops two hierarchical Transformers (HT) that describe both the cross-token and cross-document dependencies, at the same time allow extended length of input documents.  በአካባቢ እና የክፍለ አካባቢ እና የክፍለ አካባቢ መሠረቶች በመጠቀም ቃላት- እና paragraph-ደረጃ የብዙ ራስ ስብሰባዎች በመጠቀም በተፈጻሙ ተቃውሞ እና የክፍተት አዳራቢ መተላለፊያዎች (PHT &amp;VHT) በተለየ ስህተት እና የdynamics paragraph በመጠቀም የሚታወቅ ቃላት ማጠቃለያዎች አቀማመጥ፡፡ በWikiSum ውስጥ PHT &amp;VHT እና በተመሠረቱት ሞዴላዎችን ለማስተካከል ውጤት ይደረጋል፡፡ ፍሬቶቹ የሐራርተርክ ዓይነቶቻችን የክፍለ ሥርዓት ግንኙነትን በመቀበል እና የክፍለ ሰነድ ግንኙነቶችን በመያዝ የሚሻል ማሳየት እና የመስታሰቢያ ቦታዎችን በመስመር ማሳየት ይችላል፡፡ ደግሞም PHT የፍጥረት ፍጥረት እና የማስታወሻ ማዳን ችሎታዋን ለመስጠት እንመክራለን፡፡</abstract_am>
      <abstract_sq>Në lidhje me WikiSum (CITATION) që mundëson eksplorimet aplikative të Somarizimit Neural Multi-Document (MDS) për të mësuar nga grupi i të dhënave në shkallë të madhe, ky studim zhvillon dy Transformuesit hierarkikë (HT) që përshkruajnë si varësitë kryqëzimi ashtu dhe të dokumenteve, në të njëjtën kohë lejojnë gjatësinë e zgjeruar të dokumenteve të hyrjes. By incorporating word- and paragraph-level multi-head attentions in the decoder based on the parallel and vertical architectures, the proposed parallel and vertical hierarchical Transformers (PHT &amp;VHT) generate summaries utilizing context-aware word embeddings together with static and dynamics paragraph embeddings, respectively.  Një vlerësim tërësor është kryer në WikiSum për të krahasuar PHT &amp;VHT me modele të vendosur dhe për të përgjigjur pyetjes në se strukturat hierarkike ofrojnë performanca më të premtueshme se strukturat e rrafshta në detyrën MDS. Rezultatet sugjerojnë se modelet tona hierarkike gjenerojnë përmbledhje të cilësisë më të lartë duke kapur më mirë marrëdhëniet ndërdokumentore dhe duke ruajtur më shumë hapësira kujtese në krahasim me modelet e strukturës së rrafshtë. Përveç kësaj, ne rekomandojmë PHT duke marrë parasysh vlerën e saj praktike të shpejtësisë së inferencës më të lartë dhe kapacitetit më të madh të ruajtjes së kujtesës.</abstract_sq>
      <abstract_et>Seoses WikiSumiga (CITATION), mis võimaldab neuroaalse mitme dokumendi kokkuvõtte (MDS) rakenduslikke uuringuid õppida suuremahulistest andmekogumitest, arendatakse käesolevas uuringus välja kaks hierarhilist transformaatorit (HT), mis kirjeldavad nii ristmärkide kui ka ristdokumentide sõltuvust, samal ajal võimaldavad sisenddokumentide pikendatud pikkust. Paralleelsetel ja vertikaalsetel arhitektuuridel põhineva sõna- ja lõiketasandi mitmepealise tähelepanu lisades dekoodrisse, loovad väljapakutud paralleelsed ja vertikaalsed hierarhilised transformaatorid (PHT &amp;VHT) kokkuvõtted, kasutades kontekstiteadlikke sõnade põimimisi koos staatiliste ja dünaamiliste lõikude põimimisega vastavalt. WikiSumis viiakse läbi põhjalik hindamine, et võrrelda PHT ja VHT väljakujunenud mudelitega ning vastata küsimusele, kas hierarhilised struktuurid pakuvad MDS ülesandes paljutõotavamaid tulemusi kui tasased struktuurid. Tulemused näitavad, et meie hierarhilised mudelid loovad kvaliteetsemaid kokkuvõtteid, jäädvustades paremini dokumentidevahelisi seoseid ja säästes rohkem mäluruumi võrreldes lameda struktuuriga mudelitega. Lisaks soovitame PHT-d, kuna selle praktiline väärtus on suurem järelduskiirus ja suurem mälu säästmine.</abstract_et>
      <abstract_bn>উইকিসাম (সিটাটিশন) সম্পর্কে যে নেউরাল মাল্টি ডকুমেন্ট সামার্জারেশন (এমডিএস) এর প্রয়োজনীয় বিস্ফোরণের ক্ষমতা প্রদান করে বিশাল স্ক্যালের ডাটাসেট থেকে শিখতে পারে, এই গবেষণা দুটি হিয়ারেক্যাল ট্রান্সফ প্রস্তাবিত প্যারালেল এবং উল্লেখযোগ্য স্থাপত্যের উপর ভিত্তিতে ডেকোডারে শব্দ- এবং প্যারাফেক্ট- স্তরের মাধ্যমে অন্তর্ভুক্ত মনোযোগ আকর্ষণের মাধ্যমে প্রস্তাবিত প্যারালেল এবং উল্লেখযোগ্য হায়েরা পিএইচটি &amp;ভিএইচটি’র সাথে স্থাপন করা মডেলের তুলনা করার জন্য উইকিসুমে একটি সম্পূর্ণ মূল্য চালানো হয়েছে এবং প্রশ্নের উত্তর দেওয়া হয় এমডিএস কাজের ফ্ল্যাট কাঠামো ফলাফল পরামর্শ প্রদান করে যে আমাদের হিরেরার্কিক মডেলগুলো বেশী মানের সারিগুলো তৈরি করে ক্রস-নথির সম্পর্কের সাথে আরো ভালো স্মৃতি সংরক্ষণ করে ফ্ল এছাড়াও আমরা পিএইচটির প্রাকৃতিক মূল্য প্রদান করি যে তার ব্যাপারে দুর্ভোগের গতি এবং স্মৃতি সংরক্ষণের ক্ষমতা বেশ</abstract_bn>
      <abstract_ca>With regards to WikiSum (CITATION) that empowers applicative explorations of Neural Multi-Document Summarization (MDS) to learn from large scale dataset, this study develops two hierarchical Transformers (HT) that describe both the cross-token and cross-document dependencies, at the same time allow extended length of input documents.  By incorporating word- and paragraph-level multi-head attentions in the decoder based on the parallel and vertical architectures, the proposed parallel and vertical hierarchical Transformers (PHT &amp;VHT) generate summaries utilizing context-aware word embeddings together with static and dynamics paragraph embeddings, respectively.  A comprehensive evaluation is conducted on WikiSum to compare PHT &amp;VHT with established models and to answer the question whether hierarchical structures offer more promising performances than flat structures in the MDS task.  Els resultats suggereixen que els nostres models jeràrquics generen resumes de millor qualitat capturant millor relacions entre documents i estalviant més espais de memòria en comparació amb models d'estructura plana. A més, recomanem que PHT tingui el seu valor pràctic de més velocitat de inferència i més capacitat d'estalvi de memòria.</abstract_ca>
      <abstract_cs>S ohledem na WikiSum (CITATION), který umožňuje aplikační průzkum neuronové multidokumentové shrnutí (MDS) učit se z velkého množství dat, tato studie vyvíjí dva hierarchické transformátory (HT), které popisují jak cross-token, tak cross-dokument závislosti, zároveň umožňují delší délku vstupních dokumentů. Navržené paralelní a vertikální hierarchické transformátory (PHT &amp;VHT) generují souhrny využívající kontextové vložení slov spolu se statickými a dynamickými vloženími odstavců. Na WikiSumu je provedeno komplexní vyhodnocení, aby bylo možné porovnat PHT &amp;VHT se zavedenými modely a zodpovědět otázku, zda hierarchické struktury nabízejí slibnější výkony než ploché struktury v MDS úkolu. Výsledky naznačují, že naše hierarchické modely vytvářejí souhrny vyšší kvality díky lepšímu zachycení vztahů mezi dokumenty a šetří více paměťového prostoru ve srovnání s modely ploché struktury. Navíc doporučujeme PHT vzhledem k praktické hodnotě vyšší rychlosti inference a větší kapacity úspory paměti.</abstract_cs>
      <abstract_fi>Mitä tulee WikiSumiin (CITATION), joka mahdollistaa hermojen monidokumenttien yhteenvedon (MDS) applicative explorations of Neural Multi-Document Summarization, tässä tutkimuksessa kehitetään kaksi hierarkkista muuntajaa (HT), jotka kuvaavat sekä merkkien että dokumenttien välisiä riippuvuuksia ja mahdollistavat samalla syöttöasiakirjojen pidemmän pituuden. Yhdistämällä dekooderiin sana- ja kappaletason monipäiset huomiot rinnakkais- ja pystysuuntaisiin arkkitehtuureihin, ehdotetut rinnakkais- ja pystysuuntaiset hierarkkiset muuntajat (PHT &amp;VHT) tuottavat tiivistelmiä käyttäen kontekstitietoisia sanaupotuksia sekä staattisia ja dynaamisia kappaleiden upotuksia. WikiSumissa tehdään kattava arviointi, jossa verrataan PHT &amp;VHT:tä vakiintuneisiin malleihin ja vastataan kysymykseen, tarjoavatko hierarkiset rakenteet MDS-tehtävässä lupaavampaa suorituskykyä kuin litteät rakenteet. Tulokset viittaavat siihen, että hierarkkiset mallit tuottavat laadukkaampia yhteenvedoita tallentamalla dokumenttien välisiä suhteita paremmin ja säästävät enemmän muistitilaa verrattuna litteärakenteisiin malleihin. Lisäksi suosittelemme PHT:tä, koska sen käytännön arvo on suurempi päättelynopeus ja suurempi muistinsäästökapasiteetti.</abstract_fi>
      <abstract_ha>Ina amfani da WikiSum (CITATItion) wanda yake iya amfani da masu shiryarwa na masu ƙaranci na takardar Kwamfyuta na Neural Muda-Document Summarization (MDS) zuwa a sanar da shi daga tsarin mai girma, wannan littafin yana ƙarfafa biyu hiirarchical Transformers (HT) wanda ke bayyana both the korn-tagon da kuma yana dõgara ga rubutun-takardar-kore, sami da kuma a sami wannan lokacin na yarda da tsawo tsawo takardun ayuka masu shiga. Ina shigar da saurin- da-daraja masu nau'i masu cikin kodi a kan fanel da matsayin masu tsaye, da aka buƙata masu parallel da kuma masu tsayi, za'a ƙididdige fassarar da masu hijarari a tsaye (PHT &amp; vHT) mai amfani da maganar-da-fahimta masu fahimta sami da kuma rubutun na tabar da za'a fito. An samar da kimar ta jumla a WikiSum don ya sami PHT &amp; vHT da shiryoyin da aka tabbatar da shi kuma ya jiba tambayi, shin tsarin hierorchical masu nuna mafiya yi wa'adi ko da tsari masu yi wa'adi da su fi girma cikin aikin MDS. Mataimakin na shawara cewa misalinmu na hiera zata ƙari masu tsari da baƙata tsari mafi kyauta a samu'in ka sami tsari-takardar-takardar, kuma ka tsare filayenai masu ƙaranci da sami da misãlai-mai flat. Kayya, Munã shawarar da PHT ya cika kimar gaskiyar kashi na kasar kashi da awon adana kumbar.</abstract_ha>
      <abstract_sk>V zvezi z WikiSumom (CITATION), ki omogoča aplikativne raziskave nevralnega večdokumentskega povzetka (MDS), da se učijo iz obsežnega nabora podatkov, ta študija razvija dva hierarhična transformatorja (HT), ki opisujeta tako odvisnost navzkrižnih žetonov kot navzkrižnih dokumentov, hkrati pa omogočata podaljšano dolžino vhodnih dokumentov. Predlagani vzporedni in vertikalni hierarhični transformatorji (PHT &amp;VHT) z vključitvijo večglavnih pozornosti na ravni besed in odstavkov v dekoder, ki temelji na vzporedni in vertikalni arhitekturi, ustvarjajo povzetke z uporabo kontekstnega ozaveščanja besed skupaj s statičnim in dinamičnim vključevanjem odstavkov. Na WikiSumu smo izvedli celovito oceno, da bi primerjali PHT &amp;VHT z uveljavljenimi modeli in odgovorili na vprašanje, ali hierarhične strukture ponujajo bolj obetavno delovanje kot ravne strukture v nalogi MDS. Rezultati kažejo, da naši hierarhični modeli ustvarjajo povzetke višje kakovosti z boljšim zajemanjem meddokumentnih relacij in prihranijo več pomnilniških prostorov v primerjavi z modeli ravne strukture. Poleg tega priporočamo PHT glede na njegovo praktično vrednost višje hitrosti sklepanja in večje zmogljivosti varčevanja s pomnilnikom.</abstract_sk>
      <abstract_he>בנוגע לוויקיסום (CITATION) שמאפשר לחקור אפליקטיבי של סאמריזציה נוירולית של מסמכים רבים (MDS) ללמוד ממערכת נתונים בקנה מידה גדולה, המחקר הזה מפתח שני טרנספורמטרים הייררכיים (HT) שמתארים את תלויות הצלב-סימנים וגם הצלב-מסמכים, באותו הזמן מאפשרים אורך ארוך של מסמכים הכניסה. By incorporating word- and paragraph-level multi-head attentions in the decoder based on the parallel and vertical architectures, the proposed parallel and vertical hierarchical Transformers (PHT &amp;VHT) generate summaries utilizing context-aware word embeddings together with static and dynamics paragraph embeddings, respectively.  הערכה מורכבת נעשת על WikiSum כדי להשוות PHT &amp;VHT עם דוגמנים קבועים ולענות על השאלה אם מבנים הייררכיים מציעים ביצועים מבטים מבטים שווים במשימה MDS. התוצאות מציעות שהדוגמנים היררכיים שלנו יוצרים סדרות של איכות גבוהה יותר על ידי תפיסה טובה יותר מערכות יחסים בין מסמכים, ולחסוך יותר מקומות זיכרון בהשוואה לדוגמנים במבנה שווה. חוץ מזה, אנחנו ממליצים PHT בהתחשב בערך המרשמי שלו של מהירות ההנחה גבוהה יותר ויכולת חיסוך זכרון גדולה יותר.</abstract_he>
      <abstract_jv>Attribute Coverage Sayensi Rejalingan suggeruju di nambah akeh model kita jejer-nambah sing luwih dumateng kapan karo nggawe barang langgar-tokkum ngono nggawe barang langgar-sistem sing luwih dumateng politenessoffpolite"), and when there is a change ("assertivepoliteness</abstract_jv>
      <abstract_bo>With regards to WikiSum (CITATION) that empowers applicative explorations of Neural Multi-Document Summarization (MDS) to learn from large scale dataset, this study develops two hierarchical Transformers (HT) that describe both the cross-token and cross-document dependencies, at the same time allow extended length of input documents. By incorporating word- and paragraph-level multi-head attentions in the decoder based on the parallel and vertical architectures, the proposed parallel and vertical hierarchical Transformers (PHT &amp;VHT) generate summaries utilizing context-aware word embeddings together with static and dynamics paragraph embeddings, respectively. A comprehensive evaluation is conducted on WikiSum to compare PHT &amp;VHT with established models and to answer the question whether hierarchical structures offer more promising performances than flat structures in the MDS task. The results suggest that our hierarchical models generate summaries of higher quality by better capturing cross-document relationships, and save more memory spaces in comparison to flat-structure models. འོན་ཀྱང་། ང་ཚོས་PHT དེ་གིས་རང་ཉིད་ཀྱི་གནོད་ཐང་སྙིང་ཚད་མཐོ་ཤོས་དང་དྲན་སྒྲིག</abstract_bo>
      </paper>
    </volume>
</collection>