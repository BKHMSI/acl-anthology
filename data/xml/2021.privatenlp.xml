<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.privatenlp">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Privacy in Natural Language Processing</booktitle>
      <editor><first>Oluwaseyi</first><last>Feyisetan</last></editor>
      <editor><first>Sepideh</first><last>Ghanavati</last></editor>
      <editor><first>Shervin</first><last>Malmasi</last></editor>
      <editor><first>Patricia</first><last>Thaine</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.privatenlp-1</url>
    </meta>
    <frontmatter>
      <url hash="8fef7924">2021.privatenlp-1.0</url>
      <bibkey>privatenlp-2021-privacy</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Learning and Evaluating a Differentially Private Pre-trained Language Model</title>
      <author><first>Shlomo</first><last>Hoory</last></author>
      <author><first>Amir</first><last>Feder</last></author>
      <author><first>Avichai</first><last>Tendler</last></author>
      <author><first>Alon</first><last>Cohen</last></author>
      <author><first>Sofia</first><last>Erell</last></author>
      <author><first>Itay</first><last>Laish</last></author>
      <author><first>Hootan</first><last>Nakhost</last></author>
      <author><first>Uri</first><last>Stemmer</last></author>
      <author><first>Ayelet</first><last>Benjamini</last></author>
      <author><first>Avinatan</first><last>Hassidim</last></author>
      <author><first>Yossi</first><last>Matias</last></author>
      <pages>21–29</pages>
      <abstract>Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to <a href="https://en.wikipedia.org/wiki/Information_leakage">information leakage</a> and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the <a href="https://en.wikipedia.org/wiki/Privacy">privacy</a> of such individuals is to train a differentially-private model, but this usually comes at the expense of model performance. Moreover, it is hard to tell given a privacy parameter $ \epsilon$ what was the effect on the trained <a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning">representation</a>. In this work we aim to guide future practitioners and researchers on how to improve <a href="https://en.wikipedia.org/wiki/Privacy">privacy</a> while maintaining good model performance. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $ \epsilon=1 $ and with only a small degradation in performance. We experiment on a dataset of clinical notes with a <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained on a target entity extraction task, and compare it to a similar <a href="https://en.wikipedia.org/wiki/Statistical_model">model</a> trained without differential privacy. Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.</abstract>
      <url hash="804e3a52">2021.privatenlp-1.3</url>
      <doi>10.18653/v1/2021.privatenlp-1.3</doi>
      <bibkey>hoory-etal-2021-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
    <title_ar>تعلم وتقييم نموذج لغوي خاص تم تدريبه مسبقًا بشكل تفاضلي</title_ar>
      <title_fr>Apprentissage et évaluation d'un modèle linguistique préformé différentiellement privé</title_fr>
      <title_es>Aprendizaje y evaluación de un modelo lingüístico preentrenado diferencialmente privado</title_es>
      <title_pt>Aprendendo e Avaliando um Modelo de Linguagem Pré-treinado Diferencialmente Privado</title_pt>
      <title_ja>差別化された民間の事前訓練された言語モデルの学習と評価</title_ja>
      <title_zh>学与评估差分私有预训习语言模样</title_zh>
      <title_hi>सीखना और एक विभेदक रूप से निजी पूर्व-प्रशिक्षित भाषा मॉडल का मूल्यांकन करना</title_hi>
      <title_ru>Изучение и оценка дифференциально частной предварительно обученной языковой модели</title_ru>
      <title_ga>Samhail Teanga Réamh-oilte go Difreálach a Fhoghlaim agus a Mheastóireacht</title_ga>
      <title_ka>სწავლება და განსაზღვრება სხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვ</title_ka>
      <title_el>Μάθηση και αξιολόγηση ενός διαφορετικά ιδιωτικού προ-εκπαιδευμένου γλωσσικού μοντέλου</title_el>
      <title_hu>Egy differenciálisan privát, előképzett nyelvi modell tanulása és értékelése</title_hu>
      <title_it>Imparare e valutare un modello linguistico pre-addestrato differentemente privato</title_it>
      <title_kk>Өзгеше жеке алдындағы тіл үлгісін үйрену және оқу</title_kk>
      <title_lt>Mokymasis ir vertinimas</title_lt>
      <title_ms>Membelajar dan menilai Model Bahasa Terlatih Berbeza Pribadi</title_ms>
      <title_ml>വ്യത്യസ്ത പ്രൈവറ്റ് പരിശീലിച്ച ഭാഷ മോഡല്‍ പഠിക്കുകയും പരിശീലിക്കുകയും ചെയ്യുന്നു</title_ml>
      <title_mk>Учење и евалуирање на различно приватен предобучен јазик модел</title_mk>
      <title_mt>It-tagħlim u l-Evalwazzjoni ta’ Mudell tal-Lingwa Mħarreġ minn Qabel Differenzjalment Privat</title_mt>
      <title_no>Læring og evaluering av ein forskjellig privat føretreng språk- modell</title_no>
      <title_pl>Uczenie się i ocena różnicowo prywatnego modelu językowego</title_pl>
      <title_mn>Өөр төрлийн хувьд сургалтын өмнө сургалтын хэл загварыг сурах, үнэлэх</title_mn>
      <title_ro>Învățarea și evaluarea unui model lingvistic pre-instruit diferențial privat</title_ro>
      <title_so>Waxbarashada iyo qiimeynta nooca afka hore ee gaarka loo leeyahay</title_so>
      <title_sr>Učenje i procjena različitih privatnih predobučenih jezičkih modela</title_sr>
      <title_sv>Att lära sig och utvärdera en differentierat privat förklädd språkmodell</title_sv>
      <title_si>වෙනස් විශේෂයෙන් පුරුද්ගලික භාෂාව ප්‍රශ්නයක් ඉගෙනගන්න සහ අවශ්‍යය කරන්න</title_si>
      <title_ur>ایک مختلف مختلف خصوصی پیش تربین کی زبان موڈل کی تعلیم اور ارزیابی</title_ur>
      <title_ta>தனிப்பட்ட பயிற்சி மொழி மாதிரியை கற்றுக்கொள்வதும், மதிப்பிடுவதும்</title_ta>
      <title_uz>Name</title_uz>
      <title_vi>Học và Đánh giá ngôn ngữ riêng biệt</title_vi>
      <title_bg>Учене и оценка на диференциално частен предобучен езиков модел</title_bg>
      <title_da>Læring og evaluering af en differentielt privat forududdannet sprogmodel</title_da>
      <title_nl>Leren en evalueren van een Differentieel Privé Vooropgeleid Taalmodel</title_nl>
      <title_hr>Učenje i procjena različitih privatnih predobučenih jezičkih modela</title_hr>
      <title_de>Lernen und Evaluieren eines differenziell privaten vortrainierten Sprachmodells</title_de>
      <title_fa>یادگیری و ارزیابی یک مدل زبان پیش آموزش شخصی مختلف</title_fa>
      <title_sw>Kufundisha na Kupima Utawala tofauti tofauti</title_sw>
      <title_ko>서로 다른 개인 예비 교육 언어 모델을 학습하고 평가하다</title_ko>
      <title_tr>Öňünden Aýratyn Çahsy Dili Öwrenmek we Taýýarlamak</title_tr>
      <title_sq>Mësimi dhe vlerësimi i një modeli gjuhësh të trajnuar në mënyrë të ndryshme private</title_sq>
      <title_id>Belajar dan Evaluasi Model Bahasa Terlatih Berbeda Pribadi</title_id>
      <title_am>ምርጫዎች</title_am>
      <title_bn>প্রাইভেট প্রশিক্ষিত ভাষা মডেল শিক্ষা এবং মূল্যায়ন করা হচ্ছে</title_bn>
      <title_az>칐zg칲r 칐zg칲r Dil Modelini 칬yr톛nm톛k v톛 Q톛rcl톛m톛k</title_az>
      <title_hy>Learning and Evaluating a Differentially Private Pre-trained Language Model</title_hy>
      <title_ca>Aprendre i evaluar un model de llenguatge pré-entrenat diferencialment privat</title_ca>
      <title_et>Erinevalt eraõppe keelemudeli õppimine ja hindamine</title_et>
      <title_cs>Učení se a hodnocení diferenciálně soukromého předškoleného jazykového modelu</title_cs>
      <title_fi>Eri yksityisen kielimallin oppiminen ja arviointi Esikoulutetun kielimallin oppiminen</title_fi>
      <title_af>Leer en evalueer 'n Verskillende Privaat Voorgevordergevorderde Taal Model</title_af>
      <title_bs>Učenje i procjena različitih privatnih predobučenih jezičkih modela</title_bs>
      <title_jv>FindOK</title_jv>
      <title_he>ללמוד ולעריך מודל שפה מאומן מראש פרטי</title_he>
      <title_ha>@ action</title_ha>
      <title_sk>Učenje in ocenjevanje diferencialno zasebnega vnaprej usposobljenega jezikovnega modela</title_sk>
      <title_bo>སྒེར་གྱི་སྔོན་ལྟར་གྱི་སྐད་རིགས་མ་དབྱེ་བ་དང་དབྱེ་ཞིབ་བྱེད་པ</title_bo>
      <abstract_pt>Os modelos de linguagem contextuais levaram a resultados significativamente melhores em uma infinidade de tarefas de compreensão da linguagem, especialmente quando pré-treinadas nos mesmos dados da tarefa downstream. Embora esse pré-treinamento adicional geralmente melhore o desempenho, ele pode levar ao vazamento de informações e, portanto, colocar em risco a privacidade dos indivíduos mencionados nos dados de treinamento. Um método para garantir a privacidade de tais indivíduos é treinar um modelo diferencialmente privado, mas isso geralmente ocorre às custas do desempenho do modelo. Além disso, é difícil dizer, dado um parâmetro de privacidade $\epsilon$, qual foi o efeito na representação treinada. Neste trabalho, pretendemos orientar futuros profissionais e pesquisadores sobre como melhorar a privacidade, mantendo o bom desempenho do modelo. Demonstramos como treinar um modelo de linguagem pré-treinado diferencialmente privado (ou seja, BERT) com garantia de privacidade de $\epsilon=1$ e com apenas uma pequena degradação no desempenho. Experimentamos um conjunto de dados de notas clínicas com um modelo treinado em uma tarefa de extração de entidade alvo e o comparamos com um modelo semelhante treinado sem privacidade diferencial. Por fim, apresentamos experimentos mostrando como interpretar a representação diferencialmente privada e compreender as informações perdidas e mantidas nesse processo.</abstract_pt>
      <abstract_es>Los modelos lingüísticos contextuales han dado lugar a resultados significativamente mejores en una gran cantidad de tareas de comprensión lingüística, especialmente cuando se entrenan previamente con los mismos datos que la tarea posterior. Si bien esta capacitación previa adicional generalmente mejora el rendimiento, puede provocar la filtración de información y, por lo tanto, poner en riesgo la privacidad de las personas mencionadas en los datos de capacitación. Un método para garantizar la privacidad de estas personas es entrenar un modelo diferencialmente privado, pero esto generalmente se produce a expensas del rendimiento del modelo. Además, es difícil decir, dado un parámetro de privacidad $\ epsilon$, cuál fue el efecto en la representación entrenada. En este trabajo, nuestro objetivo es guiar a los futuros profesionales e investigadores sobre cómo mejorar la privacidad mientras se mantiene un buen rendimiento del modelo. Demostramos cómo entrenar un modelo lingüístico previamente entrenado y diferencialmente privado (por ejemplo, BERT) con una garantía de privacidad de $\ epsilon=1$ y con solo una pequeña degradación en el rendimiento. Experimentamos con un conjunto de datos de notas clínicas con un modelo entrenado en una tarea de extracción de la entidad objetivo y lo comparamos con un modelo similar entrenado sin privacidad diferencial. Finalmente, presentamos experimentos que muestran cómo interpretar la representación diferencialmente privada y entender la información perdida y mantenida en este proceso.</abstract_es>
      <abstract_ar>أدت نماذج اللغة السياقية إلى نتائج أفضل بشكل ملحوظ في عدد كبير من مهام فهم اللغة ، خاصةً عندما يتم تدريبها مسبقًا على نفس البيانات مثل المهمة النهائية. في حين أن هذا التدريب المسبق الإضافي عادةً ما يحسن الأداء ، إلا أنه يمكن أن يؤدي إلى تسرب المعلومات وبالتالي يخاطر بخصوصية الأفراد المذكورين في بيانات التدريب. تتمثل إحدى طرق ضمان خصوصية هؤلاء الأفراد في تدريب نموذج تفاضلي-خاص ، ولكن هذا يأتي عادةً على حساب أداء النموذج. علاوة على ذلك ، من الصعب معرفة تأثير معلمة الخصوصية $ \ epsilon $ على التمثيل المدرب. نهدف في هذا العمل إلى توجيه الممارسين والباحثين المستقبليين حول كيفية تحسين الخصوصية مع الحفاظ على أداء النموذج الجيد. نوضح كيفية تدريب نموذج لغة مُدرَّب مسبقًا تفاضليًا خاصًا (على سبيل المثال ، BERT) مع ضمان خصوصية $ \ epsilon = 1 $ وبتدهور بسيط في الأداء. نجرب على مجموعة بيانات من الملاحظات السريرية باستخدام نموذج تم تدريبه على مهمة استخراج كيان مستهدف ، ومقارنته بنموذج مشابه تم تدريبه دون خصوصية تفاضلية. أخيرًا ، نقدم تجارب توضح كيفية تفسير التمثيل التفاضلي-الخاص وفهم المعلومات المفقودة والمحافظة عليها في هذه العملية.</abstract_ar>
      <abstract_fr>Les modèles linguistiques contextuels ont permis d'obtenir de meilleurs résultats sur une pléthore de tâches de compréhension de la langue, en particulier lorsqu'elles ont été préformées sur les mêmes données que la tâche en aval. Bien que cette pré-formation supplémentaire améliore généralement les performances, elle peut entraîner une fuite d'informations et donc mettre en danger la vie privée des personnes mentionnées dans les données de formation. Une méthode pour garantir la confidentialité de ces personnes consiste à former un modèle différencié privé, mais cela se fait généralement au détriment des performances du modèle. De plus, il est difficile de dire, compte tenu d'un paramètre de confidentialité $ \ epsilon$, quel a été l'effet sur la représentation entraînée. Dans ce travail, nous visons à guider les futurs praticiens et chercheurs sur la manière d'améliorer la confidentialité tout en maintenant de bonnes performances de modèle. Nous montrons comment entraîner un modèle de langage pré-formé différentiellement privé (c'est-à-dire BERT) avec une garantie de confidentialité de $ \ epsilon=1$ et avec une faible dégradation des performances. Nous expérimentons un ensemble de données de notes cliniques avec un modèle formé sur une tâche d'extraction d'entité cible, et nous le comparons à un modèle similaire formé sans confidentialité différentielle. Enfin, nous présentons des expériences montrant comment interpréter la représentation différentielle privée et comprendre les informations perdues et conservées au cours de ce processus.</abstract_fr>
      <abstract_ja>文脈言語モデルは、特に下流のタスクと同じデータで事前にトレーニングを受けた場合、多くの言語理解タスクで有意に良い結果をもたらしました。 この追加の事前トレーニングは通常パフォーマンスを向上させますが、情報漏洩につながる可能性があり、したがってトレーニングデータに記載されている個人のプライバシーを危険にさらす可能性があります。 そのような個人のプライバシーを保証する方法の1つは、差別化されたプライベートモデルをトレーニングすることですが、これは通常、モデルのパフォーマンスを犠牲にします。 さらに、プライバシーパラメータ$\ ε $が訓練された表現にどのような影響を与えたかを判断するのは困難です。 本作では、モデルのパフォーマンスを良好に維持しながらプライバシーを向上させる方法について、将来の実践者や研究者を指導することを目指しています。 私たちは、$\ ε = 1 $のプライバシー保証とパフォーマンスのわずかな低下を伴う差別化されたプライベートな事前トレーニング言語モデル（すなわち、BERT ）をトレーニングする方法を実演します。 ターゲットエンティティ抽出タスクでトレーニングされたモデルを使用して、臨床ノートのデータセットを実験し、プライバシーの違いなしにトレーニングされた同様のモデルと比較します。 最後に、差異的プライベート表現を解釈し、このプロセスで失われ、維持される情報を理解する方法を示す実験を提示します。</abstract_ja>
      <abstract_zh>上下文言语明白,特当预练于下流之数。 虽复预练常高性能,庶几信息漏泄,危及练数之私。 一法教异,常以牺牲为价。 此外给定私参数 $\epsilon$ ,难以定训练。 此等事,吾道未来之从业者,与治人保持良好模样同时改善隐私。 我们演了如何训练一个差分私或预练言语模样(即BERT),其隐私保定为$ \epsilon = 1 $,而且性能只有很小的降下。 吾等取质于临床笔记数实验之,与无差私者相校也。 最后,我们发出些实验,展示了解释微分私有表示,并知道这些事中失护的信息。</abstract_zh>
      <abstract_ru>Контекстуальные языковые модели привели к значительно лучшим результатам по множеству задач по пониманию языка, особенно при предварительном обучении на тех же данных, что и последующая задача. Хотя это дополнительное предварительное обучение обычно улучшает производительность, оно может привести к утечке информации и, следовательно, риску для конфиденциальности лиц, упомянутых в данных обучения. Одним из способов гарантировать конфиденциальность таких лиц является обучение дифференцированно-частной модели, но обычно это происходит за счет эффективности модели. Более того, трудно сказать, учитывая параметр конфиденциальности $\epsilon$, каково было влияние на обученное представление. В этой работе мы стремимся помочь будущим практикам и исследователям улучшить конфиденциальность, сохраняя при этом хорошую эффективность модели. Мы демонстрируем, как обучить дифференциально-частную предварительно обученную языковую модель (т.е. BERT) с гарантией конфиденциальности $\epsilon=1$ и с небольшим снижением производительности. Мы экспериментируем на наборе данных клинических примечаний с моделью, обученной задаче извлечения целевого объекта, и сравниваем ее с аналогичной моделью, обученной без дифференциальной конфиденциальности. Наконец, мы представляем эксперименты, показывающие, как интерпретировать дифференциально-частное представление и понимать информацию, потерянную и поддерживаемую в этом процессе.</abstract_ru>
      <abstract_hi>प्रासंगिक भाषा मॉडल ने भाषा समझने वाले कार्यों की अधिकता पर काफी बेहतर परिणाम दिए हैं, खासकर जब डाउनस्ट्रीम कार्य के समान डेटा पर पूर्व-प्रशिक्षित किया जाता है। हालांकि यह अतिरिक्त पूर्व-प्रशिक्षण आमतौर पर प्रदर्शन में सुधार करता है, यह जानकारी रिसाव का कारण बन सकता है और इसलिए प्रशिक्षण डेटा में उल्लिखित व्यक्तियों की गोपनीयता को जोखिम में डाल सकता है। ऐसे व्यक्तियों की गोपनीयता की गारंटी देने का एक तरीका एक विभेदक-निजी मॉडल को प्रशिक्षित करना है, लेकिन यह आमतौर पर मॉडल प्रदर्शन की कीमत पर आता है। इसके अलावा, यह एक गोपनीयता पैरामीटर को देखते हुए बताना मुश्किल है $ \ epsilon $ प्रशिक्षित प्रतिनिधित्व पर क्या प्रभाव पड़ा। इस काम में हम भविष्य के चिकित्सकों और शोधकर्ताओं को मार्गदर्शन करने का लक्ष्य रखते हैं कि अच्छे मॉडल प्रदर्शन को बनाए रखते हुए गोपनीयता में सुधार कैसे किया जाए। हम प्रदर्शित करते हैं कि $\epsilon = 1$ की गोपनीयता गारंटी के साथ और प्रदर्शन में केवल एक छोटी गिरावट के साथ एक विभेदक-निजी पूर्व-प्रशिक्षित भाषा मॉडल (यानी, BERT) को कैसे प्रशिक्षित किया जाए। हम एक लक्ष्य इकाई निष्कर्षण कार्य पर प्रशिक्षित एक मॉडल के साथ नैदानिक नोट्स के डेटासेट पर प्रयोग करते हैं, और इसकी तुलना विभेदक गोपनीयता के बिना प्रशिक्षित एक समान मॉडल से करते हैं। अंत में, हम प्रयोगों को दिखाते हैं कि विभेदक-निजी प्रतिनिधित्व की व्याख्या कैसे करें और इस प्रक्रिया में खोई और बनाए रखी गई जानकारी को समझें।</abstract_hi>
      <abstract_ga>Tá torthaí i bhfad níos fearr mar thoradh ar mhúnlaí comhthéacsúla teanga ar raidhse tascanna tuiscint teanga, go háirithe nuair a cuireadh réamhoiliúint ar na sonraí céanna leis an tasc iartheachtach. Cé go bhfeabhsaíonn an réamhoiliúint bhreise seo feidhmíocht de ghnáth, d’fhéadfadh sceitheadh faisnéise a bheith mar thoradh air agus dá bhrí sin cuireann sé príobháideacht na ndaoine aonair a luaitear sna sonraí oiliúna i mbaol. Modh amháin chun príobháideacht daoine aonair den sórt sin a ráthú ná múnla difreálach príobháideach a oiliúint, ach is gnách go dtagann sé seo ar chostas feidhmíochta an mhúnla. Ina theannta sin, tá sé deacair a rá, nuair a thugtar paraiméadar príobháideachta $\epsilon$, cén éifeacht a bhí ar an ionadaíocht oilte. San obair seo tá sé mar aidhm againn cleachtóirí agus taighdeoirí amach anseo a threorú maidir le conas príobháideacht a fheabhsú agus dea-fheidhmíocht eiseamláireach a choinneáil. Léirímid conas múnla teanga réamh-oilte difreálach-phríobháideach a oiliúint (i.e., BERT) le ráthaíocht príobháideachta $\epsilon=1$ agus gan ach díghrádú beag ar fheidhmíocht. Déanaimid tástáil ar thacar sonraí de nótaí cliniciúla le múnla atá oilte ar thasc asbhainte aonáin sprice, agus cuirimid i gcomparáid é le múnla comhchosúil atá oilte gan príobháideacht dhifreálach. Ar deireadh, cuirimid turgnaimh i láthair a léiríonn conas an léiriú difreálach-phríobháideach a léirmhíniú agus an fhaisnéis a chailltear agus a chothaítear sa phróiseas seo a thuiscint.</abstract_ga>
      <abstract_hu>A kontextusnyelvi modellek számos nyelvértési feladat tekintetében jelentősen jobb eredményt eredményeztek, különösen akkor, ha előzetesen ugyanazon adatokra készültek, mint a downstream feladat. Bár ez a kiegészítő előképzés általában javítja a teljesítményt, információszivárgáshoz vezethet, és ezáltal kockáztatja a képzési adatokban említett egyének magánéletét. Az ilyen személyek magánéletének biztosításának egyik módszere egy differenciálisan magánmodell képzése, de ez általában a modell teljesítményének rovására jár. Ezenkívül nehéz megmondani egy $\epsilon$ adatvédelmi paraméter miatt, hogy mi volt a hatás a képzett reprezentációra. Ebben a munkában arra törekszünk, hogy iránymutatást nyújtsunk a jövőbeli szakembereknek és kutatóknak, hogyan lehet javítani a magánéletet, miközben fenntartjuk a jó modellteljesítményt. Bemutatjuk, hogyan képezhetünk egy differenciálisan privát, előre képzett nyelvi modellt (azaz BERT), amelynek adatvédelmi garanciája $\epsilon=1$, és csak kis mértékben romlik a teljesítmény. Klinikai megjegyzésekből álló adatkészleten kísérletezünk egy célentitás extrakciós feladatra képzett modell segítségével, és összehasonlítjuk egy hasonló, különböző adatvédelem nélkül képzett modellel. Végezetül kísérleteket mutatunk be, amelyek bemutatják, hogyan lehet értelmezni a differenciálisan privát reprezentációt és megérteni az ebben a folyamatban elveszett és fenntartott információkat.</abstract_hu>
      <abstract_ka>კონტექსტური ენის მოდელები უფრო უკეთესი შედეგი წარმოდგენა ენის გაგრძელება, განსაკუთრებით როდესაც წარმოდგენა იგივე მონაცემებით, როგორც ჩვენი დაკავშირებული მაგრამ ეს დამატებული პროცემენტის განათლება უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო შე ერთი პროცემი, რომელიც ამ ადამიანების პირადი სიცოცხლეობის გადარჩენა, არის განსხვავებული-პირადი მოდელის გარჩენა, მაგრამ ეს საშუალოდ მოდელის გამოყენება. დამატებით, ძალიან რთულია აღწეროთ პრივიატური პარამეტრი $\ epsilon$ რომელიც იყო შესწავლობული გამოსახულებაზე. ამ სამუშაოში ჩვენ მივიღეთ მომავალე პრაქტიკონტერები და მსწავლობელი გავაკეთოთ თუ როგორ უფრო უფრო უფრო უფრო უფრო უფრო უფრო უ ჩვენ გამოჩვენებთ, როგორ განსხვავებულად პრიგრამიურად პროგრამიურად განსწავლებული ენის მოდელს (მაგალითად BERT) პრიგრამიურად დარანტია $\epsilon=1$ და მხოლოდ პატარა განსხვავება პროგრამიურად. ჩვენ ექსპერიმენტით კლინიკური ნოტების მონაცემების შესახებ მოდელს, რომელიც მიზეზი ინტერქქტურის ექსპექტურაციის დავაკეთებული დავაკეთებული მოდელზე, და გადამყვეტით საბოლოოდ, ჩვენ აჩვენებთ ექსპერიმენტები, როგორ განსხვავება პირადი რესპერიმენტის განსხვავება და გავიგეთ ინფორმაციას, რომლებიც გამოიყენება და გადა</abstract_ka>
      <abstract_it>I modelli linguistici contestuali hanno portato a risultati significativamente migliori su una pletora di attività di comprensione della lingua, soprattutto se pre-addestrati sugli stessi dati del compito a valle. Anche se questo pre-training aggiuntivo migliora solitamente le prestazioni, può portare a perdite di informazioni e quindi rischiare la privacy delle persone menzionate nei dati di formazione. Un metodo per garantire la privacy di tali individui è quello di formare un modello differentemente privato, ma questo di solito viene a scapito delle prestazioni del modello. Inoltre, è difficile dire dato un parametro privacy $\epsilon$ quale sia stato l'effetto sulla rappresentazione addestrata. In questo lavoro puntiamo a guidare i futuri professionisti e ricercatori su come migliorare la privacy mantenendo buone prestazioni del modello. Dimostriamo come addestrare un modello linguistico pre-addestrato differenziatamente privato (BERT) con una garanzia di privacy di $\epsilon=1$ e con solo un piccolo degrado delle prestazioni. Sperimentiamo su un set di dati di note cliniche con un modello addestrato su un compito di estrazione di entità target e lo confrontiamo con un modello simile addestrato senza privacy differenziale. Infine, presentiamo esperimenti che mostrano come interpretare la rappresentazione differenzialmente privata e comprendere le informazioni perse e mantenute in questo processo.</abstract_it>
      <abstract_el>Τα μοντέλα περιεκτικών γλωσσών έχουν οδηγήσει σε σημαντικά καλύτερα αποτελέσματα σε μια πληθώρα εργασιών κατανόησης γλωσσών, ειδικά όταν έχουν προετοιμαστεί για τα ίδια δεδομένα με την επόμενη εργασία. Ενώ αυτή η πρόσθετη προεκπαίδευση συνήθως βελτιώνει τις επιδόσεις, μπορεί να οδηγήσει σε διαρροή πληροφοριών και συνεπώς να διακινδυνεύσει την ιδιωτικότητα των ατόμων που αναφέρονται στα δεδομένα κατάρτισης. Μια μέθοδος για να διασφαλιστεί η ιδιωτικότητα αυτών των ατόμων είναι να εκπαιδεύσει ένα διαφορετικό-ιδιωτικό μοντέλο, αλλά αυτό συνήθως έρχεται σε βάρος της απόδοσης του μοντέλου. Επιπλέον, είναι δύσκολο να πούμε δεδομένης της παραμέτρου απορρήτου $\epsilon$ ποια ήταν η επίδραση στην εκπαιδευμένη εκπροσώπηση. Σε αυτή την εργασία στοχεύουμε να καθοδηγήσουμε μελλοντικούς επαγγελματίες και ερευνητές σχετικά με το πώς να βελτιώσουμε την ιδιωτικότητα διατηρώντας παράλληλα την καλή απόδοση του μοντέλου. Επιδεικνύουμε πώς να εκπαιδεύσουμε ένα διαφοροποιημένο-ιδιωτικό προ-εκπαιδευμένο γλωσσικό μοντέλο (δηλαδή BERT) με εγγύηση απορρήτου $\epsilon=1$ και μόνο με μικρή υποβάθμιση της απόδοσης. Πειραματιζόμαστε σε ένα σύνολο δεδομένων κλινικών σημειώσεων με ένα μοντέλο εκπαιδευμένο σε μια εργασία εξαγωγής οντότητας στόχου, και το συγκρίνουμε με ένα παρόμοιο μοντέλο εκπαιδευμένο χωρίς διαφορική ιδιωτικότητα. Τέλος, παρουσιάζουμε πειράματα που δείχνουν πώς να ερμηνεύσουμε τη διαφοροποιημένη-ιδιωτική αναπαράσταση και να κατανοήσουμε τις πληροφορίες που χάνονται και διατηρούνται σε αυτή τη διαδικασία.</abstract_el>
      <abstract_kk>Контекстік тіл үлгілері тілді тапсырмаларды түсініп, өзгертілген кезде, төменгі тапсырманың бір мәліметінің алдында оқылған деректеріне көп жақсы нәтижелерін жасады. Бұл қосымша алдыңғы оқыту кәдімгі әрекеттерді жақсартқан болса, ол мәліметті ақпаратты түсіруге болады, сондықтан оқыту деректерінде айтылған адамдардың жеке тәуе Бұл адамдардың жеке тәуелсіздігін қамтамасыз ететін бір әдіс - әртүрлі жеке үлгісін оқыту, бірақ әдетте бұл үлгілі тәжірибесінің бағасы. Сонымен қатар, жұмыс істеу параметрін $\ epsilon$ дегенге қандай нәтижесін көрсету қиын. Бұл жұмыста біз болашақ тәжірибелерді және зерттеушілерді жақсы үлгі істеуді қалай жақсарту үшін қалай жақсарту үшін көмектесу мақсатымыз. Біз әрқайсыз жеке тіл үлгісін қалай оқытуды көрсетедік (мысалы, BERT) $\epsilon=1$ және тек кішкентай деградациялау үшін. Біз клиникалық жазбалардың деректер жиынына мәліметті мақсатты нысандарды тарқату тапсырмасына көмектесілген үлгілерімен тәжірибе және оларды әртүрлі жеке тәсілдік емес ұқсас Соңында, біз тәжірибелерді бұл процестің жоғалтылып қалған мәліметті түсініп, қалай түсініктерді көрсетеді.</abstract_kk>
      <abstract_ml>ഭാഷയിലുള്ള ഭാഷ മോഡലുകള്‍ ഭാഷ വിവേകത്തിന്റെ ജോലികളില്‍ ഏറ്റവും മെച്ചപ്പെട്ട ഫലങ്ങള്‍ കൊണ്ടുവന്നിരിക്കുന്നു. പ്രത്യേകിച്ച് ഡാ ഈ കൂടുതല്‍ പരിശീലനം സാധാരണ പ്രവര്‍ത്തനങ്ങള്‍ മെച്ചപ്പെടുത്തുമ്പോള്‍, അത് വിവരങ്ങളുടെ ലിക്കേജ് കാണിക്കും. അതുകൊണ്ട് പരിശീലനത് ഈ വ്യക്തികളുടെ സ്വകാര്യം ഉറപ്പ് വരുത്താന്‍ ഒരു രീതി പിന്നെ, ഒരു സ്വകാര്യ പരാമീറ്റര്‍ എപ്പിസിലോന്‍ ഡോളര്‍ കൊടുക്കുന്നത് എന്താണ് പരിശീലന പ്രധാനം. ഈ ജോലിയില്‍ നമ്മള്‍ ഭാവിയുടെ പരിശീലിക്കുന്നവരെയും ശ്രദ്ധിക്കുന്നവരെയും സ്വകാര്യം മെച്ചപ്പെടുത്തുന്നതിനെക്കു നമ്മള്‍ വ്യത്യാസ്ത്രീയ-സ്വകാര്യത്തില്‍ പരിശീലിക്കപ്പെട്ട ഭാഷ മോഡല്‍ പരിശീലിപ്പിക്കുന്നത് എങ്ങനെയാണെന്ന് പ്രദര്‍ശിപ്പിക്കുന്നു We experiment on a dataset of clinical notes with a model trained on a target entity extraction task, and compare it to a similar model trained without differential privacy.  Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.</abstract_ml>
      <abstract_lt>Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task.  Nors šis papildomas parengiamasis mokymas paprastai gerina veiklos rezultatus, jis gali sukelti informacijos nutekėjimą ir todėl gali kelti pavojų mokymo duomenise nurodytų asmenų privatumui. Vienas iš tokių asmenų privatumo užtikrinimo metodų yra mokyti skirtingai privatų model į, tačiau tai paprastai atsiranda modelio veiklos sąskaita. Be to, atsižvelgiant į privatumo parametrą $\epsilon$, sunku nustatyti, koks poveikis turėjo apmokytas atstovavimas. Šiame darbe siekiame orientuoti būsimus praktikantus ir mokslininkus į tai, kaip pagerinti privatumą ir kartu išlaikyti gerus modelio rezultatus. Mes parodome, kaip mokyti skirtingai privatų i š anksto parengtą kalbos model į (t. y. BERT), užtikrinant privatumą $\epsilon=1$ ir tik nedidelį veiklos pablogėjimą. We experiment on a dataset of clinical notes with a model trained on a target entity extraction task, and compare it to a similar model trained without differential privacy.  Galiausiai pristatome eksperimentus, rodančius, kaip aiškinti skirtingai privatų atstovavimą ir suprasti šiame procese prarastą ir išlaikytą informaciją.</abstract_lt>
      <abstract_ms>Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task.  While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data.  Satu kaedah untuk menjamin privasi individu seperti ini adalah untuk melatih model peribadi berbeza, tetapi ini biasanya datang pada biaya prestasi model. Moreover, it is hard to tell given a privacy parameter $\epsilon$ what was the effect on the trained representation.  In this work we aim to guide future practitioners and researchers on how to improve privacy while maintaining good model performance.  Kami menunjukkan bagaimana untuk melatih model bahasa yang terlatih secara berbeza-peribadi (i.e., BERT) dengan jaminan privasi $\epsilon=1$ dan hanya dengan degradation kecil dalam prestasi. Kami eksperimen pada set data nota klinik dengan model yang dilatih pada tugas ekstraksi entiti sasaran, dan membandingkannya dengan model yang sama dilatih tanpa privasi berbeza. Akhirnya, kami mempersembahkan eksperimen menunjukkan bagaimana untuk menerangkan perwakilan berbeza-peribadi dan memahami maklumat yang hilang dan disimpan dalam proses ini.</abstract_ms>
      <abstract_mt>Il-mudelli tal-lingwi kuntestwali wasslu għal riżultati sinifikanti a ħjar fuq għadd kbir ta’ kompiti ta’ fehim tal-lingwi, speċjalment meta mħarrġa minn qabel dwar l-istess dejta bħall-kompitu downstream. Filwaqt li dan it-taħriġ addizzjonali normalment itejjeb il-prestazzjoni, jista’ jwassal għal tnixxija ta’ informazzjoni u għalhekk jirriskja l-privatezza tal-individwi msemmija fid-dejta tat-taħriġ. Metodu wieħed li jiggarantixxi l-privatezza ta’ dawn l-individwi huwa t-taħriġ ta’ mudell differenzjalment privat, iżda dan ġeneralment jiġi bi spejjeż tal-prestazzjoni tal-mudell. Barra minn hekk, huwa diffiċli li wieħed jgħid meta wieħed iqis il-parametru tal-privatezza $\epsilon$ x’kien l-effett fuq ir-rappreżentanza mħarrġa. F’dan ix-xogħol għandna l-għan li niggwidaw lill-prattikanti u r-riċerkaturi futuri dwar kif tittejjeb il-privatezza filwaqt li tinżamm prestazzjoni tajba tal-mudell. Aħna nippruvaw kif in ħarrġu mudell tal-lingwa mħarreġ minn qabel (jiġifieri BERT) b’garanzija tal-privatezza ta’ $\epsilon=1$ u b’degradazzjoni żgħira biss fil-prestazzjoni. Aħna ninsperimentaw fuq sett ta’ dejta ta’ noti kliniċi b’mudell imħarreġ fuq kompitu ta’ estrazzjoni ta’ entità fil-mira, u nqabblu ma’ mudell simili mħarreġ mingħajr privatezza differenzjali. Fl-aħħar nett, qed nippreżentaw esperimenti li juru kif wieħed għandu jinterpreta r-rappreżentanza differenzjalment privata u jifhem l-informazzjoni mitlufa u miżmuma f’dan il-proċess.</abstract_mt>
      <abstract_pl>Kontekstowe modele językowe doprowadziły do znacznie lepszych wyników w wielu zadaniach rozumienia języka, zwłaszcza gdy są wstępnie przeszkolone na tych samych danych co zadanie kolejne. Chociaż takie dodatkowe szkolenie przedszkoleniowe zwykle poprawia wydajność, może prowadzić do wycieku informacji i w związku z tym zagraża prywatności osób wymienionych w danych treningowych. Jedną ze sposobów zagwarantowania prywatności takich osób jest trening modelu różnicowo-prywatnego, ale zwykle jest to kosztem wydajności modelu. Co więcej, trudno powiedzieć, biorąc pod uwagę parametr prywatności $\epsilon$ jaki był wpływ na przeszkoloną reprezentację. W niniejszej pracy chcemy wskazać przyszłym praktykom i badaczom, jak poprawić prywatność przy zachowaniu dobrej wydajności modelu. Pokazujemy, jak trenować różnicowo-prywatny model językowy (tj. BERT) z gwarancją prywatności $\epsilon=1$ i z niewielką degradacją wydajności. Eksperymentujemy na zbiorze danych notatek klinicznych z modelem przeszkolonym na zadaniu ekstrakcji jednostek docelowych i porównujemy go z podobnym modelem trenowanym bez różnicowej prywatności. Na koniec przedstawiamy eksperymenty pokazujące, jak interpretować różnicowo-prywatną reprezentację i zrozumieć informacje utracone i utrzymywane w tym procesie.</abstract_pl>
      <abstract_mk>Контекстуалните јазички модели доведоа до значително подобри резултати на многуте задачи за разбирање на јазикот, особено кога се предобучени на истите податоци како и понатамошната задача. И покрај тоа што оваа дополнителна предобука обично ја подобрува перформансата, таа може да доведе до протекување на информации и со тоа ризикува приватноста на поединците споменати во податоците за обука. Еден метод за гарантирање на приватноста на ваквите поединци е обуката на различно-приватен модел, но ова обично доаѓа на трошок на моделната изведба. Покрај тоа, тешко е да се каже со оглед на параметарот за приватност $\epsilon$ кој беше ефектот на обученото претставување. Во оваа работа ние имаме за цел да ги водиме идните практики и истражувачи како да ја подобриме приватноста и да одржиме добра резултат на моделот. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $\epsilon=1$ and with only a small degradation in performance.  We experiment on a dataset of clinical notes with a model trained on a target entity extraction task, and compare it to a similar model trained without differential privacy.  Конечно, претставуваме експерименти кои покажуваат како да се интерпретира диференцијално-приватното претставување и да се разбере информацијата изгубена и одржана во овој процес.</abstract_mk>
      <abstract_ro>Modelele lingvistice contextuale au condus la rezultate semnificativ mai bune în ceea ce privește o multitudine de sarcini de înțelegere a limbilor străine, mai ales atunci când sunt pre-instruite pe aceleași date ca și sarcina din aval. Deși această pregătire prealabilă suplimentară îmbunătățește, de obicei, performanța, poate duce la scurgeri de informații și, prin urmare, riscă confidențialitatea persoanelor menționate în datele de pregătire. O metodă de a garanta confidențialitatea acestor persoane este de a instrui un model diferențial-privat, dar acest lucru vine de obicei în detrimentul performanței modelului. Mai mult decât atât, este greu de spus dat fiind un parametru de confidențialitate $\epsilon$ care a fost efectul asupra reprezentării instruite. În această lucrare, ne propunem să ghidăm viitorii practicieni și cercetători cu privire la modul în care să îmbunătățim confidențialitatea, menținând în același timp performanța bună a modelului. Vă demonstrăm cum să instruiți un model de limbă pre-instruit diferențial privat (de exemplu, BERT) cu o garanție de confidențialitate de $\epsilon=1$ și cu doar o mică degradare a performanței. Experimentăm pe un set de date de note clinice cu un model instruit pe o sarcină de extragere a entității țintă și îl comparăm cu un model similar instruit fără confidențialitate diferențială. În cele din urmă, prezentăm experimente care arată cum să interpreteze reprezentarea diferențial-privată și să înțeleagă informațiile pierdute și menținute în acest proces.</abstract_ro>
      <abstract_no>Kontekst språk-modeller har ført til signifikante bedre resultat på ein plethora av språk-forståking av oppgåver, spesielt når før-treng på samme data som nedtrekkoppgåva. Selv om denne ekstra føreøvinga oftast forbetrar utviklinga, kan det føre til å løysa informasjon og derfor riskerer privateten av personer som er oppgjeve i opplæringsdata. Ein måte å garanti privatet på slike individuane er å trenja eit ulike privat modell, men dette vanlegvis kjem på utvidinga av modellen. I tillegg er det vanskeleg å fortelle eit privatsparameter $\epsilon$ kva var effekten på den trengte representasjonen. I dette arbeidet må vi hjelpa framtidige praktiseringar og forskere om korleis å forbetra privatet medan å beholda godt modell. Vi viser korleis å trenja eit forskjellig privat språk- modell (t.d. BERT) med eit privat garanti for $\epsilon=1$ og berre med e in liten degradasjon i utviklinga. Vi eksperimenterer på eit dataset med kliniske notat med eit modell trent på eit utpakking av målsettingsoppgåve, og sammenligner det med eit liknande modell trent utan forskjellig privat. I slutt presenterer vi eksperimenter som viser korleis å tolka den ulike private representasjonen og forstå informasjonen tapte og vedlikehalde i denne prosessen.</abstract_no>
      <abstract_mn>Сүүлийн үеийн хэл загварууд хэл ойлгох үйл ажиллагааны тухай илүү сайн үр дүн гаргасан, ялангуяа доорх үйл ажиллагаатай адилхан мэдээллийг суралцах үед. Энэ нэмэлт өмнөх сургалтын дасгал хөгжлийн үйл ажиллагааг сайжруулдаг ч, энэ нь мэдээллийг суулгах боломжтой. Иймээс сургалтын мэдээллээр хэлсэн хүмүүсийн хувийн амьдралыг эрсдэлтэй Нэг арга нь ийм хүмүүсийн хувийн амьдралыг баталгаалах нь өөр өөр хувийн загварын загварын төлөвлөгөө юм. Гэхдээ энэ нь ихэвчлэн загварын үйлдвэрлэлийн зардал дээр ирдэг. Үүнээс гадна хувийн амьдралын параметр $\epsilon$ өгсөн нь сургалтын үзүүлэлтийн нөлөө юу байсан бэ гэдгийг хэлэх нь хэцүү. Энэ ажил дээр бид ирээдүйн мэргэжилтнүүд болон судлаачид хэрхэн хувийн загварын үйл ажиллагааг сайжруулах талаар хэрхэн сайжруулах талаар удирдах зорилго байна. Бид өөр өөр өөр хувийн хэл дээр сургалтын загварыг хэрхэн суралцах вэ гэдгийг харуулж байна (т.е. BERT) $\epsilon=1$ болон үйлдвэрлэлд жижиг хэлбэртэй баталгаатай. Бид клиникийн нотуудын өгөгдлийн хэлбэр дээр зориулагдсан загварыг авах ажил дээр сургалтын загвартай туршиж, өөр өөр өөр загваргүй сургалтын төстэй загвартай харьцуулж байна. Эцэст нь, бид өөр өөр хувийн үзүүлэлтийг хэрхэн илэрхийлж, алдсан, хадгалагдсан мэдээллийг ойлгох туршилтуудыг харуулж байна.</abstract_mn>
      <abstract_so>Tusaalada luuqadda ku qoran waxay sababtay resulto aad u wanaagsan oo ku saabsan shaqada waxyaabaha garashada luuqada, khusuusan marka lagu baranayo iskuul ka horeysan isku mid ah macluumaadka shaqada hoose. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data.  Qofka shakhsiyadiisa ah waa in loo tababariyo tusaale gaar ah oo kala duduwan, laakiin sida caadiga ah waxaa ku imaan kharashka sameynta tusaale ahaan. Sidoo kale waa adag tahay in loo sheego lambarka gaarka loo leeyahay $ epsilon, waxa saamayn ku leh wakiilka tababarida. Shaqadaas waxaynu ku talo galaynaa in aan ku hogaano shaqo-bixiyayaasha mustaqbalka ah iyo cilmi-baaritaanka ku saabsan sida loo kordhiyo gaar ahaanshaha, marka loo haysto sameynta sameynta muusikada wanaagsan. Waxaannu muujinnaa sida aad u tababarido qaab kala duwan oo gaar loo leeyahay, tusaale ahaan BERT, kaas oo ah garashada gaarka loo leeyahay $ epsilon=1$ iyo in lagu sameynayo ceeb yar. Waxaan ku tijaabinaynaa qoraal macluumaad dhakhaatiirta ah oo ku qoran qaab lagu baray shaqo ka soo bixinta jidhka goalka, waxaynu isbarbareynaa tusaale la mid ah oo lagu baray oo aan gaar u lahayn. Ugu dambaysta waxaan soo bandhignaa imtixaamo aan tusnaynaa sida loo turjumo wakiilka gaarka loo leeyahay iyo sidoo kale ayaynu garanaynaa macluumaadka ka lumaya iyo la sii haystay markan.</abstract_so>
      <abstract_sv>Kontextuella språkmodeller har lett till betydligt bättre resultat på en uppsjö av språkförståelse uppgifter, särskilt när de är förberedda på samma data som nedströmsuppgiften. Även om denna extra fortbildning vanligtvis förbättrar prestationen kan den leda till informationsläckage och därmed riskerar privatlivet för de personer som nämns i utbildningsuppgifterna. En metod för att garantera privatlivet för sådana individer är att träna en differentialt privat modell, men detta sker vanligtvis på bekostnad av modellprestanda. Dessutom är det svårt att säga med en sekretessparameter $\epsilon$ vad som var effekten på den utbildade representationen. I detta arbete syftar vi till att vägleda framtida praktiker och forskare om hur man kan förbättra integriteten samtidigt som man upprätthåller goda modellprestanda. Vi visar hur man tränar en differentialt privat förklädd språkmodell (dvs BERT) med en sekretessgaranti på $\epsilon=1$ och med endast en liten försämring av prestanda. Vi experimenterar på en dataset med kliniska anteckningar med en modell utbildad på en målentitetsextraktionsuppgift, och jämför den med en liknande modell utbildad utan differentierad sekretess. Slutligen presenterar vi experiment som visar hur man tolkar den differentialt privata representationen och förstår den information som förlorats och bevarats i denna process.</abstract_sv>
      <abstract_ta>உள்ளடக்கமான மொழி மாதிரிகள் முழு மொழி புரிந்து கொள்ளும் பணிகளின் மேல் மிகவும் நல்ல முடிவு இந்த கூடுதல் முன் பயிற்சி வழக்கமாக செயல்பாட்டை மேன்மைப்படுத்தும் போது, அது தகவல் ஒட்டுதலை வழிகாட்டும் அதனால் பயிற்சி தகவலி இத்தகைய தனிப்பட்ட மாதிரியை பயிற்சி செய்ய ஒரு முறையாக, ஆனால் இது வழக்கமாக மாதிரி செயல்பாட்டின் செலவில் வருகிறது. மற்றும், ஒரு தனிப்பட்ட அளபுருவு $epsilon டாலர் கொடுக்கப்பட்டுள்ளது என்று சொல்ல மிகவும் கடினம். இந்த வேலையில் நாம் எதிர்கால பயிற்சியாளர்களையும் ஆராய்ச்சியாளர்களையும் செலுத்தும் போது நல்ல மாதிரி செயல்பாட நாம் எப்படி வேறுபட்ட தனிப்பட்ட பயிற்சி முன்பயிற்சி மொழி மாதிரியை பயிற்சி செய்ய வேண்டும் என்பதை காட்டுகிறோம் $epsilon=1$ மற்றும் செயல்பாட்டில் சி நாம் ஒரு சில குறிப்புகளின் தகவல் அமைப்பில் சோதனைப்படுத்தப்பட்டுள்ளோம் இலக்கு பொருள் பிரிப்பு பணியில் பயிற்சிக்கப்பட்ட மாதிரி ம இறுதியில், நாம் இந்த செயலில் இழந்து விட்டது மற்றும் பாதுகாக்கப்பட்ட தகவலை புரிந்து கொள்ளும் வித்தியாசமான தனிப்பட்</abstract_ta>
      <abstract_si>සම්බන්ධ භාෂාව මොඩේල් එක්ක වඩා හොඳ ප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප මේ විශේෂ ප්‍රීක්ෂණය සාමාන්‍යයෙන්ම ප්‍රීක්ෂණය වැඩ කරන්න පුළුවන්, ඒක තොරතුරු ප්‍රීක්ෂණය කරලා තියෙන්න පුළු ඒ වගේම ප්‍රතිකාරයෝ ගැන පුද්ගලිකතාවක් ගැන ආරක්ෂා කරන්න එක විදියට වෙනස් විදිහට පුද්ගලික විදිහට පුද්ගලික ව ඉතින්, පුද්ගලික ප්‍රමාණයක් දෙන්න $\epsilon$ කියලා කියන්න අමාරුයි. මේ වැඩේ අපි අදහස් කරනවා අනාගතයේ ප්‍රේක්ෂකයෝ සහ පරීක්ෂකයෝ ගැන කොහොමද පෞද්ගලිකවත් වැඩ කරන්නේ හොඳ මොඩ අපි ප්‍රදර්ශනය කරනවා කොහොමද වෙනස් විදිහට පුද්ගලික විදිහට පුද්ගලික විදිහට පුද්ගලික භාෂා මොඩේලයක් (ඉතින්, BERT) එක්ක $\epsilon=1$ ගැ අපි පරීක්ෂණය කරන්නේ ප්‍රශ්නයක් තියෙන ප්‍රශ්නයක් තියෙන්නේ ඉලක්ෂාත්මක ප්‍රශ්නයක් ප්‍රශ්නයක් තියෙන දත්ත සූද්ධ අන්තිමට, අපි පරීක්ෂණාවල් පෙන්වන්න පුළුවන් වෙනස් පුද්ගලික ප්‍රතිචාරයක් කොහොමද කියලා පෙන්වන්නේ, ඒ</abstract_si>
      <abstract_ur>کنٹکسٹیول زبان موڈل نے زبان سمجھنے کے کاموں پر بہترین نتیجے بنائے ہیں، مخصوصا جب ڈانٹروم کے کام کے مطابق پہلے دکھائے جاتے ہیں۔ اگرچہ یہ اضافہ پیش آموزش کی تعلیم معمولاً عملکرد کو اضافہ کرتی ہے، اس کے ذریعہ یہ معلومات کی اضافہ کرتی ہے اور اس کے نتیجہ میں تدریس ڈیٹ میں ذکر ہوئے شخصوں کی خصوصی کا خطر ڈالت ایک طریقہ ایسے شخصوں کی خصوصی کا امن کرنا ہے کہ ایک مختلف طریقے سے خصوصی موڈل کی آموزش کرنا ہے، لیکن یہ معمولاً موڈل کی عملکرد کے خرچ پر آتا ہے. اور اس کے علاوہ، ایک خصوصی پارامیٹ $\epsilon$ کو بتانے کا مشکل ہے کہ تعلیم کی تعلیم پر کیا اثر تھا. اس کام میں ہم مطابق مستقبل کارگروں اور تحقیقات کرنے والوں کو راہ دکھانے کا ارادہ رکھتے ہیں کہ کس طرح خصوصی کا تدبیر کریں جب وہ اچھی مدل کی عملکرد حفاظت کریں۔ ہم نشان دیتے ہیں کہ $\epsilon=1$ کی خصوصی طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طر ہم کلینیکی نوٹوں کے ایک ڈیٹ سٹ پر ایک موڈل کے ساتھ تدریس کیا گیا ہے ایک موڈل کے ذریعہ ایک ٹیٹ ایٹیٹ کے اخراج کام پر، اور اسے ایک ایسی موڈل کے مطابق تدریس کیا گیا ہے جو مختلف خصوصی کے ب آخر میں ہم تجربے پیش کرتے ہیں کہ کس طرح مختلف طریقے سے خصوصی نمونات کی تعبیر کریں اور اس طریقے میں خسارہ اور حفاظت کی معلومات کو سمجھیں۔</abstract_ur>
      <abstract_sr>Kontekstualni jezički modeli su doveli do značajno boljih rezultata na razumevanje jezičkih zadataka, posebno kada su predobučeni na istim podacima kao i zadatak koji se nalazi u potpunosti. Iako ova dodatna predobuka obično poboljšava učinkovitost, može dovesti do procurenja informacija i stoga rizikuje privatnost pojedinaca spominjenih u podacima o obuci. Jedan metod da garantuje privatnost takvih pojedinaca je trenirati diferencijalni privatni model, ali to obično dolazi na troškove model a izvršnosti. Osim toga, teško je reći s obzirom na parameter privatnosti $ epsilon$ koji je bio uticaj na obučeno predstavljanje. U ovom poslu ciljamo voditi buduće praktičnike i istraživače kako poboljšati privatnost dok održavamo dobre modele. Pokazujemo kako da treniramo diferencijalno-privatni predobučeni jezički model (tj. BERT) sa garancijom privatnosti od $\epsilon=1$ i sa samo malim degradacijom u izvedbi. Eksperimentiramo na setu podataka kliničkih nota sa modelom obučenim na zadatku izvlačenju ciljnih entiteta i uspoređujemo je sa sličnim modelom obučenim bez diferencijalne privatnosti. Konačno predstavljamo eksperimente kako da interpretiramo diferencijalno-privatno predstavljanje i razumemo izgubljene i održane informacije u ovom procesu.</abstract_sr>
      <abstract_vi>Kiểu ngôn ngữ tương ứng đã dẫn đến kết quả tốt hơn nhiều so với một loạt các công việc hiểu biết ngôn ngữ, đặc biệt khi được huấn luyện trước với những dữ liệu tương tự với các công việc xuôi dòng. Trong khi sự bổ sung tiền đào tạo này thường cải thiện hiệu suất, nó có thể dẫn tới rò rỉ thông tin và do đó nguy hiểm tính riêng tư của cá nhân được nhắc đến trong dữ liệu huấn luyện. Một phương pháp bảo đảm sự riêng tư cho những cá nhân đó là đào tạo một mô hình riêng biệt, nhưng thường là làm tổn hại đến khả năng của mô hình. Hơn nữa, dựa vào tham số riêng tư thì rất khó xác định được hiệu quả của sự đóng góp được huấn luyện. Trong công việc này chúng tôi hướng dẫn các chuyên gia và nghiên cứu tương lai về cách cải thiện sự riêng tư trong khi duy trì các mô hình tốt. Chúng tôi chỉ cách huấn luyện một mô hình ngôn ngữ được đào tạo riêng tư (gọi là BERT) với một bảo đảm an ninh riêng tư cho tên là tham nhũng\\ epsilon=1., và chỉ với một sự giảm sút giảm trình độ nhỏ. Chúng tôi thí nghiệm trên một tập tin ghi chú lâm sàng với một mô hình được đào tạo về một nhiệm vụ thực thể đích, và so sánh nó với một mô hình tương tự được huấn luyện mà không có riêng tư. Cuối cùng, chúng tôi có thí nghiệm cho thấy cách phân tích sự phân biệt chủng tộc riêng tư và hiểu được thông tin bị mất và duy trì trong quá trình này.</abstract_vi>
      <abstract_uz>@ info: whatsthis Ushbu qoʻshimcha taʼminlovchi vaqtda oddiy vazifani bajarishi mumkin, bu maʼlumot yozib olish mumkin va shunday qilib, taʼminlovchi maʼlumotdagi odamlarning shaxsiyatlarini xavfsiz qiladi. Bu odamlarning shaxsiyatlarini tasdiqlash uchun bir usul, boshqa shaxsiy modelni o'rganish mumkin, ammo bu odatda model bajarishning qiymatida keladi. Ko'rsatilgan, epsilon dollarning shaxsiy parametrlarini aytish juda qiyin edi. Bu vazifanda biz kelajakdagi ishlatuvchilarni va taʼminlovchilarni yaxshi model bajarishni davom etishda shaxsiyatlarni yaxshi ko'ramiz. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $\epsilon=1$ and with only a small degradation in performance.  Biz ma'lumotlar taʼminlovchi taʼminotlar soni taʼminlovchi modeli bilan o'rganamiz va uni o'xshash shaklga o'rganish modeliga kamaytirish mumkin. Oxirgi, biz o'zgarishni o'rganishni o'rganamiz va bu jarayonda qanday o'zgarishni o'rganishni ko'rsatamiz va qanday o'zgarishni o'rganamiz.</abstract_uz>
      <abstract_da>Kontekstuelle sprogmodeller har ført til betydeligt bedre resultater på en lang række sprogforståelsesopgaver, især når de er prætrænet på de samme data som downstream-opgaven. Selv om denne yderligere forudtræning normalt forbedrer ydeevnen, kan det føre til informationslækage og derfor risikerer privatlivets fred for personer, der er nævnt i træningsdataene. En metode til at garantere privatlivets fred for sådanne personer er at træne en differentialt privat model, men dette sker normalt på bekostning af modellens ydeevne. Desuden er det svært at sige på grund af en privatlivsparameter $\epsilon$ hvad der var effekten på den uddannede repræsentation. I dette arbejde har vi til formål at vejlede fremtidige praktikere og forskere i, hvordan man kan forbedre privatlivets fred, samtidig med at man opretholder en god model performance. Vi demonstrerer, hvordan man træner en differentialt privat præuddannet sprogmodel (dvs. BERT) med en privatlivsgaranti på $\epsilon=1$ og med kun en lille forringelse af ydeevnen. Vi eksperimenterer på et datasæt af kliniske noter med en model trænet på en målenhedsekstraktionsopgave, og sammenligner den med en lignende model trænet uden differentieret privatliv. Endelig præsenterer vi eksperimenter, der viser, hvordan man fortolker den differentialt-private repræsentation og forstår de oplysninger, der mistes og vedligeholdes i denne proces.</abstract_da>
      <abstract_nl>Contextuele taalmodellen hebben geleid tot significant betere resultaten op een overvloed aan taalbegrijpingstaken, vooral wanneer vooraf getraind op dezelfde gegevens als de downstream taak. Hoewel deze aanvullende pre-training meestal de prestaties verbetert, kan het leiden tot informatielekken en daardoor de privacy van personen die in de trainingsgegevens worden genoemd, in gevaar brengen. Een methode om de privacy van dergelijke individuen te waarborgen is om een differentieel-privé model te trainen, maar dit gaat meestal ten koste van de prestaties van het model. Bovendien is het moeilijk te zeggen met een privacyparameter $\epsilon$ wat het effect was op de getrainde vertegenwoordiging. In dit werk willen we toekomstige beoefenaars en onderzoekers begeleiden hoe ze privacy kunnen verbeteren met behoud van goede modelprestaties. We demonstreren hoe je een differentieel-privaat voorgetraind taalmodel (BERT) kunt trainen met een privacygarantie van $\epsilon=1$ en met slechts een kleine achteruitgang in prestaties. We experimenteren met een dataset klinische aantekeningen met een model dat getraind is op een doelentiteitsextractie taak, en vergelijken het met een vergelijkbaar model dat getraind is zonder differentiële privacy. Tot slot presenteren we experimenten die laten zien hoe de differentieel-private representatie kan worden geïnterpreteerd en de informatie die verloren gaat en bewaard wordt in dit proces te begrijpen.</abstract_nl>
      <abstract_hr>Kontekstualni jezički modeli doveli su do značajno boljih rezultata na razumijevanje jezika zadataka, posebno kada su predobučeni na istim podacima kao i zadatak koji se nalazi u potpunosti. Iako ova dodatna predobuka obično poboljšava učinkovitost, može dovesti do curenja informacija i stoga rizikuje privatnost pojedinaca spomenute u podacima o obuci. Jedan način za garantiranje privatnosti takvih pojedinaca je obučavanje različitih privatnih model a, ali to obično dolazi na troškove modelnog izvođenja. Osim toga, teško je reći s obzirom na parameter privatnosti $\epsilon$ koji je bio učinak na obučeno predstavljanje. U ovom poslu ciljamo voditi buduće praktičnike i istraživače kako poboljšati privatnost dok održavamo dobru modelsku funkciju. Mi pokazujemo kako trenirati diferencijalno-privatni predobučeni jezički model (tj. BERT) sa garancijom privatnosti od $\epsilon=1$ i samo malim degradacijom u izvedbi. Eksperimentiramo na skupu podataka kliničkih bilješaka s modelom obučenim na zadatku izvlačenju ciljnih subjekta i uspoređujemo je s sličnim modelom obučenim bez diferencijalne privatnosti. Konačno predstavljamo eksperimente kako interpretirati diferencijalno-privatno predstavljanje i razumijeti izgubljene i održane informacije u ovom procesu.</abstract_hr>
      <abstract_bg>Контекстуалните езикови модели са довели до значително по-добри резултати при множество задачи за разбиране на езика, особено когато са предварително обучени по същите данни като задачата надолу по веригата. Въпреки че това допълнително предварително обучение обикновено подобрява ефективността, то може да доведе до изтичане на информация и следователно рискува неприкосновеността на личния живот на лицата, посочени в данните за обучение. Един от методите за гарантиране на неприкосновеността на личния живот на такива лица е да се обучи диференциално-частен модел, но това обикновено идва за сметка на производителността на модела. Освен това е трудно да се каже, като се има предвид параметър за поверителност $\epsilon$ какъв е бил ефектът върху обучението представяне. В тази работа ние се стремим да насочим бъдещите практикуващи и изследователи как да подобрим неприкосновеността на личния живот, като същевременно поддържаме добро представяне на модела. Ние демонстрираме как да тренираме диференциално частен предварително обучен езиков модел (т.е. БЕРТ) с гаранция за поверителност $\epsilon=1$ и само с малко влошаване на ефективността. Експериментираме върху набор от клинични бележки с модел, обучен за задача за екстракция на целеви субекти, и го сравняваме с подобен модел, обучен без диференциална поверителност. Накрая, представяме експерименти, показващи как да интерпретираме диференциално-частното представяне и да разберем загубената и поддържана информация в този процес.</abstract_bg>
      <abstract_fa>مدل‌های زبان متوسط به نتیجه‌های زیادی بهتر روی یک قسمت از کارهای درک زبان، مخصوصا وقتی پیش از این روی داده‌های همانند کار پایین آموزش داده می‌شود. در حالی که این آموزش پیش از آموزش بیشتری معمولاً عملکرد را بهتر می‌کند، می‌تواند به تغییر دادن اطلاعات هدایت کند و به همین دلیل خطر خصوصی شخصی که در داده‌های آموزش اشاره می‌شود، خ یک روش برای تضمین خصوصی این افراد این است که یک مدل خصوصی و متفاوتی را آموزش دهند، اما این معمولا به خرج عملکرد مدل می رسد. علاوه بر این، برای دادن یک پارامتر خصوصی $ epsilon$ تاثیر روی نمایش آموزش آموزش چیست سخت است. در این کار ما هدف داریم که آموزگاران و محققان آینده را راهنمایی کنیم که چگونه بهتر کردن خصوصی در زمان حفظ عملکرد مدل خوب باشند. ما نشان می دهیم که چگونه یک مدل پیش آموزش زبان مختلف و خصوصی را آموزش دهیم (یعنی BERT) با تضمین خصوصی از $ epsilon=1$ و تنها با یک کم نابودی در اجرایی. ما روی یک مجموعه داده‌های نوشته‌های کلینیک آزمایش می‌کنیم با یک مدل آموزش داده شده روی یک کار خارج کردن هدف، و آن را با یک مدل مانند آموزش داده شده بدون خصوصی متفاوتی مقایسه می‌کنیم. بالاخره، ما آزمایشات را نشان می دهیم که چگونه تعبیر نمایش خصوصی متفاوتی را انجام دهند و اطلاعات را درک کنیم که در این فرایند گم شده و نگه دارند.</abstract_fa>
      <abstract_id>Model bahasa konteks telah menyebabkan hasil yang jauh lebih baik pada banyak tugas pemahaman bahasa, terutama ketika dilatih-dilatih pada data yang sama dengan tugas downstream. Sementara prapelatihan tambahan ini biasanya meningkatkan prestasi, itu bisa menyebabkan kebocoran informasi dan karena itu risiko privasi individu yang disebut dalam data pelatihan. Satu metode untuk menjamin privasi orang-orang seperti itu adalah untuk melatih model yang berbeda-pribadi, tetapi ini biasanya datang pada biaya prestasi model. Selain itu, sulit untuk mengatakan mengingat parameter privasi $\epsilon$ apa efek pada representation terlatih. In this work we aim to guide future practitioners and researchers on how to improve privacy while maintaining good model performance.  Kami menunjukkan bagaimana untuk melatih model bahasa pre-dilatih berbeda-pribadi (i.e., BERT) dengan jaminan privasi $\epsilon=1$ dan hanya dengan degradasi kecil dalam prestasi. Kami eksperimen pada set data catatan klinis dengan model yang dilatih pada tugas ekstraksi entitas sasaran, dan membandingkannya dengan model yang sama dilatih tanpa privasi berbeda. Akhirnya, kami mempersembahkan eksperimen menunjukkan bagaimana untuk menerjemahkan representation berbeda-pribadi dan memahami informasi yang hilang dan terus dalam proses ini.</abstract_id>
      <abstract_de>Kontextbasierte Sprachmodelle haben zu deutlich besseren Ergebnissen bei einer Vielzahl von Sprachverstﾃ､ndnisaufgaben gefﾃｼhrt, insbesondere wenn sie auf denselben Daten wie die nachgelagerte Aufgabe vortrainiert wurden. Wﾃ､hrend dieses zusﾃ､tzliche Vortraining in der Regel die Leistung verbessert, kann es zu Informationslecks fﾃｼhren und somit die Privatsphﾃ､re der in den Trainingsdaten genannten Personen gefﾃ､hrden. Eine Methode, um die Privatsphﾃ､re solcher Personen zu gewﾃ､hrleisten, besteht darin, ein differentiell-privates Modell zu trainieren, was jedoch in der Regel zu Lasten der Modellleistung geht. Darﾃｼber hinaus ist es schwer zu sagen, wenn man einen Datenschutzparameter $\epsilon$ verwendet, was der Effekt auf die trainierte Reprﾃ､sentation war. In dieser Arbeit mﾃｶchten wir zukﾃｼnftige Praktiker und Forscher dazu fﾃｼhren, wie sie die Privatsphﾃ､re verbessern und gleichzeitig eine gute Modellleistung beibehalten kﾃｶnnen. Wir zeigen, wie man ein differenziell-privates vortrainiertes Sprachmodell (z.B. BERT) mit einer Datenschutzgarantie von $\epsilon=1$ und mit nur geringem Leistungsverlust trainiert. Wir experimentieren an einem Datensatz klinischer Notizen mit einem Modell, das auf eine Zielentitﾃ､tsextraktionsaufgabe trainiert wurde, und vergleichen es mit einem ﾃ､hnlichen Modell, das ohne differentielle Privatsphﾃ､re trainiert wurde. Abschlieﾃ歹nd stellen wir Experimente vor, die zeigen, wie man die differentiell-private Reprﾃ､sentation interpretiert und die dabei verlorenen und erhaltenen Informationen versteht.</abstract_de>
      <abstract_af>Konteksual taal modelles het beter resultate gelei op 'n plethora van taal verstaan opdragte, veral wanneer voorafgelei word op dieselfde data as die onderstreem opdrag. Alhoewel hierdie addisionele voorvoerring gewoonlik die prestasie verbeter, kan dit lei na inligting uitlei en daarom riskeer die privateit van individuele wat in die onderwerp data ingementioneer is. Een metode om die privateit van sodanige individue te garanteer is om 'n verskillende-privaat model te trein, maar hierdie kom gewoonlik op die koste van model prestasie. Ook, dit is moeilik om 'n privateitsparameter gegee te vertel $\epsilon$ wat was die effek op die opgelei voorstelling. In hierdie werk doen ons doel om toekomstige praktisers en ondersoekers te lei oor hoe om privateit te verbeter terwyl ons goeie model prestasie onderhou. Ons wys hoe om 'n verskillende-privaat voor-onderwerp taal model te tref (i.e. BERT) met 'n privateitsgarantie van $\epsilon=1$ en met slegs 'n klein afbreiding in prestasie. Ons eksperimenteer op 'n dataset van kliniske notas met 'n model wat op 'n doel entiteit uittrek taak opgelei is, en vergelyk dit met 'n gelyklike model wat onderwerp is sonder verskillende privateit. Eindelik, ons voorstel eksperimente wat wys hoe om die verskillende-privaat verteenwoordigheid te interpreteer en verstaan die inligting verloor en onderhou in hierdie proses.</abstract_af>
      <abstract_ko>언어 환경 언어 모델은 대량의 언어 이해 임무에서 현저한 효과를 거두었고 특히 하류 임무와 같은 데이터에서 예비 훈련을 할 때 현저한 효과를 거두었다.이런 추가 예비 교육은 통상적으로 실적을 높일 수 있지만, 정보 유출을 초래하여 교육 데이터에 언급된 개인 프라이버시를 위태롭게 할 수 있다.이러한 개체의 프라이버시를 보장하는 방법의 하나는 서로 다른 사유 모델을 훈련시키는 것이지만, 이것은 통상적으로 모델의 성능을 희생하는 대가이다.또한 프라이버시 파라미터 $\epsilon$을 지정하면 훈련을 거친 표시에 어떤 영향을 미치는지 판단하기 어렵다.이 업무에서, 우리는 미래의 종사자와 연구원들이 양호한 모델 성능을 유지하는 동시에 프라이버시를 개선하는 방법을 지도하는 데 목적을 두고 있다.프라이버시 보장이 $\epsilon=1$인 상황에서 성능이 약간 떨어진 상황에서 서로 다른 개인 예훈련 언어 모델(즉 BERT)을 훈련하는 방법을 보여 드리겠습니다.우리는 임상 노트 데이터 집합에서 실험을 진행하여 목표 실체 추출 임무에서 훈련하는 모델을 사용하고 프라이버시 차이가 없는 상황에서 훈련하는 유사한 모델과 비교했다.마지막으로 우리는 이러한 차이점을 어떻게 설명하는지, 그리고 이 과정에서 잃어버리고 보존된 정보를 어떻게 이해하는지 실험을 통해 보여 주었다.</abstract_ko>
      <abstract_am>የቋንቋ ምሳሌዎች በቋንቋ ማስተዋል ስራቶች ላይ እጅግ የበለጠ ፍሬዎችን አግኝተዋል፤ በተለይም በሙሉ ዳታዎችን እንደ ታችኛው ስራ አስቀድሞ ተማርቷል፡፡ While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data.  One method to guarantee the privacy of such individuals is to train a differentially-private model, but this usually comes at the expense of model performance.  በተጨማሪም፣ የግል ብሔራዊ ተሟጋቾች የኢፌስሊዮን ዶላር ማድረግ በጣም ችግር ነው፡፡ በዚህ ስራ የፊተኛውን ባለማሪዎች እና አስተማሪዎችን መልካም ሞዴል ፈቃድ በመጠበቅ የግል ብሔርነታቸውን እንዴት እንዲያሳድግ እናሳውቃለን፡፡ የግል ልዩ-የፊተኛ የፊተኛውን የቋንቋ ምሳሌ (BERT) እንዳስተማርነው እናሳያቸዋለን፡፡ በተለየ ብልሽነት ሳይኖር በተማረ አካባቢ ስራ ላይ የተማረ ሞዴል እናስተያየዋለን፡፡ በመጨረሻም፣ የግል ብሔራዊ መልዕክት እንዴት እንዲተርጉም እና በዚህ ፕሮጀክት የጠፋውንና የተጠበቀውን መረጃ እናስተውል ዘንድ የምናሳየው ፈተናዎች እናቀርባለን፡፡</abstract_am>
      <abstract_sw>Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task.  Wakati mafunzo haya ya zamani mara nyingi huwa yanaboresha utendaji wa mafunzo, inaweza kusababisha utoaji wa habari na kwa hiyo hatarisha faragha ya watu waliotajwa katika takwimu za mafunzo. Utawala mmoja wa kuhakikisha faragha ya watu hawa ni kufundisha muundo wa binafsi tofauti, lakini mara nyingi huu huja kwa gharama ya utendaji wa mifano. Zaidi, ni vigumu kuelezea kipimo cha kipimo cha faragha cha dola za epsilon kilichotokana na uwakilishi wa mafunzo. Katika kazi hii tunalenga kuwaongoza wataalamu na watafiti wa siku za uso wa namna ya kuboresha faragha wakati wa kuendelea utendaji wa mifano mazuri. Tunaonyesha namna ya kufundisha mtindo wa lugha binafsi wa kujifunza tofauti (yaani BERT) ambao una uhakika binafsi wa kiasi cha $epsilon=1 na kwa kiwango kidogo tu cha fedha cha utendaji. Tunajaribu kwenye seti ya taarifa za nota za kliniki na modeli inayofundishwa kwenye kazi ya utekelezaji wa vifaa vya lengo, na kulinganisha na mtindo uliofanana bila faragha tofauti. Mwisho, tunatoa majaribio yanayoonyesha namna ya kutafsiri uwakilishi binafsi tofauti na kuelewa taarifa zilizopoteza na kudhibitiwa katika mchakato huu.</abstract_sw>
      <abstract_tr>Metin dil nusgalary dillerini düşünmek üçin has gowy netijesi bolup geçirdi, ýöne-de durmuşy a şak täzelikleri bilen öňünden öňünden boşadylýanda. Bu ýene-de öňki öňki öňki okuw adatça ukyplary gowurap bilýän bolsa, bu şekilde maglumatlary çykyp bilýär we şonuň üçin okuw maglumatynda aýdylan adamlaryň hususiyetini riske edip biler. Böyle adamlaryň ýalňyşlygyny goramak üçin bir ýoly üýtgeşik nusgasyny üýtgetmekdir, ýöne bu köplenç nusgasy üçin bir nusgasy bar. Ayrıca, $\epsilon$ kişisel bir parameter verilmek zor. Bu işde gelejekde praktikantlary we arkadaşlaryny gowy nusgasyny goramak üçin ýüzünligini gowy nusgasyny ýüzeltmäge maksadyk edýäris. Biz üýtgeşik, a ýratyn-aýratyn öň-bilim dili nusgasyny (meseläm, BERT) $\epsilon=1$ diňe kiçi bir wezamlyk bilen üýtgetmelidigini görkeýäris. Biz kliniki notlarda bir nusga taýýarlanan bir nusga taýýarlanan bir nusga bilen test edip, muny farklı hususiyatsyz üçin üýtgedilen bir nusga karşılaştyrýarys. Soňunda, biz munyň düýbünden aýratyn-aýratyn suratyny nädip terjime etmelidigini we bu prosesde ýiten we saglanýan informasiýany düşünjegimizi görkeýäris.</abstract_tr>
      <abstract_hy>Կոնտեքստային լեզվի մոդելները հանգեցրել են լեզվի հասկանալու խնդիրների բազմաթիվ ավելի լավ արդյունքների, հատկապես երբ նախապատրաստված են նույն տվյալների վրա, ինչպիսիք են վերջնական խնդիրները: Մինչդեռ այս ավելացյալ նախապատրաստման արդյունքը սովորաբար բարելավում է արդյունքը, այն կարող է հանգեցնել տեղեկատվության արտահոսքի և, հետևաբար, վտանգ տալ կրթության տվյալներում նշված անհատների գաղտնիության Այսպիսի անհատների գաղտնիությունը երաշխավորելու մեթոդը տարբերականորեն-մասնավոր մոդելն է, բայց սա սովորաբար արժե մոդելների արտադրողության վրա: Ավելին, դժվար է ասել, եթե հաշվի առնենք $\Epsilon$ անձնական գործընթացը, թե ինչ ազդեցություն ուներ վարժեցված ներկայացման վրա: Այս աշխատանքի ընթացքում մենք նպատակով ենք ուղղորդել ապագա փորձագետներին և հետազոտողներին, թե ինչպես բարելավել գաղտնիությունը, մինչդեռ պահպանել լավ մոդելներ: Մենք ցույց ենք տալիս, թե ինչպես վարժեցնել տարբերականորեն-մասնավոր նախապատրաստված լեզվի մոդելը (այսինքն, BER-ը)  $\Epsilon=1 դոլարով գաղտնիքի ապահովությամբ և միայն փոքր դեգրադացիայի արդյունքում: Մենք փորձում ենք կլինիկական նոտաների տվյալների համակարգի վրա մի մոդելի հետ, որը պատրաստված է նպատակային էակի վերացման խնդրի վրա, և համեմատում ենք այն նման մոդելի հետ, որը պատրաստված է առանց տարբերակային մասնավորության: Վերջապես, մենք ներկայացնում ենք փորձեր, որոնք ցույց են տալիս, թե ինչպես մեկնաբանել տարբերականորեն-մասնավոր ներկայացումը և հասկանալ այս գործընթացի ընթացքում կորցված և պահպանված տեղեկատվությունը:</abstract_hy>
      <abstract_bn>বিভিন্ন ভাষার মডেল অনেক ভালো ফলাফলের কারণে প্রাপ্ত হয়েছে, বিশেষ করে যখন প্রশিক্ষণের পূর্বে ডাউট্রিম কাজের মতো প্রশিক্ষিত হয়। এই প্রশিক্ষণ সাধারণত পূর্ব প্রশিক্ষণের কার্যক্রম উন্নতি প্রদান করে, তথ্য লিকেজের ফলে তথ্য প্রদর্শন করা যাবে এবং তাই প্রশিক্ষণের তথ্য এই ধরনের ব্যক্তিদের গোপনীয়তা নিশ্চিত করার একটি পদ্ধতি হচ্ছে একটি আলাদা ব্যক্তিগত মডেল প্রশিক্ষণ করার জন্য, কিন্তু এটা সাধারণত মডেলের প্রদর এছাড়াও, প্রশিক্ষিত প্রতিনিধিত্বের প্রভাব কি হয়েছে তা বলা কঠিন। এই কাজে আমরা ভবিষ্যতের প্রশিক্ষকদের এবং গবেষকদের পরিচালিত করার উদ্দেশ্য হচ্ছি যে ভাল মডেলের প্রদর্শন নিয়ে কিভাবে গোপনীয়তা উন আমরা প্রদর্শন করি কিভাবে বিভিন্ন ভাষায় প্রশিক্ষিত ভাষার মডেল (যেমন বিরেটি) প্রদর্শন করা হয়েছে যার ব্যক্তিগত নিশ্চিত $ এপিসিলোন=১ মার্কিন ডলার এবং প্রদর্শনে  আমরা একটি ক্লিনিক্যাল নোটের ডাটাসেটে পরীক্ষা করছি যেখানে একটি টার্গেট বিনিময়ের কাজে প্রশিক্ষিত মডেল প্রশিক্ষণ করা হয়েছে, আর এটিকে তুলনা করি  Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.</abstract_bn>
      <abstract_sq>Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task.  While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data.  Një metodë për të garantuar privatësinë e individëve të tillë është të trajnohet një model ndryshe-privat, por kjo zakonisht vjen në shpenzim të shfaqjes së modelit. Përveç kësaj, është e vështirë të tregohet me një parametrë privatësie $\epsilon$ çfarë ishte efekti në përfaqësimin e stërvitur. Në këtë punë ne synojmë të udhëzojmë praktikantët e ardhshëm dhe kërkuesit se si të përmirësojmë privatësinë duke mbajtur performancën e mirë të modelit. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $\epsilon=1$ and with only a small degradation in performance.  Ne eksperimentojmë në një grup të dhënash të shënimeve klinike me një model të stërvitur në një detyrë ekstrahimi të njësisë objektive, dhe e krahasojmë me një model të ngjashëm të stërvitur pa privatësi diferenciale. Më në fund, ne paraqesim eksperimente që tregojnë se si të interpretojmë përfaqësimin ndryshe-privat dhe të kuptojmë informacionin e humbur dhe të mbajtur në këtë proces.</abstract_sq>
      <abstract_cs>Kontextové jazykové modely vedly k výrazně lepším výsledkům na celé řadě úkolů porozumění jazyků, zejména když jsou předškoleny na stejných datech jako následný úkol. Zatímco tento dodatečný předškolení obvykle zlepšuje výkon, může vést k úniku informací, a tudíž ohrožuje soukromí jednotlivců uvedených v tréninkových údajích. Jednou ze způsobů, jak zaručit soukromí těchto jednotlivců, je trénovat diferenciálně-soukromý model, ale to obvykle jde na úkor výkonu modelu. Kromě toho je těžké říct při parametru ochrany soukromí $\epsilon$ jaký byl vliv na trénovanou reprezentaci. Cílem této práce je pomoci budoucím odborníkům a výzkumným pracovníkům, jak zlepšit soukromí při zachování dobrého výkonu modelu. Ukážeme, jak trénovat diferenciálně-privátní předškolený jazykový model (tj. BERT) se zárukou soukromí $\epsilon=1$ a s pouze malou degradací výkonu. Experimentujeme na datové sadě klinických poznámek s modelem trénovaným na extrakční úkol cílové entity a porovnáme ji s podobným modelem trénovaným bez diferenciálního soukromí. V závěru jsou prezentovány experimenty ukazující, jak interpretovat diferenciálně-soukromou reprezentaci a porozumět informacím ztraceným a udržovaným v tomto procesu.</abstract_cs>
      <abstract_bs>Kontekstualni jezički modeli su doveli do značajno boljih rezultata na razumijevanje jezika, posebno kada su predobučeni na istim podacima kao i zadatak koji se nalazi u potpunosti. Iako ova dodatna predobuka obično poboljšava učinkovitost, može dovesti do curenja informacija i stoga rizikuje privatnost pojedinaca spominjenih u podacima o obuci. Jedan metod da garantuje privatnost takvih pojedinaca je trenirati različitim privatnim modelom, ali to obično dolazi na troškove model a. Osim toga, teško je reći s obzirom na parameter privatnosti $\epsilon$ koji je bio uticaj na obučeno predstavljanje. U ovom poslu ciljamo voditi buduće praktičnike i istraživače o tome kako poboljšati privatnost dok održavamo dobre modele. Pokazujemo kako trenirati diferencijalno-privatni predobučeni jezički model (tj. BERT) sa garancijom privatnosti od $\epsilon=1$ i sa samo malim degradacijom u izvedbi. Eksperimentiramo na setu podataka kliničkih notova sa modelom obučenim na zadatku izvlačenju ciljnih entiteta i uspoređujemo je sa sličnim modelom obučenim bez diferencijalne privatnosti. Konačno predstavljamo eksperimente kako interpretirati diferencijalno-privatno predstavljanje i razumijeti izgubljene i održane informacije u ovom procesu.</abstract_bs>
      <abstract_az>Müxtəlif dil modelləri dil anlama işlərinin çoxluğuna daha yaxşı sonuçlarına yol göstərdilər, özlərinə də a şağı işləri ilə əvvəl təhsil ediləndə. Bu çox əvvəlki təhsil olaraq təhsil işlətməsini çox yaxşılaşdıran halda, bu məlumatların təhsil edilməsini təhsil edir və buna görə də təhsil məlumatlarında deyilən kişilərin xəbərsizliğini riskləndirir. Bütün kişilərin xəbərsizliğini garantiya etmək üçün bir yol, başqa-başqa şəxsi modeli təhsil etməkdir, amma bu genellikle modellərin təhsil vaxtında gəlir. Əksinə, təhsil göstərilməsi üzərində növbəsini $\epsilon$ verilmək çətin idi. Bu işdə biz gələcək təhsil sahibləri və araştırmacıları yaxşı modellərin performansını qoruyarkən gizli təhsil etməyi necə yaxşılaşdırmağı haqqında yola yönəltmək istəyirik. Biz müxtəlif təhsil edilmiş dil model in i necə təhsil etməyi göstəririk (ya da BERT) $\epsilon=1$ və ancaq küçük bir dəyişiklik göstəririk. Biz müəyyən edilmiş bir məqsədilə təhsil edilmiş modeli ilə klinik notların bir quruluğuna təcrübə edirik və onu müxtəlif təhsil olmadan təhsil edilmiş bənzər modeli ilə qarşılaşdırırıq. Sonunda, biz təcrübələr göstəririk ki, fərqli-xüsusi təcrübələrin necə yoxlamasını və bu proses içində çıxıb saxlanıldığını anlayır.</abstract_az>
      <abstract_et>Kontekstilised keelemudelid on andnud märkimisväärselt paremaid tulemusi paljude keelte mõistmise ülesannete puhul, eriti juhul, kui neid on eelnevalt koolitatud samade andmetega nagu järgneva ülesande puhul. Kuigi see täiendav eelkoolitus parandab tavaliselt tulemuslikkust, võib see põhjustada teabe leket ja seega ohustada koolitusandmetes nimetatud isikute privaatsust. Üks meetod selliste isikute privaatsuse tagamiseks on koolitada diferentsiaalselt eraõiguslikku mudelit, kuid see toimub tavaliselt mudeli jõudluse arvelt. Lisaks on privaatsusparameetri $\epsilon$ puhul raske öelda, milline oli mõju koolitatud esindusele. Selle töö eesmärk on juhendada tulevasi praktikuid ja teadlasi, kuidas parandada privaatsust, säilitades samas hea mudeli jõudluse. Näitame, kuidas treenida diferentsiaalselt privaatset eelõpetatud keelemudelit (st BERT), mille privaatsuse garantii on $\epsilon=1$ ja jõudluse väike halvenemine. Me eksperimenteerime kliiniliste märkmete andmekogumi mudeliga, mis on koolitatud sihtorganite ekstraheerimise ülesandeks, ja võrdleme seda sarnase mudeliga, mis on koolitatud ilma diferentsiaalse privaatsuseta. Lõpuks tutvustame eksperimente, mis näitavad, kuidas tõlgendada diferentsiaalselt-privaatset esindust ning mõista selles protsessis kaotatud ja säilitatud informatsiooni.</abstract_et>
      <abstract_fi>Konekstuaaliset kielimallit ovat johtaneet merkittävästi parempiin tuloksiin lukuisissa kielen ymmärtämisen tehtävissä, erityisesti kun niitä on koulutettu samoista tiedoista kuin jatkovaiheen tehtävässä. Vaikka tämä lisäkoulutus yleensä parantaa suorituskykyä, se voi johtaa tietovuotoon ja siten vaarantaa koulutustiedoissa mainittujen henkilöiden yksityisyyden. Yksi tapa taata tällaisten henkilöiden yksityisyys on kouluttaa erilaisesti yksityinen malli, mutta tämä tapahtuu yleensä mallin suorituskyvyn kustannuksella. Lisäksi on vaikea sanoa tietosuojaparametrin $\epsilon$ perusteella, mikä oli vaikutus koulutettuun edustukseen. Tässä työssä pyrimme ohjaamaan tulevia ammattilaisia ja tutkijoita siihen, miten yksityisyyttä voidaan parantaa samalla, kun mallien suorituskyky säilyy hyvänä. Esittelemme, miten koulutamme erilaisesti yksityisen esikoulutetun kielimallin (BERT), jonka tietosuojatakuu on $\epsilon=1$jasuorituskyky heikkenee vain vähän. Kokeilemme kliinisten muistiinpanojen aineistoa kohdekokonaisuuden uuttamiseen koulutetulla mallilla ja vertaamme sitä vastaavaan malliin, joka on koulutettu ilman erillistä yksityisyyttä. Lopuksi esittelemme kokeiluja, jotka osoittavat, miten erilaisesti yksityistä edustusta tulkitaan ja miten siinä menetetään ja säilytetään tietoa.</abstract_fi>
      <abstract_ca>Els models de llenguatge contextual han portat a resultats significativament millors en una gran quantitat de tasques de comprensió del llenguatge, especialment quan s'han preparat previament amb les mateixes dades que la tasca downstream. Mentre aquesta pré-formació adicional normalment millora el rendiment, pot portar a fuga d'informació i, per tant, arrisca a la privacitat dels individus mencionats en les dades de formació. One method to guarantee the privacy of such individuals is to train a differentially-private model, but this usually comes at the expense of model performance.  A més, és difícil dir, dada un paràmetre de privacitat $\epsilon$, quin va ser l'efecte en la representació entrenada. En aquesta feina busquem orientar futurs metges i investigadors sobre com millorar la privacitat mentre mantenim un bon rendiment model. Ens demostrem com entrenar un model de llenguatge pre-entrenat diferencialment privat (i.e., BERT) amb una garantia de privacitat de $\epsilon=1$ i amb només una petita degradació en el rendiment. Experimentem en un conjunt de dades de notes clíniques amb un model entrenat en una tasca d'extracció d'una entitat alvo, i la comparam amb un model similar entrenat sense privacitat diferencial. Finalment, presentem experiments mostrant com interpretar la representació diferencialment privada i entendre la informació perduda i mantenida en aquest procés.</abstract_ca>
      <abstract_ha>Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task.  A lokacin da wannan wa'adin na ƙaranci ko daidai ya improve performance, yana iya ƙara wa leakaji wa information, kuma don haka sai ya riske faranta ga mutane wanda aka ambaci cikin data na tattalin. Wata hanyoyi ta wajabta farat ɗaya ga waɗancan, yana yin wa'anar wani misali mai gaurawa a gauransa, kuma amma, ko da yaushe, wannan yana kasancẽwa a ƙara wa misalin mutane. Kayya, yã yi nau'i a gaya wani parameter farat ɗaya $applsilon Daga wannan aikin, Munã nufin mu shiryar da masu aikin aiki da mãsu ƙidãya a gaba, da watani kan yadda za su improve farat ɗaya a lokacin da za'a tsare taskõkin misali mai kyau. Tuna nuna yadda za Mu kõre misalin misalin harshen wanda aka yi wa zaman wani da aka sani (misali, BERT) da wani garanci farat ɗaya na $ applsilon=1 Dollar kuma da wani wulãkanci kaɗan a cikin aikin. Kana jarraba a kan kodi na takardar notori na kima da wata motel wanda aka yi wa wa aikin zartar abun abun shinen goani, kuma mu sami da shi ga misalin wanda aka yi wa shirin da shi na kami ko kuma ba da wani fara na daban-daban. Haƙĩƙa, Munã halatar da jarrabai idan za mu nuna yadda zã a bayyana fassarar masu tsari na gauraci kuma mu fahimta information lost da wanda aka tsare a cikin wannan aikin.</abstract_ha>
      <abstract_sk>Kontekstualni jezikovni modeli so pripeljali do bistveno boljših rezultatov pri številnih nalogah razumevanja jezika, zlasti če so bili vnaprej usposobljeni za iste podatke kot za nadaljnjo nalogo. Čeprav to dodatno predusposabljanje običajno izboljšuje uspešnost, lahko povzroči uhajanje informacij in s tem ogroža zasebnost posameznikov, navedenih v podatkih o usposabljanju. Eden od načinov zagotavljanja zasebnosti takih posameznikov je usposabljanje diferencialno-zasebnega modela, vendar to običajno pride na račun uspešnosti modela. Poleg tega je težko reči glede na parameter zasebnosti $\epsilon$ kakšen je bil učinek na usposobljeno predstavitev. V tem delu želimo voditi bodoče strokovnjake in raziskovalce o tem, kako izboljšati zasebnost in hkrati ohraniti dobro delovanje modela. Prikazujemo, kako usposabljati diferencialno zasebni vnaprej usposobljeni jezikovni model (tj. BERT) z jamstvom za zasebnost $\epsilon=1$ in le z majhnim poslabšanjem zmogljivosti. Na podatkovnem naboru kliničnih zapiskov eksperimentiramo z modelom, usposobljenim za nalogo ekstrakcije ciljne entitete, in ga primerjamo s podobnim modelom, usposobljenim brez diferencialne zasebnosti. Na koncu predstavljamo poskuse, ki kažejo, kako interpretirati diferencialno-zasebno reprezentacijo in razumeti informacije, ki so bile izgubljene in ohranjene v tem procesu.</abstract_sk>
      <abstract_jv>Menu item to Open 'Search for Open Files' dialog Nejer sampeyan ingkang dianggap sing mengko nggawe geranggap perusahaan kelas, dadi iso nggawe informasi nggawe nguasai perusahaan pribadi sing nguasai perusahaan kanggo mbatalé perusahaan kuwi nggawe dadi nyong. Wurung sistem kanggo ngerasai pribadi kanggo wong liyane kuwi, dadi mau sekolah model sing gak bener, njuk saiki iki dadi ono wektu nggo ndelok model politenessoffpolite"), and when there is a change ("assertive Nang iki jalluk awak dhéwé ngerti nggawe praksi barêng-barêng lan yatak njaluké awak dhéwé kuwi nggawe ngubah pribadi kanggo ngubah mau ngerayakno paran sing apik dhéwé. We show up as to vlacene a separately-Personal advanced language model (i.e. BERT) with a Personal garance of $epsion=1$and with only a small degradition in success. Awak dhéwé éntuk data set dadine sing dadi nggawe model sing tukang nggawe Tarjamahan jenis, karo nggawe nyimpen karo model sing kudu nggawe bener user@example:button</abstract_jv>
      <abstract_he>דוגמני שפה קונטקסטיים הובילו לתוצאות טובות יותר משמעותיות בהרבה של משימות הבנה לשפה, במיוחד כאשר התאמנו מראש על אותם נתונים כמו המשימה התחתונה. בעוד האימונים הנוספים האלה בדרך כלל משתפרים ביצועים, הם יכולים להוביל לדליפת מידע ולכן מסתכנים בפרטיות של אנשים שמזכירים בנתונים האימונים. שיטה אחת להבטיח את הפרטיות של אנשים כאלה היא לאמן מודל פרטי-שונה, אבל זה בדרך כלל מגיע על חשבון ההופעה של מודל. חוץ מזה, קשה לומר בהתחשב בפרמטר פרטיות $\epsilon$ מה היה ההשפעה על מייצג המאמן. In this work we aim to guide future practitioners and researchers on how to improve privacy while maintaining good model performance.  אנחנו מראים איך לאמן מודל שפה מאומן מראש (כלומר, BERT) בהבטחה פרטית של $\epsilon=1$ ובשיפול קטן בלבד בהופעה. אנו מנסים על קבוצת נתונים של רשומות קליניות עם דוגמנית מאומנת על משימת יציאה של היחידה המטרה, ושווה אותה עם דוגמנית דומה מאומנת ללא פרטיות דיפרנציאלית. Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.</abstract_he>
      <abstract_bo>སྐད་ཡིག་ཆ་མིག་དཔེ་དབྱེ་བ་དེ་ནི་སྐད་ཡིག་ཆ་ལྟ་བུའི་ལས་འགུལ་བཤད་ཀྱི་རྐྱེན་བ་མང་ཙམ་ཡིན། སྔོན་གྲོང་གླེང་སྔོན་གྲངས་ཀྱི་ལས་འགུལ་སྐྱོང་ཚད་ཕར་རྒྱས་གཏོང་བ་ཡིན་ནའང་། དེ་ནི་གནས་ཚུལ་གསལ་བཤད་ཀྱི་ཤུལ་མར་ཉེ དབྱེ་རིགས་འདིའི་མི་སྒེར་གྱི་རང་བཞིན་བདེ་སྦྱོར་བྱེད་ནི་ཕན་མེད་སྒེར་གྱི་མ་དཔེ་ཆས་ལ་རྒྱུན་ལྡན་མི་སྟོན་པའི་རྒྱུ་དང་། Moreover, it is hard to tell a privacy parameter $\epsilon$ what was the effect on the trained representation. ང་ཚོས་མ་འོངས་པའི་ལས་འགན་འདིས་མི་འོངས་པར་སྤྲོད་རྒྱུ་མཁན་དང་། མི་རྩོལ་ལྟ་སྟངས་ཀྱིས་མི་ཚོགས་རྣམས་རང་ཉིད་ཀྱི་མི་ We demonstrate how to train a differentially-private pre-trained language model (i.e. BERT) with a private guarantee of $\epsilon=1$ and with only a small degradation in performance. ང་ཚོས་གཞུང Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.</abstract_bo>
      </paper>
    <paper id="6">
      <title>Using Confidential Data for <a href="https://en.wikipedia.org/wiki/Domain_adaptation">Domain Adaptation</a> of Neural Machine Translation</title>
      <author><first>Sohyung</first><last>Kim</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Fatih</first><last>Turkmen</last></author>
      <pages>46–52</pages>
      <abstract>We study the problem of <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> in Neural Machine Translation (NMT) when domain-specific data can not be shared due to confidentiality or copyright issues. As a first step, we propose to fragment data into phrase pairs and use a <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">random sample</a> to fine-tune a generic NMT model instead of the full sentences. Despite the loss of long segments for the sake of confidentiality protection, we find that NMT quality can considerably benefit from this adaptation, and that further gains can be obtained with a simple tagging technique.</abstract>
      <url hash="86e00d83">2021.privatenlp-1.6</url>
      <attachment type="OptionalSupplementaryData" hash="dbf8a8e7">2021.privatenlp-1.6.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.privatenlp-1.6</doi>
      <bibkey>kim-etal-2021-using</bibkey>
      <pwccode url="https://github.com/sohyo/using-confidential-data-for-nmt" additional="false">sohyo/using-confidential-data-for-nmt</pwccode>
    </paper>
    <paper id="7">
      <title>Private Text Classification with <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a></title>
      <author><first>Samuel</first><last>Adams</last></author>
      <author><first>David</first><last>Melanson</last></author>
      <author><first>Martine</first><last>De Cock</last></author>
      <pages>53–58</pages>
      <abstract>Text classifiers are regularly applied to personal texts, leaving users of these classifiers vulnerable to <a href="https://en.wikipedia.org/wiki/Information_privacy">privacy breaches</a>. We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC). Our method enables the inference of a class label for a personal text in such a way that (1) the owner of the personal text does not have to disclose their text to anyone in an unencrypted manner, and (2) the owner of the text classifier does not have to reveal the trained model parameters to the text owner or to anyone else. To demonstrate the feasibility of our protocol for practical private text classification, we implemented it in the PyTorch-based MPC framework CrypTen, using a well-known additive secret sharing scheme in the honest-but-curious setting. We test the <a href="https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)">runtime</a> of our privacy-preserving text classifier, which is fast enough to be used in practice.</abstract>
      <url hash="a36b03e1">2021.privatenlp-1.7</url>
      <doi>10.18653/v1/2021.privatenlp-1.7</doi>
      <bibkey>adams-etal-2021-private</bibkey>
    </paper>
  </volume>
</collection>