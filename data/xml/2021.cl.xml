<collection id="2021.cl">
  <volume id="1">
    <meta>
      <booktitle>Computational Linguistics, Volume 47, Issue 1 - March 2021</booktitle>
      <month>March</month>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2021</year>
    </meta>
    <paper id="2">
      <title>Formal Basis of a Language Universal</title>
      <author><first>Milo&#353;</first><last>Stanojevi&#263;</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <doi>10.1162/coli_a_00394</doi>
      <abstract>Abstract Steedman (2020) proposes as a formal universal of natural language grammar that grammatical permutations of the kind that have given rise to transformational rules are limited to a class known to mathematicians and computer scientists as the &#8220;separable&#8221; permutations. This class of permutations is exactly the class that can be expressed in combinatory categorial grammars (CCGs). The excluded non-separable permutations do in fact seem to be absent in a number of studies of crosslinguistic variation in word order in nominal and verbal constructions. The number of permutations that are separable grows in the number n of lexical elements in the construction as the Large Schr&#246;der Number Sn&#8722;1. Because that number grows much more slowly than the n! number of all permutations, this generalization is also of considerable practical interest for computational applications such as parsing and machine translation. The present article examines the mathematical and computational origins of this restriction, and the reason it is exactly captured in CCG without the imposition of any further constraints.</abstract>
      <pages>9&#8211;42</pages>
      <url hash="f68d3f5d">2021.cl-1.2</url>
      <bibkey>stanojevic-steedman-2021-formal</bibkey>
    </paper>
    <paper id="4">
      <title>Semantic Data Set Construction from Human Clustering and Spatial Arrangement</title>
      <author><first>Olga</first><last>Majewska</last></author>
      <author><first>Diana</first><last>McCarthy</last></author>
      <author><first>Jasper J. F.</first><last>van den Bosch</last></author>
      <author><first>Nikolaus</first><last>Kriegeskorte</last></author>
      <author><first>Ivan</first><last>Vuli&#263;</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <doi>10.1162/coli_a_00396</doi>
      <abstract>Abstract Research into representation learning models of lexical semantics usually utilizes some form of intrinsic evaluation to ensure that the learned representations reflect human semantic judgments. Lexical semantic similarity estimation is a widely used evaluation method, but efforts have typically focused on pairwise judgments of words in isolation, or are limited to specific contexts and lexical stimuli. There are limitations with these approaches that either do not provide any context for judgments, and thereby ignore ambiguity, or provide very specific sentential contexts that cannot then be used to generate a larger lexical resource. Furthermore, similarity between more than two items is not considered. We provide a full description and analysis of our recently proposed methodology for large-scale data set construction that produces a semantic classification of a large sample of verbs in the first phase, as well as multi-way similarity judgments made within the resultant semantic classes in the second phase. The methodology uses a spatial multi-arrangement approach proposed in the field of cognitive neuroscience for capturing multi-way similarity judgments of visual stimuli. We have adapted this method to handle polysemous linguistic stimuli and much larger samples than previous work. We specifically target verbs, but the method can equally be applied to other parts of speech. We perform cluster analysis on the data from the first phase and demonstrate how this might be useful in the construction of a comprehensive verb resource. We also analyze the semantic information captured by the second phase and discuss the potential of the spatially induced similarity judgments to better reflect human notions of word similarity. We demonstrate how the resultant data set can be used for fine-grained analyses and evaluation of representation learning models on the intrinsic tasks of semantic clustering and semantic similarity. In particular, we find that stronger static word embedding methods still outperform lexical representations emerging from more recent pre-training methods, both on word-level similarity and clustering. Moreover, thanks to the data set&#8217;s vast coverage, we are able to compare the benefits of specializing vector representations for a particular type of external knowledge by evaluating FrameNet- and VerbNet-retrofitted models on specific semantic domains such as &#8220;Heat&#8221; or &#8220;Motion.&#8221;</abstract>
      <pages>69&#8211;116</pages>
      <url hash="d83daca3">2021.cl-1.4</url>
      <bibkey>majewska-etal-2021-semantic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="6">
      <title>Supervised and Unsupervised Neural Approaches to Text Readability</title>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <author><first>Marko</first><last>Robnik-&#352;ikonja</last></author>
      <doi>10.1162/coli_a_00398</doi>
      <abstract>Abstract We present a set of novel neural supervised and unsupervised approaches for determining the readability of documents. In the unsupervised setting, we leverage neural language models, whereas in the supervised setting, three different neural classification architectures are tested. We show that the proposed neural unsupervised approach is robust, transferable across languages, and allows adaptation to a specific readability task and data set. By systematic comparison of several neural architectures on a number of benchmark and new labeled readability data sets in two languages, this study also offers a comprehensive analysis of different neural approaches to readability classification. We expose their strengths and weaknesses, compare their performance to current state-of-the-art classification approaches to readability, which in most cases still rely on extensive feature engineering, and propose possibilities for improvements.</abstract>
      <pages>141&#8211;179</pages>
      <url hash="266db4eb">2021.cl-1.6</url>
      <bibkey>martinc-etal-2021-supervised</bibkey>
      <pwccode url="" additional="true" />
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopenglish">OneStopEnglish</pwcdataset>
    </paper>
    <paper id="7">
      <title>Depth-Bounded Statistical <fixed-case>PCFG</fixed-case> Induction as a Model of Human Grammar Acquisition</title>
      <author><first>Lifeng</first><last>Jin</last></author>
      <author><first>Lane</first><last>Schwartz</last></author>
      <author><first>Finale</first><last>Doshi-Velez</last></author>
      <author><first>Timothy</first><last>Miller</last></author>
      <author><first>William</first><last>Schuler</last></author>
      <doi>10.1162/coli_a_00399</doi>
      <abstract>Abstract This article describes a simple PCFG induction model with a fixed category domain that predicts a large majority of attested constituent boundaries, and predicts labels consistent with nearly half of attested constituent labels on a standard evaluation data set of child-directed speech. The article then explores the idea that the difference between simple grammars exhibited by child learners and fully recursive grammars exhibited by adult learners may be an effect of increasing working memory capacity, where the shallow grammars are constrained images of the recursive grammars. An implementation of these memory bounds as limits on center embedding in a depth-specific transform of a recursive grammar yields a significant improvement over an equivalent but unbounded baseline, suggesting that this arrangement may indeed confer a learning advantage.</abstract>
      <pages>181&#8211;216</pages>
      <url hash="189ce55d">2021.cl-1.7</url>
      <bibkey>jin-etal-2021-depth</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    </volume>
  <volume id="2">
    <meta>
      <booktitle>Computational Linguistics, Volume 47, Issue 2 - June 2021</booktitle>
      <month>June</month>
      <year>2021</year>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
    </meta>
    <paper id="9">
      <title>Approximating Probabilistic Models as Weighted Finite Automata</title>
      <author><first>Ananda Theertha</first><last>Suresh</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <author><first>Michael</first><last>Riley</last></author>
      <author><first>Vlad</first><last>Schogol</last></author>
      <doi>10.1162/coli_a_00401</doi>
      <abstract>Abstract Weighted finite automata (WFAs) are often used to represent probabilistic models, such as n-gram language models, because among other things, they are efficient for recognition tasks in time and space. The probabilistic source to be represented as a WFA, however, may come in many forms. Given a generic probabilistic model over sequences, we propose an algorithm to approximate it as a WFA such that the Kullback-Leibler divergence between the source model and the WFA target model is minimized. The proposed algorithm involves a counting step and a difference of convex optimization step, both of which can be performed efficiently. We demonstrate the usefulness of our approach on various tasks, including distilling n-gram models from neural models, building compact language models, and building open-vocabulary character models. The algorithms used for these experiments are available in an open-source software library.</abstract>
      <pages>221&#8211;254</pages>
      <url hash="390ee738">2021.cl-2.9</url>
      <bibkey>suresh-etal-2021-approximating</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>RYANSQL</fixed-case>: Recursively Applying Sketch-based Slot Fillings for Complex Text-to-<fixed-case>SQL</fixed-case> in Cross-Domain Databases</title>
      <author><first>DongHyun</first><last>Choi</last></author>
      <author><first>Myeong Cheol</first><last>Shin</last></author>
      <author><first>EungGyun</first><last>Kim</last></author>
      <author><first>Dong Ryeol</first><last>Shin</last></author>
      <doi>10.1162/coli_a_00403</doi>
      <abstract>Abstract Text-to-SQL is the problem of converting a user question into an SQL query, when the question and database are given. In this article, we present a neural network approach called RYANSQL (Recursively Yielding Annotation Network for SQL) to solve complex Text-to-SQL tasks for cross-domain databases. Statement Position Code (SPC) is defined to transform a nested SQL query into a set of non-nested SELECT statements; a sketch-based slot-filling approach is proposed to synthesize each SELECT statement for its corresponding SPC. Additionally, two input manipulation methods are presented to improve generation performance further. RYANSQL achieved competitive result of 58.2% accuracy on the challenging Spider benchmark. At the time of submission (April 2020), RYANSQL v2, a variant of original RYANSQL, is positioned at 3rd place among all systems and 1st place among the systems not using database content with 60.6% exact matching accuracy. The source code is available at https://github.com/kakaoenterprise/RYANSQL.</abstract>
      <pages>309&#8211;332</pages>
      <url hash="69a55600">2021.cl-2.12</url>
      <bibkey>choi-etal-2021-ryansql</bibkey>
      <pwccode url="https://github.com/kakaoenterprise/RYANSQL" additional="false">kakaoenterprise/RYANSQL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/spider-1">SPIDER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="13">
      <title><fixed-case>C</fixed-case>ausa<fixed-case>LM</fixed-case>: Causal Model Explanation Through Counterfactual Language Models</title>
      <author><first>Amir</first><last>Feder</last></author>
      <author><first>Nadav</first><last>Oved</last></author>
      <author><first>Uri</first><last>Shalit</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <doi>10.1162/coli_a_00404</doi>
      <abstract>Abstract Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning&#8211;based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high-level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.1</abstract>
      <pages>333&#8211;386</pages>
      <url hash="5bd824ae">2021.cl-2.13</url>
      <bibkey>feder-etal-2021-causalm</bibkey>
    </paper>
    <paper id="14">
      <title>Analysis and Evaluation of Language Models for Word Sense Disambiguation</title>
      <author><first>Daniel</first><last>Loureiro</last></author>
      <author><first>Kiamehr</first><last>Rezaee</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <doi>10.1162/coli_a_00405</doi>
      <abstract>Abstract Transformer-based language models have taken many fields in NLP by storm. BERT and its derivatives dominate most of the existing evaluation benchmarks, including those for Word Sense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic nuances. However, there is still little knowledge about their capabilities and potential limitations in encoding and recovering word senses. In this article, we provide an in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to lexical ambiguity. One of the main conclusions of our analysis is that BERT can accurately capture high-level sense distinctions, even when a limited number of examples is available for each word sense. Our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. However, this scenario rarely occurs in real-world settings and, hence, many practical challenges remain even in the coarse-grained setting. We also perform an in-depth comparison of the two main language model-based WSD strategies, namely, fine-tuning and feature extraction, finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data. In fact, the simple feature extraction strategy of averaging contextualized embeddings proves robust even using only three training sentences per word sense, with minimal improvements obtained by increasing the size of this training data.</abstract>
      <pages>387&#8211;443</pages>
      <url hash="20cc9d3c">2021.cl-2.14</url>
      <bibkey>loureiro-etal-2021-analysis</bibkey>
      <pwccode url="https://github.com/danlou/bert-disambiguation" additional="false">danlou/bert-disambiguation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coarsewsd-20">CoarseWSD-20</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    </volume>
  <volume id="3">
    <meta>
      <booktitle>Computational Linguistics, Volume 47, Issue 3 - November 2021</booktitle>
      <month>November</month>
      <year>2021</year>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
    </meta>
    <paper id="16">
      <title>The Taxonomy of Writing Systems: How to Measure How Logographic a System Is</title>
      <author><first>Richard</first><last>Sproat</last></author>
      <author><first>Alexander</first><last>Gutkin</last></author>
      <doi>10.1162/coli_a_00409</doi>
      <abstract>Taxonomies of writing systems since Gelb (1952) have classified systems based on what the written symbols represent: if they represent words or morphemes, they are logographic; if syllables, syllabic; if segments, alphabetic; and so forth. Sproat (2000) and Rogers (2005) broke with tradition by splitting the logographic and phonographic aspects into two dimensions, with logography being graded rather than a categorical distinction. A system could be syllabic, and highly logographic; or alphabetic, and mostly non-logographic. This accords better with how writing systems actually work, but neither author proposed a method for measuring logography. In this article we propose a novel measure of the degree of logography that uses an attention-based sequence-to-sequence model trained to predict the spelling of a token from its pronunciation in context. In an ideal phonographic system, the model should need to attend to only the current token in order to compute how to spell it, and this would show in the attention matrix activations. In contrast, with a logographic system, where a given pronunciation might correspond to several different spellings, the model would need to attend to a broader context. The ratio of the activation outside the token and the total activation forms the basis of our measure. We compare this with a simple lexical measure, and an entropic measure, as well as several other neural models, and argue that on balance our attention-based measure accords best with intuition about how logographic various systems are. Our work provides the first quantifiable measure of the notion of logography that accords with linguistic intuition and, we argue, provides better insight into what this notion means.</abstract>
      <pages>477&#8211;528</pages>
      <url hash="e822a20d">2021.cl-3.16</url>
      <bibkey>sproat-gutkin-2021-taxonomy</bibkey>
    </paper>
    <paper id="20">
      <title>Decoding Word Embeddings with Brain-Based Semantic Features</title>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <doi>10.1162/coli_a_00412</doi>
      <abstract>Word embeddings are vectorial semantic representations built with either counting or predicting techniques aimed at capturing shades of meaning from word co-occurrences. Since their introduction, these representations have been criticized for lacking interpretable dimensions. This property of word embeddings limits our understanding of the semantic features they actually encode. Moreover, it contributes to the &#8220;black box&#8221; nature of the tasks in which they are used, since the reasons for word embedding performance often remain opaque to humans. In this contribution, we explore the semantic properties encoded in word embeddings by mapping them onto interpretable vectors, consisting of explicit and neurobiologically motivated semantic features (Binder et al. 2016). Our exploration takes into account different types of embeddings, including factorized count vectors and predict models (Skip-Gram, GloVe, etc.), as well as the most recent contextualized representations (i.e., ELMo and BERT). In our analysis, we first evaluate the quality of the mapping in a retrieval task, then we shed light on the semantic features that are better encoded in each embedding type. A large number of probing tasks is finally set to assess how the original and the mapped embeddings perform in discriminating semantic categories. For each probing task, we identify the most relevant semantic features and we show that there is a correlation between the embedding performance and how they encode those features. This study sets itself as a step forward in understanding which aspects of meaning are captured by vector spaces, by proposing a new and simple method to carve human-interpretable semantic representations from distributional vectors.</abstract>
      <pages>663&#8211;698</pages>
      <url hash="945a1b8b">2021.cl-3.20</url>
      <bibkey>chersoni-etal-2021-decoding</bibkey>
    </paper>
    </volume>
  <volume id="4">
    <meta>
      <booktitle>Computational Linguistics, Volume 47, Issue 4 - December 2021</booktitle>
      <month>December</month>
      <year>2021</year>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
    </meta>
    <paper id="27">
      <title>Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization</title>
      <author><first>Panagiotis</first><last>Kouris</last></author>
      <author><first>Georgios</first><last>Alexandridis</last></author>
      <author><first>Andreas</first><last>Stafylopatis</last></author>
      <doi>10.1162/coli_a_00417</doi>
      <abstract>Abstract Nowadays, most research conducted in the field of abstractive text summarization focuses on neural-based models alone, without considering their combination with knowledge-based approaches that could further enhance their efficiency. In this direction, this work presents a novel framework that combines sequence-to-sequence neural-based text summarization along with structure and semantic-based methodologies. The proposed framework is capable of dealing with the problem of out-of-vocabulary or rare words, improving the performance of the deep learning models. The overall methodology is based on a well-defined theoretical model of knowledge-based content generalization and deep learning predictions for generating abstractive summaries. The framework is composed of three key elements: (i) a pre-processing task, (ii) a machine learning methodology, and (iii) a post-processing task. The pre-processing task is a knowledge-based approach, based on ontological knowledge resources, word sense disambiguation, and named entity recognition, along with content generalization, that transforms ordinary text into a generalized form. A deep learning model of attentive encoder-decoder architecture, which is expanded to enable a coping and coverage mechanism, as well as reinforcement learning and transformer-based architectures, is trained on a generalized version of text-summary pairs, learning to predict summaries in a generalized form. The post-processing task utilizes knowledge resources, word embeddings, word sense disambiguation, and heuristic algorithms based on text similarity methods in order to transform the generalized version of a predicted summary to a final, human-readable form. An extensive experimental procedure on three popular data sets evaluates key aspects of the proposed framework, while the obtained results exhibit promising performance, validating the robustness of the proposed approach.</abstract>
      <pages>813&#8211;859</pages>
      <url hash="57d76055">2021.cl-4.27</url>
      <bibkey>kouris-etal-2021-abstractive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="28">
      <title>The (Un)Suitability of Automatic Evaluation Metrics for Text Simplification</title>
      <author><first>Fernando</first><last>Alva-Manchego</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <doi>10.1162/coli_a_00418</doi>
      <abstract>Abstract In order to simplify sentences, several rewriting operations can be performed, such as replacing complex words per simpler synonyms, deleting unnecessary information, and splitting long sentences. Despite this multi-operation nature, evaluation of automatic simplification systems relies on metrics that moderately correlate with human judgments on the simplicity achieved by executing specific operations (e.g., simplicity gain based on lexical replacements). In this article, we investigate how well existing metrics can assess sentence-level simplifications where multiple operations may have been applied and which, therefore, require more general simplicity judgments. For that, we first collect a new and more reliable data set for evaluating the correlation of metrics and human judgments of overall simplicity. Second, we conduct the first meta-evaluation of automatic metrics in Text Simplification, using our new data set (and other existing data) to analyze the variation of the correlation between metrics&#8217; scores and human judgments across three dimensions: the perceived simplicity level, the system type, and the set of references used for computation. We show that these three aspects affect the correlations and, in particular, highlight the limitations of commonly used operation-specific metrics. Finally, based on our findings, we propose a set of recommendations for automatic evaluation of multi-operation simplifications, suggesting which metrics to compute and how to interpret their scores.</abstract>
      <pages>861&#8211;889</pages>
      <url hash="64db7533">2021.cl-4.28</url>
      <bibkey>alva-manchego-etal-2021-un</bibkey>
      <pwccode url="https://github.com/feralvam/metaeval-simplification" additional="false">feralvam/metaeval-simplification</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/turkcorpus">TurkCorpus</pwcdataset>
    </paper>
    <paper id="30">
      <title>Are Ellipses Important for Machine Translation?</title>
      <author><first>Payal</first><last>Khullar</last></author>
      <doi>10.1162/coli_a_00414</doi>
      <abstract>Abstract This article describes an experiment to evaluate the impact of different types of ellipses discussed in theoretical linguistics on Neural Machine Translation (NMT), using English to Hindi/Telugu as source and target languages. Evaluation with manual methods shows that most of the errors made by Google NMT are located in the clause containing the ellipsis, the frequency of such errors is slightly more in Telugu than Hindi, and the translation adequacy shows improvement when ellipses are reconstructed with their antecedents. These findings not only confirm the importance of ellipses and their resolution for MT, but also hint toward a possible correlation between the translation of discourse devices like ellipses with the morphological incongruity of the source and target. We also observe that not all ellipses are translated poorly and benefit from reconstruction, advocating for a disparate treatment of different ellipses in MT research.</abstract>
      <pages>927&#8211;937</pages>
      <url hash="770090b8">2021.cl-4.30</url>
      <bibkey>khullar-2021-ellipses</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>LFG</fixed-case> Generation from Acyclic <fixed-case>F</fixed-case>-Structures is <fixed-case>NP</fixed-case>-Hard</title>
      <author><first>J&#252;rgen</first><last>Wedekind</last></author>
      <author><first>Ronald M.</first><last>Kaplan</last></author>
      <doi>10.1162/coli_a_00419</doi>
      <abstract>Abstract The universal generation problem for LFG grammars is the problem of determining whether a given grammar derives any terminal string with a given f-structure. It is known that this problem is decidable for acyclic f-structures. In this brief note, we show that for those f-structures the problem is nonetheless intractable. This holds even for grammars that are off-line parsable.</abstract>
      <pages>939&#8211;946</pages>
      <url hash="3f8a673f">2021.cl-4.31</url>
      <bibkey>wedekind-kaplan-2021-lfg</bibkey>
    </paper>
  </volume>
</collection>