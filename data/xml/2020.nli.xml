<collection id="2020.nli">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the First Workshop on Natural Language Interfaces</booktitle>
      <editor><first>Ahmed Hassan</first><last>Awadallah</last></editor>
      <editor><first>Yu</first><last>Su</last></editor>
      <editor><first>Huan</first><last>Sun</last></editor>
      <editor><first>Scott Wen-tau</first><last>Yih</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="fed11dde">2020.nli-1</url>
    </meta>
    <frontmatter>
      <url hash="175d8ef8">2020.nli-1.0</url>
      <bibkey>nli-2020-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Answering Complex Questions by Combining Information from Curated and Extracted Knowledge Bases</title>
      <author><first>Nikita</first><last>Bhutani</last></author>
      <author><first>Xinyi</first><last>Zheng</last></author>
      <author><first>Kun</first><last>Qian</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <author><first>H.</first><last>Jagadish</last></author>
      <pages>1&#8211;10</pages>
      <abstract>Knowledge-based question answering (KB_QA) has long focused on simple questions that can be answered from a single knowledge source, a manually curated or an automatically extracted KB. In this work, we look at answering complex questions which often require combining information from multiple sources. We present a novel KB-QA system, Multique, which can map a complex question to a complex query pattern using a sequence of simple queries each targeted at a specific KB. It finds simple queries using a neural-network based model capable of collective inference over textual relations in extracted KB and ontological relations in curated KB. Experiments show that our proposed system outperforms previous KB-QA systems on benchmark datasets, ComplexWebQuestions and WebQuestionsSP.</abstract>
      <url hash="0a910fed">2020.nli-1.1</url>
      <doi>10.18653/v1/2020.nli-1.1</doi>
      <video href="http://slideslive.com/38929797" />
      <bibkey>bhutani-etal-2020-answering</bibkey>
    </paper>
    <paper id="2">
      <title>Towards Reversal-Based Textual Data Augmentation for <fixed-case>NLI</fixed-case> Problems with Opposable Classes</title>
      <author><first>Alexey</first><last>Tarasov</last></author>
      <pages>11&#8211;19</pages>
      <abstract>Data augmentation methods are commonly used in computer vision and speech. However, in domains dealing with textual data, such techniques are not that common. Most of the existing methods rely on rephrasing, i.e. new sentences are generated by changing a source sentence, preserving its meaning. We argue that in tasks with opposable classes (such as Positive and Negative in sentiment analysis), it might be beneficial to also invert the source sentence, reversing its meaning, to generate examples of the opposing class. Methods that use somewhat similar intuition exist in the space of adversarial learning, but are not always applicable to text classification (in our experiments, some of them were even detrimental to the resulting classifier accuracy). We propose and evaluate two reversal-based methods on an NLI task of recognising a type of a simple logical expression from its description in plain-text form. After gathering a dataset on MTurk, we show that a simple heuristic using a notion of negating the main verb has a potential not only on its own, but that it can also boost existing state-of-the-art rephrasing-based approaches.</abstract>
      <url hash="888880c6">2020.nli-1.2</url>
      <doi>10.18653/v1/2020.nli-1.2</doi>
      <video href="http://slideslive.com/38929795" />
      <bibkey>tarasov-2020-towards</bibkey>
    </paper>
    </volume>
</collection>