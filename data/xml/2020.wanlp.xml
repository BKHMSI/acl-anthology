<collection id="2020.wanlp">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Fifth Arabic Natural Language Processing Workshop</booktitle>
      <editor><first>Imed</first><last>Zitouni</last></editor>
      <editor><first>Muhammad</first><last>Abdul-Mageed</last></editor>
      <editor><first>Houda</first><last>Bouamor</last></editor>
      <editor><first>Fethi</first><last>Bougares</last></editor>
      <editor><first>Mahmoud</first><last>El-Haj</last></editor>
      <editor><first>Nadi</first><last>Tomeh</last></editor>
      <editor><first>Wajdi</first><last>Zaghouani</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="2ae5a718">2020.wanlp-1.0</url>
      <bibkey>wanlp-2020-arabic</bibkey>
    </frontmatter>
    <paper id="5">
      <title>A Semi-Supervised <fixed-case>BERT</fixed-case> Approach for <fixed-case>A</fixed-case>rabic Named Entity Recognition</title>
      <author><first>Chadi</first><last>Helwe</last></author>
      <author><first>Ghassan</first><last>Dib</last></author>
      <author><first>Mohsen</first><last>Shamas</last></author>
      <author><first>Shady</first><last>Elbassuoni</last></author>
      <pages>49&#8211;57</pages>
      <abstract>Named entity recognition (NER) plays a significant role in many applications such as information extraction, information retrieval, question answering, and even machine translation. Most of the work on NER using deep learning was done for non-Arabic languages like English and French, and only few studies focused on Arabic. This paper proposes a semi-supervised learning approach to train a BERT-based NER model using labeled and semi-labeled datasets. We compared our approach against various baselines, and state-of-the-art Arabic NER tools on three datasets: AQMAR, NEWS, and TWEETS. We report a significant improvement in F-measure for the AQMAR and the NEWS datasets, which are written in Modern Standard Arabic (MSA), and competitive results for the TWEETS dataset, which contains tweets that are mostly in the Egyptian dialect and contain many mistakes or misspellings.</abstract>
      <url hash="1a068e1d">2020.wanlp-1.5</url>
      <bibkey>helwe-etal-2020-semi</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>MAN</fixed-case>orm: A Normalization Dictionary for <fixed-case>M</fixed-case>oroccan <fixed-case>A</fixed-case>rabic Dialect Written in <fixed-case>L</fixed-case>atin Script</title>
      <author><first>Randa</first><last>Zarnoufi</last></author>
      <author><first>Hamid</first><last>Jaafar</last></author>
      <author><first>Walid</first><last>Bachri</last></author>
      <author><first>Mounia</first><last>Abik</last></author>
      <pages>155&#8211;166</pages>
      <abstract>Social media user generated text is actually the main resource for many NLP tasks. This text, however, does not follow the standard rules of writing. Moreover, the use of dialect such as Moroccan Arabic in written communications increases further NLP tasks complexity. A dialect is a verbal language that does not have a standard orthography. The written dialect is based on the phonetic transliteration of spoken words which leads users to improvise spelling while writing. Thus, for the same word we can find multiple forms of transliterations. Subsequently, it is mandatory to normalize these different transliterations to one canonical word form. To reach this goal, we have exploited the powerfulness of word embedding models generated with a corpus of YouTube comments. Besides, using a Moroccan Arabic dialect dictionary that provides the canonical forms, we have built a normalization dictionary that we refer to as MANorm. We have conducted several experiments to demonstrate the efficiency of MANorm, which have shown its usefulness in dialect normalization. We made MANorm freely available online.</abstract>
      <url hash="223258b3">2020.wanlp-1.14</url>
      <bibkey>zarnoufi-etal-2020-manorm</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>A</fixed-case>ra<fixed-case>WEAT</fixed-case>: Multidimensional Analysis of Biases in <fixed-case>A</fixed-case>rabic Word Embeddings</title>
      <author><first>Anne</first><last>Lauscher</last></author>
      <author><first>Rafik</first><last>Takieddin</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <author><first>Goran</first><last>Glava&#353;</last></author>
      <pages>192&#8211;199</pages>
      <abstract>Recent work has shown that distributional word vector spaces often encode human biases like sexism or racism. In this work, we conduct an extensive analysis of biases in Arabic word embeddings by applying a range of recently introduced bias tests on a variety of embedding spaces induced from corpora in Arabic. We measure the presence of biases across several dimensions, namely: embedding models (Skip-Gram, CBOW, and FastText) and vector sizes, types of text (encyclopedic text, and news vs. user-generated content), dialects (Egyptian Arabic vs. Modern Standard Arabic), and time (diachronic analyses over corpora from different time periods). Our analysis yields several interesting findings, e.g., that implicit gender bias in embeddings trained on Arabic news corpora steadily increases over time (between 2007 and 2017). We make the Arabic bias specifications (AraWEAT) publicly available.</abstract>
      <url hash="190afe95">2020.wanlp-1.17</url>
      <bibkey>lauscher-etal-2020-araweat</bibkey>
    </paper>
    <paper id="18">
      <title>Parallel resources for <fixed-case>T</fixed-case>unisian <fixed-case>A</fixed-case>rabic Dialect Translation</title>
      <author><first>Sam&#233;h</first><last>Kchaou</last></author>
      <author><first>Rahma</first><last>Boujelbane</last></author>
      <author><first>Lamia</first><last>Hadrich-Belguith</last></author>
      <pages>200&#8211;206</pages>
      <abstract>The difficulty of processing dialects is clearly observed in the high cost of building representative corpus, in particular for machine translation. Indeed, all machine translation systems require a huge amount and good management of training data, which represents a challenge in a low-resource setting such as the Tunisian Arabic dialect. In this paper, we present a data augmentation technique to create a parallel corpus for Tunisian Arabic dialect written in social media and standard Arabic in order to build a Machine Translation (MT) model. The created corpus was used to build a sentence-based translation model. This model reached a BLEU score of 15.03% on a test set, while it was limited to 13.27% utilizing the corpus without augmentation.</abstract>
      <url hash="69de3825">2020.wanlp-1.18</url>
      <bibkey>kchaou-etal-2020-parallel</bibkey>
    </paper>
    <paper id="21">
      <title>Improving <fixed-case>A</fixed-case>rabic Text Categorization Using Transformer Training Diversification</title>
      <author><first>Shammur Absar</first><last>Chowdhury</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <author><first>Jung</first><last>Soon-Gyo</last></author>
      <author><first>Joni</first><last>Salminen</last></author>
      <author><first>Bernard J.</first><last>Jansen</last></author>
      <pages>226&#8211;236</pages>
      <abstract>Automatic categorization of short texts, such as news headlines and social media posts, has many applications ranging from content analysis to recommendation systems. In this paper, we use such text categorization i.e., labeling the social media posts to categories like &#8216;sports&#8217;, &#8216;politics&#8217;, &#8216;human-rights&#8217; among others, to showcase the efficacy of models across different sources and varieties of Arabic. In doing so, we show that diversifying the training data, whether by using diverse training data for the specific task (an increase of 21% macro F1) or using diverse data to pre-train a BERT model (26% macro F1), leads to overall improvements in classification effectiveness. In our work, we also introduce two new Arabic text categorization datasets, where the first is composed of social media posts from a popular Arabic news channel that cover Twitter, Facebook, and YouTube, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and dialectal Arabic.</abstract>
      <url hash="ab2499d3">2020.wanlp-1.21</url>
      <bibkey>chowdhury-etal-2020-improving-arabic</bibkey>
      <pwccode url="https://github.com/shammur/arabic_news_text_classification_datasets" additional="false">shammur/arabic_news_text_classification_datasets</pwccode>
    </paper>
    <paper id="22">
      <title>Team <fixed-case>A</fixed-case>lexa at <fixed-case>NADI</fixed-case> Shared Task</title>
      <author><first>Mutaz</first><last>Younes</last></author>
      <author><first>Nour</first><last>Al-khdour</last></author>
      <author><first>Mohammad</first><last>AL-Smadi</last></author>
      <pages>237&#8211;242</pages>
      <abstract>In this paper, we discuss our team&#8217;s work on the NADI Shared Task. The task requires classifying Arabic tweets among 21 dialects. We tested out different approaches, and the best one was the simplest one. Our best submission was using Multinational Naive Bayes (MNB) classifier (Small and Hsiao, 1985) with n-grams as features. Despite its simplicity, this classifier shows better results than complicated models such as BERT. Our best submitted score was 17% F1-score and 35% accuracy.</abstract>
      <url hash="4568a554">2020.wanlp-1.22</url>
      <bibkey>younes-etal-2020-team</bibkey>
    </paper>
    <paper id="27">
      <title>Weighted combination of <fixed-case>BERT</fixed-case> and N-<fixed-case>GRAM</fixed-case> features for Nuanced <fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Abdellah</first><last>El Mekki</last></author>
      <author><first>Ahmed</first><last>Alami</last></author>
      <author><first>Hamza</first><last>Alami</last></author>
      <author><first>Ahmed</first><last>Khoumsi</last></author>
      <author><first>Ismail</first><last>Berrada</last></author>
      <pages>268&#8211;274</pages>
      <abstract>Around the Arab world, different Arabic dialects are spoken by more than 300M persons, and are increasingly popular in social media texts. However, Arabic dialects are considered to be low-resource languages, limiting the development of machine-learning based systems for these dialects. In this paper, we investigate the Arabic dialect identification task, from two perspectives: country-level dialect identification from 21 Arab countries, and province-level dialect identification from 100 provinces. We introduce an unified pipeline of state-of-the-art models, that can handle the two subtasks. Our experimental studies applied to the NADI shared task, show promising results both at the country-level (F1-score of 25.99%) and the province-level (F1-score of 6.39%), and thus allow us to be ranked 2nd for the country-level subtask, and 1st in the province-level subtask.</abstract>
      <url hash="519b307c">2020.wanlp-1.27</url>
      <bibkey>el-mekki-etal-2020-weighted</bibkey>
    </paper>
    <paper id="29">
      <title>Faheem at <fixed-case>NADI</fixed-case> shared task: Identifying the dialect of <fixed-case>A</fixed-case>rabic tweet</title>
      <author><first>Nouf</first><last>AlShenaifi</last></author>
      <author><first>Aqil</first><last>Azmi</last></author>
      <pages>282&#8211;287</pages>
      <abstract>This paper describes Faheem (adj. of understand), our submission to NADI (Nuanced Arabic Dialect Identification) shared task. With so many Arabic dialects being under-studied due to the scarcity of the resources, the objective is to identify the Arabic dialect used in the tweet, country wise. We propose a machine learning approach where we utilize word-level n-gram (n = 1 to 3) and tf-idf features and feed them to six different classifiers. We train the system using a data set of 21,000 tweets&#8212;provided by the organizers&#8212;covering twenty-one Arab countries. Our top performing classifiers are: Logistic Regression, Support Vector Machines, and Multinomial Na &#776;&#305;ve Bayes.</abstract>
      <url hash="d83329d1">2020.wanlp-1.29</url>
      <bibkey>alshenaifi-azmi-2020-faheem</bibkey>
    </paper>
    <paper id="31">
      <title>The <fixed-case>QMUL</fixed-case>/<fixed-case>HRBDT</fixed-case> contribution to the <fixed-case>NADI</fixed-case> <fixed-case>A</fixed-case>rabic Dialect Identification Shared Task</title>
      <author><first>Abdulrahman</first><last>Aloraini</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <author><first>Ayman</first><last>Alhelbawy</last></author>
      <pages>295&#8211;301</pages>
      <abstract>We present the Arabic dialect identification system that we used for the country-level subtask of the NADI challenge. Our model consists of three components: BiLSTM-CNN, character-level TF-IDF, and topic modeling features. We represent each tweet using these features and feed them into a deep neural network. We then add an effective heuristic that improves the overall performance. We achieved an F1-Macro score of 20.77% and an accuracy of 34.32% on the test set. The model was also evaluated on the Arabic Online Commentary dataset, achieving results better than the state-of-the-art.</abstract>
      <url hash="fa2a8391">2020.wanlp-1.31</url>
      <bibkey>aloraini-etal-2020-qmul</bibkey>
    </paper>
    </volume>
</collection>