<collection id="2020.insights">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the First Workshop on Insights from Negative Results in NLP</booktitle>
      <editor><first>Anna</first><last>Rogers</last></editor>
      <editor><first>Jo&#227;o</first><last>Sedoc</last></editor>
      <editor><first>Anna</first><last>Rumshisky</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="6e7b4663">2020.insights-1.0</url>
      <bibkey>insights-2020-insights</bibkey>
    </frontmatter>
    <paper id="5">
      <title>Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models</title>
      <author><first>Silvia</first><last>Terragni</last></author>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <author><first>Messina</first><last>Enza</last></author>
      <pages>32&#8211;40</pages>
      <abstract>Topic models have been widely used to discover hidden topics in a collection of documents. In this paper, we propose to investigate the role of two different types of relational information, i.e. document relationships and concept relationships. While exploiting the document network significantly improves topic coherence, the introduction of concepts and their relationships does not influence the results both quantitatively and qualitatively.</abstract>
      <url hash="7068a284">2020.insights-1.5</url>
      <attachment type="OptionalSupplementaryMaterial" hash="97511234">2020.insights-1.5.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.insights-1.5</doi>
      <video href="https://slideslive.com/38940792" />
      <bibkey>terragni-etal-2020-matters</bibkey>
    </paper>
    <paper id="6">
      <title>On Task-Level Dialogue Composition of Generative Transformer Model</title>
      <author><first>Prasanna</first><last>Parthasarathi</last></author>
      <author><first>Sharan</first><last>Narang</last></author>
      <author><first>Arvind</first><last>Neelakantan</last></author>
      <pages>41&#8211;47</pages>
      <abstract>Task-oriented dialogue systems help users accomplish tasks such as booking a movie ticket and ordering food via conversation. Generative models parameterized by a deep neural network are widely used for next turn response generation in such systems. It is natural for users of the system to want to accomplish multiple tasks within the same conversation, but the ability of generative models to compose multiple tasks is not well studied. In this work, we begin by studying the effect of training human-human task-oriented dialogues towards improving the ability to compose multiple tasks on Transformer generative models. To that end, we propose and explore two solutions: (1) creating synthetic multiple task dialogue data for training from human-human single task dialogue and (2) forcing the encoder representation to be invariant to single and multiple task dialogues using an auxiliary loss. The results from our experiments highlight the difficulty of even the sophisticated variant of transformer model in learning to compose multiple tasks from single task dialogues.</abstract>
      <url hash="ec3767f7">2020.insights-1.6</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2a607f54">2020.insights-1.6.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.insights-1.6</doi>
      <video href="https://slideslive.com/38940793" />
      <bibkey>parthasarathi-etal-2020-task</bibkey>
      <pwccode url="https://github.com/ppartha03/Dialogue-Compositionality-of-Generative-Transformer" additional="false">ppartha03/Dialogue-Compositionality-of-Generative-Transformer</pwccode>
    </paper>
    <paper id="8">
      <title>Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification</title>
      <author><first>Ashwin Geet</first><last>D&#8217;Sa</last></author>
      <author><first>Irina</first><last>Illina</last></author>
      <author><first>Dominique</first><last>Fohr</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Dana</first><last>Ruiter</last></author>
      <pages>54&#8211;59</pages>
      <abstract>Research on hate speech classification has received increased attention. In real-life scenarios, a small amount of labeled hate speech data is available to train a reliable classifier. Semi-supervised learning takes advantage of a small amount of labeled data and a large amount of unlabeled data. In this paper, label propagation-based semi-supervised learning is explored for the task of hate speech classification. The quality of labeling the unlabeled set depends on the input representations. In this work, we show that pre-trained representations are label agnostic, and when used with label propagation yield poor results. Neural network-based fine-tuning can be adopted to learn task-specific representations using a small amount of labeled data. We show that fully fine-tuned representations may not always be the best representations for the label propagation and intermediate representations may perform better in a semi-supervised setup.</abstract>
      <url hash="469cea02">2020.insights-1.8</url>
      <doi>10.18653/v1/2020.insights-1.8</doi>
      <video href="https://slideslive.com/38940795" />
      <bibkey>dsa-etal-2020-label</bibkey>
    </paper>
    <paper id="9">
      <title>Layout-Aware Text Representations Harm Clustering Documents by Type</title>
      <author><first>Catherine</first><last>Finegan-Dollak</last></author>
      <author><first>Ashish</first><last>Verma</last></author>
      <pages>60&#8211;65</pages>
      <abstract>Clustering documents by type&#8212;grouping invoices with invoices and articles with articles&#8212;is a desirable first step for organizing large collections of document scans. Humans approaching this task use both the semantics of the text and the document layout to assist in grouping like documents. LayoutLM (Xu et al., 2019), a layout-aware transformer built on top of BERT with state-of-the-art performance on document-type classification, could reasonably be expected to outperform regular BERT (Devlin et al., 2018) for document-type clustering. However, we find experimentally that BERT significantly outperforms LayoutLM on this task (p &lt;0.001). We analyze clusters to show where layout awareness is an asset and where it is a liability.</abstract>
      <url hash="7664a7ce">2020.insights-1.9</url>
      <doi>10.18653/v1/2020.insights-1.9</doi>
      <video href="https://slideslive.com/38940796" />
      <bibkey>finegan-dollak-verma-2020-layout</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/rvl-cdip">RVL-CDIP</pwcdataset>
    </paper>
    </volume>
</collection>