<collection id="2020.knlp">
  <volume id="1" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP</booktitle>
      <editor><first>Oren Sar</first><last>Shalom</last></editor>
      <editor><first>Alexander</first><last>Panchenko</last></editor>
      <editor><first>Cicero</first><last>dos Santos</last></editor>
      <editor><first>Varvara</first><last>Logacheva</last></editor>
      <editor><first>Alessandro</first><last>Moschitti</last></editor>
      <editor><first>Ido</first><last>Dagan</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="61ff76fc">2020.knlp-1.0</url>
      <bibkey>knlp-2020-knowledgeable</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Social Media Medical Concept Normalization using <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a in Ontology Enriched Text Similarity Framework</title>
      <author><first>Katikapalli Subramanyam</first><last>Kalyan</last></author>
      <author><first>Sivanesan</first><last>Sangeetha</last></author>
      <pages>21&#8211;26</pages>
      <abstract>Pattisapu et al. (2020) formulate medical concept normalization (MCN) as text similarity problem and propose a model based on RoBERTa and graph embedding based target concept vectors. However, graph embedding techniques ignore valuable information available in the clinical ontology like concept description and synonyms. In this work, we enhance the model of Pattisapu et al. (2020) with two novel changes. First, we use retrofitted target concept vectors instead of graph embedding based vectors. It is the first work to leverage both concept description and synonyms to represent concepts in the form of retrofitted target concept vectors in text similarity framework based social media MCN. Second, we generate both concept and concept mention vectors with same size which eliminates the need of dense layers to project concept mention vectors into the target concept embedding space. Our model outperforms existing methods with improvements up to 3.75% on two standard datasets. Further when trained only on mapping lexicon synonyms, our model outperforms existing methods with significant improvements up to 14.61%. We attribute these significant improvements to the two novel changes introduced.</abstract>
      <url hash="ff2a5c1b">2020.knlp-1.3</url>
      <bibkey>kalyan-sangeetha-2020-social</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>BERTC</fixed-case>hem-<fixed-case>DDI</fixed-case> : Improved Drug-Drug Interaction Prediction from text using Chemical Structure Information</title>
      <author><first>Ishani</first><last>Mondal</last></author>
      <pages>27&#8211;32</pages>
      <abstract>Traditional biomedical version of embeddings obtained from pre-trained language models have recently shown state-of-the-art results for relation extraction (RE) tasks in the medical domain. In this paper, we explore how to incorporate domain knowledge, available in the form of molecular structure of drugs, for predicting Drug-Drug Interaction from textual corpus. We propose a method, BERTChem-DDI, to efficiently combine drug embeddings obtained from the rich chemical structure of drugs (encoded in SMILES) along with off-the-shelf domain-specific BioBERT embedding-based RE architecture. Experiments conducted on the DDIExtraction 2013 corpus clearly indicate that this strategy improves other strong baselines architectures by 3.4% macro F1-score.</abstract>
      <url hash="873fd0c4">2020.knlp-1.4</url>
      <bibkey>mondal-2020-bertchem</bibkey>
    </paper>
  </volume>
</collection>