<collection id="2020.eval4nlp">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</booktitle>
      <editor><first>Steffen</first><last>Eger</last></editor>
      <editor><first>Yang</first><last>Gao</last></editor>
      <editor><first>Maxime</first><last>Peyrard</last></editor>
      <editor><first>Wei</first><last>Zhao</last></editor>
      <editor><first>Eduard</first><last>Hovy</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="f2f7d694">2020.eval4nlp-1.0</url>
      <bibkey>eval4nlp-2020-evaluation</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Item Response Theory for Efficient Human Evaluation of Chatbots</title>
      <author><first>Jo&#227;o</first><last>Sedoc</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <pages>21&#8211;33</pages>
      <abstract>Conversational agent quality is currently assessed using human evaluation, and often requires an exorbitant number of comparisons to achieve statistical significance. In this paper, we introduce Item Response Theory (IRT) for chatbot evaluation, using a paired comparison in which annotators judge which system responds better to the next turn of a conversation. IRT is widely used in educational testing for simultaneously assessing the ability of test takers and the quality of test questions. It is similarly well suited for chatbot evaluation since it allows the assessment of both models and the prompts used to evaluate them. We use IRT to efficiently assess chatbots, and show that different examples from the evaluation set are better suited for comparing high-quality (nearer to human performance) than low-quality systems. Finally, we use IRT to reduce the number of evaluation examples assessed by human annotators while retaining discriminative power.</abstract>
      <url hash="a90a3c48">2020.eval4nlp-1.3</url>
      <attachment type="OptionalSupplementaryMaterial" hash="d2341742">2020.eval4nlp-1.3.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.eval4nlp-1.3</doi>
      <video href="https://slideslive.com/38939718" />
      <bibkey>sedoc-ungar-2020-item</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>V</fixed-case>i<fixed-case>LBERTS</fixed-case>core: Evaluating Image Caption Using Vision-and-Language <fixed-case>BERT</fixed-case></title>
      <author><first>Hwanhee</first><last>Lee</last></author>
      <author><first>Seunghyun</first><last>Yoon</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Doo Soon</first><last>Kim</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>34&#8211;39</pages>
      <abstract>In this paper, we propose an evaluation metric for image captioning systems using both image and text information. Unlike the previous methods that rely on textual representations in evaluating the caption, our approach uses visiolinguistic representations. The proposed method generates image-conditioned embeddings for each token using ViLBERT from both generated and reference texts. Then, these contextual embeddings from each of the two sentence-pair are compared to compute the similarity score. Experimental results on three benchmark datasets show that our method correlates significantly better with human judgments than all existing metrics.</abstract>
      <url hash="fa4d3c89">2020.eval4nlp-1.4</url>
      <doi>10.18653/v1/2020.eval4nlp-1.4</doi>
      <video href="https://slideslive.com/38939719" />
      <bibkey>lee-etal-2020-vilbertscore</bibkey>
      <pwccode url="https://github.com/hwanheelee1993/vilbertscore" additional="false">hwanheelee1993/vilbertscore</pwccode>
    </paper>
    <paper id="5">
      <title><fixed-case>BLEU</fixed-case> Neighbors: A Reference-less Approach to Automatic Evaluation</title>
      <author><first>Kawin</first><last>Ethayarajh</last></author>
      <author><first>Dorsa</first><last>Sadigh</last></author>
      <pages>40&#8211;50</pages>
      <abstract>Evaluation is a bottleneck in the development of natural language generation (NLG) models. Automatic metrics such as BLEU rely on references, but for tasks such as open-ended generation, there are no references to draw upon. Although language diversity can be estimated using statistical measures such as perplexity, measuring language quality requires human evaluation. However, because human evaluation at scale is slow and expensive, it is used sparingly; it cannot be used to rapidly iterate on NLG models, in the way BLEU is used for machine translation. To this end, we propose BLEU Neighbors, a nearest neighbors model for estimating language quality by using the BLEU score as a kernel function. On existing datasets for chitchat dialogue and open-ended sentence generation, we find that &#8211; on average &#8211; the quality estimation from a BLEU Neighbors model has a lower mean squared error and higher Spearman correlation with the ground truth than individual human annotators. Despite its simplicity, BLEU Neighbors even outperforms state-of-the-art models on automatically grading essays, including models that have access to a gold-standard reference essay.</abstract>
      <url hash="0e857069">2020.eval4nlp-1.5</url>
      <doi>10.18653/v1/2020.eval4nlp-1.5</doi>
      <video href="https://slideslive.com/38939709" />
      <bibkey>ethayarajh-sadigh-2020-bleu</bibkey>
    </paper>
    <paper id="8">
      <title>Artemis: A Novel Annotation Methodology for Indicative Single Document Summarization</title>
      <author><first>Rahul</first><last>Jha</last></author>
      <author><first>Keping</first><last>Bi</last></author>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Mahdi</first><last>Pakdaman</last></author>
      <author><first>Asli</first><last>Celikyilmaz</last></author>
      <author><first>Ivan</first><last>Zhiboedov</last></author>
      <author><first>Kieran</first><last>McDonald</last></author>
      <pages>69&#8211;78</pages>
      <abstract>We describe Artemis (Annotation methodology for Rich, Tractable, Extractive, Multi-domain, Indicative Summarization), a novel hierarchical annotation process that produces indicative summaries for documents from multiple domains. Current summarization evaluation datasets are single-domain and focused on a few domains for which naturally occurring summaries can be easily found, such as news and scientific articles. These are not sufficient for training and evaluation of summarization models for use in document management and information retrieval systems, which need to deal with documents from multiple domains. Compared to other annotation methods such as Relative Utility and Pyramid, Artemis is more tractable because judges don&#8217;t need to look at all the sentences in a document when making an importance judgment for one of the sentences, while providing similarly rich sentence importance annotations. We describe the annotation process in detail and compare it with other similar evaluation systems. We also present analysis and experimental results over a sample set of 532 annotated documents.</abstract>
      <url hash="e8d1d984">2020.eval4nlp-1.8</url>
      <doi>10.18653/v1/2020.eval4nlp-1.8</doi>
      <video href="https://slideslive.com/38939707" />
      <bibkey>jha-etal-2020-artemis</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="9">
      <title>Probabilistic Extension of Precision, Recall, and F1 Score for More Thorough Evaluation of Classification Models</title>
      <author><first>Reda</first><last>Yacouby</last></author>
      <author><first>Dustin</first><last>Axman</last></author>
      <pages>79&#8211;91</pages>
      <abstract>In pursuit of the perfect supervised NLP classifier, razor thin margins and low-resource test sets can make modeling decisions difficult. Popular metrics such as Accuracy, Precision, and Recall are often insufficient as they fail to give a complete picture of the model&#8217;s behavior. We present a probabilistic extension of Precision, Recall, and F1 score, which we refer to as confidence-Precision (cPrecision), confidence-Recall (cRecall), and confidence-F1 (cF1) respectively. The proposed metrics address some of the challenges faced when evaluating large-scale NLP systems, specifically when the model&#8217;s confidence score assignments have an impact on the system&#8217;s behavior. We describe four key benefits of our proposed metrics as compared to their threshold-based counterparts. Two of these benefits, which we refer to as robustness to missing values and sensitivity to model confidence score assignments are self-evident from the metrics&#8217; definitions; the remaining benefits, generalization, and functional consistency are demonstrated empirically.</abstract>
      <url hash="7a0e6c99">2020.eval4nlp-1.9</url>
      <doi>10.18653/v1/2020.eval4nlp-1.9</doi>
      <video href="https://slideslive.com/38939710" />
      <bibkey>yacouby-axman-2020-probabilistic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="14">
      <title>On Aligning <fixed-case>O</fixed-case>pen<fixed-case>IE</fixed-case> Extractions with Knowledge Bases: A Case Study</title>
      <author><first>Kiril</first><last>Gashteovski</last></author>
      <author><first>Rainer</first><last>Gemulla</last></author>
      <author><first>Bhushan</first><last>Kotnis</last></author>
      <author><first>Sven</first><last>Hertling</last></author>
      <author><first>Christian</first><last>Meilicke</last></author>
      <pages>143&#8211;154</pages>
      <abstract>Open information extraction (OIE) is the task of extracting relations and their corresponding arguments from a natural language text in un- supervised manner. Outputs of such systems are used for downstream tasks such as ques- tion answering and automatic knowledge base (KB) construction. Many of these downstream tasks rely on aligning OIE triples with refer- ence KBs. Such alignments are usually eval- uated w.r.t. a specific downstream task and, to date, no direct manual evaluation of such alignments has been performed. In this paper, we directly evaluate how OIE triples from the OPIEC corpus are related to the DBpedia KB w.r.t. information content. First, we investigate OPIEC triples and DBpedia facts having the same arguments by comparing the information on the OIE surface relation with the KB rela- tion. Second, we evaluate the expressibility of general OPIEC triples in DBpedia. We in- vestigate whether&#8212;and, if so, how&#8212;a given OIE triple can be mapped to a single KB fact. We found that such mappings are not always possible because the information in the OIE triples tends to be more specific. Our evalua- tion suggests, however, that significant part of OIE triples can be expressed by means of KB formulas instead of individual facts.</abstract>
      <url hash="b2d4d5b3">2020.eval4nlp-1.14</url>
      <doi>10.18653/v1/2020.eval4nlp-1.14</doi>
      <video href="https://slideslive.com/38939720" />
      <bibkey>gashteovski-etal-2020-aligning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opiec">OPIEC</pwcdataset>
    </paper>
    <paper id="15">
      <title><fixed-case>C</fixed-case>luster<fixed-case>D</fixed-case>ata<fixed-case>S</fixed-case>plit: Exploring Challenging Clustering-Based Data Splits for Model Performance Evaluation</title>
      <author><first>Hanna</first><last>Wecker</last></author>
      <author><first>Annemarie</first><last>Friedrich</last></author>
      <author><first>Heike</first><last>Adel</last></author>
      <pages>155&#8211;163</pages>
      <abstract>This paper adds to the ongoing discussion in the natural language processing community on how to choose a good development set. Motivated by the real-life necessity of applying machine learning models to different data distributions, we propose a clustering-based data splitting algorithm. It creates development (or test) sets which are lexically different from the training data while ensuring similar label distributions. Hence, we are able to create challenging cross-validation evaluation setups while abstracting away from performance differences resulting from label distribution shifts between training and test data. In addition, we present a Python-based tool for analyzing and visualizing data split characteristics and model performance. We illustrate the workings and results of our approach using a sentiment analysis and a patent classification task.</abstract>
      <url hash="784206c5">2020.eval4nlp-1.15</url>
      <doi>10.18653/v1/2020.eval4nlp-1.15</doi>
      <video href="https://slideslive.com/38939708" />
      <bibkey>wecker-etal-2020-clusterdatasplit</bibkey>
      <pwccode url="https://github.com/boschresearch/clusterdatasplit_eval4nlp-2020" additional="false">boschresearch/clusterdatasplit_eval4nlp-2020</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="16">
      <title>Best Practices for Crowd-based Evaluation of <fixed-case>G</fixed-case>erman Summarization: Comparing Crowd, Expert and Automatic Evaluation</title>
      <author><first>Neslihan</first><last>Iskender</last></author>
      <author><first>Tim</first><last>Polzehl</last></author>
      <author><first>Sebastian</first><last>M&#246;ller</last></author>
      <pages>164&#8211;175</pages>
      <abstract>One of the main challenges in the development of summarization tools is summarization quality evaluation. On the one hand, the human assessment of summarization quality conducted by linguistic experts is slow, expensive, and still not a standardized procedure. On the other hand, the automatic assessment metrics are reported not to correlate high enough with human quality ratings. As a solution, we propose crowdsourcing as a fast, scalable, and cost-effective alternative to expert evaluations to assess the intrinsic and extrinsic quality of summarization by comparing crowd ratings with expert ratings and automatic metrics such as ROUGE, BLEU, or BertScore on a German summarization data set. Our results provide a basis for best practices for crowd-based summarization evaluation regarding major influential factors such as the best annotation aggregation method, the influence of readability and reading effort on summarization evaluation, and the optimal number of crowd workers to achieve comparable results to experts, especially when determining factors such as overall quality, grammaticality, referential clarity, focus, structure &amp; coherence, summary usefulness, and summary informativeness.</abstract>
      <url hash="8e2697a5">2020.eval4nlp-1.16</url>
      <doi>10.18653/v1/2020.eval4nlp-1.16</doi>
      <video href="https://slideslive.com/38939713" />
      <bibkey>iskender-etal-2020-best</bibkey>
    </paper>
    <paper id="17">
      <title>Evaluating Word Embeddings on Low-Resource Languages</title>
      <author><first>Nathan</first><last>Stringham</last></author>
      <author><first>Mike</first><last>Izbicki</last></author>
      <pages>176&#8211;186</pages>
      <abstract>The analogy task introduced by Mikolov et al. (2013) has become the standard metric for tuning the hyperparameters of word embedding models. In this paper, however, we argue that the analogy task is unsuitable for low-resource languages for two reasons: (1) it requires that word embeddings be trained on large amounts of text, and (2) analogies may not be well-defined in some low-resource settings. We solve these problems by introducing the OddOneOut and Topk tasks, which are specifically designed for model selection in the low-resource setting. We use these metrics to successfully tune hyperparameters for a low-resource emoji embedding task and word embeddings on 16 extinct languages. The largest of these languages (Ancient Hebrew) has a 41 million token dataset, and the smallest (Old Gujarati) has only a 1813 token dataset.</abstract>
      <url hash="015e5edb">2020.eval4nlp-1.17</url>
      <doi>10.18653/v1/2020.eval4nlp-1.17</doi>
      <video href="https://slideslive.com/38939712" />
      <bibkey>stringham-izbicki-2020-evaluating</bibkey>
    </paper>
  </volume>
</collection>