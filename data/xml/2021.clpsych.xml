<collection id="2021.clpsych">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access</booktitle>
      <editor><first>Nazli</first><last>Goharian</last></editor>
      <editor><first>Philip</first><last>Resnik</last></editor>
      <editor><first>Andrew</first><last>Yates</last></editor>
      <editor><first>Molly</first><last>Ireland</last></editor>
      <editor><first>Kate</first><last>Niederhoffer</last></editor>
      <editor><first>Rebecca</first><last>Resnik</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.clpsych-1</url>
    </meta>
    <frontmatter>
      <url hash="a311aa20">2021.clpsych-1.0</url>
      <bibkey>clpsych-2021-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Understanding who uses <fixed-case>R</fixed-case>eddit: Profiling individuals with a self-reported bipolar disorder diagnosis</title>
      <author><first>Glorianna</first><last>Jagfeld</last></author>
      <author><first>Fiona</first><last>Lobban</last></author>
      <author><first>Paul</first><last>Rayson</last></author>
      <author><first>Steven</first><last>Jones</last></author>
      <pages>1&#8211;14</pages>
      <abstract>Recently, research on mental health conditions using public online data, including Reddit, has surged in NLP and health research but has not reported user characteristics, which are important to judge generalisability of findings. This paper shows how existing NLP methods can yield information on clinical, demographic, and identity characteristics of almost 20K Reddit users who self-report a bipolar disorder diagnosis. This population consists of slightly more feminine- than masculine-gendered mainly young or middle-aged US-based adults who often report additional mental health diagnoses, which is compared with general Reddit statistics and epidemiological studies. Additionally, this paper carefully evaluates all methods and discusses ethical issues.</abstract>
      <url hash="e918a603">2021.clpsych-1.1</url>
      <doi>10.18653/v1/2021.clpsych-1.1</doi>
      <bibkey>jagfeld-etal-2021-understanding</bibkey>
      <pwccode url="https://github.com/glorisonne/reddit_bd_user_characteristics" additional="false">glorisonne/reddit_bd_user_characteristics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/smhd">SMHD</pwcdataset>
    </paper>
    <paper id="3">
      <title>Individual Differences in the Movement-Mood Relationship in Digital Life Data</title>
      <author><first>Glen</first><last>Coppersmith</last></author>
      <author><first>Alex</first><last>Fine</last></author>
      <author><first>Patrick</first><last>Crutchley</last></author>
      <author><first>Joshua</first><last>Carroll</last></author>
      <pages>25&#8211;31</pages>
      <abstract>Our increasingly digitized lives generate troves of data that reflect our behavior, beliefs, mood, and wellbeing. Such &#8220;digital life data&#8221; provides crucial insight into the lives of patients outside the healthcare setting that has long been lacking, from a better understanding of mundane patterns of exercise and sleep routines to harbingers of emotional crisis. Moreover, information about individual differences and personalities is encoded in digital life data. In this paper we examine the relationship between mood and movement using linguistic and biometric data, respectively. Does increased physical activity (movement) have an effect on a person&#8217;s mood (or vice-versa)? We find that weak group-level relationships between movement and mood mask interesting and often strong relationships between the two for individuals within the group. We describe these individual differences, and argue that individual variability in the relationship between movement and mood is one of many such factors that ought be taken into account in wellbeing-focused apps and AI systems.</abstract>
      <url hash="8508a7cd">2021.clpsych-1.3</url>
      <doi>10.18653/v1/2021.clpsych-1.3</doi>
      <bibkey>coppersmith-etal-2021-individual</bibkey>
    </paper>
    <paper id="10">
      <title>Suicide Risk Prediction by Tracking Self-Harm Aspects in Tweets: <fixed-case>NUS</fixed-case>-<fixed-case>IDS</fixed-case> at the <fixed-case>CLP</fixed-case>sych 2021 Shared Task</title>
      <author><first>Sujatha Das</first><last>Gollapalli</last></author>
      <author><first>Guilherme Augusto</first><last>Zagatti</last></author>
      <author><first>See-Kiong</first><last>Ng</last></author>
      <pages>93&#8211;98</pages>
      <abstract>We describe our system for identifying users at-risk for suicide based on their tweets developed for the CLPsych 2021 Shared Task. Based on research in mental health studies linking self-harm tendencies with suicide, in our system, we attempt to characterize self-harm aspects expressed in user tweets over a period of time. To this end, we design SHTM, a Self-Harm Topic Model that combines Latent Dirichlet Allocation with a self-harm dictionary for modeling daily tweets of users. Next, differences in moods and topics over time are captured as features to train a deep learning model for suicide prediction.</abstract>
      <url hash="90f8d91f">2021.clpsych-1.10</url>
      <attachment type="OptionalSupplementaryCode" hash="a588af6c">2021.clpsych-1.10.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="a0bd310f">2021.clpsych-1.10.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.clpsych-1.10</doi>
      <bibkey>gollapalli-etal-2021-suicide</bibkey>
    </paper>
    <paper id="12">
      <title>Using Psychologically-Informed Priors for Suicide Prediction in the <fixed-case>CLP</fixed-case>sych 2021 Shared Task</title>
      <author><first>Avi</first><last>Gamoran</last></author>
      <author><first>Yonatan</first><last>Kaplan</last></author>
      <author><first>Almog</first><last>Simchon</last></author>
      <author><first>Michael</first><last>Gilead</last></author>
      <pages>103&#8211;109</pages>
      <abstract>This paper describes our approach to the CLPsych 2021 Shared Task, in which we aimed to predict suicide attempts based on Twitter feed data. We addressed this challenge by emphasizing reliance on prior domain knowledge. We engineered novel theory-driven features, and integrated prior knowledge with empirical evidence in a principled manner using Bayesian modeling. While this theory-guided approach increases bias and lowers accuracy on the training set, it was successful in preventing over-fitting. The models provided reasonable classification accuracy on unseen test data (0.68&lt;=AUC&lt;= 0.84). Our approach may be particularly useful in prediction tasks trained on a relatively small data set.</abstract>
      <url hash="a10c45ed">2021.clpsych-1.12</url>
      <doi>10.18653/v1/2021.clpsych-1.12</doi>
      <bibkey>gamoran-etal-2021-using</bibkey>
    </paper>
    <paper id="13">
      <title>Analysis of Behavior Classification in Motivational Interviewing</title>
      <author><first>Leili</first><last>Tavabi</last></author>
      <author><first>Trang</first><last>Tran</last></author>
      <author><first>Kalin</first><last>Stefanov</last></author>
      <author><first>Brian</first><last>Borsari</last></author>
      <author><first>Joshua</first><last>Woolley</last></author>
      <author><first>Stefan</first><last>Scherer</last></author>
      <author><first>Mohammad</first><last>Soleymani</last></author>
      <pages>110&#8211;115</pages>
      <abstract>Analysis of client and therapist behavior in counseling sessions can provide helpful insights for assessing the quality of the session and consequently, the client&#8217;s behavioral outcome. In this paper, we study the automatic classification of standardized behavior codes (annotations) used for assessment of psychotherapy sessions in Motivational Interviewing (MI). We develop models and examine the classification of client behaviors throughout MI sessions, comparing the performance by models trained on large pretrained embeddings (RoBERTa) versus interpretable and expert-selected features (LIWC). Our best performing model using the pretrained RoBERTa embeddings beats the baseline model, achieving an F1 score of 0.66 in the subject-independent 3-class classification. Through statistical analysis on the classification results, we identify prominent LIWC features that may not have been captured by the model using pretrained embeddings. Although classification using LIWC features underperforms RoBERTa, our findings motivate the future direction of incorporating auxiliary tasks in the classification of MI codes.</abstract>
      <url hash="b532c9da">2021.clpsych-1.13</url>
      <doi>10.18653/v1/2021.clpsych-1.13</doi>
      <bibkey>tavabi-etal-2021-analysis</bibkey>
    </paper>
    <paper id="14">
      <title>Automatic Detection and Prediction of Psychiatric Hospitalizations From Social Media Posts</title>
      <author><first>Zhengping</first><last>Jiang</last></author>
      <author><first>Jonathan</first><last>Zomick</last></author>
      <author><first>Sarah Ita</first><last>Levitan</last></author>
      <author><first>Mark</first><last>Serper</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <pages>116&#8211;121</pages>
      <abstract>We address the problem of predicting psychiatric hospitalizations using linguistic features drawn from social media posts. We formulate this novel task and develop an approach to automatically extract time spans of self-reported psychiatric hospitalizations. Using this dataset, we build predictive models of psychiatric hospitalization, comparing feature sets, user vs. post classification, and comparing model performance using a varying time window of posts. Our best model achieves an F1 of .718 using 7 days of posts. Our results suggest that this is a useful framework for collecting hospitalization data, and that social media data can be leveraged to predict acute psychiatric crises before they occur, potentially saving lives and improving outcomes for individuals with mental illness.</abstract>
      <url hash="3273c7fe">2021.clpsych-1.14</url>
      <doi>10.18653/v1/2021.clpsych-1.14</doi>
      <bibkey>jiang-etal-2021-automatic</bibkey>
    </paper>
    <paper id="16">
      <title>Automated coherence measures fail to index thought disorder in individuals at risk for psychosis</title>
      <author><first>Kasia</first><last>Hitczenko</last></author>
      <author><first>Henry</first><last>Cowan</last></author>
      <author><first>Vijay</first><last>Mittal</last></author>
      <author><first>Matthew</first><last>Goldrick</last></author>
      <pages>129&#8211;150</pages>
      <abstract>Thought disorder &#8211; linguistic disturbances including incoherence and derailment of topic &#8211; is seen in individuals both with and at risk for psychosis. Methods from computational linguistics have increasingly sought to quantify thought disorder to detect group differences between clinical populations and healthy controls. While previous work has been quite successful at these classification tasks, the lack of interpretability of the computational metrics has made it unclear whether they are in fact measuring thought disorder. In this paper, we dive into these measures to try to better understand what they reflect. While we find group differences between at-risk and healthy control populations, we also find that the measures mostly do not correlate with existing measures of thought disorder symptoms (what they are intended to measure), but rather correlate with surface properties of the speech (e.g., sentence length) and sociodemographic properties of the speaker (e.g., race). These results highlight the importance of considering interpretability and front and center as the field continues to grow. Ethical use of computational measures like those studied here &#8211; especially in the high-stakes context of clinical care &#8211; requires us to devote substantial attention to potential biases in our measures.</abstract>
      <url hash="fc1f0849">2021.clpsych-1.16</url>
      <doi>10.18653/v1/2021.clpsych-1.16</doi>
      <bibkey>hitczenko-etal-2021-automated</bibkey>
    </paper>
    <paper id="17">
      <title>Detecting Cognitive Distortions from Patient-Therapist Interactions</title>
      <author><first>Sagarika</first><last>Shreevastava</last></author>
      <author><first>Peter</first><last>Foltz</last></author>
      <pages>151&#8211;158</pages>
      <abstract>An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. The aim of this project is to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pre-trained Sentence-BERT embeddings to train an SVM classifier yields the best results with an F1-score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions.</abstract>
      <url hash="00e3bcee">2021.clpsych-1.17</url>
      <doi>10.18653/v1/2021.clpsych-1.17</doi>
      <bibkey>shreevastava-foltz-2021-detecting</bibkey>
    </paper>
    <paper id="18">
      <title>Evaluating Automatic Speech Recognition Quality and Its Impact on Counselor Utterance Coding</title>
      <author><first>Do June</first><last>Min</last></author>
      <author><first>Ver&#243;nica</first><last>P&#233;rez-Rosas</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>159&#8211;168</pages>
      <abstract>Automatic speech recognition (ASR) is a crucial step in many natural language processing (NLP) applications, as often available data consists mainly of raw speech. Since the result of the ASR step is considered as a meaningful, informative input to later steps in the NLP pipeline, it is important to understand the behavior and failure mode of this step. In this work, we analyze the quality of ASR in the psychotherapy domain, using motivational interviewing conversations between therapists and clients. We conduct domain agnostic and domain-relevant evaluations using standard evaluation metrics and also identify domain-relevant keywords in the ASR output. Moreover, we empirically study the effect of mixing ASR and manual data during the training of a downstream NLP model, and also demonstrate how additional local context can help alleviate the error introduced by noisy ASR transcripts.</abstract>
      <url hash="70e338c6">2021.clpsych-1.18</url>
      <doi>10.18653/v1/2021.clpsych-1.18</doi>
      <bibkey>min-etal-2021-evaluating</bibkey>
    </paper>
    <paper id="20">
      <title>Safeguarding against spurious <fixed-case>AI</fixed-case>-based predictions: The case of automated verbal memory assessment</title>
      <author><first>Chelsea</first><last>Chandler</last></author>
      <author><first>Peter</first><last>Foltz</last></author>
      <author><first>Alex</first><last>Cohen</last></author>
      <author><first>Terje</first><last>Holmlund</last></author>
      <author><first>Brita</first><last>Elvev&#229;g</last></author>
      <pages>181&#8211;191</pages>
      <abstract>A growing amount of psychiatric research incorporates machine learning and natural language processing methods, however findings have yet to be translated into actual clinical decision support systems. Many of these studies are based on relatively small datasets in homogeneous populations, which has the associated risk that the models may not perform adequately on new data in real clinical practice. The nature of serious mental illness is that it is hard to define, hard to capture, and requires frequent monitoring, which leads to imperfect data where attribute and class noise are common. With the goal of an effective AI-mediated clinical decision support system, there must be computational safeguards placed on the models used in order to avoid spurious predictions and thus allow humans to review data in the settings where models are unstable or bound not to generalize. This paper describes two approaches to implementing safeguards: (1) the determination of cases in which models are unstable by means of attribute and class based outlier detection and (2) finding the extent to which models show inductive bias. These safeguards are illustrated in the automated scoring of a story recall task via natural language processing methods. With the integration of human-in-the-loop machine learning in the clinical implementation process, incorporating safeguards such as these into the models will offer patients increased protection from spurious predictions.</abstract>
      <url hash="57ac577c">2021.clpsych-1.20</url>
      <doi>10.18653/v1/2021.clpsych-1.20</doi>
      <bibkey>chandler-etal-2021-safeguarding</bibkey>
    </paper>
    <paper id="23">
      <title>Towards Understanding the Role of Gender in Deploying Social Media-Based Mental Health Surveillance Models</title>
      <author><first>Eli</first><last>Sherman</last></author>
      <author><first>Keith</first><last>Harrigian</last></author>
      <author><first>Carlos</first><last>Aguirre</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>217&#8211;223</pages>
      <abstract>Spurred by advances in machine learning and natural language processing, developing social media-based mental health surveillance models has received substantial recent attention. For these models to be maximally useful, it is necessary to understand how they perform on various subgroups, especially those defined in terms of protected characteristics. In this paper we study the relationship between user demographics &#8211; focusing on gender &#8211; and depression. Considering a population of Reddit users with known genders and depression statuses, we analyze the degree to which depression predictions are subject to biases along gender lines using domain-informed classifiers. We then study our models&#8217; parameters to gain qualitative insight into the differences in posting behavior across genders.</abstract>
      <url hash="03e554e8">2021.clpsych-1.23</url>
      <doi>10.18653/v1/2021.clpsych-1.23</doi>
      <bibkey>sherman-etal-2021-towards</bibkey>
    </paper>
    </volume>
</collection>