<collection id="2021.codi">
  <volume id="main" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Computational Approaches to Discourse</booktitle>
      <editor><first>Chlo&#233;</first><last>Braud</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Junyi Jessy</first><last>Li</last></editor>
      <editor><first>Annie</first><last>Louis</last></editor>
      <editor><first>Michael</first><last>Strube</last></editor>
      <editor><first>Amir</first><last>Zeldes</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic and Online</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="5c380ae2">2021.codi-main.0</url>
      <bibkey>codi-2021-approaches</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Developing Conversational Data and Detection of Conversational Humor in <fixed-case>T</fixed-case>elugu</title>
      <author><first>Vaishnavi</first><last>Pamulapati</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>12&#8211;19</pages>
      <abstract>In the field of humor research, there has been a recent surge of interest in the sub-domain of Conversational Humor (CH). This study has two main objectives. (a) develop a conversational (humorous and non-humorous) dataset in Telugu. (b) detect CH in the compiled dataset. In this paper, the challenges faced while collecting the data and experiments carried out are elucidated. Transfer learning and non-transfer learning techniques are implemented by utilizing pre-trained models such as FastText word embeddings, BERT language models and Text GCN, which learns the word and document embeddings simultaneously of the corpus given. State-of-the-art results are observed with a 99.3% accuracy and a 98.5% f1 score achieved by BERT.</abstract>
      <url hash="0ae5fcbe">2021.codi-main.2</url>
      <bibkey>pamulapati-mamidi-2021-developing</bibkey>
      <doi>10.18653/v1/2021.codi-main.2</doi>
    </paper>
    <paper id="9">
      <title>Comparison of methods for explicit discourse connective identification across various domains</title>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Tianai</first><last>Dong</last></author>
      <author><first>Frances</first><last>Yung</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>95&#8211;106</pages>
      <abstract>Existing parse methods use varying approaches to identify explicit discourse connectives, but their performance has not been consistently evaluated in comparison to each other, nor have they been evaluated consistently on text other than newspaper articles. We here assess the performance on explicit connective identification of three parse methods (PDTB e2e, Lin et al., 2014; the winner of CONLL2015, Wang et al., 2015; and DisSent, Nie et al., 2019), along with a simple heuristic. We also examine how well these systems generalize to different datasets, namely written newspaper text (PDTB), written scientific text (BioDRB), prepared spoken text (TED-MDB) and spontaneous spoken text (Disco-SPICE). The results show that the e2e parser outperforms the other parse methods in all datasets. However, performance drops significantly from the PDTB to all other datasets. We provide a more fine-grained analysis of domain differences and connectives that prove difficult to parse, in order to highlight the areas where gains can be made.</abstract>
      <url hash="0b025329">2021.codi-main.9</url>
      <bibkey>scholman-etal-2021-comparison</bibkey>
      <doi>10.18653/v1/2021.codi-main.9</doi>
    </paper>
    <paper id="10">
      <title>Revisiting Shallow Discourse Parsing in the <fixed-case>PDTB</fixed-case>-3: Handling Intra-sentential Implicits</title>
      <author><first>Zheng</first><last>Zhao</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <pages>107&#8211;121</pages>
      <abstract>In the PDTB-3, several thousand implicit discourse relations were newly annotated within individual sentences, adding to the over 15,000 implicit relations annotated across adjacent sentences in the PDTB-2. Given that the position of the arguments to these intra-sentential implicits is no longer as well-defined as with inter-sentential implicits, a discourse parser must identify both their location and their sense. That is the focus of the current work. The paper provides a comprehensive analysis of our results, showcasing model performance under different scenarios, pointing out limitations and noting future directions.</abstract>
      <url hash="5e65bfe2">2021.codi-main.10</url>
      <bibkey>zhao-webber-2021-revisiting</bibkey>
      <doi>10.18653/v1/2021.codi-main.10</doi>
    </paper>
    <paper id="12">
      <title>discopy: A Neural System for Shallow Discourse Parsing</title>
      <author><first>Ren&#233;</first><last>Knaebel</last></author>
      <pages>128&#8211;133</pages>
      <abstract>This paper demonstrates discopy, a novel framework that makes it easy to design components for end-to-end shallow discourse parsing. For the purpose of demonstration, we implement recent neural approaches and integrate contextualized word embeddings to predict explicit and non-explicit discourse relations. Our proposed neural feature-free system performs competitively to systems presented at the latest Shared Task on Shallow Discourse Parsing. Finally, a web front end is shown that simplifies the inspection of annotated documents. The source code, documentation, and pretrained models are publicly accessible.</abstract>
      <url hash="2de58919">2021.codi-main.12</url>
      <bibkey>knaebel-2021-discopy</bibkey>
      <doi>10.18653/v1/2021.codi-main.12</doi>
      <pwccode url="https://github.com/rknaebel/discopy" additional="false">rknaebel/discopy</pwccode>
    </paper>
    <paper id="14">
      <title>Capturing document context inside sentence-level neural machine translation models with self-training</title>
      <author><first>Elman</first><last>Mansimov</last></author>
      <author><first>G&#225;bor</first><last>Melis</last></author>
      <author><first>Lei</first><last>Yu</last></author>
      <pages>143&#8211;153</pages>
      <abstract>Neural machine translation (NMT) has arguably achieved human level parity when trained and evaluated at the sentence-level. Document-level neural machine translation has received less attention and lags behind its sentence-level counterpart. The majority of the proposed document-level approaches investigate ways of conditioning the model on several source or target sentences to capture document context. These approaches require training a specialized NMT model from scratch on parallel document-level corpora. We propose an approach that doesn&#8217;t require training a specialized model on parallel document-level corpora and is applied to a trained sentence-level NMT model at decoding time. We process the document from left to right multiple times and self-train the sentence-level model on pairs of source sentences and generated translations. Our approach reinforces the choices made by the model, thus making it more likely that the same choices will be made in other sentences in the document. We evaluate our approach on three document-level datasets: NIST Chinese-English, WMT19 Chinese-English and OpenSubtitles English-Russian. We demonstrate that our approach has higher BLEU score and higher human preference than the baseline. Qualitative analysis of our approach shows that choices made by model are consistent across the document.</abstract>
      <url hash="92f91b85">2021.codi-main.14</url>
      <bibkey>mansimov-etal-2021-capturing</bibkey>
      <doi>10.18653/v1/2021.codi-main.14</doi>
    </paper>
    </volume>
  <volume id="sharedtask" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</booktitle>
      <editor><first>Sopan</first><last>Khosla</last></editor>
      <editor><first>Ramesh</first><last>Manuvinakurike</last></editor>
      <editor><first>Vincent</first><last>Ng</last></editor>
      <editor><first>Massimo</first><last>Poesio</last></editor>
      <editor><first>Michael</first><last>Strube</last></editor>
      <editor><first>Carolyn</first><last>Ros&#233;</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="ac624928">2021.codi-sharedtask.0</url>
      <bibkey>codicrac-2021-codi</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Anaphora Resolution in Dialogue: Description of the <fixed-case>DFKI</fixed-case>-<fixed-case>T</fixed-case>alking<fixed-case>R</fixed-case>obots System for the <fixed-case>CODI</fixed-case>-<fixed-case>CRAC</fixed-case> 2021 Shared-Task</title>
      <author><first>Tatiana</first><last>Anikina</last></author>
      <author><first>Cennet</first><last>Oguz</last></author>
      <author><first>Natalia</first><last>Skachkova</last></author>
      <author><first>Siyu</first><last>Tao</last></author>
      <author><first>Sharmila</first><last>Upadhyaya</last></author>
      <author><first>Ivana</first><last>Kruijff-Korbayova</last></author>
      <pages>32&#8211;42</pages>
      <abstract>We describe the system developed by the DFKI-TalkingRobots Team for the CODI-CRAC 2021 Shared-Task on anaphora resolution in dialogue. Our system consists of three subsystems: (1) the Workspace Coreference System (WCS) incrementally clusters mentions using semantic similarity based on embeddings combined with lexical feature heuristics; (2) the Mention-to-Mention (M2M) coreference resolution system pairs same entity mentions; (3) the Discourse Deixis Resolution (DDR) system employs a Siamese Network to detect discourse anaphor-antecedent pairs. WCS achieved F1-score of 55.6% averaged across the evaluation test sets, M2M achieved 57.2% and DDR achieved 21.5%.</abstract>
      <url hash="5a6d3cc9">2021.codi-sharedtask.3</url>
      <bibkey>anikina-etal-2021-anaphora</bibkey>
      <doi>10.18653/v1/2021.codi-sharedtask.3</doi>
    </paper>
    <paper id="8">
      <title>The <fixed-case>CODI</fixed-case>-<fixed-case>CRAC</fixed-case> 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis Resolution in Dialogue: A Cross-Team Analysis</title>
      <author><first>Shengjie</first><last>Li</last></author>
      <author><first>Hideo</first><last>Kobayashi</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>71&#8211;95</pages>
      <abstract>The CODI-CRAC 2021 shared task is the first shared task that focuses exclusively on anaphora resolution in dialogue and provides three tracks, namely entity coreference resolution, bridging resolution, and discourse deixis resolution. We perform a cross-task analysis of the systems that participated in the shared task in each of these tracks.</abstract>
      <url hash="e549ffdc">2021.codi-sharedtask.8</url>
      <bibkey>li-etal-2021-codi</bibkey>
      <doi>10.18653/v1/2021.codi-sharedtask.8</doi>
    </paper>
  </volume>
</collection>