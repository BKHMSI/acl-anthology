<collection id="2020.nuse">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events</booktitle>
      <editor><first>Claire</first><last>Bonial</last></editor>
      <editor><first>Tommaso</first><last>Caselli</last></editor>
      <editor><first>Snigdha</first><last>Chaturvedi</last></editor>
      <editor><first>Elizabeth</first><last>Clark</last></editor>
      <editor><first>Ruihong</first><last>Huang</last></editor>
      <editor><first>Mohit</first><last>Iyyer</last></editor>
      <editor><first>Alejandro</first><last>Jaimes</last></editor>
      <editor><first>Heng</first><last>Ji</last></editor>
      <editor><first>Lara J.</first><last>Martin</last></editor>
      <editor><first>Ben</first><last>Miller</last></editor>
      <editor><first>Teruko</first><last>Mitamura</last></editor>
      <editor><first>Nanyun</first><last>Peng</last></editor>
      <editor><first>Joel</first><last>Tetreault</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="9075bb72">2020.nuse-1</url>
    </meta>
    <frontmatter>
      <url hash="16e67b2a">2020.nuse-1.0</url>
      <bibkey>nuse-2020-joint</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Improving the Identification of the Discourse Function of News Article Paragraphs</title>
      <author><first>Deya</first><last>Banisakher</last></author>
      <author><first>W. Victor</first><last>Yarlott</last></author>
      <author><first>Mohammed</first><last>Aldawsari</last></author>
      <author><first>Naphtali</first><last>Rishe</last></author>
      <author><first>Mark</first><last>Finlayson</last></author>
      <pages>17&#8211;25</pages>
      <abstract>Identifying the discourse structure of documents is an important task in understanding written text. Building on prior work, we demonstrate an improved approach to automatically identifying the discourse function of paragraphs in news articles. We start with the hierarchical theory of news discourse developed by van Dijk (1988) which proposes how paragraphs function within news articles. This discourse information is a level intermediate between phrase- or sentence-sized discourse segments and document genre, characterizing how individual paragraphs convey information about the events in the storyline of the article. Specifically, the theory categorizes the relationships between narrated events and (1) the overall storyline (such as Main Events, Background, or Consequences) as well as (2) commentary (such as Verbal Reactions and Evaluations). We trained and tested a linear chain conditional random field (CRF) with new features to model van Dijk&#8217;s labels and compared it against several machine learning models presented in previous work. Our model significantly outperformed all baselines and prior approaches, achieving an average of 0.71 F1 score which represents a 31.5% improvement over the previously best-performing support vector machine model.</abstract>
      <url hash="f4f0aef0">2020.nuse-1.3</url>
      <doi>10.18653/v1/2020.nuse-1.3</doi>
      <video href="http://slideslive.com/38929742" />
      <bibkey>banisakher-etal-2020-improving</bibkey>
    </paper>
    <paper id="5">
      <title>Extensively Matching for Few-shot Learning Event Detection</title>
      <author><first>Viet Dac</first><last>Lai</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <pages>38&#8211;45</pages>
      <abstract>Current event detection models under supervised learning settings fail to transfer to new event types. Few-shot learning has not been explored in event detection even though it allows a model to perform well with high generalization on new event types. In this work, we formulate event detection as a few-shot learning problem to enable to extend event detection to new event types. We propose two novel loss factors that matching examples in the support set to provide more training signals to the model. Moreover, these training signals can be applied in many metric-based few-shot learning models. Our extensive experiments on the ACE-2005 dataset (under a few-shot learning setting) show that the proposed method can improve the performance of few-shot learning.</abstract>
      <url hash="adf9c622">2020.nuse-1.5</url>
      <doi>10.18653/v1/2020.nuse-1.5</doi>
      <video href="http://slideslive.com/38929744" />
      <bibkey>lai-etal-2020-extensively</bibkey>
    </paper>
    <paper id="9">
      <title>Annotating and quantifying narrative time disruptions in modernist and hypertext fiction</title>
      <author><first>Edward</first><last>Kearns</last></author>
      <pages>72&#8211;77</pages>
      <abstract>This paper outlines work in progress on a new method of annotating and quantitatively discussing narrative techniques related to time in fiction. Specifically those techniques are analepsis, prolepsis, narrative level changes, and stream-of-consciousness and free-indirect-discourse narration. By counting the frequency and extent of the usage of these techniques, the narrative characteristics of different works from different time periods and genres can be compared. This project uses modernist fiction and hypertext fiction as its case studies.</abstract>
      <url hash="54dba309">2020.nuse-1.9</url>
      <doi>10.18653/v1/2020.nuse-1.9</doi>
      <video href="http://slideslive.com/38929748" />
      <bibkey>kearns-2020-annotating</bibkey>
    </paper>
    <paper id="10">
      <title>Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types</title>
      <author><first>Belen</first><last>Saldias</last></author>
      <author><first>Deb</first><last>Roy</last></author>
      <pages>78&#8211;86</pages>
      <abstract>Sharing personal narratives is a fundamental aspect of human social behavior as it helps share our life experiences. We can tell stories and rely on our background to understand their context, similarities, and differences. A substantial effort has been made towards developing storytelling machines or inferring characters&#8217; features. However, we don&#8217;t usually find models that compare narratives. This task is remarkably challenging for machines since they, as sometimes we do, lack an understanding of what similarity means. To address this challenge, we first introduce a corpus of real-world spoken personal narratives comprising 10,296 narrative clauses from 594 video transcripts. Second, we ask non-narrative experts to annotate those clauses under Labov&#8217;s sociolinguistic model of personal narratives (i.e., action, orientation, and evaluation clause types) and train a classifier that reaches 84.7% F-score for the highest-agreed clauses. Finally, we match stories and explore whether people implicitly rely on Labov&#8217;s framework to compare narratives. We show that actions followed by the narrator&#8217;s evaluation of these are the aspects non-experts consider the most. Our approach is intended to help inform machine learning methods aimed at studying or representing personal narratives.</abstract>
      <url hash="64dc1ef4">2020.nuse-1.10</url>
      <doi>10.18653/v1/2020.nuse-1.10</doi>
      <video href="https://slideslive.com/38939705" />
      <bibkey>saldias-roy-2020-exploring</bibkey>
      <pwccode url="https://github.com/social-machines/acl-nuse-personal-narratives" additional="false">social-machines/acl-nuse-personal-narratives</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rtn">RTN</pwcdataset>
    </paper>
    <paper id="14">
      <title>On-The-Fly Information Retrieval Augmentation for Language Models</title>
      <author><first>Hai</first><last>Wang</last></author>
      <author><first>David</first><last>McAllester</last></author>
      <pages>114&#8211;119</pages>
      <abstract>Here we experiment with the use of information retrieval as an augmentation for pre-trained language models. The text corpus used in information retrieval can be viewed as form of episodic memory which grows over time. By augmenting GPT 2.0 with information retrieval we achieve a zero shot 15% relative reduction in perplexity on Gigaword corpus without any re-training. We also validate our IR augmentation on an event co-reference task.</abstract>
      <url hash="fd6b3f36">2020.nuse-1.14</url>
      <doi>10.18653/v1/2020.nuse-1.14</doi>
      <video href="http://slideslive.com/38929754" />
      <bibkey>wang-mcallester-2020-fly</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    </volume>
</collection>