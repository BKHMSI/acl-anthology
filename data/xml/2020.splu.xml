<collection id="2020.splu">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the Third International Workshop on Spatial Language Understanding</booktitle>
      <editor><first>Parisa</first><last>Kordjamshidi</last></editor>
      <editor><first>Archna</first><last>Bhatia</last></editor>
      <editor><first>Malihe</first><last>Alikhani</last></editor>
      <editor><first>Jason</first><last>Baldridge</last></editor>
      <editor><first>Mohit</first><last>Bansal</last></editor>
      <editor><first>Marie-Francine</first><last>Moens</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="a4356e56">2020.splu-1.0</url>
      <bibkey>splu-2020-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>An Element-wise Visual-enhanced <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case>-<fixed-case>CRF</fixed-case> Model for Location Name Recognition</title>
      <author><first>Takuya</first><last>Komada</last></author>
      <author><first>Takashi</first><last>Inui</last></author>
      <pages>1&#8211;9</pages>
      <abstract>In recent years, previous studies have used visual information in named entity recognition (NER) for social media posts with attached images. However, these methods can only be applied to documents with attached images. In this paper, we propose a NER method that can use element-wise visual information for any documents by using image data corresponding to each word in the document. The proposed method obtains element-wise image data using an image retrieval engine, to be used as extra features in the neural NER model. Experimental results on the standard Japanese NER dataset show that the proposed method achieves a higher F1 value (89.67%) than a baseline method, demonstrating the effectiveness of using element-wise visual information.</abstract>
      <url hash="b09ef8ce">2020.splu-1.1</url>
      <doi>10.18653/v1/2020.splu-1.1</doi>
      <video href="https://slideslive.com/38940081" />
      <bibkey>komada-inui-2020-element</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>BERT</fixed-case>-based Spatial Information Extraction</title>
      <author><first>Hyeong Jin</first><last>Shin</last></author>
      <author><first>Jeong Yeon</first><last>Park</last></author>
      <author><first>Dae Bum</first><last>Yuk</last></author>
      <author><first>Jae Sung</first><last>Lee</last></author>
      <pages>10&#8211;17</pages>
      <abstract>Spatial information extraction is essential to understand geographical information in text. This task is largely divided to two subtasks: spatial element extraction and spatial relation extraction. In this paper, we utilize BERT (Devlin et al., 2018), which is very effective for many natural language processing applications. We propose a BERT-based spatial information extraction model, which uses BERT for spatial element extraction and R-BERT (Wu and He, 2019) for spatial relation extraction. The model was evaluated with the SemEval 2015 dataset. The result showed a 15.4% point increase in spatial element extraction and an 8.2% point increase in spatial relation extraction in comparison to the baseline model (Nichols and Botros, 2015).</abstract>
      <url hash="dec1234e">2020.splu-1.2</url>
      <doi>10.18653/v1/2020.splu-1.2</doi>
      <video href="https://slideslive.com/38940078" />
      <bibkey>shin-etal-2020-bert</bibkey>
    </paper>
    <paper id="4">
      <title>They Are Not All Alike: Answering Different Spatial Questions Requires Different Grounding Strategies</title>
      <author><first>Alberto</first><last>Testoni</last></author>
      <author><first>Claudio</first><last>Greco</last></author>
      <author><first>Tobias</first><last>Bianchi</last></author>
      <author><first>Mauricio</first><last>Mazuecos</last></author>
      <author><first>Agata</first><last>Marcante</last></author>
      <author><first>Luciana</first><last>Benotti</last></author>
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <pages>29&#8211;38</pages>
      <abstract>In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed model outperforms the baseline by a large extent (9.70% on spatial questions and 6.27% overall). By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions.</abstract>
      <url hash="7075bf3b">2020.splu-1.4</url>
      <doi>10.18653/v1/2020.splu-1.4</doi>
      <video href="https://slideslive.com/38940076" />
      <bibkey>testoni-etal-2020-alike</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/guesswhat">GuessWhat?!</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visdial">VisDial</pwcdataset>
    </paper>
    </volume>
</collection>