<collection id="2021.crac">
  <volume id="1" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference</booktitle>
      <editor><first>Maciej</first><last>Ogrodniczuk</last></editor>
      <editor><first>Sameer</first><last>Pradhan</last></editor>
      <editor><first>Massimo</first><last>Poesio</last></editor>
      <editor><first>Yulia</first><last>Grishina</last></editor>
      <editor><first>Vincent</first><last>Ng</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="b46b58c7">2021.crac-1.0</url>
      <bibkey>crac-2021-models</bibkey>
    </frontmatter>
    <paper id="4">
      <title><fixed-case>D</fixed-case>rama<fixed-case>C</fixed-case>oref: A Hybrid Coreference Resolution System for <fixed-case>G</fixed-case>erman Theater Plays</title>
      <author><first>Janis</first><last>Pagel</last></author>
      <author><first>Nils</first><last>Reiter</last></author>
      <pages>36&#8211;46</pages>
      <abstract>We present a system for resolving coreference on theater plays, DramaCoref. The system uses neural network techniques to provide a list of potential mentions. These mentions are assigned to common entities using generic and domain-specific rules. We find that DramaCoref works well on the theater plays when compared to corpora from other domains and profits from the inclusion of information specific to theater plays. On the best-performing setup, it achieves a CoNLL score of 32% when using automatically detected mentions and 55% when using gold mentions. Single rules achieve high precision scores; however, rules designed on other domains are often not applicable or yield unsatisfactory results. Error analysis shows that the mention detection is the main weakness of the system, providing directions for future improvements.</abstract>
      <url hash="96414b24">2021.crac-1.4</url>
      <bibkey>pagel-reiter-2021-dramacoref</bibkey>
      <doi>10.18653/v1/2021.crac-1.4</doi>
    </paper>
    <paper id="6">
      <title>Lazy Low-Resource Coreference Resolution: a Study on Leveraging Black-Box Translation Tools</title>
      <author><first>Semere Kiros</first><last>Bitew</last></author>
      <author><first>Johannes</first><last>Deleu</last></author>
      <author><first>Chris</first><last>Develder</last></author>
      <author><first>Thomas</first><last>Demeester</last></author>
      <pages>57&#8211;62</pages>
      <abstract>Large annotated corpora for coreference resolution are available for few languages. For machine translation, however, strong black-box systems exist for many languages. We empirically explore the appealing idea of leveraging such translation tools for bootstrapping coreference resolution in languages with limited resources. Two scenarios are analyzed, in which a large coreference corpus in a high-resource language is used for coreference predictions in a smaller language, i.e., by machine translating either the training corpus or the test data. In our empirical evaluation of coreference resolution using the two scenarios on several medium-resource languages, we find no improvement over monolingual baseline models. Our analysis of the various sources of error inherent to the studied scenarios, reveals that in fact the quality of contemporary machine translation tools is the main limiting factor.</abstract>
      <url hash="0339614b">2021.crac-1.6</url>
      <bibkey>bitew-etal-2021-lazy</bibkey>
      <doi>10.18653/v1/2021.crac-1.6</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>C</fixed-case>ore<fixed-case>LM</fixed-case>: Coreference-aware Language Model Fine-Tuning</title>
      <author><first>Nikolaos</first><last>Stylianou</last></author>
      <author><first>Ioannis</first><last>Vlahavas</last></author>
      <pages>70&#8211;81</pages>
      <abstract>Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the fine-tuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models&#8217; performance in terms of Accuracy in LAMBADA and Children&#8217;s Book Test, with and without the use of model-created coreference annotations.</abstract>
      <url hash="ba1158de">2021.crac-1.8</url>
      <attachment type="Software" hash="ce213f27">2021.crac-1.8.Software.zip</attachment>
      <bibkey>stylianou-vlahavas-2021-corelm</bibkey>
      <doi>10.18653/v1/2021.crac-1.8</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cbt">CBT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lambada">LAMBADA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="12">
      <title>On Generalization in Coreference Resolution</title>
      <author><first>Shubham</first><last>Toshniwal</last></author>
      <author><first>Patrick</first><last>Xia</last></author>
      <author><first>Sam</first><last>Wiseman</last></author>
      <author><first>Karen</first><last>Livescu</last></author>
      <author><first>Kevin</first><last>Gimpel</last></author>
      <pages>111&#8211;120</pages>
      <abstract>While coreference resolution is defined independently of dataset domain, most models for performing coreference resolution do not transfer well to unseen domains. We consolidate a set of 8 coreference resolution datasets targeting different domains to evaluate the off-the-shelf performance of models. We then mix three datasets for training; even though their domain, annotation guidelines, and metadata differ, we propose a method for jointly training a single model on this heterogeneous data mixture by using data augmentation to account for annotation differences and sampling to balance the data quantities. We find that in a zero-shot setting, models trained on a single dataset transfer poorly while joint training yields improved overall performance, leading to better generalization in coreference resolution models. This work contributes a new benchmark for robust coreference resolution and multiple new state-of-the-art results.</abstract>
      <url hash="9cea37bc">2021.crac-1.12</url>
      <bibkey>toshniwal-etal-2021-generalization</bibkey>
      <doi>10.18653/v1/2021.crac-1.12</doi>
      <pwccode url="https://github.com/shtoshni92/fast-coref" additional="false">shtoshni92/fast-coref</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontogum">OntoGUM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/preco">PreCo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikicoref">WikiCoref</pwcdataset>
    </paper>
    </volume>
</collection>