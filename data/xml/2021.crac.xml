<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.crac">
  <volume id="1" ingest-date="2021-10-28">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference</booktitle>
      <editor><first>Maciej</first><last>Ogrodniczuk</last></editor>
      <editor><first>Sameer</first><last>Pradhan</last></editor>
      <editor><first>Massimo</first><last>Poesio</last></editor>
      <editor><first>Yulia</first><last>Grishina</last></editor>
      <editor><first>Vincent</first><last>Ng</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="b46b58c7">2021.crac-1.0</url>
      <bibkey>crac-2021-models</bibkey>
    </frontmatter>
    <paper id="4">
      <title>DramaCoref : A Hybrid Coreference Resolution System for German Theater Plays<fixed-case>D</fixed-case>rama<fixed-case>C</fixed-case>oref: A Hybrid Coreference Resolution System for <fixed-case>G</fixed-case>erman Theater Plays</title>
      <author><first>Janis</first><last>Pagel</last></author>
      <author><first>Nils</first><last>Reiter</last></author>
      <pages>36–46</pages>
      <abstract>We present a <a href="https://en.wikipedia.org/wiki/System">system</a> for resolving <a href="https://en.wikipedia.org/wiki/Coreference">coreference</a> on <a href="https://en.wikipedia.org/wiki/Play_(theatre)">theater plays</a>, DramaCoref. The <a href="https://en.wikipedia.org/wiki/System">system</a> uses <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural network techniques</a> to provide a list of potential mentions. These mentions are assigned to common entities using generic and domain-specific rules. We find that DramaCoref works well on the <a href="https://en.wikipedia.org/wiki/Play_(theatre)">theater plays</a> when compared to corpora from other domains and profits from the inclusion of information specific to theater plays. On the best-performing setup, it achieves a CoNLL score of 32 % when using automatically detected mentions and 55 % when using gold mentions. Single rules achieve high precision scores ; however, <a href="https://en.wikipedia.org/wiki/Game_theory">rules</a> designed on other domains are often not applicable or yield unsatisfactory results. Error analysis shows that the mention detection is the main weakness of the <a href="https://en.wikipedia.org/wiki/System">system</a>, providing directions for future improvements.</abstract>
      <url hash="96414b24">2021.crac-1.4</url>
      <bibkey>pagel-reiter-2021-dramacoref</bibkey>
      <doi>10.18653/v1/2021.crac-1.4</doi>
    </paper>
    <paper id="6">
      <title>Lazy Low-Resource Coreference Resolution : a Study on Leveraging Black-Box Translation Tools</title>
      <author><first>Semere Kiros</first><last>Bitew</last></author>
      <author><first>Johannes</first><last>Deleu</last></author>
      <author><first>Chris</first><last>Develder</last></author>
      <author><first>Thomas</first><last>Demeester</last></author>
      <pages>57–62</pages>
      <abstract>Large annotated corpora for <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> are available for few languages. For <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>, however, strong black-box systems exist for many languages. We empirically explore the appealing idea of leveraging such translation tools for bootstrapping <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> in languages with limited resources. Two scenarios are analyzed, in which a large coreference corpus in a high-resource language is used for coreference predictions in a smaller language, i.e., by <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translating</a> either the training corpus or the test data. In our empirical evaluation of <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> using the two scenarios on several medium-resource languages, we find no improvement over monolingual baseline models. Our analysis of the various sources of error inherent to the studied scenarios, reveals that in fact the quality of contemporary machine translation tools is the main limiting factor.</abstract>
      <url hash="0339614b">2021.crac-1.6</url>
      <bibkey>bitew-etal-2021-lazy</bibkey>
      <doi>10.18653/v1/2021.crac-1.6</doi>
    </paper>
    <paper id="8">
      <title>CoreLM : Coreference-aware Language Model Fine-Tuning<fixed-case>C</fixed-case>ore<fixed-case>LM</fixed-case>: Coreference-aware Language Model Fine-Tuning</title>
      <author><first>Nikolaos</first><last>Stylianou</last></author>
      <author><first>Ioannis</first><last>Vlahavas</last></author>
      <pages>70–81</pages>
      <abstract>Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making <a href="https://en.wikipedia.org/wiki/Language_model">Language Modeling</a> very effective across many NLP task, leading to significant advancements in the field. However, <a href="https://en.wikipedia.org/wiki/Transformers_(toy_line)">Transformers</a> come with a big <a href="https://en.wikipedia.org/wiki/Computational_cost">computational cost</a>, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a>, which results in a better <a href="https://en.wikipedia.org/wiki/Language_model">Language Model</a> for a fraction of the <a href="https://en.wikipedia.org/wiki/Computational_cost">computational cost</a>. We implement our approach using GPT2 and compare the fine-tuned model to the original. Our proposed model achieves a lower <a href="https://en.wikipedia.org/wiki/Perplexity">Perplexity</a> in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models’ performance in terms of Accuracy in LAMBADA and Children’s Book Test, with and without the use of model-created coreference annotations.</abstract>
      <url hash="ba1158de">2021.crac-1.8</url>
      <attachment type="Software" hash="ce213f27">2021.crac-1.8.Software.zip</attachment>
      <bibkey>stylianou-vlahavas-2021-corelm</bibkey>
      <doi>10.18653/v1/2021.crac-1.8</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cbt">CBT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lambada">LAMBADA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="12">
      <title>On Generalization in <a href="https://en.wikipedia.org/wiki/Coreference_resolution">Coreference Resolution</a></title>
      <author><first>Shubham</first><last>Toshniwal</last></author>
      <author><first>Patrick</first><last>Xia</last></author>
      <author><first>Sam</first><last>Wiseman</last></author>
      <author><first>Karen</first><last>Livescu</last></author>
      <author><first>Kevin</first><last>Gimpel</last></author>
      <pages>111–120</pages>
      <abstract>While <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> is defined independently of dataset domain, most <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for performing <a href="https://en.wikipedia.org/wiki/Coreference_resolution">coreference resolution</a> do not transfer well to unseen domains. We consolidate a set of 8 coreference resolution datasets targeting different domains to evaluate the off-the-shelf performance of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. We then mix three datasets for training ; even though their domain, annotation guidelines, and <a href="https://en.wikipedia.org/wiki/Metadata">metadata</a> differ, we propose a method for jointly training a single model on this heterogeneous data mixture by using <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> to account for annotation differences and <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)">sampling</a> to balance the data quantities. We find that in a zero-shot setting, <a href="https://en.wikipedia.org/wiki/Statistical_model">models</a> trained on a single dataset transfer poorly while joint training yields improved overall performance, leading to better generalization in coreference resolution models. This work contributes a new <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">benchmark</a> for robust coreference resolution and multiple new state-of-the-art results.</abstract>
      <url hash="9cea37bc">2021.crac-1.12</url>
      <bibkey>toshniwal-etal-2021-generalization</bibkey>
      <doi>10.18653/v1/2021.crac-1.12</doi>
      <pwccode url="https://github.com/shtoshni92/fast-coref" additional="false">shtoshni92/fast-coref</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontogum">OntoGUM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/preco">PreCo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikicoref">WikiCoref</pwcdataset>
    <title_ar>في التعميم في Coreference Resolution</title_ar>
      <title_es>Sobre la generalización en la resolución de correferencias</title_es>
      <title_fr>Sur la généralisation dans la résolution de coréférence</title_fr>
      <title_pt>Sobre Generalização na Resolução de Correferência</title_pt>
      <title_zh>其共推理解析中泛化</title_zh>
      <title_ja>コアリファレンス分解能の一般化について</title_ja>
      <title_hi>Coreference संकल्प में सामान्यीकरण पर</title_hi>
      <title_ru>Об обобщении в ключевом разрешении</title_ru>
      <title_ga>Ar Ghinearálú i Rún Croíthagartha</title_ga>
      <title_ka>გენერალიზაციის რეზიოციაში</title_ka>
      <title_el>Για τη γενίκευση στο ψήφισμα της συναδέλφου</title_el>
      <title_hu>Általánosítás a Coreferencia-állásfoglalásban</title_hu>
      <title_it>Sulla generalizzazione nella risoluzione di Coreferenza</title_it>
      <title_kk>Жалпы қасиеттердің Айырымдылығында</title_kk>
      <title_ms>Pada Jeneralisasi dalam Resolusi Coreference</title_ms>
      <title_ml>കോര്‍ഫെന്‍സ് റിപ്പോര്‍ഷനില്‍ ജനറലേഷന്‍ ചെയ്യുമ്പോള്‍</title_ml>
      <title_lt>Dėl bendrosios konferencijos rezoliucijos</title_lt>
      <title_mk>За генерализација во резолуцијата на кореференцијата</title_mk>
      <title_mn>Хэрэглэгчдийн шийдвэрлэлийн ерөнхийлөгч</title_mn>
      <title_ro>Cu privire la generalizarea în rezoluția Coreferenței</title_ro>
      <title_pl>W sprawie uogólnienia w rezolucji współpracy</title_pl>
      <title_mt>On Generalization in Coreference Resolution</title_mt>
      <title_sr>Na generalizaciji u rezoluciji korisnosti</title_sr>
      <title_so>Waxyaabaha guud ee koreference Resolution</title_so>
      <title_no>På Generalisering i oppløysing av koreferansen</title_no>
      <title_ta>திரைத்திறனில் பொதுவாக்குதலில்</title_ta>
      <title_si>ප්‍රමාණ විශේෂණයේ සාමාන්‍ය විශේෂණය සඳහා</title_si>
      <title_sv>Generalisering i Coreference-resolutionen</title_sv>
      <title_ur>قابل رخصت رخصت میں جرائنالیزی پر</title_ur>
      <title_uz>Name</title_uz>
      <title_vi>Về việc mời gọi giải quyết</title_vi>
      <title_bg>Относно обобщението в резолюцията на кореференцията</title_bg>
      <title_hr>O generalizaciji u rezoluciji korisnosti</title_hr>
      <title_nl>Over generalisering in de resolutie van de Coreferentie</title_nl>
      <title_da>Generalisering i Coreference-resolutionen</title_da>
      <title_de>Zur Generalisierung in der Coreferenz Resolution</title_de>
      <title_id>Pada Generalisasi dalam Resolusi Koreference</title_id>
      <title_ko>논공지 소해 중의 범화</title_ko>
      <title_fa>ШҜШұ ШӘЩҲЩ„ЫҢШҜ ЪҳЩҶШұШ§Щ„ ШҜШұ ШӯЩ„вҖҢШіШ§ШІЫҢ ЩҫЫҢШҙЩҶЩҮШ§ШҜ</title_fa>
      <title_sw>Katika Ujumla wa Mazingiro</title_sw>
      <title_af>Op Generalisasie in Hoofheidsoplossing</title_af>
      <title_tr>Däşer Seçenekleri Çaşyrmakda</title_tr>
      <title_sq>Për gjeneralizimin në rezolutën e Koreferencës</title_sq>
      <title_am>ምርጫዎች</title_am>
      <title_az>캻fad톛 칂칬z칲n칲rl칲y칲nd톛 Generalizasyonda</title_az>
      <title_bn>সাধারণ সংস্থা</title_bn>
      <title_cs>O obecnění v usnesení společné reference</title_cs>
      <title_bs>O generalizaciji u rezoluciji korisnosti</title_bs>
      <title_et>Korralduse resolutsiooni üldistamine</title_et>
      <title_fi>Coreference-päätöslauselman yleistämisestä</title_fi>
      <title_hy>Կորեֆերանսի լուծումների ընդհանուր հաստատության մասին</title_hy>
      <title_ca>En la Generalització en la Resolució de Coreferència</title_ca>
      <title_jv>iku</title_jv>
      <title_sk>O splošnosti v resoluciji Coreference</title_sk>
      <title_ha>@ action</title_ha>
      <title_he>על הגנרליזציה בפתרון הקבוצה</title_he>
      <title_bo>མཐའ་དབྱིབས་གདམ་ཁ་ཚོགས་ནང་དུ་སྤྱིར་བཏང་བ</title_bo>
      <abstract_fr>Alors que la résolution de coréférence est définie indépendamment du domaine de l'ensemble de données, la plupart des modèles permettant d'effectuer une résolution de coréférence ne sont pas correctement transférés vers des domaines invisibles Nous consolidons un ensemble de 8 ensembles de données de résolution de coréférence ciblant différents domaines afin d'évaluer les performances standard des modèles. Nous mélangeons ensuite trois ensembles de données pour la formation ; même si leur domaine, leurs directives d'annotation et leurs métadonnées diffèrent, nous proposons une méthode pour entraîner conjointement un seul modèle sur ce mélange de données hétérogènes en utilisant l'augmentation des données pour tenir compte des différences d'annotation et l'échantillonnage pour équilibrer les quantités de données. Nous constatons que dans un environnement de tir zéro, les modèles entraînés sur un seul ensemble de données transfèrent mal tandis que l'entraînement conjoint améliore les performances globales, ce qui conduit à une meilleure généralisation dans les modèles de résolution de coréférence. Ce travail constitue une nouvelle référence en matière de résolution de coréférence robuste et de multiples nouveaux résultats de pointe.</abstract_fr>
      <abstract_es>Si bien la resolución de correferencia se define independientemente del dominio del conjunto de datos, la mayoría de los modelos para realizar la resolución de correferencias no se transfieren bien a dominios no visibles. Consolidamos un conjunto de 8 conjuntos de datos de resolución de correferencia dirigidos a diferentes dominios para evaluar el rendimiento estándar de los modelos. Luego, mezclamos tres conjuntos de datos para la capacitación; aunque su dominio, las pautas de anotación y los metadatos difieren, proponemos un método para entrenar conjuntamente un solo modelo en esta mezcla de datos heterogénea mediante el uso del aumento de datos para tener en cuenta las diferencias de anotación y el muestreo para equilibrar las cantidades de datos. Encontramos que en un entorno de tiro cero, los modelos entrenados en un solo conjunto de datos transfieren de manera deficiente, mientras que el entrenamiento conjunto produce un mejor rendimiento general, lo que lleva a una mejor generalización en los modelos de resolución de correferencia. Este trabajo aporta un nuevo punto de referencia para una resolución de correferencia sólida y múltiples resultados nuevos de última generación.</abstract_es>
      <abstract_pt>Embora a resolução de correferência seja definida independentemente do domínio do conjunto de dados, a maioria dos modelos para realizar a resolução de correferência não se transfere bem para domínios não vistos. Consolidamos um conjunto de 8 conjuntos de dados de resolução de correferência direcionados a diferentes domínios para avaliar o desempenho de modelos prontos para uso. Em seguida, misturamos três conjuntos de dados para treinamento; mesmo que seu domínio, diretrizes de anotação e metadados sejam diferentes, propomos um método para treinar em conjunto um único modelo nessa mistura de dados heterogênea usando aumento de dados para levar em conta as diferenças de anotação e amostragem para equilibrar as quantidades de dados. Descobrimos que, em uma configuração de tiro zero, os modelos treinados em um único conjunto de dados transferem mal, enquanto o treinamento conjunto produz um desempenho geral aprimorado, levando a uma melhor generalização em modelos de resolução de correferência. Este trabalho contribui com um novo benchmark para resolução de correferência robusta e vários novos resultados de última geração.</abstract_pt>
      <abstract_ar>بينما يتم تحديد دقة المرجع بشكل مستقل عن مجال مجموعة البيانات ، فإن معظم النماذج الخاصة بأداء دقة المرجع لا تنتقل بشكل جيد إلى المجالات غير المرئية. نقوم بدمج مجموعة من 8 مجموعات بيانات ذات دقة مرجعية تستهدف مجالات مختلفة لتقييم الأداء الجاهز للنماذج. ثم نمزج ثلاث مجموعات بيانات للتدريب ؛ على الرغم من اختلاف المجال وإرشادات التعليقات التوضيحية والبيانات الوصفية ، فإننا نقترح طريقة للتدريب المشترك لنموذج واحد على مزيج البيانات غير المتجانسة هذا باستخدام زيادة البيانات لحساب اختلافات التعليقات التوضيحية وأخذ العينات لموازنة كميات البيانات. نجد أنه في إعداد اللقطة الصفرية ، فإن النماذج المدربة على نقل مجموعة بيانات واحدة بشكل سيئ بينما يؤدي التدريب المشترك إلى تحسين الأداء العام ، مما يؤدي إلى تعميم أفضل في نماذج دقة المرجع. يساهم هذا العمل في معيار جديد للقرار المرجعي القوي ونتائج متعددة جديدة على أحدث طراز.</abstract_ar>
      <abstract_ja>コアレファレンス分解能は、データセットドメインとは独立して定義されていますが、コアレファレンス分解能を実行するためのほとんどのモデルは、見えないドメインにうまく転送されません。異なるドメインを対象とした8つのコアレファレンス分解能データセットのセットを統合して、モデルの既製のパフォーマンスを評価します。次に、トレーニングのために3つのデータセットを混合します。ドメイン、注釈ガイドライン、およびメタデータが異なるにもかかわらず、注釈の違いを説明するためにデータ拡張を使用し、データ量のバランスを取るためにサンプリングすることによって、この異種データ混合物に関する単一のモデルを共同でトレーニングする方法を提案します。ゼロショット設定では、単一のデータセット転送でトレーニングされたモデルが不十分である一方、共同トレーニングでは全体的なパフォーマンスが向上し、コアレファレンス解像度モデルの一般化が向上することがわかります。この作業は、堅牢なコアリファレンス解決と複数の最新の結果のための新しいベンチマークに貢献します。</abstract_ja>
      <abstract_ru>Хотя разрешение керна определяется независимо от домена набора данных, большинство моделей для выполнения разрешения керна плохо переносятся в невидимые домены. Мы консолидируем набор из 8 наборов данных с разрешением керна, ориентированных на различные области, чтобы оценить стандартную производительность моделей. Затем мы смешиваем три набора данных для обучения; даже несмотря на то, что их область, руководящие принципы аннотирования и метаданные различаются, мы предлагаем метод совместного обучения одной модели на этой неоднородной смеси данных с использованием дополнения данных для учета различий аннотаций и выборки для сбалансирования объемов данных. Мы обнаружили, что в условиях нулевого выстрела модели, обученные передаче одного набора данных, плохо переносятся, в то время как совместное обучение дает улучшенную общую производительность, что приводит к лучшему обобщению в моделях разрешения ядра. Эта работа вносит вклад в новый эталон для надежного разрешения ядра и множественных новых современных результатов.</abstract_ru>
      <abstract_zh>虽共推理解析独立集域义,然大抵用于行共推理解析之法,不能善传输于未见之域。 整合其异域者 8 共推理解析数集,以质其能。 然后混合三数集。 虽领域、注南与元数不同,然吾建一法,因用数以虑异采样以平数据量,而合练于异构混合物。 吾见零次之设也,单集上之传输不善,而合之以善,因而成之于共分辨率之泛化。 其事强大者共推理分辨率与数新之最先进者给之。</abstract_zh>
      <abstract_hi>जबकि coreference रिज़ॉल्यूशन डेटासेट डोमेन से स्वतंत्र रूप से परिभाषित किया गया है, coreference resolution करने के लिए अधिकांश मॉडल अनदेखी डोमेन में अच्छी तरह से स्थानांतरित नहीं होते हैं। हम मॉडल के ऑफ-द-शेल्फ प्रदर्शन का मूल्यांकन करने के लिए विभिन्न डोमेन को लक्षित करने वाले 8 कोरफेरेंस रिज़ॉल्यूशन डेटासेट के एक सेट को समेकित करते हैं। फिर हम प्रशिक्षण के लिए तीन डेटासेट मिश्रण करते हैं; भले ही उनके डोमेन, एनोटेशन दिशानिर्देश, और मेटाडेटा अलग-अलग होते हैं, हम डेटा वृद्धि का उपयोग करके इस विषम डेटा मिश्रण पर एक एकल मॉडल को संयुक्त रूप से प्रशिक्षित करने के लिए एक विधि का प्रस्ताव करते हैं एनोटेशन मतभेदों के लिए खाते में और डेटा मात्रा को संतुलित करने के लिए नमूनाकरण। हम पाते हैं कि एक शून्य-शॉट सेटिंग में, एकल डेटासेट हस्तांतरण पर प्रशिक्षित मॉडल खराब रूप से स्थानांतरित होते हैं जबकि संयुक्त प्रशिक्षण पैदावार ने समग्र प्रदर्शन में सुधार किया, जिससे कोरेफेरेंस रिज़ॉल्यूशन मॉडल में बेहतर सामान्यीकरण होता है। यह काम मजबूत coreference संकल्प और कई नए अत्याधुनिक परिणामों के लिए एक नया बेंचमार्क योगदान देता है।</abstract_hi>
      <abstract_ga>Cé go sainmhínítear taifeach croí-chomhdhála go neamhspleách ar fhearann na dtacar sonraí, ní aistrítear go maith go dtí fearainn nach bhfeictear an chuid is mó de na samhlacha chun réiteach croí-chomhdhála a dhéanamh. Comhdhlúthaímid sraith de 8 tacar sonraí réitigh croíchomhdhála a dhíríonn ar fhearainn éagsúla chun feidhmíocht as an tseilf na samhlacha a mheas. Ansin meascaimid trí thacar sonraí le haghaidh oiliúna; cé go bhfuil difríocht idir a bhfearann, treoirlínte anótála, agus meiteashonraí, molaimid modh chun samhail aonair a oiliúint ar an meascán sonraí ilchineálach seo trí úsáid a bhaint as méadú sonraí chun cuntas a thabhairt ar dhifríochtaí anótála agus sampláil chun na cainníochtaí sonraí a chothromú. Feictear dúinn i suíomh nialas lámhaigh, go n-aistríonn múnlaí oilte ar thacar sonraí amháin go dona, agus go mbíonn feabhas ar fheidhmíocht fhoriomlán ag torthaí comhoiliúna, rud a fhágann go mbíonn ginearálú níos fearr i múnlaí réitigh croífhreagartha. Cuireann an obair seo tagarmharc nua le haghaidh réiteach láidir croí-chomhdhála agus torthaí iolracha nua den scoth.</abstract_ga>
      <abstract_hu>Míg a coreferencia felbontást az adatkészlet tartományától függetlenül definiálják, a legtöbb modell a coreferencia felbontás végrehajtásához nem jut jól át láthatatlan tartományokra. A különböző tartományokat célzó 8 coreferencia felbontású adatkészletet konszolidálunk, hogy értékeljük a modellek nem használható teljesítményét. Ezután három adatkészletet keverünk össze a képzéshez; Annak ellenére, hogy tartományuk, jegyzetelési irányelvük és metaadataik eltérnek, javasoljuk egy módszert arra, hogy egyetlen modellt közösen képezzünk erről a heterogén adatkeverékről, adatbővítés alkalmazásával a jegyzetelési különbségek figyelembevételére és az adatmennyiségek kiegyensúlyozására. Úgy találjuk, hogy egy nullás beállításban az egyetlen adatkészletre képzett modellek rosszul továbbítanak, míg a közös képzés javítja az általános teljesítményt, ami a coreferencia felbontási modellek jobb általánosítását eredményezi. Ez a munka új referenciaértéket jelent a robusztus coreferencia felbontás és a több új, korszerű eredmény számára.</abstract_hu>
      <abstract_it>Mentre la risoluzione di coreferenza è definita indipendentemente dal dominio del set di dati, la maggior parte dei modelli per eseguire la risoluzione di coreferenza non si trasferisce bene a domini invisibili. Consolidamo un set di 8 set di dati con risoluzione di coreferenza mirati a diversi domini per valutare le prestazioni off-the-shelf dei modelli. Mescoliamo quindi tre set di dati per la formazione; Anche se il loro dominio, le linee guida di annotazione e i metadati differiscono, proponiamo un metodo per formare congiuntamente un singolo modello su questa miscela eterogenea di dati utilizzando l'aumento dei dati per tenere conto delle differenze di annotazione e il campionamento per bilanciare le quantità di dati. Troviamo che in un'impostazione zero-shot, i modelli addestrati su un singolo set di dati si trasferiscono male mentre l'allenamento congiunto produce prestazioni complessive migliorate, portando a una migliore generalizzazione dei modelli di risoluzione della coreferenza. Questo lavoro contribuisce a un nuovo punto di riferimento per una risoluzione robusta della coreferenza e molteplici nuovi risultati all'avanguardia.</abstract_it>
      <abstract_el>Ενώ η ανάλυση συνάφειας ορίζεται ανεξάρτητα από τον τομέα συνόλου δεδομένων, τα περισσότερα μοντέλα για την εκτέλεση της ανάλυσης συνάφειας δεν μεταφέρονται καλά σε αόρατους τομείς. Ενσωματώνουμε ένα σύνολο 8 συνόλων δεδομένων επίλυσης αλληλοδιαφορών που στοχεύουν σε διαφορετικούς τομείς για να αξιολογήσουμε την απόδοση των μοντέλων. Στη συνέχεια, αναμιγνύουμε τρία σύνολα δεδομένων για την κατάρτιση. Παρόλο που ο τομέας, οι κατευθυντήριες γραμμές σχολιασμού και τα μεταδεδομένα διαφέρουν, προτείνουμε μια μέθοδο για την κοινή εκπαίδευση ενός ενιαίου μοντέλου σε αυτό το ετερογενή μείγμα δεδομένων χρησιμοποιώντας την αύξηση δεδομένων για να υπολογίσουν τις διαφορές σχολιασμού και τη δειγματοληψία για να εξισορροπήσουν τις ποσότητες δεδομένων. Διαπιστώνουμε ότι σε μια ρύθμιση μηδενικής λήψης, μοντέλα εκπαιδευμένα σε ένα μόνο σύνολο δεδομένων μεταφέρονται ανεπαρκώς, ενώ η κοινή εκπαίδευση αποδίδει βελτιωμένη συνολική απόδοση, οδηγώντας σε καλύτερη γενίκευση στα μοντέλα ανάλυσης αλληλοδιαφορών. Η εργασία αυτή συμβάλλει σε ένα νέο σημείο αναφοράς για την εύρωστη ανάλυση της συνάφειας και πολλαπλά νέα αποτελέσματα τελευταίας τεχνολογίας.</abstract_el>
      <abstract_mk>While coreference resolution is defined independently of dataset domain, most models for performing coreference resolution do not transfer well to unseen domains.  Ние консолидираме сет 8 компјутери на податоци за резолуција на кореференција кои се насочени кон различни домени за проценка на изведувањето на моделите надвор од полицата. We then mix three datasets for training;  even though their domain, annotation guidelines, and metadata differ, we propose a method for jointly training a single model on this heterogeneous data mixture by using data augmentation to account for annotation differences and sampling to balance the data quantities.  Најдовме дека во седиште на нула снимка, моделите обучени за едно пренесување на податоци лошо, додека заедничката обука предизвикува подобрување на целокупната резултатност, што води до подобра генерализација на моделите за резолуција на конференцијата. This work contributes a new benchmark for robust coreference resolution and multiple new state-of-the-art results.</abstract_mk>
      <abstract_ka>თუმცა კონფერენციის რეზიციო განსაზღვრებულია მონაცემების დემომინისგან განსაზღვრებულია, ბევრი მოდელები კონფერენციის რეზიციოს გავაკეთებელად არ მუშაობს ჩვენ კონსოლიდირებული 8 წინასწორების მონაცემების კონსოლიდირება, რომლებიც განსხვავებული დიომენების მიზეზი, რომ მოდელების გამოკლებული კონსოციენტის შესახებ გადა შემდეგ ჩვენ სამი მონაცემების კონფიგურაციის შესახებ დავწყებთ; მათი დიომინი, ანოტაციის მინუსები და მეტადეტაციის განსხვავებულია, ჩვენ მინუსვთ ერთადერთი მოდელს ამ ჰეტეროგენური მონაცემების შემთხვევაზე ერთადერთი მოდულის შემწყვება, გამოყენებით მონაცემების აგგენტირება ჩვენ აღმოჩნეთ, რომ ნულ სტატის შენახვეში, მოდელები ერთი მონაცემების გადატანისაზე ცოტა, როცა ერთმანეთი განაცემების შესაძლებლობა უფრო უფრო უფრო უფრო უფრო უფრო უფრო უ ეს სამუშაო ახალი ბანქმერტის შესახებ ძალიან წარმოდგენებისთვის და მრავალი ახალი წარმოდგენების შესახებ.</abstract_ka>
      <abstract_lt>Nors koreferencinė rezoliucija apibrėžiama nepriklausomai nuo duomenų rinkinio domeno, dauguma koreferencinės rezoliucijos atlikimo modelių netinka nematoms domenams. Sujungiame 8 duomenų rinkinius, skirtus skirtingoms sritims, kad būtų galima įvertinti modelių eksploatacinius rodiklius. Tada deriname tris mokymo duomenų rinkinius; nors jų sritis, anotacijos gairės ir metaduomenys skiriasi, mes siūlome metodą bendram vieno modelio mokymui šiame heterogeniškame duomenų mišinyje, naudojant duomenų didinimą, kad būtų atsižvelgta į anotacijos skirtumus ir mėginių ėmimą duomenų kiekiams balansuoti. We find that in a zero-shot setting, models trained on a single dataset transfer poorly while joint training yields improved overall performance, leading to better generalization in coreference resolution models.  Šis darbas prisideda prie naujo patikimos koreferencijos sprendimo ir kelių naujų naujausių rezultatų lyginamojo rodiklio.</abstract_lt>
      <abstract_ml>ഡാറ്റാസറ്റ് ഡോമെനിനെ സ്വാതന്ത്രമായി കോര്‍ഫെന്‍സിന്‍റെ റിസ്റ്റല്‍ നിര്‍ണയിക്കുമ്പോള്‍, കോര്‍ഫെന്‍സ് റിസ്റ്റല്‍ പ മോഡലുകളുടെ ഓഫ് ഷെല്‍ഫ് പ്രവര്‍ത്തിപ്പിക്കാന്‍ വ്യത്യസ്തമായ ഡോമീനുകളെ ലക്ഷ്യം വരുത്തുന്ന 8 കോര്‍ഫെന്‍സ് റെല്‍സല്‍ ഡേറ പിന്നീട് പരിശീലനത്തിനായി മൂന്നു ഡാറ്റാസറ്റുകള്‍ കൂട്ടിച്ചേര്‍ക്കുന്നു; അവരുടെ ഡൊമെയിന്‍, അഭിപ്രായശ്ചിത്രത്തിന്റെ വഴികാട്ടികള്‍, മെറ്റേഡാറ്റാ വ്യത്യാസങ്ങള്‍ വ്യത്യസ്തമാണെങ്കിലും ഞങ്ങള്‍ ഒരു മാറ്റം പരിശീലിപ്പിക്കുന്നത് ഈ ഹെറോ ഒരു പൂര്‍ണ്ണമായ വെടിവെക്കുന്ന സെറ്റില്‍, ഒരു ഡാറ്റാസേറ്റ് മാറ്റങ്ങളില്‍ പരിശീലിക്കപ്പെട്ട മോഡലുകള്‍ തെറ്റായി പരിശീലനം നടത്തിയിരിക്ക ഈ ജോലി റോബോസ്റ്റ് കോര്‍ഫെന്‍സ് റിസല്‍മെന്‍സിനുള്ള പുതിയ ബെങ്ക്മാര്‍ക്ക് ചെയ്യുന്നു. പിന്നെ പുതിയ സ്ഥ</abstract_ml>
      <abstract_ms>Sementara resolusi rujukan ditakrif secara independen dari domain set data, kebanyakan model untuk melakukan resolusi rujukan tidak dipindahkan dengan baik ke domain yang tidak terlihat. Kami mengkonsolidasi set 8 set data resolusi koreferensi yang menargetkan domain yang berbeza untuk menilai prestasi off-the-shelf model. Kemudian kita campur tiga set data untuk latihan; walaupun domain, arah anotasi, dan metadata mereka berbeza, kami cadangkan satu kaedah untuk melatih bersama model tunggal dalam campuran data heterogene ini dengan menggunakan peningkatan data untuk menganggap perbezaan anotasi dan pengumpulan sampel untuk seimbang kuantiti data. Kami mendapati bahawa dalam seting tembakan sifar, model dilatih pada pemindahan set data tunggal kurang sementara pelatihan kongsi memberikan prestasi umum yang lebih baik, yang membawa kepada generalisasi lebih baik dalam model resolusi koreferensi. Kerja ini menyumbangkan tanda referensi baru untuk resolusi persamaan yang kuat dan pelbagai hasil state-of-the-art baru.</abstract_ms>
      <abstract_kk>Мұқсаттық айырымдылығы деректер жинақтау доменінен тәуелсіз анықталғанда, мұқсаттық айырымдылығын орындау үшін көпшілігі домендерге жақсы аударылмайды. Біз 8 сәйкестік айырмашылық деректер жиынын консолидациялау үшін басқа домендердің үлгілерін шектеу үшін басқа домендерді оқу үшін. Содан кейін бұл үш деректер жинағын біріктіреміз. Олардың домені, жаңарту бағыттаулары және метадеректері айырмаса да, біз деректердің сандарын баланстыру үшін деректерді көптеу үшін бір үлгісін біріктіру үшін, бір ретерген деректердің араласуын қолдануға арналадық. Біз нөл сүру баптауында бір деректер жиынының алмасуына оқылған үлгілері жалғастырып, біріктірілген оқыту үлгілері жалғастырып жатқан жұмыс істеу үлгілерінде жақсы жасайды. Бұл жұмыс құпты мәселелердің айырмашылығына және бірнеше жаңа күйінің нәтижесін жасайды.</abstract_kk>
      <abstract_pl>Chociaż rozdzielczość koreferencji jest definiowana niezależnie od domeny zestawu danych, większość modeli wykonywania rozdzielczości koreferencji nie jest dobrze przenoszona do niewidocznych domen. Konsolidujemy zestaw danych o rozdzielczości ośmiu współdzielczości skierowanych do różnych domen w celu oceny gotowej wydajności modeli. Następnie łączymy trzy zbiory danych dla szkoleń; Chociaż ich domena, wytyczne adnotacyjne i metadane różnią się od siebie, proponujemy metodę wspólnego szkolenia pojedynczego modelu na temat tej heterogenicznej mieszanki danych poprzez wykorzystanie rozszerzenia danych w celu uwzględnienia różnic adnotacyjnych i próbkowania w celu zrównoważenia ilości danych. Stwierdzimy, że w ustawieniach zerowych modele przeszkolone na pojedynczym zbiorze danych są źle przesyłane, podczas gdy wspólne treningi dają poprawę ogólnej wydajności, prowadząc do lepszego uogólnienia modeli rozdzielczości współdzielczej. Praca ta przyczynia się do nowego punktu odniesienia dla solidnej rozdzielczości współdziałania i wielu nowych, najnowocześniejszych wyników.</abstract_pl>
      <abstract_ro>În timp ce rezoluția coreferenței este definită independent de domeniul setului de date, majoritatea modelelor pentru efectuarea rezoluției coreferenței nu se transferă bine în domeniile nevăzute. Consolidăm un set de 8 seturi de date cu rezoluție de corefență care vizează diferite domenii pentru a evalua performanța off-the-shelf a modelelor. Apoi amestecăm trei seturi de date pentru formare; Chiar dacă domeniul lor, liniile directoare de adnotare și metadatele diferă, propunem o metodă de formare comună a unui singur model pe acest amestec de date eterogen prin utilizarea măririi datelor pentru a lua în considerare diferențele de adnotare și eșantionare pentru a echilibra cantitățile de date. Considerăm că, într-o setare zero-shot, modelele instruite pe un singur set de date transferă slab, în timp ce formarea comună oferă performanțe generale îmbunătățite, ducând la o mai bună generalizare a modelelor de rezoluție coreferență. Această lucrare contribuie la un nou punct de referință pentru rezoluția robustă a corefenței și multiple rezultate noi de ultimă generație.</abstract_ro>
      <abstract_mt>Filwaqt li r-riżoluzzjoni tal-koreferenza hija definita indipendentement mid-dominju tas-sett tad-dejta, il-biċċa l-kbira tal-mudelli għat-twettiq tar-riżoluzzjoni tal-koreferenza ma jittrasferixxux tajjeb għal dominji mhux osservati. Aħna nikkonsolidaw sett ta’ 8 settijiet ta’ dejta dwar ir-riżoluzzjoni tal-koreferenza mmirati lejn dominji differenti biex jevalwaw il-prestazzjoni off-the-shelf tal-mudelli. We then mix three datasets for training;  even though their domain, annotation guidelines, and metadata differ, we propose a method for jointly training a single model on this heterogeneous data mixture by using data augmentation to account for annotation differences and sampling to balance the data quantities.  Aħna nsibu li f’ambjent b’zero shot, mudelli mħarrġa fuq trasferiment ta’ sett wieħed ta’ dejta b’mod ħa żin filwaqt li taħriġ konġunt jirriżulta f’prestazzjoni globali mtejba, li twassal għal ġeneralizzazzjoni aħjar fil-mudelli ta’ riżoluzzjoni ta’ korreferenza. Dan ix-xogħol jikkontribwixxi għal punt ta’ riferiment ġdid għar-riżoluzzjoni robust a tal-koreferenza u għal diversi riżultati ġodda l-aktar avvanzati.</abstract_mt>
      <abstract_sr>Iako je rezolucija ljubaznosti definisana nezavisno od domena podataka, većina modela za izvršavanje rezolucije ljubaznosti ne prenose dobro u nevidljive domene. Konsolidiramo setu 8 podataka za rezoluciju pristojnosti koji ciljaju različite domene za procjenu izveštaja izveštaja modela. Onda pomiješamo tri seta podataka za obuku; Iako se njihova domena, uputstva za annotaciju i metadatove razlikuju, predlažemo metodu za zajedničku obuku jednog model a o ovoj heterogeneznoj mješavini podataka koristeći povećanje podataka kako bi se računalo za razlike za annotaciju i uzorke za ravnotežu količine podataka. Našli smo da u nulom snimanju, modeli koji su obučeni na jednom prenošenju podataka loše, dok zajednička obuka donosi poboljšanje ukupnog izvođenja, koji vodi do boljih generalizacije u modelima rezolucije. Ovaj rad doprinosi novim kriterijom za jaku rezoluciju pristojnosti i više novih rezultata umjetnosti.</abstract_sr>
      <abstract_no>Mens koreferanse oppløysing er definert uavhengig av datasettdomene, vil dei fleste modelane for å utføra koreferanse oppløysing ikkje overføra godt til ulike domene. Vi konsoliderer eit sett av 8 datasett for oppløysing av koreferansen som mål på ulike domene for å evaluere utgåva av hjelp av modeller. Vi blander derfor tre datasett for trening. Selv om dei domene, notasjonshjelpelinjene og metadata er forskjellige, foreslår vi ein metode for å kopla opplæring av eit enkelt modell på denne heterogeneske data-mixturen ved å bruka data-augmentasjon for å rekna på forskjeller på annotasjonar og samling for å balansera datakvantitetane. Vi finn at i eit nullsatt innstilling treng modeller på ein enkelt dataset overføring slik dårlig mens samanlig opplæring fører til forbetra overalt utvikling, som fører til bedre generellisering i høgreoppløysingsmodular. Dette arbeidet bidra til ein ny benchmarke for kraftig oppløysing av koreferansen og fleire nye resultat av kunsten.</abstract_no>
      <abstract_mn>Хэдийгээр зөвхөн зөвхөн шийдвэрлэл өгөгдлийн хэлбэрээс ялгаагүй тодорхойлдог ч ихэнх загвар нь зөвхөн тодорхойлдоггүй хэлбэрээр шийдвэрлэлт хийх загварууд сайн харагдахгүй Бид 8 сайхан шийдвэрлэлийн өгөгдлийн сангуудыг удирддаг. Загваруудын ажиллагааг үнэлэхэд өөр загваруудыг зориулдаг. Дараа нь бид 3 өгөгдлийн санг сургалтын тулд цуглуулдаг. Хэдийгээр бид өгөгдлийн хэмжээсүүдийг баланслахын тулд өгөгдлийн нэмэгдүүлэлтийг ашиглаж өгөгдлийн нэмэгдүүлэлтийг ашиглаж өгөгдлийн загвар, мета өгөгдлийн хэмжээсүүдийг баланслахын тулд нэг загвар суралцах боломжтой. Нэг өгөгдлийн сангийн шинжлэх ухаан дээр сургалтын загварууд нь хамтдаа сургалтын үр дүнд бүх үйл ажиллагааг улам сайжруулж, эсрэг шийдвэрлэлтийн загваруудыг илүү ерөнхийлөгчилж чадна. Энэ ажлын хувьд хүчтэй сайхан шийдвэрлэлийн шийдвэрлэлийн шинэ багц болон олон шинэ урлагийн үр дүн үүсгэдэг.</abstract_mn>
      <abstract_si>කෝරෙෆෙරෙන්ස් රිසෝල්යුෂන් දත්ත සැට් ඩෝමේන් වලින් ස්වයංක්‍රියාවක් විශේෂ කරලා තියෙන්නේ, බොහෝ මොඩේ අපි කොන්සෝලිඩ් කරනවා කෝරෙෆෙරෙෆෙන්ස් රිසෝල්යුෂ් දත්ත සෙට්ටුවක් වෙනස් ඩෝමේන්ස්ට් ලක්ෂණය කරනවා මොඩේ ඊට පස්සේ අපි දත්ත සෙට් තුනක් ප්‍රශ්නයක් වෙනුවෙන්. ඒ වගේම ඔවුන්ගේ ඩෝමේන්, අනුවාර්ථන ප්‍රවේශය, මෙටාඩේටා වෙනස් වෙනස් වුනොත්, අපි ප්‍රවේශයක් සම්බන්ධ වෙනුවෙන් ප්‍රවේශය කරන්න ප්‍රවේශය කරනවා මේ  අපිට හොයාගන්න පුළුවන් විදිහට සුන්ධ වෙඩි තියෙන්නේ, මොඩේල් එක්ක දත්ත සැට් එකක් විදිහට පරීක්ෂා කරලා තියෙන්නේ සාමාන්‍ය ප්‍රේෂ මේ වැඩේ අළුත් බෙන්ච්මාර්ක් එකක් සම්බන්ධ විශේෂණය සහ අළුත් ස්ථානය ප්‍රතිචාරයක් සම්බන්ධ වෙන</abstract_si>
      <abstract_so>Inta lagu qorayo go’aanka kaararka oo iskaa u gaar ah gudaha macluumaadka, tusaalooyin badan oo lagu sameeyo qayb-qaadashada kaarka ma bedeli karo meelaha qarsoon. Waxaannu koobnaynaa koox 8 koox oo kala duduwan oo lagu talo galay meelo kala duduwan si aan u qiimeyno sameynta dabeecada. We then mix three datasets for training;  xitaa in kastoo ay ku kala duwan yihiin deegaankooda, hagitaanka caafimaadka iyo macluumaadka, waxaynu u soo jeedaynaa qaab ku tababarida wadajirka ah oo ku wada tababarida tusaale isku mid ah oo lagu isku xiriirayo macluumaadkan la isku xiriira marka lagu isticmaalo kordhiska data si loo xisaabiyo kala duwanaansho iyo tusaale ahaan si loo balansiyo qiimaha macluumaadka. Waxaynu heli nahay in qaab nuur ah lagu tababariyo samooyin lagu soo wareejiyo koobashada wadajirka ah ay kordhiso tababar-horumar oo dhan, waxaana ka horumarinaya qaababka ku saabsan qeybta. Shaqadaasu waxay leedahay habka cusub ee ku saabsan heshiiska qofka la isticmaalay iyo arimaha cusub ee farshaxanka.</abstract_so>
      <abstract_sv>Koreferensupplösningen definieras oberoende av datauppsättningens domän, men de flesta modeller för att utföra coreferensupplösning överförs inte bra till osedda domäner. Vi konsoliderar en uppsättning av 8 datauppsättningar med coreferencelopplösning som riktar sig till olika domäner för att utvärdera modellernas prestanda. Vi blandar sedan tre dataset för utbildning; Även om deras domän, kommenteringsriktlinjer och metadata skiljer sig åt föreslår vi en metod för att gemensamt utbilda en enda modell på denna heterogena datablandning genom att använda dataförstärkning för att ta hänsyn till kommenteringsskillnader och sampling för att balansera datakvantiteterna. Vi finner att modeller som tränats på en enda datauppsättning i en noll-shot inställning överför dåligt medan gemensam träning ger förbättrad övergripande prestanda, vilket leder till bättre generalisering av coreference resolution modeller. Detta arbete bidrar med ett nytt riktmärke för robust coreferencelopplösning och flera nya toppmoderna resultat.</abstract_sv>
      <abstract_ta>குறிப்பு தெளிவுத்திறன் தகவல் அமைப்பு களத்தின் தனித்தனாக வரையறுக்கப்பட்டுள்ளது, பெரும்பாலான குறிப்பு தெளிவுத்திறனை ச நாம் மாதிரிகளின் செயல்பாட்டை மதிப்பிட 8 கோரின் தெளிவுத்திறன் தகவல் அமைப்புகளை சேர்க்க வேண்டும். பின்னர் பயிற்சிக்கு மூன்று தரவுத்தளங்களை கலக்குவோம். அவர்களுடைய களம், அறிவிப்பு வழிகாட்டிகள், மற்றும் metadata மாறுபட்டாலும், நாம் ஒரு முறையை ஒன்றாக பயிற்சி செய்ய ஒரு முறையாக, இந்த அடர்ந்த தரவு கலப்பில் ஒரு மாதிரியை பயன் ஒரு சூழ்நிலையான செயல்பாட்டில், ஒரே தரவுத்தளத்தை மாற்றும் மாதிரிகளில் பயிற்சி செய்யப்பட்டுள்ளது, ஒரு சேர்ந்த பயிற்சி மாற்றும் போது சே இந்த வேலை ரோப்ட் குறிப்பு தெளிவுத்திறன் மற்றும் பல புதிய நிலையில் கலை முடிவு</abstract_ta>
      <abstract_ur>حاﻻنکہ مہربانی رخصت ڈاٹ سٹ ڈومین کے بغیر تعریف کے طور پر تعریف کی جاتی ہے، اکثر مہربانی رخصت کرنے کے لئے مہربانی رخصت کے طور پر اچھی طرح غیب کی ڈومین کو ترغیر نہیں دیتے۔ ہم نے 8 مہربانی ریزیولوسٹ ڈیٹ سٹ کو متصل کیا ہے جو مختلف ڈومین کا موقع رکھتے ہیں کہ مدل کے غیر شالف فعالیت کا ارزش کریں۔ اس کے بعد ہم تین ڈیٹ سٹ کو تمرین کے لئے ملحق کرتے ہیں۔ اگرچہ ان کے دامنی، انٹوریٹ ہدایت لینڈ، اور متڈیٹ ڈیٹ ڈیٹ لینڈ مختلف ہوتے ہیں، ہم ایک طریقہ پیش کرتے ہیں اس طریقہ پر ایک متحدہ ڈیٹ میکسٹ پر ایک مدل کی آموزش کریں، اس طریقہ سے ڈیٹ اضافہ کرنے کے لئے ڈیٹ اضافہ کرنے کے ہم دیکھتے ہیں کہ ایک صفر-شٹ سٹینٹ میں، ایک ڈیٹ سٹ ترنسیٹ پر مطالعہ کیا گیا تھا، حالانکہ joint training yields improved overall performance, leading to better generalization in coreference resolution models. یہ کام ایک نئی بنچم مارک مضبوط مضبوط مضبوط رسولی کے لئے اور بہت سی نئی موقعیت کے نتائج کے لئے اضافہ کرتا ہے.</abstract_ur>
      <abstract_vi>Mặc dù có phải giải quyết khả năng cao được xác định độc lập với miền tập tin, nhưng hầu hết các mẫu để thực hiện giải quyết hạn mức cao không chuyển tốt đến miền không nhìn thấy. Chúng tôi củng cố một bộ dữ liệu tám khả năng giải quyết nhằm mục tiêu mỗi miền khác nhau để đánh giá hiệu suất của mô- đun. Sau đó chúng ta sẽ kết hợp ba bộ dữ liệu. Dù thuộc lĩnh vực, hướng dẫn ghi chú và siêu dữ liệu có khác nhau, chúng tôi đề xuất một phương pháp để cùng nhau huấn luyện một mô hình duy nhất về hỗn hợp dữ liệu khác nhau này bằng cách sử dụng sự gia tăng dữ liệu để tính to án sự khác nhau và lấy mẫu để cân bằng lượng dữ liệu. Chúng ta thấy các một cách chưa được chảy tới một bộ thống riêng rất xấu, trong khi sự giải trị của một bộ thống của một bộ thống một bộ trí đơn giải và tống tố Công việc này đóng góp một tiêu chuẩn mới cho quyết định khả năng chiến thắng mạnh mẽ và nhiều kết quả mới nhất.</abstract_vi>
      <abstract_uz>Name Biz modellarni qiymatish uchun boshqa domenelarni qiymatlash uchun 8 ta ta ta'minlovchi ravishda foydalanuvchimiz. We then mix three datasets for training;  Agar ularning domen, taʼminlovchi qoidalari, metadata va taʼlumotlar tarkibini o'zgartirib boʻlishi kerak bo'lsa, biz bu yetarli maʼlumot tarkibida bir modelni birlashtirish usulini talab qilamiz va maʼlumot qiymatlarini o'zgartirish uchun maʼlumot yordamida foydalanish mumkin. Biz shunday o'zgarishni o'rganamiz, bitta maʼlumotlar tarjimasida o'rganish modellari yomon, bir bir xil taʼminlovchisi umumiy amalni bajaradi, va bir xil o'zgarish modellarida yaxshi o'zgartiradi. Name</abstract_uz>
      <abstract_bg>Докато резолюцията на кореференцията се определя независимо от домейна на набора от данни, повечето модели за извършване на резолюция на кореференцията не се прехвърлят добре към невидими домейни. Консолидираме набор от 8 набора от данни за разделителна способност, насочени към различни области, за да оценим ефективността на моделите. След това смесваме три набора данни за обучение; въпреки че техните области, насоки за анотация и метаданни се различават, ние предлагаме метод за съвместно обучение на един модел за тази хетерогенна смес от данни чрез използване на увеличаване на данните, за да се отчетат разликите в анотацията, и вземане на проби за балансиране на количествата данни. Установяваме, че при нулева обстановка моделите, обучени върху един набор от данни, се пренасят слабо, докато съвместното обучение дава подобрена цялостна производителност, което води до по-добро обобщаване на моделите за резолюция на кореференцията. Тази работа допринася за нов еталон за стабилна резолюция на кореференцията и множество нови най-съвременни резултати.</abstract_bg>
      <abstract_nl>Hoewel de coreferentie-resolutie onafhankelijk van het datasetdomein wordt gedefinieerd, worden de meeste modellen voor het uitvoeren van coreferentie-resolutie niet goed overgebracht naar onzichtbare domeinen. We consolideren een set van 8 coreference resolutie datasets die gericht zijn op verschillende domeinen om de standaard prestaties van modellen te evalueren. Vervolgens mengen we drie datasets voor training; Hoewel hun domein, annotatierichtlijnen en metadata verschillen, stellen we een methode voor om gezamenlijk één model te trainen op dit heterogene gegevensmengsel door data augmentation te gebruiken om rekening te houden met annotatieverschillen en sampling om de datahoeveelheden in evenwicht te brengen. We merken dat in een zero-shot setting modellen die zijn getraind op een enkele dataset slecht overdragen terwijl gezamenlijke training betere algehele prestaties oplevert, wat leidt tot een betere generalisatie in coreferentie resolutiemodellen. Dit werk draagt bij aan een nieuwe benchmark voor robuuste coreferentie resolutie en meerdere nieuwe state-of-the-art resultaten.</abstract_nl>
      <abstract_hr>Iako se rješenje liječnosti definira nezavisno od domena podataka, većina modela za provedbu rješenja liječnosti ne prenose dobro u nevidljive domene. Konsolidiramo niz 8 podataka za rješavanje pristojnosti koji ciljaju različite domene za procjenu izvanrednih učinka modela. Onda pomiješamo tri podatke za obuku; Iako se njihova domena, uputstva za annotaciju i metapodatke razlikuju, predlažemo metodu za zajedničku obuku jednog model a o ovoj heterogeneznoj mješavini podataka koristeći povećanje podataka kako bi se računalo o različitim annotacijom i uzorak za ravnotežu količina podataka. Pronašli smo da u nulom snimanju, modeli koji su obučeni na jednom prenošenju podataka loše, dok zajednička obuka donosi poboljšanje ukupnog učinka, dovedeći do bolje generalizacije u modele rješavanja pristojnosti. Ovaj rad doprinosi novim kriterijom za jaku rezoluciju dobrote i više novih rezultata umjetnosti.</abstract_hr>
      <abstract_da>Mens coreferenceopløsning defineres uafhængigt af datasæt domæne, overføres de fleste modeller til udførelse af coreferenceopløsning ikke godt til usynlige domæner. Vi konsoliderer et sæt af 8 datasæt med coreferenceopløsning målrettet mod forskellige domæner for at evaluere modellernes off-the-shelf ydeevne. Vi blander derefter tre datasæt til uddannelse; Selvom deres domæne, retningslinjer for noteringer og metadata er forskellige, foreslår vi en metode til i fællesskab at træne en enkelt model om denne heterogene datablanding ved at bruge dataforøgelse til at tage højde for annotationsforskelle og prøveudtagning for at balancere datamængderne. Vi finder, at modeller, der trænes på et enkelt datasæt, i en nulskudsindstilling, overføres dårligt, mens fælles træning giver forbedret samlet ydeevne, hvilket fører til bedre generalisering af coreferenceopløsningsmodeller. Dette arbejde bidrager med et nyt benchmark for robust coreferenceopløsning og flere nye state-of-the-art resultater.</abstract_da>
      <abstract_de>Während die Coreferenz-Auflösung unabhängig von der Dataset-Domäne definiert wird, können die meisten Modelle zur Durchführung der Coreferenz-Auflösung nicht gut auf unsichtbare Domänen übertragen werden. Wir konsolidieren einen Satz von 8-Coreferenz-Auflösungsdatensätzen für verschiedene Domänen, um die Standardleistung von Modellen zu bewerten. Anschließend mischen wir drei Datensätze für die Ausbildung; Obwohl ihre Domäne, Annotationsrichtlinien und Metadaten unterschiedlich sind, schlagen wir eine Methode vor, um gemeinsam ein einziges Modell über diese heterogene Datenmischung zu trainieren, indem wir Daten-Augmentation verwenden, um Annotationsunterschiede zu berücksichtigen und Sampling, um die Datenmengen auszugleichen. Wir stellen fest, dass Modelle, die auf einem einzelnen Datensatz trainiert wurden, in einer Null-Schuss-Einstellung schlecht übertragen, während gemeinsames Training eine verbesserte Gesamtleistung liefert, was zu einer besseren Verallgemeinerung in Coreferenz-Auflösungsmodellen führt. Diese Arbeit leistet einen neuen Benchmark für robuste Coreferenzauflösung und mehrere neue State-of-the-Art Ergebnisse.</abstract_de>
      <abstract_id>Sementara resolusi koreferensi didefinisikan secara independen dari domain set data, kebanyakan model untuk melakukan resolusi koreferensi tidak dipindahkan dengan baik ke domain yang tidak terlihat. Kami mengkonsolidasi set 8 set data resolusi koreferensi yang menargetkan domain yang berbeda untuk mengevaluasi prestasi off-the-shelf model. Kemudian kita campur tiga set data untuk latihan; meskipun domain mereka, arah anotasi, dan metadata berbeda, kami mengusulkan metode untuk bersama-sama melatih model tunggal pada campuran data heterogene ini dengan menggunakan peningkatan data untuk memperhitungkan perbedaan anotasi dan sampel untuk seimbang jumlah data. Kami menemukan bahwa dalam setting zero-shot, model dilatih pada satu set data transfer buruk sementara pelatihan kongsi memberikan prestasi umum yang lebih baik, yang menyebabkan generalisasi lebih baik dalam model resolusi koreferensi. Pekerjaan ini berkontribusi benchmark baru untuk resolusi koreferensi yang kuat dan beberapa hasil baru state-of-the-art.</abstract_id>
      <abstract_fa>در حالی که راه حل رضایت به طور مستقل از حوزه‌های مجموعه داده‌ها تعریف می‌شود، بیشتر مدل‌ها برای انجام راه حل رضایت رضایت به حوزه‌های غیرقابل تغییر داده نمی‌شوند. ما مجموعه‌ای از 8 تنظیم داده‌های حل‌سازی که هدف‌های دامنه‌های مختلف را برای ارزیابی عملکرد مدل‌های خارج از پناهگاه می‌دهیم. سپس سه مجموعه داده برای تمرین می‌کنیم، با وجود اینکه مدل، رهبری‌های نوشته‌ها و متداده‌ها متفاوت می‌شوند، ما یک روش برای آموزش یک مدل در این ترکیب داده‌های متفاوتی با استفاده از افزایش داده‌ها برای حساب تفاوت‌های نوشته‌ها و نمونه‌هایی برای تعادل اندازه‌های داده‌ها پیشنهاد می‌کنیم. ما فهمیدیم که در یک تنظیمات صفر، مدل‌ها در یک تنظیمات داده‌های بد آموزش یافته‌اند، در حالی که تولید آموزش مشترک اجرات عمومی را بهتر می‌کند، که باعث می‌شود به بهترین تولید عمومی در مدل‌های حل‌کننده‌ای باشد. این کار یک مقدار جدید برای حل شدیدی از ارتباط و نتیجه‌های جدیدی از هنر تولید می‌کند.</abstract_fa>
      <abstract_ko>공지소해는 데이터 집합 영역에서 정의된 것이지만 공지소해를 실행하는 데 사용되는 대부분의 모델은 보이지 않는 영역으로 잘 옮겨지지 않는다.우리는 모델의 기존 성능을 평가하기 위해 서로 다른 분야에 대한 8개의 공지 소해 데이터 집합을 통합했다.그리고 우리는 세 개의 데이터 집합을 혼합하여 훈련을 진행한다.비록 그들의 역, 주석 지침과 메타데이터는 다르지만 우리는 이러한 이구 데이터 혼합에서 단일 모델을 연합하여 훈련하는 방법을 제시했다. 방법은 데이터의 확충을 이용하여 주석의 차이를 해석하고 샘플링을 통해 데이터의 양을 균형 있게 하는 것이다.우리는 영포 설정에서 단일 데이터 집합에서 훈련하는 모델의 전송 효과가 매우 나쁘고 연합 훈련은 전체적인 성능을 향상시켜 공지 소해 모델에서 더욱 좋은 범주화를 실현할 수 있음을 발견했다.이 작업은 안정된 공지 해소와 여러 개의 최신 결과에 새로운 기준을 제공했다.</abstract_ko>
      <abstract_sw>Wakati suluhisho la mafanikio linaelezwa huru ya tovuti ya taarifa, mifano mingi ya kutekeleza suluhisho la msingi hazihamishi vizuri kwenda ndani isiyo fichikana. Tunaweza kuunganisha seti ya taarifa za suluhisho la kompyuta 8 zinazolenga maeneo tofauti ili kutathmini utendaji wa mifano. Kisha tunachanganya seti tatu za taarifa kwa ajili ya mafunzo; even though their domain, annotation guidelines, and metadata differ, we propose a method for jointly training a single model on this heterogeneous data mixture by using data augmentation to account for annotation differences and sampling to balance the data quantities.  Tunapata kwamba katika mazingira ya picha sifuri, mifano iliyoendeshwa kwenye usafirishaji wa data moja kwa mbaya wakati mafunzo ya pamoja yanaongeza ufanisi wa jumla, na yanasababisha uzalishaji mzuri katika mifano ya ufumbuzi. Kazi hii inachangia bendera mpya kwa ajili ya suluhisho la mafanikio na matokeo mengi ya hali mpya ya sanaa.</abstract_sw>
      <abstract_tr>Karefleksiýa çözümlenme dataset domenyň özgürlügini tanyşdyrylýan halda, karefleksiýa çözümlenme üçin köp nusgalar daşyrmady. Biz 8 sany çekişmeler çözümlerini consolidatýarys. Farklı sahypalary nusgala etmek üçin nusgalar bar. Sonra okuw üçin üç sany veri setirini çaşyrýarys; Hatta onların domeny, duyurarlama düzenleri ve metadata farklı olmasına rağmen biz veri miktarlarını dengelemek için birlikte bir modeli eğitirmek için bir yöntemi teklif ediyoruz. 0-atjyk düzümlerinde, sanlaryň ýeke bir dataseti üýtgetmesinde ýok bir şekilde eğitilenen nusgalary tapýarys. Birleşik okuw taýýarlanmagyň hemme performansyny gowurak getirilýär we çözümler nusgalarynda has gowurak döredilýär. Bu işe güýçli ýüregiň çözümlerini we täze täze bir sanat netijesi üçin täze bir etiket täsirleýär.</abstract_tr>
      <abstract_af>Terwyl koreferensieresolusie onveilig van datastel domein gedefinieer is, die meeste modele vir die uitvoer van koreferensieresolusie doen nie goed oordra na onversekende domeine nie. Ons konsoliseer 'n stel van 8 koreferensie oplossing datastelle wat verskillende domeine doen om die af-die-shelf-prestasie van modele te evalueer. Ons het dan drie datastel vir onderwerp gemeng; selfs al is hulle domein, annotasie gidsels, en metadata verskillig, ons voorstel 'n metode vir joint onderwerp van 'n enkele model op hierdie heterogeneese data gemeng deur data vergroot te gebruik om rekening vir annotasie verskillinge en verskillinge te bereik om die data quantiteite te balanse. Ons vind dat in 'n nul-skoot instelling, modele wat op 'n enkele datastel oordrag verkeerd is, terwyl saamste onderwerking vergeet het die hele prestasie verbeter, wat na beter generellisering in koreferensie-oplossing modele lei. Hierdie werk bydra 'n nuwe benchmark vir kragtige koreferensie-oplossing en veelvuldige nuwe staat-van-kuns-resultate.</abstract_af>
      <abstract_am>ምንም እንኳን የኮርፌንቨርስቲ ግንኙነት ከዳታ ሳንተር ብልሃት የተረጋገጠ ሲሆን፣ ብዙዎቹ ምሳሌዎች የኮርፌንሬሽን ማስታወቂያውን ለመፈለግ በመልካም አይለውጡም፡፡ የዓይነቶች አካሄዱን ለማስተካከል የክፍለ ሥርዓት አካባቢ አካባቢ እናደርጋለን፡፡ ከዚያም ሦስት ዳታ ሰርተቶችን ለትምህርት እናጣብቃለን:: ምንም እንኳን አካሄዳቸው፣ ማስታወቂያው መሪ እና ማህተት ቢለዩም እንኳን፣ የዳታ ክፍተቶችን በማስተካከል እና የዳታ ክፍተቶችን ለመቆጣጠር እናሳውቃለን፡፡ በzero-shot ክፍል፣ በአንድ ዳታ-ሰርቨር ማቀናጃ ላይ የተማረ ምሳሌዎች በሙሉ ተማርኮ ይሻላል፡፡ ይህ ሥራ አዲስ የኮርፖስቲካ ማስታወቂያውን እና አዲስ የ-አርእስት ውጤቶች አዲስ አዲስ የአፍሪካ ውጤቶች ያሳያል፡፡</abstract_am>
      <abstract_az>Mərhəmətli çözünürlük verilən qurğulu domena bağımsız tərzdə belirlənirsə, çox mərhəmətli çözünürlük etmək üçün modellər gözəl görünməyən domena tərəfindən keçirilməz. Biz 8 rəftar çözünürlük verilənləri birləşdiririk ki, modellərin dəyişikliyini değerləşdirmək üçün müxtəlif domenalar nişanlayırlar. Sonra təhsil üçün üç verilən qurğu karışırıq. Onların domeini, bildirim doğruluqlarını və metadata fərqli olmasına rağmen, biz bu heterogenel veri karışması barəsində bir modeli birlikdə təhsil etmək üçün məlumatları artırmaq və məlumatları müəyyən etmək üçün məlumatları hesablamaq üçün bir modeli təklif edirik. Sıfır-fəsad qurğularında, tək veri qurğularında təhsil edilən modellər pis bir dəyişiklikdə, birlikdə təhsil təhsil ürəklərinin bütün performanslarını daha yaxşı təhsil edir, daha yaxşı generalizasyon modellərdə təhsil edir. Bu işin yeni bir benchmark möhkəm mərhəmət çözünürlərini və çoxlu yeni mərhəmət sonuçlarını sağlar.</abstract_az>
      <abstract_bn>যদিও কোরেফেন্সের রিসেশন ডাটাসেট ডোমেইনের স্বাধীনতা নির্ধারণ করা হয়, তবে কোরেফেন্সের সিদ্ধান্ত নির্ধারণের বেশীরভাগ ম মডেলের অফ-শেল্ফ প্রদর্শনের মূল্যের লক্ষ্য করার জন্য আমরা ৮টি কোরেফেন্সের রিলেশন ডাটাসেট সংশ্লিষ্ট করে দেই। তারপর আমরা প্রশিক্ষণের জন্য তিনটি ডাটাসেট মিশ্রণ করি; এমনকি যদিও তাদের ডোমেইন, বিশ্লেষণ নির্দেশ এবং মেটেডাটা বিভিন্ন ভিন্ন ভিন্ন, আমরা একত্রিত তথ্য মিশ্রিত একটি মডেল প্রশিক্ষণের জন্য একটি পদ্ধতি প্রস্তাব করি যেখানে  আমরা খুঁজে পেয়েছি যে একটি শূন্য গুলি ব্যবস্থায়, একটি ডাটাসেট ট ট্রান্সফারেশনে মডেল প্রশিক্ষণ করা হয়েছে, যেখানে যৌথ প্রশিক্ষণের ফলে সারাটার প্রয এই কাজটি রোবস্ট কর্ফেন্সের সিদ্ধান্তের জন্য নতুন বেনম্যার্ক এবং নতুন স্টেট-অফ-শিল্পের ফলাফলের জন্য প্রদান করে।</abstract_bn>
      <abstract_bs>Iako je rezolucija dobrote definisana nezavisno od domena podataka, većina modela za provedbu rješenja dobrote ne prenose dobro na nedostajene domene. Konsolidiramo setu podataka o rezoluciji 8 pristojnosti koji ciljaju različite domene da procjenjuju izvanrednu funkciju modela. Onda pomiješamo tri dataseta za obuku; Iako se njihova domena, uputstva za annotaciju i metadatove razlikuju, predlažemo metodu za zajedničku obuku jednog model a o ovoj heterogeneznoj mješavini podataka koristeći povećanje podataka kako bi se računalo o različitim annotacijom i uzorak za ravnotežu količina podataka. Pronašli smo da u nulom snimanju, modeli koji su obučeni na jednom prenošenju podataka loše, dok zajednička obuka donosi poboljšanje ukupnog učinka, koji vodi do boljih generalizacije u modelima rješavanja pristojnosti. Ovaj rad doprinosi novim kriterijom za jaku rezoluciju pristojnosti i više novih rezultata umjetnosti.</abstract_bs>
      <abstract_ca>Mentre que la resolució de coreència es defineix independentment del domini del conjunt de dades, la majoria dels models per a fer la resolució de coreència no es transfereixen bé a dominis invisibles. Consolidem un conjunt de 8 conjunts de dades de resolució de coreferencia que s'apunten a dominis diferents per avaluar el rendiment off-shelf dels models. Ens barregem tres conjunts de dades per a formar-nos; encara que el seu domini, les directrices d'anotació i les metadades difereixin, proposem un mètode per formar conjuntament un únic model sobre aquesta mezcla heterogènia de dades utilitzant l'augmentació de dades per tenir en compte les diferències d'anotació i el recolliment de mostres per equilibrar les quantitats de dades. Trobem que en un entorn de fotografies zero, els models entrenats en un conjunt únic de dades transfereixen malament mentre que l'entrenament conjunt produeix millor rendiment global, portant a una millor generalització en els models de resolució de coreferença. Aquesta feina contribueix a un nou punt de referència per a una solució de coreferencia robusta i múltiples resultats més avançats.</abstract_ca>
      <abstract_cs>Zatímco rozlišení koreference je definováno nezávisle na doméně datové sady, většina modelů pro provádění koreference se nepřenáší dobře do neviděných domén. Konsolidujeme sadu osmi datových sad s rozlišením koreferencí zaměřených na různé domény, abychom vyhodnotili standardní výkon modelů. Poté smícháme tři datové sady pro školení; I když se jejich doména, anotační směrnice a metadata liší, navrhujeme metodu společného tréninku jediného modelu na této heterogenní datové směsi pomocí rozšíření dat k zohlednění anotačních rozdílů a vzorkování pro vyvážení množství dat. Zjišťujeme, že v nastavení nulového záběru se modely trénované na jedné datové sadě špatně přenášejí, zatímco společný trénink přináší lepší celkový výkon, což vede k lepší zobecnění modelů rozlišení koreferencí. Tato práce přináší nové měřítko pro robustní rozlišení koreferencí a několik nových nejmodernějších výsledků.</abstract_cs>
      <abstract_et>Kuigi kortereferentsi resolutsioon on määratletud sõltumatult andmekogumi domeenist, ei kandu enamik kortereferentsi resolutsiooni teostamise mudeleid hästi nähtamatutesse domeenidesse. Konsolideerime 8 erinevatele valdkondadele suunatud andmekogumi, et hinnata mudelite kasutusvalmis jõudlust. Seejärel segame koolituseks kolm andmekogumit; Kuigi nende valdkond, annoteerimisjuhised ja metaandmed erinevad, pakume välja meetodi, mille abil koolitada ühiselt üks mudel selle heterogeense andmesegu kohta, kasutades andmete suurendamist annoteerimise erinevuste arvessevõtmiseks ja valimi andmekoguste tasakaalustamiseks. Leiame, et null-shot seadistuses edastavad ühe andmekogumi jaoks koolitatud mudelid halvasti, samas kui ühistreening parandab üldist jõudlust, mis toob kaasa parema üldistamise kordferentsiresolutsiooni mudelites. See töö annab uue võrdlusaluse tugevale kortereferentsi resolutsioonile ja mitmetele uutele kaasaegsetele tulemustele.</abstract_et>
      <abstract_sq>While coreference resolution is defined independently of dataset domain, most models for performing coreference resolution do not transfer well to unseen domains.  Ne konsolidojmë një sërë 8 të dhënash të rezolutës së korreferencës që synojnë fusha të ndryshme për të vlerësuar performancën jashtë raftit të modeleve. We then mix three datasets for training;  edhe pse domenia e tyre, udhëzimet e anotacionit dhe metatë ndryshojnë, ne propozojmë një metodë për trajnimin e përbashkët të një modeli të vetëm në këtë përzierje heterogjene të të dhënave duke përdorur rritjen e të dhënave për të llogaritur ndryshimet e anotacionit dhe marrjen e mostrave për të balancuar sasitë e të dhënave. Ne zbulojmë se në një vendosje zero-shot, modelet e trajnuar në një transferim të vetëm të të dhënave keq ndërsa trajnimi i përbashkët jep përmirësim të performancës së përgjithshme, duke çuar në gjeneralizim më të mirë në modelet e zgjidhjes së korreferencës. Ky punë kontribuon në një pikë të re për zgjidhjen e fortë të korreferencës dhe rezultate të shumta të reja të gjendjes së lartë.</abstract_sq>
      <abstract_fi>Vaikka koreferenssin resoluutio määritellään datajoukon toimialueesta riippumatta, useimmat koreferenssin resoluution suorittamismallit eivät siirry hyvin näkymättömiin toimialueisiin. Yhdistämme kahdeksan eri toimialoille suunnatun koreferenssiresoluution datajoukon arvioidaksemme mallien suorituskykyä. Sitten sekoitamme kolme dataa koulutusta varten. Vaikka niiden toimialue, huomautussuunnitelmat ja metatiedot eroavat toisistaan, ehdotamme menetelmää, jolla voidaan yhdessä kouluttaa yksi malli tästä heterogeenisestä tietoseoksesta käyttämällä tietojen lisäystä merkintöjen erojen huomioimiseksi ja näytteenottoa tietomäärien tasapainottamiseksi. Havaitsemme, että nollalaukauksessa yksittäiseen aineistoon koulutetut mallit siirtyvät huonosti, kun taas yhteisharjoittelu parantaa yleistä suorituskykyä, mikä johtaa parempaan yleistymiseen koreferenssiresoluutiomalleissa. Tämä työ antaa uuden vertailukohdan vahvalle koreferenssiresoluutiolle ja useille uusille huippuluokan tuloksille.</abstract_fi>
      <abstract_hy>Մինչդեռ կորֆերենսի լուծումը սահմանվում է անկախ տվյալների համակարգի տիեզերքից, կորֆերենսի լուծումը կատարելու մոդելների մեծամասնությունը լավ չի փոխանցվում անտեսված տիեզերքին: Մենք կազմակերպում ենք 8 կորեֆերանսի լուծումների տվյալների համակարգ, որոնք նպատակացնում են տարբեր բնագավառների վրա մոդելների արտադրողականությունը գնահատելու համար: We then mix three datasets for training;  չնայած դրանց բնագավառին, annoտացիայի ուղղություններին և մետատվյալներին տարբերվում են, մենք առաջարկում ենք մի մեթոդ, որպեսզի միասին վերապատրաստենք մի մոդել տվյալների խառնուրդի մասին օգտագործելով տվյալների աճը annoտացիայի տարբերությունների հաշվի առնելու և նմուշների Մենք հայտնաբերում ենք, որ զրոյական նկարների ընթացքում, մոդելները, որոնք պատրաստված են միակ տվյալների համակարգի փոխանցման վրա, վատ են, մինչդեռ միասին պատրաստման արդյունքները բարելավում են ընդհանուր արդյունքները, ինչը հանգեցնում է ավելի լավ ընդհան Այս աշխատանքը ներդրում է նոր համեմատային արտահայտություն ուժեղ կորֆերենսի լուծումների և նոր բարձրագույն արդյունքների համար:</abstract_hy>
      <abstract_jv>When corefern Resolution is defined separately of dataset domain, all modes for effecting corefern Resolution do not transfer right to unseen domain. 2 Awak dhéwé ngewehi telu dataset kanggo tukang; iki -- [Ctrl-click to open a link in a popup window] and select a new key from the [Ctrl-click] button in the middle of the reference box. Awak dhéwé éntuk kuwi nggawe 0-0 saiki, model sing ditambah akeh nguasai perusahaan dataset sing gak bener, ngono nggawe kudu nggawe barang apik dhéwé, dadi nggawe ngubah Generalizasi model sing apik dhéwé. Ngubah iné kaé gunaké perusahaan kanggo nggawe geranggé hukum sing nggawe geranggap karo hal-hal sing paling-karang mbukak.</abstract_jv>
      <abstract_ha>Waka da an bayyana juyin korsference an bayyana shi ɗai'a cikin tsarin database, masu yawa masu motsi wa da za'a aikata juyin shawara cire-bone ba su shige shi da alhẽri zuwa sauyen nan da ba'a sani ba. Tuna ƙara koɓa masu daidaita danna masu motsi na cire 8, don su yi amfani da dukka daban-daban, dõmin su canza tsarin-shelf-don-motsi. Sa'an nan kuma muna haɗa matsayin data uku dõmin wa'anar; Haƙĩƙa, kuma kõ dã shirin ayuka, da shirin ayuka da metadata sun sãɓã, sai mu buɗa wata hanyoyi wa shirin su yi wa shirin haɗi a kan wannan shirin da aka haɗa mutane da data ɗin tsohatarwa, ko da amfani da ƙaramako da data dõmin ya yi amfani da zane-zane-zane-zane ko kuma misali dõmin ya daidaita nau'in data. Muna gane cewa, a cikin tsarin sifiri-sifo, misãlai wanda aka yi wa tunkuɗe a kan transfer ɗin da aka saka bayan bayani, a lokacin da shirin haɗi ya ƙara mafarin aikin jumla, yana ƙara wa mafi kyau a cikin misãlai masu motsi na kure. Wannan aikin yana ƙara wani bangon na buƙata don rabon sararin bangon nan da aka samu mutane da fassarar-state-of-art.</abstract_ha>
      <abstract_he>למרות שהפתרון התאמה מוגדר באופן עצמאי משטח נתונים, רוב הדוגמנים לבצע פתרון התאמה לא מעבירים היטב לשטחים בלתי נראים. אנו קונצנצים קבוצה של 8 קבוצות נתונים של פיתרון קופורנציה שמתכוונים לתחומים שונים כדי להעריך ביצועים מחוץ למדף של דוגמנים. ואז אנחנו מערבבים שלושה קבוצות נתונים לאימון; למרות שהשטח שלהם, המדרגות להציאה, ומטאדאטה שונות, אנו מציעים שיטה לאימון יחד מודל אחד על תערובת הנתונים ההטרוגנית הזאת, על ידי השימוש בתעלות הנתונים כדי לחשבון על הבדלים בהציאה ומדוגמנים כדי לאזן את כמויות הנתונים. We find that in a zero-shot setting, models trained on a single dataset transfer poorly while joint training yields improved overall performance, leading to better generalization in coreference resolution models.  העבודה הזו תורמת נקודת רמז חדשה לפתרון חוזק של התאמה ומספר תוצאות חדשות.</abstract_he>
      <abstract_sk>Čeprav je ločljivost jedrske reference opredeljena neodvisno od domene nabora podatkov, se večina modelov za izvajanje ločljivosti jedrske reference ne prenaša dobro na nevidne domene. Združujemo nabor osmih naborov podatkov o ločljivosti jedrske reference, ki so usmerjeni v različna področja, da bi ocenili učinkovitost modelov na trgu. Nato združimo tri nabore podatkov za usposabljanje; čeprav se njihova domena, smernice za opombe in metapodatki razlikujejo, predlagamo metodo za skupno usposabljanje enega modela o tej heterogeni mešanici podatkov z uporabo povečanja podatkov za upoštevanje razlik v opombeh in vzorčenje za uravnoteženje količin podatkov. Ugotovili smo, da se v nastavitvi ničelnega strela modeli, usposobljeni za en sam nabor podatkov, slabo prenašajo, medtem ko skupni trening prinaša izboljšano splošno zmogljivost, kar vodi do boljše generalizacije modelov ločljivosti koreference. To delo prispeva novo merilo za robustno ločljivost jedrske reference in več novih najsodobnejših rezultatov.</abstract_sk>
      <abstract_bo>While coreference resolution is defined independently of dataset domain, most models for performing coreference resolution do not transfer well to unseen domains. ང་ཚོས་སྒྲིག་འཛུགས་གཙང་གཅད་གྱི་མིག་ཐང་གཙང་གཅད་ཀྱི་ཚད་ལྟར་བྱུང་བའི་སྒྲིག་ཡིག འོན་ཀྱང་ང་ཚོས་སློབ་གྲྭར་གྱི་གནད་སྡུད་ཚན་གསུམ་མཉམ་བྱེད་ཀྱི་ཡོད། even though their domain, annotation guidelines, and metadata differ, we propose a method for jointly training a single model on this heterogeneous data mixture by using data augmentation to account for annotation differences and sampling to balance the data quantities. The most important thing is that ང་ཚོས་ཀླད་པའི་སྒྲིག་འགོད་ཀྱི་རྣམ་པ་ཞིག་གིས་མཐུན་རྐྱེན་གྱིས་མིག ལས་ཀ་འདིས་བརྟན་པར་བཟོ་བྱེད་ཀྱི་ཡོད་ཚད་གསར་བ་ཞིག་གིས་མཐུན་བཟོ་བྱེད་ཀྱི་ཡོད།</abstract_bo>
      </paper>
    </volume>
</collection>