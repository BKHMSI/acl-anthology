<collection id="2020.msr">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Multilingual Surface Realisation</booktitle>
      <editor><first>Anya</first><last>Belz</last></editor>
      <editor><first>Bernd</first><last>Bohnet</last></editor>
      <editor><first>Thiago Castro</first><last>Ferreira</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Simon</first><last>Mille</last></editor>
      <editor><first>Leo</first><last>Wanner</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="2a8fdccc">2020.msr-1.0</url>
      <bibkey>msr-2020-multilingual</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The Third Multilingual Surface Realisation Shared Task (<fixed-case>SR</fixed-case>&#8217;20): Overview and Evaluation Results</title>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Anya</first><last>Belz</last></author>
      <author><first>Bernd</first><last>Bohnet</last></author>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <pages>1&#8211;20</pages>
      <abstract>This paper presents results from the Third Shared Task on Multilingual Surface Realisation (SR&#8217;20) which was organised as part of the COLING&#8217;20 Workshop on Multilingual Surface Realisation. As in SR&#8217;18 and SR&#8217;19, the shared task comprised two tracks: (1) a Shallow Track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (2) a Deep Track where additionally, functional words and morphological information were removed. Moreover, each track had two subtracks: (a) restricted-resource, where only the data provided or approved as part of a track could be used for training models, and (b) open-resource, where any data could be used. The Shallow Track was offered in 11 languages, whereas the Deep Track in 3 ones. Systems were evaluated using both automatic metrics and direct assessment by human evaluators in terms of Readability and Meaning Similarity to reference outputs. We present the evaluation results, along with descriptions of the SR&#8217;19 tracks, data and evaluation methods, as well as brief summaries of the participating systems. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume.</abstract>
      <url hash="dbf411db">2020.msr-1.1</url>
      <bibkey>mille-etal-2020-third</bibkey>
      <pwccode url="https://gitlab.com/talnupf/ud2deep" additional="false">talnupf/ud2deep</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
    </paper>
    <paper id="2">
      <title><fixed-case>BME</fixed-case>-<fixed-case>TUW</fixed-case> at <fixed-case>SR</fixed-case>&#8217;20: Lexical grammar induction for surface realization</title>
      <author><first>G&#225;bor</first><last>Recski</last></author>
      <author><first>&#193;d&#225;m</first><last>Kov&#225;cs</last></author>
      <author><first>Kinga</first><last>G&#233;mes</last></author>
      <author><first>Judit</first><last>&#193;cs</last></author>
      <author><first>Andras</first><last>Kornai</last></author>
      <pages>21&#8211;29</pages>
      <abstract>We present a system for mapping Universal Dependency structures to raw text which learns to restore word order by training an Interpreted Regular Tree Grammar (IRTG) that establishes a mapping between string and graph operations. The reinflection step is handled by a standard sequence-to-sequence architecture with a biLSTM encoder and an LSTM decoder with attention. We modify our 2019 system (Kov&#225;cs et al., 2019) with a new grammar induction mechanism that allows IRTG rules to operate on lemmata in addition to part-of-speech tags and ensures that each word and its dependents are reordered using the most specific set of learned patterns. We also introduce a hierarchical approach to word order restoration that independently determines the word order of each clause in a sentence before arranging them with respect to the main clause, thereby improving overall readability and also making the IRTG parsing task tractable. We participated in the 2020 Surface Realization Shared task, subtrack T1a (shallow, closed). Human evaluation shows we achieve significant improvements on two of the three out-of-domain datasets compared to the 2019 system we modified. Both components of our system are available on GitHub under an MIT license.</abstract>
      <url hash="96eab5d3">2020.msr-1.2</url>
      <bibkey>recski-etal-2020-bme</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>NILC</fixed-case> at <fixed-case>SR</fixed-case>&#8217;20: Exploring Pre-Trained Models in Surface Realisation</title>
      <author><first>Marco Antonio</first><last>Sobrevilla Cabezudo</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>50&#8211;56</pages>
      <abstract>This paper describes the submission by the NILC Computational Linguistics research group of the University of S &#771;ao Paulo/Brazil to the English Track 2 (closed sub-track) at the Surface Realisation Shared Task 2020. The success of the current pre-trained models like BERT or GPT-2 in several tasks is well-known, however, this is not the case for data-to-text generation tasks and just recently some initiatives focused on it. This way, we explore how a pre-trained model (GPT-2) performs on the UD-to-text generation task. In general, the achieved results were poor, but there are some interesting ideas to explore. Among the learned lessons we may note that it is necessary to study strategies to represent UD inputs and to introduce structural knowledge into these pre-trained models.</abstract>
      <url hash="0f5f5699">2020.msr-1.6</url>
      <bibkey>sobrevilla-cabezudo-pardo-2020-nilc</bibkey>
    </paper>
    </volume>
</collection>