<collection id="2021.gem">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)</booktitle>
      <editor><first>Antoine</first><last>Bosselut</last></editor>
      <editor><first>Esin</first><last>Durmus</last></editor>
      <editor><first>Varun Prashant</first><last>Gangal</last></editor>
      <editor><first>Sebastian</first><last>Gehrmann</last></editor>
      <editor><first>Yacine</first><last>Jernite</last></editor>
      <editor><first>Laura</first><last>Perez-Beltrachini</last></editor>
      <editor><first>Samira</first><last>Shaikh</last></editor>
      <editor><first>Wei</first><last>Xu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="ab60ad50">2021.gem-1</url>
    </meta>
    <frontmatter>
      <url hash="13027448">2021.gem-1.0</url>
      <bibkey>gem-2021-natural</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Semantic Similarity Based Evaluation for Abstractive News Summarization</title>
      <author><first>Figen</first><last>Beken Fikri</last></author>
      <author><first>Kemal</first><last>Oflazer</last></author>
      <author><first>Berrin</first><last>Yanikoglu</last></author>
      <pages>24&#8211;33</pages>
      <abstract>ROUGE is a widely used evaluation metric in text summarization. However, it is not suitable for the evaluation of abstractive summarization systems as it relies on lexical overlap between the gold standard and the generated summaries. This limitation becomes more apparent for agglutinative languages with very large vocabularies and high type/token ratios. In this paper, we present semantic similarity models for Turkish and apply them as evaluation metrics for an abstractive summarization task. To achieve this, we translated the English STSb dataset into Turkish and presented the first semantic textual similarity dataset for Turkish as well. We showed that our best similarity models have better alignment with average human judgments compared to ROUGE in both Pearson and Spearman correlations.</abstract>
      <url hash="e7d5e247">2021.gem-1.3</url>
      <doi>10.18653/v1/2021.gem-1.3</doi>
      <bibkey>beken-fikri-etal-2021-semantic</bibkey>
      <pwccode url="https://github.com/verimsu/stsb-tr" additional="false">verimsu/stsb-tr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlsum">MLSUM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nli-tr">NLI-TR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="10">
      <title>The <fixed-case>GEM</fixed-case> Benchmark: Natural Language Generation, its Evaluation and Metrics</title>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Tosin</first><last>Adewumi</last></author>
      <author><first>Karmanya</first><last>Aggarwal</last></author>
      <author><first>Pawan Sasanka</first><last>Ammanamanchi</last></author>
      <author><first>Anuoluwapo</first><last>Aremu</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <author><first>Khyathi Raghavi</first><last>Chandu</last></author>
      <author><first>Miruna-Adriana</first><last>Clinciu</last></author>
      <author><first>Dipanjan</first><last>Das</last></author>
      <author><first>Kaustubh</first><last>Dhole</last></author>
      <author><first>Wanyu</first><last>Du</last></author>
      <author><first>Esin</first><last>Durmus</last></author>
      <author><first>Ond&#345;ej</first><last>Du&#353;ek</last></author>
      <author><first>Chris Chinenye</first><last>Emezue</last></author>
      <author><first>Varun</first><last>Gangal</last></author>
      <author><first>Cristina</first><last>Garbacea</last></author>
      <author><first>Tatsunori</first><last>Hashimoto</last></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Yacine</first><last>Jernite</last></author>
      <author><first>Harsh</first><last>Jhamtani</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <author><first>Shailza</first><last>Jolly</last></author>
      <author><first>Mihir</first><last>Kale</last></author>
      <author><first>Dhruv</first><last>Kumar</last></author>
      <author><first>Faisal</first><last>Ladhak</last></author>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Mounica</first><last>Maddela</last></author>
      <author><first>Khyati</first><last>Mahajan</last></author>
      <author><first>Saad</first><last>Mahamood</last></author>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last></author>
      <author><first>Pedro Henrique</first><last>Martins</last></author>
      <author><first>Angelina</first><last>McMillan-Major</last></author>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <author><first>Moin</first><last>Nadeem</last></author>
      <author><first>Shashi</first><last>Narayan</last></author>
      <author><first>Vitaly</first><last>Nikolaev</last></author>
      <author><first>Andre</first><last>Niyongabo Rubungo</last></author>
      <author><first>Salomey</first><last>Osei</last></author>
      <author><first>Ankur</first><last>Parikh</last></author>
      <author><first>Laura</first><last>Perez-Beltrachini</last></author>
      <author><first>Niranjan Ramesh</first><last>Rao</last></author>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Juan Diego</first><last>Rodriguez</last></author>
      <author><first>Sashank</first><last>Santhanam</last></author>
      <author><first>Jo&#227;o</first><last>Sedoc</last></author>
      <author><first>Thibault</first><last>Sellam</last></author>
      <author><first>Samira</first><last>Shaikh</last></author>
      <author><first>Anastasia</first><last>Shimorina</last></author>
      <author><first>Marco Antonio</first><last>Sobrevilla Cabezudo</last></author>
      <author><first>Hendrik</first><last>Strobelt</last></author>
      <author><first>Nishant</first><last>Subramani</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <author><first>Akhila</first><last>Yerukola</last></author>
      <author><first>Jiawei</first><last>Zhou</last></author>
      <pages>96&#8211;120</pages>
      <abstract>We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.</abstract>
      <url hash="0df4bca5">2021.gem-1.10</url>
      <doi>10.18653/v1/2021.gem-1.10</doi>
      <bibkey>gehrmann-etal-2021-gem</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gem">GEM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/asset">ASSET</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commongen">CommonGen</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/czech-restaurant-information">Czech restaurant information</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dart">DART</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e2e">E2E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlsum">MLSUM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/totto">ToTTo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/turkcorpus">TurkCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilingua">WikiLingua</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xsum">XSum</pwcdataset>
    </paper>
    <paper id="11">
      <title>Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the <fixed-case>H</fixed-case>ugging<fixed-case>F</fixed-case>ace and <fixed-case>GEM</fixed-case> Data and Model Cards</title>
      <author><first>Angelina</first><last>McMillan-Major</last></author>
      <author><first>Salomey</first><last>Osei</last></author>
      <author><first>Juan Diego</first><last>Rodriguez</last></author>
      <author><first>Pawan Sasanka</first><last>Ammanamanchi</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Yacine</first><last>Jernite</last></author>
      <pages>121&#8211;135</pages>
      <abstract>Developing documentation guidelines and easy-to-use templates for datasets and models is a challenging task, especially given the variety of backgrounds, skills, and incentives of the people involved in the building of natural language processing (NLP) tools. Nevertheless, the adoption of standard documentation practices across the field of NLP promotes more accessible and detailed descriptions of NLP datasets and models, while supporting researchers and developers in reflecting on their work. To help with the standardization of documentation, we present two case studies of efforts that aim to develop reusable documentation templates &#8211; the HuggingFace data card, a general purpose card for datasets in NLP, and the GEM benchmark data and model cards with a focus on natural language generation. We describe our process for developing these templates, including the identification of relevant stakeholder groups, the definition of a set of guiding principles, the use of existing templates as our foundation, and iterative revisions based on feedback.</abstract>
      <url hash="5259f8bd">2021.gem-1.11</url>
      <doi>10.18653/v1/2021.gem-1.11</doi>
      <bibkey>mcmillan-major-etal-2021-reusable</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/asset">ASSET</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gem">GEM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="16">
      <title>Decoding Methods for Neural Narrative Generation</title>
      <author><first>Alexandra</first><last>DeLucia</last></author>
      <author><first>Aaron</first><last>Mueller</last></author>
      <author><first>Xiang Lisa</first><last>Li</last></author>
      <author><first>Jo&#227;o</first><last>Sedoc</last></author>
      <pages>166&#8211;185</pages>
      <abstract>Narrative generation is an open-ended NLP task in which a model generates a story given a prompt. The task is similar to neural response generation for chatbots; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparameters&#8212;specifically, maximum mutual information&#8212;analyzing results over multiple criteria with automatic and human evaluation. We find that (1) nucleus sampling is generally best with thresholds between 0.7 and 0.9; (2) a maximum mutual information objective can improve the quality of generated stories; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any qualitative metric.</abstract>
      <url hash="aa1c5b4f">2021.gem-1.16</url>
      <doi>10.18653/v1/2021.gem-1.16</doi>
      <bibkey>delucia-etal-2021-decoding</bibkey>
      <pwccode url="https://github.com/AADeLucia/gpt2-narrative-decoding" additional="false">AADeLucia/gpt2-narrative-decoding</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
  </volume>
</collection>