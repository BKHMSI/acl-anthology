<collection id="U19">
  <volume id="1" ingest-date="2019-12-16">
    <meta>
      <booktitle>Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association</booktitle>
      <url hash="f09ad8ed">U19-1</url>
      <editor><first>Meladel</first><last>Mistica</last></editor>
      <editor><first>Massimo</first><last>Piccardi</last></editor>
      <editor><first>Andrew</first><last>MacKinlay</last></editor>
      <publisher>Australasian Language Technology Association</publisher>
      <address>Sydney, Australia</address>
      <month>4--6 December</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="4886300f">U19-1000</url>
      <bibkey>alta-2019-australasian</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Towards A Robust Morphological Analyzer for Kunwinjku</title>
      <author><first>William</first><last>Lane</last></author>
      <author><first>Steven</first><last>Bird</last></author>
      <pages>1&#8211;9</pages>
      <abstract>Kunwinjku is an indigenous Australian language spoken in northern Australia which exhibits agglutinative and polysynthetic properties. Members of the community have expressed interest in co-developing language applications that promote their values and priorities. Modeling the morphology of the Kunwinjku language is an important step towards accomplishing the community&#8217;s goals. Finite State Transducers have long been the go-to method for modeling morphologically rich languages, and in this paper we discuss some of the distinct modeling challenges present in the morphosyntax of verbs in Kunwinjku. We show that a fairly straightforward implementation using standard features of the foma toolkit can account for much of the verb structure. Continuing challenges include robustness in the face of variation and unseen vocabulary, as well as how to handle complex reduplicative processes. Our future work will build off the baseline and challenges presented here.</abstract>
      <url hash="d6978dd8">U19-1001</url>
      <bibkey>lane-bird-2019-towards</bibkey>
    </paper>
    <paper id="3">
      <title>Readability of <fixed-case>T</fixed-case>witter Tweets for Second Language Learners</title>
      <author><first>Patrick</first><last>Jacob</last></author>
      <author><first>Alexandra</first><last>Uitdenbogerd</last></author>
      <pages>19&#8211;27</pages>
      <abstract>Optimal language acquisition via reading requires the learners to read slightly above their current language skill level. Identifying material at the right level is the essential role of automatic readability measurement. Short message platforms such as Twitter offer the opportunity for language practice while reading about current topics and engaging in conversation in small doses, and can be filtered according to linguistic criteria to suit the learner. In this research, we explore how readable tweets are for English language learners and which factors contribute to their readability. With participants from six language groups, we collected 14,659 data points, each representing a tweet from a pool of 4100 tweets, and a judgement of perceived readability. Traditional readability measures and features failed on the data-set, but demographic data showed that judgements were largely genuine and reflected reported language skill, which is consistent with other recent studies. We report on the properties of the data set and implications for future research.</abstract>
      <url hash="f153dffd">U19-1003</url>
      <bibkey>jacob-uitdenbogerd-2019-readability</bibkey>
    </paper>
    <paper id="10">
      <title>Improved Document Modelling with a Neural Discourse Parser</title>
      <author><first>Fajri</first><last>Koto</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>67&#8211;76</pages>
      <abstract>Despite the success of attention-based neural models for natural language generation and classification tasks, they are unable to capture the discourse structure of larger documents. We hypothesize that explicit discourse representations have utility for NLP tasks over longer documents or document sequences, which sequence-to-sequence models are unable to capture. For abstractive summarization, for instance, conventional neural models simply match source documents and the summary in a latent space without explicit representation of text structure or relations. In this paper, we propose to use neural discourse representations obtained from a rhetorical structure theory (RST) parser to enhance document representations. Specifically, document representations are generated for discourse spans, known as the elementary discourse units (EDUs). We empirically investigate the benefit of the proposed approach on two different tasks: abstractive summarization and popularity prediction of online petitions. We find that the proposed approach leads to substantial improvements in all cases.</abstract>
      <url hash="1e04d100">U19-1010</url>
      <bibkey>koto-etal-2019-improved</bibkey>
      <pwccode url="https://github.com/fajri91/RSTExtractor" additional="false">fajri91/RSTExtractor</pwccode>
    </paper>
    <paper id="11">
      <title>Does an <fixed-case>LSTM</fixed-case> forget more than a <fixed-case>CNN</fixed-case>? An empirical study of catastrophic forgetting in <fixed-case>NLP</fixed-case></title>
      <author><first>Gaurav</first><last>Arora</last></author>
      <author><first>Afshin</first><last>Rahimi</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>77&#8211;86</pages>
      <abstract>Catastrophic forgetting &#8212; whereby a model trained on one task is fine-tuned on a second, and in doing so, suffers a &#8220;catastrophic&#8221; drop in performance over the first task &#8212; is a hurdle in the development of better transfer learning techniques. Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different architectures and hyper-parameters affect forgetting in a network. With this study, we aim to understand factors which cause forgetting during sequential training. Our primary finding is that CNNs forget less than LSTMs. We show that max-pooling is the underlying operation which helps CNNs alleviate forgetting compared to LSTMs. We also found that curriculum learning, placing a hard task towards the end of task sequence, reduces forgetting. We analysed the effect of fine-tuning contextual embeddings on catastrophic forgetting and found that using embeddings as feature extractor is preferable to fine-tuning in continual learning setup.</abstract>
      <url hash="5d63513f">U19-1011</url>
      <bibkey>arora-etal-2019-lstm</bibkey>
    </paper>
    <paper id="14">
      <title>Detecting Chemical Reactions in Patents</title>
      <author><first>Hiyori</first><last>Yoshikawa</last></author>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <author><first>Zenan</first><last>Zhai</last></author>
      <author><first>Christian</first><last>Druckenbrodt</last></author>
      <author><first>Camilo</first><last>Thorne</last></author>
      <author><first>Saber A.</first><last>Akhondi</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>100&#8211;110</pages>
      <abstract>Extracting chemical reactions from patents is a crucial task for chemists working on chemical exploration. In this paper we introduce the novel task of detecting the textual spans that describe or refer to chemical reactions within patents. We formulate this task as a paragraph-level sequence tagging problem, where the system is required to return a sequence of paragraphs which contain a description of a reaction. To address this new task, we construct an annotated dataset from an existing proprietary database of chemical reactions manually extracted from patents. We introduce several baseline methods for the task and evaluate them over our dataset. Through error analysis, we discuss what makes the task complex and challenging, and suggest possible directions for future research.</abstract>
      <url hash="748e8b9c">U19-1014</url>
      <bibkey>yoshikawa-etal-2019-detecting</bibkey>
    </paper>
    <paper id="15">
      <title>Identifying Patients with Pain in Emergency Departments using Conventional Machine Learning and Deep Learning</title>
      <author><first>Thanh</first><last>Vu</last></author>
      <author><first>Anthony</first><last>Nguyen</last></author>
      <author><first>Nathan</first><last>Brown</last></author>
      <author><first>James</first><last>Hughes</last></author>
      <pages>111&#8211;119</pages>
      <abstract>Pain is the main symptom that patients present with to the emergency department (ED). Pain management, however, is often poorly done aspect of emergency care and patients with painful conditions can endure long waits before their pain is assessed or treated. To improve pain management quality, identifying whether or not an ED patient presents with pain is an important task and allows for further investigation of the quality of care provided. In this paper, machine learning was utilised to handle the task of automatically detecting patients who present at EDs with pain from retrospective data. Experimental results on a manually annotated dataset show that our proposed machine learning models achieve high performances, in which the highest accuracy and macro-averaged F1 are 91.00% and 90.96%, respectively.</abstract>
      <url hash="ad494e6e">U19-1015</url>
      <bibkey>vu-etal-2019-identifying</bibkey>
    </paper>
    <paper id="24">
      <title>An Improved Coarse-to-Fine Method for Solving Generation Tasks</title>
      <author><first>Wenyv</first><last>Guan</last></author>
      <author><first>Qianying</first><last>Liu</last></author>
      <author><first>Guangzhi</first><last>Han</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <pages>178&#8211;185</pages>
      <abstract>The coarse-to-fine (coarse2fine) methods have recently been widely used in the generation tasks. The methods first generate a rough sketch in the coarse stage and then use the sketch to get the final result in the fine stage. However, they usually lack the correction ability when getting a wrong sketch. To solve this problem, in this paper, we propose an improved coarse2fine model with a control mechanism, with which our method can control the influence of the sketch on the final results in the fine stage. Even if the sketch is wrong, our model still has the opportunity to get a correct result. We have experimented our model on the tasks of semantic parsing and math word problem solving. The results have shown the effectiveness of our proposed model.</abstract>
      <url hash="c185b3e4">U19-1024</url>
      <bibkey>guan-etal-2019-improved</bibkey>
    </paper>
    </volume>
</collection>