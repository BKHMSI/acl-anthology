<?xml version='1.0' encoding='utf-8'?>
<collection id="U19">
  <volume id="1" ingest-date="2019-12-16">
    <meta>
      <booktitle>Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association</booktitle>
      <url hash="f09ad8ed">U19-1</url>
      <editor><first>Meladel</first><last>Mistica</last></editor>
      <editor><first>Massimo</first><last>Piccardi</last></editor>
      <editor><first>Andrew</first><last>MacKinlay</last></editor>
      <publisher>Australasian Language Technology Association</publisher>
      <address>Sydney, Australia</address>
      <month>4--6 December</month>
      <year>2019</year>
    </meta>
    <frontmatter>
      <url hash="4886300f">U19-1000</url>
      <bibkey>alta-2019-australasian</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Towards A Robust Morphological Analyzer for Kunwinjku</title>
      <author><first>William</first><last>Lane</last></author>
      <author><first>Steven</first><last>Bird</last></author>
      <pages>1–9</pages>
      <abstract>Kunwinjku is an <a href="https://en.wikipedia.org/wiki/Australian_Aboriginal_languages">indigenous Australian language</a> spoken in northern Australia which exhibits agglutinative and polysynthetic properties. Members of the community have expressed interest in co-developing language applications that promote their values and priorities. Modeling the <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphology</a> of the <a href="https://en.wikipedia.org/wiki/Kunwinjku_language">Kunwinjku language</a> is an important step towards accomplishing the community’s goals. Finite State Transducers have long been the go-to method for modeling morphologically rich languages, and in this paper we discuss some of the distinct modeling challenges present in the <a href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">morphosyntax</a> of verbs in <a href="https://en.wikipedia.org/wiki/Kunwinjku">Kunwinjku</a>. We show that a fairly straightforward implementation using standard features of the foma toolkit can account for much of the verb structure. Continuing challenges include <a href="https://en.wikipedia.org/wiki/Robustness_(evolution)">robustness</a> in the face of <a href="https://en.wikipedia.org/wiki/Variation_(linguistics)">variation</a> and unseen vocabulary, as well as how to handle complex reduplicative processes. Our future work will build off the baseline and challenges presented here.</abstract>
      <url hash="d6978dd8">U19-1001</url>
      <bibkey>lane-bird-2019-towards</bibkey>
    </paper>
    <paper id="3">
      <title>Readability of Twitter Tweets for Second Language Learners<fixed-case>T</fixed-case>witter Tweets for Second Language Learners</title>
      <author><first>Patrick</first><last>Jacob</last></author>
      <author><first>Alexandra</first><last>Uitdenbogerd</last></author>
      <pages>19–27</pages>
      <abstract>Optimal <a href="https://en.wikipedia.org/wiki/Language_acquisition">language acquisition</a> via <a href="https://en.wikipedia.org/wiki/Reading">reading</a> requires the learners to read slightly above their current <a href="https://en.wikipedia.org/wiki/Language_proficiency">language skill level</a>. Identifying material at the right level is the essential role of automatic readability measurement. Short message platforms such as <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> offer the opportunity for <a href="https://en.wikipedia.org/wiki/Language_acquisition">language practice</a> while reading about current topics and engaging in conversation in small doses, and can be filtered according to linguistic criteria to suit the learner. In this research, we explore how readable tweets are for <a href="https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language">English language learners</a> and which factors contribute to their <a href="https://en.wikipedia.org/wiki/Readability">readability</a>. With participants from six language groups, we collected 14,659 data points, each representing a tweet from a pool of 4100 tweets, and a judgement of perceived readability. Traditional readability measures and <a href="https://en.wikipedia.org/wiki/Feature_(linguistics)">features</a> failed on the data-set, but <a href="https://en.wikipedia.org/wiki/Demography">demographic data</a> showed that judgements were largely genuine and reflected reported language skill, which is consistent with other recent studies. We report on the properties of the data set and implications for future research.</abstract>
      <url hash="f153dffd">U19-1003</url>
      <bibkey>jacob-uitdenbogerd-2019-readability</bibkey>
    </paper>
    <paper id="10">
      <title>Improved <a href="https://en.wikipedia.org/wiki/Document_modelling">Document Modelling</a> with a Neural Discourse Parser</title>
      <author><first>Fajri</first><last>Koto</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>67–76</pages>
      <abstract>Despite the success of attention-based neural models for natural language generation and classification tasks, they are unable to capture the discourse structure of larger documents. We hypothesize that explicit discourse representations have utility for NLP tasks over longer documents or document sequences, which sequence-to-sequence models are unable to capture. For abstractive summarization, for instance, conventional neural models simply match source documents and the summary in a latent space without explicit representation of text structure or relations. In this paper, we propose to use neural discourse representations obtained from a rhetorical structure theory (RST) parser to enhance document representations. Specifically, document representations are generated for discourse spans, known as the elementary discourse units (EDUs). We empirically investigate the benefit of the proposed approach on two different tasks : abstractive summarization and popularity prediction of online petitions. We find that the proposed approach leads to substantial improvements in all cases.</abstract>
      <url hash="1e04d100">U19-1010</url>
      <bibkey>koto-etal-2019-improved</bibkey>
      <pwccode url="https://github.com/fajri91/RSTExtractor" additional="false">fajri91/RSTExtractor</pwccode>
    </paper>
    <paper id="11">
      <title>Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a><fixed-case>LSTM</fixed-case> forget more than a <fixed-case>CNN</fixed-case>? An empirical study of catastrophic forgetting in <fixed-case>NLP</fixed-case></title>
      <author><first>Gaurav</first><last>Arora</last></author>
      <author><first>Afshin</first><last>Rahimi</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>77–86</pages>
      <abstract>Catastrophic forgetting   whereby a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> trained on one <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is fine-tuned on a second, and in doing so, suffers a catastrophic drop in performance over the first <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a>   is a hurdle in the development of better transfer learning techniques. Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different <a href="https://en.wikipedia.org/wiki/Network_architecture">architectures</a> and hyper-parameters affect <a href="https://en.wikipedia.org/wiki/Forgetting">forgetting</a> in a <a href="https://en.wikipedia.org/wiki/Computer_network">network</a>. With this study, we aim to understand factors which cause <a href="https://en.wikipedia.org/wiki/Forgetting">forgetting</a> during sequential training. Our primary finding is that CNNs forget less than LSTMs. We show that max-pooling is the underlying operation which helps CNNs alleviate forgetting compared to LSTMs. We also found that curriculum learning, placing a hard task towards the end of task sequence, reduces <a href="https://en.wikipedia.org/wiki/Forgetting">forgetting</a>. We analysed the effect of <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning contextual embeddings</a> on catastrophic forgetting and found that using embeddings as feature extractor is preferable to <a href="https://en.wikipedia.org/wiki/Fine-tuning">fine-tuning</a> in continual learning setup.</abstract>
      <url hash="5d63513f">U19-1011</url>
      <bibkey>arora-etal-2019-lstm</bibkey>
    <title_ar>هل تنسى LSTM أكثر من CNN؟ دراسة تجريبية للنسيان الكارثي في البرمجة اللغوية العصبية</title_ar>
      <title_es>¿Olvida un LSTM más que una CNN? Un estudio empírico del olvido catastrófico en la PNL</title_es>
      <title_pt>Um LSTM esquece mais do que uma CNN? Um estudo empírico do esquecimento catastrófico em PNL</title_pt>
      <title_ja>LSTMはCNNよりも多くを忘れますか？ NLPにおける壊滅的な忘却の実証的研究</title_ja>
      <title_zh>LSTM忘于CNN乎? NLP中灾难性遗忘之实</title_zh>
      <title_hi>क्या एक एलएसटीएम सीएनएन से अधिक भूल जाता है? एनएलपी में विनाशकारी भूलने का एक अनुभवजन्य अध्ययन</title_hi>
      <title_ga>An ndéanann LSTM dearmad ar níos mó ná CNN? Staidéar eimpíreach ar dearmad tubaisteach i NLP</title_ga>
      <title_hu>Egy LSTM többet felejt el, mint egy CNN? A katasztrofális felejtés empirikus tanulmánya az NLP-ben</title_hu>
      <title_ka>LSTM დახსოვს უფრო მეტი CNN-ზე? კატარსტროფიკაში დავიბრუნეთ იმპერიკალური კვლევა</title_ka>
      <title_kk>LSTM CNN- ден артық ұмытты ма? NLP- де ұмытты катастрофиялық зерттеу</title_kk>
      <title_lt>Does an LSTM forget more than a CNN?  An empirical study of catastrophic forgetting in NLP</title_lt>
      <title_it>Un LSTM dimentica più di una CNN? Uno studio empirico sull'oblio catastrofico nella PNL</title_it>
      <title_el>Ξεχνάει ένα LSTM περισσότερα από ένα CNN; Μια εμπειρική μελέτη της καταστροφικής λησμονής στο NLP</title_el>
      <title_mt>LSTM tinsa aktar minn CNN? Studju empiriku ta’ tinsa katastrofika f’NLP</title_mt>
      <title_mk>Дали LSTM заборава повеќе од CNN? Емпирична студија за катастрофално заборавање во НЛП</title_mk>
      <title_ml>ഒരു എല്‍സ്റ്റം സിന്‍എനിനെക്കാള്‍ മറക്കുന്നുണ്ടോ? NLP-ല്‍ മറന്നുപോകുന്ന വിപത്തുകളുടെ ഒരു ശാസ്ത്രീയ പഠനം</title_ml>
      <title_mn>LSTM CNN-ээс илүү их мартах уу? НЛП-д алдсан гамшгийн судалгаа</title_mn>
      <title_no>Glommer ein LSTM meir enn ein CNN? Name</title_no>
      <title_pl>Czy LSTM zapomina więcej niż CNN? Badanie empiryczne katastrofalnego zapomnienia w NLP</title_pl>
      <title_ms>Adakah LSTM melupakan lebih dari CNN? Sebuah kajian empirik tentang melupakan bencana di NLP</title_ms>
      <title_si>LSTM එකක් CNN වඩා වඩා අමතක වෙනවද? Name</title_si>
      <title_ro>Un LSTM uită mai mult decât un CNN? Un studiu empiric al uitarii catastrofale în PNL</title_ro>
      <title_so>LSTM ma halmaamay wax ka badan CNN? Baaritaan jimicsi ah oo dhibaato ku saabsan halmaanshada NLP</title_so>
      <title_sr>Da li LSTM zaboravi više od CNN? Impirička studija katastrofe zaboravljanja na NLP-u</title_sr>
      <title_sv>Glömmer en LSTM mer än en CNN? En empirisk studie av katastrofal glömska i NLP</title_sv>
      <title_ta>ஒரு LSTM CNN ஐ விட அதிகமாக மறந்து விடுகிறதா? NLP யில் மறந்து விட்ட பிரச்சனையின் ஒரு முக்கியமான ஆராய்ச்சி</title_ta>
      <title_ur>کیا LSTM ایک سی ان سے زیادہ بھول جاتا ہے؟ NLP میں بھول جانے والی مصیبت کی مطالعہ</title_ur>
      <title_uz>LSTM CNN'dan ko'proq yoʻqolishini saqlaymi? Name</title_uz>
      <title_vi>Một băng cử viên quên nhiều hơn cả CNN sao? Nghiên cứu kinh nghiệm về sự lãng quên thảm họa ở Njala.</title_vi>
      <title_hr>Da li LSTM zaboravi više od CNN-a? Empiričko ispitivanje katastrofe zaboravljanja na NLP-u</title_hr>
      <title_nl>Vergeet een LSTM meer dan een CNN? Een empirische studie van catastrofal vergeten in NLP</title_nl>
      <title_de>Vergisst ein LSTM mehr als ein CNN? Eine empirische Studie zum katastrophalen Vergessen in NLP</title_de>
      <title_bg>ЛСМ забравя ли нещо повече от CNN? Емпирично изследване на катастрофалното забравяне в НЛП</title_bg>
      <title_id>Does an LSTM forget more than a CNN?  Sebuah studi empirik tentang melupakan bencana di NLP</title_id>
      <title_da>Glemmer en LSTM mere end et CNN? En empirisk undersøgelse af katastrofal glemme i NLP</title_da>
      <title_fa>آيا LSTM بيشتر از CNN فراموش ميکنه؟ یک مطالعه امپراطوری از فاجعه‌ای که در NLP فراموش می‌کند</title_fa>
      <title_ko>CNN보다 LSTM이 더 건망증이 심해요?자연 언어 처리 중 재난적 망각에 대한 실증 연구</title_ko>
      <title_sw>Je, LSTM anasahau zaidi ya CNN? Utafiti wa msisimko wa ajabu unaosahau NLP</title_sw>
      <title_af>Vergeet 'n LSTM meer as 'n CNN? Name</title_af>
      <title_am>LSTM ከCNN ይልቅ ይረሳልን? በNLP ውስጥ የረሳው የግጭት ጉዳይ ትምህርት</title_am>
      <title_sq>A harron një LSTM më shumë se një CNN? Një studim empirik për harrimin katastrofik në NLP</title_sq>
      <title_tr>LSTM CNN'den köp ýatdan çykarýarmy? NLP'da ýatdan çykan katastrofiýanyň empiriýaly okuwy</title_tr>
      <title_az>LSTM CNN-d…ôn daha √ßox unutdu mu? NLP'd…ô unutduƒüu katastrofi t…ôhsil</title_az>
      <title_hy>Արդյո՞ք LSMT-ը մոռանում է CNN-ից ավելին: ՆԼՊ-ում մոռանալու ճակատագրական ուսումնասիրությունը</title_hy>
      <title_bs>Da li LSTM zaboravi više od CNN-a? Empirička studija katastrofe zaboravljanja na NLP-u</title_bs>
      <title_cs>Zapomíná LSTM víc než CNN? Empirická studie katastrofického zapomenutí v NLP</title_cs>
      <title_bn>Does an LSTM forget more than a CNN?  এনএলপিতে বিপর্যয় ভুলে যাওয়ার একটি প্রাকৃতিক গবেষণা</title_bn>
      <title_fi>Unohtaako LSTM muutakin kuin CNN? Empiirinen tutkimus katastrofaalisesta unohduksesta NLP:ssä</title_fi>
      <title_ca>Un LSTM oblida més que un CNN? Un estudi empíric d'oblidar catastròficament en NLP</title_ca>
      <title_et>Kas LSTM unustab rohkem kui CNN? Empiiriline uuring katastroofilise unustamise kohta NLP-s</title_et>
      <title_sk>Ali LSTM pozabi več kot CNN? Empirična študija katastrofalnega pozabljanja v NLP</title_sk>
      <title_he>האם LSTM שוכח יותר מCNN? מחקר אמפרי של שכחות קטסטרופיות ב-NLP</title_he>
      <title_jv>Apa kok LTT M dikandani layang sandi, tho ? Inji empir sing paling kelas karo kelas kuwi alih di NLP</title_jv>
      <title_ha>Shin an manta wani LSSM ko wani CNN? An jarraba karatun astroikin da aka manta cikin NLP</title_ha>
      <title_bo>LSTM་ཡིས་CNN་ལས་བརྗེད་དགོས་སམ། NLP ནང་དུ་སྣང་བརྗེད་པའི་ཡུལ་གླེང་བརྗེད་སྐབས་སུ་གཏོང་ཁང་ཞིག་</title_bo>
      <abstract_ar>النسيان الكارثي - حيث يتم ضبط النموذج الذي يتم تدريبه على مهمة واحدة بدقة في مهمة ثانية ، ويعاني بذلك انخفاضًا "كارثيًا" في الأداء مقارنة بالمهمة الأولى - يمثل عقبة في طريق تطوير تقنيات تعلم نقل أفضل. على الرغم من التقدم المثير للإعجاب في الحد من النسيان الكارثي ، لدينا فهم محدود لكيفية تأثير البنى المختلفة والمعلمات المفرطة على النسيان في الشبكة. من خلال هذه الدراسة ، نهدف إلى فهم العوامل التي تسبب النسيان أثناء التدريب المتسلسل. النتيجة الأساسية التي توصلنا إليها هي أن شبكات CNN تنسى أقل من LSTMs. نوضح أن max-pooling هي العملية الأساسية التي تساعد شبكات CNN على التخفيف من النسيان مقارنةً بـ LSTM. وجدنا أيضًا أن تعلم المنهج ، بوضع مهمة صعبة في نهاية تسلسل المهام ، يقلل من النسيان. قمنا بتحليل تأثير الضبط الدقيق لحفلات الزفاف السياقية على النسيان الكارثي ووجدنا أن استخدام الزخارف كمستخرج ميزة أفضل من الضبط الدقيق في إعداد التعلم المستمر.</abstract_ar>
      <abstract_pt>O esquecimento catastrófico – pelo qual um modelo treinado em uma tarefa é ajustado em uma segunda e, ao fazê-lo, sofre uma queda “catastrófica” no desempenho em relação à primeira tarefa – é um obstáculo no desenvolvimento de melhores técnicas de aprendizado de transferência. Apesar do impressionante progresso na redução do esquecimento catastrófico, temos uma compreensão limitada de como diferentes arquiteturas e hiperparâmetros afetam o esquecimento em uma rede. Com este estudo, pretendemos compreender os fatores que causam o esquecimento durante o treinamento sequencial. Nossa principal descoberta é que as CNNs esquecem menos que as LSTMs. Mostramos que o max-pooling é a operação subjacente que ajuda as CNNs a aliviar o esquecimento em comparação com as LSTMs. Também descobrimos que a aprendizagem curricular, colocando uma tarefa difícil no final da sequência de tarefas, reduz o esquecimento. Analisamos o efeito do ajuste fino de embeddings contextuais no esquecimento catastrófico e descobrimos que o uso de embeddings como extrator de recursos é preferível ao ajuste fino na configuração de aprendizado contínuo.</abstract_pt>
      <abstract_es>El olvido catastrófico, en el que un modelo entrenado en una tarea se ajusta en una segunda y, al hacerlo, sufre una caída «catastrófica» del rendimiento con respecto a la primera tarea, es un obstáculo en el desarrollo de mejores técnicas de aprendizaje por transferencia. A pesar del progreso impresionante en la reducción del olvido catastrófico, tenemos una comprensión limitada de cómo las diferentes arquitecturas e hiperparámetros afectan el olvido en una red. Con este estudio, nuestro objetivo es comprender los factores que causan el olvido durante el entrenamiento secuencial. Nuestro principal hallazgo es que las CNN olvidan menos que los LSTM. Demostramos que el max-pooling es la operación subyacente que ayuda a las CNN a aliviar el olvido en comparación con los LSTM. También descubrimos que el aprendizaje curricular, que coloca una tarea difícil hacia el final de la secuencia de tareas, reduce el olvido. Analizamos el efecto de ajustar las incrustaciones contextuales en el olvido catastrófico y descubrimos que el uso de incrustaciones como extractor de características es preferible al ajuste fino en la configuración de aprendizaje continuo.</abstract_es>
      <abstract_ja>1つのタスクで訓練されたモデルが1秒で微調整され、それによって1つ目のタスクよりもパフォーマンスが「壊滅的」に低下するという大惨事の忘却は、より良い転送学習テクニックの開発におけるハードルです。壊滅的な忘れ物を減らすことで印象的な進歩を遂げたにもかかわらず、異なるアーキテクチャとハイパーパラメータがネットワーク内の忘れ物にどのように影響するかについての理解は限られています。本研究では、連続したトレーニング中に忘れてしまう要因を理解することを目指しています。我々の主要な発見は、CNNはLSTMよりも少ないことを忘れているということです。最大プーリングは、CNNがLSTMと比較して忘れを緩和するのに役立つ基礎的な操作であることを示しています。また、課題シーケンスの終わりに向かって難しい課題を配置するカリキュラム学習は、忘れを減らすことも見出した。私たちは、壊滅的な忘却に対するコンテキスト埋め込みの微調整の効果を分析し、継続的な学習設定の微調整よりも、埋め込みを機能抽出として使用する方が望ましいことを発見しました。</abstract_ja>
      <abstract_zh>灾难性遗忘- 于一任之中微于二务,而为此者,以首受灾难性之性降 - 开善迁学之碍也。 损灾难性得深进,异架构参数网络。 以此论之,吾所以知序练之所以遗忘也。 大抵 CNN 遗忘少于 LSTM 。 明与 LSTM 比,大池化助 CNN 轻忘之基也。 又见课程学以一剧之务置事之末,可以省忘。 论其微调上下文嵌于灾难性忘,见续学嵌以为特征提取器愈于微调。</abstract_zh>
      <abstract_hi>भयावह भूलना - जिससे एक कार्य पर प्रशिक्षित एक मॉडल को दूसरे पर ठीक किया जाता है, और ऐसा करने में, पहले कार्य पर प्रदर्शन में "भयावह" गिरावट का सामना करना पड़ता है - बेहतर हस्तांतरण सीखने की तकनीकों के विकास में एक बाधा है। विनाशकारी भूल को कम करने में प्रभावशाली प्रगति के बावजूद, हमारे पास सीमित समझ है कि विभिन्न आर्किटेक्चर और हाइपर-पैरामीटर एक नेटवर्क में भूलने को कैसे प्रभावित करते हैं। इस अध्ययन के साथ, हम उन कारकों को समझने का लक्ष्य रखते हैं जो अनुक्रमिक प्रशिक्षण के दौरान भूलने का कारण बनते हैं। हमारी प्राथमिक खोज यह है कि CNN LSTMs से कम भूल जाते हैं। हम दिखाते हैं कि अधिकतम-पूलिंग अंतर्निहित ऑपरेशन है जो एलएसटीएम की तुलना में सीएनएन को भूलने में मदद करता है। हमने यह भी पाया कि पाठ्यक्रम सीखना, कार्य अनुक्रम के अंत में एक कठिन कार्य रखना, भूलने को कम करता है। हमने भयावह भूलने पर ठीक-ट्यूनिंग प्रासंगिक एम्बेडिंग के प्रभाव का विश्लेषण किया और पाया कि फीचर एक्सट्रैक्टर के रूप में एम्बेडिंग का उपयोग करना निरंतर सीखने के सेटअप में ठीक-ट्यूनिंग के लिए बेहतर है।</abstract_hi>
      <abstract_ga>Is bac é dearmad tubaisteach — ina ndéantar mionchoigeartú ar mhúnla a oiltear ar thasc amháin ar an soicind, agus trí laghdú “tubaisteach” a fhulaingt ar fheidhmíocht thar an gcéad tasc — a bheith ina bac ar fhorbairt teicnící foghlama aistrithe níos fearr. In ainneoin dul chun cinn suntasach maidir le dearmad tubaisteach a laghdú, tá tuiscint theoranta againn ar conas a théann ailtireachtaí agus hipearpharaiméadair éagsúla i bhfeidhm ar dearmad a dhéanamh i líonra. Leis an staidéar seo, tá sé mar aidhm againn na fachtóirí is cúis le dearmad le linn oiliúna seicheamhach a thuiscint. Is é an príomhthoradh atá againn ná go ndéanann CNNs dearmad ar níos lú ná LSTManna. Léirímid gurb é an chomhthiomsú uasta an oibríocht bhunúsach a chuidíonn le CNNanna dearmad a dhéanamh i gcomparáid le LSTManna. Fuaireamar amach freisin go laghdaíonn foghlaim curaclaim, trí thasc crua a chur i dtreo dheireadh an tseichimh thasc, an dearmad. Rinneamar anailís ar éifeacht mionchoigeartuithe leabaithe comhthéacsúla ar dhearmadú tubaisteach agus fuarthas amach go bhfuil sé níos fearr úsáid a bhaint as leabaithe mar shainghné-ghné ná mionchoigeartú i socrú na foghlama leanúnaí.</abstract_ga>
      <abstract_el>Η καταστροφική λησμονή, με την οποία ένα μοντέλο εκπαιδευμένο σε μια εργασία προσαρμόζεται σε ένα δευτερόλεπτο, και με αυτόν τον τρόπο, υφίσταται μια "καταστροφική" πτώση στην απόδοση έναντι της πρώτης εργασίας αποτελεί εμπόδιο στην ανάπτυξη καλύτερων τεχνικών μάθησης μεταφοράς. Παρά την εντυπωσιακή πρόοδο στη μείωση της καταστροφικής λησμονής, έχουμε περιορισμένη κατανόηση του πώς διαφορετικές αρχιτεκτονικές και υπερ-παράμετροι επηρεάζουν τη λησμονή σε ένα δίκτυο. Με αυτή τη μελέτη, στοχεύουμε στην κατανόηση παραγόντων που προκαλούν τη λήθη κατά τη διάρκεια της διαδοχικής εκπαίδευσης. Το βασικό μας εύρημα είναι ότι τα CNN ξεχνούν λιγότερα από τα LSTMs. Δείχνουμε ότι η μέγιστη συγκέντρωση είναι η υποκείμενη λειτουργία που βοηθά τα CNN να ανακουφίσουν τη λησμονή σε σύγκριση με τα LSTMs. Διαπιστώσαμε επίσης ότι η εκμάθηση του προγράμματος σπουδών, τοποθετώντας μια δύσκολη εργασία προς το τέλος της ακολουθίας εργασιών, μειώνει τη λήθη. Αναλύσαμε την επίδραση του λεπτού συντονισμού των περιεχομένων ενσωμάτωσης στην καταστροφική λησμονή και διαπιστώσαμε ότι η χρήση ενσωμάτωσης ως εξαγωγέα χαρακτηριστικών είναι προτιμότερη από τον συντονισμό στη συνεχή ρύθμιση μάθησης.</abstract_el>
      <abstract_it>L'oblio catastrofico - per cui un modello addestrato su un compito viene perfezionato su un secondo, e così facendo subisce un calo "catastrofico" delle prestazioni sul primo compito - è un ostacolo allo sviluppo di migliori tecniche di apprendimento di trasferimento. Nonostante i progressi impressionanti nella riduzione dell'oblio catastrofico, abbiamo una comprensione limitata di come diverse architetture e iperparametri influenzano l'oblio in una rete. Con questo studio, miriamo a comprendere i fattori che causano l'oblio durante l'allenamento sequenziale. La nostra principale scoperta è che le CNN dimenticano meno degli LSTMs. Mostriamo che il max-pooling è l'operazione sottostante che aiuta le CNN ad alleviare l'oblio rispetto agli LSTMs. Abbiamo anche scoperto che l'apprendimento del curriculum, ponendo un compito difficile verso la fine della sequenza di compiti, riduce l'oblio. Abbiamo analizzato l'effetto della messa a punto di incorporazioni contestuali sull'oblio catastrofico e abbiamo scoperto che l'utilizzo di incorporazioni come estrattore di funzionalità è preferibile alla messa a punto di una configurazione di apprendimento continuo.</abstract_it>
      <abstract_hu>A katasztrofális felejtés - amelynek során az egyik feladatra képzett modellt finomhangolják a második feladatra, és ennek során "katasztrofális" teljesítménycsökkenést szenved az első feladatnál - akadályozza a jobb transzfer tanulási technikák kifejlesztését. Annak ellenére, hogy a katasztrofális felejtés csökkentése terén elért lenyűgöző előrelépés történt, korlátozott a megértésünk arról, hogy a különböző architektúrák és hiperparaméterek hogyan befolyásolják a felejtést egy hálózatban. Ezzel a tanulmánnyal arra törekszünk, hogy megértsük azokat a tényezőket, amelyek a szekvenciális edzés során felejtést okoznak. Az elsődleges megállapításunk, hogy a CNN-ek kevesebbet felejtenek el, mint az LSTM-ek. Megmutatjuk, hogy a maximális pooling az alapul szolgáló művelet, amely segít a CNN-ek enyhíteni a felejtést az LSTM-ekhez képest. Azt is megállapítottuk, hogy a tanterv tanulása, amely nehéz feladatot helyez a feladatsorozat vége felé, csökkenti a felejtést. Elemeztük a kontextuális beágyazások finomhangolásának hatását a katasztrofális felejtésre, és megállapítottuk, hogy a beágyazások funkciókivonóként való használata előnyösebb a folyamatos tanulási beállítások finomhangolásával szemben.</abstract_hu>
      <abstract_ka>კატატრატური დახსოვრება - რომელიც ერთი რაოდენობაში მოდელი განაკეთებული იქნება სწორედ, და ამას გავაკეთებთ, რომ პირველი რაოდენობაზე 'კატატრატური' გამოკეთება - უკეთესი გასწავლების ტექნონიკების განვითარებაში კატატროფიკას დაბრუნდეს შემდეგ ინტერფექციური პროგრესი, ჩვენ გვაქვს განსხვავებული არქტიქტურების და ჰიპერ-პარამეტრების შესახებ ქსელში დაბრუნდეს. ამ სწავლებით ჩვენ მინდა გავიგოთ ფაქტორები, რომლებიც წარმოიდგენა სეკენტიური განაკლების განმავლობაში. ჩვენი პირველური შესაძლებლობა არის, რომ CNNs დაბრუნდება LSTMs. ჩვენ ჩვენ აჩვენებთ, რომ max-pooling არის ქვემოთ პერაცია, რომელიც CNNs-ს დახმარებს დახმარებას LSTMs-თან. ჩვენ ასევე აღმოჩნეთ, რომ სწავლა სწავლა სწავლა, ძალიან ძალიან რაოდენობა სამუშაო სამუშაო სამუშაო დავასრულება, დაბრუნება ჩვენ ანალიზიციეთ კონტექსტური კონტექსტური დაბრუნებისთვის ეფექტის გამოყენება და აღმოჩნეთ, რომ ინბედინგის გამოყენება როგორც ფუნქციების ექსტრაკტორი უფრო უფრო მეტია</abstract_ka>
      <abstract_mk>Катастрофското заборавање - со кое моделот обучен за една задача е финетизиран за секунда, и во тоа, страдаат од „катастрофален“ пад на перформансата во текот на првата задача - е пречка во развојот на подобри техники за пренесување на училиште. И покрај импресивниот напредок во намалувањето на катастрофалните заборави, имаме ограничено разбирање како различните архитектури и хиперпараметри влијаат на заборавањето во мрежата. Со оваа студија, имаме за цел да ги разбереме факторите кои предизвикуваат заборавување за време на секвенционалната обука. Нашиот примарен откритие е дека СНН забораваат помалку од ЛСТМ. We show that max-pooling is the underlying operation which helps CNNs alleviate forgetting compared to LSTMs.  Исто така, откривме дека учењето на наставните планови, ставањето тешка задача кон крајот на секвенцијата на задачите, го намалува заборавањето. Го анализиравме ефектот на подобрувањето на контекстните вградувања на катастрофалното заборавување и откривме дека користењето на вградувањата како екстрактор на карактеристики е преферибилно од подобрување во постојаното научување поставување.</abstract_mk>
      <abstract_lt>Catastrophic forgetting - whereby a model trained on one task is fine-tuned on a second, and in doing so, suffers a 'catastrophic' drop in performance over the first task - is a hurdle in the development of better transfer learning techniques.  Nepaisant įspūdingos pažangos mažinant nelaimingą pamiršimą, turime ribotą supratimą, kaip skirtingos architektūros ir hiperparatoriai veikia pamiršimą tinkle. Šiuo tyrimu siekiame suprasti veiksnius, kurie sukelia pamiršimą sekacinio mokymo metu. Pagrindinė išvada yra ta, kad CNN pamiršta mažiau nei LSTM. Mes rodome, kad maksimalus susijungimas yra pagrindinė operacija, kuri padeda CNN palengvinti užmiršimą, palyginti su LSTM. Taip pat nustatėme, kad mokymasis mokymosi programose, užduodant sunkią užduotį užduočių sekos pabaigoje, mažina pamiršimą. Išanalizavome patobulintų kontekstinių įdėjimų poveikį katastrofiniam pamirštui ir nustatėme, kad įdėjimų naudojimas kaip savybių ekstraktorius yra geresnis už patobulintą nuolatinio mokymosi sistemoje.</abstract_lt>
      <abstract_kk>Қатастрофиялық ұмыттау - бір тапсырманың бір үлгісін бір ретінде бақылау үлгісі жақсы түзетіледі, және бұл істеу үшін бірінші тапсырманың үстінде 'катастрофиялық' дегенді өзгерту - жақсы аудару оқыту техникаларын жасау Қатастрофиялық ұмыттарды көшірмелеу үшін, біз желінде айырмашылық архитектуралар мен гипер параметрлерді ұмыттауға қандай нәтижесін түсініп тұрмыз. Бұл зерттеу үшін біз келесі оқыту кезінде ұмытты факторларды түсінуге мақсатымыз. Біздің негізгі табуымыз - CNN- лер LSTMs дан аз ұмытты. Біз максималдық жинақтау - CNN- лерін LSTMs- мен салыстырып ұмыттауға көмектеседі. Біз сондай-ақ оқыту бағдарламаларды тапсырмалардың соңына қатты тапсырманы орналастырып, ұмыттауды азайтады. Біз катастрофиялық ұмыттау үшін контекстік ендірудің эффектін анализ және ендіру құралы ретінде құрастырушыларды қолдану үшін дұрыс оқыту баптауларының артықшылығын баптауға мүмкіндік береді.</abstract_kk>
      <abstract_ml>ഭീകരമായ മറക്കുന്നത് - ഒരു ജോലിയില്‍ ഒരു മോഡല്‍ പരിശീലിക്കപ്പെടുന്നത് ഒരു സെക്കന്റിനുള്ളില്‍ മാത്രമാണ്. അങ്ങനെ ചെയ്യുമ്പോള്‍ ആദ്യത്തെ ജോലിക്ക് മുകളില്‍ ഒരു പ്രകടനത വ്യത്യസ്ത്രീകരണങ്ങളും ഹൈപ്പര്‍ പരാമീറ്ററുകളും ഒരു നെറ്റ്‌വര്‍ക്കില്‍ മറക്കുന്നത് എങ്ങനെയാണെന്ന് ഞങ്ങള്‍ക്ക് മനസ്സിലാക്കാന്‍ പരിധിയുണ് ഈ പഠനത്തിന്‍റെ കാര്യത്തില്‍ നമുക്ക് മനസ്സിലാക്കാന്‍ ഉദ്ദേശിക്കുന്നു. അത് പിന്നീട് പരിശീലനത്തിന്‍ നമ്മുടെ പ്രധാനപ്പെട്ട കണ്ടുപിടിക്കുന്നത് എന്താണെന്നാല്‍ CNNs എല്‍സ്റ്റിഎസിനെക്കാള്‍ ക നമ്മള്‍ കാണിക്കുന്നത് ഏറ്റവും കൂടുതല്‍ പൂളിങ് ആണെന്നാണ് കാണിക്കുന്നത്. അത് CNNNs ലോകത്തെ മറക്കാന്‍ സഹായിക്കുന്നു. പണിയുടെ അവസാനത്തിലേക്ക് കഠിനമായ ജോലി വെക്കുന്നതും മറക്കുന്നതും ഞങ്ങള്‍ കണ്ടെത്തി. We analysed the effect of fine-tuning contextual embeddings on catastrophic forgetting and found that using embeddings as feature extractor is preferable to fine-tuning in continual learning setup.</abstract_ml>
      <abstract_ms>Catastrophic forgetting - whereby a model trained on one task is fine-tuned on a second, and in doing so, suffers a 'catastrophic' drop in performance over the first task - is a hurdle in the development of better transfer learning techniques.  Walaupun kemajuan yang menakjubkan dalam mengurangkan melupakan bencana, kita mempunyai pemahaman terbatas bagaimana arkitektur dan parameter-hyper berbeza mempengaruhi melupakan dalam rangkaian. Dengan kajian ini, kita bermaksud untuk memahami faktor yang menyebabkan melupakan semasa latihan berturut-turut. Our primary finding is that CNNs forget less than LSTMs.  Kami menunjukkan bahawa pengumpulan maksimum adalah operasi yang didasarkan yang membantu CNN mengurangi melupakan dibandingkan LSTM. Kami juga mendapati bahawa pelajaran kurikulum, meletakkan tugas sukar menuju akhir urutan tugas, mengurangkan lupaan. Kami menganalisis kesan penyesuaian penyelesaian kontekstual pada lupaan bencana dan mendapati bahawa menggunakan penyelesaian sebagai ekstraktor ciri lebih baik daripada penyesuaian dalam seting belajar terus menerus.</abstract_ms>
      <abstract_mn>Гайхалтай мартах нь - нэг ажил дээр сургалтын загвар нэг секундэд сайн зохицуулагддаг. Үүнийг хийхэд, анхны ажил дээр "гамшигтай" үйл ажиллагаа үйлдвэрлэгддэг нь илүү сайн шилжүүлэх технологиудын хөгжлийн бэрхшээл юм. Гайхалтай мартахын тулд гайхалтай хөгжлийг багасгаж байгаа ч бид өөр архитектур болон гипер параметр сүлжээнд мартах нөлөөлдөг талаар хязгаарлагддаг. Энэ судалгаанд бид дараагийн сургалтын үед мартагдаж буй хүчин зүйлсийг ойлгохын тулд зориулсан. Бидний анхны олж мэдэх зүйл бол CNN-үүд LSTMs ээс бага мартах юм. Бид хамгийн их цуглуулалт нь СНН-д LSTMs-тэй харьцуулахад тусалдаг үйл ажиллагаа юм. Мөн бид сургалтын хөтөлбөр суралцаж, ажлын дарааллын төгсгөлд хэцүү ажил хийж, мартахыг багасгаж байна. Бид гамшигтай мартаж буй гамшигтай орчинд сайхан сорилтуудын үр дүнг шинжилгээ хийсэн. Ингээд суралцах үйл ажиллагааг ашиглах нь үргэлжилсэн суралцах үйл ажиллагаанд сайхан зохицуулах илүү сайхан.</abstract_mn>
      <abstract_mt>Insa katastrofika - li permezz tagħha mudell imħarreġ fuq kompitu wieħed jiġi rfinut f'sekonda, u meta jsir dan, tbati minn tnaqqis 'katastrofiku' fil-prestazzjoni matul l-ewwel kompitu - hija ostaklu fl-iżvilupp ta' tekniki a ħjar ta' tagħlim ta' trasferiment. Minkejja l-progress impressjonanti fit-tnaqqis tal-insenza katastrofika, għandna fehim limitat ta’ kif arkitetturi u parametri iperparatteristiċi differenti jaffettwaw l-insenza f’netwerk. With this study, we aim to understand factors which cause forgetting during sequential training.  Is-sejba primarja tagħna hija li s-CNNs jinsabu inqas minn LSTMs. Aħna nuru li l-aggregazzjoni massima hija l-operazzjoni sottostanti li tgħin lis-CNNs itaffu l-insenza meta mqabbla mal-LSTMs. Instabna wkoll li t-tagħlim tal-kurrikulu, li jpoġġi kompitu diffiċli lejn it-tmiem tas-sekwenza tal-kompiti, inaqqas l-insenza. Aħna analizzaw l-effett tal-aġġustament tal-inkorporazzjonijiet kuntestwali fuq l-inseminazzjoni katastrofika u sabu li l-użu tal-inkorporazzjonijiet bħala estrattur tal-karatteristiċi huwa preferibbli milli l-aġġustament fin fit-tfassil kontinwu tat-tagħlim.</abstract_mt>
      <abstract_no>Katastrofisk glemmering – der eit modell trent på ei oppgåve er fint oppsett på ein sekund, og i å gjøre det fører ein «katastrofisk» slepp i utviklinga over den første oppgåva – er ein hjelp i utviklinga av betre læringsteknikker for overføring. Til tross av uttrykkelig framgang i å redusera katastrofisk glemme, har vi begrenset forståelse av korleis ulike arkitekturar og hyper-parametrar påvirkar å glemme i eit nettverk. Med denne studien må vi forstå faktorer som fører til å glemme under sequential trening. Vår primærfinning er at CNN glemmer mindre enn LSTMs. Vi viser at max- pooling er den underliggende operasjonen som hjelper til CNN å alleviare glemme sammenlignet med LSTMs. Vi har også funne at læring av curriculum, plassering av ei vanskelig oppgåve mot slutten av oppgåvesekvensen, reduserer å glemme. Vi analysere effekten av finnstillingskontekst innbygging på katastrofisk glemme og funne at bruk av innbygging som funksjonsekstraktor er foretrukkeleg å finnstillinga i kontinuerlært læringsoppsett.</abstract_no>
      <abstract_pl>Katastroficzne zapomnienie, polegające na tym, że model trenowany w jednym zadaniu jest dostrojony na drugim, a w ten sposób doświadcza "katastroficznego" spadku wydajności nad pierwszym zadaniem, jest przeszkodą w rozwoju lepszych technik uczenia się transferu. Pomimo imponujących postępów w redukcji katastrofalnego zapomnienia, mamy ograniczone zrozumienie tego, jak różne architektury i hiperparametry wpływają na zapomnienie w sieci. W tym opracowaniu dążymy do zrozumienia czynników, które powodują zapomnienie podczas treningu sekwencyjnego. Nasze główne ustalenie jest takie, że CNN zapominają mniej niż LSTMs. Pokazujemy, że maksymalne pooling jest podstawową operacją, która pomaga CNN złagodzić zapomnienie w porównaniu z LSTMs. Odkryliśmy również, że nauka programu nauczania, umieszczanie trudnego zadania pod koniec sekwencji zadań, zmniejsza zapomnienie. Analizowaliśmy wpływ dostrajania kontekstowych osadzeń na katastrofalne zapomnienie i stwierdziliśmy, że korzystanie z osadzeń jako ekstraktora funkcji jest lepsze niż dostrajanie w ustawieniach ciągłego uczenia się.</abstract_pl>
      <abstract_ro>Uitarea catastrofală - prin care un model instruit pentru o sarcină este reglat fin pe o a doua sarcină și, făcând acest lucru, suferă o scădere "catastrofală" a performanțelor în ceea ce privește prima sarcină - reprezintă un obstacol în dezvoltarea unor tehnici mai bune de transfer de învățare. În ciuda progreselor impresionante în reducerea uitarii catastrofale, avem o înțelegere limitată a modului în care diferitele arhitecturi și hiperparametrii afectează uitarea într-o rețea. Cu acest studiu, ne propunem să înțelegem factorii care cauzează uitarea în timpul antrenamentului secvențial. Principala noastră descoperire este că CNN uită mai puțin decât LSTMs. Noi arătăm că max-pooling este operațiunea de bază care ajută CNN-urile să atenueze uitarea în comparație cu LSTMs. De asemenea, am descoperit că învățarea curriculumului, plasând o sarcină grea spre sfârșitul secvenței sarcinilor, reduce uitarea. Am analizat efectul reglării fine a încorporărilor contextuale asupra uitării catastrofale și am constatat că utilizarea încorporărilor ca extractor de caracteristici este preferabilă în locul reglării fine în configurarea continuă a învățării.</abstract_ro>
      <abstract_sr>Katastrofski zaboravljanje - s kojim je model obučen na jednom zadatku dobro određen na sekundu, a u tome pati "katastrofski" pad ekspluatacije na prvom zadatku - je prepreka u razvoju boljih tehnika učenja transfera. Uprkos impresivnom napretku u smanjenju katastrofskog zaboravljanja, imamo ograničeno razumevanje kako različite arhitekture i hiper-parametre utiču na zaboravljanje u mreži. Sa ovim studijom, ciljamo da razumemo faktore koji uzrokuju zaboravljanje tokom sekvencijskog treninga. Naš prvi nalaz je da CNN zaboravlja manje od LSTMs. Mi pokazujemo da je maksimalno okupljanje temeljna operacija koja pomaže CNN-ima da ublaži zaboravljanje u usporedbi sa LSTMsom. Takođe smo otkrili da učenje nastavnog programa, stavljanje težak zadatak ka kraju poslovne sekvence, smanjuje zaboravljanje. Analizirali smo učinak suštinskih kontekstualnih ugrađenja na katastrofsko zaboravljanje i saznali da je koristiti ugrađenje kao ekstraktor karakteristike bolje da se finalizuje u nastavku nastavljanja učenja.</abstract_sr>
      <abstract_so>Halmaamaha dhibaatada- marka lagu sameynayo tusaale lagu tababariyey shaqo keliya, marka lagu sameeyo, wuxuu ku xanuunsadaa dhaqdhaqaaqa 'dhibaatada' oo ku dhacda shaqada ugu horraysa - waa dhibaato ku jirta horumarinta qalabka waxbarashada bedelka wanaagsan. Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different architectures and hyper-parameters affect forgetting in a network.  Waxbarashadan waxaynu ku talo galaynaa inaan fahno arrimaha sababta halmaanshada xiliga waxbarashada ka dambeeya. Helitaankeena asalka ah waa in CNNs ay halmaamaan in ka yar LSTMs. Waxan tusnaynaa in max-pooling waa hawlaha hoose ah oo caawinaya CNNNs inuu fududeeyo halmaanshada halmaanshada barbarka LSTMs. Sidoo kale waxaynu helnay in waxbarashada waxbarashada, in shaqada aad u adag uu dhamaado dhamaadka shaqada, uu halmaamay. Waxaannu baaraynay saamaynta xiliga hore ee hagaajinta, taasoo ah halmaanka dhibaatada, waxaana ogaanay in isticmaalka firaaqada ah oo kharashka ah ay ka wanaagsan tahay mid si fiican u barashada oo joogtada ah.</abstract_so>
      <abstract_si>අමතක වෙලා තියෙන්නේ - එක වැඩේ ප්‍රධානය කරපු මොඩේල් එකක් තත්වයකට හොඳින් සැකසුම් කරනවා, ඒ වගේම කරපු වෙලාවට, පළවෙනි වැඩේ සැලසුම් වෙනුවෙන් 'අනත ප්‍රශ්නයක් වෙනුවෙන් අමතක වෙලා තියෙනවා නමුත් අපිට ජාලයේ අමතක වෙලා තියෙන්න ප්‍රශ්නයක් තියෙනවා කියලා තේරුම් ගන්න ත මේ අධ්‍යානය සමඟ, අපි අදහස් කරනවා අධ්‍යානයක් තේරුම් ගන්න, ඒ විදියට අමතක කරන්න පුළුවන්. අපේ ප්‍රධාන සොයාගන්නේ CNN නිසා LSTMs වඩා අමතක කරන්න. අපි පෙන්වන්නේ විශේෂ පූලින් එක තමයි පහළින් ඉන්න තියෙන්නේ, ඒකෙන් CNNs ට LSTMs එක්ක අමතක කරන්න උදව් කරනවා. අපි හොයාගත්තා ඒ වගේම විද්‍යාලය ඉගෙන ගන්න, අමාරු වැඩක් අවසානයට දාන්න, අමතක වෙන්න. අපි විශ්ලේෂණය කරලා හොඳ සම්බන්ධ සම්බන්ධ සම්බන්ධ සම්බන්ධ විශ්ලේෂණය කරලා අමතක වෙලා තියෙනවා ඒ වගේම හොයාගත්තා ඒ වගේම සං</abstract_si>
      <abstract_ur>مصیبت بھول جاتی ہے - جس کے ذریعہ ایک کام پر آموزش کی ایک نمونڈل ایک دوسرے پر ٹھیک ٹھیک ٹھیک ہے، اور اس کے ذریعہ سے پہلی کام پر 'مصیبت' ڈوب رہی ہے - بہترین ترنسیٹر سیکھنے کی تکنیکیوں کی پیشرفت میں ایک پردہ ہے۔ ہمیں مصیبت بھول جانے کے لئے اثر انگیز پیشرفت کے باوجود، ہمیں معلوم ہے کہ ایک نیٹ ورک میں بھول جانے کے لئے کس طرح مختلف معمار اور ہیر پارامیٹر اثر دیتے ہیں۔ اس مطالعہ کے ساتھ ہم سمجھنے کا ارادہ رکھتے ہیں کہ فاکتوروں کو سمجھ سکتے ہیں جن کی تعلیم کے بعد بھول جاتی ہیں ہماری اصلی پیدا کرنا یہ ہے کہ CNN اس سے کم بھول جاتے ہیں ہم دکھاتے ہیں کہ سب سے زیادہ پولینگ کا عملیات ہے جو CNN کو LSTMs کے مقابلہ میں بھول جانے کی کمزوری کی مدد کرتی ہے ہم نے بھی پایا کہ تعلیم کی تعلیم، ایک سخت کام کو کام کی تعلیم کے پانے کی طرف رکھتا ہے، بھولنے کو کم کر دیتا ہے. ہم نے مصیبت بھول جانے پر اچھی تنظیم کی کنٹیکسٹ بنڈینگ کا اثر تحقیق کیا اور دیکھا کہ انڈینگ کو اضافہ کرنے والے کے طور پر استعمال کرنا ہمیشہ سیکھنے کے ساتھ اچھی تنظیم کرنا پسند کرتا ہے.</abstract_ur>
      <abstract_ta>மறந்து விடுகிறது - அதைக் கொண்டு ஒரு செயல்பாட்டில் ஒரு மாதிரி பயிற்சி ஒரு மாதிரி ஒரு நொடியில் நன்றாக முடிக்கப்படுகிறது, அப்படிச் செய்தால், முதல் செயல்பாட்டில் ஒர துன்பத்தை மறந்து விடுவதற்கு முன்னேற்றத்தை குறைக்கும் போதும், வேறு வித்தியாசமான அடைவுகள் மற்றும் உயர் அளபுருக்கள் ஒரு வலைப்பின்ன இந்த ஆராய்ச்சியில், நாம் காரணிகளை புரிந்து கொள்ள நினைக்கிறோம். பின்வரும் பயிற்சியில் மறந்த எங்கள் முதல் கண்டுபிடிப்பு என்னவென்றால் CNNs LSTMs ஐ விட குறைவாக மறந்து விடுகிறது. அதிகபட்ச குழுக்கம் என்பதை நாம் காண்பிக்கிறோம் என்பது அடிப்படையில் உள்ள செயல்பாடு அது CNNs மறந்து விடுவதை எளிதாக்க மேலும் நாம் கண்டுபிடித்துள்ளோம் என்று பார்த்தோம், பணி தொடர்ந்து முடிவிற்கு ஒரு கடினமான பணியை வைத்து, மறக்க நாங்கள் துன்பத்தை மறந்து விட்டு நல்ல துண்டிக்கும் நிலையான உள்ளீடுகளின் விளைவுகளை ஆய்வு செய்தோம் மற்றும் தெரிந்து கொண்டிருந்தால் கு</abstract_ta>
      <abstract_sv>Katastrofisk glömska - där en modell som tränats på en uppgift finjusteras på en annan och därmed drabbas av en "katastrofal" minskning av prestanda under den första uppgiften - är ett hinder för utvecklingen av bättre överföringsinlärningstekniker. Trots imponerande framsteg i att minska katastrofal glömska har vi begränsad förståelse för hur olika arkitekturer och hyperparametrar påverkar glömska i ett nätverk. Med denna studie syftar vi till att förstå faktorer som orsakar glömska under sekventiell träning. Vårt primära resultat är att CNN glömmer mindre än LSTM. Vi visar att max-pooling är den underliggande operationen som hjälper CNN att lindra glömska jämfört med LSTM. Vi fann också att läroplaner lärande, som placerar en svår uppgift mot slutet av uppgiftssekvensen, minskar glömska. Vi analyserade effekten av finjustering av kontextuella inbäddningar på katastrofal glömska och fann att användning av inbäddningar som funktionsextraktor är att föredra framför finjustering i kontinuerlig inlärning setup.</abstract_sv>
      <abstract_uz>Katastroq o'zgartirib qolganda - bir vazifa o'rganish modeli soniyada yaxshi o'xshash bo'ladi, va shunday qilib, birinchi vazifa orqali "katarok" bajarishini o'zgartiradi - yaxshi o'rganish texnologiyani tajriba qilishda juda harakat. Katastroqni o'zgartirish orqali juda ajoyib muvaffaqiyatli bo'lsa, biz har xil muammolar va hyper parametrlar tarmoqda o'zgartirish qanday qo'llanmagamiz. Bu o'qituvni bilan, biz keyingi taʼminlovchi paytda o'xshash sabablarni tushunishni tushunamiz. Bizning asosiy aniqlashimiz, CNNNs LSTS'dan yaroqni saqlab qoladi. Biz ko'rinishimizni ko'rsatganimiz, eng kichkina ko'paytirish asosiy operatsi, CNNS bilan o'xshash o'zgarishga yordam beradi. Biz o'rganishni o'rganishni topdik. Vazifaning davomida qiyin ishni o'tib qo'yishni o'zgartiradi. Biz paytda o'xshash o'zgartirib chiqqan paydo bo'lgan narsalarning natijasini aniqlab ko'rib chiqqamiz va bu narsalarning tashkilotlarini foydalanishi davom etishni o'rganishga o'xshash kerak.</abstract_uz>
      <abstract_vi>Thảm họa quên mất rằng- theo đó một mô hình được huấn luyện trên một nhiệm vụ được chỉnh chính xác trong một giây, và làm vậy, chịu một sự giảm kinh nghiệm'thảm họa'trên nhiệm vụ đầu tiên- là một trở ngại trong việc phát triển kỹ thuật học chuyển nhượng tốt hơn. Mặc dù tiến bộ ấn tượng trong việc giảm đi sự quên lãng thảm họa, chúng tôi có giới hạn hiểu kiến trúc và siêu tham số khác nhau tác động đến quên lãng trong mạng lưới. Với nghiên cứu này, chúng tôi hướng tới sự thấu hiểu những yếu tố gây quên lãng trong suốt quá trình đào tạo. Điểm phát hiện chính của chúng tôi là CNN quên nhiều so với LSD. Chúng tôi cho thấy Max-ping là hoạt động cơ bản giúp CNN giảm bớt sự quên lãng so với LSD. Chúng tôi cũng nhận ra rằng học hành, đặt một nhiệm vụ khó khăn vào giai đoạn kết thúc của công việc, làm giảm sự quên lãng. Chúng tôi đã phân tích kết quả của sự nhúng tay vào sự quên lãng thảm họa và phát hiện ra rằng sử dụng sự nhúng tay vào như là bộ dẫn chức năng là thích hợp hơn là chỉnh sửa lại thiết lập học liên tục.</abstract_vi>
      <abstract_bg>Катастрофното забравяне - при което модел, обучен за една задача, е фино настроен на втора и по този начин претърпява "катастрофален" спад в изпълнението на първата задача - е пречка за разработването на по-добри техники за трансферно обучение. Въпреки впечатляващия напредък в намаляването на катастрофалното забравяне, ние имаме ограничено разбиране за това как различните архитектури и хиперпараметри влияят на забравянето в мрежа. С това изследване се стремим да разберем факторите, които причиняват забравяне по време на последователно обучение. Основното ни откритие е, че CNN забравят по-малко от LSTMs. Показваме, че максималното обединяване е основната операция, която помага на CNN да облекчат забравянето в сравнение с LSTMs. Също така открихме, че учебното обучение, поставянето на трудна задача към края на поредицата от задачи, намалява забравянето. Анализирахме ефекта от фината настройка на контекстуалните вграждания върху катастрофалното забравяне и установихме, че използването на вграждания като екстрактор на функции е за предпочитане пред фината настройка при непрекъснато обучение.</abstract_bg>
      <abstract_da>Katastrofisk glemmelse - hvor en model, der er uddannet til en opgave, finjusteres på en anden og dermed lider et "katastrofalt" fald i ydeevnen i forhold til den første opgave - er en hindring for udviklingen af bedre overførselsindlæringsteknikker. På trods af imponerende fremskridt med at reducere katastrofal glemme, har vi begrænset forståelse af, hvordan forskellige arkitekturer og hyperparametre påvirker glemme i et netværk. Med denne undersøgelse forsøger vi at forstå faktorer, der forårsager glemmelse under sekventiel træning. Vores primære opdagelse er, at CNN glemmer mindre end LSTMs. Vi viser, at max-pooling er den underliggende operation, som hjælper CNN'er med at lindre glemme sammenlignet med LSTM'er. Vi fandt også ud af, at læring af læseplaner, der placerer en hård opgave mod slutningen af opgavesekvensen, reducerer glemmelse. Vi analyserede effekten af finjustering af kontekstuelle indlejringer på katastrofal glemning og fandt ud af, at brug af indlejringer som feature extractor er at foretrække frem for finjustering i kontinuerlig læring setup.</abstract_da>
      <abstract_id>Catastrophic forgetting - whereby a model trained on one task is fine-tuned on a second, and in doing so, suffers a 'catastrophic' drop in performance over the first task - is a hurdle in the development of better transfer learning techniques.  Meskipun kemajuan yang mengesankan dalam mengurangi melupakan bencana, kita memiliki pemahaman terbatas bagaimana arsitektur dan parameter-hyper berbeda mempengaruhi melupakan dalam jaringan. Dengan penelitian ini, kita bermaksud untuk memahami faktor yang menyebabkan melupakan selama latihan sekuensial. Penemuan utama kami adalah bahwa CNN lupa kurang dari LSTM. Kami menunjukkan bahwa max-pooling adalah operasi dasar yang membantu CNN mengurangi melupakan dibandingkan LSTM. Kami juga menemukan bahwa pelajaran kurikulum, menempatkan tugas sulit menuju akhir urutan tugas, mengurangi melupakan. Kami menganalisis efek penyesuaian isi kontekstual pada melupakan bencana dan menemukan bahwa menggunakan isi sebagai ekstraktor fitur lebih baik daripada penyesuaian dalam pengaturan belajar terus menerus.</abstract_id>
      <abstract_hr>Katastrofski zaboravljanje - s kojim se model obučen na jednom zadatku dobro ispravlja na sekundu, a u tome pati "katastrofski" pad ekspluatacije na prvom zadatku - je prepreka u razvoju boljih tehnika učenja transfera. Uprkos impresivnom napretku u smanjenju katastrofskog zaboravljanja, imamo ograničeno razumijevanje kako različite arhitekture i hiper-parametre utječu na zaboravljanje u mreži. S ovim proučavanjem ciljamo razumjeti faktore koji uzrokuju zaboravljanje tijekom sekvencijskog treninga. Naš primarni nalaz je da CNN zaboravlja manje od LSTMs. Pokazujemo da je maksimalno skupljanje temeljna operacija koja pomaže CNN-ima ublažiti zaboravljanje u usporedbi s LSTMsom. Također smo otkrili da učenje nastavnih programa, stavljanje težak zadatak ka kraju poslovne sekvence, smanjuje zaboravljanje. Analizirali smo učinak određenog kontekstnog ugrađenja na katastrofsko zaboravljanje i saznali da je koristiti ugrađenje kao ekstraktor karakteristike bolje da se ispravi u nastavku učenja.</abstract_hr>
      <abstract_nl>Katastrofisch vergeten, waarbij een model dat op één taak is getraind, op een tweede wordt verfijnd, en daarbij een 'catastrofale' daling in prestaties ondervindt ten opzichte van de eerste taak, is een obstakel in de ontwikkeling van betere transferleertechnieken. Ondanks indrukwekkende vooruitgang in het verminderen van catastrofale vergeten, hebben we weinig inzicht in hoe verschillende architecturen en hyperparameters het vergeten in een netwerk beïnvloeden. Met deze studie willen we factoren begrijpen die het vergeten veroorzaken tijdens sequentiële training. Onze belangrijkste bevinding is dat CNN's minder vergeten dan LSTMs. We tonen aan dat max-pooling de onderliggende operatie is die CNN's helpt vergeten te verminderen in vergelijking met LSTMs. We ontdekten ook dat curriculum leren, het plaatsen van een moeilijke taak tegen het einde van de taakvolg, het vergeten vermindert. We analyseerden het effect van fine-tuning contextuele embeddings op catastrofale vergeten en ontdekten dat het gebruik van embeddings als feature extractor de voorkeur verdient boven fine-tuning in continue learning setup.</abstract_nl>
      <abstract_ko>재난적 망각은 더 나은 이동 학습 기술을 발전시키는 장애물이다. 재난적 망각은 한 임무에서 훈련하는 모델이 두 번째 임무에서 미세하게 조정되는 것을 말한다. 이렇게 할 때 첫 번째 임무의 표현은'재난적'이 떨어진다.비록 재난적인 망각을 줄이는 데 인상적인 진전을 거두었지만, 우리는 서로 다른 체계 구조와 초파라미터가 네트워크에서의 망각에 어떻게 영향을 미치는지에 대해 아직 아는 것이 매우 적다.이 연구를 통해 우리는 순서 훈련에서 잊혀지는 요인을 이해하고자 한다.우리의 주요 발견은 CNN이 LSTM보다 잊어버린 것이 더 적다는 것이다.LSTM에 비해 가장 큰 풀은 CNN이 망각을 완화하는 데 도움을 주는 기본적인 작업이라는 것을 알 수 있다.우리는 또 과정 학습이 어려운 임무를 임무 서열의 끝에 두면 잊어버리는 것을 줄일 수 있다는 것을 발견했다.우리는 미세한 언어 환경의 삽입이 재난성 망각에 미치는 영향을 분석한 결과 지속적인 학습 환경에서 삽입을 특징으로 하는 추출이 미세한 언어 환경보다 더욱 바람직하다는 것을 발견했다.</abstract_ko>
      <abstract_de>Katastrophisches Vergessen, bei dem ein Modell, das auf einer Aufgabe trainiert wird, auf einer Sekunde feinjustiert wird und dabei einen "katastrophalen" Leistungsabfall gegenüber der ersten Aufgabe erleidet, ist eine Hürde bei der Entwicklung besserer Transferlerntechniken. Trotz beeindruckender Fortschritte bei der Verringerung des katastrophalen Vergessens haben wir nur ein begrenztes Verständnis dafür, wie verschiedene Architekturen und Hyperparameter das Vergessen in einem Netzwerk beeinflussen. Mit dieser Studie wollen wir Faktoren verstehen, die das Vergessen während des sequentiellen Trainings verursachen. Unsere wichtigste Erkenntnis ist, dass CNNs weniger vergessen als LSTMs. Wir zeigen, dass Max-Pooling die zugrunde liegende Operation ist, die CNNs hilft, Vergessen im Vergleich zu LSTMs zu lindern. Wir haben auch festgestellt, dass das Lernen im Curriculum, das eine schwierige Aufgabe gegen Ende der Aufgabenfolge platziert, das Vergessen reduziert. Wir analysierten den Effekt der Feinabstimmung kontextueller Einbettungen auf katastrophales Vergessen und fanden heraus, dass die Verwendung von Einbettungen als Feature Extractor der Feinabstimmung im kontinuierlichen Lernaufbau vorzuziehen ist.</abstract_de>
      <abstract_af>Katastrofiske vergeet - waarmee 'n model wat op een taak opgelei is, is fyn-tuned op 'n sekonde, en in dit te doen, lyf 'n 'katastrofiske' afval in prestasie oor die eerste taak - is 'n hurdle in die ontwikkeling van beter oordrag onderwerp teknike. Ons het beperk verstanding van hoe verskillende arkitektuure en hiper-parameters effekteer die vergeet in 'n netwerk. Met hierdie studie is ons doel om faktore te verstaan wat vergeet het tydens sekwensiele onderwerp. Ons primêre vinding is dat CNN vergeet minder as LSTMs. Ons wys dat max- pooling is die onderstelling operasie wat help CNN alleviate vergeet vergelyking met LSTMs. Ons het ook gevind dat die onderwerp van die program leer, die plaasïng van 'n moeilike taak na die einde van die taak sekvensie, verduur vergeet. Ons het die effek van fine-tuning contextual inbêdings gekansleer op katastrofiske vergeet en gevind dat gebruik van inbêdings as funksie uittrekker is voorkeur na fin-tuning in voortdurende leerinstelling.</abstract_af>
      <abstract_fa>فراموش کردن ناراحتی - که یک مدل آموزش یافته روی یک کار در یک ثانیه کامل ساخته می شود، و در این صورت، در نتیجه، افزایش ناراحتی در عملکرد اولین وظیفه رنج می گیرد - در توسعه تکنیک یادگیری بهتر انتقال است. با وجود پیشرفت تحت تاثیر انگیز در کاهش فراموش های فاجعه، ما درک محدودیت داریم که چگونه معماری های مختلف و پارامترهای زیادی بر فراموش کردن در شبکه تاثیر می دهند. با این مطالعه، ما هدف داریم که فاکتورها را درک کنیم که باعث می شود در زمان آموزش های سطح فراموش کنند. اولین پیدا کردن ما این است که CNN ها کمتر از LSTMs فراموش می کنند. ما نشان می دهیم که مکس جمع کردن عملیات پایه‌ای است که به CNN کمک می‌کند که فراموش کردن در مقایسه با LSTMs را کمک کند. همچنین فهمیدیم که یادگیری برنامه آموزش، یک کار سخت به پایان برنامه کار، فراموش کردن را کاهش می‌دهد. ما تاثیر تغییر تغییر موقعیتی را بر فراموش ناگوار تحلیل کردیم و فهمیدیم که استفاده از استفاده از استفاده از استفاده از استفاده از استفاده از استفاده از استفاده از استفاده از ویژه‌های ویژه ترجیح می‌دهد تا تغییر‌سازی در سازمان</abstract_fa>
      <abstract_sq>Harrimi katastrofik - me të cilin një model i trajnuar në një detyrë është i përshtatur në një sekondë, dhe duke e bërë këtë, vuan një rënie 'katastrofike' në performancë gjatë detyrës së parë - është një pengesë në zhvillimin e teknikave më të mira të mësimit transferues. Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different architectures and hyper-parameters affect forgetting in a network.  Me këtë studim, ne synojmë të kuptojmë faktorë që shkaktojnë harrimin gjatë stërvitjes sekuencore. Our primary finding is that CNNs forget less than LSTMs.  We show that max-pooling is the underlying operation which helps CNNs alleviate forgetting compared to LSTMs.  Gjetëm gjithashtu se mësimi i kurikullit, duke vënë një detyrë të vështirë drejt fundit të sekuencës së detyrave, redukton harrimin. Analizuam efektin e rregullimit të përfshirjeve kontekstuale në harrimin katastrofik dhe zbuluam se përdorimi i përfshirjeve si nxjerrës i funksioneve është më i preferuar se rregullimi në konfigurimin e mësimit të vazhdueshëm.</abstract_sq>
      <abstract_sw>Matangazo ya kusahau – ambapo modeli inayofundishwa katika kazi moja imetunzwa vizuri kwa sekunde, na kwa kufanya hivyo, inatumia kupungua kwa 'janga' katika kazi ya kwanza - ni mzunguko katika maendeleo ya teknolojia bora ya kujifunza. Pamoja na maendeleo mazuri katika kupunguza kusahau maafa, tuna uelewa mdogo wa jinsi majengo tofauti na parameter za juu yanavyoathiri kusahau kwenye mtandao wa intaneti. Kwa utafiti huu, tunalenga kuelewa sababu ambazo zinasababisha kusahau wakati wa mafunzo ya mfululizo. Ugunduzi wetu wa msingi ni kuwa CNN wanasahau chini ya LST. Tunaonyesha kwamba kupunguza kwa kiwango kikubwa ni operesheni ya msingi inayosaidia watu wa CNN kupunguza kusahau ukilinganishwa na LSTMs. Pia tuligundua kuwa elimu ya elimu, kuweka kazi ngumu kwa ajili ya mwisho wa mfululizo wa kazi, kupunguza kusahau. Tulichambua athari za matukio ya vizuri yanayotumika katika kutambua maafa ya ajabu na tukagundua kuwa kwa kutumia mabomu kama mtengenezaji anafaa kuendelea vizuri katika mfumo wa kujifunza kwa muda mrefu.</abstract_sw>
      <abstract_tr>Katastrofik 첵atdan 챌ykyp bilen - bir i힊e gelin첵채n bir nusga ikinji gezek gowy d체zle힊iril첵채r we 힊onu흫 체챌in birinji i힊i흫 체st체nde 'catastrofik' d체힊체rip bolup ge챌iril첵채r - gowy g철챌체r 철wrenme tekniklerini흫 geli힊megidir. Katastrofi첵a 첵atdan 챌ykarmak 체챌in t채sirli ilerlemek isle첵채n 첵철ne, sy첵ada d체rli arhitektura we hiper-parameterleri흫 n채hili 첵atdan 챌ykarmagyny d체힊체n첵채ndigini 챌ykarypdyk. Bu okuw bilen so흫ky okuw wagtlary 첵atdan 챌ykarmak 체챌in faktorlara d체힊체nmek ama챌landyrys. Bizi흫 ilkinji tapylygymyz, CNN-ler LSTMsden az 첵atdan 챌ykar첵andyr. Biz i흫 k철p pooling (max-pooling) CNN'leri흫 LSTMsleri흫 첵agda첵ynda unutmagy 체챌in k철mekle첵채n i힊leridir. Biz hem programlary 철wrenmek 체챌in tapdyk, i힊lerin so흫unda kyn zady 챌ykaryp, unutmagy azaltdyk. Biz catastrophik 첵atdan 챌ykarmak 체챌in g철zlenen contextual ta첵첵arlary흫 etkisini analyzdyk we tapdyk ki da흫a d체힊en 철wrenme d체z체mlerini ta첵첵arlamak 체챌in ta첵첵arlanmagy gowy g철r첵채r.</abstract_tr>
      <abstract_am>የጭንቀት ግጭት፣ በአንድ ስራ ላይ የተማረ ሞዴል በሁለተኛው ሁለተኛ የተጠቃሚ ነው፣ እንዲሁም በመጀመሪያ ስራ ላይ የጭንቀት 'ጉዳይ' የሚያስጨንቀው ነው - የተሻለ ትምህርት ተማርኮት መፍጠር ነው፡፡ Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different architectures and hyper-parameters affect forgetting in a network.  በዚህ ትምህርት፣ በኋላው ትምህርት ውስጥ የሚረሳውን ውርደትን ለማስተዋል እናስፈልጋለን፡፡ የመጀመሪያው ፍለጋችን የCNN ሰዎች ከLST ይልቅ ትንሹን ይረሳሉ፡፡ እናሳያቸዋለን የክፍለ ጉዳይ የCNNs መሳሳት ከLSTMs ጋር ለመረሳት ማቅረብ የሚችል የውጤት ሥራ ነው፡፡ የትምህርት ትምህርት እና ትክክለኛውን ስራ ወደ ፍጻሜው መጨረሻ የሚያደርገውን አግኝተናል፡፡ የአሁኑን ግንኙነት በመረሳት ላይ የመጠቀምን ውጤት አስተያየን እና የግንኙነት አካባቢዎች በጥቅምት መቀናቀል በዘወትር ትምህርት ግንኙነት በመጠቀም ይሻላል ብለን አግኝተናል፡፡</abstract_am>
      <abstract_bn>বিপর্যয় ভুলে যাচ্ছে - যেখানে একটি কাজে একটি মডেল প্রশিক্ষণ প্রদান করা হয়েছে সেকেন্ডের মধ্যে ভালোভাবে, আর এর ফলে প্রথম কাজে বিপর্যয়ের ক্ষেত্রে একটি 'বিপর্যয়' প্রদর্শনের ব বিপর্যয় ভুলে যাওয়ার ক্ষেত্রে চমৎকার অগ্রগতি সত্ত্বেও আমরা সীমিত বুঝতে পারি কিভাবে বিভিন্ন প্রতিষ্ঠান এবং হাইপার-প্যারামিটার এই গবেষণা দিয়ে আমরা বুঝতে চাই যে কারণগুলো বুঝতে পারে যার ফলে পরবর্তী প্রশিক্ষণের সময় ভুলে যায়। আমাদের প্রাথমিক খুঁজে পাওয়া যাচ্ছে যে সিএনএন এলএসএমএস এর চেয়ে কম ভুলে যাচ্ছে। আমরা দেখাচ্ছি যে ম্যাক্স-পুলিং হচ্ছে অন্তর্ভুক্ত কার্যক্রম যা সিএনএন-এর সাহায্য করে এলএসএমএস-এর তুলনায় ভুলে যাওয়ার আমরা একই সাথে পেয়েছি যে কার্কুল শিক্ষা, কাজের শেষের দিকে কঠিন কাজ রাখা, ভুলে যাচ্ছে। বিপর্যয় ভুলে যাওয়ার প্রভাব আমরা বিশ্লেষণ করেছিলাম এবং দেখেছিলাম যে বৈশিষ্ট্যের ব্যবহার ব্যবহার করে বিশেষ ক্ষেত্রে শিক্ষা ব্যবস্থা ব্যবস্থার সু</abstract_bn>
      <abstract_hy>Կատաստերոֆիկ մոռանալը, որի միջոցով մեկ խնդրի վրա վարժեցված մոդելը մեկ վայրկյանում լավ կազմակերպված է, և դա անելով, առաջին խնդրի ընթացքում առաջացած արտադրողականությունը "կոտրաֆիկ" նվազում է, խոչընդոտը ավելի լավ փոխանցման ուսուցման տեխնիկաների Չնայած զարմանահրաշ առաջընթացին քանդակային մոռացման մեջ, մենք սահմանափակ հասկացություն ունենք այն մասին, թե ինչպես տարբեր ճարտարապետությունները և հիպեր-պարամետրերը ազդում են ցանցի մոռացման վրա: With this study, we aim to understand factors which cause forgetting during sequential training.  Our primary finding is that CNNs forget less than LSTMs.  Մենք ցույց ենք տալիս, որ մաքսային հավաքածությունը հիմնական գործողությունն է, որը օգնում է CNN-ներին նվազեցնել մոռացումը համեմատած LSMT-ների հետ: Մենք նաև հայտնաբերեցինք, որ ուսումնական ծրագրերի ուսումնասիրությունը, դժվար առաջադրանք դնելով առաջադրանքի վերջում, նվազեցնում է մոռացումը: We analysed the effect of fine-tuning contextual embeddings on catastrophic forgetting and found that using embeddings as feature extractor is preferable to fine-tuning in continual learning setup.</abstract_hy>
      <abstract_ca>L'oblidança catastròfica - en la qual un model entrenat en una tasca es ajusta en un segon, i en fer-ho, pateix una 'catastròfica' baixa en el rendiment durant la primera tasca - és un obstacle al desenvolupament de millors tècniques d'aprenentatge de transfer ència. Malgrat el progrés impressionant en la reducció de l'oblide catàstrofic, tenim una comprensió limitada de com diferents arquitectures i hiperparamètres afecten l'oblide en una xarxa. Amb aquest estudi, volem entendre els factors que provoquen l'oblide durant l'entrenament seqüencial. La nostra descoberta primària és que els CNN obliden menys que els LSTM. Ens demostram que la max-pooling és l'operació subjacente que ajuda als CNN a alleviar l'oblide comparat amb els LSTM. També vam descobrir que l'aprenentatge del currículum, posant una tasca difícil cap al final de la seqüència de tasques, redueix l'oblid. Vam analitzar l'efecte d'ajustar les incorporacions contextuals en l'oblidança catastròfica i vam descobrir que utilitzar les incorporacions com a extractor de característiques és preferible que ajustar en la configuració continua d'aprenentatge.</abstract_ca>
      <abstract_cs>Katastrofické zapomenutí, kdy model trénovaný na jednom úkolu je na druhém doladěn a přitom trpí "katastrofickým" poklesem výkonu nad prvním úkolem, je překážkou ve vývoji lepších technik transferového učení. Navzdory působivému pokroku v redukci katastrofického zapomenutí máme omezené pochopení toho, jak různé architektury a hyperparametry ovlivňují zapomenutí v síti. Cílem této studie je porozumět faktorům, které způsobují zapomenutí během sekvenčního tréninku. Naším primárním zjištěním je, že CNN zapomínají méně než LSTMs. Ukazujeme, že max-pooling je základní operací, která pomáhá CNN zmírnit zapomenutí ve srovnání s LSTMs. Zjistili jsme také, že učení osnov, které umístí těžký úkol na konec sekvence úkolů, snižuje zapomenutí. Analyzovali jsme vliv jemného ladění kontextových vložení na katastrofické zapomenutí a zjistili jsme, že použití vložení jako extraktor funkcí je vhodnější než jemné ladění v nastavení kontinuálního učení.</abstract_cs>
      <abstract_et>Katastroofiline unustamine - kus ühe ülesande täitmiseks koolitatud mudel muutub teise ülesandega peeneks ja seeläbi kannatab esimese ülesandega võrreldes "katastroofilise" tulemuslikkuse languse - on takistus paremate siirdeõppetehnikate arendamisel. Hoolimata muljetavaldavatest edusammudest katastroofilise unustamise vähendamisel on meil piiratud arusaam sellest, kuidas erinevad arhitektuurid ja hüperparameetrid mõjutavad unustamist võrgus. Selle uuringuga püüame mõista tegureid, mis põhjustavad unustamist järjestikusel treeningul. Meie peamine leid on, et CNN unustab vähem kui LSTMd. Näitame, et maksimaalne koondamine on alusoperatsioon, mis aitab CNN-del leevendada unustust võrreldes LSTMdega. Samuti leidsime, et õppekava õppimine, raske ülesande asetamine ülesannete jada lõppu, vähendab unustamist. Analüüsisime kontekstipõhiste manustamiste mõju katastroofilisele unustamisele ja leidsime, et manustamiste kasutamine funktsioonide ekstraktorina on eelistatav kui peenhäälestamine pidevas õppeseadistuses.</abstract_et>
      <abstract_fi>Katastrofinen unohtaminen - jossa yhteen tehtävään koulutettua mallia hienosäädetään toiseen tehtävään ja näin ollen se kärsii "katastrofaalisesta" suorituskyvyn laskusta ensimmäiseen tehtävään verrattuna - on este parempien siirtooppimistekniikoiden kehittämiselle. Huolimatta vaikuttavasta edistyksestä katastrofaalisen unohduksen vähentämisessä meillä on rajallinen käsitys siitä, miten erilaiset arkkitehtuurit ja hyperparametrit vaikuttavat unohtamiseen verkossa. Tämän tutkimuksen avulla pyrimme ymmärtämään tekijöitä, jotka aiheuttavat unohtamista peräkkäisessä harjoittelussa. Pääasiallinen havaintomme on, että CNN:t unohtavat vähemmän kuin LSTMs:t. Osoitamme, että max-pooling on taustalla oleva operaatio, joka auttaa CNN:iä lievittämään unohdusta verrattuna LSTMiin. Huomasimme myös, että opetussuunnitelman oppiminen, vaikean tehtävän asettaminen tehtäväjärjestyksen loppuun, vähentää unohtamista. Analysoimme kontekstuaalisten upotusten hienosäätöjen vaikutusta katastrofaaliseen unohtamiseen ja totesimme, että upotusten käyttäminen ominaisuusuuttajana on parempi kuin hienosäätö jatkuvassa oppimisessa.</abstract_fi>
      <abstract_bs>Katastrofski zaboravljanje - s kojim se model obučen na jednom zadatku dobro određuje na sekundu, a u tom slučaju pati "katastrofski" pad ekspluatacije na prvom zadatku - je prepreka u razvoju boljih tehnika učenja transfera. Uprkos impresivnom napretku u smanjenju katastrofskog zaboravljanja, imamo ograničeno razumijevanje kako različite arhitekture i hiper-parametre utječu na zaboravljanje u mreži. Sa ovom studijom, ciljamo da razumijemo faktore koji uzrokuju zaboravljanje tokom sekvencijskog treninga. Naš primarni nalaz je da CNN zaboravlja manje od LSTMs. Pokazujemo da je maksimalno okupljanje temeljna operacija koja pomaže CNN-ima da ublaži zaboravljanje u usporedbi s LSTMs-om. Također smo otkrili da učenje nastavnog programa, stavljanje težak zadatak ka kraju poslovne sekvence, smanjuje zaboravljanje. Analizirali smo učinak određenog kontekstnog ugrađenja na katastrofsko zaboravljanje i saznali da je koristiti ugrađenje kao ekstraktor karakteristike bolje da se ispravi u nastavku učenja.</abstract_bs>
      <abstract_az>Katastrofik unutmaq - bir işdə təhsil edilmiş modellər bir saniyədə təhsil edilir. Bunu etmək üçün ilk işin üstündə 'katastrofik' düşüşünü çəkir - daha yaxşı təhsil öyrənmə tekniklərinin təhsil edilməsi. Katastrofik unutmaq üçün təsirli ilerleme baxmayaraq, fərqli arhitektür və hiper-parametrlərin a ğda unutmaq üçün nə təsirlərini təsir edir? Bu təcrübə ilə, sonrakı təcrübə sırasında unutduğu faktorları anlamaq istəyirik. Bizim ilk tapımız, CNN-lər LSTMs-dən daha az unutduğu şeydir. Biz max-pooling, LSTMs ilə qarşılaşdığı unutmağa kömək edir. Biz də öyrəndik ki, öyrənmək, işlərin sonuna çətin bir işi yerləşdirər, unutmağı azaldırır. Biz katastrofi unutduğu təsirlərin müxtəlif təsirlərin etkisini analiz etdik və həmişəlik öyrənmə qurğularının istifadə etməsinin müəyyən edilməsi daha yaxşıdır.</abstract_az>
      <abstract_he>שכחות קטסטרופית — שבה מודל מאומן במשימה אחת מתאים בשנייה, ובעשייתו זאת, סובל נפילה "קטסטרופית" ביצועים במהלך המשימה הראשונה — היא מחסומת בפיתוח של טכניקות לימוד העברה טובות יותר. למרות התקדמות מרשים בהפחית שכחות קטסטרופית, יש לנו הבנה מוגבלת על איך ארכיטקטורות שונות והיפר-פרמטרים משפיעים על שכחות ברשת. עם המחקר הזה, אנחנו מתכוונים להבין גורמים שגורמים לשכוח במהלך אימון רצופי. המצאה העיקרית שלנו היא ש CNN שוכחים פחות מLSTMs. אנו מראים שמקס-בריצה היא המבצע הנוסף שמעזר לסי.אן.איי להקל את השכחות בהשוואה ל-LSTMs. מצאנו גם שלמדת תוכנית לימודים, שמשמרת משימה קשה אל סוף רצף המשימה, מפחידה לשכוח. We analysed the effect of fine-tuning contextual embeddings on catastrophic forgetting and found that using embeddings as feature extractor is preferable to fine-tuning in continual learning setup.</abstract_he>
      <abstract_ha>Babu mai manta - inda an sanar da wani misalin wanda aka yi wa aikin wani aikin da shi mai kyau ne a sakan guda, kuma idan yana aikata shi, zai sakar da cewa wani "kataroki" da ɓacewa a kan aikin na farkon - shi ne wani abu mai rauni a ƙarami da zanen shige mafiya alhẽri. Babu da jarrabi mai kyau ga ƙaranci ga mantawa na mai tsanani, sai mun sami fahimta yana da jinsi musamman ko da surar-parameteri za'a yi amfani da mantawa a cikin jerin. Ga wannan littafin, Munã nufin mu fahimta masu fakta da ke halatar da mantawa a lokacin da ake yi wa danganta. Babu ganinmu na farko ne cewa CNNS ke manta da mafi ƙaranci daga LSM. Tuna nũna cewa max poopoopool shi ne aikin da ke ƙara da ke taimakon CNN ya sauƙaƙe mantawa da aka sammenliki da LSAM. Kayya, na sami karatun karatun, kuma ya sami aikin mai tsanani zuwa ƙarshen aikin, kuma ya manta. Ba mu yi anayyar amfani da matsayin da ke samu'a da sauri a cikin masaukaci na manta, kuma muka gane cewa ya fi zama mafiya cancantar a gyara a cikin tsarin da za'a karanta ta daidai.</abstract_ha>
      <abstract_sk>Katastrofsko pozabljanje - pri katerem se model, usposobljen za eno nalogo, natančno nastavi v drugi nalogi in s tem utrpi "katastrofalno" zmanjšanje uspešnosti v primerjavi s prvo nalogo - je ovira pri razvoju boljših tehnik prenosa učenja. Kljub impresivnemu napredku pri zmanjševanju katastrofalnega pozabljanja imamo omejeno razumevanje, kako različne arhitekture in hiperparametri vplivajo na pozabljanje v omrežju. S to študijo želimo razumeti dejavnike, ki povzročajo pozabljanje med zaporednim treningom. Naša glavna ugotovitev je, da CNN pozabi manj kot LSTMs. Pokazali smo, da je največja združitev osnovna operacija, ki pomaga CNN ublažiti pozabljanje v primerjavi z LSTMi. Ugotovili smo tudi, da učenje učnega načrta, ki postavlja težko nalogo proti koncu zaporedja nalog, zmanjšuje pozabljanje. Analizirali smo učinek natančnega nastavitve kontekstualnih vdelav na katastrofalno pozabljanje in ugotovili, da je uporaba vdelav kot ekstraktor funkcij bolj primerna kot natančna nastavitev pri stalnem učenju.</abstract_sk>
      <abstract_jv>Catasaro kuwi alih sing paling-paling maneh Maski Genjer-genjer saiki iki, awak dhéwé ngerasakno karo nggawe barang sliramu kuwi mau. Ndheke perusahaan punika dipunangé punika dipunangé KENs kuwi mau maning manung LA Awak dhéwé ngerasakno karo maximum-pool kuwi nggawe operasi dipun-perusahaan sing nyebutaké KNs nggawe barang nggawe barang nggawe tarjamahan karo LA Kita uga ono wektu nggoleki curric program seneng pisan, sithik sedhaya bantuan ngajar ujaran Awak dhéwé énjelisih efek karo embedding contextual</abstract_jv>
      <abstract_bo>མི་མང་གི་སྐོར་ལས་བརྗེད་དགོས་བྱུང་། དེ་ལྟ་བུའི་རྣམ་པ་ཞིག་གིས་ལས་ཀྱང་བརྟན་དཔྱད་སྒྲུབ་གཅིག་གི་ནང་དུ་བཏུབ་པ་ཞིག་ཡིན། Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different architectures and hyper-parameters affect forgetting in a network. འདི་ལྟ་བུའི་ནང་དུ་ང་ཚོས་རྐྱེན་ངག་ཅན་གྱིས་རྐྱེན་གྱིས་འཛིན་བྱེད་པའི་ཆ་རྐྱེན་དུ་རྟོགས་འདོད་ཡོད། ང་ཚོའི་རྩ་བའི་མཐོང་སྣང་ནི་CNNs ཡིས་LSTMs ལས་ཉུང་བའི་བརྗེད་ཟིན་ཡིན་པ་རེད། འུ་ཅག་གིས་ཆེ་ཤོས་མཉམ་དུ་བསྡུས་ནི་རྨང་གཞིའི་བཀོལ འོན་ཀྱང་། Curriculum learning་དེ་ལ་གནད་དོན་ལྡན་གྱི་ལས་འགུལ་རྗེས་སུ་འཇོག་བྱས་པ་ཡིན་ན། We analyzed the effect of fine-tuning contextual embeddings on catastrophic forgetting and found that using embeddings as feature extractor is preferable to fine-tuning in continual learning setup.</abstract_bo>
      </paper>
    <paper id="14">
      <title>Detecting Chemical Reactions in Patents</title>
      <author><first>Hiyori</first><last>Yoshikawa</last></author>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <author><first>Zenan</first><last>Zhai</last></author>
      <author><first>Christian</first><last>Druckenbrodt</last></author>
      <author><first>Camilo</first><last>Thorne</last></author>
      <author><first>Saber A.</first><last>Akhondi</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>100–110</pages>
      <abstract>Extracting <a href="https://en.wikipedia.org/wiki/Chemical_reaction">chemical reactions</a> from <a href="https://en.wikipedia.org/wiki/Patent">patents</a> is a crucial task for chemists working on <a href="https://en.wikipedia.org/wiki/Chemical_engineering">chemical exploration</a>. In this paper we introduce the novel task of detecting the textual spans that describe or refer to <a href="https://en.wikipedia.org/wiki/Chemical_reaction">chemical reactions</a> within patents. We formulate this task as a paragraph-level sequence tagging problem, where the system is required to return a sequence of paragraphs which contain a description of a reaction. To address this new task, we construct an annotated dataset from an existing proprietary database of chemical reactions manually extracted from <a href="https://en.wikipedia.org/wiki/Patent">patents</a>. We introduce several baseline methods for the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> and evaluate them over our <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a>. Through error analysis, we discuss what makes the task complex and challenging, and suggest possible directions for future research.</abstract>
      <url hash="748e8b9c">U19-1014</url>
      <bibkey>yoshikawa-etal-2019-detecting</bibkey>
    </paper>
    <paper id="15">
      <title>Identifying Patients with Pain in Emergency Departments using Conventional <a href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning</a> and Deep Learning</title>
      <author><first>Thanh</first><last>Vu</last></author>
      <author><first>Anthony</first><last>Nguyen</last></author>
      <author><first>Nathan</first><last>Brown</last></author>
      <author><first>James</first><last>Hughes</last></author>
      <pages>111–119</pages>
      <abstract>Pain is the main symptom that patients present with to the emergency department (ED). Pain management, however, is often poorly done aspect of <a href="https://en.wikipedia.org/wiki/Emergency_medicine">emergency care</a> and patients with painful conditions can endure long waits before their pain is assessed or treated. To improve pain management quality, identifying whether or not an ED patient presents with pain is an important task and allows for further investigation of the quality of care provided. In this paper, <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> was utilised to handle the task of automatically detecting patients who present at EDs with pain from retrospective data. Experimental results on a manually annotated dataset show that our proposed <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning models</a> achieve high performances, in which the highest <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> and macro-averaged F1 are 91.00 % and 90.96 %, respectively.</abstract>
      <url hash="ad494e6e">U19-1015</url>
      <bibkey>vu-etal-2019-identifying</bibkey>
    </paper>
    <paper id="24">
      <title>An Improved Coarse-to-Fine Method for Solving Generation Tasks</title>
      <author><first>Wenyv</first><last>Guan</last></author>
      <author><first>Qianying</first><last>Liu</last></author>
      <author><first>Guangzhi</first><last>Han</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <pages>178–185</pages>
      <abstract>The coarse-to-fine (coarse2fine) methods have recently been widely used in the generation tasks. The methods first generate a rough sketch in the coarse stage and then use the sketch to get the final result in the fine stage. However, <a href="https://en.wikipedia.org/wiki/They_(2017_film)">they</a> usually lack the correction ability when getting a wrong sketch. To solve this problem, in this paper, we propose an improved coarse2fine model with a control mechanism, with which our method can control the influence of the sketch on the final results in the fine stage. Even if the sketch is wrong, our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> still has the opportunity to get a correct result. We have experimented our <a href="https://en.wikipedia.org/wiki/Conceptual_model">model</a> on the tasks of <a href="https://en.wikipedia.org/wiki/Semantic_parsing">semantic parsing</a> and math word problem solving. The results have shown the effectiveness of our proposed <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>.</abstract>
      <url hash="c185b3e4">U19-1024</url>
      <bibkey>guan-etal-2019-improved</bibkey>
    </paper>
    </volume>
</collection>