<collection id="2021.humeval">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)</booktitle>
      <editor><first>Anya</first><last>Belz</last></editor>
      <editor><first>Shubham</first><last>Agarwal</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Ehud</first><last>Reiter</last></editor>
      <editor><first>Anastasia</first><last>Shimorina</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>April</month>
      <year>2021</year>
      <url hash="49abb657">2021.humeval-1</url>
    </meta>
    <frontmatter>
      <url hash="2207b5d3">2021.humeval-1.0</url>
      <bibkey>humeval-2021-human</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Trading Off Diversity and Quality in Natural Language Generation</title>
      <author><first>Hugh</first><last>Zhang</last></author>
      <author><first>Daniel</first><last>Duckworth</last></author>
      <author><first>Daphne</first><last>Ippolito</last></author>
      <author><first>Arvind</first><last>Neelakantan</last></author>
      <pages>25&#8211;33</pages>
      <abstract>For open-ended language generation tasks such as storytelling or dialogue, choosing the right decoding algorithm is vital for controlling the tradeoff between generation <i>quality</i> and <i>diversity</i>. However, there presently exists no consensus on which decoding procedure is best or even the criteria by which to compare them. In this paper, we cast decoding as a tradeoff between response quality and diversity, and we perform the first large-scale evaluation of decoding methods along the entire quality-diversity spectrum. Our experiments confirm the existence of the likelihood trap: the counter-intuitive observation that high likelihood sequences are often surprisingly low quality. We also find that when diversity is a priority, all methods perform similarly, but when quality is viewed as more important, nucleus sampling (Holtzman et al., 2019) outperforms all other evaluated decoding algorithms.</abstract>
      <url hash="318e99f6">2021.humeval-1.3</url>
      <video href="https://www.youtube.com/watch?v=P0SWVm30MFM" />
      <bibkey>zhang-etal-2021-trading</bibkey>
    </paper>
    <paper id="5">
      <title>Is This Translation Error Critical?: Classification-Based Human and Automatic Machine Translation Evaluation Focusing on Critical Errors</title>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Kosuke</first><last>Takahashi</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>46&#8211;55</pages>
      <abstract>This paper discusses a classification-based approach to machine translation evaluation, as opposed to a common regression-based approach in the WMT Metrics task. Recent machine translation usually works well but sometimes makes critical errors due to just a few wrong word choices. Our classification-based approach focuses on such errors using several error type labels, for practical machine translation evaluation in an age of neural machine translation. We made additional annotations on the WMT 2015-2017 Metrics datasets with fluency and adequacy labels to distinguish different types of translation errors from syntactic and semantic viewpoints. We present our human evaluation criteria for the corpus development and automatic evaluation experiments using the corpus. The human evaluation corpus will be publicly available upon publication.</abstract>
      <url hash="6d5b87b3">2021.humeval-1.5</url>
      <attachment type="Dataset" hash="3b09bc8d">2021.humeval-1.5.Dataset.zip</attachment>
      <video href="https://www.youtube.com/watch?v=myG72lA2hpo" />
      <bibkey>sudoh-etal-2021-translation</bibkey>
    </paper>
    <paper id="6">
      <title>Towards Objectively Evaluating the Quality of Generated Medical Summaries</title>
      <author><first>Francesco</first><last>Moramarco</last></author>
      <author><first>Damir</first><last>Juric</last></author>
      <author><first>Aleksandar</first><last>Savkov</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <pages>56&#8211;61</pages>
      <abstract>We propose a method for evaluating the quality of generated text by asking evaluators to count facts, and computing precision, recall, f-score, and accuracy from the raw counts. We believe this approach leads to a more objective and easier to reproduce evaluation. We apply this to the task of medical report summarisation, where measuring objective quality and accuracy is of paramount importance.</abstract>
      <url hash="2fa2ed24">2021.humeval-1.6</url>
      <bibkey>moramarco-etal-2021-towards</bibkey>
    </paper>
    <paper id="10">
      <title>Reliability of Human Evaluation for Text Summarization: Lessons Learned and Challenges Ahead</title>
      <author><first>Neslihan</first><last>Iskender</last></author>
      <author><first>Tim</first><last>Polzehl</last></author>
      <author><first>Sebastian</first><last>M&#246;ller</last></author>
      <pages>86&#8211;96</pages>
      <abstract>Only a small portion of research papers with human evaluation for text summarization provide information about the participant demographics, task design, and experiment protocol. Additionally, many researchers use human evaluation as gold standard without questioning the reliability or investigating the factors that might affect the reliability of the human evaluation. As a result, there is a lack of best practices for reliable human summarization evaluation grounded by empirical evidence. To investigate human evaluation reliability, we conduct a series of human evaluation experiments, provide an overview of participant demographics, task design, experimental set-up and compare the results from different experiments. Based on our empirical analysis, we provide guidelines to ensure the reliability of expert and non-expert evaluations, and we determine the factors that might affect the reliability of the human evaluation.</abstract>
      <url hash="b9f00b08">2021.humeval-1.10</url>
      <bibkey>iskender-etal-2021-reliability</bibkey>
      <pwccode url="https://github.com/nesliskender/reliability_humeval_summarization" additional="false">nesliskender/reliability_humeval_summarization</pwccode>
    </paper>
    <paper id="15">
      <title>Interrater Disagreement Resolution: A Systematic Procedure to Reach Consensus in Annotation Tasks</title>
      <author><first>Yvette</first><last>Oortwijn</last></author>
      <author><first>Thijs</first><last>Ossenkoppele</last></author>
      <author><first>Arianna</first><last>Betti</last></author>
      <pages>131&#8211;141</pages>
      <abstract>We present a systematic procedure for interrater disagreement resolution. The procedure is general, but of particular use in multiple-annotator tasks geared towards ground truth construction. We motivate our proposal by arguing that, barring cases in which the researchers&#8217; goal is to elicit different viewpoints, interrater disagreement is a sign of poor quality in the design or the description of a task. Consensus among annotators, we maintain, should be striven for, through a systematic procedure for disagreement resolution such as the one we describe.</abstract>
      <url hash="858f30b5">2021.humeval-1.15</url>
      <video href="https://www.youtube.com/watch?v=z-O6zZJDxOY" />
      <bibkey>oortwijn-etal-2021-interrater</bibkey>
      <pwccode url="https://github.com/yoortwijn/humevaldisres" additional="false">yoortwijn/humevaldisres</pwccode>
    </paper>
  </volume>
</collection>