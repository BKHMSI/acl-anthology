<collection id="2020.eamt">
  <volume id="1" ingest-date="2020-08-11">
    <meta>
      <booktitle>Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</booktitle>
      <editor><first>Andr&#233;</first><last>Martins</last></editor>
      <editor><first>Helena</first><last>Moniz</last></editor>
      <editor><first>Sara</first><last>Fumega</last></editor>
      <editor><first>Bruno</first><last>Martins</last></editor>
      <editor><first>Fernando</first><last>Batista</last></editor>
      <editor><first>Luisa</first><last>Coheur</last></editor>
      <editor><first>Carla</first><last>Parra</last></editor>
      <editor><first>Isabel</first><last>Trancoso</last></editor>
      <editor><first>Marco</first><last>Turchi</last></editor>
      <editor><first>Arianna</first><last>Bisazza</last></editor>
      <editor><first>Joss</first><last>Moorkens</last></editor>
      <editor><first>Ana</first><last>Guerberof</last></editor>
      <editor><first>Mary</first><last>Nurminen</last></editor>
      <editor><first>Lena</first><last>Marg</last></editor>
      <editor><first>Mikel L.</first><last>Forcada</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Lisboa, Portugal</address>
      <month>November</month>
      <year>2020</year>
      <url hash="7d46b928">2020.eamt-1</url>
    </meta>
    <frontmatter>
      <url hash="ab0a7709">2020.eamt-1.0</url>
      <bibkey>eamt-2020-european</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Efficiently Reusing Old Models Across Languages via Transfer Learning</title>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Ond&#345;ej</first><last>Bojar</last></author>
      <pages>19&#8211;28</pages>
      <abstract>Recent progress in neural machine translation (NMT) is directed towards larger neural networks trained on an increasing amount of hardware resources. As a result, NMT models are costly to train, both financially, due to the electricity and hardware cost, and environmentally, due to the carbon footprint. It is especially true in transfer learning for its additional cost of training the &#8220;parent&#8221; model before transferring knowledge and training the desired &#8220;child&#8221; model. In this paper, we propose a simple method of re-using an already trained model for different language pairs where there is no need for modifications in model architecture. Our approach does not need a separate parent model for each investigated language pair, as it is typical in NMT transfer learning. To show the applicability of our method, we recycle a Transformer model trained by different researchers and use it to seed models for different language pairs. We achieve better translation quality and shorter convergence times than when training from random initialization.</abstract>
      <url hash="dc6b5b55">2020.eamt-1.3</url>
      <bibkey>kocmi-bojar-2020-efficiently</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2018">WMT 2018</pwcdataset>
    </paper>
    <paper id="4">
      <title>Efficient Transfer Learning for Quality Estimation with Bottleneck Adapter Layer</title>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Ning</first><last>Xie</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <author><first>Yao</first><last>Deng</last></author>
      <pages>29&#8211;34</pages>
      <abstract>The Predictor-Estimator framework for quality estimation (QE) is commonly used for its strong performance. Where the predictor and estimator works on feature extraction and quality evaluation, respectively. However, training the predictor from scratch is computationally expensive. In this paper, we propose an efficient transfer learning framework to transfer knowledge from NMT dataset into QE models. A Predictor-Estimator alike model named BAL-QE is also proposed, aiming to extract high quality features with pre-trained NMT model, and make classification with a fine-tuned Bottleneck Adapter Layer (BAL). The experiment shows that BAL-QE achieves 97% of the SOTA performance in WMT19 En-De and En-Ru QE tasks by only training 3% of parameters within 4 hours on 4 Titan XP GPUs. Compared with the commonly used NuQE baseline, BAL-QE achieves 47% (En-Ru) and 75% (En-De) of performance promotions.</abstract>
      <url hash="44ec6031">2020.eamt-1.4</url>
      <bibkey>yang-etal-2020-efficient</bibkey>
    </paper>
    <paper id="6">
      <title>Incorporating External Annotation to improve Named Entity Translation in <fixed-case>NMT</fixed-case></title>
      <author><first>Maciej</first><last>Modrzejewski</last></author>
      <author><first>Miriam</first><last>Exel</last></author>
      <author><first>Bianka</first><last>Buschbeck</last></author>
      <author><first>Thanh-Le</first><last>Ha</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>45&#8211;51</pages>
      <abstract>The correct translation of named entities (NEs) still poses a challenge for conventional neural machine translation (NMT) systems. This study explores methods incorporating named entity recognition (NER) into NMT with the aim to improve named entity translation. It proposes an annotation method that integrates named entities and inside&#8211;outside&#8211;beginning (IOB) tagging into the neural network input with the use of source factors. Our experiments on English&#8594;German and English&#8594; Chinese show that just by including different NE classes and IOB tagging, we can increase the BLEU score by around 1 point using the standard test set from WMT2019 and achieve up to 12% increase in NE translation rates over a strong baseline.</abstract>
      <url hash="a639a645">2020.eamt-1.6</url>
      <bibkey>modrzejewski-etal-2020-incorporating</bibkey>
    </paper>
    <paper id="8">
      <title>A multi-source approach for <fixed-case>B</fixed-case>reton&#8211;<fixed-case>F</fixed-case>rench hybrid machine translation</title>
      <author><first>V&#237;ctor M.</first><last>S&#225;nchez-Cartagena</last></author>
      <author><first>Mikel L.</first><last>Forcada</last></author>
      <author><first>Felipe</first><last>S&#225;nchez-Mart&#237;nez</last></author>
      <pages>61&#8211;70</pages>
      <abstract>Corpus-based approaches to machine translation (MT) have difficulties when the amount of parallel corpora to use for training is scarce, especially if the languages involved in the translation are highly inflected. This problem can be addressed from different perspectives, including data augmentation, transfer learning, and the use of additional resources, such as those used in rule-based MT. This paper focuses on the hybridisation of rule-based MT and neural MT for the Breton&#8211;French under-resourced language pair in an attempt to study to what extent the rule-based MT resources help improve the translation quality of the neural MT system for this particular under-resourced language pair. We combine both translation approaches in a multi-source neural MT architecture and find out that, even though the rule-based system has a low performance according to automatic evaluation metrics, using it leads to improved translation quality.</abstract>
      <url hash="428bc35d">2020.eamt-1.8</url>
      <bibkey>sanchez-cartagena-etal-2020-multi</bibkey>
    </paper>
    <paper id="9">
      <title>Leveraging Multilingual Resources for Language Invariant Sentiment Analysis</title>
      <author><first>Allen</first><last>Antony</last></author>
      <author><first>Arghya</first><last>Bhattacharya</last></author>
      <author><first>Jaipal</first><last>Goud</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>71&#8211;79</pages>
      <abstract>Sentiment analysis is a widely researched NLP problem with state-of-the-art solutions capable of attaining human-like accuracies for various languages. However, these methods rely heavily on large amounts of labeled data or sentiment weighted language-specific lexical resources that are unavailable for low-resource languages. Our work attempts to tackle this data scarcity issue by introducing a neural architecture for language invariant sentiment analysis capable of leveraging various monolingual datasets for training without any kind of cross-lingual supervision. The proposed architecture attempts to learn language agnostic sentiment features via adversarial training on multiple resource-rich languages which can then be leveraged for inferring sentiment information at a sentence level on a low resource language. Our model outperforms the current state-of-the-art methods on the Multilingual Amazon Review Text Classification dataset [REF] and achieves significant performance gains over prior work on the low resource Sentiraama corpus [REF]. A detailed analysis of our research highlights the ability of our architecture to perform significantly well in the presence of minimal amounts of training data for low resource languages.</abstract>
      <url hash="fb4c0580">2020.eamt-1.9</url>
      <bibkey>antony-etal-2020-leveraging</bibkey>
    </paper>
    <paper id="12">
      <title>Double Attention-based Multimodal Neural Machine Translation with Semantic Image Regions</title>
      <author><first>Yuting</first><last>Zhao</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <pages>105&#8211;114</pages>
      <abstract>Existing studies on multimodal neural machine translation (MNMT) have mainly focused on the effect of combining visual and textual modalities to improve translations. However, it has been suggested that the visual modality is only marginally beneficial. Conventional visual attention mechanisms have been used to select the visual features from equally-sized grids generated by convolutional neural networks (CNNs), and may have had modest effects on aligning the visual concepts associated with textual objects, because the grid visual features do not capture semantic information. In contrast, we propose the application of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English-German and English-French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on translation performance benefited from semantic image regions.</abstract>
      <url hash="fb521973">2020.eamt-1.12</url>
      <bibkey>zhao-etal-2020-double</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="14">
      <title>Fine-grained Human Evaluation of Transformer and Recurrent Approaches to Neural Machine Translation for <fixed-case>E</fixed-case>nglish-to-<fixed-case>C</fixed-case>hinese</title>
      <author><first>Yuying</first><last>Ye</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <pages>125&#8211;134</pages>
      <abstract>This research presents a fine-grained human evaluation to compare the Transformer and recurrent approaches to neural machine translation (MT), on the translation direction English-to-Chinese. To this end, we develop an error taxonomy compliant with the Multidimensional Quality Metrics (MQM) framework that is customised to the relevant phenomena of this translation direction. We then conduct an error annotation using this customised error taxonomy on the output of state-of-the-art recurrent- and Transformer-based MT systems on a subset of WMT2019&#8217;s news test set. The resulting annotation shows that, compared to the best recurrent system, the best Transformer system results in a 31% reduction of the total number of errors and it produced significantly less errors in 10 out of 22 error categories. We also note that two of the systems evaluated do not produce any error for a category that was relevant for this translation direction prior to the advent of NMT systems: Chinese classifiers.</abstract>
      <url hash="42195dd4">2020.eamt-1.14</url>
      <bibkey>ye-toral-2020-fine</bibkey>
      <pwccode url="https://github.com/yy-ye/mqm-analysis" additional="false">yy-ye/mqm-analysis</pwccode>
    </paper>
    <paper id="15">
      <title>Correct Me If You Can: Learning from Error Corrections and Markings</title>
      <author><first>Julia</first><last>Kreutzer</last></author>
      <author><first>Nathaniel</first><last>Berger</last></author>
      <author><first>Stefan</first><last>Riezler</last></author>
      <pages>135&#8211;144</pages>
      <abstract>Sequence-to-sequence learning involves a trade-off between signal strength and annotation cost of training data. For example, machine translation data range from costly expert-generated translations that enable supervised learning, to weak quality-judgment feedback that facilitate reinforcement learning. We present the first user study on annotation cost and machine learnability for the less popular annotation mode of error markings. We show that error markings for translations of TED talks from English to German allow precise credit assignment while requiring significantly less human effort than correcting/post-editing, and that error-marked data can be used successfully to fine-tune neural machine translation models.</abstract>
      <url hash="e7af16b9">2020.eamt-1.15</url>
      <bibkey>kreutzer-etal-2020-correct</bibkey>
      <pwccode url="https://github.com/StatNLP/mt-correct-mark-interface" additional="false">StatNLP/mt-correct-mark-interface</pwccode>
    </paper>
    <paper id="17">
      <title>Fine-Grained Error Analysis on <fixed-case>E</fixed-case>nglish-to-<fixed-case>J</fixed-case>apanese Machine Translation in the Medical Domain</title>
      <author><first>Takeshi</first><last>Hayakawa</last></author>
      <author><first>Yuki</first><last>Arase</last></author>
      <pages>155&#8211;164</pages>
      <abstract>We performed a detailed error analysis in domain-specific neural machine translation (NMT) for the English and Japanese language pair with fine-grained manual annotation. Despite its importance for advancing NMT technologies, research on the performance of domain-specific NMT and non-European languages has been limited. In this study, we designed an error typology based on the error types that were typically generated by NMT systems and might cause significant impact in technical translations: &#8220;Addition,&#8221; &#8220;Omission,&#8221; &#8220;Mistranslation,&#8221; &#8220;Grammar,&#8221; and &#8220;Terminology.&#8221; The error annotation was targeted to the medical domain and was performed by experienced professional translators specialized in medicine under careful quality control. The annotation detected 4,912 errors on 2,480 sentences, and the frequency and distribution of errors were analyzed. We found that the major errors in NMT were &#8220;Mistranslation&#8221; and &#8220;Terminology&#8221; rather than &#8220;Addition&#8221; and &#8220;Omission,&#8221; which have been reported as typical problems of NMT. Interestingly, more errors occurred in documents for professionals compared with those for the general public. The results of our annotation work will be published as a parallel corpus with error labels, which are expected to contribute to developing better NMT models, automatic evaluation metrics, and quality estimation models.</abstract>
      <url hash="9555cccb">2020.eamt-1.17</url>
      <bibkey>hayakawa-arase-2020-fine</bibkey>
    </paper>
    <paper id="21">
      <title>Modelling Source- and Target- Language Syntactic Information as Conditional Context in Interactive Neural Machine Translation</title>
      <author><first>Kamal Kumar</first><last>Gupta</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>195&#8211;204</pages>
      <abstract>In interactive machine translation (MT), human translators correct errors in automatic translations in collaboration with the MT systems, which is seen as an effective way to improve the productivity gain in translation. In this study, we model source-language syntactic constituency parse and target-language syntactic descriptions in the form of supertags as conditional context for interactive prediction in neural MT (NMT). We found that the supertags significantly improve productivity gain in translation in interactive-predictive NMT (INMT), while syntactic parsing somewhat found to be effective in reducing human effort in translation. Furthermore, when we model this source- and target-language syntactic information together as the conditional context, both types complement each other and our fully syntax-informed INMT model statistically significantly reduces human efforts in a French&#8211;to&#8211;English translation task, achieving 4.30 points absolute (corresponding to 9.18% relative) improvement in terms of word prediction accuracy (WPA) and 4.84 points absolute (corresponding to 9.01% relative) reduction in terms of word stroke ratio (WSR) over the baseline.</abstract>
      <url hash="4554ca33">2020.eamt-1.21</url>
      <bibkey>gupta-etal-2020-modelling</bibkey>
    </paper>
    <paper id="28">
      <title>Evaluating the usefulness of neural machine translation for the <fixed-case>P</fixed-case>olish translators in the <fixed-case>E</fixed-case>uropean Commission</title>
      <author><first>Karolina</first><last>Stefaniak</last></author>
      <pages>263&#8211;269</pages>
      <abstract>The mission of the Directorate General for Translation (DGT) is to provide high-quality translation to help the European Commission communicate with EU citizens. To this end DGT employs almost 2000 translators from all EU official languages. But while the demand for translation has been continuously growing, following a global trend, the number of translators has decreased. To cope with the demand, DGT extensively uses a CAT environment encompassing translation memories, terminology databases and recently also machine translation. This paper examines the benefits and risks of using neural machine translation to augment the productivity of in&#8210;house DGT translators for the English&#8210;Polish language pair. Based on the analysis of a sample of NMT&#8210;translated texts and on the observations of the working practices of Polish translators it is concluded that the possible productivity gain is still modest, while the risks to quality are quite substantial.</abstract>
      <url hash="79c3e33d">2020.eamt-1.28</url>
      <bibkey>stefaniak-2020-evaluating</bibkey>
    </paper>
    <paper id="29">
      <title>Terminology-Constrained Neural Machine Translation at <fixed-case>SAP</fixed-case></title>
      <author><first>Miriam</first><last>Exel</last></author>
      <author><first>Bianka</first><last>Buschbeck</last></author>
      <author><first>Lauritz</first><last>Brandt</last></author>
      <author><first>Simona</first><last>Doneva</last></author>
      <pages>271&#8211;280</pages>
      <abstract>This paper examines approaches to bias a neural machine translation model to adhere to terminology constraints in an industrial setup. In particular, we investigate variations of the approach by Dinu et al. (2019), which uses inline annotation of the target terms in the source segment plus source factor embeddings during training and inference, and compare them to constrained decoding. We describe the challenges with respect to terminology in our usage scenario at SAP and show how far the investigated methods can help to overcome them. We extend the original study to a new language pair and provide an in-depth evaluation including an error classification and a human evaluation.</abstract>
      <url hash="199ff6ff">2020.eamt-1.29</url>
      <bibkey>exel-etal-2020-terminology</bibkey>
    </paper>
    <paper id="31">
      <title>Bifixer and Bicleaner: two open-source tools to clean your parallel data</title>
      <author><first>Gema</first><last>Ram&#237;rez-S&#225;nchez</last></author>
      <author><first>Jaume</first><last>Zaragoza-Bernabeu</last></author>
      <author><first>Marta</first><last>Ba&#241;&#243;n</last></author>
      <author><first>Sergio Ortiz</first><last>Rojas</last></author>
      <pages>291&#8211;298</pages>
      <abstract>This paper shows the utility of two open-source tools designed for parallel data cleaning: Bifixer and Bicleaner. Already used to clean highly noisy parallel content from crawled multilingual websites, we evaluate their performance in a different scenario: cleaning publicly available corpora commonly used to train machine translation systems. We choose four English&#8211;Portuguese corpora which we plan to use internally to compute paraphrases at a later stage. We clean the four corpora using both tools, which are described in detail, and analyse the effect of some of the cleaning steps on them. We then compare machine translation training times and quality before and after cleaning these corpora, showing a positive impact particularly for the noisiest ones.</abstract>
      <url hash="c45fb6c6">2020.eamt-1.31</url>
      <bibkey>ramirez-sanchez-etal-2020-bifixer</bibkey>
      <pwccode url="https://github.com/bitextor/bicleaner" additional="false">bitextor/bicleaner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="32">
      <title>An <fixed-case>E</fixed-case>nglish-<fixed-case>S</fixed-case>wahili parallel corpus and its use for neural machine translation in the news domain</title>
      <author><first>Felipe</first><last>S&#225;nchez-Mart&#237;nez</last></author>
      <author><first>V&#237;ctor M.</first><last>S&#225;nchez-Cartagena</last></author>
      <author><first>Juan Antonio</first><last>P&#233;rez-Ortiz</last></author>
      <author><first>Mikel L.</first><last>Forcada</last></author>
      <author><first>Miquel</first><last>Espl&#224;-Gomis</last></author>
      <author><first>Andrew</first><last>Secker</last></author>
      <author><first>Susie</first><last>Coleman</last></author>
      <author><first>Julie</first><last>Wall</last></author>
      <pages>299&#8211;308</pages>
      <abstract>This paper describes our approach to create a neural machine translation system to translate between English and Swahili (both directions) in the news domain, as well as the process we followed to crawl the necessary parallel corpora from the Internet. We report the results of a pilot human evaluation performed by the news media organisations participating in the H2020 EU-funded project GoURMET.</abstract>
      <url hash="a78dd32d">2020.eamt-1.32</url>
      <bibkey>sanchez-martinez-etal-2020-english</bibkey>
    </paper>
    <paper id="34">
      <title>A User Study of the Incremental Learning in <fixed-case>NMT</fixed-case></title>
      <author><first>Miguel</first><last>Domingo</last></author>
      <author><first>Mercedes</first><last>Garc&#237;a-Mart&#237;nez</last></author>
      <author><first>&#193;lvaro</first><last>Peris</last></author>
      <author><first>Alexandre</first><last>Helle</last></author>
      <author><first>Amando</first><last>Estela</last></author>
      <author><first>Laurent</first><last>Bi&#233;</last></author>
      <author><first>Francisco</first><last>Casacuberta</last></author>
      <author><first>Manuel</first><last>Herranz</last></author>
      <pages>319&#8211;328</pages>
      <abstract>In the translation industry, human experts usually supervise and post-edit machine translation hypotheses. Adaptive neural machine translation systems, able to incrementally update the underlying models under an online learning regime, have been proven to be useful to improve the efficiency of this workflow. However, this incremental adaptation is somewhat unstable, and it may lead to undesirable side effects. One of them is the sporadic appearance of made-up words, as a byproduct of an erroneous application of subword segmentation techniques. In this work, we extend previous studies on on-the-fly adaptation of neural machine translation systems. We perform a user study involving professional, experienced post-editors, delving deeper on the aforementioned problems. Results show that adaptive systems were able to learn how to generate the correct translation for task-specific terms, resulting in an improvement of the user&#8217;s productivity. We also observed a close similitude, in terms of morphology, between made-up words and the words that were expected.</abstract>
      <url hash="bb6d3db2">2020.eamt-1.34</url>
      <bibkey>domingo-etal-2020-user</bibkey>
    </paper>
    <paper id="42">
      <title>How do <fixed-case>LSP</fixed-case>s compute <fixed-case>MT</fixed-case> discounts? Presenting a company&#8217;s pipeline and its use</title>
      <author><first>Randy</first><last>Scansani</last></author>
      <author><first>Lamis</first><last>Mhedhbi</last></author>
      <pages>393&#8211;401</pages>
      <abstract>In this paper we present a pipeline developed at Acolad to test a Machine Translation (MT) engine and compute the discount to be applied when its output is used in production. Our pipeline includes three main steps where quality and productivity are measured through automatic metrics, manual evaluation, and by keeping track of editing and temporal effort during a post-editing task. Thanks to this approach, it is possible to evaluate the output quality and compute an engine-specific discount. Our test pipeline tackles the complexity of transforming productivity measurements into discounts by comparing the outcome of each of the above-mentioned steps to an estimate of the average productivity of translation from scratch. The discount is obtained by subtracting the resulting coefficient from the per-word rate. After a description of the pipeline, the paper presents its application on four engines, discussing its results and showing that our method to estimate post-editing effort through manual evaluation seems to capture the actual productivity. The pipeline relies heavily on the work of professional post-editors, with the aim of creating a mutually beneficial cooperation between users and developers.</abstract>
      <url hash="bfc11647">2020.eamt-1.42</url>
      <bibkey>scansani-mhedhbi-2020-lsps</bibkey>
    </paper>
    <paper id="45">
      <title>Comparing Post-editing based on Four Editing Actions against Translating with an Auto-Complete Feature</title>
      <author><first>F&#233;lix Do</first><last>Carmo</last></author>
      <pages>421&#8211;430</pages>
      <abstract>This article describes the results of a workshop in which 50 translators tested two experimental translation interfaces, as part of a project which aimed at studying the details of editing work. In this work, editing is defined as a selection of four actions: deleting, inserting, moving and replacing words. Four texts, machine-translated from English into European Portuguese, were post-edited in four different sessions in which each translator swapped between texts and two work modes. One of the work modes involved a typical auto-complete feature, and the other was based on the four actions. The participants answered surveys before, during and after the workshop. A descriptive analysis of the answers to the surveys and of the logs recorded during the experiments was performed. The four editing actions mode is shown to be more intrusive, but to allow for more planned decisions: although they take more time in this mode, translators hesitate less and make fewer edits. The article shows the usefulness of the approach for research on the editing task.</abstract>
      <url hash="1b512029">2020.eamt-1.45</url>
      <bibkey>carmo-2020-comparing</bibkey>
    </paper>
    <paper id="49">
      <title>Document-Level Machine Translation Evaluation Project: Methodology, Effort and Inter-Annotator Agreement</title>
      <author><first>Sheila</first><last>Castilho</last></author>
      <pages>455&#8211;456</pages>
      <abstract>Document-level (doc-level) human eval-uation of machine translation (MT) has raised interest in the community after a fewattempts have disproved claims of &#8220;human parity&#8221; (Toral et al., 2018; Laubli et al.,2018). However, little is known about bestpractices regarding doc-level human evalu-ation. The goal of this project is to identifywhich methodologies better cope with i)the current state-of-the-art (SOTA) humanmetrics, ii) a possible complexity when as-signing a single score to a text consisted of&#8216;good&#8217; and &#8216;bad&#8217; sentences, iii) a possibletiredness bias in doc-level set-ups, and iv)the difference in inter-annotator agreement(IAA) between sentence and doc-level set-ups.</abstract>
      <url hash="8bd5f97e">2020.eamt-1.49</url>
      <bibkey>castilho-2020-document</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>CEF</fixed-case> Data Marketplace: Powering a Long-term Supply of Language Data</title>
      <author><first>Amir</first><last>Kamran</last></author>
      <author><first>Dace</first><last>Dzeguze</last></author>
      <author><first>Jaap</first><last>van der Meer</last></author>
      <author><first>Milica</first><last>Panic</last></author>
      <author><first>Alessandro</first><last>Cattelan</last></author>
      <author><first>Daniele</first><last>Patrioli</last></author>
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>459&#8211;460</pages>
      <abstract>We describe the CEF Data Marketplace project, which focuses on the development of a trading platform of translation data for language professionals: translators, machine translation (MT) developers, language service providers (LSPs), translation buyers and government bodies. The CEF Data Marketplace platform will be designed and built to manage and trade data for all languages and domains. This project will open a continuous and longterm supply of language data for MT and other machine learning applications.</abstract>
      <url hash="5d7cdf1a">2020.eamt-1.51</url>
      <bibkey>kamran-etal-2020-cef</bibkey>
    </paper>
    <paper id="59">
      <title><fixed-case>MICE</fixed-case>: a middleware layer for <fixed-case>MT</fixed-case></title>
      <author><first>Joachim</first><last>Van den Bogaert</last></author>
      <author><first>Tom</first><last>Vanallemeersch</last></author>
      <author><first>Heidi</first><last>Depraetere</last></author>
      <pages>475&#8211;476</pages>
      <abstract>The MICE project (2018-2020) will deliver a middleware layer for improving the output quality of the eTranslation system of EC&#8217;s Connecting Europe Facility through additional services, such as domain adaptation and named entity recognition. It will also deliver a user portal, allowing for human post-editing.</abstract>
      <url hash="e078908c">2020.eamt-1.59</url>
      <bibkey>van-den-bogaert-etal-2020-mice</bibkey>
    </paper>
    <paper id="60">
      <title>Neural Translation for the <fixed-case>E</fixed-case>uropean <fixed-case>U</fixed-case>nion (<fixed-case>NTEU</fixed-case>) Project</title>
      <author><first>Laurent</first><last>Bi&#233;</last></author>
      <author><first>Aleix</first><last>Cerd&#224;-i-Cuc&#243;</last></author>
      <author><first>Hans</first><last>Degroote</last></author>
      <author><first>Amando</first><last>Estela</last></author>
      <author><first>Mercedes</first><last>Garc&#237;a-Mart&#237;nez</last></author>
      <author><first>Manuel</first><last>Herranz</last></author>
      <author><first>Alejandro</first><last>Kohan</last></author>
      <author><first>Maite</first><last>Melero</last></author>
      <author><first>Tony</first><last>O&#8217;Dowd</last></author>
      <author><first>Sin&#233;ad</first><last>O&#8217;Gorman</last></author>
      <author><first>M&#257;rcis</first><last>Pinnis</last></author>
      <author><first>Roberts</first><last>Rozis</last></author>
      <author><first>Riccardo</first><last>Superbo</last></author>
      <author><first>Art&#363;rs</first><last>Vasi&#316;evskis</last></author>
      <pages>477&#8211;478</pages>
      <abstract>The Neural Translation for the European Union (NTEU) project aims to build a neural engine farm with all European official language combinations for eTranslation, without the necessity to use a high-resourced language as a pivot. NTEU started in September 2019 and will run until August 2021.</abstract>
      <url hash="cb823ed0">2020.eamt-1.60</url>
      <bibkey>bie-etal-2020-neural</bibkey>
    </paper>
    <paper id="62">
      <title><fixed-case>OCR</fixed-case>, Classification

&amp; Machine Translation (<fixed-case>OCCAM</fixed-case>)</title>
      <author><first>Joachim</first><last>Van den Bogaert</last></author>
      <author><first>Arne</first><last>Defauw</last></author>
      <author><first>Frederic</first><last>Everaert</last></author>
      <author><first>Koen</first><last>Van Winckel</last></author>
      <author><first>Alina</first><last>Kramchaninova</last></author>
      <author><first>Anna</first><last>Bardadym</last></author>
      <author><first>Tom</first><last>Vanallemeersch</last></author>
      <author><first>Pavel</first><last>Smr&#382;</last></author>
      <author><first>Michal</first><last>Hradi&#353;</last></author>
      <pages>481&#8211;482</pages>
      <abstract>The OCCAM project (Optical Character recognition, ClassificAtion &amp; Machine Translation) aims at integrating the CEF (Connecting Europe Facility) Automated Translation service with image classification, Translation Memories (TMs), Optical Character Recognition (OCR), and Machine Translation (MT). It will support the automated translation of scanned business documents (a document format that, currently, cannot be processed by the CEF eTranslation service) and will also lead to a tool useful for the Digital Humanities domain.</abstract>
      <url hash="f3a2980e">2020.eamt-1.62</url>
      <bibkey>van-den-bogaert-etal-2020-ocr</bibkey>
    </paper>
    <paper id="64">
      <title>Assessing the Comprehensibility of Automatic Translations (<fixed-case>A</fixed-case>ris<fixed-case>T</fixed-case>o<fixed-case>CAT</fixed-case>)</title>
      <author><first>Lieve</first><last>Macken</last></author>
      <author><first>Margot</first><last>Fonteyne</last></author>
      <author><first>Arda</first><last>Tezcan</last></author>
      <author><first>Joke</first><last>Daems</last></author>
      <pages>485&#8211;486</pages>
      <abstract>The ArisToCAT project aims to assess the comprehensibility of &#8216;raw&#8217; (unedited) MT output for readers who can only rely on the MT output. In this project description, we summarize the main results of the project and present future work.</abstract>
      <url hash="f231fd46">2020.eamt-1.64</url>
      <bibkey>macken-etal-2020-assessing</bibkey>
    </paper>
    <paper id="69">
      <title><fixed-case>MT</fixed-case>rill project: Machine Translation impact on language learning</title>
      <author><first>Nat&#225;lia</first><last>Resende</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>497&#8211;498</pages>
      <abstract>Over the last decades, massive research investments have been made in the development of machine translation (MT) systems (Gupta and Dhawan, 2019). This has brought about a paradigm shift in the performance of these language tools, leading to widespread use of popular MT systems (Gaspari and Hutchins, 2007). Although the first MT engines were used for gisting purposes, in recent years, there has been an increasing interest in using MT tools, especially the freely available online MT tools, for language teaching and learning (Clifford et al., 2013). The literature on MT and Computer Assisted Language Learning (CALL) shows that, over the years, MT systems have been facilitating language teaching and also language learning (Nin &#771;o, 2006). It has been shown that MT tools can increase awareness of grammatical linguistic features of a foreign language. Research also shows the positive role of MT systems in the development of writing skills in English as well as in improving communication skills in English(Garcia and Pena, 2011). However, to date, the cognitive impact of MT on language acquisition and on the syntactic aspects of language processing has not yet been investigated and deserves further scrutiny. The MTril project aims at filling this gap in the literature by examining whether MT is contributing to a central aspect of language acquisition: the so-called language binding, i.e., the ability to combine single words properly in a grammatical sentence (Heyselaar et al., 2017; Ferreira and Bock, 2006). The project focus on the initial stages (pre-intermediate and intermediate) of the acquisition of English syntax by Brazilian Portuguese native speakers using MT systems as a support for language learning.</abstract>
      <url hash="3cc666fd">2020.eamt-1.69</url>
      <bibkey>resende-way-2020-mtrill</bibkey>
    </paper>
  </volume>
</collection>