<collection id="2020.louhi">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis</booktitle>
      <editor><first>Eben</first><last>Holderness</last></editor>
      <editor><first>Antonio</first><last>Jimeno Yepes</last></editor>
      <editor><first>Alberto</first><last>Lavelli</last></editor>
      <editor><first>Anne-Lyse</first><last>Minard</last></editor>
      <editor><first>James</first><last>Pustejovsky</last></editor>
      <editor><first>Fabio</first><last>Rinaldi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="323b7822">2020.louhi-1.0</url>
      <bibkey>louhi-2020-international</bibkey>
    </frontmatter>
    <paper id="2">
      <title>Simple Hierarchical Multi-Task Neural End-To-End Entity Linking for Biomedical Text</title>
      <author><first>Maciej</first><last>Wiatrak</last></author>
      <author><first>Juha</first><last>Iso-Sipila</last></author>
      <pages>12&#8211;17</pages>
      <abstract>Recognising and linking entities is a crucial first step to many tasks in biomedical text analysis, such as relation extraction and target identification. Traditionally, biomedical entity linking methods rely heavily on heuristic rules and predefined, often domain-specific features. The features try to capture the properties of entities and complex multi-step architectures to detect, and subsequently link entity mentions. We propose a significant simplification to the biomedical entity linking setup that does not rely on any heuristic methods. The system performs all the steps of the entity linking task jointly in either single or two stages. We explore the use of hierarchical multi-task learning, using mention recognition and entity typing tasks as auxiliary tasks. We show that hierarchical multi-task models consistently outperform single-task models when trained tasks are homogeneous. We evaluate the performance of our models on the biomedical entity linking benchmarks using MedMentions and BC5CDR datasets. We achieve state-of-theart results on the challenging MedMentions dataset, and comparable results on BC5CDR.</abstract>
      <url hash="99622a41">2020.louhi-1.2</url>
      <doi>10.18653/v1/2020.louhi-1.2</doi>
      <video href="https://slideslive.com/38940048" />
      <bibkey>wiatrak-iso-sipila-2020-simple</bibkey>
    </paper>
    <paper id="7">
      <title>Evaluation of Machine Translation Methods applied to Medical Terminologies</title>
      <author><first>Konstantinos</first><last>Skianis</last></author>
      <author><first>Yann</first><last>Briand</last></author>
      <author><first>Florent</first><last>Desgrippes</last></author>
      <pages>59&#8211;69</pages>
      <abstract>Medical terminologies resources and standards play vital roles in clinical data exchanges, enabling significantly the services&#8217; interoperability within healthcare national information networks. Health and medical science are constantly evolving causing requirements to advance the terminologies editions. In this paper, we present our evaluation work of the latest machine translation techniques addressing medical terminologies. Experiments have been conducted leveraging selected statistical and neural machine translation methods. The devised procedure is tested on a validated sample of ICD-11 and ICF terminologies from English to French with promising results.</abstract>
      <url hash="4241ab53">2020.louhi-1.7</url>
      <doi>10.18653/v1/2020.louhi-1.7</doi>
      <video href="https://slideslive.com/38940042" />
      <bibkey>skianis-etal-2020-evaluation</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
    </paper>
    <paper id="12">
      <title>Defining and Learning Refined Temporal Relations in the Clinical Narrative</title>
      <author><first>Kristin</first><last>Wright-Bettner</last></author>
      <author><first>Chen</first><last>Lin</last></author>
      <author><first>Timothy</first><last>Miller</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <author><first>Dmitriy</first><last>Dligach</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>James H.</first><last>Martin</last></author>
      <author><first>Guergana</first><last>Savova</last></author>
      <pages>104&#8211;114</pages>
      <abstract>We present refinements over existing temporal relation annotations in the Electronic Medical Record clinical narrative. We refined the THYME corpus annotations to more faithfully represent nuanced temporality and nuanced temporal-coreferential relations. The main contributions are in re-defining CONTAINS and OVERLAP relations into CONTAINS, CONTAINS-SUBEVENT, OVERLAP and NOTED-ON. We demonstrate that these refinements lead to substantial gains in learnability for state-of-the-art transformer models as compared to previously reported results on the original THYME corpus. We thus establish a baseline for the automatic extraction of these refined temporal relations. Although our study is done on clinical narrative, we believe it addresses far-reaching challenges that are corpus- and domain- agnostic.</abstract>
      <url hash="4f1bb59f">2020.louhi-1.12</url>
      <doi>10.18653/v1/2020.louhi-1.12</doi>
      <video href="https://slideslive.com/38940047" />
      <bibkey>wright-bettner-etal-2020-defining</bibkey>
    </paper>
    <paper id="13">
      <title>Context-Aware Automatic Text Simplification of Health Materials in Low-Resource Domains</title>
      <author><first>Tarek</first><last>Sakakini</last></author>
      <author><first>Jong Yoon</first><last>Lee</last></author>
      <author><first>Aditya</first><last>Duri</last></author>
      <author><first>Renato F.L.</first><last>Azevedo</last></author>
      <author><first>Victor</first><last>Sadauskas</last></author>
      <author><first>Kuangxiao</first><last>Gu</last></author>
      <author><first>Suma</first><last>Bhat</last></author>
      <author><first>Dan</first><last>Morrow</last></author>
      <author><first>James</first><last>Graumlich</last></author>
      <author><first>Saqib</first><last>Walayat</last></author>
      <author><first>Mark</first><last>Hasegawa-Johnson</last></author>
      <author><first>Thomas</first><last>Huang</last></author>
      <author><first>Ann</first><last>Willemsen-Dunlap</last></author>
      <author><first>Donald</first><last>Halpin</last></author>
      <pages>115&#8211;126</pages>
      <abstract>Healthcare systems have increased patients&#8217; exposure to their own health materials to enhance patients&#8217; health levels, but this has been impeded by patients&#8217; lack of understanding of their health material. We address potential barriers to their comprehension by developing a context-aware text simplification system for health material. Given the scarcity of annotated parallel corpora in healthcare domains, we design our system to be independent of a parallel corpus, complementing the availability of data-driven neural methods when such corpora are available. Our system compensates for the lack of direct supervision using a biomedical lexical database: Unified Medical Language System (UMLS). Compared to a competitive prior approach that uses a tool for identifying biomedical concepts and a consumer-directed vocabulary list, we empirically show the enhanced accuracy of our system due to improved handling of ambiguous terms. We also show the enhanced accuracy of our system over directly-supervised neural methods in this low-resource setting. Finally, we show the direct impact of our system on laypeople&#8217;s comprehension of health material via a human subjects&#8217; study (n=160).</abstract>
      <url hash="50778ff7">2020.louhi-1.13</url>
      <doi>10.18653/v1/2020.louhi-1.13</doi>
      <video href="https://slideslive.com/38940053" />
      <bibkey>sakakini-etal-2020-context</bibkey>
    </paper>
    <paper id="14">
      <title>Identifying Personal Experience Tweets of Medication Effects Using Pre-trained <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a Language Model and Its Updating</title>
      <author><first>Minghao</first><last>Zhu</last></author>
      <author><first>Youzhe</first><last>Song</last></author>
      <author><first>Ge</first><last>Jin</last></author>
      <author><first>Keyuan</first><last>Jiang</last></author>
      <pages>127&#8211;137</pages>
      <abstract>Post-market surveillance, the practice of monitoring the safe use of pharmaceutical drugs is an important part of pharmacovigilance. Being able to collect personal experience related to pharmaceutical product use could help us gain insight into how the human body reacts to different medications. Twitter, a popular social media service, is being considered as an important alternative data source for collecting personal experience information with medications. Identifying personal experience tweets is a challenging classification task in natural language processing. In this study, we utilized three methods based on Facebook&#8217;s Robustly Optimized BERT Pretraining Approach (RoBERTa) to predict personal experience tweets related to medication use: the first one combines the pre-trained RoBERTa model with a classifier, the second combines the updated pre-trained RoBERTa model using a corpus of unlabeled tweets with a classifier, and the third combines the RoBERTa model that was trained with our unlabeled tweets from scratch with the classifier too. Our results show that all of these approaches outperform the published methods (Word Embedding + LSTM) in classification performance (p &lt; 0.05), and updating the pre-trained language model with tweets related to medications could even improve the performance further.</abstract>
      <url hash="0e26c93c">2020.louhi-1.14</url>
      <doi>10.18653/v1/2020.louhi-1.14</doi>
      <video href="https://slideslive.com/38940051" />
      <bibkey>zhu-etal-2020-identifying</bibkey>
    </paper>
    </volume>
</collection>