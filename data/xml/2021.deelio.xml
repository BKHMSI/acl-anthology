<collection id="2021.deelio">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</booktitle>
      <editor><first>Eneko</first><last>Agirre</last></editor>
      <editor><first>Marianna</first><last>Apidianaki</last></editor>
      <editor><first>Ivan</first><last>Vuli&#263;</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.deelio-1</url>
    </meta>
    <frontmatter>
      <url hash="3cc0ef1a">2021.deelio-1.0</url>
      <bibkey>deelio-2021-deep</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Augmenting Topic Aware Knowledge-Grounded Conversations with Dynamic Built Knowledge Graphs</title>
      <author><first>Junjie</first><last>Wu</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <pages>31&#8211;39</pages>
      <abstract>Dialog topic management and background knowledge selection are essential factors for the success of knowledge-grounded open-domain conversations. However, existing models are primarily performed with symmetric knowledge bases or stylized with pre-defined roles between conversational partners, while people usually have their own knowledge before a real chit-chat. To address this problem, we propose a dynamic knowledge graph-based topical conversation model (DKGT). Given a dialog history context, our model first builds knowledge graphs from the context as an imitation of human&#8217;s ability to form logical relationships between known and unknown topics during a conversation. This logical information will be fed into a topic predictor to promote topic management, then facilitate background knowledge selection and response generation. To the best of our knowledge, this is the first attempt to dynamically form knowledge graphs between chatting topics to assist dialog topic management during a conversation. Experimental results manifest that our model can properly schedule conversational topics and pick suitable knowledge to generate informative responses comparing to several strong baselines.</abstract>
      <url hash="c3c44be3">2021.deelio-1.4</url>
      <doi>10.18653/v1/2021.deelio-1.4</doi>
      <bibkey>wu-zhou-2021-augmenting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/topical-chat">Topical-Chat</pwcdataset>
    </paper>
    <paper id="5">
      <title>What Makes My Model Perplexed? A Linguistic Investigation on Neural Language Models Perplexity</title>
      <author><first>Alessio</first><last>Miaschi</last></author>
      <author><first>Dominique</first><last>Brunato</last></author>
      <author><first>Felice</first><last>Dell&#8217;Orletta</last></author>
      <author><first>Giulia</first><last>Venturi</last></author>
      <pages>40&#8211;47</pages>
      <abstract>This paper presents an investigation aimed at studying how the linguistic structure of a sentence affects the perplexity of two of the most popular Neural Language Models (NLMs), BERT and GPT-2. We first compare the sentence-level likelihood computed with BERT and the GPT-2&#8217;s perplexity showing that the two metrics are correlated. In addition, we exploit linguistic features capturing a wide set of morpho-syntactic and syntactic phenomena showing how they contribute to predict the perplexity of the two NLMs.</abstract>
      <url hash="6e44f56d">2021.deelio-1.5</url>
      <doi>10.18653/v1/2021.deelio-1.5</doi>
      <bibkey>miaschi-etal-2021-makes</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="12">
      <title>What <fixed-case>BERT</fixed-case>s and <fixed-case>GPT</fixed-case>s know about your brand? Probing contextual language models for affect associations</title>
      <author><first>Vivek</first><last>Srivastava</last></author>
      <author><first>Stephen</first><last>Pilli</last></author>
      <author><first>Savita</first><last>Bhat</last></author>
      <author><first>Niranjan</first><last>Pedanekar</last></author>
      <author><first>Shirish</first><last>Karande</last></author>
      <pages>119&#8211;128</pages>
      <abstract>Investigating brand perception is fundamental to marketing strategies. In this regard, brand image, defined by a set of attributes (Aaker, 1997), is recognized as a key element in indicating how a brand is perceived by various stakeholders such as consumers and competitors. Traditional approaches (e.g., surveys) to monitor brand perceptions are time-consuming and inefficient. In the era of digital marketing, both brand managers and consumers engage with a vast amount of digital marketing content. The exponential growth of digital content has propelled the emergence of pre-trained language models such as BERT and GPT as essential tools in solving myriads of challenges with textual data. This paper seeks to investigate the extent of brand perceptions (i.e., brand and image attribute associations) these language models encode. We believe that any kind of bias for a brand and attribute pair may influence customer-centric downstream tasks such as recommender systems, sentiment analysis, and question-answering, e.g., suggesting a specific brand consistently when queried for innovative products. We use synthetic data and real-life data and report comparison results for five contextual LMs, viz. BERT, RoBERTa, DistilBERT, ALBERT and BART.</abstract>
      <url hash="9789e6f1">2021.deelio-1.12</url>
      <attachment type="OptionalSupplementaryData" hash="d10d3633">2021.deelio-1.12.OptionalSupplementaryData.pdf</attachment>
      <doi>10.18653/v1/2021.deelio-1.12</doi>
      <bibkey>srivastava-etal-2021-berts</bibkey>
    </paper>
    <paper id="13">
      <title>Attention vs non-attention for a Shapley-based explanation method</title>
      <author><first>Tom</first><last>Kersten</last></author>
      <author><first>Hugh Mee</first><last>Wong</last></author>
      <author><first>Jaap</first><last>Jumelet</last></author>
      <author><first>Dieuwke</first><last>Hupkes</last></author>
      <pages>129&#8211;139</pages>
      <abstract>The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. The extent to which such methods &#8211; that are often proposed and tested in the domain of computer vision &#8211; are appropriate to address the explainability challenges in NLP is yet relatively unexplored. In this work, we consider Contextual Decomposition (CD) &#8211; a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models &#8211; and we test the extent to which it is useful for models that contain attention operations. To this end, we extend CD to cover the operations necessary for attention-based models. We then compare how long distance subject-verb relationships are processed by models with and without attention, considering a number of different syntactic structures in two different languages: English and Dutch. Our experiments confirm that CD can successfully be applied for attention-based models as well, providing an alternative Shapley-based attribution method for modern neural networks. In particular, using CD, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models.</abstract>
      <url hash="6374db31">2021.deelio-1.13</url>
      <doi>10.18653/v1/2021.deelio-1.13</doi>
      <bibkey>kersten-etal-2021-attention</bibkey>
    </paper>
    </volume>
</collection>