<collection id="2020.iwslt">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 17th International Conference on Spoken Language Translation</booktitle>
      <editor><first>Marcello</first><last>Federico</last></editor>
      <editor><first>Alex</first><last>Waibel</last></editor>
      <editor><first>Kevin</first><last>Knight</last></editor>
      <editor><first>Satoshi</first><last>Nakamura</last></editor>
      <editor><first>Hermann</first><last>Ney</last></editor>
      <editor><first>Jan</first><last>Niehues</last></editor>
      <editor><first>Sebastian</first><last>St&#252;ker</last></editor>
      <editor><first>Dekai</first><last>Wu</last></editor>
      <editor><first>Joseph</first><last>Mariani</last></editor>
      <editor><first>Francois</first><last>Yvon</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="4afe5e25">2020.iwslt-1</url>
    </meta>
    <frontmatter>
      <url hash="3be4c876">2020.iwslt-1.0</url>
      <bibkey>iwslt-2020-international</bibkey>
    </frontmatter>
    <paper id="9">
      <title><fixed-case>SRPOL</fixed-case>&#8217;s System for the <fixed-case>IWSLT</fixed-case> 2020 End-to-End Speech Translation Task</title>
      <author><first>Tomasz</first><last>Potapczyk</last></author>
      <author><first>Pawel</first><last>Przybysz</last></author>
      <pages>89&#8211;94</pages>
      <abstract>We took part in the offline End-to-End English to German TED lectures translation task. We based our solution on our last year&#8217;s submission. We used a slightly altered Transformer architecture with ResNet-like convolutional layer preparing the audio input to Transformer encoder. To improve the model&#8217;s quality of translation we introduced two regularization techniques and trained on machine translated Librispeech corpus in addition to iwslt-corpus, TEDLIUM2 andMust_C corpora. Our best model scored almost 3 BLEU higher than last year&#8217;s model. To segment 2020 test set we used exactly the same procedure as last year.</abstract>
      <url hash="233224a1">2020.iwslt-1.9</url>
      <doi>10.18653/v1/2020.iwslt-1.9</doi>
      <bibkey>potapczyk-przybysz-2020-srpols</bibkey>
    </paper>
    <paper id="10">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>H</fixed-case>elsinki Submission to the <fixed-case>IWSLT</fixed-case>2020 Offline <fixed-case>S</fixed-case>peech<fixed-case>T</fixed-case>ranslation Task</title>
      <author><first>Ra&#250;l</first><last>V&#225;zquez</last></author>
      <author><first>Mikko</first><last>Aulamo</last></author>
      <author><first>Umut</first><last>Sulubacak</last></author>
      <author><first>J&#246;rg</first><last>Tiedemann</last></author>
      <pages>95&#8211;102</pages>
      <abstract>This paper describes the University of Helsinki Language Technology group&#8217;s participation in the IWSLT 2020 offline speech translation task, addressing the translation of English audio into German text. In line with this year&#8217;s task objective, we train both cascade and end-to-end systems for spoken language translation. We opt for an end-to-end multitasking architecture with shared internal representations and a cascade approach that follows a standard procedure consisting of ASR, correction, and MT stages. We also describe the experiments that served as a basis for the submitted systems. Our experiments reveal that multitasking training with shared internal representations is not only possible but allows for knowledge-transfer across modalities.</abstract>
      <url hash="0b46f3a0">2020.iwslt-1.10</url>
      <doi>10.18653/v1/2020.iwslt-1.10</doi>
      <video href="http://slideslive.com/38929617" />
      <bibkey>vazquez-etal-2020-university</bibkey>
    </paper>
    <paper id="11">
      <title>The <fixed-case>AFRL</fixed-case> <fixed-case>IWSLT</fixed-case> 2020 Systems: Work-From-Home Edition</title>
      <author><first>Brian</first><last>Ore</last></author>
      <author><first>Eric</first><last>Hansen</last></author>
      <author><first>Tim</first><last>Anderson</last></author>
      <author><first>Jeremy</first><last>Gwinnup</last></author>
      <pages>103&#8211;108</pages>
      <abstract>This report summarizes the Air Force Research Laboratory (AFRL) submission to the offline spoken language translation (SLT) task as part of the IWSLT 2020 evaluation campaign. As in previous years, we chose to adopt the cascade approach of using separate systems to perform speech activity detection, automatic speech recognition, sentence segmentation, and machine translation. All systems were neural based, including a fully-connected neural network for speech activity detection, a Kaldi factorized time delay neural network with recurrent neural network (RNN) language model rescoring for speech recognition, a bidirectional RNN with attention mechanism for sentence segmentation, and transformer networks trained with OpenNMT and Marian for machine translation. Our primary submission yielded BLEU scores of 21.28 on tst2019 and 23.33 on tst2020.</abstract>
      <url hash="5dca6d62">2020.iwslt-1.11</url>
      <doi>10.18653/v1/2020.iwslt-1.11</doi>
      <video href="http://slideslive.com/38929615" />
      <bibkey>ore-etal-2020-afrl</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>OPPO</fixed-case>&#8217;s Machine Translation System for the <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation Task</title>
      <author><first>Qian</first><last>Zhang</last></author>
      <author><first>Xiaopu</first><last>Li</last></author>
      <author><first>Dawei</first><last>Dang</last></author>
      <author><first>Tingxun</first><last>Shi</last></author>
      <author><first>Di</first><last>Ai</last></author>
      <author><first>Zhengshan</first><last>Xue</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <pages>114&#8211;121</pages>
      <abstract>In this paper, we demonstrate our machine translation system applied for the Chinese-Japanese bidirectional translation task (aka. open domain translation task) for the IWSLT 2020. Our model is based on Transformer (Vaswani et al., 2017), with the help of many popular, widely proved effective data preprocessing and augmentation methods. Experiments show that these methods can improve the baseline model steadily and significantly.</abstract>
      <url hash="3a418dac">2020.iwslt-1.13</url>
      <doi>10.18653/v1/2020.iwslt-1.13</doi>
      <video href="http://slideslive.com/38929611" />
      <bibkey>zhang-etal-2020-oppos</bibkey>
    </paper>
    <paper id="14">
      <title>Character Mapping and Ad-hoc Adaptation: <fixed-case>E</fixed-case>dinburgh&#8217;s <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation System</title>
      <author><first>Pinzhen</first><last>Chen</last></author>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Ulrich</first><last>Germann</last></author>
      <pages>122&#8211;129</pages>
      <abstract>This paper describes the University of Edinburgh&#8217;s neural machine translation systems submitted to the IWSLT 2020 open domain Japanese<tex-math>\leftrightarrow</tex-math>Chinese translation task. On top of commonplace techniques like tokenisation and corpus cleaning, we explore character mapping and unsupervised decoding-time adaptation. Our techniques focus on leveraging the provided data, and we show the positive impact of each technique through the gradual improvement of BLEU.</abstract>
      <url hash="cfaba852">2020.iwslt-1.14</url>
      <doi>10.18653/v1/2020.iwslt-1.14</doi>
      <video href="http://slideslive.com/38929590" />
      <bibkey>chen-etal-2020-character</bibkey>
      <pwccode url="https://github.com/marian-nmt/marian" additional="false">marian-nmt/marian</pwccode>
    </paper>
    <paper id="15">
      <title><fixed-case>CASIA</fixed-case>&#8217;s System for <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation</title>
      <author><first>Qian</first><last>Wang</last></author>
      <author><first>Yuchen</first><last>Liu</last></author>
      <author><first>Cong</first><last>Ma</last></author>
      <author><first>Yu</first><last>Lu</last></author>
      <author><first>Yining</first><last>Wang</last></author>
      <author><first>Long</first><last>Zhou</last></author>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>130&#8211;139</pages>
      <abstract>This paper describes the CASIA&#8217;s system for the IWSLT 2020 open domain translation task. This year we participate in both Chinese&#8594;Japanese and Japanese&#8594;Chinese translation tasks. Our system is neural machine translation system based on Transformer model. We augment the training data with knowledge distillation and back translation to improve the translation performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on development data with different model settings and different data processing techniques.</abstract>
      <url hash="d120b4ec">2020.iwslt-1.15</url>
      <doi>10.18653/v1/2020.iwslt-1.15</doi>
      <video href="http://slideslive.com/38929589" />
      <bibkey>wang-etal-2020-casias</bibkey>
    </paper>
    <paper id="16">
      <title>Deep Blue Sonics&#8217; Submission to <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation Task</title>
      <author><first>Enmin</first><last>Su</last></author>
      <author><first>Yi</first><last>Ren</last></author>
      <pages>140&#8211;144</pages>
      <abstract>We present in this report our submission to IWSLT 2020 Open Domain Translation Task. We built a data pre-processing pipeline to efficiently handle large noisy web-crawled corpora, which boosts the BLEU score of a widely used transformer model in this translation task. To tackle the open-domain nature of this task, back- translation is applied to further improve the translation performance.</abstract>
      <url hash="514a7105">2020.iwslt-1.16</url>
      <doi>10.18653/v1/2020.iwslt-1.16</doi>
      <video href="http://slideslive.com/38929592" />
      <bibkey>su-ren-2020-deep</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>ISTIC</fixed-case>&#8217;s Neural Machine Translation System for <fixed-case>IWSLT</fixed-case>&#8217;2020</title>
      <author><first>Jiaze</first><last>Wei</last></author>
      <author><first>Wenbin</first><last>Liu</last></author>
      <author><first>Zhenfeng</first><last>Wu</last></author>
      <author><first>You</first><last>Pan</last></author>
      <author><first>Yanqing</first><last>He</last></author>
      <pages>158&#8211;165</pages>
      <abstract>This paper introduces technical details of machine translation system of Institute of Scientific and Technical Information of China (ISTIC) for the 17th International Conference on Spoken Language Translation (IWSLT 2020). ISTIC participated in both translation tasks of the Open Domain Translation track: Japanese-to-Chinese MT task and Chinese-to-Japanese MT task. The paper mainly elaborates on the model framework, data preprocessing methods and decoding strategies adopted in our system. In addition, the system performance on the development set are given under different settings.</abstract>
      <url hash="48f513f1">2020.iwslt-1.19</url>
      <doi>10.18653/v1/2020.iwslt-1.19</doi>
      <bibkey>wei-etal-2020-istics</bibkey>
    </paper>
    <paper id="23">
      <title>The <fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> Video Speech Translation System at <fixed-case>IWSLT</fixed-case> 2020</title>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Yao</first><last>Deng</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Ning</first><last>Xie</last></author>
      <author><first>Xiaochun</first><last>Li</last></author>
      <author><first>Jiaxian</first><last>Guo</last></author>
      <pages>187&#8211;190</pages>
      <abstract>The paper presents details of our system in the IWSLT Video Speech Translation evaluation. The system works in a cascade form, which contains three modules: 1) A proprietary ASR system. 2) A disfluency correction system aims to remove interregnums or other disfluent expressions with a fine-tuned BERT and a series of rule-based algorithms. 3) An NMT System based on the Transformer and trained with massive publicly available corpus.</abstract>
      <url hash="c178ea46">2020.iwslt-1.23</url>
      <doi>10.18653/v1/2020.iwslt-1.23</doi>
      <video href="http://slideslive.com/38929616" />
      <bibkey>wang-etal-2020-hw</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>CUNI</fixed-case> Neural <fixed-case>ASR</fixed-case> with Phoneme-Level Intermediate Step for~<fixed-case>N</fixed-case>on-<fixed-case>N</fixed-case>ative~<fixed-case>SLT</fixed-case> at <fixed-case>IWSLT</fixed-case> 2020</title>
      <author><first>Peter</first><last>Pol&#225;k</last></author>
      <author><first>Sangeet</first><last>Sagar</last></author>
      <author><first>Dominik</first><last>Mach&#225;&#269;ek</last></author>
      <author><first>Ond&#345;ej</first><last>Bojar</last></author>
      <pages>191&#8211;199</pages>
      <abstract>In this paper, we present our submission to the Non-Native Speech Translation Task for IWSLT 2020. Our main contribution is a proposed speech recognition pipeline that consists of an acoustic model and a phoneme-to-grapheme model. As an intermediate representation, we utilize phonemes. We demonstrate that the proposed pipeline surpasses commercially used automatic speech recognition (ASR) and submit it into the ASR track. We complement this ASR with off-the-shelf MT systems to take part also in the speech translation track.</abstract>
      <url hash="c7893255">2020.iwslt-1.24</url>
      <doi>10.18653/v1/2020.iwslt-1.24</doi>
      <bibkey>polak-etal-2020-cuni</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>ELITR</fixed-case> Non-Native Speech Translation at <fixed-case>IWSLT</fixed-case> 2020</title>
      <author><first>Dominik</first><last>Mach&#225;&#269;ek</last></author>
      <author><first>Jon&#225;&#353;</first><last>Kratochv&#237;l</last></author>
      <author><first>Sangeet</first><last>Sagar</last></author>
      <author><first>Mat&#250;&#353;</first><last>&#381;ilinec</last></author>
      <author><first>Ond&#345;ej</first><last>Bojar</last></author>
      <author><first>Thai-Son</first><last>Nguyen</last></author>
      <author><first>Felix</first><last>Schneider</last></author>
      <author><first>Philip</first><last>Williams</last></author>
      <author><first>Yuekun</first><last>Yao</last></author>
      <pages>200&#8211;208</pages>
      <abstract>This paper is an ELITR system submission for the non-native speech translation task at IWSLT 2020. We describe systems for offline ASR, real-time ASR, and our cascaded approach to offline SLT and real-time SLT. We select our primary candidates from a pool of pre-existing systems, develop a new end-to-end general ASR system, and a hybrid ASR trained on non-native speech. The provided small validation set prevents us from carrying out a complex validation, but we submit all the unselected candidates for contrastive evaluation on the test set.</abstract>
      <url hash="e32ca293">2020.iwslt-1.25</url>
      <doi>10.18653/v1/2020.iwslt-1.25</doi>
      <video href="http://slideslive.com/38929595" />
      <bibkey>machacek-etal-2020-elitr</bibkey>
    </paper>
    <paper id="29">
      <title>Neural Simultaneous Speech Translation Using Alignment-Based Chunking</title>
      <author><first>Patrick</first><last>Wilken</last></author>
      <author><first>Tamer</first><last>Alkhouli</last></author>
      <author><first>Evgeny</first><last>Matusov</last></author>
      <author><first>Pavel</first><last>Golik</last></author>
      <pages>237&#8211;246</pages>
      <abstract>In simultaneous machine translation, the objective is to determine when to produce a partial translation given a continuous stream of source words, with a trade-off between latency and quality. We propose a neural machine translation (NMT) model that makes dynamic decisions when to continue feeding on input or generate output words. The model is composed of two main components: one to dynamically decide on ending a source chunk, and another that translates the consumed chunk. We train the components jointly and in a manner consistent with the inference conditions. To generate chunked training data, we propose a method that utilizes word alignment while also preserving enough context. We compare models with bidirectional and unidirectional encoders of different depths, both on real speech and text input. Our results on the IWSLT 2020 English-to-German task outperform a wait-k baseline by 2.6 to 3.7% BLEU absolute.</abstract>
      <url hash="34bea1f1">2020.iwslt-1.29</url>
      <doi>10.18653/v1/2020.iwslt-1.29</doi>
      <video href="http://slideslive.com/38929608" />
      <bibkey>wilken-etal-2020-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="33">
      <title>Efficient Automatic Punctuation Restoration Using Bidirectional Transformers with Robust Inference</title>
      <author><first>Maury</first><last>Courtland</last></author>
      <author><first>Adam</first><last>Faulkner</last></author>
      <author><first>Gayle</first><last>McElvain</last></author>
      <pages>272&#8211;279</pages>
      <abstract>Though people rarely speak in complete sentences, punctuation confers many benefits to the readers of transcribed speech. Unfortunately, most ASR systems do not produce punctuated output. To address this, we propose a solution for automatic punctuation that is both cost efficient and easy to train. Our solution benefits from the recent trend in fine-tuning transformer-based language models. We also modify the typical framing of this task by predicting punctuation for sequences rather than individual tokens, which makes for more efficient training and inference. Finally, we find that aggregating predictions across multiple context windows improves accuracy even further. Our best model achieves a new state of the art on benchmark data (TED Talks) with a combined F1 of 83.9, representing a 48.7% relative improvement (15.3 absolute) over the previous state of the art.</abstract>
      <url hash="45ee1725">2020.iwslt-1.33</url>
      <doi>10.18653/v1/2020.iwslt-1.33</doi>
      <video href="http://slideslive.com/38929594" />
      <bibkey>courtland-etal-2020-efficient</bibkey>
    </paper>
    </volume>
</collection>