<?xml version='1.0' encoding='utf-8'?>
<collection id="2020.iwslt">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 17th International Conference on Spoken Language Translation</booktitle>
      <editor><first>Marcello</first><last>Federico</last></editor>
      <editor><first>Alex</first><last>Waibel</last></editor>
      <editor><first>Kevin</first><last>Knight</last></editor>
      <editor><first>Satoshi</first><last>Nakamura</last></editor>
      <editor><first>Hermann</first><last>Ney</last></editor>
      <editor><first>Jan</first><last>Niehues</last></editor>
      <editor><first>Sebastian</first><last>Stüker</last></editor>
      <editor><first>Dekai</first><last>Wu</last></editor>
      <editor><first>Joseph</first><last>Mariani</last></editor>
      <editor><first>Francois</first><last>Yvon</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="4afe5e25">2020.iwslt-1</url>
    </meta>
    <frontmatter>
      <url hash="3be4c876">2020.iwslt-1.0</url>
      <bibkey>iwslt-2020-international</bibkey>
    </frontmatter>
    <paper id="9">
      <title>SRPOL’s System for the IWSLT 2020 End-to-End Speech Translation Task<fixed-case>SRPOL</fixed-case>’s System for the <fixed-case>IWSLT</fixed-case> 2020 End-to-End Speech Translation Task</title>
      <author><first>Tomasz</first><last>Potapczyk</last></author>
      <author><first>Pawel</first><last>Przybysz</last></author>
      <pages>89–94</pages>
      <abstract>We took part in the offline End-to-End English to German TED lectures translation task. We based our <a href="https://en.wikipedia.org/wiki/Solution">solution</a> on our last year’s submission. We used a slightly altered Transformer architecture with ResNet-like convolutional layer preparing the audio input to Transformer encoder. To improve the model’s quality of translation we introduced two regularization techniques and trained on machine translated Librispeech corpus in addition to iwslt-corpus, TEDLIUM2 andMust_C corpora. Our best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> scored almost 3 BLEU higher than last year’s model. To segment 2020 test set we used exactly the same procedure as last year.</abstract>
      <url hash="233224a1">2020.iwslt-1.9</url>
      <doi>10.18653/v1/2020.iwslt-1.9</doi>
      <bibkey>potapczyk-przybysz-2020-srpols</bibkey>
    <title_ar>نظام SRPOL لمهمة ترجمة الكلام من البداية إلى النهاية IWSLT 2020</title_ar>
      <title_fr>Le système de SRPOL pour la tâche de traduction vocale de bout en bout IWSLT 2020</title_fr>
      <title_pt>Sistema do SRPOL para a tarefa de tradução de fala de ponta a ponta do IWSLT 2020</title_pt>
      <title_es>Sistema de SRPOL para la tarea de traducción de voz de extremo a extremo de IWSLT 2020</title_es>
      <title_zh>SRPOL 用 IWSLT 2020 端到端语音译职之统</title_zh>
      <title_hi>IWSLT 2020 एंड-टू-एंड वाक् अनुवाद कार्य के लिए SRPOL की प्रणाली</title_hi>
      <title_ja>IWSLT 2020エンドツーエンドの音声翻訳タスクのためのSRPOLのシステム</title_ja>
      <title_ru>SRPOL 's System for the IWSLT 2020 End-to-End Speech Translation Task (Система SRPOL для задачи комплексного перевода речи IWSLT 2020)</title_ru>
      <title_ga>Córas SRPOL do Thasc Aistriúcháin Ó Dheireadh go Deireadh IWSLT 2020</title_ga>
      <title_ka>SRPOL- ის სისტემა IWSLT 2020 ბოლოდან ბოლოდან დასრულებული სიტყვების გასაგრძელება</title_ka>
      <title_el>Το σύστημα της SRPOL για το έργο ολοκληρωμένης μετάφρασης ομιλίας του IWSLT 2020</title_el>
      <title_hu>Az SRPOL rendszere az IWSLT 2020 teljes beszédfordítási feladatához</title_hu>
      <title_it>Sistema SRPOL per la traduzione vocale end-to-end di IWSLT 2020</title_it>
      <title_kk>SRPOL жүйесі IWSLT 2020 End- to End Speech Translation Task</title_kk>
      <title_ms>SRPOL's System for the IWSLT 2020 End-to-End Speech Translation Task</title_ms>
      <title_ml>IWSLT 2020 അവസാനിക്കുന്നതിനുള്ള SRPOL സിസ്റ്റം</title_ml>
      <title_mt>Is-Sistema tal-SRPOL għall-kompitu tat-traduzzjoni tal-kelma mill-aħħar sal-aħħar tal-IWSLT 2020</title_mt>
      <title_lt>SRPOL strategijos „IWSLT 2020“ vertimo žodžiu užduotis</title_lt>
      <title_mn>SRPOL's System for the IWSLT 2020 End-to-End Speech Translation Task</title_mn>
      <title_no>SRPOL- systemet for IWSLT 2020 End- to- End Speech Translation Task</title_no>
      <title_mk>Системот на СРПОЛ за задачата за преведување на говорот од крај до крај на IWSLT 2020</title_mk>
      <title_pl>System SRPOL do kompleksowego tłumaczenia mowy IWSLT 2020</title_pl>
      <title_ro>Sistemul SRPOL pentru sarcina de traducere vocală finală IWSLT 2020</title_ro>
      <title_sr>SRPOL-ov sistem za IWSLT 2020. zadatak za kraj do kraja prevoda govora</title_sr>
      <title_si>SRPOL' s System for the IWSLT 2020End- to- End Talk translation Job</title_si>
      <title_so>SRPOL's system for the IWSLT 2020 End-to-End Speech Translation Task</title_so>
      <title_ta>IWSLT 2020 முடிவு- முடிவு பேச்சு மொழிபெயர்ப்பு பணிக்கான SRPOL அமைப்பு</title_ta>
      <title_sv>SRPOL:s system för IWSLT 2020 End-to-End Speech Translation Task</title_sv>
      <title_ur>IWSLT 2020 End-to-End Speech Translation Task کے لئے SRPOL سیسٹم</title_ur>
      <title_uz>@ info: whatsthis</title_uz>
      <title_vi>Hệ thống âm thanh AROL for the IWSLT 2020 End-to-End chuyện Translation Task</title_vi>
      <title_bg>Системата на СРПОЛ за задачата за превод на реч от край до край</title_bg>
      <title_da>SRPOL's system til IWSLT 2020 End-to-End Tale Oversættelse Opgave</title_da>
      <title_nl>SRPOL's systeem voor de IWSLT 2020 End-to-End spraakvertaaltaak</title_nl>
      <title_hr>SRPOL-ov sustav za IWSLT 2020. zadatak za kraj do kraja prevoda govora</title_hr>
      <title_de>SRPOL's System für die IWSLT 2020 End-to-End Sprachübersetzung</title_de>
      <title_ko>SRPOL의 IWSLT 2020 엔드-투-엔드 음성 번역 임무 시스템</title_ko>
      <title_fa>سیستم SRPOL برای تاریخ ترجمه سخنرانی IWSLT 2020</title_fa>
      <title_sw>Mfumo wa SRPOL kwa ajili ya Task la Tafsiri ya Utafiti</title_sw>
      <title_tr>SRPOL'iň IWSLT 2020'iň End-to-End Sözi Terjime Görevi</title_tr>
      <title_am>SRPOL's System for the IWSLT 2020 End-to-End Speech Translation Task</title_am>
      <title_af>SRPOL se Stelsel vir die IWSLT 2020 End- to- End Speech Translation Task</title_af>
      <title_sq>Sistemi i SRPOL për IWSLT 2020</title_sq>
      <title_az>SRPOL's System for the IWSLT 2020 End-to-End Speech Translation Task</title_az>
      <title_bn>IWSLT ২০২০ শেষ- থেকে শেষ- পর্যন্ত ভাষণ অনুবাদের কাজের জন্য SRPOL সিস্টেম</title_bn>
      <title_hy>IwPLT 2020 թվականի վերջ-վերջ խոսքի թարգմանման հանձնարարությունը</title_hy>
      <title_bs>SRPOL-ov sustav za zadatak prevoda govora IWSLT 2020.</title_bs>
      <title_cs>Systém SRPOL pro komplexní překlad řeči IWSLT 2020</title_cs>
      <title_et>SRPOLi süsteem IWSLT 2020 lõppkõne tõlke ülesandeks</title_et>
      <title_fi>SRPOL:n järjestelmä IWSLT 2020:n puhekääntämistehtävään</title_fi>
      <title_id>Sistem SRPOL untuk Tugas Terjemahan Bicara Akhir-Akhir IWSLT 2020</title_id>
      <title_ca>El Sistema SRPOL per a la tasca de traducció de la llengua final a final IWSLT 2020</title_ca>
      <title_ha>@ action</title_ha>
      <title_jv>paper size</title_jv>
      <title_sk>Sistem SRPOL za nalogo prevajanja govora od konca do konca IWSLT 2020</title_sk>
      <title_he>מערכת SRPOL למשימה של IWSLT 2020</title_he>
      <title_bo>SRPOL's System for the IWSLT 2020 End-to-End Speech Translation Task</title_bo>
      <abstract_ar>شاركنا في مهمة ترجمة محاضرات TED من الإنجليزية إلى الألمانية من البداية إلى النهاية. لقد استندنا في حلنا إلى تقديمنا العام الماضي. استخدمنا بنية محول معدلة قليلاً مع طبقة تلافيفية تشبه ResNet لتحضير إدخال الصوت إلى ترميز المحولات. لتحسين جودة الترجمة في النموذج ، أدخلنا تقنيتين للتنظيم ودربنا على مجموعة Librispeech المترجمة آليًا بالإضافة إلى iwslt-corpus و TEDLIUM2 و Must_C corpora. أفضل طراز لدينا سجل 3 BLEU أعلى من طراز العام الماضي. لتقسيم مجموعة الاختبار لعام 2020 ، استخدمنا نفس الإجراء تمامًا كما في العام الماضي.</abstract_ar>
      <abstract_es>Participamos en la tarea de traducción de conferencias TED de inglés a alemán de principio a fin fuera de línea. Basamos nuestra solución en la presentación del año pasado. Utilizamos una arquitectura Transformer ligeramente alterada con una capa convolucional similar a Resnet que prepara la entrada de audio al codificador Transformer. Para mejorar la calidad de la traducción del modelo, introdujimos dos técnicas de regularización y nos capacitamos en corpus Librispeech traducidos automáticamente, además de los corpora iwslt-corpus, TEDLIUM2 y MUST_C. Nuestro mejor modelo obtuvo casi 3 BLEU más que el modelo del año pasado. Para segmentar el conjunto de pruebas de 2020 utilizamos exactamente el mismo procedimiento que el año pasado.</abstract_es>
      <abstract_pt>Participamos da tarefa de tradução de palestras TED de inglês para alemão de ponta a ponta offline. Baseamos nossa solução na apresentação do ano passado. Usamos uma arquitetura Transformer ligeiramente alterada com camada convolucional do tipo ResNet preparando a entrada de áudio para o codificador Transformer. Para melhorar a qualidade da tradução do modelo, introduzimos duas técnicas de regularização e treinamos em Librispeech corpus traduzido por máquina, além de iwslt-corpus, TEDLIUM2 e Must_C corpora. Nosso melhor modelo obteve quase 3 BLEU a mais do que o modelo do ano passado. Para segmentar o conjunto de testes de 2020, usamos exatamente o mesmo procedimento do ano passado.</abstract_pt>
      <abstract_fr>Nous avons participé à la tâche de traduction hors ligne de bout en bout des conférences TED de l'anglais vers l'allemand. Nous avons basé notre solution sur la base de notre soumission de l'année dernière. Nous avons utilisé une architecture Transformer légèrement modifiée avec une couche convolutionnelle de type Resnet préparant l'entrée audio de l'encodeur Transformer. Pour améliorer la qualité de traduction du modèle, nous avons introduit deux techniques de régularisation et nous nous sommes formés au corpus Librispeech traduit automatiquement en plus des corpus iwslt-corpus, TEDLIUM2 et MUST_C. Notre meilleur modèle a obtenu près de 3 points de plus que le modèle de l'année dernière. Pour segmenter le jeu de test 2020, nous avons utilisé exactement la même procédure que l'année dernière.</abstract_fr>
      <abstract_ja>オフラインのエンドツーエンドの英語からドイツ語へのTED講義翻訳タスクに参加しました。昨年の提出書類に基づいてソリューションを作成しました。私たちは、ResNetのような畳み込みレイヤーを備えた少し変更されたトランスフォーマーアーキテクチャを使用して、トランスフォーマーエンコーダへのオーディオ入力を準備しました。モデルの翻訳品質を向上させるために、2つの正規化技術を導入し、iwslt - corpus、TEDLIUM 2、およびMust_C corporaに加えて機械翻訳Librispeechコーパスのトレーニングを行いました。当社の最高のモデルは、昨年のモデルよりもほぼ3 BLEUのスコアを獲得しました。2020年のテストセットをセグメント化するために、昨年とまったく同じ手順を使用しました。</abstract_ja>
      <abstract_zh>与离线端到端英语德语TED讲座译事。 我们的解决方案基于去年提交的文件。 吾用少变之 Transformer 架构,有类 ResNet 之卷积层,以备 Transformer 编码器之音频输。 为重译质,引入二正则化,教习于机器翻译之Librispeech语料库及iwslt-corpus,TEDLIUM2 andMust_C语料库。 最佳得分比去年模出近3 BLEU。 为分2020年试集,用与去年同。</abstract_zh>
      <abstract_ru>Мы приняли участие в офлайн-задаче по переводу с английского на немецкий лекций TED. Наше решение было основано на нашем прошлогоднем представлении. Мы использовали слегка измененную архитектуру Трансформатора со сверточным слоем, подобным ResNet, подготавливающим аудио вход для кодировщика Трансформатора. Для улучшения качества перевода модели мы внедрили две методики регуляризации и обучили машинному переводу Librispeech corpus в дополнение к iwslt-corpus, TEDLIUM2 иMust_C corpa. Наша лучшая модель набрала почти на 3 БЛЮ больше, чем модель прошлого года. Для сегментации тестового набора 2020 года мы использовали точно такую же процедуру, как и в прошлом году.</abstract_ru>
      <abstract_hi>हमने ऑफ़लाइन एंड-टू-एंड अंग्रेजी से जर्मन टेड व्याख्यान अनुवाद कार्य में भाग लिया। हम अपने पिछले साल के सबमिशन पर हमारे समाधान के आधार पर. हम ResNet के साथ एक थोड़ा बदल ट्रांसफॉर्मर वास्तुकला का इस्तेमाल किया-जैसे convolutional परत ट्रांसफॉर्मर एन्कोडर के लिए ऑडियो इनपुट तैयार. अनुवाद के मॉडल की गुणवत्ता में सुधार करने के लिए हमने दो नियमितीकरण तकनीकों को पेश किया और मशीन अनुवादित लिब्रिसपीच कॉर्पस पर प्रशिक्षित किया, इसके अलावा iwslt-corpus, TEDLIUM2 andMust_C corpora। हमारे सबसे अच्छे मॉडल ने पिछले साल के मॉडल की तुलना में लगभग 3 BLEU अधिक स्कोर किया। सेगमेंट 2020 परीक्षण सेट के लिए हमने पिछले साल की तरह ही प्रक्रिया का उपयोग किया।</abstract_hi>
      <abstract_ga>Ghlacamar páirt i dtasc aistriúcháin léachtaí TED ó Bhéarla go Gearmáinis ó cheann go ceann as líne. Bhunaíomar ár réiteach ar ár n-aighneacht anuraidh. D'úsáideamar ailtireacht Trasfhoirmeora a athraíodh de bheagán agus ciseal conbhlóideach cosúil le ResNet ag ullmhú an ionchur fuaime chuig an ionchódóir Transformer. Chun cáilíocht aistriúcháin an mhúnla a fheabhsú thugamar isteach dhá theicníc rialtachta agus chuireamar oiliúint ar corpas meaisín-aistrithe Librispeech chomh maith le iwslt-corpus, TEDLIUM2 agus Must_C corpora. Scóráil ár múnla is fearr beagnach 3 BLEU níos airde ná samhail na bliana seo caite. Chun tacair tástála 2020 a dheighilt, d'úsáideamar go díreach an nós imeachta céanna agus a bhí anuraidh.</abstract_ga>
      <abstract_hu>Részt vettünk az offline End-to-End angol-német TED előadások fordítási feladatában. Megoldásunkat a tavalyi beadványunkra alapoztuk. Egy kissé módosított Transformer architektúrát használtunk ResNet-szerű konvolúciós réteggel, amely előkészítette az audio bemenetet a Transformer kódolóhoz. A modell fordítási minőségének javítása érdekében két szabályozási technikát vezettünk be, és az iwslt-corpus, a TEDLIUM2 és a Must_C corpora mellett gépi lefordítású Librispeech corpus képzésére készültünk. A legjobb modellünk majdnem 3 BLEU-t ért el a tavalyi modellnél. A 2020-as tesztkészlethez pontosan ugyanazt az eljárást használtuk, mint tavaly.</abstract_hu>
      <abstract_el>Πήραμε μέρος στο έργο μετάφρασης διαλέξεων από αγγλικά σε γερμανικά. Βασίσαμε τη λύση μας στην περσινή υποβολή μας. Χρησιμοποιήσαμε μια ελαφρώς τροποποιημένη αρχιτεκτονική μετασχηματιστή με ένα στρώμα που μοιάζει με ResNet προετοιμάζοντας την είσοδο ήχου στον κωδικοποιητή μετασχηματιστή. Για να βελτιώσουμε την ποιότητα της μετάφρασης του μοντέλου εισαγάγαμε δύο τεχνικές κανονικοποίησης και εκπαιδεύσαμε σε μηχανικά μεταφρασμένο σώμα εκτός από τα σώματα iwslt-corpus, TEDLIUM2 και Must_C. Το καλύτερό μας μοντέλο πέτυχε σχεδόν τρία υψηλότερα από το περσινό μοντέλο. Για το τμήμα 2020 σετ δοκιμών χρησιμοποιήσαμε ακριβώς την ίδια διαδικασία με πέρυσι.</abstract_el>
      <abstract_ka>ჩვენ დავიწყეთ "End-to-End" ინგლისური დაწყვეტილების დაწყვეტილებელი TED ლექციების დაწყვეტილებელი საქმე. ჩვენ წინა წლის შემდეგ ჩვენი გარეშე დავიბაზეთ. ჩვენ გამოყენეთ ცოტა შეცვლელი ტრანფორმების აქტიქტიქტიკური რესნეტების მსგავსი კონტროლუციონალური ჩატვირთვის, როგორც რესნეტების მსგავ ჩვენ მოდელის გასაგრძელებლად ორი რეგილარიზაციის ტექნოგიების გასაგრძელებლად შევცვალოთ და მაქინის გასაგრძელებლად Librispeech corpus-ს დამატებით iwslt-corpus, TEDLIUM2 და Must_C corpora-ს დამატებით. ჩვენი ყველაზე საუკეთესო მოდელი დაიწყო დამატებით 3 BLEU-ს უფრო მეტი წლის მოდელზე. 2020 წლის სეგენტის ტესტისთვის ჩვენ გამოყენეთ ისეთი პროცესია როგორც წინა.</abstract_ka>
      <abstract_it>Abbiamo preso parte al compito offline di traduzione delle lezioni TED dall'inglese al tedesco. Abbiamo basato la nostra soluzione sulla presentazione dell'ultimo anno. Abbiamo usato un'architettura Transformer leggermente modificata con un livello convoluzionale simile a ResNet per preparare l'ingresso audio al codificatore Transformer. Per migliorare la qualità della traduzione del modello abbiamo introdotto due tecniche di regolarizzazione e addestrato sul corpus Librispeech tradotto automaticamente oltre a iwslt-corpus, TEDLIUM2 e Must_C corpora. Il nostro miglior modello ha ottenuto quasi 3 BLEU in più rispetto al modello dello scorso anno. Per segmentare il set di test 2020 abbiamo utilizzato esattamente la stessa procedura dell'anno scorso.</abstract_it>
      <abstract_kk>Біз "End-to-End" ағылшыншасына неміс TED лекцияларының аудару тапсырмасына бөлікті. Біз соңғы жылдың келтірімізге шешімізді негіздеп тұрмыз. Біз ResNet секілді конверсиялық қабатты аудио енгізуін түрлендіру кодеріне дайындау үшін бірнеше өзгертілген түрлендіру архитектурасын қолдандық. Үлгінің аудармасының сапатын жақсарту үшін, біз екі үлгі түрлендіру техникасын және компьютердің аудармағындағы Librispeech корпус iwslt-corpus, TEDLIUM2 және Must_C корпорасына қосымша үйрендік. Біздің ең жақсы моделіміз өткен жылдың үлгісінен артық 3 BLEU болды. 2020 жыл сегментінің сынақтарына біз өткен жылдың бір процедурасын қолдандық.</abstract_kk>
      <abstract_lt>Mes dalyvavome anglų kalbos vertimo darbe iš eilės į vokiečių TED pamokas. Mūs ų sprendimas grindžiamas praėjusių metų pareiškimu. Naudojome šiek tiek pakeistą Transformer architektūrą su panašu į ResNet konvoliuciniu sluoksniu, ruošiant garso įvedimą į Transformer kodatorių. Siekdami pagerinti modelio vertimo kokybę, įdiegėme du reguliarizavimo metodus ir apmokėme mašin ų vertimo Librispeech corpus, be iwslt-corpus, TEDLIUM2 ir Must_C corpora. Mūs ų geriausias modelis buvo beveik 3 BLEU didesnis už praėjusių metų model į. To segment 2020 test set we used exactly the same procedure as last year.</abstract_lt>
      <abstract_mk>We took part in the offline End-to-End English to German TED lectures translation task.  Го базиравме нашето решение на минатата година. Користевме малку променета трансформарна архитектура со конволуционален слој како Ресенет кој го подготвува аудио внесот во трансформарниот кодер. За да го подобриме квалитетот на преводот на моделот, воведовме две техники за регуларизација и трениравме на машински превод Librispeech corpus, покрај iwslt-corpus, TEDLIUM2 и Must_C corpora. Нашиот најдобар модел постигна скоро 3 БЛЕ повисоки од минатогодишниот модел. To segment 2020 test set we used exactly the same procedure as last year.</abstract_mk>
      <abstract_ms>We took part in the offline End-to-End English to German TED lectures translation task.  Kita berdasarkan penyelesaian kita pada penghantaran tahun lepas. Kami menggunakan arkitektur Transformer yang sedikit diubah dengan lapisan konvolusi seperti ResNet menyediakan input audio ke pengekod Transformer. Untuk meningkatkan kualiti terjemahan model kami memperkenalkan dua teknik pengaturan dan dilatih pada Librispeech corpus terjemahan mesin selain iwslt-corpus, TEDLIUM2 dan Must_C corpora. Our best model scored almost 3 BLEU higher than last year's model.  Untuk set ujian segmen 2020 kami menggunakan prosedur yang sama dengan tahun lepas.</abstract_ms>
      <abstract_ml>ഞങ്ങള്‍ ഓഫ്‌ലൈന്‍ ആന്‍ഡ് മുഴുവന്‍ ഇംഗ്ലീഷില്‍ പങ്കുചേര്‍ത്ത് ജര്‍മ്മന്‍ ടെഡി ലേക്ഷര്‍ പരിഭാഷണത്തിന്‍റെ  നമ്മുടെ കഴിഞ്ഞ വര്‍ഷത്തെ കീഴ്പ്പെടുത്തിയിട്ട് നമ്മുടെ പരിഹാരം അടിസ്ഥാനമാക്കി. We used a slightly altered Transformer architecture with ResNet-like convolutional layer preparing the audio input to Transformer encoder.  ഈ മോഡലിന്റെ വിഭാഷത്തിന്റെ ഗുണപൂര്‍ണ്ണമാക്കാന്‍ ഞങ്ങള്‍ രണ്ടു നിയന്ത്രണത്തിന്റെ സാങ്കേതികവിദ്യകളെ പരിശീലിപ്പിച്ച് ലിബ്രിസ്പീച് കോര്‍പ്പുസിനെ  നമ്മുടെ ഏറ്റവും നല്ല മോഡല്‍ കഴിഞ്ഞ വര്‍ഷത്തെ മോഡലിനെക്കാള്‍ മൂന്നു ബിലിയുവിനെക്കാള്‍ ഉയര്‍ത്തി. 2020 ടെസ്റ്റ് സെറ്റ് ചെയ്യാന്‍ കഴിഞ്ഞ വര്‍ഷം നമ്മള്‍ അതേ പ്രക്രിയയാണ് ഉപയോഗിച്ചത്.</abstract_ml>
      <abstract_mt>We took part in the offline End-to-End English to German TED lectures translation task.  We based our solution on our last year's submission.  Użajna arkitettura Transformer kemmxejn mibdula b’saff konvoluzzjonali simili għal ResNet li jipprepara l-input awdjo għall-kodifikatur Transformer. Biex tittejjeb il-kwalità tat-traduzzjoni tal-mudell introduċejna żewġ tekniki ta’ regolarizzazzjoni u mħarrġa fuq Librispeech corpus tradott bil-magna flimkien ma’ iwslt-corpus, TEDLIUM2 u Must_C corpora. L-aħjar mudell tagħna kellu kważi 3 BLEU ogħla mill-mudell tas-sena l-oħra. Għas-sett tat-test tas-segment 2020 użajna eżattament l-istess proċedura bħas-sena l-oħra.</abstract_mt>
      <abstract_no>Vi har delt i den fråkopla ende-til-slutt engelsk til tysk TED-leksjonsoppgåva. Vi baserer løsningen vårt på løsningen vårt siste år. Vi brukte ein liten endra transformeringsarkitektur med konvolusjonell lag som ResNet-liknar som forbereder lyd-inndata til Transformeringskodar. For å forbetra omsetjingskvaliteten til modellen, introdusere vi to reguleringsteknikk og trenga på maskina omsette Librispeech corpus i tillegg til iwslt-corpus, TEDLIUM2 og Must_C corpora. Det beste modellet vårt oppretta nesten 3 BLEU høgare enn siste årsmodellen. For segment 2020-testen brukte vi nøyaktig det samme prosedyren som siste år.</abstract_no>
      <abstract_pl>Braliśmy udział w zadaniu tłumaczenia wykładów TED z języka angielskiego na niemiecki offline. Nasze rozwiązanie opieraliśmy na zeszłorocznej zgłoszeniu. Zastosowaliśmy nieco zmienioną architekturę Transformera z warstwą konwolucyjną ResNet przygotowującą wejście audio do kodera Transformera. Aby poprawić jakość tłumaczenia modelu, wprowadziliśmy dwie techniki regularyzacji oraz trenowaliśmy korpus Librispeech przetłumaczony maszynowo oprócz korpusów iwslt-corpus, TEDLIUM2 i Must_C. Nasz najlepszy model zdobył niemal 3-BLEU wyższy niż ubiegłoroczny model. Do segmentu zestawu testowego 2020 zastosowaliśmy dokładnie taką samą procedurę jak w zeszłym roku.</abstract_pl>
      <abstract_mn>Бид "End-to-End" хэлний англи хэлний төгсгөлд Германы TED лекцийн хөгжлийн даалгавраанд оролцсон. Өнгөрсөн жилийн дараа бидний шийдвэрийг үндсэн. Бид ResNet шиг бага зэрэг өөрчлөгдсөн Трансфер архитектурыг ашигласан. Аудио орлуудыг Трансфер кодер руу бэлдэж байна. Тиймээс бид загварын хөгжүүлэх чадварыг сайжруулахын тулд хоёр шууд шинжлэх ухааны техник, машины хөгжүүлэх Librispeech корпус болон iwslt-corpus, TEDLIUM2 болон Must_C корпора дамжуулагдсан. Бидний хамгийн сайн загвар өнгөрсөн жилийн загвараас бараг 3 БЛУ өндөр байсан. 2020 оны шалгалтын хувьд бид өнгөрсөн жилтэй адилхан процедурыг ашигласан.</abstract_mn>
      <abstract_ro>Am luat parte la sarcina offline de traducere a prelegerilor TED din engleză în germană. Am bazat soluția noastră pe depunerea noastră de anul trecut. Am folosit o arhitectură Transformer ușor modificată cu strat convoluțional asemănător ResNet pregătind intrarea audio la encoder Transformer. Pentru a îmbunătăți calitatea traducerii modelului am introdus două tehnici de regularizare și am instruit pe corpus Librispeech tradus automat în plus față de iwslt-corpus, TEDLIUM2 și Must_C corpora. Cel mai bun model al nostru a obținut aproape 3 BLEU mai mult decât modelul de anul trecut. Pentru segmentarea setului de testare 2020 am folosit exact aceeași procedură ca și anul trecut.</abstract_ro>
      <abstract_sr>Pridružili smo se poslu za prevod na njemačkim TED predavanjima. Na osnovu našeg rješenja na poslednjoj predanosti. Koristili smo malu izmjenjenu arhitekturu transformera sa konvolucionalnim slojem poput ResNet a, pripremajući audio unos za koder transformera. Da bi poboljšali kvalitet prevoda modela, predstavili smo dve regularizacijske tehnike i obučene na mašini prevedeno Librispeech corpus, osim iwslt-corpus, TEDLIUM2 i Must_C corpora. Naš najbolji model je dobio skoro 3 BLEU viši od prošlogodišnjeg modela. Za snimanje testa 2020. koristili smo tačno iste procedure kao i prošle godine.</abstract_sr>
      <abstract_so>Waxaannu ka qeybqaadanay shabakadda End-to-End Ingiriis-da German TED lectures translation mission. waxaynu ku aasaasnay xalaalka dhamaadka sanadkii. Waxaynu isticmaalnay dhismaha turjumista ee wax yar oo beddelan oo la mid ah ResNet darajada adag oo u diyaarinaya sawirka codka ee turjumista. Si loo hagaajiyo takhasuska turjumista, waxaan soo bandhigay laba qaabilaad oo la soodajiyo oo lagu tababariyey mashiinka lagu turjumay librispeech korpus ka sokow iwslt-corpus, TEDLIUM2 iyo Must_C corpora. Our best model scored almost 3 BLEU higher than last year's model.  qeybinta 2020 ee baaritaanka waxaynu u isticmaalnay si isku mid ah xiliga sanadkii hore.</abstract_so>
      <abstract_si>අපි ඉංග්‍රීසියට අන්තිම විදියට ජර්මන් TED ප්‍රශ්න විදියට භාවිතා කරනවා. අපි අන්තිම අවුරුද්දේ පිළිගන්නේ අපේ විසරණය. අපි ResNet-වගේ සම්පූර්ණ ස්ථානයක් සමග වෙනස් වෙන්න වෙනස් වෙන්න ස්ථානයක් පාවිච්චි කරනවා අඩියෝජිත ඇත මොඩල් එකේ වාර්ථාවේ කුළුවත් වැඩි කරන්න අපි නියමික විද්‍යාපිත විද්‍යාපිත විද්‍යාපිත විද්‍යාපිත විද්‍යාපිත විද්‍යාපිත විද්‍යාපිත ව අපේ හොඳම මෝඩේල් එක ගිය අවුරුද්දේ මෝඩේල් එකට වඩා බ්ලෝයුස් 3ක් වඩා වැඩියි. අපි පරීක්ෂණාවට පරීක්ෂණාවට පස්සේ අවුරුද්දේ වගේ සිද්ධ විධානය පාවිච්චි කරනවා.</abstract_si>
      <abstract_sv>Vi deltog i översättningsuppgiften offline från engelska till tyska TED-föreläsningar. Vi baserade vår lösning på vårt förra års inlämning. Vi använde en något förändrad Transformer arkitektur med ResNet-liknande konvulutionslager för att förbereda ljudingången till Transformer encoder. För att förbättra modellens kvalitet på översättning introducerade vi två regulariseringstekniker och tränade på maskinöversatt Librispeech corpus utöver iwslt-corpus, TEDLIUM2 och Must_C corpora. Vår bästa modell fick nästan 3 BLEU högre än förra årets modell. För att segmentera 2020 testset använde vi exakt samma procedur som förra året.</abstract_sv>
      <abstract_ta>We took part in the offline End-to-End English to German TED lectures translation task.  நாங்கள் கடந்த வருடத்தின் கட்டளையை அடிப்படையாக எங்கள் தீர்வு. நாங்கள் ஒரு சிறிய மாற்றப்பட்ட மாற்று அமைப்பை பயன்படுத்தினோம் ரெஸ்நெட் போன்ற சாதாரண அடுக்கு மாற்றும் குறியீட்டிற்க மாதிரியின் மொழிபெயர்ப்பின் தரம் மாற்ற நாங்கள் இரண்டு கட்டுப்பாட்டு தொழில்நுட்பத்தை முன்னேற்றி இயந்திரத்தில் பயிற்சி மொழிபெயர்ப்பு லிப்ரிச்ப எங்கள் சிறந்த மாதிரி கடந்த வருடத்தின் மாதிரியை விட மூன்று பிலியு உயர்ந்தது. 2020 சோதனையை பிரிக்க நாம் கடந்த வருடத்தில் அதே செயல்பாட்டை பயன்படுத்தினோம்.</abstract_ta>
      <abstract_ur>ہم نے آف لین End-to-End انگلیسی میں جرمانی TED لکتروں کی ترجمہ کا کام بنایا۔ ہم نے اگلے سال کے مسلمانوں پر ہمارا حل بنیاد رکھا ہے۔ ہم نے ایک تھوڑا بدل تغییر تغییر تغییر دینے والے معماری استعمال کیا ہے جو رس نیٹ جیسی کنوولیوشن لائر کے ساتھ آڈیو اپن ایمپ ترنسفور کوڈر کے لئے تیار کر رہے ہیں. ہم نے مدل کی تعلیم کی کیفیت کو اچھی طرح پہنچایا اور ماشین کی تعلیم لیبرائیسپیچ کورپوس کے علاوہ دوسری قانونی تکنیک کو پہنچایا۔ ہمارے سب سے بہترین نمونڈل گئی سال کی نمونڈل سے تقریباً تین بلیوس سے بلند تھا۔ سال ۲۰۰۲ کے سپٹ ٹیسٹ کے لئے ہم نے پچھلی سال کے مطابق اسی طرح استعمال کیا۔</abstract_ur>
      <abstract_uz>Biz ofline End to- End ingliz tilidagi ingliz tilida o'rganishni Olmoncha TED tahrirlash vazifasiga ega qildik. Biz yetgi yil qanday qilishga qaror qilamiz. Biz ResNet sifatida bir qisqa o'zgarishga o'zgarishga ishlatdik va Transformer kodlash uchun audio input tayyorligini tayyorlash. @ info Bizning eng yaxshi modelimiz past yil modelidagi 3 BLEU ko'p edi. 2020 ta'limni ajratish uchun biz past yil huddi bir xil vazifalarni ishlatdik.</abstract_uz>
      <abstract_vi>Chúng tôi tham gia vào cuộc dịch thuật bằng tiếng Anh giữa kết thúc và cuối cùng của Đức. Chúng ta đã dựa trên giải pháp của mình từ năm ngoái. Chúng tôi sử dụng một cấu trúc biến hình có chút thay đổi với lớp xoắn ốc giống hệt ResNet để chuẩn bị âm thanh nhập vào bộ mã hóa biến hình. Để nâng cao chất lượng bản dịch của mô hình chúng tôi đã nhập vào hai kỹ thuật hoá học và được đào tạo trên tập đoàn LibRispeech (Văn bản) được dịch ra trên máy tính, thêm cả hợp chất lỏng lẻo, định vị và phải hạ C. Mô hình tốt nhất của chúng ta đã đạt đến ba nguyên tắc cao hơn mẫu năm ngoái. Về phần kiểm tra 2020 chúng tôi đã dùng đúng thủ tục y chang năm ngoái.</abstract_vi>
      <abstract_bg>Участвахме в офлайн задачата за превод на лекции от английски до немски език. Основахме решението си на представянето от последната година. Използвахме леко променена архитектура на трансформатора с конволюционен слой, подобен на ResNet, подготвящ аудио входа към трансформатора. За да подобрим качеството на превода на модела, въведохме две техники за регулировка и обучихме машинно преведен корпус в допълнение към корпусите. Най-добрият ни модел отбеляза почти 3 по-висока оценка от миналата година. За сегмент 2020 тест комплект използвахме точно същата процедура като миналата година.</abstract_bg>
      <abstract_da>Vi deltog i offline end-to-end engelsk til tysk TED foredrag oversættelse opgave. Vi baserede vores løsning på vores sidste års indsendelse. Vi brugte en lidt ændret Transformer arkitektur med ResNet-lignende konvulutionslag forberedelse af lydindgangen til Transformer encoder. For at forbedre modellens oversættelseskvalitet introducerede vi to reguleringsteknikker og trænede i maskinoversat Librispeech corpus ud over iwslt-corpus, TEDLIUM2 og Must_C corpora. Vores bedste model scorede næsten 3 BLEU højere end sidste års model. For at segmentere 2020 testsæt brugte vi nøjagtig samme procedure som sidste år.</abstract_da>
      <abstract_nl>We namen deel aan de offline End-to-End Engels naar Duits TED lezingen vertaaltaak. We baseerden onze oplossing op onze inzending van vorig jaar. We gebruikten een licht gewijzigde Transformer architectuur met ResNet-achtige convolutionele laag die de audio-ingang naar Transformer encoder voorbereidde. Om de kwaliteit van de vertaling van het model te verbeteren hebben we twee regularisatietechnieken geïntroduceerd en getraind op machinaal vertaalde Librispeech corpus naast iwslt-corpus, TEDLIUM2 en Must_C corpora. Ons beste model scoorde bijna drie BLEU hoger dan vorig jaar. Voor het segmenteren van 2020 testset hebben we exact dezelfde procedure gebruikt als vorig jaar.</abstract_nl>
      <abstract_hr>Pridružili smo se poslu za prevod na njemačkim TED predavanjima. Započeli smo našu rješenje na podnošenju prošle godine. Koristili smo malu izmijenjenu arhitekturu transformera sa konvolucionalnim slojem poput ResNet a pripremajući audio ulaz za koder transformera. Da bi poboljšali kvalitet prevoda modela, predstavili smo dvije regularizacijske tehnike i obučene na strojevima prevedeno Librispeech corpus, osim iwslt-corpus, TEDLIUM2 i Must_C corpora. Naš najbolji model rezultirao je skoro 3 BLEU viši od prošlogodišnjeg modela. Za snimanje testa za segment 2020, koristili smo tačno isti postupak kao i prošle godine.</abstract_hr>
      <abstract_de>Wir haben an der Offline-End-Übersetzungsaufgabe für TED-Vorträge teilgenommen. Unsere Lösung stützten wir auf die Vorlage des Vorjahres. Wir verwendeten eine leicht veränderte Transformer-Architektur mit ResNet-ähnlicher Faltungsschicht, die den Audioeingang zum Transformer-Encoder vorbereitete. Um die Übersetzungsqualität des Modells zu verbessern, führten wir zwei Regularisierungstechniken ein und trainierten zusätzlich zu iwslt-corpus, TEDLIUM2 und Must_C Korpora am maschinell übersetzten Librispeech Korpus. Unser bestes Modell erzielte fast drei BLEU höher als das Vorjahresmodell. Für das Segment 2020-Testset haben wir genau das gleiche Verfahren wie im Vorjahr verwendet.</abstract_de>
      <abstract_ko>우리는 오프라인에서 끝까지 영어부터 독일어 TED 강좌 번역 임무에 참가했다.우리는 작년에 제출한 해결 방안을 바탕으로 한다.우리는 ResNet과 비슷한 볼륨층을 가지고 변환기 인코더를 위해 오디오 입력을 준비하는 약간 바뀐 변환기 구조를 사용했다.모델의 번역 품질을 향상시키기 위해 iwslt 자료 라이브러리, TEDLIUM2와Must C 자료 라이브러리 외에 두 가지 정규화 기술을 도입하고 기계 번역의Librispeech 자료 라이브러리에서 훈련을 실시했다.우리의 가장 좋은 차종의 득점은 작년 차종보다 3개의 BLEU 가까이 높다.2020 테스트집을 분할하기 위해 작년과 똑같은 프로그램을 사용했다.</abstract_ko>
      <abstract_sw>Tumeshiriki katika kazi ya tafsiri ya TED katika mtandao wa mwisho-to-End English hadi Ujerumani. We based our solution on our last year's submission.  Tulitumia jengo lililobadilishwa kidogo la Transformer na kiwango kinachofanana na ResNet ambacho kiliandaa matokeo ya sauti kwenye kodi la Transformer. Kuboresha ubora wa utafsiri wa mifano tulianzisha mbinu mbili za kudhibiti na kufundishwa kwenye mashine yanayotafsiriwa Librispeech pamoja na iwslt-corpus, TEDLIUM2 na Must_C. Mfano wetu bora uliorodhesha takribani BLEU 3 zaidi ya mtindo wa mwaka jana. Katika sehemu ya jaribio la 2020 tulitumia utaratibu huo sawa kama mwaka jana.</abstract_sw>
      <abstract_fa>ما در کار ترجمه‌های TED به انگلیسی End-to-End شرکت کردیم. ما راه حل خود را بر اساس تسلیم سال گذشته‌مان بنیاد می‌دهیم. ما از یک معماری تغییر تبدیل کننده کوچک استفاده کردیم با طبقه متغییر مانند ResNet برای آماده کردن ورودهای صوتی به رمندۀ تغییر دهنده. برای بهتر کردن کیفیت ترجمه مدل، دو تکنیک قانونی را معرفی کردیم و بر ماشین ترجمه شده‌ایم Librispeech corpus در addition to iwslt-corpus, TEDLIUM2 و Must_C corpora. بهترین مدل ما تقریباً سه بلوپ بالاتر از مدل سال گذشته بود. برای برقطه آزمایش ۲۰۰۲، ما دقیقاً همان روش را با سال گذشته استفاده کردیم.</abstract_fa>
      <abstract_af>Ons het deel in die offline End-to-End Engels na Duitse TED-leksies vertaling taak geneem. Ons gebaseer on s oplossing op ons laaste jaar se onderwerp. Ons gebruik 'n bietjie verander Transformer Arkitektuur met ResNet- like konvolusionele laag wat die oudio invoer na Transformer enkoder berei het. Om die model se kwaliteit van vertaling te verbeter, het on s twee regularisasie teknike ingevoer en op masjien vertaling Librispeech corpus in byvoeg by iwslt-corpus, TEDLIUM2 en Must_C corpora. Ons beste model het amper 3 BLES hoër as die laaste jaar se model getel. To segment 2020 test set we used exactly the same procedure as last year.</abstract_af>
      <abstract_sq>Ne morëm pjesë në detyrën e përkthimit të lezioneve të TED-it nga fundi në fund. Ne bazuam zgjidhjen tonë në dorëzimin tonë të vitit të kaluar. Ne përdorëm një arkitekturë pak të ndryshuar Transformer me shtresë konvolutive si ResNet duke përgatitur hyrjen audio në koduesin Transformer. Për të përmirësuar cilës in ë e përkthimit të modelit ne futëm dy teknika rregulluese dhe stërvitëm në Librispeech corpus përkthyer nga makina përveç iwslt-corpus, TEDLIUM2 dhe Must_C corpora. Our best model scored almost 3 BLEU higher than last year's model.  Për testin e segmentit 2020 kemi përdorur saktësisht të njëjtën procedurë si vitin e kaluar.</abstract_sq>
      <abstract_tr>Biz offline End-to-End Iňlisçe nemesçe TED sanlarynyň terjime täbligine böldik. Biz geçen ýylymyzyň teslim etmäge çözümüzü daýadyk. ResNet'e görkezilen bir şekilde üýtgeden bir Transformer arhitektegi ullandık. Ses girişini Transformer ködere taýýarlanýar. Modeliniň terjime kalitesini geliştirmek üçin biz 2 düzenlemek tekniklerini we makine terjime edilen Librispeech corpus iwslt-corpus, TEDLIUM2 we Must_C corpora dahil edildi. Biziň iň gowy nusgymyz geçen ýylyň nusgasyndan 3 BLEU köp sany boldy. 2020-nji ýyl barlamak üçin biz geçen ýyl ýaly edil bir prosedyny ulandyk.</abstract_tr>
      <abstract_am>ወደ አርንሳይ የመጨረሻ-ወደ-መጨረሻ እንግሊዘኛ ወደ ጀርመን ቴድ ትርጓሜዎችን ትርጉም አድራሻ ተጋርተናል፡፡ የቀድሞው ዓመታት ስልጣን ላይ መፍትረታችንን አቀረብን፡፡ የድምፅ ድምፅ አቀማመጥ ወደ ተርጓሚው ኮድ ማዘጋጀት በResNet የሚመስል አካውንት የተለወጠውን የፍጥረት መሠረት አቀራጠልን፡፡ በይslt-corpus፣ TEDLIUM2 እና ሙሉ_C ኮፖርት በቀር ሁለት የሥርዓት አስተዳደር ቴክኖቶችን እና በመሣሪያዎች ላይ የተዘጋጀን የሊብሪፓክ ኮፕስ እና የተማረርን የሞዴል ጥያቄን ለማድረግ ነው፡፡ የቀድሞው ዓመት በ3 ቢልዩን የበለጠ ሞዴል ተቃውሞ ነበር፡፡ በ2020 ፈተና ለመክፈት እንደገና ባለፈው ዓመታት አንድ ሥርዓት ተጠቃሚ ነበር፡፡</abstract_am>
      <abstract_az>Biz "End-to-End" İngilizce dilində Almanca TED leksiyalarının çevirilməsinə bölüşdük. Biz s on il müs əlmanlarımızın çətinliklərinə dayandıq. Biz ResNet kimi konvolucional səviyyə ilə biraz dəyişdirilmiş Transformer arhitektarını kullandıq. Transformer kodlayıcısına audio girişini hazırlamaq üçün. Modelin tərcümünün keyfiyyətini yaxşılaşdırmaq üçün iki düzgünlük tekniklərini təyin etdik və maşına Librispeech korpusu iwslt-corpus, TEDLIUM2 və Must_C korporasını təhsil etdik. Bizim ən yaxşı modellərimiz son il modelindən az qala 3 BLEU yüksək dəyişdi. 2020-ci segment test təyin etdik ki, biz dünən il ilə həqiqətən də eyni proqramı kullandıq.</abstract_az>
      <abstract_hy>Մենք մասնակցեցինք անգլերենի վերջ-վերջ անգլերենի և գերմանացի TED-ի դասընթացների թարգմանման գործին: Մենք հիմնեցինք մեր լուծումը անցյալ տարվա ներկայացման վրա: We used a slightly altered Transformer architecture with ResNet-like convolutional layer preparing the audio input to Transformer encoder.  Մոդելի թարգմանման որակը բարելավելու համար մենք ներկայացրեցինք երկու ռեգուլարիզացիոն տեխնիկա և ուսուցանում էինք մեքենայի թարգմանված գրադարձ կորպոս, ավելացնելով iսթ-կորպոս, TEDլիում2 և Մուստ_C կորպորա Մեր լավագույն մոդելը գնահատել է մոտ երեք բլեուզ ավելի բարձր, քան անցյալ տարվա մոդելը: To segment 2020 test set we used exactly the same procedure as last year.</abstract_hy>
      <abstract_ca>Vam participar en la tasca de traducció del final al final de l'anglès a les conferències alemanes TED. Vam basar la nostra solució en la presentació de l'any passat. Vam utilitzar una arquitectura Transformer una mica canviada amb capa convolucionada com ResNet preparant l'entrada d'àudio al codificador Transformer. Per millorar la qualitat de la traducció del model vam introduir dues tècniques de regularizació i vam entrenar en Librispeech corpus traduit per màquina a més d'iwslt-corpus, TEDLIUM2 i Must_C corpora. El nostre millor model va marcar gairebé 3 BLEU més alts que el model de l'any passat. Per al conjunt de tests del segment 2020 vam utilitzar exactament el mateix procediment que l'any passat.</abstract_ca>
      <abstract_bn>আমরা অফ-লাইন শেষ-থেকে ইংরেজিতে জার্মান টেড ভাষণের অনুবাদ কাজে অংশ নিয়েছি। আমরা গত বছরের আত্মসমর্পণের উপর আমাদের সমাধান ভিত্তি করেছি। আমরা রেসেন্টের মতো বিশ্বাসী স্তরের সাথে একটি সামান্য পরিবর্তনের পরিবর্তন কাঠামো ব্যবহার করেছি ট্রান্সফার্নার এনকোডারে অডিও এই মডেলের অনুবাদের মান উন্নত করার জন্য আমরা দুটি নিয়ন্ত্রণ প্রযুক্তি তুলে ধরেছি এবং ইইজসল-কোর্পাস, টেডিলিউম্২ এবং মেশিন অনুবাদ করা লিব্রিস্পেচ কোর্পাসের আমাদের সর্বোচ্চ মডেল গত বছরের মডেল থেকে প্রায় ৩ বিলিউ বেশি। ২০২০ পরীক্ষা বিভাগের জন্য আমরা গত বছর পর্যন্ত একই প্রক্রিয়া ব্যবহার করেছিলাম।</abstract_bn>
      <abstract_cs>Zúčastnili jsme se offline překladu TED přednášek z angličtiny do němčiny End-to-End. Naše řešení jsme založili na loňském předložení. Použili jsme mírně změněnou architekturu Transformeru s konvoluční vrstvou podobnou ResNet připravující zvukový vstup do snímače Transformeru. Pro zlepšení kvality překladu modelu jsme představili dvě regularizační techniky a trénovali na strojově přeloženém korpusu Librispeech kromě korpusů iwslt-corpus, TEDLIUM2 a Must_C. Náš nejlepší model dosáhl téměř o tři BLEU vyšší než loňský model. Pro segment 2020 testovací sady jsme použili přesně stejný postup jako loni.</abstract_cs>
      <abstract_bs>Pridružili smo se poslu za prevod na njemačkim TED predavanjima. Na osnovu našeg rješenja na posljednjoj godini podnošenja. Koristili smo malu izmjenjenu arhitekturu transformera sa konvolucionalnim slojem poput ResNet a, pripremajući audio ulaz u koder transformera. Da bi poboljšali kvalitet prevoda modela, predstavili smo dvije regularizacijske tehnike i obučene na mašini prevedeno Librispeech corpus osim iwslt-corpus, TEDLIUM2 i Must_C corpora. Naš najbolji model je skoro 3 BLEU viši od prošlogodišnjeg modela. Za snimanje testa za segment 2020, koristili smo tačno iste procedure kao i prošle godine.</abstract_bs>
      <abstract_fi>Osallistuimme offline End-to-End English to German TED luentojen kääntämiseen. Perustimme ratkaisumme viime vuoden toimitukseen. Käytimme hieman muunneltua Transformer-arkkitehtuuria ResNetin kaltaisella konvolutionaalisella kerroksella, joka valmisti äänituloa Transformer-kooderiin. Mallin kääntämisen laadun parantamiseksi otimme käyttöön kaksi laillistustekniikkaa ja koulutimme konekäännettyä Librispeech-korpusta iwslt-korpusen, TEDLIUM2- ja Must_C-korpusten lisäksi. Paras mallimme sai lähes 3 BLEU enemmän kuin viime vuoden malli. Segmentin 2020 testisarjaan käytimme täsmälleen samaa menettelyä kuin viime vuonna.</abstract_fi>
      <abstract_id>Kami mengambil bagian dalam tugas terjemahan dari Bahasa Inggris akhir ke akhir ke kursus TED Jerman. Kami mendasarkan solusi kami pada penyerahan tahun lalu kami. Kami menggunakan arsitektur Transformer yang sedikit berubah dengan lapisan konvolusi seperti ResNet mempersiapkan input audio ke pengekode Transformer. Untuk meningkatkan kualitas terjemahan model kami memperkenalkan dua teknik regularisasi dan dilatih di mesin terjemahan Librispeech corpus tambah iwslt-corpus, TEDLIUM2 dan Must_C corpora. Model terbaik kami skor hampir 3 BLEU lebih tinggi dari model tahun lalu. Untuk set tes segmen 2020 kami menggunakan prosedur yang sama seperti tahun lalu.</abstract_id>
      <abstract_et>Osalesime võrguühenduseta inglise keele-saksa TED loengute tõlkimise ülesandes. Me põhinesime oma lahendusel eelmisel aastal esitatud pakkumisel. Kasutasime veidi muudetud Transformeri arhitektuuri ResNeti sarnase konvolutsioonikihiga, mis valmistas helisisendi Transformeri kodeerijaks ette. Mudeli tõlkekvaliteedi parandamiseks tutvustasime lisaks iwslt-korpusele, TEDLIUM2 ja Must_C korpusele ka masintõlgitud Librispeech korpusega. Meie parim mudel sai peaaegu 3 BLEU kõrgema kui eelmisel aastal. Segmendi 2020 testikomplekti jaoks kasutasime täpselt sama protseduuri kui eelmisel aastal.</abstract_et>
      <abstract_ha>@ info: whatsthis Mun ƙaddara suluyinmu a kan Musuluncin shekara ta shida. We used a slightly altered Transformer architecture with ResNet-like convolutional layer preparing the audio input to Transformer encoder.  To improve tsarin motel'in fassarar, we introduce technical biyu masu tsaro kuma an sanar da shi a kan mashine ta fassar Librispech Cornas, addition to iwslt-Corbas, TeDLIUM2 and Must_C Corpo. Babu misalinmu na ƙari takin BLEU ya fi girma daga misalin shekara ta shida. Ga rabin jarrabi 2020, mun yi amfani da daidai jarrabo kamar shekara ta shida.</abstract_ha>
      <abstract_sk>Sodelovali smo pri prevajanju TED predavanj iz angleščine do nemščine brez povezave. Našo rešitev smo temeljili na lanskem predložitvi. Uporabili smo rahlo spremenjeno arhitekturo transformatorjev s konvolucijskim slojem, podobnim ResNetu, ki je pripravil avdio vhod v kodirnik transformatorjev. Za izboljšanje kakovosti prevajanja modela smo poleg iwslt-korpusa, TEDLIUM2 in Must_C korpusa uvedli dve tehniki regularizacije in usposabljali strojno prevedenega Librispeech korpusa. Naš najboljši model je ocenil skoraj 3 BLEU višje od lanskega modela. Za segment test set 2020 smo uporabili popolnoma enak postopek kot lani.</abstract_sk>
      <abstract_jv>Awak dh챕w챕 wis ak챔h n챗m챗n ning "End-to-End" -Inggris kanggo nganggo barang alaman kanggo tarjamahan "Tom". Awakdh챕w챕 ngerti perusahaan dh챕w챕 nang kana dh챕w챕 We used a little change Transformer architecture with Resnet-like convolution layer Reading the sound input to Transformer koder. Mbak penting nggawe kalitas model kebebasan itoleh dumadhi, awak dh챕w챕 ngerasai t챕kno sing berarti ujak karo perusahaan dibutuhke "Library" sing berarti itoleh bantuan, lan ujak-ujak i "Iwakken telu" lan "Must_C" Rasan챕 sing paling dh챕w챕, ditambah sing kator 3 BEL kuwi model sing paling tau. Genjer-genjer saiki gerang t챔st 2020 sampeyan, kita ngaweh penggunan ngono cah-cah dumadhi tau.</abstract_jv>
      <abstract_bo>ང་ཚོས་དྲ་རྒྱའི་ནང་དུ་End-to-End དབྱིན་ཡིག ང་ཚོས་རྗེས་མའི་ལོ་ངོ་མའི་བསམ་བློ་གཏོང་བའི་ཐབས་ཤེས་དེ་གཞི་རྟེན་བྱེད་ཀྱི་ཡོད། We used a slightly altered Transformer architecture with ResNet-like convolutional layer preparing the audio input to Transformer encoder. ང་ཚོའི་མ་དབྱིབས་གྱི་དཔེ་དབྱིབས་ལ་སྒྲིག་འགོད་ཀྱི་རིམ་པ་སྒྲིག ང་ཚོའི་མ་དབྱིབས་འདས་པའི་ལོ་རྣམ་གྲངས་སྔོན་གྱི་མ་དབྱིབས་ཉུང་བའི་ཚད་ལྡན་གྲངས་ཀ་འདི་ཉེ་བར་གསུ ལོ་གྲངས་སྔོན་2020་ཡི་བརྟག་ཞིབ་ཚད་ལ་རྗེས་སུ་ང་ཚོས་དུས་མཐུན་གྱི་ཐབས་ལམ་ལ་མཚུངས་པ་ཡིན།</abstract_bo>
      <abstract_he>השתתפנו בתפקיד התרגום של "סוף עד סוף" לאנגלית לרצאות TED הגרמניות. הבססנו את הפתרון שלנו על ההכנעה שלנו בשנה שעברה. השתמשנו בארכיטקטורה מעט משתנה עם שכבה משתנה דומה לרסנט שמכינה את הכניסה של הקולנוע לקודר. כדי לשפר את איכות התרגום של המודל הכרנו שתי טכניקות רגילות ואימנו על מכונת התרגם Librispeech corpus בנוסף ל iwslt-corpus, TEDLIUM2 ו Must_C corpora. המודל הטוב ביותר שלנו קיבל כמעט 3 BLEU גבוה יותר מהמודל של שנה שעברה. To segment 2020 test set we used exactly the same procedure as last year.</abstract_he>
      </paper>
    <paper id="10">
      <title>The University of Helsinki Submission to the IWSLT2020 Offline SpeechTranslation Task<fixed-case>U</fixed-case>niversity of <fixed-case>H</fixed-case>elsinki Submission to the <fixed-case>IWSLT</fixed-case>2020 Offline <fixed-case>S</fixed-case>peech<fixed-case>T</fixed-case>ranslation Task</title>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Mikko</first><last>Aulamo</last></author>
      <author><first>Umut</first><last>Sulubacak</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>95–102</pages>
      <abstract>This paper describes the University of Helsinki Language Technology group’s participation in the IWSLT 2020 offline speech translation task, addressing the translation of English audio into German text. In line with this year’s task objective, we train both cascade and end-to-end systems for spoken language translation. We opt for an end-to-end multitasking architecture with shared internal representations and a cascade approach that follows a standard procedure consisting of ASR, correction, and MT stages. We also describe the experiments that served as a basis for the submitted <a href="https://en.wikipedia.org/wiki/System">systems</a>. Our experiments reveal that multitasking training with shared internal representations is not only possible but allows for <a href="https://en.wikipedia.org/wiki/Knowledge_transfer">knowledge-transfer</a> across modalities.</abstract>
      <url hash="0b46f3a0">2020.iwslt-1.10</url>
      <doi>10.18653/v1/2020.iwslt-1.10</doi>
      <video href="http://slideslive.com/38929617" />
      <bibkey>vazquez-etal-2020-university</bibkey>
    </paper>
    <paper id="11">
      <title>The AFRL IWSLT 2020 Systems : Work-From-Home Edition<fixed-case>AFRL</fixed-case> <fixed-case>IWSLT</fixed-case> 2020 Systems: Work-From-Home Edition</title>
      <author><first>Brian</first><last>Ore</last></author>
      <author><first>Eric</first><last>Hansen</last></author>
      <author><first>Tim</first><last>Anderson</last></author>
      <author><first>Jeremy</first><last>Gwinnup</last></author>
      <pages>103–108</pages>
      <abstract>This report summarizes the Air Force Research Laboratory (AFRL) submission to the offline spoken language translation (SLT) task as part of the IWSLT 2020 evaluation campaign. As in previous years, we chose to adopt the cascade approach of using separate systems to perform <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech activity detection</a>, <a href="https://en.wikipedia.org/wiki/Speech_recognition">automatic speech recognition</a>, <a href="https://en.wikipedia.org/wiki/Sentence_segmentation">sentence segmentation</a>, and <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a>. All systems were neural based, including a fully-connected neural network for speech activity detection, a Kaldi factorized time delay neural network with recurrent neural network (RNN) language model rescoring for speech recognition, a bidirectional RNN with attention mechanism for sentence segmentation, and transformer networks trained with OpenNMT and Marian for machine translation. Our primary submission yielded BLEU scores of 21.28 on tst2019 and 23.33 on tst2020.</abstract>
      <url hash="5dca6d62">2020.iwslt-1.11</url>
      <doi>10.18653/v1/2020.iwslt-1.11</doi>
      <video href="http://slideslive.com/38929615" />
      <bibkey>ore-etal-2020-afrl</bibkey>
    </paper>
    <paper id="13">
      <title>OPPO’s Machine Translation System for the IWSLT 2020 Open Domain Translation Task<fixed-case>OPPO</fixed-case>’s Machine Translation System for the <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation Task</title>
      <author><first>Qian</first><last>Zhang</last></author>
      <author><first>Xiaopu</first><last>Li</last></author>
      <author><first>Dawei</first><last>Dang</last></author>
      <author><first>Tingxun</first><last>Shi</last></author>
      <author><first>Di</first><last>Ai</last></author>
      <author><first>Zhengshan</first><last>Xue</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <pages>114–121</pages>
      <abstract>In this paper, we demonstrate our <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation system</a> applied for the Chinese-Japanese bidirectional translation task (aka. open domain translation task) for the IWSLT 2020. Our model is based on Transformer (Vaswani et al., 2017), with the help of many popular, widely proved effective data preprocessing and augmentation methods. Experiments show that these <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> can improve the <a href="https://en.wikipedia.org/wiki/Baseline_(medicine)">baseline model</a> steadily and significantly.</abstract>
      <url hash="3a418dac">2020.iwslt-1.13</url>
      <doi>10.18653/v1/2020.iwslt-1.13</doi>
      <video href="http://slideslive.com/38929611" />
      <bibkey>zhang-etal-2020-oppos</bibkey>
    </paper>
    <paper id="14">
      <title>Character Mapping and Ad-hoc Adaptation : Edinburgh’s IWSLT 2020 Open Domain Translation System<fixed-case>E</fixed-case>dinburgh’s <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation System</title>
      <author><first>Pinzhen</first><last>Chen</last></author>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Ulrich</first><last>Germann</last></author>
      <pages>122–129</pages>
      <abstract>This paper describes the University of Edinburgh’s neural machine translation systems submitted to the IWSLT 2020 open domain JapaneseChinese translation task. On top of commonplace techniques like <a href="https://en.wikipedia.org/wiki/Lexical_analysis">tokenisation</a> and corpus cleaning, we explore character mapping and unsupervised decoding-time adaptation. Our techniques focus on leveraging the provided data, and we show the positive impact of each technique through the gradual improvement of BLEU.<tex-math>\leftrightarrow</tex-math>Chinese translation task. On top of commonplace techniques like tokenisation and corpus cleaning, we explore character mapping and unsupervised decoding-time adaptation. Our techniques focus on leveraging the provided data, and we show the positive impact of each technique through the gradual improvement of BLEU.</abstract>
      <url hash="cfaba852">2020.iwslt-1.14</url>
      <doi>10.18653/v1/2020.iwslt-1.14</doi>
      <video href="http://slideslive.com/38929590" />
      <bibkey>chen-etal-2020-character</bibkey>
      <pwccode url="https://github.com/marian-nmt/marian" additional="false">marian-nmt/marian</pwccode>
    </paper>
    <paper id="15">
      <title>CASIA’s System for IWSLT 2020 Open Domain Translation<fixed-case>CASIA</fixed-case>’s System for <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation</title>
      <author><first>Qian</first><last>Wang</last></author>
      <author><first>Yuchen</first><last>Liu</last></author>
      <author><first>Cong</first><last>Ma</last></author>
      <author><first>Yu</first><last>Lu</last></author>
      <author><first>Yining</first><last>Wang</last></author>
      <author><first>Long</first><last>Zhou</last></author>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>130–139</pages>
      <abstract>This paper describes the CASIA’s system for the IWSLT 2020 open domain translation task. This year we participate in both ChineseJapanese and JapaneseChinese translation tasks. Our <a href="https://en.wikipedia.org/wiki/System">system</a> is neural machine translation system based on Transformer model. We augment the training data with knowledge distillation and back translation to improve the <a href="https://en.wikipedia.org/wiki/Translation">translation</a> performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on <a href="https://en.wikipedia.org/wiki/Software_development_process">development data</a> with different model settings and different <a href="https://en.wikipedia.org/wiki/Data_processing">data processing techniques</a>.</abstract>
      <url hash="d120b4ec">2020.iwslt-1.15</url>
      <doi>10.18653/v1/2020.iwslt-1.15</doi>
      <video href="http://slideslive.com/38929589" />
      <bibkey>wang-etal-2020-casias</bibkey>
    </paper>
    <paper id="16">
      <title>Deep Blue Sonics’ Submission to IWSLT 2020 Open Domain Translation Task<fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation Task</title>
      <author><first>Enmin</first><last>Su</last></author>
      <author><first>Yi</first><last>Ren</last></author>
      <pages>140–144</pages>
      <abstract>We present in this report our submission to IWSLT 2020 Open Domain Translation Task. We built a data pre-processing pipeline to efficiently handle large noisy web-crawled corpora, which boosts the BLEU score of a widely used transformer model in this translation task. To tackle the open-domain nature of this task, back- translation is applied to further improve the <a href="https://en.wikipedia.org/wiki/Translation">translation</a> performance.</abstract>
      <url hash="514a7105">2020.iwslt-1.16</url>
      <doi>10.18653/v1/2020.iwslt-1.16</doi>
      <video href="http://slideslive.com/38929592" />
      <bibkey>su-ren-2020-deep</bibkey>
    </paper>
    <paper id="19">
      <title>ISTIC’s Neural Machine Translation System for IWSLT’2020<fixed-case>ISTIC</fixed-case>’s Neural Machine Translation System for <fixed-case>IWSLT</fixed-case>’2020</title>
      <author><first>Jiaze</first><last>Wei</last></author>
      <author><first>Wenbin</first><last>Liu</last></author>
      <author><first>Zhenfeng</first><last>Wu</last></author>
      <author><first>You</first><last>Pan</last></author>
      <author><first>Yanqing</first><last>He</last></author>
      <pages>158–165</pages>
      <abstract>This paper introduces technical details of machine translation system of Institute of Scientific and Technical Information of China (ISTIC) for the 17th International Conference on Spoken Language Translation (IWSLT 2020). ISTIC participated in both translation tasks of the Open Domain Translation track : Japanese-to-Chinese MT task and Chinese-to-Japanese MT task. The paper mainly elaborates on the <a href="https://en.wikipedia.org/wiki/Model-driven_architecture">model framework</a>, <a href="https://en.wikipedia.org/wiki/Data_preprocessing">data preprocessing methods</a> and decoding strategies adopted in our <a href="https://en.wikipedia.org/wiki/System">system</a>. In addition, the <a href="https://en.wikipedia.org/wiki/System">system</a> performance on the development set are given under different settings.</abstract>
      <url hash="48f513f1">2020.iwslt-1.19</url>
      <doi>10.18653/v1/2020.iwslt-1.19</doi>
      <bibkey>wei-etal-2020-istics</bibkey>
    </paper>
    <paper id="23">
      <title>The HW-TSC Video Speech Translation System at IWSLT 2020<fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> Video Speech Translation System at <fixed-case>IWSLT</fixed-case> 2020</title>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Yao</first><last>Deng</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Ning</first><last>Xie</last></author>
      <author><first>Xiaochun</first><last>Li</last></author>
      <author><first>Jiaxian</first><last>Guo</last></author>
      <pages>187–190</pages>
      <abstract>The paper presents details of our <a href="https://en.wikipedia.org/wiki/System">system</a> in the IWSLT Video Speech Translation evaluation. The <a href="https://en.wikipedia.org/wiki/System">system</a> works in a cascade form, which contains three <a href="https://en.wikipedia.org/wiki/Modular_programming">modules</a> : 1) A proprietary ASR system. 2) A disfluency correction system aims to remove interregnums or other disfluent expressions with a fine-tuned BERT and a series of rule-based algorithms. 3) An NMT System based on the Transformer and trained with massive publicly available corpus.</abstract>
      <url hash="c178ea46">2020.iwslt-1.23</url>
      <doi>10.18653/v1/2020.iwslt-1.23</doi>
      <video href="http://slideslive.com/38929616" />
      <bibkey>wang-etal-2020-hw</bibkey>
    </paper>
    <paper id="24">
      <title>CUNI Neural ASR with Phoneme-Level Intermediate Step for ~ Non-Native ~ SLT at IWSLT 2020<fixed-case>CUNI</fixed-case> Neural <fixed-case>ASR</fixed-case> with Phoneme-Level Intermediate Step for~<fixed-case>N</fixed-case>on-<fixed-case>N</fixed-case>ative~<fixed-case>SLT</fixed-case> at <fixed-case>IWSLT</fixed-case> 2020</title>
      <author><first>Peter</first><last>Polák</last></author>
      <author><first>Sangeet</first><last>Sagar</last></author>
      <author><first>Dominik</first><last>Macháček</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>191–199</pages>
      <abstract>In this paper, we present our submission to the Non-Native Speech Translation Task for IWSLT 2020. Our main contribution is a proposed speech recognition pipeline that consists of an <a href="https://en.wikipedia.org/wiki/Acoustic_model">acoustic model</a> and a phoneme-to-grapheme model. As an <a href="https://en.wikipedia.org/wiki/Intermediate_representation">intermediate representation</a>, we utilize <a href="https://en.wikipedia.org/wiki/Phoneme">phonemes</a>. We demonstrate that the proposed <a href="https://en.wikipedia.org/wiki/Pipeline_transport">pipeline</a> surpasses commercially used automatic speech recognition (ASR) and submit it into the ASR track. We complement this ASR with off-the-shelf MT systems to take part also in the speech translation track.</abstract>
      <url hash="c7893255">2020.iwslt-1.24</url>
      <doi>10.18653/v1/2020.iwslt-1.24</doi>
      <bibkey>polak-etal-2020-cuni</bibkey>
    </paper>
    <paper id="25">
      <title>ELITR Non-Native Speech Translation at IWSLT 2020<fixed-case>ELITR</fixed-case> Non-Native Speech Translation at <fixed-case>IWSLT</fixed-case> 2020</title>
      <author><first>Dominik</first><last>Macháček</last></author>
      <author><first>Jonáš</first><last>Kratochvíl</last></author>
      <author><first>Sangeet</first><last>Sagar</last></author>
      <author><first>Matúš</first><last>Žilinec</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Thai-Son</first><last>Nguyen</last></author>
      <author><first>Felix</first><last>Schneider</last></author>
      <author><first>Philip</first><last>Williams</last></author>
      <author><first>Yuekun</first><last>Yao</last></author>
      <pages>200–208</pages>
      <abstract>This paper is an ELITR system submission for the non-native speech translation task at IWSLT 2020. We describe systems for offline ASR, real-time ASR, and our cascaded approach to offline SLT and real-time SLT. We select our primary candidates from a pool of pre-existing systems, develop a new end-to-end general ASR system, and a hybrid ASR trained on non-native speech. The provided small validation set prevents us from carrying out a complex validation, but we submit all the unselected candidates for contrastive evaluation on the test set.</abstract>
      <url hash="e32ca293">2020.iwslt-1.25</url>
      <doi>10.18653/v1/2020.iwslt-1.25</doi>
      <video href="http://slideslive.com/38929595" />
      <bibkey>machacek-etal-2020-elitr</bibkey>
    </paper>
    <paper id="29">
      <title>Neural Simultaneous Speech Translation Using Alignment-Based Chunking</title>
      <author><first>Patrick</first><last>Wilken</last></author>
      <author><first>Tamer</first><last>Alkhouli</last></author>
      <author><first>Evgeny</first><last>Matusov</last></author>
      <author><first>Pavel</first><last>Golik</last></author>
      <pages>237–246</pages>
      <abstract>In simultaneous machine translation, the objective is to determine when to produce a partial translation given a continuous stream of source words, with a trade-off between <a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a> and <a href="https://en.wikipedia.org/wiki/Quality_(business)">quality</a>. We propose a neural machine translation (NMT) model that makes dynamic decisions when to continue feeding on input or generate output words. The model is composed of two main <a href="https://en.wikipedia.org/wiki/Component-based_software_engineering">components</a> : one to dynamically decide on ending a source chunk, and another that translates the consumed chunk. We train the components jointly and in a manner consistent with the <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference conditions</a>. To generate chunked training data, we propose a method that utilizes <a href="https://en.wikipedia.org/wiki/Word_alignment">word alignment</a> while also preserving enough context. We compare models with bidirectional and unidirectional encoders of different depths, both on real speech and text input. Our results on the IWSLT 2020 English-to-German task outperform a wait-k baseline by 2.6 to 3.7 % BLEU absolute.</abstract>
      <url hash="34bea1f1">2020.iwslt-1.29</url>
      <doi>10.18653/v1/2020.iwslt-1.29</doi>
      <video href="http://slideslive.com/38929608" />
      <bibkey>wilken-etal-2020-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="33">
      <title>Efficient Automatic Punctuation Restoration Using Bidirectional Transformers with Robust Inference</title>
      <author><first>Maury</first><last>Courtland</last></author>
      <author><first>Adam</first><last>Faulkner</last></author>
      <author><first>Gayle</first><last>McElvain</last></author>
      <pages>272–279</pages>
      <abstract>Though people rarely speak in complete sentences, <a href="https://en.wikipedia.org/wiki/Punctuation">punctuation</a> confers many benefits to the readers of <a href="https://en.wikipedia.org/wiki/Transcription_(linguistics)">transcribed speech</a>. Unfortunately, most ASR systems do not produce punctuated output. To address this, we propose a <a href="https://en.wikipedia.org/wiki/Solution">solution</a> for automatic punctuation that is both cost efficient and easy to train. Our <a href="https://en.wikipedia.org/wiki/Solution">solution</a> benefits from the recent trend in fine-tuning transformer-based language models. We also modify the typical framing of this task by predicting <a href="https://en.wikipedia.org/wiki/Punctuation">punctuation</a> for sequences rather than individual tokens, which makes for more efficient <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">training</a> and <a href="https://en.wikipedia.org/wiki/Statistical_inference">inference</a>. Finally, we find that aggregating predictions across multiple context windows improves accuracy even further. Our best model achieves a new state of the art on benchmark data (TED Talks) with a combined F1 of 83.9, representing a 48.7 % relative improvement (15.3 absolute) over the previous state of the art.</abstract>
      <url hash="45ee1725">2020.iwslt-1.33</url>
      <doi>10.18653/v1/2020.iwslt-1.33</doi>
      <video href="http://slideslive.com/38929594" />
      <bibkey>courtland-etal-2020-efficient</bibkey>
    </paper>
    </volume>
</collection>