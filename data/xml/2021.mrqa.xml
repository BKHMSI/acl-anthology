<collection id="2021.mrqa">
  <volume id="1" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Machine Reading for Question Answering</booktitle>
      <editor><first>Adam</first><last>Fisch</last></editor>
      <editor><first>Alon</first><last>Talmor</last></editor>
      <editor><first>Danqi</first><last>Chen</last></editor>
      <editor><first>Eunsol</first><last>Choi</last></editor>
      <editor><first>Minjoon</first><last>Seo</last></editor>
      <editor><first>Patrick</first><last>Lewis</last></editor>
      <editor><first>Robin</first><last>Jia</last></editor>
      <editor><first>Sewon</first><last>Min</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="13500ebb">2021.mrqa-1.0</url>
      <bibkey>mrqa-2021-machine</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>MFAQ</fixed-case>: a Multilingual <fixed-case>FAQ</fixed-case> Dataset</title>
      <author><first>Maxime</first><last>De Bruyn</last></author>
      <author><first>Ehsan</first><last>Lotfi</last></author>
      <author><first>Jeska</first><last>Buhmann</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>1&#8211;13</pages>
      <abstract>In this paper, we present the first multilingual FAQ dataset publicly available. We collected around 6M FAQ pairs from the web, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges: duplication of content and uneven distribution of topics. We adopt a similar setup as Dense Passage Retrieval (DPR) and test various bi-encoders on this dataset. Our experiments reveal that a multilingual model based on XLM-RoBERTa achieves the best results, except for English. Lower resources languages seem to learn from one another as a multilingual model achieves a higher MRR than language-specific ones. Our qualitative analysis reveals the brittleness of the model on simple word changes. We publicly release our dataset, model, and training script.</abstract>
      <url hash="e9c21c43">2021.mrqa-1.1</url>
      <bibkey>de-bruyn-etal-2021-mfaq</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.1</doi>
      <pwccode url="https://github.com/clips/mfaq" additional="false">clips/mfaq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mfaq">MFAQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paq">PAQ</pwcdataset>
    </paper>
    <paper id="6">
      <title>Can Question Generation Debias Question Answering Models? A Case Study on Question&#8211;Context Lexical Overlap</title>
      <author><first>Kazutoshi</first><last>Shinoda</last></author>
      <author><first>Saku</first><last>Sugawara</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>63&#8211;72</pages>
      <abstract>Question answering (QA) models for reading comprehension have been demonstrated to exploit unintended dataset biases such as question&#8211;context lexical overlap. This hinders QA models from generalizing to under-represented samples such as questions with low lexical overlap. Question generation (QG), a method for augmenting QA datasets, can be a solution for such performance degradation if QG can properly debias QA datasets. However, we discover that recent neural QG models are biased towards generating questions with high lexical overlap, which can amplify the dataset bias. Moreover, our analysis reveals that data augmentation with these QG models frequently impairs the performance on questions with low lexical overlap, while improving that on questions with high lexical overlap. To address this problem, we use a synonym replacement-based approach to augment questions with low lexical overlap. We demonstrate that the proposed data augmentation approach is simple yet effective to mitigate the degradation problem with only 70k synthetic examples.</abstract>
      <url hash="361d679b">2021.mrqa-1.6</url>
      <bibkey>shinoda-etal-2021-question</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.6</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="9">
      <title>Eliciting Bias in Question Answering Models through Ambiguity</title>
      <author><first>Andrew</first><last>Mao</last></author>
      <author><first>Naveen</first><last>Raman</last></author>
      <author><first>Matthew</first><last>Shu</last></author>
      <author><first>Eric</first><last>Li</last></author>
      <author><first>Franklin</first><last>Yang</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>92&#8211;99</pages>
      <abstract>Question answering (QA) models use retriever and reader systems to answer questions. Reliance on training data by QA systems can amplify or reflect inequity through their responses. Many QA models, such as those for the SQuAD dataset, are trained and tested on a subset of Wikipedia articles which encode their own biases and also reproduce real-world inequality. Understanding how training data affects bias in QA systems can inform methods to mitigate inequity. We develop two sets of questions for closed and open domain questions respectively, which use ambiguous questions to probe QA models for bias. We feed three deep-learning-based QA systems with our question sets and evaluate responses for bias via the metrics. Using our metrics, we find that open-domain QA models amplify biases more than their closed-domain counterparts and propose that biases in the retriever surface more readily due to greater freedom of choice.</abstract>
      <url hash="66250172">2021.mrqa-1.9</url>
      <bibkey>mao-etal-2021-eliciting</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.9</doi>
      <pwccode url="https://github.com/axz5fy3e6fq07q13/emnlp_bias" additional="false">axz5fy3e6fq07q13/emnlp_bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="10">
      <title>Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer</title>
      <author><first>Ziqing</first><last>Yang</last></author>
      <author><first>Wentao</first><last>Ma</last></author>
      <author><first>Yiming</first><last>Cui</last></author>
      <author><first>Jiani</first><last>Ye</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Shijin</first><last>Wang</last></author>
      <pages>100&#8211;105</pages>
      <abstract>Multilingual pre-trained models have achieved remarkable performance on cross-lingual transfer learning. Some multilingual models such as mBERT, have been pre-trained on unlabeled corpora, therefore the embeddings of different languages in the models may not be aligned very well. In this paper, we aim to improve the zero-shot cross-lingual transfer performance by proposing a pre-training task named Word-Exchange Aligning Model (WEAM), which uses the statistical alignment information as the prior knowledge to guide cross-lingual word prediction. We evaluate our model on multilingual machine reading comprehension task MLQA and natural language interface task XNLI. The results show that WEAM can significantly improve the zero-shot performance.</abstract>
      <url hash="649f1b24">2021.mrqa-1.10</url>
      <bibkey>yang-etal-2021-bilingual</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.10</doi>
    </paper>
    <paper id="14">
      <title>Investigating Post-pretraining Representation Alignment for Cross-Lingual Question Answering</title>
      <author><first>Fahim</first><last>Faisal</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>133&#8211;148</pages>
      <abstract>Human knowledge is collectively encoded in the roughly 6500 languages spoken around the world, but it is not distributed equally across languages. Hence, for information-seeking question answering (QA) systems to adequately serve speakers of all languages, they need to operate cross-lingually. In this work we investigate the capabilities of multilingually pretrained language models on cross-lingual QA. We find that explicitly aligning the representations across languages with a post-hoc finetuning step generally leads to improved performance. We additionally investigate the effect of data size as well as the language choice in this fine-tuning step, also releasing a dataset for evaluating cross-lingual QA systems.</abstract>
      <url hash="b36f7425">2021.mrqa-1.14</url>
      <bibkey>faisal-anastasopoulos-2021-investigating</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.14</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="15">
      <title>Semantic Answer Similarity for Evaluating Question Answering Models</title>
      <author><first>Julian</first><last>Risch</last></author>
      <author><first>Timo</first><last>M&#246;ller</last></author>
      <author><first>Julian</first><last>Gutsch</last></author>
      <author><first>Malte</first><last>Pietsch</last></author>
      <pages>149&#8211;157</pages>
      <abstract>The evaluation of question answering models compares ground-truth annotations with model predictions. However, as of today, this comparison is mostly lexical-based and therefore misses out on answers that have no lexical overlap but are still semantically similar, thus treating correct answers as false. This underestimation of the true performance of models hinders user acceptance in applications and complicates a fair comparison of different models. Therefore, there is a need for an evaluation metric that is based on semantics instead of pure string similarity. In this short paper, we present SAS, a cross-encoder-based metric for the estimation of semantic answer similarity, and compare it to seven existing metrics. To this end, we create an English and a German three-way annotated evaluation dataset containing pairs of answers along with human judgment of their semantic similarity, which we release along with an implementation of the SAS metric and the experiments. We find that semantic similarity metrics based on recent transformer models correlate much better with human judgment than traditional lexical similarity metrics on our two newly created datasets and one dataset from related work.</abstract>
      <url hash="af261719">2021.mrqa-1.15</url>
      <bibkey>risch-etal-2021-semantic</bibkey>
      <doi>10.18653/v1/2021.mrqa-1.15</doi>
    </paper>
    </volume>
</collection>