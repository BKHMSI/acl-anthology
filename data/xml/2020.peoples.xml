<collection id="2020.peoples">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media</booktitle>
      <editor><first>Malvina</first><last>Nissim</last></editor>
      <editor><first>Viviana</first><last>Patti</last></editor>
      <editor><first>Barbara</first><last>Plank</last></editor>
      <editor><first>Esin</first><last>Durmus</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="9f683407">2020.peoples-1.0</url>
      <bibkey>peoples-2020-modeling</bibkey>
    </frontmatter>
    <paper id="4">
      <title>Persuasiveness of News Editorials depending on Ideology and Personality</title>
      <author><first>Roxanne</first><last>El Baff</last></author>
      <author><first>Khalid</first><last>Al Khatib</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <pages>29&#8211;40</pages>
      <abstract>News editorials aim to shape the opinions of their readership and the general public on timely controversial issues. The impact of an editorial on the reader&#8217;s opinion does not only depend on its content and style, but also on the reader&#8217;s profile. Previous work has studied the effect of editorial style depending on general political ideologies (liberals vs.conservatives). In our work, we dig deeper into the persuasiveness of both content and style, exploring the role of the intensity of an ideology (lean vs.extreme) and the reader&#8217;s personality traits (agreeableness, conscientiousness, extraversion, neuroticism, and openness). Concretely, we train content- and style-based models on New York Times editorials for different ideology- and personality-specific groups. Our results suggest that particularly readers with extreme ideology and non &#8220;role model&#8221; personalities are impacted by style. We further analyze the importance of various text features with respect to the editorials&#8217; impact, the readers&#8217; profile, and the editorials&#8217; geographical scope.</abstract>
      <url hash="735e8007">2020.peoples-1.4</url>
      <bibkey>el-baff-etal-2020-persuasiveness</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>K</fixed-case>an<fixed-case>CMD</fixed-case>: <fixed-case>K</fixed-case>annada <fixed-case>C</fixed-case>ode<fixed-case>M</fixed-case>ixed Dataset for Sentiment Analysis and Offensive Language Detection</title>
      <author><first>Adeep</first><last>Hande</last></author>
      <author><first>Ruba</first><last>Priyadharshini</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <pages>54&#8211;63</pages>
      <abstract>We introduce Kannada CodeMixed Dataset (KanCMD), a multi-task learning dataset for sentiment analysis and offensive language identification. The KanCMD dataset highlights two real-world issues from the social media text. First, it contains actual comments in code mixed text posted by users on YouTube social media, rather than in monolingual text from the textbook. Second, it has been annotated for two tasks, namely sentiment analysis and offensive language detection for under-resourced Kannada language. Hence, KanCMD is meant to stimulate research in under-resourced Kannada language on real-world code-mixed social media text and multi-task learning. KanCMD was obtained by crawling the YouTube, and a minimum of three annotators annotates each comment. We release KanCMD 7,671 comments for multitask learning research purpose.</abstract>
      <url hash="5df5d2e3">2020.peoples-1.6</url>
      <bibkey>hande-etal-2020-kancmd</bibkey>
    </paper>
    <paper id="7">
      <title>Contextual Augmentation of Pretrained Language Models for Emotion Recognition in Conversations</title>
      <author><first>Jonggu</first><last>Kim</last></author>
      <author><first>Hyeonmok</first><last>Ko</last></author>
      <author><first>Seoha</first><last>Song</last></author>
      <author><first>Saebom</first><last>Jang</last></author>
      <author><first>Jiyeon</first><last>Hong</last></author>
      <pages>64&#8211;73</pages>
      <abstract>Since language model pretraining to learn contextualized word representations has been proposed, pretrained language models have made success in many natural language processing tasks. That is because it is helpful to use individual contextualized representations of self-attention layers as to initialize parameters for downstream tasks. Yet, unfortunately, use of pretrained language models for emotion recognition in conversations has not been studied enough. We firstly use ELECTRA which is a state-of-the-art pretrained language model and validate the performance on emotion recognition in conversations. Furthermore, we propose contextual augmentation of pretrained language models for emotion recognition in conversations, which is to consider not only previous utterances, but also conversation-related information such as speakers, speech acts and topics. We classify information based on what the information is related to, and propose position of words corresponding to the information in the entire input sequence. To validate the proposed method, we conduct experiments on the DailyDialog dataset which contains abundant annotated information of conversations. The experiments show that the proposed method achieves state-of-the-art F1 scores on the dataset and significantly improves the performance.</abstract>
      <url hash="c1492b63">2020.peoples-1.7</url>
      <bibkey>kim-etal-2020-contextual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="11">
      <title>Multilingual Emoticon Prediction of Tweets about <fixed-case>COVID</fixed-case>-19</title>
      <author><first>Stefanos</first><last>Stoikos</last></author>
      <author><first>Mike</first><last>Izbicki</last></author>
      <pages>109&#8211;118</pages>
      <abstract>Emojis are a widely used tool for encoding emotional content in informal messages such as tweets,and predicting which emoji corresponds to a piece of text can be used as a proxy for measuring the emotional content in the text. This paper presents the first model for predicting emojis in highly multilingual text.Our BERTmoticon model is a fine-tuned version of the BERT model,and it can predict emojis for text written in 102 different languages.We trained our BERTmoticon model on 54.2 million geolocated tweets sent in the first 6 months of 2020,and we apply the model to a case study analyzing the emotional reaction of Twitter users to news about the coronavirus. Example findings include a spike in sadness when the World Health Organization (WHO) declared that coronavirus was a global pandemic, and a spike in anger and disgust when the number of COVID-19 related deaths in the United States surpassed one hundred thousand. We provide an easy-to-use and open source python library for predicting emojis with BERTmoticon so that the model can easily be applied to other data mining tasks.</abstract>
      <url hash="e5732990">2020.peoples-1.11</url>
      <bibkey>stoikos-izbicki-2020-multilingual</bibkey>
      <pwccode url="https://github.com/stefanos-stk/bertmoticon" additional="false">stefanos-stk/bertmoticon</pwccode>
    </paper>
    <paper id="12">
      <title>Experiencers, Stimuli, or Targets: Which Semantic Roles Enable Machine Learning to Infer the Emotions?</title>
      <author><first>Laura Ana Maria</first><last>Oberl&#228;nder</last></author>
      <author><first>Kevin</first><last>Reich</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>119&#8211;128</pages>
      <abstract>Emotion recognition is predominantly formulated as text classification in which textual units are assigned to an emotion from a predefined inventory (e.g., fear, joy, anger, disgust, sadness, surprise, trust, anticipation). More recently, semantic role labeling approaches have been developed to extract structures from the text to answer questions like: &#8220;who is described to feel the emotion?&#8221; (experiencer), &#8220;what causes this emotion?&#8221; (stimulus), and at which entity is it directed?&#8221; (target). Though it has been shown that jointly modeling stimulus and emotion category prediction is beneficial for both subtasks, it remains unclear which of these semantic roles enables a classifier to infer the emotion. Is it the experiencer, because the identity of a person is biased towards a particular emotion (X is always happy)? Is it a particular target (everybody loves X) or a stimulus (doing X makes everybody sad)? We answer these questions by training emotion classification models on five available datasets annotated with at least one semantic role by masking the fillers of these roles in the text in a controlled manner and find that across multiple corpora, stimuli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification.</abstract>
      <url hash="51a4fc17">2020.peoples-1.12</url>
      <bibkey>oberlander-etal-2020-experiencers</bibkey>
    </paper>
    </volume>
</collection>