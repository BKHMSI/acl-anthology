<collection id="2020.nlpcovid19">
  <volume id="acl" ingest-date="2020-10-13">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on <fixed-case>NLP</fixed-case> for <fixed-case>COVID-19</fixed-case> at <fixed-case>ACL</fixed-case> 2020</booktitle>
      <editor><first>Karin</first><last>Verspoor</last></editor>
      <editor><first>Kevin Bretonnel</first><last>Cohen</last></editor>
      <editor><first>Mark</first><last>Dredze</last></editor>
      <editor><first>Emilio</first><last>Ferrara</last></editor>
      <editor><first>Jonathan</first><last>May</last></editor>
      <editor><first>Robert</first><last>Munro</last></editor>
      <editor><first>Cecile</first><last>Paris</last></editor>
      <editor><first>Byron</first><last>Wallace</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="fddac610">2020.nlpcovid19-acl.0</url>
      <bibkey>nlp-covid19-2020-nlp</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Document Classification for <fixed-case>COVID-19</fixed-case> Literature</title>
      <author><first>Bernal</first><last>Jim&#233;nez Guti&#233;rrez</last></author>
      <author><first>Juncheng</first><last>Zeng</last></author>
      <author><first>Dongdong</first><last>Zhang</last></author>
      <author><first>Ping</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Su</last></author>
      <abstract>The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset. We find that pre-trained language models outperform other models in both low and high data regimes, achieving a maximum F1 score of around 86%. We note that even the highest performing models still struggle with label correlation, distraction from introductory text and CORD-19 generalization. Both data and code are available on GitHub.</abstract>
      <url hash="e5936d8e">2020.nlpcovid19-acl.3</url>
      <bibkey>jimenez-gutierrez-etal-2020-document-classification</bibkey>
    </paper>
    <paper id="5">
      <title>Self-supervised context-aware <fixed-case>COVID-19</fixed-case> document exploration through atlas grounding</title>
      <author><first>Dusan</first><last>Grujicic</last></author>
      <author><first>Gorjan</first><last>Radevski</last></author>
      <author><first>Tinne</first><last>Tuytelaars</last></author>
      <author><first>Matthew</first><last>Blaschko</last></author>
      <abstract>In this paper, we aim to develop a self-supervised grounding of Covid-related medical text based on the actual spatial relationships between the referred anatomical concepts. More specifically, we learn to project sentences into a physical space defined by a three-dimensional anatomical atlas, allowing for a visual approach to navigating Covid-related literature. We design a straightforward and empirically effective training objective to reduce the curated data dependency issue. We use BERT as the main building block of our model and perform a quantitative analysis that demonstrates that the model learns a context-aware mapping. We illustrate two potential use-cases for our approach, one in interactive, 3D data exploration, and the other in document retrieval. To accelerate research in this direction, we make public all trained models, codebase and the developed tools, which can be accessed at https://github.com/gorjanradevski/macchina/.</abstract>
      <url hash="ea5b60e4">2020.nlpcovid19-acl.5</url>
      <bibkey>grujicic-etal-2020-self</bibkey>
      <pwccode url="https://github.com/gorjanradevski/macchina" additional="false">gorjanradevski/macchina</pwccode>
    </paper>
    <paper id="12">
      <title>Estimating the effect of <fixed-case>COVID-19</fixed-case> on mental health: Linguistic indicators of depression during a global pandemic</title>
      <author><first>JT</first><last>Wolohan</last></author>
      <abstract>This preliminary analysis uses a deep LSTM neural network with fastText embeddings to predict population rates of depression on Reddit in order to estimate the effect of COVID-19 on mental health. We find that year over year, depression rates on Reddit are up 50% , suggesting a 15-million person increase in the number of depressed Americans and a $7.5 billion increase in depression related spending. This finding suggests that utility in NLP approaches to longitudinal public-health surveillance.</abstract>
      <url hash="9bd09de3">2020.nlpcovid19-acl.12</url>
      <bibkey>wolohan-2020-estimating</bibkey>
    </paper>
    <paper id="13">
      <title>Exploration of Gender Differences in <fixed-case>COVID-19</fixed-case> Discourse on <fixed-case>R</fixed-case>eddit</title>
      <author><first>Jai</first><last>Aggarwal</last></author>
      <author><first>Ella</first><last>Rabinovich</last></author>
      <author><first>Suzanne</first><last>Stevenson</last></author>
      <abstract>Decades of research on differences in the language of men and women have established postulates about the nature of lexical, topical, and emotional preferences between the two genders, along with their sociological underpinnings. Using a novel dataset of male and female linguistic productions collected from the Reddit discussion platform, we further confirm existing assumptions about gender-linked affective distinctions, and demonstrate that these distinctions are amplified in social media postings involving emotionally-charged discourse related to COVID-19. Our analysis also confirms considerable differences in topical preferences between male and female authors in pandemic-related discussions.</abstract>
      <url hash="a2380764">2020.nlpcovid19-acl.13</url>
      <bibkey>aggarwal-etal-2020-exploration</bibkey>
      <pwccode url="https://github.com/ellarabi/covid19-demography" additional="false">ellarabi/covid19-demography</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rwwd">RWWD</pwcdataset>
    </paper>
    <paper id="15">
      <title>Cross-lingual Transfer Learning for <fixed-case>COVID-19</fixed-case> Outbreak Alignment</title>
      <author><first>Sharon</first><last>Levy</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <abstract>The spread of COVID-19 has become a significant and troubling aspect of society in 2020. With millions of cases reported across countries, new outbreaks have occurred and followed patterns of previously affected areas. Many disease detection models do not incorporate the wealth of social media data that can be utilized for modeling and predicting its spread. It is useful to ask, can we utilize this knowledge in one country to model the outbreak in another? To answer this, we propose the task of cross-lingual transfer learning for epidemiological alignment. Utilizing both macro and micro text features, we train on Italy&#8217;s early COVID-19 outbreak through Twitter and transfer to several other countries. Our experiments show strong results with up to 0.85 Spearman correlation in cross-country predictions.</abstract>
      <url hash="b944647e">2020.nlpcovid19-acl.15</url>
      <bibkey>levy-wang-2020-cross</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>NLP</fixed-case>-based Feature Extraction for the Detection of <fixed-case>COVID</fixed-case>-19 Misinformation Videos on <fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ube</title>
      <author><first>Juan Carlos</first><last>Medina Serrano</last></author>
      <author><first>Orestis</first><last>Papakyriakopoulos</last></author>
      <author><first>Simon</first><last>Hegelich</last></author>
      <abstract>We present a simple NLP methodology for detecting COVID-19 misinformation videos on YouTube by leveraging user comments. We use transfer learning pre-trained models to generate a multi-label classifier that can categorize conspiratorial content. We use the percentage of misinformation comments on each video as a new feature for video classification.</abstract>
      <url hash="2520463f">2020.nlpcovid19-acl.17</url>
      <bibkey>medina-serrano-etal-2020-nlp</bibkey>
      <pwccode url="https://github.com/JuanCarlosCSE/YouTube_misinfo" additional="false">JuanCarlosCSE/YouTube_misinfo</pwccode>
    </paper>
    </volume>
  <volume id="2" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on <fixed-case>NLP</fixed-case> for <fixed-case>COVID</fixed-case>-19 (Part 2) at <fixed-case>EMNLP</fixed-case> 2020</booktitle>
      <editor><first>Karin</first><last>Verspoor</last></editor>
      <editor><first>Kevin Bretonnel</first><last>Cohen</last></editor>
      <editor><first>Michael</first><last>Conway</last></editor>
      <editor><first>Berry</first><last>de Bruijn</last></editor>
      <editor><first>Mark</first><last>Dredze</last></editor>
      <editor><first>Rada</first><last>Mihalcea</last></editor>
      <editor><first>Byron</first><last>Wallace</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="27ecd857">2020.nlpcovid19-2.0</url>
      <bibkey>nlp-covid19-2020-nlp-covid</bibkey>
    </frontmatter>
    </volume>
</collection>