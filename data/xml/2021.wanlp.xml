<collection id="2021.wanlp">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the Sixth Arabic Natural Language Processing Workshop</booktitle>
      <editor><first>Nizar</first><last>Habash</last></editor>
      <editor><first>Houda</first><last>Bouamor</last></editor>
      <editor><first>Hazem</first><last>Hajj</last></editor>
      <editor><first>Walid</first><last>Magdy</last></editor>
      <editor><first>Wajdi</first><last>Zaghouani</last></editor>
      <editor><first>Fethi</first><last>Bougares</last></editor>
      <editor><first>Nadi</first><last>Tomeh</last></editor>
      <editor><first>Ibrahim</first><last>Abu Farha</last></editor>
      <editor><first>Samia</first><last>Touileb</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Kyiv, Ukraine (Virtual)</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="c61d72ad">2021.wanlp-1.0</url>
      <bibkey>wanlp-2021-arabic</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Benchmarking Transformer-based Language Models for <fixed-case>A</fixed-case>rabic Sentiment and Sarcasm Detection</title>
      <author><first>Ibrahim</first><last>Abu Farha</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>21&#8211;31</pages>
      <abstract>The introduction of transformer-based language models has been a revolutionary step for natural language processing (NLP) research. These models, such as BERT, GPT and ELECTRA, led to state-of-the-art performance in many NLP tasks. Most of these models were initially developed for English and other languages followed later. Recently, several Arabic-specific models started emerging. However, there are limited direct comparisons between these models. In this paper, we evaluate the performance of 24 of these models on Arabic sentiment and sarcasm detection. Our results show that the models achieving the best performance are those that are trained on only Arabic data, including dialectal Arabic, and use a larger number of parameters, such as the recently released MARBERT. However, we noticed that AraELECTRA is one of the top performing models while being much more efficient in its computational cost. Finally, the experiments on AraGPT2 variants showed low performance compared to BERT models, which indicates that it might not be suitable for classification tasks.</abstract>
      <url hash="90971353">2021.wanlp-1.3</url>
      <bibkey>abu-farha-magdy-2021-benchmarking</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/oscar">OSCAR</pwcdataset>
    </paper>
    <paper id="5">
      <title>Kawarith: an <fixed-case>A</fixed-case>rabic <fixed-case>T</fixed-case>witter Corpus for Crisis Events</title>
      <author><first>Alaa</first><last>Alharbi</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>42&#8211;52</pages>
      <abstract>Social media (SM) platforms such as Twitter provide large quantities of real-time data that can be leveraged during mass emergencies. Developing tools to support crisis-affected communities requires available datasets, which often do not exist for low resource languages. This paper introduces Kawarith a multi-dialect Arabic Twitter corpus for crisis events, comprising more than a million Arabic tweets collected during 22 crises that occurred between 2018 and 2020 and involved several types of hazard. Exploration of this content revealed the most discussed topics and information types, and the paper presents a labelled dataset from seven emergency events that serves as a gold standard for several tasks in crisis informatics research. Using annotated data from the same event, a BERT model is fine-tuned to classify tweets into different categories in the multi- label setting. Results show that BERT-based models yield good performance on this task even with small amounts of task-specific training data.</abstract>
      <url hash="210f6219">2021.wanlp-1.5</url>
      <bibkey>alharbi-lee-2021-kawarith</bibkey>
      <pwccode url="https://github.com/alaa-a-a/multi-dialect-arabic-stop-words" additional="true">alaa-a-a/multi-dialect-arabic-stop-words</pwccode>
    </paper>
    <paper id="9">
      <title><fixed-case>A</fixed-case>r<fixed-case>COV</fixed-case>-19: The First <fixed-case>A</fixed-case>rabic <fixed-case>COVID</fixed-case>-19 <fixed-case>T</fixed-case>witter Dataset with Propagation Networks</title>
      <author><first>Fatima</first><last>Haouari</last></author>
      <author><first>Maram</first><last>Hasanain</last></author>
      <author><first>Reem</first><last>Suwaileh</last></author>
      <author><first>Tamer</first><last>Elsayed</last></author>
      <pages>82&#8211;91</pages>
      <abstract>In this paper, we present ArCOV-19, an Arabic COVID-19 Twitter dataset that spans one year, covering the period from 27th of January 2020 till 31st of January 2021. ArCOV-19 is the first publicly-available Arabic Twitter dataset covering COVID-19 pandemic that includes about 2.7M tweets alongside the propagation networks of the most-popular subset of them (i.e., most-retweeted and -liked). The propagation networks include both retweetsand conversational threads (i.e., threads of replies). ArCOV-19 is designed to enable research under several domains including natural language processing, information retrieval, and social computing. Preliminary analysis shows that ArCOV-19 captures rising discussions associated with the first reported cases of the disease as they appeared in the Arab world.In addition to the source tweets and the propagation networks, we also release the search queries and the language-independent crawler used to collect the tweets to encourage the curation of similar datasets.</abstract>
      <url hash="00cd32be">2021.wanlp-1.9</url>
      <bibkey>haouari-etal-2021-arcov</bibkey>
      <pwccode url="https://gitlab.com/bigirqu/ArCOV-19" additional="false">bigirqu/ArCOV-19</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arcov-19">ArCOV-19</pwcdataset>
    </paper>
    <paper id="18">
      <title><fixed-case>ALUE</fixed-case>: <fixed-case>A</fixed-case>rabic Language Understanding Evaluation</title>
      <author><first>Haitham</first><last>Seelawi</last></author>
      <author><first>Ibraheem</first><last>Tuffaha</last></author>
      <author><first>Mahmoud</first><last>Gzawi</last></author>
      <author><first>Wael</first><last>Farhan</last></author>
      <author><first>Bashar</first><last>Talafha</last></author>
      <author><first>Riham</first><last>Badawi</last></author>
      <author><first>Zyad</first><last>Sober</last></author>
      <author><first>Oday</first><last>Al-Dweik</last></author>
      <author><first>Abed Alhakim</first><last>Freihat</last></author>
      <author><first>Hussein</first><last>Al-Natsheh</last></author>
      <pages>173&#8211;184</pages>
      <abstract>The emergence of Multi-task learning (MTL)models in recent years has helped push thestate of the art in Natural Language Un-derstanding (NLU). We strongly believe thatmany NLU problems in Arabic are especiallypoised to reap the benefits of such models. Tothis end we propose the Arabic Language Un-derstanding Evaluation Benchmark (ALUE),based on 8 carefully selected and previouslypublished tasks. For five of these, we providenew privately held evaluation datasets to en-sure the fairness and validity of our benchmark.We also provide a diagnostic dataset to helpresearchers probe the inner workings of theirmodels.Our initial experiments show thatMTL models outperform their singly trainedcounterparts on most tasks. But in order to en-tice participation from the wider community,we stick to publishing singly trained baselinesonly. Nonetheless, our analysis reveals thatthere is plenty of room for improvement inArabic NLU. We hope that ALUE will playa part in helping our community realize someof these improvements. Interested researchersare invited to submit their results to our online,and publicly accessible leaderboard.</abstract>
      <url hash="7089ea37">2021.wanlp-1.18</url>
      <bibkey>seelawi-etal-2021-alue</bibkey>
      <pwccode url="https://github.com/Alue-Benchmark/alue_baselines" additional="false">Alue-Benchmark/alue_baselines</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-question-similarity-in-arabic">Semantic Question Similarity in Arabic</pwcdataset>
    </paper>
    <paper id="19">
      <title>Quranic Verses Semantic Relatedness Using <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case></title>
      <author><first>Abdullah</first><last>Alsaleh</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <author><first>Abdulrahman</first><last>Altahhan</last></author>
      <pages>185&#8211;190</pages>
      <abstract>Bidirectional Encoder Representations from Transformers (BERT) has gained popularity in recent years producing state-of-the-art performances across Natural Language Processing tasks. In this paper, we used AraBERT language model to classify pairs of verses provided by the QurSim dataset to either be semantically related or not. We have pre-processed The QurSim dataset and formed three datasets for comparisons. Also, we have used both versions of AraBERT, which are AraBERTv02 and AraBERTv2, to recognise which version performs the best with the given datasets. The best results was AraBERTv02 with 92% accuracy score using a dataset comprised of label &#8216;2&#8217; and label '-1&#8217;, the latter was generated outside of QurSim dataset.</abstract>
      <url hash="1f3118ee">2021.wanlp-1.19</url>
      <bibkey>alsaleh-etal-2021-quranic</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>A</fixed-case>ra<fixed-case>ELECTRA</fixed-case>: Pre-Training Text Discriminators for <fixed-case>A</fixed-case>rabic Language Understanding</title>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Fady</first><last>Baly</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>191&#8211;195</pages>
      <abstract>Advances in English language representation enabled a more sample-efficient pre-training task by Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA). Which, instead of training a model to recover masked tokens, it trains a discriminator model to distinguish true input tokens from corrupted tokens that were replaced by a generator network. On the other hand, current Arabic language representation approaches rely only on pretraining via masked language modeling. In this paper, we develop an Arabic language representation model, which we name AraELECTRA. Our model is pretrained using the replaced token detection objective on large Arabic text corpora. We evaluate our model on multiple Arabic NLP tasks, including reading comprehension, sentiment analysis, and named-entity recognition and we show that AraELECTRA outperforms current state-of-the-art Arabic language representation models, given the same pretraining data and with even a smaller model size.</abstract>
      <url hash="b1925ff0">2021.wanlp-1.20</url>
      <bibkey>antoun-etal-2021-araelectra</bibkey>
      <pwccode url="https://github.com/aub-mind/araBERT" additional="false">aub-mind/araBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arcd">ARCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arsentd-lev">ArSentD-LEV</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
    </paper>
    <paper id="21">
      <title><fixed-case>A</fixed-case>ra<fixed-case>GPT</fixed-case>2: Pre-Trained Transformer for <fixed-case>A</fixed-case>rabic Language Generation</title>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Fady</first><last>Baly</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>196&#8211;207</pages>
      <abstract>Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.</abstract>
      <url hash="1c4d395f">2021.wanlp-1.21</url>
      <bibkey>antoun-etal-2021-aragpt2</bibkey>
      <pwccode url="https://github.com/aub-mind/araBERT" additional="false">aub-mind/araBERT</pwccode>
    </paper>
    <paper id="24">
      <title><fixed-case>SERAG</fixed-case>: Semantic Entity Retrieval from <fixed-case>A</fixed-case>rabic Knowledge Graphs</title>
      <author><first>Saher</first><last>Esmeir</last></author>
      <pages>219&#8211;225</pages>
      <abstract>Knowledge graphs (KGs) are widely used to store and access information about entities and their relationships. Given a query, the task of entity retrieval from a KG aims at presenting a ranked list of entities relevant to the query. Lately, an increasing number of models for entity retrieval have shown a significant improvement over traditional methods. These models, however, were developed for English KGs. In this work, we build on one such system, named KEWER, to propose SERAG (Semantic Entity Retrieval from Arabic knowledge Graphs). Like KEWER, SERAG uses random walks to generate entity embeddings. DBpedia-Entity v2 is considered the standard test collection for entity retrieval. We discuss the challenges of using it for non-English languages in general and Arabic in particular. We provide an Arabic version of this standard collection, and use it to evaluate SERAG. SERAG is shown to significantly outperform the popular BM25 model thanks to its multi-hop reasoning.</abstract>
      <url hash="4e1cea8e">2021.wanlp-1.24</url>
      <bibkey>esmeir-2021-serag</bibkey>
    </paper>
    <paper id="25">
      <title>Introducing A large <fixed-case>T</fixed-case>unisian <fixed-case>A</fixed-case>rabizi Dialectal Dataset for Sentiment Analysis</title>
      <author><first>Chayma</first><last>Fourati</last></author>
      <author><first>Hatem</first><last>Haddad</last></author>
      <author><first>Abir</first><last>Messaoudi</last></author>
      <author><first>Moez</first><last>BenHajhmida</last></author>
      <author><first>Aymen</first><last>Ben Elhaj Mabrouk</last></author>
      <author><first>Malek</first><last>Naski</last></author>
      <pages>226&#8211;230</pages>
      <abstract>On various Social Media platforms, people, tend to use the informal way to communicate, or write posts and comments: their local dialects. In Africa, more than 1500 dialects and languages exist. Particularly, Tunisians talk and write informally using Latin letters and numbers rather than Arabic ones. In this paper, we introduce a large common-crawl-based Tunisian Arabizi dialectal dataset dedicated for Sentiment Analysis. The dataset consists of a total of 100k comments (about movies, politic, sport, etc.) annotated manually by Tunisian native speakers as Positive, negative and Neutral. We evaluate our dataset on sentiment analysis task using the Bidirectional Encoder Representations from Transformers (BERT) as a contextual language model in its multilingual version (mBERT) as an embedding technique then combining mBERT with Convolutional Neural Network (CNN) as classifier. The dataset is publicly available.</abstract>
      <url hash="80118069">2021.wanlp-1.25</url>
      <bibkey>fourati-etal-2021-introducing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tsac">TSAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tunizi">TUNIZI</pwcdataset>
    </paper>
    <paper id="27">
      <title>Improving Cross-Lingual Transfer for Event Argument Extraction with Language-Universal Sentence Structures</title>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>237&#8211;243</pages>
      <abstract>We study the problem of Cross-lingual Event Argument Extraction (CEAE). The task aims to predict argument roles of entity mentions for events in text, whose language is different from the language that a predictive model has been trained on. Previous work on CEAE has shown the cross-lingual benefits of universal dependency trees in capturing shared syntactic structures of sentences across languages. In particular, this work exploits the existence of the syntactic connections between the words in the dependency trees as the anchor knowledge to transfer the representation learning across languages for CEAE models (i.e., via graph convolutional neural networks &#8211; GCNs). In this paper, we introduce two novel sources of language-independent information for CEAE models based on the semantic similarity and the universal dependency relations of the word pairs in different languages. We propose to use the two sources of information to produce shared sentence structures to bridge the gap between languages and improve the cross-lingual performance of the CEAE models. Extensive experiments are conducted with Arabic, Chinese, and English to demonstrate the effectiveness of the proposed method for CEAE.</abstract>
      <url hash="16e675d6">2021.wanlp-1.27</url>
      <bibkey>nguyen-nguyen-2021-improving</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>BERT</fixed-case>-based Multi-Task Model for Country and Province Level <fixed-case>MSA</fixed-case> and Dialectal <fixed-case>A</fixed-case>rabic Identification</title>
      <author><first>Abdellah</first><last>El Mekki</last></author>
      <author><first>Abdelkader</first><last>El Mahdaouy</last></author>
      <author><first>Kabil</first><last>Essefar</last></author>
      <author><first>Nabil</first><last>El Mamoun</last></author>
      <author><first>Ismail</first><last>Berrada</last></author>
      <author><first>Ahmed</first><last>Khoumsi</last></author>
      <pages>271&#8211;275</pages>
      <abstract>Dialect and standard language identification are crucial tasks for many Arabic natural language processing applications. In this paper, we present our deep learning-based system, submitted to the second NADI shared task for country-level and province-level identification of Modern Standard Arabic (MSA) and Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task Learning (MTL) model to tackle both country-level and province-level MSA/DA identification. The latter MTL model consists of a shared Bidirectional Encoder Representation Transformers (BERT) encoder, two task-specific attention layers, and two classifiers. Our key idea is to leverage both the task-discriminative and the inter-task shared features for country and province MSA/DA identification. The obtained results show that our MTL model outperforms single-task models on most subtasks.</abstract>
      <url hash="9776ec6e">2021.wanlp-1.31</url>
      <bibkey>el-mekki-etal-2021-bert</bibkey>
    </paper>
    <paper id="32">
      <title>Country-level <fixed-case>A</fixed-case>rabic Dialect Identification using <fixed-case>RNN</fixed-case>s with and without Linguistic Features</title>
      <author><first>Elsayed</first><last>Issa</last></author>
      <author><first>Mohammed</first><last>AlShakhori1</last></author>
      <author><first>Reda</first><last>Al-Bahrani</last></author>
      <author><first>Gus</first><last>Hahn-Powell</last></author>
      <pages>276&#8211;281</pages>
      <abstract>This work investigates the value of augmenting recurrent neural networks with feature engineering for the Second Nuanced Arabic Dialect Identification (NADI) Subtask 1.2: Country-level DA identification. We compare the performance of a simple word-level LSTM using pretrained embeddings with one enhanced using feature embeddings for engineered linguistic features. Our results show that the addition of explicit features to the LSTM is detrimental to performance. We attribute this performance loss to the bivalency of some linguistic items in some text, ubiquity of topics, and participant mobility.</abstract>
      <url hash="56b4a547">2021.wanlp-1.32</url>
      <bibkey>issa-etal-2021-country</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>A</fixed-case>rabic Dialect Identification based on a Weighted Concatenation of <fixed-case>TF</fixed-case>-<fixed-case>IDF</fixed-case> Features</title>
      <author><first>Mohamed</first><last>Lichouri</last></author>
      <author><first>Mourad</first><last>Abbas</last></author>
      <author><first>Khaled</first><last>Lounnas</last></author>
      <author><first>Besma</first><last>Benaziz</last></author>
      <author><first>Aicha</first><last>Zitouni</last></author>
      <pages>282&#8211;286</pages>
      <abstract>In this paper, we analyze the impact of the weighted concatenation of TF-IDF features for the Arabic Dialect Identification task while we participated in the NADI2021 shared task. This study is performed for two subtasks: subtask 1.1 (country-level MSA) and subtask 1.2 (country-level DA) identification. The classifiers supporting our comparative study are Linear Support Vector Classification (LSVC), Linear Regression (LR), Perceptron, Stochastic Gradient Descent (SGD), Passive Aggressive (PA), Complement Naive Bayes (CNB), MutliLayer Perceptron (MLP), and RidgeClassifier. In the evaluation phase, our system gives F1 scores of 14.87% and 21.49%, for country-level MSA and DA identification respectively, which is very close to the average F1 scores achieved by the submitted systems and recorded for both subtasks (18.70% and 24.23%).</abstract>
      <url hash="7a48b49f">2021.wanlp-1.33</url>
      <bibkey>lichouri-etal-2021-arabic</bibkey>
    </paper>
    <paper id="34">
      <title>Machine Learning-Based Approach for <fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Hamada</first><last>Nayel</last></author>
      <author><first>Ahmed</first><last>Hassan</last></author>
      <author><first>Mahmoud</first><last>Sobhi</last></author>
      <author><first>Ahmed</first><last>El-Sawy</last></author>
      <pages>287&#8211;290</pages>
      <abstract>This paper describes our systems submitted to the Second Nuanced Arabic Dialect Identification Shared Task (NADI 2021). Dialect identification is the task of automatically detecting the source variety of a given text or speech segment. There are four subtasks, two subtasks for country-level identification and the other two subtasks for province-level identification. The data in this task covers a total of 100 provinces from all 21 Arab countries and come from the Twitter domain. The proposed systems depend on five machine-learning approaches namely Complement Na&#239;ve Bayes, Support Vector Machine, Decision Tree, Logistic Regression and Random Forest Classifiers. F1 macro-averaged score of Na&#239;ve Bayes classifier outperformed all other classifiers for development and test data.</abstract>
      <url hash="00520d27">2021.wanlp-1.34</url>
      <bibkey>nayel-etal-2021-machine</bibkey>
    </paper>
    <paper id="36">
      <title>Overview of the <fixed-case>WANLP</fixed-case> 2021 Shared Task on Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Ibrahim</first><last>Abu Farha</last></author>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>296&#8211;305</pages>
      <abstract>This paper provides an overview of the WANLP 2021 shared task on sarcasm and sentiment detection in Arabic. The shared task has two subtasks: sarcasm detection (subtask 1) and sentiment analysis (subtask 2). This shared task aims to promote and bring attention to Arabic sarcasm detection, which is crucial to improve the performance in other tasks such as sentiment analysis. The dataset used in this shared task, namely ArSarcasm-v2, consists of 15,548 tweets labelled for sarcasm, sentiment and dialect. We received 27 and 22 submissions for subtasks 1 and 2 respectively. Most of the approaches relied on using and fine-tuning pre-trained language models such as AraBERT and MARBERT. The top achieved results for the sarcasm detection and sentiment analysis tasks were 0.6225 F1-score and 0.748 F1-PN respectively.</abstract>
      <url hash="b17dc947">2021.wanlp-1.36</url>
      <bibkey>abu-farha-etal-2021-overview</bibkey>
      <pwccode url="https://github.com/iabufarha/arsarcasm-v2" additional="false">iabufarha/arsarcasm-v2</pwccode>
    </paper>
    <paper id="38">
      <title>Sarcasm and Sentiment Detection In <fixed-case>A</fixed-case>rabic Tweets Using <fixed-case>BERT</fixed-case>-based Models and Data Augmentation</title>
      <author><first>Abeer</first><last>Abuzayed</last></author>
      <author><first>Hend</first><last>Al-Khalifa</last></author>
      <pages>312&#8211;317</pages>
      <abstract>In this paper, we describe our efforts on the shared task of sarcasm and sentiment detection in Arabic (Abu Farha et al., 2021). The shared task consists of two sub-tasks: Sarcasm Detection (Subtask 1) and Sentiment Analysis (Subtask 2). Our experiments were based on fine-tuning seven BERT-based models with data augmentation to solve the imbalanced data problem. For both tasks, the MARBERT BERT-based model with data augmentation outperformed other models with an increase of the F-score by 15% for both tasks which shows the effectiveness of our approach.</abstract>
      <url hash="0d407008">2021.wanlp-1.38</url>
      <bibkey>abuzayed-al-khalifa-2021-sarcasm</bibkey>
    </paper>
    <paper id="39">
      <title>Multi-task Learning Using a Combination of Contextualised and Static Word Embeddings for <fixed-case>A</fixed-case>rabic Sarcasm Detection and Sentiment Analysis</title>
      <author><first>Abdullah I.</first><last>Alharbi</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>318&#8211;322</pages>
      <abstract>Sarcasm detection and sentiment analysis are important tasks in Natural Language Understanding. Sarcasm is a type of expression where the sentiment polarity is flipped by an interfering factor. In this study, we exploited this relationship to enhance both tasks by proposing a multi-task learning approach using a combination of static and contextualised embeddings. Our proposed system achieved the best result in the sarcasm detection subtask.</abstract>
      <url hash="b3a69c39">2021.wanlp-1.39</url>
      <bibkey>alharbi-lee-2021-multi</bibkey>
    </paper>
    <paper id="41">
      <title>Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic: investigating the interest of character-level features</title>
      <author><first>Dhaou</first><last>Ghoul</last></author>
      <author><first>Ga&#235;l</first><last>Lejeune</last></author>
      <pages>329&#8211;333</pages>
      <abstract>We present three methods developed for the Shared Task on Sarcasm and Sentiment Detection in Arabic. We present a baseline that uses character n-gram features. We also propose two more sophisticated methods: a recurrent neural network with a word level representation and an ensemble classifier relying on word and character-level features. We chose to present results from an ensemble classifier but it was not very successful as compared to the best systems : 22th/37 on sarcasm detection and 15th/22 on sentiment detection. It finally appeared that our baseline could have been improved and beat those results.</abstract>
      <url hash="7ce24f47">2021.wanlp-1.41</url>
      <attachment type="Dataset" hash="f170ae85">2021.wanlp-1.41.Dataset.pdf</attachment>
      <attachment type="Software" hash="595b041b">2021.wanlp-1.41.Software.zip</attachment>
      <bibkey>ghoul-lejeune-2021-sarcasm</bibkey>
    </paper>
    <paper id="44">
      <title><fixed-case>S</fixed-case>arcasm<fixed-case>D</fixed-case>et at Sarcasm Detection Task 2021 in <fixed-case>A</fixed-case>rabic using <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case> Pretrained Model</title>
      <author><first>Dalya</first><last>Faraj</last></author>
      <author><first>Dalya</first><last>Faraj</last></author>
      <author><first>Malak</first><last>Abdullah</last></author>
      <pages>345&#8211;350</pages>
      <abstract>This paper presents one of the top five winning solutions for the Shared Task on Sarcasm and Sentiment Detection in Arabic (Subtask-1 Sarcasm Detection). The goal of the task is to identify whether a tweet is sarcastic or not. Our solution has been developed using ensemble technique with AraBERT pre-trained model. We describe the architecture of the submitted solution in the shared task. We also provide the experiments and the hyperparameter tuning that lead to this result. Besides, we discuss and analyze the results by comparing all the models that we trained or tested to achieve a better score in a table design. Our model is ranked fifth out of 27 teams with an F1 score of 0.5985. It is worth mentioning that our model achieved the highest accuracy score of 0.7830</abstract>
      <url hash="6e3bf51c">2021.wanlp-1.44</url>
      <bibkey>faraj-etal-2021-sarcasmdet</bibkey>
    </paper>
    <paper id="45">
      <title>Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic language A Hybrid Approach Combining Embeddings and Rule-based Features</title>
      <author><first>Kamel</first><last>Gaanoun</last></author>
      <author><first>Imade</first><last>Benelallam</last></author>
      <pages>351&#8211;356</pages>
      <abstract>This paper presents the ArabicProcessors team&#8217;s system designed for sarcasm (subtask 1) and sentiment (subtask 2) detection shared task. We created a hybrid system by combining rule-based features and both static and dynamic embeddings using transformers and deep learning. The system&#8217;s architecture is an ensemble of Naive bayes, MarBERT and Mazajak embedding. This process scored an F1-score of 51% on sarcasm and 71% for sentiment detection.</abstract>
      <url hash="63112aaf">2021.wanlp-1.45</url>
      <bibkey>gaanoun-benelallam-2021-sarcasm</bibkey>
    </paper>
    <paper id="50">
      <title>i<fixed-case>C</fixed-case>ompass at Shared Task on Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Malek</first><last>Naski</last></author>
      <author><first>Abir</first><last>Messaoudi</last></author>
      <author><first>Hatem</first><last>Haddad</last></author>
      <author><first>Moez</first><last>BenHajhmida</last></author>
      <author><first>Chayma</first><last>Fourati</last></author>
      <author><first>Aymen</first><last>Ben Elhaj Mabrouk</last></author>
      <pages>381&#8211;385</pages>
      <abstract>We describe our submitted system to the 2021 Shared Task on Sarcasm and Sentiment Detection in Arabic (Abu Farha et al., 2021). We tackled both subtasks, namely Sarcasm Detection (Subtask 1) and Sentiment Analysis (Subtask 2). We used state-of-the-art pretrained contextualized text representation models and fine-tuned them according to the downstream task in hand. As a first approach, we used Google&#8217;s multilingual BERT and then other Arabic variants: AraBERT, ARBERT and MARBERT. The results found show that MARBERT outperforms all of the previously mentioned models overall, either on Subtask 1 or Subtask 2.</abstract>
      <url hash="180c2655">2021.wanlp-1.50</url>
      <bibkey>naski-etal-2021-icompass</bibkey>
    </paper>
    <paper id="53">
      <title><fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case> and Farasa Segmentation Based Approach For Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic Tweets</title>
      <author><first>Anshul</first><last>Wadhawan</last></author>
      <pages>395&#8211;400</pages>
      <abstract>This paper presents our strategy to tackle the EACL WANLP-2021 Shared Task 2: Sarcasm and Sentiment Detection. One of the subtasks aims at developing a system that identifies whether a given Arabic tweet is sarcastic in nature or not, while the other aims to identify the sentiment of the Arabic tweet. We approach the task in two steps. The first step involves pre processing the provided dataset by performing insertions, deletions and segmentation operations on various parts of the text. The second step involves experimenting with multiple variants of two transformer based models, AraELECTRA and AraBERT. Our final approach was ranked seventh and fourth in the Sarcasm and Sentiment Detection subtasks respectively.</abstract>
      <url hash="9d5e7f8b">2021.wanlp-1.53</url>
      <bibkey>wadhawan-2021-arabert</bibkey>
    </paper>
  </volume>
</collection>