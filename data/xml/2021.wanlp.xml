<?xml version='1.0' encoding='utf-8'?>
<collection id="2021.wanlp">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the Sixth Arabic Natural Language Processing Workshop</booktitle>
      <editor><first>Nizar</first><last>Habash</last></editor>
      <editor><first>Houda</first><last>Bouamor</last></editor>
      <editor><first>Hazem</first><last>Hajj</last></editor>
      <editor><first>Walid</first><last>Magdy</last></editor>
      <editor><first>Wajdi</first><last>Zaghouani</last></editor>
      <editor><first>Fethi</first><last>Bougares</last></editor>
      <editor><first>Nadi</first><last>Tomeh</last></editor>
      <editor><first>Ibrahim</first><last>Abu Farha</last></editor>
      <editor><first>Samia</first><last>Touileb</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Kyiv, Ukraine (Virtual)</address>
      <month>April</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="c61d72ad">2021.wanlp-1.0</url>
      <bibkey>wanlp-2021-arabic</bibkey>
    </frontmatter>
    <paper id="3">
      <title>Benchmarking Transformer-based Language Models for Arabic Sentiment and Sarcasm Detection<fixed-case>A</fixed-case>rabic Sentiment and Sarcasm Detection</title>
      <author><first>Ibrahim</first><last>Abu Farha</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>21–31</pages>
      <abstract>The introduction of transformer-based language models has been a revolutionary step for natural language processing (NLP) research. These models, such as BERT, GPT and ELECTRA, led to state-of-the-art performance in many NLP tasks. Most of these <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> were initially developed for <a href="https://en.wikipedia.org/wiki/English_language">English</a> and other languages followed later. Recently, several Arabic-specific models started emerging. However, there are limited direct comparisons between these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a>. In this paper, we evaluate the performance of 24 of these <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> on <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Arabic sentiment</a> and sarcasm detection. Our results show that the models achieving the best performance are those that are trained on only Arabic data, including dialectal Arabic, and use a larger number of parameters, such as the recently released MARBERT. However, we noticed that AraELECTRA is one of the top performing models while being much more efficient in its <a href="https://en.wikipedia.org/wiki/Computational_cost">computational cost</a>. Finally, the experiments on AraGPT2 variants showed low performance compared to BERT models, which indicates that it might not be suitable for classification tasks.</abstract>
      <url hash="90971353">2021.wanlp-1.3</url>
      <bibkey>abu-farha-magdy-2021-benchmarking</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/oscar">OSCAR</pwcdataset>
    </paper>
    <paper id="5">
      <title>Kawarith : an Arabic Twitter Corpus for Crisis Events<fixed-case>A</fixed-case>rabic <fixed-case>T</fixed-case>witter Corpus for Crisis Events</title>
      <author><first>Alaa</first><last>Alharbi</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>42–52</pages>
      <abstract>Social media (SM) platforms such as <a href="https://en.wikipedia.org/wiki/Twitter">Twitter</a> provide large quantities of <a href="https://en.wikipedia.org/wiki/Real-time_data">real-time data</a> that can be leveraged during mass emergencies. Developing tools to support crisis-affected communities requires available datasets, which often do not exist for low resource languages. This paper introduces Kawarith a multi-dialect Arabic Twitter corpus for crisis events, comprising more than a million Arabic tweets collected during 22 crises that occurred between 2018 and 2020 and involved several types of hazard. Exploration of this content revealed the most discussed topics and information types, and the paper presents a labelled dataset from seven emergency events that serves as a gold standard for several tasks in crisis informatics research. Using annotated data from the same event, a BERT model is fine-tuned to classify tweets into different categories in the multi- label setting. Results show that BERT-based models yield good performance on this task even with small amounts of task-specific training data.</abstract>
      <url hash="210f6219">2021.wanlp-1.5</url>
      <bibkey>alharbi-lee-2021-kawarith</bibkey>
      <pwccode url="https://github.com/alaa-a-a/multi-dialect-arabic-stop-words" additional="true">alaa-a-a/multi-dialect-arabic-stop-words</pwccode>
    </paper>
    <paper id="9">
      <title>ArCOV-19 : The First Arabic COVID-19 Twitter Dataset with Propagation Networks<fixed-case>A</fixed-case>r<fixed-case>COV</fixed-case>-19: The First <fixed-case>A</fixed-case>rabic <fixed-case>COVID</fixed-case>-19 <fixed-case>T</fixed-case>witter Dataset with Propagation Networks</title>
      <author><first>Fatima</first><last>Haouari</last></author>
      <author><first>Maram</first><last>Hasanain</last></author>
      <author><first>Reem</first><last>Suwaileh</last></author>
      <author><first>Tamer</first><last>Elsayed</last></author>
      <pages>82–91</pages>
      <abstract>In this paper, we present ArCOV-19, an Arabic COVID-19 Twitter dataset that spans one year, covering the period from 27th of January 2020 till 31st of January 2021. ArCOV-19 is the first publicly-available Arabic Twitter dataset covering COVID-19 pandemic that includes about 2.7 M tweets alongside the propagation networks of the most-popular subset of them (i.e., most-retweeted and -liked). The propagation networks include both retweetsand <a href="https://en.wikipedia.org/wiki/Conversation">conversational threads</a> (i.e., threads of replies). ArCOV-19 is designed to enable research under several domains including <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, <a href="https://en.wikipedia.org/wiki/Information_retrieval">information retrieval</a>, and <a href="https://en.wikipedia.org/wiki/Social_computing">social computing</a>. Preliminary analysis shows that ArCOV-19 captures rising discussions associated with the first reported cases of the disease as they appeared in the <a href="https://en.wikipedia.org/wiki/Arab_world">Arab world</a>. In addition to the source tweets and the propagation networks, we also release the search queries and the language-independent crawler used to collect the tweets to encourage the curation of similar datasets.</abstract>
      <url hash="00cd32be">2021.wanlp-1.9</url>
      <bibkey>haouari-etal-2021-arcov</bibkey>
      <pwccode url="https://gitlab.com/bigirqu/ArCOV-19" additional="false">bigirqu/ArCOV-19</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arcov-19">ArCOV-19</pwcdataset>
    </paper>
    <paper id="18">
      <title>ALUE : Arabic Language Understanding Evaluation<fixed-case>ALUE</fixed-case>: <fixed-case>A</fixed-case>rabic Language Understanding Evaluation</title>
      <author><first>Haitham</first><last>Seelawi</last></author>
      <author><first>Ibraheem</first><last>Tuffaha</last></author>
      <author><first>Mahmoud</first><last>Gzawi</last></author>
      <author><first>Wael</first><last>Farhan</last></author>
      <author><first>Bashar</first><last>Talafha</last></author>
      <author><first>Riham</first><last>Badawi</last></author>
      <author><first>Zyad</first><last>Sober</last></author>
      <author><first>Oday</first><last>Al-Dweik</last></author>
      <author><first>Abed Alhakim</first><last>Freihat</last></author>
      <author><first>Hussein</first><last>Al-Natsheh</last></author>
      <pages>173–184</pages>
      <abstract>The emergence of Multi-task learning (MTL)models in recent years has helped push thestate of the art in Natural Language Un-derstanding (NLU). We strongly believe thatmany NLU problems in <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> are especiallypoised to reap the benefits of such models. Tothis end we propose the Arabic Language Un-derstanding Evaluation Benchmark (ALUE),based on 8 carefully selected and previouslypublished tasks. For five of these, we providenew privately held evaluation datasets to en-sure the fairness and validity of our benchmark. We also provide a diagnostic dataset to helpresearchers probe the inner workings of theirmodels. Our initial experiments show thatMTL models outperform their singly trainedcounterparts on most tasks. But in order to en-tice participation from the wider community, we stick to publishing singly trained baselinesonly. Nonetheless, our analysis reveals thatthere is plenty of room for improvement inArabic NLU. We hope that ALUE will playa part in helping our community realize someof these improvements. Interested researchersare invited to submit their results to our online, and publicly accessible leaderboard.</abstract>
      <url hash="7089ea37">2021.wanlp-1.18</url>
      <bibkey>seelawi-etal-2021-alue</bibkey>
      <pwccode url="https://github.com/Alue-Benchmark/alue_baselines" additional="false">Alue-Benchmark/alue_baselines</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-question-similarity-in-arabic">Semantic Question Similarity in Arabic</pwcdataset>
    </paper>
    <paper id="19">
      <title>Quranic Verses Semantic Relatedness Using AraBERT<fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case></title>
      <author><first>Abdullah</first><last>Alsaleh</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <author><first>Abdulrahman</first><last>Altahhan</last></author>
      <pages>185–190</pages>
      <abstract>Bidirectional Encoder Representations from Transformers (BERT) has gained popularity in recent years producing state-of-the-art performances across Natural Language Processing tasks. In this paper, we used AraBERT language model to classify pairs of verses provided by the QurSim dataset to either be semantically related or not. We have pre-processed The QurSim dataset and formed three datasets for comparisons. Also, we have used both versions of AraBERT, which are AraBERTv02 and AraBERTv2, to recognise which version performs the best with the given datasets. The best results was AraBERTv02 with 92 % accuracy score using a <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> comprised of label ‘2’ and label’ -1’, the latter was generated outside of QurSim dataset.</abstract>
      <url hash="1f3118ee">2021.wanlp-1.19</url>
      <bibkey>alsaleh-etal-2021-quranic</bibkey>
    </paper>
    <paper id="20">
      <title>AraELECTRA : Pre-Training Text Discriminators for Arabic Language Understanding<fixed-case>A</fixed-case>ra<fixed-case>ELECTRA</fixed-case>: Pre-Training Text Discriminators for <fixed-case>A</fixed-case>rabic Language Understanding</title>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Fady</first><last>Baly</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>191–195</pages>
      <abstract>Advances in English language representation enabled a more sample-efficient pre-training task by Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA). Which, instead of training a <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> to recover masked tokens, it trains a discriminator model to distinguish true input tokens from corrupted tokens that were replaced by a generator network. On the other hand, current Arabic language representation approaches rely only on pretraining via masked language modeling. In this paper, we develop an Arabic language representation model, which we name AraELECTRA. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is pretrained using the replaced token detection objective on large Arabic text corpora. We evaluate our model on multiple Arabic NLP tasks, including <a href="https://en.wikipedia.org/wiki/Reading_comprehension">reading comprehension</a>, <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>, and <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named-entity recognition</a> and we show that AraELECTRA outperforms current state-of-the-art Arabic language representation models, given the same pretraining data and with even a smaller model size.</abstract>
      <url hash="b1925ff0">2021.wanlp-1.20</url>
      <bibkey>antoun-etal-2021-araelectra</bibkey>
      <pwccode url="https://github.com/aub-mind/araBERT" additional="false">aub-mind/araBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arcd">ARCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arsentd-lev">ArSentD-LEV</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
    </paper>
    <paper id="21">
      <title>AraGPT2 : Pre-Trained Transformer for Arabic Language Generation<fixed-case>A</fixed-case>ra<fixed-case>GPT</fixed-case>2: Pre-Trained Transformer for <fixed-case>A</fixed-case>rabic Language Generation</title>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Fady</first><last>Baly</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>196–207</pages>
      <abstract>Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in <a href="https://en.wikipedia.org/wiki/Language_generation">language generation</a> for <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a>, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieves a <a href="https://en.wikipedia.org/wiki/Perplexity">perplexity</a> of 29.8 on <a href="https://en.wikipedia.org/wiki/Wikipedia">held-out Wikipedia articles</a>. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating <a href="https://en.wikipedia.org/wiki/Article_(publishing)">news articles</a> that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98 % percent <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> in detecting model-generated text. The <a href="https://en.wikipedia.org/wiki/Conceptual_model">models</a> are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.</abstract>
      <url hash="1c4d395f">2021.wanlp-1.21</url>
      <bibkey>antoun-etal-2021-aragpt2</bibkey>
      <pwccode url="https://github.com/aub-mind/araBERT" additional="false">aub-mind/araBERT</pwccode>
    </paper>
    <paper id="24">
      <title>SERAG : Semantic Entity Retrieval from Arabic Knowledge Graphs<fixed-case>SERAG</fixed-case>: Semantic Entity Retrieval from <fixed-case>A</fixed-case>rabic Knowledge Graphs</title>
      <author><first>Saher</first><last>Esmeir</last></author>
      <pages>219–225</pages>
      <abstract>Knowledge graphs (KGs) are widely used to store and access information about entities and their relationships. Given a query, the task of <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity retrieval</a> from a KG aims at presenting a ranked list of entities relevant to the query. Lately, an increasing number of <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> for entity retrieval have shown a significant improvement over traditional methods. These <a href="https://en.wikipedia.org/wiki/Physical_model">models</a>, however, were developed for English KGs. In this work, we build on one such system, named KEWER, to propose SERAG (Semantic Entity Retrieval from Arabic knowledge Graphs). Like KEWER, SERAG uses <a href="https://en.wikipedia.org/wiki/Random_walk">random walks</a> to generate <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity embeddings</a>. DBpedia-Entity v2 is considered the standard test collection for <a href="https://en.wikipedia.org/wiki/Entity–relationship_model">entity retrieval</a>. We discuss the challenges of using <a href="https://en.wikipedia.org/wiki/Information_technology">it</a> for non-English languages in general and <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> in particular. We provide an Arabic version of this standard <a href="https://en.wikipedia.org/wiki/Collection_(abstract_data_type)">collection</a>, and use it to evaluate SERAG. SERAG is shown to significantly outperform the popular BM25 model thanks to its multi-hop reasoning.</abstract>
      <url hash="4e1cea8e">2021.wanlp-1.24</url>
      <bibkey>esmeir-2021-serag</bibkey>
    </paper>
    <paper id="25">
      <title>Introducing A large Tunisian Arabizi Dialectal Dataset for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a><fixed-case>T</fixed-case>unisian <fixed-case>A</fixed-case>rabizi Dialectal Dataset for Sentiment Analysis</title>
      <author><first>Chayma</first><last>Fourati</last></author>
      <author><first>Hatem</first><last>Haddad</last></author>
      <author><first>Abir</first><last>Messaoudi</last></author>
      <author><first>Moez</first><last>BenHajhmida</last></author>
      <author><first>Aymen</first><last>Ben Elhaj Mabrouk</last></author>
      <author><first>Malek</first><last>Naski</last></author>
      <pages>226–230</pages>
      <abstract>On various Social Media platforms, people, tend to use the informal way to communicate, or write posts and comments : their local dialects. In <a href="https://en.wikipedia.org/wiki/Africa">Africa</a>, more than 1500 dialects and languages exist. Particularly, Tunisians talk and write informally using <a href="https://en.wikipedia.org/wiki/Latin_script">Latin letters</a> and numbers rather than <a href="https://en.wikipedia.org/wiki/Arabic_script">Arabic ones</a>. In this paper, we introduce a large common-crawl-based Tunisian Arabizi dialectal dataset dedicated for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a>. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> consists of a total of 100k comments (about <a href="https://en.wikipedia.org/wiki/Film">movies</a>, <a href="https://en.wikipedia.org/wiki/Politics">politic</a>, <a href="https://en.wikipedia.org/wiki/Sport">sport</a>, etc.) annotated manually by Tunisian native speakers as Positive, negative and Neutral. We evaluate our dataset on sentiment analysis task using the Bidirectional Encoder Representations from Transformers (BERT) as a contextual language model in its multilingual version (mBERT) as an embedding technique then combining mBERT with Convolutional Neural Network (CNN) as classifier. The <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> is publicly available.</abstract>
      <url hash="80118069">2021.wanlp-1.25</url>
      <bibkey>fourati-etal-2021-introducing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tsac">TSAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tunizi">TUNIZI</pwcdataset>
    </paper>
    <paper id="27">
      <title>Improving Cross-Lingual Transfer for Event Argument Extraction with Language-Universal Sentence Structures</title>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>237–243</pages>
      <abstract>We study the problem of Cross-lingual Event Argument Extraction (CEAE). The task aims to predict argument roles of entity mentions for events in text, whose language is different from the language that a <a href="https://en.wikipedia.org/wiki/Predictive_modelling">predictive model</a> has been trained on. Previous work on CEAE has shown the cross-lingual benefits of universal dependency trees in capturing shared syntactic structures of sentences across languages. In particular, this work exploits the existence of the syntactic connections between the words in the dependency trees as the anchor knowledge to transfer the representation learning across languages for CEAE models (i.e., via graph convolutional neural networks   GCNs). In this paper, we introduce two novel sources of language-independent information for CEAE models based on the <a href="https://en.wikipedia.org/wiki/Semantic_similarity">semantic similarity</a> and the universal dependency relations of the word pairs in different languages. We propose to use the two sources of information to produce shared sentence structures to bridge the gap between languages and improve the cross-lingual performance of the CEAE models. Extensive experiments are conducted with <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>, <a href="https://en.wikipedia.org/wiki/Chinese_language">Chinese</a>, and <a href="https://en.wikipedia.org/wiki/English_language">English</a> to demonstrate the effectiveness of the proposed method for CEAE.</abstract>
      <url hash="16e675d6">2021.wanlp-1.27</url>
      <bibkey>nguyen-nguyen-2021-improving</bibkey>
    </paper>
    <paper id="31">
      <title>BERT-based Multi-Task Model for Country and Province Level MSA and Dialectal Arabic Identification<fixed-case>BERT</fixed-case>-based Multi-Task Model for Country and Province Level <fixed-case>MSA</fixed-case> and Dialectal <fixed-case>A</fixed-case>rabic Identification</title>
      <author><first>Abdellah</first><last>El Mekki</last></author>
      <author><first>Abdelkader</first><last>El Mahdaouy</last></author>
      <author><first>Kabil</first><last>Essefar</last></author>
      <author><first>Nabil</first><last>El Mamoun</last></author>
      <author><first>Ismail</first><last>Berrada</last></author>
      <author><first>Ahmed</first><last>Khoumsi</last></author>
      <pages>271–275</pages>
      <abstract>Dialect and standard language identification are crucial tasks for many Arabic natural language processing applications. In this paper, we present our deep learning-based system, submitted to the second NADI shared task for country-level and province-level identification of Modern Standard Arabic (MSA) and Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task Learning (MTL) model to tackle both country-level and province-level MSA / DA identification. The latter MTL model consists of a shared Bidirectional Encoder Representation Transformers (BERT) encoder, two task-specific attention layers, and two <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a>. Our key idea is to leverage both the task-discriminative and the inter-task shared features for country and province MSA / DA identification. The obtained results show that our MTL model outperforms single-task models on most subtasks.</abstract>
      <url hash="9776ec6e">2021.wanlp-1.31</url>
      <bibkey>el-mekki-etal-2021-bert</bibkey>
    <title_ar>نموذج متعدد المهام قائم على BERT للغة العربية الفصحى على مستوى الدولة والمقاطعة والتعرف على اللهجات العربية</title_ar>
      <title_pt>Modelo multitarefa baseado em BERT para MSA em nível de país e província e identificação árabe dialetal</title_pt>
      <title_es>Modelo multitarea basado en BERT para identificación de MSA y árabe dialectal a nivel de país y provincia</title_es>
      <title_fr>Modèle multitâche basé sur BERT pour le MSA au niveau du pays et de la province et l'identification arabe dialectale</title_fr>
      <title_ja>国と都道府県レベルのMSAと方言アラビア語識別のためのBERTベースのマルチタスクモデル</title_ja>
      <title_zh>盖BERT之多任务模形,施于国省级MSA方言阿拉伯语识</title_zh>
      <title_ru>Многозадачная модель на основе BERT для СУО на уровне страны и провинции и диалектической арабской идентификации</title_ru>
      <title_hi>BERT-आधारित बहु-कार्य मॉडल देश और प्रांत स्तर MSA और बोलचाल अरबी पहचान के लिए</title_hi>
      <title_ga>Samhail IlTasc bunaithe ar BERT le haghaidh MSA Leibhéal Tíre agus Cúige agus Sainaithint Araibis Chanúnach</title_ga>
      <title_ka>BERT- დაბათი მრავალ დავალების მოდელი ქვეყანის და პროვინციის MSA და დიალექტური არაბული იდენტიფიკაციისთვის</title_ka>
      <title_hu>BERT-alapú többfeladatos modell ország- és tartományszintű MSA és dialektikus arab azonosító számára</title_hu>
      <title_el>Με βάση το BERT μοντέλο πολλαπλών εργασιών για το επίπεδο MSA σε επίπεδο χώρας και επαρχίας και διαλεκτική αραβική αναγνώριση</title_el>
      <title_it>Modello multi-task basato su BERT per MSA a livello nazionale e provinciale e identificazione dialettica araba</title_it>
      <title_kk>BERT- негіздеген ел мен өлке деңгейі MSA және диалектикалық араб идентификациясы үшін көптеген тапсырмалар үлгісі</title_kk>
      <title_ms>Model Tugas Berberasaskan-BERT untuk Aras Negara dan Provinsi MSA dan Identifikasi Arab Dialektik</title_ms>
      <title_mk>Мултизадачен модел со седиште на BERT за ниво на земја и провинција MSA и дијалектална арапска идентификација</title_mk>
      <title_mt>BERT-based Multi-Task Model for Country and Province Level MSA and Dialectal Arabic Identification</title_mt>
      <title_mn>BERT-д суурилсан олон-үйл ажиллагааны загвар MSA болон Dialectal Arabic Identification</title_mn>
      <title_pl>Model wielozadaniowy oparty na BERT dla MSA na poziomie kraju i prowincji oraz dialektalnej identyfikacji arabskiej</title_pl>
      <title_lt>BERT grindžiamas daugiafunkcinis šalies ir provincijos lygmens MSA ir dialektinio arabų identifikavimo modelis</title_lt>
      <title_ml>ബെര്‍ട്ടി അടിസ്ഥാനമാക്കിയ രാജ്യത്തിനും പ്രദേശം നില എസ്എംഎം, ഡയലക്ട്രല്‍ അറബി തിരിച്ചറിയുന്നതിനുമായി പല</title_ml>
      <title_sr>BERT-bazirani model multi-task za nivo zemlje i provincije MSA i dijalektičku arapsku identifikaciju</title_sr>
      <title_ro>Model multifuncțional bazat pe BERT pentru MSA la nivel de țară și provincie și identificare dialectă arabă</title_ro>
      <title_sv>BERT-baserad flerfunktionsmodell för MSA och dialektisk arabisk identifiering på land- och provinsnivå</title_sv>
      <title_no>BERT-basert fleire oppgåver- modell for landnivå og provinsenivå MSA og dialektisk arabisk identifikasjon</title_no>
      <title_ta>Name</title_ta>
      <title_si>BERT- අධාරිත ගොඩක් වැඩි වැඩි වැඩි වැඩක් මොඩල්</title_si>
      <title_so>BERT-based Model of Multi-Tax for Country and Province Level MSA and Dialectal Identification Carabi</title_so>
      <title_ur>BERT-based Multi-Task Model for Country and Province Level MSA and Dialectal Arabic Identification</title_ur>
      <title_vi>Mô hình tổ chức đa tác vụ BERT cho đất nước và đất nước cùng cấp chán và tự xưng tiếng Ả Rập</title_vi>
      <title_uz>Name</title_uz>
      <title_bg>Базиран на BERT многофункционален модел за MSA на ниво държава и провинция и диалектална арабска идентификация</title_bg>
      <title_hr>Model multizadataka na BERT-u za razinu zemlje i provincije MSA i dijalektičku arapsku identifikaciju</title_hr>
      <title_da>BERT-baseret Multi-Task Model for MSA og dialektisk arabisk identifikation på land- og provinsniveau</title_da>
      <title_nl>BERT-gebaseerd multitask model voor land- en provincieniveau MSA en dialectische Arabische identificatie</title_nl>
      <title_de>BERT-basiertes Multi-Task-Modell für Länder- und Provinzebene MSA und dialektale arabische Identifikation</title_de>
      <title_id>Berdasarkan BERT Multi-Task Model untuk Level Negara dan Provinsi MSA dan Identifikasi Arab Dialektik</title_id>
      <title_ko>BERT 기반 국가 및 성급 MSA 멀티태스킹 모델 및 아랍어 사투리 식별</title_ko>
      <title_fa>Model Multi Task based BERT for Country and Province Level MSA and Dialectal Arabic Identification</title_fa>
      <title_sw>Mradi wa kazi nyingi yenye msingi wa BERT kwa ajili ya Taifa na Mkuu MSA na Utambulisho wa Kiarabu wa Kiarabu</title_sw>
      <title_tr>BERT tabanly Ýurt we welaýaty derejesi MSA we Dialektik Arapça Kimligi</title_tr>
      <title_af>BERT-gebaseerde Multi-Task Model vir Land en Provinsie Vlak MSA en Dialectal Arabiese Identifikasie</title_af>
      <title_sq>Model me shumëdetyrë bazuar në BERT për nivelin e vendit dhe provincës MSA dhe identifikimin dialektal arab</title_sq>
      <title_am>BERT-based Multi-Task Model for Country and Province ደረጃዎች MSA and Dialectal Arabic Identification</title_am>
      <title_hy>BER-ի հիմնված բազմախնդիրների մոդելը երկրի և նահանգության մակարդակի MSA և դիալեկտալ արաբական ինքնության համար</title_hy>
      <title_az>BERT-tabanl캼 칖lk톛 v톛 칖niversitesi S톛viyy톛si MSA v톛 Dialektik Arap칞a Kimlik Modeli</title_az>
      <title_bn>দেশ এবং প্রদেশের স্তর এমএসএ এবং ডায়ালেক্টাল আরবী পরিচয়ের জন্য বেরেট ভিত্তিক বহুবার কাজ মডেল</title_bn>
      <title_bs>BERT-bazirani model multi-task za nivo zemlje i provincije MSA i dijalektičku arapsku identifikaciju</title_bs>
      <title_ca>Model multitasca basat en BERT per a nivell nacional i provincial MSA i identificació àrab dialectal</title_ca>
      <title_cs>Víceúkolový model založený na BERT pro MSA na úrovni zemí a provincie a dialektální arabskou identifikaci</title_cs>
      <title_et>BERT-põhine mitmeülesandemudel riigi ja provintsi tasandil MSA ja dialektuaalne araabia identifitseerimine</title_et>
      <title_fi>BERT-pohjainen monitehtävämalli maa- ja maakuntatason MSA:lle ja dialektiiviselle arabialaiselle tunnistukselle</title_fi>
      <title_jv>BERT-basa Multi-task model kanggo Kemerdekaan lan Gambar Neri</title_jv>
      <title_he>מודל משימות רבות מבוסס על BERT עבור רמת המדינה והמחוזה MSA ו זיהוי ערבי דיאלקטי</title_he>
      <title_sk>Večopravilni model BERT za MSA na ravni držav in provinc ter dialektno arabsko identifikacijo</title_sk>
      <title_bo>BERT-based Multi-Task Model for Country and Province Level MSA and Dialectal Arabic Identification</title_bo>
      <title_ha>KCharselect unicode block name</title_ha>
      <abstract_ar>يعتبر تحديد اللهجة واللغة القياسية من المهام الحاسمة للعديد من تطبيقات معالجة اللغة العربية الطبيعية. في هذه الورقة ، نقدم نظامنا القائم على التعلم العميق ، والذي تم تقديمه إلى مهمة NADI المشتركة الثانية لتحديد مستوى اللغة العربية الفصحى الحديثة (MSA) واللهجة العربية (DA) على مستوى الدولة وعلى مستوى المقاطعة. يعتمد النظام على نموذج التعلم متعدد المهام العميق الشامل (MTL) لمعالجة تحديد MSA / DA على مستوى الدولة وعلى مستوى المقاطعة. يتكون نموذج MTL الأخير من مشفر مشترك لمحولات تمثيل التشفير ثنائية الاتجاه (BERT) ، وطبقتان من طبقات الانتباه الخاصة بالمهمة ، واثنين من المصنفات. تتمثل فكرتنا الأساسية في الاستفادة من كل من الميزات التمييزية للمهام والميزات المشتركة بين المهام لتحديد البلد والمقاطعة MSA / DA. تظهر النتائج التي تم الحصول عليها أن نموذج MTL الخاص بنا يتفوق على نماذج المهام الفردية في معظم المهام الفرعية.</abstract_ar>
      <abstract_pt>A identificação do dialeto e do idioma padrão são tarefas cruciais para muitos aplicativos de processamento de idioma natural árabe. Neste artigo, apresentamos nosso sistema baseado em aprendizado profundo, submetido à segunda tarefa compartilhada do NADI para identificação em nível de país e província de árabe moderno padrão (MSA) e árabe dialetal (DA). O sistema é baseado em um modelo de aprendizagem multitarefa profunda (MTL) de ponta a ponta para lidar com a identificação MSA/DA em nível de país e província. O último modelo MTL consiste em um codificador compartilhado de Transformadores de Representação de Codificador Bidirecional (BERT), duas camadas de atenção específicas para tarefas e dois classificadores. Nossa ideia principal é alavancar os recursos de discriminação de tarefas e compartilhados entre tarefas para a identificação MSA/DA do país e da província. Os resultados obtidos mostram que nosso modelo MTL supera os modelos de tarefa única na maioria das subtarefas.</abstract_pt>
      <abstract_es>La identificación de dialectos y idiomas estándar son tareas cruciales para muchas aplicaciones de procesamiento de lenguaje natural árabe. En este artículo, presentamos nuestro sistema basado en el aprendizaje profundo, presentado a la segunda tarea compartida de NADI para la identificación a nivel nacional y provincial del árabe estándar moderno (MSA) y el árabe dialectal (DA). El sistema se basa en un modelo de aprendizaje multitarea (MTL) profundo de extremo a extremo para abordar la identificación de MSA/DA tanto a nivel nacional como provincial. El último modelo MTL consiste en un codificador Bidirectional Encoder Representation Transformers (BERT) compartido, dos capas de atención específicas de la tarea y dos clasificadores. Nuestra idea clave es aprovechar tanto las características discriminatorias de tareas como las compartidas entre tareas para la identificación de MSA/DA de país y provincia. Los resultados obtenidos muestran que nuestro modelo MTL supera a los modelos de una sola tarea en la mayoría de las subtareas.</abstract_es>
      <abstract_fr>L'identification du dialecte et de la langue standard sont des tâches cruciales pour de nombreuses applications de traitement du langage naturel arabe. Dans cet article, nous présentons notre système basé sur l'apprentissage profond, soumis à la deuxième tâche partagée NADI pour l'identification au niveau des pays et des provinces de l'arabe standard moderne (MSA) et de l'arabe dialectal (DA). Le système est basé sur un modèle d'apprentissage multitâche (MTL) approfondi de bout en bout pour traiter l'identification MSA/DA au niveau des pays et des provinces. Ce dernier modèle MTL se compose d'un codeur BERT (Bidirectional Encoder Representation Transformers) partagé, de deux couches d'attention spécifiques aux tâches et de deux classificateurs. Notre idée clé est de tirer parti à la fois des fonctionnalités discriminantes et des fonctionnalités partagées entre les tâches pour l'identification MSA/DA de pays et de province. Les résultats obtenus montrent que notre modèle MTL surpasse les modèles à tâche unique pour la plupart des sous-tâches.</abstract_fr>
      <abstract_ja>方言と標準言語の識別は、多くのアラビア語の自然言語処理アプリケーションにとって重要なタスクです。この論文では、現代標準アラビア語（ MSA ）と方言アラビア語（ DA ）の国レベルおよび州レベルの識別のために、第2のNADI共有タスクに提出された深層学習ベースのシステムを紹介します。このシステムは、エンドツーエンドのDEEPマルチタスクラーニング（ MTL ）モデルに基づいており、国レベルと州レベルのMSA/DA識別の両方に取り組んでいます。後者のMTLモデルは、共有双方向エンコーダ表現トランスフォーマー（ BERT ）エンコーダ、2つのタスク固有の注意レイヤー、および2つの分類子で構成されています。当社の主要なアイデアは、タスク区別機能とタスク間共有機能の両方を利用して、国と都道府県のMSA/DA識別を行うことです。得られた結果は、MTLモデルがほとんどのサブタスクでシングルタスクモデルよりも優れていることを示しています。</abstract_ja>
      <abstract_hi>बोली और मानक भाषा की पहचान कई अरबी प्राकृतिक भाषा प्रसंस्करण अनुप्रयोगों के लिए महत्वपूर्ण कार्य हैं। इस पेपर में, हम अपनी गहरी शिक्षा-आधारित प्रणाली प्रस्तुत करते हैं, जो आधुनिक मानक अरबी (एमएसए) और डायलेक्टल अरबी (डीए) की देश-स्तरीय और प्रांत-स्तरीय पहचान के लिए दूसरे नाडीआई साझा कार्य के लिए प्रस्तुत की गई है। यह प्रणाली देश-स्तर और प्रांत-स्तरीय एमएसए / डीए पहचान दोनों से निपटने के लिए एक एंड-टू-एंड डीप मल्टी-टास्क लर्निंग (एमटीएल) मॉडल पर आधारित है। उत्तरार्द्ध एमटीएल मॉडल में एक साझा द्विदिश एन्कोडर प्रतिनिधित्व ट्रांसफॉर्मर (BERT) एनकोडर, दो कार्य-विशिष्ट ध्यान परतें और दो क्लासिफायर शामिल हैं। हमारा प्रमुख विचार देश और प्रांत एमएसए / डीए पहचान के लिए कार्य-भेदभावपूर्ण और अंतर-कार्य साझा सुविधाओं दोनों का लाभ उठाना है। प्राप्त परिणामों से पता चलता है कि हमारा एमटीएल मॉडल अधिकांश उप-कार्यों पर एकल-कार्य मॉडल से बेहतर प्रदर्शन करता है।</abstract_hi>
      <abstract_zh>方言准言,多阿拉伯语自然语言应用程序之要务也。 本文,我们介了深度学习的系统,该系统提交给第二NADI共享职务,用于现代准阿拉伯语(MSA)和方言阿拉伯语(DA)的国家级和省级识别。 统端到端深多任务学(MTL)模形,以决国家级省级MSA / DA。 后 MTL 共双向编码器转换器 (BERT) 编码器、二特定于事者,与二器为之。 凡我大要,以职分职,以知国省MSA / DA。 结果表明,吾MTL多优于任。</abstract_zh>
      <abstract_ru>Диалект и стандартная идентификация языка являются важнейшими задачами для многих приложений обработки арабского естественного языка. В этом документе мы представляем нашу систему глубокого обучения, представленную для второй совместной задачи НАДИ по определению на страновом и провинциальном уровнях современного стандартного арабского языка (MSA) и диалектного арабского языка (DA). Система основана на сквозной модели глубокого многозадачного обучения (MTL) для решения как на страновом, так и на провинциальном уровне идентификации СУО/ПА. Последняя модель MTL состоит из общего двунаправленного преобразователя представления кодера (BERT), двух уровней внимания, специфичных для конкретной задачи, и двух классификаторов. Наша ключевая идея заключается в использовании как дискриминационных, так и межзадачных общих функций для идентификации MSA/DA страны и провинции. Полученные результаты показывают, что наша модель MTL превосходит однозадачные модели по большинству подзадач.</abstract_ru>
      <abstract_ga>Is tascanna ríthábhachtacha iad canúint agus sainaithint teanga chaighdeánach do go leor feidhmeanna próiseála teanga nádúrtha Araibis. Sa pháipéar seo, cuirimid ár gcóras domhainfhoghlama i láthair, a cuireadh faoi bhráid an dara tasc comhroinnte de chuid NADI chun Araibis Chaighdeánach Nua-Aimseartha (MSA) agus Araibis Chanúnach (DA) a shainaithint ar leibhéal tíre agus cúige. Tá an córas bunaithe ar mhúnla domhain Foghlama Ilthasc (MTL) ó cheann go ceann chun dul i ngleic le sainaithint MSA/DA ar leibhéal na tíre agus ar an gcúige. Is éard atá sa tsamhail MTL deiridh ná ionchódóir comhroinnte Ionchódóra Léiriúcháin Ionchódóra Déthreo (BERT), dhá shraith aird a bhaineann go sonrach le tasc, agus dhá aicmitheora. Is é an príomh-smaoineamh atá againn ná na gnéithe tasc-idirdhealaitheacha agus idir-tasc roinnte a ghiaráil le haghaidh sainaithint tíre agus cúige MSA/DA. Léiríonn na torthaí a fuarthas go sáraíonn ár samhail MTL samhlacha aon tasc ar fhormhór na bhfothascanna.</abstract_ga>
      <abstract_el>Η διαλεκτική και η τυπική αναγνώριση γλώσσας είναι κρίσιμες εργασίες για πολλές εφαρμογές επεξεργασίας αραβικής φυσικής γλώσσας. Στην παρούσα εργασία, παρουσιάζουμε το σύστημα που βασίζεται στη βαθιά μάθηση, που υποβλήθηκε στο δεύτερο κοινό έργο της NADI για τον προσδιορισμό σε επίπεδο χώρας και επαρχίας των σύγχρονων προτύπων αραβικών (MSA) και διαλεκτικών αραβικών (DA). Το σύστημα βασίζεται σε ένα ολοκληρωμένο μοντέλο βαθιάς μάθησης πολλαπλών εργασιών (ΜΤL) για την αντιμετώπιση τόσο σε επίπεδο χώρας όσο και σε επίπεδο επαρχίας αναγνώρισης MSA/DA. Το τελευταίο μοντέλο αποτελείται από έναν κοινόχρηστο κωδικοποιητή αναπαράστασης μετασχηματιστών αμφίδρομης κατεύθυνσης κωδικοποιητή (BERT), δύο στρώματα προσοχής ειδικά για την εργασία και δύο ταξινομητές. Βασική ιδέα μας είναι να αξιοποιήσουμε τόσο τα διακριτικά χαρακτηριστικά των καθηκόντων όσο και τα κοινά χαρακτηριστικά μεταξύ των καθηκόντων για την αναγνώριση της χώρας και της επαρχίας MSA/DA. Τα αποτελέσματα που λαμβάνονται δείχνουν ότι το μοντέλο μας ξεπερνά τα μοντέλα μιας εργασίας στις περισσότερες δευτερεύουσες εργασίες.</abstract_el>
      <abstract_kk>Диалекті және стандартты тіл идентификациясы - көп араб тілді өңдеу қолданбаларының маңызды тапсырмалары. Бұл қағазда, біз біздің тұрақты оқыту жүйесімізді таңдап, ел деңгейінде және ел деңгейінде Араб стандартты (MSA) және диалекты араб (DA) деңгейіндегі екінші ортақ тапсырмаға жі Жүйе ел деңгейінде және ауыл деңгейінде MSA/DA идентификациясын шешу үшін бірнеше тапсырмалар оқыту үлгісіне негізделген. Соңғы MTL үлгісі ортақтастырылған екі бағытты кодтардың түрлендіруші (BERT) кодтарынан, екі тапсырманың белгілі түрлендіруші қабаттарынан, екі классификациясы. Біздің негізгі идеямыз, ел мен ауыл MSA/DA идентификациясы үшін тапсырмалар дискриминациялық және тапсырмалардың ортақтастырылған мүмкіндіктерін өзгерту. Табылған нәтижелер MTL моделіміздің көпшілігінде бір тапсырма үлгілерін жасайды.</abstract_kk>
      <abstract_it>Il dialetto e l'identificazione della lingua standard sono compiti cruciali per molte applicazioni di elaborazione della lingua naturale araba. In questo articolo presentiamo il nostro sistema basato sull'apprendimento profondo, sottoposto al secondo compito condiviso NADI per l'identificazione a livello nazionale e provinciale dell'arabo standard moderno (MSA) e dell'arabo dialettale (DA). Il sistema si basa su un modello end-to-end deep Multi-Task Learning (MTL) per affrontare sia l'identificazione MSA/DA a livello nazionale che provinciale. Quest'ultimo modello MTL consiste in un codificatore BERT (Bidirectional Encoder Representation Transformers) condiviso, due livelli di attenzione specifici per attività e due classificatori. La nostra idea chiave è quella di sfruttare sia le funzionalità task-discriminative che inter-task condivise per l'identificazione MSA/DA paese e provincia. I risultati ottenuti mostrano che il nostro modello MTL supera i modelli single-task sulla maggior parte delle sottoattività.</abstract_it>
      <abstract_lt>Daugelio arabų gamtos kalbų apdorojimo programų dialektas ir standartinis kalbų identifikavimas yra esminės užduotys. Šiame dokumente pristatome savo giliai mokymosi pagrindu grindžiamą sistemą, kuri buvo pateikta antrajai bendrai NADI užduotims nustatyti šiuolaikinę standartinę arabišką (MSA) ir dialektinę arabišką (DA) šalies ir provincijų lygmeniu. The system is based on an end-to-end deep Multi-Task Learning (MTL) model to tackle both country-level and province-level MSA/DA identification.  The latter MTL model consists of a shared Bidirectional Encoder Representation Transformers (BERT) encoder, two task-specific attention layers, and two classifiers.  Mūsų pagrindinė idėja yra sutelkti dėmesį ir į užduočių diskriminacinius, ir užduočių tarpusavio bendrus požymius šalies ir provincijos MSA/DA identifikavimui. The obtained results show that our MTL model outperforms single-task models on most subtasks.</abstract_lt>
      <abstract_ka>დიალექტი და სტანდარტური ენის ინდიდინტიფიკაცია მნიშვნელოვანი რაოდენობები არაბური ენის პროცესი პროცესი პროგრამებისთვის. ამ დოკუნეში ჩვენ ჩვენი ძალიან სწავლის ბაზის სისტემა, რომელიც NADI-ს მეორე გაყოფილი საქმე დავამუშავებულია ქვეყანის დონეზე და პროვინტის დონეზე იდენტიფიკაციის მოდინარებული არაბული ( სისტემა დასაწყებელი მრავალური მოსწავლების (MTL) მოდელზე, რომელიც ორივე ქვეყნების დონე და პროვინტის დონე MSA/DA ინდიდინტიფიკაციის გამოყენება. შემდეგ MTL მოდელი იყოს გაყოფილი ორედირექციონალური კოდირენსტრანსტრანსტრანსტრაციის კოდირებისგან (BERT) კოდირებისგან, ორი დავალების განსაკუთრებული მონაცემები და ორი კო ჩვენი მნიშვნელოვანი იდეა, რომ დავაკეთებული დისკრიმინატიური და საერთო დავაკეთებული ფუნქციების განსაზღვრება ქვეყანის და პროვინტის MSA/DA ინდიდინტიფიკაციის გამოყენება. ჩვენი MTL მოდელი ერთადერთი დავალების მოდელზე უფრო მეტად გამოყენება.</abstract_ka>
      <abstract_ml>സ്വാഭാവികമായ അറബി ഭാഷ പ്രയോഗങ്ങള്‍ ഈ പത്രത്തില്‍ ഞങ്ങള്‍ നമ്മുടെ ആഴത്തെ പഠിക്കുന്ന സിസ്റ്റം കാണിക്കുന്നു, രണ്ടാമത്തെ നാഡിയില്‍ പങ്കാളിയുള്ള ജോലിക്ക് കൊടുത്തിരിക്കുന് ഈ സിസ്റ്റത്തിന്റെ അടിസ്ഥാനത്ത് ഒരു ആഴത്തിലേക്ക് അവസാനിപ്പിക്കുന്നുണ്ട്- അളവിലേക്കു് ആഴത്തിലേക്കു് ആഴത്തെ പല-ടാസ്ക് പ അവസാനത്തെ MTL മോഡല്‍ പങ്കെടുത്ത Bidirectional Encoder Representation Transformers (BERT) എന്‍കോഡര്‍, രണ്ടു ജോലി- പ്രത്യേക ശ്രദ്ധ ക്രമങ്ങള്‍, രണ്ടു വിഭിന്നതകള്‍. നമ്മുടെ പ്രധാന ആശയം രാജ്യത്തിനും പ്രദേശം MSA/DA തിരിച്ചറിയുന്നതിനുമുള്ള ജോലിയുടെ വ്യത്യാസവും വിഭാഗിച്ചിരിക്കു സമ്പാദിച്ച ഫലങ്ങള്‍ കാണിച്ചു കൊണ്ടിരിക്കുന്നത് നമ്മുടെ എംടിഎല്‍ മോഡല്‍ കൂടുതല്‍ സബ്ജോട്ടുകളില്‍ ഒരു മോ</abstract_ml>
      <abstract_mt>Id-dijaleka u l-identifikazzjoni standard tal-lingwa huma kompiti kruċjali għal ħafna applikazzjonijiet tal-ipproċessar naturali tal-lingwa Għarbija. In this paper, we present our deep learning-based system, submitted to the second NADI shared task for country-level and province-level identification of Modern Standard Arabic (MSA) and Dialectal Arabic (DA).  Is-sistema hija bbażata fuq mudell ta’ Tagħlim Multikompiti (MTL) profond minn tmiem sa tmiem biex tiġi indirizzata kemm l-identifikazzjoni MSA/DA fil-livell tal-pajjiż kif ukoll fil-livell tal-provinċja. Dan tal-a ħħar mudell MTL jikkonsisti f’kodifikatur komuni ta’ Trasformaturi tar-Rappreżentanza Bidirezzjonali tal-Kodiċi (BERT), żewġ saffi ta’ attenzjoni speċifiċi għall-kompiti, u żewġ klassifikaturi. Our key idea is to leverage both the task-discriminative and the inter-task shared features for country and province MSA/DA identification.  Ir-riżultati miksuba juru li l-mudell MTL tagħna jwettaq mudelli ta’ kompitu wieħed fuq ħafna sottomistoqsijiet.</abstract_mt>
      <abstract_hu>A tárcsázás és a szabványos nyelvazonosítás kulcsfontosságú feladat számos arab természetes nyelvfeldolgozó alkalmazás számára. Jelen tanulmányban bemutatjuk a modern standard arab (MSA) és dialektikus arab (DA) országszintű és tartományszintű azonosítására irányuló második NADI közös feladatunkat. A rendszer egy end-to-end mély Multi-Task Learning (MTL) modellre épül, amely mind az országszintű, mind a tartományszintű MSA/DA azonosítását kezeli. Az utóbbi MTL modell egy megosztott kétirányú kódoló reprezentációs transzformátorokból (BERT) kódolóból, két feladatspecifikus figyelem rétegből és két osztályozóból áll. Kulcsfontosságú ötletünk, hogy mind a feladat-diszkriminatív, mind a feladatok közötti megosztott funkciókat kihasználjuk az ország és tartomány MSA/DA azonosításához. Az elért eredmények azt mutatják, hogy MTL modellünk a legtöbb részfeladatban felülmúlja az egyfeladatos modelleket.</abstract_hu>
      <abstract_ms>Dialeksi dan pengenalan bahasa piawai adalah tugas penting bagi banyak aplikasi pemprosesan bahasa alam Arab. Dalam kertas ini, kami memperkenalkan sistem berdasarkan belajar dalam kami, dihantar ke tugas berkongsi NADI kedua untuk pengenalan aras negara dan aras provinsi Arab Standar Modern (MSA) dan Arab Dialektik (DA). Sistem ini berdasarkan model belajar berbilang tugas (MTL) yang mendalam akhir-akhir untuk menangani pengenalan MSA/DA aras negara dan provinsi. The latter MTL model consists of a shared Bidirectional Encoder Representation Transformers (BERT) encoder, two task-specific attention layers, and two classifiers.  Idea utama kita adalah untuk menggunakan kedua-dua ciri-ciri yang diskriminatif dan berkongsi antara-tugas untuk pengenalan negara dan provinsi MSA/DA. The obtained results show that our MTL model outperforms single-task models on most subtasks.</abstract_ms>
      <abstract_mn>Шинэ сонголт болон стандарт хэл тодорхойлолт нь олон Араб байгалийн хэл үйлдвэрлэх програмын тулд чухал ажил юм. Энэ цаасан дээр бид суралцах сургалтын үндсэн системийг тайлбарлаж, улсын түвшинд, орчин үеийн Араб стандарт (MSA) болон Диалект Араб (DA) болон улсын түвшинд хуваалцах хоёр дахь даалгавар руу дамжуулагдс Энэ систем улсын түвшинд болон орон нутгийн түвшинд MSA/DA танихын тулд хамгийн гүн гүнзгий олон-Task Learning (MTL) загвар дээр суурилсан. Сүүлийн MTL загвар нь хоёр ажлын төвөгтэй анхаарлын давхар, хоёр анхаарлын давхар, хоёр анхаарлын төвөгтэй холбоотой байдаг. Бидний хамгийн чухал санаа нь улс болон орон нутгийн MSA/DA-ын тодорхойлолтын хоёр даалгаварын ялгаатай болон хоорондох үйл ажиллагааг ашиглах юм. Шинэ гарсан үр дүнд бидний MTL загвар ихэнх суурь асуудлын нэг даалгаварын загварыг дамжуулдаг гэдгийг харуулсан.</abstract_mn>
      <abstract_ro>Dialectul și identificarea limbii standard sunt sarcini esențiale pentru multe aplicații de prelucrare a limbii naturale arabe. În această lucrare, prezentăm sistemul nostru bazat pe învățare profundă, supus celei de-a doua sarcini comune NADI pentru identificarea la nivel de țară și provincie a Arabei Standard Moderne (MSA) și Arabei Dialectale (DA). Sistemul se bazează pe un model complet de învățare multisarcină (MTL) pentru a aborda identificarea MSA/DA atât la nivel de țară, cât și la nivel de provincie. Cel de-al doilea model MTL constă dintr-un codificator BERT (bidirecțional Encoder Representation Transformers), două straturi de atenție specifice sarcinilor și două clasificatoare. Ideea noastră cheie este de a valorifica atât caracteristicile discriminatorii de sarcini, cât și caracteristicile partajate inter-sarcini pentru identificarea MSA/DA a țării și provinciei. Rezultatele obținute arată că modelul nostru MTL depășește modelele cu o singură sarcină la majoritatea subactivităților.</abstract_ro>
      <abstract_pl>Dialekt i standardowa identyfikacja języka są kluczowymi zadaniami dla wielu arabskich aplikacji przetwarzania języka naturalnego. W niniejszym artykule przedstawiamy nasz system oparty na głębokim uczeniu, poddany drugiemu wspólnemu zadaniu NADI dotyczącemu identyfikacji na poziomie kraju i prowincji współczesnego standardowego arabskiego (MSA) i dialektalnego arabskiego (DA). System opiera się na kompleksowym modelu wielozadaniowego uczenia się (MTL) w celu rozwiązania identyfikacji MSA/DA na poziomie kraju i prowincji. Ten ostatni model MTL składa się ze wspólnego kodera transformatorów reprezentacyjnych koderów dwukierunkowych (BERT), dwóch warstw uwagi specyficznych dla zadania oraz dwóch klasyfikatorów. Naszą kluczową ideą jest wykorzystanie zarówno funkcji dyskryminacyjnych, jak i wspólnych między zadaniami do identyfikacji MSA/DA kraju i prowincji. Uzyskane wyniki pokazują, że nasz model MTL przewyższa modele jednozadaniowe w większości podzadań.</abstract_pl>
      <abstract_sr>Dijaletiranje i standardna identifikacija jezika su ključni zadatak za mnoge aplikacije prirodne obrade arapskog jezika. U ovom papiru predstavljamo naš duboki sistem na osnovu učenja, predan drugim zajedničkom zadatku NADI za identifikaciju modernog standardnog arapskog (MSA) i dijalektnog arapskog (DA) na zemlji i pokrajinskog nivoa. Sistem se temelji na modelu učenja mnogo zadataka (MTL) na kraju do kraja kako bi se riješio identifikacija zemaljske i provincijske nivoe MSA/DA. Posljednji MTL model se sastoji od zajedničkog kodera za dvosmjerne transformacije kodera za predstavljanje kodera (BERT), dva sloja pažnje na zadatke i dva klasifikatora. Naša ključna ideja je da utičemo na diskriminaciju zadataka i zajedničke funkcije za identifikaciju zemlje i provincije MSA/DA. Nabavljeni rezultati pokazuju da naš model MTL iznosi jedan zadatak na većini podataka.</abstract_sr>
      <abstract_si>අරාබික භාෂාව පරීක්ෂණය සහ ප්‍රමාණය භාෂාව පරීක්ෂණය විශේෂ වැඩක් අරාබික භාෂාව ප්‍රක්‍ මේ පත්තරේ අපි අපේ ගොඩක් ඉගෙනීමේ පද්ධතිය පෙන්වන්නේ, දෙවෙනි NADI භාවිත වැඩකට රාජ්ය සහ ප්‍රදේශ ප්‍රමාණය අරාබිය (MSA) සහ අරාබිය (DA පද්ධතිය අවසානයෙන් අවසානයෙන් අවසානයෙන් ගොඩක් වැඩක් ඉගෙනීම (MTL) නිර්මාණයක් තියෙනවා දේශ ස්ථානය සහ ප්‍රදේශ-ස්ථා අන්තිම MTL මොඩල් එකේ සම්බන්ධ විදිහට සම්බන්ධ විදිහට සංකේතකයෙන් ප්‍රතිනිධානකය (BERT) සංකේතකයෙන්, වැඩක් විශේෂ අපේ වැදගත් අදහස තමයි රාජ්ය සහ ප්‍රදේශ MSA/DA පරික්ෂණය සඳහා වැදගත් වැදගත් වැදගත් වැදගත් වැදගත් වැදගත් වැඩේ අ ගත්ත ප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍රතිප්‍ර</abstract_si>
      <abstract_so>Aqoonsiga afka caadiga ah iyo aqoonsiga afka caadiga ah waa shaqooyin muhiim ah oo loo baaraandegayo codsiyo badan oo af Carabi ah. Qoraalkan waxaynu ku soo bandhignaa nidaamka hoose ee waxbarashada, waxaana loo dhiibay shaqada labaad ee NADI oo loo sharciyey aqoonsashada afka waddanka iyo gobolka ee asalka ah Carabiga (MSA) iyo Dialectal Carabiga (DA). Isticmaalku wuxuu ku saleysan yahay qaab aad u dheer waxbarasho shaqo badan (MTL) si uu u qabsado aqoonsiga dowladda iyo gobolka oo dhan ee MSA/DA. Tusaalada ugu dambeeya MTL waxaa ka mid ah qayb la qaybsan koordiyuhu (BERT) codsigiisa (BERT), laba qasnaan oo gaar ah oo la jeedo, iyo laba fasax. Fikiradayada muhiimka ah waa in loo soo bandhigaa aqoonsiga dowladda iyo gobolka MSA/DA. Arrimaha la helay waxay muuqataa in qaababkayaga MTL uu ka samaysto qaabab kali ah oo ku saabsan meelaha badan.</abstract_so>
      <abstract_sv>Dialekt och standardiserad språkidentifiering är viktiga uppgifter för många arabiska naturliga språkbearbetningsapplikationer. I denna uppsats presenterar vi vårt djupinlärningsbaserade system, inlämnat till den andra NADI delade uppgiften för landsnivå och provinsnivå identifiering av Modern Standard Arabic (MSA) och Dialektal Arabic (DA). Systemet är baserat på en heltäckande modell för multi-Task Learning (MTL) för att hantera både landsnivå och provinsnivå MSA/DA identifiering. Den senare MTL-modellen består av en gemensam BERT-kodare (Bidirectional Encoder Representation Transformers), två uppgiftsspecifika uppmärksamhetslager och två klassificerare. Vår huvudidé är att utnyttja både uppgiftsdiscriminerande och de delade funktionerna för land och provins MSA/DA identifiering. De erhållna resultaten visar att vår MTL-modell överträffar enkeluppgiftsmodeller på de flesta underuppgifter.</abstract_sv>
      <abstract_mk>Dialect and standard language identification are crucial tasks for many Arabic natural language processing applications.  In this paper, we present our deep learning-based system, submitted to the second NADI shared task for country-level and province-level identification of Modern Standard Arabic (MSA) and Dialectal Arabic (DA).  Системот се базира на модел од крај до крај на длабоко мултизадачно учење (МТЛ) за решавање на идентификацијата на MSA/ДА на ниво на земја и провинција. The latter MTL model consists of a shared Bidirectional Encoder Representation Transformers (BERT) encoder, two task-specific attention layers, and two classifiers.  Нашата клучна идеја е да ги искористиме дискриминативните и меѓузадачните заеднички карактеристики за идентификацијата на земјата и покраината MSA/DA. The obtained results show that our MTL model outperforms single-task models on most subtasks.</abstract_mk>
      <abstract_no>Vel og standard språk- identifisering er viktige oppgåver for mange arabiske naturspråk- handlingsprogrammer. I denne papiret presenterer vi vårt dyp læringsbasert system, som er sendt til den andre delte NADI-oppgåva for identifikasjon av landnivå og provinsenivå i moderne arabiske (MSA) og dialektiske arabiske (DA). Systemet er basert på ein dyp multioppgåver-læring (MTL) modell for å løysa både landnivå og provinsenivå MSA/DA-identifikasjon. Det siste MTL-modellen inneheld ein delt bikdirektionalt koderingstransformasjonsformatorar (BERT), to oppgåvelege oppmerkslag og to klassifikatorar. Nøkkelideen vårt er å levera både oppgåvediskriminasjonen og delte funksjonar for landet og provinset MSA/DA-identifikasjon. Dette opptekte resultatet viser at MTL-modellen vår utfører enkelte oppgåvemodeller på dei fleste underspørjingane.</abstract_no>
      <abstract_ta>Dialect and standard language identification are crucial tasks for many Arabic natural language processing applications.  இந்த காக்கியத்தில், நாம் எங்கள் ஆழமான கற்றுக்கொள்ள முறைமையை காண்பிக்கிறோம், இரண்டாவது NADI பகிர்ந்த பணிக்கு கொடுக்கப்பட்டுள்ளோம் நாடு நில முடிவிலிருந்து முடிவில் உள்ள ஆழமான பல பணி கற்றுக்கொள்ள (MTL) மாதிரி அமைப்பு அடிப்படையில் உள்ளது இரு நாடு நிலையும் மற்றும் பிராந் கடைசி MTL மாதிரி பங்கிடப்பட்ட Bidirectional Encoder Representation Transfers (BERT) குறியீடு, இரண்டு பணிக்குறிப்பிட்ட குறிப்பிட்ட கவனம் அடுக்குகள், மற்றும் இரண்டு வகை எங்கள் முக்கிய கருத்து என்னவென்றால் நாடு மற்றும் பிரநாட்டின் MSA/DA அடையாளத்திற்கும் இடையேற்ற பணி குணங்களை ஒப்புக்கொ பெற்ற முடிவுகள் பெரும்பாலான துணை பணிகளில் எங்கள் MTL மாதிரி ஒரே பணி மாதிரிகளை செயல்படுத்துகிறது என்பதை கா</abstract_ta>
      <abstract_ur>ڈائیلٹ اور استاندارڈ زبان کی شناسایی بہت سی عربی طبیعی زبان پردازش کاربریاں کے لئے اہم کام ہیں. اس کاغذ میں ہم اپنی عمیق سیکھنے کی بنیادی سیسٹم کو پیش کرتے ہیں، دوسری NADI کے ساتھ ملک سطح اور منطقه سطح کی معلومات کے لئے ملک سطح اور منطقه سطح عربی (MSA) اور دیالکتال عربی (DA) کے ذریعہ مشترک کام کے لئے پیش کیا جاتا سیسٹم ایک عمیق مثالٹاکس کی تعلیم (MTL) موڈل پر بنیاد ہے کہ دونوں ملک سطح اور منطقه سطح MSA/DA پہچان کرنے کے لئے استعمال کرے۔ آخرین MTL موڈل ایک مشترک دودئیرسینٹ اکنوڈر ریسپینسٹ ٹرنفسٹر (BERT) کا کوڈر ہے، دو کام خاص توجه لائر اور دو کلاسپیر سے ہے. ہماری اصلی فکر یہ ہے کہ کشور اور منطقه MSA/DA کی شناسایی کے لئے کام-جدائی اور inter-task کے مشترکین فرصت کو فائدہ پہنچانا ہے. پائی ہوئی نتیجے دکھاتے ہیں کہ ہمارے MTL موڈل اکثر زیادہ سٹسٹسٹ کے ذریعے ایک ٹاکس موڈل سے کام کرتا ہے۔</abstract_ur>
      <abstract_uz>Koʻp arab tilni boshqarish dasturlari uchun dialek va andoza tillar identifikati muhim vazifalar. Bu qogʻozda biz juda yaxshi o'rganish asosiy tizimimizni hosil qilamiz, davlat darajasi va provador darajasi (MSA) va Dialektal arab (DA) uchun ikkinchi narsa bilan birlashtirilgan vazifani qo'shib beramiz. Name Name Bizning muhim fikrimimiz, bu vazifani ajratish va har bir vazifaning boshqa xususiyatlarini ko'rsatish mumkin, va MSA/DA tashkilotning boshqa shakllarini ko'rsatish. Muvaffaqiyatli natijalar ko'pchilik vazifalarda MTL modellarimiz bir vazifa modellarini bajaradi.</abstract_uz>
      <abstract_vi>Việc nhận dạng ngôn ngữ chuẩn là việc quan trọng cho nhiều ứng dụng xử lý ngôn ngữ tự nhiên Ả rập. Trong tờ giấy này, chúng tôi giới thiệu hệ thống nghiên cứu sâu, được gửi đến một nhiệm vụ chung của NASI cho việc xác định cấp quốc gia và cấp độ nhận dạng của Chuột Rập chuẩn hiện đại (MSA) và Văn học Ả Rập (DA). The system is based on a end-to-end deep multiTask Learning (MTV) model to handle both country-level và tỉnh-level MSA/DA identification. Mẫu MTV cuối cùng gồm một bộ mã hóa Mô hình Diễn biến theo giao thức (BERT) bị chia sẻ, hai lớp tập trung đặc nhiệm, và hai phân loại. Ý tưởng chủ chốt của chúng ta là động lực cho cả sự phân biệt nhiệm vụ và chia sẻ các nhiệm vụ cho việc xác định đất nước và tỉnh MSA/DA. Những kết quả xác định cho thấy mô hình MTV hoàn thiện quy mô đơn vị trên hầu hết các mặt phụ đề.</abstract_vi>
      <abstract_bg>Диалектът и стандартната езикова идентификация са решаващи задачи за много приложения за обработка на арабски естествен език. В настоящата статия представяме нашата система, базирана на дълбоко обучение, представена на втората споделена задача на НАДИ за идентификация на ниво държава и провинция на съвременен стандартен арабски (МСА) и диалектален арабски (ДА). Системата се основава на модел на задълбочено многозадачи обучение от край до край, за да се справи както с идентифицирането на МСА/ДА на ниво държава, така и на ниво провинция. Последният модел се състои от споделен двупосочен кодер трансформатор за представяне на кодери (BERT), два слоя на внимание, специфични за задачата, и два класификатора. Нашата основна идея е да използваме както дискриминационните, така и споделените между задачите характеристики за идентификацията на МСА/ДА в страната и провинцията. Получените резултати показват, че нашият модел надминава моделите с една задача при повечето подзадачи.</abstract_bg>
      <abstract_da>Dialekt og standard sprogidentifikation er afgørende opgaver for mange arabiske natursprogbehandlingsapplikationer. I denne artikel præsenterer vi vores deep learning-baserede system, der er indsendt til den anden NADI delte opgave for land- og provinsniveau identifikation af Modern Standard Arabic (MSA) og Dialektal Arabic (DA). Systemet er baseret på en end-to-end dyb Multi-Task Learning (MTL) model til at tackle både landeniveau og provinsniveau MSA/DA identifikation. Sidstnævnte MTL-model består af en delt BERT-koder (Bidirectional Encoder Representation Transformers), to opgavespecifikke opmærksomhedslag og to klassificeringer. Vores nøgleidé er at udnytte både opgaveforskelsbehandling og interopgavefælles funktioner til land og provins MSA/DA identifikation. De opnåede resultater viser, at vores MTL model overgår single-task modeller på de fleste underopgaver.</abstract_da>
      <abstract_nl>Dialect en standaardtaal identificatie zijn cruciale taken voor veel Arabische natuurlijke taal verwerkingstoepassingen. In dit artikel presenteren we ons deep learning-gebaseerd systeem, dat is ingediend bij de tweede gezamenlijke NADI-taak voor de identificatie van Modern Standaard Arabisch (MSA) en Dialectaal Arabisch (DA) op landenniveau en provincieniveau. Het systeem is gebaseerd op een end-to-end diep Multi-Task Learning (MTL) model om zowel op land- als provincieniveau MSA/DA identificatie aan te pakken. Het laatste MTL-model bestaat uit een gedeelde BERT-encoder (Bidirectional Encoder Representation Transformers), twee taakspecifieke aandachtslagen en twee classificatoren. Ons belangrijkste idee is om gebruik te maken van zowel de taken-discriminerende als de inter-task gedeelde functies voor land en provincie MSA/DA identificatie. De verkregen resultaten tonen aan dat ons MTL-model op de meeste subtaken beter presteert dan single-task modellen.</abstract_nl>
      <abstract_hr>Dijaliranje i standardna identifikacija jezika su ključni zadatak za mnoge aplikacije obrade prirodnog jezika Arapskog jezika. U ovom papiru predstavljamo naš duboki sustav na temelju učenja, predan drugim zajedničkom zadatku NADI za identifikaciju modernog standardnog arapskog (MSA) i dijalektnog arapskog (DA) na zemlji i pokrajinskog nivoa. Sistem se temelji na modelu učenja višestrukih zadataka (MTL) na kraju do kraja kako bi se riješio identifikacija zemaljske i provincijske razine MSA/DA. Posljednji model MTL sastoji se od podijeljenog kodera za dvosmjerne transformacije predstavljanja kodera (BERT), dva slojeva posebne pažnje na zadatke i dva klasifikatora. Naša ključna ideja je utjecati na diskriminaciju zadataka i zajedničke funkcije za identifikaciju zemlje i provincije MSA/DA. Naučeni rezultati pokazuju da naš model MTL iznosi jednozadatačne modele na većini podataka.</abstract_hr>
      <abstract_de>Dialekt und Standard-Spracherkennung sind wichtige Aufgaben für viele arabische Anwendungen zur Verarbeitung natürlicher Sprache. In diesem Beitrag stellen wir unser Deep Learning-basiertes System vor, das der zweiten gemeinsamen NADI-Aufgabe zur Identifizierung von Modernem Standardarabisch (MSA) und Dialektalem Arabisch (DA) auf Länderebene und Provinzebene unterzogen wurde. Das System basiert auf einem End-to-End Deep Multi-Task Learning (MTL)-Modell, um sowohl die Identifikation von MSA/DA auf Landes- als auch Provinzebene zu bewältigen. Das letztgenannte MTL-Modell besteht aus einem gemeinsamen BERT-Encoder (Bidirektional Encoder Representation Transformers), zwei aufgabenspezifischen Aufmerksamkeitsschichten und zwei Klassifikatoren. Unsere Kernidee ist es, sowohl die aufgabendiskriminierenden als auch die aufgabendiskriminierenden Funktionen für die Identifikation von Land und Provinz MSA/DA zu nutzen. Die erhaltenen Ergebnisse zeigen, dass unser MTL-Modell bei den meisten Teilaufgaben Einzelaufgabenmodelle übertrifft.</abstract_de>
      <abstract_id>Dialeksi dan identifikasi bahasa standar adalah tugas penting bagi banyak aplikasi proses bahasa alam Arab. Dalam kertas ini, kami memperkenalkan sistem berdasarkan belajar dalam kami, yang dipanggil ke tugas NADI kedua yang sama untuk identifikasi tingkat negara dan provinsi dari Arab Standard Modern (MSA) dan Arab Dialektik (DA). Sistem ini berdasarkan model belajar multitask (MTL) yang mendalam dari akhir ke akhir untuk menangani identifikasi MSA/DA. Model MTL yang terakhir terdiri dari pengekode Representation Transformers Bidirectional Encoder (BERT), dua lapisan perhatian spesifik tugas, dan dua klasifikasi. Ide kunci kita adalah untuk menggunakan kedua-dua karakteristik yang diskriminatif dan intertugas berbagi untuk identifikasi negara dan provinsi MSA/DA. Hasil yang diperoleh menunjukkan bahwa model MTL kita melampaui model tugas tunggal pada kebanyakan subtasks.</abstract_id>
      <abstract_ko>사투리와 표준 언어 식별은 많은 아랍어 자연 언어 처리 응용 프로그램의 관건적인 임무이다.본고에서 우리는 심도 있는 학습을 바탕으로 하는 우리의 시스템을 소개했다. 이 시스템은 두 번째 NADI 공유 임무에 제출되어 국가급과 성급에서 현대 표준 아랍어(MSA)와 사투리 아랍어(DA)를 식별하는 데 사용된다.이 시스템은 국가급과 성급 MSA/DA 식별 문제를 해결하기 위해 끝에서 끝까지의 심도 있는 멀티태스킹 학습(MTL) 모델을 바탕으로 한다.다음 MTL 모델은 공유된 양방향 인코더 표시 변환기 (BERT) 인코더, 작업에 대한 두 개의 주의층과 두 개의 분류기로 구성되어 있다.우리의 주된 생각은 임무의 구분성과 임무 간의 공유 특성을 이용하여 국가와 성의 MSA/DA를 식별하는 것이다.그 결과 우리의 MTL 모델은 대부분의 하위 임무에서 단일 임무 모델보다 우수하다는 것을 알 수 있다.</abstract_ko>
      <abstract_fa>برگزیدن و شناسایی زبان استاندارد کارهای مهم برای بسیاری از کاربردهای پرداخت زبان طبیعی عربی است. در این کاغذ، سیستم عمیق یادگیری ما را پیشنهاد می‌کنیم، که به کار دوم مشترک NADI برای شناسایی سطح کشور و سطح استاندارد عربی استاندارد مدرن (MSA) و عربی دیالکتی (DA) ارائه می‌شود. این سیستم بر اساس یک مدل یادگیری عمیق چندین کار (MTL) برای حل شناسایی سطح کشور و سطح استان MSA/DA است. این مدل آخرین MTL از یک قالب تغییر دهنده‌های نمایش‌دهنده‌ی دو طریقه‌ای (BERT) شرکت می‌کند، دو لایه توجه ویژه‌ای برای کار و دو تنظیم‌کننده است. ایده‌ی کلید ما این است که برای شناسایی کشور و استان MSA/DA، هر یک از ویژه‌های مشترک و مشترک‌های عملیات و مشترک‌های مشترک را تأثیر دهیم. نتیجه‌های دریافت شده نشان می‌دهند که مدل MTL ما مدل‌های یک کار را بیشتر از زیر سوال‌ها انجام می‌دهد.</abstract_fa>
      <abstract_sw>Utambulisho na utambulisho wa lugha za kawaida ni kazi muhimu kwa matumizi mengi ya lugha za asili ya Kiarabu. Katika karatasi hii, tunaonyesha mfumo wetu wa kujifunza kwa kina, uliotumiwa na kazi ya pili ya NADI inayoshirikisha kwa ajili ya kutambua kiwango cha nchi na kiwango cha jimbo cha Kiarabu cha Modern Standard (MSA) na Kiarabu cha Kidialectal (DA). Mfumo huo unajikita na mtindo wa Kufunza Mitandao ya Kuu (MTL) wa mwisho wa mwisho ili kupambana na utambulisho wa kiwango cha nchi na kilicho cha mkono cha MSA/DA. Mradi wa pili wa MTL unajumuisha jumbe ya watambulishi wa Uwekezaji wa Kiongozi wa Uwekezaji (BERT), vipande viwili maalum vya kazi, na wataalamu wawili. Wazo letu muhimu ni kuitumia kazi hizo za ubaguzi na kazi zinazosambazwa kwa ajili ya utambulisho wa nchi na jimbo la MSA/DA. Matokeo yaliyopata yanaonyesha kuwa mtindo wetu wa MTL unafanya mifano ya kazi moja kwa moja kwenye kazi nyingi.</abstract_sw>
      <abstract_tr>Saýlaw we standart dil tanymasy arabça dil işleýän uygulamalar üçin örän wajyp zady. Bu kagyzda çukur öwrenmek sistemamyzy görkezip, NADI-iň ikinji derejede we provinciýasynyň Modern Standardy Arapça (MSA) we Dialektik Arapça (DA) bilen bölegi paýlaşdyrylýar. Bu sistem ýurtyň derejesi we provinciýa-derejesi MSA/DA kimligini çözmek üçin iň soňunda gaty bir çoklu-Taýgy öwrenmesi (MTL) nusgasyna daýan ýar. Soňky MTL nusgasynyň paylaşyk Bidirectional Ködleme Taýýarlançylarynyň (BERT) ködlemeleri, iki işiň häzirki üns kalamlaryň we iki klasifikatçylaryň içine bar. Biziň açyk ideýimiz ýurt we provinciýa MSA/DA kimligini hem görevi-diskriminçylyk we meselelerimiz üçin üýtgetmekdir. Ilkinji netijeler MTL nusgymyzyň köp soraglaryň ýeke-täk nusgalarynyň üstüne çykarýandygyny görkezýär.</abstract_tr>
      <abstract_af>Dialeksie en standaard taal identifikasie is belangrike taak vir baie Arabiese natuurlike taal verwerking toepassings. In hierdie papier het ons ons diep leer-gebaseerde stelsel voorgestel, voorgestel aan die tweede NADI gedeelde taak vir land-vlak en provinsie-vlak-identifikasie van Moderne Standaard Arabiese (MSA) en Dialekteel Arabiese (DA). Die stelsel is gebaseer op 'n end-to-end diep Multi-Task Learning (MTL) model om beide landvlak en provinsie-vlak MSA/DA identifikasie te probeer. Die laaste MTL model bestaan van 'n gedeelde tweederigtinglike enkoder voorstelling Transformeerders (BERT) enkoder, twee taak spesifieke aandag laag en twee klassifiseerders. Ons sleutel idee is om beide die taak-diskriminasie en die inter-taak gedeelde funksies vir land en provinsie MSA/DA-identifikasie te verwyder. Die ontvangde resultate vertoon dat ons MTL-modell een-taak-modele op die meeste subtaske uitvoer.</abstract_af>
      <abstract_am>በአካባቢው ቋንቋ ማቀናጃ እና የአካባቢ ቋንቋ ፕሮግራሞች ላይ የሚያስፈልገው ሥራ ናቸው፡፡ በዚህ ፕሮግራም ውስጥ የጥልቅ ትምህርት መሠረታችንን እና ለሁለተኛው NADI ስራ ለሀገር ደረጃና የአውራሲው የአርባዊ ስርዓት (አሜሪካ) እና ዳሌካል ዐረብኛ (አ) እና ለዳሌክቲል አረቢያ (አ). የስርቨሮች ደረጃና አውቶማቲ አሜሪካ እና የአሜሪካ ደረጃን እና የአሜሪካ መግለጫ መቆጣጠር (MTL) ሞዴል ነው፡፡ የመጨረሻው MTL ሞዴል የተካፈለ Bidirectional Encoder Representation Transformers (BERT) encoder, ሁለት task-specific ጥያቄ ደረጃዎች እና ሁለት classifiers. የዋነታችን አሳብ የስራ-ልዩነኛ እና የአገሪቱ እና የአሜሪካ/አ.አ.አ.አ.አ.አ.አ.አ. የተገኘው ውጤቶች የMTL ሞዴል በብዙ ስራ ላይ አንድ ዶላዎችን የሚያደርግ ነው፡፡</abstract_am>
      <abstract_sq>Dialekti dhe identifikimi standard i gjuhës janë detyra vendimtare për shumë aplikacione të përdorimit të gjuhës natyrore arabe. Në këtë letër, ne paraqesim sistemin tonë të thellë bazuar në mësim, të paraqitur në detyrën e dytë të përbashkët të NADI për identifikimin e nivelit të vendit dhe të nivelit të krahinës të Arabit Modern Standard (MSA) dhe Arabit Dialektik (DA). Sistemi është bazuar në një model të thellë Mësimi Multi-Task (MTL) për të trajtuar si identifikimin e nivelit të vendit ashtu edhe të nivelit të krahinës MSA/DA. Modeli i fundit MTL përbëhet nga një kodues i përbashkët i përfaqësimit të koduesit dy-drejtues (BERT), dy shtresa vëmendjeje specifike për detyrat dhe dy klasifikues. Ideja jonë kryesore është të përdorim si funksionet diskriminuese për detyrat, ashtu edhe funksionet e përbashkëta ndër detyrat për identifikimin e vendit dhe provincës MSA/DA. Rezultatet e fituara tregojnë se modeli ynë MTL paraqet modelet me një detyrë të vetme në shumicën e nëndetyrave.</abstract_sq>
      <abstract_hy>Dialect and standard language identification are crucial tasks for many Arabic natural language processing applications.  Այս թղթի մեջ մենք ներկայացնում ենք մեր խորը ուսումնասիրությամբ հիմնված համակարգը, որը ներկայացվել է երկրի և նահանգային մակարդակի երկրորդ համագործակցած առաջադրանքին ժամանակակից ստանդարտ արաբերենի (MSA) և դիալեկտալ արաբերենի (DA) նշանակության համար: Համակարգը հիմնված է վերջ-վերջ խորը բազմախնդիրների ուսումնասիրության (MTL) մոդելի վրա, որպեսզի վերաբերվի երկրի և նահանգային մակարդակի MSA-ի և DA-ի հայտնաբերությանը: Վերջին MTL մոդելը կազմված է երկաուղղակի կոդավորիչների ներկայացման փոխակերպիչների (BER) կոդավորիչներից, երկու հատուկ ուշադրության շերտերից և երկու դասակարգիչներից: Մեր հիմնական գաղափարն այն է, որ մենք օգտագործենք աշխատանքների խտրականությունը, ինչպես նաև աշխատանքների միջև ընդհանուր հատկությունները երկրի և նահանգների MSA-ի և DA-ի հայտնաբերման համար: Ընդհանուր արդյունքները ցույց են տալիս, որ մեր MTL մոդելը առավել լավ է ընդունում մեկ խնդրի մոդելները մեծ մասում:</abstract_hy>
      <abstract_az>Seçim və standart dil tanımlaması çoxlu ərəb dili işləmə proqramları üçün çoxlu mövcuddur. Bu kağızda, bizim derin öyrənmək sistemimizi göstərdik, ülke seviyesi və province seviyesi Modern Standard Arabic (MSA) və Dialectal Arabic (DA) təşkil edilən ikinci NADI paylaşılmış iş işinə göndərildik. Sistem ölkə səviyyəsi və province-səviyyəsi MSA/DA kimliğini çəkmək üçün çoxlu-Task Öyrənməsi (MTL) modeli ilə dayanılır. Son MTL modeli paylaşılan iki yönəlli Encoder Representation Transformers (BERT) kodlayıcısıdır, iki işin müəyyən edilmiş paylaşım layers və iki klassifiklərdir. Bizim anahtar fikrimiz, ülke və province MSA/DA kimliğinin paylaşılması üçün işlər-diskriminatlı və işləri ilə birlikdə paylaşılan özellikləri təsir etməkdir. Qazandıqları sonuçlar MTL modeliyimizin çox çətinliklərdə tək iş modellərini göstərir.</abstract_az>
      <abstract_bn>অনেক আরবী ভাষার প্রাকৃতিক প্রক্রিয়ার অ্যাপ্লিকেশনের জন্য ডায়ালেক্ট এবং স্থান্য ভাষা পরিচিতি গুরুত এই কাগজটিতে আমরা আমাদের গভীর শিক্ষাভিত্তিক সিস্টেম উপস্থাপন করি, দ্বিতীয় নাডিআই দ্বিতীয় কাজে প্রদেশের দ্বিতীয় স্তর এবং প্রদেশের পরিচিতির জন্য দেশের স্তর এবং প্রদেশের স্তরের এমএসএ/DA পরিচিতির সাথে মোকাবেলা করার জন্য এই সিস্টেমের ভিত্তিতে রয়েছে। সর্বশেষ MTL মডেলের মধ্যে একটি শেয়ার করা বাইডেডিয়াল এনকোডার প্রতিনিধিত্ব প্রতিনিধি ট্রান্সফার্মার (BERT) এনকোডার, দুটি কাজ-নির্দ আমাদের গুরুত্বপূর্ণ ধারণা হচ্ছে দেশ এবং প্রদেশের এমএসএ/ডিএ পরিচয়ের জন্য কাজ-বৈষম্য এবং কাজের মধ্যে কাজের বিভিন্ন বৈষম্যে অর্জনের ফলাফল দেখাচ্ছে যে আমাদের এমটিএল মডেল বেশীরভাগ সাবটাসের মধ্যে একাকাজের মডেল প্রদর্শন করে।</abstract_bn>
      <abstract_ca>La dialecta i l'identificació de llenguatges estàndard són tasques crucials per a moltes aplicacions de processament de llenguatges naturals àrabs. En aquest paper, presentem el nostre sistema basat en l'aprenentatge profund, submetit a la segona tasca compartida de la NADI per identificar a nivell nacional i provincial l'àrab Modern Standard (MSA) i l'àrab Dialectal (DA). El sistema està basat en un model profund d'aprenentatge multitascat (MTL) de final a final per abordar la identificació MSA/DA a nivell nacional i provincial. L'últim model MTL consisteix en un codificador compartit de transformadors de representació bidireccional del codificador (BERT), dues capes d'atenció específica per a les tasques i dos classificadors. La nostra idea clau és aprofitar les característiques compartides per a la identificació dels països i provincies MSA/DA. Els resultats obtenits demostren que el nostre model MTL supera els models d'una sola tasca en la majoria de subtaskes.</abstract_ca>
      <abstract_bs>Dijaletiranje i standardna identifikacija jezika su ključni zadatak za mnoge aplikacije prirodne obrade arapskog jezika. U ovom papiru predstavljamo naš duboki sistem na osnovu učenja, predan drugim zajedničkom zadatku NADI za identifikaciju modernog standardnog arapskog (MSA) i dijalektnog arapskog (DA) na zemlji i pokrajinskog nivoa. Sistem se temelji na model dubokog učenja multizadataka (MTL) na kraju do kraja kako bi se riješio identifikacija zemaljske i provincijske razine MSA/DA. Posljednji MTL model se sastoji od zajedničkog kodera za dvosmjerne transformacije predstavljanja kodera (BERT), dva slojeva posebne pažnje na zadatke i dva klasifikatora. Naša ključna ideja je utjecati na diskriminaciju zadataka i zajedničke funkcije za identifikaciju zemlje i provincije MSA/DA. Naučeni rezultati pokazuju da naš model MTL iznosi jednozadatak na većini podataka.</abstract_bs>
      <abstract_et>Dialekt ja standardne keele identifitseerimine on olulised ülesanded paljude araabia looduskeele töötlemise rakenduste jaoks. Käesolevas töös tutvustame meie sügavõppepõhist süsteemi, mis on esitatud teisele NADI jagatud ülesandele kaasaegse standardse araabia (MSA) ja dialektilise araabia (DA) riigi tasandil ja provintsi tasandil tuvastamiseks. Süsteem põhineb täielikul mitme ülesandega õppe mudelil, et käsitleda nii riigi kui ka provintsi tasandi MSA/DA identifitseerimist. Viimane MTL mudel koosneb ühisest kahesuunalisest kodeerijate esindamise transformaatorist (BERT), kahest ülesandepõhisest tähelepanu kihist ja kahest klassifitseerijast. Meie põhieesmärk on kasutada nii ülesandeid diskrimineerivaid kui ka ülesandevahelisi ühiseid funktsioone riigi ja provintsi MSA/DA identifitseerimiseks. Saadud tulemused näitavad, et meie MTL mudel ületab enamiku alamülesannete puhul ühe ülesandega mudeleid.</abstract_et>
      <abstract_cs>Dialekt a standardní identifikace jazyka jsou klíčovými úkoly pro mnoho aplikací zpracování arabského přirozeného jazyka. V tomto článku představujeme náš systém založený na hlubokém učení, který byl předložen druhému sdílenému úkolu NADI pro identifikaci moderní standardní arabštiny (MSA) a dialektální arabštiny (DA) na úrovni zemí a provincií. Systém je založen na komplexním modelu víceúkolového učení (MTL) pro řešení identifikace MSA/DA na úrovni zemí i provincie. Druhý MTL model se skládá ze sdíleného snímače reprezentačních transformátorů (BERT), dvou vrstev pozornosti specifických pro úkoly a dvou klasifikátorů. Naší klíčovou myšlenkou je využít jak diskriminační, tak mezi úkoly sdílené funkce pro identifikaci zemí a provincie MSA/DA. Získané výsledky ukazují, že náš MTL model překonává jednotlivé modely na většině dílčích úkolů.</abstract_cs>
      <abstract_fi>Dialekti ja vakiokielen tunnistus ovat tärkeitä tehtäviä monissa arabian luonnollisen kielen käsittelysovelluksissa. Tässä artikkelissa esittelemme syvään oppimiseen perustuvaa järjestelmäämme, joka on toimitettu toiseen NADI-yhteiseen tehtävään modernin standardin arabian (MSA) ja dialektisen arabian (DA) maa- ja maakuntatason tunnistamiseksi. Järjestelmä perustuu kattavaan monitehtäväoppimisen (MTL) malliin, jossa käsitellään sekä maakohtaista että maakuntatason MSA/DA-tunnistetta. Jälkimmäinen MTL-malli koostuu jaetusta kaksisuuntaisesta koodaajien edustamisen muuntajasta (BERT), kahdesta tehtäväkohtaisesta huomiokerroksesta ja kahdesta luokittelijasta. Keskeisenä ajatuksenamme on hyödyntää sekä tehtäviä syrjiviä että tehtävien välisiä yhteisiä ominaisuuksia maan ja maakunnan MSA/DA tunnistamisessa. Saadut tulokset osoittavat, että MTL-mallimme suoriutuu yhden tehtävän malleista useimmissa alitehtävissä.</abstract_fi>
      <abstract_jv>Ngawe Nan pepulan iki, kita nambah sistem sing nggawe basa gambar n' al sistem sing nyebutakne ning disebarke sistem sing dibenalke nggo nambah segala kanggo nambah dhéwé lan nambah-nambah sing berarti Gambar uwong (SMA) lan diaelectal arab sing isi (Da). Sistem dadi basa ning model end-to-end deep Multi-task Learning The last MTL model is compromise a share Bidirectial Kita pangunahing punika wis diguwasi nggawe task-Diskripatif lan inter-task diputara pawaran kanggo langgambar lan mulasar SMA/Da Rasane sing wis mbut banjur dadi model MTL dumateng model sing kotak-task" lan nganggo barang pangutung</abstract_jv>
      <abstract_sk>Dialekt in standardna identifikacija jezika sta ključna naloga za številne aplikacije za obdelavo arabskega naravnega jezika. V prispevku predstavljamo sistem globokega učenja, ki je bil predložen drugi NADI skupni nalogi za identifikacijo sodobne standardne arabščine (MSA) in dialektične arabščine (DA) na ravni države in province. Sistem temelji na modelu poglobljenega večopravilnega učenja (MTL) od konca do konca za obravnavo identifikacije MSA/DA na ravni države in province. Slednji model MTL je sestavljen iz skupnega kodirnika BERT (Bidirectional Encoder Representation Transformers), dveh plasti pozornosti, specifičnih za opravila, in dveh klasifikatorjev. Naša ključna ideja je izkoristiti diskriminativne in mednaložbene skupne značilnosti za identifikacijo MSA/DA države in province. Dobljeni rezultati kažejo, da naš model MTL presega enonalogne modele pri večini podopravil.</abstract_sk>
      <abstract_he>דיאלקט וזיהוי שפה סטנדרטי הם משימות חשובות עבור שיעורי עיבוד שפת טבעיים רבים ערביים. בעיתון הזה, אנו מציגים את מערכת המבוססת על הלימודים העמוקה שלנו, שנשלחה למשימה המשותפת השנייה של NADI עבור זיהוי רמה מדינה ורמה המחוזית של ערבית סטנדרטית מודרנית (MSA) ו ערבית דיאלקטית (DA). המערכת מבוססת על דוגמנית למידה רבה-משימות (MTL) עמוקה סוף-סוף כדי להתמודד עם זיהוי MSA/DA ברמה מדינה וגם ברמה המחוזית. דוגמנית MTL האחרונה מורכבת ממתקן מייצג משותף של קודד שתי כיוונים (BERT), שתי שכבות תשומת לב ספציפית למשימות, ושני מסווגים. Our key idea is to leverage both the task-discriminative and the inter-task shared features for country and province MSA/DA identification.  התוצאות הנקבלות מראות שמודל MTL שלנו מוביל מודלים משימה אחת ברוב השאלות.</abstract_he>
      <abstract_ha>Shiryoyin harshe na shiryoyin ayuka na ƙayyade Ga wannan takardan, Munã halatar da tsarin da aka samar da shi masu ƙaranci, da aka saka zuwa na NADA da aikin na biyu wanda aka raba wa aikin NADA na ƙasan-daraja da sifar-daraja na Madaidaici (MAA) da Larabci na Dialakal (DA). @ item: inmenu Text Completion @ info: whatsthis Maɓallinmu na wajen gaura da za'a cika aikin da za'a yi gaura da fassarar aiki da kuma masu raba fassarar aikin da ke iya haɗa su ga wuri da bayani na MAA/DA. Ana sami matsala ya nuna cewa misalinmu na MTL na samar da misalin aiki guda a kan masu yawa.</abstract_ha>
      <abstract_bo>དབྱིབས་དམ་པ་དང་སྔོན་སྒྲིག་སྐད་རིགས་དམིགས་འཛུགས་ནི་ཨ་རབ་ཀྱི་སྤྱིར་བཏང་བའི་སྐད་རིགས In this paper, we present our deep learning-based system, submitted to the second NADI shared task for country-level and province-level identification of Modern Standard Arabic (MSA) and Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task Learning (MTL) model to tackle both country-level and province-level MSA/DA identification. The latter MTL model consists of a shared Bidirectional Encoder Representation Transformers (BERT) encoder, two task-specific attention layers, and two classifiers. ང་ཚོའི་གཙོ་ཆེ་བསམ་ནི་རྒྱལ་ཁབ་དང་རྒྱལ་ཁབ་གྱི་ཁྱད་ཆོས་དང་ཐོག་ལས་མཐུན་སྣེ་ཁྱད་དུ་གཏོང་ནི་ གྲངས་འབོར་ཡོད་པའི་འབྲས་བ་མང་ཙམ་མཁན་ལ་ང་ཚོའི་MTL མིག་དཔེ་དབྱིབས་གཞན་གཅིག་ལས་སྒྲུབ་བྱེད་མི་འདུག</abstract_bo>
      </paper>
    <paper id="32">
      <title>Country-level Arabic Dialect Identification using RNNs with and without <a href="https://en.wikipedia.org/wiki/Linguistic_feature">Linguistic Features</a><fixed-case>A</fixed-case>rabic Dialect Identification using <fixed-case>RNN</fixed-case>s with and without Linguistic Features</title>
      <author><first>Elsayed</first><last>Issa</last></author>
      <author><first>Mohammed</first><last>AlShakhori1</last></author>
      <author><first>Reda</first><last>Al-Bahrani</last></author>
      <author><first>Gus</first><last>Hahn-Powell</last></author>
      <pages>276–281</pages>
      <abstract>This work investigates the value of augmenting <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a> with <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a> for the Second Nuanced Arabic Dialect Identification (NADI) Subtask 1.2 : Country-level DA identification. We compare the performance of a simple word-level LSTM using pretrained embeddings with one enhanced using feature embeddings for engineered linguistic features. Our results show that the addition of explicit features to the <a href="https://en.wikipedia.org/wiki/Linear_time-invariant_system">LSTM</a> is detrimental to performance. We attribute this performance loss to the bivalency of some linguistic items in some text, ubiquity of topics, and participant mobility.</abstract>
      <url hash="56b4a547">2021.wanlp-1.32</url>
      <bibkey>issa-etal-2021-country</bibkey>
    </paper>
    <paper id="33">
      <title>Arabic Dialect Identification based on a Weighted Concatenation of TF-IDF Features<fixed-case>A</fixed-case>rabic Dialect Identification based on a Weighted Concatenation of <fixed-case>TF</fixed-case>-<fixed-case>IDF</fixed-case> Features</title>
      <author><first>Mohamed</first><last>Lichouri</last></author>
      <author><first>Mourad</first><last>Abbas</last></author>
      <author><first>Khaled</first><last>Lounnas</last></author>
      <author><first>Besma</first><last>Benaziz</last></author>
      <author><first>Aicha</first><last>Zitouni</last></author>
      <pages>282–286</pages>
      <abstract>In this paper, we analyze the impact of the weighted concatenation of TF-IDF features for the Arabic Dialect Identification task while we participated in the NADI2021 shared task. This study is performed for two <a href="https://en.wikipedia.org/wiki/Design_of_experiments">subtasks</a> : subtask 1.1 (country-level MSA) and subtask 1.2 (country-level DA) identification. The classifiers supporting our comparative study are Linear Support Vector Classification (LSVC), Linear Regression (LR), <a href="https://en.wikipedia.org/wiki/Perceptron">Perceptron</a>, Stochastic Gradient Descent (SGD), Passive Aggressive (PA), Complement Naive Bayes (CNB), MutliLayer Perceptron (MLP), and RidgeClassifier. In the evaluation phase, our <a href="https://en.wikipedia.org/wiki/System">system</a> gives F1 scores of 14.87 % and 21.49 %, for country-level MSA and DA identification respectively, which is very close to the average F1 scores achieved by the submitted <a href="https://en.wikipedia.org/wiki/System">systems</a> and recorded for both <a href="https://en.wikipedia.org/wiki/Task_(project_management)">subtasks</a> (18.70 % and 24.23 %).</abstract>
      <url hash="7a48b49f">2021.wanlp-1.33</url>
      <bibkey>lichouri-etal-2021-arabic</bibkey>
    </paper>
    <paper id="34">
      <title>Machine Learning-Based Approach for Arabic Dialect Identification<fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Hamada</first><last>Nayel</last></author>
      <author><first>Ahmed</first><last>Hassan</last></author>
      <author><first>Mahmoud</first><last>Sobhi</last></author>
      <author><first>Ahmed</first><last>El-Sawy</last></author>
      <pages>287–290</pages>
      <abstract>This paper describes our systems submitted to the Second Nuanced Arabic Dialect Identification Shared Task (NADI 2021). Dialect identification is the task of automatically detecting the source variety of a given text or speech segment. There are four <a href="https://en.wikipedia.org/wiki/Russian_language">subtasks</a>, two subtasks for country-level identification and the other two <a href="https://en.wikipedia.org/wiki/Russian_language">subtasks</a> for province-level identification. The data in this <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> covers a total of 100 provinces from all 21 Arab countries and come from the <a href="https://en.wikipedia.org/wiki/Twitter">Twitter domain</a>. The proposed systems depend on five machine-learning approaches namely Complement Nave Bayes, <a href="https://en.wikipedia.org/wiki/Support-vector_machine">Support Vector Machine</a>, <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">Decision Tree</a>, <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic Regression</a> and Random Forest Classifiers. F1 macro-averaged score of Nave Bayes classifier outperformed all other <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a> for development and test data.</abstract>
      <url hash="00520d27">2021.wanlp-1.34</url>
      <bibkey>nayel-etal-2021-machine</bibkey>
    </paper>
    <paper id="36">
      <title>Overview of the WANLP 2021 Shared Task on Sarcasm and Sentiment Detection in Arabic<fixed-case>WANLP</fixed-case> 2021 Shared Task on Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Ibrahim</first><last>Abu Farha</last></author>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>296–305</pages>
      <abstract>This paper provides an overview of the WANLP 2021 shared task on sarcasm and sentiment detection in <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>. The shared task has two subtasks : sarcasm detection (subtask 1) and <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> (subtask 2). This shared task aims to promote and bring attention to Arabic sarcasm detection, which is crucial to improve the performance in other tasks such as <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a>. The dataset used in this shared task, namely ArSarcasm-v2, consists of 15,548 tweets labelled for <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a>, <a href="https://en.wikipedia.org/wiki/Sentimentality">sentiment</a> and <a href="https://en.wikipedia.org/wiki/Dialect">dialect</a>. We received 27 and 22 submissions for subtasks 1 and 2 respectively. Most of the approaches relied on using and fine-tuning pre-trained language models such as AraBERT and MARBERT. The top achieved results for the sarcasm detection and sentiment analysis tasks were 0.6225 <a href="https://en.wikipedia.org/wiki/F-number">F1-score</a> and 0.748 <a href="https://en.wikipedia.org/wiki/F-number">F1-PN</a> respectively.</abstract>
      <url hash="b17dc947">2021.wanlp-1.36</url>
      <bibkey>abu-farha-etal-2021-overview</bibkey>
      <pwccode url="https://github.com/iabufarha/arsarcasm-v2" additional="false">iabufarha/arsarcasm-v2</pwccode>
    </paper>
    <paper id="38">
      <title>Sarcasm and Sentiment Detection In Arabic Tweets Using BERT-based Models and Data Augmentation<fixed-case>A</fixed-case>rabic Tweets Using <fixed-case>BERT</fixed-case>-based Models and Data Augmentation</title>
      <author><first>Abeer</first><last>Abuzayed</last></author>
      <author><first>Hend</first><last>Al-Khalifa</last></author>
      <pages>312–317</pages>
      <abstract>In this paper, we describe our efforts on the shared task of sarcasm and sentiment detection in <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> (Abu Farha et al., 2021). The shared <a href="https://en.wikipedia.org/wiki/Task_(computing)">task</a> consists of two sub-tasks : Sarcasm Detection (Subtask 1) and <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> (Subtask 2). Our experiments were based on fine-tuning seven BERT-based models with <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> to solve the imbalanced data problem. For both tasks, the MARBERT BERT-based model with <a href="https://en.wikipedia.org/wiki/Data_augmentation">data augmentation</a> outperformed other models with an increase of the <a href="https://en.wikipedia.org/wiki/F-score">F-score</a> by 15 % for both tasks which shows the effectiveness of our approach.</abstract>
      <url hash="0d407008">2021.wanlp-1.38</url>
      <bibkey>abuzayed-al-khalifa-2021-sarcasm</bibkey>
    </paper>
    <paper id="39">
      <title>Multi-task Learning Using a Combination of Contextualised and Static Word Embeddings for Arabic Sarcasm Detection and Sentiment Analysis<fixed-case>A</fixed-case>rabic Sarcasm Detection and Sentiment Analysis</title>
      <author><first>Abdullah I.</first><last>Alharbi</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>318–322</pages>
      <abstract>Sarcasm detection and <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> are important tasks in <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">Natural Language Understanding</a>. Sarcasm is a type of <a href="https://en.wikipedia.org/wiki/Emotional_expression">expression</a> where the sentiment polarity is flipped by an interfering factor. In this study, we exploited this relationship to enhance both tasks by proposing a multi-task learning approach using a combination of static and contextualised embeddings. Our proposed system achieved the best result in the sarcasm detection subtask.</abstract>
      <url hash="b3a69c39">2021.wanlp-1.39</url>
      <bibkey>alharbi-lee-2021-multi</bibkey>
    </paper>
    <paper id="41">
      <title>Sarcasm and Sentiment Detection in <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> : investigating the interest of character-level features<fixed-case>A</fixed-case>rabic: investigating the interest of character-level features</title>
      <author><first>Dhaou</first><last>Ghoul</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <pages>329–333</pages>
      <abstract>We present three <a href="https://en.wikipedia.org/wiki/Methodology">methods</a> developed for the Shared Task on Sarcasm and Sentiment Detection in <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a>. We present a <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> that uses character n-gram features. We also propose two more sophisticated methods : a <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network</a> with a word level representation and an <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble classifier</a> relying on word and character-level features. We chose to present results from an <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble classifier</a> but it was not very successful as compared to the best systems : 22th/37 on sarcasm detection and 15th/22 on sentiment detection. It finally appeared that our <a href="https://en.wikipedia.org/wiki/Baseline_(configuration_management)">baseline</a> could have been improved and beat those results.</abstract>
      <url hash="7ce24f47">2021.wanlp-1.41</url>
      <attachment type="Dataset" hash="f170ae85">2021.wanlp-1.41.Dataset.pdf</attachment>
      <attachment type="Software" hash="595b041b">2021.wanlp-1.41.Software.zip</attachment>
      <bibkey>ghoul-lejeune-2021-sarcasm</bibkey>
    </paper>
    <paper id="44">
      <title>SarcasmDet at Sarcasm Detection Task 2021 in Arabic using AraBERT Pretrained Model<fixed-case>S</fixed-case>arcasm<fixed-case>D</fixed-case>et at Sarcasm Detection Task 2021 in <fixed-case>A</fixed-case>rabic using <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case> Pretrained Model</title>
      <author><first>Dalya</first><last>Faraj</last></author>
      <author><first>Dalya</first><last>Faraj</last></author>
      <author><first>Malak</first><last>Abdullah</last></author>
      <pages>345–350</pages>
      <abstract>This paper presents one of the top five winning solutions for the Shared Task on Sarcasm and Sentiment Detection in <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> (Subtask-1 Sarcasm Detection). The goal of the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> is to identify whether a tweet is sarcastic or not. Our solution has been developed using <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble technique</a> with AraBERT pre-trained model. We describe the architecture of the submitted <a href="https://en.wikipedia.org/wiki/Solution">solution</a> in the <a href="https://en.wikipedia.org/wiki/Task_(computing)">shared task</a>. We also provide the experiments and the hyperparameter tuning that lead to this result. Besides, we discuss and analyze the results by comparing all the <a href="https://en.wikipedia.org/wiki/Mathematical_model">models</a> that we trained or tested to achieve a better score in a table design. Our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> is ranked fifth out of 27 teams with an F1 score of 0.5985. It is worth mentioning that our <a href="https://en.wikipedia.org/wiki/Mathematical_model">model</a> achieved the highest <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy score</a> of 0.7830</abstract>
      <url hash="6e3bf51c">2021.wanlp-1.44</url>
      <bibkey>faraj-etal-2021-sarcasmdet</bibkey>
    </paper>
    <paper id="45">
      <title>Sarcasm and Sentiment Detection in <a href="https://en.wikipedia.org/wiki/Arabic">Arabic language</a> A Hybrid Approach Combining Embeddings and Rule-based Features<fixed-case>A</fixed-case>rabic language A Hybrid Approach Combining Embeddings and Rule-based Features</title>
      <author><first>Kamel</first><last>Gaanoun</last></author>
      <author><first>Imade</first><last>Benelallam</last></author>
      <pages>351–356</pages>
      <abstract>This paper presents the ArabicProcessors team’s system designed for sarcasm (subtask 1) and sentiment (subtask 2) detection shared task. We created a hybrid system by combining rule-based features and both static and dynamic embeddings using <a href="https://en.wikipedia.org/wiki/Linear_transform">transformers</a> and <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>. The system’s architecture is an ensemble of <a href="https://en.wikipedia.org/wiki/Naive_bayes">Naive bayes</a>, MarBERT and Mazajak embedding. This process scored an F1-score of 51 % on <a href="https://en.wikipedia.org/wiki/Sarcasm">sarcasm</a> and 71 % for sentiment detection.</abstract>
      <url hash="63112aaf">2021.wanlp-1.45</url>
      <bibkey>gaanoun-benelallam-2021-sarcasm</bibkey>
    </paper>
    <paper id="50">
      <title>iCompass at Shared Task on <a href="https://en.wikipedia.org/wiki/Sarcasm">Sarcasm</a> and Sentiment Detection in Arabic<fixed-case>C</fixed-case>ompass at Shared Task on Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Malek</first><last>Naski</last></author>
      <author><first>Abir</first><last>Messaoudi</last></author>
      <author><first>Hatem</first><last>Haddad</last></author>
      <author><first>Moez</first><last>BenHajhmida</last></author>
      <author><first>Chayma</first><last>Fourati</last></author>
      <author><first>Aymen</first><last>Ben Elhaj Mabrouk</last></author>
      <pages>381–385</pages>
      <abstract>We describe our submitted <a href="https://en.wikipedia.org/wiki/System">system</a> to the 2021 Shared Task on Sarcasm and Sentiment Detection in <a href="https://en.wikipedia.org/wiki/Arabic">Arabic</a> (Abu Farha et al., 2021). We tackled both subtasks, namely Sarcasm Detection (Subtask 1) and <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> (Subtask 2). We used state-of-the-art pretrained contextualized text representation models and fine-tuned them according to the downstream task in hand. As a first approach, we used Google’s multilingual BERT and then other Arabic variants : AraBERT, ARBERT and MARBERT. The results found show that MARBERT outperforms all of the previously mentioned models overall, either on Subtask 1 or Subtask 2.</abstract>
      <url hash="180c2655">2021.wanlp-1.50</url>
      <bibkey>naski-etal-2021-icompass</bibkey>
    </paper>
    <paper id="53">
      <title>AraBERT and Farasa Segmentation Based Approach For Sarcasm and Sentiment Detection in Arabic Tweets<fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case> and Farasa Segmentation Based Approach For Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic Tweets</title>
      <author><first>Anshul</first><last>Wadhawan</last></author>
      <pages>395–400</pages>
      <abstract>This paper presents our strategy to tackle the EACL WANLP-2021 Shared Task 2 : Sarcasm and Sentiment Detection. One of the subtasks aims at developing a <a href="https://en.wikipedia.org/wiki/System">system</a> that identifies whether a given <a href="https://en.wikipedia.org/wiki/Twitter">Arabic tweet</a> is sarcastic in nature or not, while the other aims to identify the sentiment of the <a href="https://en.wikipedia.org/wiki/Twitter">Arabic tweet</a>. We approach the <a href="https://en.wikipedia.org/wiki/Task_(project_management)">task</a> in two steps. The first step involves pre processing the provided <a href="https://en.wikipedia.org/wiki/Data_set">dataset</a> by performing insertions, deletions and segmentation operations on various parts of the text. The second step involves experimenting with multiple variants of two transformer based models, AraELECTRA and AraBERT. Our final approach was ranked seventh and fourth in the Sarcasm and Sentiment Detection subtasks respectively.</abstract>
      <url hash="9d5e7f8b">2021.wanlp-1.53</url>
      <bibkey>wadhawan-2021-arabert</bibkey>
    </paper>
  </volume>
</collection>