<collection id="2021.sustainlp">
  <volume id="1" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</booktitle>
      <publisher>Association for Computational Linguistics</publisher>
      <editor><first>Nafise Sadat</first><last>Moosavi</last></editor>
      <editor><first>Iryna</first><last>Gurevych</last></editor>
      <editor><first>Angela</first><last>Fan</last></editor>
      <editor><first>Thomas</first><last>Wolf</last></editor>
      <editor><first>Yufang</first><last>Hou</last></editor>
      <editor><first>Ana</first><last>Marasovi&#263;</last></editor>
      <editor><first>Sujith</first><last>Ravi</last></editor>
      <address>Virtual</address>
      <month>November</month>
      <year>2021</year>
    </meta>
    <frontmatter>
      <url hash="3928e276">2021.sustainlp-1.0</url>
      <bibkey>sustainlp-2021-simple</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Low Resource Quadratic Forms for Knowledge Graph Embeddings</title>
      <author><first>Zachary</first><last>Zhou</last></author>
      <author><first>Jeffery</first><last>Kline</last></author>
      <author><first>Devin</first><last>Conathan</last></author>
      <author><first>Glenn</first><last>Fung</last></author>
      <pages>1&#8211;10</pages>
      <abstract>We address the problem of <i>link prediction</i> between entities and relations of knowledge graphs. State of the art techniques that address this problem, while increasingly accurate, are computationally intensive. In this paper we cast link prediction as a sparse convex program whose solution defines a quadratic form that is used as a ranking function. The structure of our convex program is such that standard support vector machine software packages, which are numerically robust and efficient, can solve it. We show that on benchmark data sets, our model&#8217;s performance is competitive with state of the art models, but training times can be reduced by a factor of 40 using only CPU-based (and not GPU-accelerated) computing resources. This approach may be suitable for applications where balancing the demands of graph completion performance against computational efficiency is a desirable trade-off.</abstract>
      <url hash="ada90a48">2021.sustainlp-1.1</url>
      <attachment type="Software" hash="5d29bbd1">2021.sustainlp-1.1.Software.zip</attachment>
      <bibkey>zhou-etal-2021-low</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.1</doi>
    </paper>
    <paper id="3">
      <title>Limitations of Knowledge Distillation for Zero-shot Transfer Learning</title>
      <author><first>Saleh</first><last>Soltan</last></author>
      <author><first>Haidar</first><last>Khan</last></author>
      <author><first>Wael</first><last>Hamza</last></author>
      <pages>22&#8211;31</pages>
      <abstract>Pretrained transformer-based encoders such as BERT have been demonstrated to achieve state-of-the-art performance on numerous NLP tasks. Despite their success, BERT style encoders are large in size and have high latency during inference (especially on CPU machines) which make them unappealing for many online applications. Recently introduced compression and distillation methods have provided effective ways to alleviate this shortcoming. However, the focus of these works has been mainly on monolingual encoders. Motivated by recent successes in zero-shot cross-lingual transfer learning using multilingual pretrained encoders such as mBERT, we evaluate the effectiveness of Knowledge Distillation (KD) both during pretraining stage and during fine-tuning stage on multilingual BERT models. We demonstrate that in contradiction to the previous observation in the case of monolingual distillation, in multilingual settings, distillation during pretraining is more effective than distillation during fine-tuning for zero-shot transfer learning. Moreover, we observe that distillation during fine-tuning may hurt zero-shot cross-lingual performance. Finally, we demonstrate that distilling a larger model (BERT Large) results in the strongest distilled model that performs best both on the source language as well as target languages in zero-shot settings.</abstract>
      <url hash="40ec1cfc">2021.sustainlp-1.3</url>
      <bibkey>soltan-etal-2021-limitations</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.3</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="7">
      <title>Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering</title>
      <author><first>Georgios</first><last>Sidiropoulos</last></author>
      <author><first>Nikos</first><last>Voskarides</last></author>
      <author><first>Svitlana</first><last>Vakulenko</last></author>
      <author><first>Evangelos</first><last>Kanoulas</last></author>
      <pages>58&#8211;63</pages>
      <abstract>In simple open-domain question answering (QA), dense retrieval has become one of the standard approaches for retrieving the relevant passages to infer an answer. Recently, dense retrieval also achieved state-of-the-art results in multi-hop QA, where aggregating information from multiple pieces of information and reasoning over them is required. Despite their success, dense retrieval methods are computationally intensive, requiring multiple GPUs to train. In this work, we introduce a hybrid (lexical and dense) retrieval approach that is highly competitive with the state-of-the-art dense retrieval models, while requiring substantially less computational resources. Additionally, we provide an in-depth evaluation of dense retrieval methods on limited computational resource settings, something that is missing from the current literature.</abstract>
      <url hash="0132b005">2021.sustainlp-1.7</url>
      <bibkey>sidiropoulos-etal-2021-combining</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.7</doi>
      <pwccode url="https://github.com/gsidiropoulos/hybrid_retrieval_for_efficient_qa" additional="false">gsidiropoulos/hybrid_retrieval_for_efficient_qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
    <paper id="8">
      <title>Learning to Rank in the Age of Muppets: Effectiveness&#8211;Efficiency Tradeoffs in Multi-Stage Ranking</title>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>ChengCheng</first><last>Hu</last></author>
      <author><first>Yuqi</first><last>Liu</last></author>
      <author><first>Hui</first><last>Fang</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>64&#8211;73</pages>
      <abstract>It is well known that rerankers built on pretrained transformer models such as BERT have dramatically improved retrieval effectiveness in many tasks. However, these gains have come at substantial costs in terms of efficiency, as noted by many researchers. In this work, we show that it is possible to retain the benefits of transformer-based rerankers in a multi-stage reranking pipeline by first using feature-based learning-to-rank techniques to reduce the number of candidate documents under consideration without adversely affecting their quality in terms of recall. Applied to the MS MARCO passage and document ranking tasks, we are able to achieve the same level of effectiveness, but with up to 18&#215; increase in efficiency. Furthermore, our techniques are orthogonal to other methods focused on accelerating transformer inference, and thus can be combined for even greater efficiency gains. A higher-level message from our work is that, even though pretrained transformers dominate the modern IR landscape, there are still important roles for &#8220;traditional&#8221; LTR techniques, and that we should not forget history.</abstract>
      <url hash="88a71479">2021.sustainlp-1.8</url>
      <bibkey>zhang-etal-2021-learning-rank</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.8</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="13">
      <title>Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing</title>
      <author><first>Haoyu</first><last>He</last></author>
      <author><first>Xingjian</first><last>Shi</last></author>
      <author><first>Jonas</first><last>Mueller</last></author>
      <author><first>Sheng</first><last>Zha</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <author><first>George</first><last>Karypis</last></author>
      <pages>119&#8211;133</pages>
      <abstract>Knowledge Distillation (KD) offers a natural way to reduce the latency and memory/energy usage of massive pretrained models that have come to dominate Natural Language Processing (NLP) in recent years. While numerous sophisticated variants of KD algorithms have been proposed for NLP applications, the key factors underpinning the optimal distillation performance are often confounded and remain unclear. We aim to identify how different components in the KD pipeline affect the resulting performance and how much the optimal KD pipeline varies across different datasets/tasks, such as the data augmentation policy, the loss function, and the intermediate representation for transferring the knowledge between teacher and student. To tease apart their effects, we propose Distiller, a meta KD framework that systematically combines a broad range of techniques across different stages of the KD pipeline, which enables us to quantify each component&#8217;s contribution. Within Distiller, we unify commonly used objectives for distillation of intermediate representations under a universal mutual information (MI) objective and propose a class of MI-objective functions with better bias/variance trade-off for estimating the MI between the teacher and the student. On a diverse set of NLP datasets, the best Distiller configurations are identified via large-scale hyper-parameter optimization. Our experiments reveal the following: 1) the approach used to distill the intermediate representations is the most important factor in KD performance, 2) among different objectives for intermediate distillation, MI-performs the best, and 3) data augmentation provides a large boost for small training datasets or small student networks. Moreover, we find that different datasets/tasks prefer different KD algorithms, and thus propose a simple AutoDistiller algorithm that can recommend a good KD pipeline for a new dataset.</abstract>
      <url hash="71c94360">2021.sustainlp-1.13</url>
      <bibkey>he-etal-2021-distiller</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.13</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="14">
      <title>Shrinking Bigfoot: Reducing wav2vec 2.0 footprint</title>
      <author><first>Zilun</first><last>Peng</last></author>
      <author><first>Akshay</first><last>Budhkar</last></author>
      <author><first>Ilana</first><last>Tuil</last></author>
      <author><first>Jason</first><last>Levy</last></author>
      <author><first>Parinaz</first><last>Sobhani</last></author>
      <author><first>Raphael</first><last>Cohen</last></author>
      <author><first>Jumana</first><last>Nassour</last></author>
      <pages>134&#8211;141</pages>
      <abstract>Wav2vec 2.0 is a state-of-the-art speech recognition model which maps speech audio waveforms into latent representations. The largest version of wav2vec 2.0 contains 317 million parameters. Hence, the inference latency of wav2vec 2.0 will be a bottleneck in production, leading to high costs and a significant environmental footprint. To improve wav2vec&#8217;s applicability to a production setting, we explore multiple model compression methods borrowed from the domain of large language models. Using a teacher-student approach, we distilled the knowledge from the original wav2vec 2.0 model into a student model, which is 2 times faster, 4.8 times smaller than the original model. More importantly, the student model is 2 times more energy efficient than the original model in terms of CO2 emission. This increase in performance is accomplished with only a 7% degradation in word error rate (WER). Our quantized model is 3.6 times smaller than the original model, with only a 0.1% degradation in WER. To the best of our knowledge, this is the first work that compresses wav2vec 2.0.</abstract>
      <url hash="b90fb3f6">2021.sustainlp-1.14</url>
      <bibkey>peng-etal-2021-shrinking</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.14</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="17">
      <title>Unsupervised Contextualized Document Representation</title>
      <author><first>Ankur</first><last>Gupta</last></author>
      <author><first>Vivek</first><last>Gupta</last></author>
      <pages>166&#8211;173</pages>
      <abstract>Several NLP tasks need the effective repre-sentation of text documents.Arora et al.,2017 demonstrate that simple weighted aver-aging of word vectors frequently outperformsneural models. SCDV (Mekala et al., 2017)further extends this from sentences to docu-ments by employing soft and sparse cluster-ing over pre-computed word vectors. How-ever, both techniques ignore the polysemyand contextual character of words.In thispaper, we address this issue by proposingSCDV+BERT(ctxd), a simple and effective un-supervised representation that combines con-textualized BERT (Devlin et al., 2019) basedword embedding for word sense disambigua-tion with SCDV soft clustering approach. Weshow that our embeddings outperform origi-nal SCDV, pre-train BERT, and several otherbaselines on many classification datasets. Wealso demonstrate our embeddings effective-ness on other tasks, such as concept match-ing and sentence similarity.In addition,we show that SCDV+BERT(ctxd) outperformsfine-tune BERT and different embedding ap-proaches in scenarios with limited data andonly few shots examples.</abstract>
      <url hash="d9b2a07c">2021.sustainlp-1.17</url>
      <bibkey>gupta-gupta-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.sustainlp-1.17</doi>
      <pwccode url="https://github.com/vgupta123/contextualize_scdv" additional="false">vgupta123/contextualize_scdv</pwccode>
    </paper>
  </volume>
</collection>